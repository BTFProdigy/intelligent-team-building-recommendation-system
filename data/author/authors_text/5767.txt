Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 523?532,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Japanese Predicate Argument Structure Analysis using Decision Lists
Hirotoshi Taira, Sanae Fujita, Masaaki Nagata
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho,
Keihanna Science City,
Kyoto 619-0237, Japan
{{taira,sanae}@cslab.kecl, nagata.masaaki@lab}.ntt.co.jp
Abstract
This paper describes a new automatic method
for Japanese predicate argument structure
analysis. The method learns relevant features
to assign case roles to the argument of the tar-
get predicate using the features of the words
located closest to the target predicate under
various constraints such as dependency types,
words, semantic categories, parts of speech,
functional words and predicate voices. We
constructed decision lists in which these fea-
tures were sorted by their learned weights. Us-
ing our method, we integrated the tasks of se-
mantic role labeling and zero-pronoun iden-
tification, and achieved a 17% improvement
compared with a baseline method in a sen-
tence level performance analysis.
1 Introduction
Recently, predicate argument structure analysis has
attracted the attention of researchers because this
information can increase the precision of text pro-
cessing tasks, such as machine translation, informa-
tion extraction (Hirschman et al, 1999), question
answering (Narayanan and Harabagiu, 2004) (Shen
and Lapata, 2007), and summarization (Melli et
al., 2005). In English predicate argument structure
analysis, large corpora such as FrameNet (Fillmore
et al, 2001), PropBank (Palmer et al, 2005) and
NomBank (Meyers et al, 2004) have been created
and utilized. Recently, the GDA Corpus (Hashida,
2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al,
2002) and NAIST Text Corpus (Iida et al, 2007)
were constructed in Japanese, and these corpora
have become the target of an automatic Japanese
predicate argument structure analysis system. We
conducted Japanese predicate argument structure
(PAS) analysis for the NAIST Text Corpus, which
is the largest of these three corpora, and, as far as
we know, this is the first time PAS analysis has been
conducted for whole articles of the corpus.
The NAIST Text Corpus has the following char-
acteristics, i) semantic roles for both predicates and
event nouns are annotated in the corpus, ii) three ma-
jor case roles,1 namely the ga, wo and ni-cases in
Japanese are annotated for the base form of pred-
icates and event nouns, iii) both the case roles in
sentences containing the target predicates and those
outside the sentences (zero-pronouns) are annotated,
and iv) coreference relations are also annotated.
As regards i), recently there has been an increase
in the number of papers dealing with nominalized
predicates (Pradhan et al, 2004) (Jiang and Ng,
2006) (Xue, 2006) (Liu and Ng, 2007). For exam-
ple, ?trip? in the sentence ?During my trip to Italy, I
met him.? refers not only to the event ?I met him?
but also to the event ?I traveled to Italy.? As in this
example, nouns sometimes have argument structures
referring to an event. Such nouns are called event
nouns (Komachi et al, 2007) in the NAIST Text
Corpus. At the same time, the problems related to
compound nouns are also important. In Japanese, a
compound noun sometimes simultaneously contains
both an event noun and its arguments. For example,
the compound noun, ????? (corporate buyout)?
contains an event noun ??? (buyout)? and its ac-
cusative, ??? (corporate).? However, compound
1Kyoto Text Corpus has about 15 case roles.
523
nouns provide no information about syntactic de-
pendency or about case markers, so it is difficult to
specify the predicate-argument structure. Komachi
et al investigated the argument structure of event
nouns using the co-occurrence of target nouns and
their case roles in the same sentence (Komachi et
al., 2007). In these approaches, predicates and event
nouns are dealt with separately. Here, we try to
unify these different argument structures using de-
cision lists.
As regards ii), for example, in the causative sen-
tence, ???????????????? (Mary
makes Tom fix dinner),? the basic form of the
causative verb, ????? (make fix)? is ??? (fix),?
and its nominative is ??? (Tom)? and the ac-
cusative case role (wo-case) is ??? (dinner),? al-
though the surface case particle is ni (dative). We
must deal with syntactic transformations in passive,
causative, and benefactive constructions when ana-
lyzing the corpus.
As regards iii) and iv), in Japanese, zero pronouns
often occur, especially when the argument has al-
ready been mentioned in previous sentences. There
have been many studies of zero-pronoun identifica-
tion (Walker et al, 1994) (Nakaiwa, 1997) (Iida et
al., 2006).
In this paper, we present a general procedure for
handling both the case role assignment of predicates
and event nouns, and zero-pronoun identification.
We use the decision list learning of rules to find the
closest words with various constraints, because with
decision lists the readability of learned lists is high
and the learning is fast.
The rest of this paper is organized as follows. We
describe the NAIST Text Corpus, which is our tar-
get corpus in Section 2. We describe our proposed
method in Section 3. The result of experiments us-
ing the NAIST Text Corpus and our method are re-
ported in Section 4 and our conclusions are provided
in Section 5.
2 NAIST Text Corpus
In the NAIST Text Corpus, three major obligatory
Japanese case roles are annotated, namely the ga-
case (nominative or subjective case), the wo-case
(accusative or direct object) and the ni-case (da-
tive or in-direct object). The NAIST Text Corpus
is based on the Kyoto Text Corpus Ver. 3.0, which
contains 38,384 sentences in 2,929 texts taken from
news articles and editorials in a Japanese newspaper,
the ?Mainichi Shinbun?.
We divided these case roles into four types by lo-
cation in the article as in (Iida et al, 2006), i) the
case role depends on the predicate or the predicate
depends on the case role in the intra-sentence (?de-
pendency relations?), ii) the case role does not de-
pend on the predicate and the predicate does not de-
pend on the case role in the intra-sentence (?zero-
anaphoric (intra-sentential)?), iii) the case role is
not in the sentence containing the predicate (?zero-
anaphoric (inter-sentential)?), and iv) the case role
and the predicate are in the same phrase (?in same
phrase?). Here, we do not deal with exophora.
We show the distribution of the above four types
in test samples in our split of the NAIST Text
Corpus in Tables 1 and 2. In predicates, the
?dependency relations? type in the wo-case and
the ni-case occur frequently. In event nouns,
the ?zero-anaphoric (intra-sentential)? and ?zero-
anaphoric (inter-sentential)? types in the ga-case oc-
cur frequently. With respect to the ?in same phrase?
type, the wo-case occurs frequently.
3 Predicate Argument Structure Analysis
using Features of Closest Words
In this section, we describe our algorithm. In the
algorithm, we used various constraints when search-
ing for the words located closest to the target predi-
cate. We described these constraints as features with
the direct products of dependency types (ic, oc, ga c,
wo c, ni c, sc, nc, fw and bw), generalization levels
(words, semantic categories, parts of speech), func-
tional words and voices.
3.1 Dependency Types
In Japanese, the functional words in a phrase (Bun-
setsu in Japanese) and the interdependency of bun-
setsu phrases are important for determining the
predicate argument structure. In accordance with
the character of the dependency between the case
roles and the predicates or event nouns, we divided
Japanese word dependency into the following seven
types that cover all dependency types in Japanese.
Additionally, we use two optional dependency types.
524
Table 1: Distribution of case roles for predicates (Test Data)
predicate
ga (Nominative) wo (Accusative) ni (Dative)
all 15,996 (100.00%) 8,348 (100.00%) 4,871 (100.00%)
dependency relations 9,591 ( 59.96%) 7,184 ( 86.06%) 4,276 ( 87.78%)
zero-anaphoric (intra-sentential) 3,856 ( 24.11%) 870 ( 10.42%) 360 ( 7.39%)
zero-anaphoric (inter-sentential) 2,496 ( 15.60%) 225 ( 2.70%) 132 ( 2.71%)
in same phrase 53 ( 0.33%) 69 ( 0.83%) 103 ( 2.11%)
Table 2: Distribution of case roles for event nouns (Test Data)
event noun
ga (Nominative) wo (Accusative) ni (Dative)
all 4,099 (100.00%) 2,314 (100.00%) 423 (100.00%)
dependency relations 977 (23.84%) 648 (28.00%) 105 (24.82%)
zero-anaphoric (intra-sentential) 1,672 (40.79%) 348 (15.04%) 135 (31.91%)
zero-anaphoric (inter-sentential) 1,040 (25.37%) 165 (7.13%) 44 (10.40%)
in same phrase 410 (10.00%) 1,153 (49.83%) 139 (32.86%)
Figure 1: Type ic
3.1.1 Incoming Connection Type (ic)
With this type, the target case role is the head-
word of a bunsetsu phrase and the case role phrase
depends on the target predicate phrase (Figure 1).
3.1.2 Outgoing Connection Type (oc)
With this type, the target case role is the headword
of a phrase and a phrase containing a target predicate
or event noun depends on the case role phrase (Fig-
ure 2).
Figure 2: Type oc
525
Figure 3: Type sc
Figure 4: Type ga c, wo c, ni c
3.1.3 ?Within the Same Phrase? Type (sc)
With this type, the target case role and the target
predicate or event noun are in the same phrase (Fig-
ure 3).
3.1.4 ?Connection into Other Case role Types
(ga c, wo c, ni c)
With these types, a phrase containing the target
case role depends on a phrase containing another
predetermined case role (Figure 4). We use the terms
?ga c?, ?wo c? and ?ni c? when the predetermined
case roles are the ga-case, wo-case and ni-case, re-
spectively.
Figure 5: Type nc
3.1.5 Non-connection Type (nc)
With this type, a phrase containing the target case
role and a phrase containing the target predicate or
event noun are in the same article, but these phrases
do not depend on each other (Figure 5).
3.1.6 Optional Type (fw and bw)
Type fw and bw stand for ?forward? and ?back-
ward? types, respectively. Type fw means the word
located closest to the target predicate or event noun
without considering functional words or voices.
With fw, the word is located between the top of the
article containing the target predicate and the target
predicate or event noun. Similarly, type bw means
the word located closest to the target predicate or
noun, which is located between the targeted predi-
cate or event noun, and the tail of the article con-
taining the predicate.
3.2 Generalization Levels
We used three levels of generalization for every case
role candidate, that is, word, semantic category, and
part of speech. Every word is annotated with a part
of speech in the Kyoto Text Corpus, and we used
these annotations. With regard to semantic cate-
gories, we annotated every word with a semantic
category based on a Japanese thesaurus, Nihongo
Goi Taikei. The thesaurus consists of a hierarchy
of 2,710 semantic classes, defined for over 264,312
nouns, with a maximum depth of twelve (Ikehara et
al., 1997). We mainly used the semantic classes of
526
Figure 6: Top 3 levels of the Japanese thesaurus, ?Ni-
hongo Goi Taikei?
the third level, and partly the fourth level, which are
similar to semantic roles. We show the top three lev-
els of the Nihongo Goi Taikei common noun the-
saurus in Figure 6. We annotated the words with
their semantic category by hand.
3.3 Functional Word and Voice
We used a functional word in the phrase containing
the target case role and active and passive voices for
the predicate as base features.
3.4 Training Algorithm
The training algorithm used for our method is shown
in Figure 7. First, the algorithm constructs features
that search for the words located closest to the tar-
get predicate under various constraints. Next, the
algorithm learns by using linear Support Vector Ma-
chines (SVMs) (Vapnik, 1995). SVMs learn effec-
tive features by the one vs. rest method for every
case role. We used TinySVM 2 as an SVM imple-
mentation. Moreover, we construct decision lists
sorted by weight from linear SVMs. Finally, the al-
gorithm calculates the existing probabilities of case
roles for every predicate or event noun. This step
2http://chasen.org/t?aku/software/TinySVM/
produces the criterion that decides whether or not
we will determine the case roles when there is no in-
terdependency between the case role candidate and
the predicate.
Our split of the NAIST Text Corpus has only
62,264 training samples for 2,874 predicates, and we
predict that there will be a shortage of training sam-
ples when adopting traditional learning algorithms,
such as learning algorithms using entropy. So, we
used SVMs with a high generalization capability to
learn the decision lists.
3.5 Test Algorithm
The test algorithm of our method is shown in Fig-
ure 8. In the test phase, we analyzed test samples
using decision lists and the existing probabilities of
case roles learned in the training phase. In step 1, we
determined case roles using a decision list consisting
of features exhibiting case role and predicate inter-
dependency, that is, ic, oc, ga c, wo c, and ni c. This
is because there are many cases in Japanese where
the syntactic constraint is stronger than the seman-
tic constraint when we determine the case roles. In
step 2, we determined case roles using a decision list
of sc (?in same phrase?) for the case roles that were
not determined in step 1. This step was mainly for
event nouns. Japanese event nouns frequently form
compound nouns that contain case roles. In step 3,
we decided whether or not to proceed to the next
step by using the existing probabilities of case roles.
If the probability was less than a certain threshold
(50%), then the algorithm stopped. In step 4, we de-
termined case roles using a decision list of the fea-
tures that have no interdependency, that is, nc, fw
and bw. This step will be executed when the target
case role is syntactically necessary and determined
by the co-occurrence of the case roles and predicate
or event noun without syntactic clues, such as de-
pendency, functional words and voices.
4 Experimental Results
4.1 Experimental Setting
We performed our experiments using the NAIST
Text Corpus 1.4? (Iida et al, 2007). We used
49,527 predicates and 12,737 event nouns from arti-
cles published from January 1st to January 11th and
the editorials from January to August as training ex-
527
for each predicate pi in all predicates appeared in the training corpus do
feature list(pi) = {} ; n ? 0
clear (x, y)
for each instance pij of pi, in the training corpus do
Clear order() for all features
aij ? the article including pij
Wij ? the number of words in aij
pred index ? the word index of pij in aij
for (m = pred index? 1; m ? 1; m??) do
n + +
dep type = get dependency type(wm, pij)
if dep type == ?ic?, ?nc?, ?ga c?, ?wo c? or ?ni c? then inc order(n, dep type, wm, pij)
else if dep type == ?sc? then inc order(n, dep type, ??, ??)
endif
inc order(n, ?fw?, ??, ??)
if wm is the ga-case role then yn,ga ? 1 else yn,ga ? 0
if wm is the wo-case role then yn,wo ? 1 else yn,wo ? 0
if wm is the ni-case role then yn,ni ? 1 else yn,ni ? 0
end for
for (m = pred index + 1; m ? Wij ; m + +) do
n + +
dep type = get dependency type(wm, pij)
if dep type == ?oc?, ?nc?, ?ga c?, ?wo c? or ?ni c? then inc order(n, dep type, wm, pij)
else if dep type == ?sc? then inc order(n, dep type, ??, ??)
endif
inc order(n, ?bw?, ??, ??)
if wm is the ga-case role then yn,ga ? 1 else yn,ga ? 0
if wm is the wo-case role then yn,wo ? 1 else yn,wo ? 0
if wm is the ni-case role then yn,ni ? 1 else yn,ni ? 0
end for
end for
Learn linear SVMs using (x1, y1,ga), ..., (xn, yn,ga)
Learn linear SVMs using (x1, y1,wo), ..., (xn, yn,wo)
Learn linear SVMs using (x1, y1,ni), ..., (xn, yn,ni)
Make the decision list for pi, sorting features by weight.
Calculate the existing probabilities of case roles for pi.
end for
procedure get dependency type(wm, pij)
if phrase(wm) depends on phrase(pij) then return ?ic?
else if phrase(pij) depends on phrase(wm) then return ?oc?
else if phrase(wm) depends on phrase(pga) then return ?ga c?
else if phrase(wm) depends on phrase(pwo) then return ?wo c?
else if phrase(wm) depends on phrase(pni) then return ?ni c?
else if phrase(wm) equals phrase(pij) then return ?sc?
else return ?nc?
end procedure
procedure inc order(n, dep type, func, voice)
Set a feature fw = (wm, dep type, func, voice) ; order(fw)++ ; if order(fw) == 1 then xn,fw ? 1
Set a feature fs = (sem(wm), dep type, func, voice) ; order(fs)++ ; if order(fs) == 1 then xn,fs ? 1
Set a feature fp = (pos(wm), dep type, func, voice) ; order(fp)++ ; if order(fp) == 1 then xn,fp ? 1
feature list(pi) ? feature list(pi)
?
{fw, fs, fp}
end procedure
Figure 7: Training algorithm
528
Step 1. Determine case roles using a decision list concerning ic, oc, ga c, wo c and ni c.
Step 2. Determine case roles using a decision list concerning sc for undetermined case roles in
Step.1.
Step 3. If the existing probability of case roles < 50 % then the program ends.
Step 4. Determine case roles using a decision list concerning nc, fw and bw types.
Figure 8: Test algorithm
amples. We used 11,023 predicates and 3,161 event
nouns from articles published on January 12th and
13th and the September editorials as development
examples. And we used 19,501 predicate and 5,276
event nouns from articles dated January 14th to 17th
and editorials dated October to December as test ex-
amples. This is a typical way to split the data.
We used the annotations in the Kyoto Text Corpus
as the interdependency of bunsetsu phrases. We used
both individual and multiple words as case roles. We
used the phrase boundaries annotated in the NAIST
Text Corpus in the training phase, and used those
annotated automatically by our system using POSs
and simple rules in the test phase. The accuracy of
the automatic annotation is about 90%.
4.2 Baseline Method
To evaluate our algorithm, we conducted experi-
ments using a baseline method. With the method,
we used only nouns that depended on predicates or
event nouns as case role candidates. If the functional
word (post-positional case) in the phrase is ?ga?,?wo?
and ?ni?, we determined the ga-case, wo-case, or ni-
case for the candidates. Next, as regards event nouns
in compound nouns, if there was another word in a
compound noun containing an event noun and it co-
occurred with the event noun as a case role with a
higher probability in the training samples, then the
word was selected for the case role.
4.3 Entropy Method
The conventional approach for making decision lists
utilizes the entropy of samples selected by the
rules (Yarowsky, 1994) (Goodman, 2002). We per-
formed comparative experiments using Yarowsky?s
entropy algorithm (Yarowsky, 1994).
Table 3: Existing probabilities of case roles for predicates
and event nouns
Predicate Existing Probability
or Event Noun ga (NOM) wo (ACC) ni (DAT)
?? (use) 44.72% 82.92% 5.33%
?? (negotiation) 77.41% 30.70% 0.00%
?? (participation) 87.09% 0.00% 72.46%
??? (based on) 81.89% 0.00% 100.00%
4.4 Overall Results
The overall results are shown in Table 7. Here, ?en-
tropy? indicates Yarowsky?s algorithm, which uses
entropy (Yarowsky, 1994). Throughout the test data,
the F-measure (%) of our method exceeded that of
the baseline system and the ?entropy? system. With
the ga-case (nominative) in particular, the F-measure
increased 9 points.
Table 3 shows some examples of the existing
probabilities of case roles for predicates or event
nouns. When the probabilities are extreme values
such as the ni-case (dative) of?? (negotiation), the
wo-case (accusative) of?? (participation), and the
wo-case and ni-base of ??? (based on), we can
decide to fill the targeted case role or not with high
precision. However, it is difficult to decide to fill
the targeted case role or not when the probability is
close to 50 percent as in the ga-case of?? (use).
We show the learned decision list of the ic type
(the case role depends on the predicate or event
noun), sc type (in the same phrase) and the other
types for event noun?? (negotiation) in Tables 4, 5
and 6, respectively. Here, ?word? in the ?level?
column means ?base form of predicate? and ?sem?
means ?semantic category of predicate.? In the ic
and sc type decision lists, features with semantic
categories, such as ?REGION?, ?LOCATION? and
?EVENT?, occupy a higher order. In contrast, in
the list of the other types, the features that occupy
the higher order are the features of the word base
529
Table 4: Decision list for ic type of event noun?? (negotiation)
order case dep type level head word functional voice weight
word
1 ga ic word ???????? (North Korea) ? (of) active 0.9820
2 ga ic sem ?? (REGION) ? (of) active 0.6381
3 ga ic word ???? (both Japan and U.S.) ? (of) active 0.5502
4 wo ic word ?????? (establishment of joint ventures) ? (of) active 0.5288
5 wo ic word ?????? (telecommunications) ? (of) active 0.4142
6 wo ic word ???????? (North Korea) ?? (for) active 0.3168
7 wo ic word ?? (ACTION) ? (of) active 0.3083
8 ga ic sem ???? (OOV NOUN) ? (of) active 0.2939
9 wo ic word ????????? (car and auto parts sector) ? (of) active 0.2775
10 wo ic sem ? (LOCATION) ? (of) active 0.2471
Table 5: Decision list for sc type of event noun?? (negotiation)
order case dep type level head word weight
1 wo sc sem ?? (EVENT) 1.1738
2 wo sc word ?? (arrangement) 1.0000
3 ga sc word ???? (airline of Japan and China) 0.9392
4 wo sc sem ?? (MENTAL STATE) 0.8958
5 ga sc word ?????????? (financial services of Japan and U.S.) 0.8371
6 wo sc word ???? (contract extension) 0.7870
7 wo sc word ?? (joint venture) 0.7865
8 wo sc word ????? (intellectual property rights) 0.7224
9 wo sc word ??????? (car and auto parts) 0.7196
10 ga sc word ?? (Japan and North Korea) 0.6771
Table 6: Decision list for other types of event noun?? (negotiation)
order case dep type level head word functional word voice weight
1 ga fw word ?? (Japan and U.S.) 1.9954
2 ga fw word ?? (Taiwan) 1.9952
3 ga fw word ?? (U.S. and North Korea) 1.4979
4 ga fw word ?? (U.K. and China) 1.1773
5 ga nc word ?? (both nations) ? (TOP) active 1.1379
6 wo fw word ????? (diplomatic normalization) 1.0000
7 ga bw word ?? (U.S. and North Korea) 1.0000
8 ga fw word ?? (capital and labor) 1.0000
9 wo fw word ????? (automotive area) 1.0000
10 ga nc word ?? (both sides) ? (TOP) active 1.0000
Table 7: Overall results for NAIST Text Corpus (F-measure(%))
training data test data
sentence ga (NOM) wo (ACC) ni (DAT) sentence ga (NOM) wo (ACC) ni (DAT)
baseline 25.32 32.58 74.51 82.70 21.34 30.08 69.48 76.62
entropy 73.46 89.53 92.72 91.09 33.10 45.67 73.28 77.77
our method 64.81 86.76 92.52 92.20 38.06 55.07 75.82 80.45
530
Table 8: Results for predicates in test sets (F-measure(%))
baseline / our method
ga (Nominative) wo (Accusative) ni (Dative)
all 34.44 / 57.40 77.00 / 79.50 79.83 / 83.15
dependency relations 51.96 / 75.53 85.42 / 88.20 81.83 / 89.51
zero-anaphoric (intra-sentential) 0.00 / 30.15 0.00 / 11.41 0.00 / 3.66
zero-anaphoric (inter-sentential) 1.85 / 23.45 3.00 / 9.32 0.00 / 11.76
in same phrase 0.00 / 75.00 0.00 / 51.78 0.00 / 84.65
Table 9: Results for event nouns (F-measure(%))
baseline / our method
ga (Nominative) wo (Accusative) ni (Dative)
all 11.05 / 45.64 32.30 / 61.80 20.85 / 38.88
dependency relations 12.98 / 68.01 25.00 / 62.46 40.00 / 56.05
zero-anaphoric (intra-sentential) 0.00 / 36.19 0.00 / 20.46 0.00 / 6.62
zero-anaphoric (inter-sentential) 1.40 / 23.25 1.06 / 10.37 0.00 / 3.51
in same phrase 58.76 / 78.93 47.44 / 77.96 28.91 / 58.13
form. This means local knowledge of relations be-
tween case roles and predicates or event nouns in
the word level is more important than semantic level
knowledge.
4.5 Results for Predicates in Test Sets
We show the results we obtained for predicates in
Table 8. The results reveal that our method is supe-
rior to the baseline system. Our algorithm is partic-
ularly effective in the ga-case.
4.6 Results for Event Nouns in Test Sets
We show the results we obtained for event nouns in
Table 9. This also shows that our method is superior
to the baseline system. The precision with sc type
is high and our method is effective as regards event
nouns.
5 Conclusion
We presented a new method for Japanese automatic
predicate argument structure analysis using deci-
sion lists based on the features of the words located
closest to the target predicate under various con-
straints. The method learns the relative weights of
these different features for case roles and ranks them
using decision lists. Using our method, we inte-
grated the knowledge of case role determination and
zero-pronoun identification, and generally achieved
a high precision in Japanese PAS analysis. In par-
ticular, we can extract knowledge at various levels
from the corpus for event nouns. In future, we will
use richer constraints and research better ways of
distinguishing whether or not cases are obligatory.
Acknowledgments
We thank Ryu Iida and Yuji Matsumoto of NAIST
for the definitions of the case roles in the NAIST
Text Corpus and functional words, and Franklin
Chang for valuable comments.
References
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proc. of the Pacific Asian
Conference on Language, Information and Computa-
tion (PACLING).
Joshua Goodman. 2002. An incremental decision
list learner. In Proc. of the ACL-02 Conference
on Empirical Methods in Natural Language Process-
ing(EMNLP02), pages 17?24.
Kouichi Hashida. 2005. Global document annotation
(GDA) manual. http://i-content.org/GDA/.
Lynette Hirschman, Patricia Robinson, Lisa Ferro, Nancy
Chinchor, Erica Brown, Ralph Grishman, and Beth
Sundheim. 1999. Hub-4 Event?99 general guidelines.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proc. of the 21st International Confer-
531
ence on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 625?632.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Annotating a Japanese text corpus
with predicate-argument and coreference relations. In
Proc. of ACL 2007 Workshop on Linguistic Annota-
tion, pages 132?139.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Nihongo Goi
Taikei, A Japanese Lexicon. Iwanami Shoten, Tokyo.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. In Proc. of the Conference on Empirical Meth-
ods in Natural Language Processing.
Daisuke Kawahara, Sadao Kurohashi, and Koichi
Hashida. 2002. Construction of a Japanese relevance-
tagged corpus (in Japanese). Proc. of the 8th Annual
Meeting of the Association for Natural Language Pro-
cessing, pages 495?498.
Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Learning-based argument structure
analysis of event-nouns in Japanese. In Proc. of the
Conference of the Pacific Association for Computa-
tional Linguistics (PACLING), pages 120?128.
Chang Liu and Hwee Tou Ng. 2007. Learning predictive
structures for semantic role labeling of NomBank. In
Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 208?215.
Gabor Melli, Yang Wang, Yudong Liu, Mehdi M.
Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,
and Fred Popowich. 2005. Description of SQUASH,
the SFU question answering summary handler for the
DUC-2005 summarization task. In Proc. of DUC
2005.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In Proc. of HLT-NAACL 2004 Workshop on
Frontiers in Corpus Annotation.
Hiromi Nakaiwa. 1997. Automatic identification of zero
pronouns and their antecedents within aligned sen-
tence pairs. In Proc. of the 3rd Annual Meeting of
the Association for Natural Language Processing (in
Japanese).
Srini Narayanan and Sanda Harabagiu. 2004. Question
answering based on semantic structures. In Proc. of
the 20th International Conference on Computational
Linguistics (COLING).
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Sameer Pradhan, Waybe Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow seman-
tic parsing using support vector machines. In Proc.
of the Human Language Technology Conference/North
American Chapter of the Association of Computa-
tional Linguistics HLT/NAACL 2004.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proc. of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP/CoNLL), pages 12?21.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer-Verlag, New York.
M. Walker, M. Iida, and S. Cote. 1994. Japanese dis-
course and the process of centering. Computational
Linguistics, 20(2):193?233.
Nianwen Xue. 2006. Semantic role labeling of nomi-
nalized predicates in Chinese. In Proc. of the HLT-
NAACL, pages 431?438.
David Yarowsky. 1994. Decision lists for lexical am-
biguity resolution: Application to accent restoration
in Spanish and French. In Proc. of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 88?95.
532
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 713?720,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Clustered Global Phrase Reordering Model
for Statistical Machine Translation
Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Souraku-gun
Kyoto, 619-0237 Japan
nagata.masaaki@labs.ntt.co.jp,
Kuniko Saito
NTT Cyber Space Laboratories
1-1 Hikarinooka, Yokoshuka-shi
Kanagawa, 239-0847 Japan
saito.kuniko@labs.ntt.co.jp
Kazuhide Yamamoto, Kazuteru Ohashi  
Nagaoka University of Technology
1603-1, Kamitomioka, Nagaoka City
Niigata, 940-2188 Japan
ykaz@nlp.nagaokaut.ac.jp, ohashi@nlp.nagaokaut.ac.jp
Abstract
In this paper, we present a novel global re-
ordering model that can be incorporated
into standard phrase-based statistical ma-
chine translation. Unlike previous local
reordering models that emphasize the re-
ordering of adjacent phrase pairs (Till-
mann and Zhang, 2005), our model ex-
plicitly models the reordering of long dis-
tances by directly estimating the parame-
ters from the phrase alignments of bilin-
gual training sentences. In principle, the
global phrase reordering model is condi-
tioned on the source and target phrases
that are currently being translated, and
the previously translated source and tar-
get phrases. To cope with sparseness, we
use N-best phrase alignments and bilin-
gual phrase clustering, and investigate a
variety of combinations of conditioning
factors. Through experiments, we show,
that the global reordering model signifi-
cantly improves the translation accuracy
of a standard Japanese-English translation
task.
1 Introduction
Global reordering is essential to the translation of
languages with different word orders. Ideally, a
model should allow the reordering of any distance,
because if we are to translate from Japanese to En-
glish, the verb in the Japanese sentence must be
moved from the end of the sentence to the begin-
ning just after the subject in the English sentence.

Graduated in March 2006
Standard phrase-based translation systems use
a word distance-based reordering model in which
non-monotonic phrase alignment is penalized
based on the word distance between successively
translated source phrases without considering the
orientation of the phrase alignment or the identi-
ties of the source and target phrases (Koehn et al,
2003; Och and Ney, 2004). (Tillmann and Zhang,
2005) introduced the notion of a block (a pair of
source and target phrases that are translations of
each other), and proposed the block orientation
bigram in which the local reordering of adjacent
blocks are expressed as a three-valued orienta-
tion, namely Right (monotone), Left (swapped),
or Neutral. A block with neutral orientation is sup-
posed to be less strongly linked to its predecessor
block: thus in their model, the global reordering is
not explicitly modeled.
In this paper, we present a global reordering
model that explicitly models long distance re-
ordering1. It predicts four type of reordering
patterns, namely MA (monotone adjacent), MG
(monotone gap), RA (reverse adjacent), and RG
(reverse gap). There are based on the identities of
the source and target phrases currently being trans-
lated, and the previously translated source and tar-
get phrases. The parameters of the reordering
model are estimated from the phrase alignments of
training bilingual sentences. To cope with sparse-
ness, we use N-best phrase alignments and bilin-
gual phrase clustering.
In the following sections, we first describe the
global phrase reordering model and its param-
1It might be misleading to call our reordering model
?global? since it is at most considers two phrases. A truly
global reordering model would take the entire sentence struc-
ture into account.
713
eter estimation method including N-best phrase
alignments and bilingual phrase clustering. Next,
through an experiment, we show that the global
phrase reordering model significantly improves
the translation accuracy of the IWSLT-2005
Japanese-English translation task (Eck and Hori,
2005).
2 Baseline Translation Model
In statistical machine translation, the translation of
a source (foreign) sentence   is formulated as the
search for a target (English) sentence  that max-
imizes the conditional probability  
 	
, which
can be rewritten using the Bayes rule as,






 	

 
	 	
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 61?64, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Portable Translator Capable of Recognizing Characters on
Signboard and Menu Captured by Built-in Camera
Hideharu Nakajima, Yoshihiro Matsuo, Masaaki Nagata, Kuniko Saito
NTT Cyber Space Laboratories, NTT Corporation
Yokosuka, 239-0847, Japan
 
nakajima.hideharu, matsuo.yoshihiro, nagata.masaaki, saito.kuniko 
@lab.ntt.co.jp
Abstract
We present a portable translator that rec-
ognizes and translates phrases on sign-
boards and menus as captured by a built-
in camera. This system can be used on
PDAs or mobile phones and resolves the
difficulty of inputting some character sets
such as Japanese and Chinese if the user
doesn?t know their readings. Through the
high speed mobile network, small images
of signboards can be quickly sent to the
recognition and translation server. Since
the server runs state of the art recogni-
tion and translation technology and huge
dictionaries, the proposed system offers
more accurate character recognition and
machine translation.
1 Introduction
Our world contains many signboards whose phrases
provide useful information. These include destina-
tions and notices in transportation facilities, names
of buildings and shops, explanations at sightseeing
spots, and the names and prices of dishes in restau-
rants. They are often written in just the mother
tongue of the host country and are not always ac-
companied by pictures. Therefore, tourists must be
provided with translations.
Electronic dictionaries might be helpful in trans-
lating words written in European characters, because
key-input is easy. However, some character sets
such as Japanese and Chinese are hard to input if
the user doesn?t know the readings such as kana and
pinyin. This is a significant barrier to any translation
service. Therefore, it is essential to replace keyword
entry with some other input approach that supports
the user when character readings are not known.
One solution is the use of optical character recog-
nition (OCR) (Watanabe et al, 1998; Haritaoglu,
2001; Yang et al, 2002). The basic idea is the
connection of OCR and machine translation (MT)
(Watanabe et al, 1998) and implementation with
personal data assistant (PDA) has been proposed
(Haritaoglu, 2001; Yang et al, 2002). These are
based on the document OCR which first tries to ex-
tract character regions; performance is weak due to
the variation in lighting conditions. Although the
system we propose also uses OCR, it is character-
ized by the use of a more robust OCR technology
that doesn?t first extract character regions, by lan-
guage processing to offset the OCR shortcomings,
and by the use of the client-server architecture and
the high speed mobile network (the third generation
(3G) network).
2 System design
Figure 1 overviews the system architecture. After
the user takes a picture by the built-in camera of a
PDA, the picture is sent to a controller in a remote
server. At the server side, the picture is sent to the
OCR module which usually outputs many charac-
ter candidates. Next, the word recognizer identifies
word sequences in the candidates up to the number
specified by the user. Recognized words are sent to
the language translator.
The PDA is linked to the server via wireless com-
61
PDA with 
built-in camera and
mobile phone
Language
Translator
image
character candidates
Word
Recognizer
OCR
Controller
character candidates
word candidates
word candidates
translation
image translation
Figure 1: System architecture: http protocol is used
between PDAs and the controller.
munication. The current OCR software is Windows-
based while the other components are Linux pro-
grams. The PDA uses Windows.
We also implemented the system for mobile
phones using the i-mode and FOMA devices pro-
vided by NTT-DoCoMo.
3 Each component
3.1 Appearance-based full search OCR
Research into the recognition of characters in nat-
ural scenes has only just begun (Watanabe et al,
1998; Haritaoglu, 2001; Yang et al, 2002; Wu et
al., 2004). Many conventional approaches first ex-
tract character regions and then classify them into
each character category. However, these approaches
often fail at the extraction stage, because many pic-
tures are taken under less than desirable conditions
such as poor lighting, shading, strain, and distortion
in the natural scene. Unless the recognition target is
limited to some specific signboard (Wu et al, 2004),
it is hard for the conventional OCR techniques to
obtain sufficient accuracy to cover a broad range of
recognition targets.
To solve this difficulty, Kusachi et al proposed
a robust character classifier (Kusachi et al, 2004).
The classifier uses appearance-based character ref-
erence pattern for robust matching even under poor
capture conditions, and searches the most probable
Figure 2: Many character candidates raised by
appearance-based full search OCR: Rectangles de-
note regions of candidates. The picure shows that
candidates are identified in background regions too.
region to identify candidates. As full details are
given in their paper (Kusachi et al, 2004), we focus
here on just its characteristic performance.
As this classifier identifies character candidates
from anywhere in the picture, the precision rate is
quite low, i.e. it lists a lot of wrong candidates. Fig-
ure 2 shows a typical result of this OCR. Rectangles
indicate erroneous candidates, even in background
regions. On the other hand , as it identifies multiple
candidates from the same location, it achieves high
recall rates at each character position (over 80%)
(Kusachi et al, 2004). Hence, if character positions
are known, we can expect that true characters will be
ranked above wrong ones, and greater word recog-
nition accuracies would be achieved by connecting
highly ranked characters in each character position.
This means that location estimation becomes impor-
tant.
3.2 Word recognition
Modern PDAs are equipped with styluses. The di-
rect approach to obtaining character location is for
the user to indicate them using the stylus. However,
pointing at all the locations is tiresome, so automatic
estimation is needed. Completely automatic recog-
nition leads to extraction errors so we take the mid-
dle approach: the user specifies the beginning and
ending of the character string to be recognized and
translated. In Figure 3, circles on both ends of the
string denote the user specified points. All the lo-
cations of characters along the target string are esti-
mated from these two locations as shown in Figure
3 and all the candidates as shown in Figure 2.
62
Figure 3: Two circles at the ends of the string are
specified by the user with stylus. All the charac-
ter locations (four locations) are automatically esti-
mated.
3.2.1 Character locations
Once the user has input the end points, assumed
to lie close to the centers of the end characters, the
automatic location module determines the size and
position of the characters in the string. Since the
characters have their own regions delineated by rect-
angles and have x,y coordinates (as shown in Fig-
ure 2), the module considers all candidates and rates
the arrangement of rectangles according to the dif-
ferences in size and separation along the sequences
of rectangles between both ends of the string. The
sequences can be identified by any of the search al-
gorithms used in Natural Language Processing like
the forward Dynamic Programming and backward
A* search (adopted in this work). The sequence with
the highest score, least total difference, is selected as
the true rectangle (candidate) sequence. The centers
of the rectangles are taken as the locations of the
characters in the string.
3.2.2 Word search
The character locations output by the automatic
location module are not taken as specifying the cor-
rect characters, because multiple character candi-
dates are possible at the same location. Therefore,
we identify the words in the string by the probabil-
ities of character combinations. To increase the ac-
curacy, we consider all candidates around each es-
timated location and create a character matrix, an
example of which is shown in Figure 4. At each
location, we rank the candidates according to their
OCR scores, the highest scores occupy the top row.
Next, we apply an algorithm that consists of simi-
lar character matching, similar word retrieval, and
word sequence search using language model scores
    

 
  
 
 
 	
	 	
	 
 
 

   
   
   
  
Figure 4: A character matrix: Character candidates
are bound to each estimated location to make the
matrix. Bold characters are true.
(Nagata, 1998).
The algorithm is applied from the start to the end
of the string and examines all possible combinations
of the characters in the matrix. At each location, the
algorithm finds all words, listed in a word dictionary,
that are possible given the location; that is, the first
location restricts the word candidates to those that
start with this character. Moreover, to counter the
case in which the true character is not present in the
matrix, the algorithm identifies those words in the
dictionary that contain characters similar to the char-
acters in the matrix and outputs those words as word
candidates. The connectivity of neighboring words
is represented by the probability defined by the lan-
guage model. Finally, forward Dynamic Program-
ming and backward A* search are used to find the
word sequence with highest probability. The string
in the Figure 3 is recognized as ? Using the Web as a Bilingual Dictionary
Masaaki NAGATA
NTT Cyber Space Laboratories
1-1 Hikarinooka, Yokoshuka-shi
Kanagawa, 239-0847 Japan
nagata@nttnly.isl.ntt.co.jp
Teruka SAITO
Chiba University
1-33 Yayoi-cho, Inage-ku
Chiba-shi, Chiba, 263-8522 Japan
t-saito@icsd4.tj.chiba-u.ac.jp
Kenji SUZUKI
Toyohashi University of Technology
1-1 Hibarigaoka, Tempaku-cho, Toyohashi-shi
Aichi, 441-8580 Japan
ksuzuki@ss.ics.tut.ac.jp
Abstract
We present a system for extracting an
English translation of a given Japanese
technical term by collecting and scor-
ing translation candidates from the web.
We first show that there are a lot of par-
tially bilingual documents in the web
that could be useful for term translation,
discovered by using a commercial tech-
nical term dictionary and an Internet
search engine. We then present an al-
gorithm for obtaining translation candi-
dates based on the distance of Japanese
and English terms in web documents,
and report the results of a preliminary
experiment.
1 Introduction
In the field of computational linguistics, the term
?bilingual text? is often used as a synonym for
?parallel text?, which is a pair of texts written in
two different languages with the same semantic
contents. In Asian languages such as Japanese,
Chinese and Korean, however, there are a large
number of ?partially bilingual texts?, in which the
monolingual text of an Asian language contains
several sporadically interlaced English words as
follows:
?
 
	
Efficient Decoding for Statistical Machine Translation
with a Fully Expanded WFST Model
Hajime Tsukada
NTT Communication Science Labs.
2-4 Hikaridai Seika-cho Soraku-gun
Kyoto 619-0237
Japan
tsukada@cslab.kecl.ntt.co.jp
Masaaki Nagata
NTT Cyber Space Labs.
1-1 Hikari-no-Oka Yokosuka-shi
Kanagawa 239-0847
Japan
nagata.masaaki@lab.ntt.co.jp
Abstract
This paper proposes a novel method to compile sta-
tistical models for machine translation to achieve
efficient decoding. In our method, each statistical
submodel is represented by a weighted finite-state
transducer (WFST), and all of the submodels are ex-
panded into a composition model beforehand. Fur-
thermore, the ambiguity of the composition model
is reduced by the statistics of hypotheses while de-
coding. The experimental results show that the pro-
posed model representation drastically improves the
efficiency of decoding compared to the dynamic
composition of the submodels, which corresponds
to conventional approaches.
1 Introduction
Recently, research on statistical machine translation
has grown along with the increase in computational
power as well as the amount of bilingual corpora.
The basic idea of modeling machine translation was
proposed by Brown et al (1993), who assumed that
machine translation can be modeled on noisy chan-
nels. The source language is encoded from a target
language by a noisy channel, and translation is per-
formed as a decoding process from source language
to target language.
Knight (1999) showed that the translation prob-
lem defined by Brown et al (1993) is NP-
complete. Therefore, with this model it is al-
most impossible to search for optimal solutions in
the decoding process. Several studies have pro-
posed methods for searching suboptimal solutions.
Berger et al (1996) and Och et al (2001) pro-
posed such depth-first search methods as stack de-
coders. Wand and Waibel (1997) and Tillmann and
Ney (2003) proposed breadth-first search methods,
i.e. beam search. Germann (2001) and Watanabe
and Sumita (2003) proposed greedy type decoding
methods. In all of these search algorithms, better
representation of the statistical model in systems
can improve the search efficiency.
For model representation, a search method based
on weighted finite-state transducer (WFST) (Mohri
et al, 2002) has achieved great success in the speech
recognition field. The basic idea is that each statis-
tical model is represented by a WFST and they are
composed beforehand; the composed model is op-
timized by WFST operations such as determiniza-
tion and minimization. This fully expanded model
permits efficient searches. Our motivation is to ap-
ply this approach to machine translation. However,
WFST optimization operations such as determiniza-
tion are nearly impossible to apply to WFSTs in ma-
chine translation because they are much more am-
biguous than speech recognition. To reduce the am-
biguity, we propose a WFST optimization method
that considers the statistics of hypotheses while de-
coding.
Some approaches have applied WFST to sta-
tistical machine translation. Knight and Al-
Onaizan (1998) proposed the representation of
IBM model 3 with WFSTs; Bangalore and Ric-
cardi (2001) studied WFST models in call-routing
tasks, and Kumar and Byrne (2003) modeled
phrase-based translation by WFSTs. All of these
studies mainly focused on the representation of each
submodel used in machine translation. However,
few studies have focued on the integration of each
WFST submodel to improve the decoding efficiency
of machine translation.
To this end, we propose a method that expands
all of the submodels into a composition model, re-
ducing the ambiguity of the expanded model by the
statistics of hypotheses while decoding. First, we
explain the translation model (Brown et al, 1993;
Knight and Al-Onaizan, 1998) that we used as a
base for our decoding research. Second, our pro-
posed method is introduced. Finally, experimental
results show that our proposed method drastically
improves decoding efficiency.
2 IBM Model
For our decoding research, we assume the IBM-
style modeling for translation proposed in Brown et
al. (1993). In this model, translation from Japanese
 to English  attempts to find the  that maximizes


 
. Using Bayes? rule,


 
is rewritten as
	
	


 
	
	

 
 




where



is referred to as a language model and

 
 

is referred to as a translation model. In this
paper, we use word trigram for a language model
and IBM model 3 for a translation model.
The translation model is represented as follows
considering all possible word alignments.

 
 

Multi-Language Named-Entity Recognition System based on HMM 
Kuniko SAITO and Masaaki NAGATA 
NTT Cyber Space Laboratories, NTT Corporation 
1-1 Hikari-no-oka Yokosuka-Shi Kanagawa, 239-0847 Japan 
{saito.kuniko,nagata.masaaki}@lab.ntt.co.jp 
Abstract 
We introduce a multi-language 
named-entity recognition system based on 
HMM. Japanese, Chinese, Korean and 
English versions have already been 
implemented. In principle, it can analyze 
any other language if we have training 
data of the target language. This system 
has a common analytical engine and it can 
handle any language simply by changing 
the lexical analysis rules and statistical 
language model. In this paper, we 
describe the architecture and accuracy of 
the named-entity system, and report 
preliminary experiments on automatic 
bilingual named-entity dictionary 
construction using the Japanese and 
English named-entity recognizer. 
1.  Introduction 
There is increasing demand for cross-language 
information retrieval. Due to the development of the 
World Wide Web, we can access information 
written in not only our mother language but also 
foreign languages. One report has English as the 
dominant language of web pages (76.6 %), followed 
by Japanese (2.77 %), German (2.28 %), Chinese 
(1.69 %), French (1.09 %), Spanish (0.81 %), and 
Korean (0.65 %) [1]. Internet users who are not 
fluent in English finds this situation far from 
satisfactory; the many useful information sources in 
English are not open to them. 
To implement a multi-language information 
retrieval system, it is indispensable to develop 
multi-language text analysis techniques such as 
morphological analysis and named-entity 
recognition. They are needed in many natural 
language processing applications such as machine 
translation, information retrieval, and information 
extraction. 
We developed a multi-language named-entity 
recognition system based on HMM. This system is 
mainly for Japanese, Chinese, Korean and English, 
but it can handle any other language if we have 
training data of the target language. This system has 
a common analytical engine and only the lexical 
analysis rules and statistical language model need 
be changed to handle any other language. Previous 
works on multi-language named-entity recognition 
are mainly for European languages [2]. Our system 
is the first one that can handle Asian languages, as 
far as we know.  
In the following sections, we first describe the 
system architecture and language model of our 
named-entity recognition system. We then describe 
the evaluation results of our system. Finally, we 
report preliminary experiments on the automatic 
construction of a bilingual named-entity dictionary. 
 
2. System Architecture 
Our goal is to build a practical multi-language 
named-entity recognition system for multi-language 
information retrieval. To accomplish our aim, there 
are several conditions that should be fulfilled. First 
is to solve the differences between the features of 
languages. Second is to have a good adaptability to 
a variety of genres because there are an endless 
variety of texts on the WWW. Third is to combine 
high accuracy and processing speed because the 
users of information retrieval are sensitive to 
processing speed. To fulfill the first condition, we 
divided our system architecture into language 
dependent parts and language independent parts. 
For the second and third conditions, we used a 
combination of statistical language model and 
optimal word sequence search. Details of the 
language model and word sequence search are 
discussed in more depth later; we start with an 
explanation of the system's architecture. 
Figure 1 overviews the multi-language 
named-entity recognition system. We have 
implemented Japanese (JP), Chinese (CN), Korean 
(KR) and English (EN) versions, but it can, in 
principle, treat any other language. 
There are two language dependent aspects. One 
involves the character encoding system, and the 
other involves the language features themselves 
such as orthography, the kinds of character types, 
and word segmentation. We adopted a character 
code converter for the former and a lexical analyzer 
for the latter.  
In order to handle language independent aspects, 
we adopted N-best word sequence search and a 
statistical language model in the analytical engine.  
The following sections describe the character 
code converter, lexical analyzer, and analytical 
engine. 
 
2.1. Character Code Conversion 
 If computers are to handle multilingual text, 
it is essential to decide the character set and 
its encoding. The character set is a collection 
of characters and encoding is a mapping 
between numbers and characters. One 
character set could have several encoding 
schemes. Hundreds of character sets and 
attendant encoding schemes are used on a 
regional basis. Most of them are standards 
from the countries where the language is 
spoken, and differ from country to country. 
Examples include JIS from Japan, GB from 
China and KSC from Korea; EUC-JP, 
EUC-CN and EUC-KR are the corresponding 
encoding schemes [3]. We call these encoding 
schemes ?local codes? in this paper. It is 
impossible for local code to handle two 
different character sets at the same time, so 
Unicode was invented to bring together all 
the languages of the world [4]. In Unicode, 
character type is defined as Unicode property 
through the assignment of a range of code 
points such as alphanumerics, symbols, kanji 
(Chinese character), hiragana (Japanese 
syllabary character), hangul (Korean 
character) and so on. The proposed lexical 
analyzer allows us to define arbitrary 
properties other than those defined by the 
Unicode standard. 
 The character code converter changes the 
input text encoding from local code to 
Unicode and the output from Unicode to local 
code. That is, the internal code of our system 
is Unicode (UCS-4). Our system can accept 
EUC-JP, EUC-CN, EUC-KR and UTF-8 as 
input-output encoding schemes. In principle, 
we can use any encoding scheme if the 
encoding has round-trip conversion mapping 
between Unicode. We assume that the input 
encoding is either specified by the user, or 
automatically detected by using conventional 
techniques such as [5]. 
 
2.2. Lexical Analyzer 
 The lexical analyzer recognizes words in the 
input sentence. It also plays an important 
role in solving the language differences, that 
is, it generates adequate word candidates for 
every language. 
The lexical analyzer uses regular 
expressions and is controlled by lexical 
analysis rules that reflect the differences in 
language features. We assume the following 
three language features; 
1. character type and word length 
2. orthography and spacing 
3. word candidate generation 
The features can be set as parameters in the 
lexical analyzer. We explain these three 
features in the following sections. 
Figure 1. System Overview 
NE recognized text (local code) 
Language X Plain text (local code) 
Lexical Analysis Rule 
Character Code Converter (local code to Unicode)
Character Code Converter (Unicode to local code)
Lexical Analyzer 
Word Candidates
JP    CN
 
Statistical  
Language Model
(Dictionaries) 
KR   EN
NE  
Recognizer 
Morph 
AnalyzerN-best 
Word 
Sequence 
Search 
Analytical Engine
2.2.1 Character Type and Word Length 
Table 1 shows the varieties of character 
types in each language. Character types 
influence the average word length. For 
example, in Japanese, kanji (Chinese 
character) words have about 2 characters 
and katakana (phonetic character used 
primarily to represent loanwords) words are 
about 5 characters long such as ??????
(password)?. In Chinese, most kanji words 
have 2 characters but proper nouns for 
native Chinese are usually 3 characters, and 
those representing loanwords are about 4 
characters long such as ????? (Beckham)?. 
In Korean, one hangul corresponds to one 
kanji and one hangul consists of one 
consonant - one vowel - one consonant, so 
loanwords written in hangul are about 3 
characters long such as ???? (internet)?. 
Character type and word length are related 
to word candidate generation in section 2.2.3.  
 
Table 1. Character Types 
 
 
 
 
 
 
 
2.2.2 Orthography and Spacing 
 There is an obvious difference in 
orthography between each language, that is, 
European languages put a space between 
words while Japanese and Chinese do not. In 
Korean, spaces are used to delimit phrases 
(called as eojeol in Korean) not words, and 
space usage depends greatly on the 
individual. 
Therefore, another important role of the 
lexical analyzer is to handle spaces. In 
Japanese and Chinese, spaces should usually 
be recognized as tokens, but in English and 
Korean, spaces must be ignored because it 
indicates words or phrases. For example, the 
following analysis results are preferred; 
 I have a pen ? 
  ?I/pronoun? ?have/verb? ?a/article? ?pen/noun? 
and never must be analyzed as follows; 
  ?I/pronoun? ? /space? ?have/verb? ? /space?  
?a/article? ? /space? ?pen/noun? 
There are, however, many compound nouns 
that include spaces such as ?New York?, 
?United States? and so on. In this case, spaces 
must be recognized as a character in a 
compound word. In Korean, it is necessary 
not only to segment one phrase separated by 
a space like Japanese, but also to recognize 
compound words including spaces like 
English. 
 These differences in handling spaces are 
related to the problem of whether spaces 
must be included in the statistical language 
model or not. In Japanese and Chinese, it is 
rare for spaces to appear in a sentence, so 
the appearance of a space is an important 
clue in improving analysis accuracy. In 
English and Korean, however, they are used 
so often that they don?t have any important 
meaning in the contextual sense.   
 The lexical analyzer can treat spaces 
appropriately. The rules for Japanese and 
Chinese, always recognize a space as a token, 
while for those for English and Korean 
consider spaces only a part of compound 
words such as ?New York?. 
 
2.2.3 Word Candidate Generation 
 In our system, the analytical engine can list 
all dictionary word candidates from the input 
string by dictionary lookup. However, it is 
also necessary to generate word candidates 
for other than dictionary words, i.e. unknown 
words candidates. We use the lexical 
analyzer to generate word candidates that 
are not in the dictionary. 
It is more difficult to generate word 
candidates for Asian languages than for 
European languages, because Asian 
languages don?t put a space between words 
as mentioned above. 
 The first step in word candidate generation 
is to make word candidates from the input 
string. The simplest way is to list all 
substrings as word candidates at every point 
in the sentence. This technique can be used 
for any language but its disadvantage is that 
there are so many linguistically meaningless 
candidates that it takes too long to calculate 
the probabilities of all combinations of the 
EN 
JP 
 
CN 
KR 
alphabet symbol number 
alphabet symbol number kanji hiragana 
katakana 
alphabet symbol number kanji 
alphabet symbol number kanji hangul 
candidates in the following analytical process. 
A much more effective approach is to limit 
word candidates to only those substrings 
that are likely to be words. 
 The character types are often helpful in 
word candidate generation. For example, a 
cross-linguistic characteristic is that 
numbers and symbols are often used for 
serial numbers, phone numbers, block 
numbers, and so on, and some distinctive 
character strings of alphabets and symbols 
such as ?http://www?? and 
?name@abc.mail.address? are URLs, 
Email-addresses and so on. This is not 
foolproof since the writing styles often differ 
from language to language. Furthermore, it 
is better to generate such kinds of word 
candidates based on the longest match 
method because substrings of these 
candidates do not usually constitute a word. 
 In Japanese, a change between character 
types often indicates a word boundary. For 
example, katakana words are loanwords and 
so must be generated based on the longest 
match method. In Chinese and Korean, 
sentences mainly consist of one character 
type, such as kanji or hangul, so the 
character types are not as effective for word 
recognition as they are in Japanese. However, 
changes from kanji or hangul to 
alphanumerics and symbols often indicate 
word changes. 
And word length is also useful to put a 
limit on the length of word candidates. It is a 
waste of time to make long kanji words 
(length is 5 or more characters) in Japanese 
unless the substring matched with the 
dictionary, because its average length is 
about 2 characters. In Korean, although 
hanguls (syllabaries) are converted into a 
sequence of hangul Jamo (consonant or 
vowel) internally in order to facilitate the 
morphological analysis, the length of hangul 
words are defined in hangul syllabaries. 
We designed the lexical analyzer so that it 
can correctly treat spaces and word 
candidate generation depending on the 
character types for each language. Table 2 
shows sample lexical analysis rules for 
Japanese (JP) and English (EN). For 
example, in Japanese, if character type is 
kanji or hiragana, the lexical analyzer 
attempts to output word candidates with 
lengths of 1 to 3. If character type is 
katakana, alphabet, or number, it generates 
one candidate based on the longest match 
method until character type changes. If the 
input is ?1500km?, word candidates are ?1500? 
and ?km?. Subset character strings such as ?1?, 
?15?, ?500?, ?k? and ?m? are never output as 
candidates. It is possible for a candidate to 
consist of several character types. Japanese 
has many words that consist of kanji and 
hiragana such as ????(away from)?. In any 
language there are many words that consist 
of numbers and alphabetic characters such 
as ?2nd?, or alphabetic characters and 
symbols such as ?U.N.?. Furthermore, if we 
want to treat positional notation and decimal 
numbers, we may need to change the 
Unicode properties, that is, we add ?.? and ?,? 
to number-property. The character type 
?compound? in English rule indicates 
compound words. The lexical analyzer 
generates a compound word (up to 2 words 
long) with recognition of the space between 
them. In Japanese, a space is always 
recognized as one word, a symbol. 
Table 3 shows the word candidates output 
by the lexical analyzer following the rules of 
Table 2. The Japanese and English inputs 
are parallel sentences. It is apparent that the 
efficiency of word candidate generation 
improves dramatically compared to the case 
of generating all character strings as 
Character Type 
kanji 
hiragana 
katakana 
alphabet 
number 
symbol 
kanji ? hiragana 
alphabet 
number 
symbol 
compound 
Word Length 
1-3 
1-3 
until type changes
until type changes 
until type changes 
1 
1-3 
until type changes 
until type changes 
1 
up to 2 words 
JP
 
 
 
 
EN
Table 2.  Lexical Analysis Rule 
candidates at every point in a sentence. In 
Japanese, kanji and hiragana strings become 
several candidates with lengths of 1 to 3, and 
alphabet and katakana strings become one 
candidate based on the longest match 
method until character type changes. In 
English, single words and compound words 
are recognized as candidates. Only the 
candidates that are not in the dictionary 
become unknown word candidates in the 
analytical engine. 
 
2.3. Analytical engine 
The analytical engine consists of N-best 
word sequence search and a statistical 
language model. Our system uses a word 
bigram model for morphological analysis and 
a hidden Markov model for named-entity 
recognition. These models are trained from 
tagged corpora that have been manually 
word segmented, part-of-speech tagged, and 
named-entity recognized respectively. Since 
N-best word sequence search and statistical 
language model don?t depend on language, 
we can apply this analytical engine to all 
languages. This makes it possible to treat 
any language if a corpus is available for 
training the language model. The next 
section explains the hidden Markov model 
used for named-entity recognition. 
 
3. Named-entity Recognition Model 
 The named-entity task is to recognize 
entities such as organizations, personal 
names, and locations. Several papers have 
tackled named-entity recognition through 
the use of Markov model (HMM) [6], 
maximum entropy method (ME) [7, 8], and 
support vector machine (SVM) [9]. It is 
generally said that HMM is inferior to ME 
and SVM in terms of accuracy, but is 
superior with regard to training and 
processing speed. That is, HMM is suitable 
for applications that require realtime 
response or have to process large amounts of 
text such as information retrieval. We 
extended the original HMM reported by BBN. 
BBN?s named-entity system is for English 
and offers high accuracy. 
 
 The HMM used in BBN's system is 
described as follows. Let the morpheme 
sequence be nwwW L1=  and Name Class 
(NC) sequence be nNCNCNC L1= . Here, NC 
represents the type of named entity such as 
organization, personal name, or location. The 
joint probability of word sequence and NC 
sequence ),(),( ii NCwPNCWP ?= are 
calculated as follows; 
(1) if 1?? ii NCNC  
),|(),|(),( 111 ??? ?= iiiiiiii NCNCwPwNCNCPNCwP  
(2) if 1?= ii NCNC  and 1+= ii NCNC  
),|(),( 1 iiiii NCwwPNCwP ?=  
(3) if 1?= ii NCNC  and 1+? ii NCNC  
),|(),|(),( 1 iiiiiii NCwendPNCwwPNCwP ><?= ?  
Here, the special symbol >< end  indicates 
the end of an NC sequence.  
 In this model, morphological analysis and 
named-entity recognition can be performed 
at the same time. This is preferable for Asian 
languages because they have some ambiguity 
about word segmentation. To adapt BBN?s 
HMM for Asian languages, we extended the 
original HMM as follows. Due to the 
Tokyo Disneyland 
is 10 km away from 
the Tokyo station. 
?????????? 
???????10km 
????? 
? 
?? 
???????? 
? 
? 
? ?? ??? 
? ?? ??? 
? ?? ??? 
? ?? 
? 
10 
km 
? ?? ??? 
? ?? ??? 
? ?? ??? 
? ?? 
? 
?Tokyo? 
?Tokyo Disneyland? 
?Disneyland? 
?Disneyland is? 
?is? 
?10? 
?km? ?km away? 
?away? ?away from? 
?from? ?from the? 
?the? ?the Tokyo? 
?Tokyo? ?Tokyo station? 
?station? 
?.? 
Input sentence 
Word Candidates 
Table 3. Outputs of Lexical Analyzer 
ambiguity of word segmentation, 
morphological analysis is performed prior to 
applying the HMM; the analysis uses a word 
bigram model and N-best candidates (of 
morpheme sequence) are output as a word 
graph structure. Named-entity recognition is 
then performed over this word graph using 
the HMM. We use a forward-DP 
backward-A* N-best search algorithm to get 
N-best morpheme sequence candidates [10]. 
In this way, multiple morpheme candidates 
are considered in named-entity recognition 
and the problem of word segmentation 
ambiguity is mitigated. 
 BBN's original HMM used a back-off model 
and smoothing to avoid the sparse data 
problem. We changed this smoothing to 
linear interpolation to improve the accuracy, 
and in addition, we used not only the 
morpheme frequency of terms but also part 
of speech frequency. Table 4 shows the linear 
interpolation scheme used here. Underlined 
items are added in our model. The weight for 
each probability was decided from 
experiments. 
 
4. Experiments 
 To evaluate our system, we prepared 
original corpora for Japanese, Chinese, 
Korean and English. The material was 
mainly taken from newspapers and Web 
texts. We used the morpheme analysis 
definition of Pen Tree Bank for English [11], 
Jtag for Japanese [12], Beijing Univ. for 
Chinese [13] and MATEC99 for Korean [14]. 
The named-entity tag definitions were based 
on MUC [15] for English and IREX [16] for 
Japanese. We defined Chinese and Korean 
named-entity tags following the Japanese 
IREX specifications. Table 5 shows 
dictionary and corpus size. Dictionary words 
means the size of the dictionary for 
morphological analysis. Total words and 
sentences represent the size of the corpus for 
named-entity recognition.  
Named-entity accuracy is expressed in 
terms of recall and precision. We also use the 
F-measure to indicate the overall 
performance. It is calculated as follows; 
(4)       precisionrecall
precisionrecallF
+
??
=
2  
Table 6 shows the F-measure for all 
languages. Since we used our original 
corpora in this evaluation, we cannot 
compare our results to those of previous 
works. Accordingly, we also evaluated SVM 
using our original corpora (see Table 6) [17]. 
The accuracy of HMM and SVM were 
approximately equivalent. But the analysis 
speed of HMM was ten times faster than 
that of SVM [9]. This means that our system 
is very fast and has state-of-the-art accuracy 
in four languages.  
 We noted that the accuracy of SVM is 
unusually lower than that of HMM for 
Japanese. We have not yet confirmed the 
cause of this, but a plausible argument is as 
follows. First, the word segmentation 
ambiguity has a worse affect on accuracy 
than expected. Since current SVM 
implementations can not handle N-best 
morpheme candidates and lower-order 
candidates are not considered in 
named-entity recognition. Second, SVM may 
not suit the analysis of irregular, 
ill-structured, and informal sentences such 
as Web texts. Our original corpus data was 
dictionary words
17,546
436,157
147,585
182,523
total words 
144,708 
143,408 
410,188 
1,456,130 
sentences
5,921
4,793
12,824
39,943
EN
JP
CN
KR
Table 5. Dictionary and Corpus Size 
Table 6. Named Entity Accuracy (F-measure(%))
HMM
88.2
81.0
84.5
79.9
SVM 
84.7 
57.3 
89.5 
82.1 
EN
JP
CN
KR
),|( 11 ?? iii wNCNCP  ),|( 11 ?ii NCNCwP  ),|( 11 NCwwP ii ?
),|( 11 ?? iii posNCNCP   ),|( 11 ?ii NCNCposP  ),|( 11 NCposposP ii ?
)|( 1?ii NCNCP      )|( 1NCwP i       )|( 1NCwP i  
)(NCP            )|( 1NCposP i     )|( 1NCposP i  
NCofnumber/1  
Table 4. Linear Interpolation Scheme
taken from newspapers and Web texts, the 
former contains complete and grammatical 
sentences unlike the latter. It is often said 
that HMM is robust enough to analyze these 
dirty sentences. It is, anyhow, our next step 
to analyze the results of named-entity 
recognition in more detail. 
 
5. Application to Bilingual Lexicon 
Extraction from Parallel Text 
 In order to illustrate the benefit of our 
multi-language named-entity recognition 
system, we conducted a simple experiment 
on extracting bilingual named-entity lexicons 
from parallel texts. It is very difficult to 
gather bilingual lexicons of named entities 
because there are an enormous number of 
new named entities. Establishing a bilingual 
named-entity dictionary automatically would 
be extremely useful. 
There are 3 steps in extracting a bilingual 
lexicon as follows; 
1. recognize named entity from parallel text  
2. extract bilingual lexicon candidates 
3. winnow the candidates to yield a 
  reasonable lexicon list 
The multi-language named-entity 
recognition system is used in the first step. 
In this step, the parallel texts are analyzed 
morphologically and named entities are 
recognized. 
In the second step, bilingual lexicon 
candidates are listed automatically under 
the following conditions; 
?word sequence up to 5 words 
?include one or more named entities 
?does not include function words  
The cooccurrence frequency of candidates is 
calculated at the same time. 
 In the third step, reasonable lexicons are 
created from the candidates. To judge the 
suitability of the candidates to be entered 
into a bilingual lexicon, we use the 
translation model called the IBM model [18]. 
Let a word sequence in language X  be 
lxxX L1=  and let the corresponding word 
sequence in language Y  be myyY L1= . Here, 
)1( lixi ??  and )1( mjy j ??  represent one 
word. In IBM model 1, the conditional 
probability )|( XYP  is calculated as follows; 
(5)       ???=
==
+
l
i ij
m
jl
xytXYP m 11)1(
)|()|( ?  
where ?  is constant. )|( ij xyt  is translation 
probability and is estimated by applying the 
EM algorithm to a large number of parallel 
texts.  
 Since the longer word sequences X and Y 
are, the smaller )|( XYP  becomes, the value 
of )|( XYP  cannot be compared when a word 
sequence length changes. Therefore, we 
improved equation (5) to take into account 
the difference in a word sequence length and 
cooccurrence frequency as follows; 
(6)  )|(
)|()()()|( XYE
XYPYmatchXmatchfreqXYS ???=  
    freq     : cooccurrence frequency of  
X and Y in parallel text 
)(Xmatch : ratio of 0)|( ?ij xyt  in X 
        )(Ymatch : ratio of 0)|( ?ij xyt  in Y 
        ???=
==
+
l
i ij
m
jl
xytXYE m 11)1(
)|()|( ?  
)|( ij xyt is the average of )|( ij xyt . )|( XYS  is 
used as a measure of candidate suitability. 
We used Japanese-English news article 
alignment data as parallel texts that is 
released by CRL [19, 20]. In this data, 
articles and sentences are aligned 
automatically. We separated the parallel text 
into a small set (about 1000 sentences) and a 
North Korea 
United States 
International Monetary Fund 
Soviet Union 
Middle East 
North Atlantic  
Treaty Organization 
U.S. President Bill Clinton 
North American  
Free Trade Agreement 
European Community 
Taiwan Strait 
Clinton administration 
U.N. General Assembly 
Tokyo Stock Exchange 
??? 
?? 
?????? 
?? 
?? 
???????? 
 
????????? 
???????? 
 
????? 
???? 
??????? 
???? 
??????? 
Table 7. List of Bilingual Lexicons 
large set (about 150 thousand sentences). We 
extracted bilingual lexicons from a small set 
and )|( ij xyt  was estimated from a large set. 
Table 7 shows bilingual lexicons that 
achieved very high scores. It can be said that 
they are adequate as bilingual lexicons. 
Though a more detailed evaluation is a 
future task, the accuracy is about 86 % for 
the top 50 candidates. This suggests that the 
proposed system can be applied to bilingual 
lexicon extraction for automatically creating 
bilingual dictionaries of named entities. 
 
Conclusion 
We developed a multi-language named-entity 
recognition system based on HMM. We have 
implemented Japanese, Chinese, Korean and 
English versions, but in principle it can handle any 
language if we have training data for the target 
language. Our system is very fast and has 
state-of-the-art accuracy. 
 
References 
[1] Google: 1.6 Billion Served. Wired, December 
2000, pp.118-119 (2000). 
[2] Cucerzan, S. and Yarowsky, D.: Language 
Independent Named Entity Recognition 
Combining Morphological and Contextual 
Evidence, Proceedings of the 1999 Joint 
SIGDAT Conference on Empirical Method in 
Natural Language Processing and Very Large 
Corpora (EMNLP/VLC-99), College Park, pp. 
90-99 (1999) 
[3] Lunde, K.: CJKV Information Processing, 
O?REILY, (1999). 
[4] The Unicode Consortium.: The Unicode Standard, 
version 3.0, Addison-Wesley Longman, (2000).   
[5] Kikui, G.: Identifying the Coding System and 
Language of On-line Documents on the Internet, 
Proceedings of the 16th International 
Conference on Computational Linguistics 
(COLING-96), pp. 652?657 (1996) 
[6] Bikel, D. M., Schwartz, R. and Weischedel, R. M.: 
An Algorithm that Learns What?s in a Name, 
Machine Learning, Vol. 34, No. 1-3, pp. 211-231 
(1999) 
[7] Borthwick, A., Sterling, J., Agichtein, E. and 
Grishman, R.: Exploiting Diverse Knowledge 
Sources via Maximum Entropy, Proceedings of 
the 6th Workshop on Very Large Corpora 
(VLC-98), pp. 152-160 (1998). 
[8] Uchimoto, K., Murada, M., Ma, Q., Ozaku, H. and 
Isahara, H.: Named Entity Extraction Based on 
A Maximum Entropy Model and Transformation 
Rules, Proceedings of the 38th Annual Meeting 
of the Association for Computational Linguistics 
(ACL-00), pp. 326-335 (2000). 
[9] Isozaki, H. and Kazawa, H.: Efficient Support 
Vector Classifiers for Named Entity Recognition, 
Proceedings of the 19th International 
Conference on Computational Linguistics 
(COLING-02), pp. 390-396 (2002). 
[10] Nagata, M.: A Stochastic Japanese 
Morphological Analyzer Using a Forward-DP 
Backward-A* N-Best Search Algorithm, 
Proceedings of the 15th International 
Conference on Computational Linguistics 
(COLING-94), pp. 201-207 (1994). 
[11] Marcus, M. P., Santorini, B. and Marcinkiewicz, 
M. A.: Building a large annotated corpus of 
English: The Penn Treebank, Computational 
Linguistics, Vol. 19, No.2, pp. 313-330 (1993). 
[12] Fuchi, T. and Takagi, S.: Japanese 
Morphological Analyzer using Word 
Co-occurrence -JTAG-, Proceedings of 36th 
Annual Meeting of the Association for 
Computational Linguistics and 17th International 
Conference on Computational Linguistics 
(ACL-COLING-98),  pp. 409-413 (1998). 
[13] Yu, Shiwen. et. al.: The Grammatical 
Knowledge-base of Contemporary Chinese --- A 
Complete Specification (?????????
???), Tsinghua University Press, (1992).  
[14] ETRI.: Part-of-Speech Tagset Guidebook ?? 
?? ?? ???), Unpublished Manual, (1999) 
[15] DARPA: Proceedings of the 7th Message 
Understanding Conference (MUC-7) (1998). 
[16] IREX Committee (ed.), 1999. Proceedings of 
the IREX workshop. http://nlp.cs.nyu.edu/irex/ 
[17] Kudo, T and Matsumoto, Y.: Chunking with 
Support Vector Machines, Proceedings of the 
Second Meeting of the North American Chapter 
of the Association for Computational Linguistics 
(NAACL-01), pp. 192-199 (2001). 
[18] Brown, P.F., Pietra, S. A. D., Pietra, V. J. D. and 
Mercer, R. L.: The Mathematics of Statistical 
Machine Translation: Parameter Estimation, 
Computational Linguistics, Vol. 19, No. 2, pp. 
263-311 (1993) 
[19] Utiyama, M. and Isahara, H.: Reliable Measures 
for Aligning Japanese-English News Article and 
Sentences, Proceedings of the 41st Annual 
Meeting of the Association for Computational 
Linguistics (ACL-03) (2003). 
[20] Japanese-English News Article Alignment Data, 
http://www2.crl.go.jp/jt/a132/members/mutiyam 
a/jea/index.html (2003) 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 331?339,
Beijing, August 2010
Enriching Dictionaries with Images from the Internet
- Targeting Wikipedia and a Japanese Semantic Lexicon: Lexeed -
Sanae Fujita
NTT Communication Science Lab.
sanae@cslab.kecl.ntt.co.jp
Masaaki Nagata
NTT Communication Science Lab.
nagata.masaaki@lab.ntt.co.jp
Abstract
We propose a simple but effective method
for enriching dictionary definitions with
images based on image searches. Vari-
ous query expansion methods using syn-
onyms/hypernyms (or related words) are
evaluated. We demonstrate that our
method is effective in obtaining high-
precision images that complement dictio-
nary entries, even for words with abstract
or multiple meanings.
1 Introduction
The Internet is an immense resource for images.
If we can form connections between these im-
ages and dictionary definitions, we can create
rich dictionary resources with multimedia infor-
mation. Such dictionaries have the potential to
provide educational (Popescu et al, 2006), cross-
langauge information retrieval (Hayashi et al,
2009) or assistive communication tools especially
for children, language learners, speakers of differ-
ent languages, and people with disabilities such
as dyslexia (Mihalcea and Leong, 2008; Goldberg
et al, 2009).
Additionally, a database of typical images con-
nected to meanings has the potential to fill the
gaps between images and meanings (semantic
gap). There are many studies which aim to cross
the semantic gap (Ide and Yanai, 2009; Smeulders
et al, 2000; Barnard et al, 2003) from the point
of view of image recognition. However the se-
mantic classes of target images are limited (e.g.
Caltech-101, 2561). Yansong and Lapata (2008)
tried to construct image databases annotated with
keywords from Web news images with their cap-
tions and articles, though the semantic coverage is
1http://www.vision.caltech.edu/Image Datasets/Caltech101,
256/
unknown. In this paper, we aim to supply sev-
eral suitable images for dictionary definitions. We
propose a simple but effective method based on an
Internet image search.
There have been several studies related to sup-
plying images for a dictionary or thesaurus. Bond
et al (2009) applied images obtained from the
Open Clip Art Library (OCAL) to Japanese Word-
Net.2 They obtained candidate images by compar-
ing the hierarchical structures of OCAL and Word-
Net, and then judged whether or not the image was
suitable for the synset by hand. OCAL benefits
from being in the public domain; however, it can-
not cover a wide variety of meanings because of
the limited number of available images.
Fujii and Ishikawa (2005) collected images
and text from the Internet by querying lemma,
and linked them to an open encyclopedia, CY-
CLONE.3 They guessed the meaning of the im-
ages by disambiguating the surrounding text. This
is a straightforward approach, but it is difficult to
use it to collect images with minor meanings, be-
cause in most cases the Internet search querying
lemma only provides images related to the most
common meaning. For example, lemma y?
arch may mean ??architecture?? or ??home run??
in Japanese, but a lemma search provided no im-
age of the latter at least in the top 500.
There are some resources which link images to
target synsets selected from WordNet (Fellbaum,
1998). For example, PicNet (Borman et al, 2005),
ImageNet (Deng et al, 2009) and image ontology
(Popescu et al, 2006, 2007; Zinger et al, 2006)
collect candidate images from the Internet. PicNet
and ImageNet ask Web users to judge their suitabil-
ity, and Zinger et al (2006); Popescu et al (2007)
automatically filtered out unsuitable images us-
ing visual characteristics. These approaches can
2http://nlpwww.nict.go.jp/wn-ja/
3http://cyclone.cl.cs.titech.ac.jp/
331
?
?????????????????????????????
INDEX y? arch (POS: noun)
SENSE 1
?
???????????
DEFINITION ?1 k?1 G41 D08m1 W89 6G3 m?1 T?2
Buildings with bow-shaped top. Or its architectural style.
EXAMPLE G2 ?1 H?=Gy?1 @wo4 ? d
That bridge has 2 arches.
HYPERNYM m1 building,T?2 style
SEM. CLASS ?865:house (main building)? (? ?2:concrete?),
?2435:pattern, method? (? ?1000:abstract?)
?
???????????
?
?
IMAGE
?
?
SENSE 3
?
?????????
DEFINITION ?1 @??D1 ????1  A home run in baseball.
EXAMPLE ???1 %?{?4 ????2 D?U3 Gy?3 ki<4 8
A batter blasted the ball over the right-field wall.
HYPERNYM ??D1 honruida
SYNONYM ????1 home run, DOMAIN ?1 baseball
SEM. CLASS ?1680:sport? (? ?1000:abstract?)
?
?????????
?
??
IMAGE
?
??
?
?????????????????????????????
Figure 1: Simplified Entry for Lexeed & Hinoki:y? arch
collect a large number of highly accurate images.
However, target synsets are limited at present, and
the coverage of polysemous words is unknown.
We present a comparison with ImageNet and im-
age ontology (Popescu et al, 2006) in ? 3.
In this paper, to cover a broad range of mean-
ings, we use an Internet search. In advance, we ex-
pand the number of queries per meaning using in-
formation extracted from definition sentences. In
? 3, we investigate the usability and effectiveness
of several types of information targeting two dif-
ferent types of dictionaries, a Japanese Semantic
Lexicon: Lexeed and a Web Dictionary: Japanese
Wikipedia4 (? 2). We show that our method is sim-
ple but effective. We also analyze senses that are
difficult to portray using images.
2 Resources
2.1 Japanese Semantic Lexicon: Lexeed
We use Lexeed, a Japanese Semantic Lexicon
(Kasahara et al, 2004) as a target dictionary (see
Figure 1). Lexeed includes the 29,000 most famil-
iar words in Japanese, split into 48,000 senses.
Each entry contains the word itself and its part
of speech (POS) along with definition and ex-
ample sentences and links to the Goi-Taikei (GT)
Japanese Ontology (Ikehara et al, 1997). In ad-
dition, we extracted related words such as hyper-
nyms, synonyms, and domains, from the defini-
4http://ja.wikipedia.org/
Table 1: Size of Lexeed and Japanese Wikipedia
(disambiguation)
Lexeed Wikipedia Shared
No. Lemma
Entries 29,272 33,299 2,228
Senses 48,009 197,9121 19,703
Ave. Senses/Entry 1.6 5.9 8.8
Max. Senses/Entry 57 320 148
Monosemous 19,080 74 2
Ave. Words/Definition2 14.4 10.7 11.0
1From the all 215,883 lists, we extracted lists showing
senses obtained by heuristics (see lines 2,3,4,6,7,9 and
10 for Figure 2).
2Analyzed by Mecab, http://mecab.sourceforge.net/
tions (called Hinoki Ontology). The images in Fig-
ure 1 are samples provided using our method.
2.2 Web Dictionary :Japanese Wikipedia
We used Wikipedia?s disambiguation pages,5 as a
target dictionary (see Figure 2). A disambigua-
tion page lists articles (eg. ??European Union??,
??Ehime University??) associated with the same
lemma (eg. ?EU?). Our goal is to provide images
for each article listed. As shown in Figure 2, they
include various writing styles.
2.3 Comparison of Lexeed and Wikipedia
Table 1 shows the sizes of Lexeed and Wikipedia?s
disambiguation pages, and the shared entries.
Shared entries are rare, and account for less than
5Version 20091011.
332
Original (in Japanese)
1 ???EU???
2 * [[AJ?]]
3 * [[Europa Universalis]]??? - [[??
????{?????z?]]G[[??????
?????]]
4 * [[??d?]](Ehime University) - [[??
z]][[???]]Dd??G[[ ?d?]]
5 ???Eu???
6 * [[?}??}?]]G??d
7 * [[????y?]] - ?"?H
8 ???eu???
9 * [[.eu]] - AJ?G[[ 9??{?]]
10 * [[????]]G[[ISO 639|ISO 639-1
????]]
Gloss
1 ???EU???
2 * [[European Union]]
3 * [[Europa Universalis]] series - a [[histori-
cal computer game]] by [[Paradox Interactive]]
4 * [[Ehime University]] - a [[National Univer-
sity]] in [[Matsuyama]],[[Ehime Prefecture]]
5 ???Eu???
6 * [[Europium]]?s chemical element symbol
7 * [[euphonium]] - a brass instrument
8 ???eu???
9 * [[.eu]] - [[country-code top-level domain]]
for the European Union
10 * [[ISO 639|ISO 639-1 language code]] of
[[Basque]]
[[ ]] shows a link in Wikipedia. And we assign each line a number for easy citation.
Figure 2: Simplified Example of Wikipedia?s Disambiguation Page: ?EU (disambiguation)?
10 % of the total 67. As regards Lexeed, 16,685
entries (57 %) do not appear in any of Wikipedia?s
lemmas, not only in disambiguation pages.8
As shown in Table 1, Wikipedia has many
senses, but most of them are proper nouns. For
example, in Lexeed,???? sunflower is monose-
mous, but in Wikipedia, 67 senses are listed,
including 65 proper nouns besides ??plant??
and ??sunflower oil??. On the other hand,
in Wikipedia, y? arch has only one sense,
??architecture?? corresponding to Lexeed?s y
?1 arch, and has no disambiguation page.
As mentioned above, Lexeed and Wikipedia have
very different types of entries and senses. This
research aims to investigate the possibility of
supplying appropriate images for such different
senses, and a method for obtaining better images.
3 Experiment to Supply Images for
Word Senses
In this paper, we propose a simple method for
supplying appropriate images for each dictionary
sense of a word. We collect candidate images
from the Internet by using a querying image
search. To obtain images even for minor senses,
we expand the query by appending queries ex-
6Shared lemmas are 6I buckwheat noodle, ?{??
cycle,???} owl, etc.
7Lemmas only in Wikipedia are {??? Aesop, ??
Biot/Veoh,?Gi fall name, etc.
8Lemmas only in Lexeed are? pay later, ????
humorous,e> selection, etc.
tracted from definitions for each sense.
In this paper, we investigated two main types
of expansion, that is, the appending of mainly
synonyms (SYN), and related words including hy-
pernyms (LNK). For information retrieval, query
expansion using synonyms has been adopted in
several studies (Voorhees, 1994; Fang and Zhai,
2006; Unno et al, 2008). Our LNK is similar to
methods used in Deng et al (2009), but we note
that their goal is not to give images to polysemous
words (which is our intention). Popescu et al
(2006) also used synonyms (all terms in a synset)
and hypernyms (immediate supertype in WordNet),
but they did not investigate the effectiveness of
each expansion and they forcus only on selected
object synsets.
3.1 Experimental and Evaluation Method
We collected five candidate images for each sense
from the Internet by querying an image search en-
gine.9 Then we manually evaluated the suitabil-
ity of the image for explaining the target sense.
The evaluator determined whether or not the im-
age was appropriate (T), acceptable (M), or inap-
propriate (F). The evaluator also noted the reasons
for F.
Figure 3 shows an example for8WF' onion.
As shown in Figure 3, the evaluator determined T,
M or F for each candidate image.
9We used Google AJAX images API,
http://code.google.com/intl/ja/apis/ajaxsearch/
333
(1) (2) (3) (4) (5)
T (Appropriate) F (Inappropriate) M (Acceptable) T (Appropriate) T (Appropriate)
Figure 3: Examples of Candidate Images and Evaluations for8WF' onion
Table 2: Data for Hinoki Ontology
Type No. % Example
Lemma Related Word
Hypernym 47,054 69.1 y?1 arch T?
Synonym 14,068 20.6 y?3 arch ???? homer
Domain 1,868 2.7 y?3 arch ? baseball
Hyponym 757 1.1 7c61 buy and sell 7d sell
Meronym 686 1.0 ?+1 lean ?? fish meat
Abbreviation 383 0.6 ?2 A(sia) y?y Asia
Other name 216 0.3 F0-X2 shave ????? plug outlet
Other 3102 4.6 ^X?&1 papillote ? fish
Total 68,134 100
For an image that is related but that does not ex-
plain the sense, the evaluation is F. For example,
for 8WF' onion, the images of onion dishes
such as (2) in Figure 3 are F. On the other hand,
the images that show onions themselves such as
(1), (4) and (5) in Figure 3 are T. With (3) in Fig-
ure 3, the image may show the onion itself or a
field of onions, therefore the evaluation is M.
One point of judgment, specifically between T
and M, is whether the image is typical or not. With
8WF' onion, most typical images are similar to
(1), (4) and (5). The image (3) may not be typi-
cal but is helpful for understanding, and (2) may
lead to a misunderstanding if this is the only im-
age shown to the dictionary user. This is why (3)
is judged to be M and (2) is judged to be F.
We evaluated 200 target senses for Lexeed, and
100 for Wikipedia.10
3.2 Experiment: Lexeed
In this paper, we expand queries using the Hi-
noki Ontology (Bond et al, 2004), which includes
related words extracted from the definition sen-
tences. Table 2 shows the data for the Hinoki On-
tology.
For SYN, we expand queries using synonyms,
abbreviations, other names in Table 2, and vari-
10We performed an image search in September 2009 for
Lexeed, and in December 2009 for Wikipedia.
ant spellings found in the dictionary. On the other
hand, for LNK, we use all the remaining rela-
tions, namely hypernyms, domains, etc. Addi-
tionally, we use only normal spellings with no ex-
pansion, when the target words are monosemous
(MONO). One exception should be noted. When
the normal spelling employs hiragana (Japanese
syllabary characters), we expand it using a vari-
ant spelling. For example,AlU dragonfly is ex-
panded by the variant spelling?? dragonfly.
To investigate the trends and difficulties based
on various conditions, we split the Lexeed senses
into four types, namely, concrete and monose-
mous (MC), or polysemous (PC), not concrete and
monosemous (MA), or polysemous (PA). We se-
lected 50 target senses for evaluation randomly
for each type. The target senses were randomly
selected without distinguishing them in terms of
their POS.
Note that we regard the sense as being some-
thing concrete that is linked to GT?s seman-
tic classes subsumed by ?2:concrete?, such as
8WF' onion (? ?677:crop/harvest/farm
products? ? ?2:concrete?).
3.3 Results and Discussion: Lexeed
Table 3 shows the ratio of T (appropriate), M (ac-
ceptable) and F (inappropriate) images for the tar-
get sense. We calculated the ratio using all five
candidate images, for example, in Figure 3, the
334
ratio of appropriate images is 60 % (three of five).
In Table 3, the baseline shows a case where the
query only involves the lemma (normal spelling).
As shown in Table 3, SYN has higher precision
than LNK. This means that SYN can focus on
the appropriate sense. With polysemous words
(PC, PA), expansion works more effectively, and
helps to supply appropriate images for each sense.
However, with MC, both LNK and SYN have less
precision. This is because the target senses of
MC are majorities, so expansion is adversely af-
fected. Although MONO alone has good precision,
because hiragana is often used as readings and
has high ambiguity, appending the variant spelling
helps us to focus on the appropriate sense.
Here, we focus on LNK of PC, and then analyze
the reasons for F (Table 5). In Table 5, in 24.3%
of cases it is ?difficult to portray the sense using
images? (The numbers of senses for which it is
?difficult to portray the sense using images? are,
3 of MC, 9 of PC, 10 of MA, and 16 of PA. We
investigate such senses in more detail in ? 3.4.).
For such senses, no method can provide suit-
able images, as might be expected. Therefore, we
exclude targets where it is ?difficult to portray the
sense using images?, then we recalculated the ra-
tio of appropriate images. Table 4 shows the ca-
pability of our proposed method for senses that
can be explored using images. This leads to 66.3
% precision (15.3% improvement) even for most
difficult target type, PA.
Again, when we look at Table 5, reasons 2-5
(33.3 %) will be improved. In particular, ?hy-
pernym leads to ambiguity? makes up more than
10%. Hypernyms sometimes work well, but
sometimes they lead to other words included in
the hypernyms. For example, appending the hy-
pernym ? foods to Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 204?209,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Noise-aware Character Alignment for Bootstrapping Statistical Machine
Transliteration from Bilingual Corpora
Katsuhito Sudoh?? Shinsuke Mori? Masaaki Nagata?
?NTT Communication Science Laboratories
?Graduate School of Informatics, Kyoto University
?Academic Center for Computing and Media Studies, Kyoto University
sudoh.katsuhito@lab.ntt.co.jp
Abstract
This paper proposes a novel noise-aware char-
acter alignment method for bootstrapping sta-
tistical machine transliteration from automat-
ically extracted phrase pairs. The model is
an extension of a Bayesian many-to-many
alignment method for distinguishing non-
transliteration (noise) parts in phrase pairs. It
worked effectively in the experiments of boot-
strapping Japanese-to-English statistical ma-
chine transliteration in patent domain using
patent bilingual corpora.
1 Introduction
Transliteration is used for providing translations for
source language words that have no appropriate
counterparts in target language, such as some tech-
nical terms and named entities. Statistical machine
transliteration (Knight and Graehl, 1998) is a tech-
nology to solve it in a statistical manner. Bilin-
gual dictionaries can be used to train its model, but
many of their entries are actually translation but not
transliteration. Such non-transliteration pairs hurt
the transliteration model and should be eliminated
beforehand.
Sajjad et al (2012) proposed a method to iden-
tify such non-transliteration pairs, and applied it
successfully to noisy word pairs obtained from au-
tomatic word alignment on bilingual corpora. It
enables the statistical machine transliteration to be
bootstrapped from bilingual corpora. This approach
is beneficial because it does not require carefully-
developed bilingual transliteration dictionaries and
it can learn domain-specific transliteration patterns
from bilingual corpora in the target domain. How-
ever, their transliteration mining approach is sample-
wise; that is, it makes a decision whether a bilingual
phrase pair is transliteration or not. Suppose that
a compound word in a language A is transliterated
into two words in another language B. Their corre-
spondence may not be fully identified by automatic
word alignment and a wrong alignment between the
compound word in A and only one component word
in B is found. The sample-wise mining cannot make
a correct decision of partial transliteration on the
aligned candidate, and may introduces noise to the
statistical transliteration model.
This paper proposes a novel transliteration mining
method for such partial transliterations. The method
uses a noise-aware character alignment model that
distinguish non-transliteration (noise) parts from
transliteration (signal) parts. The model is an ex-
tension of a Bayesian alignment model (Finch and
Sumita, 2010) and can be trained by a sampling al-
gorithm extended for a constraint on noise. Our
experiments of Japanese-to-English transliteration
achieved 16% relative error reduction in transliter-
ation accuracy from the sample-wise method. The
main contribution of this paper is two-fold:
? we formulate alignment over string pairs with
partial noise and present a solution with a
noise-aware alignment model;
? we proved its effectiveness by experiments
with frequent unknown words in actual
Japanese-to-English patent translation data.
204
2 Bayesian many-to-many alignment
We briefly review a Bayesian many-to-many charac-
ter alignment proposed by Finch and Sumita (2010)
on which our model is based. The model is based
on a generative process of bilingual substring pairs
?s?, t?? by the following Dirichlet process (DP):
G|?,G0 ? DP(?,G0)
?s?, t??|G ? G,
where G is a probability distribution over substring
pairs according to a DP prior with base measure G0
and hyperparameter ?. G0 is modeled as a joint
spelling model as follows:
G0 (?s?, t??) =
?|s?|s
|s?|! e
??sv?|s?|s ?
?|t?|t
|?t|! e
??tv?|t?|t . (1)
This is a simple joint probability of the spelling
models, in which each alphabet appears based on
a uniform distribution over the vocabulary (of size
vs and vt) and each string length follows a Poisson
distribution (with the average length ?s and ?t).
The model handles infinite number of substring
pairs according to the Chinese Restaurant Process
(CRP). The probability of a substring pair ?s?k, t?k?
is based on the counts of all other substring pairs as
follows:
p
(
?s?k, t?k?| {?s?, t??}?k
)
= N (?s?k, t?k?) + ?G0 (?s?k, t?k?)?
i N (?s?i, t?i?) + ?
. (2)
Here {?s?, t??}?k means a set of substring pairs ex-
cluding ?s?k, t?k?, and N (?s?k, t?k?) is the number of
?s?k, t?k? in the current sample space. This align-
ment model is suitable for representing very sparse
distribution over arbitrary substring pairs, thanks to
reasonable CRP-based smoothing for unseen pairs
based on the spelling model.
3 Proposed method
We propose an extended many-to-many alignment
model that can handle partial noise. We extend the
model in the previous section by introducing a noise
symbol and state-based probability calculation.
? ?
k e y 
(a) no noise
? ?
f l y 
noise
noise
(b) noise
? ?
g i v 
? ? ?
noisee 
(c) partial noise: English
side should be ?give up?
? ?
k e y 
? ?
g i v 
? ? ?
noisee 
r e c 
? ? ?noise
o v e r 
(d) partial noise: Japanese side
should be ??????
Figure 1: Three types of noise in transliteration data.
Solid lines are correct many-to-many alignment links.
3.1 Partial noise in transliteration data
Figure 1 shows transliteration examples with ?no
noise,? ?noise,? and ?partial noise.? Solid lines in the
figure show correct many-to-many alignment links.
The examples (a) and (b) can be distinguished ef-
fectively by Sajjad et al (2012). We aim to do align-
ment as in the examples (c) and (d) by distinguishing
its non-transliteration (noise) part, which cannot be
handled by the existing methods.
3.2 Noise-aware alignment model
We introduce a noise symbol to handle partial noise
in the many-to-many alignment model. Htun et al
(2012) extended the many-to-many alignment for
the sample-wise transliteration mining, but its noise
model only handles the sample-wise noise and can-
not distinguish partial noise. We model partial noise
in the CRP-based joint substring model.
Partial noise in transliteration data typically ap-
pears in compound words as mentioned earlier, be-
cause their counterparts consisting of two or more
words may not be fully covered in automatically ex-
tracted words and phrases as shown in Figure 1(c).
Another type of partial noise is derived from mor-
phological differences due to inflection, which usu-
ally appear in the sub-word level as prefixes and suf-
fixes as shown in Figure 1(d). According to this
intuition, we assume that partial noise appears in
the beginning and/or end of transliteration data (in
case of sample-wise noise, we assume the noise is in
the beginning). This assumption derives a constraint
between signal and noise parts that helps to avoid
a welter of transliteration and non-transliteration
parts. It also has a shortcoming that it is generally
205
? ?
t h e 
? ? ?
sp 
? ? ?sp 
e t c h i n g sp m a s k s 
noise noise noise
Figure 2: Example of many-to-many alignment with par-
tial noise in the beginning and end. ?noise? stands for the
noise symbol and ?sp? stands for a white space.
not appropriate for noise in the middle, but handling
arbitrary number of noise parts increases computa-
tional complexity and sparseness. We rely on this
simple assumption in this paper and consider a more
complex mid-noise problem as future work.
Figure 2 shows a partial noise example in both
the beginning and end. This example is actually
correct translation but includes noise in a sense of
transliteration; an article ?the? is wrongly included
in the phrase pair (no articles are used in Japanese)
and a plural noun ?masks? is transliterated into
?????(mask). These non-transliteration parts are
aligned to noise symbols in the proposed model. The
noise symbols are treated as zero-length substrings
in the model, same as other substrings.
3.3 Constrained Gibbs sampling
Finch and Sumita (2010) used a blocked Gibbs sam-
pling algorithm with forward-filtering backward-
sampling (FFBS) (Mochihashi et al, 2009). We ex-
tend their algorithm for our noise-aware model us-
ing a state-based calculation over the three states:
non-transliteration part in the beginning (noiseB),
transliteration part (signal), non-transliteration part
in the end (noiseE).
Figure 3 illustrates our FFBS steps. At first in
the forward filtering, we begin with transition to
noiseB and signal. The calculation of forward
probabilities itself is almost the same as Finch and
Sumita (2010) except for state transition constraints:
from noiseB to signal, from signal to noiseE. The
backward-sampling traverses a path by probability-
based sampling with true posteriors, starting from
the choice of the ending state among noiseB (means
full noise), signal, and noiseE. This algorithm in-
creases the computational cost by three times to con-
sider three different states, compared to that of Finch
and Sumita (2010).
noiseB
signal
noiseE
noiseB
signal
noiseE
s
s
s
s
s
s
t
t
t
t
t
t
(a) Forward filtering
noiseB
signal
noiseE
noiseB
signal
noiseE
s
s
s
s
s
s
t
t
t
t
t
t
(b) Backward sampling
Figure 3: State-based FFBS for the proposed model.
4 Experiments
We conducted experiments comparing the pro-
posed method with the conventional sample-wise
method for the use in bootstrapping statistical
machine transliteration using Japanese-to-English
patent translation dataset (Goto et al, 2013).
4.1 Training data setup
First, we trained a phrase table on the 3.2M paral-
lel sentences by a standard training procedure using
Moses, with Japanese tokenization using MeCab1.
We obtained 591,840 phrase table entries whose
Japanese side was written in katakana (Japanese
phonogram) only2. Then, we iteratively ran the
method of Sajjad et al (2012) on these entries and
eliminate non-transliteration pairs, until the num-
ber of pairs converged. Finally we obtain 104,563
katakana-English pairs after 10 iterations; they were
our baseline training set mined by sample-wise
method. We used Sajjad et al?s method as pre-
processing for filtering sample-wise noise while the
proposed method could also do that, because the
proposed method took much more training time for
all phrase table entries.
4.2 Transliteration experiments
The transliteration experiment used a translation-
based implementation with Moses, using a
1http://code.google.com/p/mecab/
2This katakana-based filtering is a language dependent
heuristic for choosing potential transliteration candidate, be-
cause transliterations in Japanese are usually written in
katakana.
206
character-based 7-gram language model trained on
300M English patent sentences. We compared three
transliteration models below.
The test set was top-1000 unknown (in the
Japanese-to-English translation model) katakana
words appeared in 400M Japanese patent sentences.
They covered 15.5% of all unknown katakanawords
and 8.8% of all unknown words (excluding num-
bers); that is, more than a half of unknown words
were katakana words.
4.2.1 Sample-wise method (BASELINE)
We used the baseline training set to train sta-
tistical machine transliteration model for our base-
line. The training procedure was based on Moses:
MGIZA++ word alignment, grow-diag-final-and
alignment symmetrization and phrase extraction
with the maximum phrase length of 7.
4.2.2 Proposed method (PROPOSED)
We applied the proposed method to the baseline
training set with 30 sampling iterations and elimi-
nated partial noise. The transliteration model was
trained in the same manner as BASELINE after elim-
inating noise.
The hyperparameters, ?, ?s, and ?t, were op-
timized using a held-out set of 2,000 katakana-
English pairs that were randomly chosen from a
general-domain bilingual dictionary. The hyperpa-
rameter optimization was based on F-score values
on the held-out set with varying ? among 0.01, 0.02,
0.05, 0.1, 1.0, and ?s among 1, 2, 3, 5.
Table 1 compares the statistics on the training sets
of BASELINE and PROPOSED. Note that we ap-
plied the proposed method to BASELINE data (the
sample-wise method was already applied until con-
vergence). The proposed method eliminated only
two transliteration candidates in sample-wise but
also eliminated 5,714 (0.64%) katakana and 55,737
(4.1%) English characters3.
4.2.3 Proposed method using aligned joint
substrings as phrases (PROPOSED-JOINT)
The many-to-many character alignment actually
induces substring pairs, which can be used as
3The reason of larger number of partial noise in English side
would be a syntactic difference as shown in Figure 2 and the
katakana-based filtering heuristics.
Table 1: Statistics of the training sets.
Method #pairs #Ja chars. #En chars.
BASELINE 104,563 899,080 1,372,993
PROPOSED 104,561 893,366 1,317,256
phrases in statistical machine transliteration and
improved transliteration performance (Finch and
Sumita, 2010). We extracted them by: 1) generate
many-to-many word alignment, in which all possi-
ble word alignment links in many-to-many corre-
spondences (e.g., 0-0 0-1 0-2 1-0 1-1 1-2 for ?? ?,
c o m?), 2) run phrase extraction and scoring same as
a standard Moses training. This procedure extracts
longer phrases satisfying the many-to-many align-
ment constraints than the simple use of extracted
joint substring pairs as phrases.
4.3 Results
Table 2 shows the results. We used three evalua-
tion metrics: ACC, F-score, and BLEUc. ACC is
a sample-wise accuracy and F-score is a character-
wise F-measure-like score (Li et al, 2010). BLEUc
is BLEU (Papineni et al, 2002) in the character level
with n=4.
PROPOSED achieved 63% in ACC (16% rela-
tive error reduction from BASELINE), and 94.6% in
F-score (25% relative error reduction from BASE-
LINE). These improvements clearly showed an ad-
vantage of the proposed method over the sample-
wise mining. BLEUc showed a similar improve-
ments. Recall that BASELINE and PROPOSED had
a small difference in their training data, actually
0.64% (katakana) and 4.1% (English) in the num-
ber of characters. The results suggest that the partial
noise can hurt transliteration models.
PROPOSED-JOINT showed similar performance
as PROPOSED with a slight drop in BLEUc, al-
though many-to-many substring alignment was ex-
pected to improve transliteration as reported by
Finch and Sumita (2010). The difference may be
due to the difference in coverage of the phrase
tables; PROPOSED-JOINT retained relatively long
substrings by the many-to-many alignment con-
straints in contrast to the less-constrained grow-
diag-final-and alignments in PROPOSED. Since the
training data in our bootstrapping experiments con-
207
Table 2: Japanese-to-English transliteration results for
top-1000 unknown katakana words. ACC and F-score
stand for the ones used in NEWS workshop, BLEUc is
character-wise BLEU.
Method ACC F-score BLEUc
BASELINE 0.56 0.929 0.864
PROPOSED 0.63 0.946 0.897
PROPOSED-JOINT 0.63 0.943 0.888
tained many similar phrases unlike dictionary-based
data in Finch and Sumita (2010), the phrase table of
PROPOSED-JOINT may have a small coverage due
to long and sparse substring pairs with large prob-
abilities even if the many-to-many alignment was
good. This sparseness problem is beyond the scope
of this paper and worth further study.
4.4 Alignment Examples
Figure 4 shows examples of the alignment results in
the training data. As expected, partial noise both in
Japanese and English was identified correctly in (a),
(b), and (c). There were some alignment errors in the
signal part in (b), in which characters in boundary
positions were aligned incorrectly to adjacent sub-
strings. These alignment errors did not directly de-
grade the partial noise identification but may cause
a negative effect on overall alignment performance
in the sampling-based optimization. (d) is a nega-
tive example in which partial noise was incorrectly
aligned. (c) and (d) have similar partial noise in their
English word endings, but it could not be identified
in (d). One possible reason for that is the sparse-
ness problem mentioned above, as shown in erro-
neous long character alignments in (d).
5 Conclusion
This paper proposed a noise-aware many-to-many
alignment model that can distinguish partial noise in
transliteration pairs for bootstrapping statistical ma-
chine transliteration model from automatically ex-
tracted phrase pairs. The model and training al-
gorithm are straightforward extension of those by
Finch and Sumita (2010). The proposed method
was proved to be effective in Japanese-to-English
transliteration experiments in patent domain.
Future work will investigate the proposed method
? ?
a n sp 
? sp ?
a r c sp t a n g e n t 
? ? ? ? ?noise noise
(a) Correctly aligned
? ? 
d o p 
? ? ? 
i n g sp e n e r g y 
? ? ? ? ? ? ? ? ? 
noise noise
(b) Some alignment errors in transliteration part
? ? 
f o r 
? ? 
m e d 
noise
(c) Correctly aligned
? ? 
c u s 
? ? ? 
t o m i z e d 
? noise
(d) Errors in partial noise
Figure 4: Examples of noise-aware many-to-many align-
ment in the training data. ? stands for a zero-length sub-
string. Dashed lines show incorrect alignments, and bold
grey lines mean their corrections.
in other domains and language pairs. The partial
noise would appear in other language pairs, typ-
ically between agglutinative and non-agglutinative
languages. It is also worth extending the approach
into word alignment in statistical machine transla-
tion.
Acknowledgments
We would like to thank anonymous reviewers for
their valuable comments and suggestions.
References
Andrew Finch and Eiichiro Sumita. 2010. A Bayesian
Model of Bilingual Segmentation for Transliteration.
In Proceedings of the seventh International Workshop
on Spoken Language Translation (IWSLT), pages 259?
266.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K. Tsou. 2013. Overview of the Patent Ma-
chine Translation Task at the NTCIR-10 Workshop. In
The 10th NTCIR Conference, June.
Ohnmar Htun, Andrew Finch, Eiichiro Sumita, and
Yoshiki Mikami. 2012. Improving Transliteration
Mining by Integrating Expert Knowledge with Statis-
tical Approaches. International Journal of Computer
Applications, 58(17):12?22, November.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
208
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-
vouchine. 2010. Whitepaper of NEWS 2010 Shared
Task on Transliteration Generation. In Proceedings
of the 2010 Named Entities Workshop, pages 12?20,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian Unsupervised Word Segmentation
with Nested Pitman-Yor Language Modeling. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 100?108, Suntec, Singapore, August.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and semi-
supervised transliteration mining. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
469?477, Jeju Island, Korea, July. Association for
Computational Linguistics.
209
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1382?1386,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Shift-Reduce Word Reordering for Machine Translation
Katsuhiko Hayashi?, Katsuhito Sudoh, Hajime Tsukada, Jun Suzuki, Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
?hayashi.katsuhiko@lab.ntt.co.jp
Abstract
This paper presents a novel word reordering
model that employs a shift-reduce parser for
inversion transduction grammars. Our model
uses rich syntax parsing features for word re-
ordering and runs in linear time. We apply it to
postordering of phrase-based machine trans-
lation (PBMT) for Japanese-to-English patent
tasks. Our experimental results show that our
method achieves a significant improvement
of +3.1 BLEU scores against 30.15 BLEU
scores of the baseline PBMT system.
1 Introduction
Even though phrase-based machine translation
(PBMT) (Koehn et al, 2007) and tree-based MT
(Graehl and Knight, 2004; Chiang, 2005; Galley
et al, 2006) systems have achieved great success,
many problems remain for distinct language pairs,
including long-distant word reordering.
To improve such word reordering, one promis-
ing way is to separate it from the translation pro-
cess as preordering (Collins et al, 2005; DeNero
and Uszkoreit, 2011) or postordering (Sudoh et al,
2011; Goto et al, 2012). Many studies utilize a rule-
based or a probabilistic model to perform a reorder-
ing decision at each node of a syntactic parse tree.
This paper presents a parser-based word reorder-
ing model that employs a shift-reduce parser for in-
version transduction grammars (ITG) (Wu, 1997).
To the best of our knowledge, this is the first study
on a shift-reduce parser for word reordering.
The parser-based reordering approach uses rich
syntax parsing features for reordering decisions.
Our propoesd method can also easily define such
.
Source-
ordered Target
Sentence (HFE)
Source Sen-
tence (J)
Target Sen-
tence (E)
reordering
Figure 1: A description of the postordering MT system.
non-local features as theN -gram words of reordered
strings. Even when using these non-local features,
the complexity of the shift-reduce parser does not
increase at all due to give up achieving an optimal
solution. Therefore, it works much more efficient.
In our experiments, we apply our proposed
method to postordering for J-to-E patent tasks be-
cause their training data for reordering have little
noise and they are ideal for evaluating reordering
methods. Although our used J-to-E setups need
a language-dependent scheme and we describe our
proposed method as a J-to-E postordering method,
the key algorithm is language-independent and it can
be applicable to preordering as well as postordering
if the training data for reordering are available.
2 Postordering by Parsing
As shown in Fig.1, postordering (Sudoh et al, 2011)
has two steps; the first is a translation step that trans-
lates an input sentence into source-ordered transla-
tions. The second is a reordering step in which the
translations are reordered in the target language or-
der. The key to postordering is the second step.
Goto et al (2012) modeled the second step by
parsing and created training data for a postordering
parser using a language-dependent rule called head-
finalization. The rule moves syntactic heads of a
lexicalized parse tree of an English sentence to the
1382
.S(saw)
. .VP(saw)
. .PP(with)
. .NP(telescope)
. .N(telescope)
.telescope.
D(a)
.a
.
PR(with)
.with
.
VP(saw)
. .NP(girl)
. .N(girl)
.girl.
D(a)
.a
.
V(saw)
.saw
.
NP(I)
.N(I)
.I
.
. .mita. .wo.shoujyo.de.bouenkyo. .wawatashi
.S(saw)
. .VP#(saw)
. .VP#(saw)
. .V(saw)
.saw
.
NP(wo)?a/an?
. .WO(wo)
.wo
N(girl)
.girl.
PP#(with)
. .PR(with)
.with
NP(telescope)?a/an?
.N(telescope)
.telescope
.
NP(wa)?no articles?
. .WA(wa)
.wa.
N(I)
.I
.
. .mita. .wo.shoujyo.de.bouenkyo. .wawatashi
Figure 2: An example of the head-finzalizaton process for an English-Japanese sentence pair: the left-hand side tree
is the original English tree, and the right-hand side tree is its head-final English tree.
end of the corresponding syntactic constituents. As
a result, the terminal symbols of the English tree are
sorted in a Japanese-like order. In Fig.2, we show an
example of head-finalization and a tree on the right-
hand side is a head-finalized English (HFE) tree of
an English tree on the left-hand side. We annotate
each parent node of the swapped edge with # sym-
bol. For example, a nonterminal symbol PP#(with)
shows that a noun phrase ?a/an telescope? and a
word ?with? are inverted.
For better word alignments, Isozaki et al (2012)
also deleted articles ?the? ?a? ?an? from English be-
cause Japanese has no articles, and inserted Japanese
particles ?ga? ?wo? ?wa? into English sentences.
We privilege the nonterminals of a phrase modified
by a deleted article to determine which ?the? ?a/an?
or ?no articles? should be inserted at the front of the
phrase. Note that an original English sentence can
be recovered from its HFE tree by using # symbols
and annotated articles and deleting Japanese parti-
cles.
As well as Goto et al (2012), we solve postorder-
ing by a parser whose model is trained with a set
of HFE trees. The main difference between Goto et
al. (2012)?s model and ours is that while the former
simply used the Berkeley parser (Petrov and Klein,
2007), our shift-reduce parsing model can use such
non-local task specific features as theN -gram words
of reordered strings without sacrificing efficiency.
Our method integrates postediting (Knight and
Chander, 1994) with reordering and inserts articles
into English translations by learning an additional
?insert? action of the parser. Goto et al (2012)
solved the article generation problem by using an
N -gram language model, but this somewhat compli-
cates their approach. Compared with other parsers,
one advantage of the shift-reduce parser is to easily
define such additional operations as ?insert?.
HFE trees can be defined as monolingual ITG
trees (DeNero and Uszkoreit, 2011). Our monolin-
gual ITG G is a tuple G = (V, T, P, I, S) where V
is a set of nonterminals, T is a set of terminals, P
is a set of production rules, I is a set of nontermi-
nals on which ?the? ?a/an? or ?no articles? must be
determined, and S is the start symbol.
Set P consists of terminal production rules that
are responsible for generating word w(? T ):
X ? w
and binary production rules in two forms:
X ? YZ
X# ? YZ
where X, X#, Y and Z are nonterminals. On
the right-hand side, the second rule generates two
phrases Y and Z in the reverse order. In our experi-
ments, we removed all unary production rules.
3 Shift-Reduce Parsing
Given an input sentence w1 . . . wn, the shift-reduce
parser uses a stack of partial derivations, a buffer of
input words, and a set of actions to build a parse tree.
The following is the parser?s configuration:
? : ?i, j, S? : pi
where ? is the step size, S is a stack of elements
s0, s1, . . . , i is the leftmost span index of the stack
1383
top element s0, j is an index of the next input word
of the buffer, and pi is a set of predictor states1.
Each stack element has at least the following com-
ponents of its partial derivation tree:
s = {H, h, wleft, wright, a}
where H is a root nonterminal or a part-of-speech tag
of the subtree, h is a head index of H, a is a variable
to which ?the? ?a/an? ?no articles? or null are as-
signed, and wleft, wright are the leftmost and right-
most words of phrase H. When referring to compo-
nent ?, we use a s.? notation.
Our proposed system has 4 actions shift-X, insert-
x, reduce-MR-X and reduce-SR-X.
The shift-X action pushes the next input word
onto the stack and assigns a part-of-speech tag X to
the word. The deduction step is as follows:
X ? wj ? P
p
? ?? ?
? : ?i, j, S|s?0? : pi
? + 1 : ?j, j + 1, S|s?0|s0)? : {p}
where s0 is {X, j, wj , wj , null}.
The insert-x action determines whether to gener-
ate ?the? ?a/an? or ?no articles? (= x):
s?0.X ? I ? (s?0.a ?= ?the? ? s?0.a ?= ?a/an?)
? : ?i, j, S|s?0)? : pi
? + 1 : ?i, j, S|s0? : pi
where s?0 is {X, h, wleft, wright, a} and s0 is
{X, h, wleft, wright, x} (i ? h, left, right < j).
The side condition prevents the parser from inserting
articles into phrase X more than twice. During pars-
ing, articles are not explicitly inserted into the input
string: they are inserted into it when backtracking to
generate a reordered string after parsing.
The reduce-MR-X action has a deduction rule:
X ? Y Z ? P ? q ? pi
q
? ?? ?
: ?k, i, S|s?2|s?1? : pi? ? : ?i, j, S|s?1|s?0? : pi
? + 1 : ?k, j, S|s?2|s0? : pi?
1Since our notion of predictor states is identical to that in
(Huang and Sagae, 2010), we omit the details here.
s0.wh ? s0.th s0.H s0.H ? s0.th s0.wh ? s0.H
s1.wh ? s1.th s1.H s1.H ? s1.th s1.wh ? s1.H
s2.th ? s2.H s2.wh ? s2.H q0.w q1.w q2.w
s0.tl ? s0.L s0.wl ? s0.L s1.tl ? s1.L s1.wl ? s1.L
s0.wh ? s0.H ? s1.wh ? s1.H s0.H ? s1.wh s0.wh ? s1.H
s0.H ? s1.H s0.wh ? s0.H ? q0.w s0.H ? q0.w
s1.wh ? s1.H ? q0.w s1.H ? q0.w s1.th ? q0.w ? q1.w
s0.wh ? s0.H ? s1.H ? q0.w s0.H ? s1.wh ? s1.H ? q0.w
s0.H ? s1.H ? q0.w s0.th ? s1.th ? q0.w
s0.wh ? s1.H ? q0.w ? q1.w s0.H ? q0.w ? q1.w
s0.th ? q0.w ? q1.w s0.wh ? s0.H ? s1.H ? s2.H
s0.H ? s1.wh ? s1.H ? s2.H s0.H ? s1.H ? s2.wh ? s2.H
s0.H ? s1.H ? s2.H s0.th ? s1.th ? s2.th
s0.H ? s0.R ? s0.L s1.H ? s1.R ? s1.L s0.H ? s0.R ? q0.w
s0.H ? s0.L ? s1.H s0.H ? s0.L ? s1.wh s0.H ? s1.H ? s1.L
s0.wh ? s1.H ? s1.R
s0.wleft ? s1.wright s0.tleft ? s1.tright
s0.wright ? s1.wleft s0.tright ? s1.tleft
s0.a ? s0.wleft s0.a ? s0.tleft s0.a ? s0.wleft ? s1.wright
s0.a ? s0.tleft ? s1.tright s0.a ? s0.wh s0.a ? s0.th
Table 1: Feature templates: s.L and s.R denote the left
and right subnodes of s. l and r are head indices of L and
R. q denotes a buffer element. t is a part-of-speech tag.
where s?0 is {Z, h0, wleft0, wright0, a0} and s?1 is
{Y, h1, wleft1, wright1, a1}. The action generates s0
by combining s?0 and s?1 with binary rule X?Y Z:
s0 = {X, h0, wleft1, wright0, a1}.
New nonterminal X is lexicalized with head word
wh0 of right nonterminal Z. This action expands Y
and Z in a straight order. The leftmost word of
phrase X is set to leftmost word wleft1 of Y, and the
rightmost word of phrase X is set to rightmost word
wright0 of Z. Variable a is set to a1 of Y.
The difference between reduce-MR-X and
reduce-SR-X actions is new stack element s0. The
reduce-SR-X action generates s0 by combining s?0
and s?1 with binary rule X# ?Y Z:
s0 = {X#, h0, wleft0, wright1, a0}.
This action expands Y and Z in a reverse order, and
the leftmost word of X# is set to wleft0 of Z, and the
rightmost word of X# is set to wright1 of Y. Variable
a is set to a0 of Z.
We use a linear model that is discriminatively
trained with the averaged perceptron (Collins and
Roark, 2004). Table 1 shows the feature templates
used in our experiments and we call the features in
the bottom two rows ?non-local? features.
1384
train dev test9 test10
# of sent. 3,191,228 2,000 2,000 2,300
ave. leng. (J) 36.4 36.6 37.0 43.1
ave. leng. (E) 33.3 33.3 33.7 39.6
Table 2: NTCIR-9 and 10 data statistics.
4 Experiments
4.1 Experimental Setups
We conducted experiments for NTCIR-9 and 10
patent data using a Japanese-English language pair.
Mecab2 was used for the Japanese morphological
analysis. The data are summarized in Table 2.
We used Enju (Miyao and Tsujii, 2008) for pars-
ing the English training data and converted parse
trees into HFE trees by a head-finalization scheme.
We extracted grammar rules from all the HFE trees
and randomly selected 500,000 HFE trees to train
the shift-reduce parser.
We used Moses (Koehn et al, 2007) with lexical-
ized reordering and a 6-gram language model (LM)
trained using SRILM (Stolcke et al, 2011) to trans-
late the Japanese sentences into HFE sentences.
To recover the English sentences, our shift-reduce
parser reordered only the 1-best HFE sentence. Our
strategy is much simpler than Goto et al (2012)?s
because they used a linear inteporation of MT cost,
parser cost and N -gram LM cost to generate the best
English sentence from the n-best HFE sentences.
4.2 Main Results
The main results in Table 3 indicate our method was
significantly better and faster than the conventional
PBMT system. Our method also ourperformed Goto
et al (2012)?s reported systems as well as a tree-
based (moses-chart) system3. Our proposed model
with ?non-local? features (w/ nf.) achieved gains
against that without the features (w/o nf.). Further
feature engineering may improve the accuracy more.
4.3 Analysis
We show N -gram precisions of PBMT (dist=6,
dist=20) and proposed systems in Table 5. The re-
sults clearly show that improvements of 1-gram pre-
2https://code.google.com/p/mecab/
3All the data and the MT toolkits used in our experiments
are the same as theirs.
test9 test10
BLEU RIBES BLEU RIBES
HFE w/ art. 28.86 73.45 29.9 73.52
proposed 32.93 76.68 33.25 76.74
w/o art. 19.86 75.62 20.17 75.63
N -gram 32.15 76.52 32.28 76.46
Table 4: The effects of article generation: ?w/o art.? de-
notes evaluation scores for translations of the best system
(?proposed?) in Table 3 from which articles are removed.
?HFE w/ art.? system used HFE data with articles and
generated them by MT system and the shift-reduce parser
performed only reordering. ?N -gram? system inserted
articles into the translations of ?w/o art.? by Goto et al
(2012)?s article generation method.
(1?4)-gram precision
moses (dist=6) 67.1 / 36.9 / 20.7 / 11.5
moses (dist=20) 67.7 / 38.9 / 23.0 / 13.7
proposed 68.9 / 40.6 / 25.7 / 16.7
Table 5: N -gram precisions of moses (dist=6, dist=20)
and proposed systems for test9 data.
cisions are the main factors that contribute to bet-
ter performance of our proposed system than PBMT
systems. It seems that the gains of 1-gram presicions
come from postediting (article generation).
In table 4, we show the effectiveness of our joint
reordering and postediting approach (?proposed?).
The ?w/o art.? results clearly show that generating
articles has great effects on MT evaluations espe-
cially for BLEU metric. Comparing ?proposed? and
?HFEw/ art.? systems, these results show that poste-
diting is much more effective than generating arti-
cles by MT. Our joint approach also outperformed
?N -gram? postediting system.
5 Conclusion
We proposed a shift-reduce word ordering model
and applied it to J-to-E postordering. Our experi-
mental results indicate our method can significantly
improve the performance of a PBMT system.
Future work will investigate our method?s use-
fulness on various language datasets. We plan to
study more general methods that use word align-
ments to embed swap information in trees (Galley
et al, 2006).
1385
test9 test10
BLEU RIBES time (sec.) BLEU RIBES time (sec.)
PBMT (dist=6) 27.1 67.76 2.66 27.92 68.13 3.18
PBMT (dist=12) 29.55 69.84 4.15 30.03 69.88 4.93
PBMT (dist=20) 29.98 69.87 6.22 30.15 69.43 7.19
Tree-based MT** (Goto et al, 2012) 29.53 69.22 ? ? ? ?
PBMT (dist=20)** (Goto et al, 2012) 30.13 68.86 ? ? ? ?
Goto et al (2012)** 31.75 72.57 ? ? ? ?
PBMT (dist=0) + proposed w/o nf. (beam=12) 32.59 76.35 1.46 + 0.01 32.83 76.44 1.7 + 0.01
PBMT (dist=0) + proposed w/o nf. (beam=48) 32.61 76.58 1.46 + 0.06 32.86 76.6 1.7 + 0.06
PBMT (dist=0) + proposed w/ nf. (beam=12) 32.91 76.38 1.46 + 0.01 33.15 76.53 1.7 + 0.02
PBMT (dist=0) + proposed w/ nf. (beam=48) 32.93 76.68 1.46 + 0.07 33.25 76.74 1.7 + 0.07
Table 3: System comparison: time represents the average second per sentence. ** denotes ?not our experiments?.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 111.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531?540.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 193?203.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
961?968.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for japanese-english statisti-
cal machine translation. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 311?316.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105?112.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077?1086.
Hideki Isozaki, Jun Suzuki, Hajime Tsukada, Masaaki
Nagata, Sho Hoshino, and Yusuke Miyao. 2012.
HPSG-based preprocessing for English-to-Japanese
translation. ACM Transactions on Asian Language In-
formation Processing (TALIP), 11(3).
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of the
National Conference on Artificial Intelligence, pages
779?779.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177?180.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human language tech-
nologies 2007: the conference of the North American
chapter of the Association for Computational Linguis-
tics, pages 404?411.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and outlook.
In Proceedings of IEEE Automatic Speech Recognition
and Understanding Workshop.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering in
statistical machine translation. In Proc. MT Summit.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
1386
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1515?1520,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Single-Document Summarization as a Tree Knapsack Problem
Tsutomu Hirao? Yasuhisa Yoshida? Masaaki Nishino? Norihito Yasuda? Masaaki Nagata?
?NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,yoshida.y,nishino.masaaki,
nagata.masaaki}@lab.ntt.co.jp
? Japan Science and Technology Agency
North 14 West 9, Sapporo, Hokkaido, 060-0814, Japan
yasuda@erato.ist.hokudai.ac.jp
Abstract
Recent studies on extractive text summariza-
tion formulate it as a combinatorial optimiza-
tion problem such as a Knapsack Problem, a
Maximum Coverage Problem or a Budgeted
Median Problem. These methods successfully
improved summarization quality, but they did
not consider the rhetorical relations between
the textual units of a source document. Thus,
summaries generated by these methods may
lack logical coherence. This paper proposes a
single document summarization method based
on the trimming of a discourse tree. This is
a two-fold process. First, we propose rules
for transforming a rhetorical structure theory-
based discourse tree into a dependency-based
discourse tree, which allows us to take a tree-
trimming approach to summarization. Sec-
ond, we formulate the problem of trimming
a dependency-based discourse tree as a Tree
Knapsack Problem, then solve it with integer
linear programming (ILP). Evaluation results
showed that our method improved ROUGE
scores.
1 Introduction
State-of-the-art extractive text summarization meth-
ods regard a document (or a document set) as a set
of textual units (e.g. sentences, clauses, phrases)
and formulate summarization as a combinatorial op-
timization problem, i.e. selecting a subset of the set
of textual units that maximizes an objective with-
out violating a length constraint. For example, Mc-
Donald (2007) formulated text summarization as a
Knapsack Problem, where he selects a set of textual
units that maximize the sum of significance scores
of each unit. Filatova et al (2004) proposed a
summarization method based on a Maximum Cov-
erage Problem, in which they select a set of textual
units that maximizes the weighted sum of the con-
ceptual units (e.g. unigrams) contained in the set.
Although, their greedy solution is only an approxi-
mation, Takamura et al (2009a) extended it to ob-
tain the exact solution. More recently, Takamura et
al. (2009b) regarded summarization as a Budgeted
Median Problem and obtain exact solutions with in-
teger linear programming.
These methods successfully improved ROUGE
(Lin, 2004) scores, but they still have one critical
shortcoming. Since these methods are based on sub-
set selection, the summaries they generate cannot
preserve the rhetorical structure of the textual units
of a source document. Thus, the resulting summary
may lack coherence and may not include significant
textual units from a source document.
One powerful and potential way to overcome the
problem is to include discourse tree constraints in
the summarization procedure. Marcu (1998) re-
garded a document as a Rhetorical Structure The-
ory (RST) (William Charles, Mann and Sandra An-
near, Thompson, 1988)-based discourse tree (RST-
DT) and selected textual units according to a prefer-
ence ranking derived from the tree structure to make
a summary. Daume? et al (2002) proposed a docu-
ment compression method that directly models the
probability of a summary given an RST-DT by us-
ing a noisy-channel model. These methods generate
well-organized summaries, however, since they do
not formulate summarizations as combinatorial op-
1515
Root?
N? S?
N? S? N? S?
S? N? N? S? S? N? S? N?
N? N?
N? S?
Elaboration?
Elaboration?
Background?
Elaboration?
Elaboration?
Contrast? Contrast?
Evidence?
Example?
Concession? Antithesis?
Figure 1: Example RST-DT from (Marcu, 1998).
timization problems, the optimality of the generated
summaries is not guaranteed.
In this paper, we propose a single document sum-
marization method based on the trimming of a dis-
course tree based on the Tree Knapsack Problem. If
a discourse tree explicitly represents parent-child re-
lationships between textual units, we can apply the
well-known tree-trimming approach to a discourse
tree and reap the benefit of combinatorial optimiza-
tion methods. In other words, to apply the tree-
trimming approach, we need a tree whose all nodes
represent textual units. Unfortunately, the RST-DT
does not allow it, because textual units in the RST-
DT are located only on leaf nodes and parent-child
relationship between textual units are represented
implicitly at higher positions in a tree. Therefore, we
first propose rules that transform an RST-DT into a
dependency-based discourse tree (DEP-DT) that ex-
plicitly defines the parent-child relationships. Sec-
ond, we treat it as a rooted subtree selection, in other
words, a Tree Knapsack Problem and formulate the
problem as an ILP.
2 From RST-DT to DEP-DT
2.1 RST-DT
According to RST, a document is represented as an
RST-DT whose terminal nodes correspond to ele-
mentary discourse units (EDU)s1 and whose non-
terminal nodes indicate the role of the contiguous
1EDUs roughly correspond to clauses.
Root?
N? S?
N? S? N? S?
S? N? N? S? S? N? S? N?
N? N?
N? S?
0	
1	
2	
4	 6	 7	
5	
3	
8	
10	
9	
11	
12	
13	
14	 15	 18	
16	
17	
Figure 2: Heads of non-terminal nodes.
EDUs namely, ?nucleus (N)? or ?satellite (S)?. A nu-
cleus is more important than a satellite in terms of
the writer?s purpose. That is, a satellite is a child of
a nucleus in the RST-DT. Some discourse relations
such as ?Elaboration?, ?Contrast? and ?Evidence? be-
tween a nucleus and a satellite or two nuclei are de-
fined. Figure 1 shows an example of an RST-DT.
2.2 DEP-DT
An RST-DT is not suitable for tree trimming because
it does not always explicitly define parent-child re-
lationships between textual units. For example, if
we consider how to trim the RST-DT in Figure 1,
when we drop e8, we have to drop e7 because of the
parent-child relationship defined between e7 and e8,
i.e. e7 is a satellite (child) of the nucleus (parent)
e8. On the other hand, we cannot judge whether we
have to drop e9 or e10 because the parent-child rela-
tionships are not explicitly defined between e8 and
e9, e8 and e10. This view motivates us to produce a
discourse tree that explicitly defines parent-child re-
lationships and whose root node represents the most
important EDU in a source document. If we can ob-
tain such a tree, it is easy to formulate summariza-
tion as a Tree Knapsack Problem.
To construct a discourse tree that represents
the parent-child relationships between EDUs, we
propose rules for transforming an RST-DT to a
dependency-based discourse tree (DEP-DT). The
procedure is defined as follows:
1. For each non-terminal node excluding the par-
1516
Depth=1
Depth=2
Depth=3
Depth=4
Figure 3: The DEP-DT obtained from the RST-DT in Fig-
ure 1.
ent of an EDU in the RST-DT, we define a
?head?. Here, a ?head? of a non-terminal node
is the leftmost descendant EDUwhose parent is
N. In Figure 2, ?H? indicates the ?head? of each
node.
2. For each EDU whose parent is N, we pick the
nearest S with a ?head? from the EDU?s ances-
tors and we add the EDU to the DEP-DT as a
child of the head of the S?s parent. If there is no
nearest S, the EDU is the root of the DEP-DT.
For example, in Figure 2, the nearest S to e3
that has a head is node 5 and the head of node
5?s parent is e2. Thus, e3 is a child of e2.
3. For each EDU whose parent is S, we pick the
nearest non-terminal with a ?head? from the an-
cestors and we add the EDU to the DEP-DT as
a child of the head of the non-terminal node.
For example, the nearest non-terminal node of
e9 that has a head is node 16 and the head of
node 16 is e10. Thus, e9 is a child of e10.
Figure 3 shows the DEP-DT obtained from the
RST-DT in Figure 1. The DEP-DT expresses the
parent-child relationship between the EDUs. There-
fore, we have to drop e7, e9 and e10 when we drop
e8. Note that, by applying the rules, discourse rela-
tions defined between non-terminals of an RST-DT
are eliminated. However, we believe that these re-
lations are no needed for the summarization that we
are attempting to realize.
3 Tree Knapsack Model for
Single-Document Summarization
3.1 Formalization
We denote T as a set of all possible rooted subtrees
obtained from a DEP-DT. F (t) is the significance
score for a rooted subtree t ? T and L is the maxi-
mum number of words allowed in a summary. The
optimal subtree t? is defined as follows:
t? = argmax
t?T
F (t) (1)
s.t. Length(t) ? L. (2)
Here, we define F (t) as
F (t) =
?
e?E(t)
W(e)
Depth(e)
. (3)
E(t) is the set of EDUs contained in t, Depth(e)
is the depth of an EDU e within the DEP-DT. For
example, Depth(e2) = 1, Depth(e6) = 4 for the
DEP-DT of Figure 3. W(e) is defined as follows:
W(e) =
?
w?W (e)
tf(w,D). (4)
W (e) is the set of words contained in e and
tf(w,D) is the term frequency of word w in a docu-
ment D.
3.2 ILP Formulation
We formulate the optimization problem in the pre-
vious section as a Tree Knapsack Problem, which is
a kind of Precedence-Constrained Knapsack Prob-
lem (Samphaiboon and Yamada, 2000) and we can
obtain the optimal rooted subtree by solving the fol-
lowing ILP problem2:
maximize
x
N
?
i=1
W(ei)
Depth(ei)
xi (5)
s.t.
N
?
i=1
?ixi ? L (6)
?i : xparent(i) ? xi (7)
?i : xi ? {0, 1}, (8)
2A similar approach has been applied to sentence compres-
sion (Filippova and Strube, 2008).
1517
ROUGE-1 ROUGE-2
F R F R
TKP(G) .310H,K,L .321G,H,K,L .108 .112H
TKP(H) .281H .284H .092 .093
Marcu(G) .291H .272H .101 .093
Marcu(H) .236 .219 .073 .068
MCP .279 .295H .073 .077
KP .251 .266H .071 .075
LEAD .255 .240 .092 .086
Table 1: ROUGE scores of the RST discourse treebank
dataset. In the table, G,H,K,L indicate a method sta-
tistically significant against Marcu(G), Marcu(H), KP,
LEAD, respectively.
where x is an N -dimensional binary vector that
represents the summary, i.e. xi=1 denotes that the i-
th EDU is included in the summary. N is the number
of EDUs in a document, ?i is the length (the number
of words) of the i-th EDU, and parent(i) indicates
the ID of the parent of the i-th EDU in the DEP-DT.
Constraint (6) ensures that the length of a summary
is less than limit L. Constraint (7) ensures that a
summary is a rooted subtree of the DEP-DT. Thus,
xparent(i) is always 1 when the i-th EDU is included
in the summary.
In general, the Tree Knapsack Problem is NP-
hard, but fortunately we can obtain the optimal solu-
tion in a feasible time by using ILP solvers for doc-
uments of practical tree size. In addition, bottom-
up DP (Lukes, 1974) and depth-first DP algorithms
(Cho and Shaw, 1997) are known to find the optimal
solution efficiently.
4 Experimental Evaluation
4.1 Settings
We conducted an experimental evaluation on the test
collection for single document summarization eval-
uation contained in the RST Discourse Treebank
(RST-DTB)(Carlson et al, 2001) distributed by the
Linguistic Data Consortium (LDC)3. The RST-DTB
Corpus includes 385 Wall Street Journal articles
with RST annotation, and 30 of these documents
also have one human-made reference summary. The
average length of the reference summaries corre-
sponds to about 10 % of the words in the source
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2002T07
document.
We compared our method (TKP) with Marcu?s
method (Marcu) (Marcu, 1998), a simple knapsack
model (KP), a maximum coverage model (MCP)
and a lead method (LEAD). MCP is known to be a
state-of-the-art method for multiple document sum-
marization and we believe that MCP also performs
well in terms of single document summarization.
LEAD is also a widely used summarizer that simply
takes the first K textual units of the document. Al-
though this is a simple heuristic rule, it is known as
a state-of-the-art summarizer (Nenkova and McKe-
own, 2011).
For our method, we examined two types of
DEP-DT. One was obtained from the gold RST-
DT. The other was obtained from the RST-DT pro-
duced by a state-of-the-art RST parser, HILDA (du-
Verle and Prendinger, 2009; Hernault et al, 2010).
For Marcu?s method, we examined both the gold
RST-DT and HILDA?s RST-DT. We re-implemented
HILDA and re-trained it on the RST-DT Corpus ex-
cluding the 30 documents used in the evaluation.
The F-score of the parser was around 0.5. For KP,
we exclude constraint (7) from the ILP formulation
of TKP and set the depth of all EDUs in equations
(3) and (5) at 1. For MCP, we use tf (equation (4))
as the word weight.
We evaluated the summarization systems with
ROUGE version 1.5.5 4. Performance metrics were
the recall (R) and F-score (F) of ROUGE-1,2.
4.2 Results and Discussion
Table 1 shows the evaluation results. In the ta-
ble, TKP(G) and TKP(H) denote methods with the
DEP-DT obtained from the gold RST-DT and from
HILDA, respectively. Marcu(G) and Marcu(H) de-
note Marcu?s method described in (Marcu, 1998)
with gold RST-DT and with HILDA, respectively.
We performed a multiple comparison test for the dif-
ferences among ROUGE scores, we calculated the p-
values between systems with the Wilcoxon signed-
rank test (Wilcoxon, 1945) and used the False Dis-
covery Rate (FDR) (Benjamini and Hochberg, 1995)
to calculate adjusted p-values, in order to limit false
positive rate to 5%.
From the table, TKP(G) and Marcu(G) achieved
4Options used: -n 2 -s -m -x
1518
Reference:
The Fuji apple may one day replace the Red Delicious as the number one U.S. apple. Since the Red Delicious has been
over-planted and prices have dropped to new lows, the apple industry seems ready for change. Along with growers, supermarkets
are also trying different varieties of apples. Although the Fuji is smaller and not as perfectly shaped as the Red Delicious, it is
much sweeter, less mealy and has a longer shelf life.
TKP(G):
We?ll still have mom and apple pie. A Japanese apple called the Fuji. Some fruit visionaries say the Fuji could someday tumble
the Red Delicious from the top of America?s apple heap. It has a long shelf life. Now, even more radical changes seem afoot. The
Delicious hegemony won?t end anytime soon. New apple trees grow slowly. But the apple industry is ripe for change. There?s a
Fuji apple cult.
Marcu(G):
We?ll still have mom and apple pie. On second thought, make that just mom. The Fuji could someday tumble the Red Delicious
from the top of America?s apple heap. Now, even more radical changes seem afoot. The Delicious hegemony won?t end anytime
soon. More than twice as many Red Delicious apples are grown as the Golden variety, America?s No. 2 apple. But the apple
industry is ripe for change.
MCP:
Called the Fuji. It has a long shelf life. New apple trees grow slowly. Its roots are patriotic. I?m going to have to get another job
this year. Scowls. They still buy apples mainly for big, red good looks. Japanese researchers have bred dozens of strains of Fujis.
Mr. Auvil, the Washington grower, says. Stores sell in summer. The ? big boss ? at a supermarket chain even rejected his Red
Delicious recently. Many growers employ.
LEAD:
Soichiro Honda?s picture now hangs with Henry Ford?s in the U.S. Automotive Hall of Fame, and the game-show ? Jeopardy ? is
soon to be Sony-owned. But no matter how much Japan gets under our skin, we?ll still have mom and apple pie. On second
thought, make that just mom. A Japanese apple called the Fuji is cropping up in orchards the way Hondas did on U.S. roads.
Figure 4: Summaries obtained from wsj 1128.
better results than MCP, KP and LEAD, although
some of the comparisons are not significant. In par-
ticular, TKP(G) achieved the highest ROUGE scores
on all measures. On ROUGE-1 Recall, TKP(G) sig-
nificantly outperformed Marcu(G), Marcu(H), KP
and LEAD. These results support the effectiveness
of our method that utilizes the discourse structure.
Comparing TKP(H) with Marcu(H), the former
achieved higher scores with statistical significance
on ROUGE-1. In addition, Marcu(H) was outper-
formed by MCP, KP and LEAD. The results confirm
the effectiveness of our summarization model and
trimming proposal for DEP-DT. Moreover, the dif-
ference between TKP(G) and TKP(H) was smaller
than that between Marcu(G) and Marcu(H). This
implies that our method is more robust against dis-
course parser error than Marcu?s method.
Figure 4 shows the example summaries gener-
ated by TKP(G), Marcu(G), MCP and LEAD, re-
spectively for an article, wsj 1128. Since TKP(G)
and Marcu(G) utilize a discourse tree, the summary
generated by TKP(G) is similar to that generated by
Marcu(G) but it is different from those generated by
MCP and LEAD.
5 Conclusion
This paper proposed rules for transforming an RST-
DT to a DEP-DT to obtain the parent-child relation-
ships between EDUs. We treated a single document
summarization method as a Tree Knapsack Problem,
i.e. the summarizer selects the best rooted subtree
from a DEP-DT. To demonstrate the effectiveness of
our method, we conducted an experimental evalua-
tion using 30 documents selected from the RST Dis-
course Treebank Corpus. The results showed that
our method achieved the highest ROUGE-1,2 scores.
References
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the false discovery rate: A practical and powerful
approach to multiple testing. Journal of the Royal Sta-
tistical Society, Series B (Methodological), 57(1):289?
300.
Lynn Carlson, Daniel Marcu, andMary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proc. of
the SIGDIAL01, pages 1?10.
Geon Cho and Dong X Shaw. 1997. A depth-first
dynamic programming algorithm for the tree knap-
1519
sack problem. INFORMS Journal on Computing,
9(4):431?438.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proc. of the 40th
ACL, pages 449?456.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine clas-
sification. In Proc. of the Joint Conference of the 47th
ACL and 4th IJCNLP, pages 665?673.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence extraction. In Proc. of the 20th COLING,
pages 397?403.
Katja Filippova and Michael Strube. 2008. Dependency
tree based sentence compression. In Proc. of the 5th
International Natural Language Generation Confer-
ence (INLG), pages 25?32.
Hugo Hernault, Helmut Prendinger, David A duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A discourse
parser using support vector machine classification. Di-
alogue and Discourse, 1(3):1?33.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proc. of Workshop on
Text Summarization Branches Out, pages 74?81.
J. A. Lukes. 1974. Efficient algorithm for the partition-
ing of trees. IBM Journal of Research and Develop-
ment, 18(3):217?224.
Daniel Marcu. 1998. Improving summarization through
rhetorical parsing tuning. In Proc. of the 6th Workshop
on Very Large Corpora, pages 206?215.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Proc.
of the 29th ECIR, pages 557?564.
Ani Nenkova and Kathaleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in In-
formation Retrieval, 5(2-3):103?233.
Natthawut Samphaiboon and Takeo Yamada. 2000.
Heuristic and exact algorithms for the precedence-
constrained knapsack problem. Journal of Optimiza-
tion Theory and Applications, 105(3):659?676.
Hiroya Takamura and Manabu Okumura. 2009a. Text
summarization model based on maximum coverage
problem and its variant. In Proc. of the 12th EACL,
pages 781?789.
Hiroya Takamura and Manabu Okumura. 2009b. Text
summarization model based on the budgeted median
problem. In Proceedings of the 18th CIKM.
Frank Wilcoxon. 1945. Individual comparisons by rank-
ing methods. Biometrics Bulletin, 1(6):80?83.
William Charles, Mann and Sandra Annear, Thompson.
1988. Rhetorical Structure Theory: Toward a func-
tional theory of text organization. Text, 8(3):243?281.
1520
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1834?1839,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Dependency-based Discourse Parser for Single-Document Summarization
Yasuhisa Yoshida, Jun Suzuki, Tsutomu Hirao, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{yoshida.y,suzuki.jun,hirao.tsutomu,nagata.masaaki}@lab.ntt.co.jp
Abstract
The current state-of-the-art single-
document summarization method gen-
erates a summary by solving a Tree
Knapsack Problem (TKP), which is the
problem of finding the optimal rooted sub-
tree of the dependency-based discourse
tree (DEP-DT) of a document. We can
obtain a gold DEP-DT by transforming a
gold Rhetorical Structure Theory-based
discourse tree (RST-DT). However, there
is still a large difference between the
ROUGE scores of a system with a gold
DEP-DT and a system with a DEP-DT
obtained from an automatically parsed
RST-DT. To improve the ROUGE score,
we propose a novel discourse parser
that directly generates the DEP-DT. The
evaluation results showed that the TKP
with our parser outperformed that with
the state-of-the-art RST-DT parser, and
achieved almost equivalent ROUGE
scores to the TKP with the gold DEP-DT.
1 Introduction
Discourse structures of documents are believed
to be highly beneficial for generating informa-
tive and coherent summaries. Several discourse-
based summarization methods have been devel-
oped, such as (Marcu, 1998; Daum?e III and
Marcu, 2002; Hirao et al., 2013; Kikuchi et al.,
2014). Moreover, the current best ROUGE score
for the summarization benchmark data of the RST-
discourse Treebank (Carlson et al., 2002) has been
provided by (Hirao et al., 2013), whose method
also utilizes discourse trees. Thus, the discourse-
based summarization approach is one promising
way to obtain high-quality summaries.
One possible weakness of discourse-based sum-
marization techniques is that they rely greatly on
the accuracy of the discourse parser they use.
For example, the above discourse-based summa-
rization methods utilize discourse trees based on
the Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988) for their discourse information.
Unfortunately, the current state-of-the-art RST
parser, as described in (Hernault et al., 2010),
is insufficient as an off-the-shelf discourse parser.
In fact, there is empirical evidence that the qual-
ity (i.e., ROUGE score) of summaries from auto-
parsed discourse trees is significantly degraded
compared with those generated from gold dis-
course trees (Marcu, 1998; Hirao et al., 2013).
From this background, the goal of this paper
is to develop an appropriate discourse parser for
discourse-based summarization. We first focus on
one of the best discourse-based single document
summarization methods as proposed in (Hirao et
al., 2013). Their method formulates a single doc-
ument summarization problem as a Tree Knap-
sack Problem (TKP) over a dependency-based dis-
course tree (DEP-DT). In their method, DEP-DTs
are automatically transformed from (auto-parsed)
RST-discourse trees (RST-DTs) by heuristic rules.
Instead, we develop a DEP-DT parser, that di-
rectly provides DEP-DTs for their state-of-the-art
discourse-based summarization method. We show
that summaries generated by our parser improve
the ROUGE scores to almost the same level as
those generated by gold DEP-DTs. We also inves-
tigate the way in which the parsing accuracy helps
to improve the ROUGE scores.
2 Single-Document Summarization as a
Tree Knapsack Problem
Hirao et al. (2013) formulated single-document
summarization as a TKP that is run on the DEP-
DT. They obtained a summary by trimming the
DEP-DT, i.e. the summary is a rooted subtree of
the DEP-DT.
Suppose that we have N EDUs in a document,
1834
Root!
N! S!
N! S! N! S!
S! N! N! S! S! N! S! N!
N! N!
N! S!
Elaboration!
Background!
Elaboration!
Elaboration!
Contrast! Contrast!
Evidence!
Example!
Concession! Antithesis!
(a)
Background Elabora.on
Elabora.on
Elabora.on Elabora.on
Concession Example
Evidence
An.thesis
(b)
Background Elabora.on Elabora.on
Elabora.on Elabora.on Concession Example
Evidence An.thesis
(c)
Figure 1: Examples of RST-DT and DEP-DT. e
1
, ? ? ? , e
10
are EDUs. (a) Example of an RST-DT from
(Marcu, 1998). n
1
, ? ? ? , n
19
are the non-terminal nodes. (b) Example of the DEP-DT obtained from the
incorrect RST-DT that is made by swapping the Nucleus-Satellite relationship of the node n
2
and the
node n
3
. (c) The correct DEP-DT obtained from the RST-DT in (a).
and the i-th EDU e
i
has l
i
words. L is the maxi-
mum number of words allowed in a summary. In
the TKP, if we select e
i
, we need to select its par-
ent EDU in the DEP-DT. We denote parent(i) as
the index of the parent of e
i
in the DEP-DT. x is
an N -dimensional binary vector that represents a
summary, i.e. x
i
= 1 denotes that e
i
is included in
the summary. The TKP is defined as the following
ILP problem:
maximize
x
?
N
i=1
F (e
i
)x
i
s.t.
?
N
i=1
l
i
x
i
? L
?i : x
parent(i)
? x
i
?i : x
i
? {0, 1},
where F (e
i
) is the score of e
i
. We define F (e
i
) as
follows:
F (e
i
) =
?
w?W (e
i
)
tf(w,D)
Depth(e
i
)
,
where W (e
i
) is the set of words contained in e
i
.
tf(w,D) is the term frequency of word w in a doc-
ument D. Depth(e
i
) is the depth of e
i
in the DEP-
DT.
3 Tree Knapsack Problem with
Dependency-based Discourse Parser
3.1 Motivation
In (Hirao et al., 2013), they automatically ob-
tain the DEP-DT by transforming from the parsed
RST-DT. We simply followed their method for ob-
taining the DEP-DTs
1
. The transformation algo-
rithm can be found in detail in (Hirao et al., 2013).
Figure 1(a) shows an example of the RST-DT. Ac-
cording to RST, a document is represented as a tree
whose terminal nodes correspond to elementary
discourse units (EDUs) and whose non-terminal
nodes indicate the role of the contiguous EDUs,
namely, ?nucleus (N)? or ?satellite (S)?. Since a nu-
cleus is more important than a satellite in terms of
the writer?s purpose, a satellite is always a child of
a nucleus in the RST-DT. Some discourse relations
between a nucleus and a satellite or two nuclei are
defined.
Since the TKP of (Hirao et al., 2013) employs
a DEP-DT obtained from an automatically parsed
RST-DT, their method strongly relies on the ac-
curacy of the RST parser. For example, in Fig-
ure 1(a), if the RST-DT parser incorrectly sets
the node n
2
as Satellite and the node n
3
as Nu-
cleus, we obtain an incorrect DEP-DT in Figure
1(b) because the transformation algorithm uses
the Nucleus-Satellite relationships in the RST-DT.
The dependency relationships in Figure 1(b) are
quite different from that of the correct DEP-DT in
Figure 1(c). In this example, the parser failed to
determine the most salient EDU e
2
, that is the root
EDU of the gold DEP-DT. Thus, the summary ex-
tracted from this DEP-DT will have a low ROUGE
score.
The results motivated us to design a new dis-
course parser fully trained on the DEP-DTs and
1
Li et al. also defined a similar transformation algorithm
(Li et al., 2014). In this paper, we follow the transformation
algorithm defined in (Hirao et al., 2013).
1835
Discourse)Dependency)Parser
Document Summary
Discourse)Dependency)Parser
Tree)Knapsack)Problem
Parser)Training)Phase
Summariza;on)Phase
e2#e1# e3# e8# e10#e4# e5# e7# e9#e6#
Elabora3on# Elabora3on#Elabora3on# Elabora3on# An3thesis#Example#Evidence# Concession#
Background# e2#e1# e3# e8# e10#e4# e5# e7# e9#e6#
Elabora3on# Elabora3on#Elabora3on# Elabora3on# An3thesis#Example#Evidence# Concession#
Background# e2#e1# e3# e8# e10#e4# e5# e7# e9#e6#
Elabora3on# Elabora3on#Elabora3on# Elabora3on# An3thesis#Example#Evidence# Concession#
Background#
DEP=DTs
e2#e1# e3# e8# e10#e4# e5# e7# e9#e6#
Elabora3on# Elabora3on#Elabora3on# Elabora3on# An3thesis#Example#Evidence# Concession#
Background#DEP=DT
Root$N$ S$N$ S$ N$ S$S$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$Root$N$ S$N$ S$ N$ S$S$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$Root$N$ S$N$ S$ N$ SS$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$
RST=DTs
Transforma;on)Algorithm
(a)
e2#e1# e3# e8# e10#e4# e5# e7# e9#e6#
Elabora3on# Elabora3on#Elabora3on# Elabora3on# An3thesis#Example#Evidence# Concession#
Background#
RST$Parser
Document Root$N$ S$N$ S$ N$ S$S$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$ SummaryRST$Parser
Transforma3on$Algorithm Tree$Knapsack$Problem
Parser$Training$Phase
Summariza3on$Phase
Root$N$ S$N$ S$ N$ S$S$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$Root$N$ S$N$ S$ N$ S$S$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$Root$N$ S$N$ S$ N$ SS$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$
RST>DTs
RST>DT DEP>DT
(b)
Figure 2: (a) Overview of our proposed method. In the parser training phase, the parser is trained on
the DEP-DTs, and in the summarization phase, the document is directly parsed into the DEP-DT. (b)
Overview of (Hirao et al., 2013). In the parser training phase, the parser is trained on RST-DTs, and
in the summarization phase, the document is parsed into the RST-DT, and then transformed into the
DEP-DT.
that could directly generate the DEP-DT. Figure
2(a) shows an overview of the TKP combined with
our DEP-DT parser. In the parser training phase,
we transform RST-DTs into DEP-DTs, and di-
rectly train our parser with the DEP-DTs. In the
summarization phase, our method parses a raw
document directly into a DEP-DT, and generates
a summary with the TKP.
3.2 Description of Discourse Dependency
Parser
Our parser is based on the first-order Maximum
Spanning Tree (MST) algorithm (McDonald et al.,
2005b). Our parser extracts the features from the
EDU e
i
and the EDU e
j
. We use almost the fea-
tures as those shown in (Hernault et al., 2010).
Lexical N-gram features use the beginning (or
end) lexical N-grams (N ? {1, 2, 3}) in e
i
and
e
j
. We also include POS tags for the beginning
(or end) lexical N-grams (N ? {1, 2, 3}) in e
i
and
e
j
. Organizational features include the distance
between e
i
and e
j
. They also include the num-
ber of tokens, and features for identifying whether
or not e
i
and e
j
belong to the same sentence (or
paragraph). Soricut et al. (2003) introduced dom-
inance set features. They include syntactic labels
and the lexical heads of head and attachment nodes
along with their dominance relationship. We can-
not use the strong compositionality features and
rhetorical structure features described in (Her-
nault et al., 2010) because we have to know the
subtree structures in advance when using these
features.
To train the parser, we choose the Margin In-
fused Relaxed Algorithm (MIRA) (McDonald et
al., 2005a; Crammer et al., 2006). We denote
s(w,y) = w
T
f
y
as a score function given a
weight vector w and a DEP-DT y. L(y,y
?
) is
a loss function, and we define it as the number of
EDUs that have an incorrect parent EDU in a pre-
dicted DEP-DT y
?
= arg max
y
s(w,y). Then, we
solve the following optimization problem:
min
w
||w ?w
(t)
||
s.t. s(w,y) ? s(w,y
?
) ? L(y,y
?
),
(1)
where w
(t)
is a weight vector in the t-th iteration.
3.3 Redesign of Loss Function for Tree
Knapsack Problem
When we make a summary by solving a TKP, we
do not necessarily need a DEP-DT where all of the
parent-child relationships are correct. This is be-
cause we rarely select the EDUs around the leaves
in the DEP-DT. On the other hand, the parent-
child relationships around the root EDU in the
DEP-DT are important because we often select the
EDUs around the root EDU. Incorporating these
intuitions enables us to develop a DEP-DT parser
optimized for the TKP. To incorporate this infor-
mation, we define the following loss function:
L
Depth
(y,y
?
) =
?
(i,r,j)?y
[1 ? I(y
?
, i, j)]
Depth(e
i
)
, (2)
where I(y
?
, i, j) is an indicator function that
equals 1 if EDU e
j
is the parent of EDU e
i
in the
1836
DEP-DT y
?
and 0 otherwise. In Section 4, we re-
port results with the original loss function L(?, ?)
and with the modified loss function L
Depth
(?, ?).
4 Experimental Evaluation
4.1 Corpus
We used the RST-DT corpus (Carlson et al., 2002)
for our experimental evaluations. The corpus con-
sists of 385 Wall Street Journal articles with RST
annotation, and 30 of these documents also have
one human-made reference summary. We used
these 30 documents as the test documents for the
summarization evaluation, and used the remaining
355 RST annotated documents as the training data
for the parser. Note that we did not use the 30 test
documents for the summarization evaluation when
we trained the parser.
4.2 Summarization Evaluation
We compared the following three systems that dif-
fer in the way they obtain the DEP-DT.
TKP-GOLD Used a DEP-DT converted from a
gold RST-DT.
TKP-DIS-DEP Used a DEP-DT automatically
parsed by our discourse dependency-based
parser (DIS-DEP). Figure 2(a) shows an
overview of this system.
TKP-DIS-DEP-LOSS Used a DEP-DT automat-
ically parsed by our discourse dependency-
based parser (DIS-DEP). Figure 2(a) shows
an overview of this system. It is trained with
the loss function defined in equation (2).
TKP-HILDA Used a DEP-DT obtained by trans-
forming a RST-DT parsed by HILDA, a state-
of-the-art RST-DT parser (Hernault et al.,
2010). Figure 2(b) shows an overview of this
system.
Hirao et al. (2013) proved that TKP-HILDA
outperformed other methods including Marcu?s
method (Marcu, 1998), a simple knapsack model,
a maximum coverage model and LEAD method
that simply takes the first L tokens (L = summary
length). Thus, we only employed TKP-HILDA as
our baseline.
We follow the evaluation conditions described
in (Hirao et al., 2013). The number of tokens in
each summary is determined by the number in the
ROUGE-1 ROUGE-2
TKP-GOLD 0.321 0.112
TKP-DIS-DEP 0.319 0.109
TKP-DIS-DEP-LOSS 0.323 0.121
TKP-HILDA 0.284 0.093
Table 1: ROUGE Recall scores
human-annotated reference summary. The aver-
age length of the reference summaries corresponds
to about 10% of the words in the source document.
This is also the commonly used evaluation con-
dition for single-document summarization evalu-
ation on the RST-DT corpus. We employed the
recall of ROUGE-1, 2 as the evaluation measures.
Table 1 shows ROUGE scores on the RST-DT
corpus. We can see TKP-DIS-DEP and TKP-
DIS-DEP-LOSS outperformed TKP-HILDA, and
achieved almost the same ROUGE scores as TKP-
GOLD. Wilcoxon?s signed rank test in terms
of ROUGE rejected the null hypothesis, ?there
is a difference between TKP-HILDA and TKP-
DIS-DEP (or TKP-DIS-DEP-LOSS)? (Wilcoxon,
1945). This would be because test documents are
relatively small.
We analyzed the differences between the pro-
posed systems (TKP-DIS-DEP and TKP-DIS-
DEP-LOSS) and TKP-HILDA. First, we evaluated
the overlaps between the EDUs in summaries gen-
erated by the system and the EDUs in summaries
generated by TKP-GOLD. To see the overlaps, we
calculated the average F-value using Recall and
Precision defined as follows: Recall = |S
s
?
S
g
|/|S
g
|, Precision = |S
s
? S
g
|/|S
s
|, where S
s
is a set of EDUs in a summary generated by a sys-
tem, and S
g
a set of EDUs in a summary generated
by TKP-GOLD. The first line in Table 2 shows the
results. TKP-DIS-DEP and TKP-DIS-DEP-LOSS
outperformed TKP-HILDA as regards the aver-
age F-values. The result revealed that TKP-DIS-
DEP and TKP-DIS-DEP-LOSS have more EDUs
in common with TKP-GOLD than TKP-HILDA.
This result is evidence that TKP-DIS-DEP and
TKP-DIS-DEP-LOSS outperformed TKP-HILDA
in terms of ROUGE score.
Second, we evaluated the root accuracy (RA),
the rate at which a parser can find the root of DEP-
DTs. Since the root of a gold DEP-DT is the most
salient EDU in a document, it should be included
in the summary. The second line in Table 2 shows
that our methods succeeded in extracting the root
1837
TKP-DIS-DEP TKP-DIS-DEP-LOSS TKP-HILDA
Avg F-value 0.532
?
0.532
?
0.415
RA 0.933
?
0.933
?
0.733
Avg DAS 0.847
?
0.843
?
0.596
?: significantly better than TKP-HILDA (p < .05)
Table 2: Average F-value, Root Accuracy (RA), and average Dependency Accuracy in Summary (DAS).
Wilcoxon?s signed rank test in terms of average F-value, RA and DAS accepted the null hypothesis.
TKP-GOLD:
Elcotel Inc. expects fiscal second-quarter earnings to trail 1988 results. Elcotel, a telecommunications company, had net
income of $272,000, or five cents a share, in its year-earlier second quarter. The lower results, Mr. Pierce said. Elcotel will
also benefit from moving into other areas. Elcotel has also developed an automatic call processor. Automatic call processors
will provide that system for virtually any telephone, Mr. Pierce said, not just phones.
TKP-DIS-DEP, TKP-DIS-DEP-LOSS:
Elcotel Inc. expects fiscal second-quarter earnings to trail 1988 results. Elcotel, a telecommunications company, had net
income of $272,000, or five cents a share, in its year-earlier second quarter. George Pierce, chairman and chief executive officer,
said in an interview. Although Mr. Pierce expects that line of business to strengthen in the next year. Elcotel will also benefit
from moving into other areas. Elcotel has also developed an automatic call processor.
TKP-HILDA:
Elcotel Inc. expects fiscal second-quarter earnings to trail 1988 results. That several new products will lead to a ?much
stronger? performance in its second half. George Pierce, chairman and chief executive officer, said in an interview. Mr.
Pierce said Elcotel should realize a minimum of $10 of recurring net earnings for each machine each month. Elcotel has also
developed an automatic call processor. Automatic call processors will provide that system for virtually any telephone.
Figure 3: Summaries of wsj 2317. The sentences shown in bold-face are the root EDUs in each DEP-DT
of the summary.
of DEP-DT with high accuracy.
Third, to evaluate the coherency of the gener-
ated summaries, we compared the average Depen-
dency Accuracy in Summary (DAS), which is de-
fined as follows:
DAS(S) =
1
|S|
?
e?S
?(e),
?(e) =
{
1 (if parent(e) ? S)
0 (otherwise),
where S is a set of EDUs contained in the sum-
mary and parent(e) returns the parent EDU of e
in the gold DEP-DT. DAS(S) measures the rate of
the correct parent-child relationships in S. When
DAS equals 1, the summary is a rooted subtree of
the gold DEP-DT. The third line in Table 2 shows
the results. The results demonstrate that the sum-
maries generated by TKP-DIS-DEP or TKP-DIS-
DEP-LOSS tend to preserve the upper level depen-
dency relationships between the EDUs within the
gold DEP-DT.
Figure 3 shows summaries of wsj 2317 gener-
ated by the three systems. The EDUs correspond-
ing to the root of the DEP-DT are used in each
system shown in boldface. We can see that the
root EDU in the gold DEP-DT is found in the
summaries generated by TKP-DIS-DEP and TKP-
DIS-DEP-LOSS, but not in the summary gener-
ated by TKP-HILDA.
5 Conclusion
In this paper, we proposed a novel dependency-
based discourse parser for single-document sum-
marization. The parser enables us to obtain the
DEP-DT without transforming the RST-DT. The
evaluation results showed that the TKP with our
parser outperformed that with the state-of-the-art
RST-DT parser, and achieved almost equivalent
ROUGE scores to the TKP with the gold DEP-DT.
References
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2002. Rst discourse treebank,
ldc2002t07.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research, 7:551?585.
1838
Hal Daum?e III and Daniel Marcu. 2002. A noisy-
channel model for document compression. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 449
? 456, Philadelphia, PA, July 6 ? 12.
Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,
et al. 2010. Hilda: a discourse parser using support
vector machine classification. Dialogue and Dis-
course, 1(3).
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,
Norihito Yasuda, and Masaaki Nagata. 2013.
Single-document summarization as a tree knapsack
problem. In Proceedings of the 2013 Conference on
EMNLP, pages 1515?1520.
Yuta Kikuchi, Tsutomu Hirao, Hiroya Takamura, Man-
abu Okumura, and Masaaki Nagata. 2014. Single
document summarization based on nested tree struc-
ture. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 315?320, Baltimore,
Maryland, June. Association for Computational Lin-
guistics.
Sujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li.
2014. Text-level discourse dependency parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 25?35, Baltimore, Maryland,
June. Association for Computational Linguistics.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Daniel Marcu. 1998. Improving summarization
through rhetorical parsing tuning. In Proc. of The
6th Workshop on VLC, pages 206?215.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 91?98, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.
Frank Wilcoxon. 1945. Individual Comparisons by
Ranking Methods. Biometrics Bulletin, 1(6):80?83,
December.
1839
Proceedings of the ACL 2010 Conference Short Papers, pages 137?141,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Word Alignment with Synonym Regularization
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
{shindo,a.fujino}@cslab.kecl.ntt.co.jp
nagata.masaaki@lab.ntt.co.jp
Abstract
We present a novel framework for word
alignment that incorporates synonym
knowledge collected from monolingual
linguistic resources in a bilingual proba-
bilistic model. Synonym information is
helpful for word alignment because we
can expect a synonym to correspond to
the same word in a different language.
We design a generative model for word
alignment that uses synonym information
as a regularization term. The experimental
results show that our proposed method
significantly improves word alignment
quality.
1 Introduction
Word alignment is an essential step in most phrase
and syntax based statistical machine translation
(SMT). It is an inference problem of word cor-
respondences between different languages given
parallel sentence pairs. Accurate word alignment
can induce high quality phrase detection and trans-
lation probability, which leads to a significant im-
provement in SMT performance. Many word
alignment approaches based on generative mod-
els have been proposed and they learn from bilin-
gual sentences in an unsupervised manner (Vo-
gel et al, 1996; Och and Ney, 2003; Fraser and
Marcu, 2007).
One way to improve word alignment quality
is to add linguistic knowledge derived from a
monolingual corpus. This monolingual knowl-
edge makes it easier to determine corresponding
words correctly. For instance, functional words
in one language tend to correspond to functional
words in another language (Deng and Gao, 2007),
and the syntactic dependency of words in each lan-
guage can help the alignment process (Ma et al,
2008). It has been shown that such grammatical
information works as a constraint in word align-
ment models and improves word alignment qual-
ity.
A large number of monolingual lexical seman-
tic resources such as WordNet (Miller, 1995) have
been constructed in more than fifty languages
(Sagot and Fiser, 2008). They include word-
level relations such as synonyms, hypernyms and
hyponyms. Synonym information is particularly
helpful for word alignment because we can ex-
pect a synonym to correspond to the same word
in a different language. In this paper, we explore a
method for using synonym information effectively
to improve word alignment quality.
In general, synonym relations are defined in
terms of word sense, not in terms of word form. In
other words, synonym relations are usually con-
text or domain dependent. For instance, ?head?
and ?chief? are synonyms in contexts referring to
working environment, while ?head? and ?forefront?
are synonyms in contexts referring to physical po-
sitions. It is difficult, however, to imagine a con-
text where ?chief? and ?forefront? are synonyms.
Therefore, it is easy to imagine that simply replac-
ing all occurrences of ?chief? and ?forefront? with
?head? do sometimes harm with word alignment
accuracy, and we have to model either the context
or senses of words.
We propose a novel method that incorporates
synonyms from monolingual resources in a bilin-
gual word alignment model. We formulate a syn-
onym pair generative model with a topic variable
and use this model as a regularization term with a
bilingual word alignment model. The topic vari-
able in our synonym model is helpful for disam-
biguating the meanings of synonyms. We extend
HM-BiTAM, which is a HMM-based word align-
ment model with a latent topic, with a novel syn-
onym pair generative model. We applied the pro-
posed method to an English-French word align-
ment task and successfully improved the word
137
Figure 1: Graphical model of HM-BiTAM
alignment quality.
2 Bilingual Word Alignment Model
In this section, we review a conventional gener-
ative word alignment model, HM-BiTAM (Zhao
and Xing, 2008).
HM-BiTAM is a bilingual generative model
with topic z, alignment a and topic weight vec-
tor ? as latent variables. Topic variables such
as ?science? or ?economy? assigned to individual
sentences help to disambiguate the meanings of
words. HM-BiTAM assumes that the nth bilin-
gual sentence pair, (En, Fn), is generated under a
given latent topic zn ? {1, . . . , k, . . . ,K}, where
K is the number of latent topics. Let N be the
number of sentence pairs, and In and Jn be the
lengths of En and Fn, respectively. In this frame-
work, all of the bilingual sentence pairs {E,F} =
{(En, Fn)}Nn=1 are generated as follows.
1. ? ? Dirichlet (?): sample topic-weight vector
2. For each sentence pair (En, Fn)
(a) zn ? Multinomial (?): sample the topic
(b) en,i:In |zn ? p (En |zn;? ): sample English
words from a monolingual unigram model given
topic zn
(c) For each position jn = 1, . . . , Jn
i. ajn ? p (ajn |ajn?1;T ): sample an align-
ment link ajn from a first order Markov pro-
cess
ii. fjn ? p (fjn |En, ajn , zn;B ): sample a
target word fjn given an aligned source
word and topic
where alignment ajn = i denotes source word ei
and target word fjn are aligned. ? is a parame-
ter over the topic weight vector ?, ? = {?k,e} is
the source word probability given the kth topic:
p (e |z = k ). B = {Bf,e,k} represents the word
translation probability from e to f under the kth
topic: p (f |e, z = k ). T =
{
Ti,i?
}
is a state tran-
sition probability of a first order Markov process.
Fig. 1 shows a graphical model of HM-BiTAM.
The total likelihood of bilingual sentence pairs
{E,F} can be obtained by marginalizing out la-
tent variables z, a and ?,
p (F,E; ?) =
?
z
?
a

p (F,E, z, a, ?; ?) d?, (1)
where ? = {?, ?, T,B} is a parameter set. In
this model, we can infer word alignment a by max-
imizing the likelihood above.
3 Proposed Method
3.1 Synonym Pair Generative Model
We design a generative model for synonym pairs
{f, f ?} in language F , which assumes that the
synonyms are collected from monolingual linguis-
tic resources. We assume that each synonym pair
(f, f ?) is generated independently given the same
?sense? s. Under this assumption, the probability
of synonym pair (f, f ?) can be formulated as,
p
(
f, f ?
)
?
?
s
p (f |s ) p
(
f ? |s
)
p (s) . (2)
We define a pair (e, k) as a representation of
the sense s, where e and k are a word in a dif-
ferent language E and a latent topic, respectively.
It has been shown that a word e in a different
language is an appropriate representation of s in
synonym modeling (Bannard and Callison-Burch,
2005). We assume that adding a latent topic k for
the sense is very useful for disambiguating word
meaning, and thus that (e, k) gives us a good ap-
proximation of s. Under this assumption, the syn-
onym pair generative model can be defined as fol-
lows.
p
(
{
f, f ?
}
; ??
)
?
?
(f,f ?)
?
e,k
p(f |e, k; ??)p(f ?|e, k; ??)p(e, k; ??),(3)
where ?? is the parameter set of our model.
3.2 Word Alignment with Synonym
Regularization
In this section, we extend the bilingual genera-
tive model (HM-BiTAM) with our synonym pair
model. Our expectation is that synonym pairs
138
Figure 2: Graphical model of synonym pair gen-
erative process
correspond to the same word in a different lan-
guage, thus they make it easy to infer accurate
word alignment. HM-BiTAM and the synonym
model share parameters in order to incorporate
monolingual synonym information into the bilin-
gual word alignment model. This can be achieved
via reparameterizing ?? in eq. 3 as,
p
(
f
?
?
?
e, k; ??
)
? p (f |e, k;B ) , (4)
p
(
e, k; ??
)
? p (e |k;? ) p (k;?) . (5)
Overall, we re-define the synonym pair model
with the HM-BiTAM parameter set ?,
p(
{
f, f ?
}
; ?)
? 1?
k? ?k?
?
(f,f ?)
?
k,e
?k?k,eBf,e,kBf ?,e,k. (6)
Fig. 2 shows a graphical model of the synonym
pair generative process. We estimate the param-
eter values to maximize the likelihood of HM-
BiTAM with respect to bilingual sentences and
that of the synonym model with respect to syn-
onym pairs collected from monolingual resources.
Namely, the parameter estimate, ??, is computed
as
?? = argmax
?
{
log p(F,E; ?) + ? log p(
{
f, f ?
}
; ?)
}
,
(7)
where ? is a regularization weight that should
be set for training. We can expect that the second
term of eq. 7 to constrain parameter set ? and
avoid overfitting for the bilingual word alignment
model. We resort to the variational EM approach
(Bernardo et al, 2003) to infer ?? following HM-
BiTAM. We omit the parameter update equation
due to lack of space.
4 Experiments
4.1 Experimental Setting
For an empirical evaluation of the proposed
method, we used a bilingual parallel corpus of
English-French Hansards (Mihalcea and Pedersen,
2003). The corpus consists of over 1 million sen-
tence pairs, which include 447 manually word-
aligned sentences. We selected 100 sentence pairs
randomly from the manually word-aligned sen-
tences as development data for tuning the regu-
larization weight ?, and used the 347 remaining
sentence pairs as evaluation data. We also ran-
domly selected 10k, 50k, and 100k sized sentence
pairs from the corpus as additional training data.
We ran the unsupervised training of our proposed
word alignment model on the additional training
data and the 347 sentence pairs of the evaluation
data. Note that manual word alignment of the
347 sentence pairs was not used for the unsuper-
vised training. After the unsupervised training, we
evaluated the word alignment performance of our
proposed method by comparing the manual word
alignment of the 347 sentence pairs with the pre-
diction provided by the trained model.
We collected English and French synonym pairs
from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4
(Sagot and Fiser, 2008), respectively. WOLF is a
semantic resource constructed from the Princeton
WordNet and various multilingual resources. We
selected synonym pairs where both words were in-
cluded in the bilingual training set.
We compared the word alignment performance
of our model with that of GIZA++ 1.03 1 (Vo-
gel et al, 1996; Och and Ney, 2003), and HM-
BiTAM (Zhao and Xing, 2008) implemented by
us. GIZA++ is an implementation of IBM-model
4 and HMM, and HM-BiTAM corresponds to ? =
0 in eq. 7. We adopted K = 3 topics, following
the setting in (Zhao and Xing, 2006).
We trained the word alignment in two direc-
tions: English to French, and French to English.
The alignment results for both directions were re-
fined with ?GROW? heuristics to yield high preci-
sion and high recall in accordance with previous
work (Och and Ney, 2003; Zhao and Xing, 2006).
We evaluated these results for precision, recall, F-
measure and alignment error rate (AER), which
are standard metrics for word alignment accuracy
(Och and Ney, 2000).
1http://fjoch.com/GIZA++.html
139
10k Precision Recall F-measure AER
GIZA++ standard 0.856 0.718 0.781 0.207
with SRH 0.874 0.720 0.789 0.198
HM-BiTAM standard 0.869 0.788 0.826 0.169
with SRH 0.884 0.790 0.834 0.160
Proposed 0.941 0.808 0.870 0.123
(a)
50k Precision Recall F-measure AER
GIZA++ standard 0.905 0.770 0.832 0.156
with SRH 0.903 0.759 0.825 0.164
HM-BiTAM standard 0.901 0.814 0.855 0.140
with SRH 0.899 0.808 0.853 0.145
Proposed 0.947 0.824 0.881 0.112
(b)
100k Precision Recall F-measure AER
GIZA++ standard 0.925 0.791 0.853 0.136
with SRH 0.934 0.803 0.864 0.126
HM-BiTAM standard 0.898 0.851 0.874 0.124
with SRH 0.909 0.860 0.879 0.114
Proposed 0.927 0.862 0.893 0.103
(c)
Table 1: Comparison of word alignment accuracy.
The best results are indicated in bold type. The
additional data set sizes are (a) 10k, (b) 50k, (c)
100k.
4.2 Results and Discussion
Table 1 shows the word alignment accuracy of the
three methods trained with 10k, 50k, and 100k ad-
ditional sentence pairs. For all settings, our pro-
posed method outperformed other conventional
methods. This result shows that synonym infor-
mation is effective for improving word alignment
quality as we expected.
As mentioned in Sections 1 and 3.1, the main
idea of our proposed method is to introduce la-
tent topics for modeling synonym pairs, and then
to utilize the synonym pair model for the regu-
larization of word alignment models. We expect
the latent topics to be useful for modeling poly-
semous words included in synonym pairs and to
enable us to incorporate synonym information ef-
fectively into word alignment models. To con-
firm the effect of the synonym pair model with
latent topics, we also tested GIZA++ and HM-
BiTAM with what we call Synonym Replacement
Heuristics (SRH), where all of the synonym pairs
in the bilingual training sentences were simply re-
placed with a representative word. For instance,
the words ?sick? and ?ill? in the bilingual sentences
# vocabularies 10k 50k 100k
English standard 8578 16924 22817
with SRH 5435 7235 13978
French standard 10791 21872 30294
with SRH 9737 20077 27970
Table 2: The number of vocabularies in the 10k,
50k and 100k data sets.
were replaced with the word ?sick?. As shown in
Table 2, the number of vocabularies in the English
and French data sets decreased as a result of em-
ploying the SRH.
We show the performance of GIZA++ and HM-
BiTAM with the SRH in the lines entitled ?with
SRH? in Table 1. The GIZA++ and HM-BiTAM
with the SRH slightly outperformed the standard
GIZA++ and HM-BiTAM for the 10k and 100k
data sets, but underperformed with the 50k data
set. We assume that the SRH mitigated the over-
fitting of these models into low-frequency word
pairs in bilingual sentences, and then improved the
word alignment performance. The SRH regards
all of the different words coupled with the same
word in the synonym pairs as synonyms. For in-
stance, the words ?head?, ?chief? and ?forefront? in
the bilingual sentences are replaced with ?chief?,
since (?head?, ?chief?) and (?head?, ?forefront?) are
synonyms. Obviously, (?chief?, ?forefront?) are
not synonyms, which is detrimented to word align-
ment.
The proposed method consistently outper-
formed GIZA++ and HM-BiTAM with the SRH
in 10k, 50k and 100k data sets in F-measure.
The synonym pair model in our proposed method
can automatically learn that (?head?, ?chief?) and
(?head?, ?forefront?) are individual synonyms with
different meanings by assigning these pairs to dif-
ferent topics. By sharing latent topics between
the synonym pair model and the word alignment
model, the synonym information incorporated in
the synonym pair model is used directly for train-
ing word alignment model. The experimental re-
sults show that our proposed method was effec-
tive in improving the performance of the word
alignment model by using synonym pairs includ-
ing such ambiguous synonym words.
Finally, we discuss the data set size used for un-
supervised training. As shown in Table 1, using
a large number of additional sentence pairs im-
proved the performance of all the models. In all
our experimental settings, all the additional sen-
140
tence pairs and the evaluation data were selected
from the Hansards data set. These experimental
results show that a larger number of sentence pairs
was more effective in improving word alignment
performance when the sentence pairs were col-
lected from a homogeneous data source. However,
in practice, it might be difficult to collect a large
number of such homogeneous sentence pairs for
a specific target domain and language pair. One
direction for future work is to confirm the effect
of the proposed method when training the word
alignment model by using a large number of sen-
tence pairs collected from various data sources in-
cluding many topics for a specific language pair.
5 Conclusions and Future Work
We proposed a novel framework that incorpo-
rates synonyms from monolingual linguistic re-
sources in a word alignment generative model.
This approach utilizes both bilingual and mono-
lingual synonym resources effectively for word
alignment. Our proposed method uses a latent
topic for bilingual sentences and monolingual syn-
onym pairs, which is helpful in terms of word
sense disambiguation. Our proposed method im-
proved word alignment quality with both small
and large data sets. Future work will involve ex-
amining the proposed method for different lan-
guage pairs such as English-Chinese and English-
Japanese and evaluating the impact of our pro-
posed method on SMT performance. We will also
apply our proposed method to a larger data sets
of multiple domains since we can expect a fur-
ther improvement in word alignment accuracy if
we use more bilingual sentences and more mono-
lingual knowledge.
References
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 597?604. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P.
Dawid, D. Heckerman, A. F. M. Smith, and M. West.
2003. The variational bayesian EM algorithm for in-
complete data: with application to scoring graphical
model structures. In Bayesian Statistics 7: Proceed-
ings of the 7th Valencia International Meeting, June
2-6, 2002, page 453. Oxford University Press, USA.
Y. Deng and Y. Gao. 2007. Guiding statistical word
alignment models with prior knowledge. In Pro-
ceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 1?8,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
A. Fraser and D. Marcu. 2007. Getting the struc-
ture right for word alignment: LEAF. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 51?60, Prague, Czech Republic,
June. Association for Computational Linguistics.
Y. Ma, S. Ozdowska, Y. Sun, and A. Way. 2008.
Improving word alignment using syntactic depen-
dencies. In Proceedings of the ACL-08: HLT Sec-
ond Workshop on Syntax and Structure in Statisti-
cal Translation (SSST-2), pages 69?77, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
R. Mihalcea and T. Pedersen. 2003. An evaluation
exercise for word alignment. In Proceedings of the
HLT-NAACL 2003 Workshop on building and using
parallel texts: data driven machine translation and
beyond-Volume 3, page 10. Association for Compu-
tational Linguistics.
G. A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):41.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, pages 440?447. Association for Computational
Linguistics Morristown, NJ, USA.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
B. Sagot and D. Fiser. 2008. Building a free French
wordnet from multilingual resources. In Proceed-
ings of Ontolex.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of the 16th Conference on Computa-
tional Linguistics-Volume 2, pages 836?841. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL on Main Conference
Poster Sessions, page 976. Association for Compu-
tational Linguistics.
B. Zhao and E. P. Xing. 2008. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation.
In Advances in Neural Information Processing Sys-
tems 20, pages 1689?1696, Cambridge, MA. MIT
Press.
141
Proceedings of the ACL 2010 Conference Short Papers, pages 162?167,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Predicate Argument Structure Analysis using Transformation-based
Learning
Hirotoshi Taira Sanae Fujita Masaaki Nagata
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho, Souraku-gun, Kyoto 619-0237, Japan
{taira,sanae}@cslab.kecl.ntt.co.jp nagata.masaaki@lab.ntt.co.jp
Abstract
Maintaining high annotation consistency
in large corpora is crucial for statistical
learning; however, such work is hard,
especially for tasks containing semantic
elements. This paper describes predi-
cate argument structure analysis using?
transformation-based learning. An advan-
tage of transformation-based learning is
the readability of learned rules. A dis-
advantage is that the rule extraction pro-
cedure is time-consuming. We present
incremental-based, transformation-based
learning for semantic processing tasks. As
an example, we deal with Japanese pred-
icate argument analysis and show some
tendencies of annotators for constructing
a corpus with our method.
1 Introduction
Automatic predicate argument structure analysis
(PAS) provides information of ?who did what
to whom? and is an important base tool for
such various text processing tasks as machine
translation information extraction (Hirschman et
al., 1999), question answering (Narayanan and
Harabagiu, 2004; Shen and Lapata, 2007), and
summarization (Melli et al, 2005). Most re-
cent approaches to predicate argument structure
analysis are statistical machine learning methods
such as support vector machines (SVMs)(Pradhan
et al, 2004). For predicate argument struc-
ture analysis, we have the following represen-
tative large corpora: FrameNet (Fillmore et al,
2001), PropBank (Palmer et al, 2005), and Nom-
Bank (Meyers et al, 2004) in English, the Chi-
nese PropBank (Xue, 2008) in Chinese, the
GDA Corpus (Hashida, 2005), Kyoto Text Corpus
Ver.4.0 (Kawahara et al, 2002), and the NAIST
Text Corpus (Iida et al, 2007) in Japanese.
The construction of such large corpora is strenu-
ous and time-consuming. Additionally, maintain-
ing high annotation consistency in such corpora
is crucial for statistical learning; however, such
work is hard, especially for tasks containing se-
mantic elements. For example, in Japanese cor-
pora, distinguishing true dative (or indirect object)
arguments from time-type argument is difficult be-
cause the arguments of both types are often ac-
companied with the ?ni? case marker.
A problem with such statistical learners as SVM
is the lack of interpretability; if accuracy is low, we
cannot identify the problems in the annotations.
We are focusing on transformation-based learn-
ing (TBL). An advantage for such learning meth-
ods is that we can easily interpret the learned
model. The tasks in most previous research are
such simple tagging tasks as part-of-speech tag-
ging, insertion and deletion of parentheses in syn-
tactic parsing, and chunking (Brill, 1995; Brill,
1993; Ramshaw and Marcus, 1995). Here we ex-
periment with a complex task: Japanese PASs.
TBL can be slow, so we proposed an incremen-
tal training method to speed up the training. We
experimented with a Japanese PAS corpus with a
graph-based TBL. From the experiments, we in-
terrelated the annotation tendency on the dataset.
The rest of this paper is organized as follows.
Section 2 describes Japanese predicate structure,
our graph expression of it, and our improved
method. The results of experiments using the
NAIST Text Corpus, which is our target corpus,
are reported in Section 3, and our conclusion is
provided in Section 4.
2 Predicate argument structure and
graph transformation learning
First, we illustrate the structure of a Japanese sen-
tence in Fig. 1. In Japanese, we can divide a sen-
tence into bunsetsu phrases (BP). A BP usually
consists of one or more content words and zero,
162
BP
CW FW
Kare no tabe ta okashi
He ?s
CW FW
eat PAST snack
wa kinou
TOP buy PAST
kat
CW FW CW FW
The snack he ate is one I bought at the store yesterday.
Kareno tabeta okashiwa kinou misede katta.
Sentence
Syntactic dependency between bunsetsus
PRED: Predicate
BPBPBPBP
yesterday
mise de ta
shop at
BP
CWCW FW
BP: Bunsetsu phrase
PRED
ARGARGARGARG
PRED
Nom. Acc.
Time Acc.
Loc.
CW: Content Word
FW: Functional Word
ARG: Argument
Nom: Nominative
Acc: Accusative
Time: Time
Loc: Location
Argument Types
Dat: Dative
Figure 1: Graph expression for PAS
one, or more than one functional words. Syn-
tactic dependency between bunsetsu phrases can
be defined. Japanese dependency parsers such as
Cabocha (Kudo and Matsumoto, 2002) can extract
BPs and their dependencies with about 90% accu-
racy.
Since predicates and arguments in Japanese are
mainly annotated on the head content word in
each BP, we can deal with BPs as candidates of
predicates or arguments. In our experiments, we
mapped each BP to an argument candidate node
of graphs. We also mapped each predicate to a
predicate node. Each predicate-argument relation
is identified by an edge between a predicate and an
argument, and the argument type is mapped to the
edge label. In our experiments below, we defined
five argument types: nominative (subjective), ac-
cusative (direct objective), dative (indirect objec-
tive), time, and location. We use five transforma-
tion types: a) add or b) delete a predicate node, c)
add or d) delete an edge between an predicate and
an argument node, e) change a label (= an argu-
ment type) to another label (Fig. 2). We explain
the existence of an edge between a predicate and
an argument labeled t candidate node as that the
predicate and the argument have a t type relation-
ship.
Transformation-based learning was proposed
by (Brill, 1995). Below we explain our learn-
ing strategy when we directly adapt the learning
method to our graph expression of PASs. First, un-
structured texts from the training data are inputted.
After pre-processing, each text is mapped to an
initial graph. In our experiments, the initial graph
has argument candidate nodes with corresponding
BPs and no predicate nodes or edges. Next, com-
a) `Add Pred Node?
PRED
BP BP BP
PRED
BP BP BP
b) `Delete Pred Node?
ARG
PRED
Nom.
ARG
PRED
c) `Add Edge?
d) `Delete Edge?
Nom.
ARG
PRED
Acc.
ARG
PRED
e) `Change Edge Label?
Figure 2: Transform types
paring the current graphs with the gold standard
graph structure in the training data, we find the dif-
ferent statuses of the nodes and edges among the
graphs. We extract such transformation rule candi-
dates as ?add node? and ?change edge label? with
constraints, including ?the corresponding BP in-
cludes a verb? and ?the argument candidate and the
predicate node have a syntactic dependency.? The
extractions are executed based on the rule tem-
plates given in advance. Each extracted rule is
evaluated for the current graphs, and error reduc-
tion is calculated. The best rule for the reduction
is selected as a new rule and inserted at the bottom
of the current rule list. The new rule is applied to
the current graphs, which are transferred to other
graph structures. This procedure is iterated until
the total errors for the gold standard graphs be-
come zero. When the process is completed, the
rule list is the final model. In the test phase, we it-
eratively transform nodes and edges in the graphs
mapped from the test data, based on rules in the
model like decision lists. The last graph after all
rule adaptations is the system output of the PAS.
In this procedure, the calculation of error reduc-
tion is very time-consuming, because we have to
check many constraints from the candidate rules
for all training samples. The calculation order is
O(MN), where M is the number of articles and
N is the number of candidate rules. Additionally,
an edge rule usually has three types of constraints:
?pred node constraint,? ?argument candidate node
constraint,? and ?relation constraint.? The num-
ber of combinations and extracted rules are much
larger than one of the rules for the node rules.
Ramshaw et al proposed an index-based efficient
reduction method for the calculation of error re-
duction (Ramshaw and Marcus, 1994). However,
in PAS tasks, we need to check the exclusiveness
of the argument types (for example, a predicate ar-
gument structure does not have two nominative ar-
163
guments), and we cannot directly use the method.
Jijkoun et al only used candidate rules that hap-
pen in the current and gold standard graphs and
used SVM learning for constraint checks (Jijkoun
and de Rijke, 2007). This method is effective
for achieving high accuracy; however, it loses the
readability of the rules. This is contrary to our aim
to extract readable rules.
To reduce the calculations while maintaining
readability, we propose an incremental method
and describe its procedure below. In this proce-
dure, we first have PAS graphs for only one arti-
cle. After the total errors among the current and
gold standard graphs become zero in the article,
we proceed to the next article. For the next article,
we first adapt the rules learned from the previous
article. After that, we extract new rules from the
two articles until the total errors for the articles be-
come zero. We continue these processes until the
last article. Additionally, we count the number of
rule occurrences and only use the rule candidates
that happen more than once, because most such
rules harm the accuracy. We save and use these
rules again if the occurrence increases.
3 Experiments
3.1 Experimental Settings
We used the articles in the NAIST Text Cor-
pus version 1.4? (Iida et al, 2007) based on the
Mainichi Shinbun Corpus (Mainichi, 1995), which
were taken from news articles published in the
Japanese Mainichi Shinbun newspaper. We used
articles published on January 1st for training ex-
amples and on January 3rd for test examples.
Three original argument types are defined in the
NAIST Text Corpus: nominative (or subjective),
accusative (or direct object), and dative (or indi-
rect object). For evaluation of the difficult anno-
tation cases, we also added annotations for ?time?
and ?location? types by ourselves. We show the
dataset distribution in Table 1. We extracted the
BP units and dependencies among these BPs from
the dataset using Cabocha, a Japanese dependency
parser, as pre-processing. After that, we adapted
our incremental learning to the training data. We
used two constraint templates in Tables 2 and 3
for predicate nodes and edges when extracting the
rule candidates.
Table 1: Data distribution
Training Test
# of Articles 95 74
# of Sentences 1,129 687
# of Predicates 3,261 2,038
# of Arguments 3,877 2,468
Nom. 1,717 971
Acc. 1,012 701
Dat. 632 376
Time 371 295
Loc. 145 125
Table 4: Total performances (F1-measure (%))
Type System P R F1
Pred. Baseline 89.4 85.1 87.2
Our system 91.8 85.3 88.4
Arg. Baseline 79.3 59.5 68.0
Our system 81.9 62.4 70.8
3.2 Results
Our incremental method takes an hour. In com-
parison, the original TBL cannot even extract one
rule in a day. The results of predicate and argu-
ment type predictions are shown in Table 4. Here,
?Baseline? is the baseline system that predicts the
BSs that contain verbs, adjectives, and da form
nouns (?to be? in English) as predicates and pre-
dicts argument types for BSs having syntactical
dependency with a predicted predicate BS, based
on the following rules: 1) BSs containing nomina-
tive (ga) / accusative (wo) / dative (ni) case mark-
ers are predicted to be nominative, accusative, and
dative, respectively. 2) BSs containing a topic case
marker (wa) are predicted to be nominative. 3)
When a word sense category from a Japanese on-
tology of the head word in BS belongs to a ?time?
or ?location? category, the BS is predicted to be a
?time? and ?location? type argument. In all preci-
sion, recall, and F1-measure, our system outper-
formed the baseline system.
Next, we show our system?s learning curve in
Fig. 3. The number of final rules was 68. This
indicates that the first twenty rules are mainly ef-
fective rules for the performance. The curve also
shows that no overfitting happened. Next, we
show the performance for every argument type in
Table 5. ?TBL,? which stands for ?transformation-
based learning,? is our system. In this table,
the performance of the dative and time types im-
proved, even though they are difficult to distin-
guish. On the other hand, the performance of the
location type argument in our system is very low.
Our method learns rules as decreasing errors of
164
Table 2: Predicate node constraint templates
Pred. Node Constraint Template Rule Example
Constraint Description Pred. Node Constraint Operation
pos1 noun, verb, adjective, etc. pos1=?ADJECTIVE? add pred node
pos2 independent, attached word, etc. pos2=?DEPENDENT WORD? del pred node
pos1 & pos2 above two features combination pos1=?VERB? & pos2=?ANCILLARY WORD? add pred node
?da? da form (copula) ?da form? add pred node
lemma word base form lemma=?%? add pred node
Table 3: Edge constraint templates
Edge Constraint Template Rule Example
Arg. Cand. Pred. Node Relation Edge Constraint OperationConst. Const. Const.
FW (=func.
word)
? dep(arg? pred) FW of Arg. =?wa(TOP)? & dep(arg? pred) add NOM edge
? FW dep(arg? pred) FW of Pred. =?na(ADNOMINAL)? & dep(arg
? pred)
add NOM edge
SemCat
(=semantic
category)
? dep(arg? pred) SemCat of Arg. = ?TIME? & dep(arg? pred) add TIME edge
FW passive form dep(arg? pred) FW of Arg. =?ga(NOM) & Pred.: passive form chg edge label
NOM? ACC
? kform (= type
of inflected
form)
? kform of Pred. = continuative ?ta? form add NOM edge
SemCat Pred. SemCat ? SemCat of Arg. = ?HUMAN? & Pred. SemCat
= ?PHYSICAL MOVE?
add NOM edge
0
10
20
30
40
50
60
70
80
F1-measure (%)
10 20 30 40 50 60
0
70
rules
Figure 3: Learning curves: x-axis = number of
rules; y-axis: F1-measure (%)
all arguments, and the performance of the location
type argument is probably sacrificed for total error
reduction because the number of location type ar-
guments is much smaller than the number of other
argument types (Table 1), and the improvement of
the performance-based learning for location type
arguments is relatively low. To confirm this, we
performed an experiment in which we gave the
rules of the baseline system to our system as initial
rules and subsequently performed our incremen-
tal learning. ?Base + TBL? shows the experiment.
The performance for the location type argument
improved drastically. However, the total perfor-
mance of the arguments was below the original
TBL. Moreover, the ?Base + TBL? performance
surpassed the baseline system. This indicates that
our system learned a reasonable model.
Finally, we show some interesting extracted
rules in Fig. 4. The first rule stands for an ex-
pression where the sentence ends with the per-
formance of something, which is often seen in
Japanese newspaper articles. The second and third
rules represent that annotators of this dataset tend
to annotate time types for which the semantic cate-
gory of the argument is time, even if the argument
looks like the dat. type, and annotators tend to an-
notate dat. type for arguments that have an dat.
165
if BP contains the word `%? ,
Add Pred. Node
PRED
Dat. / Time
ARG
PRED
if func. wd. is `DAT? case,
Rule No.20 CW
`%?
BP
Rule No.15
Time / Dat.
ARG
PRED
Rule No.16
Change Edge Label
Change Edge Label
Dat. ?Time
SemCat is `Time?
Example
Example
?? ?
BP
CW
BP
CW
BP
CW
kotae-ta hito-wa
87%-de
answer-ed people-TOP
87%-be
`People who answered are 87%?
PRED
7? ?
BP
CW
BP
CW
7ka-ni
staato-suru
7th DAT start will
`will start on the 7th
?
ARG
PRED
FW
FW FW FW
??? ?? ?
???? ??
Time
ARG
PRED
Dat.
Rule No.16 is applied
Figure 4: Examples of extracted rules
Table 5: Results for every arg. type (F-measure
(%))
System Args. Nom. Acc. Dat. Time Loc.
Base 68.0 65.8 79.6 70.5 51.5 38.0
TBL 70.8 64.9 86.4 74.8 59.6 1.7
Base + TBL 69.5 63.9 85.8 67.8 55.8 37.4
type case marker.
4 Conclusion
We performed experiments for Japanese predicate
argument structure analysis using transformation-
based learning and extracted rules that indicate the
tendencies annotators have. We presented an in-
cremental procedure to speed up rule extraction.
The performance of PAS analysis improved, espe-
cially, the dative and time types, which are difficult
to distinguish. Moreover, when time expressions
are attached to the ?ni? case, the learned model
showed a tendency to annotate them as dative ar-
guments in the used corpus. Our method has po-
tential for dative predictions and interpreting the
tendencies of annotator inconsistencies.
Acknowledgments
We thank Kevin Duh for his valuable comments.
References
Eric Brill. 1993. Transformation-based error-driven
parsing. In Proc. of the Third International Work-
shop on Parsing Technologies.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank
which provides deep semantics. In Proc. of the Pa-
cific Asian Conference on Language, Information
and Computation (PACLING).
Kouichi Hashida. 2005. Global document annotation
(GDA) manual. http://i-content.org/GDA/.
Lynette Hirschman, Patricia Robinson, Lisa
Ferro, Nancy Chinchor, Erica Brown,
Ralph Grishman, and Beth Sundheim.
1999. Hub-4 Event?99 general guidelines.
http://www.itl.nist.gov/iaui/894.02/related projects/muc/.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proc. of ACL 2007 Workshop on Linguistic
Annotation, pages 132?139.
Valentin Jijkoun and Maarten de Rijke. 2007. Learn-
ing to transform linguistic graphs. In Proc. of
the Second Workshop on TextGraphs: Graph-
Based Algorithms for Natural Language Processing
(TextGraphs-2), pages 53?60. Association for Com-
putational Linguistics.
166
Daisuke Kawahara, Sadao Kurohashi, and Koichi
Hashida. 2002. Construction of a Japanese
relevance-tagged corpus (in Japanese). Proc. of the
8th Annual Meeting of the Association for Natural
Language Processing, pages 495?498.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of the 6th Conference on Natural Language
Learning 2002 (CoNLL 2002).
Mainichi. 1995. CD Mainichi Shinbun 94. Nichigai
Associates Co.
Gabor Melli, Yang Wang, Yudong Liu, Mehdi M.
Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,
and Fred Popowich. 2005. Description of
SQUASH, the SFU question answering summary
handler for the DUC-2005 summarization task. In
Proc. of DUC 2005.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In Proc. of HLT-NAACL 2004
Workshop on Frontiers in Corpus Annotation.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proc. of the 20th International Conference on Com-
putational Linguistics (COLING).
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Sameer Pradhan, Waybe Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proc. of
the Human Language Technology Conference/North
American Chapter of the Association of Computa-
tional Linguistics HLT/NAACL 2004.
Lance Ramshaw and Mitchell Marcus. 1994. Explor-
ing the statistical derivation of transformational rule
sequences for part-of-speech tagging. In The Bal-
ancing Act: Proc. of the ACL Workshop on Com-
bining Symbolic and Statistical Approaches to Lan-
guage.
Lance Ramshaw and Mitchell Marcus. 1995. Text
chunking using transformation-based learning. In
Proc. of the third workshop on very large corpora,
pages 82?94.
Dan Shen and Mirella Lapata. 2007. Using se-
mantic roles to improve question answering. In
Proc. of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP/CoNLL), pages 12?21.
Nianwen Xue. 2008. Labeling Chinese predicates
with semantic roles. Computational Linguistics,
34(2):224?255.
167
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 206?211,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Insertion Operator for Bayesian Tree Substitution Grammars
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
{shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
Abstract
We propose a model that incorporates an in-
sertion operator in Bayesian tree substitution
grammars (BTSG). Tree insertion is helpful
for modeling syntax patterns accurately with
fewer grammar rules than BTSG. The exper-
imental parsing results show that our model
outperforms a standard PCFG and BTSG for
a small dataset. For a large dataset, our model
obtains comparable results to BTSG, making
the number of grammar rules much smaller
than with BTSG.
1 Introduction
Tree substitution grammar (TSG) is a promising for-
malism for modeling language data. TSG general-
izes context free grammars (CFG) by allowing non-
terminal nodes to be replaced with subtrees of arbi-
trary size.
A natural extension of TSG involves adding an
insertion operator for combining subtrees as in
tree adjoining grammars (TAG) (Joshi, 1985) or
tree insertion grammars (TIG) (Schabes and Wa-
ters, 1995). An insertion operator is helpful for ex-
pressing various syntax patterns with fewer gram-
mar rules, thus we expect that adding an insertion
operator will improve parsing accuracy and realize a
compact grammar size.
One of the challenges of adding an insertion op-
erator is that the computational cost of grammar in-
duction is high since tree insertion significantly in-
creases the number of possible subtrees. Previous
work on TAG and TIG induction (Xia, 1999; Chi-
ang, 2003; Chen et al, 2006) has addressed the prob-
lem using language-specific heuristics and a maxi-
mum likelihood estimator, which leads to overfitting
the training data (Post and Gildea, 2009).
Instead, we incorporate an insertion operator in a
Bayesian TSG (BTSG) model (Cohn et al, 2011)
that learns grammar rules automatically without
heuristics. Our model uses a restricted variant of
subtrees for insertion to model the probability dis-
tribution simply and train the model efficiently. We
also present an inference technique for handling a
tree insertion that makes use of dynamic program-
ming.
2 Overview of BTSG Model
We briefly review the BTSG model described in
(Cohn et al, 2011). TSG uses a substitution operator
(shown in Fig. 1a) to combine subtrees. Subtrees for
substitution are referred to as initial trees, and leaf
nonterminals in initial trees are referred to as fron-
tier nodes. Their task is the unsupervised induction
of TSG derivations from parse trees. A derivation
is information about how subtrees are combined to
form parse trees.
The probability distribution over initial trees is de-
fined by using a Pitman-Yor process prior (Pitman
and Yor, 1997), that is,
e |X ? GX
GX |dX , ?X ? PYP (dX , ?X , P0 (? |X )) ,
where X is a nonterminal symbol, e is an initial tree
rooted with X , and P0 (? |X ) is a base distribution
over the infinite space of initial trees rooted with X .
dX and ?X are hyperparameters that are used to con-
trol the model?s behavior. Integrating out all possi-
ble values of GX , the resulting distribution is
206
p (ei |e?i, X, dX , ?X ) = ?ei,X + ?XP0 (ei, |X ) , (1)
where ?ei,X =
n?iei,X
?dX ?tei,X
?X+n
?i
?,X
and ?X =
?X+dX ?t?,X
?X+n
?i
?,X
. e?i = e1, . . . , ei?1 are previously gen-
erated initial trees, and n?iei,X is the number of times
ei has been used in e?i. tei,X is the number of ta-
bles labeled with ei. n
?i
?,X =
?
e n
?i
e,X and t?,X =?
e te,X are the total counts of initial trees and ta-
bles, respectively. The PYP prior produces ?rich get
richer? statistics: a few initial trees are often used
for derivation while many are rarely used, and this is
shown empirically to be well-suited for natural lan-
guage (Teh, 2006b; Johnson and Goldwater, 2009).
The base probability of an initial tree, P0 (e |X ),
is given as follows.
P0 (e |X ) =
?
r?CFG(e)
PMLE (r)?
?
A?LEAF(e)
sA
?
?
B?INTER(e)
(1? sB) , (2)
where CFG (e) is a set of decomposed CFG produc-
tions of e, PMLE (r) is a maximum likelihood esti-
mate (MLE) of r. LEAF (e) and INTER (e) are sets
of leaf and internal symbols of e, respectively. sX is
a stopping probability defined for each X .
3 Insertion Operator for BTSG
3.1 Tree Insertion Model
We propose a model that incorporates an insertion
operator in BTSG. Figure 1b shows an example of
an insertion operator. To distinguish them from ini-
tial trees, subtrees for insertion are referred to as
auxiliary trees. An auxiliary tree includes a special
nonterminal leaf node labeled with the same sym-
bol as the root node. This leaf node is referred to
as a foot node (marked with the subscript ?*?). The
definitions of substitution and insertion operators are
identical with those of TIG and TAG.
Since it is computationally expensive to allow any
auxiliary trees, we tackle the problem by introduc-
ing simple auxiliary trees, i.e., auxiliary trees whose
root node must generate a foot node as an immediate
child. For example, ?(N (JJ pretty) N*)? is a simple
auxiliary tree, but ?(S (NP ) (VP (V think) S*))? is
(a)
(b)
Figure 1: Example of (a) substitution and (b) inser-
tion (dotted line).
not. Note that we place no restriction on the initial
trees.
Our restricted formalism is a strict subset of TIG.
We briefly refer to some differences between TAG,
TIG and our insertion model. TAG generates tree
adjoining languages, a strict superset of context-
free languages, and the computational complexity
of parsing is O
(
n6
)
. TIG is a similar formalism
to TAG, but it does not allow wrapping adjunction
in TAG. Therefore, TIG generates context-free lan-
guages and the parsing complexity is O
(
n3
)
, which
is a strict subset of TAG. On the other hand, our
model prohibits neither wrapping adjunction in TAG
nor simultaneous adjunction in TIG, and allows only
simple auxiliary trees. The expressive power and
computational complexity of our formalism is iden-
tical to TIG, however, our model allows us to de-
fine the probability distribution over auxiliary trees
as having the same form as BTSG model. This en-
sures that we can make use of a dynamic program-
ming technique for training our model, which we de-
scribe the detail in the next subsection.
We define a probability distribution over simple
auxiliary trees as having the same form as eq. 1, that
is,
207
p (ei |e?i, X, d?X , ?
?
X ) = ?
?
ei,X + ?
?
XP
?
0 (ei, |X ) , (3)
where d?X and ?
?
X are hyperparameters of the in-
sertion model, and the definition of
(
??ei,X , ?
?
X
)
is
the same as that of (?ei,X , ?X) in eq. 1.
However, we need modify the base distribution
over simple auxiliary trees, P ?0 (e |X ), as follows,
so that all probabilities of the simple auxiliary trees
sum to one.
P ?0 (e |X ) = P
?
MLE (TOP (e))?
?
r?INTER_CFG(e)
PMLE (r)
?
?
A?LEAF(e)
sA ?
?
B?INTER(e)
(1? sB) , (4)
where TOP (e) is the CFG production that
starts with the root node of e. For example,
TOP (N (JJ pretty) (N*)) returns ?N ? JJ N*?.
INTER_CFG (e) is a set of CFG productions of e
excluding TOP (e). P ?MLE (r
?) is a modified MLE
for simple auxiliary trees, which is given by
{
C(r?)
C(X?X?Y )+C(X?Y X?) if r
?includes a foot node
0 else
where C (r?) is the frequency of r? in parse trees.
It is ensured that P ?0 (e |X ) generates a foot node as
an immediate child.
We define the probability distribution over both
initial trees and simple auxiliary trees with a PYP
prior. The base distribution over initial trees is de-
fined as P0 (e |X ), and the base distribution over
simple auxiliary trees is defined as P ?0 (e |X ). An
initial tree ei replaces a frontier node with prob-
ability p (ei |e?i, X, dX , ?X ). On the other hand,
a simple auxiliary tree e?i inserts an internal node
with probability aX?p?
(
e?i
?
?e??i, X, d
?
X , ?
?
X
)
, where
aX is an insertion probability defined for each X .
The stopping probabilities are common to both ini-
tial and auxiliary trees.
3.2 Grammar Decomposition
We develop a grammar decomposition technique,
which is an extension of work (Cohn and Blunsom,
2010) on BTSG model, to deal with an insertion
operator. The motivation behind grammar decom-
position is that it is hard to consider all possible
Figure 2: Derivation of Fig. 1b transformed by
grammar decomposition.
CFG rule probability
NP(NP (DT the) (N girl))?DT(DT the)Nins (N girl) (1? aDT)? aN
DT(DT the) ? the 1
Nins (N girl) ?Nins (N girl)(N (JJ pretty) N*) ?
?
(N (JJ pretty) N*),N
Nins (N girl)(N (JJ pretty) N*) ? JJ(JJ pretty)N(N girl) (1? aJJ)? 1
JJ(JJ pretty) ?pretty 1
N(N girl) ?girl 1
Table 1: The rules and probabilities of grammar de-
composition for Fig. 2.
derivations explicitly since the base distribution as-
signs non-zero probability to an infinite number of
initial and auxiliary trees. Alternatively, we trans-
form a derivation into CFG productions and assign
the probability for each CFG production so that its
assignment is consistent with the probability distri-
butions. We can efficiently calculate an inside prob-
ability (described in the next subsection) by employ-
ing grammar decomposition.
Here we provide an example of the derivation
shown in Fig. 1b. First, we can transform the deriva-
tion in Fig. 1b to another form as shown in Fig. 2.
In Fig. 2, all the derivation information is embed-
ded in each symbol. That is, NP(NP (DT the) (N girl)) is
a root symbol of the initial tree ?(NP (DT the) (N
girl))?, which generates two child nodes: DT(DT the)
and N(N girl). DT(DT the) generates the terminal node
?the?. On the other hand, Nins (N girl) denotes that
N(N girl) is inserted by some auxiliary tree, and
Nins (N girl)(N (JJ pretty) N*) denotes that the inserted simple aux-
iliary tree is ?(N (JJ pretty) (N*))?. The inserted
auxiliary tree, ?(N (JJ pretty) (N*))?, must generate
a foot node: ?(N girl)? as an immediate child.
208
Second, we decompose the transformed tree into
CFG productions and then assign the probability for
each CFG production as shown in Table 1, where
aDT, aN and aJJ are insertion probabilities for non-
terminal DT, N and JJ, respectively. Note that the
probability of a derivation according to Table 1 is
the same as the probability of a derivation obtained
from the distribution over the initial and auxiliary
trees (i.e. eq. 1 and eq. 3).
In Table 1, we assume that the auxiliary tree
?(N (JJ pretty) (N*))? is sampled from the first
term of eq. 3. When it is sampled from the sec-
ond term, we alternatively assign the probability
??(N (JJ pretty) N*), N.
3.3 Training
We use a blocked Metropolis-Hastings (MH) algo-
rithm (Cohn and Blunsom, 2010) to train our model.
The MH algorithm learns BTSG model parameters
efficiently, and it can be applied to our insertion
model. The MH algorithm consists of the following
three steps. For each sentence,
1. Calculate the inside probability (Lari and
Young, 1991) in a bottom-up manner using the
grammar decomposition.
2. Sample a derivation tree in a top-down manner.
3. Accept or reject the derivation sample by using
the MH test.
The MH algorithm is described in detail in (Cohn
and Blunsom, 2010). The hyperparameters of our
model are updated with the auxiliary variable tech-
nique (Teh, 2006a).
4 Experiments
We ran experiments on the British National Cor-
pus (BNC) Treebank 3 and the WSJ English Penn
Treebank. We did not use a development set since
our model automatically updates the hyperparame-
ters for every iteration. The treebank data was bina-
rized using the CENTER-HEAD method (Matsuzaki
et al, 2005). We replaced lexical words with counts
? 1 in the training set with one of three unknown
1Results from (Cohn and Blunsom, 2010).
2Results for length ? 40.
3http://nclt.computing.dcu.ie/~jfoster/resources/
corpus method F1
CFG 54.08
BNC BTSG 67.73
BTSG + insertion 69.06
CFG 64.99
BTSG 77.19
WSJ BTSG + insertion 78.54
(Petrov et al, 2006) 77.931
(Cohn and Blunsom, 2010) 78.40
Table 2: Small dataset experiments
# rules (# aux. trees) F1
CFG 35374 (-) 71.0
BTSG 80026 (0) 85.0
BTSG + insertion 65099 (25) 85.3
(Post and Gildea, 2009) - 82.62
(Cohn and Blunsom, 2010) - 85.3
Table 3: Full Penn Treebank dataset experiments
words using lexical features. We trained our model
using a training set, and then sampled 10k deriva-
tions for each sentence in a test set. Parsing results
were obtained with the MER algorithm (Cohn et al,
2011) using the 10k derivation samples. We show
the bracketing F1 score of predicted parse trees eval-
uated by EVALB4, averaged over three independent
runs.
In small dataset experiments, we used BNC (1k
sentences, 90% for training and 10% for testing) and
WSJ (section 2 for training and section 22 for test-
ing). This was a small-scale experiment, but large
enough to be relevant for low-resource languages.
We trained the model with an MH sampler for 1k
iterations. Table 2 shows the parsing results for
the test set. We compared our model with standard
PCFG and BTSG models implemented by us.
Our insertion model successfully outperformed
CFG and BTSG. This suggests that adding an inser-
tion operator is helpful for modeling syntax trees ac-
curately. The BTSG model described in (Cohn and
Blunsom, 2010) is similar to ours. They reported
an F1 score of 78.40 (the score of our BTSG model
was 77.19). We speculate that the performance gap
is due to data preprocessing such as the treatment of
rare words.
4http://nlp.cs.nyu.edu/evalb/
209
(N?P (N?P ) (: ?))
(N?P (N?P ) (ADVP (RB respectively)))
(P?P (P?P ) (, ,))
(V?P (V?P ) (RB then))
(Q?P (Q?P ) (IN of))
( ?SBAR ( ?SBAR ) (RB not))
(S? (S? ) (: ;))
Table 4: Examples of lexicalized auxiliary trees ob-
tained from our model in the full treebank dataset.
Nonterminal symbols created by binarization are
shown with an over-bar.
We also applied our model to the full WSJ Penn
Treebank setting (section 2-21 for training and sec-
tion 23 for testing). The parsing results are shown in
Table 3. We trained the model with an MH sampler
for 3.5k iterations.
For the full treebank dataset, our model obtained
nearly identical results to those obtained with BTSG
model, making the grammar size approximately
19% smaller than that of BTSG. We can see that only
a small number of auxiliary trees have a great impact
on reducing the grammar size. Surprisingly, there
are many fewer auxiliary trees than initial trees. We
believe this to be due to the tree binarization and our
restricted assumption of simple auxiliary trees.
Table 4 shows examples of lexicalized auxiliary
trees obtained with our model for the full treebank
data. We can see that punctuation (???, ?,?, and ?;?)
and adverb (RB) tend to be inserted in other trees.
Punctuation and adverb appear in various positions
in English sentences. Our results suggest that rather
than treat those words as substitutions, it is more rea-
sonable to consider them to be ?insertions?, which is
intuitively understandable.
5 Summary
We proposed a model that incorporates an inser-
tion operator in BTSG and developed an efficient
inference technique. Since it is computationally ex-
pensive to allow any auxiliary trees, we tackled the
problem by introducing a restricted variant of aux-
iliary trees. Our model outperformed the BTSG
model for a small dataset, and achieved compara-
ble parsing results for a large dataset, making the
number of grammars much smaller than the BTSG
model. We will extend our model to original TAG
and evaluate its impact on statistical parsing perfor-
mance.
References
J. Chen, S. Bangalore, and K. Vijay-Shanker. 2006.
Automated extraction of Tree-Adjoining Grammars
from treebanks. Natural Language Engineering,
12(03):251?299.
D. Chiang, 2003. Statistical Parsing with an Automati-
cally Extracted Tree Adjoining Grammar, chapter 16,
pages 299?316. CSLI Publications.
T. Cohn and P. Blunsom. 2010. Blocked inference in
Bayesian tree substitution grammars. In Proceedings
of the ACL 2010 Conference Short Papers, pages 225?
230, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Induc-
ing tree-substitution grammars. Journal of Machine
Learning Research. To Appear.
M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on unsu-
pervised word segmentation with adaptor grammars.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 317?325, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
A.K. Joshi. 1985. Tree adjoining grammars: How much
context-sensitivity is required to provide reasonable
structural descriptions? Natural Language Parsing:
Psychological, Computational, and Theoretical Per-
spectives, pages 206?250.
K. Lari and S.J. Young. 1991. Applications of stochastic
context-free grammars using the inside-outside algo-
rithm. Computer Speech & Language, 5(3):237?257.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 75?82. Association
for Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Computa-
tional Linguistics (ICCL-ACL), pages 433?440, Syd-
ney, Australia, July. Association for Computational
Linguistics.
210
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. The Annals of Probability, 25(2):855?900.
M. Post and D. Gildea. 2009. Bayesian learning of a
tree substitution grammar. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 45?48,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Y. Schabes and R.C. Waters. 1995. Tree insertion gram-
mar: a cubic-time, parsable formalism that lexicalizes
context-free grammar without changing the trees pro-
duced. Fuzzy Sets and Systems, 76(3):309?317.
Y. W. Teh. 2006a. A Bayesian interpretation of interpo-
lated Kneser-Ney. Technical Report TRA2/06, School
of Computing, National University of Singapore.
Y. W. Teh. 2006b. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics (ICCL-
ACL), pages 985?992.
F. Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of the 5th Natu-
ral Language Processing Pacific Rim Symposium (NL-
PRS), pages 398?403.
211
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 429?433,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Is Machine Translation Ripe for Cross-lingual Sentiment Classification?
Kevin Duh and Akinori Fujino and Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikari-dai, Seika-cho, Kyoto 619-0237, JAPAN
{kevin.duh,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
Abstract
Recent advances in Machine Translation (MT)
have brought forth a new paradigm for build-
ing NLP applications in low-resource scenar-
ios. To build a sentiment classifier for a
language with no labeled resources, one can
translate labeled data from another language,
then train a classifier on the translated text.
This can be viewed as a domain adaptation
problem, where labeled translations and test
data have some mismatch. Various prior work
have achieved positive results using this ap-
proach.
In this opinion piece, we take a step back and
make some general statements about cross-
lingual adaptation problems. First, we claim
that domain mismatch is not caused by MT
errors, and accuracy degradation will occur
even in the case of perfect MT. Second, we ar-
gue that the cross-lingual adaptation problem
is qualitatively different from other (monolin-
gual) adaptation problems in NLP; thus new
adaptation algorithms ought to be considered.
This paper will describe a series of carefully-
designed experiments that led us to these con-
clusions.
1 Summary
Question 1: If MT gave perfect translations (seman-
tically), do we still have a domain adaptation chal-
lenge in cross-lingual sentiment classification?
Answer: Yes. The reason is that while many trans-
lations of a word may be valid, the MT system might
have a systematic bias. For example, the word ?awe-
some? might be prevalent in English reviews, but in
translated reviews, the word ?excellent? is generated
instead. From the perspective of MT, this translation
is correct and preserves sentiment polarity. But from
the perspective of a classifier, there is a domain mis-
match due to differences in word distributions.
Question 2: Can we apply standard adaptation algo-
rithms developed for other (monolingual) adaptation
problems to cross-lingual adaptation?
Answer: No. It appears that the interaction between
target unlabeled data and source data can be rather
unexpected in the case of cross-lingual adaptation.
We do not know the reason, but our experiments
show that the accuracy of adaptation algorithms in
cross-lingual scenarios have much higher variance
than monolingual scenarios.
The goal of this opinion piece is to argue the need
to better understand the characteristics of domain
adaptation in cross-lingual problems. We invite the
reader to disagree with our conclusion (that the true
barrier to good performance is not insufficient MT
quality, but inappropriate domain adaptation meth-
ods). Here we present a series of experiments that
led us to this conclusion. First we describe the ex-
periment design (?2) and baselines (?3), before an-
swering Question 1 (?4) and Question 2 (?5).
2 Experiment Design
The cross-lingual setup is this: we have labeled data
from source domain S and wish to build a sentiment
classifier for target domain T . Domain mismatch
can arise from language differences (e.g. English vs.
translated text) or market differences (e.g. DVD vs.
Book reviews). Our experiments will involve fixing
429
T to a common testset and varying S. This allows us
to experiment with different settings for adaptation.
We use the Amazon review dataset of Pretten-
hofer (2010)1, due to its wide range of languages
(English [EN], Japanese [JP], French [FR], Ger-
man [DE]) and markets (music, DVD, books). Un-
like Prettenhofer (2010), we reverse the direction of
cross-lingual adaptation and consider English as tar-
get. English is not a low-resource language, but this
setting allows for more comparisons. Each source
dataset has 2000 reviews, equally balanced between
positive and negative. The target has 2000 test sam-
ples, large unlabeled data (25k, 30k, 50k samples
respectively for Music, DVD, and Books), and an
additional 2000 labeled data reserved for oracle ex-
periments. Texts in JP, FR, and DE are translated
word-by-word into English with Google Translate.2
We perform three sets of experiments, shown in
Table 1. Table 2 lists all the results; we will interpret
them in the following sections.
Target (T ) Source (S)
1 Music-EN Music-JP, Music-FR, Music-DE,
DVD-EN, Book-EN
2 DVD-EN DVD-JP, DVD-FR, DVD-DE,
Music-EN, Book-EN
3 Book-EN Book-JP, Book-FR, Book-DE,
Music-EN, DVD-EN
Table 1: Experiment setups: Fix T , vary S.
3 How much performance degradation
occurs in cross-lingual adaptation?
First, we need to quantify the accuracy degrada-
tion under different source data, without consider-
ation of domain adaptation methods. So we train
a SVM classifier on labeled source data3, and di-
rectly apply it on test data. The oracle setting, which
has no domain-mismatch (e.g. train on Music-EN,
test on Music-EN), achieves an average test accu-
racy of (81.6 + 80.9 + 80.0)/3 = 80.8%4. Aver-
1http://www.webis.de/research/corpora/webis-cls-10
2This is done by querying foreign words to build a bilingual
dictionary. The words are converted to tfidf unigram features.
3For all methods we try here, 5% of the 2000 labeled source
samples are held-out for parameter tuning.
4See column EN of Table 2, Supervised SVM results.
age cross-lingual accuracies are: 69.4% (JP), 75.6%
(FR), 77.0% (DE), so degradations compared to or-
acle are: -11% (JP), -5% (FR), -4% (DE).5 Cross-
market degradations are around -6%6.
Observation 1: Degradations due to market and
language mismatch are comparable in several cases
(e.g. MUSIC-DE and DVD-EN perform similarly
for target MUSIC-EN). Observation 2: The ranking
of source language by decreasing accuracy is DE >
FR > JP. Does this mean JP-EN is a more difficult
language pair for MT? The next section will show
that this is not necessarily the case. Certainly, the
domain mismatch for JP is larger than DE, but this
could be due to phenomenon other than MT errors.
4 Where exactly is the domain mismatch?
4.1 Theory of Domain Adaptation
We analyze domain adaptation by the concepts of
labeling and instance mismatch (Jiang and Zhai,
2007). Let pt(x, y) = pt(y|x)pt(x) be the target
distribution of samples x (e.g. unigram feature vec-
tor) and labels y (positive / negative). Let ps(x, y) =
ps(y|x)ps(x) be the corresponding source distribu-
tion. We assume that one (or both) of the following
distributions differ between source and target:
? Instance mismatch: ps(x) 6= pt(x).
? Labeling mismatch: ps(y|x) 6= pt(y|x).
Instance mismatch implies that the input feature
vectors have different distribution (e.g. one dataset
uses the word ?excellent? often, while the other uses
the word ?awesome?). This degrades performance
because classifiers trained on ?excellent? might not
know how to classify texts with the word ?awe-
some.? The solution is to tie together these features
(Blitzer et al, 2006) or re-weight the input distribu-
tion (Sugiyama et al, 2008). Under some assump-
tions (i.e. covariate shift), oracle accuracy can be
achieved theoretically (Shimodaira, 2000).
Labeling mismatch implies the same input has
different labels in different domains. For exam-
ple, the JP word meaning ?excellent? may be mis-
translated as ?bad? in English. Then, positive JP
5See ?Adapt by Language? columns of Table 2. Note
JP+FR+DE condition has 6000 labeled samples, so is not di-
rectly comparable to other adaptation scenarios (2000 samples).
Nevertheless, mixing languages seem to give good results.
6See ?Adapt by Market? columns of Table 2.
430
Target Classifier Oracle Adapt by Language Adapt by Market
EN JP FR DE JP+FR+DE MUSIC DVD BOOK
MUSIC-EN Supervised SVM 81.6 68.5 75.2 76.3 80.3 - 76.8 74.1
Adapted TSVM 79.6 73.0 74.6 77.9 78.6 - 78.4 75.6
DVD-EN Supervised SVM 80.9 70.1 76.4 77.4 79.7 75.2 - 74.5
Adapted TSVM 81.0 71.4 75.5 76.3 78.4 74.8 - 76.7
BOOK-EN Supervised SVM 80.0 69.6 75.4 77.4 79.9 73.4 76.2 -
Adapted TSVM 81.2 73.8 77.6 76.7 79.5 75.1 77.4 -
Table 2: Test accuracies (%) for English Music/DVD/Book reviews. Each column is an adaptation scenario using
different source data. The source data may vary by language or by market. For example, the first row shows that for
the target of Music-EN, the accuracy of a SVM trained on translated JP reviews (in the same market) is 68.5, while the
accuracy of a SVM trained on DVD reviews (in the same language) is 76.8. ?Oracle? indicates training on the same
market and same language domain as the target. ?JP+FR+DE? indicates the concatenation of JP, FR, DE as source
data. Boldface shows the winner of Supervised vs. Adapted.
reviews will be associated with the word ?bad?:
ps(y = +1|x = bad) will be high, whereas the true
conditional distribution should have high pt(y =
?1|x = bad) instead. There are several cases for
labeling mismatch, depending on how the polarity
changes (Table 3). The solution is to filter out these
noisy samples (Jiang and Zhai, 2007) or optimize
loosely-linked objectives through shared parameters
or Bayesian priors (Finkel and Manning, 2009).
Which mismatch is responsible for accuracy
degradations in cross-lingual adaptation?
? Instance mismatch: Systematic MT bias gener-
ates word distributions different from naturally-
occurring English. (Translation may be valid.)
? Label mismatch: MT error mis-translates a word
into something with different polarity.
Conclusion from ?4.2 and ?4.3: Instance mis-
match occurs often; MT error appears minimal.
Mis-translated polarity Effect
? ? 0 Loose a discriminative
e.g. (?good? ? ?the?) feature
0 ? ? Increased overlap in
e.g. (?the? ? ?good?) positive/negative data
+ ? ? and ? ? + Association with
e.g. (?good? ? ?bad?) opposite label
Table 3: Label mismatch: mis-translating positive (+),
negative (?), or neutral (0) words have different effects.
We think the first two cases have graceful degradation,
but the third case may be catastrophic.
4.2 Analysis of Instance Mismatch
To measure instance mismatch, we compute statis-
tics between ps(x) and pt(x), or approximations
thereof: First, we calculate a (normalized) average
feature from all samples of source S, which repre-
sents the unigram distribution of MT output. Simi-
larly, the average feature vector for target T approx-
imates the unigram distribution of English reviews
pt(x). Then we measure:
? KL Divergence between Avg(S) and Avg(T ),
where Avg() is the average vector.
? Set Coverage of Avg(T ) on Avg(S): how many
word (type) in T appears at least once in S.
Both measures correlate strongly with final accu-
racy, as seen in Figure 1. The correlation coefficients
are r = ?0.78 for KL Divergence and r = 0.71 for
Coverage, both statistically significant (p < 0.05).
This implies that instance mismatch is an important
reason for the degradations seen in Section 3.7
4.3 Analysis of Labeling Mismatch
We measure labeling mismatch by looking at dif-
ferences in the weight vectors of oracle SVM and
adapted SVM. Intuitively, if a feature has positive
weight in the oracle SVM, but negative weight in the
adapted SVM, then it is likely a MT mis-translation
7The observant reader may notice that cross-market points
exhibit higher coverage but equal accuracy (74-78%) to some
cross-lingual points. This suggests that MT output may be more
constrained in vocabulary than naturally-occurring English.
431
68 70 72 74 76 78 80 82
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Accuracy
KL
 D
iv
er
ge
nc
e
68 70 72 74 76 78 80 82
0.4
0.5
0.6
0.7
0.8
0.9
Accuracy
Te
st
 C
ov
er
ag
e
Figure 1: KL Divergence and Coverage vs. accuracy. (o)
are cross-lingual and (x) are cross-market data points.
is causing the polarity flip. Algorithm 1 (with
K=2000) shows how we compute polarity flip rate.8
We found that the polarity flip rate does not cor-
relate well with accuracy at all (r = 0.04). Conclu-
sion: Labeling mismatch is not a factor in perfor-
mance degradation. Nevertheless, we note there is a
surprising large number of flips (24% on average). A
manual check of the flipped words in BOOK-JP re-
vealed few MT mistakes. Only 3.7% of 450 random
EN-JP word pairs checked can be judged as blatantly
incorrect (without sentence context). The majority
of flipped words do not have a clear sentiment ori-
entation (e.g. ?amazon?, ?human?, ?moreover?).
5 Are standard adaptation algorithms
applicable to cross-lingual problems?
One of the breakthroughs in cross-lingual text clas-
sification is the realization that it can be cast as do-
main adaptation. This makes available a host of pre-
existing adaptation algorithms for improving over
supervised results. However, we argue that it may be
8The feature normalization in Step 1 is important to ensure
that the weight magnitudes are comparable.
Algorithm 1 Measuring labeling mismatch
Input: Weight vectors for source ws and target wt
Input: Target data average sample vector avg(T )
Output: Polarity flip rate f
1: Normalize: ws = avg(T ) * ws; wt = avg(T ) * wt
2: Set S+ = { K most positive features in ws}
3: Set S? = { K most negative features in ws}
4: Set T+ = { K most positive features in wt}
5: Set T? = { K most negative features in wt}
6: for each feature i ? T+ do
7: if i ? S? then f = f + 1
8: end for
9: for each feature j ? T? do
10: if j ? S+ then f = f + 1
11: end for
12: f = f2K
better to ?adapt? the standard adaptation algorithm
to the cross-lingual setting. We arrived at this con-
clusion by trying the adapted counterpart of SVMs
off-the-shelf. Recently, (Bergamo and Torresani,
2010) showed that Transductive SVMs (TSVM),
originally developed for semi-supervised learning,
are also strong adaptation methods. The idea is to
train on source data like a SVM, but encourage the
classification boundary to divide through low den-
sity regions in the unlabeled target data.
Table 2 shows that TSVM outperforms SVM in
all but one case for cross-market adaptation, but
gives mixed results for cross-lingual adaptation.
This is a puzzling result considering that both use
the same unlabeled data. Why does TSVM exhibit
such a large variance on cross-lingual problems, but
not on cross-market problems? Is unlabeled target
data interacting with source data in some unexpected
way?
Certainly there are several successful studies
(Wan, 2009; Wei and Pal, 2010; Banea et al, 2008),
but we think it is important to consider the possi-
bility that cross-lingual adaptation has some fun-
damental differences. We conjecture that adapting
from artificially-generated text (e.g. MT output)
is a different story than adapting from naturally-
occurring text (e.g. cross-market). In short, MT is
ripe for cross-lingual adaptation; what is not ripe is
probably our understanding of the special character-
istics of the adaptation problem.
432
References
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity analy-
sis using machine translation. In Proc. of Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP).
Alessandro Bergamo and Lorenzo Torresani. 2010. Ex-
ploiting weakly-labeled web images to improve ob-
ject classification: a domain adaptation approach. In
Advances in Neural Information Processing Systems
(NIPS).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Jenny Rose Finkel and Chris Manning. 2009. Hierarchi-
cal Bayesian domain adaptation. In Proc. of NAACL
Human Language Technologies (HLT).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Proc. of the As-
sociation for Computational Linguistics (ACL).
Peter Prettenhofer and Benno Stein. 2010. Cross-
language text classification using structural correspon-
dence learning. In Proc. of the Association for Com-
putational Linguistics (ACL).
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inferenc, 90.
Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima,
Hisashi Kashima, Paul von Bu?nau, and Motoaki
Kawanabe. 2008. Direct importance estimation for
covariate shift adaptation. Annals of the Institute of
Statistical Mathematics, 60(4).
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proc. of the Association for
Computational Linguistics (ACL).
Bin Wei and Chris Pal. 2010. Cross lingual adaptation:
an experiment on sentiment classification. In Proceed-
ings of the ACL 2010 Conference Short Papers.
433
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 636?641,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Condensed Feature Representations from Large Unsupervised
Data Sets for Supervised Learning
Jun Suzuki, Hideki Isozaki, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{suzuki.jun, isozaki.hideki, nagata.masaaki}@lab.ntt.co.jp
Abstract
This paper proposes a novel approach for ef-
fectively utilizing unsupervised data in addi-
tion to supervised data for supervised learn-
ing. We use unsupervised data to gener-
ate informative ?condensed feature represen-
tations? from the original feature set used in
supervised NLP systems. The main con-
tribution of our method is that it can of-
fer dense and low-dimensional feature spaces
for NLP tasks while maintaining the state-of-
the-art performance provided by the recently
developed high-performance semi-supervised
learning technique. Our method matches the
results of current state-of-the-art systems with
very few features, i.e., F-score 90.72 with
344 features for CoNLL-2003 NER data, and
UAS 93.55 with 12.5K features for depen-
dency parsing data derived from PTB-III.
1 Introduction
In the last decade, supervised learning has become
a standard way to train the models of many natural
language processing (NLP) systems. One simple but
powerful approach for further enhancing the perfor-
mance is to utilize a large amount of unsupervised
data to supplement supervised data. Specifically,
an approach that involves incorporating ?clustering-
based word representations (CWR)? induced from
unsupervised data as additional features of super-
vised learning has demonstrated substantial perfor-
mance gains over state-of-the-art supervised learn-
ing systems in typical NLP tasks, such as named en-
tity recognition (Lin and Wu, 2009; Turian et al,
2010) and dependency parsing (Koo et al, 2008).
We refer to this approach as the iCWR approach,
The iCWR approach has become popular for en-
hancement because of its simplicity and generality.
The goal of this paper is to provide yet another
simple and general framework, like the iCWR ap-
proach, to enhance existing state-of-the-art super-
vised NLP systems. The differences between the
iCWR approach and our method are as follows; sup-
pose F is the original feature set used in supervised
learning, C is the CWR feature set, andH is the new
feature set generated by our method. Then, with the
iCWR approach, C is induced independently from
F , and used in addition to F in supervised learning,
i.e., F ? C. In contrast, in our method H is directly
induced from F with the help of an existing model
already trained by supervised learning with F , and
used in place of F in supervised learning.
The largest contribution of our method is that
it offers an architecture that can drastically reduce
the number of features, i.e., from 10M features
in F to less than 1K features in H by construct-
ing ?condensed feature representations (COFER)?,
which is a new and very unique property that can-
not be matched by previous semi-supervised learn-
ing methods including the iCWR approach. One
noteworthy feature of our method is that there is no
need to handle sparse and high-dimensional feature
spaces often used in many supervised NLP systems,
which is one of the main causes of the data sparse-
ness problem often encountered when we learn the
model with a supervised leaning algorithm. As a
result, NLP systems that are both compact and high-
performance can be built by retraining the model
with the obtained condensed feature set H.
2 Condensed Feature Representations
Let us first define the condensed feature set H. In
this paper, we call the feature set generally used in
supervised learning, F , the original feature set. Let
N andM represent the numbers of features inF and
H, respectively. We assume M?N , and generally
M N . A condensed feature hm ?H is charac-
636
Potencies are multiplied by a positive constant ?
0 1 2-1-2 3 4
Section 3.3: Feature potency quantization
Feature potency Section 3.4: Condensed feature construction
F N  (e.g., N=100M)Original feature set Section 3.1: Feature potency estimation
Features mapped into this area will be zeroed by the effect of C
C
0
-C Section 3.2: Feature potency discountingFeature potency?  0
Feature potency 
? ( Integer Space N )Condensed feature setH Each condensed feature is represented as a set of features in the original feature set F.-1/?1/?3/?? -2/?M  (e.g., M=1K) The potencies are also utilized as an (M+1)-th condensed feature
H
(Quantized feature potency)
Features mapped into zero are discarded and never mapped into any condensed features
Figure 1: Outline of our method to construct a condensed
feature set.
r?(x) =
X
y?Y(x)
r(x,y)/|Y(x)|.
V +D (fn) =
X
x?D
fn(x, y?)(r(x, y?) ? r?(x))
V ?D (fn) = ?
X
x?D
X
y?Y(x)\y?
fn(x,y)(r(x,y) ? r?(x))
Rn=
X
x?D
X
y?Y(x)
r(x,y)fn(x,y), An=
X
x?D
r?(x)
X
y?Y(x)
fn(x,y)
Figure 2: Notations used in this paper.
terized as a set of features in F , that is, hm =Sm
where Sm ? F . We assume that each original fea-
ture fn?F maps, at most, to one condensed feature
hm. This assumption prevents two condensed fea-
tures from containing the same original feature, and
some original features from not being mapped to any
condensed feature. Namely, Sm ? Sm? =? for all m
and m?, where m 6=m?, and
?M
m=1 Sm?F hold.
The value of each condensed feature is calcu-
lated by summing the values of the original fea-
tures assigned to it. Formally, let X and Y repre-
sent the sets of all possible inputs and outputs of
a target task, respectively. Let x ? X be an in-
put, and y ? Y(x) be an output, where Y(x) ? Y
represents the set of possible outputs given x. We
write the n-th feature function of the original fea-
tures, whose value is determined by x and y, as
fn(x,y), where n ? {1, . . . , N}. Similarly, we
write them-th feature function of the condensed fea-
tures as hm(x,y), where m?{1, . . . ,M}. We state
that the value of hm(x,y) is calculated as follows:
hm(x,y)=
?
fn?Sm fn(x,y).
3 Learning COFERs
The remaining part of our method consists of the
way to map the original features into the condensed
features. For this purpose, we define the feature po-
tency, which is evaluated by employing an existing
supervised model with unsupervised data sets. Fig-
ure 1 shows a brief sketch of the process to construct
the condensed features described in this section.
3.1 Self-taught-style feature potency estimation
We assume that we have a model trained by super-
vised learning, which we call the ?base supervised
model?, and the original feature set F that is used
in the base supervised model. We consider a case
where the base supervised model is a (log-)linear
model, and use the following equation to select the
best output y? given x:
y?=argmax
y?Y(x)
?N
n=1 wnfn(x,y), (1)
where wn is a model parameter (or weight) of fn.
Linear models are currently the most widely-used
models and are employed in many NLP systems.
To simplify the explanation, we define function
r(x,y), where r(x,y) returns 1 if y = y? is obtained
from the base supervised model given x, and 0 oth-
erwise. Let r?(x) represent the average of r(x,y) in
x (see Figure 2 for details). We also define V +D (fn)
and V ?D (fn) as shown in Figure 2 where D repre-
sents the unsupervised data set. V +D (fn) measures
the positive correlation with the best output y? given
by the base supervised model since this is the sum-
mation of all the (weighted) feature values used in
the estimation of the one best output y? over all x in
the unsupervised data D. Similarly, V ?D (fn) mea-
sures the negative correlation with y?. Next, we de-
fine VD(fn) as the feature potency of fn: VD(fn) =
V +D (fn)? V
?
D (fn).
An intuitive explanation of VD(fn) is as follows;
if |VD(fn)| is large, the distribution of fn has either
a large positive or negative correlation with the best
output y? given by the base supervised model. This
implies that fn is an informative and potent feature
in the model. Then, the distribution of fn has very
small (or no) correlation to determine y? if |VD(fn)|
is zero or near zero. In this case, fn can be evaluated
as an uninformative feature in the model. From this
perspective, we treat VD(fn) as a measure of feature
potency in terms of the base supervised model.
The essence of this idea, evaluating features
against each other on a certain model, is widely
used in the context of semi-supervised learning,
i.e., (Ando and Zhang, 2005; Suzuki and Isozaki,
637
2008; Druck and McCallum, 2010). Our method
is rough and a much simpler framework for imple-
menting this fundamental idea of semi-supervised
learning developed for NLP tasks. We create a
simple framework to achieve improved flexibility,
extendability, and applicability. In fact, we apply
the framework by incorporating a feature merging
and elimination architecture to obtain effective con-
densed feature sets for supervised learning.
3.2 Feature potency discounting
To discount low potency values, we redefine feature
potency as V ?D(fn) instead of VD(fn) as follows:
V ?D(fn) =
?
?
?
log [Rn+C]?log[An] if Rn?An<?C
0 if ? C?Rn?An?C
log [Rn?C]?log[An] if C<Rn?An
where Rn and An are defined in Figure 2. Note
that VD(fn) = V +D (fn) ? V
?
D (fn) = Rn ? An.
The difference from VD(fn) is that we cast it in the
log-domain and introduce a non-negative constant
C. The introduction of C is inspired by the L1-
regularization technique used in supervised learning
algorithms such as (Duchi and Singer, 2009; Tsu-
ruoka et al, 2009). C controls how much we dis-
count VD(fn) toward zero, and is given by the user.
3.3 Feature potency quantization
We define V ?D(fn) as V ?D(fn) = d?V ?D(fn)e if
V ?D(fn) > 0 and V ?D(fn) = b?V ?D(fn)c otherwise,
where ? is a positive user-specified constant. Note
that V ?D(fn) always becomes an integer, that is,
V ?D(fn) ?N where N = {. . . ,?2,?1, 0, 1, 2, . . .}.
This calculation can be seen as mapping each fea-
ture into a discrete (integer) space with respect to
V ?D(fn). ? controls the range of V ?D(fn) mapping
into the same integer.
3.4 Condensed feature construction
Suppose we have M different quantized feature po-
tency values in V ?D(fn) for all n, which we rewrite
as {um}Mm=1. Then, we define Sm as a set of fn
whose quantized feature potency value is um. As
described in Section 2, we define the m-th con-
densed feature hm(x,y) as the summation of all
the original features fn assigned to Sm. That is,
hm(x,y) =
?
fn?Sm fn(x,y). This feature fusion
process is intuitive since it is acceptable if features
with the same (similar) feature potency are given the
same weight by supervised learning since they have
the same potency with regard to determining y?. ?
determines the number of condensed features to be
made; the number of condensed features becomes
large if ? is large. Obviously, the upper bound of
the number of condensed features is the number of
original features.
To exclude possibly unnecessary original features
from the condensed features, we discard feature fn
for all n if un = 0. This is reasonable since, as de-
scribed in Section 3.1, a feature has small (or no)
effect in achieving the best output decision in the
base supervised model if its potency is near 0. C in-
troduced in Section 3.2 mainly influences how many
original features are discarded.
Additionally, we also utilize the ?quantized? fea-
ture potency values themselves as a new feature.
The reason behind is that they are also very infor-
mative for supervised learning. Their use is impor-
tant to further boost the performance gain offered
by our method. For this purpose, we define ?(x,y)
as ?(x,y) =
?M
m=1(um/?)hm(x,y). We then
use ?(x,y) as the (M + 1)-th feature of our con-
densed feature set. As a result, the condensed fea-
ture set obtained with our method is represented as
H = {h1(x,y), . . . , hM (x,y), ?(x,y)}.
Note that the calculation cost of ?(x,y) is negli-
gible. We can calculate the linear discriminant func-
tion g(x,y) as: g(x,y) =
?M
m=1 wmhm(x,y) +
wM+1?(x,y) =
?M
m=1 w?mhm(x,y), where w?m =
(wm + wM+1um/?). We emphasize that once
{wm}M+1m=1 are determined by supervised learning,
we can calculate w?m in a preliminary step before
the test phase. Thus, our method also takes the form
of a linear model. The number of features for our
method is essentially M even if we add ?.
3.5 Application to Structured Prediction Tasks
We modify our method to better suit structured pre-
diction problems in terms of calculation cost. For a
structured prediction problem, it is usual to decom-
pose or factorize output structure y into a set of lo-
cal sub-structures z to reduce the calculation cost
and to cope with the sparsity of the output space
Y . This factorization can be accomplished by re-
stricting features that are extracted only from the in-
formation within decomposed local sub-structure z
638
and given input x. We write z ? y when the lo-
cal sub-structure z is a part of output y, assuming
that output y is constructed by a set of local sub-
structures. Then formally, the n-th feature is written
as fn(x, z), and fn(x,y) =
?
z?y fn(x, z) holds.
Similarly, we introduce r(x, z), where r(x, z) = 1
if z ? y?, and r(x, z) = 0 otherwise, namely z /? y?.
We define Z(x) as the set of all local sub-
structures possibly generated for all y in Y(x).
Z(x) can be enumerated easily, unless we use typi-
cal first- or second-order factorization models by the
restriction of efficient decoding algorithms, which is
the typical case for many NLP tasks such as named
entity recognition and dependency parsing.
Finally, we replace all Y(x) with Z(x), and use
fn(x, z) and r(x, z) instead of fn(x,y) and r(x,y),
respectively, in Rn and An. When we use these sub-
stitutions, there is no need to incorporate an efficient
algorithm such as dynamic programming into our
method. This means that our feature potency esti-
mation can be applied to the structured prediction
problem at low cost.
3.6 Efficient feature potency computation
Our feature potency estimation described in Section
3.1 to 3.3 is highly suitable for implementation in
the MapReduce framework (Dean and Ghemawat,
2008), which is a modern distributed parallel com-
puting framework. This is because Rn and An can
be calculated by the summation of a data-wise cal-
culation (map phase), and V ?D(fn) can be calculated
independently by each feature (reduce phase). We
emphasize that our feature potency estimation can
be performed in a ?single? map-reduce process.
4 Experiments
We conducted experiments on two different NLP
tasks, namely NER and dependency parsing. To fa-
cilitate comparisons with the performance of previ-
ous methods, we adopted the experimental settings
used to examine high-performance semi-supervised
NLP systems; i.e., NER (Ando and Zhang, 2005;
Suzuki and Isozaki, 2008) and dependency pars-
ing (Koo et al, 2008; Chen et al, 2009; Suzuki
et al, 2009). For the supervised datasets, we used
CoNLL?03 (Tjong Kim Sang and DeMeulder, 2003)
shared task data for NER, and the Penn Treebank III
(PTB) corpus (Marcus et al, 1994) for dependency
parsing. We prepared a total of 3.72 billion token
text data as unsupervised data following the instruc-
tions given in (Suzuki et al, 2009).
4.1 Comparative Methods
We mainly compare the effectiveness of COFER
with that of CWR derived by the Brown algorithm.
The iCWR approach yields the state-of-the-art re-
sults with both dependency parsing data derived
from PTB-III (Koo et al, 2008), and the CoNLL?03
shared task data (Turian et al, 2010). By compar-
ing COFER with iCWR we can clarify its effective-
ness in terms of providing better features for super-
vised learning. We use the term active features to
refer to features whose corresponding model param-
eter is non-zero after supervised learning. It is well-
known that we can discard non-active features from
the trained model without any loss after finishing su-
pervised learning. Finally, we compared the perfor-
mance in terms of the number of active features in
the model given by supervised learning. We note
here that the number of active features for COFER
is the number of features hm if w?m = 0, which is
not wm = 0 for a fair comparison.
Unlike COFER, iCWR does not have any archi-
tecture to winnow the original feature set used in
supervised learning. For a fair comparison, we
prepared L1-regularized supervised learning algo-
rithms, which try to reduce the non-zero parameters
in a model. Specifically, we utilized L1-regularized
CRF (L1CRF) optimized by OWL-QN (Andrew
and Gao, 2007) for NER, and the online struc-
tured output learning version of FOBOS (Duchi
and Singer, 2009; Tsuruoka et al, 2009) with L1-
regularization (ostL1FOBOS) for dependency pars-
ing. In addition, we also examined L2 regular-
ized CRF (Lafferty et al, 2001) optimized by L-
BFGS (Liu and Nocedal, 1989) (L2CRF) for NER,
and the online structured output learning version of
the Passive-Aggressive algorithm (ostPA) (Cram-
mer et al, 2006) for dependency parsing to illus-
trate the baseline performance regardless of the ac-
tive feature number.
4.2 Settings for COFER
We utilized baseline supervised learning mod-
els as the base supervised models of COFER.
639
86.0
88.0
90.0
92.0
94.0
96.0
1.0E+01 1.0E+03 1.0E+05 1.0E+07 1.0E+09
iCWR+COFER: L2CRF iCWR+COFER: L1CRFCOFER: L2CRF COFER: L1CRFiCWR: L2CRF iCWR: L1CRFSup.L2CRF Sup.L1CRF
F-scor
e
# of active features [log-scale]
?=1e+01?=1e+02 ?=1e+04
?=1e+00
proposed
90.0
91.0
92.0
93.0
94.0
95.0
1.E+02 1.E+04 1.E+06 1.E+08
iCWR+COFER: ostPA iCWR+COFER: ostL1FOBOSCOFER: ostPA COFER: ostL1FOBOSiCWR: ostPA iCWR: ostL1FOBOSSup.ostPA Sup.ostL1FOBOS
# of active features [log-scale]
Unlabe
led At
tachme
nt Sco
re ?=1e+00
?=1e+05?=1e+01 ?=1e+03proposed
(a) NER (F-score) (b) dep. parsing (UAS)
Figure 3: Performance vs. size of active features in the
trained model on the development sets
In addition, we also report the results when we
treat iCWR as COFER?s base supervised mod-
els (iCWR+COFER). This is a very natural and
straightforward approach to combining these two.
We generally handle several different types of fea-
tures such as words, part-of-speech tags, word sur-
face forms, and their combinations. Suppose we
have K different feature types, which are often de-
fined by feature templates, i.e., (Suzuki and Isozaki,
2008; Lin andWu, 2009). In our experiments, we re-
strict the merging of features during the condensed
feature construction process if and only if the fea-
tures are the same feature type. As a result, COFER
essentially consists ofK different condensed feature
sets. The numbers of feature typesK were 79 and 30
for our NER and dependency parsing experiments,
respectively. We note that this kind of feature par-
tition by their types is widely used in the context of
semi-supervised learning (Ando and Zhang, 2005;
Suzuki and Isozaki, 2008).
4.3 Results and Discussion
Figure 3 displays the performance on the develop-
ment set with respect to the number of active fea-
tures in the trained models given by each supervised
learning algorithm. In both NER and dependency
parsing experiments, COFER significantly outper-
formed iCWR. Moreover, COFER was surprisingly
robust in relation to the number of active features
in the model. These results reveal that COFER pro-
vides effective feature sets for certain NLP tasks.
We summarize the noteworthy results in Figure 3,
and also the performance of recent top-line systems
for NER and dependency parsing in Table 1. Over-
all, COFER matches the results of top-line semi-
NER system dev. test #.USD #.AF
Sup.L1CRF 90.40 85.08 0 0.57M
iCWR: L1CRF 93.33 89.99 3,720M 0.62M
COFER: L1CRF (? = 1e+ 00) 93.42 88.81 3,720M 359
(? = 1e+ 04) 93.60 89.22 3,720M 2.46M
iCWR+COFER: (? = 1e+ 00) 94.39 90.72 3,720M 344
L1CRF (? = 1e+ 04) 94.91 91.02 3,720M 5.94M
(Ando and Zhang, 2005) 93.15 89.31 27M N/A
(Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A
(Ratinov and Roth, 2009) 93.50 90.57 N/A N/A
(Turian et al, 2010) 93.95 90.36 37M N/A
(Lin and Wu, 2009) N/A 90.90 700,000M N/A
Dependency parser dev. test #.USD #.AF
ostL1FOBOS 93.15 92.82 0 6.80M
iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M
COFER:ostL1FOBOS (? = 1e+ 03) 93.53 93.23 3,720M 20.7K
(? = 1e+ 05) 93.91 93.71 3,720M 3.23M
iCWR+COFER: (? = 1e+ 03) 93.93 93.55 3,720M 12.5K
ostL1FOBOS (? = 1e+ 05) 94.33 94.22 3,720M 5.77M
(Koo and Collins, 2010) 93.49 93.04 0 N/A
(Martins et al, 2010) N/A 93.26 0 55.25M
(Koo et al, 2008) 93.30 93.16 43M N/A
(Chen et al, 2009) N/A 93.16 43M N/A
(Suzuki et al, 2009) 94.13 93.79 3,720M N/A
Table 1: Comparison with previous top-line systems on
test data. (#.USD: unsupervised data size. #.AF: the size
of active features in the trained model.)
supervised learning systems even though it uses far
fewer active features.
In addition, the combination of iCWR+COFER
significantly outperformed the current best results
by achieving a 0.12 point gain from 90.90 to 91.02
for NER, and a 0.43 point gain from 93.79 to 94.22
for dependency parsing, with only 5.94M and 5.77M
features, respectively.
5 Conclusion
This paper introduced the idea of condensed feature
representations (COFER) as a simple and general
framework that can enhance the performance of ex-
isting supervised NLP systems. We also proposed
a method that efficiently constructs condensed fea-
ture sets through discrete feature potency estima-
tion over unsupervised data. We demonstrated that
COFER based on our feature potency estimation can
offer informative dense and low-dimensional feature
spaces for supervised learning, which is theoreti-
cally preferable to the sparse and high-dimensional
feature spaces often used in many NLP tasks. Exist-
ing NLP systems can be made more compact with
higher performance by retraining their models with
our condensed features.
640
References
Rie Kubota Ando and Tong Zhang. 2005. A High-
Performance Semi-Supervised Learning Method for
Text Chunking. In Proceedings of 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 1?9.
Galen Andrew and Jianfeng Gao. 2007. Scalable
Training of L1-regularized Log-linear Models. In
Zoubin Ghahramani, editor, Proceedings of the 24th
Annual International Conference on Machine Learn-
ing (ICML 2007), pages 33?40. Omnipress.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving Dependency
Parsing with Subtrees from Auto-Parsed Data. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 570?579.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online Passive-
Aggressive Algorithms. Journal of Machine Learning
Research, 7:551?585.
Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce:
Simplified Data Processing on Large Clusters. Com-
mun. ACM, 51(1):107?113.
Gregory Druck and Andrew McCallum. 2010. High-
Performance Semi-Supervised Learning using Dis-
criminatively Constrained Generative Models. In Pro-
ceedings of the International Conference on Machine
Learning (ICML 2010), pages 319?326.
John Duchi and Yoram Singer. 2009. Efficient On-
line and Batch Learning Using Forward Backward
Splitting. Journal of Machine Learning Research,
10:2899?2934.
Terry Koo and Michael Collins. 2010. Efficient Third-
Order Dependency Parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1?11.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of the International Conference on Ma-
chine Learning (ICML 2001), pages 282?289.
Dekang Lin and Xiaoyun Wu. 2009. Phrase Cluster-
ing for Discriminative Learning. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1030?1038.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimization.
Math. Programming, Ser. B, 45(3):503?528.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo Parsers: Depen-
dency Parsing by Approximate Variational Inference.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34?
44.
Lev Ratinov and Dan Roth. 2009. Design Challenges
and Misconceptions in Named Entity Recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL-2009),
pages 147?155.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
Sequential Labeling and Segmentation Using Giga-
Word Scale Unlabeled Data. In Proceedings of ACL-
08: HLT, pages 665?673.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael
Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for Depen-
dency Parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 551?560.
Erik Tjong Kim Sang and Fien De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proceed-
ings of CoNLL-2003, pages 142?147.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic Gradient Descent Training
for L1-regularized Log-linear Models with Cumula-
tive Penalty. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 477?485.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 384?394.
641
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?10,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning to Translate with Multiple Objectives
Kevin Duh? Katsuhito Sudoh Xianchao Wu Hajime Tsukada Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikari-dai, Seika-cho, Kyoto 619-0237, JAPAN
kevinduh@is.naist.jp, lastname.firstname@lab.ntt.co.jp
Abstract
We introduce an approach to optimize a ma-
chine translation (MT) system on multiple
metrics simultaneously. Different metrics
(e.g. BLEU, TER) focus on different aspects
of translation quality; our multi-objective ap-
proach leverages these diverse aspects to im-
prove overall quality.
Our approach is based on the theory of Pareto
Optimality. It is simple to implement on top of
existing single-objective optimization meth-
ods (e.g. MERT, PRO) and outperforms ad
hoc alternatives based on linear-combination
of metrics. We also discuss the issue of metric
tunability and show that our Pareto approach
is more effective in incorporating new metrics
from MT evaluation for MT optimization.
1 Introduction
Weight optimization is an important step in build-
ing machine translation (MT) systems. Discrimi-
native optimization methods such as MERT (Och,
2003), MIRA (Crammer et al, 2006), PRO (Hop-
kins and May, 2011), and Downhill-Simplex (Nelder
and Mead, 1965) have been influential in improving
MT systems in recent years. These methods are ef-
fective because they tune the system to maximize an
automatic evaluation metric such as BLEU, which
serve as surrogate objective for translation quality.
However, we know that a single metric such as
BLEU is not enough. Ideally, we want to tune to-
wards an automatic metric that has perfect corre-
lation with human judgments of translation quality.
?*Now at Nara Institute of Science & Technology (NAIST)
While many alternatives have been proposed, such a
perfect evaluation metric remains elusive.
As a result, many MT evaluation campaigns now
report multiple evaluation metrics (Callison-Burch
et al, 2011; Paul, 2010). Different evaluation met-
rics focus on different aspects of translation quality.
For example, while BLEU (Papineni et al, 2002)
focuses on word-based n-gram precision, METEOR
(Lavie and Agarwal, 2007) allows for stem/synonym
matching and incorporates recall. TER (Snover
et al, 2006) allows arbitrary chunk movements,
while permutation metrics like RIBES (Isozaki et
al., 2010; Birch et al, 2010) measure deviation in
word order. Syntax (Owczarzak et al, 2007) and se-
mantics (Pado et al, 2009) also help. Arguably, all
these metrics correspond to our intuitions on what is
a good translation.
The current approach of optimizing MT towards
a single metric runs the risk of sacrificing other met-
rics. Can we really claim that a system is good if
it has high BLEU, but very low METEOR? Simi-
larly, is a high-METEOR low-BLEU system desir-
able? Our goal is to propose a multi-objective op-
timization method that avoids ?overfitting to a sin-
gle metric?. We want to build a MT system that
does well with respect to many aspects of transla-
tion quality.
In general, we cannot expect to improve multi-
ple metrics jointly if there are some inherent trade-
offs. We therefore need to define the notion of Pareto
Optimality (Pareto, 1906), which characterizes this
tradeoff in a rigorous way and distinguishes the set
of equally good solutions. We will describe Pareto
Optimality in detail later, but roughly speaking, a
1
hypothesis is pareto-optimal if there exist no other
hypothesis better in all metrics. The contribution of
this paper is two-fold:
? We introduce PMO (Pareto-based Multi-
objective Optimization), a general approach for
learning with multiple metrics. Existing single-
objective methods can be easily extended to
multi-objective using PMO.
? We show that PMO outperforms the alterna-
tive (single-objective optimization of linearly-
combined metrics) in multi-objective space,
and especially obtains stronger results for met-
rics that may be difficult to tune individually.
In the following, we first explain the theory of
Pareto Optimality (Section 2), and then use it to
build up our proposed PMO approach (Section 3).
Experiments on NIST Chinese-English and PubMed
English-Japanese translation using BLEU, TER, and
RIBES are presented in Section 4. We conclude by
discussing related work (Section 5) and opportuni-
ties/limitations (Section 6).
2 Theory of Pareto Optimality
2.1 Definitions and Concepts
The idea of Pareto optimality comes originally from
economics (Pareto, 1906), where the goal is to char-
acterize situations when a change in allocation of
goods does not make anybody worse off. Here, we
will explain it in terms of MT:
Let h ? L be a hypothesis from an N-best list L.
We have a total of K different metrics Mk(h) for
evaluating the quality of h. Without loss of gen-
erality, we assume metric scores are bounded be-
tween 0 and 1, with 1 being perfect. Each hypoth-
esis h can be mapped to a K-dimensional vector
M(h) = [M1(h);M2(h); ...;MK(h)]. For exam-
ple, suppose K = 2, M1(h) computes the BLEU
score, and M2(h) gives the METEOR score of h.
Figure 1 illustrates the set of vectors {M(h)} in a
10-best list.
For two hypotheses h1, h2, we write M(h1) >
M(h2) if h1 is better than h2 in all metrics, and
M(h1) ? M(h2) if h1 is better than or equal
to h2 in all metrics. When M(h1) ? M(h2) and
Mk(h1) > Mk(h2) for at least one metric k, we say
that h1 dominates h2 and write M(h1) . M(h2).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
metric1
me
tric2
Figure 1: Illustration of Pareto Frontier. Ten hypotheses
are plotted by their scores in two metrics. Hypotheses
indicated by a circle (o) are pareto-optimal, while those
indicated by a plus (+) are not. The line shows the convex
hull, which attains only a subset of pareto-optimal points.
The triangle (4) is a point that is weakly pareto-optimal
but not pareto-optimal.
Definition 1. Pareto Optimal: A hypothesis h? ?
L is pareto-optimal iff there does not exist another
hypothesis h ? L such that M(h) . M(h?).
In Figure 1, the hypotheses indicated by circle
(o) are pareto-optimal, while those with plus (+) are
not. To visualize this, take for instance the pareto-
optimal point (0.4,0.7). There is no other point with
either (metric1 > 0.4 and metric2 ? 0.7), or (met-
ric1 ? 0.4 and metric2 > 0.7). On the other hand,
the non-pareto point (0.6,0.4) is ?dominated? by an-
other point (0.7,0.6), because for metric1: 0.7 > 0.6
and for metric2: 0.6 > 0.4.
There is another definition of optimality, which
disregards ties and may be easier to visualize:
Definition 2. Weakly Pareto Optimal: A hypothesis
h? ? L is weakly pareto-optimal iff there is no other
hypothesis h ? L such that M(h) > M(h?).
Weakly pareto-optimal points are a superset of
pareto-optimal points. A hypothesis is weakly
pareto-optimal if there is no other hypothesis that
improves all the metrics; a hypothesis is pareto-
optimal if there is no other hypothesis that improves
at least one metric without detriment to other met-
rics. In Figure 1, point (0.1,0.8) is weakly pareto-
optimal but not pareto-optimal, because of the com-
peting point (0.3,0.8). Here we focus on pareto-
optimality, but note our algorithms can be easily
2
modified for weakly pareto-optimality. Finally, we
can introduce the key concept used in our proposed
PMO approach:
Definition 3. Pareto Frontier: Given an N-best list
L, the set of all pareto-optimal hypotheses h ? L is
called the Pareto Frontier.
The Pareto Frontier has two desirable properties
from the multi-objective optimization perspective:
1. Hypotheses on the Frontier are equivalently
good in the Pareto sense.
2. For each hypothesis not on the Frontier, there
is always a better (pareto-optimal) hypothesis.
This provides a principled approach to optimiza-
tion: i.e. optimizing towards points on the Frontier
and away from those that are not, and giving no pref-
erence to different pareto-optimal hypotheses.
2.2 Reduction to Linear Combination
Multi-objective problems can be formulated as:
arg max
w
[M1(h);M2(h); . . . ;Mk(h)] (1)
where h = Decode(w, f)
Here, the MT system?s Decode function, parame-
terized by weight vector w, takes in a foreign sen-
tence f and returns a translated hypothesis h. The
argmax operates in vector space and our goal is to
find w leading to hypotheses on the Pareto Frontier.
In the study of Pareto Optimality, one central
question is: To what extent can multi-objective prob-
lems be solved by single-objective methods? Equa-
tion 1 can be reduced to a single-objective problem
by scalarizing the vector [M1(h); . . . ;Mk(h)] with
a linear combination:
arg max
w
K?
k=1
pkMk(h) (2)
where h = Decode(w, f)
Here, pk are positive real numbers indicating the rel-
ative importance of each metric (without loss of gen-
erality, assume
?
k pk = 1). Are the solutions to
Eq. 2 also solutions to Eq. 1 (i.e. pareto-optimal)
and vice-versa? The theory says:
Theorem 1. Sufficient Condition: If w? is solution
to Eq. 2, then it is weakly pareto-optimal. Further,
if w? is unique, then it is pareto-optimal.
Theorem 2. No Necessary Condition: There may
exist solutions to Eq. 1 that cannot be achieved by
Eq. 2, irregardless of any setting of {pk}.
Theorem 1 is a positive result asserting that lin-
ear combination can give pareto-optimal solutions.
However, Theorem 2 states the limits: in partic-
ular, Eq. 2 attains only pareto-optimal points that
are on the convex hull. This is illustrated in Fig-
ure 1: imagine sweeping all values of p1 = [0, 1]
and p2 = 1? p1 and recording the set of hypotheses
that maximizes
?
k pkMk(h). For 0.6 < p1 ? 1 we
get h = (0.9, 0.1), for p1 = 0.6 we get (0.7, 0.6),
and for 0 < p1 < 0.6 we get (0.4, 0.8). At no
setting of p1 do we attain h = (0.4, 0.7) which
is also pareto-optimal but not on the convex hull.1
This may have ramifications for issues like metric
tunability and local optima. To summarize, linear-
combination is reasonable but has limitations. Our
proposed approach will instead directly solve Eq. 1.
Pareto Optimality and multi-objective optimiza-
tion is a deep field with active inquiry in engineer-
ing, operations research, economics, etc. For the in-
terested reader, we recommend the survey by Mar-
ler and Arora (2004) and books by (Sawaragi et al,
1985; Miettinen, 1998).
3 Multi-objective Algorithms
3.1 Computing the Pareto Frontier
Our PMO approach will need to compute the Pareto
Frontier for potentially large sets of points, so we
first describe how this can be done efficiently. Given
a set of N vectors {M(h)} from an N-best list L,
our goal is extract the subset that are pareto-optimal.
Here we present an algorithm based on iterative
filtering, in our opinion the simplest algorithm to
understand and implement. The strategy is to loop
through the list L, keeping track of any dominant
points. Given a dominant point, it is easy to filter
out many points that are dominated by it. After suc-
cessive rounds, any remaining points that are not fil-
1We note that scalarization by exponentiated-combination
?
k pkMk(h)
q , for a suitable q > 0, does satisfy necessary
conditions for pareto optimality. However the proper tuning of q
is not known a priori. See (Miettinen, 1998) for theorem proofs.
3
Algorithm 1 FindParetoFrontier
Input: {M(h)}, h ? L
Output: All pareto-optimal points of {M(h)}
1: F = ?
2: while L is not empty do
3: h? = shift(L)
4: for each h in L do
5: if (M(h?) . M(h)): remove h from L
6: else if (M(h) . M(h?)): remove h from L; set
h? = h
7: end for
8: Add h? to Frontier Set F
9: for each h in L do
10: if (M(h?) . M(h)): remove h from L
11: end for
12: end while
13: Return F
tered are necessarily pareto-optimal. Algorithm 1
shows the pseudocode. In line 3, we take a point h?
and check if it is dominating or dominated in the for-
loop (lines 4-8). At least one pareto-optimal point
will be found by line 8. The second loop (lines 9-11)
further filters the list for points that are dominated by
h? but iterated before h? in the first for-loop.
The outer while-loop stops exactly after P iter-
ations, where P is the actual number of pareto-
optimal points in L. Each inner loop costs O(KN)
so the total complexity is O(PKN). Since P ? N
with the actual value depending on the probability
distribution of {M(h)}, the worst-case run-time is
O(KN2). For a survey of various Pareto algorithms,
refer to (Godfrey et al, 2007). The algorithm we de-
scribed here is borrowed from the database literature
in what is known as skyline operators.2
3.2 PMO-PRO Algorithm
We are now ready to present an algorithm for multi-
objective optimization. As we will see, it can be seen
as a generalization of the pairwise ranking optimiza-
tion (PRO) of (Hopkins and May, 2011), so we call
it PMO-PRO. PMO-PRO approach works by itera-
tively decoding-and-optimizing on the devset, sim-
2The inquisitive reader may wonder how is Pareto related
to databases. The motivation is to incorporate preferences into
relational queries(Bo?rzso?nyi et al, 2001). For K = 2 metrics,
they also present an alternative faster O(N logN) algorithm by
first topologically sorting along the 2 dimensions. All domi-
nated points can be filtered by one-pass by comparing with the
most-recent dominating point.
ilar to many MT optimization methods. The main
difference is that rather than trying to maximize a
single metric, we maximize the number of pareto
points, in order to expand the Pareto Frontier
We will explain PMO-PRO in terms of the
pseudo-code shown in Algorithm 2. For each sen-
tence pair (f, e) in the devset, we first generate an
N-best list L ? {h} using the current weight vector
w (line 5). In line 6, we evaluate each hypothesis
h with respect to the K metrics, giving a set of K-
dimensional vectors {M(h)}.
Lines 7-8 is the critical part: it gives a ?la-
bel? to each hypothesis, based on whether it is
in the Pareto Frontier. In particular, first we call
FindParetoFrontier (Algorithm 1), which re-
turns a set of pareto hypotheses; pareto-optimal hy-
potheses will get label 1 while non-optimal hypothe-
ses will get label 0. This information is added to
the training set T (line 8), which is then optimized
by any conventional subroutine in line 10. We will
follow PRO in using a pairwise classifier in line 10,
which finds w? that separates hypotheses with labels
1 vs. 0. In essence, this is the trick we employ to
directly optimize on the Pareto Frontier. If we had
used BLEU scores rather than the {0, 1} labels in
line 8, the entire PMO-PRO algorithm would revert
to single-objective PRO.
By definition, there is no single ?best? result
for multi-objective optimization, so we collect all
weights and return the Pareto-optimal set. In line 13
we evaluate each weight w on K metrics across the
entire corpus and call FindParetoFrontier
in line 14.3 This choice highlights an interesting
change of philosophy: While setting {pk} in linear-
combination forces the designer to make an a priori
preference among metrics prior to optimization, the
PMO strategy is to optimize first agnostically and
a posteriori let the designer choose among a set of
weights. Arguably it is easier to choose among so-
lutions based on their evaluation scores rather than
devising exact values for {pk}.
3.3 Discussion
Variants: In practice we find that a slight modifi-
cation of line 8 in Algorithm 2 leads to more sta-
3Note this is the same FindParetoFrontier algorithm as used
in line 7. Both operate on sets of points in K-dimensional
space, induced from either weights {w} or hypotheses {h}.
4
Algorithm 2 Proposed PMO-PRO algorithm
Input: Devset, max number of iterations I
Output: A set of (pareto-optimal) weight vectors
1: Initialize w. LetW = ?.
2: for i = 1 to I do
3: Let T = ?.
4: for each (f, e) in devset do
5: {h} =DecodeNbest(w,f )
6: {M(h)}=EvalMetricsOnSentence({h}, e)
7: {f} =FindParetoFrontier({M(h)})
8: foreach h ? {h}:
if h ? {f}, set l=1, else l=0; Add (l, h) to T
9: end for
10: w?=OptimizationSubroutine(T , w)
11: Add w? toW; Set w = w?.
12: end for
13: M(w) =EvalMetricsOnCorpus(w,devset) ?w ? W
14: Return FindParetoFrontier({M(w)})
ble results for PMO-PRO: for non-pareto hypothe-
ses h /? {f}, we set label l =
?
kMk(h)/K in-
stead of l= 0, so the method not only learns to dis-
criminate pareto vs. non-pareto but also also learns
to discriminate among competing non-pareto points.
Also, like other MT works, in line 5 the N-best list is
concatenated to N-best lists from previous iterations,
so {h} is a set with i ?N elements.
General PMO Approach: The strategy we out-
lined in Section 3.2 can be easily applied to other
MT optimization techniques. For example, by re-
placing the optimization subroutine (line 10, Algo-
rithm 2) with a Powell search (Och, 2003), one can
get PMO-MERT4. Alternatively, by using the large-
margin optimizer in (Chiang et al, 2009) and mov-
ing it into the for-each loop (lines 4-9), one can
get an online algorithm such PMO-MIRA. Virtually
all MT optimization algorithms have a place where
metric scores feedback into the optimization proce-
dure; the idea of PMO is to replace these raw scores
with labels derived from Pareto optimality.
4 Experiments
4.1 Evaluation Methodology
We experiment with two datasets: (1) The PubMed
task is English-to-Japanese translation of scientific
4A difference with traditional MERT is the necessity of
sentence-BLEU (Liang et al, 2006) in line 6. We use sentence-
BLEU for optimization but corpus-BLEU for evaluation here.
abstracts. As metrics we use BLEU and RIBES
(which demonstrated good human correlation in
this language pair (Goto et al, 2011)). (2) The
NIST task is Chinese-to-English translation with
OpenMT08 training data and MT06 as devset. As
metrics we use BLEU and NTER.
? BLEU = BP ? (?precn)1/4. BP is brevity
penality. precn is precision of n-gram matches.
? RIBES = (? + 1)/2 ? prec1/41 , with Kendall?s
? computed by measuring permutation between
matching words in reference and hypothesis5.
? NTER=max(1?TER, 0), which normalizes
Translation Edit Rate6 so that NTER=1 is best.
We compare two multi-objective approaches:
1. Linear-Combination of metrics (Eq. 2),
optimized with PRO. We search a range
of combination settings: (p1, p2) =
{(0, 1), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3), (1, 0)}.
Note (1, 0) reduces to standard single-metric
optimization of e.g. BLEU.
2. Proposed Pareto approach (PMO-PRO).
Evaluation of multi-objective problems can be
tricky because there is no single figure-of-merit.
We thus adopted the following methodology: We
run both methods 5 times (i.e. using the 5 differ-
ent (p1, p2) setting each time) and I = 20 iterations
each. For each method, this generates 5x20=100 re-
sults, and we plot the Pareto Frontier of these points
in a 2-dimensional metric space (e.g. see Figure 2).
A method is deemed better if its final Pareto Fron-
tier curve is strictly dominating the other. We report
devset results here; testset trends are similar but not
included due to space constraints.7
5from www.kecl.ntt.co.jp/icl/lirg/ribes
6from www.umd.edu/?snover/tercom
7An aside: For comparing optimization methods, we believe
devset comparison is preferable to testset since data mismatch
may confound results. If one worries about generalization, we
advocate to re-decode the devset with final weights and evaluate
its 1-best output (which is done here). This is preferable to sim-
ply reporting the achieved scores on devset N-best (as done in
some open-source scripts) since the learned weight may pick
out good hypotheses in the N-best but perform poorly when
re-decoding the same devset. The re-decode devset approach
avoids being overly optimistic while accurately measuring op-
timization performance.
5
Train Devset #Feat Metrics
PubMed 0.2M 2k 14 BLEU, RIBES
NIST 7M 1.6k 8 BLEU, NTER
Table 1: Task characteristics: #sentences in Train/Dev, #
of features, and metrics used. Our MT models are trained
with standard phrase-based Moses software (Koehn and
others, 2007), with IBM M4 alignments, 4gram SRILM,
lexical ordering for PubMed and distance ordering for the
NIST system. The decoder generates 50-best lists each
iteration. We use SVMRank (Joachims, 2006) as opti-
mization subroutine for PRO, which efficiently handle all
pairwise samples without the need for sampling.
4.2 Results
Figures 2 and 3 show the results for PubMed and
NIST, respectively. A method is better if its Pareto
Frontier lies more towards the upper-right hand cor-
ner of the graph. Our observations are:
1. PMO-PRO generally outperforms Linear-
Combination with any setting of (p1, p2).
The Pareto Frontier of PMO-PRO dominates
that of Linear-Combination. This implies
PMO is effective in optimizing towards Pareto
hypotheses.
2. For both methods, trading-off between met-
rics is necessary. For example in PubMed,
the designer would need to make a choice be-
tween picking the best weight according to
BLEU (BLEU=.265,RIBES=.665) vs. another
weight with higher RIBES but poorer BLEU,
e.g. (.255,.675). Nevertheless, both the PMO
and Linear-Combination with various (p1, p2)
samples this joint-objective space broadly.
3. Interestingly, a multi-objective approach can
sometimes outperform a single-objective opti-
mizer in its own metric. In Figure 2, single-
objective PRO focusing on optimizing RIBES
only achieves 0.68, but PMO-PRO using both
BLEU and RIBES outperforms with 0.685.
The third observation relates to the issue of metric
tunability (Liu et al, 2011). We found that RIBES
can be difficult to tune directly. It is an extremely
non-smooth objective with many local optima?slight
changes in word ordering causes large changes in
RIBES. So the best way to improve RIBES is to
0.2 0.21 0.22 0.23 0.24 0.25 0.26 0.270.665
0.67
0.675
0.68
0.685
0.69
0.695
bleu
ribe
s
 
 Linear CombinationPareto (PMO?PRO)
Figure 2: PubMed Results. The curve represents the
Pareto Frontier of all results collected after multiple runs.
0.146 0.148 0.15 0.152 0.154 0.156 0.158 0.16 0.162 0.1640.694
0.695
0.696
0.697
0.698
0.699
0.7
0.701
0.702
0.703
0.704
bleu
nte
r
 
 Linear CombinationPareto (PMO?PRO)
Figure 3: NIST Results
not to optimize it directly, but jointly with a more
tunable metric BLEU. The learning curve in Fig-
ure 4 show that single-objective optimization of
RIBES quickly falls into local optimum (at iteration
3) whereas PMO can zigzag and sacrifice RIBES in
intermediate iterations (e.g. iteration 2, 15) leading
to a stronger result ultimately. The reason is the
diversity of solutions provided by the Pareto Fron-
tier. This finding suggests that multi-objective ap-
proaches may be preferred, especially when dealing
with new metrics that may be difficult to tune.
4.3 Additional Analysis and Discussions
What is the training time? The Pareto approach
does not add much overhead to PMO-PRO. While
FindParetoFrontier scales quadratically by size of
N-best list, Figure 5 shows that the runtime is triv-
6
0 2 4 6 8 10 12 14 16 18 200.63
0.64
0.65
0.66
0.67
0.68
0.69
iteration
ribe
s
 
 
Single?Objective RIBES
Pareto (PMO?PRO)
Figure 4: Learning Curve on RIBES: comparing single-
objective optimization and PMO.
0 100 200 300 400 500 600 700 800 900 10000
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Set size |L|
Run
time
 (sec
onds
)
 
 
Algorithm 1
TopologicalSort (footnote 2)
Figure 5: Avg. runtime per sentence of FindPareto
ial (0.3 seconds for 1000-best). Table 2 shows
the time usage breakdown in different iterations for
PubMed. We see it is mostly dominated by decod-
ing time (constant per iteration at 40 minutes on
single 3.33GHz processor). At later iterations, Opt
takes more time due to larger file I/O in SVMRank.
Note Decode and Pareto can be ?embarrasingly par-
allelized.?
Iter Time Decode Pareto Opt Misc.
(line 5) (line 7) (line 10) (line 6,8)
1 47m 85% 1% 1% 13%
10 62m 67% 6% 8% 19%
20 91m 47% 15% 22% 16%
Table 2: Training time usage in PMO-PRO (Algo 2).
How many Pareto points? The number of pareto
0 2 4 6 8 10 12 14 16 185
10
15
20
25
30
35
Iterations
Num
ber 
of P
aret
o P
oint
s
 
 
NIST
PubMed
Figure 6: Average number of Pareto points
hypotheses gives a rough indication of the diversity
of hypotheses that can be exploited by PMO. Fig-
ure 6 shows that this number increases gradually per
iteration. This perhaps gives PMO-PRO more direc-
tions for optimizing around potential local optimal.
Nevertheless, we note that tens of Pareto points is far
few compared to the large size of N-best lists used
at later iterations of PMO-PRO. This may explain
why the differences between methods in Figure 3
are not more substantial. Theoretically, the num-
ber will eventually level off as it gets increasingly
harder to generate new Pareto points in a crowded
space (Bentley et al, 1978).
Practical recommendation: We present the
Pareto approach as a way to agnostically optimize
multiple metrics jointly. However, in practice, one
may have intuitions about metric tradeoffs even if
one cannot specify {pk}. For example, we might
believe that approximately 1-point BLEU degra-
dation is acceptable only if RIBES improves by
at least 3-points. In this case, we recommend
the following trick: Set up a multi-objective prob-
lem where one metric is BLEU and the other is
3/4BLEU+1/4RIBES. This encourages PMO to ex-
plore the joint metric space but avoid solutions that
sacrifice too much BLEU, and should also outper-
form Linear Combination that searches only on the
(3/4,1/4) direction.
5 Related Work
Multi-objective optimization for MT is a relatively
new area. Linear-combination of BLEU/TER is
7
the most common technique (Zaidan, 2009), some-
times achieving good results in evaluation cam-
paigns (Dyer et al, 2009). As far as we known, the
only work that directly proposes a multi-objective
technique is (He and Way, 2009), which modifies
MERT to optimize a single metric subject to the
constraint that it does not degrade others. These
approaches all require some setting of constraint
strength or combination weights {pk}. Recent work
in MT evaluation has examined combining metrics
using machine learning for better correlation with
human judgments (Liu and Gildea, 2007; Albrecht
and Hwa, 2007; Gimnez and Ma`rquez, 2008) and
may give insights for setting {pk}. We view our
Pareto-based approach as orthogonal to these efforts.
The tunability of metrics is a problem that is gain-
ing recognition (Liu et al, 2011). If a good evalu-
ation metric could not be used for tuning, it would
be a pity. The Tunable Metrics task at WMT2011
concluded that BLEU is still the easiest to tune
(Callison-Burch et al, 2011). (Mauser et al, 2008;
Cer et al, 2010) report similar observations, in ad-
dition citing WER being difficult and BLEU-TER
being amenable. One unsolved question is whether
metric tunability is a problem inherent to the metric
only, or depends also on the underlying optimization
algorithm. Our positive results with PMO suggest
that the choice of optimization algorithm can help.
Multi-objective ideas are being explored in other
NLP areas. (Spitkovsky et al, 2011) describe a tech-
nique that alternates between hard and soft EM ob-
jectives in order to achieve better local optimum in
grammar induction. (Hall et al, 2011) investigates
joint optimization of a supervised parsing objective
and some extrinsic objectives based on downstream
applications. (Agarwal et al, 2011) considers us-
ing multiple signals (of varying quality) from online
users to train recommendation models. (Eisner and
Daume? III, 2011) trades off speed and accuracy of
a parser with reinforcement learning. None of the
techniques in NLP use Pareto concepts, however.
6 Opportunities and Limitations
We introduce a new approach (PMO) for training
MT systems on multiple metrics. Leveraging the
diverse perspectives of different evaluation metrics
has the potential to improve overall quality. Based
on Pareto Optimality, PMO is easy to implement
and achieves better solutions compared to linear-
combination baselines, for any setting of combi-
nation weights. Further we observe that multi-
objective approaches can be helpful for optimiz-
ing difficult-to-tune metrics; this is beneficial for
quickly introducing new metrics developed in MT
evaluation into MT optimization, especially when
good {pk} are not yet known. We conclude by draw-
ing attention to some limitations and opportunities
raised by this work:
Limitations: (1) The performance of PMO is
limited by the size of the Pareto set. Small N-best
lists lead to sparsely-sampled Pareto Frontiers, and
a much better approach would be to enlarge the hy-
pothesis space using lattices (Macherey et al, 2008).
How to compute Pareto points directly from lattices
is an interesting open research question. (2) The
binary distinction between pareto vs. non-pareto
points ignores the fact that 2nd-place non-pareto
points may also lead to good practical solutions. A
better approach may be to adopt a graded definition
of Pareto optimality as done in some multi-objective
works (Deb et al, 2002). (3) A robust evaluation
methodology that enables significance testing for
multi-objective problems is sorely needed. This will
make it possible to compare multi-objective meth-
ods on more than 2 metrics. We also need to follow
up with human evaluation.
Opportunities: (1) There is still much we do
not understand about metric tunability; we can learn
much by looking at joint metric-spaces and exam-
ining how new metrics correlate with established
ones. (2) Pareto is just one approach among many
in multi-objective optimization. A wealth of meth-
ods are available (Marler and Arora, 2004) and more
experimentation in this space will definitely lead to
new insights. (3) Finally, it would be interesting to
explore other creative uses of multiple-objectives in
MT beyond multiple metrics. For example: Can we
learn to translate faster while sacrificing little on ac-
curacy? Can we learn to jointly optimize cascaded
systems, such as as speech translation or pivot trans-
lation? Life is full of multiple competing objectives.
Acknowledgments
We thank the reviewers for insightful feedback.
8
References
Deepak Agarwal, Bee-Chung Chen, Pradheep Elango,
and Xuanhui Wang. 2011. Click shaping to optimize
multiple objectives. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?11, pages 132?140,
New York, NY, USA. ACM.
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level mt evalu-
ation. In ACL.
J. L. Bentley, H. T. Kung, M. Schkolnick, and C. D.
Thompson. 1978. On the average number of max-
ima in a set of vectors and applications. Journal of the
Association for Computing Machinery (JACM), 25(4).
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT evaluation: Evaluating reorder-
ing. Machine Translation, 24(1).
S. Bo?rzso?nyi, D. Kossmann, and K. Stocker. 2001. The
skyline operator. In Proceedings of the 17th Interna-
tional Conference on Data Engineering (ICDE).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Daniel Cer, Christopher Manning, and Daniel Jurafsky.
2010. The best lexical metric for phrase-based statis-
tical MT system optimization. In NAACL HLT.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine translation.
In NAACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passiveag-
gressive algorithms. Journal of Machine Learning Re-
search, 7.
Kalyanmoy Deb, Amrit Pratap, Sammer Agarwal, and
T. Meyarivan. 2002. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation, 6(2).
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The university of maryland statistical
machine translation system for the fourth workshop on
machine translation. In Proc. of the Fourth Workshop
on Machine Translation.
Jason Eisner and Hal Daume? III. 2011. Learning speed-
accuracy tradeoffs in nondeterministic inference algo-
rithms. In COST: NIPS 2011 Workshop on Computa-
tional Trade-offs in Statistical Learning.
Jesu?s Gimnez and Llu??s Ma`rquez. 2008. Heterogeneous
automatic mt evaluation through non-parametric met-
ric combinations. In ICJNLP.
Parke Godfrey, Ryan Shipley, and Jarek Gyrz. 2007. Al-
gorithms and analyses for maximal vector computa-
tion. VLDB Journal, 16.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of the NTCIR-9 Workshop Meeting.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1489?1499, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Yifan He and Andy Way. 2009. Improving the objec-
tive function in minimum error rate training. In MT
Summit.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010. Automatic evaluation of translation quality for
distant language pairs. In EMNLP.
T. Joachims. 2006. Training linear SVMs in linear time.
In KDD.
P. Koehn et al 2007. Moses: open source toolkit for
statistical machine translation. In ACL.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for mt evaluation with high levels of cor-
relation with human judgments. In Workshop on Sta-
tistical Machine Translation.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In NAACL.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP.
R. T. Marler and J. S. Arora. 2004. Survey of
multi-objective optimization methods for engineering.
Structural and Multidisciplinary Optimization, 26.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2008.
Automatic evaluation measures for statistical machine
9
translation system optimization. In International Con-
ference on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
Kaisa Miettinen. 1998. Nonlinear Multiobjective Opti-
mization. Springer.
J.A. Nelder and R. Mead. 1965. The downhill simplex
method. Computer Journal, 7(308).
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
Sebastian Pado, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2-3).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In ACL.
Vilfredo Pareto. 1906. Manuale di Economica Politica,
(Translated into English by A.S. Schwier as Manual of
Political Economy, 1971). Societa Editrice Libraria,
Milan.
Michael Paul. 2010. Overview of the iwslt 2010 evalua-
tion campaign. In IWSLT.
Yoshikazu Sawaragi, Hirotaka Nakayama, and Tetsuzo
Tanino, editors. 1985. Theory of Multiobjective Opti-
mization. Academic Press.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen em: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1269?1280, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. In The Prague Bulletin of Mathe-
matical Linguistics.
10
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440?448,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Bayesian Symbol-Refined Tree Substitution Grammars
for Syntactic Parsing
Hiroyuki Shindo? Yusuke Miyao? Akinori Fujino? Masaaki Nagata?
?NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
{shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
?National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan
yusuke@nii.ac.jp
Abstract
We propose Symbol-Refined Tree Substitu-
tion Grammars (SR-TSGs) for syntactic pars-
ing. An SR-TSG is an extension of the con-
ventional TSG model where each nonterminal
symbol can be refined (subcategorized) to fit
the training data. We aim to provide a unified
model where TSG rules and symbol refine-
ment are learned from training data in a fully
automatic and consistent fashion. We present
a novel probabilistic SR-TSG model based
on the hierarchical Pitman-Yor Process to en-
code backoff smoothing from a fine-grained
SR-TSG to simpler CFG rules, and develop
an efficient training method based on Markov
Chain Monte Carlo (MCMC) sampling. Our
SR-TSG parser achieves an F1 score of 92.4%
in the Wall Street Journal (WSJ) English Penn
Treebank parsing task, which is a 7.7 point im-
provement over a conventional Bayesian TSG
parser, and better than state-of-the-art discrim-
inative reranking parsers.
1 Introduction
Syntactic parsing has played a central role in natural
language processing. The resulting syntactic analy-
sis can be used for various applications such as ma-
chine translation (Galley et al, 2004; DeNeefe and
Knight, 2009), sentence compression (Cohn and La-
pata, 2009; Yamangil and Shieber, 2010), and ques-
tion answering (Wang et al, 2007). Probabilistic
context-free grammar (PCFG) underlies many sta-
tistical parsers, however, it is well known that the
PCFG rules extracted from treebank data via maxi-
mum likelihood estimation do not perform well due
to unrealistic context freedom assumptions (Klein
and Manning, 2003).
In recent years, there has been an increasing inter-
est in tree substitution grammar (TSG) as an alter-
native to CFG for modeling syntax trees (Post and
Gildea, 2009; Tenenbaum et al, 2009; Cohn et al,
2010). TSG is a natural extension of CFG in which
nonterminal symbols can be rewritten (substituted)
with arbitrarily large tree fragments. These tree frag-
ments have great advantages over tiny CFG rules
since they can capture non-local contexts explic-
itly such as predicate-argument structures, idioms
and grammatical agreements (Cohn et al, 2010).
Previous work on TSG parsing (Cohn et al, 2010;
Post and Gildea, 2009; Bansal and Klein, 2010) has
consistently shown that a probabilistic TSG (PTSG)
parser is significantly more accurate than a PCFG
parser, but is still inferior to state-of-the-art parsers
(e.g., the Berkeley parser (Petrov et al, 2006) and
the Charniak parser (Charniak and Johnson, 2005)).
One major drawback of TSG is that the context free-
dom assumptions still remain at substitution sites,
that is, TSG tree fragments are generated that are
conditionally independent of all others given root
nonterminal symbols. Furthermore, when a sentence
is unparsable with large tree fragments, the PTSG
parser usually uses naive CFG rules derived from
its backoff model, which diminishes the benefits ob-
tained from large tree fragments.
On the other hand, current state-of-the-art parsers
use symbol refinement techniques (Johnson, 1998;
Collins, 2003; Matsuzaki et al, 2005). Symbol
refinement is a successful approach for weaken-
ing context freedom assumptions by dividing coarse
treebank symbols (e.g. NP and VP) into sub-
categories, rather than extracting large tree frag-
ments. As shown in several studies on TSG pars-
ing (Zuidema, 2007; Bansal and Klein, 2010), large
440
tree fragments and symbol refinement work comple-
mentarily for syntactic parsing. For example, Bansal
and Klein (2010) have reported that deterministic
symbol refinement with heuristics helps improve the
accuracy of a TSG parser.
In this paper, we propose Symbol-Refined Tree
Substitution Grammars (SR-TSGs) for syntactic
parsing. SR-TSG is an extension of the conventional
TSG model where each nonterminal symbol can be
refined (subcategorized) to fit the training data. Our
work differs from previous studies in that we focus
on a unified model where TSG rules and symbol re-
finement are learned from training data in a fully au-
tomatic and consistent fashion. We also propose a
novel probabilistic SR-TSG model with the hierar-
chical Pitman-Yor Process (Pitman and Yor, 1997),
namely a sort of nonparametric Bayesian model, to
encode backoff smoothing from a fine-grained SR-
TSG to simpler CFG rules, and develop an efficient
training method based on blocked MCMC sampling.
Our SR-TSG parser achieves an F1 score of
92.4% in the WSJ English Penn Treebank pars-
ing task, which is a 7.7 point improvement over a
conventional Bayesian TSG parser, and superior to
state-of-the-art discriminative reranking parsers.
2 Background and Related Work
Our SR-TSG work is built upon recent work on
Bayesian TSG induction from parse trees (Post and
Gildea, 2009; Cohn et al, 2010). We firstly review
the Bayesian TSG model used in that work, and then
present related work on TSGs and symbol refine-
ment.
A TSG consists of a 4-tuple, G = (T,N, S,R),
where T is a set of terminal symbols, N is a set of
nonterminal symbols, S ? N is the distinguished
start nonterminal symbol and R is a set of produc-
tions (a.k.a. rules). The productions take the form
of elementary trees i.e., tree fragments of height
? 1. The root and internal nodes of the elemen-
tary trees are labeled with nonterminal symbols, and
leaf nodes are labeled with either terminal or nonter-
minal symbols. Nonterminal leaves are referred to
as frontier nonterminals, and form the substitution
sites to be combined with other elementary trees.
A derivation is a process of forming a parse tree.
It starts with a root symbol and rewrites (substi-
tutes) nonterminal symbols with elementary trees
until there are no remaining frontier nonterminals.
Figure 1a shows an example parse tree and Figure
1b shows its example TSG derivation. Since differ-
ent derivations may produce the same parse tree, re-
cent work on TSG induction (Post and Gildea, 2009;
Cohn et al, 2010) employs a probabilistic model of
a TSG and predicts derivations from observed parse
trees in an unsupervised way.
A Probabilistic Tree Substitution Grammar
(PTSG) assigns a probability to each rule in the
grammar. The probability of a derivation is defined
as the product of the probabilities of its component
elementary trees as follows.
p (e) =
?
x?e?e
p (e |x) ,
where e = (e1, e2, . . .) is a sequence of elemen-
tary trees used for the derivation, x = root (e) is the
root symbol of e, and p (e |x) is the probability of
generating e given its root symbol x. As in a PCFG,
e is generated conditionally independent of all oth-
ers given x.
The posterior distribution over elementary trees
given a parse tree t can be computed by using the
Bayes? rule:
p (e |t) ? p (t |e) p (e) .
where p (t |e) is either equal to 1 (when t and e
are consistent) or 0 (otherwise). Therefore, the task
of TSG induction from parse trees turns out to con-
sist of modeling the prior distribution p (e). Recent
work on TSG induction defines p (e) as a nonpara-
metric Bayesian model such as the Dirichlet Pro-
cess (Ferguson, 1973) or the Pitman-Yor Process to
encourage sparse and compact grammars.
Several studies have combined TSG induction and
symbol refinement. An adaptor grammar (Johnson
et al, 2007a) is a sort of nonparametric Bayesian
TSG model with symbol refinement, and is thus
closely related to our SR-TSG model. However,
an adaptor grammar differs from ours in that all its
rules are complete: all leaf nodes must be termi-
nal symbols, while our model permits nonterminal
symbols as leaf nodes. Furthermore, adaptor gram-
mars have largely been applied to the task of unsu-
pervised structural induction from raw texts such as
441
(a) (b) (c)
Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of
(a). The refinement annotation is hyphenated with a nonterminal symbol.
morphology analysis, word segmentation (Johnson
and Goldwater, 2009), and dependency grammar in-
duction (Cohen et al, 2010), rather than constituent
syntax parsing.
An all-fragments grammar (Bansal and Klein,
2010) is another variant of TSG that aims to uti-
lize all possible subtrees as rules. It maps a TSG
to an implicit representation to make the grammar
tractable and practical for large-scale parsing. The
manual symbol refinement described in (Klein and
Manning, 2003) was applied to an all-fragments
grammar and this improved accuracy in the English
WSJ parsing task. As mentioned in the introduc-
tion, our model focuses on the automatic learning of
a TSG and symbol refinement without heuristics.
3 Symbol-Refined Tree Substitution
Grammars
In this section, we propose Symbol-Refined Tree
Substitution Grammars (SR-TSGs) for syntactic
parsing. Our SR-TSG model is an extension of
the conventional TSG model where every symbol of
the elementary trees can be refined to fit the train-
ing data. Figure 1c shows an example of SR-TSG
derivation. As with previous work on TSG induc-
tion, our task is the induction of SR-TSG deriva-
tions from a corpus of parse trees in an unsupervised
fashion. That is, we wish to infer the symbol sub-
categories of every node and substitution site (i.e.,
nodes where substitution occurs) from parse trees.
Extracted rules and their probabilities can be used to
parse new raw sentences.
3.1 Probabilistic Model
We define a probabilistic model of an SR-TSG based
on the Pitman-Yor Process (PYP) (Pitman and Yor,
1997), namely a sort of nonparametric Bayesian
model. The PYP produces power-law distributions,
which have been shown to be well-suited for such
uses as language modeling (Teh, 2006b), and TSG
induction (Cohn et al, 2010). One major issue as
regards modeling an SR-TSG is that the space of the
grammar rules will be very sparse since SR-TSG al-
lows for arbitrarily large tree fragments and also an
arbitrarily large set of symbol subcategories. To ad-
dress the sparseness problem, we employ a hierar-
chical PYP to encode a backoff scheme from the SR-
TSG rules to simpler CFG rules, inspired by recent
work on dependency parsing (Blunsom and Cohn,
2010).
Our model consists of a three-level hierarchy. Ta-
ble 1 shows an example of the SR-TSG rule and its
backoff tree fragments as an illustration of this three-
level hierarchy. The topmost level of our model is a
distribution over the SR-TSG rules as follows.
e |xk ? Gxk
Gxk ? PYP
(
dxk , ?xk , P
sr-tsg (? |xk )
)
,
where xk is a refined root symbol of an elemen-
tary tree e, while x is a raw nonterminal symbol
in the corpus and k = 0, 1, . . . is an index of the
symbol subcategory. Suppose x is NP and its sym-
bol subcategory is 0, then xk is NP0. The PYP has
three parameters: (dxk , ?xk , P
sr-tsg). P sr-tsg (? |xk )
442
SR-TSG SR-CFG RU-CFG
Table 1: Example three-level backoff.
is a base distribution over infinite space of symbol-
refined elementary trees rooted with xk, which pro-
vides the backoff probability of e. The remaining
parameters dxk and ?xk control the strength of the
base distribution.
The backoff probability P sr-tsg (e |xk ) is given by
the product of symbol-refined CFG (SR-CFG) rules
that e contains as follows.
P sr-tsg (e |xk ) =
?
f?F (e)
scf ?
?
i?I(e)
(1? sci)
? H (cfg-rules (e |xk ))
? |xk ? Hxk
Hxk ? PYP
(
dx, ?x, P
sr-cfg (? |xk )
)
,
where F (e) is a set of frontier nonterminal nodes
and I (e) is a set of internal nodes in e. cf and ci
are nonterminal symbols of nodes f and i, respec-
tively. sc is the probability of stopping the expan-
sion of a node labeled with c. SR-CFG rules are
CFG rules where every symbol is refined, as shown
in Table 1. The function cfg-rules (e |xk ) returns
the SR-CFG rules that e contains, which take the
form of xk ? ?. Each SR-CFG rule ? rooted
with xk is drawn from the backoff distribution Hxk ,
and Hxk is produced by the PYP with parameters:(
dx, ?x, P sr-cfg
)
. This distribution over the SR-CFG
rules forms the second level hierarchy of our model.
The backoff probability of the SR-CFG rule,
P sr-cfg (? |xk ), is given by the root-unrefined CFG
(RU-CFG) rule as follows,
P sr-cfg (? |xk ) = I (root-unrefine (? |xk ))
? |x ? Ix
Ix ? PYP
(
d?x, ?
?
x, P
ru-cfg (? |x )
)
,
where the function root-unrefine (? |xk ) returns
the RU-CFG rule of ?, which takes the form of x?
?. The RU-CFG rule is a CFG rule where the root
symbol is unrefined and all leaf nonterminal sym-
bols are refined, as shown in Table 1. Each RU-CFG
rule ? rooted with x is drawn from the backoff distri-
bution Ix, and Ix is produced by a PYP. This distri-
bution over the RU-CFG rules forms the third level
hierarchy of our model. Finally, we set the back-
off probability of the RU-CFG rule, P ru-cfg (? |x),
so that it is uniform as follows.
P ru-cfg (? |x ) =
1
|x? ?|
.
where |x? ?| is the number of RU-CFG rules
rooted with x. Overall, our hierarchical model en-
codes backoff smoothing consistently from the SR-
TSG rules to the SR-CFG rules, and from the SR-
CFG rules to the RU-CFG rules. As shown in (Blun-
som and Cohn, 2010; Cohen et al, 2010), the pars-
ing accuracy of the TSG model is strongly affected
by its backoff model. The effects of our hierarchical
backoff model on parsing performance are evaluated
in Section 5.
4 Inference
We use Markov Chain Monte Carlo (MCMC) sam-
pling to infer the SR-TSG derivations from parse
trees. MCMC sampling is a widely used approach
for obtaining random samples from a probability
distribution. In our case, we wish to obtain deriva-
tion samples of an SR-TSG from the posterior dis-
tribution, p (e |t,d,?, s).
The inference of the SR-TSG derivations corre-
sponds to inferring two kinds of latent variables:
latent symbol subcategories and latent substitution
443
sites. We first infer latent symbol subcategories for
every symbol in the parse trees, and then infer latent
substitution sites stepwise. During the inference of
symbol subcategories, every internal node is fixed as
a substitution site. After that, we unfix that assump-
tion and infer latent substitution sites given symbol-
refined parse trees. This stepwise learning is simple
and efficient in practice, but we believe that the joint
learning of both latent variables is possible, and we
will deal with this in future work. Here we describe
each inference algorithm in detail.
4.1 Inference of Symbol Subcategories
For the inference of latent symbol subcategories, we
adopt split and merge training (Petrov et al, 2006)
as follows. In each split-merge step, each symbol
is split into at most two subcategories. For exam-
ple, every NP symbol in the training data is split into
either NP0 or NP1 to maximize the posterior prob-
ability. After convergence, we measure the loss of
each split symbol in terms of the likelihood incurred
when removing it, then the smallest 50% of the
newly split symbols as regards that loss are merged
to avoid overfitting. The split-merge algorithm ter-
minates when the total number of steps reaches the
user-specified value.
In each splitting step, we use two types of blocked
MCMC algorithm: the sentence-level blocked
Metroporil-Hastings (MH) sampler and the tree-
level blocked Gibbs sampler, while (Petrov et al,
2006) use a different MLE-based model and the EM
algorithm. Our sampler iterates sentence-level sam-
pling and tree-level sampling alternately.
The sentence-level MH sampler is a recently pro-
posed algorithm for grammar induction (Johnson et
al., 2007b; Cohn et al, 2010). In this work, we apply
it to the training of symbol splitting. The MH sam-
pler consists of the following three steps: for each
sentence, 1) calculate the inside probability (Lari
and Young, 1991) in a bottom-up manner, 2) sample
a derivation tree in a top-down manner, and 3) ac-
cept or reject the derivation sample by using the MH
test. See (Cohn et al, 2010) for details. This sampler
simultaneously updates blocks of latent variables as-
sociated with a sentence, thus it can find MAP solu-
tions efficiently.
The tree-level blocked Gibbs sampler focuses on
the type of SR-TSG rules and simultaneously up-
dates all root and child nodes that are annotated
with the same SR-TSG rule. For example, the
sampler collects all nodes that are annotated with
S0 ? NP1VP2, then updates those nodes to an-
other subcategory such as S0 ? NP2VP0 according
to the posterior distribution. This sampler is simi-
lar to table label resampling (Johnson and Goldwa-
ter, 2009), but differs in that our sampler can update
multiple table labels simultaneously when multiple
tables are labeled with the same elementary tree.
The tree-level sampler also simultaneously updates
blocks of latent variables associated with the type of
SR-TSG rules, thus it can find MAP solutions effi-
ciently.
4.2 Inference of Substitution Sites
After the inference of symbol subcategories, we
use Gibbs sampling to infer the substitution sites of
parse trees as described in (Cohn and Lapata, 2009;
Post and Gildea, 2009). We assign a binary variable
to each internal node in the training data, which in-
dicates whether that node is a substitution site or not.
For each iteration, the Gibbs sampler works by sam-
pling the value of each binary variable in random
order. See (Cohn et al, 2010) for details.
During the inference, our sampler ignores
the symbol subcategories of internal nodes of
elementary trees since they do not affect the
derivation of the SR-TSG. For example, the
elementary trees ?(S0 (NP0 NNP0) VP0)? and
?(S0 (NP1 NNP0) VP0)? are regarded as being the
same when we calculate the generation probabilities
according to our model. This heuristics is help-
ful for finding large tree fragments and learning
compact grammars.
4.3 Hyperparameter Estimation
We treat hyperparameters {d,?} as random vari-
ables and update their values for every MCMC it-
eration. We place a prior on the hyperparameters as
follows: d ? Beta (1, 1), ? ? Gamma (1, 1). The
values of d and ? are optimized with the auxiliary
variable technique (Teh, 2006a).
444
5 Experiment
5.1 Settings
5.1.1 Data Preparation
We ran experiments on the Wall Street Journal
(WSJ) portion of the English Penn Treebank data
set (Marcus et al, 1993), using a standard data
split (sections 2?21 for training, 22 for development
and 23 for testing). We also used section 2 as a
small training set for evaluating the performance of
our model under low-resource conditions. Hence-
forth, we distinguish the small training set (section
2) from the full training set (sections 2-21). The tree-
bank data is right-binarized (Matsuzaki et al, 2005)
to construct grammars with only unary and binary
productions. We replace lexical words with count
? 5 in the training data with one of 50 unknown
words using lexical features, following (Petrov et al,
2006). We also split off all the function tags and
eliminated empty nodes from the data set, follow-
ing (Johnson, 1998).
5.1.2 Training and Parsing
For the inference of symbol subcategories, we
trained our model with the MCMC sampler by us-
ing 6 split-merge steps for the full training set and 3
split-merge steps for the small training set. There-
fore, each symbol can be subdivided into a maxi-
mum of 26 = 64 and 23 = 8 subcategories, respec-
tively. In each split-merge step, we initialized the
sampler by randomly splitting every symbol in two
subcategories and ran the MCMC sampler for 1000
iterations. After that, to infer the substitution sites,
we initialized the model with the final sample from
a run on the small training set, and used the Gibbs
sampler for 2000 iterations. We estimated the opti-
mal values of the stopping probabilities s by using
the development set.
We obtained the parsing results with the MAX-
RULE-PRODUCT algorithm (Petrov et al, 2006) by
using the SR-TSG rules extracted from our model.
We evaluated the accuracy of our parser by brack-
eting F1 score of predicted parse trees. We used
EVALB1 to compute the F1 score. In all our exper-
iments, we conducted ten independent runs to train
our model, and selected the one that performed best
on the development set in terms of parsing accuracy.
1http://nlp.cs.nyu.edu/evalb/
Model F1 (small) F1 (full)
CFG 61.9 63.6
*TSG 77.1 85.0
SR-TSG (P sr-tsg) 73.0 86.4
SR-TSG (P sr-tsg, P sr-cfg) 79.4 89.7
SR-TSG (P sr-tsg, P sr-cfg, P ru-cfg) 81.7 91.1
Table 2: Comparison of parsing accuracy with the
small and full training sets. *Our reimplementation
of (Cohn et al, 2010).
Figure 2: Histogram of SR-TSG and TSG rule sizes
on the small training set. The size is defined as the
number of CFG rules that the elementary tree con-
tains.
5.2 Results and Discussion
5.2.1 Comparison of SR-TSG with TSG
We compared the SR-TSG model with the CFG
and TSG models as regards parsing accuracy. We
also tested our model with three backoff hierarchy
settings to evaluate the effects of backoff smoothing
on parsing accuracy. Table 2 shows the F1 scores
of the CFG, TSG and SR-TSG parsers for small and
full training sets. In Table 2, SR-TSG (P sr-tsg) de-
notes that we used only the topmost level of the hi-
erarchy. Similary, SR-TSG (P sr-tsg, P sr-cfg) denotes
that we used only the P sr-tsg and P sr-cfg backoff mod-
els.
Our best model, SR-TSG (P sr-tsg, P sr-cfg, P ru-cfg),
outperformed both the CFG and TSG models on
both the small and large training sets. This result
suggests that the conventional TSG model trained
from the vanilla treebank is insufficient to resolve
445
Model F1 (? 40) F1 (all)
TSG (no symbol refinement)
Post and Gildea (2009) 82.6 -
Cohn et al (2010) 85.4 84.7
TSG with Symbol Refinement
Zuidema (2007) - *83.8
Bansal et al (2010) 88.7 88.1
SR-TSG (single) 91.6 91.1
SR-TSG (multiple) 92.9 92.4
CFG with Symbol Refinement
Collins (1999) 88.6 88.2
Petrov and Klein (2007) 90.6 90.1
Petrov (2010) - 91.8
Discriminative
Carreras et al (2008) - 91.1
Charniak and Johnson (2005) 92.0 91.4
Huang (2008) 92.3 91.7
Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the
development set (? 100).
structural ambiguities caused by coarse symbol an-
notations in a training corpus. As we expected, sym-
bol refinement can be helpful with the TSG model
for further fitting the training set and improving the
parsing accuracy.
The performance of the SR-TSG parser was
strongly affected by its backoff models. For exam-
ple, the simplest model, P sr-tsg, performed poorly
compared with our best model. This result suggests
that the SR-TSG rules extracted from the training
set are very sparse and cannot cover the space of
unknown syntax patterns in the testing set. There-
fore, sophisticated backoff modeling is essential for
the SR-TSG parser. Our hierarchical PYP model-
ing technique is a successful way to achieve back-
off smoothing from sparse SR-TSG rules to simpler
CFG rules, and offers the advantage of automatically
estimating the optimal backoff probabilities from the
training set.
We compared the rule sizes and frequencies of
SR-TSG with those of TSG. The rule sizes of SR-
TSG and TSG are defined as the number of CFG
rules that the elementary tree contains. Figure 2
shows a histogram of the SR-TSG and TSG rule
sizes (by unrefined token) on the small training set.
For example, SR-TSG rules: S1 ? NP0VP1 and
S0 ? NP1VP2 were considered to be the same to-
ken. In Figure 2, we can see that there are almost
the same number of SR-TSG rules and TSG rules
with size = 1. However, there are more SR-TSG
rules than TSG rules with size ? 2. This shows
that an SR-TSG can use various large tree fragments
depending on the context, which is specified by the
symbol subcategories.
5.2.2 Comparison of SR-TSG with Other
Models
We compared the accuracy of the SR-TSG parser
with that of conventional high-performance parsers.
Table 3 shows the F1 scores of an SR-TSG and con-
ventional parsers with the full training set. In Ta-
ble 3, SR-TSG (single) is a standard SR-TSG parser,
446
and SR-TSG (multiple) is a combination of sixteen
independently trained SR-TSG models, following
the work of (Petrov, 2010).
Our SR-TSG (single) parser achieved an F1 score
of 91.1%, which is a 6.4 point improvement over
the conventional Bayesian TSG parser reported by
(Cohn et al, 2010). Our model can be viewed as
an extension of Cohn?s work by the incorporation
of symbol refinement. Therefore, this result con-
firms that a TSG and symbol refinement work com-
plementarily in improving parsing accuracy. Com-
pared with a symbol-refined CFG model such as the
Berkeley parser (Petrov et al, 2006), the SR-TSG
model can use large tree fragments, which strength-
ens the probability of frequent syntax patterns in
the training set. Indeed, the few very large rules of
our model memorized full parse trees of sentences,
which were repeated in the training set.
The SR-TSG (single) is a pure generative model
of syntax trees but it achieved results comparable to
those of discriminative parsers. It should be noted
that discriminative reranking parsers such as (Char-
niak and Johnson, 2005) and (Huang, 2008) are con-
structed on a generative parser. The reranking parser
takes the k-best lists of candidate trees or a packed
forest produced by a baseline parser (usually a gen-
erative model), and then reranks the candidates us-
ing arbitrary features. Hence, we can expect that
combining our SR-TSG model with a discriminative
reranking parser would provide better performance
than SR-TSG alone.
Recently, (Petrov, 2010) has reported that com-
bining multiple grammars trained independently
gives significantly improved performance over a sin-
gle grammar alone. We applied his method (referred
to as a TREE-LEVEL inference) to the SR-TSG
model as follows. We first trained sixteen SR-TSG
models independently and produced a 100-best list
of the derivations for each model. Then, we erased
the subcategory information of parse trees and se-
lected the best tree that achieved the highest likeli-
hood under the product of sixteen models. The com-
bination model, SR-TSG (multiple), achieved an F1
score of 92.4%, which is a state-of-the-art result for
the WSJ parsing task. Compared with discriminative
reranking parsers, combining multiple grammars by
using the product model provides the advantage that
it does not require any additional training. Several
studies (Fossum and Knight, 2009; Zhang et al,
2009) have proposed different approaches that in-
volve combining k-best lists of candidate trees. We
will deal with those methods in future work.
Let us note the relation between SR-CFG, TSG
and SR-TSG. TSG is weakly equivalent to CFG and
generates the same set of strings. For example, the
TSG rule ?S ? (NP NNP) VP? with probability p
can be converted to the equivalent CFG rules as fol-
lows: ?S ? NPNNP VP ? with probability p and
?NPNNP ? NNP? with probability 1. From this
viewpoint, TSG utilizes surrounding symbols (NNP
of NPNNP in the above example) as latent variables
with which to capture context information. The
search space of learning a TSG given a parse tree
is O (2n) where n is the number of internal nodes
of the parse tree. On the other hand, an SR-CFG
utilizes an arbitrary index such as 0, 1, . . . as latent
variables and the search space is larger than that of a
TSG when the symbol refinement model allows for
more than two subcategories for each symbol. Our
experimental results comfirm that jointly modeling
both latent variables using our SR-TSG assists accu-
rate parsing.
6 Conclusion
We have presented an SR-TSG, which is an exten-
sion of the conventional TSG model where each
symbol of tree fragments can be automatically sub-
categorized to address the problem of the condi-
tional independence assumptions of a TSG. We pro-
posed a novel backoff modeling of an SR-TSG
based on the hierarchical Pitman-Yor Process and
sentence-level and tree-level blocked MCMC sam-
pling for training our model. Our best model sig-
nificantly outperformed the conventional TSG and
achieved state-of-the-art result in a WSJ parsing
task. Future work will involve examining the SR-
TSG model for different languages and for unsuper-
vised grammar induction.
Acknowledgements
We would like to thank Liang Huang for helpful
comments and the three anonymous reviewers for
thoughtful suggestions. We would also like to thank
Slav Petrov and Hui Zhang for answering our ques-
tions about their parsers.
447
References
Mohit Bansal and Dan Klein. 2010. Simple, Accurate
Parsing with an All-Fragments Grammar. In In Proc.
of ACL, pages 1098?1107.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proc. of EMNLP, pages 1204?1213.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proc. of ACL, 1:173?180.
Shay B Cohen, David M Blei, and Noah A Smith. 2010.
Variational Inference for Adaptor Grammars. In In
Proc. of HLT-NAACL, pages 564?572.
Trevor Cohn and Mirella Lapata. 2009. Sentence Com-
pression as Tree Transduction. Journal of Artificial
Intelligence Research, 34:637?674.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing Tree-Substitution Grammars. Journal
of Machine Learning Research, 11:3053?3096.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational Lin-
guistics, 29:589?637.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
Tree Adjoining Machine Translation. In Proc. of
EMNLP, page 727.
Thomas S Ferguson. 1973. A Bayesian Analysis of
Some Nonparametric Problems. Annals of Statistics,
1:209?230.
Victoria Fossum and Kevin Knight. 2009. Combining
Constituent Parsers. In Proc. of HLT-NAACL, pages
253?256.
Michel Galley, Mark Hopkins, Kevin Knight, Daniel
Marcu, Los Angeles, and Marina Del Rey. 2004.
What?s in a Translation Rule? Information Sciences,
pages 273?280.
Liang Huang. 2008. Forest Reranking : Discriminative
Parsing with Non-Local Features. In Proc. of ACL,
19104:0.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In In Proc. of HLT-NAACL, pages 317?325.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007a. Adaptor Grammars : A Frame-
work for Specifying Compositional Nonparametric
Bayesian Models. Advances in Neural Information
Processing Systems 19, 19:641?648.
Mark Johnson, Thomas L Griffiths, and Sharon Goldwa-
ter. 2007b. Bayesian Inference for PCFGs via Markov
chain Monte Carlo. In In Proc. of HLT-NAACL, pages
139?146.
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D Manning. 2003. Accurate
Unlexicalized Parsing. In Proc. of ACL, 1:423?430.
K Lari and S J Young. 1991. Applications of Stochas-
tic Context-Free Grammars Using the Inside?Outside
Algorithm. Computer Speech and Language, 5:237?
257.
Mitchell P Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. of ACL, pages 75?82.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of ACL, pages
433?440.
Slav Petrov. 2010. Products of Random Latent Variable
Grammars. In Proc. of HLT-NAACL, pages 19?27.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25:855?900.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In In Proc. of ACL-
IJCNLP, pages 45?48.
Yee Whye Teh. 2006a. A Bayesian Interpretation of
Interpolated Kneser-Ney. NUS School of Computing
Technical Report TRA2/06.
YW Teh. 2006b. A Hierarchical Bayesian Language
Model based on Pitman-Yor Processes. In Proc. of
ACL, 44:985?992.
J Tenenbaum, TJ O?Donnell, and ND Goodman. 2009.
Fragment Grammars: Exploring Computation and
Reuse in Language. MIT Computer Science and Arti-
ficial Intelligence Laboratory Technical Report Series.
Mengqiu Wang, Noah A Smith, and Teruko Mitamura.
2007. What is the Jeopardy Model ? A Quasi-
Synchronous Grammar for QA. In Proc. of EMNLP-
CoNLL, pages 22?32.
Elif Yamangil and Stuart M Shieber. 2010. Bayesian
Synchronous Tree-Substitution Grammar Induction
and Its Application to Sentence Compression. In In
Proc. of ACL, pages 937?947.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li.
2009. K-Best Combination of Syntactic Parsers. In
Proc. of EMNLP, pages 1552?1560.
Willem Zuidema. 2007. Parsimonious Data-Oriented
Parsing. In Proc. of EMNLP-CoNLL, pages 551?560.
448
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 100?104,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparative Study of Target Dependency Structures
for Statistical Machine Translation
Xianchao Wu?, Katsuhito Sudoh, Kevin Duh?, Hajime Tsukada, Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai Seika-cho, Soraku-gun Kyoto 619-0237 Japan
wuxianchao@gmail.com,sudoh.katsuhito@lab.ntt.co.jp,
kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp
Abstract
This paper presents a comparative study of
target dependency structures yielded by sev-
eral state-of-the-art linguistic parsers. Our ap-
proach is to measure the impact of these non-
isomorphic dependency structures to be used
for string-to-dependency translation. Besides
using traditional dependency parsers, we also
use the dependency structures transformed
from PCFG trees and predicate-argument
structures (PASs) which are generated by an
HPSG parser and a CCG parser. The experi-
ments on Chinese-to-English translation show
that the HPSG parser?s PASs achieved the best
dependency and translation accuracies.
1 Introduction
Target language side dependency structures have
been successfully used in statistical machine trans-
lation (SMT) by Shen et al (2008) and achieved
state-of-the-art results as reported in the NIST 2008
Open MT Evaluation workshop and the NTCIR-9
Chinese-to-English patent translation task (Goto et
al., 2011; Ma and Matsoukas, 2011). A primary ad-
vantage of dependency representations is that they
have a natural mechanism for representing discon-
tinuous constructions, which arise due to long-
distance dependencies or in languages where gram-
matical relations are often signaled by morphology
instead of word order (McDonald and Nivre, 2011).
It is known that dependency-style structures can
be transformed from a number of linguistic struc-
?Now at Baidu Inc.
?Now at Nara Institute of Science & Technology (NAIST)
tures. For example, using the constituent-to-
dependency conversion approach proposed by Jo-
hansson and Nugues (2007), we can easily yield de-
pendency trees from PCFG style trees. A seman-
tic dependency representation of a whole sentence,
predicate-argument structures (PASs), are also in-
cluded in the output trees of (1) a state-of-the-art
head-driven phrase structure grammar (HPSG) (Pol-
lard and Sag, 1994; Sag et al, 2003) parser, Enju1
(Miyao and Tsujii, 2008) and (2) a state-of-the-art
CCG parser2 (Clark and Curran, 2007). The moti-
vation of this paper is to investigate the impact of
these non-isomorphic dependency structures to be
used for SMT. That is, we would like to provide a
comparative evaluation of these dependencies in a
string-to-dependency decoder (Shen et al, 2008).
2 Gaining Dependency Structures
2.1 Dependency tree
We follow the definition of dependency graph and
dependency tree as given in (McDonald and Nivre,
2011). A dependency graph G for sentence s is
called a dependency tree when it satisfies, (1) the
nodes cover all the words in s besides the ROOT;
(2) one node can have one and only one head (word)
with a determined syntactic role; and (3) the ROOT
of the graph is reachable from all other nodes.
For extracting string-to-dependency transfer
rules, we use well-formed dependency structures,
either fixed or floating, as defined in (Shen et al,
2008). Similarly, we ignore the syntactic roles
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
2http://groups.inf.ed.ac.uk/ccg/software.html
100
 when the fluid pressure cylinder 31 is used , fluid is gradually applied . 
t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 
c2 c5 c7 c9 c11 c12 c14 c15 c17 c20 c22 c24 c25 
c3 
c4 
c6 
c8 
c10 c13 
c18 
c19 
c21 
c23 
c16 
c1 
c0 
conj_ 
arg12 
det_ 
arg1 
adj_ 
arg1 
noun_ 
arg1 
noun_ 
arg0 
adj_ 
arg1 
aux_ 
arg12 
verb_ 
arg12 
punct_ 
arg1 
noun_ 
arg0 
aux_ 
arg12 
adj_ 
arg1 
verb_ 
arg12 
* + 
* + 
* + 
* 
+ 
* + 
* + 
* + 
*  
* + 
* + 
* + 
* + 
* + 
+ 
Figure 1: HPSG tree of an example sentence. ?*?/
?+?=syntactic/semantic heads. Arrows in red (upper)=
PASs, orange (bottom)=word-level dependencies gener-
ated from PASs, blue=newly appended dependencies.
both during rule extracting and target dependency
language model (LM) training.
2.2 Dependency parsing
Graph-based and transition-based are two predom-
inant paradigms for data-driven dependency pars-
ing. The MST parser (McDonald et al, 2005) and
the Malt parser (Nivre, 2003) stand for two typical
parsers, respectively. Parsing accuracy comparison
and error analysis under the CoNLL-X dependency
shared task data (Buchholz and Marsi, 2006) have
been performed by McDonald and Nivre (2011).
Here, we compare them on the SMT tasks through
parsing the real-world SMT data.
2.3 PCFG parsing
For PCFG parsing, we select the Berkeley parser
(Petrov and Klein, 2007). In order to generate word-
level dependency trees from the PCFG tree, we use
the LTH constituent-to-dependency conversion tool3
written by Johansson and Nugues (2007). The head
finding rules4 are according to Magerman (1995)
and Collins (1997). Similar approach has been orig-
inally used by Shen et al (2008).
2.4 HPSG parsing
In the Enju English HPSG grammar (Miyao et al,
2003) used in this paper, the semantic content of
3http://nlp.cs.lth.se/software/treebank converter/
4http://www.cs.columbia.edu/ mcollins/papers/heads
a sentence/phrase is represented by a PAS. In an
HPSG tree, each leaf node generally introduces a
predicate, which is represented by the pair made up
of the lexical entry feature and predicate type fea-
ture. The arguments of a predicate are designated by
the arrows from the argument features in a leaf node
to non-terminal nodes (e.g., t0?c3, t0?c16).
Since the PASs use the non-terminal nodes in the
HPSG tree (Figure 1), this prevents their direct us-
age in a string-to-dependency decoder. We thus need
an algorithm to transform these phrasal predicate-
argument dependencies into a word-to-word depen-
dency tree. Our algorithm (refer to Figure 1 for an
example) for changing PASs into word-based depen-
dency trees is as follows:
1. finding, i.e., find the syntactic/semantic head
word of each argument node through a bottom-
up traversal of the tree;
2. mapping, i.e., determine the arc directions
(among a predicate word and the syntac-
tic/semantic head words of the argument nodes)
for each predicate type according to Table 1.
Then, a dependency graph will be generated;
3. checking, i.e., post modifying the dependency
graph according to the definition of dependency
tree (Section 2.1).
Table 1 lists the mapping from HPSG?s PAS types
to word-level dependency arcs. Since a non-terminal
node in an HPSG tree has two kinds of heads, syn-
tactic or semantic, we will generate two dependency
graphs after mapping. We use ?PAS+syn? to repre-
sent the dependency trees generated from the HPSG
PASs guided by the syntactic heads. For semantic
heads, we use ?PAS+sem?.
For example, refer to t0 = when in Figure 1.
Its arg1 = c16 (with syntactic head t10), arg2
= c3 (with syntactic head t6), and PAS type =
conj arg12. In Table 1, this PAS type corresponds
to arg2?pred?arg1, then the result word-level de-
pendency is t6(is)?t0(when)?t10(is).
We need to post modify the dependency graph af-
ter applying the mapping, since it is not guaranteed
to be a dependency tree. Referring to the definition
of dependency tree (Section 2.1), we need the strat-
egy for (1) selecting only one head from multiple
101
PAS Type Dependency Relation
adj arg1[2] [arg2 ?] pred ? arg1
adj mod arg1[2] [arg2 ?] pred ? arg1 ? mod
aux[ mod] arg12 arg1/pred ? arg2 [? mod]
conj arg1[2[3]] [arg2[/arg3]] ? pred ? arg1
comp arg1[2] pred ? arg1 [? arg2]
comp mod arg1 arg1 ? pred ? mod
noun arg1 pred ? arg1
noun arg[1]2 arg2 ? pred [? arg1]
poss arg[1]2 pred ? arg2 [? arg1]
prep arg12[3] arg2[/arg3] ? pred ? arg1
prep mod arg12[3] arg2[/arg3] ? pred ? arg1 ? mod
quote arg[1]2 [arg1 ?] pred ? arg2
quote arg[1]23 [arg1/]arg3 ? pred ? arg2
lparen arg123 pred/arg2 ? arg3 ? arg1
relative arg1[2] [arg2 ?] pred ? arg1
verb arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]] ? pred
verb mod arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]]?pred?mod
app arg12,coord arg12 arg2/pred ? arg1
det arg1,it arg1,punct arg1 pred ? arg1
dtv arg2 pred ? arg2
lgs arg2 arg2 ? pred
Table 1: Mapping fromHPSG?s PAS types to dependency
relations. Dependent(s)? head(s), / = and, [] = optional.
heads and (2) appending dependency relations for
those words/punctuation that do not have any head.
When one word has multiple heads, we only keep
one. The selection strategy is that, if this arc was
deleted, it will cause the biggest number of words
that can not reach to the root word anymore. In case
of a tie, we greedily pack the arc that connect two
words wi and wj where |i? j| is the biggest. For all
the words and punctuation that do not have a head,
we greedily take the root word of the sentence as
their heads. In order to fully use the training data,
if there are directed cycles in the result dependency
graph, we still use the graph in our experiments,
where only partial dependency arcs, i.e., those target
flat/hierarchical phrases attached with well-formed
dependency structures, can be used during transla-
tion rule extraction.
2.5 CCG parsing
We also use the predicate-argument dependencies
generated by the CCG parser developed by Clark
and Curran (2007). The algorithm for generating
word-level dependency tree is easier than processing
the PASs included in the HPSG trees, since the word
level predicate-argument relations have already been
included in the output of CCG parser. The mapping
from predicate types to the gold-standard grammat-
ical relations can be found in Table 13 in (Clark and
Curran, 2007). The post-processing is like that de-
scribed for HPSG parsing, except we greedily use
the MST?s sentence root when we can not determine
it based on the CCG parser?s PASs.
3 Experiments
3.1 Setup
We re-implemented the string-to-dependency de-
coder described in (Shen et al, 2008). Dependency
structures from non-isomorphic syntactic/semantic
parsers are separately used to train the transfer
rules as well as target dependency LMs. For intu-
itive comparison, an outside SMT system is Moses
(Koehn et al, 2007).
For Chinese-to-English translation, we use the
parallel data from NIST Open Machine Translation
Evaluation tasks. The training data contains 353,796
sentence pairs, 8.7M Chinese words and 10.4M En-
glish words. The NIST 2003 and 2005 test data
are respectively taken as the development and test
set. We performed GIZA++ (Och and Ney, 2003)
and the grow-diag-final-and symmetrizing strategy
(Koehn et al, 2007) to obtain word alignments. The
Berkeley Language Modeling Toolkit, berkeleylm-
1.0b35 (Pauls and Klein, 2011), was employed to
train (1) a five-gram LM on the Xinhua portion of
LDC English Gigaword corpus v3 (LDC2007T07)
and (2) a tri-gram dependency LM on the English
dependency structures of the training data. We re-
port the translation quality using the case-insensitive
BLEU-4 metric (Papineni et al, 2002).
3.2 Statistics of dependencies
We compare the similarity of the dependencies with
each other, as shown in Table 2. Basically, we in-
vestigate (1) if two dependency graphs of one sen-
tence share the same root word and (2) if the head of
one word in one sentence are identical in two depen-
dency graphs. In terms of root word comparison, we
observe that MST and CCG share 87.3% of iden-
tical root words, caused by borrowing roots from
MST to CCG. Then, it is interesting that Berkeley
and PAS+syn share 74.8% of identical root words.
Note that the Berkeley parser is trained on the Penn
treebank (Marcus et al, 1994) yet the HPSG parser
is trained on the HPSG treebank (Miyao and Tsujii,
5http://code.google.com/p/berkeleylm/
102
Dependency Precision Recall BLEU-Dev BLEU-Test # phrases # hier rules # illegal dep trees # directed cycles
Moses-1 - - 0.3349 0.3207 5.4M - - -
Moses-2 - - 0.3445 0.3262 0.7M 4.5M - -
MST 0.744 0.750 0.3520 0.3291 2.4M 2.1M 251 0
Malt 0.732 0.738 0.3423 0.3203 1.5M 1.3M 130,960 0
Berkeley 0.800 0.806 0.3475 0.3312 2.4M 2.2M 282 0
PAS+syn 0.818 0.824 0.3499 0.3376 2.2M 1.9M 10,411 5,853
PAS+sem 0.777 0.782 0.3484 0.3343 2.1M 1.6M 14,271 9,747
CCG 0.701 0.705 0.3442 0.3283 1.7M 1.3M 61,015 49,955
Table 3: Comparison of dependency and translation accuracies. Moses-1 = phrasal, Moses-2 = hierarchical.
Malt Berkeley PAS PAS CCG
+syn +sem
MST 70.5 62.5 69.2 53.3 87.3
(77.3) (64.6) (58.5) (58.1) (61.7)
Malt 66.2 73.0 46.8 62.9
(63.2) (57.7) (56.6) (58.1)
Berkeley 74.8 44.2 56.5
(64.3) (56.0) (59.2)
PAS+ 59.3 62.9
syn (79.1) (61.0)
PAS+ 60.0
sem (58.8)
Table 2: Comparison of the dependencies of the English
sentences in the training data. Without () = % of similar
root words; with () = % of similar head words.
2008). In terms of head word comparison, PAS+syn
and PAS+sem share 79.1% of identical head words.
This is basically due to that we used the similar
PASs of the HPSG trees. Interestingly, there are only
59.3% identical root words shared by PAS+syn and
PAS+sem. This reflects the significant difference be-
tween syntactic and semantic heads.
We also manually created the golden dependency
trees for the first 200 English sentences in the train-
ing data. The precision/recall (P/R) are shown in
Table 3. We observe that (1) the translation accura-
cies approximately follow the P/R scores yet are not
that sensitive to their large variances, and (2) it is
still tough for domain-adapting from the treebank-
trained parsers to parse the real-world SMT data.
PAS+syn performed the best by avoiding the errors
of missing of arguments for a predicate, wrongly
identified head words for a linguistic phrase, and in-
consistency dependencies inside relatively long co-
ordinate structures. These errors significantly influ-
ence the number of extractable translation rules and
the final translation accuracies.
Note that, these P/R scores on the first 200 sen-
tences (all from less than 20 newswire documents)
shall only be taken as an approximation of the total
training data and not necessarily exactly follow the
tendency of the final BLEU scores. For example,
CCG is worse than Malt in terms of P/R yet with a
higher BLEU score. We argue this is mainly due to
that the number of illegal dependency trees gener-
ated by Malt is the highest. Consequently, the num-
ber of flat/hierarchical rules generated by using Malt
trees is the lowest. Also, PAS+sem has a lower P/R
than Berkeley, yet their final BLEU scores are not
statistically different.
3.3 Results
Table 3 also shows the BLEU scores, the number of
flat phrases and hierarchical rules (both integrated
with target dependency structures), and the num-
ber of illegal dependency trees generated by each
parser. From the table, we have the following ob-
servations: (1) all the dependency structures (except
Malt) achieved a significant better BLEU score than
the phrasal Moses; (2) PAS+syn performed the best
in the test set (0.3376), and it is significantly better
than phrasal/hierarchical Moses (p < 0.01), MST
(p < 0.05), Malt (p < 0.01), Berkeley (p < 0.05),
and CCG (p < 0.05); and (3) CCG performed as
well as MST and Berkeley. These results lead us to
argue that the robustness of deep syntactic parsers
can be advantageous in SMT compared with tradi-
tional dependency parsers.
4 Conclusion
We have constructed a string-to-dependency trans-
lation platform for comparing non-isomorphic tar-
get dependency structures. Specially, we proposed
an algorithm for generating word-based dependency
trees from PASs which are generated by a state-of-
the-art HPSG parser. We found that dependency
trees transformed from these HPSG PASs achieved
the best dependency/translation accuracies.
103
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments and suggestions.
References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-
linear models. Computational Linguistics, 33(4):493?
552.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain, July.
Association for Computational Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of NTCIR-9, pages 559?578.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
In Proceedings of NODALIDA, Tartu, Estonia, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Jeff Ma and Spyros Matsoukas. 2011. Bbn?s systems
for the chinese-english sub-task of the ntcir-9 patentmt
evaluation. In Proceedings of NTCIR-9, pages 579?
584.
David Magerman. 1995. Statistical decision-tree models
for parsing. In In Proceedings of of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 276?283.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the Workshop on HLT, pages 114?119,
Plainsboro.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37(1):197?230.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 91?98, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Lingustics, 34(1):35?80.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Probabilistic modeling of argument struc-
tures including non-local dependencies. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 285?
291, Borovets.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT,
pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
Number 152 in CSLI Lecture Notes. CSLI Publica-
tions.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08:HLT, pages 577?585, Colum-
bus, Ohio.
104
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 18?23,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Supervised Model Learning with Feature Grouping
based on a Discrete Constraint
Jun Suzuki and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{suzuki.jun, nagata.masaaki}@lab.ntt.co.jp
Abstract
This paper proposes a framework of super-
vised model learning that realizes feature
grouping to obtain lower complexity mod-
els. The main idea of our method is to
integrate a discrete constraint into model
learning with the help of the dual decom-
position technique. Experiments on two
well-studied NLP tasks, dependency pars-
ing and NER, demonstrate that our method
can provide state-of-the-art performance
even if the degrees of freedom in trained
models are surprisingly small, i.e., 8 or
even 2. This significant benefit enables us
to provide compact model representation,
which is especially useful in actual use.
1 Introduction
This paper focuses on the topic of supervised
model learning, which is typically represented as
the following form of the optimization problem:
w? = arg min
w
{
O(w;D)
}
,
O(w;D) = L(w;D) + ?(w),
(1)
where D is supervised training data that consists
of the corresponding input x and output y pairs,
that is, (x,y) ? D. w is an N -dimensional vector
representation of a set of optimization variables,
which are also interpreted as feature weights.
L(w;D) and ?(w) represent a loss function and
a regularization term, respectively. Nowadays, we,
in most cases, utilize a supervised learning method
expressed as the above optimization problem to
estimate the feature weights of many natural lan-
guage processing (NLP) tasks, such as text clas-
sification, POS-tagging, named entity recognition,
dependency parsing, and semantic role labeling.
In the last decade, the L1-regularization tech-
nique, which incorporates L1-norm into ?(w),
has become popular and widely-used in many
NLP tasks (Gao et al, 2007; Tsuruoka et al,
2009). The reason is that L1-regularizers encour-
age feature weights to be zero as much as pos-
sible in model learning, which makes the resul-
tant model a sparse solution (many zero-weights
exist). We can discard all features whose weight
is zero from the trained model1 without any loss.
Therefore, L1-regularizers have the ability to eas-
ily and automatically yield compact models with-
out strong concern over feature selection.
Compact models generally have significant and
clear advantages in practice: instances are faster
loading speed to memory, less memory occupa-
tion, and even faster decoding is possible if the
model is small enough to be stored in cache mem-
ory. Given this background, our aim is to establish
a model learning framework that can reduce the
model complexity beyond that possible by sim-
ply applying L1-regularizers. To achieve our goal,
we focus on the recently developed concept of au-
tomatic feature grouping (Tibshirani et al, 2005;
Bondell and Reich, 2008). We introduce a model
learning framework that achieves feature group-
ing by incorporating a discrete constraint during
model learning.
2 Feature Grouping Concept
Going beyond L1-regularized sparse modeling,
the idea of ?automatic feature grouping? has re-
cently been developed. Examples are fused
lasso (Tibshirani et al, 2005), grouping pur-
suit (Shen and Huang, 2010), and OSCAR (Bon-
dell and Reich, 2008). The concept of automatic
feature grouping is to find accurate models that
have fewer degrees of freedom. This is equiva-
lent to enforce every optimization variables to be
equal as much as possible. A simple example is
that w?1 = (0.1, 0.5, 0.1, 0.5, 0.1) is preferred over
w?2 = (0.1, 0.3, 0.2, 0.5, 0.3) since w?1 and w?2
have two and four unique values, respectively.
There are several merits to reducing the degree
1This paper refers to model after completion of (super-
vised) model learning as ?trained model?
18
of freedom. For example, previous studies clari-
fied that it can reduce the chance of over-fitting to
the training data (Shen and Huang, 2010). This is
an important property for many NLP tasks since
they are often modeled with a high-dimensional
feature space, and thus, the over-fitting problem is
readily triggered. It has also been reported that it
can improve the stability of selecting non-zero fea-
tures beyond that possible with the standard L1-
regularizer given the existence of many highly cor-
related features (Jo?rnsten and Yu, 2003; Zou and
Hastie, 2005). Moreover, it can dramatically re-
duce model complexity. This is because we can
merge all features whose feature weight values are
equivalent in the trained model into a single fea-
ture cluster without any loss.
3 Modeling with Feature Grouping
This section describes our proposal for obtaining
a feature grouping solution.
3.1 Integration of a Discrete Constraint
Let S be a finite set of discrete values, i.e., a set
integer from ?4 to 4, that is, S={?4,. . . , ?1, 0,
1, . . . , 4}. The detailed discussion how we define
S can be found in our experiments section since
it deeply depends on training data. Then, we de-
fine the objective that can simultaneously achieve
a feature grouping and model learning as follows:
O(w;D) =L(w;D) + ?(w)
s.t. w ? SN . (2)
where SN is the cartesian power of a set S . The
only difference with Eq. 1 is the additional dis-
crete constraint, namely, w ? SN . This con-
straint means that each variable (feature weight)
in trained models must take a value in S, that is,
w?n ? S , where w?n is the n-th factor of w?, and
n ? {1, . . . , N}. As a result, feature weights in
trained models are automatically grouped in terms
of the basis of model learning. This is the basic
idea of feature grouping proposed in this paper.
However, a concern is how we can efficiently
optimize Eq. 2 since it involves a NP-hard combi-
natorial optimization problem. The time complex-
ity of the direct optimization is exponential against
N . Next section introduces a feasible algorithm.
3.2 Dual Decomposition Formulation
Hereafter, we strictly assume that L(w;D) and
?(w) are both convex in w. Then, the proper-
ties of our method are unaffected by the selection
of L(w;D) and ?(w). Thus, we ignore their spe-
cific definition in this section. Typical cases can
be found in the experiments section. Then, we re-
formulate Eq. 2 by using the dual decomposition
technique (Everett, 1963):
O(w,u;D) =L(w;D) + ?(w) + ?(u)
s.t. w = u, and u ? SN . (3)
Difference from Eq. 2, Eq. 3 has an additional term
?(u), which is similar to the regularizer ?(w),
whose optimization variables w and u are tight-
ened with equality constraint w = u. Here, this
paper only considers the case ?(u) = ?22 ||u||22 +
?1||u||1, and ?2 ? 0 and ?1 ? 02. This objec-
tive can also be viewed as the decomposition of
the standard loss minimization problem shown in
Eq. 1 and the additional discrete constraint regu-
larizer by the dual decomposition technique.
To solve the optimization in Eq. 3, we lever-
age the alternating direction method of multiplier
(ADMM) (Gabay and Mercier, 1976; Boyd et al,
2011). ADMM provides a very efficient optimiza-
tion framework for the problem in the dual decom-
position form. Here, ? represents dual variables
for the equivalence constraint w=u. ADMM in-
troduces the augmented Lagrangian term ?2 ||w ?
u||22 with ?>0 which ensures strict convexity and
increases robustness3.
Finally, the optimization problem in Eq. 3 can
be converted into a series of iterative optimiza-
tion problems. Detailed derivation in the general
case can be found in (Boyd et al, 2011). Fig. 1
shows the entire model learning framework of our
proposed method. The remarkable point is that
ADMM works by iteratively computing one of the
three optimization variable sets w, u, and ? while
holding the other variables fixed in the iterations
t = 1, 2, . . . until convergence.
Step1 (w-update): This part of the optimiza-
tion problem shown in Eq. 4 is essentially Eq. 1
with a ?biased? L2-regularizer. ?bias? means here
that the direction of regularization is toward point
a instead of the origin. Note that it becomes a
standard L2-regularizer if a = 0. We can select
any learning algorithm that can handle the L2-
regularizer for this part of the optimization.
Step2 (u-update): This part of the optimization
problem shown in Eq. 5 can be rewritten in the
2Note that this setting includes the use of only L1-, L2-,
or without regularizers (L1 only: ?1>0 and ?2=0, L2 only:
?1=0 and ?2>0, and without regularizer: ?1=0, ?2=0).
3Standard dual decomposition can be viewed as ?=0
19
Input: Training data:D, parameters:?, ?, primal, and dual
Initialize: w(1) = 0, u(1) = 0, ?(1) = 0, and t = 1.
Step1 w-update:
Solve w(t+1) = argminw{O(w;D,u(t),?(t))}.For our case,
O(w;D,u,?) =O(w;D) + ?2 ||w ? a||
2
2, (4)
where a = u??.
Step2 u-update:
Solve u(t+1) = argminu{O(u;D,w(t+1),?(t))}.For our case,
O(u;D,w,?) = ?22 ||u||
2
2 + ?1||u||1 +
?
2 ||b? u||
2
2
s.t. u ? SN ,
(5)where b = w +?
Step3 ?-update:
?(t+1) =?(t) + ?(w(t+1) ? u(t+1)) (6)
Step4 convergence check:
||w(t+1) ? u(t+1)||22/N < primal
||u(t+1) ? u(t)||22/N < dual
(7)
Break the loop if the above two conditions are reached,
or go back to Step1 with t = t+ 1.
Output: u(t+1)
Figure 1: Entire learning framework of our
method derived from ADMM (Boyd et al, 2011).
following equivalent simple form:
u?= arg minu{12 ||u? b?||22 + ??1||u||1}s.t. u ? SN , (8)
where b? = ??2+?b, and ??1 = ?1?2+? . Thisoptimization is still a combinatorial optimization
problem. However unlike Eq. 2, this optimization
can be efficiently solved.
Fig. 2 shows the procedure to obtain the exact
solution of Eq. 5, namely u(t+1). The remarkable
point is that the costly combinatorial optimization
problem is disappeared, and instead, we are only
required to perform two feature-wise calculations
whose total time complexities isO(N log |S|) and
fully parallelizable. The similar technique has
been introduced in Zhong and Kwok (2011) for
discarding a costly combinatorial problem from
the optimization with OSCAR-regularizers with
the help of proximal gradient methods, i.e., (Beck
and Teboulle, 2009).
We omit to show the detailed derivation of
Fig. 2 because of the space reason. However, this
is easily understandable. The key properties are
the following two folds; (i) The objective shown
in Eq. 8 is a convex and also symmetric function
with respect to u??, where u?? is the optimal solution
of Eq. 8 without the discrete constraint. Therefore,
the optimal solution u? is at the point where the
Input: b? = (b?n)Nn=1, ??1, and S.
1, Find the optimal solution of Eq. 8 without the constraint.
The optimization of mixed L2 and L1-norms is known
to have a closed form solution, i.e., (Beck and Teboulle,
2009), that is;
u??n = sgn(b?n)max(0, |b?n| ? ??1),
where (u??n)Nn=1 = u??.
2, Find the nearest valid point in SN from u?? in terms of the
L2-distance;
u?n = argmin
u?S
(u??n ? u)2
where (u?n)Nn=1 = u?. This can be performed by a binary
search, whose time complexity is generally O(log |S|).
Output: u?
Figure 2: Procedure for solving Step2
nearest valid point given SN from u?? in terms of
the L2-distance. (ii) The valid points given SN are
always located at the vertexes of axis-aligned or-
thotopes (hyperrectangles) in the parameter space
of feature weights. Thus, the solution u?, which is
the nearest valid point from u??, can be obtained by
individually taking the nearest value in S from u??n
for all n.
Step3 (?-update): We perform gradient ascent
on dual variables to tighten the constraint w = u.
Note that ? is the learning rate; we can simply set
it to 1.0 for every iteration (Boyd et al, 2011).
Step4 (convergence check): It can be evaluated
both primal and dual residuals as defined in Eq. 7
with suitably small primal and dual.
3.3 Online Learning
We can select an online learning algorithm for
Step1 since the ADMM framework does not re-
quire exact minimization of Eq. 4. In this case, we
perform one-pass update through the data in each
ADMM iteration (Duh et al, 2011). Note that the
total calculation cost of our method does not in-
crease much from original online learning algo-
rithm since the calculation cost of Steps 2 through
4 is relatively much smaller than that of Step1.
4 Experiments
We conducted experiments on two well-studied
NLP tasks, namely named entity recognition
(NER) and dependency parsing (DEPAR).
Basic settings: We simply reused the settings
of most previous studies. We used CoNLL?03
data (Tjong Kim Sang and De Meulder, 2003)
for NER, and the Penn Treebank (PTB) III cor-
pus (Marcus et al, 1994) converted to depen-
dency trees for DEPAR (McDonald et al, 2005).
20
Our decoding models are the Viterbi algorithm
on CRF (Lafferty et al, 2001), and the second-
order parsing model proposed by (Carreras, 2007)
for NER and DEPAR, respectively. Features
are automatically generated according to the pre-
defined feature templates widely-used in the pre-
vious studies. We also integrated the cluster fea-
tures obtained by the method explained in (Koo et
al., 2008) as additional features for evaluating our
method in the range of the current best systems.
Evaluation measures: The purpose of our ex-
periments is to investigate the effectiveness of our
proposed method in terms of both its performance
and the complexity of the trained model. There-
fore, our evaluation measures consist of two axes.
Task performance was mainly evaluated in terms
of the complete sentence accuracy (COMP) since
the objective of all model learning methods eval-
uated in our experiments is to maximize COMP.
We also report the F?=1 score (F-sc) for NER,
and the unlabeled attachment score (UAS) for DE-
PAR for comparison with previous studies. Model
complexity is evaluated by the number of non-zero
active features (#nzF) and the degree of freedom
(#DoF) (Zhong and Kwok, 2011). #nzF is the
number of features whose corresponding feature
weight is non-zero in the trained model, and #DoF
is the number of unique non-zero feature weights.
Baseline methods: Our main baseline is L1-
regularized sparse modeling. To cover both batch
and online leaning, we selected L1-regularized
CRF (L1CRF) (Lafferty et al, 2001) optimized by
OWL-QN (Andrew and Gao, 2007) for the NER
experiment, and the L1-regularized regularized
dual averaging (L1RDA) method (Xiao, 2010)4
for DEPAR. Additionally, we also evaluated L2-
regularized CRF (L2CRF) with L-BFGS (Liu and
Nocedal, 1989) for NER, and passive-aggressive
algorithm (L2PA) (Crammer et al, 2006)5 for DE-
PAR since L2-regularizer often provides better re-
sults than L1-regularizer (Gao et al, 2007).
For a fair comparison, we applied the proce-
dure of Step2 as a simple quantization method
to trained models obtained from L1-regularized
model learning, which we refer to as (QT).
4RDA provided better results at least in our experiments
than L1-regularized FOBOS (Duchi and Singer, 2009), and
its variant (Tsuruoka et al, 2009), which are more familiar to
the NLP community.
5L2PA is also known as a loss augmented variant of one-
best MIRA, well-known in DEPAR (McDonald et al, 2005).
4.1 Configurations of Our Method
Base learning algorithm: The settings of our
method in our experiments imitate L1-regularized
learning algorithm since the purpose of our
experiments is to investigate the effectiveness
against standard L1-regularized learning algo-
rithms. Then, we have the following two possible
settings; DC-ADMM: we leveraged the baseline
L1-regularized learning algorithm to solve Step1,
and set ?1 = 0 and ?2 = 0 for Step2. DCwL1-
ADMM: we leveraged the baselineL2-regularized
learning algorithm, but without L2-regularizer, to
solve Step1, and set ?1 > 0 and ?2 = 0 for Step2.
The difference can be found in the objective func-
tion O(w,u;D) shown in Eq. 3;
(DC-ADMM) : O(w,u;D)=L(w;D)+?1||w||1
(DCwL1-ADMM) : O(w,u;D)=L(w;D)+?1||u||1
In other words, DC-ADMM utilizes L1-
regularizer as a part of base leaning algorithm
?(w)=?1||w||1, while DCwL1-ADMM discards
regularizer of base learning algorithm ?(w), but
instead introducing ?(u) = ?1||u||1. Note that
these two configurations are essentially identical
since objectives are identical, even though the
formulation and algorithm is different. We only
report results of DC-ADMM because of the space
reason since the results of DCwL1-ADMM were
nearly equivalent to those of DC-ADMM.
Definition of S: DC-ADMM can utilize any fi-
nite set for S. However, we have to carefully se-
lect it since it deeply affects the performance. Ac-
tually, this is the most considerable point of our
method. We preliminarily investigated the several
settings. Here, we introduce an example of tem-
plate which is suitable for large feature set. Let
?, ?, and ? represent non-negative real-value con-
stants, ? be a positive integer, ? = {?1, 1}, and
a function f?,?,?(x, y) = y(??x + ?). Then, we
define a finite set of values S as follows:
S?,?,?,? ={f?,?,?(x, y)|(x, y) ? S???} ? {0},
where S? is a set of non-negative integers from
zero to ? ? 1, that is, S? ={m}??1m=0. For example,
if we set ? = 0.1, ? = 0.4, ?= 4, and ? = 3, then
S?,?,?,? = {?2.0, ?0.8, ?0.5, 0, 0.5, 0.8, 2.0}.
The intuition of this template is that the distribu-
tion of the feature weights in trained model often
takes a form a similar to that of the ?power law?
in the case of the large feature sets. Therefore, us-
ing an exponential function with a scale and bias
seems to be appropriate for fitting them.
21
81.0
83.0
85.0
87.0
89.0
91.0
1.0E+00 1.0E+03 1.0E+06
DC-ADMML1CRF (w/ QT)L1CRFL2CRFCompl
ete Sen
tence A
ccurac
y quantized
# of degrees of freedom (#DoF) [log-scale] 30.0
35.0
40.0
45.0
50.0
55.0
1.0E+00 1.0E+03 1.0E+06
DC-ADMML1RAD (w/ QT)L1RDAL2PACom
plete S
entenc
e Accu
racy quantized
# of degrees of freedom (#DoF) [log-scale]
(a) NER (b) DEPAR
Figure 3: Performance vs. degree of freedom in
the trained model for the development data
Note that we can control the upper bound of
#DoF in trained model by ?, namely if ? = 4 then
the upper bound of #DoF is 8 (doubled by posi-
tive and negative sides). We fixed ? = 1, ? = 1,
?2 = 0, ? = 4 (or 2 if ? ? 5), ? = ?/2 in all ex-
periments. Thus the only tunable parameter in our
experiments is ? for each ?.
4.2 Results and Discussions
Fig. 3 shows the task performance on the develop-
ment data against the model complexities in terms
of the degrees of freedom in the trained models.
Plots are given by changing the ? value for DC-
ADMM andL1-regularized methods with QT. The
plots of the standard L1-regularized methods are
given by changing the regularization constants ?1.
Moreover, Table 1 shows the final results of our
experiments on the test data. The tunable param-
eters were fixed at values that provided the best
performance on the development data.
According to the figure and table, the most re-
markable point is that DC-ADMM successfully
maintained the task performance even if #DoF (the
degree of freedom) was 8, and the performance
drop-offs were surprisingly limited even if #DoF
was 2, which is the upper bound of feature group-
ing. Moreover, it is worth noting that the DC-
ADMM performance is sometimes improved. The
reason may be that such low degrees of freedom
prevent over-fitting to the training data. Surpris-
ingly, the simple quantization method (QT) pro-
vided fairly good results. However, we empha-
size that the models produced by the QT approach
offer no guarantee as to the optimal solution. In
contrast, DC-ADMM can truly provide the opti-
mal solution of Eq. 3 since the discrete constraint
is also considered during the model learning.
In general, a trained model consists of two parts:
Test Model complex.
NER COMP F-sc #nzF #DoF
L2CRF 84.88 89.97 61.6M 38.6M
L1CRF 84.85 89.99 614K 321K
(w/ QT ?=4) 78.39 85.33 568K 8
(w/ QT ?=2) 73.40 81.45 454K 4
(w/ QT ?=1) 65.53 75.87 454K 2
DC-ADMM (?=4) 84.96 89.92 643K 8
(?=2) 84.04 89.35 455K 4
(?=1) 83.06 88.62 364K 2
Test Model complex.
DEPER COMP UAS #nzF #DoF
L2PA 49.67 93.51 15.5M 5.59M
L1RDA 49.54 93.48 7.76M 3.56M
(w/ QT ?=4) 38.58 90.85 6.32M 8
(w/ QT ?=2) 34.19 89.42 3.08M 4
(w/ QT ?=1) 30.42 88.67 3.08M 2
DC-ADMM (?=4) 49.83 93.55 5.81M 8
(?=2) 48.97 93.18 4.11M 4
(?=1) 46.56 92.86 6.37M 2
Table 1: Comparison results of the methods on test
data (K: thousand, M: million)
feature weights and an indexed structure of fea-
ture strings, which are used as the key for obtain-
ing the corresponding feature weight. This paper
mainly discussed how to reduce the size of the for-
mer part, and described its successful reduction.
We note that it is also possible to reduce the lat-
ter part especially if the feature string structure is
TRIE. We omit the details here since it is not the
main topic of this paper, but by merging feature
strings that have the same feature weights, the size
of entire trained models in our DEPAR case can be
reduced to about 10 times smaller than those ob-
tained by standard L1-regularization, i.e., to 12.2
MB from 124.5 MB.
5 Conclusion
This paper proposed a model learning framework
that can simultaneously realize feature grouping
by the incorporation of a simple discrete con-
straint into model learning optimization. This
paper also introduced a feasible algorithm, DC-
ADMM, which can vanish the infeasible combi-
natorial optimization part from the entire learning
algorithm with the help of the ADMM technique.
Experiments showed that DC-ADMM drastically
reduced model complexity in terms of the degrees
of freedom in trained models while maintaining
the performance. There may exist theoretically
cleverer approaches to feature grouping, but the
performance of DC-ADMM is close to the upper
bound. We believe our method, DC-ADMM, to be
very useful for actual use.
22
References
Galen Andrew and Jianfeng Gao. 2007. Scal-
able Training of L1-regularized Log-linear Models.
In Zoubin Ghahramani, editor, Proceedings of the
24th Annual International Conference on Machine
Learning (ICML 2007), pages 33?40. Omnipress.
Amir Beck and Marc Teboulle. 2009. A Fast Iter-
ative Shrinkage-thresholding Algorithm for Linear
Inverse Problems. SIAM Journal on Imaging Sci-
ences, 2(1):183?202.
Howard D. Bondell and Brian J. Reich. 2008. Simulta-
neous Regression Shrinkage, Variable Selection and
Clustering of Predictors with OSCAR. Biometrics,
64(1):115.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato,
and Jonathan Eckstein. 2011. Distributed Opti-
mization and Statistical Learning via the Alternat-
ing Direction Method of Multipliers. Foundations
and Trends in Machine Learning.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 957?961.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line Passive-Aggressive Algorithms. Journal of Ma-
chine Learning Research, 7:551?585.
John Duchi and Yoram Singer. 2009. Efficient On-
line and Batch Learning Using Forward Backward
Splitting. Journal of Machine Learning Research,
10:2899?2934.
Kevin Duh, Jun Suzuki, and Masaaki Nagata. 2011.
Distributed Learning-to-Rank on Streaming Data
using Alternating Direction Method of Multipliers.
In NIPS?11 Big Learning Workshop.
Hugh Everett. 1963. Generalized Lagrange Multiplier
Method for Solving Problems of Optimum Alloca-
tion of Resources. Operations Research, 11(3):399?
417.
Daniel Gabay and Bertrand Mercier. 1976. A Dual
Algorithm for the Solution of Nonlinear Variational
Problems via Finite Element Approximation. Com-
puters and Mathematics with Applications, 2(1):17
? 40.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 824?831, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Rebecka Jo?rnsten and Bin Yu. 2003. Simulta-
neous Gene Clustering and Subset Selection for
Sample Classification Via MDL. Bioinformatics,
19(9):1100?1109.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Pars-
ing. In Proceedings of ACL-08: HLT, pages 595?
603.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the International
Conference on Machine Learning (ICML 2001),
pages 282?289.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Math. Programming, Ser. B, 45(3):503?528.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online Large-margin Training of
Dependency Parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91?98.
Xiaotong Shen and Hsin-Cheng Huang. 2010. Group-
ing Pursuit Through a Regularization Solution Sur-
face. Journal of the American Statistical Associa-
tion, 105(490):727?739.
Robert Tibshirani, Michael Saunders, Saharon Ros-
set, Ji Zhu, and Keith Knight. 2005. Sparsity and
Smoothness via the Fused Lasso. Journal of the
Royal Statistical Society Series B, pages 91?108.
Erik Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition.
In Proceedings of CoNLL-2003, pages 142?147.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic Gradient Descent Training
for L1-regularized Log-linear Models with Cumu-
lative Penalty. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 477?485.
Lin Xiao. 2010. Dual Averaging Methods for Regular-
ized Stochastic Learning and Online Optimization.
Journal of Machine Learning Research, 11:2543?
2596.
Leon Wenliang Zhong and James T. Kwok. 2011.
Efficient Sparse Modeling with Automatic Feature
Grouping. In ICML.
Hui Zou and Trevor Hastie. 2005. Regularization and
Variable Selection via the Elastic Net. Journal of the
Royal Statistical Society, Series B, 67:301?320.
23
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 212?216,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Latent Semantic Matching: Application to Cross-language Text
Categorization without Alignment Information
Tsutomu Hirao and Tomoharu Iwata and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,iwata.tomoharu,nagata.masaaki}@lab.ntt.co.jp
Abstract
Unsupervised object matching (UOM) is
a promising approach to cross-language
natural language processing such as bilin-
gual lexicon acquisition, parallel corpus
construction, and cross-language text cat-
egorization, because it does not require
labor-intensive linguistic resources. How-
ever, UOM only finds one-to-one corre-
spondences from data sets with the same
number of instances in source and target
domains, and this prevents us from ap-
plying UOM to real-world cross-language
natural language processing tasks. To al-
leviate these limitations, we proposes la-
tent semantic matching, which embeds
objects in both source and target lan-
guage domains into a shared latent topic
space. We demonstrate the effectiveness
of our method on cross-language text cat-
egorization. The results show that our
method outperforms conventional unsu-
pervised object matching methods.
1 Introduction
Unsupervised object matching is a method for
finding one-to-one correspondences between ob-
jects across different domains without knowledge
about the relation between the domains. Kernel-
ized sorting (Novi et al, 2010) and canonical cor-
relation analysis based methods (Haghighi et al,
2008; Tripathi et al, 2010) are two such exam-
ples of unsupervised object matching, which have
been shown to be quite useful for cross-language
natural language processing (NLP) tasks. One of
the most important properties of the unsupervised
object matching is that it does not require any lin-
guistic resources which connects between the lan-
guages. This distinguishes it from other cross-
language NLP methods such as machine transla-
tion based and projection based approaches (Du-
mais et al, 1996; Gliozzo and Strapparava, 2005;
Platt et al, 2010), which we need bilingual dictio-
naries or parallel sentences.
When we apply unsupervised object matching
methods to cross-language NLP tasks, there are
two critical problems. The first is that they only
find one-to-one matching. The second is they re-
quire the same size of source- and target-data. For
example, the correct translation of a word is not
always unique. French words ?maison?, ?appart-
ment? and ?domicile? can be regarded as transla-
tion of an English word ?home?. In addition, En-
glish vocabulary size is not equal to that of French.
These discussions motivate us to introduce a
shared space in which both source and target do-
main objects will reside. If we can obtain such
a shared space, we can match objects within the
space, because we can use standard distance met-
rics on this space. This will also enable us to use
various kinds of non-strict matching. For exam-
ple, k-nearest objects in the source domain will be
retrieved for a query object in the target domain.
In this paper, we propose a simple but effective
method to find the shared space by assuming that
two languages have common latent topics, which
we call latent semantic matching. With latent se-
mantic matching, we first find latent topics in two
domains independently. Then, the topics in two
domains are aligned by kernelized sorting, and ob-
jects are embedded in a shared latent topic space.
Latent topic representations are successfully used
in a wide range of NLP tasks, such as information
retrieval and text classification, because they rep-
resent intrinsic information of documents (Deer-
wester et al, 1990). By matching latent topics,
we can find relation between source and target do-
mains, and additionally we can handle different
numbers of objects in two domains.
We compared latent semantic matching with
conventional unsupervised object matching meth-
212
ods on the task of cross-language text categoriza-
tion, i.e. classifying target side unlabeled docu-
ments by label information obtained from source
side documents. The results show that, with more
source side documents, our method achieved the
highest classification accuracy.
2 Related work
Many cross-language text processing methods
have been proposed that require correspondences
between source and target languages. For exam-
ple, (Dumais et al, 1996) proposed cross-lingual
latent semantic indexing, and (Platt et al, 2010)
employed oriented principle component analysis
and canonical correlation analysis (CCA). They
concatenate the document pairs (source document
and its translation) obtained from a document-
level parallel corpus. They then apply multi-
variate analysis to acquire the translingual projec-
tion. There are extensions of latent Dirichlet alo-
cation (LDA) (Blei et al, 2003) for cross-language
analysis, such as multilingual topic models (Boyd-
Graber and Blei, 2009), joint LDA (Jagadeesh
and Daume III, 2010) and multilingual LDA (Xi-
aochuan et al, 2011). They require a bilingual dic-
tionary or document-level parallel corpora.
Unsupervised object matching methods have
been proposed recently (Novi et al, 2010;
Haghighi et al, 2008; Yamada and Sugiyama,
2011). These methods are promising in terms of
language portability because they do not require
external language resources. (Novi et al, 2010)
proposed kernelized sorting (KS); it finds one-to-
one correspondences between objects in different
domains by permuting a set to maximize the de-
pendence between two sets. Here, the Hilbert-
Schmidt independence criterion is used for mea-
suring dependence. (Djuric et al, 2012) proposed
convex kernelized sorting as an extension of KS.
(Yamada and Sugiyama, 2011) proposed least-
squares object matching which maximizes the
squared-loss mutual information between matched
pairs. (Haghighi et al, 2008) proposed another
framework, matching CCA (MCCA), based on a
probabilistic interpretation of CCA (Bach and Jor-
dan, 2005). MCCA simultaneously finds latent
variables that represent correspondences and la-
tent features so that the latent features of corre-
sponding examples exhibit the maximum correla-
tion. However, these unsupervised object match-
ing methods have limitations. They require that
the source and target domains have the same data
size, and they find one-to-one correspondences.
There are critical weaknesses of these methods
when we attempt to apply them to real world
cross-language NLP applications.
3 Latent Semantic Matching
We propose latent semantic matching to find a
shared latent space by assuming that two lan-
guages have common latent topics. Our method
consists of following four steps: (1) for both
source and target domains, we map the documents
to a K-dimensional latent topic space indepen-
dently, (2) we find the one-to-one correspondences
between topics across source and target domains
by unsupervised object matching, (3) we permute
topics of the target side according to the corre-
spondences, while fixing the topics of the source
side, and (4) finally, we map documents in the
source and target domains to a shared latent space
by using permuted and fixed topics.
3.1 Topic Extraction as Dimension Reduction
Suppose that we have N documents in the source
domain. sn=(sni)Ii=1 is the nth document rep-
resented as a multi-dimensional column vector in
the domain, i.e. each document is represented as
a bag-of-words vector. Here, each element of the
vectors indicates the TF?IDF score of the corre-
sponding word in the document. I is the size of the
feature set, i.e., the vocabulary size in the source
domain. Also, we have M documents in the tar-
get domain. tm=(tmj)Jj=1 is the mth document
represented as a multi-dimensional vector. J is
the vocabulary size in the target domain. Thus,
the data set in the source domain is represented by
an I ? N matrix, S=(s1, ? ? ? , sN ), the data set
in the target is represented by a J ? M matrix,
T=(t1, ? ? ? , tM ).
We factorize these matrices using nonnegative
matrix factorization (Lee and Seung, 2000) to find
topics as follows:
S ?WSHS , (1)
T ?WTHT . (2)
WS is an I?K matrix that represents a set of top-
ics, i.e. each column vector denotes word weights
for each topic. HS is a K ? N matrix that de-
notes a set of latent semantic representations of
documents in the source domain, i.e. each row
213
?????? WS HS? * = 0 0 1 00 1 0 00 0 0 11 0 0 0
ment. I is the size of feature set, i.e., the size of vocabulary inthe source domain. Also, we have M documents in a targetdomain. tm = (tmj)Jj=1 is the m-th document representedas a multi-dimensional vector. J is the size of vocabulary inthe target domain. Thus, the data set in the source domain isrepresented as the I ?N matrix, S, the data set in the targetis represented as the J ?M matrix, T .Here, we assume that these matrices are approximated asthe product of low rank matrices as follows:
S ? WSHS , (1)T ? WTHT (2)WS is I?K matrix, which represents a set of topic propor-tions in the source domain, i.e., each column vector denotestopic proportion. HS is K ? N matrix, which denotes a setof documents in the K-dimensional latent space which cor-responds to the source domain, i.e., each row vector denotesthe document in the latent space. The k(1 ? k ? K)-th basisin the latent space corresponds to the k-th topic proportion.WT is I ? K matrix, which represents a set of topic pro-portions in the target domain. HT is K ? N matrix, whichdenotes a set of documents in the latent topic space with di-mentionaly K. K is less than I , J . In this paper, we employNon-negative Matrix Factorization (NMF) [Lee and Seung,2000] to factorize the original matrices.According to the factorization of the original matrices, wecan map the documents in the source and target domain tolatent topic space with dimentionaly K, independently.3.2 Finding Optimal Topic Alignments byUnsupervised Object MatchingTo connect the different latent space, the basis of the spacehave to be aligned each other. That is, topic proportion ex-tracted from the source language must be aligned that fromthe target language. This is reasonable consideration becausewe can assume the same latent concept for both language.For example, a topic proportion obtained from English docu-ments can be aligned a topic proportion obtained from Frenchdocuments. For all k and k?, k-th column vector in WS arealigned k?-th column vector in WT .However, we can not measure similarity between the topicproportions because we do not have any language resourcessuch as dictionary. Therefore, we utilize unsupervised ob-ject matching method to find one-to-one correspondences be-tween topic proportions. In this paper, we employ KernelizedSorting (KS) [Novi et al, 2010]. Of cource, we can replaceKS to another unsupervised object matching sush as MCCA[Haghighi et al, 2008], LSOM [Yamada and Sugiyama,2011].KS finds the best one-to-one matching by followings:
pi? = argmaxpi??K tr(G?SpiTG?Tpi),s.t. pi1K = 1K and piT1K = 1K . (3)pi is K?K matrix which represents one-to-one correspon-dence between topic proportion, i.e., piij = 1 indicates i-thtopic proportion in the source language corresponds to j-th
one of the target language. ? indicates set of all possibleK ? K matrices which store one-to-one corresponrence. GdenotesK?K kernel matrix obtained from topic proportion,Gij = K(WTi,:,W:,j), and G? is the centerd matrix of G. K(, )is a kernel function. 1K is K-dimensional column vector ofall ones. pi? is obtained by iterative procedure. According topi?, we can permutate the basis of the latent space obtainedfrom source language. See fig hoge.
S ? WSHS . (4)On the other hand, we can directly fomulate objective func-tion of unsupervised mapping. If the topic proportions arealigned each other, the correlation matrix (or gram matrix)obtained from source language is proportional to one fromtarget language:
||GS ? ?GT ||2 = 0. (5)? denotes the hyperparameter for tuning the socore range be-tween two gram matrices.By minimize the error of the matrix factorization (equa-tion (1),(2)) and the difference between correlation matrices(equation (6)), the objective function is defined as follow:
E = ?S ?WSHS?2+ ?T ?WTHT ?2+ ?||GS ? ?GT ||2. (6)
? is cost parameter between first, second argu-ment and third argument. The optimal parameters(WS,WT ,HS,HT ) are obtained by minimizing theobjective function. To mimimize the objective, gradient de-scend can be used. but However that is not convex function,we only obtained local optimal. Thefore, we employed abovetwo step procedure??????This objective function is not convex. That meanswe can only obtain local optimal parameters. By min-imizing equation (6), we can obtain a set of parameter(WS,WT ,HS,HT ) for unsupervised mapping. we couldbe employed gradient based algorithm but, as the first step,we employ former two step optimization procedure.3.3 Cross-lingual Text Categorization viaUnsupervised Mappingm-th document in the target domain (tm) is mapped to thesource domain as follows,
s(tm) = HT$:,mWS . (7)Here, HT :,m denotes the m-th column vector of HT , s(tm)is I dimentional vector.When each document in the source domain has a classlabel yn, we can train a classifier on the training data set{sn, yn}Nn=1. Therefore, the class label of the mapped docu-ment in the target domain s(tm) is assigned by the classifier.In the later experiments, we employ k(= 10)-NN as a classi-fier.WT HTMI NJ KKKTST
Figure 1: Topic alignments.
vector denotes an embedding of a document in the
K-dimensional latent space. Similarly, WT is an
I ?K matrix that represents a set of topics in the
target domain, and HT is a K ? M matrix that
denotes a set of latent semantic representations of
target documents. K is less than I and J .
By factorizing the original matrices, we can in-
dependently map the documents in the source and
target domains to the latent topic spaces whose di-
mensionality is K.
3.2 Finding Optimal Topic Alignments by
Unsupervised Object Matching
To connect the different latent spaces, topics ex-
tracted from the source language must be aligned
to one from the target language. This is reasonable
because we can assume that both languages share
the same latent concept.
However, we cannot quantify the similarity be-
tween the topics because we do not have any ex-
ternal language resources such as a dictionary.
Therefore, we utilize unsupervised object match-
ing method to find one-to-one correspondences
between topics. In this paper, we employ kernel-
ized sorting (KS) (Novi et al, 2010). KS finds the
best one-to-one matching as follows:
pi? = argmax
pi??K
tr(GSpi?GTpi),
s.t. pi1K=1K and pi?1K=1K . (3)
Here, pi is a K?K matrix that represents the one-
to-one correspondence between topics, i.e. piij=1
indicates that the ith topic in the source language
corresponds to the jth one of the target language.
Overall Average
KS 0.252 ? 0.112
CKS 0.249 ? 0.033
LSOM 0.278 ? 0.086
LSM(300) 0.298 ? 0.077
LSM(600) 0.359 ? 0.062
Table 1: Average accuracy over all language pairs
?K indicates the set of all possible matrices stor-
ing one-to-one correspondences. G denotes the
K ? K kernel matrix obtained from topic pro-
portion, Gij=K(W?i,: ,W:,j), and G is the centered
matrix of G. K(, ) is a kernel function. 1K is a
K-dimensional column vector of all ones. pi? is
obtained by iterative procedure.
According to pi?, we obtain permuted matrices,
WT=WTpi? and HT=pi??HT , and the product
of permuted matrices is the same with that of un-
permuted matrices as follows:
T ?WTHT=WTHT . (4)
Fig. 1 shows the topic alignment procedure.
Since documents from both domains are repre-
sented in a shared latent space, we can directly cal-
culate the similarity between the nth document in
the source domain and the mth document in the
target domain based on HT :,m (mth column vec-
tor of HT ) and HS:,n (nth column vector of HS).
4 Cross-language Text Categorization
via Latent Semantic Matching
Cross-language text categorization is the task of
exploiting labeled documents in the source lan-
guage (e.g. English) to classify documents in
the target language (e.g. French). Suppose we
have training data set {sn, yn}Nn=1 in the source
language domain. yn ? Y is the class label
for the nth document. We can train a classifier
in the K-dimensional latent space with data set
{H?S:,n, yn}Nn=1. H?S:,n is the projected vector of
sn. Also, the mth document in the target language
domain tm is projected into the latent space as
H?T :,m. Here, the documents in both domains are
projected into the same size latent space and the
basis vectors of the spaces are aligned. Therefore,
we can classify a document in the target domain
tm by a classifier trained with {H?S:,n, yn}Nn=1.
214
Books
English Hack, Parent, tale, subversion, Interesting, centre, Paper, T., prejudice, Murphy
German Lydia, Sebastian, Seelenbrecher, Patient, Fitzek, Patrick, Fiktion, Patientenakte, Realitt, Klinik
Electronics
English SD800, Angle, Digital, Optical, Silver, understnad, camra, 7.1MP, P3N, 10MP
German *****, 550D, 600D, Objektiv, Canon, ablichten, Body, Werkzeug, Kamera, einliet
Kitchen
English Briel, Electra-Craft, Chamonix, machine, Due, crema, supervisor, technician, espresso, tamp
German ESGE, Prierkopf, Zauberstab, Gummikupplung, Suppe/Sauce, Braun , Bolognese, prieren, Testsieger, Topf
Music
English Amy, Poison, Doherty, Schottin, Mid, Prince, Song, ausdrucksstark , Tempo, knocking
German Norah, mini, ?Little, ?Rome, ?Come, Gardot, Lana, listenings , dreamlike, digipak
Watch
English watch, indicate, timex, HRM, month, icon, Timex, datum, troubleshooting, reasonable
German Orient, Diver, Lnette, Leuchtpunkt, Zahlenringes, Handgelenksdurchmesser, Stoppsekunde, Uhrforum,
Konsumbereiche, Schwingungen/Std
Table 2: Examples of aligned latent topics
5 Experimental Evaluation
5.1 Experimental Settings
We compared our method, latent semantic match-
ing (LSM), with three unsupervised object match-
ing methods: Kernelized Sorting (KS), Convex
Kernelized Sorting (CKS), Least-Squares Object
Matching (LSOM). We set the number of the la-
tent topics K to 100 and employed the k-nearest
neighbor method (k=10) as the classifier.
For, KS, CKS and LSOM, we find the one-
to-one correspondence between documents in the
source language and documents in the target lan-
guage. Then, we assign class labels of the target
documents according to the correspondence.
In order to build a corpus with various lan-
guage pairs for evaluation, we crawled product
reviews from Amazon U.S., German, France and
Japan with five categories: ?Books?, ?Electronics?,
?Music?, ?Kitchen?, ?Watch?. The corpus is nei-
ther sentence level parallel nor comparable. For
each category, we randomly select 60 documents
as the test data (M=300) for all methods and 60
documents as the training data (N=300) for KS,
CKS, LSOM and LSM(300). We also compared
latent semantic matching with 120 training docu-
ments for each category (N=600), and called this
method LSM(600). Note that since KS, CKS and
LSOM require that the data sizes are the same for
source and target domains, they cannot use train-
ing data more than test data. To avoid local opti-
mum solutions of NMF, we executed our methods
with 100 different initialization values and chose
the solution that achieved the best objective func-
tion of KS.
5.2 Results and Discussion
Table 1 shows average accuracies with standard
division over all language pairs. From the table,
classification accuracy of all methods significantly
outperformed random classifier (accuracy=0.2).
The results showed the effectiveness of both un-
supervised object matching and latent semantic
matching. When comparing LSM(300) with KS,
CKS and LSOM, LSM(300) obtained better re-
sults than these unsupervised object matching
methods. The result supports the effectiveness of
the latent topic matching. Moreover, LSM(600)
achieved the highest accuracy. There are large dif-
ferences between LSM(600) and the others. This
result implies not only the effectiveness of the la-
tent topic matching but also increasing the number
of source side documents (labeled training data)
contributes to improving classification accuracy.
This is natural in terms of supervised learning but
only our method can deal with source side docu-
ments that are larger in number.
Table 2 shows examples of latent topics in
English and German extracted and aligned by
LSM(600). We can see that some author names,
words related to camera, and cooking equipment
appear in ?Books?, ?Electronics? and ?Kitchen?
topics, respectively. Similarity, there are some
artists? names in ?Music? and watch brands in
?Watch?.
215
6 Conclusion
As an extension of unsupervised object matching,
this paper proposed latent semantic matching that
considers the shared latent space between two lan-
guage domains. To generate such a space, top-
ics of the target space are permuted by exploit-
ing unsupervised object matching. We can mea-
sure distances between objects by standard met-
rics, which enable us retrieving k-nearest objects
in the source domain for a query object in the tar-
get domain. This is a significant advantage over
conventional unsupervised object matching meth-
ods. We used Amazon review corpus to demon-
strate the effectiveness of our method on cross-
language text categorization. The results showed
that our method outperformed conventional object
matching methods with the same number of train-
ing samples. Moreover, our method achieved even
higher performance by utilizing more documents
in the source domain.
Acknowledgements
The authors would like to thank Nemanja Djuric
for providing code for Convex Kernelized Sorting
and the three anonymous reviewers for thoughtful
suggestions.
References
Francis Bach and Michael Jordan. 2005. A probabilis-
tic interpretation of canonical correlation analysis.
Technical report, Department of Statistics, Univer-
sity of California, Berkeley.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR, 3(Jan.):993?
1022.
Jordan Boyd-Graber and David Blei. 2009. Multilin-
gual topic model for unaligned text. In Proc. of the
25th UAI, pages 75?82.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Nemanja Djuric, Mihajlo Grbovic, and Slobodan
Vucetic. 2012. Convex kernelized sorting. In Proc.
of the 26th AAAI, pages 893?899.
Susan Dumais, Lanauer Thomas, and Michael Littman.
1996. Automatic cross-linguistic information re-
trieval using latent semantic indexing. In Proc.
of the Workshop on Cross-Linguistic Information
Retieval in SIGIR, pages 16?23.
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage text categorization by acquiring multilingual
domain models from comparable corpora. In Proc.
of the ACL Workshop on Building and Using Paral-
lel Texts, pages 9?16.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. of ACL-08:
HLT, pages 771?779.
Jagarlamudi Jagadeesh and Hal Daume III. 2010. Ex-
tracting multilingual topics from unaligned corpora.
In Proc of the 32nd ECIR, pages 444?456.
Daniel Lee and Sebastian Seung. 2000. Algorithm
for non-negative matrix factorization. In Advances
in Neural Information Processing Systems 13, pages
556?562.
Quadrianto Novi, Smola Alexander, Song Le, and
Tuytelaars Tinne. 2010. Kernelized sorting. IEEE
Trans. on Pattern Analysis and Machine Intelli-
gence, 32(10):1809?1821.
Jhon Platt, Kristina Toutanova, andWen-tau Yih. 2010.
Translingual document representation from discrim-
inative projections. In Proc. of the 2010 Conference
on EMNLP, pages 251?261.
Abhishek Tripathi, Arto Klami, and Sami Virpioja.
2010. Bilingual sentence matching using kernel
CCA. In Proc. of the 2010 IEEE International
Workshop on MLSP, pages 130?135.
Ni Xiaochuan, Sun Lian-Tao, Hu Jian, and Chen
Zheng. 2011. Cross lingual text classification by
mining multilingual topics from wikipedia. In Proc.
of the 4th WSDM, pages 375?384.
Makoto Yamada and Masashi Sugiyama. 2011. Cross-
domain object matching with model selection. In
Proc. of the 14th AISTATS, pages 807?815.
216
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 315?320,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Single Document Summarization based on Nested Tree Structure
Yuta Kikuchi
?
Tsutomu Hirao
?
Hiroya Takamura
?
?
Tokyo Institute of technology
4295, Nagatsuta, Midori-ku, Yokohama, 226-8503, Japan
{kikuchi,takamura,oku}@lr.pi.titech.ac.jp
?
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,nagata.masaaki}@lab.ntt.co.jp
Manabu Okumura
?
Masaaki Nagata
?
Abstract
Many methods of text summarization
combining sentence selection and sen-
tence compression have recently been pro-
posed. Although the dependency between
words has been used in most of these
methods, the dependency between sen-
tences, i.e., rhetorical structures, has not
been exploited in such joint methods. We
used both dependency between words and
dependency between sentences by con-
structing a nested tree, in which nodes
in the document tree representing depen-
dency between sentences were replaced by
a sentence tree representing dependency
between words. We formulated a sum-
marization task as a combinatorial opti-
mization problem, in which the nested
tree was trimmed without losing impor-
tant content in the source document. The
results from an empirical evaluation re-
vealed that our method based on the trim-
ming of the nested tree significantly im-
proved the summarization of texts.
1 Introduction
Extractive summarization is one well-known ap-
proach to text summarization and extractive meth-
ods represent a document (or a set of documents)
as a set of some textual units (e.g., sentences,
clauses, and words) and select their subset as a
summary. Formulating extractive summarization
as a combinational optimization problem greatly
improves the quality of summarization (McDon-
ald, 2007; Filatova and Hatzivassiloglou, 2004;
Takamura and Okumura, 2009). There has re-
cently been increasing attention focused on ap-
proaches that jointly optimize sentence extraction
and sentence compression (Tomita et al, 2009;
Qian and Liu, 2013; Morita et al, 2013; Gillick
and Favre, 2009; Almeida and Martins, 2013;
Berg-Kirkpatrick et al, 2011). We can only ex-
tract important content by trimming redundant
parts from sentences.
However, as these methods did not include the
discourse structures of documents, the generated
summaries lacked coherence. It is important for
generated summaries to have a discourse struc-
ture that is similar to that of the source docu-
ment. Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988) is one way of introduc-
ing the discourse structure of a document to a
summarization task (Marcu, 1998; Daum?e III and
Marcu, 2002; Hirao et al, 2013). Hirao et al
recently transformed RST trees into dependency
trees and used them for single document summa-
rization (Hirao et al, 2013). They formulated the
summarization problem as a tree knapsack prob-
lem with constraints represented by the depen-
dency trees.
We propose a method of summarizing a single
document that utilizes dependency between sen-
tences obtained from rhetorical structures and de-
pendency between words obtained from a depen-
dency parser. We have explained our method with
an example in Figure 1. First, we represent a doc-
ument as a nested tree, which is composed of two
types of tree structures: a document tree and a
sentence tree. The document tree is a tree that has
sentences as nodes and head modifier relationships
between sentences obtained by RST as edges. The
sentence tree is a tree that has words as nodes
and head modifier relationships between words
obtained by the dependency parser as edges. We
can build the nested tree by regarding each node of
the document tree as a sentence tree. Finally, we
formulate the problem of single document sum-
marization as that of combinatorial optimization,
which is based on the trimming of the nested tree.
315
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  trainning  for  a  race.
The  race  is  held  on  next  month.
?
  Source document                                   
John was running on a track in the park.
He looks very tired.
Mike said he is training for a race.
The race is held on next month.
  Summary                                              
John was running on a track.
he is training for a race. *
The race is held on next month.
EDU?? ???? ??? ????
0
2
4
6
8
10
12
14
16
EDU
selsection
sentence subtree
selection
sentence
selection
reference 
summary
Nu
m
be
r o
f  
se
lec
ted
 se
nt
en
ce
s 
fro
m
 so
ur
ce
 do
cu
m
en
t
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  training  for  a  race.
The  race  is  held  next  month.
?
  Source document                                   
John was running on a track in the park.
He looks very tired.
Mike said he is training for a race.
The race is held next month.
  Summary                                              
John was running on a track.
he is training for a race. *
The race is held next month.
Figure 1: Overview of our method. The source document is represented as a nested tree. Our method
simultaneously selects a rooted document subtree and sentence subtree from each node.
Our method jointly utilizes relations between sen-
tences and relations between words, and extracts
a rooted document subtree from a document tree
whose nodes are arbitrary subtrees of the sentence
tree.
Elementary Discourse Units (EDUs) in RST are
defined as the minimal building blocks of dis-
course. EDUs roughly correspond to clauses.
Most methods of summarization based on RST use
EDUs as extraction textual units. We converted
the rhetorical relations between EDUs to the re-
lations between sentences to build the nested tree
structure. We could thus take into account both
relations between sentences and relations between
words.
2 Related work
Extracting a subtree from the dependency tree of
words is one approach to sentence compression
(Tomita et al, 2009; Qian and Liu, 2013; Morita
et al, 2013; Gillick and Favre, 2009). However,
these studies have only extracted rooted subtrees
from sentences. We allowed our model to extract
a subtree that did not include the root word (See
the sentence with an asterisk ? in Figure 1). The
method of Filippova and Strube (2008) allows the
model to extract non-rooted subtrees in sentence
compression tasks that compress a single sentence
with a given compression ratio. However, it is not
trivial to apply their method to text summariza-
tion because no compression ratio is given to sen-
tences. None of these methods use the discourse
structures of documents.
Daum?e III and Marcu (2002) proposed a noisy-
channel model that used RST. Although their
method generated a well-organized summary, no
optimality of information coverage was guaran-
teed and their method could not accept large texts
because of the high computational cost. In addi-
- The scare over Alar, a growth regulator
- that makes apples redder and crunchier
- but may be carcinogenic,
- made consumers shy away from the Delicious,
- though they were less affected than the McIntosh.
Figure 2: Example of one sentence. Each line cor-
responds to one EDU.
tion, their method required large sets of data to cal-
culate the accurate probability. There have been
some studies that have used discourse structures
locally to optimize the order of selected sentences
(Nishikawa et al, 2010; Christensen et al, 2013).
3 Generating summary from nested tree
3.1 Building Nested Tree with RST
A document in RST is segmented into EDUs and
adjacent EDUs are linked with rhetorical relations
to build an RST-Discourse Tree (RST-DT) that has
a hierarchical structure of the relations. There are
78 types of rhetorical relations between two spans,
and each span has one of two aspects of a nu-
cleus and a satellite. The nucleus is more salient
to the discourse structure, while the other span, the
satellite, represents supporting information. RST-
DT is a tree whose terminal nodes correspond
to EDUs and whose nonterminal nodes indicate
the relations. Hirao et al converted RST-DTs
into dependency-based discourse trees (DEP-DTs)
whose nodes corresponded to EDUs and whose
edges corresponded to the head modifier relation-
ships of EDUs. See Hirao et al for details (Hirao
et al, 2013).
Our model requires sentence-level dependency.
Fortunately we can simply convert DEP-DTs to
obtain dependency trees between sentences. We
specifically merge EDUs that belong to the same
sentence. Each sentence has only one root EDU
that is the parent of all the other EDUs in the sen-
tence. Each root EDU in a sentence has the parent
316
max.
n
?
i
m
i
?
j
w
ij
z
ij
s.t.
?
n
i
?
m
i
j
z
ij
? L; (1)
x
parent(i)
? x
i
; ?i (2)
z
parent(i,j)
? z
ij
+ r
ij
? 0; ?i, j (3)
x
i
? z
ij
; ?i, j (4)
?
m
i
j
z
ij
? min(?, len(i))x
i
; ?i (5)
?
m
i
j
r
ij
= x
i
; ?i (6)
?
j /?R
c
(i)
r
ij
= 0; ?i (7)
r
ij
? z
ij
; ?i, j (8)
r
ij
+ z
parent(i,j)
? 1; ?i, j (9)
r
iroot(i)
= z
iroot(i)
; ?i (10)
?
j?sub(i)
z
ij
? x
i
; ?i (11)
?
j?obj(i)
z
ij
? x
i
; ?i (12)
Figure 3: ILP formulation (x
i
, z
ij
, r
ij
? {0, 1})
EDU in another sentence. Hence, we can deter-
mine the parent-child relations between sentences.
As a result, we obtain a tree that represents the
parent-child relations of sentences, and we can use
it as a document tree. After the document tree is
obtained, we use a dependency parser to obtain the
syntactic dependency trees of sentences. Finally,
we obtain a nested tree.
3.2 ILP formulation
Our method generates a summary by trimming a
nested tree. In particular, we extract a rooted docu-
ment subtree from the document tree, and sentence
subtrees from sentence trees in the document tree.
We formulate our problem of optimization in this
section as that of integer linear programming. Our
model is shown in Figure 3.
Let us denote by w
ij
the term weight of word
ij (word j in sentence i). x
i
is a variable that
is one if sentence i is selected as part of a sum-
mary, and z
ij
is a variable that is one if word ij
is selected as part of a summary. According to the
objective function, the score for the resulting sum-
mary is the sum of the term weights w
ij
that are
included in the summary. We denote by r
ij
the
variable that is one if word ij is selected as a root
of an extracting sentence subtree. Constraint (1)
guarantees that the summary length will be less
than or equal to limit L. Constraints (2) and (3)
are tree constraints for a document tree and sen-
tence trees. r
ij
in Constraint (3) allows the system
to extract non-rooted sentence subtrees, as we pre-
viously mentioned. Function parent(i) returns the
parent of sentence i and function parent(i, j) re-
turns the parent of word ij. Constraint (4) guaran-
tees that words are only selected from a selected
sentence. Constraint (5) guarantees that each se-
lected sentence subtree has at least ? words. Func-
tion len(i) returns the number of words in sentence
i. Constraints (6)-(10) allow the model to extract
subtrees that have an arbitrary root node. Con-
straint (6) guarantees that there is only one root
per selected sentence. We can set the candidate
for the root node of the subtree by using constraint
(7). The R
c
(i) returns a set of the nodes that are
the candidates of the root nodes in sentence i. It
returned the parser?s root node and the verb nodes
in this study. Constraint (8) maintains consistency
between z
ij
and r
ij
. Constraint (9) prevents the
system from selecting the parent node of the root
node. Constraint (10) guarantees that the parser?s
root node will only be selected when the system
extracts a rooted sentence subtree. The root(i) re-
turns the word index of the parser?s root. Con-
straints (11) and (12) guarantee that the selected
sentence subtree has at least one subject and one
object if it has any. The sub(i) and obj(i) return
the word indices whose dependency tag is ?SUB?
and ?OBJ?.
3.3 Additional constraint for grammaticality
We added two types of constraints to our model
to extract a grammatical sentence subtree from a
dependency tree:
z
ik
= z
il
, (13)
?
k?s(i,j)
z
ik
= |s(i, j)|x
i
. (14)
Equation (13) means that words z
ik
and z
il
have
to be selected together, i.e., a word whose depen-
dency tag is PMOD or VC and its parent word, a
negation and its parent word, a word whose de-
pendency tag is SUB or OBJ and its parent verb,
a comparative (JJR) or superlative (JJS) adjective
and its parent word, an article (a/the) and its par-
ent word, and the word ?to? and its parent word.
Equation (14) means that the sequence of words
has to be selected together, i.e., a proper noun se-
quence whose POS tag is PRP$, WP%, or POS
and a possessive word and its parent word and the
words between them. The s(i, j) returns the set of
word indices that are selected together with word
ij.
317
Table 1: ROUGE score of each model. Note that
the top two rows are both our proposals.
ROUGE-1
Sentence subtree 0.354
Rooted sentence subtree 0.352
Sentence selection 0.254
EDU selection (Hirao et al, 2013) 0.321
LEAD
EDU
0.240
LEAD
snt
0.157
4 Experiment
4.1 Experimental Settings
We experimentally evaluated the test collection for
single document summarization contained in the
RST Discourse Treebank (RST-DTB) (Carlson et
al., 2001) distributed by the Linguistic Data Con-
sortium (LDC)
1
. The RST-DTB Corpus includes
385 Wall Street Journal articles with RST anno-
tations, and 30 of these documents also have one
manually prepared reference summary. We set the
length constraint, L, as the number of words in
each reference summary. The average length of
the reference summaries corresponded to approxi-
mately 10% of the length of the source document.
This dataset was first used by Marcu et al for
evaluating a text summarization system (Marcu,
1998). We used ROUGE (Lin, 2004) as an eval-
uation criterion.
We compared our method (sentence subtree)
with that of EDU selection (Hirao et al, 2013).
We examined two other methods, i.e., rooted sen-
tence subtree and sentence selection. These two
are different from our method in the way that they
select a sentence subtree. Rooted sentence subtree
only selects rooted sentence subtrees
2
. Sentence
selection does not trim sentence trees. It simply
selects full sentences from a document tree
3
. We
built all document trees from the RST-DTs that
were annotated in the corpus.
We set the term weight, w
ij
, for our model as:
w
ij
=
log(1 + tf
ij
)
depth(i)
2
, (15)
where tf
ij
is the term frequency of word ij in a
document and depth(i) is the depth of sentence
1
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2002T07
2
We achieved this by making R
c
(i) only return the
parser?s root node in Figure 7.
3
We achieved this by setting ? to a very large number.
i within the sentence-level DEP-DT that we de-
scribed in Section 3.1. For Constraint (5), we set
? to eight.
4.2 Results and Discussion
4.2.1 Comparing ROUGE scores
We have summarized the Recall-Oriented Under-
study for Gisting Evaluation (ROUGE) scores for
each method in Table 1. The score for sentence
selection is low (0.254). However, introducing
sentence compression to the system greatly im-
proved the ROUGE score (0.354). The score is
also higher than that with EDU selection, which
is a state-of-the-art method. We applied a multi-
ple test by using Holm?s method and found that
our method significantly outperformed EDU se-
lection and sentence selection. The difference be-
tween the sentence subtree and the rooted sentence
subtree methods was fairly small. We therefore
qualitatively analyzed some actual examples that
will be discussed in Section 4.2.2. We also exam-
ined the ROUGE scores of two LEAD
4
methods
with different textual units: EDUs (LEAD
EDU
)
and sentences (LEAD
SNT
). Although LEAD
works well and often obtains high ROUGE scores
for news articles, the scores for LEAD
EDU
and
LEAD
SNT
were very low.
4.2.2 Qualitative Evaluation of Sentence
Subtree Selection
This subsection compares the methods of subtree
selection and rooted subtree selection. Figure 4
has two example sentences for which both meth-
ods selected a subtree as part of a summary. The
{?} indicates the parser?s root word. The [?] indi-
cates the word that the system selected as the root
of the subtree. Subtree selection selected a root in
both examples that differed from the parser?s root.
As we can see, subtree selection only selected im-
portant subtrees that did not include the parser?s
root, e.g., purpose-clauses and that-clauses. This
capability is very effective because we have to
contain important content in summaries within
given length limits, especially when the compres-
sion ratio is high (i.e., the method has to gener-
ate much shorter summaries than the source docu-
ments).
4
LEADmethods simply take the firstK textual units from
a source document until the summary length reaches L.
318
Original sentence : John Kriz, a Moody?s vice president, {said} Boston Safe Deposit?s performance has been
hurt this year by a mismatch in the maturities of its assets and liabilities.
Rooted subtree selection : John Kriz a Moody?s vice president [{said}] Boston Safe Deposit?s performance has been
hurt this year
Subtree selection : Boston Safe Deposit?s performance has [been] hurt this year
Original sentence : Recent surveys by Leo J. Shapiro & Associates, a market research firm in Chicago,
{suggest} that Sears is having a tough time attracting shoppers because it hasn?t yet done
enough to improve service or its selection of merchandise.
Rooted subtree selection : surveys [{suggest}] that Sears is having a time
Subtree selection : Sears [is] having a tough time attracting shoppers
Figure 4: Example sentences and subtrees selected by each method.
Table 2: Average number of words that individual
extracted textual units contained.
Subtree Sentence EDU
15.29 18.96 9.98
4.2.3 Fragmentation of Information
Many studies that have utilized RST have simply
adopted EDUs as textual units (Mann and Thomp-
son, 1988; Daum?e III and Marcu, 2002; Hirao et
al., 2013; Knight and Marcu, 2000). While EDUs
are textual units for RST, they are too fine grained
as textual units for methods of extractive summa-
rization. Therefore, the models have tended to se-
lect small fragments from many sentences to max-
imize objective functions and have led to frag-
mented summaries being generated. Figure 2 has
an example of EDUs. A fragmented summary
is generated when small fragments are selected
from many sentences. Hence, the number of sen-
tences in the source document included in the re-
sulting summary can be an indicator to measure
the fragmentation of information. We counted
the number of sentences in the source document
that each method used to generate a summary
5
.
The average for our method was 4.73 and its me-
dian was four sentences. In contrast, methods
of EDU selection had an average of 5.77 and a
median of five sentences. This meant that our
method generated a summary with a significantly
smaller number of sentences
6
. In other words, our
method relaxed fragmentation without decreasing
the ROUGE score. There are boxplots of the num-
bers of selected sentences in Figure 5. Table 2 lists
the number of words in each textual unit extracted
by each method. It indicates that EDUs are shorter
than the other textual units. Hence, the number of
sentences tends to be large.
5
Note that the number for the EDU method is not equal to
selected textual units because a sentence in the source docu-
ment may contain multiple EDUs.
6
We used the Wilcoxon signed-rank test (p < 0.05).
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  trainning  for  a  race.
The  race  is  held  on  next  month.
  Source document                                   John was running on a track in the park.He looks very tired.Mike said he is training for a race.The race is held on next month.
  Summary                                              John was running on a track.he is training for a race.The race is held on next month.
EDU?? ???? ??? ????0
2
4
6
8
10
12
14
16
EDUselsection sentence subtreeselection sentenceselection reference summary
Num
ber o
f  sel
ected
 sent
ence
s 
from
 sour
ce do
cume
nt
Figure 5: Number of sentences that each method
selected.
5 Conclusion
We proposed a method of summarizing a sin-
gle document that included relations between sen-
tences and relations between words. We built a
nested tree and formulated the problem of summa-
rization as that of integer linear programming. Our
method significantly improved the ROUGE score
with significantly fewer sentences than the method
of EDU selection. The results suggest that our
method relaxed the fragmentation of information.
We also discussed the effectiveness of sentence
subtree selection that did not restrict rooted sub-
trees. Although ROUGE scores are widely used
as evaluation metrics for text summarization sys-
tems, they cannot take into consideration linguis-
tic qualities such as human readability. Hence, we
plan to conduct evaluations with people
7
.
We only used the rhetorical structures between
sentences in this study. However, there were also
rhetorical structures between EDUs inside individ-
ual sentences. Hence, utilizing these for sentence
compression has been left for future work. In addi-
tion, we used rhetorical structures that were man-
ually annotated. There have been related studies
on building RST parsers (duVerle and Prendinger,
2009; Hernault et al, 2010) and by using such
parsers, we should be able to apply our model to
other corpora or to multi-document settings.
7
For example, the quality question metric from the Docu-
ment Understanding Conference (DUC).
319
References
Miguel Almeida and Andre Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In ACL, pages
196?206, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
ACL, pages 481?490, Portland, Oregon, USA, June.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In SIGDIAL, pages 1?10.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In NAACL:HLT, pages
1163?1173.
Hal Daum?e III and Daniel Marcu. 2002. A noisy-
channel model for document compression. ACL,
pages 449?456.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine
classification. In IJCNLP, pages 665?673.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In COLING.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In INLG,
pages 25?32.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In ILP, pages 10?18.
Hugo Hernault, Helmut Prendinger, David duVerle,
and Mitsuru Ishizuka. 2010. Hilda: A discourse
parser using support vector machine classification.
Dialogue & Discourse, 1(3):1?30.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,
Norihito Yasuda, and Masaaki Nagata. 2013.
Single-document summarization as a tree knapsack
problem. In EMNLP, pages 1515?1520.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In National Conference on Artificial Intelli-
gence (AAAI), pages 703?710.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out, pages 74?81.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, pages 243?281.
Daniel Marcu. 1998. Improving summarization
through rhetorical parsing tuning. In In Proc. of the
6th Workshop on Very Large Corpora, pages 206?
215.
Ryan T. McDonald. 2007. A study of global infer-
ence algorithms in multi-document summarization.
In ECIR, pages 557?564.
Hajime Morita, Ryohei Sasano, Hiroya Takamura, and
Manabu Okumura. 2013. Subtree extractive sum-
marization via submodular maximization. In ACL,
pages 1023?1032.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Genichiro Kikui. 2010. Opinion summa-
rization with integer linear programming formula-
tion for sentence extraction and ordering. In COL-
ING, pages 910?918.
Xian Qian and Yang Liu. 2013. Fast joint compres-
sion and summarization via graph cuts. In EMNLP,
pages 1492?1502.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on the budgeted median
problem. In CIKM, pages 1589?1592.
Kohei Tomita, Hiroya Takamura, and Manabu Oku-
mura. 2009. A new approach of extractive sum-
marization combining sentence selection and com-
pression. IPSJ SIG Notes, pages 13?20.
320
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 375?383,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
N-best Reranking by Multitask Learning
Kevin Duh Katsuhito Sudoh Hajime Tsukada Hideki Isozaki Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{kevinduh,sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp
nagata.masaaki@lab.ntt.co.jp
Abstract
We propose a new framework for N-best
reranking on sparse feature sets. The idea
is to reformulate the reranking problem as
a Multitask Learning problem, where each
N-best list corresponds to a distinct task.
This is motivated by the observation that
N-best lists often show significant differ-
ences in feature distributions. Training a
single reranker directly on this heteroge-
nous data can be difficult.
Our proposed meta-algorithm solves this
challenge by using multitask learning
(such as ?1/?2 regularization) to discover
common feature representations across N-
best lists. This meta-algorithm is simple to
implement, and its modular approach al-
lows one to plug-in different learning algo-
rithms from existing literature. As a proof
of concept, we show statistically signifi-
cant improvements on a machine transla-
tion system involving millions of features.
1 Introduction
Many natural language processing applications,
such as machine translation (MT), parsing, and
language modeling, benefit from the N-best
reranking framework (Shen et al, 2004; Collins
and Koo, 2005; Roark et al, 2007). The advan-
tage of N-best reranking is that it abstracts away
the complexities of first-pass decoding, allowing
the researcher to try new features and learning al-
gorithms with fast experimental turnover.
In the N-best reranking scenario, the training
data consists of sets of hypotheses (i.e. N-best
lists) generated by a first-pass system, along with
their labels. Given a new N-best list, the goal is
to rerank it such that the best hypothesis appears
near the top of the list. Existing research have fo-
cused on training a single reranker directly on the
entire data. This approach is reasonable if the data
is homogenous, but it fails when features vary sig-
nificantly across different N-best lists. In partic-
ular, when one employs sparse feature sets, one
seldom finds features that are simultaneously ac-
tive on multiple N-best lists.
In this case, we believe it is more advantageous
to view the N-best reranking problem as a multi-
task learning problem, where each N-best list cor-
responds to a distinct task. Multitask learning, a
subfield of machine learning, focuses on how to
effectively train on a set of different but related
datasets (tasks). Our heterogenous N-best list data
fits nicely with this assumption.
The contribution of this work is three-fold:
1. We introduce the idea of viewing N-best
reranking as a multitask learning problem.
This view is particularly apt to any general
reranking problem with sparse feature sets.
2. We propose a simple meta-algorithm that
first discovers common feature representa-
tions across N-bests (via multitask learning)
before training a conventional reranker. Thus
it is easily applicable to existing systems.
3. We demonstrate that our proposed method
outperforms the conventional reranking ap-
proach on a English-Japanese biomedical
machine translation task involving millions
of features.
The paper is organized as follows: Section 2 de-
scribes the feature sparsity problem and Section 3
presents our multitask solution. The effectiveness
of our proposed approach is validated by experi-
ments demonstrated in Section 4. Finally, Sections
5 and 6 discuss related work and conclusions.
2 The Problem of Sparse Feature Sets
For concreteness, we will describe N-best rerank-
ing in terms of machine translation (MT), though
375
our approach is agnostic to the application. In MT
reranking, the goal is to translate a foreign lan-
guage sentence f into an English sentence e by
picking from a set of likely translations. A stan-
dard approach is to use a linear model:
e? = argmax
e?N(f)
wT ? h(e, f) (1)
where h(e, f) is a D-dimensional feature vector,
w is the weight vector to be trained, and N(f) is
the set of likely translations of f , i.e. the N-best
list. The feature h(e, f) can be any quantity de-
fined in terms of the sentence pair, such as transla-
tion model and language model probabilities.
Here we are interested in situations where the
feature definitions can be quite sparse. A com-
mon methodology in reranking is to first design
feature templates based on linguistic intuition and
domain knowledge. Then, numerous features are
instantiated based on the training data seen. For
example, the work of (Watanabe et al, 2007) de-
fines feature templates based on bilingual word
alignments, which lead to extraction of heavily-
lexicalized features of the form:
h(e, f) =
?
?
?
?
?
?
?
1 if foreign word ?Monsieur?
and English word ?Mr.?
co-occur in e,f
0 otherwise
(2)
One can imagine that such features are sparse
because it may only fire for input sentences that
contain the word ?Monsieur?. For all other input
sentences, it is an useless, inactive feature.
Another common feature involves word ngram
templates, for example:
h(e, f) =
?
?
?
1 if English trigram
?Mr. Smith said? occurs in e
0 otherwise
(3)
In this case, all possible trigrams seen in the N-
best list are extracted as features. One can see
that this kind of feature can be very sensitive to
the first-pass decoder: if the decoder has loose re-
ordering constraints, then we may extract expo-
nentially many nonsense ngram features such as
?Smith said Mr.? and ?said Smith Mr.?. Granted,
the reranker training algorithm may learn that
these nonsense ngrams are indicative of poor hy-
potheses, but it is unlikely that the exact same non-
sense ngrams will appear given a different test sen-
tence.
In summary, the following issues compound to
create extremely sparse feature sets:
1. Feature templates are heavily-lexicalized,
which causes the number of features to grow
unbounded as the the amount of data in-
creases.
2. The input (f ) has high variability (e.g. large
vocabulary size), so that features for different
inputs are rarely shared.
3. The N-best list output also exhibits high vari-
ability (e.g. many different word reorder-
ings). Larger N may improve reranking per-
formance, but may also increase feature spar-
sity.
When the number of features is too large, even
popular reranking algorithms such as SVM (Shen
et al, 2004) and MIRA (Watanabe et al, 2007;
Chiang et al, 2009) may fail. Our goal here is to
address this situation.
3 Proposed Reranking Framework
In the following, we first give an intuitive com-
parison between single vs. multiple task learning
(Section 3.1), before presenting the general meta-
algorithm (Section 3.2) and particular instantia-
tions (Section 3.3).
3.1 Single vs. Multiple Tasks
Given a set of I input sentences {f i}, the training
data for reranking consists of a set of I N-best lists
{(Hi,yi)}i=1,...,I , where Hi are features and yi
are labels.
To clarify the notation:1 for an input sentence
f i, there is a N-best list N(f i). For a N-best list
N(f i), there are N feature vectors corresponding
to the N hypotheses, each with dimension D. The
collection of feature vectors for N(f i) is repre-
sented by Hi, which can be seen as a D ? N
matrix. Finally, the N -dimensional vector of la-
bels yi indicates the translation quality of each hy-
pothesis in N(f i). The purpose of the reranker
training algorithm is to find good parameters from
{(Hi,yi)}.
1Generally we use bold font h to represent a vector, bold-
capital font H to represent a matrix. Script h and h(?) may
be scalar, function, or sentence (depends on context).
376
The conventional method of training a single
reranker (single task formulation) involves opti-
mizing a generic objective such as:
argmin
w
I
?
i=1
L(w,Hi,yi) + ??(w) (4)
where w ? RD is the reranker trained on all lists,
and L(?) is some loss function. ?(w) is an op-
tional regularizer, whose effect is traded-off by the
constant ?. For example, the SVM reranker for
MT (Shen et al, 2004) defines L(?) to be some
function of sentence-level BLEU score, and ?(w)
to be the large margin regularizer.2
On the other hand, multitask learning involves
solving for multiple weights, w1,w2, . . . ,wI ,
one for each N-best list. One class of multitask
learning algorithms, Joint Regularization, solves
the following objective:
arg min
w1,..,wI
I
?
i=1
L(wi,Hi,yi) + ??(w1, ..,wI )
(5)
The loss decomposes by task but the joint regu-
larizer ?(w1, ..,wI) couples together the different
weight parameters. The key is to note that multi-
ple weights allow the algorithm to fit the heteroge-
nous data better, compared to a single weight vec-
tor. Yet these weights are still tied together so that
some information can be shared across N-best lists
(tasks).
One instantiation of Eq. 5 is ?1/?2 regular-
ization: ?(w1, ..,wI) , ||W||1,2, where W =
[w1|w2| . . . |wI ]T is a I-by-D matrix of stacked
weight vectors. The norm is computed by first tak-
ing the 2-norm on columns of W, then taking a
1-norm on the resulting D-length vector. This en-
courages the optimizer to choose a small subset of
features that are useful across all tasks.
For example, suppose two different sets of
weight vectors Wa and Wb for a 2 lists, 4 fea-
tures reranking problem. The ?1/?2 norm for Wa
is 14; the ?1/?2 norm for Wb is 12. If both have
the same loss L(?) in Eq. 5, the multitask opti-
mizer would prefer Wb since more features are
shared:
Wa :
?
4 0 0 3
0 4 3 0
?
Wb :
?
4 3 0 0
0 4 3 0
?
4 4 3 3 ? 14 4 5 3 0 ? 12
2In MT, evaluation metrics like BLEU do not exactly de-
compose across sentences, so for some training algorithms
this loss is an approximation.
3.2 Proposed Meta-algorithm
We are now ready to present our general reranking
meta-algorithm (see Algorithm 1), termed Rerank-
ing by Multitask Learning (RML).
Algorithm 1 Reranking by Multitask Learning
Input: N-best data {(Hi,yi)}i=1,...,I
Output: Common feature representation hc(e, f)
and weight vector wc
1: [optional] RandomHashing({Hi})
2: W = MultitaskLearn({(Hi ,yi)})
3: hc = ExtractCommonFeature(W)
4: {Hic} = RemapFeature({Hi}, hc)
5: wc = ConventionalReranker({(Hic ,yi)})
The first step, random hashing, is optional. Ran-
dom hashing is an effective trick for reducing the
dimension of sparse feature sets without suffer-
ing losses in fidelity (Weinberger et al, 2009;
Ganchev and Dredze, 2008). It works by collaps-
ing random subsets of features. This step can be
performed to speed-up multitask learning later. In
some cases, the original feature dimension may be
so large that hashed representations may be neces-
sary.
The next two steps are key. A multitask learn-
ing algorithm is run on the N-best lists, and a com-
mon feature space shared by all lists is extracted.
For example, if one uses the multitask objective
of Eq. 5, the result of step 2 is a set of weights
W. ExtractCommonFeature(W) then returns the
feature id?s (either from original or hashed repre-
sentation) that receive nonzero weight in any of
W.3 The new features hc(e, f) are expected to
have lower dimension than the original features
h(e, f). Section 3.3 describes in detail different
multitask methods that can be plugged-in to this
step.
The final two steps involve a conventional
reranker. In step 4, we remap the N-best list
data according to the new feature representations
hc(e, f). In step 5, we train a conventional
reranker on this common representation, which by
now should have overcome sparsity issues. Us-
ing a conventional reranker at the end allows us
to exploit existing rerankers designed for specific
NLP applications. In a sense, our meta-algorithm
simply involves a change of representation for
the conventional reranking scenario, where the
3For example in Wb, features 1-3 have nonzero weights
and are extracted. Feature 4 is discarded.
377
new representation is found by multitask methods
which are well-suited to heterogenous data.
3.3 Multitask Objective Functions
Here, we describe various multitask methods that
can be plugged in Step 2 of Algorithm 1. Our
goal is to demonstrate that a wide range of existing
methods from the multitask learning literature can
be brought to our problem. We categorize multi-
task methods into two major approaches:
1. Joint Regularization: Eq. 5 is an exam-
ple of joint regularization, with ?1/?2 norm being
a particular regularizer. The idea is to use the reg-
ularizer to ensure that the learned functions of re-
lated tasks are close to each other. The popular
?1/?2 objective can be optimized by various meth-
ods, such as boosting (Obozinski et al, 2009) and
convex programming (Argyriou et al, 2008). Yet
another regularizer is the ?1/?? norm (Quattoni et
al., 2009), which replaces the 2-norm with a max.
One could also define a regularizer to ensure
that each task-specific wi is close to some average
parameter, e.g.
?
i ||wi ? wavg||2. If we inter-
pret wavg as a prior, we begin to see links to Hier-
archical Bayesian methods for multitask learning
(Finkel and Manning, 2009; Daume, 2009).
2. Shared Subspace: This approach assumes
that there is an underlying feature subspace that
is common to all tasks. Early works on multi-
task learning implement this by neural networks,
where different tasks have different output layers
but share the same hidden layer (Caruana, 1997).
Another method is to write the weight vector
as two parts w = [u;v] and let the task-specific
function be uT ? h(e, f) + vT ? ? ? h(e, f) (Ando
and Zhang, 2005). ? is a D??D matrix that maps
the original features to a subspace common to all
tasks. The new feature representation is computed
by the projection hc(e, f) , ? ? h(e, f).
Multitask learning is a vast field and relates to
areas like collaborative filtering (Yu and Tresp,
2005) and domain adaptation. Most methods as-
sume some common representation and is thus ap-
plicable to our framework. The reader is urged to
refer to citations in, e.g. (Argyriou et al, 2008) for
a survey.
4 Experiments and Results
As a proof of concept, we perform experiments
on a MT system with millions of features. We
use a hierarchical phrase-based system (Chiang,
100 101 102 103 104
10?7
10?6
10?5
10?4
10?3
10?2
10?1
100
P(
fea
tur
e o
cc
urs
 in
 x 
lis
ts)
x
Figure 1: This log-log plot shows that there are
many rare features and few common features. The
probability that a feature occurs in x number of N-
best lists behaves according to the power-law x??,
where ? = 2.28.
2007) to generate N-best lists (N=100). Sparse
features used in reranking are extracted according
to (Watanabe et al, 2007). Specifically, the major-
ity are lexical features involving joint occurrences
of words within the N-best lists and source sen-
tences.
It is worth noting that the fact that the first pass
system is a hierarchical system is not essential to
the feature extraction step; similar features can be
extracted with other systems as first-pass, e.g. a
phrase-based system. That said, the extent of the
feature sparsity problem may depend on the per-
formance of the first-pass system.
We experiment with medical domain MT, where
large numbers of technical vocabulary cause spar-
sity challenges. Our corpora consists of English
abstracts from PubMed4 with their Japanese trans-
lations. The first-pass system is built on hierarchi-
cal phrases extracted from 17k sentence pairs and
target (Japanese) language models trained on 800k
medical-domain sentences. For our reranking ex-
periments, we used 500 lists as the training set5,
500 lists as held-out, and another 500 for test.
4.1 Data Characteristics
We present some statistics to illustrate the feature
sparsity problem: From 500 N-best lists, we ex-
tracted a total of 2.4 million distinct features. By
type, 75% of these features occur in only one N-
best list in the dataset. Less than 3% of features
4A database of the U.S. National Library of Medicine.
5In MT, training data for reranking is sometimes referred
to as ?dev set? to distinguish from the data used in first-pass.
Also, while the 17k bitext may seem small compared to other
MT work, we note that 1st pass translation quality (around 28
BLEU) is high enough to evaluate reranking methods.
378
occur in ten or more lists. The distribution of fea-
ture occurrence is clearly Zipfian, as seen in the
power-law plot in Figure 1.
We can also observe the feature growth rate (Ta-
ble 1). This is the number of new features intro-
duced when an additional N-best list is seen. It is
important to note that on average, 2599 new fea-
tures are added everytime a new N-best list is seen.
This is as much as 2599/4188 = 62% of the ac-
tive features. Imagine an online training algorithm
(e.g. MIRA or perceptron) on this kind of data:
whenever a loss occurs and we update the weight
vector, less than half of the weight vector update
applies to data we have seen thus far. Herein lies
the potential for overfitting.
From observing the feature grow rate, one may
hypothesize that adding large numbers of N-best
lists to the training set (500 in the experiments
here) may not necessarily improve results. While
adding data potentially improves the estimation
process, it also increases the feature space dramat-
ically. Thus we see the need for a feature extrac-
tion procedure.
(Watanabe et al, 2007) also reports the possibil-
ity of overfitting in their dataset (Arabic-English
newswire translation), especially when domain
differences are present. Here we observe this ten-
dency already on the same domain, which is likely
due to the highly-specialized vocabulary and the
complex sentence structures common in research
paper abstracts.
4.2 MT Results
Our goal is to compare different feature represen-
tations in reranking: The baseline reranker uses
the original sparse feature representation. This is
compared to feature representations discovered by
three different multitask learning methods:
? Joint Regularization (Obozinski et al, 2009)
? Shared Subspace (Ando and Zhang, 2005)
? Unsupervised Multitask Feature Selection
(Abernethy et al, 2007).6
We use existing implementations of the above
methods.7 The conventional reranker (Step 5, Al-
6This is not a standard multitask algorithm since most
multitask algorithms are supervised. We include it to see
if unsupervised or semi-supervised multitask algorithms is
promising. Intuitively, the method tries to select subsets of
features that are correlated across multiple tasks using ran-
dom sampling (MCMC). Features that co-occur in different
tasks form a high probability path.
7Available at http://multitask.cs.berkeley.edu
Nbest id #NewFt #SoFar #Active
1 3900 3900 3900
2 7535 11435 7913
3 6078 17513 7087
4 3868 21381 4747
5 1896 23277 2645
6 3542 26819 4747
....
100 2440 289118 4299
101 1639 290757 2390
102 3468 294225 4755
103 2350 296575 3824
Average 2599 ? 4188
Table 1: Feature growth rate: For N-best list i in
the table, we have (#NewFt = number of new fea-
tures introduced since N-best i ? 1) ; (#SoFar =
Total number of features defined so far); and (#Ac-
tive = number of active features for N-best i). E.g.,
we extracted 7535 new features from N-best 2;
combined with the 3900 from N-best 1, the total
features so far is 11435.
gorithm 1) used in all cases is SVMrank.8 Our
initial experiments show that the SVM baseline
performance is comparable to MIRA training, so
we use SVM throughout. The labels for the SVM
are derived as in (Shen et al, 2004), where top
10% of hypotheses by smoothed sentence-BLEU
is ranked before the bottom 90%. All multitask
learning methods work on hashed features of di-
mension 4000 (Step 1, Algorithm 1). This speeds
up the training process.
All hyperparameters of the multitask method
are tuned on the held-out set. In particular, the
most important is the number of common features
to extract, which we pick from {250, 500, 1000}.
Table 2 shows the results by BLEU (Papineni
et al, 2002) and PER. The Oracle results are ob-
tained by choosing the best hypothesis per N-best
list by sentence-level BLEU, which achieved 36.9
BLEU in both Train and Test. A summary of our
observations is:
1. The baseline (All sparse features) overfits. It
achieves the oracle BLEU score on the train
set (36.9) but performs poorly on the test
(28.6).
2. Similar overfitting occurs when traditional ?1
regularization is used to select features on
8Available at http://svmlight.joachims.org
379
the sparse feature representation9 . ?1 reg-
ularization is a good method of handling
sparse features for classification problems,
but in reranking the lack of tying between
lists makes this regularizer inappropriate. A
small set of around 1200 features are chosen:
they perform well independently on each task
in the training data, but there is little sharing
with the test data.
3. All three multitask methods obtained features
that outperformed the baseline. The BLEU
scores are 28.8, 28.9, 29.1 for Unsupervised
Feature Selection, Joint Regularization, and
Shared Subspace, respectively, which all out-
perform the 28.6 baseline. All improvements
are statistically significant by bootstrap sam-
pling test (1000 samples, p < 0.05) (Zhang
et al, 2004).
4. Shared Subspace performed the best. We
conjecture this is because its feature projec-
tion can create new feature combinations that
is more expressive than the feature selection
used by the two other methods.
5. PER results are qualitatively similar to BLEU
results.
6. As a further analysis, we are interested in see-
ing whether multitask learning extracts novel
features, especially those that have low fre-
quency. Thus, we tried an additional feature
representation (feature threshold) which only
keeps features that occur in more than x N-
bests, and concatenate these high-frequency
features to the multitask features. The fea-
ture threshold alone achieves nice BLEU re-
sults (29.0 for x > 10), but the combination
outperforms it by statistically significant mar-
gins (29.3-29.6). This implies that multitask
learning is extracting features that comple-
ment well with high frequency features.
For the multitask features, improvements of 0.2
to 1.0 BLEU are modest but consistent. Figure
2 shows the BLEU of bootstrap samples obtained
as part of the statistical significance test. We see
that multitask almost never underperform base-
line in any random sampling of the data. This im-
plies that the proposed meta-algorithm is very sta-
9Optimized by the Vowpal Wabbit toolkit:
http://hunch.net/vw/
ble, i.e. it is not a method that sometimes improves
and sometimes degrades.
Finally, a potential question to ask is: what
kinds of features are being selected by the
multitask learning algorithms? We found that
that two kinds of features are usually selected:
one is general features that are not lexicalized,
such as ?count of phrases?, ?count of dele-
tions/insertions?, ?number of punctuation marks?.
The other kind is lexicalized features, such as
those in Equations 2 and 3, but involving functions
words (like the Japanese characters ?wa?, ?ga?,
?ni?, ?de?) or special characters (such as numeral
symbol and punctuation). These are features that
can be expected to be widely applicable, and it is
promising that multitask learning is able to recover
these from the millions of potential features. 10
?0.2 0 0.2 0.4 0.6 0.8 1 1.2
0
50
100
150
200
250
300
BLEU(shared subspace)?BLEU(baseline sparse feature)
Bo
ot
st
ra
p 
sa
m
pl
es
Figure 2: BLEU difference of 1000 bootstrap sam-
ples. 95% confidence interval is [.15, .90] The
proposed approach therefore seems to be a stable
method.
5 Related Work in NLP
Previous reranking work in NLP can be classified
into two different research focuses:
1. Engineering better features: In MT, (Och
and others, 2004) investigates features extracted
from a wide variety of syntactic representations,
such as parse tree probability on the outputs. Al-
though their results show that the proposed syntac-
tic features gave little improvements, they point to
some potential reasons, such as domain mismatch
for the parser and overfitting by the reranking
10Note: In order to do this analysis, we needed to run Joint
Regularization on the original feature representation, since
the hashed representations are less interpretable. This turns
out to be computationally prohibitive in the time being so we
only ran on a smaller data set of 50 lists. Recently new op-
timization methods that are orders of magnitude faster have
been developed (Liu et al, 2009), which makes larger-scale
experiments possible.
380
Train Test Test
Feature Representation #Feature BLEU BLEU PER
(baselines)
First pass 20 29.5 28.5 38.3
All sparse features (Main baseline) 2.4M 36.9 28.6 38.2
All sparse features w/ ?1 regularization 1200 36.5 28.5 38.6
Random hash representation 4000 33.0 28.5 38.2
(multitask learning)
Unsupervised FeatureSelect 500 32.0 28.8 37.7
Joint Regularization 250 31.8 28.9 37.5
Shared Subspace 1000 32.9 29.1 37.3
(combination w/ high-frequency features)
(a) Feature threshold x > 100 3k 31.7 27.9 38.2
(b) Feature threshold x > 10 60k 35.8 29.0 37.9
Unsupervised FeatureSelect + (b) 60.5k 36.2 29.3 37.6
Joint Regularization + (b) 60.25k 36.1 29.4 37.5
Shared Subspace + (b) 61k 36.2 29.6 37.3
Oracle (best possible) ? 36.9 36.9 33.1
Table 2: Results for different feature sets, with corresponding feature size and train/test BLEU/PER. All
multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared
Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency
features also give significant improvements over the high frequency features alone.
method. Recent work by (Chiang et al, 2009) de-
scribes new features for hierarchical phrase-based
MT, while (Collins and Koo, 2005) describes
features for parsing. Evaluation campaigns like
WMT (Callison-Burch et al, 2009) and IWSLT
(Paul, 2009) also contains a wealth of information
for feature engineering in various MT tasks.
2. Designing better training algorithms: N-
best reranking can be seen as a subproblem of
structured prediction, so many general structured
prediction algorithms (c.f. (Bakir et al, 2007))
can be applied. In fact, some structured predic-
tion algorithms, such as the MIRA algorithm used
in dependency parsing (McDonald et al, 2005)
and MT (Watanabe et al, 2007) uses iterative
sets of N-best lists in its training process. Other
training algorithms include perceptron-style algo-
rithms (Liang et al, 2006), MaxEnt (Charniak and
Johnson, 2005), and boosting variants (Kudo et al,
2005).
The division into two research focuses is conve-
nient, but may be suboptimal if the training algo-
rithm and features do not match well together. Our
work can be seen as re-connecting the two focuses,
where the training algorithm is explicitly used to
help discover better features.
Multitask learning is currently an active subfield
within machine learning. There has already been
some applications in NLP: For example, (Col-
lobert and Weston, 2008) uses a deep neural net-
work architecture for multitask learning on part-
of-speech tagging, chunking, semantic role label-
ing, etc. They showed that jointly learning these
related tasks lead to overall improvements. (De-
selaers et al, 2009) applies similar methods for
machine transliteration. In information extraction,
learning different relation types can be naturally
cast as a multitask problem (Jiang, 2009; Carlson
et al, 2009). Our work can be seen as following
the same philosophy, but applied to N-best lists.
In other areas, (Reichart et al, 2008) introduced
an active learning strategy for annotating multitask
linguistic data. (Blitzer et al, 2006) applies the
multitask algorithm of (Ando and Zhang, 2005)
to domain adaptation problems in NLP. We expect
that more novel applications of multitask learning
will appear in NLP as the techniques become scal-
able and standard.
6 Discussion and Conclusion
N-best reranking is a beneficial framework for ex-
perimenting with large feature sets, but unfortu-
nately feature sparsity leads to overfitting. We ad-
dressed this by re-casting N-best lists as multitask
381
learning data. Our MT experiments show consis-
tent statistically significant improvements.
From the Bayesian view, multitask formulation
of N-best lists is actually very natural: Each N-
best is generated by a different data-generating
distribution since the input sentences are different,
i.e. p(e|f1) 6= p(e|f2). Yet these N-bests are re-
lated since the general p(e|f) distribution depends
on the same first-pass models.
The multitask learning perspective opens up
interesting new possibilities for future work, e.g.:
? Different ways to partition data into tasks,
e.g. clustering lists by document structure, or
hierarchical clustering of data
? Multitask learning on lattices or N-best lists
with larger N. It is possible that a larger hy-
pothesis space may improve the estimation of
task-specific weights.
? Comparing multitask learning to sparse on-
line learning of batch data, e.g. (Tsuruoka et
al., 2009).
? Modifying the multitask objective to incorpo-
rate application-specific loss/decoding, such
as Minimum Bayes Risk (Kumar and Byrne,
2004)
? Using multitask learning to aid large-scale
feature engineering and visualization.
Acknowledgments
We have received numerous helpful comments
throughout the course of this work. In partic-
ular, we would like to thank Albert Au Yeung,
Jun Suzuki, Shinji Watanabe, and the three anony-
mous reviewers for their valuable suggestions.
References
Jacob Abernethy, Peter Bartlett, and Alexander
Rakhlin. 2007. Multitask learning with expert ad-
vice. In COLT.
Rie Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks
and unlabeled data. JMLR.
Andreas Argyriou, Theodoros Evgeniou, and Massim-
iliano Pontil. 2008. Convex multitask feature learn-
ing. Machine Learning, 73(3).
G. Bakir, T. Hofmann, B. Scholkopf, A. Smola,
B. Taskar, and S. V. N. Vishwanathan, editors. 2007.
Predicting structured data. MIT Press.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning.
In EMNLP.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In
WMT.
Andrew Carlson, Justin Betteridge, Estevam Hruschka,
and Tom Mitchell. 2009. Coupling semi-supervised
learning of categories and relations. In NAACL
Workshop on Semi-supervised learning for NLP
(SSLNLP).
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural langauge parsing. Computa-
tional Linguistics, 31(1).
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In ICML.
Hal Daume. 2009. Bayesian multitask learning with
latent hierarchies. In UAI.
Thomas Deselaers, Sasa Hasan, Oliver Bender, and
Hermann Ney. 2009. A deep learning approach to
machine transliteration. In WMT.
Jenny Rose Finkel and Chris Manning. 2009. Hier-
archical Bayesian domain adaptation. In NAACL-
HLT.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In ACL-
2008 Workshop on Mobile Language Processing.
Jing Jiang. 2009. Multitask transfer learning for
weakly-supervised relation extraction. In ACL.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree fea-
tures. In ACL.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
382
J. Liu, S. Ji, and J. Ye. 2009. Multi-task feature learn-
ing via efficient l2,1-norm minimization. In UAI.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large margin training of de-
pendency parsers. In ACL.
Guillaume Obozinski, Ben Taskar, and Michael Jor-
dan. 2009. Joint covariate selection and joint sub-
space selection for multiple classification problems.
Statistics and Computing.
F.J. Och et al 2004. A smorgasbord of features for
statistical machine translation. In HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
Michael Paul. 2009. Overview of the iwslt 2009 eval-
uation campaign. In IWSLT.
Ariadna Quattoni, Xavier Carreras, Michael Collins,
and Trevor Darrell. 2009. An efficient projection
for L1-Linfinity regularization. In ICML.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for lin-
guistic annotations. In ACL.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Computer Speech and Language, 21(2).
Libin Shen, Anoop Sarkar, and Franz Och. 2004. Dis-
criminative reranking for machine translation. In
HLT-NAACL.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In ACL-IJCNLP.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In EMNLP-
CoNLL.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In ICML.
Kai Yu and Volker Tresp. 2005. Learning to learn and
collaborative filtering. In NIPS-2005 Workshop on
Inductive Transfer.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
LREC.
383
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418?427,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Divide and Translate: Improving Long Distance Reordering in Statistical
Machine Translation
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu Hirao, Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan
sudoh@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a novel method
for long distance, clause-level reordering
in statistical machine translation (SMT).
The proposed method separately translates
clauses in the source sentence and recon-
structs the target sentence using the clause
translations with non-terminals. The non-
terminals are placeholders of embedded
clauses, by which we reduce complicated
clause-level reordering into simple word-
level reordering. Its translation model
is trained using a bilingual corpus with
clause-level alignment, which can be au-
tomatically annotated by our alignment
algorithm with a syntactic parser in the
source language. We achieved signifi-
cant improvements of 1.4% in BLEU and
1.3% in TER by using Moses, and 2.2%
in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT, for
the English-to-Japanese translation of re-
search paper abstracts in the medical do-
main.
1 Introduction
One of the common problems of statistical ma-
chine translation (SMT) is to overcome the differ-
ences in word order between the source and target
languages. This reordering problem is especially
serious for language pairs with very different word
orders, such as English-Japanese. Many previous
studies on SMT have addressed the problem by
incorporating probabilistic models into SMT re-
ordering. This approach faces the very large com-
putational cost of searching over many possibili-
ties, especially for long sentences. In practice the
search can be made tractable by limiting its re-
ordering distance, but this also renders long dis-
tance movements impossible. Some recent stud-
ies avoid the problem by reordering source words
prior to decoding. This approach faces difficul-
ties when the input phrases are long and require
significant word reordering, mainly because their
reordering model is not very accurate.
In this paper, we propose a novel method for
translating long sentences that is different from
the above approaches. Problematic long sentences
often include embedded clauses1 such as rela-
tive clauses. Such an embedded (subordinate)
clause can usually be translated almost indepen-
dently of words outside the clause. From this
viewpoint, we propose a divide-and-conquer ap-
proach: we aim to translate the clauses sepa-
rately and reconstruct the target sentence using the
clause translations. We first segment a source sen-
tence into clauses using a syntactic parser. The
clauses can include non-terminals as placeholders
for nested clauses. Then we translate the clauses
with a standard SMT method, in which the non-
terminals are reordered as words. Finally we re-
construct the target sentence by replacing the non-
terminals with their corresponding clause transla-
tions. With this method, clause-level reordering is
reduced to word-level reordering and can be dealt
with efficiently. The models for clause translation
are trained using a bilingual corpus with clause-
level alignment. We also present an automatic
clause alignment algorithm that can be applied to
sentence-aligned bilingual corpora.
In our experiment on the English-to-Japanese
translation of multi-clause sentences, the proposed
method improved the translation performance by
1.4% in BLEU and 1.3% in TER by using Moses,
and by 2.2% in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT.
The main contribution of this paper is two-fold:
1Although various definitions of a clause can be
considered, this paper follows the definition of ?S?
(sentence) in Enju. It basically follows the Penn Tree-
bank II scheme but also includes SINV, SQ, SBAR. See
http://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enju-
output-spec.html#correspondence for details.
418
1. We introduce the idea of explicit separa-
tion of in-clause and outside-clause reorder-
ing and reduction of outside-clause reorder-
ing into common word-level reordering.
2. We propose an automatic clause alignment
algorithm, by which our approach can be
used without manual clause-level alignment.
This paper is organized as follows. The next
section reviews related studies on reordering. Sec-
tion 3 describes the proposed method in detail.
Section 4 presents and discusses our experimen-
tal results. Finally, we conclude this paper with
our thoughts on future studies.
2 Related Work
Reordering in SMT can be roughly classified into
two approaches, namely a search in SMT decod-
ing and preprocessing.
The former approach is a straightforward way
that models reordering in noisy channel transla-
tion, and has been studied from the early period
of SMT research. Distance-based reordering is a
typical approach used in many previous studies re-
lated to word-based SMT (Brown et al, 1993) and
phrase-based SMT (Koehn et al, 2003). Along
with the advances in phrase-based SMT, lexical-
ized reordering with a block orientation model was
proposed (Tillmann, 2004; Koehn et al, 2005).
This kind of reordering is suitable and commonly
used in phrase-based SMT. On the other hand,
a syntax-based SMT naturally includes reorder-
ing in its translation model. A lot of research
work undertaken in this decade has used syntac-
tic parsing for linguistically-motivated translation.
(Yamada and Knight, 2001; Graehl and Knight,
2004; Galley et al, 2004; Liu et al, 2006). Wu
(1997) and Chiang (2007) focus on formal struc-
tures that can be extracted from parallel corpora,
instead of a syntactic parser trained using tree-
banks. These syntactic approaches can theoret-
ically model reordering over an arbitrary length,
however, long distance reordering still faces the
difficulty of searching over an extremely large
search space.
The preprocessing approach employs deter-
ministic reordering so that the following trans-
lation process requires only short distance re-
ordering (or even a monotone). Several previ-
ous studies have proposed syntax-driven reorder-
ing based on source-side parse trees. Xia and
McCord (2004) extracted reordering rules auto-
matically from bilingual corpora for English-to-
French translation; Collins et al (2005) used
linguistically-motivated clause restructuring rules
for German-to-English translation; Li et al (2007)
modeled reordering on parse tree nodes by us-
ing a maximum entropy model with surface and
syntactic features for Chinese-to-English trans-
lation; Katz-Brown and Collins (2008) applied
a very simple reverse ordering to Japanese-to-
English translation, which reversed the word order
in Japanese segments separated by a few simple
cues; Xu et al (2009) utilized a dependency parser
with several hand-labeled precedence rules for re-
ordering English to subject-object-verb order like
Korean and Japanese. Tromble and Eisner (2009)
proposed another reordering approach based on a
linear ordering problem over source words with-
out a linguistically syntactic structure. These pre-
processing methods reorder source words close
to the target-side order by employing language-
dependent rules or statistical reordering models
based on automatic word alignment. Although
the use of language-dependent rules is a natural
and promising way of bridging gaps between lan-
guages with large syntactic differences, the rules
are usually unsuitable for other language groups.
On the other hand, statistical methods can be ap-
plied to any language pairs. However, it is very
difficult to reorder all source words so that they are
monotonic with the target words. This is because
automatic word alignment is not usually reliable
owing to data sparseness and the weak modeling
of many-to-many word alignments. Since such
a reordering is not complete or may even harm
word ordering consistency in the source language,
these previous methods further applied reordering
in their decoding. Li et al (2007) used N-best
reordering hypotheses to overcome the reordering
ambiguity.
Our approach is different from those of previous
studies that aim to perform both short and long dis-
tance reordering at the same time. The proposed
method distinguishes the reordering of embedded
clauses from others and efficiently accomplishes it
by using a divide-and-conquer framework. The re-
maining (relatively short distance) reordering can
be realized in decoding and preprocessing by the
methods described above. The proposed frame-
work itself does not depend on a certain language
pair. It is based on the assumption that a source
419
language clause is translated to the corresponding
target language clause as a continuous segment.
The only language-dependent resource we need is
a syntactic parser of the source language. Note
that clause translation in the proposed method is a
standardMT problem and therefore any reordering
method can be employed for further improvement.
This work is inspired by syntax-based meth-
ods with respect to the use of non-terminals. Our
method can be seen as a variant of tree-to-string
translation that focuses only on the clause struc-
ture in parse trees and independently translates the
clauses. Although previous syntax-based methods
can theoretically model this kind of derivation, it
is practically difficult to decode long multi-clause
sentences as described above.
Our approach is also related to sentence sim-
plification and is intended to obtain simple and
short source sentences for better translation. Kim
and Ehara (1994) proposed a rule-based method
for splitting long Japanese sentences for Japanese-
to-English translation; Furuse et al (1998) used
a syntactic structure to split ill-formed inputs in
speech translation. Their splitting approach splits
a sentence sequentially to obtain short segments,
and does not undertake their reordering.
Another related field is clause identification
(Tjong et al, 2001). The proposed method is not
limited to a specific clause identification method
and any method can be employed, if their clause
definition matches the proposed method where
clauses are independently translated.
3 Proposed Method
The proposed method consists of the following
steps illustrated in Figure 1.
During training:
1) clause segmentation of source sentences with
a syntactic parser (section 3.1)
2) alignment of target words with source clauses
to develop a clause-level aligned corpus (section
3.2)
3) training the clause translation models using
the corpus (section 3.3)
During testing:
1) clause translation with the clause translation
models (section 3.4)
2) sentence reconstruction based on non-
terminals (section 3.5)
Bilingual
Corpus
(Training)
source
target
parse & clause
segmentation
parse &
clause
segmen-
tation
Source Sentences
(clause-segmented)
Word Alignment
Model
Target Word Bigram
Language Model
LM training
word
alignment
Bilingual Corpus
(clause-aligned)
automatic clause alignment
Clause
Translation Models
(Phrase Table, N-gram LMs, ...)
training from scratch
Bilingual
Corpus
(Development)
(clause-segmented)
MERT
Test Sentence
Sentence
Translation
clause
clause
clause
clause
translation
clause
translation
clause
translation
sentence reconstruction
based on non-terminals
translation
Original (sentence-aligned)
corpus can also be used
Figure 1: Overview of proposed method.
3.1 Clause Segmentation of Source Sentences
Clauses in source sentences are identified by a
syntactic parser. Figure 2 shows a parse tree for
the example sentence below. The example sen-
tence has a relative clause modifying the noun
book. Figure 3 shows the word alignment of this
example.
English: John lost the book that was borrowed
last week from Mary.
Japanese: john wa (topic marker) senshu (last
week) mary kara (from) kari (borrow) ta
(past tense marker) hon (book) o (direct ob-
ject marker) nakushi (lose) ta (past tense
marker) .
We segment the source sentence at the clause level
and the example is rewritten with two clauses as
follows.
? John lost the book s0 .
? that was borrowed last week from Mary
s0 is a non-terminal symbol the serves as a place-
holder of the relative clause. We allow an arbitrary
420
SS
John
lost
the
book
that
was
borrowed
from Mary
last week
Figure 2: Parse tree for example English sentence.
Node labels are omitted except S.
John
lo
st
the
book
that
w
as
borrow
ed
from
M
ary
last
w
eek
john
wa
ta
nakushi
o
hon
ta
kari
kara
mary
senshu
Figure 3: Word alignment for example bilingual
sentence.
number of non-terminals in each clause2. A nested
clause structure can be represented in the same
manner using such non-terminals recursively.
3.2 Alignment of Target Words with Source
Clauses
To translate source clauses with non-terminal sym-
bols, we need models trained using a clause-level
aligned bilingual corpus. A clause-level aligned
corpus is defined as a set of parallel, bilingual
clause pairs including non-terminals that represent
embedded clauses.
We assume that a sentence-aligned bilingual
corpus is available and consider the alignment of
target words with source clauses. We can manu-
ally align these Japanese words with the English
clauses as follows.
? john wa s0 hon o nakushi ta .
2In practice not so many clauses are embedded in a single
sentence but we found some examples with nine embedded
clauses for coordination in our corpora.
John lost the book s0 .
? senshu mary kara kari ta
that was borrowed last week from Mary
Since the cost of manual clause alignment is
high especially for a large-scale corpus, a natu-
ral question to ask is whether this resource can be
obtained from a sentence-aligned bilingual corpus
automatically with no human input. To answer
this, we now describe a simple method for deal-
ing with clause alignment data from scratch, us-
ing only the word alignment and language model
probabilities inferred from bilingual and monolin-
gual corpora.
Our method is based on the idea that automatic
clause alignment can be viewed as a classification
problem: for an English sentence with N words (e
= (e1, e2, . . . , eN )) andK clauses (e?1,e?2,. . . ,e?K),
and its Japanese translation with M words (f
= (f1, f2, . . . , fM )), the goal is to classify each
Japanese word into one of {1, . . . ,K} classes. In-
tuitively, the probability that a Japanese word fm
is assigned to class k ? {1, . . . ,K} depends on
two factors:
1. The probability of translating fm into the En-
glish words of clause k (i.e.
?
e?e?k p(e|fm)).
We expect fm to be assigned to a clause
where this value is high.
2. The language model probability
(i.e. p(fm|fm?1)). If this value is high,
we expect fm and fm?1 to be assigned to the
same clause.
We implement this intuition using a graph-
based method. For each English-Japanese sen-
tence pair, we construct a graph with K clause
nodes (representing English clauses) and M word
nodes (representing Japanese words). The edge
weights between word and clause nodes are de-
fined as the sum of lexical translation probabilities
?
e?e?k p(e|fm). The edge weights between words
are defined as the bigram probability p(fm|fm?1).
Each clause node is labeled with a class ID k ?
{1, . . . ,K}. We then propagate these K labels
along the graph to label the M word nodes. Fig-
ure 4 shows the graph for the example sentence.
Many label propagation algorithms are avail-
able. The important thing is to use an algo-
rithm that encourages node pairs with strong edge
weights to receive the same label. We use the label
propagation algorithm of (Zhu et al, 2003). If we
421
John  lost  the  book  that  was  borrowed ...
clause(1) clause(2)
John Mary fromlast weektopicmarker
p(John |           )
+ p(lost |           )
+ ...
p(that |        )
+ p(was |        )
+ ...
p(     |         ) p(         |            ) p(        |         )p(            |     )
john kara
karajohn
john wa senshu mary kara
wa  john senshu  wa mary  senshu kara mary
Figure 4: Graph-based representation of the ex-
ample sentence. We propagate the clause labels to
the Japanese word nodes on this graph to form the
clause alignments.
assume the labels are binary, the following objec-
tive is minimized:
argmin
l?RK+M
?
i,j
wij(li ? lj)2 (1)
where wij is the edge weight between nodes i
and j (1 ? i ? K + M , 1 ? j ? K +
M ), and l (li ? {0, 1}) is a vector of labels
on the nodes. The first K elements of l, lc =
(l1, l2, ..., lK)T , are constant because the clause
nodes are pre-labeled. The remaining M ele-
ments, lf = (lK+1, lK+2, ..., lK+M )T , are un-
known and to be determined. Here, we consider
the decomposition of the weight matrixW = [wij ]
into four blocks after the K-th row and column as
follows:
W =
[
W cc W cf
W fc W ff
]
(2)
The solution of eqn. (1), namely lf , is given by the
following equation:
lf = (Dff ?W ff )?1W fc lc (3)
where D is the diagonal matrix with di =
?
j wij
and is decomposed similarly to W . Each element
of lf is in the interval (0, 1) and can be regarded
as the label propagation probability. A detailed ex-
planation of this solution can be found in Section 2
of (Zhu et al, 2003). For our multi-label problem
with K labels, we slightly modified the algorithm
by expanding the vector l to an (M + K) ? K
binary matrix L = [ l1 l2 ... lK ].
After the optimization, we can normalize Lf
to obtain the clause alignment scores t(lm =
k|fm) between each Japanese word fm and En-
glish clause k. Theoretically, we can simply out-
put the clause id k? for each fm by finding k? =
argmaxk t(lm = k|fm). In practice, this may
sometimes lead to Japanese clauses that have too
many gaps, so we employ a two-stage procedure
to extract clauses that are more contiguous.
First, we segment the Japanese sentence into K
clauses based on a dynamic programming algo-
rithm proposed by Malioutov and Barzilay (2006).
We define an M ? M similarity matrix S = [sij ]
with sij = exp(?||li?lj ||) where li is (K + i)-th
row vector in the label matrix L. sij represents
the similarity between the i-th and j-th Japanese
words with respect to their clause alignment score
distributions; if the score distributions are sim-
ilar then sij is large. The details of this algo-
rithm can be found in (Malioutov and Barzilay,
2006). The clause segmentation gives us contigu-
ous Japanese clauses f?1, f?2, ..., f?K , thus min-
imizing inter-segment similarity and maximizing
intra-segment similarity. Second, we determine
the clause labels of the segmented clauses, based
on clause alignment scores T = [Tkk? ] for English
and automatically-segmented Japanese clauses:
Tkk? =
?
fm?f? k?
t(lm = k|fm) (4)
where f?k? is the j?-th Japanese clause. In descend-
ing order of the clause alignment score, we greed-
ily determine the clause label 3.
3.3 Training Clause Translation Models
We train clause translation models using the
clause-level aligned corpus. In addition we can
also include the original sentence-aligned corpus.
We emphasize that we can use standard techniques
for heuristically extracted phrase tables, word n-
gram language models, and so on.
3.4 Clause Translation
By using the source language parser, a multi-
clause source sentence is reduced to a set of
clauses. We translate these clauses with a common
SMT method using the clause translation models.
Here we present another English example I
bought the magazine which Tom recommended
yesterday. This sentence is segmented into clauses
as follows.
3Although a full search is available when the number of
clauses is small, we employ a greedy search in this paper.
422
? I bought the magazine s0 .
? which Tom recommended yersterday
These clauses are translated into Japanese:
? watashi (I) wa (topic marker) s0
zasshi (magazine) o (direct object marker)
kat (buy) ta (past tense marker).
? tom ga (subject marker) kino (yesterday)
susume (recommend) ta (past tense marker)
3.5 Sentence Reconstruction
We reconstruct the target sentence from the clause
translations, based on non-terminals. Starting
from the clause translation of the top clause, we re-
cursively replace non-terminal symbols with their
corresponding clause translations. Here, if a non-
terminal is eventually deleted in SMT decoding,
we simply concatenate the translation behind its
parent clause.
Using the example above, we replace the non-
terminal symbol s0 with the second clause and
obtain the Japanese sentence:
watashi wa tom ga kino susume ta zasshi o kat ta .
4 Experiment
We conducted the following experiments on the
English-to-Japanese translation of research paper
abstracts in the medical domain. Such techni-
cal documents are logically and formally writ-
ten, and sentences are often so long and syntac-
tically complex that their translation needs long
distance reordering. We believe that the medical
domain is suitable as regards evaluating the pro-
posed method.
4.1 Resources
Our bilingual resources were taken from the med-
ical domain. The parallel corpus consisted of
research paper abstracts in English taken from
PubMed4 and the corresponding Japanese transla-
tions.
The training portion consisted of 25,500 sen-
tences (no-clause-seg.; original sentences with-
out clause segmentation). 4,132 English sen-
tences in the corpus were composed of multi-
ple clauses and were separated at the clause level
4http://www.ncbi.nlm.nih.gov/pubmed/
by the procedure in section 3.1. As the syntac-
tic parser, we used the Enju5 (Miyao and Tsu-
jii, 2008) English HPSG parser. For these train-
ing sentences, we automatically aligned Japanese
words with each English clause as described in
section 3.2 and developed a clause-level aligned
corpus, called auto-aligned corpus. We prepared
manually-aligned (oracle) clauses for reference,
called oracle-aligned clauses. The clause align-
ment error rate of the auto-aligned corpus was
14% (number of wrong clause assignments di-
vided by total number of words). The develop-
ment and test portions each consisted of 1,032
multi-clause sentences. because this paper focuses
only on multi-clause sentences. Their English-
side was segmented into clauses in the same man-
ner as the training sentences, and the development
sentences had oracle clause alignment for MERT.
We also used the Life Science Dictionary6 for
training. We extracted 100,606 unique English
entries from the dictionary including entries with
multiple translation options, which we expanded
to one-to-one entries, and finally we obtained
155,692 entries.
English-side tokenization was obtained using
Enju, and we applied a simple preprocessing that
removed articles (a, an, the) and normalized plu-
ral forms to singular ones. Japanese-side tokeniza-
tion was obtained using MeCab7 with ComeJisyo8
(dictionary for Japanese medical document tok-
enization). Our resource statistics are summarized
in Table 1.
4.2 Model and Decoder
We used two decoders in the experiments,
Moses9 (Koehn et al, 2007) and our in-
house hierarchical phrase-based SMT (almost
equivalent to Hiero (Chiang, 2007)). Moses
used a phrase table with a maximum phrase
length of 7, a lexicalized reordering model with
msd-bidirectional-fe, and a distortion
limit of 1210. Our hierarchical phrase-based SMT
used a phrase table with a maximum rule length of
7 and a window size (Hiero?s ?) of 12 11. Both
5http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
6http://lsd.pharm.kyoto-u.ac.jp/en/index.html
7http://mecab.sourceforge.net/
8http://sourceforge.jp/projects/comedic/ (in Japanese)
9http://www.statmt.org/moses/
10Unlimited distortion was also tested but the results were
worse.
11A larger window size could not be used due to its mem-
ory requirements.
423
Table 1: Data statistics on training, development,
and test sets. All development and test sentences
are multi-clause sentences.
Training
Corpus Type #words #sentences
Parallel E 690,536
(no-clause-seg.) J 942,913
25,550
Parallel E 135,698
(auto-aligned) J 183,043
4,132
(oracle-aligned) J 183,147
(10,766 clauses)
E 263,175 155.692Dictionary
J 291,455 (entries)
Development
Corpus Type #words #sentences
Parallel E 34,417 1,032
(oracle-aligned) J 46,480 (2,683 clauses)
Test
Corpus Type #words #sentences
Parallel E 34,433 1,032
(clause-seg.) J 45,975 (2,737 clauses)
decoders employed two language models: a word
5-gram language model from the Japanese sen-
tences in the parallel corpus and a word 4-gram
language model from the Japanese entries in the
dictionary. The feature weights were optimized
for BLEU (Papineni et al, 2002) by MERT, using
the development sentences.
4.3 Compared Methods
We compared four different training and test con-
ditions with respect to the use of clauses in training
and testing. The development (i.e., MERT) condi-
tions followed the test conditions. Two additional
conditions with oracle clause alignment were also
tested for reference.
Table 2 lists the compared methods. First,
the proposed method (proposed) used the auto-
aligned corpus in training and clause segmen-
tation in testing. Second, the baseline method
(baseline) did not use clause segmentation in ei-
ther training or testing. Using this standard base-
line method, we focused on the advantages of the
divide-and-conquer translation itself. Third, we
tested the same translation models as used with
the proposed method for test sentences without
clause segmentation, (comp.(1)). Although this
comparison method cannot employ the proposed
clause-level reordering, it was expected to be bet-
ter than the baseline method because its transla-
tion model can be trained more precisely using the
finely aligned clause-level corpus. Finally, the sec-
ond comparison method (comp.(2)) translated seg-
mented clauses with the baseline (without clause
segmentation) model, as if each of them was a sin-
gle sentence. Its translation of each clause was
expected to be better than that of the baseline be-
cause of the efficient search over shortened inputs,
while its reordering of clauses (non-terminals) was
unreliable due to the lack of clause information
in training. Its sentence reconstruction based on
non-terminals was the same as with the proposed
method. Although non-terminals in the second
comparison method were out-of-vocabulary words
and may be deleted in decoding, all of them sur-
vived and we could reconstruct sentences from
translated clauses throughout the experiments. In
addition, two other conditions were tested: us-
ing oracle-aligned clauses in training: the pro-
posed method trained using oracle-aligned (ora-
cle) clauses and the first comparison method using
oracle-aligned (oracle-comp.) clauses.
4.4 Results
Table 3 shows the results in BLEU, Transla-
tion Edit Rate (TER) (Snover et al, 2006),
and Position-independent Word-error Rate (PER)
(Och et al, 2001), obtained with Moses and our
hierarchical phrase-based SMT, respectively. Bold
face results indicate the best scores obtained with
the compared methods (excluding oracles).
The proposed method consistently outper-
formed the baseline. The BLEU improve-
ments with the proposed method over the base-
line and comparison methods were statistically
significant according to the bootstrap sampling
test (p < 0.05, 1,000 samples) (Zhang et al,
2004). With Moses, the improvement when us-
ing the proposed method was 1.4% (33.19% to
34.60%) in BLEU and 1.3% (57.83% to 56.50%)
in TER, with a slight improvement in PER
(35.84% to 35.61%). We observed: oracle ?
proposed ? comp.(1) ? baseline ? comp.(2)
by the Bonferroni method, where the symbol
A ? B means ?A?s improvement over B is
statistically significant.? With the hierarchical
phrase-based SMT, the improvement was 2.2%
(32.39% to 34.55%) in BLEU, 3.5% (58.36% to
54.87%) in TER, and 1.5% in PER (36.42% to
34.79%). We observed: oracle ? proposed ?
424
Table 2: Compared methods.
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. proposed comp.(2) oracle
no-clause-seg. comp.(1) baseline oracle-comp.
{comp.(1), comp.(2)} ? baseline by the Bon-
ferroni method. The oracle results were better than
these obtained with the proposed method but the
differences were not very large.
4.5 Discussion
We think the advantage of the proposed method
arises from three possibilities: 1) better translation
model training using the fine-aligned corpus, 2) an
efficient decoder search over shortened inputs, and
3) an effective clause-level reordering model real-
ized by using non-terminals.
First, the results of the first comparison method
(comp.(1)) indicate an advantage of the transla-
tion models trained using the auto-aligned corpus.
The training of the translation models, namely
word alignment and phrase extraction, is difficult
for long sentences due to their large ambiguity.
This result suggests that the use of clause-level
alignment provides fine-grained word alignments
and precise translation models. We can also ex-
pect that the model of the proposed method will
work better for the translation of single-clause sen-
tences.
Second, the average and median lengths (in-
cluding non-terminals) of the clause-seg. test set
were 13.2 and 10 words, respectively. They were
much smaller than those of no-clause-seg. at 33.4
and 30 words and are expected to help realize
an efficient SMT search. Another observation is
the relationship between the number of clauses
and translation performance, as shown in Fig-
ure 5. The proposed method achieved a greater im-
provement in sentences with a greater number of
clauses. This suggests that our divide-and-conquer
approach works effectively for multi-clause sen-
tences. Here, the results of the second comparison
method (comp.(2)) with Moses were worse than
the baseline results, while there was an improve-
ment with our hierarchical phrase-based SMT.
This probably arose from the difference between
the decoders when translating out-of-vocabulary
words. The non-terminals were handled as out-of-
vocabulary words under the comp.(2) condition.
52
54
56
58
60
62
64
66
2 4 53
TE
R
 (%
)
The number of clauses
baseline
proposed
comp.(2)
Figure 5: Relationship between TER and number
of clauses for proposed, baseline, and comp.(2)
when using our hierarchical phrase-based SMT.
Moses generated erroneous translations around
such non-terminals that can be identified at a
glance, while our hierarchical phrase-based SMT
generated relatively good translations. This may
be a decoder-dependent issue and is not an essen-
tial problem.
Third, the results obtained with the proposed
method reveal an advantage in reordering in ad-
dition to the previous two advantages. The differ-
ence between the PERs with the proposed method
and the baseline with Moses was small (0.2%)
in spite of the large differences in BLEU and
TER (about 1.5%). This suggests that the pro-
posed method is better in word ordering and im-
plies our method is also effective in reordering.
With the hierarchical phrase-based SMT, the pro-
posed method showed a large improvement from
the baseline and comparison methods, especially
in TER which was better than the best Moses
configuration (proposed). This suggests that the
decoding of long sentences with long-distance
reordering is not easy even for the hierarchical
phrase-based SMT due to its limited window size,
while the hierarchical framework itself can natu-
rally model a long-distance reordering. If we try to
find a derivation with such long-distance reorder-
ing, we will probably be faced with an intractable
search space and computation time. Therefore,
we can conclude that the proposed divide-and-
425
Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,
TER, and PER.
Moses : BLEU (%) / TER (%) / PER (%)
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. 34.60 / 56.50 / 35.61 32.14 / 58.78 / 36.08 35.31 / 55.12 / 34.42
no-clause-seg. 34.22 / 56.90 / 35.20 33.19 / 57.83 / 35.84 34.24 / 56.67 / 35.03
Hierarchical : BLEU (%) / TER (%) / PER (%)
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. 34.55 / 54.87 / 34.79 33.03 / 56.70 / 36.03 35.08 / 54.22 / 34.77
no-clause-seg. 33.41 / 57.02 / 35.86 32.39 / 58.36 / 36.42 33.83 / 56.26 / 34.96
conquer approach provides more practical long-
distance reordering at the clause level.
We also analyzed the difference between auto-
matic and manual clause alignment. Since auto-
aligned corpus had many obvious alignment er-
rors, we suspected these noisy clauses hurt the
clause translation model. However, they were not
serious in terms of final translation performance.
So we can conclude that our proposed divide-and-
conquer approach is promising for long sentence
translation. Although we aimed to see whether we
could bootstrap using existing bilingual corpora in
this paper, we imagine better clause alignment can
be obtained with some supervised classifiers.
One problem with the divide-and-conquer ap-
proach is that its independently-translated clauses
potentially cause disfluencies in final sentence
translations, mainly due to wrong inflections. A
promising solution is to optimize a whole sentence
translation by integrating search of each clause
translation but this may require a much larger
search space for decoding. More simply, we may
be able to approximate it using n-best clause trans-
lations. This problem should be addressed for fur-
ther improvement in future studies.
5 Conclusion
In this paper we proposed a clause-based divide-
and-conquer approach for SMT that can re-
duce complicated clause-level reordering to sim-
ple word-level reordering. The proposed method
separately translates clauses with non-terminals by
using a well-known SMT method and reconstructs
a sentence based on the non-terminals, to reorder
long clauses. The clause translation models are
trained using a bilingual corpus with clause-level
alignment, which can be obtained with an un-
supervised graph-based method using sentence-
aligned corpora. The proposed method improves
the translation of long, multi-clause sentences and
is especially effective for language pairs with
large word order differences, such as English-to-
Japanese.
This paper focused only on clauses as segments
for division. However, other long segments such
as prepositional phrases are similarly difficult to
reorder correctly. The divide-and-conquer ap-
proach itself can be applied to long phrases, and
it is worth pursuing such an extension. As another
future direction, we must develop a more sophis-
ticated method for automatic clause alignment if
we are to use the proposed method for various lan-
guage pairs and domains.
Acknowledgments
We thank the U. S. National Library of Medicine
for the use of PubMed abstracts and Prof. Shuji
Kaneko of Kyoto University for the use of Life
Science Dictionary. We also thank the anonymous
reviewers for their valuable comments.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL, pages 531?540.
426
Osamu Furuse, Setsuo Yamada, and Kazuhide Ya-
mamoto. 1998. Splitting long or ill-formed in-
put for robust spoken-language translation. In Proc.
COLING-ACL, pages 421?427.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. NAACL, pages 273?280.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proc. HLT-NAACL, pages 105?
112.
Jason Katz-Brown and Michael Collins. 2008. Syntac-
tic reordering in preprocessing for Japanese-English
translation: MIT system description for NTCIR-7
patent translation task. In Proc. NTCIR-7, pages
409?414.
Yeun-Bae Kim and Terumasa Ehara. 1994. A method
for partitioning of long Japanese sentences with sub-
ject resolution in J/E machine translation. In Proc.
International Conference on Computer Processing
of Oriental Languages, pages 467?473.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 263?270.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proc. ACL, pages 720?727.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String alignment template for statistical machine
translation. In Proc. Coling-ACL, pages 609?616.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
Coling-ACL, pages 25?32.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statis-
tical machine translation. In Proc. the ACL Work-
shop on Data-Driven Methods in Machine Transla-
tion, pages 55?62.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Proc.
HLT-NAACL, pages 101?104.
Erik F. Tjong, Kim Sang, and Herve? De?jean. 2001. In-
troduction to the CoNLL-2001 shared task: Clause
identification. In Proc. CoNLL, pages 53?57.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007?1016.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. COLING, pages 508?514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for Subject-Object-Verb languages. In Proc.
HLT-NAACL, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL,
pages 523?530.
Ying Zhang, Stephan Vogel, and Alex Weibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proc. LREC, pages 2051?2054.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proc. ICML, pages
912?919.
427
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 11?18,
Beijing, August 2010
Using Goi-Taikei as an Upper Ontology to Build a Large-Scale Japanese
Ontology from Wikipedia
Masaaki Nagata
NTT Communication Science
Laboratories
nagata.masaaki@labs.ntt.co.jp
Yumi Shibaki and Kazuhide Yamamoto
Nagaoka University of
Technology
{shibaki,yamamoto}@jnlp.org
Abstract
We present a novel method for build-
ing a large-scale Japanese ontology from
Wikipedia using one of the largest
Japanese thesauri, Nihongo Goi-Taikei
(referred to hereafter as ?Goi-Taikei?) as
an upper ontology. First, The leaf cat-
egories in the Goi-Taikei hierarchy are
semi-automatically aligned with seman-
tically equivalent Wikipedia categories.
Then, their subcategories are created au-
tomatically by detecting is-a links in the
Wikipedia category network below the
junction using the knowledge defined in
Goi-Taikei above the junction. The re-
sulting ontology has a well-defined taxon-
omy in the upper level and a fine-grained
taxonomy in the lower level with a large
number of up-to-date instances. A sam-
ple evaluation shows that the precisions of
the extracted categories and instances are
92.8% and 98.6%, respectively.
1 Introduction
In recent years, we have become increasingly
aware of the need for up-to-date knowledge bases
offering broad-coverage in order to implement
practical semantic inference engines for advanced
applications such as question answering, summa-
rization and textual entailment recognition. One
promising approach involves automatically ex-
tracting a large comprehensive ontology from
Wikipedia, a freely available online encyclopedia
with a wide variety of information. One problem
with previous such efforts is that the resulting on-
tology is either fragmentary or trivial.
Ponzetto and Strube (2007) presents a set of
lightweight heuristics such as head matching and
modifier matching for distinguishing between is-
a and not-is-a links in the Wikipedia category
network. The most powerful heuristics is head
matching in which a category link is labeled as
is-a if the two categories share the same head
lemma, such as CAPITALS IN ASIA and CAPI-
TALS. For Japanese, Sakurai et al (2008) present
a method equivalent to head matching in Japanese.
As Japanese is a head final language, they intro-
duced a heuristics called suffix matching in which
a category link is labeled as is-a if one category
is the suffix of the other category, such as  
 (airports in Japan) and  (airports). The
problem with the ontology extracted by these two
methods is that it is not a single interconnected
taxonomy, but a set of taxonomic trees.
One way to make a single taxonomy is to use
an existing large-scale taxonomy as a core for the
resulting ontology. In YAGO, Suchanek et al
(2007) merged English WordNet and Wikipedia
by adding instances (namely Wikipedia articles)
to the is-a hierarchy of WordNet. Of the cate-
gories assigned to a Wikipedia article, they re-
garded one with a plural head noun as the article?s
hypernym, which is called a conceptual category.
They then linked the conceptual category to a
WordNet synset by heuristic rules including head
matching. For Japanese, Kobayashi et al (2008)
present an attempt equivalent to YAGO, where
they merged Goi-Taikei and Japanese Wikipedia.
The problem with these two methods is that the
core taxonomy is extended only one level al-
though many new instances are added. They can-
not make the most of the fine-grained taxonomic
11
information contained in the Wikipedia category
network.
In this paper, we present a novel method for
building a single interconnected ontology from
Wikipedia, with a fine-grained taxonomy in the
lower level, by using a manually constructed the-
saurus as its upper ontology. In the following
sections, we first describe the language resources
used in this work. We then describe a semi-
automatic method for building the ontology and
report our experimental results.
2 Language Resources
2.1 Nihongo Goi-Taikei
Nihongo Goi-Taikei (     , ?compre-
hensive outline of Japanese vocabulary?)1 is one
of the largest and best known Japanese thesauri
(Ikehara et al, 1997). It was originally developed
as a dictionary for a Japanese-to-English machine
translation system in the early 90?s. It was then
published as a book in 5 volumes in 1997 and as
a CD-ROM in 1999. It contains about 300,000
Japanese words and the meanings of each word
are described by using 2,715 hierarchical seman-
tic categories. Each word has up to 5 semantic
categories in order of frequency in use, and each
category is assigned with a unique ID number and
category name such as 4:person and 388:place2.
Goi-Taikei has different semantic category hi-
erarchies for common nouns, proper nouns, and
verbs, respectively. We used only the common
noun category in this work. For simplicity, we
mapped all proper nouns in the proper noun cate-
gory to the equivalent common noun category us-
ing the category mapping table shown in the Goi-
Taikei book.
Figure 1 shows the top three layers for common
nouns3. For example, the transliterated Japanese
word raita ( 
	 ) has two semantic cate-
gories 353:author and 915:household appli-
ance. The former originates with the English
1Referred to as ?Goi-Taikei? unless otherwise noted.
2We use Sans Serif for the Goi-Taikei category and
SMALL CAPS for the Wikipedia category. The Goi-Taikei
category is prefixed with ID number.
3The maximum depth of the common noun hierarchy is
12. Most links are is-a relations, but some are part-of rela-
tions, which are explicitly marked
word ?writer? while the latter originates with En-
glish word ?lighter?. By climbing up the Goi-
Taikei category hierarchy, we can infer that the
former refers to a human being (4:person) while
the latter refers to a physical object (533:con-
crete object).
2.2 Japanese Wikipedia
Wikipedia is a free, multilingual, on-line ency-
clopedia actively developed by a large number of
volunteers. Japanese Wikipedia now has about
500,000 articles. Figure 2 shows examples of an
article page and a category page. An article page
has a title, body, and categories. In most articles,
the first sentence of the body gives the definition
of the title. A category also has a title, body, and
categories. Its title is prefixed with ?Category:?
and its body includes a list of articles that belong
to the category.
Although the Wikipedia category system is or-
ganized in a hierarchal manner, it is not a tax-
onomy but a thematic classification. An article
could belong to many categories and the category
network has loops. The relations between linked
categories are chaotic, but the lower the category
link is in the hierarchy, the more it is likely to be
an is-a relation. For example, the category link
between  (COCKTAIL) and  (ALCO-
HOLIC BEVERAGE) is an is-a relation. Although
the article 	 (shaker) is in the category
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 1?9,
Beijing, August 2010
Constructing Large-Scale Person Ontology from Wikipedia 
Yumi Shibaki 
Nagaoka University of 
Technology 
shibaki@jnlp.org 
Masaaki Nagata 
NTT Communication 
Science Laboratories 
nagata.masaaki@ 
labs.ntt.co.jp 
Kazuhide Yamamoto 
Nagaoka University of 
Technology 
yamamoto@jnlp.org 
Abstract 
This paper presents a method for con-
structing a large-scale Person Ontology 
with category hierarchy from Wikipe-
dia. We first extract Wikipedia category 
labels which represent person (hereafter, 
Wikipedia Person Category, WPC) by 
using a machine learning classifier. We 
then construct a WPC hierarchy by de-
tecting is-a relations in the Wikipedia 
category network. We then extract the 
titles of Wikipedia articles which 
represent person (hereafter, Wikipedia 
person instance, WPI). Experiments 
show that the accuracy of WPC extrac-
tion is 99.3% precision and 98.4% re-
call, while that of WPI extraction is 
98.2% and 98.6%, respectively. The ac-
curacies are significantly higher than 
the previous methods. 
1  Introduction 
In recent years, we have become increasingly 
aware of the need for, up-to-date knowledge 
bases offering broad coverage in order to im-
plement practical semantic inference engines 
for advanced applications such as question 
answering, summarization and textual entail-
ment recognition. General ontologies, such as 
WordNet (Fellbaum et al, 1998), and Nihongo 
Goi-Taikei (Ikehara et al, 1997), contain gen-
eral knowledge of wide range of fields. How-
ever, it is difficult to instantly add new know-
ledge, particularly proper nouns, to these gen-
eral ontologies. Therefore, Wikipedia has 
come to be used as a useful corpus for know-
ledge extraction because it is a free and large-
scale online encyclopedia that continues to be 
actively developed. For example, in DBpedia 
(Bizer et al 2009), RDF triples are extracted 
from the Infobox templates within Wikipedia 
articles. In YAGO (Suchanek et al 2007), an 
appropriate WordNet synset (most likely cate-
gory) is assigned to a Wikipedia category as a 
super-category, and Wikipedia articles are ex-
tracted as instances of the category.  
As a first step to make use of proper noun 
and related up-to-date information in Wikipedia, 
we focus on person names and the articles and 
categories related to them because it contains a 
large number of articles and categories that in-
dicate person, and because large-scale person 
ontology is useful for applications such as per-
son search and named entity recognition. Ex-
amples of a person article are personal name 
and occupational title such as ?Ichiro? and ?Fi-
nancial planner,? while an example of a person 
category is occupational title such as 
?Sportspeople.? 
The goal of this study is to construct a large-
scale and comprehensive person ontology by 
extracting person categories and is-a relations1 
among them. We first apply a classifier based 
on machine learning to all Wikipedia categories 
to extract categories that represent person. If 
both of the linked Wikipedia categories are per-
son categories, the category link is labeled as 
an is-a relation. We then use a heuristic-based 
rule to extract the title of articles that represent 
person as person instance from the person cate-
gories. 
In the following sections, we first describe 
the language resources and the previous works. 
We then introduce our method for constructing 
the person ontology and report our experimen-
tal results. 
                                                 
1 ?is-a relation? is defined as a relation between A and B 
when ?B is a (kind of) A.? 
1
2 Language Resources 
2.1  Japanese Wikipedia 
Wikipedia is a free, multilingual, on-line en-
cyclopedia that is being actively developed by a 
large number of volunteers. Wikipedia has ar-
ticles and categories. The data is open to the 
public as XML files2. Figure 1 shows an exam-
ple of an article. An article page has a title, 
body, and categories. In most articles, the first 
sentence of the body is the definition sentence 
of the title.  Although the Wikipedia category 
system is organized in a hierarchal manner, it is 
a thematic classification, not a taxonomy. The 
relation between category and subcategory and 
that between a category and articles listed on it 
are not necessarily an is-a relation. A category 
could have two or more super categories and 
the category network could have loops.  
 
?????????Michelle Wie, 1989?10?11?- ?
???????????
Michelle Wie (Michelle Wie, born October 11, 
1989 ) is a golf player. 
Category : American golfers | 1989 births
Mi lle Wie
category
title of article
definitio  sentence
 
Figure 1: Example of title, body (definition 
sentence), and categories for article page in 
Japanese Wikipedia (top) and its translation 
(bottom) 
2.2 Nihongo Goi-Taikei  
To construct the ontology, we first apply a ma-
chine learning based classifier to determine if a 
category label indicates a person or not. A Wi-
kipedia category label is often a common com-
pound noun or a noun phrase, and the head 
word of a Japanese compound noun and noun 
phrase is usually the last word. We assume the 
semantic category of the last word is an impor-
tant feature for classification.  
Nihongo Goi-Taikei (hereafter, Goi-Taikei) 
is one of the largest and best known Japanese 
thesauri. Goi-Taikei contains different semantic 
category hierarchies for common nouns, proper 
nouns, and verbs. In this work, we use only the 
                                                 
2http://download.wikimedia.org/jawiki 
common noun category (Figure 2). It consists 
of approximately 100,000 Japanese words (he-
reafter, instance) and the meanings of each 
word are described by using about 2,700 hie-
rarchical semantic categories. Words (In-
stances) with multiple meanings (ambiguous 
words) are assigned multiple categories in Goi-
Taikei. For example, the transliterated Japanese 
word (instance) raita (???? ) has two 
meanings of ?writer? and ?lighter,? and so be-
longs to two categories, ?353:author 3 ? and 
?915:household.?  
Japanese WordNet (approximately 90,000 
entries as of May 2010), which has recently 
been released to the public (Bonds et al, 2008), 
could be an alternative to Goi-Taikei as a large-
scale Japanese thesaurus. We used Goi-Taikei 
in this work because Japanese WordNet was 
translated from English WordNet and it is not 
known whether it covers the concepts unique to 
Japanese. 
3 Previous Works 
3.1 Ponzetto?s method and Sakurai?s me-
thod 
Ponzetto et al (2007) presented a set of 
lightweight heuristics such as head matching 
and modifier matching for distinguishing is-a 
links from not-is-a links in the Wikipedia cate-
gory network. The main heuristic, ?Syntax-
based methods? is based on head matching, in 
which a category link is labeled as is-a relation 
if the two categories share the same head lem-
ma, such as CAPITALS IN ASIA and CAPI-
TALS. Sakurai et al (2008) presented a method 
equivalent to head matching for Japanese Wi-
kipedia. As Japanese is a head final language, 
they introduced the heuristic called suffix 
matching; it labels a category link as a is-a rela-
tion if one category is the suffix of the other 
category, such as ?????(airports in Ja-
pan) and ??(airports). In the proposed me-
thod herein, if a Wikipedia category and its 
parent category are both person categories, the 
category link is labeled as is-a relation. There-
fore, is-a relations, which cannot be extracted 
by Ponzetto?s or Sakurai?s method, can be ex-
tracted. 
                                                 
3 The Goi-Taikei category is prefixed with ID number. 
2
246:personalities
and competitors
5:humans 223:officials 219:semi-man
249:actor 251:competitor
453:shrine 221:spirit
4:people
151:ethnic group
152:ethnic group 153:race 55:boy 56:girl
1:common noun
2:concrete
1000:abstract
3:agents 388:places 533:objects
362:organizations 389:facilities 468:nature 534:animate
1235:events
1936:job
1937:business1939:occupation 
1065:title
1069:number1066:name
2483:nature 2507:state
385:nation383:assembly
Writer?????
353:author 915:household appliance
lighter?????
706:inanimate
Semantic category hierarchy for common nouns
1236:human
activities
1001:abstract
things
2422:abstract
relationship
About 2,700 categories
About 100,000 instances
 
Figure 2: Part of a category hierarchy for common nouns in Nihongo Goi-Taikei 
3.2 Kobayashi?s method 
Kobayashi et al (2008) presented a tech-
nique to make a Japanese ontology equivalent 
to YAGO; it assigns Goi-Taikei categories to 
Japanese Wikipedia categories. These two me-
thods and our method are similar in that a Wi-
kipedia category and the title of an article are 
regarded as a category and an instance, respec-
tively. Kobayashi et al automatically extract 
hypernyms from the definition sentence of each 
article in advance (referred to hereafter as ?D-
hypernym.?) They apply language-dependent 
lexico-syntactic patterns to the definition sen-
tence to extract the D-hypernym. Here are some 
examples. 
 
??[hypernym]?????? <EOS> 
one of [hypernym] 
 
??[hypernym]???<EOS> 
is a [hypernym] 
 
 [hypernym] <EOS> 
is a [hypernym] ? 
 
where <EOS> refer to the beginning of a 
sentence 
For example, from the article in Figure 1, the 
words ?????? (golf player)? is extracted 
as the D-hypernym of the article ??????
??? (Michelle Wie).? 
Figure 3 outlines the Kobayashi?s method. 
First, for a Wikipedia category, if its last word 
matches an instance of Goi-Taikei category, all 
such Goi-Taikei categories are extracted as a 
candidate of the Wikipedia category?s super-
class. If the last word of the D-hypernym of the 
Wikipedia article listed on the Wikipedia cate-
gory matches an instance of the Goi-Taikei cat-
egory, the Goi-Taikei category is extracted as 
the super-class of the Wikipedia category and 
its instances (Wikipedia articles) (Figure 3). 
Although the Kobayashi?s method is a general 
one, it can be used to construct person ontology 
if the super-class candidates are restricted to 
those Goi-Taikei categories which represent 
person. Titl             ????????
Michelle Wie
Hypernym   ?????
Golf player
?????????????
American golfers
Wikipedia category
Match
last word
Person category
?_person
?????_golfer
??_player
???_artist
?
?
?
Goi-Taikei
Wikipedia article
Title            ALPG???
ALPG Tour
Hypernym   ??????
Golf tour
Doesn?t 
match
Wikipedia article ?
 
Figure 3: The outline of Kobayashi?s method 
3.3 Yamashita?s method 
Yamashita made an open source software 
which extracts personal names from Japanese 
Wikipedia4. He extracted the titles of articles 
listed on the categories ???(? births) (e.g., 
2000 births). As these categories are used to 
sort the names of people, horses, and dogs by 
born year, he used a simple pattern matching 
                                                 
4http://coderepos.org/share/browser/lang/perl/misc/wikipe
jago 
3
rules to exclude horses and dogs. In the expe-
riment in Section 5, we implemented his me-
thod by using not only ??? (births)? but also 
???  (deaths)? and ????  (th-century 
deaths),? ????  (s deaths),? ????  (s 
births),? and ????  (th births)? to extract 
personal names. As far as we know, it is the 
only publicly available software to extract a 
large number of person names from the Japa-
nese Wikipedia. For the comparison with our 
method, it should be noted that his method 
cannot extract person categories. 
4 Ontology Building Method 
4.1 Construction of Wikipedia person cat-
egory hierarchy (WPC) 
We extract the WPC by using a machine learn-
ing classifier. If a Wikipedia category and its 
parent category are both person categories, the 
category link is labeled as an is-a relation. This 
means that all is-a relations in our person on-
tology are extracted from the original Wikipe-
dia category hierarchy using only a category 
classifier. This is because we investigated 
1,000 randomly sampled links between person 
categories and found 98.7% of them were is-a 
relations. Figure 4 shows an example of the 
Wikipedia category hierarchy and the con-
structed WPC hierarchy. 
 
Music Technology
Composers
Broadcasting
Wikipedia person
category (WPC)
Announcer productions
Announcers
is-a
is-a
is-a
is-a
Category without
parent and child
R ot category
Musicians
Conductors
Engineers
Announcers
Musicians
Conductors
Composers
Japanese conductors
Engineers
Japanese conductors
 
Figure 4: Example of Wikipedia category hie-
rarchy (top) and constructed Wikipedia person 
category hierarchy (bottom) 
We detect whether the Wikipedia category 
label represents a person by using Support Vec-
tor Machine (SVM). The semantic category of 
the words in the Wikipedia category label and 
those in the neighboring categories are used for 
the features. We use the following three aspects 
of the texts that exist around the target category 
for creating the features: 
 
1. Structural relation between the target cat-
egory and the text in Wikipedia.  (6 kinds) 
 
2.  Span of the text.  (2 kinds) 
 
3. Semantic category of the text derived 
from Goi-Taikei. (4 kinds) 
 
We examined 48 features by combining the 
above three aspects (6*2*4). 
   The following are the six structural relations 
in Wikipedia between the target category and 
the text information: 
 
Structural relation 
A. The target Wikipedia category label.  
 
B. All parent category labels of the target cat-
egory.  
 
C. All child category labels of the target   cat-
egory.  
 
D. All sibling category labels of the target 
category.  
 
E. All D-hypernym5 from each article listed on 
the target category.  
 
F. All D-hypernyms extracted from the ar-
ticles with the same name as the target cate-
gory. 
 
As for F, for example, when the article ??
???(bassist) is listed on the category: ?
? ? ? ? (bassist), we regard the D-
hypernym of the article as the hypernym of  
the category. 
 
As most category labels and D-hypernyms are 
common nouns, they are likely to match in-
stances in Goi-Taikei which lists possible se-
mantic categories of words.  
                                                 
5As for D-hypernym extraction patterns, we used almost 
the same patterns described in previous works on Japa-
nese sources such as (Kobayashi et al 2008; Sumida et al, 
2008), which are basically equivalent to the works on 
English sources such as (Hearst, 1992). 
4
   After the texts located at various structural 
relations A-F are collected, they are matched to 
the instances of Goi-Taikei in two different 
spans: 
 
Span of the text 
?. All character strings of the text 
 
?. The last word of the text 
 
For the span ?, the text is segmented into 
words using a Japanese morphological analyzer. 
The last word is used because the last word 
usually represents the meaning of the entire 
noun phrase (semantic head word) in Japanese.  
In the proposed method, hierarchical seman-
tic categories of Goi-Taikei are divided into 
two categories; ?Goi-Taikei person categories? 
and other categories. Goi-Taikei person catego-
ry is defined as those categories that represent 
person, that is, all categories under ?5:humans? 
and ?223:officials,? and ?1939: occupation? 
and ?1066:name? in Goi-Taikei hierarchy as 
shown in Figure 1.  
For each structural relation A-F  and span ? 
and ?, we calculate four relative frequencies 
a-d, which represents the manner in which the 
span of texts match the instance of Goi-Taikei 
person category. It basically indicates the de-
gree to which the span of text is likely to mean 
a person.  
 
Semantic type 
a. The span of text matches only instances of 
Goi-Taikei person categories. 
 
b. The span of text matches only instances of 
categories other than Goi-Taikei person cat-
egories. 
 
c. The span of text matches both instances of 
Goi-Taikei person categories and those of 
other categories. 
 
d. The span of text does not match any in-
stances of Goi-Taikei. 
 
For example, when the target category is ???
?? (musicians) in Figure 5 and the feature in 
question is B-? (the last word of its parent 
categories), the word ??? (whose senses are 
family and house) falls into semantic type c, 
and the word ???? (music) falls into seman-
tic type b. Therefore, the frequency of semantic 
types a, b, c, d are 0, 1, 1, 0, respectively, in the 
features related to B-?, and the relative fre-
quencies used for the feature value related B-? 
are 0, 0.5, 0.5, 0, respectively. In this way, we 
use 48 relative frequencies calculated from the 
combinations of structural relation A-F, span 
? and ?, and semantic type a-d, as the feature 
vector for the SVM.  
 ?Target category
?Similar category
?Last word
???_Artists ??_Music
???_Musicians
??????_Jazz composers
???_Composers
???_Musicians by instrument
?? Art ??????_People by occupation
 
Figure 5: Example of Wikipedia category hie-
rarchy when the target category is ????? 
4.2  Similar category 
In Wikipedia, there are categories that do not 
have articles and those with few neighboring 
categories. Here, we define the neighboring 
categories for a category as those categories 
that can be reached through a few links from 
the category. In these cases, there is a possibili-
ty that there is not enough text information 
from which features (mainly semantic category 
of words) can be extracted, which could de-
grade the accuracy. 
The proposed method overcomes this prob-
lem by detecting categories similar to the target 
category (the category in question) from its 
neighboring categories for extracting sufficient 
features to perform classification. Here, "simi-
lar category" is defined as parent, child, and 
sibling categories whose last word matches the 
last word of the target category. This is because 
there is a high possibility that the similar cate-
gories and the target category have similar 
meaning if they share the same last word in the 
category labels. If the parent (child) category is 
determined as a similar category, its parent 
(child) category is also determined as a similar 
category if the last word is the same. The pro-
cedure is repeated as long as they share the 
same last word.  
Figure 5 shows an example of similar cate-
gories when the target category is ?Musicians.? 
In this case, features extracted from A-F of 
5
similar categories are added to features ex-
tracted using A-F of the target category, ?Mu-
sicians.? For example, similar category ?Art-
ists? has ?Art? and ?People by occupation? as 
B (parent categories of the target category) in 
Figure 5, therefore ?Art? and ?People by occu-
pation? are added to B of ?Musicians.? 
4.3 Extracting Wikipedia person instance 
(WPI) 
The proposed method extracts, as WPIs the 
titles of articles listed as WPCs that meet the 
following four requirements.  
 
1. The last word of the D-hypernym of the 
title of the Wikipedia article matches an in-
stance of Goi-Taikei person category.  
 
2. The last word of the title of Wikipedia ar-
ticle matches an instance of Goi-Taike per-
son category. 
 
3. At least one of the Wikipedia categories as-
signed to the Wikipedia article matches the 
following patterns: 
 
(??|???|???|??|???|???)<EOS> 
( deaths | th-century deaths | ?s deaths | births | th-births | ?s 
births ) <EOS> 
 
These categories are used to sort a large 
number of person names by year.  
 
 
4. Wikipedia categories assigned to the Wiki-
pedia article satisfy the following condition: 
 
   
5.0categories  Wikipediaofnumber  All
4.1Section in   WPCsextracted ofNumber ???
 
 
This condition is based on the observation 
that the more WPCs a Wikipedia article is 
assigned to, the more it is likely to be a WPI. 
We set the threshold 0.5 from the results of a 
preliminary experiment.  
5 Experiments  
5.1 Experimental setup 
We used the XML file of the Japanese Wiki-
pedia as of July 24, 2008. We removed irrele-
vant pages by using keywords (e.g., ?image:,? 
?Help:?) in advance. This cleaning yielded 
477,094 Wikipedia articles and 39,782 Wiki-
pedia categories. We manually annotated each 
category to indicate whether it represents per-
son (positive) or not (negative). For ambiguous 
cases, we used the following criteria:  
 
?Personal name by itself (e.g., Michael Jack-
son) is not regarded as WPC because usually 
it does not have instances. (Note: personal 
name as article title is regarded as WPI. )  
 
?Occupational title (e.g., Lawyers) is regarded 
as WPC because it represents a person. 
 
?Family (e.g., Brandenburg family) and Eth-
nic group (e.g., Sioux) are regarded as WPC. 
 
?Group name (e.g., The Beatles) is not re-
garded as WPC. 
 
In order to develop a person category classifier, 
we randomly selected 2,000 Wikipedia catego-
ries (positive:435, negative:1,565) from all cat-
egories for training6. We used the remaining 
37,767 categories for evaluation. To evaluate 
WPI extraction accuracy, we used Wikipedia 
articles not listed on the Wikipedia categories 
used for training. 417,476 Wikipedia articles 
were used in the evaluation.  
To evaluate our method, we used TinySVM-
0.09 7  with a linear kernel for classification, 
and the Japanese morphological analyzer JU-
MAN-6.0 8  for word segmentation. The com-
parison methods are Kobayashi?s method and 
Yamashita?s method under the same conditions 
as our method. 
5.2 Experimental results  
Table 1 shows the WPCs extraction accuracy. 
Precision and recall of proposed method are 6.5 
points and 14.8 points better than those of Ko-
bayashi's method, respectively. 
 
Precision Recall F-measure
Kobayashi?s
method
92.8%
(6727/7247)
83.6%
(6727/8050)
88.0%
Proposed
method
99.3%
(7922/7979)
98.4%
(7922/8050)
98.8%
 
Table 1: The Wikipedia person categories 
(WPCs) extraction accuracy 
                                                 
6We confirmed that the accuracy will level off about 
2,000 training data by experiment. Details will be de-
scribed in Section 6. 
7http://chasen.org/~taku/software/TinySVM/ 
8http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman.html 
6
To confirm our assumption on the links be-
tween WPCs, we randomly selected 1,000 pairs 
of linked categories from extracted WPCs, and 
manually investigated whether both 
represented person and were linked by is-a re-
lation. We found that precision of these pairs 
was 98.3%. 
Errors occurred when the category link be-
tween  person categories in the Wikipedia cate-
gory network was not an is-a relation, such as 
???(Chiba clan) ? ????(Ohsuga clan). 
However, this case is infrequent, because 
98.7% of the links between person categories 
did exhibit an is-a relation (as described in Sec-
tion 4.1).  
Table 2 shows the WPIs extraction accuracy. 
We randomly selected 1,000 Wikipedia articles 
from all categories in Wikipedia, and manually 
created evaluation data (positive:281, nega-
tive:719). The recall of the proposed method 
was 98.6%, 21.0 points higher than that of Ya-
mashita?s method. Our method topped the F-
measure of Kobayashi?s method by 3.4 points. 
Among 118,552 extracted as WPIs by our me-
thod, 116,418 articles were expected be correct. 
In our method, errors occurred when WPI was 
not listed on any WPCs. However, this case is 
very rare. Person instances are almost always 
assigned to at least one WPC. Thus, we can 
achieve high coverage for WPIs even if we fo-
cus only on WPCs. We randomly selected 
1,000 articles from all articles and obtained 277 
person instances by a manual evaluation. Fur-
thermore, we investigated the 277 person in-
stances, and found that only two instances were 
not classified into any WPCs (0.7%). 
 
Precision Recall F-measure
Yamashita's
method
100.0%
(218/218)
77.6%
(218/281)
87.4%
Kobayashi's
method
96%
(264/275)
94.0%
(264/281)
95.0%
Proposed
method
98.2%
(277/282)
98.6%
(277/281)
98.4%
Table 2: The Wikipedia person instance 
(WPIs) extraction accuracy 
 
Table 3 shows the extracted WPC-WPI pairs 
(e.g., American golfers-Michelle Wie, Artists-
Meritorious Artist) extraction accuracy. We 
randomly selected 1,000 pairs of Wikipedia 
category and Wikipedia article from all such 
pairs in Wikipedia, and manually investigated 
whether both category and article represented a 
person and whether they were linked by an is-a 
relation (positive:296, negative:704). Precision 
and recall of proposed method are 2.1 points 
and 11.8 points higher than those of Kobaya-
shi's method, respectively. Among all 274,728 
extracted as WPC-WPI pairs by our method, 
269,233 was expected be correct. 
 
Precision Recall F-measure
Kobayashi?s
method
95.9%
(259/270)
87.5%
(259/296)
91.5%
Proposed
method
98.0%
(294/300)
99.3%
(294/296)
98.7%
Table 3: The extraction accuracy of the pairs 
of Wikipedia person category and person in-
stance (WPC-WPI) 
6 Discussions 
We constructed a WPC hierarchy using the 
8,357 categories created by combining ex-
tracted categories and training categories. The 
resulting WPC hierarchy has 224 root catego-
ries (Figure 4). Although the majority of the 
constructed ontology is interconnected, 194 
person categories had no parent or child (2.3 % 
of all person categories). In rare cases, the cat-
egory network has loops (e.g., ?Historians? and 
?Scholars of history? are mutually interlinked).  
Shibaki et al (2009) presented a method for 
building a Japanese ontology from Wikipedia 
using Goi-Taikei, as its upper ontology. This 
method can create a single connected taxono-
my with a single root category. We also hope 
to create a large-scale, single-root, and inter-
connected person ontology by using some up-
per ontology.   
Our method is able to extract WPCs that do 
not match any Goi-Taikei instance (e.g., Vi-
olinists and Animators). Furthermore, our me-
thod is able to detect many ambiguous Wikipe-
dia category labels correctly as person category. 
For example, ?????????? (fashion 
model)? is ambiguous because the last word 
???? (model)? is ambiguous among three 
senses: person, artificial object, and abstract 
relation. Kobayashi?s method cannot extract a 
WPC if the last word of the category label does 
not match any instance in Goi-Taikei. Their 
method is error-prone if the last word has mul-
7
tiple senses in Goi-Taikei because it is based on 
simple pattern matching. Our method can han-
dle unknown and ambiguous category labels 
since it uses machine learning-based classifiers 
whose features are extracted from neighboring 
categories. 
Our method can extract is-a person category 
pairs that could not be extracted by Ponzetto et 
al. (2007) and Sakurai et al (2008). Their me-
thods use head matching in which a category 
link is labeled as an is-a relation only if the 
head words of category labels are matched. 
However, our method can extract is-a relations 
without reference to surface character strings, 
such as ????????(Journalists)? and 
?????????(Sports writers).? Among 
all 14,408 Wikipedia category pairs extracted 
as is-a relations in our method, 5,558 (38.6%) 
did not match their head words.  
We investigated the learning curve of the 
machine learning-based classifier for extracting 
WPCs, in order to decide the appropriate 
amount of training data for future updates.  
As we have already manually tagged all 
39,767 Wikipedia categories, we randomly se-
lected 30,000 categories and investigated the 
performance of our method when the number 
of the training data was changed from 1,000 to 
30,000. The evaluation data was the remaining 
9,767 categories.  
 
precision
recall
f-value
100.0
99.0
98.0
97.0
Precision
Recall
F-measure
The number of training data
P
r
e
c
i
s
i
o
n
 /
 R
e
c
a
l
l
 /
 F
-
m
e
a
s
u
r
e
 [
%
]
0 10k 20k 30k
 
Figure 6: The effect of training data size to 
WPC extraction accuracy 
 
Figure 6 shows the precision, recall, and F-
measure for different training data sizes. F-
measure differed only 0.4 points from 1,000 
samples (98.5%) to 30,000 samples (98.9%). 
Figure 6 shows that the proposed method of-
fers high accuracy in detecting WPCs with only 
a few thousand training examples.  
Our method uses similar categories for 
creating features as well as the target Wikipe-
dia category (Section 4.1). We compared the 
proposed method to a variant that does not use 
similar categories to confirm the effectiveness 
of this technique. Furthermore, our method 
uses the Japanese thesaurus, Goi-Taikei, to 
look up the semantic category of the words for 
creating the features for machine learning. We 
also compared the proposed method with the 
one that does not use semantic category (de-
rived from Goi-Taikei) but instead uses word 
surface form for creating features (This one 
uses similar categories).  
   Figure 7 shows the performance of the clas-
sifiers for each type of features. We can clearly 
observe that using similar categories results in 
higher F-measure, regardless of the training 
data size. We also observe that when there is 
little training data, the method using word sur-
face form as features results in drastically low-
er F-measures. In addition, its accuracy was 
consistently lower than the others even if the 
training data size was increased. Therefore, we 
can conclude that using similar category and 
Goi-Taikei are very important for creating good 
features for classification. 
 
????
????
????
Proposed method
Without using similar category
Without using Goi-Taikei
F
-
m
e
a
s
u
r
e
 [
%
]
The number of training data
100.0
96.0
98.0
94.0
0
92.0
90.0
10k 20k 30k
 
Figure 7: The effects of using similar catego-
ries and Goi-Taikei 
 
In future, we will attempt to apply our method 
to other Wikipedia domains, such as organiza-
tions and products. We will also attempt to use 
other Japanese thesauri, such as Japanese 
WordNet. Furthermore, we hope to create a 
large-scale and single connected ontology. As a 
final note, we plan to open the person ontology 
constructed in this paper to the public on Web 
in the near future. 
 
8
 References 
Bizer, C., J. Lehmann, G. Kobilarov, S. Auer, C. 
Becker, R. Cyganiak, and S. Hellmann. 2009.  
?DBpedia - A crystallization point for the web of 
data,? Web Semantics: Science, Services and 
Agents on the World Wide Web, vol. 7, No.3,  
pages 154-165. 
Bond, Francis, Hitoshi Isahara, Kyoko Kanzaki, and 
Kiyotaka Uchimoto. 2008. Boot-strapping a 
wordnet using multiple existing wordnets. In 
Proceedings of the 6th International Conference 
on Language Resources and Evaluation (LREC), 
pages 28-30. 
Fellbaum, Christiane. 1998. WordNet: An Electron-
ic Lexical Database, Language, Speech, and 
Communication Series. MIT Press. 
Hearst, Marti A. 1992. Automatic acquisition of 
hyponyms from large text corpora. In Proceed-
ings of the 14th Conference on Computational 
Linguistics (COLING), pages 539-545. 
Ikehara, Satoru, Masahiro Miyazaki, Satoshi Shi-
rai,Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogu-
ra, Yoshifumi Ooyama, and Yoshihiko Hayashi, 
editors. 1997. Nihongo Goi-Taikei ? a Japanese 
Lexicon. Iwanami Shoten. (in Japanese). 
Kobayashi, Akio, Shigeru Masuyama, and Satoshi 
Sekine. 2008. A method for automatic construc-
tion of general ontology merging goitaikei and 
Japanese Wikipedia. In Information Processing 
Society of Japan (IPSJ) SIG Technical Re-
port2008-NL-187 (in Japanese), pages 7-14. 
Ponzetto, S. P. and Michael Strube. 2007. Deriving 
a large scale taxonomy from Wikipedia. In Pro-
ceedings of the 22nd Conference on the Ad-
vancement of Artificial Intelligence (AAAI), pag-
es 1440?1445. 
Sakurai, Shinya, Takuya Tejima, Masayuki Ishika-
wa, Takeshi Morita, Noriaki Izumi, and Takahira 
Yamaguchi. 2008. Applying Japanese Wikipedia 
for building up a general ontology. In Japanese 
Society of Artificial Intelligence (JSAI) Technical 
Report SIG-SWO-A801-06 (in Japanese), pages 
1-8. 
 Shibaki, Yumi, Masaaki Nagata and Kazuhide Ya-
mamoto. 2009. Construction of General Ontolo-
gy from Wikipedia using a Large-Scale Japanese 
Thesaurus. In Information Processing Society of 
Japan (IPSJ) SIG Technical Report2009-NL-
194-4. (in Japanese). 
 
Suchanek, Fabian M., Gjergji Kasneci, and Ger-
hardWeikum. 2007. Yago: A core of semantic 
knowledge unifying wordnet and Wikipedia. In 
Proceedings of the 16th International Conference 
on World Wide Web (WWW), pages 697-706. 
Sumida, Asuka, Naoki Yoshinaga, and Kentaro To-
risawa. 2008. Boosting precision and recall of 
hyponymy relation acquisition from hierarchical 
layouts in Wikipedia. In Proceedings of the Sixth 
Language Resources and Evaluation Confe-
rence(LREC), pages 28?30. 
9
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 57?66,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Head Finalization Reordering for Chinese-to-Japanese
Machine Translation
Han Dan+ Katsuhito Sudoh? Xianchao Wu??
Kevin Duh?? Hajime Tsukada? Masaaki Nagata?
+The Graduate University For Advanced Studies, Tokyo, Japan
?NTT Communication Science Laboratories, NTT Corporation
+handan@nii.ac.jp, ?wuxianchao@baidu.com, ?kevinduh@is.naist.jp
?{sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp
Abstract
In Statistical Machine Translation, reorder-
ing rules have proved useful in extracting
bilingual phrases and in decoding during
translation between languages that are struc-
turally different. Linguistically motivated
rules have been incorporated into Chinese-
to-English (Wang et al, 2007) and English-
to-Japanese (Isozaki et al, 2010b) transla-
tion with significant gains to the statistical
translation system. Here, we carry out a lin-
guistic analysis of the Chinese-to-Japanese
translation problem and propose one of the
first reordering rules for this language pair.
Experimental results show substantially im-
provements (from 20.70 to 23.17 BLEU)
when head-finalization rules based on HPSG
parses are used, and further gains (to 24.14
BLEU) were obtained using more refined
rules.
1 Introduction
In state-of-the-art Statistical Machine Translation
(SMT) systems, bilingual phrases are the main
building blocks for constructing a translation given
a sentence from a source language. To extract
those bilingual phrases from a parallel corpus,
the first step is to discover the implicit word-
to-word correspondences between bilingual sen-
tences (Brown et al, 1993). Then, a symmetriza-
tion matrix is built (Och and Ney, 2004) by us-
ing word-to-word alignments, and a wide variety
?Now at Baidu Japan Inc.
? Now at Nara Institute of Science and Technology
(NAIST)
of heuristics can be used to extract the bilingual
phrases (Zens et al, 2002; Koehn et al, 2003).
This method performs relatively well when the
source and the target languages have similar word
order, as in the case of French, Spanish, and En-
glish. However, when translating between lan-
guages with very different structures, as in the case
of English and Japanese, or Japanese and Chinese,
the quality of extracted bilingual phrases and the
overall translation quality diminishes.
In the latter scenario, a simple but effective strat-
egy to cope with this problem is to reorder the
words of sentences in one language so that it re-
sembles the word order of another language (Wu
et al, 2011; Isozaki et al, 2010b). The advan-
tages of this strategy are two fold. The first ad-
vantage is at the decoding stage, since it enables
the translation to be constructed almost monoton-
ically. The second advantage is at the training
stage, since automatically estimated word-to-word
alignments are likely to be more accurate and sym-
metrization matrices reveal more evident bilingual
phrases, leading to the extraction of better quality
bilingual phrases and cleaner phrase tables.
In this work, we focus on Chinese-to-Japanese
translation, motivated by the increasing interaction
between these two countries and the need to im-
prove direct machine translation without using a
pivot language. Despite the countries? close cul-
tural relationship, their languages significantly dif-
fer in terms of syntax, which poses a severe diffi-
culty in statistical machine translation. The syntac-
tic relationship of this language pair has not been
carefully studied before in the machine translation
57
field, and our work aims to contribute in this direc-
tion as follows:
? We present a detailed syntactic analysis of
several reordering issues in Chinese-Japanese
translation using the information provided by
an HPSG-based deep parser.
? We introduce novel reordering rules based on
head-finalization and linguistically inspired
refinements to make words in Chinese sen-
tences resemble Japanese word order. We em-
pirically show its effectiveness (e.g. 20.70 to
24.23 BLEU improvement).
The paper is structured as follows. Section 2 in-
troduces the background and gives an overview of
similar techniques related to this work. Section 3
describes the proposed method in detail. Exper-
imental evaluation of the performance of the pro-
posed method is described in section 4. There is an
error analysis on the obtained results in section 5.
Conclusions and a short description on future work
derived from this research are given in the final
section.
2 Background
2.1 Head Finalization
The structure of languages can be characterized
by phrase structures. The head of a phrase is the
word that determines the syntactic category of the
phrase, and its modifiers (also called dependents)
are the rest of the words within the phrase. In En-
glish, the head of a phrase can be usually found
before its modifiers. For that reason, English is
called a head-initial language (Cook and Newson,
1988). Japanese, on the other hand, is head-final
language (Fukui, 1992), since the head of a phrase
always appears after its modifiers.
In certain applications, as in the case of ma-
chine translation, word reordering can be a promis-
ing strategy to ease the task when working with
languages with different phrase structures like En-
glish and Japanese. Head Finalization is a success-
ful syntax-based reordering method designed to re-
order sentences from a head-initial language to re-
semble the word order in sentences from a head-
final language (Isozaki et al, 2010b). The essence
of this rule is to move the syntactic heads to the
end of its dependency by swapping child nodes in
a phrase structure tree when the head child appears
before the dependent child.
Isozaki et al (2010b) proposed a simple method
of Head Finalization, by using an HPSG-based
deep parser for English (Miyao and Tsujii, 2008)
to obtain phrase structures and head information.
The score results from several mainstream evalua-
tion methods indicated that the translation quality
had been improved; the scores of Word Error Rate
(WER) and Translation Edit Rate (TER) (Snover
et al, 2006) had especially been greatly reduced.
2.2 Chinese Deep Parsing
Syntax-based reordering methods need parsed sen-
tences as input. Isozaki et al (2010b) used Enju,
an HPSG-based deep parser for English, but they
also discussed using other types of parsers, such
as word dependency parsers and Penn Treebank-
style parsers. However, to use word dependency
parsers, they needed an additional heuristic rule to
recover phrase structures, and Penn Treebank-style
parsers are problematic because they output flat
phrase structures (i.e. a phrase may have multiple
dependents, which causes a problem of reorder-
ing within a phrase). Consequently, compared to
different types of parsers, Head-Final English per-
forms the best on the basis of English Enju?s pars-
ing result.
In this paper, we follow their observation, and
use the HPSG-based parser for Chinese (Chinese
Enju) (Yu et al, 2011) for Chinese syntactic pars-
ing. Since Chinese Enju is based on the same pars-
ing model as English Enju, it provides rich syn-
tactic information including phrase structures and
syntactic/semantic heads.
Figure 1 shows an example of an XML output
from Chinese Enju for the sentence ?wo (I) qu (go
to) dongjing (Tokyo) he (and) jingdu (Ky-
oto).? The label <cons> and <tok> represent
the non-terminal nodes and terminal nodes, respec-
tively. Each node is identified by a unique ?id?
and has several attributes. The attribute ?head?
indicates which child node is the syntactic head.
In this figure, <head=?c4? id=?c3?> means that
the node that has id=?c4? is the syntactic head of
the node that has id=?c3?.
58
Figure 1: An XML output for a Chinese sentence from
Chinese Enju. For clarity, we only draw information
related to the phrase structure and the heads.
2.3 Related Work
Reordering is a popular strategy for improving
machine translation quality when source and tar-
get languages are structurally very different. Re-
searchers have approached the reordering problem
in multiple ways. The most basic idea is pre-
ordering (Xia and McCord, 2004; Collins et al,
2005), that is, to do reordering during preprocess-
ing time, where the source side of the training and
development data and sentences from a source lan-
guage that have to be translated are first reordered
to ease the training and the translation, respec-
tively. In (Xu et al, 2009), authors used a depen-
dency parser to introduce manually created pre-
ordering rules to reorder English sentences when
translating into five different SOV(Subject-Object-
Verb) languages. Other authors (Genzel, 2010; Wu
et al, 2011) use automatically generated rules in-
duced from parallel data. Tillmann (2004) used a
lexical reordering model, and Galley et al (2004)
followed a syntactic-based model.
In this work, however, we are centered in the
design of manual rules inspired by the Head Final-
ization (HF) reordering (Isozaki et al, 2010b). HF
reordering is one of the simplest methods for pre-
ordering that significantly improves word align-
ments and leads to a better translation quality. Al-
though the method is limited to translation where
the target language is head-final, it requires neither
training data nor fine-tuning. To our knowledge,
HF is the best method to reorder languages when
translating into head-final languages like Japanese.
The implementation of HF method for English-
to-Japanese translation appears to work well. A
reasonable explanation for this is the close match
between the concept of ?head? in this language
pair. However, for Chinese-to-Japanese, there are
differences in the definitions of numbers of impor-
tant syntactic concepts, including the definition of
the syntactic head. We concluded that the diffi-
culties we encountered in using HF to Chinese-to-
Japanese translation were the result of these differ-
ences in the definition of ?head?. As we believe
that such differences are also likely to be observed
in other language pairs, the present work is gener-
ally important for head-initial to head-final trans-
lation as it shows a systematic linguistic analysis
that consistently improves the effectivity of the HF
method.
3 Syntax-based Reordering Rules
This section describes our method for syntax-
based reordering for Chinese-to-Japanese transla-
tion. We start by introducing Head Finalization
for Chinese (HFC), which is a simple adaptation
of Isozaki et al (2010b)?s method for English-to-
Japanese translation. However, we found that this
simple method has problems when applied to Chi-
nese, due to peculiarities in Chinese syntax. In
Section 3.2, we analyze several distinctive cases of
the problem in detail. And following this analysis,
Section 3.3 proposes a refinement of the original
HFC, with a couple of exception rules for reorder-
ing.
3.1 Head Finalization for Chinese (HFC)
Since Chinese and English are both known to be
head-initial languages1, the reordering rule intro-
duced in (Isozaki et al, 2010b) ideally would re-
order Chinese sentences to follow the word order
1As Gao (2008) summarized, whether Chinese is a head-
initial or a head-final language is open for debate. Neverthe-
less, we take the view that most Chinese sentence structures
are head-initial since the written form of Chinese mainly be-
haves as an head-initial language.
59
Figure 2: Simple example for Head-Final Chinese. The left figure shows the parsing tree of the original sentence
and its English translation. The right figure shows the reordered sentence along with its Japanese translation.
( ?*? indicate the syntactic head).
of their Japanese counterparts.
Figure 2 shows an example of a head finalized
Chinese sentence based on the output from Chi-
nese Enju shown in Figure 1. Notice that the
coordination exception rule described in (Isozaki
et al, 2010b) also applies to Chinese reordering.
This exception rule says that child nodes are not
swapped if the node is a coordination2. Another
exception rule is for punctuation symbols, which
are also preserved in their original order. In this
case, as can be seen in the example in Figure 2, the
nodes of c3, c6, and c8 had not been swapped with
their dependency. In this account, only the verb
?qu? had been moved to the end of the sentence,
following the same word order as its Japanese
translation.
3.2 Discrepancies in Head Definition
Head Finalization relies on the idea that head-
dependent relations are largely consistent among
different languages while word orders are differ-
ent. However, in Chinese, there has been much
debate on the definition of head3, possibly because
Chinese has fewer surface syntactic features than
other languages like English and Japanese. This
causes some discrepancies between the definitions
2Coordination is easily detected in the output of
Enju; it is marked by the attributes xcat="COOD" or
schema="coord-left/right" as shown in Figure 1.
3In this paper, we only consider the syntactic head.
of the head in Chinese and Japanese, which leads
to undesirable reordering of Chinese sentences.
Specifically, in preliminary experiments we ob-
served unexpected reorderings that are caused by
the differences in the head definitions, which we
describe below.
3.2.1 Aspect Particle
Although Chinese has no syntactic tense marker,
three aspect particles following verbs can be used
to identify the tense semantically. They are ?le0?
(did), ?zhe0? (doing), and ?guo4? (done), and
their counterparts in Japanese are ?ta?, ?teiru?,
and ?ta?, respectively. Both the first word and
third word can represent the past tense, but the
third one is more often used in the past perfect.
The Chinese parser4 treated aspect particles as
dependents of verbs, whereas their Japanese coun-
terparts are identified as the head. For exam-
ple in Table 15, ?qu? (go) and ?guo? (done)
aligned with ?i? and ?tta?, respectively. How-
ever, since ?guo? is treated as a dependent of
?qu?, by directly implementing the Head Final
Chinese (HFC), the sentence will be reordered like
4The discussions in this section presuppose the syntactic
analysis done by Chinese Enju, but most of the analysis is
consistent with the common explanation for Chinese syntax.
5English translation (En); Chinese original sentence
(Ch); reordered Chinese by Head-Final Chinese (HFC); re-
ordered Chinese by Refined Head-Final Chinese (R-HFC)
and Japanese translation (Ja).
60
HFC in Table 1, which does not follow the word
order of the Japanese (Ja) translation. In contrast,
the reordered sentence from refined-HFC (R-HFC)
can be translated monotonically.
En I have been to Tokyo.
Ch wo qu guo dongjing.
HFC wo dongjing guo qu.
R-HFC wo dongjing qu guo.
Ja watashi (wa) Tokyo (ni) i tta.
Table 1: An example for Aspect Particle. Best word
alignment Ja-Ch (En): ?watashi? ? ?wo?(I); ?Tokyo? ?
?dongjing? (Tokyo); ?i? ? ?qu? (been); ?tta? ? ?guo?
(have).
3.2.2 Adverbial Modifier ?bu4?
Both in Chinese and Japanese, verb phrase mod-
ifiers typically occur in pre-verbal positions, espe-
cially when the modifiers are adverbs. Since ad-
verbial modifiers are dependents in both Chinese
and Japanese, head finalization works perfectly for
them. However, there is an exceptional adverb,
?bu4?, which means negation and is usually trans-
lated into ?nai?, which is always at the end of the
sentence in Japanese and thus is the head. For ex-
ample in Table 2, the word ?kan? (watch) will be
identified as the head and the word ?bu? is its de-
pendent; on the contrary, in the Japanese transla-
tion (Ja), the word ?nai?, which is aligned with
?bu?, will be identified as the head. Therefore,
the Head Final Chinese is not in the same order,
but the reordered sentence by R-HFC obtained the
same order with the Japanese translation.
En I do not watch TV.
Ch wo bu kan dianshi.
HFC wo dianshi bu kan.
R-HFC wo dianshi kan bu.
Ja watashi (wa) terebi (wo) mi nai.
Table 2: An example for Adverbial Modifier bu4.
Best word alignment Ja-Ch (En): ?watashi? ? ?wo? (I);
?terebi? ? ?dianshi? (TV); ?mi? ? ?kan? (watch); ?nai?
? ?bu? (do not).
3.2.3 Sentence-final Particle
Sentence-final particles often appear at the end
of a sentence to express a speaker?s attitude:
e.g. ?ba0, a0? in Chinese, and ?naa, nee? in
Japanese. Although they appear in the same posi-
tion in both Chinese and Japanese, in accordance
with the differences of head definition, they are
identified as the dependent in Chinese while they
are the head in Japanese. For example in Table 3,
since ?a0? was identified as the dependent, it had
been reordered to the beginning of the sentence
while its Japanese translation ?nee? is at the end
of the sentence as the head. Likewise, by refining
the HFC, we can improve the word alignment.
En It is good weather.
Ch tianqi zhenhao a.
HFC a tianqi zhenhao.
R-HFC tianqi zhenhao a.
Ja ii tennki desu nee.
Table 3: An example for Sentence-final Particle.
Best word alignment Ja-Ch (En): ?tennki? ? ?tianqi?
(weather); ?ii? ? ?zhenhao? (good); ?nee? ? ?a? (None).
3.2.4 Et cetera
In Chinese, there are two expressions for rep-
resenting the meaning of ?and other things? with
one Chinese character: ?deng3? and ?deng3
deng3?, which are both identified as dependent
of a noun. In contrast, in Japanese, ?nado? is al-
ways the head because it appears as the right-most
word in a noun phrase. Table 4 shows an example.
En Fruits include apples, etc.
Ch shuiguo baokuo pingguo deng.
HFC shuiguo deng pingguo baokuo.
R-HFC shuiguo pingguo deng baokuo.
Ja kudamono (wa) ringo nado (wo)
fukunde iru.
Table 4: An example for Et cetera. Best word alignment
Ja-Ch (En): ?kudamono? ? ?shuiguo? (Fruits); ?ringo?
? ?pingguo? (apples); ?nado? ? ?deng? (etc.); ?fukunde
iru? ? ?baokuo? (include).
61
AS Aspect particle
SP Sentence-final particle
ETC et cetera (i.e. deng3 and deng3 deng3)
IJ Interjection
PU Punctuation
CC Coordinating conjunction
Table 5: The list of POSs for exception reordering rules
3.3 Refinement of HFC
In the preceding sections, we have discussed syn-
tactic constructions that cause wrong application
of Head Finalization to Chinese sentences. Fol-
lowing the observations, we propose a method to
improve the original Head Finalization reordering
rule to obtain better alignment with Japanese.
The idea is simple: we define a list of POSs,
and when we find one of them as a dependent
child of the node, we do not apply reordering. Ta-
ble 5 shows the list of POSs we define in the cur-
rent implementation6. While interjections are not
discussed in detail, we should obviously not re-
order to interjections because they are position-
independent. The rules for PU and CC are ba-
sically equivalent to the exception rules proposed
by (Isozaki et al, 2010b).
4 Experiments
The corpus we used as training data comes
from the China Workshop on Machine Transla-
tion (CWMT) (Zhao et al, 2011). This is a
Japanese-Chinese parallel corpus in the news do-
main, containing 281, 322 sentence pairs. We also
collected another Japanese-Chinese parallel cor-
pus from news containing 529, 769 sentences and
merged it with the CWMT corpus to create an ex-
tended version of the CWMT corpus. We will re-
fer to this corpus as ?CWMT ext.? We split an in-
verted multi-reference set into a development and a
test set containing 1, 000 sentences each. In these
two sets, the Chinese input was different, but the
Japanese reference was identical. We think that
this split does not pose any severe problem to the
comparison fairness of the experiment, since no
new phrases are added during tuning and the ex-
perimental conditions remain equal for all tested
6The POSs are from Penn Chinese Treebank.
Ch Ja
CWMT
Sentences 282K
Run. words 2.5M 3.2M
Avg. sent. leng. 8.8 11.5
Vocabulary 102K 42K
CWMT ext.
Sentences 811K
Run. words 14.7M 17M
Avg. sent. leng. 18.1 20.9
Vocabulary 249K 95K
Dev.
Sentences 1000
Run. words 29.9K 35.7K
Avg. sent. leng. 29.9 35.7
OoV w.r.t. CWMT 485 106
OoV w.r.t. CWMT ext. 244 53
Test
Sentences 1000
Run. words 25.8K 35.7K
Avg. sent. leng. 25.8 35.7
OoV w.r.t. CWMT 456 106
OoV w.r.t. CWMT ext. 228 53
Table 6: Characteristics of CWMT and extended
CWMT Chinese-Japanese corpus. Dev. stands for De-
velopment, OoV for ?Out of Vocabulary? words, K for
thousands of elements, and M for millions of elements.
Data statistics were collected after tokenizing.
methods. Detailed Corpus statistics can be found
in Table 6.
To parse Chinese sentences, we used Chinese
Enju (Yu et al, 2010), an HPSG-based parser
trained with the Chinese HPSG treebank converted
from Penn Chinese Treebank. Chinese Enju re-
quires segmented and POS-tagged sentences to
do parsing. We used the Stanford Chinese seg-
menter (Chang et al, 2008) and Stanford POS-
tagger (Toutanova et al, 2003) to obtain the seg-
mentation and POS-tagging of the Chinese side of
the training, development, and test sets.
The baseline system was trained following
the instructions of recent SMT evaluation cam-
paigns (Callison-Burch et al, 2010) by using the
MT toolkit Moses (Koehn et al, 2007) in its de-
fault configuration. Phrase pairs were extracted
from symmetrized word alignments and distor-
tions generated by GIZA++ (Och and Ney, 2003)
using the combination of heuristics ?grow-diag-
final-and? and ?msd-bidirectional-fe?. The lan-
guage model was a 5-gram language model es-
timated on the target side of the parallel cor-
pora by using the modified Kneser-Ney smooth-
ing (Chen and Goodman, 1999) implemented in
62
the SRILM (Stolcke, 2002) toolkit. The weights
of the log-linear combination of feature functions
were estimated by using MERT (Och, 2003) on the
development set described in Table 6.
The effectiveness of the reorderings proposed
in Section 3.3 was assessed by using two preci-
sion metrics and two error metrics on translation
quality. The first evaluation metric is BLEU (Pap-
ineni et al, 2002), a very common accuracy metric
in SMT that measures N -gram precision, with a
penalty for too short sentences. The second eval-
uation metric was RIBES (Isozaki et al, 2010a), a
recent precision metric used to evaluate translation
quality between structurally different languages. It
uses notions on rank correlation coefficients and
precision measures. The third evaluation metric is
TER (Snover et al, 2006), another error metric that
computes the minimum number of edits required
to convert translated sentences into its correspond-
ing references. Possible edits include insertion,
deletion, substitution of single words, and shifts of
word sequences. The fourth evaluation metric is
WER, an error metric inspired in the Levenshtein
distance at word level. BLEU, WER, and TER
were used to provide a sense of comparison but
they do not significantly penalize long-range word
order errors. For this reason, RIBES was used to
account for this aspect of translation quality.
The baseline system was trained and tuned us-
ing the same configuration setup described in this
section, but no reordering rule was implemented at
the preprocessing stage.
Three systems have been run to translate the test
set for comparison when the systems were trained
using the two training data sets. They are the
baseline system, the system consisting in the na??ve
implementation of HF reordering, and the system
with refined HFC reordering rules. Assessment of
translation quality can be found in Table 7.
As can be observed in Table 7, the translation
quality, as measured by precision and error met-
rics, was consistently and significantly increased
when the HFC reordering rule was used and was
significantly improved further when the refinement
proposed in this work was used. Specifically, the
BLEU score increased from 19.94 to 20.79 when
the CWMT corpus was used, and from 23.17 to
24.14 when the extended CWMT corpus was used.
AS SP ETC IJ PU COOD
3.8% 0.8% 1.3% 0.0%* 21.0% 38.3%
Table 8: Weighted recall of each exception rule during
reordering on CWMT ext. training data, dev data, and
test data. (* actual value 0.0016%.)
Table 8 shows the recall of each exception rule
listed in Section 3, and was computed by counting
the times an exception rule was triggered divided
by the number of times the head finalization rule
applied. Data was collected for CWMT ext. train-
ing, dev and test sets. Although the exception rules
related to aspect particles, Et cetera, sentence-final
particles and interjections have a comparatively
lower frequency of application than punctuation
or coordination exception rules, the improvements
they led to are significant.
5 Error Analysis
In Section 3 we have analyzed syntactic differ-
ences between Chinese and Japanese that led to
the design of an effective refinement. A manual
error analysis of the results of our refined reorder-
ing rules showed that some more reordering issues
remain and, although they are not side effects of
our proposed rule, they are worth mentioning in
this separate section.
5.1 Serial Verb Construction
Serial verb construction is a phenomenon occur-
ring in Chinese, where several verbs are put to-
gether as one unit without any conjunction be-
tween them. The relationship between these
verbs can be progressive or parallel. Apparently,
Japanese has a largely corresponding construc-
tion, which indicates that no reordering should
be applied. An example to illustrate this fact in
Chinese is ?weishi (maintain) shenhua (deepen)
zhongriguanxi (Japan-China relations) de
(of) gaishan (improvement) jidiao (basic
tone).?7 The two verbs ?weishi? (in Japanese,
iji) and ?shenhua? (in Japanese, shinka) are
used together, and they follow the same order as
in Japanese: ?nicchukankei (Japan-China re-
7English translation: Maintain and deepen the improved
basic tone of Japan-China relations.
63
CWMT CWMT ext.
BLEU RIBES TER WER BLEU RIBES TER WER
baseline 16.74 71.24 70.86 77.45 20.70 74.21 66.10 72.36
HFC 19.94 73.49 65.19 71.39 23.17 75.35 61.38 67.74
refined HFC 20.79 75.09 64.91 70.39 24.14 77.17 59.67 65.31
Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for
training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed
refinement of head finalization reordering rules.
lations) no (of) kaizan (improvement) kityo
(basic tone) wo iji (maintain) shinka (deepen)
suru (do).?
5.2 Complementizer
A ?complementizer? is a particle used to intro-
duce a complement. In English, a very common
complementizer is the word ?that? when making a
clausal complement, while in Chinese it can de-
note other types of word, such as verbs, adjec-
tives or quantifiers. The complementizer is iden-
tified as the dependent of the verb that it modi-
fies. For instance, a Chinese sentence: ?wo (I)
mang wan le (have finished the work).? This
can be translated into Japanese: ?watashi (I) wa
shigoto (work) wo owa tta (have finished).? In
Chinese, the verb ?mang? is the head while ?wan?
is the complementizer, and its Japanese counter-
part ?owa tta? has the same word order.
However, during the reordering, ?mang? will be
placed at the end of the sentence and ?wan? in the
beginning, leading to an inconsistency with respect
to the Japanese translation where the complemen-
tizer ?tta? is the head.
5.3 Verbal Nominalization and Nounal
Verbalization
As discussed by Guo (2009), compared to English
and Japanese, Chinese has little inflectional mor-
phology, that is, no inflection to denote tense, case,
etc. Thus, words are extremely flexible, making
verb nominalization and noun verbalization appear
frequently and commonly without any conjugation
or declension. As a result, it is difficult to do dis-
ambiguation during POS tagging and parsing. For
example, the Chinese word ?kaifa? may have
two syntactic functions: verb (develop) and noun
(development). Thus, it is difficult to reliably tag
without considering the context. In contrast, in
Japanese, ?suru? can be used to identify verbs.
For example, ?kaihatu suru? (develop) is a
verb and ?kaihatu? (development) is a noun.
This ambiguity is prone to not only POS tagging
error but also parsing error, and thus affects the
identification of heads, which may lead to incor-
rect reordering.
5.4 Adverbial Modifier
Unlike the adverb ?bu4? we discussed in Sec-
tion 3.2, the ordinary adverbial modifier comes
directly before the verb it modifies both in Chi-
nese and Japanese, but not in English. Nev-
ertheless, in accordance with the principle of
identifying the head for Chinese, the adverb
will be treated as the dependent and it will
not be reordered following the verb it modi-
fied. As a result, the alignment between adverbs
and verbs is non-monotonic. This can be ob-
served in the Chinese sentence ?guojia (coun-
try) yanli (severely) chufa (penalize) jiage
(price) weifa (violation) xingwei (behavior)?8,
and its Japanese translation: ?kuni (country) wa
kakaku (price) no ihou (violation) koui (be-
havior) wo kibisiku (severely) syobatu (penal-
ize).? Both in Chinese and Japanese, the adverbial
modifier ?yanli? and ?kibisiku? are directly
in front of the verb ?chufa? and ?syobatu?, re-
spectively. However, the verb in Chinese is identi-
fied as the head and will be reordered to the end of
the sentence without the adverb.
8English translation: The country severely penalizes vio-
lations of price restrictions.
64
5.5 POS tagging and Parsing Errors
There were word reordering issues not caused
solely by differences in syntactic structures. Here
we summarize two that are difficult to remedy dur-
ing reordering and that are hard to avoid since re-
ordering rules are highly dependent on the tagger
and parser.
? POS tagging errors
In Chinese, for example, the word ?Iran?
was tagged as ?VV? or ?JJ? instead of ?NR?.
This led to identifying ?Iran? as a head in
accordance with the head definition in Chi-
nese, and it was reordered undesirably.
? Parsing errors
For example, in the Chinese verb phrase
?touzi (invest) 20 yi (200 million)
meiyuan (dollars)?, ?20? and ?yi? were
identified as dependent of ?touzi? and
?meiyuan?, respectively, which led to an
unsuitable reordering for posterior word
alignment.
6 Conclusion and Future Work
In the present work, we have proposed novel
Chinese-to-Japanese reordering rules inspired
in (Isozaki et al, 2010b) based on linguistic analy-
sis on Chinese HPSG and differences among Chi-
nese and Japanese. Although a simple implemen-
tation of HF to reorder Chinese sentences per-
forms well, translation quality was substantially
improved further by including linguistic knowl-
edge into the refinement of the reordering rules.
In Section 5, we found more patterns on reorder-
ing issues when reordering Chinese sentences to
resemble Japanese word order. The extraction of
those patterns and their effective implementation
may lead to further improvements in translation
quality, so we are planning to explore this possi-
bility.
In this work, syntactic information from a deep
parser has been used to reorder words better. We
believe that using semantic information can fur-
ther increase the expressive power of reordering
rules. With that objective, Chinese Enju can be
used since it provides the semantic head of nodes
and can interpret sentences by using their semantic
dependency.
Acknowledgments
This work was mainly developed during an intern-
ship at NTT Communication Science Laborato-
ries. We would like to thank Prof. Yusuke Miyao
for his invaluable support on this work.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of ma-
chine translation. In Computational Linguistics, vol-
ume 19, pages 263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the joint 5th workshop on Statistical Ma-
chine Translation and MetricsMATR. Association
for Computational Linguistics, July.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word seg-
mentation for machine translation performance. In
Proceedings of the 3rd Workshop on SMT, pages
224?232, Columbus, Ohio. Association for Compu-
tational Linguistics.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359?393.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 531?540, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Vivian James Cook and Mark Newson. 1988. Chom-
sky?s Universal Grammar: An introduction. Oxford:
Basil Blackwell.
Naoki Fukui. 1992. Theory of Projection in Syntax.
CSLI Publisher and Kuroshio Publisher.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. Whats in a translation rule?
In Proceedings of HLT-NAACL.
Qian Gao. 2008. Word order in mandarin: Reading and
speaking. In Proceedings of the 20th North Ameri-
can Conference on Chinese Linguistics (NACCL-20),
volume 2, pages 611?626.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ?10,
65
pages 376?384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yuqing Guo. 2009. Treebank-based acquisition of
Chinese LFG resources for parsing and generation.
Ph.D. thesis, Dublin City University.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of Empirical Methods on Nat-
ural Language Processing (EMNLP).
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of
WMTMetricsMATR, pages 244?251.
P. Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings
HLT/NAACL?03, pages 48?54.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the ACL Demo and Poster Sessions, 2007, pages
177?180, June 25?27.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic hpsg parsing. Computa-
tional Linguistics, 34:35?80, March.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st annual conference of the Association for Com-
putational Linguistics, 2003, pages 160?167, July 7?
12.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual conference of the Association for Com-
putational Linguistics, 2002, pages 311?318, July 6?
12.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
international conference on Spoken Language Pro-
cessing, 2002, pages 901?904, September 16?20.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings OF HLT-NAACL, pages 252?259.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 737?
745, Prague, Czech Republic, June. Association for
Computational Linguistics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proceedings of 5th International Joint Con-
ference on Natural Language Processing, pages 29?
37, Chiang Mai, Thailand, November. Asian Feder-
ation of Natural Language Processing.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the 20th international
conference on Computational Linguistics, COLING
?04, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 245?253, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Kun Yu, Yusuke Miyao, Xiangli Wang, Takuya Mat-
suzaki, and Jun ichi Tsujii. 2010. Semi-
automatically developing chinese hpsg grammar
from the penn chinese treebank for deep parsing. In
COLING (Posters)?10, pages 1417?1425.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In Proceedings of
the 12th International Conference on Parsing Tech-
nologies, pages 48?57.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of
KI?02, pages 18?32.
Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, Yun
Huang, and Qun Liu. 2011. Evaluation report
for the 7th china workshop on machine translation
(cwmt2011). The 7th China Workshop on Machine
Translation (CWMT2011).
66
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 111?118,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Zero Pronoun Resolution can Improve the Quality of J-E Translation
Hirotoshi Taira, Katsuhito Sudoh, Masaaki Nagata
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho, Keihanna Science City
Kyoto 619-0237, Japan
{taira.hirotoshi,sudoh.katsuhito,nagata.masaaki}@lab.ntt.co.jp
Abstract
In Japanese, particularly, spoken Japanese,
subjective, objective and possessive cases are
very often omitted. Such Japanese sentences
are often translated by Japanese-English sta-
tistical machine translation to the English sen-
tence whose subjective, objective and posses-
sive cases are omitted, and it causes to de-
crease the quality of translation. We per-
formed experiments of J-E phrase based trans-
lation using Japanese sentence, whose omitted
pronouns are complemented by human. We
introduced ?antecedent F-measure? as a score
for measuring quality of the translated En-
glish. As a result, we found that it improves
the scores of antecedent F-measure while the
BLEU scores were almost unchanged. Every
effectiveness of the zero pronoun resolution
differs depending on the type and case of each
zero pronoun.
1 Introduction
Today, statistical translation systems have been able
to translate between languages at high accuracy us-
ing a lot of corpora . However, the quality of trans-
lation of Japanese to English is not high compar-
ing with the other language pairs that have the sim-
ilar syntactic structure such as the French-English
pair. Particularly, the quality of translation from
spoken Japanese to English is in low. There are
many reasons for the low quality. One is the dif-
ferent syntactic structures, that is, Japanese sentence
structure is SOV while English one is SVO. This
problem has been partly solved by head finalization
techniques (Isozaki et al, 2010). Another big prob-
lem is that subject, object and possessive cases are
often eliminated in Japanese, particularly, spoken
Japanese (Nariyama, 2003). In the case of Japanese
to English translation, the source language has lesser
information in surface than the target language, and
the quality of the translation tends to be low. We
show the example of the omissions in Fig 1. In this
example, the Japanese subject watashi wa (?I?) and
the object anata ni (?to you?) are eliminated in the
sentence. These omissions are not problems for hu-
man speakers and hearers because people easily rec-
ognize who is the questioner or responder (that is,
?I? and ?you?) from the context. However, gener-
ally speaking, the recognition is difficult for statisti-
cal translation systems.
Some European languages allow the elimination
of subject. We show an example in Spanish in Fig 2.
In this case, the subject is eliminated, and it leaves
traces including the case and the sex, on the related
verb. The Spanish word, tengo is the first person
singular form of the verb, tener (it means ?have?).
So it is easier to resolve elimination comparing with
Japanese one for SMT.
Otherwise, Japanese verbs usually have no inflec-
tional form depending on the case and sex. So,
we need take another way for elimination resolu-
tion. For example, if the eliminated Japanese sub-
ject is always ?I? when the sentence is declara-
tive, and the subject is always ?you? when the sen-
tence is a question sentence, phrase based transla-
tion systems are probably able to translate subject-
eliminated Japanese sentences to correct English
sentences. However, the hypothesis is not always
111
Jpn: (watashi wa)  (anata ni) shoushou ukagai tai  koto ga ari masu .
Eng:   I have     some  questions    to        ask     to   you    .
Omission of
subject
Omission of
object
Figure 1: Example of Japanese Ellipsis (Zero Pronoun)
Spa: (yo) Tengo   algunas preguntas  para  hacerle a  usted  .
Eng:   I have     some  questions    to        ask     to   you    .
Omission of subject
Figure 2: Spanish Ellipsis
true.
In this paper, we show that the quality of spoken
Japanese to English translation can improve using
a phrase-based translation system if we can use an
ideal elimination resolution system. However, we
also show that a simple elimination resolution sys-
tem is not effective to the improvement and it is nec-
essary to recognize correctly the modality of the sen-
tence.
2 Previous Work
There are a few researches for adaptation of ellip-
sis resolution to statistical translation systems while
there are a lot of researches for one to rule-based
translation systems in Japanese (Yoshimoto, 1988;
Dohsaka, 1990; Nakaiwa and Yamada, 1997; Ya-
mamoto et al, 1997).
As a research of SMT using elimination resolu-
tion, we have (Furuichi et al, 2011). However, the
target of the research is illustrative sentences in En-
glish to Japanese dictionary. Our research aims spo-
ken language translation and it is different from the
paper.
3 Setup of the Data of Subjects and
Objects Ellipsis in Spoken Japanese
3.1 Ellipsis Resolved Data by Human
In this section, we describe the data used in our ex-
periments. We used BTEC (Basic Travel Expres-
sion Corpus) corpus (Kikui et al, 2003) distributed
in IWSLT07 (Fordyce, 2007). The corpus consists
of tourism-related sentences similar to those that
are usually found in phrasebooks for tourists going
abroad. The characteristics of the dataset are shown
in Table 1. We used ?train? for training, ?devset1-
3? for tuning, and ?test? for evaluation. We did not
use the ?devset4? and ?devset5? sets because of the
different number of English references.
We annotated zero pronouns and the antecedents
to the sentences by hand. Here, zero pronoun is de-
fined as an obligatory case noun phrase that is not
expressed in the utterance but can be understood
through other utterances in the discourse, context, or
out-of-context knowledge (Yoshimoto, 1988). We
annotated the zero pronouns based on pronouns in
the translated English sentences. The BTEC corpus
has multi-references in English. We first chose the
most syntactically and lexically similar translation
in the references and annotated zero pronouns in it.
Our target pronouns are I, my, me, mine, myself, we,
our, us, ours, ourselves, you, your, yourself, your-
selves, he, his, him, himself, she, her, herself, it, its,
itself, they, their, them, theirs and themselves in En-
glish. We show the distribution of the annotation
types in the test set in Table 2.
3.2 Baseline System
We also examined a simple baseline zero pronoun
resolution system for the same data. We defined
112
Table 1: Data distribution
train devset1-3 devset4 devset5 test
# of References 1 16 7 7 16
# of Source Segments 39,953 1,512 489 500 489
Japanese predicate as verb, adjective, and copula (da
form) in the experiments. If the inputted Japanese
sentence contains predicates and it does not contain
?wa? (a binding particle and a topic marker), ?mo? (a
binding particle, which means ?also? and can often
replace ?wa? and ?ga?), and ?ga? (a case particle and
subjective marker), the system regards the sentence
as a candidate sentence to solve the zero pronouns.
Then, if the candidate sentence is declarative, the
system inserts ?watashi wa (I)? when the predicate
is a verb, and ?sore wa (it)? when the predicate is a
adjective or a copula. In the same way, if the candi-
date sentence is a question, the system inserts ?anata
wa (you)? when the predicate is a verb, and ?sore wa
(it)? when the predicate is a adjective or a copula.
These inserted position is the beginning of the sen-
tence. In the case that the sentence is imperative, the
system does not solve the zero pronouns (Fig. 3).
4 Experiments
4.1 Experimental Setting
Fig. 4 shows the outline of the procedure of our ex-
periment. We used Moses (Koehn et al, 2007) for
the training of the translation and language models,
tuning with MERT (Och, 2003) and the decoding.
First, we prepared the data for learning which con-
sists of parallel English and Japanese sentences. We
used MeCab 1 as Japanese tokenizer and the tok-
enizer in Moses Tool kit as English tokenizer. We
used default settings for the parameters of Moses.
Next, Moses learns language model and translation
model from the Japanese and English sentence pairs.
Then, the learned model was tuned by completed
sentences with MERT. and Moses decoded the com-
pleted Japanese sentences to English sentences.
4.2 Evaluation Method
We used BLEU (Papineni et al, 2002) and an-
tecedent Precision, Recall and F-measure for the
1http://mecab.sourceforge.net/
evaluation of the performances, comparing the sys-
tem outputs with the English references of test data.
Using only BLEU score is not adequate for evalua-
tion of pronoun translation (Hardmeier et al, 2010).
We were inspired empty node recovery evaluation
by (Johnson, 2002) and defined antecedent Preci-
sion (P), Recall (R) and F-measure (F) as follows,
P =
|G ? S|
|S|
R =
|G ? S|
|G|
F =
2PR
P +R
Here, S is the set of each pronoun in English
translated by decoder, G is the set of the gold stan-
dard zero pronoun.
We evaluated the effect of performance of every
case among completed sentences by human, ones by
the baseline system, and the original sentences.
4.3 Experimental Result
We show the BLEU scores in Table 3. and the an-
tecedent precision, recall and F-measure in Table 4.
The BLEU scores for experiments using our base-
line system and human annotation, are slightly bet-
ter than for one without ellipsis resolution, 45.4%
and 45.6%, respectively. However, the scores of an-
tecedent F-measure have major difference between
?original? and ?human?. Particularly, the recall is im-
proved. Each 1st, 2nd and 3rd person score is better
than original one.
5 Discussion and Conclusion
We performed experiments of J-E phrase based
translation using Japanese sentences, whose omit-
ted pronouns are complemented by human and a
baseline system. Using ?antecedent F-measure? as a
score for measuring the quality of the translated En-
glish, it improves the score of antecedent F-measure.
Every effectiveness of the zero pronoun resolution
113
ano eiga-wo mimashita.
the movie-OBJ     watched
Declarative sentence
Watashi-wa ano eiga-wo mimashita.
I-TOP the     movie-OBJ     watched
(=  ?I watched the movie.? )   
Question sentence
ano eiga-wo mimashita ka ?
the    movie-OBJ     watched       QUES  ?
Anata-wa ano eiga-wo mimashita ka ?
You-TOP the     movie-OBJ     watched      QUES  ?
(=  ?Did you watch the movie?? )   
Imperative sentence
ano eiga-wo minasai.
the    movie-OBJ     watch-IMP
ano eiga-wo minasai.
the    movie-OBJ     watch-IMP
(=  ?Watch the movie.? )   
Figure 3: Our baseline system of zero pronoun resolution
differed, depending on the type and case of each zero
pronoun. The F-measures for the first person pro-
noun were smaller than expected ones, Rather, the
scores for and possessive pronouns second person
were greater (Table. 3).
We show a better, a worse, and an unchanged
cases of translation using the baseline system of
the elimination resolution in Fig. 5. The left-hand
is the result of the alignment between the origi-
nal Japanese sentence and the decoded English sen-
tence. The right-hand is the result of one using
the Japanese the baseline system solved zero pro-
nouns. In the ?better? case, the alignment of todoke-
te (send) is better than one of the original sen-
tence, and ?Can you? is compensated by the solved
zero pronoun anata-wa (you-TOP). Otherwise, in
the ?worse? case, our baseline system could not rec-
ognize that the sentence is imperative, and inserted
watashi-wa (I-TOP) incorrectly into the sentence. It
indicates that we need a highly accurate recogni-
tion of the modalities of sentences for more correct
completion of the antecedent of zero pronouns. In
the ?unchanged? case, the translation results are the
same. However, the alignment of the right-hand is
more correct than one of the left-hand.
References
Kohji Dohsaka. 1990. Identifying the referents of zero-
pronouns in japanese based on pragmatic constraint in-
terpretation. In Proceedings of ECAI, pages 240?245.
C.S. Fordyce. 2007. Overview of the iwslt 2007 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation, pages 1?
12.
M. Furuichi, J. Murakami, M. Tokuhisa, and M. Murata.
2011. The effect of complement subject in japanese
to english statistical machine translation (in Japanese).
In Proceedings of the 17th Annual Meeting of The
114
English
Parallel Corpus for Training
Japanese
Shoushou ukagai tai koto ga ari masu ga.?
I have some questions to ask .
Decoder ?Moses?
Parallel Corpus for Test
Completed Sentences
honkon  ryokou ni tsuite 
siri  tain  desu  ga.
exo1 wa  honkon  ryokou ni tsuite 
siri  tain  desu  ga.
System Output
Training
Translation Model
Language Model
Decoding
I?d like to know about
the Hong Kong trip.
English
I would like to know about
the Hong Kong trip.
Evaluation
Japanese
- - - -
- - - -
- - - -
- - - -
Zero pronoun annotation by hand
or baseline system 
Tuning
Japanese English
Parallel Corpus for Tuning
- - - - - - - -
Zero pronoun annotation 
by hand or baseline system 
Completed Sentences
- - - - - - - -
Figure 4: Outline of the experiment
Association for Natural Language Processing (NLP-
2012).
C. Hardmeier, M. Federico, and F.B. Kessler. 2010.
Modelling pronominal anaphora in statistical machine
translation. In Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 283?289.
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010.
Head finalization: A simple reordering rule for sov
languages. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 244?251. Association for Computational
Linguistics.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
136?143, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.
G. Kikui, E. Sumita, T. Takezawa, and S. Yamamoto.
2003. Creating corpora for speech-to-speech transla-
tion. In Proceedings of EUROSPEECH, pages 381?
384.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proc.
of the 45th Annual Conference of the Association for
115
Better
worse
Unchanged 
map-BY   point_out would       QUES
chizu-de   sashi-te morae-masu ka.
Would you point them out on this map ?
You-TOP map-BY   point_out would     QUES
anata-wa chizu-de   sashi-te morae-masu ka.
Would you point them out on this map ?
Hurry up 
Isoi-de  .
Hurry up .
(Ref)  Hurry up.
I-TOP    hurry up 
watashi-wa Isoi-de  .
I  ?m  in a hurry .
(Ref)  Would you point one out on this map?
Today?s    evening       by      send          would           QUES 
Kyou-no  yuugata made-ni todoke-te morae-masu ka .
It   by  this  evening  ?
(Ref) Can you deliver them by this evening?
you-TOP Today?s    evening       by      send          would           QUES 
anata-wa kyou-no  yuugata made-ni todoke-te morae-masu ka .
Can you   send it   by   this evening  ?
Figure 5: Effectiveness of zero pronoun resolution for decoding
Computational Linguistics (ACL-07), Demonstration
Session, pages 177?180.
H. Nakaiwa and S. Yamada. 1997. Automatic identifi-
cation of zero pronouns and their antecedents within
aligned sentence pairs. In Proc. of the 3rd Annual
Meeting of the Association for Natural Language Pro-
cessing.
S. Nariyama. 2003. Ellipsis and reference tracking
in Japanese, volume 66. John Benjamins Publishing
Company.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proc. of the ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proc. of the 40th An-
nual Conference of the Association for Computational
Linguistics (ACL-02).
K. Yamamoto, E. Sumita, O. Furuse, and H. Iida. 1997.
Ellipsis resolution in dialogues via decision-tree learn-
ing. In Proc. of NLPRS, volume 97. Citeseer.
K. Yoshimoto. 1988. Identifying zero pronouns in
japanese dialogue. In Proceedings of the 12th con-
ference on Computational linguistics-Volume 2, pages
779?784. Association for Computational Linguistics.
116
Table 2: The Type Distributions of Zero Pronouns in Test Set
Type Pronoun #
First personal pronoun i 121
my 39
me 32
mine 1
myself 0
we 7
our 2
us 2
ours 0
ourselves 0
total 204
Second personal pronoun you 95
your 23
yours 0
yourself 0
yourselves 0
total 118
Third personal pronoun he 1
his 0
him 0
himself 0
she 0
her 2
hers 0
herself 0
it 51
its 0
itself 0
they 2
their 0
them 5
theirs 0
themselves 0
total 61
all total 383
Table 3: BLEU score
BLEU F(Avg.) P R F (1st person) F (2nd person) F (3rd person)
original 45.1 59.7 63.8 56.1 61.6 59.9 52.3
baseline 45.4 58.5 64.1 53.7 61.2 59.2 47.7
human 45.6 71.8 67.5 76.7 70.6 77.6 63.7
117
Table 4: Antecedent precision, recall and F-measure for every pronoun
i (ref:121) my (ref:39) me (ref:32)
BLEU P R F P R F P R F
original 45.1 56.8 51.2 53.9 55.5 51.2 53.3 58.0 56.2 57.1
baseline 45.4 51.8 46.2 48.9 67.8 48.7 56.7 66.6 50.0 57.1
human 45.6 50.9 68.6 58.4 65.2 76.9 70.5 61.2 59.3 60.3
we (ref:7) our (ref:2) us (ref:2)
P R F P R F P R F
original 20.0 14.2 16.6 100.0 50.0 66.6 0.00 0.00 0.00
baseline 25.0 14.2 18.1 100.0 50.0 66.6 0.00 0.00 0.00
human 40.0 28.5 33.3 100.0 50.0 66.6 0.00 0.00 0.00
you (ref:95) your (ref:23)
P R F P R F
original 55.3 54.7 55.0 80.0 52.1 63.1
baseline 57.1 54.7 55.9 58.8 43.4 50.0
human 68.4 80.0 73.7 73.0 82.6 77.5
it (ref:51) its (ref:0)
P R F P R F
original 56.1 45.1 50.0 0.00 0.00 0.00
baseline 51.2 41.1 45.6 0.00 0.00 0.00
human 58.3 54.9 56.5 0.00 0.00 0.00
they (ref:2) their (ref:0) them (ref:5)
P R F P R F P R F
original 100.0 50.0 66.6 0.00 0.00 0.00 0.00 0.00 0.00
baseline 100.0 50.0 66.6 0.00 0.00 0.00 0.00 0.00 0.00
human 58.3 54.9 56.5 0.00 0.00 0.00 0.00 0.00 0.00
118
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 25?33,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using Unlabeled Dependency Parsing for Pre-reordering
for Chinese-to-Japanese Statistical Machine Translation
Dan Han1,2 Pascual Mart??nez-Go?mez2,3 Yusuke Miyao1,2
Katsuhito Sudoh4 Masaaki Nagata4
1The Graduate University For Advanced Studies
2National Institute of Informatics, 3The University of Tokyo
4NTT Communication Science Laboratories, NTT Corporation
{handan,pascual,yusuke}@nii.ac.jp
{sudoh.katsuhito,nagata.masaaki}@lab.ntt.co.jp
Abstract
Chinese and Japanese have a different sen-
tence structure. Reordering methods are
effective, but need reliable parsers to ex-
tract the syntactic structure of the source
sentences. However, Chinese has a loose
word order, and Chinese parsers that ex-
tract the phrase structure do not perform
well. We propose a framework where only
POS tags and unlabeled dependency parse
trees are necessary, and linguistic knowl-
edge on structural difference can be en-
coded in the form of reordering rules. We
show significant improvements in transla-
tion quality of sentences from news do-
main, when compared to state-of-the-art
reordering methods.
1 Introduction
Translation between Chinese and Japanese lan-
guages gains interest as their economic and polit-
ical relationship intensifies. Despite their linguis-
tic influences, these languages have different syn-
tactic structures and phrase-based statistical ma-
chine translation (SMT) systems do not perform
well. Current word alignment models (Och and
Ney, 2003) account for local differences in word
order between bilingual sentences, but fail at cap-
turing long distance word alignments. One of
the main problems in the search of the best word
alignment is the combinatorial explosion of word
orders, but linguistically-motivated heuristics can
help to guide the search.
This work explores syntax-informed pre-
reordering for Chinese; that is, we obtain syntactic
structures of Chinese sentences, reorder the words
to resemble the Japanese word order, and then
translate the reordered sentences using a phrase-
based SMT system. However, Chinese parsers
have difficulties in extracting reliable syntactic in-
formation, mainly because Chinese has a loose
word order and few syntactic clues such as inflec-
tion and function words.
On one hand, parsers implementing head-driven
phrase structure grammars infer a detailed con-
stituent structure, and such a rich syntactic struc-
ture can be exploited to design well informed re-
ordering methods. However, inferring abundant
syntactic information often implies introducing er-
rors, and reordering methods that heavily rely on
detailed information are sensitive to those parsing
errors (Han et al, 2012).
On the other hand, dependency parsers are com-
mitted to the simpler task of finding dependency
relations and dependency labels, which can also be
useful to guide reordering (Xu et al, 2009). How-
ever, reordering methods that rely on those depen-
dency labels will also be prone to errors, specially
in the case of Chinese since it has a richer set of
dependency labels when compared to other lan-
guages. Since improving parsers for Chinese is
challenging, we thus aim at reducing the influence
of parsing errors in the reordering procedure.
We present a hybrid approach that boosts the
performance of phrase-based SMT systems by
pre-reordering the source language using unla-
beled parse trees augmented with constituent
information derived from Part-of-Speech tags.
Specifically, we propose a framework to pre-
reorder a Subject-Verb-Object (SVO) language,
in order to improve its translation to a Subject-
Object-Verb (SOV) language, where the only re-
quired syntactic information are POS tags and un-
labeled dependency parse trees. We test the per-
formance of our pre-reordering method and com-
pare it to state-of-the-art reordering methods in the
news domain for Chinese.
In the next section, we describe similar work on
pre-reordering methods for language pairs that in-
25
volve either Chinese or Japanese, and explain how
our method builds upon them. From a linguis-
tic perspective, we describe in section 3 our ob-
servations of reordering issues between Chinese
and Japanese and detail how our framework solves
those issues. In section 4 we assess to what extent
our pre-reordering method succeeds in reordering
words in Chinese sentences to resemble the order
of Japanese sentences, and measure its impact on
translation quality. The last section is dedicated to
discuss our findings and point to future directions.
2 Related Work
Although there are many works on pre-reordering
methods for other languages to English translation
or inverse (Xia and McCord, 2004; Xu et al, 2009;
Habash, 2007; Wang et al, 2007; Li et al, 2007;
Wu et al, 2011), reordering method for Chinese-
to-Japanese translation, which is a representative
of long distance language pairs, has received little
attention.
The most related work to ours is in (Han et al,
2012), in which the authors introduced a refined
reordering approach by importing an existing re-
ordering method for English proposed in (Isozaki
et al, 2010b). These reordering strategies are
based on Head-driven phrase structure grammars
(HPSG) (Pollard and Sag, 1994), in that the re-
ordering decisions are made based on the head of
phrases. Specifically, HPSG parsers (Miyao and
Tsujii, 2008; Yu et al, 2011) are used to extract the
structure of sentences in the form of binary trees,
and head branches are swapped with their depen-
dents according to certain heuristics to resemble
the word order of the target language. However,
those strategies are sensitive to parsing errors, and
the binary structure of their parse trees impose
hard constraints in sentences with loose word or-
der. Moreover, as Han et al (2012) noted, reorder-
ing strategies that are derived from the HPSG the-
ory may not perform well when the head definition
is inconsistent in the language pair under study. A
typical example for the language pair of Chinese
and Japanese that illustrates this phenomenon is
the adverb ?bu4?, which is the dependent of its
verb in Chinese but the head in Japanese.
The work in (Xu et al, 2009) used an English
dependency parser and formulated handcrafted re-
ordering rules with dependency labels, POS tags
and weights as triplets and implemented them re-
cursively into sentences. This design, however,
limited the extensibility of their method. Our ap-
proach follows the idea of using dependency tree
structures and POS tags, but we discard the infor-
mation on dependency labels since we did not find
them informative to guide our reordering strate-
gies in our preliminary experiments, partly due to
Chinese showing less dependencies and a larger
label variability (Chang et al, 2009).
3 Methodology
In Subject-Verb-Object (SVO) languages, objects
usually follow their verbs, while in Subject-
Object-Verb (SOV) languages, objects precede
them. Our objective is to reorder words in Chinese
sentences (SVO) to resemble the word order of
Japanese sentences (SOV). For that purpose, our
method consists in moving verbs to the right-hand
side of their objects. However, it is challenging
to correctly identify the appropriate verbs and ob-
jects that trigger a reordering, and this section will
be dedicated to that end.
More specifically, the first step of our method
consists in identifying the appropriate verb (and
certain words close to it) that need to be moved to
the right-hand side of its object argument. Verbs
(and those accompanying words) will move as a
block, preserving the relative order among them.
We will refer to them as verbal blocks (Vbs). The
second step will consist in identifying the right-
most argument object of the verb under considera-
tion, and moving the verbal block to the right-hand
side of it. Finally, certain invariable grammatical
particles in the original vicinity of the verb will
also be reordered, but their positions will be de-
cided relative to their verb.
In what follows, we describe in detail how to
identify verbal blocks, their objects and the invari-
able grammatical particles that will play a role in
our reordering method. As mentioned earlier, the
only information that will be used to perform this
task will be the POS tags of the words and their
unlabeled dependency structures.
3.1 Identifying verbal blocks (Vbs)
Verbal blocks are composed of a head (Vb-H)
and possibly accompanying dependents (Vb-D).
In the Chinese sentence ?wo3 (I) chi1 le5 (ate) li2
(pear).?1, ?chi1? refers to the English verb ?eat?
1In this paper, we represent a Chinese character by using
Pinyin plus a tone number (there are 5 tones in Chinese). In
the example, ?chi1(eat)? is a verb and ?le5(-ed)? is an aspect
particle that adds preterit tense to the verb.
26
Vb-H VV VE VC VA P
Vb-D AD AS SP MSP CC VV VE VC VA
BEI LB SB
RM-D NN NR NT PN OD CD M FW CC
ETC LC DEV DT JJ SP IJ ON
Oth-DEP LB SB CS
Table 1: Lists of POS tags in Chinese used to iden-
tify blocks of words to reorder (Vb-H, Vb-D, BEI
lists), the POS tags of their dependents (RM-D
lists) which indicate the reordering position, and
invariable grammatical particles (Oth-DEP) that
need to be reordered.
and the aspect particle ?le5? adds a preterit tense
to the verb. The words ?chi1 le5? are an example
of verbal block that should be reordered as a block
without altering its inner word order, i.e. ?wo3
(I) li2 (pear) chi1 le5 (ate).?, which matches the
Japanese SOV order.
Possible heads of verbal blocks (Vb-H) are
verbs (words with POS tags VV, VE, VC and VA),
or prepositions (words with POS tag P). The Vb-H
entry of Table 1 contains the list of POS tags for
heads of verbal blocks. We use prepositions for
Vb-H identification since they behave similarly to
verbs in Chinese and should be moved to the right-
most position in a prepositional phrase to resemble
the Japanese word order. There are three condi-
tions that a word should meet to be considered as
a Vb-H:
i) Its POS tag is in the set of Vb-H in Table 1.
ii) It is a dependency head, which indicates that
it may have an object as a dependent.
iii) It has no dependent whose POS tag is in the
set of BEI in Table 1. BEI particles indicate
that the verb is in passive voice and should
not be reordered since it already resembles
the Japanese order.
Chinese language does not have inflection, con-
jugation, or case markers (Li and Thompson,
1989). For that reason, some adverbs (AD), as-
pect particles (AS) or sentence-final particles (SP)
are used to signal modality, indicate grammati-
cal tense or add aspectual value to verbs. Words
in this category preserve the order when translat-
ing to Japanese, and they will be candidates to be
part of the verbal block (Vb-D) and accompany
the verb when it is reordered. Other words in this
category are coordinating conjunctions (CC) that
connect multiple verbs, and both resultative ?de5?
(DER) and manner ?de5? (DEV). The full list of
POS tags used to identify Vb-Ds can be found in
Table 1. To be a Vb-D, there are three necessary
conditions as well:
i) Its POS tag is in the Vb-D entry in Table 1.
ii) It is a dependent of a word that is already in
the Vb.
iii) It is next to its dependency head or only a
coordination conjunction is in between.
To summarize, to build verbal blocks (Vbs) we
first find the words that meet the three Vb-H con-
ditions. Then, we test the Vb-D conditions on the
words adjacent to the Vb-Hs and extend the verbal
blocks to them if they meet the conditions. This
process is iteratively applied to the adjacent words
of a block until no more words can be added to the
verbal block, possibly nesting other verbal blocks
if necessary.
Figure 1a 2 shows an example of a dependency
tree of a Chinese sentence that will be used to il-
lustrate Vb identification. By observing the POS
tags of the words in the sentence, only the words
?bian1 ji4 (edit)? and ?chu1 ban3 (publish)? have
a POS tag (i.e. VV) in the Vb-H entry of Table 1.
Moreover, both words are dependency heads and
do not have any dependent whose POS tag is in
the BEI entry of Table 1. Thus, ?bian1 ji4 (edit)?
and ?chu1 ban3 (publish)? will be selected as Vb-
Hs and form, by themselves, two separate incipi-
ent Vbs. We arbitrarily start building the Vb from
the word ?chu1 ban3 (publish)?, by analyzing its
adjacent words that are its dependents.
We observe that only ?le5 (-ed)? is adjacent to
?chu1 ban3 (publish)?, it is its dependent, and its
POS tag is in the Vb-D list. Since ?le5 (-ed)?
meets all three conditions stated above, ?le5 (-ed)?
will be included in the Vb originated by ?chu1
ban3 (publish)?. The current Vb thus consists of
the sequence of tokens ?chu1 ban3 (publish)? and
?le5 (-ed)?, and the three conditions for Vb-D are
tested on the adjacent words of this block. Since
the adjacent words (or words separated by a coor-
dinating conjunction) do not meet the conditions,
the block is not further extended. Figure 1b shows
the dependency tree where the Vb block that con-
sists of the words ?chu1 ban3 (publish)? and ?le5
(-ed)? is represented by a rectangular box.
By checking in the same way, there are three
dependents that meet the requirements of being
2For all the dependency parsing trees in this paper, arrows
are pointing from heads to their dependents.
27
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .?? .?? .? .?? .? .? .? .? .?.School .has already .edit (-ed) .and .publish .-ed .a . .book.NN .AD .VV .CC .VV .AS .CD .M .NN .PU
.ROOT.o .o
.o.o .o .o .o .o .o
(a) Original dependency tree
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .?? .?? .? .?? .? .? .? .? .?.School .has already .edit (-ed) .and .publish .-ed .a . .book
.NN .AD .VV .CC .VV .AS .CD .M .NN .PU
.ROOT .o.o .o .o.o
(b) Vbs in rectangular boxes
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .? .? .? .?? .?? .? .?? .? .?.School .a . .book .has already .edit (-ed) .and .publish .-ed
(c) Merged and reordered Vb
Figure 1: An example that shows how to de-
tect and reorder a Verbal block (Vb) in a sen-
tence. In the first two figures 1a and 1b, Chi-
nese Pinyin, Chinese tokens, word-to-word En-
glish translations, and POS tags of each Chinese
token are listed in four lines. In Figure 1c, there
are Chinese Pinyin, reordered Chinese sentence
and its word-to-word English counterpart.
Vb-Ds for ?bian1 ji4 (edit)?: ?yi3 jing1 (has al-
ready)?, ?he2 (and)? and ?chu1 ban4 (publish)?
and hence this Vb consists of three tokens and one
Vb. The outer rectangular box in Figure 1b shows
that the Vb ?bian1 ji4 (edit)? as the Vb-H. Fig-
ure 1c shows an image of how this Vb will be
reordered while the inner orders are kept. Note
that the order of building Vbs from which Vb-Hs,
?chu1 ban3 (publish)? or ?bian1 ji4 (edit)? will not
affect any change of the final result.
3.2 Identifying objects
In the most general form, objects are dependents
of verbal blocks3 that act as their arguments.
While the simplest objects are nouns (N) or pro-
nouns (PN), they can also be comprised of noun
phrases or clauses (Downing and Locke, 2006)
such as nominal groups, finite clauses (e.g. that
clauses, wh-clauses) or non-finite clauses (e.g. -
ing clauses), among others.
For every Vb in a verb phrase, clause, or sen-
tence, we define the right-most object dependent
(RM-D) as the word that:
3Dependents of verbal blocks are dependents of any word
within the verbal block.
..ta1 .chi1 .le5 .wu3 fan4 . .qu4 .xue2 xiao4 ..? .? .? .?? .? .? .?? .?.he .eat .-ed .lunch .(and) .go(to) .school ..PN .VV .AS .NN .PU .VV .NN .PU
.ROOT.o .o .o.o
.o.o .o
? ?? ? ? ? ?? ? ?he lunch eat -ed school go(to)
V ?????? O V ???? OS
O ????? V O ???? VS
English Translation: He ate lunch, and went to school.
Figure 2: An example of a Chinese sentence with
a coordination of verb phrases as predicate. Sub-
ject(S), verbs(V), and objects(O) are displayed for
both verb phrases. Lines between the original Chi-
nese sentence and the reordered Chinese sentence
indicate the reordering trace of Verbal blocks(Vb).
i) its POS tag is in the RM-D entry of Table 1,
ii) its dependency head is inside of the verbal
block, and
iii) is the right-most object among all objects of
the verbal block.
All verbal blocks in the phrase, clause, or sen-
tence will move to the right-hand side of their cor-
respondent RM-Ds recursively. Figure 1b and Fig-
ure 1c show a basic example of object identifica-
tion. The Chinese word corresponding to ?shu1
(book)? is a dependent of a word within the verbal
block and its POS tag is within the RM-D entry
list of Table 1 (i.e. NN). For this reason, ?shu1
(book)? is identified as the right-most dependent
of the verbal block (Vb), and the Vb will move to
the right-hand side of it to resemble the Japanese
word order.
A slightly more complex example can be found
in Figure 2. In this example, there is a coordina-
tion structure of verb phrases, and the dependency
tree shows that the first verb, ?chi1 (eat)?, ap-
pears as the dependency head of the second verb,
?qu4 (go)?. The direct right-most object depen-
dent (RM-D) of the first verb, ?chi1 (eat)?, is the
word ?wu3 fan4 (lunch)?, and the verb ?chi1 (eat)?
will be moved to the right-hand side of its object
dependent.
There are cases, however, where there is no co-
ordination structure of verb phrases but a simi-
lar dependency relation occurs between two verbs.
Figure 3 illustrates one of these cases, where the
main verb ?gu3 li4 (encourage)? has no direct de-
28
..xue2 xiao4 .gu3 li4 .xue2 sheng1 .can1 yu3 .she4 hui4 .shi2 jian4 ..?? .?? .?? .?? .?? .?? .?.school .encourage .student .participate .social .practice.NN .VV .NN .VV .NN .NN .PU
.o .ROOT
.o
.o.o .o .o
?? ?? ?? ?? ?? ?? ?school student social practice participate encourage
S ???? V ?????? O
S ???? V ?????????? O
S ?????? O ??????? V
S ???????????? O ??????????? VEnglish Translation: School encourages student to participate in social practice.
Figure 3: An example of a Chinese sentence in
which an embedded clause appears as the object
of the main verb. Subjects (S), verbs (V), and ob-
jects (O) are displayed for both the sentence and
the clause. Lines between the original Chinese
sentence and the reordered Chinese sentence in-
dicate the reordering trace of Verbal blocks (Vb).
pendent that can be considered as an object since
no direct dependent has a POS tag in the RM-D en-
try of Table 1. Instead, an embedded clause (SVO)
appears as the object argument of the main verb,
and the main verb ?gu3 li4 (encourage)? appears
as the dependency head of the verb ?can1 yu2 (par-
ticipate)?.
In the news domain, reported speech is a fre-
quent example that follows this pattern. In our
method, if the main verb of the sentence (labeled
as ROOT) has dependents but none of them is a
direct object, we move the main verb to the end of
the sentence. As for the embedded clause ?xue2
sheng1 (student) can1 yu2 (participate) she4 hui4
(social) shi2 jian4 (practice)?, the verbal block of
the clause is the word ?can1 yu2 (participate)?
and its object is ?shi2 jian4 (practice)?. Apply-
ing our reordering method, the clause order results
in ?xue2 sheng1 (student) she4 hui4 (social) shi2
jian4 (practice) can1 yu2 (participate)?. The result
is an SOV sentence with an SOV clause, which
resembles the Japanese word order.
3.3 Identifying invariable grammatical
particles
In Chinese, certain invariable grammatical parti-
cles that accompany verbal heads have a different
word order relative to their heads, when compared
to Japanese. Those particles are typically ?bei4?
particle (POS tags LB and SB) and subordinating
conjunctions (POS tag CS). Those particles appear
on the left-hand side of their dependency heads in
Chinese, and they should be moved to the right-
hand side of their dependency heads for them to
resemble the Japanese word order. Reordering in-
variable grammatical particles in our framework
can be summarized as:
i) Find dependents of a verbal head (Vb-H)
whose POS tags are in the Oth-DEP entry of
Table 1.
ii) Move those particles to the right-hand side of
their (possibly reordered) heads.
iii) If there is more than one such particle, move
them keeping the relative order among them.
3.4 Summary of the reordering framework
Based on the definitions above, our dependency
parsing based pre-reordering framework can be
summarized in the following steps:
1. Obtain POS tags and an unlabeled depen-
dency tree of a Chinese sentence.
2. Obtain reordering candidates: Vbs.
3. Obtain the object (RM-D) of each Vb.
4. Reorder each Vb in two exclusive cases by
following the order:
(a) If RM-D exists, reorder Vb to be the
right-hand side of RM-D.
(b) If Vb-H is ROOT and its RM-D does not
exist, reorder Vb to the end of the sen-
tence.
(c) If none of above two conditions is met,
no reordering happens.
5. Reorder grammatical particles (Oth-DEPs) to
the right-hand side of their corresponding
Vbs.
Note that, unlike other works in reordering dis-
tant languages (Isozaki et al, 2010b; Han et al,
2012; Xu et al, 2009), we do not prevent chunks
from crossing punctuations or coordination struc-
tures. Thus, our method allows to achieve an
authentic global reordering in reported speech,
which is an important reordering issue in news do-
mains.
In order to illustrate our method, a more compli-
cated Chinese sentence example is given in Fig-
ure 4, which includes the unlabeled dependency
29
..xin1wen2 .bao2dao3 . .sui2zhe5 .jing1ji4 .de5 .fa1zhan3 . .sheng4dan4jie2 .zhu2jian4 .jin4ru4 .le5 .zhong1guo2 . .cheng2wei2 .shang1jia1 .jia1qiang2 .li4cu4 .mai3qi4 .de5 .yi1 .ge4 .ji2ri4 ..?? .?? .? .?? .?? .? .?? .? .??? .?? .?? .? .?? .? .?? .?? .?? .?? .?? .? .? .? .?? .?.news .report . .with .economic .?s .development . .Christmas .gradually .enter .-ed .China . .become .businesses .strengthen .urge .purchase .?s .one .kind .festival ..NN .VV .PU .P .NN .DEG .NN .PU .NN .AD .VV .AS .NR .PU .VV .NN .VV .VV .NN .DEC .CD .M .NN .PU
.ROOT
.o.o .o .o .o.o.o
.o .o .o .o.o .o
.o
.o
.o .o.o .o .o .o.o.o
?? ? ?? ? ?? ?? ? ??? ?? ?? ?? ? ? ?? ?? ?? ?? ? ? ? ???? ?? ?
???? ?? ?? ?? ??? ?????? ? ??? ?? ? ??? ? ?? ? ? ?? ?? ? ?? ? ?? ? ?????? ???Entire English translation: News reports, with the economic development, Christmas has gradually entered into China, and becomes one of the festivals that businesses use to promote commerce.
Figure 4: Dependency parse tree of a complex Chinese sentence example, and word alignments for
reordered sentence with its Japanese counterpart. The first four lines are Chinese Pinyin, tokens, word-
to-word English translations, and the POS tags of each Chinese token. The fifth line shows the reordered
Chinese sentence while the sixth line is the segmented Japanese translation. The entire English transla-
tion for the sentence is showed in the last line.
parsing tree of the original Chinese sentence, and
the word alignment between reordered Chinese
sentence and its Japanese counterpart, etc.
Based on both POS tags and the unlabeled de-
pendency tree, first step of our method is to obtain
all Vbs. For all heads in the tree, according to the
definition of Vb introduced in Section 3.1, there
are six tokens which will be recognized as the can-
didates of Vb-Hs, that is ?bao4 dao3 (report)?,
?sui2 zhe5 (with)?, ?jin4 ru4 (enter)?, ?cheng2
wei2 (become)?, ?jia1 qiang2 (strengthen)?, and
?li4 cu4 (urge)?. Then, for each of the candidate,
its direct dependents will be checked if they are
Vb-Ds. For instance, for the verb of ?jin4 ru4 (en-
ter)?, its dependents of ?zhu2 jian4 (gradually)?
and ?le5 (-ed)? will be considered as the Vb-Ds.
For the case of ?jia1 qiang2 (strengthen)?, instead
of being a Vb-H, it will be recognized as Vb-D
of the Vb ?li4 cu4 (urge)? since it is one of the
direct dependents of ?li4 cu4 (urge)? with a qual-
ified POS tag for Vb-D. Therefore, there are five
Vbs in total, which are ?bao4 dao3 (report)?, ?sui2
zhe5 (with)?, ?zhu2 jian4 (gradually) jin4 ru4 (en-
ter) le5 (-ed)?, ?cheng2 wei2 (become)?, and ?jia1
qiang2 (strengthen) li4 cu4 (urge)?.
The next step is to identify RM-D for each
Vb, if there is one. By checking all conditions,
four Vbs have their RM-Ds: ?fa1 zhan3 (develop-
ment)? is the RM-D of the Vb ?sui2 zhe5 (with)?;
?zhong1 guo2 (China)? is the RM-D of the Vb
?zhu2 jian4 (gradually) jin4 ru4 (enter) le5 (-ed)?;
?jie2 ri4 (festival)? is the RM-D of the Vb ?cheng2
wei2 (become)?; ?mai3 qi4 (purchase)? is the RM-
D of the Vb ?jia1 qiang2 (strengthen) li4 cu4
(urge)?.
After obtaining all RM-Ds, we find those Vbs
that have RM-Ds and move them to right of their
RM-Ds. As for the case of ?bao4 dao3 (report)?,
since it is the root and does not have any matched
RM-D, it will be moved to the end of the sen-
tence, before any final punctuation. Finally, since
there is no any invariable grammatical particle in
the sentence that need to be reordered, reordering
has been finished. From the alignments between
the reordered Chinese and its Japanese translation
showed in the figure, an almost monotonic word
alignment has been achieved.
For comparison purposes, particle seed words
had been inserted into the reordered sentences in
the same way as the Refined-HFC method, which
is using the information of predicate argument
structure output by Chinese Enju (Yu et al, 2011).
We therefore can not entirely disclaim the use
of the HPSG parser at the present stage in our
method. However, we believe that dependency
parser can provide enough information for insert-
ing particles.
4 Experiments
We conducted experiments to assess how our pro-
posed dependency-based pre-reordering for Chi-
nese (DPC) impacts on translation quality, and
compared it to a baseline phrase-based system
and a Refined-HFC pre-reordering for Chinese to
Japanese translation.
We used two Chinese-Japanese training data
30
News CWMT+News
BLEU RIBES BLEU RIBES
Baseline 39.26 84.83 38.96 85.01
Ref-HFC 39.22 84.88 39.26 84.68
DPC 39.93 85.23 39.94 85.22
Table 3: Evaluation of translation quality of two
test sets when CWMT, News and the combination
of both corpora were used for training.
sets of parallel sentences, namely an in-house-
collected Chinese-Japanese news corpus (News),
and the News corpus augmented with the
CWMT (Zhao et al, 2011) corpus. We extracted
disjoint development and test sets from News cor-
pus, containing 1, 000 and 2, 000 sentences re-
spectively. Table 2 shows the corpora statistics.
We used MeCab 4 (Kudo and Matsumoto, 2000)
and the Stanford Chinese segmenter 5 (Chang et
al., 2008) to segment Japanese and Chinese sen-
tences. POS tags of Chinese sentences were ob-
tained using the Berkeley parser 6 (Petrov et al,
2006), while dependency trees were extracted us-
ing Corbit 7 (Hatori et al, 2011). Following the
work in (Han et al, 2012), we re-implemented
the Refined-HFC using the Chinese Enju to ob-
tain HPSG parsing trees. For comparison purposes
with the work in (Isozaki et al, 2010b), particle
seed words were inserted at a preprocessing stage
for Refined-HFC and our DPC method.
DPC and Refined-HFC pre-reordering strate-
gies were followed in the pipeline by a standard
Moses-based baseline system (Koehn et al, 2007),
using a default distance reordering model and a
lexicalized reordering model ?msd-bidirectional-
fe?. A 5-gram language model was built using
SRILM (Stolcke, 2002) on the target side of the
corresponding training corpus. Word alignments
were extracted using MGIZA++ (Gao and Vogel,
2008) and the parameters of the log-linear combi-
nation were tuned using MERT (Och, 2003).
Table 3 summarizes the results of the Baseline
system (no pre-reordering nor particle word inser-
tion), the Refined-HFC (Ref-HFC) and our DPC
method, using the well-known BLEU score (Pap-
ineni et al, 2002) and a word order sensitive met-
ric named RIBES (Isozaki et al, 2010a).
4http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html
5http://nlp.stanford.edu/software/segmenter.shtml
6http://nlp.cs.berkeley.edu/Software.shtml
7http://triplet.cc/software/corbit
As it can be observed, our DPC method obtains
around 0.7 BLEU points of improvement when
compared to the second best system in both cor-
pora. When measuring the translation quality in
terms of RIBES, our method obtains an improve-
ment of 0.3 and 0.2 points when compared to the
second best system in News and CWMT + News
corpora, respectively. We suspect that corpus di-
versity might be one of the reasons for Refined-
HFC not to show any advantage in this setting.
We tested the significance of BLEU improve-
ment for Refined-HFC and DPC when compared
to the baseline phrase-based system. Refined-HFC
tests obtained p-values 0.355 and 0.135 on News
and CWMT + News corpora, while our proposed
DPC method obtained p-values 0.002 and 0.0,
which indicates significant improvements over the
phrase-based system.
5 Conclusions
In the present paper, we have analyzed the dif-
ferences in word order between Chinese and
Japanese sentences. We captured the regulari-
ties of ordering differences between Chinese and
Japanese sentences, and proposed a framework to
reorder Chinese sentences to resemble the word
order of Japanese.
Our framework consists in three steps. First,
we identify verbal blocks, which consist of Chi-
nese words that will move all together as a block
without altering their relative inner order. Sec-
ond, we identify the right-most object of the verbal
block, and move the verbal block to the right of it.
Finally, we identify invariable grammatical parti-
cles in the original vicinity of the verbal block and
move them relative to their dependency heads.
Our framework only uses the unlabeled depen-
dency structure of sentences and POS tag informa-
tion of words. We compared our system to a base-
line phrase-based SMT system and a refined head-
finalization system. Our method obtained a Chi-
nese word order that is more similar to Japanese
word order, and we showed its positive impact on
translation quality.
6 Discussion and future work
In the literature, there are mainly two types of
parsers that have been used to extract sentence
structure and guide reordering. The first type cor-
responds to parsers that extract phrase structures
(i.e. HPSG parsers). These parsers infer a rich
31
News CWMT+News
Chinese Japanese Chinese Japanese
Training
Sentences 342, 050 621, 610
Running words 7,414,749 9,361,867 9,822,535 12,499,112
Vocabulary 145,133 73,909 214,085 98,333
News Devel.
Sentences 1, 000 ?
Running words 46,042 56,748 ? ?
Out of Vocab. 255 54 ? ?
News Test
Sentences 2, 000 ?
Running words 51,534 65,721 ? ?
Out of Vocab. 529 286 ? ?
Table 2: Basic statistics of our corpora. News Devel. and News Test were used to tune and test the
systems trained with both training corpora. Data statistics were collected after tokenizing and filtering
out sentences longer than 64 tokens.
annotation of the sentence in terms of semantic
structure or phrase heads. Other reordering strate-
gies use a different type of parsers, namely depen-
dency parsers. These parsers extract dependency
information among words in the sentence, often
consisting in the dependency relation between two
words and the type of relation (dependency label).
Reordering strategies that use syntactic infor-
mation have proved successful, but they are likely
to magnify parsing errors if their reordering rules
heavily rely on abundant parse information. This
is aggravated when reordering Chinese sentences,
due to its loose word order and large variety of
possible dependency labels.
In this work, we based our study of ordering
differences between Chinese and Japanese solely
on dependency relations and POS tags. This con-
trasts with the work in (Han et al, 2012) that re-
quires phrase structures, phrase-head information
and POS tags, and the work in (Xu et al, 2009)
that requires dependency relations, dependency la-
bels and POS tags.
In spite of the fact that our method uses less syn-
tactic information, it succeeds at reordering sen-
tences with reported speech even in presence of
punctuation symbols. It is worth saying that re-
ported speech is very common in the news domain,
which might be one of the reasons of the supe-
rior translation quality achieved by our reordering
method. Our method also accounted for ordering
differences in serial verb constructions, comple-
mentizers and adverbial modifiers, which would
have required an increase in the complexity of the
reordering logic in other methods.
To the best of our knowledge, dependency
parsers are more common than HPSG parsers
across languages, and our method can potentially
be applied to translate under-resourced languages
into other languages with a very different sentence
structure, as long as they count with dependency
parsers and reliable POS taggers.
Implementing our method for other languages
would first require a linguistic study on the re-
ordering differences between the two distant lan-
guage pairs. However, some word ordering differ-
ences might be consistent across SVO and SOV
language pairs (such as verbs going before or after
their objects), but other ordering differences may
need special treatment for the language pair under
consideration (i.e. Chinese ?bei? particles).
There are two possible directions to extend the
present work. The first one would be to refine the
current method to reduce its sensitivity to POS tag-
ging or dependency parse errors, and to extend our
linguistic study on ordering differences between
Chinese and Japanese languages. The second di-
rection would be to manually or automatically find
common patterns of ordering differences between
SVO and SOV languages. The objective would be
then to create a one-for-all reordering method that
induces monotonic word alignments between sen-
tences from distant language pairs, and that could
also be easily extended to account for the unique
characteristics of the source language of interest.
Acknowledgments
We would like to thank Dr. Takuya Matsuzaki for
his precious advice on this work and Dr. Jun Ha-
tori for his support on using Corbit.
32
References
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
of the 3rd Workshop on SMT, pages 224?232.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative re-
ordering with Chinese grammatical relations fea-
tures. In Proc. of the Third Workshop on Syntax and
Structure in Statistical Translation, pages 51?59.
Angela Downing and Philip Locke. 2006. English
grammar: a university course. Routledge.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Proc. of Machine
Translation Summit XI, pages 215?222.
Dan Han, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,
Hajime Tsukada, and Masaaki Nagata. 2012. Head
finalization reordering for Chinese-to-Japanese ma-
chine translation. In Proc. of the Sixth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, pages 57?66.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Ju-
nichi Tsujii. 2011. Incremental joint POS tagging
and dependency parsing in Chinese. In Proc. of
5th International Joint Conference on Natural Lan-
guage Processing, pages 1216?1224.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proc. of EMNNLP.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for SOV languages. In Proc. of WMT-
MetricsMATR, pages 244?251.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proc. of ACL ?07, Demonstration Sessions, pages
177?180.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proc. of the EMNLP/VLC-2000, pages
18?25.
Charles N Li and Sandra Annear Thompson. 1989.
Mandarin Chinese: A functional reference gram-
mar. Univ of California Press.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proc. of ACL, page 720.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34:35?80.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29:19?51.
Franz J. Och. 2003. Minimum error rate training
for statistical machine translation. In Proc. of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st COL-
ING and the 44th ACL, pages 433?440.
Carl Jesse Pollard and Ivan A. Sag. 1994. Head-
driven phrase structure grammar. The University
of Chicago Press and CSLI Publications.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the 7th interna-
tional conference on Spoken Language Processing,
2002, pages 901?904.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proc. of the 2007 Joint Con-
ference on EMNLP-CoNLL, pages 737?745.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proc. of 5th International Joint Conference
on Natural Language Processing, pages 29?37.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. of the 20th international
conference on Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proc. of
HLT: NA-ACL 2009, pages 245?253.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the
difficulties in Chinese deep parsing. In Proc. of the
12th International Conference on Parsing Technolo-
gies, pages 48?57.
Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, Yun
Huang, and Qun Liu. 2011. Evaluation report
for the 7th China workshop on machine translation
(CWMT2011). The 7th China Workshop on Ma-
chine Translation (CWMT2011).
33
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 108?118,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Constructing a Practical Constituent Parser from a Japanese Treebank with
Function Labels
Takaaki Tanaka and Masaaki Nagata
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
{tanaka.takaaki, nagata.masaaki}@lab.ntt.co.jp
Abstract
We present an empirical study on construct-
ing a Japanese constituent parser, which can
output function labels to deal with more de-
tailed syntactic information. Japanese syn-
tactic parse trees are usually represented as
unlabeled dependency structure between bun-
setsu chunks, however, such expression is in-
sufficient to uncover the syntactic information
about distinction between complements and
adjuncts and coordination structure, which is
required for practical applications such as syn-
tactic reordering of machine translation. We
describe a preliminary effort on constructing
a Japanese constituent parser by a Penn Tree-
bank style treebank semi-automatically made
from a dependency-based corpus. The eval-
uations show the parser trained on the tree-
bank has comparable bracketing accuracy as
conventional bunsetsu-based parsers, and can
output such function labels as the grammatical
role of the argument and the type of adnominal
phrases.
1 Introduction
In Japanese NLP, syntactic structures are usually
represented as dependencies between grammatical
chunks called bunsetsus. A bunsetsu is a grammat-
ical and phonological unit in Japanese, which con-
sists of an independent-word such as noun, verb
or adverb followed by a sequence of zero or more
dependent-words such as auxiliary verbs, postposi-
tional particles or sentence final particles. It is one
of main features of Japanese that bunsetsu order is
much less constrained than phrase order in English.
Since dependency between bunsetsus can treat flexi-
ble bunsetsu order, most publicly available Japanese
parsers including CaboCha (Kudo et al, 2002) and
KNP (Kawahara et al, 2006) return bunsetsu-based
dependency as syntactic structure. Such bunsetsu-
based parsers generally perform with high accuracy
and have been widely used for various NLP applica-
tions.
However, bunsetsu-based representations also
have serious shortcomings for dealing with Japanese
sentence hierarchy. The internal structure of a bun-
setsu has strong morphotactic constraints in contrast
to flexible bunsetsu order. A Japanese predicate
bunsetsu consists of a main verb followed by a se-
quence of auxiliary verbs and sentence final parti-
cles. There is an almost one-dimensional order in
the verbal constituents, which reflects the basic hi-
erarchy of the Japanese sentence structure including
voice, tense, aspect and modality. Bunsetsu-based
representation cannot provide the linguistic structure
that reflects the basic sentence hierarchy.
Moreover, bunsetsu-based structures are unsuit-
able for representing such nesting structure as co-
ordinating conjunctions. For instance, bunsetsu rep-
resentation of a noun phrase ???-? (technology-
GEN) / ??-? (improvement-CONJ) / ??-?
(economy-GEN) / ?? (growth) ? technology im-
provement and economic growth does not allow
us to easily interpret it, which means ((technol-
ogy improvement) and (economic growth)) or (tech-
nology (improvement and economic growth)), be-
cause bunsetsu-based dependencies do not con-
vey information about left boundary of each noun
phrase (Asahara, 2013). This drawback complicates
108
operating syntactically meaningful units in such ap-
plications as statistical machine translation, which
needs to recognize syntactic units in building a trans-
lation model (e.g. tree-to-string and tree-to-tree) and
in preordering source language sentences.
Semantic analysis, such as predicate-argument
structure analysis, is usually done as a pipeline pro-
cess after syntactic analysis (Iida et al, 2011 ;
Hayashibe et al, 2011 ); but in Japanese, the dis-
crepancy between syntactic and semantic units cause
difficulties integrating semantic analysis with syn-
tactic analysis.
Our goal is to construct a practical constituent
parser that can deal with appropriate grammatical
units and output grammatical functions as semi-
semantic information, e.g., grammatical or seman-
tic roles of arguments and gapping types of relative
clauses. We take an approach to deriving a grammar
from manually annotated corpora by training prob-
abilistic models like current statistical constituent
parsers of de facto standards (Petrov et al, 2006;
Klein et al, 2003 ; Charniak, 2000; Bikel, 2004).
We used a constituent-based treebank that Uematsu
et al (2013) converted from an existing bunsetsu-
based corpus as a base treebank, and retag the non-
terminals and transform the tree structures in de-
scribed in Section 3. We will present the results of
evaluations of the parser trained with the treebank in
Section 4, and show some analyses in Section 5.
2 Related work
The number of researches on Japanese constituent-
based parser is quite few compared to that of
bunsetsu-dependency-based parser. Most of them
have been conducted under lexicalized grammatical
formalism.
HPSG (Head-driven Phrase Structure Gram-
mar) (Sag et al, 2003 ) is a representative one.
Gunji et al (1987) proposed JPSG (Japanese Phrase
Structure Grammar) that is theoretically precise to
handle the free word order problem of Japanese. Na-
gata et al ( 1993 ) built a spoken-style Japanese
grammar and a parser running on it. Siegel et al(
2002 ) constructed a broad-coverage linguistically
precise grammar JACY, which integrates semantics,
MRS (Minimal Recursion Semantics) (Copestake,
2005). Bond et al ( 2008 ) built a large-scale
Japanese treebank Hinoki based on JACY and used
it for parser training.
Masuichi et al(2003) developed a Japanese LFG
(Lexicalized-Functional Grammar) (Kaplan et al,
1982) parser whose grammar is sharing the de-
sign with six languages. Uematsu et al (2013)
constructed a CCG (Combinatory Categorial Gram-
mar) bank based on the scheme proposed by
Bekki (2010), by integrating several corpora includ-
ing a constituent-based treebank converted from a
dependency-base corpus.
These approaches above use a unification-based
parser, which offers rich information integrating
syntax, semantics and pragmatics, however, gener-
ally requires a high computational cost. We aim
at constructing a more light-weighted and practical
constituent parser, e.g. a PCFG parser, from Penn
Treebank style treebank with function labels. Gab-
bard et al (2006) introduced function tags by modi-
fying those in Penn Treebank to their parser. Even
though Noro et al (2005) built a Japanese corpus for
deriving Japanese CFG, and evaluated its grammar,
they did not treat the predicate-argument structure or
the distinction of adnominal phrases.
This paper is also closely related to the work of
Korean treebank transformations (Choi et al, 2012).
Most of the Korean corpus was built using grammat-
ical chunks eojeols, which resemble Japanese bun-
setsus and consist of content words and morphemes
that represent grammatical functions. Choi et al
transformed the eojeol-based structure of Korean
treebanks into entity-based to make them more suit-
able for parser training. We converted an existing
bunsetsu-based corpus into a constituent-based one
and integrating other information into it for training
a parser.
3 Treebank for parser training
In this section, we describe the overview of our tree-
bank for training a parser.
3.1 Construction of a base treebank
Our base treebank is built from a bunsetsu-
dependency-based corpus, the Kyoto Corpus (Kuro-
hashi et al, 2003), which is a collection of news-
paper articles, that is widely used for training data
for Japanese parsers and other applications. We
109
SIP-MAT[nad]:A
VP[nad]:A
VP[nad]:A
AUX
?
-PAST
VB[nad]
??
give
PP-OBJ
PCS
?
-ACC
NN
?
book
PP-OB2
PCS
?
-DAT
NN
??
student
S
IP-MAT[nad]:A
VP[nad]:A
AUX
?
-PAST
VB[nad]
??
give
PP-OBJ
PCS
?
-ACC
NN
?
book
PP-OB2
PCS
?
-DAT
NN
??
student
(I) gave the student a book.
binary tree n-ary (flattened) tree
Figure 1: Verb Phrase with subcategorization and voice information
NN General noun
NNP Proper noun
NPR Pronoun
NV Verbal noun
NADJ Adjective noun
NADV Adverbial noun (incl. temporal noun)
NNF Formal noun (general)
NNFV Formal noun (adverbial)
PX Prefix
SX Suffix
NUM Numeral
CL Classifier
VB Verb
ADJ Adjective
ADNOM Adnominal adjective
ADV Adverb
PCS Case particle
PBD Binding particle
PADN Adnominal particle
PCO Parallel particle
PCJ Conjunctive particle
PEND Sentence-ending particle
P Particle (others)
AUX Auxiliary verb
CONJ Conjunction
PNC Punctuation
PAR Parenthesis
SYM Symbol
FIL Filler
Table 1: Preterminal tags
automatically converted from dependency structure
to phrase structure by the previously described
method (Uematsu et al, 2013), and conversion er-
rors of structures and tags were manually corrected.
We adopted the annotation schema used in
Japanese Keyaki treebank (Butler et al, 2012) and
Annotation Manual for the Penn Historical Corpora
and the PCEEC (Santorini, 2010) as reference to re-
tag the nonterminals and transform the tree struc-
tures.
The original Kyoto Corpus has fine-grained part-
of-speech tags, which we converted into simpler
preterminal tags shown in Table 1 for training by
lookup tables. First the treebank?s phrase tags ex-
cept function tags are assigned by simple CFG rule
sets, then, function tags are added by integrating the
information from the other resources or manually
annotated. We integrate predicate-argument infor-
mation from the NAIST Text Corpus (NTC) (Iida et
al., 2007 ) into the treebank by automatically con-
verting and adding tag suffixes (e.g. -SBJ, -ARG0
described in section 3.3) to the original tags of the
argument phrases. The structure information about
coordination and apposition are manually annotated.
3.2 Complementary information
We selectively added the following information as
tag suffixes and tested their effectiveness.
Inflection We introduced tag suffixes for inflec-
tion as clues to identify the attachment position of
the verb and adjective phrases, because Japanese
verbs and adjectives have inflections, which depends
110
(no label) base form
cont continuative form
attr attributive form
neg negative form
hyp hypothetical form
imp imperative form
stem stem
Table 2: Inflection tag suffixes
on their modifying words and phrases (e.g. noun
and verb phrases). Symbols in Table 2 are attached
to tags VB, ADJ and AUX, based on their inflection
form. The inflection information is propagated to the
phrases governing the inflected word as a head. We
adopted these symbols from the notation of Japanese
CCG described in (Bekki, 2010).
Subcategorization and voice Each verb has a
subcategorization frame, which is useful for build-
ing verb phrase structure. For instance, ??
tsukamu ?grasp? takes two arguments, nominative
and accusative cases, ??? ataeru ?give? takes
three arguments: nominative, accusative and dative
cases. We also added suffixes to verb tags to de-
note which arguments they require (n:nominative,
a:accusative and d: dative). For instance, the
verb??? ?give? takes three arguments (nomina-
tive, accusative and dative cases), it is tagged with
VB[nad].
We retrieve this information from a Japanese case
frame dictionary, Nihongo Goitaikei (Ikehara et al,
1997), which has 14,000 frames for 6,000 verbs and
adjectives. As an option, we also added voice infor-
mation (A:active, P:passive and C:causative) to the
verb phrases, because it effectively helps to discrim-
inate cases.
3.3 Annotation schema
We introduce phrase and function tags in Table 3 and
use them selectively based on the options described
below.
Tree Structure We first built a treebank with bi-
nary tree structure (except the root and terminal
nodes), because it is comparably easy to convert
the existing Japanese dependency-based corpus to
it. We converted the dependency-based corpus by
a previously described method in (Uematsu et al,
2013). The binary tree?s structure has the follow-
NP Noun phrase
PP Postposition phrase
VP Verb phrase
ADJP Adjective phrase
ADVP Adverbial phrase
CONJP Conjunction phrase
S Sentence (=root)
IP Inflectional phrase
IP-MAT Matrix clause
IP-ADV Adverb clause
IP-REL Gapping relative clause
IP-ADN Non-gapping adnominal clause
CP Complementizer phrase
CP-THT Sentential complement
Function tags
semantic role for mandatory argument (gap notation)
-ARG0 ( arg0)
-ARG1 ( arg1)
-ARG2 ( arg2)
grammatical role for mandatory argument (gap notation)
-SBJ ( sbj) Subjective case
-OBJ ( obj) Objective case
-OB2 ( ob2) Indirect object case
arbitrary argument
-TMP Temporal case
-LOC Locative case
-COORD Coordination (for n-ary)
-NCOORD Left branch of NP coord. (for binary)
-VCOORD Left branch of VP coord. (for binary)
-APPOS Apposition
-QUE Question
Table 3: Phrase tags
ing characteristics about verb phrase (VP) and post-
position phrase (PP): VP from the same bunsetsu
is a left-branching subtree and the PP-VP structure
(roughly corresponding to the argument-predicate
structure) is a right-branching subtree. Pure binary
trees tend to be very deep and difficult to annotate
and interpret by humans. We also built an n-ary tree
version by flattening these structures.
The predicate-argument structure, which is usu-
ally represented by PPs and a VP in the treebank,
particularly tends to be deep in binary trees based
on the number of arguments. To flatten the structure,
we remove the internal VP nodes by intermediately
re-attaching all of the argument PPs to the VP that
dominates the predicate. Figure 1 shows an example
of flattening the PP-VP structure.
For noun phrases, since compound nouns and nu-
merals cause deep hierarchy, the structure that in-
cludes them is flattened under the parent NP. The
coordinating structure is preserved, and each NP el-
ement of the coordination is flattened
111
IP-MAT
VP
VP
P
?
-PAST
VB
????
chase
PP-OBJ
PCS
?
-ACC
NN
?
cat
PP-SBJ
PCS
?
-NOM
NN
?
dog
NP
NP
NN
?
dog
IP-REL sbj
VP
P
?
-PAST
VB
????
chase
PP-OBJ
PCS
?
-ACC
NN
?
cat
NP
NP
NN
??
photo
IP-ADN
VP
VP
AUX
??
-PROG
VP
P
?
VB
????
chase
PP-OBJ
PCS
?
-ACC
NN
?
cat
PP-SBJ
PCS
?
-NOM
NP
?
dog
The dog chased the cat. The dog that chased the cat The photo of a dog chasing a cat
Figure 2: Leftmost tree shows annotation of grammatical roles in a basic inflectional phrase. Right two trees show
examples of adnominal phrases.
Predicates and arguments The predicate?s argu-
ment is basically marked with particles, which rep-
resent cases in Japanese; thus, they are represented
as a postpositional phrase, which is composed of
a noun phrase and particles. The leftmost tree in
Figure 2 is an example of the parse result of the
following sentence: ?-? inu-ga ?dog-NOM? ?-
? neko-o ?cat-ACC?????? oikaketa ?chased?
(The dog chased the cat.)
We annotated predicate arguments by two dif-
ferent schemes (different tag sets) in our treebank:
grammatical roles and semantic roles. In using a tag
set based on grammatical roles, the arguments are
assigned with the suffixes based on their syntactic
roles in the sentence, like Penn Treebank: SBJ (sub-
ject), OBJ (direct object), and OB2 (indirect object).
Figure 2 is annotated by this scheme.
Alternatively, the arguments are labeled based on
their semantic roles from case frame of predicates,
like PropBank (Palmer et al, 2005 ): ARG0, ARG1
and ARG2. These arguments are annotated by con-
verting semantic roles defined in the case frame dic-
tionary Goitaikei into simple labels, the labels are
not influenced by case alternation.
In both annotation schemes, we also annotated
two types of arbitrary arguments with semantic role
labels: LOC (locative) and TMP (temporal), which
can be assigned consistently and are useful for vari-
ous applications.
Adnominal clauses Clauses modifying noun
phrases are divided into two types: (gapping) rela-
tive and non-gapping adnominal clauses. Relative
clauses are denoted by adding function tag -REL to
phrase tag IP. Such a gap is directly attached to
IP-REL tag as a suffix consisting of an underscore
and small letters in our treebank, e.g., IP-REL sbj
for a subject-gap relative clause, so that the parser
can learn the type of gap simultaneously, unlike
the Penn Treebank style, where gaps are marked
as trace ?*T*?. For instance, note the structure of
the following noun phrase, which is shown in the
middle tree in Figure 2: ?-? neko-o ?cat-ACC?
????? oikake-ta ?to chase? ? inu ?dog?
?neko-o (cat-ACC) oikaketa (chase) inu? (The dog
that chased the cat.). We also adopt another type of
gap notation that resembles the predicate-argument
structure: semantic role notation. In the example
above, tag IP-REL arg0 is attached to the relative
clause instead.
We attach tag IP-ADN to another type of ad-
nominal clauses, which has no gap, the modified
noun phrase is not an argument of the predicate in
the adnominal clause. The rightmost in Figure 2 is
an example of a non-gapping clause: ?-? inu-ga
?dog-NOM? ?-? neko-o ?cat-ACC? ?????
?? oikake-teiru ?chasing? ?? shashin ?photo?
(A photo of a dog chasing a cat.), where there is no
predicate-argument relation between the verb ??
??? chase and the noun?? photo.
112
Coordination and apposition The notation of
such parallel structure as coordination and apposi-
tion differs based on the type of tree structure. For
binary trees, the coordination is represented by a
left-branching tree, which is a conjunction or a con-
junction particle that first joined a left hand con-
stituent; the phrase is marked as a modifier consist-
ing of coordination (-NCOORD and -VCOORD for
NP and VP coordinations), as shown on the left side
of Figure 3. On the other hand, in n-ary trees, all the
coordination elements and conjunctions are aligned
flatly under the parent phrase with suffix -COORD.
The apposition is represented in the same way using
tag -APPOS instead.
Phrase and sentential elements Since predicate
arguments are often omitted in Japanese, discrimi-
nation between the fragment of larger phrases and
sentential elements is not clear. In treebank, we em-
ploy IP and CP tags for inflectional and comple-
mentizer phrases, assuming that tags with function
tag suffixes to the phrase correspond to the max-
imum projection of the predicate (verb or adjec-
tive). The matrix phrase and the adverbial phrase
have IP-MAT and IP-ADV tags respectively. This
annotation schema is adopted based on the Penn
Historical Corpora (Santorini, 2010) and Japanese
Keyaki treebank (Butler et al, 2012) as previously
described, while IP in our treebank is not so flat as
them.
Such sentential complements as that-clauses in
English are tagged with CP-THT. In other words,
the sentential elements, which are annotated with
SBAR, S, and trace *T* in the Penn Treebank, are
tagged with CP or IP in our treebank.
4 Evaluation
The original Kyoto Corpus has 38,400 sentences
and they were automatically converted to constituent
structures. The function tags are also added to the
corpus by integrating predicate-argument informa-
tion in the NAIST Text corpus. Since the conver-
sion contains errors of structures and tags, about half
of them were manually checked to avoid the effects
of the conversion errors.
We evaluated our treebank?s effectiveness for
parser training with 18,640 sentences, which were
divided into three sets: 14,895 sentences for a train-
Tag set LF1 Comp UF1 Comp
binary tree
Base 88.4 34.0 89.6 37.9
Base inf 88.5? 33.5 90.0? 39.3
Fullsr 80.7 13.6 88.4 35.9
Fullsr inf 81.1? 15.5 ? 88.7? 36.9
Fullsr lex 79.8? 13.1 87.7? 34.3
Fullsr vsub 80.3? 12.5 87.9? 35.1
Fullsr vsub alt 78.6? 13.3 86.7? 32.5?
Fullgr 81.0 15.6 88.5 37.3
Fullgr inf 81.3? 15.3 88.8 37.2
Fullgr lex 80.3? 14.2 87.9? 33.6?
Fullgr vsub 81.2 15.5 88.5 35.2
Fullgr vsub alt 77.9? 11.7? 86.0? 29.9?
n-ary tree
Fullsr 76.7 11.4 85.3 28.0
Fullsr inf 76.9 11.6 85.4 28.7
Fullsr lex 76.5 11.1 84.7? 27.9
Fullsr vsub 76.5 10.8 84.9? 26.2
Fullsr vsub alt 76.6 11.0 84.8? 27.2
Fullgr 77.2 13.2 85.3 29.2
Fullgr inf 77.4 12.0? 85.5 28.3
Fullgr lex 77.6 12.2? 85.0 28.5
Fullgr vsub 77.1 12.7? 84.8? 28.8
Fullgr vsub alt 76.9 12.2? 84.7? 26.3?
Table 4: Parse results displayed by labeled and unla-
beled F1 metrics and proportion of sentences completely
matching gold standard (Comp). Base contains only ba-
sic tags, not grammatical function tags. Figures with ???
indicate statistically significant differences (? = 0.05)
from the results without complementary information, i.e.,
Fullsr or Fullgr.
113
NP
NN
??
interest
PP
PADN
?
-GEN
NP
NNP
B ?
B Company
PP-NCOORD
PCJ
?
CONJ
NNP
A ?
A Company
NP
NN
??
interest
PP
PADN
?
-GEN
NP-COORD
NNP
B ?
B Company
PCJ
?
CONJ
NNP
A ?
A Company
the interests of A Company and B Company
Figure 3: Noun phrase coordination
tag set UAS
binary tree
Base 89.1
Base inf 89.4
Fullsr 87.9
Fullsr inf 88.3
Fullgr 88.0
Fullgr inf 88.5?
n-ary (flattened) tree
Fullsr 82.8
Fullsr inf 83.3
Fullgr 82.9
Fullgr inf 83.0
Table 5: Dependency accuracies of the results converted
into bunsetsu dependencies.
ing set, 1,860 sentences for a test set, and the re-
mainder for a development set.
The basic evaluations were under the condition of
using the original tag sets: the basic set Base, which
contains all the preterminal tags in Table 1 and the
phrase tags in Table Table 3, except the IP and CP
tags, and the full set Full, which has Base + IP, CP
tags, and all the function tags. The basic set Base
is provided to evaluate the constituent parser perfor-
mance in case that we need better performance at the
cost of limiting the information.
We used two types of function tag sets: Full sr for
semantic roles and Full gr for grammatical roles.
We added the following complementary informa-
tion to the tags and named the new tag sets Base or
Full and suffix:
inf: add inflection information to the POS tag
(verbs, adjectives, and auxiliary verbs) and the
phrase tags (Table 2).
lex: lexicalize the closed words, i.e., auxiliary
verbs and particles.
vsub: add verb subcategorization to the verb and
verb phrase tags.
vsub alt: add verb subcategorization and case al-
ternation to the verb and verb phrase tags.
In comparing the system output with the gold stan-
dard, we remove the complementary information to
ignore different level of annotation, thus, we do not
discriminate between VB[na] and VB[nad] for
example.
We used the Berkeley parser (Petrov et al, 2006)
for our evaluation and trained with six iterations for
latent annotations. In training the n-ary trees, we
used a default Markovization parameter (h = 0, v =
1), because the parser performed the best with the
development set.
Table 4 shows the parsing results of the test sets.
On the whole, the binary tree outperformed the n-
ary tree. This indicates that the binary tree struc-
ture was converted from bunsetsu-based dependen-
cies, whose characteristics are described in Section
3.3, and is better for parser training than the partially
flattened structure.
114
As for additional information, the inflection suf-
fixes slightly improved the F1-metrics. This is
mainly because the inflection information gives the
category of the attached phrase (e.g., the attributive
form for noun phrases). The others did not provide
any improvement, even though we expected the sub-
categorization and case alternation information to
help the parser detect and discriminate the grammat-
ical roles, probably because we simply introduced
the information by concatenating the suffixes to the
base tags to adapt an off-the-shelf parser in our eval-
uation. For instance, VB[n] and VB[na] are rec-
ognized as entirely independent categories; a sophis-
ticated model, which can treat them hierarchically,
would improve the performance.
For comparison with a bunsetsu-based depen-
dency parser, we convert the parser output into unla-
beled bunsetsu dependencies by the following sim-
ple way. We first extract all bunsetsu chunks in
a sentence and find a minimum phrase including
each bunsetsu chunk from a constituent structure.
For each pair of bunsetsus having a common parent
phrase, we add a dependency from the left bunsetsu
to the right one, since Japanese is a head-final lan-
guage.
The unlabeled attachment scores of the converted
dependencies are shown as the accuracies in Table 5,
since most bunsetsu-based dependency parsers out-
put only unlabeled structure.
The Base inf results are comparable with the
bunsetsu-dependency results (90.46%) over the
same corpus (Kudo et al, 2002)1, which has only
the same level of information. Constituent parsing
with treebank almost matched the current bunsetsu
parsing.
5 Analysis
In this section, we analyze the error of parse results
from the point of view of the discrimination of gram-
matical and semantic roles, adnominal clause and
coordination.
Grammatical and semantic roles Predicate argu-
ments usually appeared as PP, which is composed of
noun phrases and particles. We focus on PPs with
function labels. Table 6 shows the PP results with
1The division for the training and test sets is different.
tag P R F1
PP-ARG0 64.9 75.0 69.6
PP-ARG1 70.6 80.1 75.1
PP-ARG2 60.3 68.5 64.1
PP-TMP 40.1 43.6 41.8
PP-LOC 23.8 17.2 20.0
tag P R F1
PP-SBJ 69.6 81.5 75.1
PP-OBJ 72.6 83.5 77.7
PP-OB2 63.6 71.4 67.3
PP-TMP 45.0 48.0 46.5
PP-LOC 21.3 15.9 18.2
Table 6: Discrimination of semantic role and grammati-
cal role labels (upper: semantic roles, lower: grammatical
role)
system \ gold PP-SBJ PP-OBJ PP-OB2
PP-SBJ *74.9 6.5 2.3
PP-OBJ 5.8 *80.1 0.5
PP-OB2 1.7 0.3 *68.5
PP-TMP 0.2 0.0 0.5
PP-LOC 0.2 0.0 0.4
PP 6.5 2.0 16.8
other labels 0.5 0.2 0.3
no span 10.2 10.9 11.0
system \ gold PP-TMP PP-LOC
PP-SBJ 4.7 4.1
PP-OBJ 0.0 0.0
PP-OB2 6.0 13.8
PP-TMP *43.6 2.8
PP-LOC 2.0 *17.2
PP 37.6 49.7
other labels 1.4 5.0
no span 4.7 7.4
Table 7: Confusion matrix for grammatical role labels
(recall). Figures with ?*? indicate recall.(binary tree,
Fullgr)
tag P R F1
IP-REL sbj 48.4 54.3 51.1
IP-REL obj 27.8 22.7 24.9
IP-REL ob2 17.2 29.4 21.7
IP-ADN 50.9 55.4 53.1
CP-THT 66.1 66.6 66.3
Table 8: Results of adnominal phrase and sentential ele-
ment (binary tree, Fullgr)
115
grammatical and semantic labels under the Fullsr
and Fullgr conditions respectively.
The precision and the recall of mandatory argu-
ments did not reach a high level. The results are
related to predicate argument structure analysis in
Japanese. But, they cannot be directly compared,
because the parser in this evaluation must output a
correct target phrase and select it as an argument, al-
though most researches select a word using a gold
standard parse tree. Hayashibe et al ( 2011 ) re-
ported the best precision of ARG0 discrimination to
be 88.42 % 2, which is the selection results from
the candidate nouns using the gold standard parse
tree of NTC. If the cases where the correct candi-
dates did not appear in the parser results are ex-
cluded (10.8 %), the precision is 72.7 %. The main
remaining error is to label to non-argument PP with
suffix -ARG0 (17.0%), thus, we must restrain the
overlabeling to improve the precision.
The discrimination of grammatical role is higher
than that of semantic role, which is more directly es-
timated by case particles following the noun phrases.
The confusion matrix for the recall in Table 7 shows
main problem is parse error, where correct phrase
span does not exist (no span), and marks 10-11%.
The second major error is discrimination from bare
PPs (PPs without suffixes), mainly because the clues
to judge whether the arguments are mandatory or ar-
bitrary lack in the treebank. Since even the manda-
tory arguments are often omitted in Japanese, it is
not facilitate to identify arguments of predicates by
using only syntactic information.
Adnominal phrases We need to discriminate be-
tween two types of adnominal phrases as described
in Section 3.3: IP-REL and IP-ADN. Table 8
shows the discrimination results of the adnominal
phrase types. The difference between IP-REL
(gapped relative clauses) and IP-ADN is closely re-
lated to the discrimination of the grammatical role:
whether the antecedent is the argument of the head
predicate of the relative clause.
Table 8 shows the discrimination results of the
adnominal phrases. The results indicate the diffi-
culties of discriminating the type of gaps of rela-
2The figure is calculated only for the arguments that appear
as the dependents of predicates, excluding the omitted argu-
ments.
tive clause IP-REL. The confusion matrix in Ta-
ble 9 shows that the discrimination between gaps
and non-gaps, i.e., IP-REL and IP-ADN, is moder-
ate as for IP-REL sbj and IP-REL obj. How-
ever, IP-REL ob2 is hardly recognized, because
it is difficult to determine whether the antecedent,
which is marked with particle ?ni?, is a mandatory ar-
gument (IP-REL ob2) or not (IP-ADN). Increas-
ing training samples would improve the discrimina-
tion, since there are only 290 IP-REL ob2 tags for
8,100 IP-ADN tags in the training set.
Naturally discrimination only by syntactic infor-
mation has limitation; this baseline can be improved
by incorporating semantic information.
Coordination Figure 10 shows the coordination
results, which are considered the baseline for only
using syntactic information. Improvement is possi-
ble by incorporating semantic information, since the
disambiguation of coordination structure essentially
needs semantic information.
6 Conclusion
We constructed a Japanese constituent-based parser
to be released from the constraints of bunsetsu-based
analysis to simplify the integration of syntactic and
semantic analysis. Our evaluation results indicate
that the basic performance of the parser trained with
the treebank almost equals bunsetsus-based parsers
and has the potential to supply detailed syntactic
information by grammatical function labels for se-
mantic analysis, such as predicate-argument struc-
ture analysis.
Future work will be to refine the annotation
scheme to improve parser performance and to eval-
uate parser results by adapting them to such NLP
applications as machine translation.
Acknowledgments
We would like to thank Associate Professor Yusuke
Miyao, Associate Professor Takuya Matsuzaki of
National Institute of Informatics and Sumire Ue-
matsu of the University of Tokyo for providing us
the language resources and giving us valuable sug-
gestions.
116
system \ gold IP-REL sbj IP-REL obj
IP-REL sbj *55.0 30.3
IP-REL obj 8.5 *33.3
IP-REL ob2 0.5 0.0
IP-ADN 10.0 9.0
IP-ADV 0.3 0.0
VP 8.5 7.6
other labels 1.2 6.2
no span 16.0 13.6
system \ gold IP-REL ob2 IP-ADN
IP-REL sbj 29.4 7.5
IP-REL obj 0.0 0.6
IP-REL ob2 *11.8 0.0
IP-ADN 23.5 *57.3
IP-ADV 0.5 0.3
VP 17.6 9.3
other labels 5.4 3.0
no span 11.8 22.0
Table 9: Confusion matrix for adnominal phrases (recall).
Figures with ?*? indicate recall.(binary tree, Fullgr)
tag P R F1
NP-COORD 62.6 60.7 61.6
VP-COORD 57.6 50.0 53.5
NP-APPOS 46.0 40.0 42.8
Table 10: Results of coordination and apposition (binary
tree, Fullgr)
References
Masayuki Asahara. 2013. Comparison of syntactic de-
pendency annotation schemata . In Proceedings of
the 3rd Japanese Corpus Linguistics Workshop, In
Japanese.
Daisuke Bekki. 2010. Formal theory of Japanese syntax.
Kuroshio Shuppan, In Japanese.
Daniel M. Bikel. 2004. A distributional analysis of a
lexicalized statistical parsing model. In Proceedings
of Empirical Methods in Natural Language Processing
(EMNLP 2004), Vol.4, pp. 182?189.
Francis Bond, Sanae Fujita and Takaaki Tanaka. 2008.
The Hinoki syntactic and semantic treebank of
Japanese. In Journal of Language Resources and
Evaluation, Vol.42, No. 2, pp. 243?251
Alastair Butler, Zhu Hong, Tomoko Hotta, Ruriko
Otomo, Kei Yoshimoto and Zhen Zhou. 2012. Keyaki
Treebank: phrase structure with functional informa-
tion for Japanese. In Proceedings of Text Annotation
Workshop.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, (NAACL 2000), pp. 132?139.
DongHyun Choi, Jungyeul Park and Key-Sun Choi.
2012. Korean treebank transformation for parser train-
ing. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (ACL
2012), pp. 78-88.
Ann Copestake, Dan Flickinger, Carl Pollard and Ivan A.
Sag. 2005. Minimal recursion semantics: an introduc-
tion. Research on Language and Computation, Vol. 3,
No. 4, pp. 281-332.
Ryan Gabbard, Mitchell Marcus and Seth Kulick. 2006.
Fully parsing the Penn Treebank. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association of Compu-
tational Linguistics (HLT-NAACL 2006), pp. 184?191.
Takao Gunji. 1987 Japanese phrase structure grammar:
a unification-based approach. D.Reidel.
Yuta Hayashibe, Mamoru Komachi and Yujzi Mat-
sumoto. 2011. Japanese predicate argument struc-
ture analysis exploiting argument position and type. In
Proceedings of the 5th International Joint Conference
on Natural Language Processing (IJCNLP 2011), pp.
201-209.
Ryu Iida, Mamoru Komachi Kentaro Inui and Yuji Mat-
sumoto. 2007. Annotating a Japanese text corpus with
predicate-argument and coreference relations. In Pro-
ceedings of Linguistic Annotation Workshop, pp. 132?
139.
Ryu Iida, Massimo Poesio. 2011. A cross-lingual ILP
solution to zero anaphora resolution. In Proceedings
117
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL-HLT 2011), pp. 804-813.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Kentaro Ogura, Yoshifumi Ooyama and Yoshi-
hiko Hayashi. 1998. Nihongo Goitaikei. Iwanami
Shoten, In Japanese.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
Functional Grammar: a formal system for grammat-
ical representation. In the Mental Representation of
Grammatical Relations (Joan Bresnan ed.), pp. 173?
281. The MIT Press.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association of Compu-
tational Linguistics (HLT-NAACL 2006), pp. 176?183.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
processing. Advances in Neural Information Process-
ing Systems, 15:3?10.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the 6th Conference on Natural Language
Learning (CoNLL-2002), Volume 20, pp. 1?7.
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing
system. In Abeille (ed.), Treebanks: Building and us-
ing parsed corpora, Chap. 14, pp. 249?260. Kluwer
Academic Publishers.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. In Journal of Com-
putational Linguistics. Vol.19, No.2, pp. 313?330.
Hiroshi Masuichi, Tomoko Okuma, Hiroki Yoshimura
and Yasunari Harada. 2003 Japanese parser on the ba-
sis of the Lexical-Functional Grammar formalism and
its evaluation. In Proceedings of the 17th Pacific Asia
Conference on Language, Information and Computa-
tion (PACLIC 17), pp. 298-309.
Masaaki Nagata and Tsuyoshi Morimoto,. 1993.
A unification-based Japanese parser for speech-to-
speech translation. In IEICE Transaction on Informa-
tion and Systems. Vol.E76-D, No.1, pp. 51?61.
Tomoya Noro, Taiichi Hashimoto, Takenobu Tokunaga
and Hotsumi Tanaka. 2005. Building a large-scale
Japanese syntactically annotated corpus for deriving a
CFG. in Proceedings of Symposium on Large-Scale
Knowledge Resources (LKR2005), pp..159 ? 162.
Matha Palmer, Daniel Gildea and Paul Kingsbury. 2005.
The Proposition Bank: n annotated corpus of semantic
roles. Computational Linguistics, Vol.31 No. 1, pp.
71?106.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein.. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association
for Computational Linguistics (COLING-ACL 2006),
pp. 433-440.
Ivan A. Sag, Thomas Wasow and Emily M. Bender,.
2003. Syntactic theory: a formal introduction. 2nd
Edition, CSLI Publications.
Beatrice Santorini. 2010. Annotation manual for the
Penn Historical Corpora and the PCEEC (Release 2).
Department of Linguistics, University of Pennsylva-
nia.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of Japanese. In Proceedings of the
3rd Workshop on Asian Language Resources and In-
ternational Standardization at the 19th International
Conference on Computational Linguistics, Vol. 12, pp.
1?8.
Sumire Uematsu, Takuya Matsuzaki, Hiroaki Hanaoka,
Yusuke Miyao and Hideki Mima. 2013. Integrat-
ing multiple dependency corpora for inducing wide-
coverage Japanese CCG resources. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013), pp. 1042?1051.
118
