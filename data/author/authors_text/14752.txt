                                                               Edmonton, May-June 2003
                                                                     Tutorials , pg. 3
                                                              Proceedings of HLT-NAACL
Use and Acquisition of Semantic Language Model 
 
Kuansan Wang Ye-Yi Wang Alex Acero 
Speech Technology Group, Microsoft Research 
One Microsoft Way, Redmond, WA 98052, USA 
http://research.microsoft.com/srg 
 
 
Abstract 
Semantic language model is a technique that 
utilizes the semantic structure of an utterance 
to better rank the likelihood of words compos-
ing the sentence. When used in a conversa-
tional system, one can dynamically integrate 
the dialog state and domain semantics into the 
semantic language model to better guide the 
speech recognizer executing the decoding 
process. We describe one such application that 
employs semantic language model to cope 
with spontaneous speech in a robust manner. 
The semantic language model, though can be 
manually crafted without data, can benefit 
significantly from data driven machine learn-
ing techniques. An example based approach is 
also described here to demonstrate a viable 
approach. 
1 Introduction 
Any spoken language understanding system must deal 
with two critical issues: how to accurately infer user?s 
intention from speech, and how to do it robustly amidst 
the prevalent spontaneous speech effects where users 
would inevitably stutter, hesitate, and self correct them-
selves on a regular basis. To address these issues, it has 
been proposed (Miller et al, 1994; Wang, 2000; Esteve 
et al, 2003) that one can extend the statistical pattern 
recognition framework commonly used for automatic 
speech recognition (ASR) to the spoken language un-
derstanding (SLU) problem. The ?pattern? to be recog-
nized for ASR is a string of word, and for SLU, a tree 
of semantic objects that represent the domain entities 
and tasks that describe the user?s intention. As is the 
case for ASR where a language model plays the pivotal 
role in guiding the recognizer to compose plausible 
string hypotheses, a pattern recognition based SLU re-
lies on what is often called the semantic language model 
(SLM) to detect semantic objects and construct a parse 
tree from the user?s utterance. Because the end outcome 
is a parse tree, SLM is usually realized using the struc-
tured language model techniques so that the semantic 
structure of the utterance can be included in modeling 
the language (Wang, 2000; Erdogan et al, 2002). 
In this article, we describe an application of SLM in 
the semantic synchronous understanding (SSU) frame-
work for multimodal conversational systems. A key idea 
of SSU is to immediately recognize and parse user?s 
utterance, accepting only speech segments conforming 
to the prediction of SLM while the user is still speaking. 
Since the SLM can be updated in real-time during the 
course of interaction, irrelevant expressions, including 
the spontaneous speech, can be gracefully rejected 
based on what makes sense to the dialog context. In Sec. 
2, we describe a study on the efficacy of SSU for a mo-
bile personal information management (PIM) applica-
tion called MiPad (Huang et al, 2000). The SLM used 
there was manually derived with combined CFG and N-
gram (Microsoft, 1999; Wang, 2002) by consulting the 
structure of the PIM back end without any user data. 
Obviously, the linguistic coverage of the SLM can be 
further improved with modern data-driven learning 
techniques. In Sec. 3, we describe one such learning 
technique that can utilize the manually crafted model as 
a bootstrapping template to enrich the SLM when suit-
able amount of training data become available. 
2 SSU MiPad? 
MiPad is a Web based PIM application that facilitates 
multimodal access to personal email, calendar, and con-
tact information. MiPad users can combine speech 
commands with pen gestures to query PIM database, 
compose or modify email messages or appointments. 
We recently implemented a version of MiPad in HTML 
and SALT, taking the native support of SSU in SALT 
                                                          
? A video demonstration of SSU MiPad is  available for 
download at http://research.microsoft.com/srg/videos/Mi-
PadDemo_2Mbit.wmv 
(Wang, 2002). Whenever a semantic object is detected, 
the PIM logic based on the current semantic parse is 
executed and the screen updated accordingly. The na-
ture of SSU insures that the user receives immediate 
feedback on the process of SLU, and therefore can re-
phrase rejected and correct misrecognized speech seg-
ments. Studies (Wang, 2003) that contrast SSU with 
conventional turn taking based system show that, be-
cause SSU copes with spontaneous speech better, it elic-
its longer user utterances and hence fewer sentences are 
needed to complete a task. The highly interactive nature 
of SSU lends itself to more effective dynamic visual 
prompting, leading lower out of domain utterances. SSU 
also simplifies the confirmation strategy as every se-
mantic object can be implicitly confirmed. Users have 
no trouble dealing with this strategy. In fact, users natu-
rally correct and rephrase based on the immediate feed-
back, making their speech even more spontaneous. All 
these results are statistically significant. Finally and 
most intriguingly, users feel they accomplish tasks 
faster in the SSU system even though the through puts 
from both systems are statistically tied. 
3 SLM Learning 
SLU utilizes SLM to infer user?s intention from speech. 
Before sufficient data make it practical to use machine 
learning techniques, SLM often has to be developed 
manually. The manual development process is labor-
intensive, requires expertise in linguistics and speech 
understanding, and often lacks good coverage because it 
is hard for a developer to anticipate all possible lan-
guage constructions that different users may choose to 
express their minds. The manually developed model is 
therefore not robust to extra-grammaticality commonly 
found in spontaneous speech. An approach to address 
this problem is to employ a robust parser to loosen the 
constraints specified in the SLM, which sometimes re-
sults in unpredictable system behavior (Wang, 2001).  
The robust parser approach also mandates a separate 
understanding pass from speech recognition. The results 
tend to be suboptimal since the first pass, optimizing 
ASR word accuracy, does not necessarily lead to a 
higher overall SLU accuracy (Wang and Acero, 2003b).  
We have developed example-based grammar leaning 
algorithms to acquire SLM for speech understanding. It 
is shown (Wang and Acero, 2002) that a grammar learn-
ing algorithm may result in a semantic context free 
grammar that has better coverage than manually au-
thored grammar.  It is demonstrated (Wang and Acero, 
2003a) that a statistical model can also be obtained by 
the learning algorithm, and the model itself is robust to 
extra-grammaticality in spontaneous speech. Therefore, 
a robust parser is no longer necessary. Most impor-
tantly, such a statistical SLM can be incorporated di-
rectly into the search algorithm for ASR, making a 
single pass, joint speech recognition and understanding 
process such as SSU possible.  Because of that, the 
model can be trained directly to optimize the under-
standing accuracy. It is shown (Wang and Acero, 
2003b) that the single pass approach achieved a 17% 
understanding accuracy improvement even though there 
is a signification word error rate increase, suggesting 
that optimizing ASR and SLU accuracy may indeed be 
two very different businesses after all. 
References 
Erdogan H. Sarikaya R. Gao Y. Picheny M. 2002. Se-
mantic structured language models. Proc. ICSLP-
2002, Denver, CO. 
Esteve Y. Raymond C. Bechet F. De Mori R. 2003. 
Conceptual decoding for spoken dialog systems. 
Proc. EuroSpeech-2003, Geneva, Switzerland. 
Huang X. et al MiPad: A next generation PDA proto-
type. Proc. ICSLP-2000, Beijing, China. 
Microsoft Corporation. 1999. Speech Application Pro-
gram Interface (SAPI), Version 5. 
Miller S. Bobrow R. Ingria R. and Schwartz R.  1994. 
Hidden understanding models of natural language.  
Proc. 32nd Annual Meeting of ACL, Las Cruces, NM. 
Wang K.  2000. A plan-based dialog system with prob-
abilistic inferences.  Proc. ICSLP-2000,  Beijing, 
China. 
Wang K. 2002. SALT: A spoken language understand-
ing interface for Web-based multimodal dialog sys-
tems.  Proc. ICSLP-2002, Denver, CO. 
Wang K. 2003.  Semantic synchronous understanding 
for robust spoken language applications.  Proc. 
ASRU-2003, St. Thomas, Virgin Islands. 
Wang Y. 2001. Robust language understanding in Mi-
Pad. Proc. EuroSpeech-2001. Aalborg, Demark. 
Wang Y.  Acero A. 2002. Evaluation of spoken lan-
guage grammar learning in the ATIS domain. Proc. 
ICASSP-2002, Orlando, FL. 
Wang Y.  Acero A. 2003a. Combination of CFG and N-
gram modeling in semantic grammar learning.  Proc. 
EuroSpeech-2003.  Geneva, Switzerland. 
Wang Y.  Acero A. 2003b. Is word error rate a good 
indicator for spoken language understanding accu-
racy?  Proc. ASRU-2003,  St. Thomas, Virgin Is-
lands. 
 
NAACL HLT Demonstration Program, pages 31?32,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Voice-Rate: A Dialog System for Consumer Ratings 
 
Geoffrey Zweig, Y.C. Ju, Patrick Nguyen, Dong Yu, 
Ye-Yi Wang and Alex Acero 
Speech Research Group 
Microsoft Corp. 
Redmond, WA 98052 
{gzweig,yuncj,panguyen,dongyu, yeyi-
wang,alexac}@microsoft.com 
 
  
Abstract 
Voice-Rate is an automated dialog system 
which provides access to over one million 
ratings of products and businesses. By 
calling a toll-free number, consumers can 
access ratings for products, national busi-
nesses such as airlines, and local busi-
nesses such as restaurants. Voice-Rate 
also has a facility for recording and ana-
lyzing ratings that are given over the 
phone. The service has been primed with 
ratings taken from a variety of web 
sources, and we are augmenting these 
with user ratings. Voice-Rate can be ac-
cessed by dialing 1-877-456-DATA. 
1 Overview 
 Voice-Rate is an automated dialog system de-
signed to help consumers while they are shopping. 
The target user is a consumer who is considering 
making an impulse purchase and would like to get 
more information. He or she can take out a cell-
phone, call Voice-Rate, and get rating information 
to help decide whether to buy the item. Here are 
three sample scenarios: 
 
 Sally has gone to Home Depot to buy 
some paint to touch-up scratches on the 
wall at home. She?ll use exactly the same 
color and brand as when she first painted 
the wall, so she knows what she wants. 
While at Home Depot, however, Sally sees 
some hand-held vacuum cleaners and de-
cides it might be nice to have one. But, she 
is unsure whether which of the available 
models is better: The ?Black & Decker 
CHV1400 Cyclonic DustBuster,? the 
?Shark SV736? or the ?Eureka 71A.? Sally 
calls Voice-Rate and gets the ratings and 
makes an informed purchase. 
 John is on vacation with his family in Seat-
tle. After going up in the Space Needle, 
they walk by ?Abbondanza Pizzeria? and 
are considering lunch there. While it looks 
good, there are almost no diners inside, 
and John is suspicious. He calls Voice-
Rate and discovers that in fact the restau-
rant is highly rated, and decides to go 
there. 
 Returning from his vacation, John drops 
his rental car off at the airport. The rental 
company incorrectly asserts that he has 
scratched the car, and causes a big hassle, 
until they finally realize that they already 
charged the last customer for the same 
scratch. Unhappy with the surly service, 
John calls Voice-Rate and leaves a warn-
ing for others.  
 
Currently, Voice-Rate can deliver ratings for over 
one million products, two hundred thousand res-
taurants in over sixteen hundred cities; and about 
three thousand national businesses.  
2 Technical Challenges 
To make Voice-Rate operational, it was necessary 
to solve the key challenges of name resolution and 
disambiguation. Users rarely make an exactly cor-
rect specification of a product or business, and it is 
necessary both to utilize a ?fuzzy-match? for name 
lookup, and to deploy a carefully designed disam-
biguation strategy.  
31
Voice-Rate solves the fuzzy-matching process by 
treating spoken queries as well as business and 
product names as documents, and then performing 
TF-IDF based lookup. For a review of name 
matching methods, see e.g. Cohen et al, 2003. In 
the ideal case, after a user asks for a particular 
product or business, the best-matching item as 
measured by TF-IDF would be the one intended by 
the user. In reality, of course, this is often not the 
case, and further dialog is necessary to determine 
the user?s intent. For concreteness, we will illu-
strate the disambiguation process in the context of 
product identification. 
 
When a user calls Voice-Rate and asks for a prod-
uct review, the system solicits the user for the 
product name, does TF-IDF lookup, and presents 
the highest-scoring match for user confirmation. If 
the user does not accept the retrieved item, Voice-
Rate initiates a disambiguation dialog.  
 
Aside from inadequate product coverage, which 
cannot be fixed at runtime, there are two possible 
sources for error: automatic speech recognition 
(ASR) errors, and TF-IDF lookup errors.  The dis-
ambiguation process begins by eliminating the 
first. To do this, it asks the user if his or her exact 
words were the recognized text, and if not to repeat 
the request. This loop iterates twice, and if the us-
er?s exact words still have not been identified, 
Voice-Rate apologizes and hangs up. 
 
Once the user?s exact words have been validated, 
Voice-Rate gets a positive identification on the 
product category. From the set of high-scoring TF-
IDF items, a list of possible categories is compiled. 
For example, for ?The Lord of the Rings The Two 
Towers,? there are items in Video Games, DVDs, 
Music, VHS, Software, Books, Websites, and Toys 
and Games. These categories are read to the user, 
who is asked to select one. All the close-matching 
product names in the selected category are then 
read to the user, until one is selected or the list is 
exhausted.  
3 Related Work 
To our knowledge, Voice-Rate is the first large 
scale ratings dialog system. However, the technol-
ogy behind it is closely related to previous dialog 
systems, especially directory assistance or ?411? 
systems (e.g. Kamm et al, 1994, Natarajan et al, 
2002, Levin et al, 2005, Jan et al, 2003).  A gen-
eral discussion of name-matching techniques such 
as TF-IDF can be found in (Cohen et al, 2003, 
Bilenko et al, 2003). 
 
The second area of related research has to do with 
web rating systems. Interesting work on extracting 
information from such ratings can be found in, e.g. 
(Linden et al, 2003, Hu et al, 2004, Gammon et 
al., 2005). Work has also been done using text-
based input to determine relevant products (Chai et 
al., 2002).  Our own work differs from this in that 
it focuses on spoken input, and in its breadth ? 
covering both products and businesses. 
References  
M. Bilenko, R. Mooney, W. W. Cohen, P. Ravikumar and S. 
Fienberg. 2003. Adaptive Name-Matching in Information 
Integration. IEEE Intelligent Systems 18(5): 16-23 (2003).  
J. Chai, V. Horvath, N. Nicolov, M. Stys, N. Kambhatla, W. 
Zadrozny and P. Melville.  2002. Natural Language Assis-
tant- A Dialog System for Online Product Recommenda-
tion. AI Magazine (23), 2002 
 
W. W. Cohen, P Ravikumar and S. E. Fienberg . 2003. A 
comparison of string distance metrics for name-matching 
tasks. Proceedings of the IJCAI-2003 Workshop on Infor-
mation, 2003 
M.  Gamon, A. Aue, S. Corston-Oliver and E. Ringger. 2005. 
Pulse: Mining Customer Opinions from Free Text. In Lec-
ture Notes in Computer Science. Vol. 3646. Springer Ver-
lag. (IDA 2005)., pages 121-132. 
M. Hu and B. Liu. 2004. Mining and summarizing customer 
reviews. Proceedings of the 2004 ACM SIGKDD interna-
tional conference. 
 
E. E. Jan, B. Maison, L. Mangu and G. Zweig. 2003. Auto-
matic construction of Unique Signatures and Confusable 
sets for Natural Language Directory Assistance Applica-
tion.  Eurospeech 2003 
C. A. Kamm, K. M. Yang, C. R. Shamieh and S. Singhal. 
1994. Speech recognition issues for directory assistance 
applications. Second IEEE Workshop on Interactive Voice 
Technology for Telecommunications Applications. 
 
E. Levin and A. M. Man?. 2005. Voice User Interface Design 
for Automated Directory Assistance Eurospeech 2005. 
G. Linden, B. Smith and J. York. Amazon.com recommenda-
tions: item-to-item collaborative filtering. 2003.  Internet 
Computing, IEEE , vol.7, no.1pp. 76- 80. 
 
P. Natarajan, R. Prasad, R. Schwartz and J. Makhoul. 2002. A 
Scalable Architecture for Directory Assistance Automation, 
ICASSP 2002, Orlando, Florida. 
32
Proceedings of the 43rd Annual Meeting of the ACL, pages 443?450,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Position Specific Posterior Lattices for Indexing Speech
Ciprian Chelba and Alex Acero
Microsoft Research
Microsoft Corporation
One Microsoft Way
Redmond, WA 98052
{chelba, alexac}@microsoft.com
Abstract
The paper presents the Position Specific
Posterior Lattice, a novel representation
of automatic speech recognition lattices
that naturally lends itself to efficient in-
dexing of position information and subse-
quent relevance ranking of spoken docu-
ments using proximity.
In experiments performed on a collec-
tion of lecture recordings ? MIT iCam-
pus data ? the spoken document rank-
ing accuracy was improved by 20% rela-
tive over the commonly used baseline of
indexing the 1-best output from an auto-
matic speech recognizer. The Mean Aver-
age Precision (MAP) increased from 0.53
when using 1-best output to 0.62 when us-
ing the new lattice representation. The ref-
erence used for evaluation is the output of
a standard retrieval engine working on the
manual transcription of the speech collec-
tion.
Albeit lossy, the PSPL lattice is also much
more compact than the ASR 3-gram lat-
tice from which it is computed ? which
translates in reduced inverted index size
as well ? at virtually no degradation in
word-error-rate performance. Since new
paths are introduced in the lattice, the OR-
ACLE accuracy increases over the origi-
nal ASR lattice.
1 Introduction
Ever increasing computing power and connectivity
bandwidth together with falling storage costs re-
sult in an overwhelming amount of data of vari-
ous types being produced, exchanged, and stored.
Consequently, search has emerged as a key applica-
tion as more and more data is being saved (Church,
2003). Text search in particular is the most active
area, with applications that range from web and in-
tranet search to searching for private information re-
siding on one?s hard-drive.
Speech search has not received much attention
due to the fact that large collections of untranscribed
spoken material have not been available, mostly
due to storage constraints. As storage is becoming
cheaper, the availability and usefulness of large col-
lections of spoken documents is limited strictly by
the lack of adequate technology to exploit them.
Manually transcribing speech is expensive and
sometimes outright impossible due to privacy con-
cerns. This leads us to exploring an automatic ap-
proach to searching and navigating spoken docu-
ment collections.
Our current work aims at extending the standard
keyword search paradigm from text documents to
spoken documents. In order to deal with limitations
of current automatic speech recognition (ASR) tech-
nology we propose an approach that uses recogni-
tion lattices ? which are considerably more accu-
rate than the ASR 1-best output.
A novel contribution is the use of a representation
of ASR lattices which retains only position informa-
tion for each word. The Position Specific Posterior
443
Lattice (PSPL) is a lossy but compact representa-
tion of a speech recognition lattice that lends itself
to the standard inverted indexing done in text search
? which retains the position as well as other con-
textual information for each hit.
Since our aim is to bridge the gap between text
and speech -grade search technology, we take as our
reference the output of a text retrieval engine that
runs on the manual transcription.
The rest of the paper is structured as follows: in
the next section we review previous work in the
area, followed by Section 3 which presents a brief
overview of state-of-the-art text search technology.
We then introduce the PSPL representation in Sec-
tion 4 and explain its use for indexing and searching
speech in the next section. Experiments evaluating
ASR accuracy on iCampus, highlighting empirical
aspects of PSPL lattices as well as search accuracy
results are reported in Section 6. We conclude by
outlining future work.
2 Previous Work
The main research effort aiming at spoken docu-
ment retrieval (SDR) was centered around the SDR-
TREC evaluations (Garofolo et al, 2000), although
there is a large body of work in this area prior to
the SDR-TREC evaluations, as well as more recent
work outside this community. Most notable are the
contributions of (Brown et al, 1996) and (James,
1995).
One problem encountered in work published prior
or outside the SDR-TREC community is that it
doesn?t always evaluate performance from a doc-
ument retrieval point of view ? using a metric
like Mean Average Precision (MAP) or similar, see
trec_eval (NIST, www) ? but rather uses word-
spotting measures, which are more technology-
rather than user- centric. We believe that ultimately
it is the document retrieval performance that matters
and the word-spotting accuracy is just an indicator
for how a SDR system might be improved.
The TREC-SDR 8/9 evaluations ? (Garofolo et
al., 2000) Section 6 ? focused on using Broadcast
News speech from various sources: CNN, ABC,
PRI, Voice of America. About 550 hrs of speech
were segmented manually into 21,574 stories each
comprising about 250 words on the average. The
approximate manual transcriptions ? closed cap-
tioning for video ? used for SDR system compar-
ison with text-only retrieval performance had fairly
high WER: 14.5% video and 7.5% radio broadcasts.
ASR systems tuned to the Broadcast News domain
were evaluated on detailed manual transcriptions
and were able to achieve 15-20% WER, not far from
the accuracy of the approximate manual transcrip-
tions. In order to evaluate the accuracy of retrieval
systems, search queries ??topics? ? along with bi-
nary relevance judgments were compiled by human
assessors.
SDR systems indexed the ASR 1-best output and
their retrieval performance ? measured in terms of
MAP ? was found to be flat with respect to ASR
WER variations in the range of 15%-30%. Simply
having a common task and an evaluation-driven col-
laborative research effort represents a huge gain for
the community. There are shortcomings however to
the SDR-TREC framework.
It is well known that ASR systems are very brit-
tle to mismatched training/test conditions and it is
unrealistic to expect error rates in the range 10-15%
when decoding speech mismatched with respect to
the training data. It is thus very important to con-
sider ASR operating points which have higher WER.
Also, the out-of-vocabulary (OOV) rate was very
low, below 1%. Since the ?topics?/queries were
long and stated in plain English rather than using
the keyword search paradigm, the query-side OOV
(Q-OOV) was very low as well, an unrealistic situ-
ation in practice. (Woodland et al, 2000) evaluates
the effect of Q-OOV rate on retrieval performance
by reducing the ASR vocabulary size such that the
Q-OOV rate comes closer to 15%, a much more re-
alistic figure since search keywords are typically rare
words. They show severe degradation in MAP per-
formance ? 50% relative, from 44 to 22.
The most common approach to dealing with OOV
query words is to represent both the query and the
spoken document using sub-word units ? typically
phones or phone n-grams ? and then match se-
quences of such units. In his thesis, (Ng, 2000)
shows the feasibility of sub-word SDR and advo-
cates for tighter integration between ASR and IR
technology. Similar conclusions are drawn by the
excellent work in (Siegler, 1999).
As pointed out in (Logan et al, 2002), word level
444
indexing and querying is still more accurate, were
it not for the OOV problem. The authors argue in
favor of a combination of word and sub-word level
indexing. Another problem pointed out by the pa-
per is the abundance of word-spotting false-positives
in the sub-word retrieval case, somewhat masked by
the MAP measure.
Similar approaches are taken by (Seide and Yu,
2004). One interesting feature of this work is a two-
pass system whereby an approximate match is car-
ried out at the document level after which the costly
detailed phonetic match is carried out on only 15%
of the documents in the collection.
More recently, (Saraclar and Sproat, 2004) shows
improvement in word-spotting accuracy by using
lattices instead of 1-best. An inverted index from
symbols ? word or phone ? to links allows to
evaluate adjacency of query words but more gen-
eral proximity information is harder to obtain ? see
Section 4. Although no formal comparison has been
carried out, we believe our approach should yield a
more compact index.
Before discussing our architectural design deci-
sions it is probably useful to give a brief presentation
of a state-of-the-art text document retrieval engine
that is using the keyword search paradigm.
3 Text Document Retrieval
Probably the most widespread text retrieval model is
the TF-IDF vector model (Baeza-Yates and Ribeiro-
Neto, 1999). For a given query Q = q1 . . . qi . . . qQ
and document Dj one calculates a similarity mea-
sure by accumulating the TF-IDF score wi,j for each
query term qi, possibly weighted by a document spe-
cific weight:
S(Dj ,Q) =
Q?
i=1
wi,j
wi,j = fi,j ? idfi
where fi,j is the normalized frequency of word qi in
document Dj and the inverse document frequency
for query term qi is idfi = log Nni where N is the
total number of documents in the collection and ni
is the number of documents containing qi.
The main criticism to the TF-IDF relevance score
is the fact that the query terms are assumed to be
independent. Proximity information is not taken into
account at all, e.g. whether the words LANGUAGE
and MODELING occur next to each other or not in
a document is not used for relevance scoring.
Another issue is that query terms may be encoun-
tered in different contexts in a given document: ti-
tle, abstract, author name, font size, etc. For hy-
pertext document collections even more context in-
formation is available: anchor text, as well as other
mark-up tags designating various parts of a given
document being just a few examples. The TF-IDF
ranking scheme completely discards such informa-
tion although it is clearly important in practice.
3.1 Early Google Approach
Aside from the use of PageRank for relevance rank-
ing, (Brin and Page, 1998) also uses both proxim-
ity and context information heavily when assigning
a relevance score to a given document ? see Sec-
tion 4.5.1 of (Brin and Page, 1998) for details.
For each given query term qi one retrieves the list
of hits corresponding to qi in document D. Hits
can be of various types depending on the context in
which the hit occurred: title, anchor text, etc. Each
type of hit has its own type-weight and the type-
weights are indexed by type.
For a single word query, their ranking algorithm
takes the inner-product between the type-weight
vector and a vector consisting of count-weights (ta-
pered counts such that the effect of large counts is
discounted) and combines the resulting score with
PageRank in a final relevance score.
For multiple word queries, terms co-occurring in a
given document are considered as forming different
proximity-types based on their proximity, from adja-
cent to ?not even close?. Each proximity type comes
with a proximity-weight and the relevance score in-
cludes the contribution of proximity information by
taking the inner product over all types, including the
proximity ones.
3.2 Inverted Index
Of essence to fast retrieval on static document col-
lections of medium to large size is the use of an in-
verted index. The inverted index stores a list of hits
for each word in a given vocabulary. The hits are
grouped by document. For each document, the list
of hits for a given query term must include position
? needed to evaluate counts of proximity types ?
445
as well as all the context information needed to cal-
culate the relevance score of a given document us-
ing the scheme outlined previously. For details, the
reader is referred to (Brin and Page, 1998), Sec-
tion 4.
4 Position Specific Posterior Lattices
As highlighted in the previous section, position in-
formation is crucial for being able to evaluate prox-
imity information when assigning a relevance score
to a given document.
In the spoken document case however, we are
faced with a dilemma. On one hand, using 1-best
ASR output as the transcription to be indexed is sub-
optimal due to the high WER, which is likely to lead
to low recall ? query terms that were in fact spo-
ken are wrongly recognized and thus not retrieved.
On the other hand, ASR lattices do have much bet-
ter WER ? in our case the 1-best WER was 55%
whereas the lattice WER was 30% ? but the posi-
tion information is not readily available: it is easy to
evaluate whether two words are adjacent but ques-
tions about the distance in number of links between
the occurrences of two query words in the lattice are
very hard to answer.
The position information needed for recording a
given word hit is not readily available in ASR lat-
tices ? for details on the format of typical ASR
lattices and the information stored in such lattices
the reader is referred to (Young et al, 2002). To
simplify the discussion let?s consider that a tradi-
tional text-document hit for given word consists of
just (document id, position).
The occurrence of a given word in a lattice ob-
tained from a given spoken document is uncertain
and so is the position at which the word occurs in
the document.
The ASR lattices do contain the information
needed to evaluate proximity information, since on a
given path through the lattice we can easily assign a
position index to each link/word in the normal way.
Each path occurs with a given posterior probability,
easily computable from the lattice, so in principle
one could index soft-hits which specify
(document id, position,
posterior probability)
for each word in the lattice. Since it is likely that
s_1
s_i
s_q
nP(l_1)P(l_i)
P(l_q)
Figure 1: State Transitions
more than one path contains the same word in the
same position, one would need to sum over all pos-
sible paths in a lattice that contain a given word at a
given position.
A simple dynamic programming algorithm which
is a variation on the standard forward-backward al-
gorithm can be employed for performing this com-
putation. The computation for the backward pass
stays unchanged, whereas during the forward pass
one needs to split the forward probability arriving
at a given node n, ?n, according to the length l ?
measured in number of links along the partial path
that contain a word; null (?) links are not counted
when calculating path length ? of the partial paths
that start at the start node of the lattice and end at
node n:
?n[l] .=
?
pi:end(pi)=n,length(pi)=l
P (pi)
The backward probability ?n has the standard defi-
nition (Rabiner, 1989).
To formalize the calculation of the position-
specific forward-backward pass, the initialization,
and one elementary forward step in the forward pass
are carried out using Eq. (1), respectively ? see Fig-
ure 1 for notation:
?n[l + 1] =
q?
i=1
?si [l + ?(li, ?)] ? P (li)
?start[l] =
{1.0, l = 0
0.0, l 6= 0 (1)
The ?probability? P (li) of a given link li is stored
as a log-probability and commonly evaluated in
ASR using:
logP (li) = FLATw ? [1/LMw ? logPAM (li)+
logPLM (word(li))? 1/LMw ? logPIP ] (2)
446
where logPAM (li) is the acoustic model score,
logPLM (word(li)) is the language model score,
LMw > 0 is the language model weight, logPIP >
0 is the ?insertion penalty? and FLATw is a flat-
tening weight. In N -gram lattices where N ? 2,
all links ending at a given node n must contain the
same word word(n), so the posterior probability of
a given word w occurring at a given position l can
be easily calculated using:
P (w, l|LAT ) =
?
n s.t. ?n[l]??n>0
?n[l]??n
?start ? ?(w,word(n))
The Position Specific Posterior Lattice (PSPL) is a
representation of the P (w, l|LAT ) distribution: for
each position bin l store the words w along with their
posterior probability P (w, l|LAT ).
5 Spoken Document Indexing and Search
Using PSPL
Spoken documents rarely contain only speech. Of-
ten they have a title, author and creation date. There
might also be a text abstract associated with the
speech, video or even slides in some standard for-
mat. The idea of saving context information when
indexing HTML documents and web pages can thus
be readily used for indexing spoken documents, al-
though the context information is of a different na-
ture.
As for the actual speech content of a spoken doc-
ument, the previous section showed how ASR tech-
nology and PSPL lattices can be used to automati-
cally convert it to a format that allows the indexing
of soft hits ? a soft index stores posterior proba-
bility along with the position information for term
occurrences in a given document.
5.1 Speech Content Indexing Using PSPL
Speech content can be very long. In our case the
speech content of a typical spoken document was ap-
proximately 1 hr long; it is customary to segment a
given speech file in shorter segments.
A spoken document thus consists of an ordered
list of segments. For each segment we generate a
corresponding PSPL lattice. Each document and
each segment in a given collection are mapped to an
integer value using a collection descriptor file which
lists all documents and segments. Each soft hit in
our index will store the PSPL position and posterior
probability.
5.2 Speech Content Relevance Ranking Using
PSPL Representation
Consider a given query Q = q1 . . . qi . . . qQ and
a spoken document D represented as a PSPL. Our
ranking scheme follows the description in Sec-
tion 3.1.
The words in the document D clearly belong to
the ASR vocabulary V whereas the words in the
query may be out-of-vocabulary (OOV). As argued
in Section 2, the query-OOV rate is an important
factor in evaluating the impact of having a finite
ASR vocabulary on the retrieval accuracy. We as-
sume that the words in the query are all contained
in V; OOV words are mapped to UNK and cannot be
matched in any document D.
For all query terms, a 1-gram score is calculated
by summing the PSPL posterior probability across
all segments s and positions k. This is equivalent
to calculating the expected count of a given query
term qi according to the PSPL probability distribu-
tion P (wk(s)|D) for each segment s of document
D. The results are aggregated in a common value
S1?gram(D,Q):
S(D, qi) = log
[
1 +
?
s
?
k
P (wk(s) = qi|D)
]
S1?gram(D,Q) =
Q?
i=1
S(D, qi) (3)
Similar to (Brin and Page, 1998), the logarithmic ta-
pering off is used for discounting the effect of large
counts in a given document.
Our current ranking scheme takes into account
proximity in the form of matching N -grams present
in the query. Similar to the 1-gram case, we cal-
culate an expected tapered-count for each N-gram
qi . . . qi+N?1 in the query and then aggregate the re-
sults in a common value SN?gram(D,Q) for each
order N :
S(D, qi . . . qi+N?1) = (4)
log
[
1 +?s
?
k
?N?1
l=0 P (wk+l(s) = qi+l|D)
]
SN?gram(D,Q) =
Q?N+1?
i=1
S(D, qi . . . qi+N?1)
447
The different proximity types, one for each N -
gram order allowed by the query length, are com-
bined by taking the inner product with a vector of
weights.
S(D,Q) =
Q?
N=1
wN ? SN?gram(D,Q) (5)
Only documents containing all the terms in the
query are returned. In the current implementation
the weights increase linearly with the N-gram order.
Clearly, better weight assignments must exist, and
as the hit types are enriched beyond using just N -
grams, the weights will have to be determined using
machine learning techniques.
It is worth noting that the transcription for any
given segment can also be represented as a PSPL
with exactly one word per position bin. It is easy to
see that in this case the relevance scores calculated
according to Eq. (3-4) are the ones specified by 3.1.
6 Experiments
We have carried all our experiments on the iCampus
corpus prepared by MIT CSAIL. The main advan-
tages of the corpus are: realistic speech recording
conditions ? all lectures are recorded using a lapel
microphone ? and the availability of accurate man-
ual transcriptions ? which enables the evaluation of
a SDR system against its text counterpart.
6.1 iCampus Corpus
The iCampus corpus (Glass et al, 2004) consists
of about 169 hours of lecture materials: 20 Intro-
duction to Computer Programming Lectures (21.7
hours), 35 Linear Algebra Lectures (27.7 hours), 35
Electro-magnetic Physics Lectures (29.1 hours), 79
Assorted MIT World seminars covering a wide vari-
ety of topics (89.9 hours). Each lecture comes with
a word-level manual transcription that segments the
text into semantic units that could be thought of as
sentences; word-level time-alignments between the
transcription and the speech are also provided. The
speech style is in between planned and spontaneous.
The speech is recorded at a sampling rate of 16kHz
(wide-band) using a lapel microphone.
The speech was segmented at the sentence level
based on the time alignments; each lecture is consid-
ered to be a spoken document consisting of a set of
one-sentence long segments determined this way ?
see Section 5.1. The final collection consists of 169
documents, 66,102 segments and an average docu-
ment length of 391 segments.
We have then used a standard large vocabulary
ASR system for generating 3-gram ASR lattices and
PSPL lattices. The 3-gram language model used for
decoding is trained on a large amount of text data,
primarily newswire text. The vocabulary of the ASR
system consisted of 110kwds, selected based on fre-
quency in the training data. The acoustic model
is trained on a variety of wide-band speech and it
is a standard clustered tri-phone, 3-states-per-phone
model. Neither model has been tuned in any way to
the iCampus scenario.
On the first lecture L01 of the Introduction to
Computer Programming Lectures the WER of the
ASR system was 44.7%; the OOV rate was 3.3%.
For the entire set of lectures in the Introduction
to Computer Programming Lectures, the WER was
54.8%, with a maximum value of 74% and a mini-
mum value of 44%.
6.2 PSPL lattices
We have then proceeded to generate 3-gram lattices
and PSPL lattices using the above ASR system. Ta-
ble 1 compares the accuracy/size of the 3-gram lat-
tices and the resulting PSPL lattices for the first lec-
ture L01. As it can be seen the PSPL represen-
Lattice Type 3-gram PSPL
Size on disk 11.3MB 3.2MB
Link density 16.3 14.6
Node density 7.4 1.1
1-best WER 44.7% 45%
ORACLE WER 26.4% 21.7%
Table 1: Comparison between 3-gram and PSPL lat-
tices for lecture L01 (iCampus corpus): node and
link density, 1-best and ORACLE WER, size on disk
tation is much more compact than the original 3-
gram lattices at a very small loss in accuracy: the
1-best path through the PSPL lattice is only 0.3%
absolute worse than the one through the original 3-
gram lattice. As expected, the main reduction comes
from the drastically smaller node density ? 7 times
smaller, measured in nodes per word in the refer-
ence transcription. Since the PSPL representation
448
introduces new paths compared to the original 3-
gram lattice, the ORACLE WER path ? least error-
ful path in the lattice ? is also about 20% relative
better than in the original 3-gram lattice ? 5% ab-
solute. Also to be noted is the much better WER in
both PSPL/3-gram lattices versus 1-best.
6.3 Spoken Document Retrieval
Our aim is to narrow the gap between speech and
text document retrieval. We have thus taken as our
reference the output of a standard retrieval engine
working according to one of the TF-IDF flavors, see
Section 3. The engine indexes the manual transcrip-
tion using an unlimited vocabulary. All retrieval re-
sults presented in this section have used the stan-
dard trec_eval package used by the TREC eval-
uations.
The PSPL lattices for each segment in the spo-
ken document collection were indexed as explained
in 5.1. In addition, we generated the PSPL repre-
sentation of the manual transcript and of the 1-best
ASR output and indexed those as well. This allows
us to compare our retrieval results against the results
obtained using the reference engine when working
on the same text document collection.
6.3.1 Query Collection and Retrieval Setup
The missing ingredient for performing retrieval
experiments are the queries. We have asked a few
colleagues to issue queries against a demo shell us-
ing the index built from the manual transcription.
The only information1 provided to them was the
same as the summary description in Section 6.1.
We have collected 116 queries in this manner. The
query out-of-vocabulary rate (Q-OOV) was 5.2%
and the average query length was 1.97 words. Since
our approach so far does not index sub-word units,
we cannot deal with OOV query words. We have
thus removed the queries which contained OOV
words ? resulting in a set of 96 queries ? which
clearly biases the evaluation. On the other hand, the
results on both the 1-best and the lattice indexes are
equally favored by this.
1Arguably, more motivated users that are also more famil-
iar with the document collection would provide a better query
collection framework
6.3.2 Retrieval Experiments
We have carried out retrieval experiments in the
above setup. Indexes have been built from:
? trans: manual transcription filtered through
ASR vocabulary
? 1-best: ASR 1-best output
? lat: PSPL lattices.
No tuning of retrieval weights, see Eq. (5), or link
scoring weights, see Eq. (2) has been performed. Ta-
ble 2 presents the results. As a sanity check, the re-
trieval results on transcription ? trans ? match
almost perfectly the reference. The small difference
comes from stemming rules that the baseline engine
is using for query enhancement which are not repli-
cated in our retrieval engine. The results on lat-
tices (lat) improve significantly on (1-best) ?
20% relative improvement in mean average preci-
sion (MAP).
trans 1-best lat
# docs retrieved 1411 3206 4971
# relevant docs 1416 1416 1416
# rel retrieved 1411 1088 1301
MAP 0.99 0.53 0.62
R-precision 0.99 0.53 0.58
Table 2: Retrieval performance on indexes built
from transcript, ASR 1-best and PSPL lattices, re-
spectively
6.3.3 Why Would This Work?
A legitimate question at this point is: why would
anyone expect this to work when the 1-best ASR ac-
curacy is so poor?
In favor of our approach, the ASR lattice WER is
much lower than the 1-best WER, and PSPL have
even lower WER than the ASR lattices. As re-
ported in Table 1, the PSPL WER for L01 was
22% whereas the 1-best WER was 45%. Consider
matching a 2-gram in the PSPL ?the average query
length is indeed 2 wds so this is a representative sit-
uation. A simple calculation reveals that it is twice
? (1 ? 0.22)2/(1 ? 0.45)2 = 2 ? more likely to
find a query match in the PSPL than in the 1-best ?
if the query 2-gram was indeed spoken at that posi-
tion. According to this heuristic argument one could
expect a dramatic increase in Recall. Another aspect
449
is that people enter typical N-grams as queries. The
contents of adjacent PSPL bins are fairly random in
nature so if a typical 2-gram is found in the PSPL,
chances are it was actually spoken. This translates
in little degradation in Precision.
7 Conclusions and Future work
We have developed a new representation for ASR
lattices ? the Position Specific Posterior Lattice
(PSPL) ? that lends itself naturally to indexing
speech content and integrating state-of-the-art IR
techniques that make use of proximity and context
information. In addition, the PSPL representation is
also much more compact at no loss in WER ? both
1-best and ORACLE.
The retrieval results obtained by indexing the
PSPL and performing adequate relevance ranking
are 20% better than when using the ASR 1-best out-
put, although still far from the performance achieved
on text data.
The experiments presented in this paper are truly
a first step. We plan to gather a much larger num-
ber of queries. The binary relevance judgments ? a
given document is deemed either relevant or irrele-
vant to a given query in the reference ?ranking? ?
assumed by the standard trec_eval tool are also
a serious shortcoming; a distance measure between
rankings of documents needs to be used. Finally, us-
ing a baseline engine that in fact makes use of prox-
imity and context information is a priority if such
information is to be used in our algorithms.
8 Acknowledgments
We would like to thank Jim Glass and T J Hazen at
MIT for providing the iCampus data. We would also
like to thank Frank Seide for offering valuable sug-
gestions and our colleagues for providing queries.
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto, 1999.
Modern Information Retrieval, chapter 2, pages 27?
30. Addison Wesley, New York.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
M. G. Brown, J. T. Foote, G. J. F. Jones, K. Spa?rck Jones,
and S. J. Young. 1996. Open-vocabulary speech in-
dexing for voice and video mail retrieval. In Proc.
ACM Multimedia 96, pages 307?316, Boston, Novem-
ber.
Kenneth Ward Church. 2003. Speech and language pro-
cessing: Where have we been and where are we going?
In Proceedings of Eurospeech, Geneva, Switzerland.
J. Garofolo, G. Auzanne, and E. Voorhees. 2000. The
TREC spoken document retrieval track: A success
story. In Proceedings of the Recherche d?Informations
Assiste par Ordinateur: ContentBased Multimedia In-
formation Access Conference, April.
James Glass, T. J. Hazen, Lee Hetherington, and Chao
Wang. 2004. Analysis and processing of lecture audio
data: Preliminary investigations. In HLT-NAACL 2004
Workshop: Interdisciplinary Approaches to Speech
Indexing and Retrieval, pages 9?12, Boston, Mas-
sachusetts, May.
David Anthony James. 1995. The Application of Classi-
cal Information Retrieval Techniques to Spoken Docu-
ments. Ph.D. thesis, University of Cambridge, Down-
ing College.
B. Logan, P. Moreno, and O. Deshmukh. 2002. Word
and sub-word indexing approaches for reducing the ef-
fects of OOV queries on spoken audio. In Proc. HLT.
Kenney Ng. 2000. Subword-Based Approaches for Spo-
ken Document Retrieval. Ph.D. thesis, Massachusetts
Institute of Technology.
NIST. www. The TREC evaluation package. In www-
nlpir.nist.gov/projects/trecvid/trecvid.tools/trec eval.
L. R. Rabiner. 1989. A tutorial on hidden markov mod-
els and selected applications in speech recognition. In
Proceedings IEEE, volume 77(2), pages 257?285.
Murat Saraclar and Richard Sproat. 2004. Lattice-based
search for spoken utterance retrieval. In HLT-NAACL
2004, pages 129?136, Boston, Massachusetts, May.
F. Seide and P. Yu. 2004. Vocabulary-independent search
in spontaneous speech. In Proceedings of ICASSP,
Montreal, Canada.
Matthew A. Siegler. 1999. Integration of Continuous
Speech Recognition and Information Retrieval for Mu-
tually Optimal Performance. Ph.D. thesis, Carnegie
Mellon University.
P. C. Woodland, S. E. Johnson, P. Jourlin, and K. Spa?rck
Jones. 2000. Effects of out of vocabulary words in
spoken document retrieval. In Proceedings of SIGIR,
pages 372?374, Athens, Greece.
Steve Young, Gunnar Evermann, Thomas Hain, Dan
Kershaw, Gareth Moore, Julian Odell, Dan Povey
Dave Ollason, Valtcho Valtchev, and Phil Woodland.
2002. The HTK Book. Cambridge University Engi-
neering Department, Cambridge, England, December.
450
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 41?44, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
SPEECH OGLE: Indexing Uncertainty for Spoken Document Search
Ciprian Chelba and Alex Acero
Microsoft Research
Microsoft Corporation
One Microsoft Way
Redmond, WA 98052
{chelba, alexac}@microsoft.com
Abstract
The paper presents the Position Specific
Posterior Lattice (PSPL), a novel lossy
representation of automatic speech recog-
nition lattices that naturally lends itself
to efficient indexing and subsequent rele-
vance ranking of spoken documents.
In experiments performed on a collec-
tion of lecture recordings ? MIT iCam-
pus data ? the spoken document rank-
ing accuracy was improved by 20% rela-
tive over the commonly used baseline of
indexing the 1-best output from an auto-
matic speech recognizer.
The inverted index built from PSPL lat-
tices is compact ? about 20% of the size
of 3-gram ASR lattices and 3% of the size
of the uncompressed speech ? and it al-
lows for extremely fast retrieval. Further-
more, little degradation in performance is
observed when pruning PSPL lattices, re-
sulting in even smaller indexes ? 5% of
the size of 3-gram ASR lattices.
1 Introduction
Ever increasing computing power and connectivity
bandwidth together with falling storage costs result
in an overwhelming amount of data of various types
being produced, exchanged, and stored. Conse-
quently, search has emerged as a key application as
more and more data is being saved (Church, 2003).
Text search in particular is the most active area, with
applications that range from web and private net-
work search to searching for private information re-
siding on one?s hard-drive.
Speech search has not received much attention
due to the fact that large collections of untranscribed
spoken material have not been available, mostly
due to storage constraints. As storage is becoming
cheaper, the availability and usefulness of large col-
lections of spoken documents is limited strictly by
the lack of adequate technology to exploit them.
Manually transcribing speech is expensive and
sometimes outright impossible due to privacy con-
cerns. This leads us to exploring an automatic ap-
proach to searching and navigating spoken docu-
ment collections (Chelba and Acero, 2005).
2 Text Document Retrieval in the Early
Google Approach
Aside from the use of PageRank for relevance rank-
ing, the early Google also uses both proximity and
context information heavily when assigning a rel-
evance score to a given document (Brin and Page,
1998), Section 4.5.1.
For each given query term qi one retrieves the list
of hits corresponding to qi in document D. Hits
can be of various types depending on the context in
which the hit occurred: title, anchor text, etc. Each
type of hit has its own type-weight and the type-
weights are indexed by type.
For a single word query, their ranking algorithm
takes the inner-product between the type-weight
vector and a vector consisting of count-weights (ta-
pered counts such that the effect of large counts is
discounted) and combines the resulting score with
41
PageRank in a final relevance score.
For multiple word queries, terms co-occurring in a
given document are considered as forming different
proximity-types based on their proximity, from adja-
cent to ?not even close?. Each proximity type comes
with a proximity-weight and the relevance score in-
cludes the contribution of proximity information by
taking the inner product over all types, including the
proximity ones.
3 Position Specific Posterior Lattices
As highlighted in the previous section, position in-
formation is crucial for being able to evaluate prox-
imity information when assigning a relevance score
to a given document.
In the spoken document case however, we are
faced with a dilemma. On one hand, using 1-best
ASR output as the transcription to be indexed is sub-
optimal due to the high WER, which is likely to lead
to low recall ? query terms that were in fact spo-
ken are wrongly recognized and thus not retrieved.
On the other hand, ASR lattices do have a much bet-
ter WER ? in our case the 1-best WER was 55%
whereas the lattice WER was 30% ? but the posi-
tion information is not readily available.
The occurrence of a given word in a lattice ob-
tained from a given spoken document is uncertain
and so is the position at which the word occurs in the
document. However, the ASR lattices do contain the
information needed to evaluate proximity informa-
tion, since on a given path through the lattice we can
easily assign a position index to each link/word in
the normal way. Each path occurs with a given pos-
terior probability, easily computable from the lattice,
so in principle one could index soft-hits which spec-
ify (document id, position, posterior probability) for
each word in the lattice.
A simple dynamic programming algorithm which
is a variation on the standard forward-backward al-
gorithm can be employed for performing this com-
putation. The computation for the backward proba-
bility ?n stays unchanged (Rabiner, 1989) whereas
during the forward pass one needs to split the for-
ward probability arriving at a given node n, ?n, ac-
cording to the length of the partial paths that start at
the start node of the lattice and end at node n:
?n[l] =
?
pi:end(pi)=n,length(pi)=l
P (pi)
The posterior probability that a given node n occurs
at position l is thus calculated using:
P (n, l|LAT ) = ?n[l] ? ?nnorm(LAT )
The posterior probability of a given word w occur-
ring at a given position l can be easily calculated
using:
P (w, l|LAT ) =
?
n s.t. P (n,l)>0 P (n, l|LAT ) ? ?(w,word(n))
The Position Specific Posterior Lattice (PSPL) is
nothing but a representation of the P (w, l|LAT )
distribution. For details on the algorithm and prop-
erties of PSPL please see (Chelba and Acero, 2005).
4 Spoken Document Indexing and Search
Using PSPL
Speech content can be very long. In our case the
speech content of a typical spoken document was
approximately 1 hr long. It is customary to segment
a given speech file in shorter segments. A spoken
document thus consists of an ordered list of seg-
ments. For each segment we generate a correspond-
ing PSPL lattice. Each document and each segment
in a given collection are mapped to an integer value
using a collection descriptor file which lists all doc-
uments and segments.
The soft hits for a given word are
stored as a vector of entries sorted by
(document id, segment id). Document
and segment boundaries in this array, respectively,
are stored separately in a map for convenience of
use and memory efficiency. The soft index simply
lists all hits for every word in the ASR vocabulary;
each word entry can be stored in a separate file if we
wish to augment the index easily as new documents
are added to the collection.
4.1 Speech Content Relevance Ranking Using
PSPL Representation
Consider a given query Q = q1 . . . qi . . . qQ and
a spoken document D represented as a PSPL. Our
ranking scheme follows the description in Section 2.
42
For all query terms, a 1-gram score is calculated
by summing the PSPL posterior probability across
all segments s and positions k. This is equivalent
to calculating the expected count of a given query
term qi according to the PSPL probability distribu-
tion P (wk(s)|D) for each segment s of document
D. The results are aggregated in a common value
S1?gram(D,Q):
S(D, qi) = log
[
1 +
?
s
?
k
P (wk(s) = qi|D)
]
S1?gram(D,Q) =
Q?
i=1
S(D, qi) (1)
Similar to (Brin and Page, 1998), the logarithmic ta-
pering off is used for discounting the effect of large
counts in a given document.
Our current ranking scheme takes into account
proximity in the form of matching N -grams present
in the query. Similar to the 1-gram case, we cal-
culate an expected tapered-count for each N-gram
qi . . . qi+N?1 in the query and then aggregate the re-
sults in a common value SN?gram(D,Q) for each
order N :
S(D, qi . . . qi+N?1) =
log
[
1 +?s
?
k
?N?1
l=0 P (wk+l(s) = qi+l|D)
]
SN?gram(D,Q) =
Q?N+1?
i=1
S(D, qi . . . qi+N?1) (2)
The different proximity types, one for each N -
gram order allowed by the query length, are com-
bined by taking the inner product with a vector of
weights.
S(D,Q) =
Q?
N=1
wN ? SN?gram(D,Q)
It is worth noting that the transcription for any given
segment can also be represented as a PSPL with ex-
actly one word per position bin. It is easy to see that
in this case the relevance scores calculated accord-
ing to Eq. (1-2) are the ones specified by 2.
Only documents containing all the terms in the
query are returned. We have also enriched the query
language with the ?quoted functionality? that al-
lows us to retrieve only documents that contain exact
PSPL matches for the quoted phrases, e.g. the query
??L M?? tools will return only documents con-
taining occurrences of L M and of tools.
5 Experiments
We have carried all our experiments on the iCam-
pus corpus (Glass et al, 2004) prepared by MIT
CSAIL. The main advantages of the corpus are: re-
alistic speech recording conditions ? all lectures are
recorded using a lapel microphone ? and the avail-
ability of accurate manual transcriptions ? which
enables the evaluation of a SDR system against its
text counterpart.
The corpus consists of about 169 hours of lec-
ture materials. Each lecture comes with a word-level
manual transcription that segments the text into se-
mantic units that could be thought of as sentences;
word-level time-alignments between the transcrip-
tion and the speech are also provided. The speech
was segmented at the sentence level based on the
time alignments; each lecture is considered to be a
spoken document consisting of a set of one-sentence
long segments determined this way. The final col-
lection consists of 169 documents, 66,102 segments
and an average document length of 391 segments.
5.1 Spoken Document Retrieval
Our aim is to narrow the gap between speech and
text document retrieval. We have thus taken as our
reference the output of a standard retrieval engine
working according to one of the TF-IDF flavors. The
engine indexes the manual transcription using an un-
limited vocabulary. All retrieval results presented
in this section have used the standard trec_eval
package used by the TREC evaluations.
The PSPL lattices for each segment in the spoken
document collection were indexed. In terms of rel-
ative size on disk, the uncompressed speech for the
first 20 lectures uses 2.5GB, the ASR 3-gram lat-
tices use 322MB, and the corresponding index de-
rived from the PSPL lattices uses 61MB.
In addition, we generated the PSPL representa-
tion of the manual transcript and of the 1-best ASR
output and indexed those as well. This allows us to
compare our retrieval results against the results ob-
tained using the reference engine when working on
the same text document collection.
43
5.1.1 Query Collection and Retrieval Setup
We have asked a few colleagues to issue queries
against a demo shell using the index built from the
manual transcription.We have collected 116 queries
in this manner. The query out-of-vocabulary rate (Q-
OOV) was 5.2% and the average query length was
1.97 words. Since our approach so far does not in-
dex sub-word units, we cannot deal with OOV query
words. We have thus removed the queries which
contained OOV words ? resulting in a set of 96
queries.
5.1.2 Retrieval Experiments
We have carried out retrieval experiments in the
above setup. Indexes have been built from: trans,
manual transcription filtered through ASR vocabu-
lary; 1-best, ASR 1-best output; lat, PSPL lat-
tices. Table 1 presents the results. As a sanity check,
trans 1-best lat
# docs retrieved 1411 3206 4971
# relevant docs 1416 1416 1416
# rel retrieved 1411 1088 1301
MAP 0.99 0.53 0.62
R-precision 0.99 0.53 0.58
Table 1: Retrieval performance on indexes built
from transcript, ASR 1-best and PSPL lattices
the retrieval results on transcription ? trans ?
match almost perfectly the reference. The small dif-
ference comes from stemming rules that the baseline
engine is using for query enhancement which are not
replicated in our retrieval engine.
The results on lattices (lat) improve signifi-
cantly on (1-best) ? 20% relative improvement
in mean average precision (MAP). Table 2 shows the
retrieval accuracy results as well as the index size for
various pruning thresholds applied to the lat PSPL.
MAP performance increases with PSPL depth, as
expected. A good compromise between accuracy
and index size is obtained for a pruning threshold
of 2.0: at very little loss in MAP one could use an
index that is only 20% of the full index.
6 Conclusions and Future work
We have developed a new representation for ASR
lattices ? the Position Specific Posterior Lattice ?
pruning MAP R-precision Index Size
threshold (MB)
0.0 0.53 0.54 16
0.1 0.54 0.55 21
0.2 0.55 0.56 26
0.5 0.56 0.57 40
1.0 0.58 0.58 62
2.0 0.61 0.59 110
5.0 0.62 0.57 300
10.0 0.62 0.57 460
1000000 0.62 0.57 540
Table 2: Retrieval performance on indexes built
from pruned PSPL lattices, along with index size
that lends itself to indexing speech content. The
retrieval results obtained by indexing the PSPL are
20% better than when using the ASR 1-best output.
The techniques presented can be applied to in-
dexing contents of documents when uncertainty is
present: optical character recognition, handwriting
recognition are examples of such situations.
7 Acknowledgments
We would like to thank Jim Glass and T J Hazen
at MIT for providing the iCampus data. We would
also like to thank Frank Seide for offering valuable
suggestions on our work.
References
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Ciprian Chelba and Alex Acero. 2005. Position specific
posterior lattices for indexing speech. In Proceedings
of ACL, Ann Arbor, Michigan, June.
Kenneth Ward Church. 2003. Speech and language pro-
cessing: Where have we been and where are we going?
In Proceedings of Eurospeech, Geneva, Switzerland.
James Glass, Timothy J. Hazen, Lee Hetherington, and
Chao Wang. 2004. Analysis and processing of lec-
ture audio data: Preliminary investigations. In HLT-
NAACL 2004 Workshop: Interdisciplinary Approaches
to Speech Indexing and Retrieval, pages 9?12, Boston,
Massachusetts, USA, May 6.
L. R. Rabiner. 1989. A tutorial on hidden markov mod-
els and selected applications in speech recognition. In
Proceedings IEEE, volume 77(2), pages 257?285.
44
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 882?889,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Combining Statistical and Knowledge-based Spoken Language 
Understanding in Conditional Models 
Ye-Yi Wang, Alex Acero, Milind Mahajan 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{yeyiwang,alexac,milindm}@microsoft.com
John Lee 
Spoken Language Systems 
MIT CSAIL  
Cambridge, MA 02139, USA 
jsylee@csail.mit.edu 
 
Abstract 
Spoken Language Understanding (SLU) 
addresses the problem of extracting semantic 
meaning conveyed in an utterance. The 
traditional knowledge-based approach to this 
problem is very expensive -- it requires joint 
expertise in natural language processing and 
speech recognition, and best practices in 
language engineering for every new domain. 
On the other hand, a statistical learning 
approach needs a large amount of annotated 
data for model training, which is seldom 
available in practical applications outside of 
large research labs. A generative HMM/CFG 
composite model, which integrates easy-to-
obtain domain knowledge into a data-driven 
statistical learning framework, has previously 
been introduced to reduce data requirement. 
The major contribution of this paper is the 
investigation of integrating prior knowledge 
and statistical learning in a conditional model 
framework. We also study and compare  
conditional random fields (CRFs) with 
perceptron learning for SLU. Experimental 
results show that the conditional models 
achieve more than 20% relative reduction in 
slot error rate over the HMM/CFG model, 
which had already achieved an SLU accuracy 
at the same level as the best results reported 
on the ATIS data. 
1 Introduction 
Spoken Language Understanding (SLU) 
addresses the problem of extracting meaning 
conveyed in an utterance. Traditionally, the 
problem is solved with a knowledge-based 
approach, which requires joint expertise in 
natural language processing and speech 
recognition, and best practices in language 
engineering for every new domain. In the past 
decade many statistical learning approaches have 
been proposed, most of which exploit generative 
models, as surveyed in (Wang, Deng et al, 
2005). While the data-driven approach addresses 
the difficulties in knowledge engineering, it 
requires a large amount of labeled data for model 
training, which is seldom available in practical 
applications outside of large research labs. To 
alleviate the problem, a generative HMM/CFG 
composite model has previously been introduced 
(Wang, Deng et al, 2005). It integrates a 
knowledge-based approach into a statistical 
learning framework, utilizing prior knowledge to 
compensate for the dearth of training data. In the 
ATIS evaluation (Price, 1990), this model 
achieves the same level of understanding 
accuracy (5.3% error rate on standard ATIS 
evaluation) as the best system (5.5% error rate), 
which is a semantic parsing system based on a 
manually developed grammar. 
Discriminative training has been widely used 
for acoustic modeling in speech recognition 
(Bahl, Brown et al, 1986; Juang, Chou et al, 
1997; Povey and Woodland, 2002). Most of the 
methods use the same generative model 
framework, exploit the same features, and apply 
discriminative training for parameter 
optimization. Along the same lines, we have 
recently exploited conditional models by directly 
porting the HMM/CFG model to Hidden 
Conditional Random Fields (HCRFs) 
(Gunawardana, Mahajan et al, 2005), but failed 
to obtain any improvement. This is mainly due to 
the vast parameter space, with the parameters 
settling at local optima. We then simplified the 
original model structure by removing the hidden 
variables, and introduced a number of important 
overlapping and non-homogeneous features. The 
resulting Conditional Random Fields (CRFs) 
(Lafferty, McCallum et al, 2001) yielded a 21% 
relative improvement in SLU accuracy. We also 
applied a much simpler perceptron learning 
algorithm on the conditional model and observed 
improved SLU accuracy as well.  
In this paper, we will first introduce the 
generative HMM/CFG composite model, then 
discuss the problem of directly porting the model 
to HCRFs, and finally introduce the CRFs and 
882
the features that obtain the best SLU result on 
ATIS test data. We compare the CRF and 
perceptron training performances on the task. 
2 Generative Models 
The HMM/CFG composite model (Wang, Deng 
et al, 2005) adopts a pattern recognition 
approach to SLU. Given a word sequence W , an 
SLU component needs to find the semantic 
representation of the meaning M  that has the 
maximum a posteriori probability ( )Pr |M W :   
 
( )
( ) ( )
? arg max Pr |
arg max Pr | Pr
M
M
M M W
W M M
=
= ?  
The composite model integrates domain 
knowledge by setting the topology of the prior 
model, ( )Pr ,M according to the domain 
semantics; and by using PCFG rules as part of 
the lexicalization model ( )Pr |W M . 
The domain semantics define an application?s 
semantic structure with semantic frames. 
Figure 1 shows a simplified example of three 
semantic frames in the ATIS domain. The two 
frames with the ?toplevel? attribute are also 
known as commands. The ?filler? attribute of a 
slot specifies the semantic object that can fill it. 
Each slot may be associated with a CFG rule, 
and the filler semantic object must be 
instantiated by a word string that is covered by 
that rule. For example, the string ?Seattle? is 
covered by the ?City? rule in a CFG. It can 
therefore fill the ACity (ArrivalCity) or the 
DCity (DepartureCity) slot, and instantiate a 
Flight frame.  This frame can then fill the Flight 
slot of a ShowFlight frame. Figure 2 shows a 
semantic representation according to these 
frames.   
 
< frame name=?ShowFlight? toplevel=?1?>   
        <slot name=?Flight? filler=?Flight?/>   
< /frame>   
< frame name=?GroundTrans? toplevel=?1?>   
       < slot name=?City? filler=?City?/>   
< /frame>   
< frame name=?Flight?>   
        <slot name=?DCity? filler=?City?/>   
       < slot name=?ACity? filler=?City?/>   
< /frame>   
Figure 1. Simplified domain semantics for the ATIS 
domain.  
The semantic prior model comprises the 
HMM topology and state transition probabilities. 
The topology is determined by the domain 
semantics, and the transition probabilities can be 
estimated from training data. Figure 3 shows the 
topology of the underlying states in the statistical 
model for the semantic frames in Figure 1. On 
top is the transition network for the two top-level 
commands. At the bottom is a zoomed-in view 
for the ?Flight? sub-network. State 1 and state 4 
are called precommands. State 3 and state 6 are 
called postcommands. States 2, 5, 8 and 9 
represent slots. A slot is actually a three-state 
sequence ? the slot state is preceded by a 
preamble state and followed by a postamble 
state, both represented by black circles. They 
provide contextual clues for the slot?s identity. 
<ShowFlight>   
      < Flight>   
          < DCity filler=?City?>Seattle< /DCity>   
          <ACity filler=?City?>Boston< /ACity>   
      < /Flight>   
< /ShowFlight>   
Figure 2. The semantic representation for ?Show me 
the flights departing from Seattle arriving at Boston? 
is an instantiation of the semantic frames in Figure 1. 
 
 
Figure 3. The HMM/CFG model?s state topology, as 
determined by the semantic frames in Figure 1.  
The lexicalization model, ( )Pr |W M , depicts 
the process of sentence generation from the 
topology by estimating the distribution of words 
emitted by a state. It uses state-dependent n-
grams to model the precommands, 
postcommands, preambles and postambles, and 
uses knowledge-based CFG rules to model the 
slot fillers. These rules help compensate for the 
dearth of domain-specific data.  In the remainder 
of this paper we will say a string is ?covered by a 
CFG non-terminal (NT)?, or equivalently, is 
?CFG-covered for s? if the string can be parsed 
by the CFG rule corresponding to the slot s.  
 
Given the semantic representation in Figure 2, 
the state sequence through the model topology in 
883
Figure 3 is deterministic, as shown in Figure 4. 
However, the words are not aligned to the states 
in the shaded boxes. The parameters in their 
corresponding n-gram models can be estimated 
with an EM algorithm that treats the alignments 
as hidden variables. 
 
 
Figure 4. Word/state alignments. The segmentation 
of the word sequences in the shaded region is hidden. 
The HMM/CFG composite model was 
evaluated in the ATIS domain (Price, 1990). The 
model was trained with ATIS3 category A 
training data (~1700 annotated sentences) and 
tested with the 1993 ATIS3 category A test 
sentences (470 sentences with 1702 reference 
slots).  The slot insertion-deletion-substitution 
error rate (SER) of the test set is 5.0%, leading to 
a 5.3% semantic error rate in the standard end-to-
end ATIS evaluation, which is slightly better 
than the best manually developed system (5.5%). 
Moreover, a steep drop in the error rate is 
observed after training with only the first two 
hundred sentences.  This demonstrates that the 
inclusion of prior knowledge in the statistical 
model helps alleviate the data sparseness 
problem. 
3 Conditional Models 
We investigated the application of conditional 
models to SLU. The problem is formulated as 
assigning a label l  to each element in an 
observation .o  Here, o  consists of a word 
sequence 1o
?  and a list of CFG non-terminals 
(NT) that cover its subsequences, as illustrated in  
Figure 5. The task is to label ?two? as the ?Num-
of-tickets? slot of the ?ShowFlight? command, 
and ?Washington D.C.? as the ArrivalCity slot 
for the same command. To do so, the model must 
be able to resolve several kinds of ambiguities: 
 
1. Filler/non-filler ambiguity, e.g., ?two? can 
either fill a Num-of-tickets slot, or its 
homonym ?to? can form part of the preamble 
of an ArrivalCity slot. 
2. CFG ambiguity, e.g., ?Washington? can be 
CFG-covered as either City or State. 
3. Segmentation ambiguity, e.g., [Washington] 
[D.C.] vs. [Washington D.C.]. 
4. Semantic label ambiguity, e.g., ?Washington 
D.C.? can fill either an ArrivalCity or 
DepartureCity slot. 
 
Figure 5. The observation includes a word sequence 
and the subsequences covered by CFG non-terminals.  
3.1 CRFs and HCRFs 
Conditional Random Fields (CRFs) (Lafferty, 
McCallum et al, 2001) are undirected 
conditional graphical models that assign the 
conditional probability of a state (label) sequence 
1s
?  with respect to a vector of features 1 1( )f os
? ?, . 
They are of the following form: 
( )1 11( ) exp ( )( )o f oop s sz? ?? ??| ; = ? , .;  (1) 
Here ( )
1
1( ) exp ( )
s
z s
?
?? ?; = ? ,?o f o  normalizes 
the distribution over all possible state sequences. 
The parameter vector ?  is trained conditionally 
(discriminatively). If we assume that 1s
?  is a 
Markov chain given o  and the feature functions 
only depend on two adjacent states, then  
1
( 1) ( )
1
( )
1   = exp ( )
( )
t t
k k
k t
p s
f s s t
z
?
?
?
??
?
=
| ;
? ?, , ,? ?; ? ?? ?
o
o
o
 (2) 
In some cases, it may be natural to exploit 
features on variables that are not directly 
observed. For example, a feature for the Flight 
preamble may be defined in terms of an observed 
word and an unobserved state in the shaded 
region in Figure 4: 
( 1) ( )
FlightInit,flights
( )
( )
1 if =FlightInit  = flights;
    =
0 otherwise                                 
o
o
t t
t t
f s s t
s
? , , ,
? ???
 (3) 
In this case, the state sequence 1s
?  is only 
partially observed in the meaning representation 
5 8: ( ) "DCity" ( ) "ACity"M M s M s= ? = for the 
words ?Seattle? and ?Boston?. The states for the 
remaining words are hidden. Let ( )M?  represent 
the set of all state sequences that satisfy the 
constraints imposed by .M  To obtain the 
conditional probability of ,M we need to sum 
over all possible labels for the hidden states: 
884
 1
( 1) ( )
1( )
( )
1   exp ( )
( )
t t
k k
k ts M
p M
f s s t
z ?
?
?
??
?
=??
| ; =
? ?, , ,? ?; ? ?? ? ?
o
o
o
 
CRFs with features dependent on hidden state 
variables are called Hidden Conditional Random 
Fields (HCRFs). They have been applied to tasks 
such as phonetic classification (Gunawardana, 
Mahajan et al, 2005) and object recognition 
(Quattoni, Collins et al, 2004). 
3.2 Conditional Model Training 
We train CRFs and HCRFs with gradient-based 
optimization algorithms that maximize the log 
posterior. The gradient of the objective function 
is  
( ) ( )
( ) ( )
1
1
1
1
( ) ( )
( )
P P s
P P s
L s
s
?
?
?
?
?
? ?
?
, | ,
|
? ?? = , ;? ?
? ?? , ;? ?
l o l o
o o
E f o
             E f o


 
which is the difference between the conditional 
expectation of the feature vector given the 
observation sequence and label sequence, and the 
conditional expectation given the observation 
sequence alone. With the Markov assumption in 
Eq. (2), these expectations can be computed 
using a forward-backward-like dynamic 
programming algorithm. For CRFs, whose 
features do not depend on hidden state 
sequences, the first expectation is simply the 
feature counts given the observation and label 
sequences. In this work, we applied stochastic 
gradient descent (SGD) (Kushner and Yin, 1997) 
for parameter optimization. In our experiments 
on several different tasks, it is faster than L-
BFGS (Nocedal and Wright, 1999), a quasi-
Newton optimization algorithm. 
3.3 CRFs and Perceptron Learning 
Perceptron training for conditional models 
(Collins, 2002) is an approximation to the SGD 
algorithm, using feature counts from the Viterbi 
label sequence in lieu of expected feature counts. 
It eliminates the need of a forward-backward 
algorithm to collect the expected counts, hence 
greatly speeds up model training.  This algorithm 
can be viewed as using the minimum margin of a 
training example (i.e., the difference in the log 
conditional probability of the reference label 
sequence and the Viterbi label sequence) as the 
objective function instead of the conditional 
probability: 
( ) ( ) ( )
l
l o l o
'
' log | ; max log ' | ;L P P? ? ?= ?  
Here again, o  is the observation and l  is its 
reference label sequence. In perceptron training, 
the parameter updating stops when the Viterbi 
label sequence is the same as the reference label 
sequence. In contrast, the optimization based on 
the log posterior probability objective function 
keeps pulling probability mass from all incorrect 
label sequences to the reference label sequence 
until convergence. 
In both perceptron and CRF training, we 
average the parameters over training iterations 
(Collins, 2002). 
4 Porting HMM/CFG Model to HCRFs 
In our first experiment, we would like to exploit 
the discriminative training capability of a 
conditional model without changing  the 
HMM/CFG model?s topology and feature set.  
Since the state sequence is only partially labeled, 
an HCRF is used to model the conditional 
distribution of the labels. 
4.1 Features 
We used the same state topology and features as 
those in the HMM/CFG composite model.  The 
following indicator features are included: 
Command prior features capture the a priori 
likelihood of different top-level commands:  
 
( 1) ( )
( )
( )
1 if =0 C( )
    = , CommandSet
0 otherwise              
oPR t t
t
cf s s t
t s c
c
? , , ,
? ? = ? ???
 
Here C(s) stands for the name of the command 
that corresponds to the transition network 
containing state s. 
State Transition features capture the likelihood 
of transition from one state to another: 
( 1) ( )
( 1) ( ) 1 2
1 2
,1 2
1 if 
( ) ,   
0 otherwise              
where  is a legal transition according to the 
state topology.
o
t t
TR t t
s s
s s s s
f s s t
s s
?
? ? = , =, , , = ??
?
 
Unigram and Bigram features capture the 
likelihoods of words emitted by a state: 
885
( )
( 1) ( )
1
( 1) ( )
1
( 1) ( ) 1
1 2
,
, ,1 2
1 if 
( ) ,
0 otherwise              
( )
1 if 
     = ,
0 otherwise                                                
 
o
o
o
o o
t t
UG t t
BG t t
t t t t
s w
s w w
s s w
f s s t
f s s t
s s s s w w
?
?
?
?
? ?
? = ? =, , , = ??
, , ,
? = ? = ? = ? =??
( ) 1 2    | isFiller ; , TrainingDatas s w w w? ? ? ?
 
The condition 1isFiller( )s  restricts 1s  to be a slot 
state and not a pre- or postamble state. 
4.2 Experiments 
The model is trained with SGD with the 
parameters initialized in two ways. The flat start 
initialization sets all parameters to 0. The 
generative model initialization uses the 
parameters trained by the HMM/CFG model. 
Figure 6 shows the test set slot error rates 
(SER) at different training iterations. With the 
flat start initialization (top curve), the error rate 
never comes close to the 5% baseline error rate 
of the HMM/CFG model. With the generative 
model initialization, the error rate is reduced to 
4.8% at the second iteration, but the model 
quickly gets over-trained afterwards. 
0
5
10
15
20
25
30
35
0 20 40 60 80 100 120
Figure 6. Test set slot error rates (in %) at different 
training iterations. The top curve is for the flat start 
initialization, the bottom for the generative model  
initialization. 
The failure of the direct porting of the 
generative model to the conditional model can be 
attributed to the following reasons: 
? The conditional log-likelihood function is 
no longer a convex function due to the 
summation over hidden variables. This 
makes the model highly likely to settle on 
a local optimum. The fact that the flat start 
initialization failed to achieve the accuracy 
of the generative model initialization is a 
clear indication of the problem. 
? In order to account for words in the test 
data, the n-grams in the generative model 
are properly smoothed with back-offs to 
the uniform distribution over the 
vocabulary. This results in a huge number 
of parameters, many of which cannot be 
estimated reliably in the conditional 
model, given that model regularization is 
not as well studied as in n-grams.  
? The hidden variables make parameter 
estimation less reliable, given only a small 
amount of training data. 
5 CRFs for SLU 
An important lesson we have learned from the 
previous experiment is that we should not think 
generatively when applying conditional models. 
While it is important to find cues that help 
identify the slots, there is no need to exhaustively 
model the generation of every word in a 
sentence. Hence, the distinctions between pre- 
and postcommands, and pre- and postambles are 
no longer necessary. Every word that appears 
between two slots is labeled as the preamble state 
of the second slot, as illustrated in Figure 7. This 
labeling scheme effectively removes the hidden 
variables and simplifies the model to a CRF. It 
not only expedites model training, but also 
prevents parameters from settling at a local 
optimum, because the log conditional probability 
is now a convex function. 
 
Figure 7.  Once the slots are marked in the 
simplified model topology, the state sequence is fully 
marked, leaving no hidden variables and resulting in a 
CRF. Here, PAC stands for ?preamble for arrival 
city,? and PDC for ?preamble for departure city.?  
The command prior and state transition 
features (with fewer states) are the same as in the 
HCRF model. For unigrams and bigrams, only 
those that occur in front of a CFG-covered string 
are considered.  If the string is CFG-covered for 
slot s, then the unigram and bigram features for 
the preamble state of s are included. Suppose the 
words ?that departs? occur at positions 
1 and t t?  in front of the word ?Seattle,? which 
is CFG-covered by the non-terminal City.  Since 
City can fill a DepartureCity or ArrivalCity slot, 
the four following features are introduced:  
886
( 1) ( ) ( 1) ( )
1 1PDC,that PAC,that
( ) ( ) 1o oUG t t UG t tf s s t f s s t? ?? ?, , , = , , , =
And  
 
( 1) ( )
1
( 1) ( )
1
PDC,that,departs
PAC,that,departs
( )
( ) 1
o
o
BG t t
BG t t
f s s t
f s s t
?
?
?
?
, , , =
, , , =  
Formally, 
( )
( 1) ( )
1
( 1) ( )
1
( 1) ( ) 1
1 2
,
, ,1 2
1 if 
( ) ,
0 otherwise              
( )
1 if 
     = ,
0 otherwise                                           
o
o
o
o o
t t
UG t t
BG t t
t t t t
s w
s w w
s s w
f s s t
f s s t
s s s w w
?
?
?
?
? ?
? = ? =, , , = ??
, , ,
? = = ? = ? =??
 
 
( )
1 2 1 2
     | isFiller ;
     , | in  the training data,   and   
appears in front of sequence that is CFG-covered
for .
s s
w w w w w w
s
? ?
?  
5.1 Additional Features 
One advantage of CRFs over generative models 
is the ease with which overlapping features can 
be incorporated. In this section, we describe 
three additional feature sets. 
 
The first set addresses a side effect of not 
modeling the generation of every word in a 
sentence. Suppose a preamble state has never 
occurred in a position that is confusable with a 
slot state s, and a word that is CFG-covered for s 
has never occurred as part of the preamble state 
in the training data. Then, the unigram feature of 
the word for that preamble state has weight 0, 
and there is thus no penalty for mislabeling the 
word as the preamble. This is one of the most 
common errors observed in the development set. 
The chunk coverage for preamble words feature 
introduced to model the likelihood of a CFG-
covered word being labeled as a preamble: 
( 1) ( )
( ) ( )
,
( )
1 if  C( ) covers( , )  isPre( )    
0 otherwise                                                   
t tCC
t tt
c NT
f s s t
s c NT s
?
?????
, , ,
= ? ?=
o
o
 
where isPre( )s  indicates that s is a preamble 
state.  
Often, the identity of a slot depends on the 
preambles of the previous slot. For example, ?at 
two PM? is a DepartureTime in ?flight from 
Seattle to Boston at two PM?, but it is an 
ArrivalTime in ?flight departing from Seattle 
arriving in Boston at two PM.? In both cases, the 
previous slot is ArrivalCity, so the state 
transition features are not helpful for 
disambiguation.  The identity of the time slot 
depends not on the ArrivalCity slot, but on its 
preamble. Our second feature set, previous-slot 
context, introduces this dependency to the model: 
( 1) ( )
( 1) ( )
1 2 1
1 1 2
, ,1 2
( )
1 if ( , , 1)
     =  isFiller( )  Slot( ) Slot( )
0 otherwise                                                
PC t t
t t
ws sf s s t
s s s s w s t
s s s
?
?
, , ,
? = ? = ? ?? ?? ? ? ????
o
o  
Here Slot( )s  stands for the slot associated with 
the state ,s  which can be a filler state or a 
preamble state, as shown in Figure 7. 
1( , , 1)os t? ?  is the set of k words (where k is an 
adjustable window size) in front of the longest 
sequence that ends at position 1t ? and that is 
CFG-covered by 1Slot( )s . 
The third feature set is intended to penalize 
erroneous segmentation, such as segmenting 
?Washington D.C.? into two separate City slots. 
The chunk coverage for slot boundary feature is 
activated when a slot boundary is covered by a 
CFG non-terminal NT, i.e., when words in two 
consecutive slots (?Washington? and ?D.C.?) can 
also be covered by one single slot: 
( 1) ( )
( )
1
( 1) ( )
( 1) ( )
,
( )
          if  C( ) covers( , )1
          isFiller( )  isFiller( )    
          
0 otherwise                        
t tSB
t t
t
t t
t t
c NT
f s s t
s c NT
s s
s s
?
?
?
?
?????????
, , ,
= ?
? ?=
? ?
o
o
 
This feature set shares its weights with the 
chunk coverage features for preamble words, 
and does not introduce any new parameters. 
 
Features # of Param. SER 
Command Prior 6   
+State Transition +1377 18.68%
+Unigrams +14433 7.29% 
+Bigrams +58191 7.23% 
+Chunk Cov Preamble Word +156 6.87% 
+Previous-Slot Context +290 5.46% 
+Chunk Cov Slot Boundaries +0 3.94% 
Table 1. Number of additional parameters and the 
slot error rate after each new feature set is introduced. 
5.2 Experiments 
Since the objective function is convex, the 
optimization algorithm does not make any 
significant difference on SLU accuracy. We 
887
trained the model with SGD.  Other optimization 
algorithm like Stochastic Meta-Decent 
(Vishwanathan, Schraudolph et al, 2006) can be 
used to speed up the convergence. The training 
stopping criterion is cross-validated with the 
development set. 
Table 1 shows the number of new parameters 
and the slot error rate (SER) on the test data, 
after each new feature set is introduced. The new 
features improve the prediction of slot identities 
and reduce the SER by 21%, relative to the 
generative HMM/CFG composite model. 
The figures below show in detail the impact of 
the n-gram, previous-slot context and chunk 
coverage features.  The chunk coverage feature 
has three settings: 0 stands for no chunk 
coverage features; 1 for chunk coverage features 
for preamble words only; and 2 for both words 
and slot boundaries.  
Figure 8 shows the impact of the order of n-
gram features. Zero-order means no lexical 
features for preamble states are included. As the 
figure illustrates, the inclusion of CFG rules for 
slot filler states and domain-specific knowledge 
about command priors and slot transitions have 
already produced a reasonable SER under 15%. 
Unigram features for preamble states cut the 
error by more than 50%, while the impact of 
bigram features is not consistent -- it yields a 
small positive or negative difference depending 
on other experimental parameter settings. 
0%
2%
4%
6%
8%
10%
12%
14%
16%
0 1 2Ngram Order 
Slo
t E
rro
r R
ate
ChunkCoverage=0
ChunkCoverage=1
ChunkCoverage=2
 
Figure 8.  Effects of the order of n-grams on SER. 
The window size for the previous-slot context features 
is 2.  
Figure 9 shows the impact of the CFG chunk 
coverage feature.  Coverage for both preamble 
words and slot boundaries help improve the SLU 
accuracy. 
Figure 10 shows the impact of the window 
size for the previous-slot context feature. Here, 0 
means that the previous-slot context feature is 
not used. When the window size is k, the k words 
in front of the longest previous CFG-covered 
word sequence are included as the previous-slot 
unigram context features. As the figure 
illustrates, this feature significantly reduces SER, 
while the window size does not make any 
significant difference.  
0%
2%
4%
6%
8%
10%
12%
14%
16%
0 1 2
Chunk Coverage 
Slo
t E
rro
r R
ate
n=0
n=1
n=2
 
Figure 9. Effects of the chunk coverage feature. The 
window size for the previous-slot context feature is 2. 
The three lines correspond to different n-gram orders, 
where 0-gram indicates that no preamble lexical 
features are used.  
It is important to note that overlapping 
features like ,  and CC SB PCf f f  could not be easily 
incorporated into a generative model. 
0%
2%
4%
6%
8%
10%
12%
0 1 2
Window Size 
Slo
t E
rro
r R
ate n=0
n=1
n=2
 
Figure 10. Effects of the window size of the 
previous-slot context feature. The three lines represent 
different orders of n-grams (0, 1, and 2). Chunk 
coverage features for both preamble words and slot 
boundaries are used. 
5.3 CRFs vs. Perceptrons 
Table 2 compares the perceptron and CRF 
training algorithms, using chunk coverage 
features for both preamble words and slot 
boundaries, with which the best accuracy results 
888
are achieved. Both improve upon the 5% 
baseline SER from the generative HMM/CFG 
model. CRF training outperforms the perceptron 
in most settings, except for the one with unigram 
features for preamble states and with window 
size 1 -- the model with the fewest parameters. 
One possible explanation is as follows.  The 
objective function in CRFs is a convex function, 
and so SGD can find the single global optimum 
for it.  In contrast, the objective function for the 
perceptron, which is the difference between two 
convex functions, is not convex.  The gradient 
ascent approach in perceptron training is hence 
more likely to settle on a local optimum as the 
model becomes more complicated. 
 
  PSWSize=1 PSWSize=2 
  Perceptron CRFs Perceptron CRFs
n=1 3.76% 4.11% 4.23% 3.94%
n=2 4.76% 4.41% 4.58% 3.94%
Table 2. Perceptron vs. CRF training.  Chunk 
coverage features are used for both preamble words 
and slot boundaries. PSWSize stands for the window 
size of the previous-slot context feature. N is the order 
of the n-gram features. 
The biggest advantage of perceptron learning 
is its speed.  It directly counts the occurrence of 
features given an observation and its reference 
label sequence and Viterbi label sequence, with 
no need to collect expected feature counts with a 
forward-backward-like algorithm.  Not only is 
each iteration faster, but fewer iterations are 
required, when using SLU accuracy on a cross-
validation set as the stopping criterion. Overall, 
perceptron training is 5 to 8 times faster than 
CRF training. 
6 Conclusions 
This paper has introduced a conditional model 
framework that integrates statistical learning 
with a knowledge-based approach to SLU. We 
have shown that a conditional model reduces 
SLU slot error rate by more than 20% over the 
generative HMM/CFG composite model. The 
improvement was mostly due to the introduction 
of new overlapping features into the model. We 
have also discussed our experience in directly 
porting a generative model to a conditional 
model, and demonstrated that it may not be 
beneficial at all if we still think generatively in 
conditional modeling; more specifically, 
replicating the feature set of a generative model 
in a conditional model may not help much. The 
key benefit of conditional models is the ease with 
which they can incorporate overlapping and non-
homogeneous features. This is consistent with 
the finding in the application of conditional 
models for POS tagging (Lafferty, McCallum et 
al., 2001). The paper also compares different 
training algorithms for conditional models.  In 
most cases, CRF training is more accurate, 
however, perceptron training is much faster. 
References 
Bahl, L., P. Brown, et al 1986. Maximum mutual 
information estimation of hidden Markov model 
parameters for speech recognition. IEEE 
International Conference on Acoustics, Speech, 
and Signal Processing. 
Collins, M. 2002. Discriminative Training Methods 
for Hidden Markov Models: Theory and 
Experiments with Perceptron Algorithms. EMNLP, 
Philadelphia, PA. 
Gunawardana, A., M. Mahajan, et al 2005. Hidden 
conditional random fields for phone classification. 
Eurospeech, Lisbon, Portugal. 
Juang, B.-H., W. Chou, et al 1997. "Minimum 
classification error rate methods for speech 
recognition." IEEE Transactions on Speech and 
Audio Processing 5(3): 257-265. 
Kushner, H. J. and G. G. Yin. 1997. Stochastic 
approximation algorithms and applications, 
Springer-Verlag. 
Lafferty, J., A. McCallum, et al 2001. Conditional 
random fields: probabilistic models for segmenting 
and labeling sequence data. ICML. 
Nocedal, J. and S. J. Wright. 1999. Numerical 
optimization, Springer-Verlag. 
Povey, D. and P. C. Woodland. 2002. Minimum 
phone error and I-smoothing for improved 
discriminative training. IEEE International 
Conference on Acoustics, Speech, and Signal 
Processing. 
Price, P. 1990. Evaluation of spoken language system: 
the ATIS domain. DARPA Speech and Natural 
Language Workshop, Hidden Valley, PA. 
Quattoni, A., M. Collins and T. Darrell.  2004. 
Conditional Random Fields for Object 
Recognition.  NIPS. 
Vishwanathan, S. V. N., N. N. Schraudolph, et al 
2006. Accelerated Training of conditional random 
fields with stochastic meta-descent. The Learning 
Workshop, Snowbird, Utah. 
Wang, Y.-Y., L. Deng, et al 2005. "Spoken language 
understanding --- an introduction to the statistical 
framework." IEEE Signal Processing Magazine 
22(5): 16-31. 
 
889
Adaptation of Maximum Entropy Capitalizer: Little Data Can Help a Lot
Ciprian Chelba and Alex Acero
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{chelba,alexac}@microsoft.com
Abstract
A novel technique for maximum ?a posteriori?
(MAP) adaptation of maximum entropy (MaxEnt)
and maximum entropy Markov models (MEMM) is
presented.
The technique is applied to the problem of recov-
ering the correct capitalization of uniformly cased
text: a ?background? capitalizer trained on 20Mwds
of Wall Street Journal (WSJ) text from 1987 is
adapted to two Broadcast News (BN) test sets ?
one containing ABC Primetime Live text and the
other NPR Morning News/CNN Morning Edition
text ? from 1996.
The ?in-domain? performance of the WSJ capi-
talizer is 45% better than that of the 1-gram base-
line, when evaluated on a test set drawn from WSJ
1994. When evaluating on the mismatched ?out-of-
domain? test data, the 1-gram baseline is outper-
formed by 60%; the improvement brought by the
adaptation technique using a very small amount of
matched BN data ? 25-70kwds ? is about 20-25%
relative. Overall, automatic capitalization error rate
of 1.4% is achieved on BN data.
1 Introduction
Automatic capitalization is a practically relevant
problem: speech recognition output needs to be
capitalized; also, modern word processors perform
capitalization among other text proofing algorithms
such as spelling correction and grammar checking.
Capitalization can be also used as a preprocessing
step in named entity extraction or machine trans-
lation. We study the impact of using increasing
amounts of training data as well as using a small
amount of adaptation data on this simple problem
that is well suited to data-driven approaches since
vast amounts of ?training? data are easily obtainable
by simply wiping the case information in text.
As in previous approaches, the problem is framed
as an instance of the class of sequence labeling
problems. A case frequently encountered in prac-
tice is that of using mismatched ? out-of-domain,
in this particular case we used Broadcast News ?
test data. For example, one may wish to use a capi-
talization engine developed on newswire text for e-
mail or office documents. This typically affects neg-
atively the performance of a given model, and more
sophisticated models tend to be more brittle. In the
capitalization case we have studied, the relative per-
formance improvement of the MEMM capitalizer
over the 1-gram baseline drops from in-domain ?
WSJ ? performance of 45% to 35-40% when used
on the slightly mismatched BN data.
In order to take advantage of the adaptation data
in our scenario, a maximum a-posteriori (MAP)
adaptation technique for maximum entropy (Max-
Ent) models is developed. The adaptation procedure
proves to be quite effective in further reducing the
capitalization error of the WSJ MEMM capitalizer
on BN test data. It is also quite general and could
improve performance of MaxEnt models in any sce-
nario where model adaptation is desirable. A further
relative improvement of about 20% is obtained by
adapting the WSJ model to Broadcast News (BN)
text. Overall, the MEMM capitalizer adapted to BN
data achieves 60% relative improvement in accuracy
over the 1-gram baseline.
The paper is organized as follows: the next sec-
tion frames automatic capitalization as a sequence
labeling problem, presents previous approaches as
well as the widespread and highly sub-optimal 1-
gram capitalization technique that is used as a base-
line in most experiments in this work and others.
The MEMM sequence labeling technique is briefly
reviewed in Section 3. Section 4 describes the
MAP adaptation technique used for the capitaliza-
tion of out-of-domain text. The detailed mathemat-
ical derivation is presented in Appendix A. The ex-
perimental results are presented in Section 5, fol-
lowed by conclusions and suggestions for future
work.
2 Capitalization as Sequence Tagging
Automatic capitalization can be seen as a sequence
tagging problem: each lower-case word receives a
tag that describes its capitalization form. Similar to
the work in (Lita et al, 2003), we tag each word in
a sentence with one of the tags:
? LOC lowercase
? CAP capitalized
? MXC mixed case; no further guess is made as to
the capitalization of such words. A possibility
is to use the most frequent one encountered in
the training data.
? AUC all upper case
? PNC punctuation; we decided to have a sep-
arate tag for punctuation since it is quite fre-
quent and models well the syntactic context in
a parsimonious way
For training a given capitalizer one needs to convert
running text into uniform case text accompanied by
the above capitalization tags. For example,
PrimeTime continues on ABC .PERIOD
Now ,COMMA from Los Angeles ,COMMA
Diane Sawyer .PERIOD
becomes
primetime_MXC continues_LOC on_LOC
abc_AUC .period_PNC
now_CAP ,comma_PNC from_LOC los_CAP
angeles_CAP ,comma_PNC diane_CAP
sawyer_CAP .period_PNC
The text is assumed to be already segmented into
sentences. Any sequence labeling algorithm can
then be trained for tagging lowercase word se-
quences with capitalization tags.
At test time, the uniform case text to be capital-
ized is first segmented into sentences1 after which
each sentence is tagged.
2.1 1-gram capitalizer
A widespread algorithm used for capitalization is
the 1-gram tagger: for every word in a given vo-
cabulary (usually large, 100kwds or more) use the
most frequent tag encountered in a large amount of
training data. As a special case for automatic capi-
talization, the most frequent tag for the first word in
a sentence is overridden by CAP, thus capitalizing
on the fact that the first word in a sentence is most
likely capitalized2.
1Unlike the training phase, the sentence segmenter at test
time is assumed to operate on uniform case text.
2As with everything in natural language, it is not hard to
find exceptions to this ?rule?.
Due to its popularity, both our work and that
of (Lita et al, 2003) uses the 1-gram capitalizer as
a baseline. The work in (Kim and Woodland, 2004)
indicates that the same 1-gram algorithm is used in
Microsoft Word 2000 and is consequently used as
a baseline for evaluating the performance of their
algorithm as well.
2.2 Previous Work
We share the approach to capitalization as sequence
tagging with that of (Lita et al, 2003). In their ap-
proach, a language model is built on pairs (word,
tag) and then used to disambiguate over all possible
tag assignments to a sentence using dynamic pro-
gramming techniques.
The same idea is explored in (Kim and Woodland,
2004) in the larger context of automatic punctuation
generation and capitalization from speech recogni-
tion output. A second approach they consider for
capitalization is the use a rule-based tagger as de-
scribed by (Brill, 1994), which they show to outper-
form the case sensitive language modeling approach
and be quite robust to speech recognition errors and
punctuation generation errors.
Departing from their work, our approach builds
on a standard technique for sequence tagging,
namely MEMMs, which has been successfully ap-
plied to part-of-speech tagging (Ratnaparkhi, 1996).
The MEMM approach models the tag sequence T
conditionally on the word sequence W , which has a
few substantial advantages over the 1-gram tagging
approach:
? discriminative training of probability model
P (T |W ) using conditional maximum likeli-
hood is well correlated with tagging accuracy
? ability to use a rich set of word-level fea-
tures in a parsimonious way: sub-word fea-
tures such as prefixes and suffixes, as well as
future words3 are easily incorporated in the
probability model
? no concept of ?out-of-vocabulary? word: sub-
word features are very useful in dealing with
words not seen in the training data
? ability to integrate rich contextual features into
the model
More recently, certain drawbacks of MEMM mod-
els have been addressed by the conditional random
field (CRF) approach (Lafferty et al, 2001) which
slightly outperforms MEMMs on a standard part-
of-speech tagging task. In a similar vein, the work
3Relative to the current word, whose tag is assigned a prob-
ability value by the MEMM.
of (Collins, 2002) explores the use of discrimina-
tively trained HMMs for sequence labeling prob-
lems, a fair baseline for such cases that is often over-
looked in favor of the inadequate maximum likeli-
hood HMMs.
The work on adapting the MEMM model param-
eters using MAP smoothing builds on the Gaussian
prior model used for smoothing MaxEnt models, as
presented in (Chen and Rosenfeld, 2000). We are
not aware of any previous work on MAP adapta-
tion of MaxEnt models using a prior, be it Gaus-
sian or a different one, such as the exponential prior
of (Goodman, 2004). Although we do not have a
formal derivation, the adaptation technique should
easily extend to the CRF scenario.
A final remark contrasts rule-based approaches
to sequence tagging such as (Brill, 1994) with
the probabilistic approach taken in (Ratnaparkhi,
1996): having a weight on each feature in the Max-
Ent model and a sound probabilistic model allows
for a principled way of adapting the model to a new
domain; performing such adaptation in a rule-based
model is unclear, if at all possible.
3 MEMM for Sequence Labeling
A simple approach to sequence labeling is the max-
imum entropy Markov model. The model assigns a
probability P (T |W ) to any possible tag sequence
T = t
1
. . . tn = T n
1
for a given word sequence
W = w
1
. . . wn. The probability assignment is
done according to:
P (T |W ) =
n
?
i=1
P (ti|xi(W,T
i?1
1
))
where ti is the tag corresponding to word i and
xi(W,T
i?1
1
) is the conditioning information at posi-
tion i in the word sequence on which the probability
model is built.
The approach we took is the one in (Rat-
naparkhi, 1996), which uses xi(W,T i?1
1
) =
{wi, wi?1, wi+1, ti?1, ti?2}. We note that the prob-
ability model is causal in the sequencing of tags (the
probability assignment for ti only depends on previ-
ous tags ti?1, ti?2) which allows for efficient algo-
rithms that search for the most likely tag sequence
T ?(W ) = arg maxT P (T |W ) as well as ensures a
properly normalized conditional probability model
P (T |W ).
The probability P (ti|xi(W,T i?1
1
)) is modeled
using a maximum entropy model. The next section
briefly describes the training procedure; for details
the reader is referred to (Berger et al, 1996).
3.1 Maximum Entropy State Transition Model
The sufficient statistics that are ex-
tracted from the training data are tuples
(y,#, x) = (ti,#, xi(W,T
i?1
1
)) where ti is
the tag assigned in context xi(W,T i?1
1
) =
{wi, wi?1, wi+1, ti?1, ti?2} and # denotes the
count with which this event has been observed in
the training data. By way of example, the event
associated with the first word in the example in
Section 2 is (*bdw* denotes a special boundary
type):
MXC 1
currentword=primetime
previousword=*bdw*
nextword=continues
t1=*bdw* t1,2=*bdw*,*bdw*
prefix1=p prefix2=pr prefix3=pri
suffix1=e suffix2=me suffix3=ime
The maximum entropy probability model P (y|x)
uses features which are indicator functions of the
type:
f(y, x) = {1,
0,
if y = MXC and x.w
i
= primetime
o/w
Assuming a set of features F whose cardinality is
F , the probability assignment is made according to:
p
?
(y|x) = Z?1(x,?) ? exp
[
F
?
i=1
?ifi(x, y)
]
Z(x,?) =
?
y
exp
[
F
?
i=1
?ifi(x, y)
]
where ? = {?
1
. . . ?F } ? RF is the set of real-
valued model parameters.
3.1.1 Feature Selection
We used a simple count cut-off feature selection al-
gorithm which counts the number of occurrences of
all features in a predefined set after which it discards
the features whose count is less than a pre-specified
threshold. The parameter of the feature selection al-
gorithm is the threshold value; a value of 0 will keep
all features encountered in the training data.
3.1.2 Parameter Estimation
The model parameters ? are estimated such that
the model assigns maximum log-likelihood to the
training data subject to a Gaussian prior centered
at 0, ? ? N (0, diag(?2i )), that ensures smooth-
ing (Chen and Rosenfeld, 2000):
L(?) =
?
x,y
p?(x, y) log p
?
(y|x) (1)
?F
?
i=1
?2i
2?2i
+ const(?)
As shown in (Chen and Rosenfeld, 2000) ? and re-
derived in Appendix A for the non-zero mean case
? the update equations are:
?(t+1)i = ?
(t)
i + ?i, where ?i satisfies:
?
x,y
p?(x, y)fi(x, y) ?
?i
?2i
=
?i
?2i
+ (2)
?
x,y
p?(x)p
?
(y|x)fi(x, y)exp(?if
#(x, y))
In our experiments the variances are tied to ?i = ?
whose value is determined by line search on devel-
opment data such that it yields the best tagging ac-
curacy.
4 MAP Adaptation of Maximum Entropy
Models
In the adaptation scenario we already have a Max-
Ent model trained on the background data and we
wish to make best use of the adaptation data by bal-
ancing the two. A simple way to accomplish this is
to use MAP adaptation using a prior distribution on
the model parameters.
A Gaussian prior for the model parameters ?
has been previously used in (Chen and Rosen-
feld, 2000) for smoothing MaxEnt models. The
prior has 0 mean and diagonal covariance: ? ?
N (0, diag(?2i )). In the adaptation scenario, the
prior distribution used is centered at the parameter
values ?0 estimated from the background data in-
stead of 0: ? ? N (?0, diag(?2i )).
The regularized log-likelihood of the adaptation
training data becomes:
L(?) =
?
x,y
p?(x, y) log p
?
(y|x) (3)
?
F
?
i=1
(?i ? ?0i )
2
2?2i
+ const(?)
The adaptation is performed in stages:
? apply feature selection algorithm on adaptation
data and determine set of features Fadapt.
? build new model by taking the union of the
background and the adaptation feature sets:
F = Fbackground ? Fadapt; each of the
background features receives the correspond-
ing weight ?i determined on the background
training data; the new features
Fadapt \ Fbackground
4 introduced in the model
receive 0 weight. The resulting model is thus
equivalent with the background model.
? train the model such that the regularized log-
likelihood of the adaptation training data is
maximized. The prior mean is set at ?0 =
?background ? 0; ? denotes concatenation be-
tween the parameter vector for the background
model and a 0-valued vector of length |Fadapt\
Fbackground| corresponding to the weights for
the new features.
As shown in Appendix A, the update equations are
very similar to the 0-mean case:
?
x,y
p?(x, y)fi(x, y) ?
(?i ? ?0i )
?2i
=
?i
?2i
+ (4)
?
x,y
p?(x)p
?
(y|x)fi(x, y)exp(?if
#(x, y))
The effect of the prior is to keep the model param-
eters ?i close to the background ones. The cost of
moving away from the mean for each feature fi is
specified by the magnitude of the variance ?i: a
small variance ?i will keep the weight ?i close to
its mean; a large variance ?i will make the regu-
larized log-likelihood (see Eq. 3) insensitive to the
prior on ?i, allowing the use of the best value ?i for
modeling the adaptation data.
Another observation is that not only the features
observed in the adaptation data get updated: even
if Ep?(x,y)[fi] = 0, the weight ?i for feature fi will
still get updated if the feature fi triggers for a con-
text x encountered in the adaptation data and some
predicted value y ? not necessarily present in the
adaptation data in context x.
In our experiments the variances were tied to
?i = ? whose value was determined by line search
on development data drawn from the adaptation
data. The common variance ? will thus balance
optimally the log-likelihood of the adaptation data
with the ?0 mean values obtained from the back-
ground data.
Other tying schemes are possible: separate val-
ues could be used for the Fadapt \ Fbackground and
Fbackground feature sets, respectively. We did not
experiment with various tying schemes although
this is a promising research direction.
4.1 Relationship with Minimum Divergence
Training
Another possibility to adapt the background model
is to do minimum KL divergence (MinDiv) train-
4We use A \B to denote set difference.
ing (Pietra et al, 1995) between the background
exponential model B ? assumed fixed ? and an
exponential model A built using the Fbackground ?
Fadapt feature set. It can be shown that, if we
smooth the A model with a Gaussian prior on the
feature weights that is centered at 0 ? following
the approach in (Chen and Rosenfeld, 2000) for
smoothing maximum entropy models ? then the
MinDiv update equations for estimating A on the
adaptation data are identical to the MAP adaptation
procedure we proposed5.
However, we wish to point out that the equiva-
lence holds only if the feature set for the new model
A is Fbackground ? Fadapt. The straightforward ap-
plication of MinDiv training ? by using only the
Fadapt feature set for A ? will not result in an
equivalent procedure to ours. In fact, the differ-
ence in performance between this latter approach
and ours could be quite large since the cardinality
of Fbackground is typically several orders of mag-
nitude larger than that of Fadapt and our approach
also updates the weights corresponding to features
in Fbackground \ Fadapt. Further experiments are
needed to compare the performance of the two ap-
proaches.
5 Experiments
The baseline 1-gram and the background MEMM
capitalizer were trained on various amounts of
WSJ (Paul and Baker, 1992) data from 1987 ? files
WS87_{001-126}. The in-domain test data used
was file WS94_000 (8.7kwds).
As for the adaptation experiments, two different
sets of BN data were used, whose sizes are summa-
rized in Table 1:
1. BN CNN/NPR data. The train-
ing/development/test partition consisted of a
3-way random split of file BN624BTS. The
resulting sets are denoted CNN-trn/dev/tst,
respectively
2. BN ABC Primetime data. The training set con-
sisted of file BN623ATS whereas the develop-
ment/test set consisted of a 2-way random split
of file BN624ATS
5.1 In-Domain Experiments
We have proceeded building both 1-gram and
MEMM capitalizers using various amounts of back-
ground training data. The model sizes for the 1-
gram and MEMM capitalizer are presented in Ta-
ble 2. Count cut-off feature selection has been used
5Thanks to one of the anonymous reviewers for pointing out
this possible connection.
Data set Partition
train devel test
WSJ 2-20M ? 8.7k
CNN 73k 73k 73k
ABC 25k 8k 8k
Table 1: Background and adaptation training, devel-
opment, and test data partition sizes
for the MEMM capitalizer with the threshold set at
5, so the MEMM model size is a function of the
training data. The 1-gram capitalizer used a vocab-
ulary of the most likely 100k wds derived from the
training data.
Model No. Param. (103)
Training Data Size (106) 2.0 3.5 20.0
1-gram 100 100 100
MEMM 76 102 238
Table 2: Background models size (number of pa-
rameters) for various amounts of training data
We first evaluated the in-domain and out-of-
domain relative performance of the 1-gram and the
MEMM capitalizers as a function of the amount of
training data. The results are presented in Table 3.
The MEMM capitalizer performs about 45% better
Model Test Data Cap ERR (%)
Training Data Size (106) 2.0 3.5 20.0
1-gram WSJ-tst 5.4 5.2 4.4
MEMM WSJ-tst 2.9 2.5 2.3
1-gram ABC-dev 3.1 2.9 2.6
MEMM ABC-dev 2.2 2.0 1.6
1-gram CNN-dev 4.4 4.2 3.5
MEMM CNN-dev 2.7 2.5 2.1
Table 3: Background models performance on in-
domain (WSJ-test) and out-of-domain (BN-dev)
data for various amounts of training data
than the 1-gram one when trained and evaluated on
Wall Street Journal text. The relative performance
improvement of the MEMM capitalizer over the 1-
gram baseline drops to 35-40% when using out-of-
domain Broadcast News data. Both models benefit
from using more training data.
5.2 Adaptation Experiments
We have then adapted the best MEMM model built
on 20Mwds on the two BN data sets (CNN/ABC)
and compared performance against the 1-gram and
the unadapted MEMM models.
There are a number of parameters to be tuned
on development data. Table 4 presents the varia-
tion in model size with different count cut-off values
for the feature selection procedure on the adaptation
data. As can be seen, very few features are added to
the background model. Table 5 presents the varia-
tion in log-likelihood and capitalization accuracy on
the CNN adaptation training and development data,
respectively. The adaptation procedure was found
Cut-off 0 5 106
No. features 243,262 237,745 237,586
Table 4: Adapted model size as a function of count
cut-off threshold used for feature selection on CNN-
trn adaptation data; the entry corresponding to the
cut-off threshold of 106 represents the number of
features in the background model
to be insensitive to the number of reestimation it-
erations, and, more surprisingly, to the number of
features added to the background model from the
adaptation data, as shown in 5. The most sensitive
parameter is the prior variance ?2, as shown in Fig-
ure 1; its value is chosen to maximize classification
accuracy on development data. As expected, low
values of ?2 result in no adaptation at all, whereas
high values of ?2 fit the training data very well, and
result in a dramatic increase of training data log-
likelihood and accuracies approaching 100%.
Cut- LogL Cap ACC (%)
off ?2 (nats) CNN-trn CNN-dev
0 0.01 -4258.58 98.00 97.98
0 3.0 -1194.45 99.63 98.62
5 0.01 -4269.72 98.00 97.98
5 3.0 -1369.26 99.55 98.60
106 0.01 -4424.58 98.00 97.96
106 3.0 -1467.46 99.52 98.57
Table 5: Adapted model performance for various
count cut-off and ?2 variance values; log-likelihood
and accuracy on adaptation data CNN-trn as well
as accuracy on held-out data CNN-dev; the back-
ground model results (no new features added) are
the entries corresponding to the cut-off threshold of
106
Finally, Table 6 presents the results on test data
for 1-gram, background and adapted MEMM. As
can be seen, the background MEMM outperforms
the 1-gram model on both BN test sets by about
35-40% relative. Adaptation improves performance
even further by another 20-25% relative. Overall,
the adapted models achieve 60% relative reduction
in capitalization error over the 1-gram baseline on
both BN test sets. An intuitively satisfying result
is the fact that the cross-test set performance (CNN
0 1 2 3 4 5 6
?4500
?4000
?3500
?3000
?2500
?2000
?1500
?1000
?2 variance
Lo
gL
(tr
ain
)
Adaptation: Training LogL (nats) with ?2
0 1 2 3 4 5 6
97.5
98
98.5
99
99.5
100
?2 variance
Ac
cu
ra
cy
Adaptation: Training and Development Capitalization Accuracy with ?2
Figure 1: Variation of training data log-likelihood,
and training/development data (- -/? line) capitaliza-
tion accuracy as a function of the prior variance ?2
Cap ERR (%)
Model Adapt Data ABC-tst CNN-tst
1-gram ? 2.7 3.7
MEMM ? 1.8 2.2
MEMM ABC-trn 1.4 1.7
MEMM CNN-trn 2.4 1.4
Table 6: Background and adapted models perfor-
mance on BN test data; two adaptation/test sets are
used: ABC and CNN
adapted model evaluated on ABC data and the other
way around) is worse than the adapted one.
6 Conclusions and Future Work
The MEMM tagger is very effective in reducing
both in-domain and out-of-domain capitalization er-
ror by 35%-45% relative over a 1-gram capitaliza-
tion model.
We have also presented a general technique for
adapting MaxEnt probability models. It was shown
to be very effective in adapting a background
MEMM capitalization model, improving the accu-
racy by 20-25% relative. An overall 50-60% reduc-
tion in capitalization error over the standard 1-gram
baseline is achieved. A surprising result is that the
adaptation performance gain is not due to adding
more, domain-specific features but rather making
better use of the background features for modeling
the in-domain data.
As expected, adding more background training
data improves performance but a very small amount
of domain specific data also helps significantly if
one can make use of it in an effective way. The
?There?s no data like more data? rule-of-thumb
could be amended by ?..., especially if it?s the right
data!?.
As future work we plan to investigate the best
way to blend increasing amounts of less-specific
background training data with specific, in-domain
data for this and other problems.
Another interesting research direction is to ex-
plore the usefulness of the MAP adaptation of Max-
Ent models for other problems among which we
wish to include language modeling, part-of-speech
tagging, parsing, machine translation, information
extraction, text routing.
Acknowledgments
Special thanks to Adwait Ratnaparkhi for making
available the code for his MEMM tagger and Max-
Ent trainer.
References
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational
Linguistics, 22(1):39?72, March.
Eric Brill. 1994. Some Advances in
Transformation-Based Part of Speech Tag-
ging. In National Conference on Artificial
Intelligence, pages 722?727.
Stanley F. Chen and Ronald Rosenfeld. 2000. A
Survey of Smoothing Techniques for Maximum
Entropy Models. IEEE Transactions on Speech
and Audio Processing, 8(1):37?50.
Michael Collins. 2002. Discriminative Training
Methods for Hidden Markov Models: Theory
and Experiments with Perceptron Algorithms.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages
1?8, University of Pennsylvania, Philadelphia,
PA, July. ACL.
Joshua Goodman. 2004. Exponential Priors for
Maximum Entropy Models. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Main Proceedings, pages 305?
312, Boston, Massachusetts, USA, May 2 - May
7. Association for Computational Linguistics.
Ji-Hwan Kim and Philip C. Woodland. 2004. Au-
tomatic Capitalization Generation for Speech In-
put. Computer Speech and Language, 18(1):67?
90, January.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and La-
beling Sequence Data. In Proc. 18th Interna-
tional Conf. on Machine Learning, pages 282?
289. Morgan Kaufmann, San Francisco, CA.
L. Lita, A. Ittycheriah, S. Roukos, and N. Kamb-
hatla. 2003. tRuEcasIng. In Proccedings of
ACL, pages 152?159, Sapporo, Japan.
Doug B. Paul and Janet M. Baker. 1992. The design
for the Wall Street Journal-based CSR corpus. In
Proceedings of the DARPA SLS Workshop. Febru-
ary.
S. Della Pietra, V. Della Pietra, and J. Lafferty.
1995. Inducing features of random fields. Tech-
nical Report CMU-CS-95-144, School of Com-
puter Science, Carnegie Mellon University, Pitts-
burg, PA.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Eric Brill
and Kenneth Church, editors, Proceedings of
the Conference on Empirical Methods in Natu-
ral Language Processing, pages 133?142. Asso-
ciation for Computational Linguistics, Somerset,
New Jersey.
Appendix
A Modified IIS for MaxEnt MAP
Adaptation Using a Gaussian Prior
The regularized log-likelihood of the training data
? to be maximized by the MAP adaptation training
algorithm ? is:
L(?) =
?
x,y
p?(x, y) log p
?
(y|x)
?
F
?
i=1
(?i ? ?0i )
2
2?2i
+ const(?)
=
?
x,y
p?(x, y)
F
?
i=1
?ifi(x, y) ?
?
x,y
p?(x, y) log
?
y?
exp
[
F
?
i=1
?ifi(x, y
?)
]
?
F
?
i=1
(?i ? ?0i )
2
2?2i
+ const(?)
=
?
x,y
p?(x, y)
F
?
i=1
?ifi(x, y) ?
?
x
p?(x) log
?
y?
exp
[
F
?
i=1
?ifi(x, y
?)
]
?
F
?
i=1
(?i ? ?0i )
2
2?2i
+ const(?)
where the last equality holds because the argument
of the log is independent of y.
The derivation of the updates follows very closely
the one in (Chen and Rosenfeld, 2000) for smooth-
ing a MaxEnt model by using a Gaussian prior
with 0 mean and diagonal covariance matrix. At
each iteration we seek to find a set of updates for
?, ? = {?i}, that increase the regularized log-
likelihood L(?) by the largest amount possible.
After a few basic algebraic manipulations, the
difference in log-likelihood caused by a ? change
in the ? values becomes:
L(? + ?) ? L(?)
=
?
x,y
p?(x, y)
F
?
i=1
?ifi(x, y) ?
?
x
p?(x) log
?
y
p
?
(y|x) exp
[
F
?
i=1
?ifi(x, y)
]
?
F
?
i=1
2(?i ? ?0i )?i + ?i
2
2?2i
Following the same lower bounding technique as
in (Chen and Rosenfeld, 2000) by using log x ?
x?1 and Jensen?s inequality for the U -convexity of
the exponential we obtain:
L(? + ?) ? L(?)
?
?
x,y
p?(x, y)
F
?
i=1
?ifi(x, y) + 1 ?
?
x,y
p?(x)p
?
(y|x)
F
?
i=1
fi(x, y)
f#(x, y)
exp(?if
#(x, y))
?
F
?
i=1
2(?i ? ?0i )?i + ?i
2
2?2i
= A(?,?)
where f#(x, y) =
?F
i=1 fi(x, y). Taking the first
and second partial derivative of A(?,?) with re-
spect to ?i we obtain, respectively:
?A(?,?)
??i
= Ep?(x,y)[fi] ?
(?i ? ?0i )
?2i
+
?i
?2i
? Ep?(x)p
?
(y|x)
[
fi ? exp(?if
#)
]
and
?2A(?,?)
??i
2
= ?
1
?2i
?Ep?(x)p
?
(y|x)
[
fi ? f
#
? exp(?if
#)
]
< 0
Since A(0,?) = 0 and ?
2A(?,?)
??
i
2
< 0, by solving
for the unique root ?? of ?A(?,?)??
i
= 0 we obtain
the maximum value of A(?,?) ? which is non-
negative and thus guarantees that the regularized
log-likelihood does not decrease at each iteration:
L(? + ??) ? L(?) ? 0.
Solving for the root of ?A(?,?)??
i
= 0 results in the
update Eq. 4 and is equivalent to finding the solution
to:
Ep?(x,y) [fi] ?
(?i ? ?0i )
?2i
=
?i
?2i
+
?
x,y
p?(x)p
?
(y|x)fi(x, y)exp(?if
#(x, y))
A convenient way to solve this equation is to substi-
tute ?i = exp(?i) and ai = Ep?(x,y) [fi] ?
(?
i
??0
i
)
?2
i
and then use Newton?s method for finding the solu-
tion to ai = f(?i), where f(?) is:
f(?) =
log ?
?2i
+
?
x,y
p?(x)p
?
(y|x)fi(x, y)?
f#(x,y)
The summation on the right hand side reduces to
accumulating the coefficients of a polynomial in
? whose maximum degree is the highest possible
value of f#(x, y) on any context x encountered in
the training data and any allowed predicted value
y ? Y .
