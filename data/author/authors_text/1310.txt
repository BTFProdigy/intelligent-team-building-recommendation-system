Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 60?67,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A fast and accurate method for detecting English-Japanese parallel texts
Ken?ichi Fukushima, Kenjiro Taura and Takashi Chikayama
University of Tokyo
ken@tkl.iis.u-tokyo.ac.jp
{tau,chikayama}@logos.ic.i.u-tokyo.ac.jp
Abstract
Parallel corpus is a valuable resource used
in various fields of multilingual natural
language processing. One of the most
significant problems in using parallel cor-
pora is the lack of their availability. Re-
searchers have investigated approaches to
collecting parallel texts from the Web. A
basic component of these approaches is
an algorithm that judges whether a pair
of texts is parallel or not. In this paper,
we propose an algorithm that accelerates
this task without losing accuracy by pre-
processing a bilingual dictionary as well
as the collection of texts. This method
achieved 250,000 pairs/sec throughput on
a single CPU, with the best F1 score
of 0.960 for the task of detecting 200
Japanese-English translation pairs out of
40, 000. The method is applicable to texts
of any format, and not specific to HTML
documents labeled with URLs. We report
details of these preprocessing methods and
the fast comparison algorithm. To the
best of our knowledge, this is the first re-
ported experiment of extracting Japanese?
English parallel texts from a large corpora
based solely on linguistic content.
1 Introduction
?Parallel text? is a pair of texts which is written
in different languages and is a translation of each
other. A compilation of parallel texts offered in a
serviceable form is called a ?parallel corpus?. Par-
allel corpora are very valuable resources in various
fields of multilingual natural language processing
such as statistical machine translation (Brown et
al., 1990), cross-lingual IR (Chen and Nie, 2000),
and construction of dictionary (Nagao, 1996).
However, it is generally difficult to obtain paral-
lel corpora of enough quantity and quality. There
have only been a few varieties of parallel corpora.
In addition, their languages have been biased to-
ward English?French and their contents toward of-
ficial documents of governmental institutions or
software manuals. Therefore, it is often difficult
to find a parallel corpus that meets the needs of
specific researches.
To solve this problem, approaches to collect
parallel texts from the Web have been proposed.
In the Web space, all sorts of languages are used
though English is dominating, and the content of
the texts seems to be as diverse as all activities of
the human-beings. Therefore, this approach has a
potential to break the limitation in the use of par-
allel corpora.
Previous works successfully built parallel cor-
pora of interesting sizes. Most of them uti-
lized URL strings or HTML tags as a clue to ef-
ficiently find parallel documents (Yang and Li,
2002; Nadeau and Foster, 2004). Depending on
such information specific to webpages limits the
applicability of the methods. Even for webpages,
many parallel texts not conforming to the presup-
posed styles will be left undetected. In this work,
we have therefore decided to focus on a generally
applicable method, which is solely based on the
textual content of the documents. The main chal-
lenge then is how to make judgements fast.
Our proposed method utilizes a bilingual dictio-
nary which, for each word in tne language, gives
the list of translations in the other. The method
preprocesses both the bilingual dictionary and the
collection of texts to make a comparison of text
pairs in a subsequent stage faster. A comparison
60
of a text pair is carried out simply by compar-
ing two streams of integers without any dictionary
or table lookup, in time linear in the sum of the
two text sizes. With this method, we achieved
250,000 pairs/sec throughput on a single Xeon
CPU (2.4GHz). The best F1 score is 0.960, for a
dataset which includes 200 true pairs out of 40,000
candidate pairs. Further comments on these num-
bers are given in Section 4.
In addition, to the best of our knowledge,
this is the first reported experiment of extracitng
Japanese?English parallel texts using a method
solely based on their linguistic contents.
2 Related Work
There have been several attempts to collect paral-
lel texts from the Web. We will mention two con-
trasting approaches among them.
2.1 BITS
Ma and Liberman collected English?German par-
allel webpages (Ma and Liberman, 1999). They
began with a list of websites that belong to a do-
main accosiated with German?speaking areas and
searched for parallel webpages in these sites. For
each site, they downloaded a subset of the site
to investigate what language it is written in, and
then, downloaded all pages if it was proved to be
English?German bilingual. For each pair of En-
glish and German document, they judged whether
it is a mutual translation. They made a decision
in the following manner. First, they searched a
bilingual dictionary for all English?German word
pairs in the text pair. If a word pair is found in
the dictionary, it is recognized as an evidence of
translation. Finally, they divided the number of
recognized pairs by the sum of the length of the
two texts and regard this value as a score of trans-
lationality. When this score is greater than a given
threshold, the pair is judged as a mutual transla-
tion. They succeeded in creating about 63MB par-
allel corpus with 10 machines through 20 days.
The number of webpages is considered to have
increased far more rapidly than the performance of
computers in the past seven years. Therefore, we
think it is important to reduce the cost of calcula-
tion of a system.
2.2 STRAND
If we simply make a dicision for all pairs in a col-
lection of texts, the calculation takes ?(n2) com-
parisons of text pairs where n is the number of
documents in the collection. In fact, most re-
searches utilize properties peculiar to certain par-
allel webpages to reduce the number of candidate
pairs in advance. Resnik and Smith focused on the
fact that a page pair tends to be a mutual transla-
tion when their URL strings meet a certain condi-
tion, and examined only page pairs which satisfy
it (Resnik and Smith, 2003). A URL string some-
times contains a substring which indicates the lan-
guage in which the page is written. For example,
a webpage written in Japanese sometimes have a
substring such as j, jp, jpn, n, euc or sjis in
its URL. They regard a pair of pages as a candidate
when their URLs match completely after remov-
ing such language-specific substrings and, only for
these candidates, did they make a detailed com-
parison with bilingual dictionary. They were suc-
cessful in collecting 2190 parallel pairs from 8294
candidates. However, this URL condition seems
so strict for the purpose that they found 8294 can-
didate pairs from as much as 20 Tera bytes of web-
pages.
3 Proposed Method
3.1 Problem settings
There are several evaluation criteria for parallel
text mining algorithms. They include accuracy,
execution speed, and generality. We say an algo-
rithm is general when it can be applied to texts of
any format, not only to webpages with associated
information specific to webpages (e.g., URLs and
tags). In this paper, we focus on developing a fast
and general algorithm for determining if a pair of
texts is parallel.
In general, there are two complementary ways
to improve the speed of parallel text mining. One
is to reduce the number of ?candidate pairs? to be
compared. The other is to make a single compar-
ison of two texts faster. An example of the for-
mer is Resnik and Smith?s URL matching method,
which is able to mine parallel texts from a very
large corpora of Tera bytes. However, this ap-
proach is very specific to the Web and, even if we
restrict our interest to webpages, there may be a
significant number of parallel pages whose URLs
do not match the prescribed pattern and therefore
are filtered out. Our method is in the latter cat-
egory, and is generally applicable to texts of any
format. The approach depends only on the lin-
guistic content of texts. Reducing the number of
61
 




	


	






		






Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1372?1376,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Simple Customization of Recursive Neural Networks
for Semantic Relation Classification
Kazuma Hashimoto?, Makoto Miwa??, Yoshimasa Tsuruoka?, and Takashi Chikayama?
?The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{hassy, tsuruoka, chikayama}@logos.t.u-tokyo.ac.jp
??The University of Manchester, 131 Princess Street, Manchester, M1 7DN, UK
makoto.miwa@manchester.ac.uk
Abstract
In this paper, we present a recursive neural
network (RNN) model that works on a syn-
tactic tree. Our model differs from previous
RNN models in that the model allows for an
explicit weighting of important phrases for the
target task. We also propose to average param-
eters in training. Our experimental results on
semantic relation classification show that both
phrase categories and task-specific weighting
significantly improve the prediction accuracy
of the model. We also show that averaging the
model parameters is effective in stabilizing the
learning and improves generalization capacity.
The proposed model marks scores competitive
with state-of-the-art RNN-based models.
1 Introduction
Recursive Neural Network (RNN) models are
promising deep learning models which have been
applied to a variety of natural language processing
(NLP) tasks, such as sentiment classification, com-
pound similarity, relation classification and syntactic
parsing (Hermann and Blunsom, 2013; Socher et al,
2012; Socher et al, 2013). RNN models can repre-
sent phrases of arbitrary length in a vector space of
a fixed dimension. Most of them use minimal syn-
tactic information (Socher et al, 2012).
Recently, Hermann and Blunsom (2013) pro-
posed a method for leveraging syntactic informa-
tion, namely CCG combinatory operators, to guide
composition of phrases in RNN models. While their
models were successfully applied to binary senti-
ment classification and compound similarity tasks,
there are questions yet to be answered, e.g., whether
such enhancement is beneficial in other NLP tasks
as well, and whether a similar improvement can
be achieved by using syntactic information of more
commonly available types such as phrase categories
and syntactic heads.
In this paper, we present a supervised RNN model
for a semantic relation classification task. Our model
is different from existing RNN models in that impor-
tant phrases can be explicitly weighted for the task.
Syntactic information used in our model includes
part-of-speech (POS) tags, phrase categories and
syntactic heads. POS tags are used to assign vec-
tor representations to word-POS pairs. Phrase cate-
gories are used to determine which weight matrices
are chosen to combine phrases. Syntactic heads are
used to determine which phrase is weighted during
combining phrases. To incorporate task-specific in-
formation, phrases on the path between entity pairs
are further weighted.
The second contribution of our work is the intro-
duction of parameter averaging into RNN models.
In our preliminary experiments, we observed that
the prediction performance of the model often fluc-
tuates significantly between training iterations. This
fluctuation not only leads to unstable performance
of the resulting models, but also makes it difficult to
fine-tune the hyperparameters of the model. Inspired
by Swersky et al (2010), we propose to average the
model parameters in the course of training. A re-
cent technique for deep learning models of similar
vein is dropout (Hinton et al, 2012), but averaging
is simpler to implement.
Our experimental results show that our model per-
1372
Figure 1: A recursive representations of a phrase ?a
word vector? with POS tags of the words (DT, NN and
NN respectively). For example, the two word-POS pairs
?word NN? and ?vector NN? with a syntactic category
N are combined to represent the phrase ?word vector?.
forms better than standard RNN models. By av-
eraging the model parameters, our model achieves
performance competitive with the MV-RNN model
in Socher et al (2012), without using computation-
ally expensive word-dependent matrices.
2 An Averaged RNN Model with Syntax
Our model is a supervised RNN that works on a bi-
nary syntactic tree. As our first step to leverage in-
formation available in the tree, we distinguish words
with the same spelling but POS tags in the vector
space. Our model also uses different weight ma-
trices dependent on the phrase categories of child
nodes (phrases or words) in combining phrases. Our
model further weights those nodes that appear to be
important.
Compositional functions of our model follow
those of the SU-RNN model in Socher et al (2013).
2.1 Word-POS Vector Representations
Our model simply assigns vector representations to
word-POS pairs. For example, a word ?caused?
can be represented in two ways: ?caused VBD? and
?caused VBN?. The vectors are represented as col-
umn vectors in a matrix We ? Rd?|V|, where d is
the dimension of a vector and V is a set of all word-
POS pairs we consider.
2.2 Compositional Functions with Syntax
In construction of parse trees, we associate each of
the tree node with its d-dimensional vector represen-
tation computed from vector representations of its
subtrees. For leaf nodes, we look up word-POS vec-
tor representations in V. Figure 1 shows an example
of such recursive representations. A parent vector
p ? Rd?1 is computed from its direct child vectors
cl and cr? Rd?1:
p = tanh(?lW
Tcl ,Tcr
l cl+?rW
Tcl ,Tcrr cr+bTcl ,Tcr ),
where W Tcl ,Tcrl and W
Tcl ,Tcrr ? Rd?d are weight
matrices that depend on the phrase categories of cl
and cr. Here, cl and cr have phrase categories Tcl
and Tcr respectively (such as N, V, etc.). bTcl ,Tcr ?
Rd?1 is a bias vector. To incorporate the impor-
tance of phrases into the model, two subtrees of a
node may have different weights ?l ? [0, 1] and
?r(= 1 ? ?l), taking phrase importance into ac-
count. The value of ?l is manually specified and
automatically applied to all nodes based on prior
knowledge about the task. In this way, we can com-
pute vector representations for phrases of arbitrary
length. We denote a set of such matrices as Wlr and
bias vectors as b.
2.3 Objective Function and Learning
As with other RNN models, we add on the top of a
node x a softmax classifier. The classifier is used to
predict a K-class distribution d(x) ? RK?1 over a
specific task to train our model:
d(x) = softmax(W labelx+ blabel), (1)
where W label ? RK?d is a weight matrix and
blabel ? RK?1 is a bias vector. We denote t(x) ?
RK?1 as the target distribution vector at node x.
t(x) has a 0-1 encoding: the entry at the correct la-
bel of t(x) is 1, and the remaining entries are 0. We
then compute the cross entropy error between d(x)
and t(x):
E(x) = ?
K
?
k=1
tk(x)logdk(x),
and define an objective function as the sum of E(x)
over all training data:
J(?) =
?
x
E(x) + ?
2
???2,
where ? = (We,Wlr, b,W label, blabel) is the set of
our model parameters that should be learned. ? is a
vector of regularization parameters.
1373
To compute d(x), we can directly leverage any
other nodes? feature vectors in the same tree. We
denote such additional feature vectors as x?i ? Rd?1,
and extend Eq. (1):
d(x) = softmax(W labelx+
?
i
W addi x?i +blabel),
where W addi ? RK?d are weight matrices for addi-
tional features. We denote these matrices W addi as
W add. We also add W add to ?:
? = (We,Wlr, b,W label,W add, blabel).
The gradient of J(?)
?J(?)
??
=
?
x
?E(x)
??
+ ??
is efficiently computed via backpropagation through
structure (Goller and Ku?chler, 1996). To minimize
J(?), we use batch L-BFGS1 (Hermann and Blun-
som, 2013; Socher et al, 2012).
2.4 Averaging
We use averaged model parameters
? = 1
T + 1
T
?
t=0
?t
at test time, where ?t is the vector of model parame-
ters after t iterations of the L-BFGS optimization.
Our preliminary experimental results suggest that
averaging ? except We works well.
3 Experimental Settings
We used the Enju parser (Miyao and Tsujii, 2008)
for syntactic parsing. We used 13 phrase categories
given by Enju.
3.1 Task: Semantic Relation Classification
We evaluated our model on a semantic relation clas-
sification task: SemEval 2010 Task 8 (Hendrickx et
al., 2010). Following Socher et al (2012), we re-
garded the task as a 19-class classification problem.
There are 8,000 samples for training, and 2,717 for
1We used libLBFGS provided at http://www.
chokkan.org/software/liblbfgs/.
Figure 2: Classifying the relation between two entities.
test. For the validation set, we randomly sampled
2,182 samples from the training data.
To predict a class label, we first find the minimal
phrase that covers the target entities and then use the
vector representation of the phrase (Figure 2).
As explained in Section 2.3, we can directly con-
nect features on any other nodes to the softmax clas-
sifier. In this work, we used three such internal fea-
tures: two vector representations of target entities
and one averaged vector representation of words be-
tween the entities2.
3.2 Weights on Phrases
We tuned the weight ?l (or ?r) introduced in Sec-
tion 2.2 for this particular task. There are two fac-
tors: syntactic heads and syntactic path between tar-
get entities. Our model puts a weight ? ? [0.5, 1]
on head phrases, and 1 ? ? on the others. For re-
lation classification tasks, syntactic paths between
target entities are important (Zhang et al, 2006), so
our model also puts another weight ? ? [0.5, 1] on
phrases on the path, and 1 ? ? on the others. When
both child nodes are on the path or neither of them
on the path, we set ? = 0.5. The two weight fac-
tors are summed up and divided by 2 to be the final
weights ?l and ?r to combine the phrases. For ex-
ample, we set ?l = (1??)+?2 and ?r =
?+(1??)
2
when the right child node is the head and the left
child node is on the path.
3.3 Initialization of Model Parameters and
Tuning of Hyperparameters
We initialized We with 50-dimensional word vec-
tors3 trained with the model of Collobert et
2Socher et al (2012) used richer features including words
around entity pairs in their implementation.
3The word vectors are provided at http://ronan.
collobert.com/senna/. We used the vectors without any
modifications such as normalization.
1374
Method F1 (%)
Our model 79.4
RNN 74.8
MV-RNN 79.1
RNN w/ POS, WordNet, NER 77.6
MV-RNN w/ POS, WordNet, NER 82.4
SVM w/ bag-of-words 73.1
SVM w/ lexical and semantic features 82.2
Table 1: Comparison of our model with other methods on
SemEval 2010 task 8.
Method F1 (%)
Our model 79.4
Our model w/o phrase categories (PC) 77.7
Our model w/o head weights (HW) 78.8
Our model w/o path weights (PW) 78.7
Our model w/o averaging (AVE) 76.9
Our model w/o PC, HW, PW, AVE 74.1
Table 2: Contributions of syntactic and task-specific in-
formation and averaging.
al. (2011), and Wlr with I2 + ?, where I ? Rd?d
is an identity matrix. Here, ? is zero-mean gaussian
random variable with a variance of 0.01. The ini-
tialization of Wlr is the same as that of Socher et
al. (2013). The remaining model parameters were
initialized with 0.
We tuned hyperparameters in our model using the
validation set for each experimental setting. The hy-
perparameters include the regularization parameters
for We, Wlr, W label and W add, and the weights ?
and ?. For example, the best performance for our
model with all the proposed methods was obtained
with the values: 10?6, 10?4, 10?3, 10?3, 0.7 and
0.9 respectively.
4 Results and Discussion
Table 1 shows the performance of our model and that
of previously reported systems on the test set. The
performance of an SVM system with bag-of-words
features was reported in Rink and Harabagiu (2010),
and the performance of the RNN and MV-RNN
models was reported in Socher et al (2012). Our
model achieves an F1 score of 79.4% and outper-
forms the RNN model (74.8% F1) as well as the
simple SVM-based system (73.1% F1). More no-
Figure 3: F1 vs Training iterations.
tably, the score of our model is competitive with that
of the MV-RNN model (79.1% F1), which is com-
putationally much more expensive. Readers are re-
ferred to Hermann and Blunsom (2013) for the dis-
cussion about the computational complexity of the
MV-RNN model. We improved the performance of
RNN models on the task without much increasing
the complexity. This is a significant practical advan-
tage of our model, although its expressive power is
not the same as that of the MV-RNN model.
Our model outperforms the RNN model with one
lexical and two semantic external features: POS
tags, WordNet hypernyms and named entity tags
(NER) of target word pairs (external features). The
MV-RNN model with external features shows bet-
ter performance than our model. An SVM with rich
lexical and semantic features (Rink and Harabagiu,
2010) also outperforms ours. Note, however, that
this is not a fair comparison because those mod-
els use rich external resources such as WordNet and
named entity tags.
4.1 Contributions of Proposed Methods
We conducted additional experiments to quantify the
contributions of phrase categories, heads, paths and
averaging to our classification score. As shown in
Table 2, our model without phrase categories, heads
or paths still outperforms the RNN model with ex-
ternal features. On the other hand, our model with-
out averaging yields a lower score than the RNN
model with external features, though it is still bet-
1375
ter than the RNN model alone. Without utiliz-
ing these four properties, our model obtained only
74.1% F1. These results indicate that syntactic and
task-specific information and averaging contribute
to the performance improvement. The improvement
is achieved by a simple modification of composi-
tional functions in RNN models.
4.2 Effects of Averaging in Training
Figure 3 shows the training curves in terms of F1
scores. These curves clearly demonstrate that pa-
rameter averaging helps to stabilize the learning and
improve generalization capacity.
5 Conclusion
We have presented an averaged RNN model for se-
mantic relation classification. Our experimental re-
sults show that syntactic information such as phrase
categories and heads improves the performance, and
the task-specific weighting is also beneficial. The
results also demonstrate that averaging the model
parameters not only stabilizes the learning but also
improves the generalization capacity of the model.
As future work, we plan to combine deep learning
models with richer information such as predicate-
argument structures.
Acknowledgments
We thank the anonymous reviewers for their insight-
ful comments.
References
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural Language Processing (almost) from Scratch.
In JMLR, 12:2493?2537.
Christoph Goller and Andreas Ku?chler. 1996. Learning
Task-Dependent Distributed Representations by Back-
propagation Through Structure. In ICNN.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid ?O Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano and Stan Szpakowicz.
2010. SemEval-2010 Task 8: Multi-Way Classication
of Semantic Relations Between Pairs of Nominals. In
SemEval 2010.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role
of Syntax in Vector Space Models of Compositional Se-
mantics. In ACL.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever and Ruslan R. Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. In arXiv:1207.0580.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics, 34(1):35?80, MIT Press.
Bryan Rink and Sanda Harabagiu. 2010. UTD: Clas-
sifying Semantic Relations by Combining Lexical and
Semantic Resources. In SemEval 2010.
Richard Socher, Brody Huval, Christopher D. Manning
and Andrew Y. Ng. 2012. Semantic Compositionality
Through Recursive Matrix-Vector Spaces. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning and
Andrew Y. Ng. 2013. Parsing with Compositional
Vector Grammars. In ACL.
Kevin Swersky, Bo Chen, Ben Marlin and Nando de Fre-
itas. 2010. A tutorial on stochastic approximation al-
gorithms for training Restricted Boltzmann Machines
and Deep Belief Nets. In ITA workshop.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006.
A Composite Kernel to Extract Relations between En-
tities with Both Flat and Structured Features. In COL-
ING/ACL.
1376
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 88?92, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UTTime: Temporal Relation Classification using Deep Syntactic Features
Natsuda Laokulrat
The University of Tokyo
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
natsuda@logos.t.u-tokyo.ac.jp
Makoto Miwa
The University of Manchester
131 Princess Street,
Manchester, M1 7DN, UK
makoto.miwa@manchester.ac.uk
Yoshimasa Tsuruoka
The University of Tokyo
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
tsuruoka@logos.t.u-tokyo.ac.jp
Takashi Chikayama
The University of Tokyo
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
chikayama@logos.t.u-tokyo.ac.jp
Abstract
In this paper, we present a system, UTTime,
which we submitted to TempEval-3 for Task
C: Annotating temporal relations. The sys-
tem uses logistic regression classifiers and ex-
ploits features extracted from a deep syntactic
parser, including paths between event words in
phrase structure trees and their path lengths,
and paths between event words in predicate-
argument structures and their subgraphs. UT-
Time achieved an F1 score of 34.9 based
on the graphed-based evaluation for Task C
(ranked 2nd) and 56.45 for Task C-relation-
only (ranked 1st) in the TempEval-3 evalua-
tion.
1 Introduction
Temporal annotation is the task of identifying tem-
poral relationships between pairs of temporal enti-
ties, namely temporal expressions and events, within
a piece of text. The temporal relationships are im-
portant to support other NLP applications such as
textual entailment, document summarization, and
question answering. The temporal annotation task
consists of several subtasks, including temporal ex-
pression extraction, event extraction, and temporal
link identification and relation classification.
In TempEval-3, there are three subtasks of the
temporal annotation process offered, i.e., Task A:
Temporal expression extraction and normalization,
Task B: Event extraction, and Task C: Annotating
temporal relations. This paper presents a system
to handle Task C. Based on the annotated data pro-
vided, this subtask requires identifying pairs of tem-
poral entities and classifying the pairs into one of the
14 relation types according to TimeML (Pustejovsky
et al, 2005), i.e., BEFORE, AFTER, IMMEDIATELY BE-
FORE, IMMEDIATELY AFTER, INCLUDES, IS INCLUDED,
DURING, DURING INVERSE, SIMULTANEOUS, IDENTITY,
BEGINS, BEGUN BY, END, and ENDED BY.
The motivation behind our work is to utilize syn-
tactic and semantic relationships between a pair of
temporal entities in the temporal relation classifica-
tion task, since we believe that these relationships
convey the temporal relation. In addition to general
features, which are easily extracted from sentences
(e.g., part of speech tags, lemmas, synnonyms), we
use features extracted using a deep syntactic parser.
The features from the deep parser can be divided into
two groups: features from phrase structure trees and
features from predicate-argument structures. These
features are only applicable in the case that the tem-
poral entities appear in the same sentence, so we use
only the general features for inter-sentence relations.
Predicate-argument structure expresses semantic
relations between words. This information can be
extracted from a deep syntactic parser. Features
from predicate-argument structures can capture im-
portant temporal information (e.g., prepositions of
time) from sentences effectively.
The remaining part of this paper is organized as
follows. We explain our approach in detail in Sec-
tion 2 and then show the evaluation and results in
Section 3. Finally, we conclude with directions for
future work in Section 4.
2 Approach
Our system, UTTime, is based on a supervised ma-
chine learning approach. UTTime performs two
tasks; TLINK identification and classification. In
88
other words, UTTime identifies pairs of temporal en-
tities and classifies these pairs into temporal relation
types.
2.1 TLINK identification
A pair of temporal entities that have a temporal rela-
tion is called a TLINK. The system first determines
which pairs of temporal entities are linked by using
a ruled-based approach as a baseline approach.
All the TempEval-3?s possible pairs of temporal
entities are extracted by a set of simple rules; pairs
of temporal entities that satisfy one of the following
rules are considered as TLINKs.
? Event and document creation time
? Events in the same sentence
? Event and temporal expression in the same sen-
tence
? Events in consecutive sentences
2.2 TLINK classification
Each TLINK is classified into a temporal relation
type. We use a machine learning approach for the
temporal relation classification. Two L2-regularized
logistic regression classifiers, LIBLINEAR (Fan et
al., 2008), are used; one for event-event TLINKs,
and another one for event-time TLINKs. In addition
to general features at different linguistic levels, fea-
tures extracted by a deep syntactic parser are used.
The general features we employed are:
? Event and timex attributes
All attributes associated with events (class,
tense, aspect, modality, and polarity) and
temporal expressions (type, value, func-
tionInDocument, and temporalFunction) are
used. For event-event TLINKs, we also use
tense/class/aspect match, tense/class/aspect bi-
grams as features (Chambers et al, 2007).
? Morphosyntactic information
Words, part of speech tags, lemmas within a
window before/after event words are extracted
using Stanford coreNLP (Stanford NLP Group,
2012).
? Lexical semantic information
Figure 1: Phrase structure tree
Synonyms of event word tokens from WordNet
lexical database (Fellbaum, 1998) are used as
features.
? Event-Event information
For event-event TLINKs, we use
same sentence feature to differentiate pairs
of events in the same sentence from pairs of
events from different sentences (Chambers et
al., 2007).
In the case that temporal entities of a particu-
lar TLINK are in the same sentence, we extract
two new types of sentence-level semantic informa-
tion from a deep syntactic parser. We use the Enju
parser (Miyao and Tsujii, 2008). It analyzes syn-
tactic/semantic structures of sentences and provides
phrase structures and predicate-argument structures.
The features we extract from the deep parser are
? Paths between event words in the phrase struc-
ture tree, and up(?)/down(?) lengths of paths.
We use 3-grams of paths as features instead of
full paths since these are too sparse. An ex-
ample is shown in Figure 1. In this case, the
path between the event words, estimates and
worth, is VBZ?, VX?, VP?, VP?, VP, PP?, PX?, IN?.
The 3-grams of the path are, therefore, {VBZ?-
VX?-VP?, VX?-VP?-VP?, VP?-VP?-VP, VP?-VP-PP?,
VP-PP?-PX?, PP?-PX-?-IN?}. The up/down path
89
Figure 2: Predicate argument structure
lengths are 4 (VBZ?, VX?, VP?, VP?) and 3 (PP?,
PX?, IN?) respectively.
? Paths between event words in predicate-
argument structure, and their subgraphs.
For the previous example, we can express the
relations in predicate-argument structure repre-
sentation as
? verb arg12: estimate (she, properties)
? prep arg12: worth (estimate, dollars)
In this case, the path between the event words,
estimates and worth, is?prep arg12:arg1. That
is, the type of the predicate worth is prep arg12
and it has estimate as the first argument (arg1).
The path from estimate to worth is in reverse
direction (?).
The next example sentence, John saw mary be-
fore the meeting, gives an idea of a more com-
plex predicate-argument structure as shown
in Figure 2. The path between the event
words, saw and meeting is ?prep arg12:arg1,
prep arg12:arg2.
We use (v, e, v) and (e, v, e) tuples of the
edges and vertices on the path as features.
For example, in Figure 2, the (v,e,v) tuples
are (see, ?prep arg12:arg1, before) and (be-
fore, prep arg12:arg2, meeting). In the same
way, the (e,v,e) tuple is (?prep arg12:arg1,
before, prep arg12:arg2). The subgraphs
of (v, e, v) and (e, v, e) tuples are also
used, including (see, ?prep arg12:arg1,
*), (*, ?prep arg12:arg1, before), (*,
?prep arg12:arg1, *), (*, prep arg12:arg2,
meeting), (before, prep arg12:arg2, *), (*,
prep arg12:arg2, *), (*, before, prep arg12:arg2),
(?prep arg12:arg1, before, *), (*, before, *).
From the above example, the features from pred-
icate argument structure can properly capture the
preposition before. It can also capture a preposi-
tion from a compound sentence such as John met
Mary before he went back home. The path between
the event words met and went are (?conj arg12:arg1,
conj arg12:arg2) and the (v, e, v) and (e, v, e)
tuples are (met, ?conj arg12:arg1, before), (before,
conj arg12:arg2, went), and (?prep arg12:arg1, be-
fore, prep arg12:arg2).
2.3 Hybrid approach
The rule-based approach described in Section 2.1
produces many unreasonable and excessive links.
We thus use a machine learning approach to filter
out those unreasonable links by training the model
in Section 2.2 with an additional relation type, UN-
KNOWN, for links that satisfy the rules in Section
2.1 but do not appear in the training data.
In this way, for Task C, we first extract all the links
that satisfy the rules and classify the relation types of
those links. After classifying temporal relations, we
remove the links that are classified as UNKNOWN.
3 Evaluation
The scores are calculated by the graph-based eval-
uation metric proposed by UzZaman and Allen
(2011). We trained the models with TimeBank and
AQUAINT corpora. We also trained our models on
the training set with inverse relations. The perfor-
mance analysis is based on 10-fold cross validation
on the development data.
3.1 Task C
In Task C, a system has to identify appropriate tem-
poral links and to classify each link into one tempo-
ral relation type. For Task C evaluation, we compare
the results of the models trained with and without the
features from the deep parser. The results are shown
in Table 1. The rule-based approach gives a very low
precision.
3.2 Task C-relation-only
Task C-relation-only provides a system with all the
appropriate temporal links and only needs the sys-
tem to classify the relation types. Since our goal is to
exploit the features from the deep parser, in Task C-
relation-only, we measured the contribution of those
features to temporal relation classification in Table
2.
90
Features F1 P R
gen. (rule) 22.51 14.32 52.58
gen. + ph. + pas. (rule) 22.61 14.30 54.01
gen. + ph. + pas. (hyb.) 33.52 36.23 31.19
gen. + ph. + pas. (hyb. + inv.) 39.53 37.56 41.70
Table 1: Result of Task C. (rule: rule-based approach,
hyb.: hybrid approach, gen.: general features, ph.:phrase
structure tree features, pas.:predicate-argument structure
features, and inv.: Inverse relations are used for training.)
Features F1 P R
gen. 64.42 64.59 64.25
gen. + ph. 65.24 65.42 65.06
gen. + pas. 66.40 66.55 66.25
gen. + ph. + pas. 66.39 66.55 66.23
gen. + ph. + pas. (inv.) 65.30 65.39 65.20
Table 2: Result of Task C-relation-only. (gen.:
general features, ph.:phrase structure tree features,
pas.:predicate-argument structure features, and inv.: In-
verse relations are used for training.)
The predicate-argument-structure features con-
tributed to the improvement more than those of
phrase structures in both precision and recall. The
reason is probably that the features from phrase
structures that we used did not imply a temporal re-
lation of events in the sentence. For instance, the
sentence ?John saw Mary before the meeting? gives ex-
actly the same path as of the sentence ?John sawMary
after the meeting?.
3.3 Results on test data
Tables 3 and 4 show the results on the test data,
which were manually annotated and provided by the
TempEval-3 organizer. We also show the scores of
the other systems in the tables. For the evaluation
on the test data, we used the models trained with
general features, phrase structure tree features, and
predicate-argument structure features.
UTTime-5 ranked 2nd best in Task C. Interest-
ingly, training the models with inverse relations im-
proved the system only when using the hybrid ap-
proach. This means that the inverse relations did not
improve the temporal classification but helped the
system filter out unreasonable links (UNKNOWN)
in the hybrid approach. As expected, the ruled-based
approach got a very high recall score at the expense
of precision. UTTime-1, although it achieved the F1
Approach F1 P R
rule (UTTime-1) 24.65 15.18 65.64
rule + inv (UTTime-3) 24.28 15.1 61.99
hyb. (UTTime-4) 28.81 37.41 23.43
hyb. + inv. (UTTime-5) 34.9 35.94 33.92
cleartk 36.26 37.32 35.25
NavyTime 31.06 35.48 27.62
JU-CSE 26.41 21.04 35.47
KUL-KULTaskC 24.83 23.35 26.52
Table 3: Result of Tack C on test data. (rule: rule-based
approach, hyb.: hybrid approach, and inv.: Inverse rela-
tions are used for training.)
Approach F1 P R
gen. + ph. + pas. (UTTime-1) 56.45 55.58 57.35
gen. + ph. + pas. (UTTime-2) 54.26 53.2 55.36
gen. + ph. + pas. (inv.) (UTTime-3) 54.7 53.85 55.58
NavyTime 46.83 46.59 47.07
JU-CSE 34.77 35.07 34.48
Table 4: Result of Task C-relation-only on test data.
(gen.: general features, ph.:phrase structure tree features,
pas.:predicate-argument structure features, and inv.: In-
verse relations are used for training.)
score of only 24.65, got the highest recall among all
the systems.
For Task C-relation-only, we achieved the highest
F1 score, precision, and recall. UTTime-2 basically
had the same models as that of UTTime-1, but we
put different weights for each relation type. The re-
sults show that using the weights did not improve
the score in graph-based evaluation.
4 Conclusion
The system, UTTime, identifying temporal links and
classifying temporal relation, is proposed. The links
were identified based on the rule-based approach
and then some links were filtered out by a classi-
fier. The filtering helped improve the system consid-
erably. For the relation classification task, the fea-
tures extracted from phrase structures and predicate-
argument structures were proposed, and the features
improved the classification in precision, recall, and
F-score.
In future work, we hope to improve the classifica-
tion performance by constructing timegraphs (Miller
and Schubert, 1999), so that the system can use in-
formation from neighbor TLINKs as features.
91
References
James Pustejovsky, Robert Ingria, Roser Saur??, Jose?
Castan?o, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz, Inderjeet Mani 2005. The spec-
ification language TimeML. The Language of Time: A
reader, pages 545?557
Stanford Natural Language Processing Group. 2012.
Stanford CoreNLP.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification.
Nathanael Chambers, Shan Wang and Dan Jurafsky.
2007. Classifying Temporal Relations between
Events. In ACL 2007, pages 173?176.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics. 34(1). pages 35?80, MIT Press.
Naushad UzZaman and James F. Allen. 2011. Temporal
Evaluation. In ACL 2011, pages 351?356.
Stephanie A. Miller and Lenhart K. Schubert. 1999.
Time Revisited. In Computational Intelligence 6,
pages 108?118.
92
