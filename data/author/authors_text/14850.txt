An Agent-Based Approach to Chinese Word Segmentation 
Samuel W.K. Chan 
Dept. of Decision Sciences 
The Chinese University of Hong Kong 
Hong Kong, China 
swkchan@cuhk.edu.hk 
Mickey W.C. Chong 
Dept. of Decision Sciences 
The Chinese University of Hong Kong
Hong Kong, China 
mickey@baf.msmail.cuhk.edu.hk
 
 
Abstract 
This paper presents the results of our sys-
tem that has participated in the word seg-
mentation task in the Fourth SIGHAN 
Bakeoff. Our system consists of several ba-
sic components which include the pre-
processing, token identification and the 
post-processing. An agent-based approach 
is introduced to identify the weak segmen-
tation points. Our system has participated 
in two open and five closed tracks in five 
major corpora. Our results have attained 
top five in most of the tracks in the bakeoff. 
In particular, it is ranked first in the open 
track of the corpus from Academia Sinica, 
second in the closed track of the corpus 
from City University of Hong Kong, third 
in two closed tracks of the corpora from 
State Language Commission of P.R.C. and 
Academia Sinica. 
1 Introduction 
Our word segmentation system consists of three 
major components, namely, the pre-processing, 
token identification and the post-processing. In this 
paper, an overview of our system is briefly intro-
duced and the structure of the paper is as follows. 
Section 2 presents the system description. An agent 
based approach is introduced in the system. Asso-
ciated to each agent in the system, a vote is cast to 
indicate the certainty by each agent in the system. 
In Section 3, we describe the experimental results 
of our system, followed by the conclusion.  
 
2 System Description 
2.1 Preprocessing  
In the preprocessing, the traditional Chinese char-
acters, punctuation marks and other symbols are 
first identified. Instead of training all these sym-
bols with the traditional Chinese characters in an 
agent-based system, an initial, but rough, segmen-
tation points (SPr) are first inserted to distinguish 
the symbols and Chinese characters. For example, 
for the input sentence shown in Figure 1, segmen-
tation points are first assumed in the sentence as 
shown in the Figure 2, where ?/? indicates the pres-
ence of a segmentation point. This roughly seg-
mented sentence is then subject to an agent-based 
learning algorithm to have the token identification.   
 
?? 6 ? 05 ??????????????
????????????? 
Figure 1: Original sentence for the process 
 
??/ 6/ ?/ 05/ ????????????
????????/ ?/ ????/ ?/ ? 
Figure 2: Rough segmented sentence from pre-
processing 
 
2.2 Token Identification 
In this stage, a learning algorithm is first devised 
and implemented. The algorithm is based on an 
agent based model which is a computational model 
for simulating the actions and interactions of an 
orchestra of autonomous agents in the determina-
tion of the possible segmentation points (SPl) 
(Weiss, 1999; Wooldridge, 2002). Each agent will 
112
Sixth SIGHAN Workshop on Chinese Language Processing
make its own decision, i.e., either true or false, for 
the insertion of ?/? between the two characters. 
Moreover, associated with each decision, there is a 
vote that reflects the certainty of the decision. For 
each training corpus, we have trained more than 
200 intelligent agents, each of which exhibits cer-
tain aspects of segmentation experience and lan-
guage behaviors. In making the final verdict, the 
system will consult all the related agents by sum-
ming up their votes. For example, as shown in Ta-
ble 1, the vote that supports there is a segmentation 
point between the characters ? and ?  is zero 
while 57.33 votes recommend that there should 
have no break point. All these votes are logged for 
the further post-processing. 
 
C1 C2 Vote(T) Vote(F) ND Outcome
? ? 0 57.33 1.000 false 
? ? 44.52 6.54 0.744 true 
? ? 0 57.74 1.000 false 
? ? 64.61 0 1.000 true 
? ? 0 60.23 1.000 false 
? ? 56.29 0.99 0.965 true 
? ? 0.58 58.22 0.980 false 
? ? 58.21 0 1.000 true 
? ? 57.80 0 1.000 true 
? ? 0 51.34 1.000 false 
? ? 48.70 0 1.000 true 
? ? 60.04 0 1.000 true 
? ? 0 53.97 1.000 false 
? ? 46.19 2.00 0.917 true 
? ? 0 58.32 1.000 false 
? ? 62.44 0 1.000 true 
? ? 59.16 0 1.000 true 
? ? 4.89 40.81 0.786 false 
? ? 45.83 3.41 0.862 true 
? ? 0 60.91 1.000 false 
? ? 0 59.39 1.000 false 
? ? 54.44 0.48 0.983 true 
? ? 11.98 27.94 0.400 false 
Table 1: Votes from agents and the ND of the cor-
responding segment point. 
 
??/ 6 / ?/ 05 / ?/ ??/ ??/ ??/ ?
/ ??/ ?/ ??/ ??/ ?/ ??/ ??/ ?/ 
??/ ??/ ?/ ? 
Figure 3: Segmented sentence based on the votes 
from all agents. 
 
2.3 Post-processing 
In our experience, our system is most likely to 
generate over-segmented sentences. Several tech-
niques have implemented in our post-processing to 
merge several tokens into ones. As shown in the 
previous steps, we have introduced two main types 
of segmentation points, SPr and SPl. In the type SPr, 
segmentation points are pre-inserted between sym-
bol and Chinese characters. For example, the token 
6 ? will become 6/ ? in the early beginning. 
Obviously, this kind of errors should be identified 
and the segmentation points should be removed. 
Similarly, in SPl, segmentation points are decided 
by the votes. Our post-processing is to identify the 
weak segmentation points which are having tie-
break votes. A normalized difference (ND) is de-
fined for the certainty of the segmentation.  
falsetrue
falsetrue
VoteVote
VoteVote
ND +
?=  Eqn.(1)
 
The smaller the value of the ND, the lesser the cer-
tainty of the segmentation point. We define the 
segmentation point as weak if the value of ND is 
smaller than a threshold. For a weak segmentation 
point, the system will consult a dictionary and 
search for the presence of the token in the diction-
ary. The segmentation point will be removed if 
found. Otherwise, the system will leave as it is. As 
shown in the Table 1, almost all segmentation 
points with the ND value equal to 1. This shows 
that all the votes from the agents support the same 
decision. However, it seems that not all agents 
have the same decision to the last characters pair 
????, with ND equal to 0.4. If the threshold is 
set to be 0.4, the segmentation point will be re-
examined in our post-processing. 
Our dictionary is constructed by tokens from the 
training corpus and the local context of the text 
that is being segmented. That is to say, besides the 
corpus, the tokens from the previous segmented 
text will also contribute to the construction of the 
dictionary. On the other hand, Chinese idiom 
should be in one token as found in most dictionar-
ies. However, idiom sometimes would be identi-
fied as a short phrase and segmented into several 
pieces. In this case, we tend to merge these small 
fragments into one long token. On the other hand, 
different training sources may produce different 
segmentation rules and, thus, produce different 
113
Sixth SIGHAN Workshop on Chinese Language Processing
segmentation results. In the open tracks, some 
handlers are tailor-made for different testing data. 
These include handlers for English characters, date, 
time, organization. 
 
??/ 6 ?/ 05 ?/ ??/ ??/ ??/ ?/ ?
?/ ?/ ??/ ??/ ?/ ??/ ??/ ?/ ?
?/ ??/ ?/ ? 
Figure 4: Final result of the segmentation 
 
3 Experiments and Results 
We have participated in five closed tracks and two 
open tracks in the bakeoff. While we have built a 
dictionary from each training data set for the 
closed tracks, a dictionary of more than 150,000 
entries is maintained for the open tracks. Table 2 
shows the size of the training data sets. 
 
Source of training data Size 
Academia Sinica (CKIP) 721,551
City University of Hong Kong (CityU) 1,092,687
University of Colorado (CTB) 642,246
State Language Commission of P.R.C. 
(NCC) 
917,255
Shanxi University (SXU) 528,238
Table 2: Size of the training data in the bakeoff. 
 
Tables 3 and 4 show the recall (R), precision (P), 
F-score (F) and our ranking in the bakeoff. All the 
rankings are produced based on the best run of the 
participating teams in the tracks. 
 
 R P F Rank
CityU 0.9513 0.9430 0.9471 2nd 
CKIP 0.9455 0.9371 0.9413 3rd 
NCC 0.9365 0.9365 0.9365 3rd 
SXU 0.9558 0.9552 0.9555 5th 
Table 3: Performance of our system in the closed 
tracks of word segmentation task in the bakeoff. 
 
 R P F Rank
CKIP 0.9586 0.9541 0.9563 1st 
NCC 0.9440 0.9517 0.9478 4th 
Table 4: Performance of our system in the open 
tracks of word segmentation task in the bakeoff.  
 
From the above tables, we have the following ob-
servations: 
y First, our system is performing well if it is a 
sufficient large set of training data. This is 
evidenced by the results found in the training 
data from CKIP, CityU and NCC.   
y Second, the dictionaries play an important 
role in our open tracks. While we have main-
tained a dictionary with 150,000 traditional 
Chinese words, no such a device is for our 
simplified characters corpora. Certainly, there 
is a room for our further improvement. 
4 Conclusion 
In this paper, we have presented the general over-
view of our segmentation system. Even though, it 
is our first time to participate the bakeoff, the ap-
proach is promising. Further exploration is needed 
to enhance the system.   
Acknowledgement 
The work described in this paper was partially 
supported by the grants from the Research Grants 
Council of the Hong Kong Special Administrative 
Region, China (Project Nos. CUHK4438/04H and 
CUHK4706/05H). 
 
References 
Weiss, G. (1999) Multiagent Systems, A Modern 
Approach to Distributed Artificial Intelligence, MIT 
Press. 
Wooldridge, M. (2002). An Introduction to MultiAgent 
Systems, John Wiley. 
 
114
Sixth SIGHAN Workshop on Chinese Language Processing
Coling 2010: Poster Volume, pages 117?125,
Beijing, August 2010
Tree Topological Features for Unlexicalized Parsing
Samuel W. K. Chan? Lawrence Y. L. Cheung# Mickey W. C. Chong?
?Dept. of Decision Sciences 
Chinese University of Hong Kong 
#Dept. of Linguistics & Modern Languages 
Chinese University of Hong Kong 
{swkchan, yllcheung, mickey_chong}@cuhk.edu.hk 
Abstract 
As unlexicalized parsing lacks word to-
ken information, it is important to inves-
tigate novel parsing features to improve 
the accuracy. This paper studies a set of 
tree topological (TT) features. They 
quantitatively describe the tree shape 
dominated by each non-terminal node. 
The features are useful in capturing lin-
guistic notions such as grammatical 
weight and syntactic branching, which 
are factors important to syntactic proc-
essing but overlooked in the parsing lit-
erature. By using an ensemble classifier-
based model, TT features can signifi-
cantly improve the parsing accuracy of 
our unlexicalized parser. Further, the 
ease of estimating TT feature values 
makes them easy to be incorporated into 
virtually any mainstream parsers.  
1 Introduction 
Many state-of-the-art parsers work with lexical-
ized parsing models that utilize the information 
and statistics of word tokens (Magerman, 1995; 
Collins, 1999, 2003; Charniak, 2000). The per-
formance of lexicalized models is susceptible to 
vocabulary variation as lexical statistics is often 
corpus-specific (Ratnaparkhi, 1999; Gildea, 
2001). As parsers are typically evaluated using 
the Penn Treebank (Marcus et al, 1993), which 
is based on financial news, the problems of 
lexicalized parsing could easily be overlooked. 
Unlexicalized models, on the other hand, are 
less sensitive to lexical variation and are more 
portable across domains. Though the perform-
ance of unlexicalized models was believed not 
to exceed that of lexicalized models (Klein & 
Manning, 2003), Petrov & Klein (2007) show 
that unlexicalized parsers can match lexicalized 
parsers in performance using the grammar rule 
splitting technique. Given the practical advan-
tages and the latest development, unlexicalized 
parsing deserves further scrutiny.  
A profitable direction of research on unlexi-
calized parsing is to investigate novel parsing 
features. This paper examines a set of what we 
call tree topological (TT) features, including 
phrase span, phrase height, tree skewness, etc. 
This study is motivated by the fact that conven-
tional parsers rarely consider the shape of 
subtrees dominated by these nodes and rely 
primarily on matching tags. As a result, an NP 
with a complicated structure is treated the same 
as an NP that dominates only one word. How-
ever, our study shows that TT features are use-
ful predictors of phrase boundaries, a critical 
ambiguity resolution issue. TT features have 
two more advantages. First, TT features capture 
linguistic properties, such as branching and 
grammatical ?heaviness?, across different syn-
tactic structures. Second, they are easily com-
putable without the need for extra language re-
sources.  
The organization of the paper is as follows. 
Section 2 reviews the features commonly used 
in parsing. Section 3 provides the details of TT 
features in the unlexicalized parser. The parser 
is evaluated in Section 4. In Section 5, we 
discuss the effectiveness and advantages of TT 
features in parsing and possible enhancement. 
This is followed by a conclusion in Section 6. 
2 Related Work 
2.1 Parsing Features 
This section reviews major types of information 
in parsing.  
117
Tags: The dominant types of information that 
drive parsing and chunking algorithms are 
POS/syntactic tags, context-free grammar (CFG) 
rules, and their statistical properties. Matching 
tags against CFG rules to form phrases is central 
to all basic parsing algorithms such as Cocke-
Kasami-Younger (CKY) algorithm, and the Ear-
ley algorithm, and the chart parsing.  
Word Token-based: Machine learning and sta-
tistical modelling emerged in the 90s as an ideal 
computational approach to feature-rich parsing. 
Classifiers can typically capitalize on a large set 
of features in decision making. Magerman 
(1995), Ratnaparkh (1999) and Charniak (2000) 
used classifiers to model dependencies between 
word pairs. They popularized the use word to-
kens as attributes in lexicalized parsing. Collins 
(1999, 2003) also integrated information like 
head word and distance from head into the sta-
tistical model to enhance probabilistic chart 
parsing. Since then, word tokens, head words 
and their statistical derivatives have become 
standard features in many parsers. Word token 
information is also fundamental to dependency 
parsing (K?bler et al, 2009) because depend-
ency grammar is rooted in the idea that the head 
and the dependent word are related by different 
dependency relations.  
Semantic-based: Some efforts have also been 
made to consider semantic features, such as 
sense tags, in parsing. Words are first tagged 
with semantic classes, often using WordNet-
based resources. The lexical semantic class can 
be instructive to the selection of the correct 
parse from a set of candidate structures. It has 
been reported that the lexical semantics of 
words is effective in resolving structural ambi-
guity, especially PP-attachment (Black et al, 
1992; Stetina & Nagao, 1997; Agirre et al, 
2008). Nevertheless, the use of semantic fea-
tures has still been relatively rare. They incur 
overheads in acquiring semantic language re-
sources, such as sense-tagged corpora and 
WordNet databases. Semantic-based parsing 
also requires accurate sense-tagging.  
Since substantial gain from tag features is 
unlikely in the near future and deriving seman-
tic features is often a tremendous task, there is a 
pressing need to seek for new features, particu-
larly in unlexicalized parsing. 
2.2 Linguistic-motivated Features 
In this section, a review of the linguistic motiva-
tion behind the TT features is provided. 
Grammatical Weight: Apart from syntactic 
categories, linguists have long observed that the 
number of words (often referred to as ?weight? 
or ?heaviness?) in a phrase can affect syntactic 
processing of sentences (Quirk et al, 1985; Wa-
sow, 1997; Rosenbach, 2005). It corresponds 
roughly to the span feature described in Section 
3.2. The effect of grammatical weight often 
manifests in word order variation. Heavy NP 
shift, dative alternation, particle movement and 
extraposition in English are canonical examples 
where ?heavy? chunks get dislocated to the end 
of a sentence. In his corpus analysis, Wasow 
(1997) found that weight is a very crucial factor 
in determining dative alternation. Hawkins 
(1994) also argued that due to processing con-
straints, the human syntactic processor tends to 
group an incoming stream of words as rapidly 
as possible, preferring smaller chunks on the left.
Tree Topology: CFG-based parsing approach 
hides the structural properties of the dominated 
subtree from the associated syntactic tag. Struc-
tural topology, or tree shape, however, can be 
useful in guiding the parser to group tags into 
phrases. Structures significantly deviating from 
left/right branching, e.g. center embedding, are 
much more difficult to process and rare in pro-
duction (Gibson, 1998). Another example is the 
resolution of scope ambiguity in coordinate 
structures (CSs). CSs are common but notori-
ously difficult to parse due to scope ambiguity 
when the conjuncts are complex (Collins, 1999; 
K?bler et al, 2009). One good cue to the prob-
lem is that humans prefer CSs with parallel in-
ternal syntactic structures (Frazier et al, 2000). 
In a corpus-based study, Dubey et al (2008) 
show that structural repetition across conjuncts 
is significantly more frequent. The implication 
to parsing is that preference should be given to 
bracketing in which conjuncts are structurally 
similar. TT information can inform the parser of 
the structural properties of phrases.  
3 An Ensemble-based Parser 
To accommodate a large set of features, we opt 
for classifier-based parsing because classifiers 
118
can easily handle many features, as pointed out 
in Ratnaparkhi (1999). This is different from 
chart parsing models popular in many parsers 
(e.g. Collins, 2003) which require special statis-
tical modelling. Our parser starts from a string 
of POS tags without any hints from words. As 
in other similar approaches (Abney 1991; Ram-
shaw & Marcus, 1995; Sang, 2001; Sagae & 
Lavie, 2005), the first and the foremost problem 
that has to be resolved is to identify the bound-
ary points of phrases, without any explicit 
grammar rules. Here we adopt the ensemble 
learning technique to unveil boundary points, or 
chunking points hereafter. Two heterogeneous 
and mutually independent attribute feature sets 
are introduced in Section 3.2 and 3.3.   
3.1 Basic Architecture of the Parser 
Our parser has two modules, namely, a chunker 
and a phrase recognizer. The chunker locates 
the boundaries of chunks while the phrase rec-
ognizer predicts the non-terminal syntactic tag 
of the identified chunks, e.g. NP, VP, etc. In the 
chunker, we explore a new approach that aims 
at identifying chunk boundaries. Assume that 
the input of the chunker is a tag sequence <x0 ? 
xn ? xm> where 0 ? n ? m. Let yn be the point of 
focus between two consecutive tags xn and xn+1. 
The chunker classifies all focus points as either 
a chunking point or a merging point at the rele-
vant level. A focus point yn is a merging point if 
xn and xn+1 are siblings of the same parent node 
in the target parse tree. Otherwise, yn is a chunk-
ing point. Consider the tag sequence and the 
expected classification of points in the example 
below. Chunking points are marked with ?%? 
and merging points with ?+?. 
PRP % VBZ % DT % RB   +  JJ  % NN 
He    is    a    very    nice  guy  
The point between RB and JJ is a merging 
point because they are siblings of the parent 
node ADJP in the target parse tree. The point 
between DT and RB is a chunking point since 
DT and RB are not siblings and do not share the 
same parent node. Chunks are defined as the 
consecutive tag sequences not split up by %. 
When a focus point yn is classified as a chunk-
ing point, it effectively means that no fragment 
preceding yn can combine with any fragment 
following yn to form a phrase, i.e. a distituent.  
Both the chunker and the recognizer are 
trained using the Penn Treebank (Marcus et al, 
1993). In addition, we adopt the ensemble tech-
nique to combine two sets of heterogeneous fea-
tures. The method yields a much more accurate 
predictive power (Dietterich, 2000). One neces-
sary and sufficient condition for an ensemble of 
classifiers to be more accurate than any of its 
individual members is that the classifiers must 
be diverse. Table 1 summaries the basic ration-
ale of the parser. The two feature sets will be 
further explained in Section 3.2 and 3.3.  
 Prepare training data from the Treebank based 
on topological & information-theoretic features 
 Train the chunker and phrase recognizer using 
the ensemble technique 
 For any input tag sequence l,  
WHILE l contains more than one element DO 
IDENTIFY the status, + or %, of each focus 
point in l
RECOGNIZE the syntactic tag (ST) of each 
identified chunk 
UPDATE l with the new ST sequence
ENDWHILE 
 Display the parse tree 
Table 1. Basic rationale of the parser 
The learning module acquires the knowledge 
encoded in the Penn Treebank to support vari-
ous classification tasks. The input tag sequence 
is first fed into the chunker. The phrase recog-
nizer then analyzes the chunker?s output and 
assigns non-terminal syntactic tags (e.g. NP, VP, 
etc.) to identified chunks. The updated tag se-
quence is fed back to the chunker for processing 
at the next level. The iteration continues until a 
complete parse is formed. 
3.2 Tree Topological Feature Set 
Tree topological (TT) features describe the 
shape of subtrees quantitatively. Our approach 
to addressing this problem involves examining a 
set of topological features, without any assump-
tion of the word tokens. They all have been im-
plemented for chunking.  
Node Coordinates (NCs): NCs include the level 
of focus (LF) and the relative position (RP) of 
the target subtree. The level of focus is defined 
as the total number of levels under the target 
node, with the terminal level inclusive while the 
RP indicates the linear position of the target 
node in that level. As in Figure 1, the LF for 
119
subtree A and B are the same; however, the RP
for subtree A is smaller than that for subtree B.  
Span Ratio (SR): The SR is defined as the total 
number of terminal nodes spanned under the 
target node and is divided by the length of the 
sentence. In Figure 1, the span ratio for the tar-
get node VP at subtree B is 5/12. This ratio il-
lustrates not only how many terminal nodes are 
covered by the target node, but also how far the 
target node is from the root S.  
Aspect Ratio (AR): The AR of a target node in a 
subtree is defined as the ratio of the total num-
ber of non-terminal nodes involved to the total 
number of terminal nodes spanned. The AR is 
also indicative of the average branching factor 
of the subtree.  
Skewness Measure (SM): The SM estimates the 
degree to which the subtree leans towards either 
left or right. In this research, the SM of a subtree 
is evaluated by the distribution of the length of 
the paths connecting the target node and each 
terminal node it dominates. The length of a path 
from a target node V to a terminal node T is the 
number of edges between V and T. For a tree 
with n terminal nodes, there are n paths. A pivot 
is defined as the [n/2]th terminal node when n is 
odd and between [n/2]th and [(n+1)/2]th termi-
nal nodes if n is even, where [ ] is a ceiling 
function. The SM is defined as  
( )
??
??
?
?
??
??
?
?
?
=
?
?
=
>
3
1
3
0
1
?
?
?
?
n
i
ii
i
xx
SM
i
          Eqn (1) 
where xi is the length of the i-th path pointing to 
the i-th terminal node, x and ? are the average 
and standard deviation of the length of all paths 
at that level of focus (LF). ?i is the distance 
measured from the i-th terminal node to the 
pivot. The distance is positive if the terminal 
node is to the left of the pivot, zero if it is right 
at the pivot, and negative if the terminal node is 
to the right of the pivot. Obviously, if the 
lengths of all paths are the same in the tree, the 
numerator of Eqn (1) will be crossed out and the 
SM returns to zero. The pivot also provides an 
axis of vertical flipping where the SM still holds. 
The farther the terminal node from the pivot, the 
longer the distance. The distances ? provide the 
moment factors to quantify the skewness of 
trees. For illustration, let us consider subtree B
with the target node VP at level of focus (LF) = 
4 in Figure 1. Since there are five terminal 
nodes, the pivot is at the third node VB. The 
lengths of the paths xi from left to right in the 
subtree are 1, 2, 3, 4, 4 and the moment factors 
?i for the paths are 2, 1, 0, -1, -2. Assuming that 
x and ? for all the trees in the Treebank at 
level 4 are, say, 2.9 and 1.2 respectively, then 
SM = -3.55. It implies that subtree B under the 
target node VP has a strong right branching ten-
dency, even though it has a very uniform 
branching factor which is usually defined as the 
number of children at each node. 









	
	

 

       
 

