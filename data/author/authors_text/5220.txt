Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 967?975, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Translation Quality by Discarding Most of the Phrasetable
J Howard Johnson and Joel Martin
Interactive Information Group
National Research Council Canada
Ottawa, Ontario, Canada
firstname.lastname@nrc.gc.ca
George Foster and Roland Kuhn
Interactive Language Technologies Group
National Research Council Canada
Gatineau, Que?bec, Canada
firstname.lastname@nrc.gc.ca
Abstract
It is possible to reduce the bulk of phrase-
tables for Statistical Machine Translation us-
ing a technique based on the significance
testing of phrase pair co-occurrence in the
parallel corpus. The savings can be quite
substantial (up to 90%) and cause no reduc-
tion in BLEU score. In some cases, an im-
provement in BLEU is obtained at the same
time although the effect is less pronounced
if state-of-the-art phrasetable smoothing is
employed.
1 Introduction
An important part of the process of Statistical Ma-
chine Translation (SMT) involves inferring a large
table of phrase pairs that are translations of each
other from a large corpus of aligned sentences.
These phrase pairs together with estimates of con-
ditional probabilities and useful feature weights,
called collectively a phrasetable, are used to match
a source sentence to produce candidate translations.
The choice of the best translation is made based
on the combination of the probabilities and feature
weights, and much discussion has been made of how
to make the estimates of probabilites, how to smooth
these estimates, and what features are most useful
for discriminating among the translations.
However, a cursory glance at phrasetables pro-
duced often suggests that many of the translations
are wrong or will never be used in any translation.
On the other hand, most obvious ways of reducing
the bulk usually lead to a reduction in translation
quality as measured by BLEU score. This has led to
an impression that these pairs must contribute some-
thing in the grand scheme of things and, certainly,
more data is better than less.
Nonetheless, this bulk comes at a cost. Large ta-
bles lead to large data structures that require more
resources and more time to process and, more im-
portantly, effort directed in handling large tables
could likely be more usefully employed in more fea-
tures or more sophisticated search.
In this paper, we show that it is possible to prune
phrasetables using a straightforward approach based
on significance testing, that this approach does not
adversely affect the quality of translation as mea-
sured by BLEU score, and that savings in terms of
number of discarded phrase pairs can be quite sub-
stantial. Even more surprising, pruning can actu-
ally raise the BLEU score although this phenomenon
is less prominent if state of the art smoothing of
phrasetable probabilities is employed.
Section 2 reviews the basic ideas of Statistical
Machine Translation as well as those of testing sig-
nificance of associations in two by two contingency
tables departing from independence. From this, a
filtering algorithm will be described that keeps only
phrase pairs that pass a significance test. Section 3
outlines a number of experiments that demonstrate
the phenomenon and measure its magnitude. Sec-
tion 4 presents the results of these experiments. The
paper concludes with a summary of what has been
learned and a discussion of continuing work that
builds on these ideas.
967
2 Background Theory
2.1 Our Approach to Statistical Machine
Translation
We define a phrasetable as a set of source phrases (n-
grams) s? and their translations (m-grams) t?, along
with associated translation probabilities p(s?|t?) and
p(t?|s?). These conditional distributions are derived
from the joint frequencies c(s?, t?) of source / tar-
get n,m-grams observed in a word-aligned parallel
corpus. These joint counts are estimated using the
phrase induction algorithm described in (Koehn et
al., 2003), with symmetrized word alignments gen-
erated using IBM model 2 (Brown et al, 1993).
Phrases are limited to 8 tokens in length (n,m ? 8).
Given a source sentence s, our phrase-based SMT
system tries to find the target sentence t? that is the
most likely translation of s. To make search more
efficient, we use the Viterbi approximation and seek
the most likely combination of t and its alignment a
with s, rather than just the most likely t:
t? = argmax
t
p(t|s) ? argmax
t,a
p(t,a|s),
where a = (s?1, t?1, j1), ..., (s?K , t?K , jK); t?k are tar-
get phrases such that t = t?1...t?K ; s?k are source
phrases such that s = s?j1 ...s?jK ; and s?k is the trans-
lation of the kth target phrase t?k.
To model p(t,a|s), we use a standard loglinear
approach:
p(t,a|s) ? exp
[
?
i
?ifi(s, t,a)
]
where each fi(s, t,a) is a feature function, and
weights ?i are set using Och?s algorithm (Och,
2003) to maximize the system?s BLEU score (Pa-
pineni et al , 2001) on a development corpus. The
features used are: the length of t; a single-parameter
distortion penalty on phrase reordering in a, as de-
scribed in (Koehn et al, 2003); phrase translation
model probabilities; and 4-gram language model
probabilities log p(t), using Kneser-Ney smooth-
ing as implemented in the SRILM toolkit (Stolcke,
2002).
Phrase translation model probabilities are features
of the form:
log p(s|t,a) ?
K?
k=1
log p(s?k|t?k)
i.e., we assume that the phrases s?k specified by a are
conditionally independent, and depend only on their
aligned phrases t?k.
The ?forward? phrase probabilities p(t?|s?) are not
used as features, but only as a filter on the set of
possible translations: for each source phrase s? that
matches some ngram in s, only the 30 top-ranked
translations t? according to p(t?|s?) are retained. One
of the reviewers has pointed out correctly that tak-
ing only the top 30 translations will interact with the
subject under study; however, this pruning technique
has been used as a way of controlling the width of
our beam search and rebalancing search parameters
would have complicated this study and taken it away
from our standard practice.
The phrase translation model probabilities are
smoothed according to one of several techniques as
described in (Foster et al, 2006) and identified in the
discussion below.
2.2 Significance testing using two by two
contingency tables
Each phrase pair can be thought of as am n,m-gram
(s?, t?) where s? is an n-gram from the source side of
the corpus and t? is an m-gram from the target side
of the corpus.
We then define: C(s?, t?) as the number of parallel
sentences that contain one or more occurrences of
s? on the source side and t? on the target side; C(s?)
the number of parallel sentences that contain one or
more occurrences of s? on the source side; and C(t?)
the number of parallel sentences that contain one or
more occurrences of t? on the target side. Together
with N , the number of parallel sentences, we have
enough information to draw up a two by two contin-
gency table representing the unconditional relation-
ship between s? and t?. This table is shown in Table
1.
A standard statistical technique used to assess the
importance of an association represented by a con-
tingency table involves calculating the probability
that the observed table or one that is more extreme
could occur by chance assuming a model of inde-
pendence. This is called a significance test. Intro-
ductory statistics texts describe one such test called
the Chi-squared test.
There are other tests that more accurately apply
to our small tables with only two rows and columns.
968
Table 1: Two by two contingency table for s? and t?
C(s?, t?) C(s?)? C(s?, t?) C(s?)
C(t?)? C(s?, t?) N ? C(s?)? C(t?) + C(s?, t?) N ? C(s?)
C(t?) N ? C(t?) N
In particular, Fisher?s exact test calculates probabil-
ity of the observed table using the hypergeometric
distibution.
ph(C(s?, t?)) =
(
C(s?)
C(s?, t?)
)(
N ? C(s?)
C(t?)? C(s?, t?)
)
(
N
C(t?)
)
The p-value associated with our observed table is
then calculated by summing probabilities for tables
that have a larger C(s?, t?)).
p-value(C(s?, t?)) =
??
k=C(s?,t?)
ph(k)
This probability is interpreted as the probability
of observing by chance an association that is at least
as strong as the given one and hence its significance.
Agresti (1996) provides an excellent introduction to
this topic and the general ideas of significance test-
ing in contingency tables.
Fisher?s exact test of significance is considered a
gold standard since it represents the precise proba-
bilities under realistic assumptions. Tests such as the
Chi-squared test or the log-likelihood-ratio test (yet
another approximate test of significance) depend on
asymptotic assumptions that are often not valid for
small counts.
Note that the count C(s?, t?) can be larger or
smaller than c(s?, t?) discussed above. In most cases,
it will be larger, because it counts all co-occurrences
of s? with t? rather than just those that respect the
word alignment. It can be smaller though because
multiple co-occurrences can occur within a single
aligned sentence pair and be counted multiple times
in c(s?, t?). On the other hand, C(s?, t?) will not count
all of the possible ways that an n,m-grammatch can
occur within a single sentence pair; it will count the
match only once per sentence pair in which it occurs.
Moore (2004) discusses the use of signifi-
cance testing of word associations using the log-
likelihood-ratio test and Fisher?s exact test. He
shows that Fisher?s exact test is often a practical
method if a number of techniques are followed:
1. approximating the logarithms of factorials us-
ing commonly available numerical approxima-
tions to the log gamma function,
2. using a well-known recurrence for the hyperge-
ometic distribution,
3. noting that few terms usually need to be
summed, and
4. observing that convergence is usually rapid.
2.3 Significance pruning
The idea behind significance pruning of phrasetables
is that not all of the phrase pairs in a phrasetable are
equally supported by the data and that many of the
weakly supported pairs could be removed because:
1. the chance of them occurring again might be
low, and
2. their occurrence in the given corpus may be the
result of an artifact (a combination of effects
where several estimates artificially compensate
for one another). This concept is usually re-
ferred to as overfit since the model fits aspects
of the training data that do not lead to improved
prediction.
Phrase pairs that cannot stand on their own by
demonstrating a certain level of significance are sus-
pect and removing them from the phrasetable may
969
be beneficial in terms of reducing the size of data
structures. This will be shown to be the case in rather
general terms.
Note that this pruning may and quite often will
remove all of the candidate translations for a source
phrase. This might seem to be a bad idea but it must
be remembered that deleting longer phrases will al-
low combinations of shorter phrases to be used and
these might have more and better translations from
the corpus. Here is part of the intuition about how
phrasetable smoothing may interact with phrasetable
pruning: both are discouraging longer but infrequent
phrases from the corpus in favour of combinations of
more frequent, shorter phrases.
Because the probabilities involved below will be
so incredibly tiny, we will work instead with the neg-
ative of the natural logs of the probabilities. Thus
instead of selecting phrase pairs with a p-value less
than exp(?20), we will select phrase pairs with a
negative-log-p-value greater than 20. This has the
advantage of working with ordinary-sized numbers
and the happy convention that bigger means more
pruning.
2.4 C(s?, t?) = 1, 1-1-1 Tables and the ?
Threshold
An important special case of a table occurs when a
phrase pair occurs exactly once in the corpus, and
each of the component phrases occurs exactly once
in its side of the parallel corpus.
These phrase pairs will be referred to as 1-1-1
phrase pairs and the corresponding tables will be
called 1-1-1 contingency tables because C(s?) = 1,
C(t?) = 1, and C(s?, t?) = 1.
Moore (2004) comments that the p-value for these
tables under Fisher?s exact test is 1/N . Since we are
using thresholds of the negative logarithm of the p-
value, the value ? = log(N) is a useful threshold to
consider.
In particular, ? +  (where  is an appropriately
small positive number) is the smallest threshold that
results in none of the 1-1-1 phrase pairs being in-
cluded. Similarly, ? ?  is the largest threshold that
results in all of the 1-1-1 phrase pairs being included.
Because 1-1-1 phrase pairs can make up a large part
of the phrase table, this is important observation for
its own sake.
Since the contingency table with C(s?, t?) = 1 hav-
ing the greatest significance (lowest p-value) is the
1-1-1 table, using the threshold of ?+  can be used
to exclude all of the phrase pairs occurring exactly
once (C(s?, t?) = 1).
The common strategy of deleting all of the 1-
count phrase pairs is very similar in effect to the use
of the ? +  threshold.
3 Experiments
3.1 WMT06
The corpora used for most of these experiments are
publicly available and have been used for a num-
ber of comparative studies (Workshop on Statisti-
cal Machine Translation, 2006). Provided as part of
the materials for the shared task are parallel corpora
for French?English, Spanish?English, and German?
English as well as language models for English,
French, Spanish, and German. These are all based
on the Europarl resources (Europarl, 2003).
The only change made to these corpora was to
convert them to lowercase and to Unicode UTF-8.
Phrasetables were produced by symmetrizing IBM2
conditional probabilities as described above.
The phrasetables were then used as a list of
n,m-grams for which counts C(s?, t?), C(s?), and
C(t?) were obtained. Negative-log-p-values under
Fisher?s exact test were computed for each of the
phrase pairs in the phrasetable and the entry was
censored if the negative-log-p-value for the test was
below the pruning threshold. The entries that are
kept are ones that are highly significant.
A number of combinations involving many differ-
ent pruning thresholds were considered: no pruning,
10, ??, ?+, 15, 20, 25, 50, 100, and 1000. In ad-
dition, a number of different phrasetable smoothing
algorithms were used: no smoothing, Good-Turing
smoothing, Kneser-Ney 3 parameter smoothing and
the loglinear mixture involving two features called
Zens-Ney (Foster et al, 2006).
3.2 Chinese
To test the effects of significance pruning on larger
corpora, a series of experiments was run on a much
larger corpus based on that distributed for MT06
Chinese?English (NIST MT, 2006). Since the ob-
jective was to assess how the method scaled we used
our preferred phrasetable smoothing technique of
970
1000100101
BLEU by Pruning Threshold
no smoothing
3
3
333 3
3
3
3
GT (+1)
+ +
+++ +
+
+
+
KN3 (+2)
2 2222 2
2
2
2
ZN (+3)
? ???? ?
?
?
?
107
106
105
1000100101
Phrasetable Size by Pruning Threshold
size3 3
333
3
3
3
3
107106105
BLEU by Phrasetable Size
no smoothing
3
3
3333
3
3
3
GT (+1)
++
++++
+
+
+
KN3 (+2)
222222
2
2
2
ZN (+3)
??????
?
?
?
Figure 1: WMT06: Results for French ?? English.
[to separate the curves, graphs for smoothed meth-
ods are shifted by +1, +2, or +3 BLEU points]
Table 2: Corpus Sizes and ? Values
number of
parallel sentences ?
WMT06: fr?? en 688,031 13.4415892
WMT06: es?? en 730,740 13.501813
WMT06: de?? en 751,088 13.5292781
Chinese?English: best 3,164,228 14.9674197
Chinese?English: UN-v2 4,979,345 15.4208089
Zens-Ney and separated our corpus into two phrase-
tables, one based on the UN corpus and the other
based on the best of the remaining parallel corpora
available to us.
Different pruning thresholds were considered: no
pruning, 14, 16, 18, 20, and 25. In addition, another
more aggressive method of pruning was attempted.
Moore points out, correctly, that phrase pairs that oc-
cur in only one sentence pair, (C(s?, t?) = 1 ), are less
reliable and might require more special treatment.
These are all pruned automatically at thresholds of
16 and above but not at threshold of 14. A spe-
cial series of runs was done for threshold 14 with all
of these singletons removed to see whether at these
thresholds it was the significance level or the prun-
ing of phrase pairs with (C(s?, t?) = 1 ) that was more
important. This is identified as 14? in the results.
4 Results
The results of the experiments are described in Ta-
bles 2 through 6.
Table 2 presents the sizes of the various parallel
corpora showing the number of parallel sentences,
N , for each of the experiments, together with the ?
thresholds (? = log(N)).
Table 3 shows the sizes of the phrasetables that
result from the various pruning thresholds described
for the WMT06 data. It is clear that this is extremely
aggressive pruning at the given levels.
Table 4 shows the corresponding phrasetable sizes
for the large corpus Chinese?English data. The
pruning is not as aggressive as for the WMT06 data
but still quite sizeable.
Tables 5 and 6 show the main results for the
WMT06 and the Chinese?English large corpus ex-
periments. To make these results more graphic, Fig-
ure 1 shows the French ?? English data from the
WMT06 results in the form of three graphs. Note
971
Table 3: WMT06: Distinct phrase pairs by pruning threshold
threshold fr?? en es?? en de?? en
none 9,314,165 100% 11,591,013 100% 6,954,243 100%
10 7,999,081 85.9% 10,212,019 88.1% 5,849,593 84.1%
??  6,014,294 64.6% 7,865,072 67.9% 4,357,620 62.7%
? +  1,435,576 15.4% 1,592,655 13.7% 1,163,296 16.7%
15 1,377,375 14.8% 1,533,610 13.2% 1,115,559 16.0%
20 1,152,780 12.4% 1,291,113 11.1% 928,855 13.4%
25 905,201 9.7% 1,000,264 8.6% 732,230 10.5%
50 446,757 4.8% 481,737 4.2% 365,118 5.3%
100 235,132 2.5% 251,999 2.2% 189,655 2.7%
1000 22,873 0.2% 24,070 0.2% 16,467 0.2%
Table 4: Chinese?English: Distinct phrase pairs by pruning threshold
threshold best UN-v2
none 18,858,589 100% 20,228,273 100%
14 7,666,063 40.7% 13,276,885 65.6%
16 4,280,845 22.7% 7,691,660 38.0%
18 4,084,167 21.7% 7,434,939 36.8%
20 3,887,397 20.6% 7,145,827 35.3%
25 3,403,674 18.0% 6,316,795 31.2%
also pruning C(s?, t?) = 1
14? 4,477,920 23.7% 7,917,062 39.1%
that an artificial separation of 1 BLEU point has
been introduced into these graphs to separate them.
Without this, they lie on top of each other and hide
the essential point. In compensation, the scale for
the BLEU co-ordinate has been removed.
These results are summarized in the following
subsections.
4.1 BLEU as a function of threshold
In tables 5 and 6, the largest BLEU score for each
set of runs has been marked in bold font. In addition,
to highlight that there are many near ties for largest
BLEU, all BLEU scores that are within 0.1 of the
best are also marked in bold.
When this is done it becomes clear that pruning
at a level of 20 for the WMT06 runs would not re-
duce BLEU in most cases and in many cases would
actually increase it. A pruning threshold of 20 cor-
responds to discarding roughly 90% of the phrase-
table.
For the Chinese?English large corpus runs, a level
of 16 seems to be about the best with a small in-
crease in BLEU and a 60% ? 70% reduction in the
size of the phrasetable.
4.2 BLEU as a function of depth of pruning
Another view of this can be taken from Tables 5
and 6. The fraction of the phrasetable retained is
a more or less simple function of pruning threshold
as shown in Tables 3 and 4. By including the per-
centages in Tables 5 and 6, we can see that BLEU
goes up as the fraction approaches between 20% and
30%.
This seems to be a relatively stable observation
across the experiments. It is also easily explained by
its strong relationship to pruning threshold.
4.3 Large corpora
Table 6 shows that this is not just a small corpus phe-
nomenon. There is a sizeable benefit both in phrase-
table reduction and a modest improvement to BLEU
even in this case.
4.4 Is this just the same as phrasetable
smoothing?
One question that occurred early on was whether this
improvement in BLEU is somehow related to the
improvement in BLEU that occurs with phrasetable
smoothing.
972
It appears that the answer is, in the main, yes, al-
though there is definitely something else going on.
It is true that the benefit in terms of BLEU is less-
ened for better types of phrasetable smoothing but
the benefit in terms of the reduction in bulk holds. It
is reassuring to see that no harm to BLEU is done by
removing even 80% of the phrasetable.
4.5 Comment about C(s?, t?) = 1
Another question that came up is the role of phrase
pairs that occur only once: C(s?, t?) = 1. In particu-
lar as discussed above, the most significant of these
are the 1-1-1 phrase pairs whose components also
only occur once: C(s?) = 1, and C(t?) = 1. These
phrase pairs are amazingly frequent in the phrase-
tables and are pruned in all of the experiments ex-
cept when pruning threshold is equal to 14.
The Chinese?English large corpus experiments
give us a good opportunity to show that significance
level seems to be more an issue than the case that
C(s?, t?) = 1.
Note that we could have kept the phrase pairs
whose marginal counts were greater than one but
most of these are of lower significance and likely
are pruned already by the threshold. The given con-
figuration was considered the most likely to yield a
benefit and its poor performance led to the whole
idea being put aside.
5 Conclusions and Continuing Work
To sum up, the main conclusions are five in number:
1. Phrasetables produced by the standard Diag-
Andmethod (Koehn et al, 2003) can be aggres-
sively pruned using significance pruning with-
out worsening BLEU.
2. If phrasetable smoothing is not done, the BLEU
score will improve under aggressive signifi-
cance pruning.
3. If phrasetable smoothing is done, the improve-
ment is small or negligible but there is still no
loss on aggressive pruning.
4. The preservation of BLEU score in the pres-
ence of large-scale pruning is a strong effect in
small and moderate size phrasetables, but oc-
curs also in much larger phrasetables.
5. In larger phrasetables based on larger corpora,
the percentage of the table that can be dis-
carded appears to decrease. This is plausible
since a similar effect (a decrease in the benefit
of smoothing) has been noted with phrasetable
smoothing (Foster et al, 2006). Together these
results suggest that, for these corpus sizes, the
increase in the number of strongly supported
phrase pairs is greater than the increase in the
number of poorly supported pairs, which agrees
with intuition.
Although there may be other approaches to prun-
ing that achieve a similar effect, the use of Fisher?s
exact test is mathematically and conceptually one of
the simplest since it asks a question separately for
each phrase pair: ?Considering this phase pair in
isolation of any other analysis on the corpus, could it
have occurred plausibly by purely random processes
inherent in the corpus construction?? If the answer
is ?Yes?, then it is hard to argue that the phrase pair
is an association of general applicability from the
evidence in this corpus alone.
Note that the removal of 1-count phrase pairs is
subsumed by significance pruning with a threshold
greater than ? and many of the other simple ap-
proaches (from an implementation point of view)
are more difficult to justify as simply as the above
significance test. Nonetheless, there remains work
to do in determining if computationally simpler ap-
proaches do as well. Moore?s work suggests that
log-likelihood-ratio would be a cheaper and accurate
enough alternative, for example.
We will now return to the interaction of the se-
lection in our beam search of the top 30 candidates
based on forward conditional probabilities. This will
affect our results but most likely in the following
manner:
1. For very small thresholds, the beam will be-
come much wider and the search will take
much longer. In order to allow the experiments
to complete in a reasonable time, other means
will need to be employed to reduce the choices.
This reduction will also interact with the sig-
nificance pruning but in a less understandable
manner.
2. For large thresholds, there will not be 30
973
choices and so there will be no effect.
3. For intermediate thresholds, the extra prun-
ing might reduce BLEU score but by a small
amount because most of the best choices are
included in the search.
Using thresholds that remove most of the phrase-
table would no doubt qualify as large thresholds so
the question is addressing the true shape of the curve
for smaller thresholds and not at the expected operat-
ing levels. Nonetheless, this is a subject for further
study, especially as we consider alternatives to our
?filter 30? approach for managing beam width.
There are a number of important ways that this
work can and will be continued. The code base for
taking a list of n,m-grams and computing the re-
quired frequencies for signifance evaluation can be
applied to related problems. For example, skip-n-
grams (n-grams that allow for gaps of fixed or vari-
able size) may be studied better using this approach
leading to insight about methods that weakly ap-
proximate patterns.
The original goal of this work was to better un-
derstand the character of phrasetables, and it re-
mains a useful diagnostic technique. It will hope-
fully lead to more understanding of what it takes
to make a good phrasetable especially for languages
that require morphological analysis or segmentation
to produce good tables using standard methods.
The negative-log-p-value promises to be a useful
feature and we are currently evaluating its merits.
6 Acknowledgement
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).?
References
Alan Agresti. 1996. An Introduction to Categorical Data
Analysis. Wiley.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra and Robert L. Mercer. 1993. The Mathemat-
ics of Statistical Machine Translation: Parameter es-
timation. Computational Linguistics, 19(2):263?312,
June.
Philipp Koehn 2003. Europarl: A Mul-
tilingual Corpus for Evaluation of Ma-
chine Translation. Unpublished draft. see
http://www.iccs.inf.ed.ac.uk/?pkoehn
/publications/europarl.pdf
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Machine
Translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, Sydney, Australia.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing (ICASSP) 1995, pages
181?184, Detroit, Michigan. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Eduard
Hovy, editor, Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 127?133, Edmonton, Alberta, Canada, May.
NAACL.
Robert C. Moore. 2004. On Log-Likelihood-Ratios and
the Significance of Rare Events. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, Barcelona, Spain.
NIST. 2006. NIST MT Benchmark Test. see
http://www.nist.gov/speech/tests/mt/
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the Association for Computa-
tional Linguistics(ACL), Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of Machine Translation. Technical Report
RC22176, IBM, September.
NAACL Workshop on Statistical Machine Translation.
2006. see http://www.statmt.org/wmt06/
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP) 2002, Denver, Colorado, September.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Pro-
ceedings of Human Language Technology Conference
/ North American Chapter of the ACL, Boston, May.
974
Table 5: WMT06 Results: BLEU by type of smoothing and pruning threshold
threshold phrasetable % fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
relative frequency: no smoothing
none 100% 25.39 27.26 20.74 27.29 27.17 14.71
10 84?88% 25.97 27.81 21.08 27.82 27.71 15.09
??  63?68% 26.32 28.00 21.27 28.11 28.09 15.19
? +  14?17% 26.34 28.27 21.22 28.16 28.08 15.24
15 13?15% 26.36 28.50 21.14 28.20 28.18 15.29
20 11?13% 26.51 28.45 21.36 28.28 28.06 15.28
25 8?10% 26.50 28.38 21.28 28.32 27.97 15.25
50 4?5% 26.26 27.88 20.87 28.05 27.90 15.08
100 2% 25.66 27.07 20.07 27.38 27.11 14.66
1000 0.2% 20.49 21.66 15.23 22.51 22.31 11.36
Good-Turing
none 100% 25.96 28.14 21.17 27.84 27.95 15.13
10 84?88% 26.33 28.33 21.38 28.18 28.27 15.22
??  63?68% 26.54 28.63 21.50 28.36 28.39 15.31
? +  14?17% 26.24 28.49 21.15 28.22 28.16 15.28
15 13?15% 26.48 28.03 21.21 28.27 28.21 15.31
20 11?13% 26.65 28.45 21.41 28.36 28.14 15.25
25 8?10% 26.54 28.56 21.31 28.35 28.04 15.28
50 4?5% 26.26 27.78 20.94 28.07 27.95 15.08
100 2% 25.70 27.07 20.12 27.41 27.13 14.66
1000 0.2% 20.49 21.66 15.52 22.53 22.31 11.37
Kneser-Ney (3 parameter)
none 100% 26.89 28.70 21.78 28.64 28.71 15.50
10 84?88% 26.79 28.78 21.71 28.63 28.41 15.35
15 13?15% 26.49 28.69 21.34 28.60 28.57 15.52
20 11?13% 26.73 28.67 21.54 28.56 28.44 15.41
25 8?10% 26.84 28.70 21.29 28.54 28.21 15.42
50 4?5% 26.44 28.16 20.93 28.17 28.05 15.17
100 2% 25.72 27.27 20.11 27.50 27.26 14.58
1000 0.2% 20.48 21.70 15.28 22.58 22.36 11.33
Zens-Ney
none 100% 26.87 29.07 21.55 28.75 28.54 15.50
10 84?88% 26.81 29.00 21.65 28.72 28.52 15.54
15 13?15% 26.92 28.67 21.74 28.79 28.32 15.44
20 11?13% 26.93 28.47 21.72 28.69 28.42 15.45
25 8?10% 26.85 28.79 21.58 28.59 28.27 15.37
50 4?5% 26.51 27.96 20.96 28.30 27.96 15.27
100 2% 25.82 27.34 20.02 27.57 27.30 14.51
1000 0.2% 20.50 21.76 15.46 22.68 22.33 11.56
Table 6: Chinese Results: BLEU by pruning threshold
threshold phrasetable % nist04 nist05 nist06-GALE nist06-NIST
Zens-Ney Smoothing applied to all phrasetables
none 100% 32.14 30.69 13.06 27.97
14 40?65% 32.66 31.14 13.11 28.35
16 22?38% 32.73 30.97 13.14 28.00
18 21?36% 31.56 30.45 12.49 27.03
20 20?35% 32.00 30.73 12.50 27.33
25 18?31% 30.54 29.58 11.68 26.12
also pruning C(s?, t?) = 1
14? 23?39% 32.08 30.99 12.75 27.66
975
Unsupervised Learning of Morphology for English and Inuktitut
Howard Johnson
Institute for Information Technology,
National Research Council
Howard.Johnson@nrc.gc.ca
Joel Martin
Institute for Information Technology
National Research Council
Joel.Martin@nrc.gc.ca
Abstract
We describe a simple unsupervised technique
for learning morphology by identifying hubs
in an automaton.  For our purposes, a hub is a
node in a graph with in-degree greater than
one and out-degree greater than one.   We cre-
ate a word-trie, transform it into a minimal
DFA, then identify hubs.  Those hubs mark
the boundary between root and suffix,
achieving similar performance to more com-
plex mixtures of techniques.
1 Introduction
To recognize a morpheme boundary, for example be-
tween a root and a suffix, a learner must have seen at
least two roots with that suffix and at least two suffixes
with that root.  For instance, 'helpful', 'helpless', 'harm-
ful?, and 'harmless' would be enough evidence to guess
that those words could be divided as 'help/ful',
'help/less', 'harm/ful', and 'harm/less'.  Without seeing
varying roots and varying suffixes, there is no reason to
prefer one division to another.
We can represent a language's morphology as a
graph or automaton, with the links labeled by characters
and the nodes organizing which characters can occur
after specific prefixes.  In such an automaton, the mor-
pheme boundaries would be hubs, that is, nodes with in-
degree greater than one and out-degree greater than one.
Furthermore, this automaton could be simplified by path
compression to remove all nodes with in-degree and
out-degree of one.  The remaining automaton could be
further modified to produce a graph with one source,
one sink, and all other nodes would be hubs.
A hub-automaton, as described above, matches the
intuitive idea that a language's morphology allows one
to assemble a word by chaining morphemes together.
This representation highlights the morphemes while also
representing morphotactic information.  Phonological
information can be represented in the same graph but
may be more economically represented in a separate
transducer that can be composed with the hub-
automaton.
For identifying the boundary between roots and suf-
fixes, the idea of hubs is essentially the same as Gold-
smith?s (2001) signatures or the variations between
Gaussier?s (1999) p-similarity words.  A signature is a
set of suffixes, any of which can be added to several
roots to create a word.  For example, in English any
suffix in the set: NULL, ?s?, ?ed?, ?ing?, can be added to
?want? or ?wander? to form a word.  Here, NULL means
the empty suffix.
In a hub automaton, the idea is more general than in
previous work and applies to more complex morpholo-
gies, such as those for agglutinative or polysynthetic
languages.  In particular, we are interested in unsuper-
vised learning of Inuktitut morphology in which a single
lexical unit can often include a verb, two pronouns, ad-
verbs, and temporal information.
In this paper, we describe a very simple technique
for identifying hubs as a first step in building a hub-
automaton.  We show that, for English, this technique
does as well as more complex collections of techniques
using signatures.  We then show that the technique also
works, in a limited way, for Inuktitut.  We close with a
discussion of the limitations and our plans for more
complete learning of hub-automata.
2 Searching for hubs
The simplest way to build a graph from a raw corpus of
words is to construct a trie.  A trie is a tree representa-
tion of the distinct words with a character label on each
branch.  The trie can be transformed into a minimal,
acyclic DFA (deterministic finite automaton), sharing
nodes that have identical continuations.  There are well
known algorithms for doing this (Hopcroft & Ullman,
1969).  For example, suppose that, in a given corpus, the
prefix ?friend? occurs only with the suffixes ?NULL?,
?s?, and ?ly? and the word ?kind? occurs only with the
same suffixes.  The minimal DFA has merged the nodes
that represent those suffixes, and as a result has fewer
links and fewer nodes than the original trie.
In this DFA, some hubs will be obvious, such as for
the previous example.  These are morpheme boundaries.
There will be other nodes that are not obvious hubs.
Some may have high out-degree but an in-degree of
one; others will have high in-degree but an out-degree
of one.
Many researchers, including Schone and Jurafsky
(2000), Harris (1958), and D?jean (1998), suggest
looking for nodes with high branching (out-degree) or a
large number of continuations.  That technique is also
used as the first step in Goldsmith?s (2001) search for
signatures.  However, without further processing, such
nodes are not reliable morpheme boundaries.
Other candidate hubs are those nodes with high out-
degree that are direct descendants, along a single path,
of a node with high in-degree.  In essence, these are
stretched hubs.  Figure 1 shows an idealized view of a
hub and a stretched hub.
Figure 1: An idealized view of a hub and a
stretched hub.  The lines are links in the automaton
and each would be labeled with a character.  The
ovals are nodes and are only branching points.
In a minimized DFA of the words in a corpus, we
can identify hubs and the last node in stretched hubs as
morpheme boundaries.  These roughly correspond to the
signatures found by other methods.
The above-mentioned technique for hub searching
misses boundaries if a particular signature only appears
once in a corpus.  For instance, the signature for ?help?
might be ?ed?, ?s?, ?less?, ?lessly?, and NULL; and sup-
pose there is no other word in the corpus with the same
signature.  The morpheme boundaries ?help-less? and
?help-ed? will not be found.
The way to generalize the hub-automaton to include
words that were never seen is to merge hubs.  This is a
complex task in general.  In this paper, we propose a
very simple method.  We suggest merging each node
that is a final state (at the end of a word) with each hub
or stretched hub that has in-degree greater than two.
Doing so sharply increases the number of words ac-
cepted by the automaton.  It will identify more correct
morpheme boundaries at the expense of including some
non-words.
These two techniques, hub searching and simple
node merging, were implemented in a program called
?HubMorph? (hub-automaton morphology).
3 Related Work
Most previous work in unsupervised learning of mor-
phology has focused on learning the division between
roots and suffixes (e.g., Sproat, 1992; Gaussier, 1999;
D?jean, 1996; Goldsmith, 2001).  The hope is that the
same techniques will work for extracting prefixes.
However, even that will not handle the complex combi-
nations of infixes that are possible in agglutinative lan-
guages like Turkish or polysynthetic languages like
Inuktitut.
This paper presents a generalization of one class of
techniques that search for signatures or positions in a
trie with a large branching factor.   Goldsmith (2001)
presents a well-developed and robust version of this
class and has made his system, Linguistica, freely avail-
able (Goldsmith, 2002).
Linguistica applies a wide array of techniques in-
cluding heuristics and the application of the principle of
Minimum Description Length (MDL) to find the best
division of words into roots and suffixes, as well as pre-
fixes in some cases.  The first of these techniques finds
the points in a word with the highest number of possible
successors in other words. With all these techniques,
Linguistica seeks optimal breakpoints in each word.  In
this case, optimal means the minimal number of bits
necessary to encode the whole collection.
There are also techniques that attempt to use seman-
tic cues, arguing that knowing the signatures is not suf-
ficient for the task.  For example, Yarowsky and
Wicentowski (2000; cf. Schone & Jurafsky, 2000) pre-
sent a method for determining whether singed can be
split into sing and ed based on whether singed and sing
appear in the same contexts.  Adopting a technique like
this would increase the precision of HubMorph.  In ad-
dition, some semantic approach is absolutely essential
for identifying fusional morphology, where the word
(sang) is not a simple composition of a root (sing) and
morphemes.
4 Evaluation
As noted above, Linguistica uses many techniques to
learn morphology, including a fairly complex system for
counting bits.  We tested whether the two techniques
presented in this paper, hub searching and simple node
merging, achieve the same performance as Linguistica.
If so, the simpler techniques might be preferred. Also,
we would be justified using them for more complex
morphologies.
The input to Linguistica and HubMorph was the text
of Tom Sawyer.  The performance of both was com-
pared against a gold standard division of the distinct
words in that novel.  The gold standard was based on
dictionary entries and the judgment of two English
speakers.
In matching the gold standard words to divisions
predicted by either system, we made the following as-
sumptions. a) Words with hyphens are split at the hy-
phen to match Linguistica?s assumption. b) If the gold
standard has a break before and after a single character,
to capture non-concatenative modification, either break
matches.  An example would be ?mud-d-y?. c) An apos-
trophe at a morpheme boundary is ignored for compari-
son matching to allow it to stick to the root or to the
suffix. d) The suffix split proposed must result in a suf-
fix of 5 or fewer characters, again to match Linguis-
tica?s assumption.
Table 1 show the results of this comparison for Lin-
guistica, hub-searching alone, and HubMorph (both hub
searching and node merging).  Hub-searching alone is
sufficient to achieve the same precision as Linguistica
and nearly the same recall.  Both of the techniques to-
gether are sufficient to achieve the same precision and
recall as Linguistica.  The recall for all is low because
the list of words in Tom Sawyer is not long enough to
include most acceptable combinations of roots and suf-
fixes.  A longer input word list would improve this
score.
System Recall Precision
Linguistica 0.5753 0.9059
Hub-Searching 0.4451 0.9189
HubMorph 0.5904 0.9215
Table 1: The recall and precision of Linguistica,
Hub-searching alone, and HubMorph.  Recall is the
proportion of distinct words from Tom Sawyer that
are correctly divided into root and suffix.  Precision
is the proportion of predicted divisions that are cor-
rect.
5 Discussion
HubMorph achieves the same performance as Linguis-
tica on the words in Tom Sawyer.  It does so with a
general technique based on building a hub-automaton.
In addition to being simple, HubMorph can be general-
ized to deal with more complex morphologies.
We have applied HubMorph to Inuktitut for dividing
such words as ikajuqtaulauqsimajunga (?I was helped in
the recent past?, ikajuq-tau-lauq-sima-junga).  The path
in a hub automaton for most Inuktitut words would have
many hubs, because the words have many divisions.
Currently, there are many limitations.  The search
for hubs in the middle of words is very difficult and
requires merging nodes to induce new words.  This will
be necessary because Inuktitut theoretically has billions
of words and only a small fraction of them has occurred
in our source (the Nunavut, Canada Hansards).
Also, because each word has many morphemes, it is
difficult to correctly detect the divisions for roots and
suffixes.  In general, there are no prefixes in Inuktitut,
only infixes and suffixes.
Finally, there are many dialects of Inuktitut and
many spelling variations.  In general, the written lan-
guage is phonetic and the spelling reflects all the varia-
tions in speech.
When HubMorph performs unsupervised learning of
Inuktitut roots, it achieves a precision of 31.8% and a
recall of 8.1%.    It will be necessary to learn more of
the infixes and suffixes to improve these scores.
We believe that hub-automata will be the basis of a
general solution for IndoEuropean languages as well as
for Inuktitut.
References
D?jean, H. 1998. Morphemes as necessary concepts for
structures: Discovery from untagged corpora. Uni-
v e r s i t y  o f  Caen-Basse Normandie.
http://citeseer.nj.nec.com/19299.html
Gaussier E. (1999). Unsupervised learning of deriva-
tional morphology from inflectional lexicons. In:
Kehler A and Stolcke A, eds, ACL workshop on Un-
supervised Methods in Natural Language Learning,
College Park, MD.
Goldsmith, J.A. (2001). Unsupervised Learning of the
Morphology of a Natural Language.  Computational
Linguistics, 27:2 pp. 153-198.
Goldsmith, J.A. (2002). Linguistica software.
http://humanities.uchicago.edu/faculty/goldsmith/Lin
guistica2000/.
Harris, Z. (1951). Structural Linguistics. University of
Chicago Press.
Hopcroft, J.E. & Ullman, J.D. (1969). Formal Lan-
guages and their Relation to Automata. Addison-
Wesley, Reading, MA.
Schone, P., & Jurafsky, D. (2000). Knowledge-free in-
duction of morphology using latent semantic analy-
sis. In Proceedings of CoNLL-2000 and LLL-2000,
pp. 67--72 Lisbon, Portugal.
Sproat, R. (1992). Morphology and Computation, Cam-
bridge, MA, MIT Press.
Yarowsky, D. & Wicentowski, R. (2000). Minimally
supervised morphological analysis by multimodal
alignment. In K. Vijay-Shanker and Chang-Ning
Huang, editors, Proceedings of the 38th Meeting of
the Association for Computational Linguistics, pages
207-216, Hong Kong.
Aligning and Using an English-Inuktitut Parallel Corpus
Joel Martin, Howard Johnson, Benoit Farley and Anna Maclachlan
Institute for Information Technology
National Research Council Canada
firstname.lastname@nrc-cnrc.gc.ca
Abstract
A parallel corpus of texts in English and
in Inuktitut, an Inuit language, is presented.
These texts are from the Nunavut Hansards.
The parallel texts are processed in two phases,
the sentence alignment phase and the word cor-
respondence phase. Our sentence alignment
technique achieves a precision of 91.4% and
a recall of 92.3%. Our word correspondence
technique is aimed at providing the broadest
coverage collection of reliable pairs of Inuktitut
and English morphemes for dictionary expan-
sion. For an agglutinative language like Inuk-
titut, this entails considering substrings, not
simply whole words. We employ a Pointwise
Mutual Information method (PMI) and attain a
coverage of 72.3% of English words and a pre-
cision of 87%.
1 Introduction
We present an aligned parallel corpus of Inuktitut and
English from the Nunavut Hansards. The alignment at
the sentence level and the word correspondence follow
techniques described in the literature with augmentations
suggested by the specific properties of this language pair.
The lack of lexical resources for Inuktitut, the unrelated-
ness of the two languages, the fact that the languages use
a different script and the richness of the morphology in
Inuktitut have guided our choice of technique. Sentences
have been aligned using the length-based dynamic pro-
gramming approach of Gale and Church (1993) enhanced
with a small number of lexical and non-alphabetic an-
chors. Word correspondences have been identified with
the goal of finding an extensive high quality candidate
glossary for English and Inuktitut words. Crucially, the
algorithm considers not only full word correspondences,
as most approaches do, but also multiple substring corre-
spondences resulting in far greater coverage.
2 An English-Inuktitut Corpus
2.1 The Parallel Texts
The corpus of parallel texts we present consists of
3,432,212 words of English and 1,586,423 words of Inuk-
titut from the Nunavut Hansards. These Hansards are
available to the public in electronic form in both English
and Inuktitut (www.assembly.nu.ca). The Legislative As-
sembly of the newly created territory of Nunavut began
sitting on April 1, 1999. Our corpus represents 155 days
of transcribed proceedings of the Nunavut Legislative As-
sembly from that first session through to November 1,
2002, which was part way through the sixth session of
the assembly.
We gather and process these 155 documents in vari-
ous ways described in the rest of this paper and make
available a sentence-aligned version of the parallel texts
(www.InuktitutComputing.ca/NunavutHansards). Like
the French-English Canadian Hansards of parliamentary
proceedings, this corpus represents a valuable resource
for Machine Translation research and corpus research as
well as for the development of language processing tools
for Inuktitut. The work reported here takes some first
steps toward these ends, and it is hoped that others will
find ways to expand on this work. One reason that the
Canadian Hansards, a large parallel corpus of English-
French, are particularly useful for research is that they
are comparatively noise free as parallel text collections
go (Simard and Plamondon, 1996). This should be true
of the Nunavut Hansard collection as well. The Canadian
Hansard is transcribed in both languages so what was said
in English is transcribed in English and then translated
into French and vice versa. For the Nunavut Hansard, in
contrast, a complete English version of the proceedings
is prepared and then this is translated into Inuktitut, even
when the original proceedings were spoken in Inuktitut.
2.2 The Inuktitut Language
Inuktitut is the language of the Inuit living in North East-
ern Canada, that is, Nunavut (Keewatin and Baffin Is-
land), Nunavik and Labrador. It includes six closely
related spoken dialects: Kivalliq, Aivilik, North Baffin,
South Baffin, Arctic Quebec (Nunavik), and Labrador.
Inuktitut is a highly agglutinative language. Noun and
verb roots occur with two main types of suffixes and there
are many instantiations of these suffixes. The seman-
tic suffixes modify the meaning of the root (over 250 of
these in North Baffin dialect) and the grammatical suf-
fixes indicate features like agreement and mood (approx-
imately 700 verbal endings and over 300 nominal endings
in North Baffin dialect).
A single word in Inuktitut is often translated with
multiple English words, sometimes corresponding to a
full English clause. For example, the Inuktitut word
r???'???rk?'n
n
c
?k???rk?rk (which is transliterated as
qaisaaliniaqquunngikkaluaqpuq) corresponds to these
eight English words: ?Actually he will probably not come
early today?. The verbal root is qai ?come?, the semantic
suffixes are -saali-, -niaq-, -qquu-, -nngit- and -galuaq-
meaning ?soon?, ?a little later today or tomorrow?, ?proba-
bility?, ?negation?, and ?actuality? respectively, and finally
the grammatical suffix -puq expresses the 3rd person sin-
gular of the indicative mood. This frequently occurring
one-to-many correspondence represents a challenge for
word correspondence. The opposite challenging situa-
tion, namely instances of many-to-one correspondences,
also arises for this language pair but less frequently. The
latter is therefore not addressed in this paper.
Yet another challenge is the morphophonological com-
plexity of Inuktitut as reflected in the orthography, which
has two components. First, the sheer number of pos-
sible suffixes mentioned above is problematic. Second,
the shape of these suffixes is variable. That is, there are
significant orthographic changes to the individual mor-
phemes when they occur in the context of a word. This
type of variability can be seen in the above example at the
interface of -nngit- and -galuaq-, which together become
-nngikkaluaq-.
Finally, it is important to note that Inuktitut has a syl-
labic script for which there is a standard Romanization.
To give an idea of how the scripts compare, our corpus
of parallel texts consists of 20,124,587 characters of En-
glish and 13,457,581 characters in Inuktitut syllabics as
compared to 21,305,295 characters of Inuktitut in Roman
script.
3 Sentence Alignment
3.1 Sentence Alignment Approach
The algorithm used to align English-Inuktitut sentences is
an extension of that presented in Gale and Church (1993).
It does not identify crossing alignments where the sen-
tence order within paragraphs in the parallel texts differs.
Sentence alignments typically involve one English sen-
tence matching one Inuktitut sentence (a 1-to-1 bead),
but may also involve 2-to-1, 1-to-2, 0-to-1, 1-to-0 and
2-to-2 sentence matching patterns, or beads. Using such
a length-based approach where the length of sentences
is measured in characters is appropriate for our language
pair since the basic assumption generally holds. Namely,
longer English sentences typically correspond to longer
Inuktitut sentences as measured in characters.
One problem with the approach, as pointed out by
Macklovitch and Hannan (1998), is that from the point
where a paragraph is misaligned, it is difficult to ensure
proper alignment for the remainder of the paragraph. We
observed this effect in our alignment. We also observed
that the large number of small paragraphs with almost
identical length caused problems for the algorithm.
Many alignment approaches have addressed such prob-
lems by making use of additional linguistic clues specific
to the languages to be aligned. For our language pair,
it was not feasible to use most of these. For example,
some alignment techniques make good use of cognates
(Simard and Plamondon, 1996). The assumption is that
words in the two languages that share the first few let-
ters are usually translations of each other. English and
Inuktitut, however, are too distantly related to have many
cognates. Even the translation of a proper name does not
usually result in a cognate for our language pair, since the
translation between scripts induces a phonetic translation
rather than a character-preserving translation of the name,
as these pairs illustrate Peter, Piita; Canada, Kanata;
McLean, Makalain.
Following a suggestion in Gale and Church (1993),
the alignment was aided by the use of additional an-
chors that were available for the language pair. These
anchors consisted of non-alphabetic sequences (such as
9:00, 42-1(1) and 1999) and 8 reliable word cor-
respondences that occurred frequently in the corpus, in-
cluding words beginning with these character sequences
speaker/uqaqti and motion/pigiqati, for ex-
ample.
3.2 Steps in Sentence Alignment
Preprocessing: Preprocessing the Inuktitut and the En-
glish raised separate issues. For English, the main is-
sue was ensuring that illegal or unusual characters are
mapped to other characters to simplify later processing.
For Inuktitut the main issue was the array of encodings
used for the syllabic script. Inuktitut syllabics can be rep-
resented using a 7-bit encoding called ProSyl, which is
in many cases extended to an 8-bit encoding Tunngavik.
Each syllabic character can be encoded in multiple ways
that need to be mapped into a uniform scheme, such as
Unicode. Each separate file was converted to HTML us-
ing a commercial product LogicTran r2net. Then, the
Perl package HTML::TreeBuilder was used to purge the
text of anomalies and set up the correct mappings. The
output of this initial preprocessing step was a collection
of HTML files in pure Unicode UTF8.
Boundary Identification: The next step was to iden-
tify the paragraph and sentence boundaries for the Inuk-
titut and English texts. Sentences were split at periods,
question marks, colons and semi-colons except where the
following character was a lower case letter or a number.
This resulted in a number of errors but was quite accurate
in general. Paragraph boundaries were inserted where
such logical breaks occurred as signaled in the HTML
and generally correspond to natural breaks in the orig-
inal document. Using HTML indicators contributed to
the number of very short paragraphs, especially toward
the beginning of each document. As mentioned in sec-
tion 3.1, these short paragraphs were problematic for the
alignment algorithm. The collection consists of 348,619
sentences in 112,346 paragraphs in English and 352,486
sentences in 118,733 paragraphs in Inuktitut. After this
step, document, paragraph and sentence boundaries were
available to use as hard and soft boundaries for the Gale
and Church algorithm.
Syllabic Script Conversion: The word correspon-
dence phase required a Roman script representation of
the Inuktitut texts. The conversion from unicode syllab-
ics to Roman characters was performed at this stage in the
sentence alignment process using the standard ICI con-
version method.
Anchors: The occurrences of the lexical anchors men-
tioned above were found and used with a dynamic pro-
gramming search to find the path with the largest number
of alignments. This algorithm was written in Perl and re-
quired about two hours to process the whole corpus. All
alignments that occurred in the first two sentences of each
paragraph were marked as hard boundaries for the Gale
and Church (1993) program as provided in their paper.
3.3 Sentence Alignment Evaluation
Three representative days of Hansard (1999/04/01,
2001/02/21 and 2002/10/29) were selected and manually
aligned at the sentence level as a gold standard. Precision
and recall were then measured as suggested in Isabelle
and Simard (1996).
Results: The number of sentence alignments in the
gold standard was 3424. The number automatically
aligned by our method was 3459. The number of
those automatic alignments that were correct as measured
against the gold standard was 3161. This represents a pre-
cision of 91.4% and a recall rate of 92.3%. For compari-
son, the Gale and Church (1993) program, which did not
make use of additional anchors, had poorer results over
our corpus. Their one-pass approach, which ignores para-
graph boundaries, had a precision of 66.7% and a recall
of 71.5%. Their two-pass approach, which aligns para-
graphs in one pass and then aligns sentences in a second
pass, had a precision of 85.6% and a recall of 87.0%.
4 Word Correspondence
Having built a sentence-aligned parallel corpus, we next
attempted to use that corpus. Our goal was to extract
as many reliable word associations as possible to aid in
developing a morphological analyzer and in expanding
Inuktitut dictionaries. The output of this glossary discov-
ery phase is a list of suggested pairings that a human can
consider for inclusion in a dictionary. Inuktitut dictio-
naries often disagree because of spelling and dialectical
differences. As well, many contemporary words are not
in the existing dictionaries. The parallel corpus presented
here can be used to augment the dictionaries with current
words, thereby providing an important tool for students,
translators, and others.
In our approach, a glossary is populated with pairs of
words that are consistent translations of each other. For
many language pairs, considering whole word to whole
word correspondences for inclusion in a glossary would
yield good results. However, because Inuktitut is aggluti-
native, the method must discover pairs of an English word
and the corresponding root of the Inuktitut word, or the
corresponding Inuktitut suffix, or sometimes the whole
Inuktitut word. In other words, it is essential to consider
substrings of words for good coverage for a language pair
like ours.
4.1 Substring Correspondence Method
Searching for substring correspondences is reduced to a
counting exercise. For any pair of substrings, you need to
know how many parallel regions contained the pair, how
many regions in one language contained the first, how
many regions in the other language contained the second,
and how many regions there are in total. For example,
the English word ?today? and the Inuktitut word ?ullumi?
occur in 2092 parallel regions. The word ?today? appears
in a total of 3065 English regions; and ?ullumi? appears
in 2702 Inuktitut regions. All together, there are 332,154
aligned regions. It is fairly certain that these two words
should be a glossary pair because each usually occurs as
a translation of the other.
The PMI Measure: We measure the degree of asso-
ciation between any two substrings, one in the English
and one in the Inuktitut, using Pointwise Mutual Infor-
mation (PMI). PMI measures the amount of information
that each substring conveys about the occurrence of the
other. We recognize that PMI is badly behaved when the
counts are near 1. To protect against that problem, we
compute the 99.99999% confidence intervals around the
PMI (Lin, 1999), and use the lower bound as a measure
of association. This lower bound rises as the PMI rises
or as the amount of data increases. Many measures of
association would likely work as well as the lower confi-
dence bound on PMI. We used that bound as a metric in
this study for three reasons. First, that metric led to bet-
ter performance than Chi-squared on this data. Second, it
addressed the problem of low frequency events. Third, it
makes the correct judgment on Gale and Church?s well-
known chambre-communes problem (Gale and Church,
1991).
The decision to include pairs of substrings in the glos-
sary proceeds as follows. Include the highest PMI scoring
pairs if neither member of the pair has yet been included.
If two pairs are tied, check whether the Inuktitut members
of the pairs are in a substring relation. If they are, then
add the pair with the longer substring to the glossary; if
not, then add neither pair.
Many previous efforts have used a similar methodol-
ogy but were only able to focus on word to word cor-
respondences (Gale and Church, 1991). Here, the En-
glish words can correspond to any substring in any Inuk-
titut word in the aligned region. This means that statis-
tics have to be maintained for many possible pairs. Un-
der our approach, we maintain all these statistics for all
English words, all Inuktitut words as well as substrings
with length of between one and 10 Roman characters, and
all co-occurrences that have frequency greater than three.
This approach thereby addresses the challenge of Inuk-
titut roots and multiple semantic suffixes corresponding
to individual English words. It also addresses the chal-
lenge of orthographic variation at morpheme boundaries
to some degree since it will truncate morphemes appro-
priately in many cases.
4.2 Glossary Evaluation
This method suggested 4362 word-substring pairs for in-
clusion in a glossary. This represents a 72.3% coverage of
English word occurrences in the corpus (omitting words
of fewer than 3 characters). One hundred of these word-
substring pairs were chosen at random and judged for ac-
curacy using two existing dictionaries and a partial suffix
list. An Inuktitut substring was said to match an English
word exactly if the Inuktitut root plus all the suffixes car-
ried the same meaning as the English word and conveyed
the same grammatical features (e.g., grammatical number
and case). The correspondence was said to be good if the
Inuktitut root plus the left-most lexical suffixes conveyed
the same meaning as the English word. In those cases, the
Inuktitut word conveyed additional semantic or grammat-
ical information.
About half of the exact matches were uninflected
proper nouns. A typical example of the other exact
matches is the pair inuup and person?s. In this pair, inu-
means person and -up is the singular genitive case. A typ-
ical example of a good match is the pair pigiaqtitara and
deal. In this pair, pigiaqti- means deal and -tara conveys
first person singular subject and third person singular ob-
ject. For example, ?I deal with him?.
Of the 100 pairs, 43 were deemed exact matches and
44 were deemed good matches. The remaining 13 were
incorrect. Taken together 87% of the pairs in the sample
were useful to include in a glossary. This level of perfor-
mance will improve as we introduce morphological anal-
ysis to both the Inuktitut and English words.
5 Conclusion
We have shown that aligning an English text with a highly
agglutinative language text can have very useful out-
comes. The alignment of the corpus to the sentence level
was achieved accurately enough to build a usable parallel
corpus. This is demonstrated by the fact that we could
create a glossary tool on the basis of this corpus that
suggested glossary pairings for 72.3% of English words
in the text with a precision of 87%. We hope that our
work will generate further interest in this newly available
English-Inuktitut parallel corpus.
Acknowledgements We would like to thank Gavin
Nesbitt of the Legislative Assembly of Nunavut for pro-
viding the Hansards, Peter Turney for useful sugges-
tions, and Canadian Heritage for financial support of this
project.
References
William A. Gale and Kenneth Ward Church. 1991. Iden-
tifying word correspondance in parallel text. In Pro-
ceedings of the DARPA NLP Workshop.
William A. Gale and Kenneth Ward Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?103.
Pierre Isabelle and Michel Simard. 1996. Propo-
sitions pour la repre?sentation et l?e?valuation des
alignements de textes paralle`les. [http://www.lpl.univ-
aix.fr/projects/arcade/2nd/sent/metrics.html]. In Rap-
port technique, CITI.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the ACL.
Elliot Macklovitch and Marie-Louise Hannan. 1998.
Line ?em up: Advances in alignment technology and
their impact on translation support tools. Machine
Translation, 13(1).
Michel Simard and Pierre Plamondon. 1996. Bilingual
sentence alignment: Balancing robustness and accu-
racy. In Proceedings of the Conference of the Associa-
tion for Machine Translation in the Americas (AMTA).
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 65?74,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Word Alignment for Languages with Scarce Resources
Joel Martin
National Research Council
Ottawa, ON, K1A 0R6
Joel.Martin@cnrc-nrc.gc.ca
Rada Mihalcea
University of North Texas
Denton, TX 76203
rada@cs.unt.edu
Ted Pedersen
University of Minnesota
Duluth, MN 55812
tpederse@umn.edu
Abstract
This paper presents the task definition,
resources, participating systems, and
comparative results for the shared task
on word alignment, which was organized
as part of the ACL 2005 Workshop on
Building and Using Parallel Texts. The
shared task included English?Inuktitut,
Romanian?English, and English?Hindi
sub-tasks, and drew the participation of ten
teams from around the world with a total of
50 systems.
1 Defining a Word Alignment Shared Task
The task of word alignment consists of finding cor-
respondences between words and phrases in parallel
texts. Assuming a sentence aligned bilingual corpus
in languages L1 and L2, the task of a word alignment
system is to indicate which word token in the corpus
of language L1 corresponds to which word token in
the corpus of language L2.
This year?s shared task follows on the success of
the previous word alignment evaluation that was or-
ganized during the HLT/NAACL 2003 workshop on
?Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond? (Mihalcea and Ped-
ersen, 2003). However, the current edition is dis-
tinct in that it has a focus on languages with scarce
resources. Participating teams were provided with
training and test data for three language pairs, ac-
counting for different levels of data scarceness: (1)
English?Inuktitut (2 million words training data),
(2) Romanian?English (1 million words), and (3)
English?Hindi (60,000 words).
Similar to the previous word alignment evaluation
and with the Machine Translation evaluation exercises
organized by NIST, two different subtasks were de-
fined: (1) Limited resources, where systems were al-
lowed to use only the resources provided. (2) Un-
limited resources, where systems were allowed to use
any resources in addition to those provided. Such re-
sources had to be explicitly mentioned in the system
description.
Test data were released one week prior to the dead-
line for result submissions. Participating teams were
asked to produce word alignments, following a com-
mon format as specified below, and submit their out-
put by a certain deadline. Results were returned to
each team within three days of submission.
1.1 Word Alignment Output Format
The word alignment result files had to include one line
for each word-to-word alignment. Additionally, they
had to follow the format specified in Figure 1. Note
that the
  
and confidence fields overlap in their
meaning. The intent of having both fields available
was to enable participating teams to draw their own
line on what they considered to be a Sure or Probable
alignment. Both these fields were optional, with some
standard values assigned by default.
1.1.1 A Running Word Alignment Example
Consider the following two aligned sentences:
[English]  s snum=18  They had gone .  /s 
[French]  s snum=18  Ils e?taient alle?s .  /s 
A correct word alignment for this sentence is:
18 1 1
18 2 2
18 3 3
18 4 4
65
sentence no position L1 position L2 [    ] [confidence]
where:
sentence no represents the id of the sentence within the
test file. Sentences in the test data already have an id as-
signed. (see the examples below)
position L1 represents the position of the token that is
aligned from the text in language L1; the first token in each
sentence is token 1. (not 0)
position L2 represents the position of the token that is
aligned from the text in language L2; again, the first token
is token 1.
S P can be either S or P, representing a Sure or Probable
alignment. All alignments that are tagged as S are also con-
sidered to be part of the P alignments set (that is, all align-
ments that are considered ?Sure? alignments are also part of
the ?Probable? alignments set). If the    field is missing, a
value of S will be assumed by default.
confidence is a real number, in the range (0-1] (1 meaning
highly confident, 0 meaning not confident); this field is op-
tional, and by default confidence number of 1 was assumed.
Figure 1: Word Alignment file format
stating that: all the word alignments pertain to sen-
tence 18, the English token 1 They aligns with the
French token 1 Ils, the English token 2 had aligns with
the French token 2 e?taient, and so on. Note that punc-
tuation is also aligned (English token 4 aligned with
French token 4), and counts toward the final evalua-
tion figures.
Alternatively, systems could also provide an
  
marker and/or a confidence score, as shown in the fol-
lowing example:
18 1 1 1
18 2 2 P 0.7
18 3 3 S
18 4 4 S 1
with missing
   
fields considered by default S, and
missing confidence scores considered by default 1.
1.2 Annotation Guide for Word Alignments
The word alignment annotation guidelines are similar
to those used in the 2003 evaluation.
1. All items separated by a white space are consid-
ered to be a word (or token), and therefore have
to be aligned (punctuation included).
2. Omissions in translation use the NULL token,
i.e. token with id 0.
3. Phrasal correspondences produce multiple word-
to-word alignments.
2 Resources
The shared task included three different language
pairs, accounting for different language and data
characteristics. Specifically, the three subtasks ad-
dressed the alignment of words in English?Inuktitut,
Romanian?English, and English?Hindi parallel texts.
For each language pair, training data were provided to
participants. Systems relying only on these resources
were considered part of the Limited Resources sub-
task. Systems making use of any additional resources
(e.g. bilingual dictionaries, additional parallel cor-
pora, and others) were classified under the Unlimited
Resources category.
2.1 Training Data
Three sets of training data were made available. All
data sets were sentence-aligned, and pre-processed
(i.e. tokenized and lower-cased), with identical pre-
processing procedures used for training, trial, and test
data.
English?Inuktitut. A collection of sentence-
aligned English?Inuktitut parallel texts from the
Legislative Assembly of Nunavut (Martin et al,
2003). This collection consists of approximately
2 million Inuktitut tokens (1.6 million words) and
4 million English tokens (3.4 million words). The
Inuktitut data was originally encoded in Unicode
representing a syllabics orthography (qaniujaaqpait),
but was transliterated to an ASCII encoding of the
standardized roman orthography (qaliujaaqpait) for
this evaluation.
Romanian?English. A set of Romanian?English
parallel texts, consisting of about 1 million Romanian
words, and about the same number of English words.
This is the same training data set as used in the 2003
word alignment evaluation (Mihalcea and Pedersen,
2003). The data consists of:
 Parallel texts collected from the Web using a
semi-supervised approach. The URLs format
for pages containing potential parallel transla-
tions were manually identified (mainly from the
archives of Romanian newspapers). Next, texts
were automatically downloaded and sentence
aligned. A manual verification of the alignment
was also performed. These data collection pro-
cess resulted in a corpus of about 850,000 Roma-
nian words, and about 900,000 English words.
66
 Orwell?s 1984, aligned within the MULTEXT-
EAST project (Erjavec et al, 1997), with about
130,000 Romanian words, and a similar number
of English words.
 The Romanian Constitution, for about 13,000
Romanian words and 13,000 English words.
English?Hindi. A collection of sentence aligned
English?Hindi parallel texts, from the Emille project
(Baker et al, 2004), consisting of approximately En-
glish 60,000 words and about 70,000 Hindi words.
The Hindi data was encoded in Unicode Devangari
script, and used the UTF?8 encoding. The English?
Hindi data were provided by Niraj Aswani and Robert
Gaizauskas from University of Sheffield (Aswani and
Gaizauskas, 2005b).
2.2 Trial Data
Three sets of trial data were made available at the
same time training data became available. Trial sets
consisted of sentence aligned texts, provided together
with manually determined word alignments. The
main purpose of these data was to enable participants
to better understand the format required for the word
alignment result files. For some systems, the trial data
has also played the role of a validation data set used
for system parameter tuning. Trial sets consisted of
25 English?Inuktitut and English?Hindi aligned sen-
tences, and a larger set of 248 Romanian?English
aligned sentences (the same as the test data used in
the 2003 word alignment evaluation).
2.3 Test Data
A total of 75 English?Inuktitut, 90 English?Hindi,
and 200 Romanian?English aligned sentences were
released one week prior to the deadline. Participants
were required to run their word alignment systems on
one or more of these data sets, and submit word align-
ments. Teams were allowed to submit an unlimited
number of results sets for each language pair.
2.3.1 Gold Standard Word Aligned Data
The gold standard for the three language pair align-
ments were produced using slightly different align-
ment procedures.
For English?Inuktitut, annotators were instructed to
align Inuktitut words or phrases with English phrases.
Their goal was to identify the smallest phrases that
permit one-to-one alignments between English and
Inuktitut. These phrase alignments were converted
into word-to-word alignments in the following man-
ner. If the aligned English and Inuktitut phrases
each consisted of a single word, that word pair was
assigned a Sure alignment. Otherwise, all possi-
ble word-pairs for the aligned English and Inuktitut
phrases were assigned a Probable alignment. Dis-
agreements between the two annotators were decided
by discussion.
For Romanian?English and English?Hindi, anno-
tators were instructed to assign an alignment to all
words, with specific instructions as to when to as-
sign a NULL alignment. Annotators were not asked
to assign a Sure or Probable label. Instead, we had an
arbitration phase, where a third annotator judged the
cases where the first two annotators disagreed. Since
an inter-annotator agreement was reached for all word
alignments, the final resulting alignments were con-
sidered to be Sure alignments.
3 Evaluation Measures
Evaluations were performed with respect to four dif-
ferent measures. Three of them ? precision, recall,
and F-measure ? represent traditional measures in In-
formation Retrieval, and were also frequently used
in previous word alignment literature. The fourth
measure was originally introduced by (Och and Ney,
2000), and proposes the notion of quality of word
alignment.
Given an alignment   , and a gold standard align-
ment  , each such alignment set eventually consist-
ing of two sets   ,   , and  ,  corresponding
to Sure and Probable alignments, the following mea-
sures are defined (where  is the alignment type, and
can be set to either S or P).
	

 


 

 (1)




 




 (2)



 
	





 (3)



ffProceedings of the ACL Workshop on Building and Using Parallel Texts, pages 129?132,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
PORTAGE: A Phrase-based Machine Translation System 
 
Fatiha Sadat+, Howard Johnson++, Akakpo Agbago+, George Foster+,               
Roland Kuhn+, Joel Martin++ and Aaron Tikuisis?
 
+ NRC Institute for Information 
Technology 
101 St-Jean-Bosco Street  
Gatineau, QC K1A 0R6, Canada 
++ NRC Institute for Information 
Technology 
1200 Montreal Road  
Ottawa, ON K1A 0R6, Canada 
?University of Waterloo 
200 University Avenue W., 
Waterloo, Ontario, Canada 
 
firstname.lastname@cnrc-nrc.gc.ca aptikuis@uwaterloo.ca  
 
Abstract 
This paper describes the participation of 
the Portage team at NRC Canada in the 
shared task1 of ACL 2005 Workshop on 
Building and Using Parallel Texts. We dis-
cuss Portage, a statistical phrase-based 
machine translation system, and present 
experimental results on the four language 
pairs of the shared task. First, we focus on 
the French-English task using multiple re-
sources and techniques. Then we describe 
our contribution on the Finnish-English, 
Spanish-English and German-English lan-
guage pairs using the provided data for the 
shared task.  
1 Introduction 
The rapid growth of the Internet has led to a rapid 
growth in the need for information exchange among 
different languages. Machine Translation (MT) and 
related technologies have become essential to the 
information flow between speakers of different lan-
guages on the Internet. Statistical Machine Transla-
tion (SMT), a data-driven approach to producing 
translation systems, is becoming a practical solution 
to the longstanding goal of cheap natural language 
processing.  
In this paper, we describe Portage, a statistical 
phrase-based machine translation system, which we 
evaluated on all different language pairs that were 
provided for the shared task.  As Portage is a very 
                                                           
1 http://www.statmt.org/wpt05/mt-shared-task/ 
new system, our main goal in participating in the 
workshop was to test it out on different language 
pairs, and to establish baseline performance for the 
purpose of comparison against other systems and 
against future improvements.  To do this, we used a 
fairly standard configuration for phrase-based SMT, 
described in the next section. 
Of the language pairs in the shared task, French-
English is particularly interesting to us in light of 
Canada?s demographics and policy of official bilin-
gualism. We therefore divided our participation into 
two parts: one stream for French-English and an-
other for Finnish-, German-, and Spanish-English. 
For the French-English stream, we tested the use of 
additional data resources along with hand-coded 
rules for translating numbers and dates. For the 
other streams, we used only the provided resources 
in a purely statistical framework (although we also 
investigated several automatic methods of coping 
with Finnish morphology). 
The remainder of the paper is organized as fol-
lows. Section 2 describes the architecture of the 
Portage system, including its hand-coded rules for 
French-English.  Experimental results for the four 
pairs of languages are reported in Section 3. Section 
4 concludes and gives pointers to future work. 
2 Portage  
Portage operates in three main phases: preprocess-
ing of raw data into tokens, with translation sugges-
tions for some words or phrases generated by rules; 
decoding to produce one or more translation hy-
potheses; and error-driven rescoring to choose the 
best final hypothesis. (A fourth postprocessing 
phase was not needed for the shared task.) 
129
2.1 Preprocessing 
Preprocessing is a necessary first step in order to 
convert raw texts in both source and target lan-
guages into a format suitable for both model train-
ing and decoding (Foster et al, 2003).  For the 
supplied Europarl corpora, we relied on the existing 
segmentation and tokenization, except for French, 
which we manipulated slightly to bring into line 
with our existing conventions (e.g., converting l ? 
an  into l? an).  For the Hansard corpus used to 
supplement our French-English resources (de-
scribed in section 3 below), we used our own 
alignment based on Moore?s algorithm (Moore, 
2002), segmentation, and tokenization procedures. 
Languages with rich morphology are often prob-
lematic for statistical machine translation because 
the available data lacks instances of all possible 
forms of a word to efficiently train a translation sys-
tem. In a language like German, new words can be 
formed by compounding (writing two or more 
words together without a space or a hyphen in be-
tween). Segmentation is a crucial step in preproc-
essing languages such as German and Finnish texts.
In addition to these simple operations, we also 
developed a rule-based component to detect num-
bers and dates in the source text and identify their 
translation in the target text. This component was 
developed on the Hansard corpus, and applied to the 
French-English texts (i.e. Europarl and Hansard), on 
the development data in both languages, and on the 
test data. 
2.2 Decoding 
Decoding is the central phase in SMT, involving a 
search for the hypotheses t that have highest prob-
abilities of being translations of the current source 
sentence s according to a model for P(t|s). Our 
model for P(t|s) is a log-linear combination of four 
main components: one or more trigram language 
models, one or more phrase translation models, a 
distortion model, and a word-length feature. The 
trigram language model is implemented in the 
SRILM toolkit (Stolcke, 2002). The phrase-based 
translation model is similar to the one described in 
(Koehn, 2004), and relies on symmetrized IBM 
model 2 word-alignments for phrase pair induction. 
The distortion model is also very similar to 
Koehn?s, with the exception of a final cost to ac-
count for sentence endings.  
s
To set weights on the components of the log-
linear model, we implemented Och?s algorithm 
(Och, 2003).  This essentially involves generating, 
in an iterative process, a set of nbest translation hy-
potheses that are representative of the entire search 
space for a given set of source sentences. Once this 
is accomplished, a variant of Powell?s algorithm is 
used to find weights that optimize BLEU score 
(Papineni et al 2002) over these hypotheses, com-
pared to reference translations. Unfortunately, our 
implementation of this algorithm converged only 
very slowly to a satisfactory final nbest list, so we 
used two different ad hoc strategies for setting 
weights: choosing the best values encountered dur-
ing
, with the exception of a 
ch as the ability to decode either 
w ards.  
 transla-
 
rent language pairs of the 
sha d t
hared t
- 
 the iterations of Och?s algorithm (French-
English), and a grid search (all other languages).  
To perform the actual translation, we used our 
decoder, Canoe, which implements a dynamic-
programming beam search algorithm based on that 
of Pharaoh (Koehn, 2004). Canoe is input-output 
compatible with Pharaoh
few extensions su
back ards or forw
2.3 Rescoring 
To improve raw output from Canoe, we used a 
rescoring strategy: have Canoe generate a list of 
nbest translations rather than just one, then reorder 
the list using a model trained with Och?s method to 
optimize BLEU score. This is identical to the final 
pass of the algorithm described in the previous sec-
tion, except for the use of a more powerful log-
linear model than would have been feasible to use 
inside the decoder. In addition to the four basic fea-
tures of the initial model, our rescoring model in-
cluded IBM2 model probabilities in both directions 
(i.e., P(s|t) and P(t|s)); and an IBM1-based feature 
designed to detect whether any words in one lan-
guage seemed to be left without satisfactory
tions in the other language. This missing-word
feature was also applied in both directions. 
3 Experiments on the Shared Task 
We conducted experiments and evaluations on 
Portage using the diffe
re ask. The training data was provided for the 
ask as follows:  
Training data of 688,031 sentences in 
French and English. A similarly sized cor-
130
pus is provided for Finnish, Spanish and 
German with matched English translations. 
orpus was used to generate both 
lan
e translations into English, was 
 
 Portage for a comparative study ex-
ploiting and combining different resources and 
tec
 
3. arl corpus 
4. 
rd corpora as training data and 
 
t  mod est  
p icipation at th h-English tas 9.53. 
od D  Decoding+Rescoring
- Development test data of 2,000 sentences in 
the four languages.  
In addition to the provided data, a set of 
6,056,014 sentences extracted from Hansard corpus, 
the official record of Canada?s parliamentary de-
bates, was used in both French and English lan-
guages. This c
guage and translation models for use in decoding 
and rescoring. 
The development test data was split into two 
parts: The first part that includes 1,000 sentences in 
each language with reference translations into Eng-
lish served in the optimization of weights for both 
the decoding and rescoring models. In this study, 
number of n-best lists was set to 1,000. The second 
part, which includes 1,000 sentences in each lan-
guage with referenc
used in the evaluation of the performance of the
translation models. 
3.1 Experiments on the French-English Task 
Our goal for this language pair was to conduct ex-
periments on
hniques:  
1. Method E is based on the Europarl corpus 
as training data, 
2. Method E-H is based on both Europarl and 
Hansard corpora as training data, 
Method E-p is based on the Europ
as training data and parsing numbers and 
dates in the preprocessing phase, 
Method E-H-p is based on both Europarl 
and Hansa
parsing numbers and date in the preprocess-
ing phase. 
Results are shown in Table 1 for the French-
English task. The first column of Table 1 indicates 
the method, the second column gives results for 
decoding with Canoe only, and the third column for 
decoding and rescoring with Canoe. For comparison 
between the four methods, there was an improve-
ment in terms of BLEU scores when using two lan-
guage models and two translation models generated 
from Europarl and Hansard corpora; however, pars-
ing numbers and dates had a negative impact on the
ranslation els. The b  BLEU score for our
art e Frenc k was 2
Meth ecoding
E 27.71 29.22 
E-H 28.71 29.53 
E-p 26.45 28.21 
E-H-p 28.29 28.56 
Ta
ed 
f 
of increased trade within North 
merica but also functions as a good counterpoint 
for French-English. 
 
ble 1. BLEU scores for the French-English test 
sentences  
 
A noteworthy feature of these results is that the 
improvement given by the out-of-domain Hansard 
corpus was very slight. Although we suspect that 
somewhat better performance could have been 
achieved by better weight optimization, this result 
clearly underscores the importance of matching 
training and test domains. A related point is that our 
number and date translation rules actually caused a 
performance drop due to the fact that they were op-
timized for typographical conventions prevalent in 
Hansard, which are quite different from those used 
in Europarl. 
Our best result ranked third in the shared 
WPT05 French-English task , with a difference of 
0.74 in terms of BLEU score from the first rank
participant, and a difference of 0.67 in terms o
BLEU score from the second ranked participant. 
3.2 Experiments on other Pairs of Languages 
The WPT05 workshop provides a good opportunity 
to achieve our benchmarking goals with corpora 
that provide challenging difficulties. German and 
Finnish are languages that make considerable use of 
compounding. Finnish, in addition, has a particu-
larly complex morphology that is organized on 
principles that are quite different from any in Eng-
lish. This results in much longer word forms each of 
which occurs very infrequently. 
Our original intent was to propose a number of pos-
sible statistical approaches to analyzing and split-
ting these word forms and improving our results. 
Since none of these yielded results as good as the 
baseline, we will continue this work until we under-
stand what is really needed. We also care very 
much about translating between French and English 
in Canada and plan to spend a lot of extra effort on 
difficulties that occur in this case. Translation be-
tween Spanish and English is also becoming more 
mportant as a result i
A
131
Language Pair Decoding+Rescoring
Finnish-English 20.95 
German-English 23.21 
Spanish English 29.08 
Ta
and 1.56 in 
m ores, respectively, compared to 
the first ranked participant.   
l 
ation, greater use of morphological 
R
Fr
Meeting of the Association for Computational 
Fr
Statistical Machine Transla-
Ge
id 
Ke
e 
Ki
al Meeting of the Association for Com-
M
ne Trans-
Oc
 of the 40th Annual Meet-
Fr
 Proceedings of 
Ph parl: A multilingual corpusfor 
 P
ation 
Models. In Proceedings of the Association for Ma-
chine Translation in the Americas AMTA 2004. 
ble 2 BLEU scores for the Finnish-English, Ger-
man-English and Spanish-English test sentences  
 
To establish our baseline, the only preprocessing 
we did was lowercasing (using the provided tokeni-
zation). Canoe was run without any special settings, 
although weights for distortion, word penalty, lan-
guage model, and translation model were optimized 
using a grid search, as described above. Rescoring 
was also done, and usually resulted in at least an 
extra BLEU point.  
Our final results are shown in Table 2. Ranks at 
the shared WPT05 Finnish-, German-, and Spanish-
English tasks were assigned as second, third and 
fourth, with differences of 1.06, 1.87 
ter s of BLEU sc
4 Conclusion 
We have reported on our participation in the shared 
task of the ACL 2005 Workshop on Building and 
Using Parallel Texts, conducting evaluations of 
Portage, our statistical machine translation system, 
on all four language pairs. Our best BLEU scores 
for the French-, Finnish-, German-, and Spanish-
English at this stage were 29.5, 20.95, 23.21 and 
29.08, respectively. In total, eleven teams took part 
at the shared task and most of them submitted re-
sults for all pairs of languages.  Our results distin-
guished the NRC team at the third, second, third 
and fourth ranks with slight differences with the 
first ranked participants. 
A major goal of this work was to evaluate Port-
age at its first stage of implementation on different 
pairs of languages. This evaluation has served to 
identify some problems with our system in the areas 
of weight optimization and number and date rules. 
It has also indicated the limits of using out-of-
domain corpora, and the difficulty of morphologi-
cally complex languages like Finnish. 
Current and planned future work includes the 
exploitation of comparable corpora for statistica
machine transl
knowledge, and better features for nbest rescoring. 
eferences 
Andreas Stolcke. 2002. SRILM - an Extensible Language 
Modeling Toolkit. In ICSLP-2002, 901-904. 
anz Josef Och, Hermann Ney. 2000. Improved Statisti-
cal Alignment Models. In Proceedings of the 38th An-
nual 
Linguistics, Hong Kong, China, October 2000, 440-
447. 
anz Josef Och, Daniel Gildea, Sanjeev Khudanpur, 
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar 
Kumar, Libin Shen, David Smith, Katherine Eng, 
Viren Jain, Zhen Jin, Dragomir Radev. 2004. A Smor-
gasbord of Features for 
tion. In Proceeding of the HLT/NAACL 2004, 
Boston, MA, May 2004. 
orge Foster, Simona Gandrabur, Philippe Langlais, 
Pierre Plamondon, Graham Russell and Michel Si-
mard. 2003. Statistical Machine Translation: Rap
Development with Limited Resources. In Proceedings 
of MT Summit IX 2003, New Orleans, September.  
vin Knight, Ishwar Chander, Matthew Haines, Va-
sileios Hatzivassiloglou, Eduard Hovy, Masayo Iida, 
Steve K. Luk, Richard Whitney, and Kenji Yamada. 
1995. Filling Knowledge Gaps in a Broad-Coverag
MT System. In Proceedings of the International Joint 
Conference on Artificial Intelligence (IJCAI), 1995. 
shore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings of 
the 40th Annu
putational Linguistics ACL, Philadelphia, July 2002, 
pp. 311-318. 
oore, Robert. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Machine Transla-
tion: From Research to Real Users (Proceedings of the 
5th Conference of the Association for Machi
lation in the Americas, Tiburon, California), Springer-
Verlag, Heidelberg, Germany, pp. 135-244. 
h, F. J. and H. Ney. 2002. Discriminative Training 
and Maximum Entropy Models for Statistical Machine 
Translation. In Proceedings
ing of the Association for Computational Linguistics, 
Philadelphia, pp. 295?302. 
anz Josef Och, 2003. Minimum Error Rate Training 
for Statistical Machine Translation. In
the 41st Annual Meeting of the Association for Com-
putational Linguistics, Sapporo, July. 
ilipp Koehn. 2002. Euro
evaluation of machine translation. Ms., University of 
Southern California. 
hilipp Koehn. 2004. Pharaoh: a Beam Search Decoder 
for Phrase-based Statistical Machine Transl
132
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 32?41,
Baltimore, Maryland, USA. June 27, 2014.
c
?2014 Association for Computational Linguistics
Semantic Role Labeling of Emotions in Tweets
Saif M. Mohammad, Xiaodan Zhu, and Joel Martin
National Research Council Canada
Ottawa, Ontario, Canada K1A 0R6
{saif.mohammad,xiaodan.zhu,joel.martin}@nrc-cnrc.gc.ca
Abstract
Past work on emotion processing has fo-
cused solely on detecting emotions, and
ignored questions such as ?who is feeling
the emotion (the experiencer)?? and ?to-
wards whom is the emotion directed (the
stimulus)??. We automatically compile a
large dataset of tweets pertaining to the
2012 US presidential elections, and anno-
tate it not only for emotion but also for
the experiencer and the stimulus. We then
develop a classifier for detecting emotion
that obtains an accuracy of 56.84 on an
eight-way classification task. Finally, we
show how the stimulus identification task
can also be framed as a classification task,
obtaining an F-score of 58.30.
1 Introduction
Detecting emotions in text has a number of ap-
plications including tracking sentiment towards
politicians, movies, and products (Pang and Lee,
2008), identifying what emotion a newspaper
headline is trying to evoke (Bellegarda, 2010),
developing more natural text-to-speech systems
(Francisco and Gerv?as, 2006), detecting how peo-
ple use emotion-bearing-words and metaphors to
persuade and coerce others (for example, in pro-
paganda) (K?ovecses, 2003), tracking response to
natural disasters (Mandel et al., 2012), and so
on. With the rapid proliferation of microblogging,
there is growing amount of emotion analysis re-
search on newly available datasets of Twitter posts
(Mandel et al., 2012; Purver and Battersby, 2012;
Mohammad, 2012b). However, past work has fo-
cused solely on detecting emotional state. It has
ignored questions such as ?who is feeling the emo-
tion (the experiencer)?? and ?towards whom is the
emotion directed (the stimulus)??.
In this paper, we present a system that analyzes
tweets to determine who is feeling what emotion,
and towards whom. We use tweets from the 2012
US presidential elections as our dataset, since we
expect political tweets to be particularly rich in
emotions. Further, the dataset will be useful for
applications such as determining political align-
ment of tweeters (Golbeck and Hansen, 2011;
Conover et al., 2011b), identifying contentious
issues (Maynard and Funk, 2011), detecting the
amount of polarization in the electorate (Conover
et al., 2011a), and so on.
Detecting the who, what, and towards whom
of emotions is essentially a semantic role-labeling
problem (Gildea and Jurafsky, 2002). The seman-
tic frame for ?emotions? in FrameNet (Baker et al.,
1998) is shown in Table 1. In this work, we fo-
cus on the roles of Experiencer, State, and Stim-
ulus. Note, however, that the state or emotion is
often not explicitly present in text. Other roles
such as Reason, Degree, and Event are also of sig-
nificance, and remain suitable avenues for future
work.
We automatically compile a large dataset of
2012 US presidential elections using a small num-
ber of hand-chosen hashtags. Next we annotate
the tweets for Experiencer, State, and Stimulus
by crowdsourcing to Amazon?s Mechanical Turk.
1
We analyze the annotations to determine the dis-
tributions of different types of roles, and show that
the dataset is rich in emotions. We develop a clas-
sifier for emotion detection that obtains an accu-
racy of 56.84. We find that most of the tweets
express emotions of the tweeter, and only a few
are indicative of the emotions of someone else.
Finally, we show how the stimulus identification
task can be framed as a classification task that cir-
cumvents more complicated problems of detecting
entity mentions and coreferences. Our supervised
classifier obtains an F-score of 58.30 on this task.
1
https://www.mturk.com/mturk/welcome
32
Table 1: The FrameNet frame for emotions. The three roles investigated in this paper are shown in bold.
Role Description
Core:
Event The Event is the occasion or happening that Experiencers in a certain emotional state participate in.
Experiencer The Experiencer is the person or sentient entity that experiences or feels the emotions.
Expressor The body part, gesture, or other expression of the Experiencer that reflects his or her emotional state.
State The State is the abstract noun that describes a more lasting experience by the Experiencer.
Stimulus The Stimulus is the person, event, or state of affairs that evokes the emotional response in the Experiencer.
Topic The Topic is the general area in which the emotion occurs. It indicates a range of possible Stimulus.
Non-Core:
Circumstances The Circumstances is the condition(s) under which the Stimulus evokes its response.
Degree The extent to which the Experiencer?s emotion deviates from the norm for the emotion.
Empathy target The Empathy target is the individual or individuals with which the Experiencer identifies emotionally.
Manner Any way the Experiencer experiences the Stimulus which is not covered by more specific frame elements.
Parameter The Parameter is a domain in which the Experiencer experiences the Stimulus.
Reason The Reason is the explanation for why the Stimulus evokes a certain emotional response.
2 Related Work
Our work here is related to emotion analysis, se-
mantic role labeling (SRL), and information ex-
traction (IE).
Much of the past work on emotion detection
focuses on emotions argued to be the most ba-
sic. For example, Ekman (1992) proposed six ba-
sic emotions?joy, sadness, anger, fear, disgust,
and surprise. Plutchik (1980) argued in favor
of eight?Ekman?s six, surprise, and anticipation.
Many of the automatic systems use affect lexi-
cons pertaining to these basic emotions such as
the NRC Emotion Lexicon (Mohammad and Tur-
ney, 2010), WordNet Affect (Strapparava and Val-
itutti, 2004), and the Affective Norms for English
Words.
2
Affect lexicons are lists of words and as-
sociated emotions.
Emotion analysis techniques have been applied
to many different kinds of text (Mihalcea and Liu,
2006; Genereux and Evans, 2006; Neviarouskaya
et al., 2009; Mohammad, 2012a). More recently
there has been work on tweets as well (Bollen
et al., 2011; Tumasjan et al., 2010; Mohammad,
2012b). Bollen et al. (2011) measured tension,
depression, anger, vigor, fatigue, and confusion
in tweets. Tumasjan et al. (2010) study Twitter
as a forum for political deliberation. Mohammad
(2012b) developed a classifier to identify emotions
using tweets with emotion word hashtags as la-
beled data. However, none of this work explores
the many semantic roles of emotion.
Semantic role labeling (SRL) identifies seman-
tic arguments and roles with regard to a predicate
2
http://www.purl.org/net/NRCEmotionLexicon
http://csea.phhp.ufl.edu/media/anewmessage.html
in a sentence (Gildea and Jurafsky, 2002; M`arquez
et al., 2008; Palmer et al., 2010). More recently,
there has also been some work on semantic role
labeling of tweets for verb and nominal predi-
cates (Liu et al., 2012; Liu et al., 2011). There
exists work on extracting opinions and the top-
ics of opinions, however most of it if focused on
opinions about product features (Popescu and Et-
zioni, 2005; Zhang et al., 2010; Kessler and Ni-
colov, 2009). For example, (Kessler and Nicolov,
2009) identifies semantic relations between sen-
timent expressions and their targets for car and
digital-camera reviews. However, there is no work
on semantic role labeling of emotions in tweets.
We use many of the ideas developed in the senti-
ment analysis work and apply them to detect the
stimulus of emotions in the electoral tweets data.
Our work here is also related to template filling
in information extraction (IE), for example as de-
fined in MUC (Grishman, 1997), which extracts
information (entities) from a document to fill out
a pre-defined template, such as the date, location,
target, and other information about an event.
3 Challenges of Semantic Role Labeling
of Emotions in Tweets
Semantic role labeling of emotions in tweets poses
certain unique challenges. Firstly, there are many
differences between tweets and linguistically well-
formed texts, such as written news (Liu et al.,
2012; Ritter et al., 2011). Tweets are often less
well-formed?they tend to be colloquial, have
misspellings, and have non-standard tokens. Thus,
methods depending heavily on deep language un-
derstanding such as syntactic parsing (Kim and
Hovy, 2006) are less reliable.
33
Secondly, in a traditional SRL system, an ar-
gument frame is a cohesive structure with strong
dependencies between the arguments. Thus it is
often beneficial to develop joint models to identify
the various elements of a frame (Toutanova et al.,
2005). However, these assumptions are less viable
when dealing with emotions in tweets. For exam-
ple, there is no reason to believe that people with a
certain name will have the same emotions towards
the same entities. On the other hand, if we make
use of information beyond the target tweet to inde-
pendently identify the political leanings of a per-
son, then that information can help determine the
person?s emotions towards certain entities. How-
ever, that is beyond the scope of this paper. Thus
we develop independent classifiers for identifying
experiencer, state, and stimulus.
Often, the goal in SRL and IE template filling
is the labeling of text spans in the original text.
However, emotions are often not explicitly stated
in text. Thus we develop a system that assigns an
emotion to a tweet even though that emotion is not
explicitly mentioned. The stimulus of the emo-
tion may also not be mentioned. Consider Happy
to see #4moreyears come into reality. The stimu-
lus of the emotion joy is to see #4moreyears come
into reality. However, the tweet essentially con-
veys the tweeter?s joy towards Barack Obama be-
ing re-elected as president. One may argue that
the true stimulus here is Barack Obama. Thus it is
useful to normalize mentions and resolve the co-
reference, for example, all mentions of Barack H.
Obama, Barack, Obama, and #4moreyears should
be directed to the same entity. Thus, we ground
(in the same sense as in language grounding) the
emotional arguments to the predefined entities.
Through our experiments we show the target of an
emotion in political tweets is often one among a
handful of entities. Thus we develop a classifier to
identify which of these pre-chosen entities is the
stimulus in a given tweet.
4 Data Collection and Annotation
4.1 Identifying Electoral Tweets
We created a corpus of tweets by polling the Twit-
ter Search API, during August and September
2012, for tweets that contained commonly known
hashtags pertaining to the 2012 US presidential
elections. Table 2 shows the query terms we
used. Apart from 21 hashtags, we also collected
tweets with the words Obama, Barack, or Rom-
Table 2: Query terms used to collect tweets per-
taining to the 2012 US presidential elections.
#4moreyears #Barack #campaign2012
#dems2012 #democrats #election
#election2012 #gop2012 #gop
#joebiden2012 #mitt2012 #Obama
#ObamaBiden2012 #PaulRyan2012 #president
#president2012 #Romney #republicans
#RomneyRyan2012 #veep2012 #VP2012
Barack Obama Romney
ney. We used these additional terms because they
are names of the two presidential candidates, and
the probability that these words were used to refer
to somebody else in tweets posted in August and
September of 2012 was low.
The Twitter Search API was polled every four
hours to obtain new tweets that matched the query.
Close to one million tweets were collected, which
we will make freely available to the research com-
munity. The query terms which produced the high-
est number of tweets were those involving the
names of the presidential candidates, as well as
#election2012, #campaign, #gop, and #president.
We used the metadata tag ?iso language code?
to identify English tweets. Since this tag is not al-
ways accurate, we also discarded tweets that did
not have at least two valid English words. We
used the Roget Thesaurus as the English word in-
ventory.
3
This step also helps discard very short
tweets and tweets with a large proportion of mis-
spelled words. Since we were interested in deter-
mining the source and target of emotions in tweets,
we decided to focus on original tweets as opposed
to retweets. We discarded retweets, which can eas-
ily be identified through the presence of RT, rt, or
Rt in the tweet (usually in the beginning of the
post). Finally, there remained close to 170,000
original English tweets.
4.2 Annotating Emotions by Crowdsourcing
We used Amazon?s Mechanical Turk service to
crowdsource the annotation of the electoral tweets.
We randomly selected about 2,000 tweets, each by
a different Twitter user. We set up two question-
naires on Mechanical Turk for the tweets. The first
questionnaire was used to determine the number
of emotions in a tweet and also whether the tweet
was truly relevant to the US politics.
3
www.gutenberg.org/ebooks/10681
34
Questionnaire 1: Emotions in the US election tweets
Tweet: Mitt Romney is arrogant as hell.
Q1. Which of the following best describes the emotions in
this tweet?
? This tweet expresses or suggests an emotional attitude
or response to something.
? This tweet expresses or suggests two or more contrast-
ing emotional attitudes or responses.
? This tweet has no emotional content.
? There is some emotion here, but the tweet does not give
enough context to determine which emotion it is.
? It is not possible to decide which of the above options
is appropriate.
Q2. Is this tweet about US politics and elections?
? Yes, this tweet is about US politics and elections.
? No, this tweet has nothing to do with US politics or
anybody involved in it.
These questionnaires are called HITs (Human In-
telligence Tasks) in Mechanical Turk parlance. We
posted 2042 HITs corresponding to 2042 tweets.
We requested responses from at least three anno-
tators for each HIT. The response to a HIT by an
annotator is called an assignment. In Mechanical
Turk, an annotator may provide assignments for as
many HITs as they wish. Thus, even though only
three annotations are requested per HIT, dozens
of annotators contribute assignments for the 2,042
tweets.
The tweets that were marked as having one
emotion were chosen for annotation by the Ques-
tionnaire 2. We requested responses from at least
five annotators for each of these HITs. Below is
an example:
Questionnaire 2:
Who is feeling what, and towards whom?
Tweet: Mitt Romney is arrogant as hell.
Q1. Who is feeling or who felt an emotion?
Q2. What emotion? Choose one of the options from below
that best represents the emotion.
? anger or annoyance or hostility or fury
? anticipation or expectancy or interest
? disgust or dislike
? fear or apprehension or panic or terror
? joy or happiness or elation
? sadness or gloominess or grief or sorrow
? surprise
? trust or like
Table 3: Questionnaire 1: Percentage of tweets
in each category of Q1. Only those tweets that
were annotated by at least two annotators were in-
cluded. A tweet belongs to category X if it is an-
notated with X more often than all other categories
combined. There were 1889 such tweets in total.
Percentage
of tweets
suggests an emotional attitude 87.98
suggests two contrasting attitudes 2.22
no emotional content 8.21
some emotion; not enough context 1.32
unknown; not enough context 0.26
all 100.0
Q3. Towards whom or what?
After performing a small pilot annotation
effort, we realized that the stimulus in most of
the electoral tweets was one among a handful
of entities. Thus we reformulated question 3 as
shown below:
Q3. What best describes the target of the emotion?
? Barack Obama and/or Joe Biden
? Mitt Romney and/or Paul Ryan
? Some other individual
? Democratic party, democrats, or DNC
? Republican party, republicans, or RNC
? Some other institution
? Election campaign, election process, or elections
? The target is not specified in the tweet
? None of the above
4.3 Annotation Analyses
For each annotator and for each question, we cal-
culated the probability with which the annotator
agreed with the response chosen by the majority
of the annotators. We identified poor annotators as
those that had an agreement probability more than
two standard deviations away from the mean. All
annotations by these annotators were discarded.
We determine whether a tweet is to be assigned
a particular category based on strong majority
vote. That is, a tweet belongs to category X if
it was annotated by at least three annotators and
only if at least half of the annotators agreed with
each other. Percentage of tweets in each of the five
categories of Q1 are shown in Table 3. Observe
that the majority category for Q1 is ?suggests an
emotion??87.98% of the tweets were identified
as having an emotional attitude.
35
Table 4: Questionnaire 2: Percentage of tweets
in the categories of Q2. Only those tweets that
were annotated by at least three annotators were
included. A tweet belongs to category X if it is
annotated with X more often than all other cate-
gories combined. There were 965 such tweets.
Percentage
Emotion of tweets
anger 7.41
anticipation 5.01
disgust 47.75
fear 1.98
joy 6.58
sadness 0.83
surprise 6.37
trust 24.03
all 100.00
Responses to Q2 showed that a large majority
(95.56%) of the tweets were relevant to US pol-
itics and elections. This shows that the hashtags
shown earlier in Table 2 were effective in identify-
ing political tweets.
As mentioned earlier, only those tweets that
were marked as having an emotion (with high
agreement) were annotated further through Ques-
tionnaire 2.
Responses to Q1 of Questionnaire 2 revealed
that in the vast majority of the cases (99.825%),
the tweets contains emotions of the tweeter. The
data did include some tweets that referred to emo-
tions of others such as Romney, GOP, and pres-
ident, but these instances are rare. Tables 4 and
5 give the distributions of the various options for
Questions 2, and 3 of Questionnaire 2. Table 4
shows that disgust (49.32%) is by far the most
dominant emotion in the tweets of 2012 US pres-
idential elections. The next most prominent emo-
tion is that of trust (23.73%). About 61% of the
tweets convey negative emotions towards some-
one or something. Table 5 shows that the stimulus
of emotions was often one of the two presidential
candidates (close to 55% of the time)?Obama:
29.90%, Romney: 24.87%.
4.3.1 Inter-Annotator Agreement
We calculated agreement statistics on the full set
of annotations, and not just on the annotations with
a strong majority as described in the previous sec-
tion. Table 6 shows inter-annotator agreement
(IAA) for the questions?the average percentage of
times two annotators agree with each other. An-
other way to gauge agreement is by calculating
the average probability with which an annotator
Table 5: Questionnaire 2: Percentage of tweets in
the categories of Q3. A tweet belongs to category
X if it is annotated with X more often than all other
categories combined. There were 973 such tweets.
Percentage
Whom of tweets
Barack Obama and/or Joe Biden 29.90
Mitt Romney and/or Paul Ryan 24.87
Some other individual 5.03
Democratic party, democrats, or DNC 2.46
Republican party, republicans, or RNC 8.42
Some other institution 1.23
Election campaign or process 4.93
The target is not specified in the tweet 1.95
None of the above 21.17
all 100.00
Table 6: Agreement statistics: inter-annotator
agreement (IAA) and average probability of
choosing the majority class (APMS).
IAA APMS
Questionnaire 1:
Q1 78.02 0.845
Q2 96.76 0.974
Questionnaire 2:
Q1 52.95 0.731
Q2 59.59 0.736
Q3 44.47 0.641
picks the majority class. The last column in Ta-
ble 6 shows the average probability of picking the
majority class (APMS) by the annotators (higher
numbers indicate higher agreement). Observe that
there is high agreement on determining whether a
tweet has an emotion or not, and on determining
whether the tweet is related to the 2012 US pres-
idential elections or not. The questions in Ques-
tionnaire 2 pertaining to the experiencer, state, and
stimulus were less straightforward and tend to re-
quire more context than just the target tweet for
a clear determination, but yet the annotations had
moderate agreement.
4.4 Access to the data
All of the data is made freely available through the
first author?s website:
http://www.purl.org/net/PoliticalTweets2012
It includes: (1) the complete set of tweets collected
from the Twitter API with hashtags shown in Ta-
ble 2, (2) the subset of English tweets, (3) Ques-
tionnaires 1 and 2, (4) and tweets annotated as per
Questionnaires 1 and 2.
36
5 Automatically Detecting Semantic
Roles of Emotions in Tweets
Since in most instances (99.83%) the experiencer
of emotions in a tweet is the tweeter, we focus
on automatically detecting the other two semantic
roles: the emotional state and the stimulus.
Due to the unique challenges of semantic role
labeling of emotions in tweets described earlier
in the paper, we treat the detection of emotional
state and stimulus as two subtasks for which
we train state-of-the-art support vector machine
(SVM) classifiers. SVM is a learning algorithm
proved to be effective on many classification tasks
and robust on large feature spaces. In our ex-
periments, we exploited several different classi-
fiers and found SVM outperforms others such as
maximum-entropy models (i.e., logistic regres-
sion). We also tested the most popular kernels
such as the polynomial and RBF kernels with dif-
ferent parameters in stratified ten-fold cross val-
idation. We found that a simple linear kernel
yielded the best performance. We used the Lib-
SVM package (Chang and Lin, 2011).
As mentioned earlier, there is fair amount of
work on emotion detection in non-tweet texts
(Boucouvalas, 2002; Holzman and Pottenger,
2003; Ma et al., 2005; John et al., 2006; Mihalcea
and Liu, 2006; Genereux and Evans, 2006; Aman
and Szpakowicz, 2007; Tokuhisa et al., 2008;
Neviarouskaya et al., 2009) as well as on tweets
(Kim et al., 2009; Tumasjan et al., 2010; Bollen et
al., 2011; Mohammad, 2012b; Choudhury et al.,
2012; Wang et al., 2012). In the experiments be-
low we draw from various successfully used fea-
tures described in these papers. More specifically,
the system we use builds on the classifier and fea-
tures used in two previous systems: (1) the sys-
tem described in (Mohammad, 2012b) which was
shown to perform significantly better than some
other previous systems on the news paper head-
lines corpus and the system described in (Moham-
mad et al., 2013) which ranked first (among 44
participating teams) in a 2013 SemEval competi-
tion on detecting sentiment in tweets).
The goal of the experiments in this section is
to apply a state-of-the art emotion detection sys-
tem on the electoral tweets data. We want to
set up baseline performance for emotion detec-
tion on this new dataset and also validate the data
by showing that automatic classifiers can obtain
results that are greater than random and major-
ity baselines. In Section 5.2, we apply the SVM
classifier and various features for the first time on
the task of detecting the stimulus of an emotion in
tweets. In each experiment, we report results of
ten-fold stratified cross-validation.
5.1 Detecting emotional state
5.1.1 Features
We included the following features for detecting
emotional state in tweets.
Word n-grams: We included unigrams (single
words) and bigrams (two-word sequences) into
our feature set. All words were stemmed with
Porter?s stemmer (Porter, 1980).
Punctuations: number of contiguous sequences of
exclamation marks, question marks, or a combina-
tion of them.
Elongated words: the number of words with the
final character repeated 3 or more times (soooo,
mannnnnn, etc). (Elongated words have been used
similarly in (Brody and Diakopoulos, 2011).)
Emoticons: presence/absence of positive and neg-
ative emoticons. The emoticon and its polar-
ity were determined through a regular expres-
sion adopted from Christopher Potts? tokenizing
script.
4
Emotion Lexicons: We used the NRC word?
emotion association lexicon (Mohammad and Tur-
ney, 2010) to check if a tweet contains emo-
tional words. The lexicon contains human anno-
tations of emotion associations for about 14,200
word types. The annotation includes whether
a word is positive or negative (sentiments), and
whether it is associated with the eight basic emo-
tions (joy, sadness, anger, fear, surprise, antici-
pation, trust, and disgust). If a tweet has three
words that have associations with emotion joy,
then the LexEmo emo joy feature takes a value
of 3. We also counted the number of words
with regard to the Osgood?s (Osgood et al., 1957)
semantic differential categories (LexOsg) built
for Wordnet (LexOsg wn) and General Inquirer
(LexOsg gi). To reduce noise, we only consid-
ered the words that have an adjective or adverb
sense in Wordnet.
Negation features: We examined tweets to deter-
mine whether they contained negators such as no,
not, and shouldn?t. An additional feature deter-
mined whether the negator was located close to an
4
http://sentiment.christopherpotts.net/tokenizing.html
37
Table 7: Results for emotion detection.
Accuracy
random baseline 30.26
majority baseline 47.75
automatic SVM system 56.84
upper bound 69.80
Table 8: The accuracies obtained with one of the
feature groups removed. The number in brackets
is the difference with the all features score. The
biggest drop is shown in bold.
Difference from
Experiment Accuracy all features
all features 56.84 0
all - ngrams 53.35 -3.49
all - word ngrams 54.44 -2.40
all - char. ngrams 56.32 -0.52
all - lexicons 54.34 -2.50
all - manual lex. 55.17 -1.67
all - auto lex. 55.38 -1.46
all - negation 55.80 -1.04
all - encodings (elongated words, emoticons, punctns.,
uppercase) 56.82 -0.02
emotion word (as determined by the emotion lex-
icon) in the tweet and in the dependency parse of
the tweet. The list of negation words was adopted
from Christopher Potts? sentiment tutorial.
5
Position features: We included a set of position
features to capture whether the feature terms de-
scribed above appeared at the beginning or the end
of the tweet. For example, if one of the first five
terms in a tweet is a joy word, then the feature
LexEmo joy begin was triggered.
Combined features Though non-linear models
like SVM (with non-linear kernels) can cap-
ture interactions between features, we explic-
itly combined some of our features. For ex-
ample, we concatenated all emotion categories
found in a given tweet. If the tweet contained
both surprise and disgust words, a binary feature
?LexEmo surprise disgust? was triggered. Also,
if a tweet contained more than one joy word
and no other emotion words, then the feature
LexEmo joy only was triggered.
5.1.2 Results
Table 7 shows the results. We included two base-
lines here: the random baseline corresponds to a
system that randomly guesses the emotion of a
tweet, whereas the majority baseline assigns all
5
http://sentiment.christopherpotts.net/lingstruc.html
tweets to the majority category (disgust). Since
the data is significantly skewed towards disgust,
the majority baseline is relative high.
The automatic system obtained by the classi-
fier in identifying the emotions (56.84), which is
significantly higher than the majority baseline. It
should be noted that the highest scores in the Se-
mEval 2013 task of detecting sentiment analysis of
tweets was around 69% (Mohammad et al., 2013).
That task even though related involved only three
classes (positive, negative, and neutral). Thus it is
not surprising that for an 8-way classification task,
the performance is somewhat lower.
The upper bound of the task here is not 100%?
human annotators do not always agree with each
other. To estimate the upper bound we can expect
an automatic system to achieve, for each tweet we
randomly sampled an human annotation from its
multiple annotations and treated it as a system out-
put. We compare it with the majority category
chosen from the remaining human annotations for
that tweet. Such sampling is conducted over all
tweets and then evaluated. The results table shows
this upper bound.
Table 8 shows results of ablation experiments?
the accuracies obtained with one of the feature
groups removed. The higher the drop in per-
formance, the more useful is that feature. Ob-
serve that the ngrams are the most useful fea-
tures, followed by the emotion lexicons. Most of
the gain from ngrams come through word ngrams,
but character ngrams provide small gains as well.
Both the manual and automatic sentiment lexi-
cons were found to be useful to a similar degree.
Paying attention to negation was also beneficial,
whereas emotional encodings such as elongated
words, emoticons, and punctuations did not help
much. It is possible that much of the discrimi-
nating information they might have is already pro-
vided by unigram and character ngram features.
5.2 Detecting emotion stimulus
As discussed earlier, instead of detecting and la-
beling the original text spans, we ground the emo-
tion stimulus directly to the predefined entities.
This allows us to circumvent mention detection
and co-reference resolution on linguistically less
well-formed text. We treat the problem as a classi-
fication task, in which we classify a tweet into one
of the categories defined in Table 5. We believe
that a similar approach is also possible in other
38
Table 9: Results for detecting stimulus.
P R F
random baseline 16.45 20.87 18.39
majority baseline 34.45 38.00 36.14
automatic rule-based system 43.47 55.15 48.62
automatic SVM system 57.30 59.32 58.30
upper bound 82.87 81.36 82.11
domains such as natural disaster tweets and epi-
demic surveillance tweets. We perform a ten-fold
stratified cross-validation.
5.2.1 Features
We used the features below for detecting emotion
stimulus:
Word ngrams: Same as described earlier for
emotional state.
Lexical features: We collected lexicons that
contain a variety of words and phrases describing
the categories in Table 5. For example, the Re-
publican party may be called as ?gop? or ?Grand
Old Party?; all such words or phrases are all put
into the lexicon called ?republican?. We counted
how many words in a given tweet are from each of
these lexicons.
Hashtag features: Hashtags related to the U.S.
election were collected. We organized them into
different categories and use them to further smooth
the sparseness. For example, ?#4moreyear? and
?#obama? are put into the same hashtag lexicon
and any occurrence of such hashtags in a tweet
triggers the feature ?hashtag obama generalized?,
indicating that this is a general version of hashtag
related to president Barack Obama.
Position features: Same as described earlier for
emotional state.
Combined features As discussed earlier, we ex-
plicitly combined some of the above features. For
example, we first concatenate all lexicon and hash-
tag categories found in a given tweet?if the tweet
contains both the general hashtag of ?obama?
and ?romney?, a binary feature ?Hashtag general
obama romney? takes the value of 1.
5.2.2 Results
Table 9 shows the results obtained by the system.
Overall, the system obtains an F-measure of 58.30.
The table also shows upper-bound and baselines
calculated just as described earlier for the emo-
tional state category. We added results for an
additional baseline, rule-based system, here that
chose the stimulus to be: Obama if the tweet had
the terms obama or #obama; Romney if the tweet
had the terms romney or #romney; Republicans if
the tweet had the terms republican, republicans,
or #republicans; Democrats if the tweet had the
terms democrats, democrat, or #democrats; and
Campaign if the tweet had the terms #election or
#campaign. If two or more of the above rules are
triggered in the same tweet, then a label is chosen
at random. This rule-based system based on hand-
chosen features obtains an F-score of 48.62, show-
ing that there are sufficiently many tweets where
key words alone are not sufficient to disambiguate
the true stimulus. Observe that the SVM-based au-
tomatic system performs markedly better than the
majority baseline and also the rule-based system
baseline.
6 Conclusions and Future Work
In this paper, we framed emotion detection as a se-
mantic role labeling problem, focusing not just on
emotional state but also on experiencer and stimu-
lus. We chose tweets about the 2012 US presiden-
tial elections as our target domain. We automati-
cally compiled a large dataset of these tweets using
hashtags, and annotated them first for presence of
emotions, and then for the different semantic roles
of emotions. All of the data is made freely avail-
able.
We found that a large majority of these tweets
(88.1%) carry some emotional attitude towards
someone or something. Further, tweets that con-
vey disgust are twice as prevalent than those that
convey trust. We found that most tweets express
emotions of the tweeter themselves, and the stim-
ulus is often one among a few handful of entities.
We developed a classifier for emotion detection
that obtained an accuracy of 56.84 on an eight-
way classification task. Finally, we showed how
the stimulus identification task can be framed as
a classification task in which our system outper-
forms competitive baselines.
Our future work involves exploring the use of
more tweets from the same user to determine their
political leanings, and use that as an additional fea-
ture in emotion detection. We are also interested in
automatically identifying other semantic roles of
emotions such as degree, reason, and empathy tar-
get (described in Table 1). We believe that a more
sophisticated sentiment analysis applications and
a better understanding of affect require the deter-
mination of semantic roles of emotion.
39
References
Saima Aman and Stan Szpakowicz. 2007. Identifying
expressions of emotion in text. In Vclav Matou?sek
and Pavel Mautner, editors, Text, Speech and Dia-
logue, volume 4629 of Lecture Notes in Computer
Science, pages 196?205. Springer Berlin / Heidel-
berg.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA.
Association for Computational Linguistics.
Jerome Bellegarda. 2010. Emotion analysis using la-
tent affective folding and embedding. In Proceed-
ings of the NAACL-HLT 2010 Workshop on Compu-
tational Approaches to Analysis and Generation of
Emotion in Text, Los Angeles, California.
Johan Bollen, Alberto Pepe, and Huina Mao. 2011.
Modeling public mood and emotion: Twitter senti-
ment and socio-economic phenomena. In The Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM), Barcelona, Spain.
Anthony C. Boucouvalas. 2002. Real time text-
to-emotion engine for expressive internet commu-
nication. Emerging Communication: Studies on
New Technologies and Practices in Communication,
5:305?318.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 562?570, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
Munmun De Choudhury, Scott Counts, and Michael
Gamon. 2012. Not all moods are created equal! ex-
ploring human emotional states in social media. In
The International AAAI Conference on Weblogs and
Social Media (ICWSM).
M D Conover, J Ratkiewicz, M Francisco, B Gonc,
A Flammini, and F Menczer. 2011a. Political po-
larization on Twitter. Networks, 133(26):89?96.
Michael D Conover, Bruno Goncalves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011b. Predicting the political alignment
of Twitter users. In IEEE Third International
Conference on Privacy Security Risk and Trust and
IEEE Third International Conference on Social
Computing, pages 192?199. IEEE.
Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion, 6(3):169?200.
Virginia Francisco and Pablo Gerv?as. 2006. Auto-
mated mark up of affective information in english
texts. In Petr Sojka, Ivan Kopecek, and Karel Pala,
editors, Text, Speech and Dialogue, volume 4188 of
Lecture Notes in Computer Science, pages 375?382.
Springer Berlin / Heidelberg.
Michel Genereux and Roger Evans. 2006. Distin-
guishing affective states in weblogs. In AAAI-2006
Spring Symposium on Computational Approaches to
Analysing Weblogs, pages 27?29, Stanford, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jennifer Golbeck and Derek Hansen. 2011. Com-
puting political preference among twitter followers.
In Proceedings of the SIGCHI Conference on Hu-
man Factors in Computing Systems, CHI ?11, pages
1105?1108, New York, NY. ACM.
Ralph Grishman. 1997. Information extraction: Tech-
niques and challenges. In SCIE, pages 10?27.
Lars E. Holzman and William M. Pottenger. 2003.
Classification of emotions in internet chat: An appli-
cation of machine learning using speech phonemes.
Technical report, Leigh University.
David John, Anthony C. Boucouvalas, and Zhe Xu.
2006. Representing emotional momentum within
expressive internet communication. In Proceed-
ings of the 24th IASTED international conference on
Internet and multimedia systems and applications,
pages 183?188, Anaheim, CA. ACTA Press.
Jason S. Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking of
linguistic configurations. In 3rd Int?l AAAI Confer-
ence on Weblogs and Social Media (ICWSM 2009).
S. Kim and E. Hovy. 2006. Extracting opinions, opin-
ion holders, and topics expressed in online news me-
dia text. In Proceedings of the Workshop on Senti-
ment and Subjectivity in Text, pages 1?8.
Elsa Kim, Sam Gilbert, Michael J. Edwards, and Er-
hardt Graeff. 2009. Detecting sadness in 140
characters: Sentiment analysis of mourning Michael
Jackson on twitter.
Zolt?an K?ovecses. 2003. Metaphor and Emotion: Lan-
guage, Culture, and Body in Human Feeling (Stud-
ies in Emotion and Social Interaction). Cambridge
University Press.
X. Liu, K. Li, M. Zhou, and Z. Xiong. 2011. En-
hancing semantic role labeling for tweets using self-
training. In AAAI.
X. Liu, Z. Fu, F. Wei, and M. Zhou. 2012. Collective
nominal semantic role labeling for tweets. In AAAI.
Chunling Ma, Helmut Prendinger, and Mitsuru
Ishizuka. 2005. Emotion estimation and reasoning
based on affective textual interaction. In J. Tao and
R. W. Picard, editors, First International Conference
on Affective Computing and Intelligent Interaction
(ACII-2005), pages 622?628, Beijing, China.
40
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during Hurricane Irene. In Proceedings of the Sec-
ond Workshop on Language in Social Media, LSM
?12, pages 27?36, Stroudsburg, PA. Association for
Computational Linguistics.
Llu??s M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: an introduction to the special
issue. Computational Linguistics, 34(2):145?159.
Diana Maynard and Adam Funk. 2011. Automatic
detection of political opinions in tweets. gateacuk,
7117:81?92.
Rada Mihalcea and Hugo Liu. 2006. A corpus-
based approach to finding happiness. In AAAI-2006
Spring Symposium on Computational Approaches to
Analysing Weblogs, pages 139?144. AAAI Press.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Us-
ing mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Seman-
tic Evaluation Exercises (SemEval-2013), Atlanta,
Georgia, USA.
Saif Mohammad. 2012a. Portable features for clas-
sifying emotional text. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 587?591, Montr?eal,
Canada.
Saif M. Mohammad. 2012b. #Emotional tweets. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics - Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, SemEval
?12, pages 246?255, Stroudsburg, PA.
Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2009. Compositionality principle in
recognition of fine-grained emotions from text. In
Proceedings of the Third International Conference
on Weblogs and Social Media, pages 278?281, San
Jose, California.
C.E. Osgood, Suci G., and P. Tannenbaum. 1957.
The measurement of meaning. University of Illinois
Press.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling. Synthesis Lectures
on Human Language Technologies, 3(1):1?103.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Robert Plutchik. 1980. A general psychoevolutionary
theory of emotion. Emotion: Theory, research, and
experience, 1(3):3?33.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 339?346, Stroudsburg, PA,
USA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14:130?137.
Matthew Purver and Stuart Battersby. 2012. Ex-
perimenting with distant supervision for emotion
classification. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association
for Computational Linguistics, EACL ?12, pages
482?491, Stroudsburg, PA. Association for Compu-
tational Linguistics.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimen-
tal study. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1524?1534.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-Affect: An affective extension of WordNet.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (LREC-
2004), pages 1083?1086, Lisbon, Portugal.
Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classification using massive exam-
ples extracted from the web. In Proceedings of the
22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 881?
888, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 589?596, Stroudsburg, PA. Association
for Computational Linguistics.
Andranik Tumasjan, Timm O Sprenger, Philipp G
Sandner, and Isabell M Welpe. 2010. Predicting
elections with Twitter : What 140 characters reveal
about political sentiment. Word Journal Of The In-
ternational Linguistic Association, pages 178?185.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P. Sheth. 2012. Harnessing twitter
?big data? for automatic emotion identification. In
Proceedings of the 2012 ASE/IEEE International
Conference on Social Computing, SOCIALCOM-
PASSAT ?12, pages 587?592, Washington, DC,
USA. IEEE Computer Society.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ?10, pages
1462?1470, Stroudsburg, PA, USA. Association for
Computational Linguistics.
41
