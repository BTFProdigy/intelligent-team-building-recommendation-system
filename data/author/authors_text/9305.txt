An Expert Lexicon Approach to Identifying English Phrasal Verbs 
 
Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang, Rohini Srihari  
 
Cymfony Inc. 
600 Essjay Road 
Williamsville, NY 14221, USA 
{wei, xzhang, cniu, yjiang, rohini}@Cymfony.com
 
Abstract 
Phrasal Verbs are an important feature 
of the English language. Properly 
identifying them provides the basis for 
an English parser to decode the related 
structures. Phrasal verbs have been a 
challenge to Natural Language 
Processing (NLP) because they sit at 
the borderline between lexicon and 
syntax. Traditional NLP frameworks 
that separate the lexicon module from 
the parser make it difficult to handle 
this problem properly.  This paper 
presents a finite state approach that 
integrates a phrasal verb expert lexicon 
between shallow parsing and deep 
parsing to handle morpho-syntactic 
interaction. With precision/recall 
combined performance benchmarked 
consistently at 95.8%-97.5%, the 
Phrasal Verb identification problem 
has basically been solved with the 
presented method.    
1 Introduction 
Any natural language processing (NLP) system 
needs to address the issue of handling  multiword 
expressions, including Phrasal Verbs (PV) [Sag 
et al 2002; Breidt et al 1996]. This paper 
presents a proven approach to identifying 
English PVs based on pattern matching using a 
formalism called Expert Lexicon.   
Phrasal Verbs are an important feature of the 
English language since they form about one third 
of the English verb vocabulary. 1  Properly 
                                                     
1 For the verb vocabulary of our system based on  
machine-readable dictionaries and two Phrasal Verb 
dictionaries, phrasal verb entries constitute 33.8% of 
the entries. 
recognizing PVs is an important condition for  
English parsing. Like single-word verbs, each 
PV has its own lexical features including 
subcategorization features that determine its 
structural patterns [Fraser 1976; Bolinger 1971; 
Pelli 1976; Shaked 1994], e.g., look for has 
syntactic subcategorization and semantic features 
similar to those of search; carry?on shares 
lexical features with continue. Such lexical 
features can be represented in the PV lexicon in 
the same way as those for single-word verbs, but 
a parser can only use them when the PV is 
identified. 
Problems like PVs are regarded as ?a pain in 
the neck for NLP? [Sag et al 2002]. A proper 
solution to this problem requires tighter 
interaction between syntax and lexicon than 
traditionally available [Breidt et al 1994].  
Simple lexical lookup leads to severe 
degradation in both precision and recall, as our 
benchmarks show (Section 4). The recall 
problem is mainly due to separable PVs such as 
turn?off which allow for syntactic units to be 
inserted inside the PV compound, e.g., turn it off, 
turn the radio off.  The precision problem is 
caused by the ambiguous function of the particle. 
For example, a simple lexical lookup will mistag 
looked for as a phrasal verb in sentences such as 
He looked for quite a while but saw nothing. 
In short, the traditional NLP framework that 
separates the lexicon module from a parser 
makes it difficult to handle this problem properly.  
This paper presents an expert lexicon approach 
that integrates the lexical module with contextual 
checking based on shallow parsing results.  
Extensive blind benchmarking shows that this 
approach is very effective for identifying phrasal 
verbs, resulting in the precision/recall combined 
F-score of about 96%.   
 The remaining text is structured as follows. 
Section 2 presents the problem and defines the 
task. Section 3 presents the Expert Lexicon 
formalism and illustrates the use of this 
formalism in solving this problem. Section 4 
shows the benchmarking and analysis, followed 
by conclusions in Section 5. 
2 Phrasal Verb Challenges  
This section defines the problems we intend to 
solve, with a checklist of tasks to accomplish.  
2.1 Task Definition 
First, we define the task as the identification of 
PVs in support of deep parsing, not as the parsing 
of the structures headed by a PV. These two are 
separated as two tasks not only because of 
modularity considerations, but more importantly 
based on a natural labor division between NLP 
modules.  
Essential to the second argument is that these 
two tasks are of a different linguistic nature: the 
identification task belongs to (compounding) 
morphology (although it involves a syntactic 
interface) while the parsing task belongs to 
syntax. The naturalness of this division is 
reflected in the fact that there is no need for a 
specialized, PV-oriented parser. The same parser, 
mainly driven by lexical subcategorization 
features, can handle the structural problems for 
both phrasal verbs and other verbs. The 
following active and passive structures involving 
the PVs look after (corresponding to watch) and 
carry?on (corresponding to continue) are 
decoded by our deep parser after PV 
identification: she is being carefully ?looked 
after? (watched); we should ?carry on? (continue) 
the business for a while. 
There has been no unified definition of PVs 
among linguists. Semantic compositionality is 
often used as a criterion to distinguish a PV from 
a syntactic combination between a verb and its 
associated adverb or prepositional phrase 
[Shaked 1994]. In reality, however, PVs reside in 
a continuum from opaque to transparent in terms 
of semantic compositionality [Bolinger 1971]. 
There exist fuzzy cases such as take something 
away2 that may be included either as a PV or as a 
regular syntactic sequence. There is agreement 
                                                     
2  Single-word verbs like ?take? are often 
over-burdened with dozens of senses/uses. Treating 
marginal cases like ?take?away? as independent 
phrasal verb entries has practical benefits in relieving 
the burden and the associated noise involving ?take?.   
on the vocabulary scope for the majority of PVs, 
as reflected in the overlapping of PV entries from 
major English dictionaries.   
English PVs are generally classified into three 
major types. Type I usually takes the form of an 
intransitive verb plus a particle word that 
originates from a preposition. Hence the resulting 
compound verb has become transitive, e.g., look 
for, look after, look forward to, look into, etc. 
Type II typically takes the form of a transitive 
verb plus a particle from the set {on, off, up, 
down}, e.g., turn?on, take?off, wake?up, 
let?down. Marginal cases of particles may also 
include {out, in, away} such as take?away, 
kick ?in, pull?out.3   
Type III takes the form of an intransitive verb 
plus an adverb particle, e.g., get by, blow up, burn 
up, get off, etc. Note that Type II and Type III 
PVs have considerable overlapping in  
vocabulary, e.g., The bomb blew up vs. The 
clown blew up the balloon. The overlapping 
phenomenon can be handled by assigning both a 
transitive feature and an intransitive feature to the 
identified PVs in the same way that we treat the 
overlapping of single-word verbs.   
The first issue in handling PVs is inflection. A 
system for identifying PVs should match the 
inflected forms, both regular and irregular, of the 
leading verb.  
The second is the representation of the lexical 
identity of recognized PVs. This is to establish a 
PV (a compound word) as a syntactic atomic unit 
with all its lexical properties determined by the 
lexicon [Di Sciullo and Williams 1987]. The 
output of the identification module based on a PV 
lexicon should support syntactic analysis and 
further processing. This translates into two 
sub-tasks: (i) lexical feature assignment, and (ii) 
canonical form representation. After a PV is 
identified, its lexical features encoded in the PV 
lexicon should be assigned for a parser to use. 
The representation of a canonical form for an 
identified PV is necessary to allow for individual 
rules to be associated with identified PVs in 
further processing and to facilitate verb retrieval 
in applications. For example, if we use turn_off 
as the canonical form for the PV turn?off, 
identified in both he turned off the radio and he 
                                                     
3 These three are arguably in the gray area. Since they 
do not fundamentally affect the meaning of the 
leading verb, we do not have to treat them as phrasal 
verbs.  In principle, they can also be treated as  adverb 
complements of verbs.   
turned the radio off, a search for turn_off will 
match all and only the mentions of this PV.  
The fact that PVs are separable hurts recall. In 
particular, for Type II, a Noun Phrase (NP) object 
can be inserted inside the compound verb. NP 
insertion is an intriguing linguistic phenomenon 
involving the morpho-syntactic interface: a 
morphological compounding process needs to 
interact with the formation of a syntactic unit.  
Type I PVs also have the separability problem, 
albeit to a lesser degree.  The possible inserted 
units are adverbs in this case, e.g., look 
everywhere for, look carefully after.   
What hurts precision is spurious matches of 
PV negative instances. In a sentence with the 
structure V+[P+NP], [V+P] may be mistagged as 
a PV, as seen in the following pairs of examples 
for Type I and Type II:  
 
(1a) She [looked for] you yesterday. 
(1b) She looked [for quite a while] (but saw  
nothing). 
(2a) She [put on] the coat. 
(2b) She put [on the table] the book she  
borrowed yesterday. 
 
To summarize, the following is a checklist of 
problems that a PV identification system should 
handle: (i) verb inflection, (ii) lexical identity 
representation, (iii) separability, and (iv) 
negative instances. 
2.2 Related Work 
Two lines of research are reported in addressing 
the PV problem: (i) the use of a high-level 
grammar formalism that integrates the 
identification with parsing, and (ii) the use of a 
finite state device in identifying PVs as a lexical 
support for the subsequent parser. Both 
approaches have their own ways of handling the 
morpho-syntactic interface. 
[Sag et al 2002] and [Villavicencio et al 
2002] present their project LinGO-ERG that 
handles PV identification and parsing together. 
LingGO-ERG is based on Head-driven Phrase 
Structure Grammar (HPSG), a unification-based 
grammar formalism. HPSG provides a 
mono-stratal lexicalist framework that facilitates 
handling intricate morpho-syntactic interaction. 
PV-related morphological and syntactic 
structures are accounted for by means of a lexical 
selection mechanism where the verb morpheme 
subcategorizes for its syntactic object in addition 
to its particle morpheme. 
The LingGO-ERG lexicalist approach is 
believed to be effective. However, their coverage 
and testing of the PVs seem preliminary. The 
LinGO-ERG lexicon contains 295 PV entries, 
with no report on benchmarks.  
In terms of the restricted flexibility and 
modifiability of a system, the use of high-level 
grammar formalisms such as HPSG to integrate 
identification in deep parsing cannot be 
compared with the alternative finite state 
approach [Breidt et al 1994]. 
[Breidt et al1994]?s approach is similar to our 
work. Multiword expressions including idioms, 
collocations, and compounds as well as PVs are 
accounted for by using local grammar rules 
formulated as regular expressions. There is no 
detailed description for English PV treatment 
since their work focuses on multilingual, 
multi-word expressions in general. The authors 
believe that the local grammar implementation of 
multiword expressions can work with general 
syntax either implemented in a high-level 
grammar formalism or implemented as a local 
grammar for the required morpho-syntactic 
interaction, but this interaction is not 
implemented into an integrated system and hence 
it is impossible to properly measure performance 
benchmarks. 
There is no report on implemented solutions 
covering the entire English PVs that are fully 
integrated into an NLP system and are well tested 
on sizable real life corpora, as is presented in this 
paper.   
3 Expert Lexicon Approach  
This section illustrates the system architecture 
and presents the underlying Expert Lexicon (EL) 
formalism, followed by the description of the 
implementation details.  
3.1 System Architecture 
Figure 1 shows the system architecture that 
contains the PV Identification Module based on 
the PV Expert Lexicon.  
This is a pipeline system mainly based on 
pattern matching implemented in local grammars 
and/or expert lexicons [Srihari et al2003]. 4 
                                                     
4 POS and NE tagging are hybrid systems involving 
both hand-crafted rules and statistical learning.  
English parsing is divided into two tasks: shallow 
parsing and deep parsing. The shallow parser 
constructs Verb Groups (VGs) and basic Noun 
Phrases (NPs), also called BaseNPs [Church 
1988]. The deep parser utilizes syntactic 
subcategorization features and semantic features 
of a head (e.g., VG) to decode both syntactic and 
logical dependency relationships such as 
Verb-Subject, Verb-Object, Head-Modifier, etc. 
 
 
Part-of-Speech  
(POS) Tagging 
General
Lexicon Lexical lookup 
Named Entity  
(NE) Taggig 
Shallow Parsing 
PV Identification 
Deep parsing 
General
Lexicon 
PV Expert 
Lexicon 
Figure 1. System Architecture 
 
The general lexicon lookup component 
involves stemming that transforms regular or 
irregular inflected verbs into the base forms to 
facilitate the later phrasal verb matching. This 
component also performs indexing of the word 
occurrences in the processed document for 
subsequent expert lexicons.  
The PV Identification Module is placed 
between the Shallow Parser and the Deep Parser. 
It requires shallow parsing support for the 
required syntactic interaction and the PV output 
provides lexical support for deep parsing. 
Results after shallow parsing form a proper 
basis for PV identification. First, the inserted NPs 
and adverbial time NEs are already constructed 
by the shallow parser and NE tagger. This makes 
it easy to write pattern matching rules for 
identifying separable PVs. 
Second, the constructed basic units NE, NP 
and VG provide conditions for 
constraint-checking in PV identification. For 
example, to prevent spurious matches in 
sentences like she put the coat on the table, it is 
necessary to check that the post-particle unit 
should NOT be an NP.  The VG chunking also 
decodes the voice, tense and aspect features that 
can be used as additional constraints for PV 
identification. A sample macro rule 
active_V_Pin that checks the ?NOT passive? 
constraint and the ?NOT time?, ?NOT location? 
constraints is shown in 3.3.  
3.2 Expert Lexicon Formalism 
The Expert Lexicon used in our system is an 
index-based formalism that can associate pattern 
matching rules with lexical entries. It is 
organized like a lexicon, but has the power of a 
lexicalized local grammar.   
All Expert Lexicon entries are indexed, 
similar to the case for the finite state tool in 
INTEX [Silberztein 2000]. The pattern matching 
time is therefore reduced dramatically compared 
to a sequential finite state device [Srihari et al 
2003].5   
The expert lexicon formalism is designed to 
enhance the lexicalization of our system, in 
accordance with the general trend of lexicalist 
approaches to NLP. It is especially beneficial in 
handling problems like PVs and many individual 
or idiosyncratic linguistic phenomena that can 
not be covered by non-lexical approaches.  
Unlike the extreme lexicalized word expert 
system in [Small and Rieger 1982] and similar to 
the IDAREX local grammar formalism [Breidt et 
al.1994], our EL formalism supports a 
parameterized macro mechanism that can be 
used to capture the general rules shared by a set 
of individual entries. This is a particular useful 
mechanism that will save time for computational 
lexicographers in developing expert lexicons, 
especially for phrasal verbs, as shall be shown in 
Section 3.3 below. 
The Expert Lexicon tool provides a flexible 
interface for coordinating lexicons and syntax: 
any number of expert lexicons can be placed at 
any levels, hand-in-hand with other 
non-lexicalized modules in the pipeline 
architecture of our system.    
                                                     
5 Some other unique features of our EL formalism 
include: (i) providing the capability of proximity 
checking as rule constraints in addition to pattern 
matching using regular expressions so that the rule 
writer or lexicographer can exploit the combined 
advantages of both, and (ii) the propagation 
functionality of semantic tagging results, to 
accommodate principles like one sense per discourse. 
3.3 Phrasal Verb Expert Lexicon 
To cover the three major types of PVs, we use the 
macro mechanism to capture the shared patterns. 
For example, the NP insertion for Type II PV is 
handled through a macro called V_NP_P, 
formulated in pseudo code as follows.  
 
V_NP_P($V,$P,$V_P,$F1, $F2,?)  := 
Pattern:  
$V  
NP 
(?right?|?back?|?straight?) 
$P  
NOT NP 
Action:  
$V: %assign_feature($F1, $F2,?)   
%assign_canonical_form($V_P) 
$P: %deactivate 
 
This macro represents cases like Take the coat 
off, please; put it back on, it?s raining now. It 
consists of two parts: ?Pattern? in regular 
expression form (with parentheses for optionality, 
a bar for logical OR, a quoted string for checking 
a word or head word) and ?Action? (signified by 
the prefix %). The parameters used in the macro 
(marked by the prefix $) include the leading verb 
$V, particle $P, the canonical form $V_P, and 
features $Fn.  After the defined pattern is matched, 
a Type II separable verb is identified. The Action 
part ensures that the lexical identity be 
represented properly, i.e. the assignment of the 
lexical features and the canonical form. The 
deactivate action flags the particle as being part 
of the phrasal verb.  
In addition, to prevent a spurious case in (3b), 
the macro V_NP_P checks the contextual 
constraints that no NP (i.e. NOT NP) should 
follow a PV particle. In our shallow parsing, NP 
chunking does not include identified time NEs, 
so it will not block the PV identification in (3c). 
    
(3a) She [put the coat on]. 
(3b) She put the coat [on the table]. 
(3c) She [put the coat on] yesterday. 
 
All three types of PVs when used without NP 
insertion are handled by the same set of macros, 
due to the formal patterns they share. We use a 
set of macros instead of one single macro, 
depending on the type of particle and the voice of 
the verb, e.g., look for calls the macro 
[active_V_Pfor | passive_V_Pfor], fly in calls the 
macro [active_V_Pin | passive_V_Pin], etc.  
The distinction between active rules and 
passive rules lies in the need for different 
constraints. For example, a passive rule needs to 
check the post-particle constraint [NOT NP] to 
block the spurious case in (4b).  
 
(4a) He [turned on] the radio. 
(4b)  The world [had been turned] [on its 
head] again. 
 
As for particles, they also require different 
constraints in order to block spurious matches. 
For example, active_V_Pin (formulated below) 
requires the constraints ?NOT location NOT 
time? after the particle while active_V_Pfor only 
needs to check ?NOT time?, shown in (5) and (6). 
 
(5a) Howard [had flown in] from Atlanta. 
(5b) The rocket [would fly] [in 1999]. 
(6a) She was [looking for] California on the 
map. 
(6b) She looked [for quite a while]. 
 
active_V_Pin($V, in, $V_P,$F1, $F2,?)  := 
Pattern:  
$V NOT passive 
(Adv|time) 
$P  
NOT location NOT time 
Action:  
$V: %assign_feature($F1, $F2, ?)   
%assign_canonical_form($V_P) 
$P: %deactivate 
 
The coding of the few PV macros requires 
skilled computational grammarians and a 
representative development corpus for rule 
debugging. In our case, it was approximately 15 
person-days of skilled labor including data 
analysis, macro formulation and five iterations of 
debugging against the development corpus. But 
after the PV macros are defined, lexicographers 
can quickly develop the PV entries: it only cost 
one person-day to enter the entire PV vocabulary 
using the EL formalism and the implemented 
macros. We used the Cambridge International 
Dictionary of Phrasal Verbs and Collins Cobuild 
Dictionary of Phrasal Verbs as the major 
reference for developing our PV Expert 
Lexicon. 6  This expert lexicon contains 2,590 
entries. The EL-rules are ordered with specific 
rules placed before more general rules. A sample 
of the developed PV Expert Lexicon is shown 
below (the prefix @ denotes a macro call): 
 
abide:  @V_P_by(abide, by, abide_by, V6A, 
APPROVING_AGREEING) 
accede: @V_P_to(accede, to, accede_to, V6A, 
APPROVING_AGREEING) 
add:  @V_P(add, up, add_up, V2A, 
MATH_REASONING); 
 @V_NP_P(add, up, add_up, V6A, 
MATH_REASONING) 
???? 
 
In the above entries, V6A and V2A are 
subcategorization features for transitive and 
intransitive verb respectively, while 
APPROVING_AGREEING and 
MATH_REASONING are semantic features. 
These features provide the lexical basis for the 
subsequent parser. 
The PV identification method as described 
above resolves all the problems in the checklist. 
The following sample output shows the 
identification result: 
 
NP[That]  
VG[could slow: slow_down/V6A/MOVING] 
NP[him]  
down/deactivated . 
4 Benchmarking 
Blind benchmarking was done by two 
non-developer testers manually checking the 
results. In cases of disagreement, a third tester 
was involved in examining the case to help 
resolve it. We ran benchmarking on both the 
formal style and informal style of English text.  
4.1 Corpus Preparation 
Our development corpus (around 500 KB) 
consists of the MUC-7 (Message Understanding 
                                                     
6 Some entries that are listed in these dictionaries do 
not seem to belong to phrasal verb categories, e.g., 
relieve?of (as used in relieve somebody of something), 
remind?of (as used in remind somebody of 
something), etc.  It is generally agreed that such cases 
belong to syntactic patterns in the form of 
V+NP+P+NP that can be captured by 
subcategorization.  We have excluded these cases.  
Conference-7) dryrun corpus and an additional 
collection of news domain articles from TREC 
(Text Retrieval Conference) data.  The PV expert 
lexicon rules, mainly the macros, were developed 
and debugged using the development corpus.    
The first testing corpus (called English-zone 
corpus) was downloaded from a website that is 
designed to teach PV usage in Colloquial English 
(http://www.english-zone.com/phrasals/w-phras
als.html). It consists of 357 lines of sample 
sentences containing 347 PVs. This addresses the 
sparseness problem for the less frequently used 
PVs that rarely get benchmarked in running text 
testing. This is a concentrated corpus involving 
varieties of PVs from text sources of an informal 
style, as shown below.7 
 
"Would you care for some dessert? We have 
ice cream, cookies, or cake." 
Why are you wrapped up in that blanket? 
After John's wife died, he had to get through 
his sadness. 
After my sister cut her hair by herself, we had 
to take her to a hairdresser to even her 
hair out! 
After the fire, the family had to get by without 
a house. 
 
We have prepared two collections from the 
running text data to test written English of a more 
formal style in the general news domain:  (i) the 
MUC-7 formal run corpus (342 KB) consisting 
of 99 news articles, and (ii) a collection of 23,557 
news articles (105MB) from the TREC data.   
4.2 Performance Testing 
There is no available system known to the NLP 
community that claims a capability for PV 
treatment and could thus be used for a reasonable 
performance comparison. Hence, we have 
devised a bottom-line system and a baseline 
system for comparison with our EL-driven 
system. The bottom-line system is defined as a 
simple lexical lookup procedure enhanced with 
the ability to match inflected verb forms but with 
no capability of checking contextual constraints. 
There is no discussion in the literature on what 
                                                     
7 Proper treatment of PVs is most important in parsing 
text sources involving Colloquial English, e.g., 
interviews, speech transcripts, chat room archives. 
There is an increasing demand for NLP applications in 
handling this type of data.    
constitutes a reasonable baseline system for PV. 
We believe that a baseline system should have 
the additional, easy-to-implement ability to jump 
over inserted object case pronouns (e.g., turn it 
on) and adverbs (e.g., look everywhere for) in PV 
identification.  
Both the MUC-7 formal run corpus and the 
English-zone corpus were fed into the 
bottom-line  and the baseline systems as well as 
our EL-driven system described in Section 3.3. 
The benchmarking results are shown in Table 1 
and Table 2. The F-score is a combined measure 
of precision and recall, reflecting the overall 
performance of a system. 
Table 1.  Running Text Benchmarking 1 
 Bottom-line Baseline EL 
Correct 303 334 338 
Missing 58 27 23 
Spurious 33 34 7 
Precision 90.2% 88.4% 98.0% 
Recall 83.9% 92.5% 93.6% 
F-score 86.9% 91.6% 95.8% 
Table 2.  Sampling Corpus Benchmarking 
 Bottom-line Baseline EL 
Correct 215 244 324 
Missing 132 103 23 
Spurious 0 0 0 
Precision 100% 100% 100% 
Recall 62.0% 70.3% 93.4% 
F-score 76.5% 82.6% 96.6% 
 
Compared with the bottom-line performance 
and the baseline performance, the F-score for the 
presented method has surged 9-20 percentage 
points and 4-14 percentage points, respectively. 
The high precision (100%) in Table 2 is due to 
the fact that, unlike running text, the sampling 
corpus contains only positive instances of PV. 
This weakness, often associated with sampling 
corpora, is overcome by benchmarking running 
text corpora (Table 1 and Table 3).   
To compensate for the limited size of the 
MUC formal run corpus, we used the testing 
corpus from the TREC data. For such a large 
testing corpus (23,557 articles, 105MB), it is 
impractical for testers to read every article to 
count mentions of all PVs in benchmarking. 
Therefore, we selected three representative PVs 
look for, turn?on and blow?up and used the 
head verbs (look, turn, blow), including their 
inflected forms, to retrieve all sentences that 
contain those verbs. We then ran the retrieved 
sentences through our system for benchmarking 
(Table 3).  
All three of the blind tests show fairly 
consistent benchmarking results (F-score 
95.8%-97.5%), indicating that these benchmarks 
reflect the true capability of the presented system, 
which targets the entire PV vocabulary instead of 
a selected subset. Although there is still some 
room for further enhancement (to be discussed 
shortly), the PV identification problem is 
basically solved. 
Table 3.  Running Text Benchmarking 2 
 ?look for? ?turn?on? ?blow?up?
 Correct 1138 128 650 
 Missing 76 0 33 
 Spurious 5 9 0 
 Precision 99.6% 93.4% 100.0% 
 Recall 93.7% 100.0% 95.2% 
 F-score 96.6% 97.5% 97.5% 
4.3 Error Analysis 
There are two major factors that cause errors: (i) 
the impact of errors from the preceding modules 
(POS and Shallow Parsing), and (ii) the mistakes 
caused by the PV Expert Lexicon itself.  
The POS errors caused more problems than 
the NP grouping errors because the inserted NP 
tends to be very short, posing little challenge to 
the BaseNP shallow parsing. Some verbs 
mis-tagged as nouns by POS were missed in PV 
identification. 
There are two problems that require the 
fine-tuning of the PV Identification Module. First, 
the macros need further adjustment in their 
constraints. Some constraints seem to be too 
strong or too weak.  For example, in the Type I 
macro, although we expected the possible 
insertion of an adverb, however, the constraint on 
allowing for only one optional adverb and not 
allowing for a time adverbial is still too strong. 
As a result, the system failed to identify 
listening?to and meet?with in the following 
cases: ?was not listening very closely on 
Thursday to American concerns about human 
tights? and ... meet on Friday with his Chinese... 
The second type of problems cannot be solved 
at the macro level. These are individual problems 
that should be handled by writing specific rules 
for the related PV. An example is the possible 
spurious match of the PV have?out in the 
sentence ...still have our budget analysts out 
working the numbers. Since have is a verb with 
numerous usages, we should impose more 
individual constraints for NP insertion to prevent 
spurious matches, rather than calling a common 
macro shared by all Type II verbs.  
4.4 Efficiency Testing 
To test the efficiency of the index-based PV 
Expert Lexicon in comparison with a sequential  
Finite State Automaton (FSA) in the PV 
identification task, we conducted the following 
experiment.  
The PV Expert Lexicon was compiled as a 
regular local grammar into a large automaton that 
contains 97,801 states and 237,302 transitions. 
For a file of 104 KB (the MUC-7 dryrun corpus 
of 16,878 words), our sequential FSA  runner 
takes over 10 seconds for processing on the  
Windows NT platform with a Pentium PC. This 
processing only requires 0.36 second using the 
indexed PV Expert Lexicon module. This is 
about 30 times faster.   
5 Conclusion 
An effective and efficient approach to phrasal 
verb identification is presented. This approach 
handles both separable and inseparable phrasal 
verbs in English. An Expert Lexicon formalism 
is used to develop the entire phrasal verb lexicon 
and its associated pattern matching rules and 
macros.  This formalism allows the phrasal verb 
lexicon to be called between two levels of 
parsing for the required morpho-syntactic 
interaction in phrasal verb identification. 
Benchmarking using both the running text corpus 
and sampling corpus shows that the presented 
approach provides a satisfactory solution to this 
problem. 
In future research, we plan to extend the 
successful experiment on phrasal verbs to other 
types of multi-word expressions and idioms 
using the same expert lexicon formalism. 
Acknowledgment 
This work was partly supported by a grant from 
the Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under 
contract F30602-03-C-0044. The authors wish to 
thank Carrie Pine and Sharon Walter of AFRL 
for supporting and reviewing this work. Thanks 
also go to the anonymous reviewers for their 
constructive comments. 
References 
Breidt. E., F. Segond and G. Valetto. 1994. Local 
Grammars for the Description of Multi-Word 
Lexemes and Their Automatic Recognition in 
Text.  Proceedings of Comlex-2380 - Papers 
in Computational Lexicography, Linguistics 
Institute, HAS, Budapest, 19-28. 
Breidt, et al 1996. Formal description of 
Multi-word Lexemes with the Finite State 
formalism: IDAREX. Proceedings of 
COLING 1996, Copenhagen.  
Bolinger, D. 1971. The Phrasal Verb in English.  
Cambridge, Mass., Harvard University Press. 
Church, K. 1988. A stochastic parts program and 
noun phrase parser for unrestricted text. 
Proceedings of ANLP 1988.  
Di Sciullo, A.M. and E. Williams. 1987.  On The 
Definition of Word. The MIT Press, 
Cambridge, Massachusetts. 
Fraser, B. 1976. The Verb Particle Combination 
in English.  New York: Academic Press. 
Pelli, M. G. 1976. Verb Particle Constructions in 
American English.  Zurich: Francke Verlag 
Bern. 
Sag, I., T. Baldwin, F. Bond, A. Copestake and D. 
Flickinger. 2002. Multiword Expressions: A 
Pain in the Neck for NLP. Proceedings of 
CICLING 2002, Mexico City, Mexico, 1-15. 
Shaked, N. 1994. The Treatment of Phrasal 
Verbs in a Natural Language Processing 
System, Dissertation, CUNY. 
Silberztein, M. 2000. INTEX: An FST Toolbox. 
Theoretical Computer Science, Volume 
231(1): 33-46. 
Small, S. and C. Rieger. 1982. Parsing and 
comprehending with word experts (a theory 
and its realisation). W. Lehnert and M. 
Ringle, editors, Strategies for Natural 
Language Processing. Lawrence Erlbaum 
Associates, Hillsdale, NJ.  
Srihari, R., W. Li, C. Niu and T. Cornell. 2003. 
InfoXtract: An Information Discovery Engine 
Supported by New Levels of Information 
Extraction. Proceeding of HLT-NAACL 
Workshop on Software Engineering and 
Architecture of Language Technology 
Systems, Edmonton, Canada. 
Villavicencio, A. and A. Copestake. 2002. 
Verb-particle constructions in a 
computational grammar of English.  
Proceedings of the Ninth International 
Conference on Head-Driven Phrase Structure 
Grammar, Seoul, South Korea. 
Extracting Exact Answers to Questions Based on Structural Links?
Wei Li, Rohini K. Srihari, Xiaoge Li, M. Srikanth, Xiuhong Zhang, Cheng Niu
Cymfony Inc.
600 Essjay Road, Williamsville, NY 14221. USA.
{wei, rohini, xli, srikanth, xzhang, cniu}@cymfony.com
Keywords:  Question Answering, Information Extraction, Semantic Parsing, Dependency Link
?  This  work was partly supported by a grant from the Air Force Research Laboratory?s Information Directorate 
(AFRL/IF), Rome, NY, under contracts F30602-00-C-0037 and F30602-00-C-0090.
Abstract
This paper presents a novel approach to
extracting phrase-level answers in a question 
answering system. This approach uses
structural support provided by an integrated 
Natural Language Processing (NLP) and
Information Extraction (IE) system. Both
questions and the sentence-level candidate
answer strings are parsed by this NLP/IE
system into binary dependency structures.
Phrase-level answer extraction is modelled by 
comparing the structural similarity involving 
the question-phrase and the candidate answer-
phrase.
There are two types of structural support. The 
first type involves predefined, specific entity 
associa tions such as Affiliation, Position, Age 
for a person entity. If a question asks about 
one of these associations, the answer-phrase
can be determined as long as the system
decodes such pre-defined dependency links 
correctly, despite the syntactic difference
used in expressions between the question and 
the candidate answer string. The second type 
involves generic grammatical relationships
such as V-S (verb-subject), V-O (verb-
object).
Preliminary experimental results show an
improvement in both precision and recall in 
extracting phrase-level answers, compared
with a baseline system which only uses Named 
Entity constraints. The proposed methods are 
particularly effective in cases where the
question-phrase does not correspond to a
known named entity type and in cases where 
there are multiple candidate answer-phrases
satisfying the named entity constraints.
Introduction
Natural language Question Answering (QA) is 
recognized as a capability with great potential.
The NIST-sponsored Text Retrieval Conference
(TREC) has been the driving force for developing 
this technology through its QA track since TREC-8
(Voorhees 1999). There has been significant
progress and interest in QA research in recent
years (Voorhees 2000, Pasca and Harabagiu 2001).
QA is different than search engines in two aspects: 
(i) instead of a string of keyword search terms, the 
query is a natural language question, necessitating 
question parsing, (ii) instead of a list of documents 
or URLs, a list of candidate answers at phrase level 
or sentence level are expected to be returned in 
response to a query, hence the need for text
processing beyond keyword indexing, typically
supported by Natural Language Processing (NLP) 
and Information Extraction (IE) (Chinchor and
Marsh 1998, Hovy, Hermjakob and Lin 2001, Li 
and Srihari 2000). Examples of the use of NLP and 
IE in Question Answering include shallow parsing 
(Kupiec, 1993), semantic parsing (Litkowski
1999), Named Entity tagging (Abney et al 2000, 
Srihari and Li 1999) and high-level IE (Srihari 
and Li, 2000).
Identifying exact or phrase-level answers is a
much more challenging task than sentence-level
answers. Good performance on the latter can be 
achieved by using sophisticated passage retrieval 
techniques and/or shallow level NLP/IE
processing (Kwok et al 2001, Clarke et al 2001). 
The phrase-level answer identification involves
sophisticated NLP/IE and it is difficult to apply 
only IR techniques for this task (Prager et al 
1999). These two tasks are closely related. Many 
systems (e.g. Prager et al1999; Clark et al2001) 
take a two-stage approach. The first stage
involves retrieving sentences or paragraphs in
documents as candidate answer strings. Stage
Two focuses on extracting phrase-level exact
answers from the candidate answer strings.
This paper focuses on methods involving Stage 
Two. The input is a sentence pair consisting of a 
question and a sentence-level candidate answer 
string. The output is defined to be a phrase, called 
answer-point, extracted from the candidate
answer string. In order to identify the answer-
point, the pair of strings are parsed by the same 
system to generate binary dependency structures 
for both specific entity associations and generic 
grammatical relationships. An integrated Natural 
Language Processing (NLP) and Information
Extraction (IE) engine is used to extract named 
entities (NE) and their associations and to decode 
grammatical relationships. The system searches
for an answer-point by comparing the structural 
similarity involving the question-phrase and a
candidate answer-phrase. Generic grammatical
relationships are used as a back-off for specific 
entity associations when the question goes beyond 
the scope of the specific associations or when the 
system fails to identify the answer-point which 
meets the specific  entity association constraints. 
The proposed methods are particularly helpful in 
cases where the question-phrase does not
correspond to a known named entity type and in 
cases where there are multiple candidate answer-
points to select from.
The rest of the paper is structured as follows: 
Section 1 presents the NLP/IE engine used,
sections 2 discusses how to identify and formally 
represent what is being asked, section 3 presents 
the algorithm on identifying exact answers
leveraging structural support, section 4 presents 
case studies and benchmarks, and section 5 is the 
conclusion.
Kernel IE Modules Linguistic  Modules
Entity
Association
Named
Entity
Part-Of-
Speech
Asking-point
Identification
O
ut
pu
t(
En
ti
ty
, 
Ph
ra
se
 a
nd
 S
tr
uc
tu
ra
l 
Li
nk
s)
Shallow
Parsing
Semantic
Parsing
Tokenizer
Input
Figure 1: InfoXtract? NLP/IE System Architecture
1 NLP/IE Engine Description
The NLP/IE engine used in the QA system
described here is named InfoXtract?. It consists 
of an NLP component and IE component, each 
consisting of a set of pipeline modules (Figure 1). 
The NLP component serves as underlying support 
for IE. A brief description of these modules is 
given below.
? Part-of-Speech Tagging: tagging syntactic
categories such as noun, verb, adjective, etc. 
? Shallow Parsing: grouping basic linguistic
units as building blocks for structural links, 
such as Basic Noun Phrase, Verb Group, etc. 
? Asking-point Identification: analysis of
question sentences to determine what is being 
asked
? Semantic Parsing: decoding grammatical
dependency relationships at the logical level 
between linguistic units, such as Verb-Subject
(V-S), Verb-Object (V-O), Head-Modifier
(H-M) relationships; both active patterns and 
passive patterns will be parsed into the same 
underlying logical S-V-O relationships
? Named Entity Tagger: classifying proper
names and other phrases to different
categories such as Person, Organization,
Location, Money, etc.
? Entity Association Extractor: relating named 
entities with predefined associations such as 
Affiliation, Position, Age, Spouse, Address,
etc.
The NE tagger in our system is benchmarked to 
achieve close to human performance, around or 
above 90% precision and recall for most
categories of NE. This performance provides
fundamental support to QA. Many questions
require a named entity or information associated 
with a named entity as answers. A subset of the 
NE hierarchy used in our system is illustrated
below:
Person: woman, man
Organization: company, government,
association, school, army, mass-media
Location: city, province, country, continent, 
ocean, lake, etc.
Time Expressions: hour, part-of-day, day-of-
week, date, month, season, year, decade, 
century, duration
Numerical Expressions: percentage, money, 
number, weight, length, area, etc.
Contact expressions: email, address,
telephone, etc.
The Entity Association module correlates named 
entities and extracts their associations with other 
entities or phrases.  These are specific, predefined 
relationships for entities of person and
organization. Currently, our system can extract
the following entity associations with high
precision (over 90%) and modest recall ranging 
from 50% to 80% depending on the size of
grammars written for each specific association. 
Person: affiliation, position, age, spouse,
birth-place, birth-time, etc.
Organization: location, staff, head, products,
found-time, founder, etc.
Entity associations are semantic structures very
useful in supporting QA. For example, from the 
sentence Grover Cleveland , who in June 1886
married 21-year-old Frances Folsom,?the IE
engine can identify the following associations:
Spouse: Grover Cleveland ?Frances Folsom
Spouse: Frances?Grover Cleveland 
Age:  Frances Folsom?21-year-old
A question asking about such an association, say, 
Q11: Who was President Cleveland ?s wife, will be 
parsed into the following association link between 
a question-phrase ?Who? and the entity ?Cleveland? 
(see Section 2): Spouse: Cleveland ? Who. The 
semantic similarity between this structure and the 
structure Spouse: Grover Cleveland ? Frances 
Folsom can determine the answer point to be
?Frances Folsom?.
The Semantic Parsing module decodes the
grammatical dependency relationships: V-S, V-O,
V-C (Verb-Complement), H-M of time, location, 
reason, manner, purpose, result, etc. This module 
extends the shallow parsing module through the 
use of a cascade of handcrafted pattern matching 
rules.  Manual benchmarking shows results with 
the following performance:
H-M: Precision 77.5%
V-O: Precision 82.5%
V-S: Precision 74%
V-C: Precision 81.4%
In our semantic parsing, not only passive patterns
will be decoded into the same underlying
structures as active patterns, but structures for
verbs such as acquire and for de-verbal nouns such 
as acquisition lead to the same dependency links, 
as shown below.
AOL acquired Netscape in 1998. ?
V-S: acquired? AOL
V-O: acquired ? Netscape
H-M: acquired ? in 1998 (time-modifier)
Netscape was acquired by AOL in 1998. ?
V-S: was acquired ? by AOL
V-O: was acquired ? Netscape
H-M: was acquired ? in 1998 (time-modifier)
the acquisition of Netscape by AOL in 1998??
V-S: acquisition ? by AOL
V-O: acquisition ? of Netscape 
H-M: acquired ? in 1998 (time-modifier)
These links can be used as structural support to 
answer questions like Who acquired Netscape or
which company was acquired by AOL.
Obviously, our semantic parser goes one step
further than parsers which only decode syntactic 
relationships. It consumes some surface structure 
variations to provide the power of comparing the 
structural similarity at logical level. However,
compared with the entity association structures 
which sits at deep semantic level, the logical SVO 
(Subject-Verb-Object) structures still cannot
capture semantic relations which are expressed
using different head verbs with different
structures. An example is the pair : X borrows Y 
from Z versus Z lends Y to X.
2 Asking Point Link Identification
Asking point link identification is a crucial step in 
a QA system. It provides the necessary
information decoded from question processing for 
a system to locate the corresponding answer-
points from candidate answer strings. 
The Asking-point (Link) Identification Module is 
charged with the task of parsing wh-phrases in 
their context into three categories: NE Asking-
point, Asking-point Association  Link and
Asking-point Grammar  Link. Asking Point refers
to the question phrases with its constraints  that a 
corresponding answer-point should satisfy in
matching. Asking-point Link is the decoded binary 
relationship from the asking point to another unit 
in the question. 
The identification of the NE asking point is
essentially mapping the wh-phrase to the NE
types or subtypes. For example, which year is 
mapped to [which year]/NeYear, how old mapped 
to [how old]/NeAge, and how long mapped to 
[how long]/NeLength or [how long]/NeDuration, 
etc.
The identification of the Asking-point Association
Link is to decide whether the incoming question 
asks about a predefined association relationship. 
For Asking-point Association  Link, the module 
needs to identify the involved entity and the asked 
association. For example, the Asking-point
Association  Link for How old is John Smith is the 
AGE relationship of the NePerson John Smith,
represented as AGE: John Smith ? [how
old]/NeAge.
The wh-phrases which may or may not be mapped 
to NE asking points and whose dependency links 
are beyond predefined associations lead to Asking-
point Grammar Links, e.g. How did Julian Hill 
discover nylon? This asking-point link is
represented as H-M: discover ? [How]/manner-
modifier. As seen, an asking-point grammar link 
only involves generic grammatical constraints: in 
this case, the constraints for a candidate answer-
point to satisfy during matching are H-M link with 
?discover? as head and a phrase which must be a 
modifier of manner. 
These three types of asking points and their
possible links form a natural hierarchy that can be 
used to facilitate the backoff strategy for the
answer-point extraction module (see Section 3): 
Asking-point Association Link ? Asking-point
Grammar Link ? NE Asking Point.  This
hierarchy defines the sequence of matching steps 
which should be followed during the answer-point
extraction.
The backoff from Asking-point Association  Link 
to Asking-point Grammar  Link is necessary as the 
latter represents more generic structural constraints 
than the former. For example, in the sentence
where is IBM located, the Asking-point
Association Link is LOCATION: IBM ?
[where]/NeLocation while the default Grammar
Link is H-M: located ? [where]/location-
modifier. When the specific association constraints 
cannot be satisfied, the system should attempt to 
locate an answer-point by searching for a location-
modifier of the key verb ?located?.
The NE asking point constraints are also marked 
for asking-point association links and those asking-
point grammar links whose wh-phrases can be
mapped to NE asking points. Backing off to the 
NE asking point is required in cases where the 
asking-point association constraints and
grammatical structural constraints cannot be
satisfied. For How old is John Smith, the asking-
point grammar  link is represented as H-M: John 
Smith ? [how old]/NeAge. If the system cannot 
find a corresponding AGE association or a
modifier of NeAge for the entity John Smith to
satisfy the structural constraints, it will at least 
attempt to locate a candidate answer-point by
enforcing the NE asking point constraints NeAge. 
When there is only one NeAge in the answer
string, the system can extract it as the only
possible answer-point even if the structural
constraints are not honored.
3 Answer Point Identification
The answer-point identification is accomplished 
through  matching the asking-point to candidate 
answer-points using the following back-off
algorithm based on the processing results of the 
question and the sentence-level candidate answer 
string.
(1) if there is Asking-point Association
Link, call Match(asking-point association 
link, candidate answer-point association 
link) to search for the corresponding
association to locate answer-point
(2) if step (1) fails and there is an asking-
point grammar link, call Match(asking-
point grammar link, candidate answer-
point grammar link) to search for the
corresponding grammar link to locate the 
answer-point
(3) if step (2) fails and there is an NE asking 
point, search for the corresponding NEs: 
if there is only one corresponding NE, 
then extract this as the answer-point else 
mark all corresponding NEs as candidate 
answer-points
The function Match(asking-point link, candidate 
answer-point link) is defined as (i) exact match or 
synonym match of the related units (synonym
match currently confined to verb vs. de-verbal
noun); (ii) match the relation type directly (e.g. V-
S matches V-S, AGE matches AGE, etc.); (iii) 
match the type of asking point and answer point 
(e.g. NePerson asking point matches NePerson and
its sub-types NeMan and NeWoman; ?how?
matches manner-modifier; etc.): either through
direct link or indirect link based on conjunctive 
link (ConjLink) or equivalence link (S-P, subject-
predicative or appositive relations between two
NPs).
Step (1) and Step (2) attempt to leverage the
structural support from parsing and high-level
information extraction beyond NE. It is worth
noticing that in our experiment, the structural
support used for answer-point identification only 
checks the binary links involving the asking point 
and the candidate answer points, instead of full 
template matching as proposed in (Srihari and Li, 
2000).
Full template matching is best exemplified by the 
following example. If the incoming question is 
Who won the Nobel Prize in 1991, and the
candidate answer string is John Smith won the
Nobel Prize in 1991, the question template and 
answer template are shown below:
win
V-S: NePerson [Who]
V-O: NP [the Nobel Prize]
H-M: NeYear [1991]
win
V-S: NePerson [John Smith]
V-O: NP [the Nobel Prize]
H-M: NeYear [1991]
The template matching will match the asking point 
Who with the answer point John Smith because for 
all the dependency links in the trees, the
information is all compatible (in this case, exact
match). This is the ideal case of full template
matching and guarantees the high precision of the 
extracted answer point.
However, in practice, full template matching is 
neither realistic for most of cases nor necessary for 
achieving the objective of extracting answer points 
in a two-stage approach. It is not realistic because 
natural language semantic parsing is such a
challenging problem that a perfect dependency tree 
(or full template) which pieces together every
linguistic unit is not always easy to decode. For
InfoXtract,, in most cases, the majority, but not 
all, of the decoded binary dependency links are 
accurate, as shown in the benchmarks above. In 
such situations, insisting on checking every
dependency link of a template tree is too strong a 
condition to meet. On the other hand, it is actually 
not necessary to check all the links in the
dependency trees for full template matching. With 
the modular design and work division between
sentence level candidate answer string generation 
module (Stage One) and answer-point extraction 
from the candidate answer strings (Stage Two), 
all the candidate answer strings are already
determined by previous modules as highly
relevant. In this situation, a simplified partial
template matching, namely, ?asking/answer point 
binary relation matching?, will be sufficient to 
select the answer-point, if present, from the
candidate answer string. In other words, the
system only needs to check this one dependency 
link in extracting the answer-point. For the
previous example, only the asking/answer point 
binary dependency links need to be matched as 
illustrated below:
V-S win?[Who]/NePerson
V-S win?[John Smith]/NeMan
Some sample results are given in section 4 to
illustrate how answer-points are identified based 
on matching binary relations involving
asking/answer points. 
4 Experiments and Results
In order to conduct the feasibility study on the 
proposed method, we selected the first 100
questions from the TREC-8 QA track pool and 
the corresponding first candidate answer
sentences for this preliminary experiment. The 
Stage One processing for generating candidate 
answer sentences was conducted by the existing 
ranking module of our QA system. The Stage
Two processing for answer-point identification
was accomplished by using the algorithm
described in Section 3.
As shown in Table 1, out of the 100 question-
answer pairs we selected, 9 have detected
association links involving asking/answer points, 
44 are found to have grammar links involving 
asking/answer points. 
Table 1: Experiment Results
detected correct fail precision recall
Association
Links 9 8 1 89% 8%
Grammar
Links 44 39 6 89% 39%
NE Points 
(Baseline) 76 41 35 54% 41%
Overall
performance 86 71 14 83% 71%
As for NE asking points, 76 questions were
identified to require some type of NE as answers.
Assume that a baseline answer-point identification 
system only uses NE asking points as constraints, 
out of the 76 questions requiring NEs as answers, 
41 answer-points were identified successfully
because there was only one NE in the answer
string which matches the required NE type. The 
failed cases in matching NE asking point
constraints include two situations: (i) no NE exists 
in the answer string; (ii) multiple NEs satisfy the 
type constraints of NE asking points (i.e. more 
than one candidate answer-points found from the 
answer string) or there is type conflict during the 
matching of NE asking/answer points. Therefore, 
the baseline system would achieve 54% precision 
and 41% recall based on the standard precision and 
recall formulas: 
Precision = Correct / Detected
Recall = Correct / Relevant. 
In comparison, in our answer-point identification 
system which leverages structural support from
both the entity association links and grammar links 
as well as the NE asking points, both the precision 
and recall are raised: from the baseline 54% to 
83% for precision and from 41% to 71% for recall. 
The significant improvement in precision and
recall is attributed to the performance of structural 
matching in identifying exact answers. This
demonstrates the benefits of making use of
sophisticated NLP/IE technology, beyond NE and 
shallow parsing.
Using grammar links alone, exact answers were
identified for 39 out of the 44 candidate answer-
points satisfying the types of grammar links in 100 
cases. During matching, 6 cases failed either due to 
the parsing error or due to the type conflict
between the asking/answer points (e.g. violating 
the type constraints such as manner-modifier on 
the answer-point for ?how? question). The high 
precision and modest recall in using the grammar 
constraints is understandable as the grammar links 
impose very strong constraints on both the nodes 
and the structural type. The high precision
performance indicates that grammar links not
only have the distinguishing power to identify
exact answers in the presence of multiple NE 
options but also recognize answers in the absence 
of asking point types.
Even stronger structural support comes from the 
semantic relations decoded by the entity
association extraction module.  In this case, the 
performance is naturally high-precision (89%)
low-recall (8%) as predefined association links 
are by nature more sparse than generic
grammatical relations.
In the following, we illustrate with some
examples with questions from the TREC-8 QA 
task on how the match function identified in
Section 3 applies to different question types.
Q4: How much did Mercury spend on
advertising in 1993? ? asking-point grammar 
link:
V-O spend ? [How much]/NeMoney
A: Last year the company spent Pounds 12m
on advertising. ? candidate answer-point
grammar link:
V-O spent?[Pounds 12m]/NeMoney
Answer-point Output: Pounds 12m
This case requires (i) exact match in its original 
verb form between spend and spent; (ii) V-O type 
match; and (iii) asking/answer point type
NeMoney match through direct link.
Q63: What nuclear-powered Russian
submarine sank in the Norwegian Sea on April 
7, 1989?? asking-point grammar link: 
H-M submarine?[What]
A: NEZAVISIMAYA GAZETA on the
Komsomolets nuclear-powered submarine
which sank in the Norwegian Sea five years 
ago:? candidate answer-point grammar link:
H-M submarine?Komsomolets
Answer-point Output: Komsomolets
This case requires (i) exact match of submarine;
(ii) H-M type match; and (iii) asking/answer point 
match through direct link:  there are no asking
point type constraints because the asking point 
goes beyond existing NE. This case highlights the 
power of semantic parsing in answer-point
extraction. Since there are no type constraints on 
answer point,1 candidate answer points cannot be 
extracted without bringing in structural context by 
checking the NE type. Most of what-related asking 
points such as those in the patterns
?what/which?N?, ?what type/kind of ?N? go
beyond NE and require this type of structural
relation checking to locate the exact answer. The 
case below is another example.
Q79: What did Shostakovich write for
Rostropovich?? asking-point grammar link: 
V-O write?[What]
A: The Polonaise from Tchaikovsky?s opera
Eugene was a brief but cracking opener and its 
brilliant bluster was no sooner in our ears than 
forcibly contradicted by the bleak depression of 
Shostakovich?s second cello concerto, Op. 126,
a late work written for Rostropovich in 1966 
between the thirteenth and fourteenth
symphonies. ? candidate answer-point
grammar link:
V-O written?[a late work]/NP
S-P [Op. 126]/NP ?[a late work]/NP
Answer-point Output: Op. 126
This case requires (i) exact match in its original 
verb form between ?written? and ?write?;
(ii) V-O type match; and (iii) asking/answer point 
match through indirect link based on equivalence 
link S-P. When there are no NE constraints on the 
answer point, a proper name or an initial-
capitalized NP is preferred over an ordinary,
lower-case NP as an answer point. This heuristic is 
built-in so that ?Op. 126? is output as the answer-
point in this case instead of ?a late work?.
1 Strictly speaking, there are some type constraints on 
the answer point. The type constraints are something to 
the effect of ?a name for a kind of ship? which goes 
beyond the existing NE types defined.
Conclusion
This paper presented an approach to exact answer 
identification to questions using only binary
structural links involving the question-phrases.
Based on the experiments conducted, some
preliminary conclusions can be arrived at.
? The Entity Association extraction helps in 
pinpointing exact answers precisely
? Grammar dependency links enable the
system to not only identify exact answers 
but answer questions not covered by the 
predefined set of available
NEs/Associations
? Binary dependency links instead of full 
structural templates provide sufficient and 
effective structural leverage for extracting 
exact answers 
Some cases remain difficult however, beyond the 
current level of NLP/IE.  For example,
Q92: Who released the Internet worm in the 
late 1980s?? asking point link: 
V-S (released, NePerson[Who])
A: Morris, suspended from graduate studies at 
Cornell University at Syracuse, N,Y,, is
accused of designing and disseminating in
November, 1988, a rogue program or ?worm? 
that immobilized some 6,000 computers linked 
to a research network, including some used by 
NASA and the Air Force.? answer point link:
V-S (disseminating, NePerson[Morris]) 
In order for this case to be handled, the following 
steps are required: (i) the semantic parser should 
be able to ignore the past participle postmodifier 
phrase headed by ?suspended?; (ii) the V-O
dependency should be decoded between ?is
accused? and ?Morris?; (iii) the V-S dependency 
should be decoded between ?designing and
disseminating? and ?Morris? based on the pattern 
rule ?accuse NP of Ving?? V-S(Ving, NP); (iv) 
the conjunctive structure should map the V-S
(?designing and disseminating?, ?Morris?) into two 
V-S links; (v)  ?disseminate? and ?release? should
be linked somehow for synonym expansion.  It 
may be unreasonable to expect an NLP/IE system 
to accomplish all of these, but each of the above 
challenges indicates some directions for further 
research in this topic.
We would like to extend the experiments on a 
larger set of questions to further investigate the 
effectiveness of structural support in extracting
exact answers. The TREC-9 and TREC 2001 QA 
pool and the candidate answer sentences generated 
by both NLP-based or IR-based QA systems would 
be ideal for further testing this method.
5 Acknowledgement
The authors wish to thank Walter Gadz and Carrie 
Pine of AFRL for supporting this work. Thanks 
also go to anonymous reviewers for their valuable 
comments.
References
Abney, S., Collins, M and Singhal, A. (2000) Answer
Extraction. In Proceedings of ANLP -2000, Seattle.
Chinchor, N. and Marsh, E. (1998) MUC -7 Information 
Extraction Task Definition (version 5.1), In
?Proceedings of MUC-7?. Also published at
http://www.muc.saic.com/
Clarke, C. L. A., Cormack, G. V. and Lynam, T. R. 
(2001), Exploiting Redundancy in Question
Answering. In Proceedings of SIGIR?01, New
Orleans, LA.
Hovy, E.H., U. Hermjakob, and Chin-Yew Lin. 2001. 
The Use of External Knowledge of Factoid QA. In 
Proceedings of the 10th Text Retrieval Conference 
(TREC 2001), Gaithersburg, MD, U.S.A., November 
13-16, 2001
Kupiec, J. (1993) MURAX: A Robust Linguistic
Approach For Question Answering Using An On-Line
Encyclopaedia . In Proceedings of SIGIR-93,
Pittsburgh, PA.
Kwok, K. L., Grunfeld, L., Dinstl, N. and Chan, M. 
(2001), TREC2001 Question-Answer, Web and Cross 
Language Experiments using PIRCS. In Proceedings 
of TREC-10, Gaithersburg, MD.
Li, W. and Srihari, R. (2000) A Domain Independent 
Event Extraction Toolkit , Phase 2 Final Technical 
Report, Air Force Research Laboratory/Rome, NY.
Litkowski, K. C. (1999) Question-Answering Using
Semantic Relation Triples. In Proceedings of TREC-
8, Gaithersburg, MD.
Pasca, M. and Harabagiu, S. M. High Performance
Question/Answering. In Proceedings of SIGIR 2001: 
pages 366-374
Prager, J., Radev, D., Brown, E., Coden, A. and Samn, 
V., The use of predictive annotation for question
answering in TREC8. In Proceedings of TREC-8,
Gaithersburg, MD.
Srihari, R. and Li, W. (1999) Information Extraction 
supported Question Answering. In Proceedings of
TREC-8, Gaithersberg, MD.
Srihari, R and Li, W. (2000b). A Question Answering 
System Supported by Information Extraction. In 
Proceedings of ANLP 2000, Seattle.
Voorhees, E. (1999), The TREC-8 Question Answering 
Track Report, In Proceedings of TREC-8,
Gaithersburg, MD. 
Voorhees, E. (2000), Overview of the TREC-9
Question Answering Track , In Proceedings of
TREC-9, Gaithersburg, MD. 
