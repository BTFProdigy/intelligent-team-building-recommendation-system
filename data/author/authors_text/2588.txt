NIL Is Not Nothing: Recognition of Chinese  
Network Informal Language Expressions 
Abstract 
Informal language is actively used in net-
work-mediated communication, e.g. chat 
room, BBS, email and text message. We refer 
the anomalous terms used in such context as 
network informal language (NIL) expres-
sions. For example, ??(ou3)? is used to re-
place ?? (wo3)? in Chinese ICQ. Without 
unconventional resource, knowledge and 
techniques, the existing natural language 
processing approaches exhibit less effective-
ness in dealing with NIL text. We propose to 
study NIL expressions with a NIL corpus and 
investigate techniques in processing NIL ex-
pressions. Two methods for Chinese NIL ex-
pression recognition are designed in NILER 
system. The experimental results show that 
pattern matching method produces higher 
precision and support vector machines 
method higher F-1 measure. These results are 
encouraging and justify our future research 
effort in NIL processing. 
1 Introduction 
The rapid global proliferation of Internet applica-
tions has been showing no deceleration since the 
new millennium. For example, in commerce more 
and more physical customer services/call centers 
are replaced by Internet solutions, e.g. via MSN, 
ICQ, etc. Network informal language (NIL) is ac-
tively used in these applications. Following this 
trend, we forecast that NIL would become a key 
language for human communication via network.  
Today NIL expressions are ubiquitous. They 
appear, for example, in chat rooms, BBS, email, 
text message, etc. There is growing importance in 
understanding NIL expressions from both technol-
ogy and humanity research points of view. For 
instance, comprehension of customer-operator dia-
logues in the aforesaid commercial application 
would facilitate effective Customer Relationship 
Management (CRM).  
Recently, sociologists showed many interests in 
studying impact of network-mediated communica-
tion on language evolution from psychological and 
cognitive perspectives (Danet, 2002; McElhearn, 
2000; Nishimura, 2003). Researchers claim that 
languages have never been changing as fast as to-
day since inception of the Internet; and the lan-
guage for Internet communication, i.e. NIL, gets 
more concise and effective than formal language.  
Processing NIL text requires unconventional 
linguistic knowledge and techniques. Unfortu-
nately, developed to handle formal language text, 
the existing natural language processing (NLP) 
approaches exhibit less effectiveness in dealing 
with NIL text. For example, we use ICTCLAS 
(Zhang et al, 2003) tool to process sentence ???
???????(Is he going to attend 
a meeting?)?. The word segmentation result is 
??|?|?|?|?|??|?|??. In this sentence , ??
?? (xi4 ba1 xi4)? is a NIL expression 
which means ?is he ?.?? in this case. It can be 
concluded that without identifying the expression, 
further Chinese text processing techniques are not 
able to produce reasonable result. 
This problem leads to our recent research in 
?NIL is Not Nothing? project, which aims to pro-
duce techniques for NIL processing, thus  avails 
understanding of change patterns and behaviors in 
language (particularly in Internet language) evolu-
tion. The latter could make us more adaptive to the 
dynamic language environment in the cyber world.  
Recently some linguistic works have been car-
ried out on NIL for English. A shared dictionary 
Yunqing Xia,  Kam-Fai Wong,  Wei Gao
Department of Systems Engineering and Engineering Management 
The Chinese University of Hong Kong, Shatin, Hong Kong 
{yqxia, kfwong, wgao}@se.cuhk.edu.hk 
95
has been compiled and made available online. It 
contains 308 English NIL expressions including 
English abbreviations, acronyms and emoticons. 
Similar efforts for Chinese are rare. This is be-
cause Chinese language has not been widely used 
on the Internet until ten years ago. Moreover, Chi-
nese NIL expression involves processing of Chi-
nese Pinyin and dialects, which results in higher 
complexity in Chinese NIL processing.  
In ?NIL is Not Nothing? project, we develop a 
comprehensive Chinese NIL dictionary. This is a 
difficult task because resource of NIL text is rather 
restricted. We download a collection of BBS text 
from an Internet BBS system and construct a NIL 
corpus by annotating NIL expressions in this col-
lection by hand. An empirical study is conducted 
on the NIL expressions with the NIL corpus and a 
knowledge mining tool is designed to construct the 
NIL dictionary and generate statistical NIL fea-
tures automatically. With these knowledge and 
resources, the NIL processing system, i.e. NILER, 
is developed to extract NIL expressions from NIL 
text by employing state-of-the-art information ex-
traction techniques.  
The remaining sections of this paper are organ-
ized as follow. In Section 2, we observe formation 
of NIL expressions. In Section 3 we present the 
related works. In Section 4, we describe NIL cor-
pus and the knowledge engineering component in 
NIL dictionary construction and NIL features gen-
eration. In Section 5 we present the methods for 
NIL expression recognition. We outline the ex-
periments, discussions and error analysis in Sec-
tion 6, and finally Section 7 concludes the paper. 
2 The Ways NIL Expressions Are Typi-
cally Formed 
NIL expressions were first introduced for expedit-
ing writing or computer input, especially for online 
chat where the input speed is crucial to prompt and 
effective communication. For example, it is rather 
annoying to input full Chinese sentences in text-
based chatting environment, e.g. over the mobile 
phone. Thus abbreviations and acronyms are then 
created by forming words in capital with the first 
letters of a series of either English words or Chi-
nese Pinyin.  
Chinese Pinyin is a popular approach to Chi-
nese character input. Some Pinyin input methods 
incorporate lexical intelligence to support word or 
phrase input. This improves input rate greatly. 
However, Pinyin input is not error free. Firstly, 
options are usually prompted to user and selection 
errors result in homophone, e.g. ???(ban1 
zu2)? and ??? (ban1 zhu3)?. Secondly, 
input with incorrect Pinyin or dialect produces 
wrong Chinese words with similar pronunciation, 
e.g. ???(xi1 fan4)? and ???(xi3 hua-
n1)?. Nonetheless, prompt communication spares 
little time to user to correct such a mistake. The 
same mistake in text is constantly repeated, and the 
wrong word thus becomes accepted by the chat 
community. This, in fact, is one common way that 
a new Chinese NIL expression is created. 
We collect a large number of ?sentences? 
(strictly speaking, not all of them are sentences) 
from a Chinese BBS system and identify NIL ex-
pressions by hand. An empirical study on NIL ex-
pressions in this collection shows that NIL 
expressions can be classified into four classes as 
follow based on their origins. 
1) Abbreviation (A). Many Chinese NIL expres-
sions are derived from abbreviation of Chi-
nese Pinyin. For example, ?PF? equals to ??
?(pei4 fu2)? which means ?admire?.  
2) Foreign expression (F). Popular Informal ex-
pressions from foreign languages such as 
English are adopted, e.g. ?ASAP? is used for 
?as soon as possible?. 
3) Homophone (H). A NIL expression is some-
times generated by borrowing a word with 
similar sound (i.e. similar Pinyin). For exam-
ple ??? ? equals ??? ? which means 
?like?. ???? and ???? hold homophony 
in a Chinese dialect. 
4) Transliteration (T) is a transcription from one 
alphabet to another and a letter-for-letter or 
sound-for-letter spelling is applied to repre-
sent a word in another language. For exam-
ple, ???(bai4 bai4)? is transliteration 
of ?bye-bye?. 
A thorough observation, in turn, reveals that, 
based on the ways NIL expressions are formed 
and/or their part of speech (POS) attributes, we 
observe a NIL expression usually takes one of the 
forms presented in Table 1 and Table 2. 
The above empirical study is essential to NIL 
lexicography and feature definition. 
96
3 Related Works 
NIL expression recognition, in particular, can be 
considered as a subtask of information extraction 
(IE). Named entity recognition (NER) happens to 
hold similar objective with NIL expression recog-
nition, i.e. to extract meaningful text segments 
from unstructured text according to certain pre-
defined criteria. 
NER is a key technology for NLP applications 
such as IE and question & answering. It typically 
aims to recognize names for person, organization, 
location, and expressions of number, time and cur-
rency. The objective is achieved by employing 
either handcrafted knowledge or supervised learn-
ing techniques. The latter is currently dominating 
in NER amongst which the most popular methods 
are decision tree (Sekine et al, 1998; Pailouras et 
al., 2000), Hidden Markov Model (Zhang et al, 
2003; Zhao, 2004), maximum entropy (Chieu and 
Ng, 2002; Bender et al, 2003), and support vector 
machines (Isozaki and Kazawa, 2002; Takeuchi 
and Collier, 2002; Mayfield, 2003). 
From the linguistic perspective, NIL expres-
sions are rather different from named entities in 
nature. Firstly, named entity is typically noun or 
noun phrase (NP), but NIL expression can be any 
kind, e.g. number ?94? in NIL represents ????
which is a verb meaning ?exactly be?. Secondly, 
named entities often have well-defined meanings 
in text and are tractable from a standard dictionary; 
but NIL expressions are either unknown to the dic-
tionary or ambiguous. For example, ???? ap-
pears in conventional dictionary with the meaning 
of Chinese porridge, but in NIL text it represents ?
??? which surprisingly represents ?like?. The 
issue that concerns us is that these expressions like  
???? may also appear in NIL text with their 
formal meaning. This leads to ambiguity and 
makes it more difficult in NIL processing.  
Another notable work is the project of ?Nor-
malization of Non-standard Words? (Sproat et al, 
2001) which aims to detect and normalize the 
?Non-Standard Words (NSW)? such as digit se-
quence; capital word or letter sequence; mixed 
case word; abbreviation; Roman numeral; URL 
and e-mail address. In our work, we consider most 
types of the NSW in English except URL and 
email address. Moreover, we consider Chinese 
NIL expressions that contain same characters as 
the normal words. For example, ??? ? and           
???? both appear in common dictionaries, but 
they carry anomalous meanings in NIL text. Am-
biguity arises and basically brings NIL expressions 
recognition beyond the scope of NSW detection.  
According to the above observations, we pro-
pose to employ the existing IE techniques to han-
dle NIL expressions. Our goal is to develop a NIL 
expression recognition system to facilitate net-
work-mediated communication. For this purpose, 
we first construct the required NIL knowledge re-
sources, namely, a NIL dictionary and n-gram sta-
tistical features. 
Table 2: NIL expression forms based on POS attribute.  
POS
Attribute 
# of NIL  
Expressions Examples 
Number 1 ?W? represents ??(wan4)?and means ?ten thousand?. 
Pronoun 9 ??? represents ??? and means ?I?. 
Noun 29 
?LG? represents ???(lao3 
gong1)? and means ?hus-
band?.
Adjective 250 ?FB? represents ???(fu3 
bai4)? and means ?corrupt?. 
Verb 34 
???(cong1 bai2)? repre-
sents ???(chong3 bai4)?
and means ?adore?. 
Adverb 10 ??(fen3)? represents ??
(hen3)? and means ?very?. 
Exclamation  9 
??(nie0)? represents ??
(ne0)? and equals a descrip-
tive exclamation. 
Phrase 309 ?AFK? represents ?Away From Keyboard?.  
Table 1: NIL expression forms based on word formation. 
Word  
Formation 
# of NIL  
Expressions Examples 
Chinese  
Word or 
Phrase
33 ???? represents ???? and means ?like?. 
Sequence of 
English 
Capitals  
341 ?PF? represents ???? and means ?admire?. 
Number 8
?94(jiu3 si4)? represents
???(jiu4 shi4)? and 
means ?exactly be?. 
Mixture of  
the Above 
Forms
30
?8?(ba1 cuo4)? repre-
sents ???(bu3 cuo4)?
and means ?not bad?. 
Emoticons 239 ?:-(? represents a sad emotion.
97
4 Knowledge Engineering 
Recognition of NIL expressions relies on uncon-
ventional linguistic knowledge such as NIL dic-
tionary and NIL features. We construct a NIL 
corpus and develop a knowledge engineering 
component to obtain these knowledge by running a 
knowledge mining tool on the NIL corpus. The 
knowledge mining tool is a text processing pro-
gram that extracts NIL expressions and their at-
tributes and contextual information, i.e. n-grams, 
from the NIL corpus. Workflow for this compo-
nent is presented in Figure 1. 
4.1  NIL Corpus
The NIL corpus is a collection of network informal 
sentences which provides training data for NIL 
dictionary and statistical NIL features. The NIL 
corpus is constructed by annotating a collection of 
NIL text manually. 
Obtaining real chat text is difficult because of 
the privacy restriction. Fortunately, we find BBS 
text within ????(da4 zui3 qu1)? zone in 
YESKY system (http://bbs.yesky.com/bbs/) re-
flects remarkable colloquial characteristics and 
contains a vast amount of NIL expressions. We 
download BBS text posted from December 2004 
and February 2005 in this zone. Sentences with 
NIL expressions are selected by human annotators, 
and NIL expressions are manually identified and 
annotated with their attributes. We finally col-
lected 22,432 sentences including 451,193 words 
and 22,648 NIL expressions. 
The NIL expressions are marked up with 
SGML. The typical example, i.e. ???????
???? in Section 1, is annotated as follows. 
where NILEX is the SGML tag to label a NIL ex-
pression, which entails NIL linguistic attributes 
including class, normal, pinyin, segments, pos, and 
posseg (see Section 4.2). H is a value of class (see 
Section 2). Value VERB demotes verb, ADJ adjec-
tive, NUM number and AUX auxiliary. 
4.2  NIL Dictionary 
The NIL dictionary is a structured databank that 
contains NIL expression entries. Each entity in 
turn entails nine attributes described as follow. 
1. ID: an unique identification number for the 
NIL expression, e.g. 915800; 
2. string: string of the NIL expression, e.g. ??
???;
3. class: class of the NIL expression (see Sec-
tion 2), e.g. ?H? for homophony;  
4. pinyin: Chinese Pinyin for the NIL expres-
sion, e.g. ?xi4 ba1 xi4?; 
5. normal: corresponding normal text for the 
NIL expression, e.g. ?????;  
6. segments: word segments of the NIL expres-
sion, e.g. ??|?|??; 
7. pos: POS tag associated with the expression, 
e.g. ?VERB? denoting a verb;  
8. posseg: a POS tag list for the word seg-
ments, e.g. ?VERB|AUX|VERB?;  
9. frequency: number of occurrences of the 
NIL expression. 
We run the knowledge mining tool to extract all 
annotated NIL expressions together with their at-
tributes from the NIL corpus. The NIL expressions 
are then each assigned an ID number and inserted 
into an indexed data file, i.e. the NIL dictionary. 
Current NIL dictionary contains 651 NIL entries.  
4.3  NIL Feature Set 
The NIL features are required by support vector 
machines method in NIL expression recognition. 
We define two types of statistical features for NIL 
expressions, i.e. Chinese word n-grams and POS 
tag n-grams. Bigger n leads to more contextual 
?<NILEX string=????? class=?H? normal=??
??? pinyin=?xi4 ba1 xi4? segments=??|?|??
pos=?VERB? posseg=?ADJ|NUM|ADJ?>???
</NILEX>?????
Figure 1: Workflow for NIL knowledge engineering 
component. NILE refers to NIL expression, which is 
identified and annotated by human annotator.  
NILE 
Annotation 
 Original Text  
 Collection 
NIL Corpus 
NIL 
Dictionary 
NIL 
Features 
Extract  
A Sentence 
Knowledge Mining Tool 
Word Segmentation & POS Tagging 
(ICTCLAS) 
98
information, but results in higher computational 
complexity. To compromise, we generate n-grams 
with n = 1, 2, 3, 4. For example,   ???/????
is a bi-gram for ????? in terms of word seg-
mentation, and its POS tag bi-gram is 
?PRONOUN/ VERB?.  
We run the knowledge mining tool on the NIL 
corpus to produce all n-grams for Chinese words 
and their POS tags in which NIL expression ap-
pears. 8379 features were generated including 
7416 word-based n-grams and 963 POS tag-based 
n-grams. These statistical NIL features are linked 
to the corresponding NIL dictionary entries by 
their global NIL expression IDs. 
Besides, we consider some morphological fea-
tures including being/containing a number, some 
English capitals or Chinese characters. These fea-
tures can be extracted by parsing string of the NIL 
expressions. 
5 NILER System 
5.1  Architecture 
We develop NILER system to recognize NIL ex-
pressions in NIL text and convert them to normal 
language text. The latter functionality is discussed 
in other literatures. Architecture of NILER system 
is presented in Figure 2. 
The input chat text is first segmented and POS 
tagged with ICTCLAS tool. Because ICTCLAS is 
not able to identify NIL expressions, some expres-
sions are broken into several segments. NIL ex-
pression recognizer processes the segments and 
POS tags and identifies the NIL expressions.  
5.2  NIL Expression Recognizer 
We implement two methods in NIL expression 
recognition, i.e. pattern matching and support vec-
tor machines. 
5.2.1  Method I: Pattern Matching  
Pattern matching (PM) is a traditional method in 
information extraction systems. It uses a hand-
crafted rule set and dictionary for this purpose. 
Because it?s simple, fast and independent of cor-
pus, this method is widely used in IE tasks. 
By applying NIL dictionary, candidates of NIL 
expressions are first extracted from the input text 
with longest matching. As ambiguity occurs con-
stantly, 24 patterns are produced and employed to 
disambiguate. We first extract those word and POS 
tag n-grams from the NIL corpus and create pat-
terns by generalizing them manually. An illustra-
tive pattern is presented as follows. 
?]_[)_(8]_[ ?!!! anyvunitvnotanyv
where anyv _  and unitv _  are variables denoting 
any word and any unit word respectively;  )(xnot
is the negation operator. The illustrative pattern 
determines ?8? to be a NIL expression if it is suc-
ceeded by a unit word. With this pattern, ?8? 
within sentence ?????  ???? (He has 
been working for eight hours.)? is not recognized 
as a NIL expression.  
5.2.2  Method II: Support Vector Machines  
Support vector machines (SVM) method produces 
high performance in many classification tasks 
(Joachims, 1998; Kudo and Matsumoto, 2001). As 
SVM can handle large numbers of features effi-
ciently, we employ SVM classification method to 
NIL expression recognition. 
Suppose we have a set of training data for a 
two-class classification problem {(x1,y1), (x2,
y2),?,(xN, yN)}, where ),...2,1( NiRx Di  ?  is a fea-
ture vector of the i-th order sample in the training 
set and }1,1{ ?iy  is the label for the sample. 
The goal of SVM is to find a decision function that 
accurately predicts y for unseen x. A non-linear 
SVM classifier gives a decision function 
))(()( xgsignxf   for an input vector x, where  
?
 
 
l
i
ii bzxKxg
1
),()( Y
The szi  are so-called support vectors, and 
represents the training samples. iY  and b  are pa-
rameters for SVM motel. l is number of training 
samples. ),( zxK  is a kernel function that implic-
NIL 
Dictionary 
NIL 
Features 
 Chat Text 
NIL Expression 
 List 
NIL Expression 
Recognizer 
Word Segmentation 
Word POS Tagging 
(ICTCLAS) 
Figure 2: Architecture of NILER system. 
99
itly maps vector x into a higher dimensional space. 
A typical kernel is defined as dot products, i.e.  
)(),( zxkzxK x .
Based on the training process, the SVM algo-
rithm constructs the support vectors and parame-
ters. When text is input for classification, it is first 
converted into feature vector x. The SVM method 
then classifies the vector x by determining sign of 
g(x), in which 1)(  xf  means that word x is posi-
tive and otherwise if 1)(  xf . The SVM algo-
rithm was later extended in SVMmulticlass to predict 
multivariate outputs (Joachims, 1998).  
In NIL expression recognition, we consider 
NIL corpus as training set and the annotated NIL 
expressions as samples. NIL expression recogni-
tion is achieved with the five-class SVM classifi-
cation task, in which four classes are those defined 
in Section 2 and reflected by class attribute within 
NIL annotation scheme. The fifth class is 
NOCLASS, which means the input text is not any 
NIL expression class.  
6 Experiments
6.1  Experiment Description 
We conduct experiments to evaluate the two meth-
ods in performing the task of NIL expression rec-
ognition. In training phase we use NIL corpus to 
construct NIL dictionary and pattern set for PM 
method, and generate statistical NIL features, sup-
port vectors and parameters for SVM methods. To 
observe how performance is influenced by the vol-
ume of training data, we create five NIL corpora, 
i.e. C#1~C#5, with five numbers of NIL sentences, 
i.e. 10,000, 13,000, 16,000, 19,000 and 22,432, by 
randomly selecting sentence from NIL corpus de-
scribed in Section 4.1.  
To generate test set, we download 5,690 sen-
tences from YESKY system which cover BBS text 
in March 2005. We identify and annotate NIL ex-
pressions within these sentences manually and 
consider the annotation results as gold standard.  
We first train the system with the five corpora 
to produce five versions of NIL dictionary, pattern 
set, statistical NIL feature set and SVM model. We 
then run the two methods with each version of the 
above knowledge over the test set to produce rec-
ognition results automatically. We compare these 
results against the gold stand and present experi-
mental results with criteria including precision, 
recall and F1-measure. 
6.2  Experimental Results 
We present experimental results of the two meth-
ods on the five corpora in Table 3. 
Table 3: Experimental results for the two methods on the five 
corpora. PRE denotes precision, REC denotes recall, and F1 
denotes F1-Measure. 
PM SVM Corpus
PRE REC F1 PRE REC F1 
C#1 0.742 0.547 0.630 0.683 0.703 0.693 
C#2 0.815 0.634 0.713 0.761 0.768 0.764 
C#3 0.873 0.709 0.783 0.812 0.824 0.818 
C#4 0.904 0.759 0.825 0.847 0.851 0.849 
C#5 0.915 0.793 0.850 0.867 0.875 0.871 
6.3  Discussion I: The Two Methods 
To compare performance of the two methods, we 
present the experimental results with smoothed 
curves for precision, recall and F1-Mesure in Fig-
ure 3, Figure 4 and Figure 5 respectively. 
0.65
0.7
0.75
0.8
0.85
0.9
0.95
0 1 2 3 4 5 6
Pattern Matching
SVM
Figure 3: Smoothed precision curves over the five corpora.  
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0 1 2 3 4 5 6
Pattern Matching
SVM
Figure 4: Smoothed recall curves over the five corpora.   
Figure 3 reveals that PM method produces 
higher precision, i.e. 91.5%, and SVM produces 
higher recall, i.e. 79.3%, and higher F1-Measure, 
i.e. 87.1%, with corpus C#5. It can be inferred that 
PM method is self-restrained. In other words, if a 
NIL expression is identified with this method, it is 
very likely that the decision is right. However, the 
weakness is that more NIL expressions are ne-
glected. On the other hand, SVM method outper-
100
forms PM method regarding overall capability, i.e. 
F1-Measure, according to Figure 5. 
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0 1 2 3 4 5 6
Pattern Matching
SVM
Figure 5: Smoothed F1-Measure curves over the five corpora. 
We argue that each method holds strength and 
weakness. Different methods should be adopted to 
cater to different application demands. For exam-
ple, in CRM text processing, we might favor preci-
sion. So PM method may be the better choice. On 
the other hand, to perform the task of chat room 
security monitoring, recall is more important. Then 
SVM method becomes the better option. We claim 
that there exists an optimized approach which 
combines the two methods and yields higher preci-
sion and better robustness at the same time. 
6.4  Discussion II: How Volume Influences Per-
formance 
To observe how training corpus influences per-
formance in the two methods regarding volume, 
we present experimental results with smoothed 
quality curves for the two method in Figure 6 and 
Figure 7 respectively. 
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
0 1 2 3 4 5 6
PRE
REC
F1
Figure 6: Smoothed quality curves for PM method over the 
five corpora.
0.65
0.7
0.75
0.8
0.85
0.9
0 1 2 3 4 5 6
PRE
REC
F1
Figure 7: Smoothed quality curves for SVM method 
over the five corpora. 
The smoothed quality curves in Figure 6 and 
Figure 7 reveal the tendency that bigger volume of 
training data leads to better processing quality. 
Meanwhile, the improvement tends to decrease 
along with increasing of volume. It thus predicts 
that there exists a corpus with a certain volume 
that produces the best quality according to the ten-
dency. Although current corpus is not big enough 
to prove the optimal volume, the tendency re-
vealed by the curves is obvious. 
6.5  Error Analysis 
We present two examples to analyze errors occur 
within our experiments. 
Err.1 Ambiguous NIL Expression 
Example 1:
[Sentence]: ??? 8??
[Meaning]:  I still don?t understand. 
[NIL expression found(Y/N)? ]: Y 
[Normal language text]: ??????
Error in Example 1 is caused by failure in iden-
tifying ? ? ? (mi3 bai2)?. Because ? ?
(mi3)? succeeds ?8(ba1)? in the word seg-
ments, i.e. ??|??|8|?|??, and it can be used as 
a unit word, PM method therefore refuses to iden-
tify ?8(ba)? as a NIL expression according to the 
pattern described in Section 5.2.1. In fact, ????
is an unseen NIL expression. SVM method suc-
cessfully recognizes ??? ? to be ??? (mi3
you3)?, thus recognizes ?8?. In our experiments 
56 errors in PM method suffer the same failure, 
while SVM method identifies 48 of them. This 
demonstrates that PM method is self-restrained 
and SVM method is relatively scalable in process-
ing NIL text. 
Err.2 Unseen NIL expression 
Example 2: 
[Sentence]: ??? 4U??
[Meaning]: Just came back from 4U. 
[NIL expression found (Y/N)?] : N
Actually, there is no NIL expression in example 
2. But because of a same 1-gram with ?4D?, i.e. 
?4?, SVM outputs ?4U? as a NIL expression. In 
fact, it is the name for a mobile dealer. There are 
78 same errors in SVM method in our experi-
ments, which reveals that SVM method is some-
times over-predicting. In other words, some NIL 
expressions are recognized with SVM method by 
mistake, which results in lower precision. 
101
7 Conclusions and Future Works 
Network informal language processing is a new 
NLP research application, which seeks to recog-
nize and normalize NIL expressions automatically 
in a robust and adaptive manner. This research is 
crucial to improve capability of NLP techniques in 
dealing with NIL text.  With empirical study on 
Chinese network informal text and NIL expres-
sions, we propose two NIL expression recognition 
methods, i.e. pattern matching and support vector 
machines. The experimental results show that PM 
method produces higher precision, i.e. 91.5%, and 
SVM method higher F-1 measure, i.e. 87.1%. 
These results are encouraging and justify our fu-
ture research effort in NIL processing. 
Research presented in this paper is preliminary 
but significant. We address future works as follow. 
Firstly, NIL corpus constructed in our work is fun-
damental. Not only will difficulty in seeking for 
text resource be overcome, but a large quantity of 
manpower will be allocated to this laborious and 
significant work. Secondly, new NIL expressions 
will appear constantly with booming of network-
mediated communication. A powerful NIL expres-
sion recognizer will be designed to improve adap-
tivity of the recognition methods and handle the 
unseen NIL expressions effectively. Finally, we 
state that research in this paper targets in special at 
NIL expressions in China mainland. Due to cul-
tural/geographical variance, NIL expressions in 
Hong Kong and Taiwan could be different. Further  
research will be conducted to adapt our methods to 
other NIL communities.  
References 
Bender, O., Och, F. J. and Ney, H. 2003. Maximum En-
tropy Models for Named Entity Recognition,
CoNLL-2003,  pp. 148-151.  
Chieu, H. L. and Ng, H. T. 2002. Named Entity Recog-
nition: A Maximum Entropy Approach Using Global 
Information. COLING-02, pp. 190-196. 
Danet, B. 2002. The Language of Email, European Un-
ion Summer School, University of Rome. 
Isozaki, H. and Kazawa, H. 2002. Efficient Support 
Vector Classifiers for Named Entity Recognition,
COLING-02, pp. 390-396.. 
Joachims, T. 1998. Text categorization with Support 
Vector Machines: Learning with many relevant fea-
tures. ECML?98, pp. 137-142. 
Kudo, T. and Matsumoto, Y. 2001. Chunking with Sup-
port Vector Machines. NAACL 2001, pp.192-199. 
Mayfield, J. 2003. Paul McNamee; Christine Piatko, 
Named Entity Recognition using Hundreds of Thou-
sands of Features, CoNLL-2003, pp. 184-187. 
McElhearn, K. 2000. Writing Conversation - An Analy-
sis of Speech Events in E-mail Mailing Lists,
http://www.mcelhearn.com/cmc.html, Revue Fran-
?aise de Linguistique Appliqu?e, volume V-1.  
Nishimura, Y. 2003. Linguistic Innovations and Inter-
actional Features of Casual Online Communication 
in Japanese, JCMC 9 (1). 
Pailouras, G., Karkaletsis, V. and Spyropoulos, C. D. 
2000. Learning Decision Trees for Named-Entity 
Recognition and Classification. Workshop on Ma-
chine Learning for Information Extraction, 
ECAI(2000).   
Sekine, S., Grishman, R. and Shinnou, H. 1998. A Deci-
sion Tree Method for Finding and Classifying Names 
in Japanese Texts, WVLC 98. 
Snitt, E. N. 2000. The Use of Language on the Internet,
http://www.eng.umu.se/vw2000/Emma/lin-
guistics1.htm. 
Sproat, R., Black, A.,  Chen, S., Kumar, S., Ostendorf, 
M. and Richards, M. 2001. Normalization of Non-
standard Words. Computer Speech and Languages, 
15(3):287- 333. 
Takeuchi, K. and Collier, N. 2002. Use of Support Vec-
tor Machines in Extended Named Entity Recognition.
CoNLL-2002, pp. 119-125.  
Zhang, Z., Yu, H., Xiong, D. and Liu, Q. 2003. HMM-
based Chinese Lexical Analyzer ICTCLAS. In the 2nd
SIGHAN workshop affiliated with ACL?03, pp. 184-
187.  
Zhao, S. 2004. Named Entity Recognition in Biomedical 
Texts Using an HMM model, COLING-04 workshop 
on Natural Language Processing in Biomedicine and 
its Applications.  
102
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1075?1083,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Exploiting Bilingual Information to Improve Web Search
Wei Gao1, John Blitzer2, Ming Zhou3, and Kam-Fai Wong1
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong, China
{wgao,kfwong}@se.cuhk.edu.hk
2Computer Science Division, University of California at Berkeley, CA 94720-1776, USA
blitzer@cs.berkeley.edu
3Microsoft Research Asia, Beijing 100190, China
mingzhou@microsoft.com
Abstract
Web search quality can vary widely across
languages, even for the same information
need. We propose to exploit this variation
in quality by learning a ranking function
on bilingual queries: queries that appear in
query logs for two languages but represent
equivalent search interests. For a given
bilingual query, along with correspond-
ing monolingual query log and monolin-
gual ranking, we generate a ranking on
pairs of documents, one from each lan-
guage. Then we learn a linear ranking
function which exploits bilingual features
on pairs of documents, as well as standard
monolingual features. Finally, we show
how to reconstruct monolingual ranking
from a learned bilingual ranking. Us-
ing publicly available Chinese and English
query logs, we demonstrate for both lan-
guages that our ranking technique exploit-
ing bilingual data leads to significant im-
provements over a state-of-the-art mono-
lingual ranking algorithm.
1 Introduction
Web search quality can vary widely across lan-
guages, even for a single query and search en-
gine. For example, we might expect that rank-
ing search results for the query Wj? ?Y?
(Thomas Hobbes) to be more difficult in Chinese
than it is in English, even while holding the ba-
sic ranking function constant. At the same time,
ranking search results for the query Han Feizi (8
:) is likely to be harder in English than in Chi-
nese. A large portion of web queries have such
properties that they are originated in a language
different from the one they are searched.
This variance in problem difficulty across lan-
guages is not unique to web search; it appears in
a wide range of natural language processing prob-
lems. Much recent work on bilingual data has fo-
cused on exploiting these variations in difficulty
to improve a variety of monolingual tasks, includ-
ing parsing (Hwa et al, 2005; Smith and Smith,
2004; Burkett and Klein, 2008; Snyder and Barzi-
lay, 2008), named entity recognition (Chang et al,
2009), and topic clustering (Wu and Oard, 2008).
In this work, we exploit a similar intuition to im-
prove monolingual web search.
Our problem setting differs from cross-lingual
web search, where the goal is to return machine-
translated results from one language in response to
a query from another (Lavrenko et al, 2002). We
operate under the assumption that for many mono-
lingual English queries (e.g., Han Feizi), there ex-
ist good documents in English. If we have Chinese
information as well, we can exploit it to help find
these documents. As we will see, machine trans-
lation can provide important predictive informa-
tion in our setting, but we do not wish to display
machine-translated output to the user.
We approach our problem by learning a rank-
ing function for bilingual queries ? queries that
are easily translated (e.g., with machine transla-
tion) and appear in the query logs of two languages
(e.g., English and Chinese). Given query logs
in both languages, we identify bilingual queries
with sufficient clickthrough statistics in both sides.
Large-scale aggregated clickthrough data were
proved useful and effective in learning ranking
functions (Dou et al, 2008). Using these statis-
tics, we can construct a ranking over pairs of docu-
ments, one from each language. We use this rank-
ing to learn a linear scoring function on pairs of
documents given a bilingual query.
We find that our bilingual rankings have good
monolingual ranking properties. In particular,
given an optimal pairwise bilingual ranking, we
show that simple heuristics can effectively approx-
imate the optimal monolingual ranking. Using
1075
1 10 100 1,000 10,000 50,0000
5
10
15
20
25
30
35
40
45
50
Frequency (# of times that queries are issued)
Pro
por
tion
 of 
bilin
gua
l qu
erie
s (%
)
 
 
English
Chinese
Figure 1: Proportion of bilingual queries in the
query logs of different languages.
these heuristics and our learned pairwise scoring
function, we can derive a ranking for new, unseen
bilingual queries. We develop and test our bilin-
gual ranker on English and Chinese with two large,
publicly available query logs from the AOL search
engine1 (English query log) (Pass et al, 2006)
and the Sougou search engine2 (Chinese query
log) (Liu et al, 2007). For both languages, we
achieve significant improvements over monolin-
gual Ranking SVM (RSVM) baselines (Herbrich
et al, 2000; Joachims, 2002), which exploit a va-
riety of monolingual features.
2 Bilingual Query Statistics
We designate a query as bilingual if the concept
has been searched by users of both two languages.
As a result, not only does it occur in the query log
of its own language, but its translation also appears
in the log of the second language. So a bilingual
query yields reasonable queries in both languages.
Of course, most queries are not bilingual. For ex-
ample, our English log contains map of Alabama,
but not our Chinese log. In this case, we wouldn?t
expect the Chinese results for the query?s transla-
tion, ?n?j?C, to be helpful in ranking the
English results.
In total, we extracted 4.8 million English
queries from AOL log, of which 1.3% of their
translations appear in Sogou log. Similarly, of our
3.1 million Chinese queries from Sogou log, 2.3%
of their translations appear in AOL log. By to-
tal number of queries issued (i.e., counting dupli-
1http://search.aol.com
2http://www.sogou.com
cates), the proportion of bilingual queries is much
higher. As Figure 1 shows as the number of times
a query is issued increases, so does the chance of
it being bilingual. In particular, nearly 45% of the
highest-frequency English queries and 35% of the
highest-frequency Chinese queries are bilingual.
3 Learning to Rank Using Bilingual
Information
Given a set of bilingual queries, we now de-
scribe how to learn a ranking function for mono-
lingual data that exploits information from both
languages. Our procedure has three steps: Given
two monolingual rankings, we construct a bilin-
gual ranking on pairs of documents, one from each
language. Then we learn a linear scoring function
for pairs of documents that exploits monolingual
information (in both languages) and bilingual in-
formation. Finally, given this ranking function on
pairs and a new bilingual query, we reconstruct a
monolingual ranking for the language of interest.
This section addresses these steps in turn.
3.1 Creating Bilingual Training Data
Without loss of generality, suppose we rank En-
glish documents with constraints from Chinese
documents. Given an English log Le and a Chi-
nese log Lc, our ranking algorithm takes as input
a bilingual query pair q = (qe, qc) where qe ? Le
and qc ? Lc, a set of returned English documents
{ei}Ni=1 from qe, and a set of constraint Chinese
documents {cj}nj=1 from qc. In order to create
bilingual ranking data, we first generate monolin-
gual ranking data from clickthrough statistics. For
each language-query-document triple, we calcu-
late the aggregated click count across all users and
rank documents according to this statistic. We de-
note the count of a page as C(ei) or C(cj).
The use of clickthrough statistics as feedback
for learning ranking functions is not without con-
troversy, but recent empirical results on large
data sets suggest that the aggregated user clicks
provides an informative indicator of relevance
preference for a query. Joachims et al (2007)
showed that relative feedback signals generated
from clicks correspond well with human judg-
ments. Dou et al (2008) revealed that a straight-
forward use of aggregated clicks can achieve a bet-
ter ranking than using explicitly labeled data be-
cause clickthrough data contain fine-grained dif-
ferences between documents useful for learning an
1076
Table 1: Clickthrough data of a bilingual query
pair extracted from query logs.
Bilingual query pair (Mazda,jH)
doc URL click #
e1 www.mazda.com 229
e2 www.mazdausa.com 185
e3 www.mazda.co.uk 5
e4 www.starmazda.com 2
e5 www.mazdamotosports.com 2
. . . . . .
c1 www.faw-mazda.com 50
c2 price.pcauto.com.cn/brand.
jsp?bid=17
43
c3 auto.sina.com.cn/salon/
FORD/MAZDA.shtml
20
c4 car.autohome.com.cn/brand/
119/
18
c5 jsp.auto.sohu.com/view/
brand-bid-263.html
9
. . . . . .
accurate and reliable ranking. Therefore, we lever-
age aggregated clicks for comparing the relevance
order of documents. Note that there is nothing
specific to our technique that requires clickthrough
statistics. Indeed, our methods could easily be em-
ployed with human annotated data. Table 1 gives
an example of a bilingual query pair and the ag-
gregated click count of each result page.
Given two monolingual documents, a prefer-
ence order can be inferred if one document is
clicked more often than another. To allow for
cross-lingual information, we extend the order of
individual documents into that of bilingual docu-
ment pairs: given two bilingual document pairs,
we will write
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
to indi-
cate that the pair of
(
e(1)i , c
(1)
j
)
is ranked higher
than the pair of
(
e(2)i , c
(2)
j
)
.
Definition 1
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
if and
only if one of the following relations hold:
1. C(e(1)i ) > C(e
(2)
i ) and C(c
(1)
j ) ? C(c
(2)
j )
2. C(e(1)i ) ? C(e
(2)
i ) and C(c
(1)
j ) > C(c
(2)
j )
Note, however, that from a purely monolingual
perspective, this definition introduces orderings on
documents that should not initially have existed.
For English ranking, for example, we may have(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
even when C(e(1)i ) =
C(e(2)i ). This leads us to the following asymmet-
ric definition of  that we use in practice:
Definition 2
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
if and
only if C(e(1)i ) > C(e
(2)
i ) and C(c
(1)
j ) ? C(c
(2)
j )
With this definition, we can unambiguously
compare the relevance of bilingual document pairs
based on the order of monolingual documents.
The advantages are two-fold: (1) we can treat mul-
tiple cross-lingual document similarities the same
way as the commonly used query-document fea-
tures in a uniform manner of learning; (2) with the
similarities, the relevance estimation on bilingual
document pairs can be enhanced, and this in return
can improve the ranking of documents.
3.2 Ranking Model
Given a pair of bilingual queries (qe, qc), we
can extract the set of corresponding bilin-
gual document pairs and their click counts
{(ei, cj), (C(ei), C(cj))}, where i = 1, . . . , N
and j = 1, . . . , n. Based on that, we produce a
set of bilingual ranking instances S = {?ij, zij},
where each ?ij = {xi;yj; sij} is the feature
vector of (ei, cj) consisting of three components:
xi = f(qe, ei) is the vector of monolingual rele-
vancy features of ei, yi = f(qc, cj) is the vector
of monolingual relevancy features of cj , and sij =
sim(ei, cj) is the vector of cross-lingual similari-
ties between ei and cj , and zij = (C(ei), C(cj))
is the corresponding click counts.
The task is to select the optimal function that
minimizes a given loss with respect to the order
of ranked bilingual document pairs and the gold.
We resort to Ranking SVM (RSVM) (Herbrich et
al., 2000; Joachims, 2002) learning for classifica-
tion on pairs of instances. Compared the base-
line RSVM (monolingual), our algorithm learns
to classify on pairs of bilingual document pairs
rather than on pairs of individual documents.
Let f being a linear function:
f~w(ei, cj) = ~wx ? xi + ~wy ? yj + ~ws ? sij (1)
where ~w = {~wx; ~wy; ~ws} denotes the weight vec-
tor, in which the elements correspond to the rele-
vancy features and similarities. For any two bilin-
gual document pairs, their preference relation is
measured by the difference of the functional val-
ues of Equation 1:
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
?
f~w
(
e(1)i , c
(1)
j
)
? f~w
(
e(2)i , c
(2)
j
)
> 0 ?
~wx ?
(
x(1)i ? x
(2)
i
)
+ ~wy ?
(
y(1)j ? y
(2)
j
)
+
~ws ?
(
s(1)ij ? s
(2)
ij
)
> 0
1077
We then create a new training corpus based on the
preference ordering of any two such pairs: S? =
{??ij, z
?
ij}, where the new feature vector becomes
??ij =
{
x(1)i ? x
(2)
i ;y
(1)
j ? y
(2)
j ; s
(1)
ij ? s
(2)
ij
}
,
and the class label
z?ij =
?
?
?
+1, if
(
e(1)i , c
(1)
j
)

(
e(2)i , c
(2)
j
)
;
?1, if
(
e(2)i , c
(2)
j
)

(
e(1)i , c
(1)
j
)
is a binary preference value depending on the or-
der of bilingual document pairs. The problem is to
solve SVM objective: min
~w
1
2?~w?
2 + ?
?
i
?
j ?ij
subject to bilingual constraints: z?ij ? (~w ? ?
?
ij) ?
1? ?ij and ?ij ? 0.
There are potentially ? = nN bilingual docu-
ment pairs for each query, and the number of com-
parable pairs may be much larger due to the com-
binatorial nature (but less than ?(? ? 1)/2). To
speed up training, we resort to stochastic gradient
descent (SGD) optimizer (Shalev-Shwartz et al,
2007) to approximate the true gradient of the loss
function evaluated on a single instance (i.e., per
constraint). The parameters are then adjusted by
an amount proportional to this approximate gradi-
ent. For large data set, SGD-RSVM can be much
faster than batch-mode gradient descent.
3.3 Inference
The solution ~w forms a vector orthogonal to the
hyper-plane of RSVM. To predict the order of
bilingual document pairs, the ranking score can
be simply calculated by Equation 1. However, a
prominent problem is how to derive the full order
of monolingual documents for output from the or-
der of bilingual document pairs. To our knowl-
edge, there is no precise conversion algorithm in
polynomial time. We thus adopt two heuristics for
approximating the true document score:
? H-1 (max score): Choose the maximum
score of the pair as the score of document,
i.e., score(ei) = maxj(f(ei, cj)).
? H-2 (mean score): Average over all the
scores of pairs associated with the ranked
document as the score of this document, i.e.,
score(ei) = 1/n
?
j f(ei, cj).
Intuitively, for the rank score of a single docu-
ment, H-2 combines the ?voting? scores from its n
constraint documents weighted equally, while H-1
simply chooses the maximum one. A formal ap-
proach to the problem is to leverage rank aggre-
gation formalism (Dwork et a., 2001; Liu et al,
2007), which will be left for our future work. The
two simple heuristics are employed here because
of their simplicity and efficiency. The time com-
plexity of the approximation is linear to the num-
ber of ranked documents given n is constant.
4 Features and Similarities
Standard features for learning to rank include vari-
ous query-document features, e.g., BM25 (Robert-
son, 1997), as well as query-independent features,
e.g., PageRank (Brin and Page, 1998). Our feature
space consists of both these standard monolingual
features and cross-lingual similarities among doc-
uments. The cross-lingual similarities are valu-
ated using different translation mechanisms, e.g.,
dictionary-based translation or machine transla-
tion, or even without any translation at all.
4.1 Monolingual Relevancy Features
In learning to rank, the relevancy between query
and documents and the measures based on link
analysis are commonly used as features. The dis-
cussion on their details is beyond the scope of this
paper. Readers may refer to (Liu et al, 2007)
for the definitions of many such features. We im-
plement six of these features that are considered
the most typical shown as Table 2. These include
sets of measures such as BM25, language-model-
based IR score, and PageRank. Because most con-
ventional IR and web search relevancy measures
fall into this category, we call them altogether IR
features in what follows. Note that for a given
bilingual document pair (e, c), the monolingual IR
features consist of relevance score vectors f(qe, e)
in English and f(qc, c) in Chinese.
4.2 Cross-lingual Document Similarities
To measure the document similarity across dif-
ferent languages, we define the similarity vector
sim(e, c) as a series of functions mapping a bilin-
gual document pair to positive real numbers. In-
tuitively, a good similarity function is one which
maps cross-lingual relevant documents into close
scores and maintains a large distance between ir-
relevant and relevant documents. Four categories
of similarity measures are employed.
Dictionary-based Similarity (DIC): For
dictionary-based document translation, we use
1078
Table 2: List of monolingual relevancy measures
used as IR features in our model.
IR Feature Description
BM25 Okapi BM25 score (Robertson, 1997)
BM25 PRF Okapi BM25 score with pseudo-
relevance feedback (Robertson and
Jones, 1976)
LM DIR Language-model-based IR score with
Dirichlet smoothing (Zhai and Lafferty,
2001)
LM JM Language-model-based IR score with
Jelinek-Mercer smoothing (Zhai and
Lafferty, 2001)
LM ABS Language-model-based IR score with
absolute discounting (Zhai and Lafferty,
2001)
PageRank PageRank score (Brin and Page, 1998)
the similarity measure proposed by Mathieu et
al. (2004). Given a bilingual dictionary, we let
T (e, c) denote the set of word pairs (we, wc) such
that we is a word in English document e, and wc
is a word in Chinese document c, and we is the
English translation of wc. We define tf(we, e)
and tf(wc, c) to be the term frequency of we in
e and that of wc in c, respectively. Let df(we)
and df(wc) be the English document frequency
for we and Chinese document frequency for
wc. If ne (nc) is the total number of English
(Chinese), then the bilingual idf is defined as
idf(we, wc) = log ne+ncdf(we)+df(wc) . Then the
cross-lingual document similarity is calculated by
sim(e, c) =
?
(we,wc)?T (e,c)
tf(we,e)tf(wc,c)idf(we,wc)2
?
Z
where Z is a normalization coefficient (see Math-
ieu et al (2004) for detail). This similarity func-
tion can be understood as the cross-lingual coun-
terpart of the monolingual cosine similarity func-
tion (Salton, 1998).
Similarity Based on Machine Translation
(MT): For machine translation, the cross-lingual
measure actually becomes a monolingual similar-
ity between one document and another?s transla-
tion. We therefore adopt cosine function for it di-
rectly (Salton, 1998).
Translation Ratio (RATIO): Translation ratio
is defined as two sets of ratios of translatable terms
using a bilingual dictionary: RATIO FOR ? what
percent of words in e can be translated to words in
c; RATIO BACK ? what percent of words in c can
be translated back to words in e.
URL LCS Ratio (URL): The ratio of longest
common subsequence (Cormen et al, 2001) be-
tween the URLs of two pages being compared.
This measure is useful to capture pages in different
languages but with similar URLs such as www.
airbus.com, www.airbus.com.cn, etc.
Note that each set of similarities above except
URL includes 3 values based on different fields of
web page: title, body, and title+body.
5 Experiments and Results
This section presents evaluation metric, data sets
and experiments for our proposed ranker.
5.1 Evaluation Metric
Commonly adopted metrics for ranking, such as
mean average precision (Buckley and Voorhees,
2000) and Normalized Discounted Cumulative
Gain (Ja?rvelin and Keka?la?inen, 2000), is designed
for data sets with human relevance judgment,
which is not available to us. Therefore, we
use the Kendall?s tau coefficient (Kendall, 1938;
Joachims, 2002) to measure the degree of correla-
tion between two rankings. For simplicity, let?s as-
sume strict orderings of any given ranking. There-
fore we ignore all the pairs with ties (instances
with the identical click count). Kendall?s tau is
defined as ?(ra, rb) = (P ? Q)/(P + Q), where
P is the number of concordant pairs and Q is the
number of disconcordant pairs in the given order-
ings ra and rb. The value is a real number within
[?1,+1], where ?1 indicates a complete inver-
sion, and +1 stands for perfect agreement, and a
value of zero indicates no correlation.
Existing ranking techniques heavily depend on
human relevance judgment that is very costly to
obtain. Similar to Dou et al(2008), our method
utilizes the automatically aggregated click count in
query logs as the gold for deriving the true order
of relevancy, but we use the clickthrough of dif-
ferent languages. We average Kendall?s tau values
between the algorithm output and the gold based
on click frequency for all test queries.
5.2 Data Sets
Query logs can be the basis for constructing high
quality ranking corpus. Due to the proprietary
issue of log, no public ranking corpus based on
real-world search engine log is currently avail-
able. Moreover, to build a predictable bilingual
ranking corpus, the logs of different languages are
needed and have to meet certain conditions: (1)
they should be sufficiently large so that a good
number of bilingual query pairs could be identi-
1079
Table 3: Statistics on AOL and Sogou query logs.
AOL(EN) Sogou(CH)
# sessions 657,426 5,131,000
# unique queries 10,154,743 3,117,902
# clicked queries 4,811,650 3,117,590
# clicked URLs 1,632,788 8,627,174
time span 2006/03-05 2006/08
size 2.12GB 1.56GB
fied; (2) for the identified query pairs, there should
be sufficient statistics of associated clickthrough
data; (3) The click frequency should be well dis-
tributed at both sides so that the preference order
between bilingual document pairs can be derived
for SVM learning.
For these reasons, we use two independent and
publicly accessible query logs to construct our
bilingual ranking corpus: English AOL log3 and
Chinese Sogou log4. Table 3 shows some statis-
tics of these two large query logs.
We automatically identify 10,544 bilingual
query pairs from the two logs using the Java
API for Google Translate5, in which each query
has certain number of clicked URLs. To bet-
ter control the bilingual equivalency of queries,
we make sure the bilingual queries in each of
these pairs are bi-directional translations. Then
we download all their clicked pages, which re-
sults in 70,180 English6 and 111,197 Chinese doc-
uments. These documents form two independent
collections, which are indexed separately for re-
trieval and feature calculation.
For good quality, it is necessary to have suffi-
cient clickthrough data for each query. So we fur-
ther identify 1,084 out of 10,544 bilingual query
pairs, in which each query has at least 10 clicked
and downloadable documents. This smaller col-
lection is used for learning our model, which con-
tains 21,711 English and 28,578 Chinese docu-
ments7. In order to compute cross-lingual doc-
ument similarities based on machine translation
3http://gregsadetsky.com/aol-data/
4http://www.sogou.com/labs/dl/q.html
5http://code.google.com/p/
google-api-translate-java/
6AOL log only records the domain portion of the clicked
URLs, which misleads document downloading. We use the
?search within site or domain? function of a major search en-
gine to approximate the real clicked URLs by keeping the
first returned result for each query.
7Because Sogou log has a lot more clicked URLs, for bal-
ancing with the number of English pages, we kept at most 50
pages per Chinese query.
Table 4: Kendall?s tau values of English ranking.
The significant improvements over baseline (99%
confidence) are bolded with the p-values given in
parenthesis. * indicates significant improvement
over IR (no similarity). n = 5.
Models Pair H-1 (max) H-2 (mean)
RSVM (baseline) n/a 0.2424 0.2424
IR (no similarity) 0.2783 0.2445 0.2445
IR+DIC 0.2909 0.2453 0.2496
IR+MT 0.2858
0.2488* 0.2494*
(p=0.0003) (p=0.0004)
IR+DIC+MT 0.2901 0.2481
0.2514*
(p=0.0009)
IR+DIC+RATIO 0.2946 0.2466
0.2519*
(p=0.0004)
IR+DIC+MT
+RATIO
0.2940
0.2473* 0.2539*
(p=0.0009) (p=1.5e-5)
IR+DIC+MT
+RATIO+URL
0.2979
0.2533* 0.2577*
(p=2.2e-5) (p=4.4e-7)
(see Section 4.2), we automatically translate all
these 50,298 documents using Google Translate,
i.e., English to Chinese and vice versa. Then the
bilingual document pairs are constructed, and all
the monolingual features and cross-lingual simi-
larities are computed (see Section 4.1&4.2).
5.3 English Ranking Performance
Here we examine the ranking performance of our
English ranker under different similarity settings.
We use traditional RSVM (Herbrich et al, 2000;
Joachims, 2002) without any bilingual considera-
tion as the baseline, which uses only English IR
features. We conduct this experiment using all the
1,084 bilingual query pairs with 4-fold cross vali-
dation (each fold with 271 query pairs). The num-
ber of constraint documents n is empirically set as
5. The results are shown in Table 4.
Clearly, bilingual constraints are helpful to
improve English ranking. Our pairwise set-
tings unanimously outperforms the RSVM base-
line. The paired two-tailed t-test (Smucker et
al., 2007) shows that most improvements resulted
from heuristic H-2 (mean score) are statistically
significant at 99% confidence level (p<0.01). Rel-
atively fewer significant improvements can be
made by heuristic H-1 (max score). This is be-
cause the maximum score on pair is just a rough
approximation to the optimal document score. But
this simple scheme works surprisingly well and
still consistently outperforms the baseline.
Note that our bilingual model with only IR fea-
tures, i.e., IR (no similarity), also outperforms the
baseline. The reason is that in this setting there are
1080
1 2 3 4 5 6 7 8 9 100.23
0.235
0.24
0.245
0.25
0.255
0.26
# of constraint documents in a different language
Ke
nda
ll?s 
tau
 
 
RSVM (baseline)
IR+DIC
IR+MT
IR+DIC+MT
IR+DIC+RAIO+MT
IR+DIC+RAIO+MT+URL
Figure 2: English ranking results vary with the
number of constraint Chinese documents.
IR features of n Chinese documents introduced in
addition to the IR features of English documents
in the baseline.
The DIC similarity does not work as effectively
as MT. This may be due to the limitation of bilin-
gual dictionary alone for translating documents,
where the issues like out-of-vocabulary words and
translation ambiguity are common but can be bet-
ter dealt with by MT. When DIC is combined with
RATIO, which considers both forward and back-
ward translation of words, it can capture the corre-
lation between bilingually very similar pages, thus
performs better.
We find that the URL similarity, although sim-
ple, is very useful and improves 1.5?2.4% of
Kendall?s tau value than not using it. This is be-
cause the URLs of the top Chinese (constraint)
documents are often similar to many of returned
English URLs which are generally more regu-
lar. For example, in query pair (Toyota Camry,
TProceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 872?883, Dublin, Ireland, August 23-29 2014.
Utilizing Microblogs for Automatic News Highlights Extraction
Zhongyu Wei
The Chinese University of Hong Kong
Shatin, N.T.
Hong Kong
zywei@se.cuhk.edu.hk
Wei Gao
Qatar Computing Research Institute
Qatar Foundation
Daha, Qatar
wgao@qf.org.qa
Abstract
Story highlights form a succinct single-document summary consisting of 3-4 highlight sentences
that reflect the gist of a news article. Automatically producing news highlights is very challeng-
ing. We propose a novel method to improve news highlights extraction by using microblogs. The
hypothesis is that microblog posts, although noisy, are not only indicative of important pieces of
information in the news story, but also inherently ?short and sweet? resulting from the artificial
compression effect due to the length limit. Given a news article, we formulate the problem as two
rank-then-extract tasks: (1) we find a set of indicative tweets and use them to assist the ranking
of news sentences for extraction; (2) we extract top ranked tweets as a substitute of sentence ex-
traction. Results based on our news-tweets pairing corpus indicate that the method significantly
outperform some strong baselines for single-document summarization.
1 Introduction
People in this era are overloaded by their daily exposure to large amount of online information. To make
life easier, some news websites like CNN.com and USAToday.com provide ?Story Highlights? in their
news articles for readers to get the gist of story quickly. The highlights of an article typically contain
3-4 summary sentences in bullet-points form that are representative of and shorter than the original new
sentences in the article. An example of story highlights of an article is shown in Figure 1 (marked in red
rectangle) that are written in a compact, almost telegraphic style. In contrast to the original content of
the article, significant compression is obtained by shortening and paraphrasing.
Unfortunately, the production of such good-quality highlights needs to be done manually which is
very expensive. Existing methods face grand technical challenges for automating the process. The task is
complex in nature due to a broad range of linguistic constraints which ultimately requires wide-coverage
of language understanding beyond the capabilities of current NLP technology (Woodsend and Lapata,
2010). Most automatic systems simplify the problem using extractive approach. By using linguistic
or statistical information or both, the key units or concepts can be identified from sentences or across
multiple documents, and then the sentences are scored and extracted according to their informativeness
with the presence of the key components.
The extractive approach has two salient problems: (1) it is commonly ineffective to locate key sen-
tences, meaning that the presence of linguistically and/or statistically important units does not necessarily
indicate a highlight sentence. This is evidenced by the fact that sophisticated systems for Document Un-
derstanding Conference (DUC) summarization task cannot significantly outperform a trivial baseline that
simply selects first n sentences of the document (Nenkova, 2005); (2) sentence extracts as highlights are
extraordinarily verbose in general, which need to be post-processed for substantial compression. But
sentence compression may breach the readability or grammaticality (Clarke and Lapata, 2008).
With the popularity of social media, online news providers are moving towards offering more inter-
action with news readers via microblogging service like Twitter. Many Twitter users also post tweets
Work conducted at Qatar Computing Research Institute (QCRI) when the first author was employed as an intern
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
872
Figure 1: A CNN news article with story highlights (Highlights are marked by red rectangle, and the
news sentences related to the highlights are enclosed in green rectangles) and some relevant tweets one
can observe independently on Twitter (marked by light blue rectangles on the left)
about news together with their URLs. Such increased cross-media interaction recasts the role of different
information sources that are useful for this task in a sense that interesting correlations between the news
and relevant microblogs could be captured and leveraged to boost the performance.
To address these considerations, we make two hypotheses based on our observation that can be crucial
to highlights extraction. (1) Indicative effect: microblog users? mentioning about the pieces of news is
indicative of the importance of the corresponding sentences; (2) Human compression effect: important
portions of a news article have been rewritten by microblog users in a more condensed style owing to
length limit. Accordingly, we formulate our problem as two independent rank-then-extract tasks: firstly,
we find a set of indicative tweets and use them to assist the ranking of news sentences for extraction;
secondly, we extract top-ranked tweets (with the help of news sentences) as a substitute of sentences
extraction since they are typically shorter. Based on our news-tweets pairing corpus, the results of ex-
periments following both directions indicate that our methods outperform some strong baselines for
single-document summarization.
2 Related Work
Our work intersects the summarization of single document and microblogs. Single-document summa-
rization has been studied for years starting from Luhn and Peter (1958). Based on local content informa-
tion of a document (Wong et al., 2008; Barzilay et al., 1997; Marcu, 1997), researchers proposed various
statistical or semantic approaches using classification (Wong et al., 2008), Integer Linear Programming
(ILP) (Li et al., 2013), sequential models (Shen et al., 2007) and graphical models (Litvak and Last,
2008; Hirao et al., 2013). For the concision of summary, sentence compression or word deletion was
used (Knight and Marcu, 2002) for preprocessing. Joint models combining compression and selection
of sentences were also studied (Woodsend and Lapata, 2010; Li et al., 2013).
Summarizing microblog content is to distill the large quantities of tweets into a concise and represen-
tative description of a target event. Sharifi et al. (2010) proposed a graph-based phrase reinforcement
873
algorithm (PRA) to generate a one-sentence summary from a collection of tweets. By using linguistic
features, Judd and Kalita (2013) improved the performance of PRA. Sharifi et al. (2010) and Inouye
et al. (2011) presented a hybrid TF-IDF approach for extracting tweets with the presence of important
terms. More fine-grained summarization was proposed by considering sub-events and combining the
summaries extracted from each sub-topic (Nichols et al., 2012; Zubiaga et al., 2012; Duan et al., 2012).
The research for coupling news and microblogs attracted much attention recently. Suba?si?c and
Berendt (2011) and Zhao et al. (2011) independently compared tweets to online news to identify fea-
tures for news detection in tweets. Phelan et al. (2011) used tweets to recommend news articles based on
user preferences. Gao et al. (2012) produced cross-media news summaries by capturing the complemen-
tary information from both sides. Kothari et al. (2013) and
?
Stajner et al. (2013) investigated detecting
news comments from Twitter for extending news information provided. Guo et al. (2013) proposed a
graphical model to identify news for a given tweet to provide contextual support for NLP tasks.
Some work attempted to use different kinds of resources to help document summarization, such as
Wikipedia and query log of search engine (Svore et al., 2007), clickthrough data (Sun et al., 2005),
users? comments on news (Hu et al., 2008), and social media context of the articles (Yang et al., 2011).
Our work is closely related to Svore et al. (2007) that considered incorporating third-party resource in
the ranking process, but the access to query logs is extremely limited, and Wikipedia content is relatively
static which cannot reflect timely information like social media.
We also share the same testbed with Woodsend and Lapata (2010). They selected and compressed
news sentences with a joint model using ILP by considering phrase as basic extract element. Their
method requires a large training corpus for deriving accurate salient scores of phrases, and also the
feasible solution of ILP model with hard constraints does not necessarily exist.
Yang et al. (2011) proposed a unified supervised model called dual wing factor graph to simultaneously
summarize Web documents and tweets based on structural mining from social context. Despite of similar
motivation, our work has some key differences from theirs: (1) Our ground-truth come from standard
news highlights, and our target summary keeps consistent no matter which source of information our
highlights are extracted from. They built ground-truth summaries separately for each side by manually
choosing no less than 5 tweets and 10 news sentences. So, our standard is more difficult to reach since
our ground-truth summaries are not extracts of the original sentences or tweets; (2) Our approach is very
different. We use ranking-based algorithm which is more adequate than their classification approach
because there are much fewer positive candidates than negative ones, and the class distribution is very
imbalanced (like information retrieval tasks). Also, they were focused on mining the implicit structural
information from retweeting and user following networks, while we focus on content-based correlations.
3 Corpus Construction
There is no news-tweets coupling data set publicly available for the purpose of news highlights produc-
tion
1
. We constructed the first of such corpus for this application by our own, for which an event-oriented
strategy was adopted to collect the highlights-document-tweets couplings by using a social search en-
gine. We manually identified 17 salient news events taking place in recent two years. For each event,
we manually generated a set of core queries which were used to retrieve the relevant tweets via Topsy
2
search API. Then we gathered the retrieved tweets containing embedded URLs that point to the news
articles on CNN and USAToday websites that provide story highlights, and extracted the content of the
news articles and the associated highlights.
For each article, we collected all the tweets in the retrieved tweet set above that contain links to the
article to form our highlights-document-tweets couplings based on the following rules: (1) We delete
those extremely short tweets with less than 5 tokens and the tweets that are suspected copies from news
title and highlights. For example, we try our best to remove all the suspectable tweets including the cases
1
We realize the news-tweets coupling data set released recently for NLP tasks by Guo et al. (Guo et al., 2013). However,
this data set is not suitable for our task for two reasons: (1) There are 12,704 news articles but only 34,888 tweets. Although
part of the news are from CNN which contain story highlights, the number of tweets per article is too limited, not to mention
finding useful candidates; (2) The full text of news content is not provided, with only the first few sentences of articles instead.
2
http://topsy.com
874
Documents Highlights Tweets
Total # 121 455 78,419
Sentence # per news 53.6?25.6 3.7?0.4 648.1?1161.7
Token # per news 1123.0?495.8 49.6?10.0 10364.5?24749.2
Token # per sentence 21.0?11.6 13.2?3.2 16.0?5.3
Table 1: Overview statistics on the corpus (mean and standard deviation)
Event Doc # Highlight # Tweet # Event Doc # Highlight # Tweet #
Aurora shooting 14 54 12,463 African runner murder 8 29 9,461
Boston bombing 38 147 21,683 Syria chemical weapons use 1 4 331
Connecticut shooting 13 47 3,021 US military in Syria 2 7 719
Edward Snowden 5 17 1,955 DPRK Nuclear Test 2 8 3,329
Egypt balloon crash 3 12 836 Asiana Airlines Flight 214 11 42 8,353
Hurricane Sandy 4 15 607 Moore Tornado 5 19 1,259
Russian meteor 3 11 6,841 US Flu Season 7 23 6,304
Chinese Computer Attacks 2 8 507 Williams Olefins Explosion 1 4 268
cause of the Super Bowl blackout 2 8 482 Total 121 455 78,419
Table 2: Distribution of documents, highlights and tweets with respect to different events
like ?RT @someone HIGHLIGHT URL?; (2) If there are more than 100 tweets linked to an article, the
article is kept, otherwise the artcile is removed. Note that using explicit hyperlinks is not the only way for
identifying the couplings but the most straightforward one. Here we simply resort to this straightforward
method to build the corpus for verifying our two hypotheses raised in Section 1. Thorough investigation
on the construction of an enhanced highlights-oriented coupling corpus is left for our future work.
The statistics of the resulted corpus are given in Table 1 which is also made accessible
3
. As shown in
the table, the average number of relevant tweets to a document is about 648. Since some of the events
are much more popular than others, the standard deviation of the number of tweets associated with a
document is as high as 1,162. The highlights are characterized as high compression rate compared to the
length of news articles. In addition, a single highlight sentence on average is only 2/3 the length of a news
sentence, and more interestingly the average length of tweets is very close to that of highlight sentences,
which suggests that the relevant tweets can be a reasonable source of candidates for extraction.
Table 2 shows the distribution of documents, highlights and tweets with respect to the 17 news events
we collected.
4 Our Approach
Given a news article containing n sentences S = {s
1
, s
2
, ...s
n
} and a set of m relevant tweets T =
{t
1
, t
2
, ..., t
m
}, we aim to extract x sentences from the set S or the same number of tweets from set T as
highlights covering the main theme of the article. We define the two tasks as follows:
? Task 1 ? sentences extraction: Given auxiliary T , extract x elements H(S) =
{s
(1)
, s
(2)
, ..., s
(x)
|s
(i)
? S, 1 ? i ? x} from S as highlights.
? Task 2 ? tweets extraction: Given auxiliary S, extract x elementsH(T ) = {t
(1)
, t
(2)
, ..., t
(x)
|t
(i)
?
T, 1 ? i ? x} from T as highlights.
Most single-document summarization methods (Woodsend and Lapata, 2010; Yang et al., 2011) treat
the extraction as a classification problem which assigns either positive or negative label to the extract
candidates. We argue that it is more adequate to model it as a ranking problem because there is far
more unsuitable candidates than suitable ones for being the highlights. Such kind of imbalanced class
distribution makes classification a secondary solution.
Our model learns to rank all the candidate sentences in task 1 or candidate tweets in task 2, and then
extracts the top-x ranked instances as output highlights. We adopt an effective pair-wise ranking model
RankBoost (Freund et al., 2003) for that using the RankLib package
4
. RankBoost takes pairs of instances
3
http://www1.se.cuhk.edu.hk/
?
zywei/data/hilightextraction.zip
4
http://sourceforge.net/p/lemur/wiki/RankLib/
875
Category Name Description
Local Sentence Feature (LSF)
IsFirst Whether s is the first sentence in the news
Pos The position of s in the news
TitleSimi Token overlap between s and news title
ImportUnigram Importance score of s according to the unigram distribution in the news
ImportBigram Importance score of s according to the bigram distribution in the news
Local Tweet Feature (LTF)
Length Token number in t
HashTag HashTag related features (presence and count)
URL URL related features (count)
Mention Mention related features (presence and count)
ImportTFIDF Importance score of t based on unigram Hybrid TF-IDF algorithm (Sharifi et al., 2010)
ImportPRA Importance score of t based on phrase reinforcement algorithm (Sharifi et al., 2010)
TopicNE Named entity related features (NE count and seven binary values indicating the presence of each category)
TopicLDA LDA-based topic model features (maximum relevance with sub-topics, etc.)
QualityOOV Out-of-vocabulary words related features (count and percentage)
QualityLM Quality score of t according to language model (Unigram, bigram and trigram)
QualityDepend Quality score of t according to dependency bank (Han and Baldwin, 2011)
Cross-Media Feature (CCF)
MaxCosine Maximum cosine value between the target instance and auxiliary instances
MaxROUGE1F Maximum ROUGE-1 F score between the target instance and auxiliary instances
MaxROUGE1P Maximum ROUGE-1 precision value between the target instance and auxiliary instances
MaxROUGE1R Maximum ROUGE-1 recall value between the target instance and auxiliary instances
LeadSenSimi
?
ROUGE-1 F score between leading news sentences and t
TitleSimi
?
ROUGE-1 F score between news title and t
MaxSenPos
?
The position of sentences that obtain maximum ROUGE-1 F score with t
SimiUnigram Similarity based on the distribution of (local) unigram frequency in the auxiliary resource
SimiUniTFIDF Similarity based on the distribution of (local) unigram TF-IDF in the auxiliary resource
SimiTopEntity Similarity based on the (local) presence and count of most frequent entities in the auxiliary resource
SimiTopUnigram Similarity based on the (local) presence and count of most frequent unigrams in the auxiliary resource
Table 3: Feature description (t: a tweet; s: a news sentence; *: features used in task 2 only)
(I
i
, I
j
) as input for training and their preference order as labels. In our case, instance pair can be the pair
of sentences or tweets, and the pairwise order is determined by the salient score of each instance that is
the maximum ROUGE-1 (Lin, 2004) F-value between the instance and the corresponding ground-truth
highlight sentences. Given the gold standard highlights H
g
= {h
1
, h
2
, ..., h
x
}, the salient score of an
instance is calculated as score(I
i
) = max
k
{ROUGE-1(I
i
, h
k
)}.
Note that in task 2 the number of tweets pairs generated in training can be extremely large because
of the number of tweets in popular topical news articles (see Table 2) that may degrade the efficiency of
training. Some ad-hoc workaround is employed to make the problem tractable. As opposed to using all
the possible pairs, we divide the tweets into b bins, where the bins are bounded by continuous ranges of
salient scores. We fix the length of different ranges by fitting the distributions of salient score values.
Tuned on a subset with 20% randomly selected training instances, the value of b is determined as 4.
Then, the pairs are formed across these brackets.
5 Feature Design
The feature space of the two tasks are designed to intersect at the cross-media correlation part. The local
features describe the instance to be ranked (i.e., either a news sentence or a tweet), and the cross-media
correlation features capture the similarity of the instance with the counterparts in the auxiliary resource.
The features consist of three subsets of informativeness measures including local sentence features
(LSF), local tweet features (LTF) and cross-media correlation features (CCF). In task 1, we can use
LSF or both LSF and CCF for rank learning; and in task 2, we can use LTF or combine LTF and CCF.
The full feature list is described in Table 5. For local sentence features, we implement the 5 document
features defined in (Svore et al., 2007) for single-document summarization task. This is for the ease of
comparison with the existing approach. In this section, we will only describe the local tweet features and
the cross-media correlation features in more detail.
5.1 Local Tweet Features
Local tweet features are proposed to capture the importance of a tweet based on local information in
three aspects, including twitter-specific, topic-related, and writing-quality measures.
876
5.1.1 Twitter-specific measures
Twitter-specific features indicate the basic content-based characteristics of a tweet such as length, the
characteristics specifically provided by Twitter platform such as hashtags, mentions and embedded urls,
and two scoring functions used by state-of-the-art tweet summarization algorithms including Hybrid TF-
IDF (Sharifi et al., 2010) and PRA (Sharifi et al., 2010). Hybrid TF-IDF is a variant of traditional TF-IDF
weighting for tweets collection which treats each tweet as a document when computing IDF while the
whole tweets set as a document when computing TF. We calculate the feature ImportTFIDF of a tweet
based on the TF and IDF values of its tokens. PRA is a phrase reinforcement algorithm that can produce
a one-sentence summary for a given tweets set. We follow the idea of PRA to generate the token graph
of our tweets set and compute the weight for each token node. We then measure the importance of a
tweet by summating the weights of all its tokens, which becomes the ImportPRA feature.
5.1.2 Topic-related measures
Topic-related features are used to capture important tweets based on the topical information embodied
by named entities (NE) or latent topic semantics. TopicNE is proposed to utilize NE as indicator for
describing an event. We resort to Stanford Name Entity Recognizer
5
to extract seven types of named
entities including time, location, organization, person, money, percent and date. Based on that, we count
entities in the tweet, and then obtain seven additional binary values indicating the presence of each
category. TopicLDA is used to capture sub-topics. Intuitively, if a tweet is highly related to some sub-
topic in the event, it is more important. We use LDA (Blei et al., 2003) to identify the sub-topics in
the tweets set. Based on the resulted sub-topics and term distribution, we first calculate the maximum
relevance value between the tweet and all sub-topics as a feature. Then, we obtain the distribution of
relevance values of the tweet with respect to all sub-topics and compute the entropy of this distribution
as another feature. The lower the entropy is, the higher the degree of topical concentration for the tweet.
We use the default setting of the toolkit mallet
6
and set the number of sub-topics as 10 empirically.
5.1.3 Writing-quality measures
Writing-quality features indicate if a tweet is written in a formal way. Intuitively if more formally a
tweet is written, it is more likely to be extracted. QualityOOV measures to what extent a tweet contains
out-of-vocabulary (OOV) tokens. We simply calculate the number and the percentage of the OOV words
in the tweet as features
7
. QualityLM measures writing quality of a tweet based on language model.
We train uni-gram, bi-gram and tri-gram language models using maximum-likelihood estimation. By
summating the probabilities of all the tokens in the tweet regarding the three different language models,
we obtain three n-gram-based writing-quality features. QualityDepend measures the writing quality
based on dependency relation. The dependency feature is generated following Han et al. (2011). Instead
of using the technique for normalizing tweet text, we apply it for assessing the grammaticality of tweets
8
.
5.2 Cross-media Correlation Features
We observe that Twitter users like to quote or rewrite the important pieces of new content in the posts.
If a news sentence is referred or paraphrased by many tweets, it is assumed to be indicated as more
important. On the other hand, a tweet, besides its local importance indicator, may be more important if
it is similar to the theme of the news content. Therefore, cross-media correlation features are designed
to incorporate the auxiliary information source for helping instance ranking. In task 1, news articles are
local content and the corresponding tweets are considered auxiliary, and in task 2 their roles are reversed.
5.2.1 Instance-level similarities
Instance-level similarities indicate if there are auxiliary instances similar to the current local instance
and to what extent they are similar. These features reveal if the current instance has strong correlation
5
http://nlp.stanford.edu/software/CRF-NER.shtml
6
http://mallet.cs.umass.edu/index.php
7
The words not found in a common English dictionary, GNU aspell dictionary v0.60.6, are treated as OOV
8
Both dependency bank and language model here are based on New York Times corpus (http://www.ldc.upenn.
edu/Catalog/CatalogEntry.jsp?catalogId=LDC2008T19)
877
across the media boundary. We use four general metrics including cosine, ROUGE-1 F-value, ROUGE-1
precision score and ROUGE-1 recall score to measure the surface similarity between news sentence and
tweet. And the other three features, namely LeadSenSimi, TitleSimi and MaxSenPos are only used in task
2 for ranking tweets when news sentences are considered as auxiliary. This is because leading sentences
and title of news are considered as the most informative content. The more similar a tweet to them, the
more important it can be. Also, position information is often used for document summarization. We
borrow the position of the most similar sentence as bridge to measure the importance of a given tweet.
5.2.2 Semantic-space-level similarities
Semantic-space-level similarities reflect the importance of the current local instance based on the distri-
bution of its semantic units in the auxiliary resource. We propose two features to represent the distribution
of the semantic units that are based on unigram frequency and unigram TF-IDF, and named as SimiU-
nigram and SimiUniTFIDF, respectively. We first obtain a unigram distribution on the auxiliary space,
and compute the similarity of a local instance by summing over the probabilities of all its unigrams in
the distribution. Additionally, we also identify some most frequent named entities and unigrams in the
auxiliary information source, and then compute the presence and the count of them in the current local
instance as additional features, which are named as SimiTopEntity and SimiTopUnigram.
6 Experiments and Results
6.1 Setup
Task 1 extracts highlights from news articles. For comparison, we use the following approaches: (1)
Lead sentence chooses the first x sentences from the given news article, which is a strong baseline
that no DUC system could beat with large margin (Nenkova, 2005); (2) Phrase ILP (Woodsend and
Lapata, 2010) generates highlights from news with the joint model combining sentence compression
and selection, which treats phrases and clauses as extract unit; (3) Sentence ILP (Woodsend and Lapata,
2010) is a variant of Phrase ILP that treats sentence as extract unit; (4) LexRank (news) summarizes
the given news using the typical multi-document summarization algorithm LexRank (Erkan and Radev,
2004); (5) Ours (LSF) is our ranking method based on the local sentence features which are equivalent to
the features used by Svore et al. (2007); (6) Ours (LSF+CCF) is our method combining LSF and CCF.
Task 2 extracts highlights from tweets where we use the following approaches: (1) LexRank (tweets)
uses LexRank (Erkan and Radev, 2004) with tweets as the mere input; (2) Ours (LTF) is our ranking
method based on local tweet features; (3) Ours (LTF+CCF) is our method combining LTF and CCF.
Unlike single news document where redundant sentences are rare, the redundancy of tweets is serious.
Many summarization algorithms are sensitive to redundancy in the input. It is thus problematic for
tweets as the source of extraction. Hence we apply Maximal Marginal Relevance (MMR) (Carbonell
and Goldstein, 1998) for reducing tweets redundancy in task 2. The parameter in MMR used to gauge
the threshold of redundancy is tuned based on 20% randomly selected training data. Overall, we conduct
5-fold cross-validation for evaluation. The highlights of each news article are used as ground truth. In
the output, we fix the number of highlights extracted x as 4. We report ROUGE-1 and ROUGE-2 scores
with ROUGE-1 as the major evaluation metric.
6.2 Results
The overall performance can be seen in Table 4, from which we have the following findings:
? Indeed, Lead sentence is a very strong baseline that performs much better than most of other meth-
ods. It is only a little worse than LexRank (news) and much worse than Ours (LSF+CCF).
? LexRank (news) performs the second best in task 1. However, the performance of LexRank (tweets)
is the worst in task 2. This is because LexRank is proposed for summarizing regular documents and its
performance is affected seriously by the short, noisy texts like tweets.
? Sentence ILP and Phrase ILP perform similarly and do not show clear advantage over other base-
lines. This is different from what Woodsend and Lapata (2010) has obtained. This implies that their
model is sensitive to the size of training data where the ILP model may be undertrained here with the
878
Approach
ROUGE-1 ROUGE-2
F P R F P R
Lead sentence 0.263 0.211 0.374 0.101 0.080 0.147
LexRank (news) 0.264 0.226 0.332 0.088 0.074 0.112
Sentence ILP 0.238 0.209 0.293 0.068 0.058 0.088
Phrase ILP 0.236 0.215 0.281 0.069 0.061 0.086
Ours (LSF) 0.256 0.214 0.345 0.093 0.076 0.129
Ours (LSF+CCF) 0.292 0.239 0.398 0.110 0.089 0.155
LexRank (tweets) 0.212 0.204 0.226 0.064 0.061 0.068
Ours (LTF) 0.264 0.280 0.274 0.095 0.106 0.098
Ours (LTF+CCF) 0.295 0.320 0.295 0.105 0.118 0.105
Table 4: Overall performance (Bold: best performance of the task; Underlined: significance (p < 0.01)
compared to our best model; Italic: significance (p < 0.05) compared to our best model)
amount of training data available. In addition, we find there are lots of infeasible solutions for the ILP
model, indicating that the hard constraints are not relaxed enough for the relatively small data set.
? Ours (LSF+CCF) and Ours (LTF+CCF) achieve the best performance on task1 and task2, respec-
tively, and they significantly outperform all other methods in terms of ROUGE-1 F-score based on the
result of paired two-tailed t-test. By incorporating CCF, we improve the performance of local features
significantly. This justifies that cross-media correlations are indeed useful for improving the quality of
exaction from both directions.
? Comparing Ours (LSF+CCF) and Ours (LTF+CCF), although their ROUGE-1 F-scores are compa-
rable, the former is better on ROUGE-1 recall and the ROUGE-1 precision of the latter is much higher.
This is because news sentences are usually longer than tweets. So the highlights extracted from news
article cover more highlight tokens than those from tweets. The length of generated summary and ground
truth can be seen in Table 5, where tweet extracts are much closer to the ground-truth highlights. And
tweets appear to be a more suitable source for highlights extraction because of the human compression
effect on the tweets.
Tokens # per sentence Tokens # per summary
Ground-truth highlights 13.2?3.2 49.6?10.0
Ours (LSF+CCF) 24.3?11.8 91.3?18.4
Ours (LTF+CCF) 16.1?5.4 55.3?16.1
Table 5: Comparison of the length of extracted highlights and that of ground truth
6.3 Analysis
Table 6 shows an example for analyzing our extracted highlights compared to the ground-truth. In
example 1 (left column), with the help of tweets, Ours (LSF+CCF) can output good highlight sentences
N2 and N3 which cannot be extracted by Ours (LSF). On the side of tweets, T2 is newly extracted by
Ours (LTF+CCF) after considering CCF. Furthermore, highlights extracted from tweets also bring extra
good highlight T3 which is similar to H1. We find that H1 is rewritten from an original sentence which
is three times longer, so it is difficult for extractive method to locate the original sentence in the article.
Even if the sentence could be identified, the information was verbose still. Interestingly, some Twitter
user produces a tweet like T3 by paraphrasing and shortening which is captured by the algorithm.
Although cross-media correlations are helpful, two out of four ground-truth highlight sentences are
covered by the extracted good highlights in example 1. Also, the good extracts from different sources
may not cover the same set of ground-truth. Therefore, maybe we can try to combine the extracts from
both sides for further improvement.
879
1: Positive example 2: Negative example
H1. Luxor province bans all hot air balloon flights until further notice HH1. Snowden grew up in Elizabeth City, N.C., but family moved to Ellicott
City, Md.
H2. The Tuesday accident was the world?s deadliest hot air balloon accident
in at least 20 years
HH2. In 2003, he enlisted in the Army, but broke both his legs during Special
Forces training
H3. Officials: Passengers in the balloon included 19 foreign tourists HH3. His first NSA job was as a security guard at an agency facility at the
University of Maryland
H4. No foul play is suspected, official says
N1. Cairo An official investigation into the cause of a balloon accident that
killed 19 people in Egypt could take two weeks, ...
NN1. A 29-year-old former CIA employee who admitted responsibility Sun-
day for one of the most extraordinary ...
N2. [+] The Tuesday accident was the world?s deadliest hot air balloon
accident in at least 20 years.
NN2. He told the newspaper he is willing to stand behind his actions in
public because ?I know I have done nothing wrong.?
N3. [+] Tuesday?s crash prompted the governor to ban all hot air balloon
flights until further notice.
NN3. He told the newspaper that the NSA ?routinely lies? to Congress about
the scope of its surveillance in the United States.
N4. How safe is hot air ballooning? NN4. [+] I can?t in good conscience allow the U.S. government to destroy
privacy, internet freedom and basic...
NN. [-] His first NSA job was as a security guard at an agency facility at
the University of Maryland in College Park, ...
T1. CNN: official investigation into yesterday air balloon accident in Luxor
could take 2 weeks
TT1. I can?t in good conscience allow the U.S. government to destroy pri-
vacy, Snowden told the Guardian.
T2. [+] Governor bans all hot air balloon flights until further notice. TT2. whistleblower Edward Snowden: I do not expect to see home again,
though that is what I want.
T3. Foul play not suspected in fatal balloon accident TT3. More on ex CIA Snowden: I have done nothing wrong
T4. Official: Egypt balloon explosion probe can take 2 weeks TT4. Ex-CIA employee: Obama advanced surveillance policies, not re-
formed them.
Table 6: Examples of extracted highlights (H&HH items are the ground-truth highlights, N&NN items
are the highlights extracted from news by Ours (LSF+CCF), and T&TT items are the highlights ex-
tracted from tweets by Ours (LTF+CCF); Bold: Good highlight; [+]: Newly extracted highlights using
correlation features; [-]: Lost highlights after adding correlation features)
Example 2 (right column) shows tweets may not be always useful. Ours (LSF+CCF) adds a bad
highlight NN4 but removes a good one NN. We find that NN4 is very similar to TT1. So the introduction
of NN4 is believed as the result of influence from TT1. NN is squeezed out of the summary since we
find it lack of tweets in our set similar to NN. Currently, we only use explicit links for tweets-document
couplings. It might be helpful if we could expand the set to cover more informative tweets.
6.4 Contribution of Features
We further investigate the contribution of different features in our feature set (see Table 5) to the learned
ranking models. We choose the best models from the two tasks, i.e., Ours (LSF+CCF) and Ours
(LTF+CC), and find out the top-10 weighted features for each model. To get the feature weights, for
each feature we aggregate the weight values of its corresponding weak ranker selected during the itera-
tion in RankBoost training, that is, for a weak ranker repeatedly selected in different rounds, its weights
obtained from those rounds are added up to obtain as the feature weight. Table 7 lists the top-10 features
and their corresponding weight values.
Cross-media correlation features, which are underlined, appear overwhelmingly important to the sen-
tences extraction task with the modelOurs (LSF+CCF), where they take eight places in the top-10 feature
list. This confirms the indicative effect of tweets. In tweets extraction task, the model Ours (LTF+CCF)
does not seem to be so dependent on the cross-media correlation features, but still there are five of them
appearing important in the list. In particular, the similarities between tweets and the leading news sen-
tences such as SimiTopUnigram and LeadSenSimi are shown very helpful. This is because the leading
part of the article can be more indicative of important tweets. Besides, the writing-quality measures of
tweets are also very useful as it is shown that all the three quality-related features are among the top ten.
7 Conclusion and Future work
In this paper, we explore to utilize microblogs for automatic highlights extraction from two perspectives
using learning-based ranking models. Firstly, we extract important sentences from news article by using
a set of relevant tweets that provide indicative support for the informativeness of candidate sentences;
Secondly, we extract important tweets from the relevant tweets set associated with the given article by
taking the advantage of the fact that tweets are comparably concise as highlights. The results show
that our methods significantly outperform state-of-the-art baseline approaches for single-document sum-
880
Task 1: Ours (LSF+CCF) Task 2: Ours (LTF+CCF)
Feature Weight Feature Weight
ImportUnigram 4.7912 SimiTopUnigram (count) 1.9300
MaxROUGE1R 2.1049 LeadSenSimi (third) 1.8367
MaxROUGE1F 0.6511 QualityLM (Bigram) 1.4513
SimiTopUnigram (count) 0.6260 MaxROUGE1R 1.1925
SimiUnigram 0.5424 QualityLM (Unigram) 0.9441
MaxROUGE1P 0.1922 LeadSenSimi (second) 0.9224
SimiTFIDF 0.1534 QualityDepend 0.8306
SimiTopEntity (count) 0.0311 TopicNE (person) 0.7937
SimiTopEntity (presence) 0.0051 ImportTFIDF 0.7423
TitleSimi 0.0050 LeadSenSimi (fourth) 0.6072
Table 7: Top 10 features and their weights resulting from the best ranking models in the two tasks
(underline: Cross-media correlation features)
marization. Our feature study further discovers that the cross-media correlations are overwhelmingly
important to sentence extraction, and for tweets extraction the quality-related features are comparably
important as cross-media correlation measures. Also, tweets extraction appears more suitable for pro-
ducing highlights owing to the human compression effect of tweets.
For the future work, we plan to enlarge the relevant tweets collection by including relevant tweets
not linked by URLs; we can combine the extracts from both sides for further improvement; we can also
strengthen our model by capturing some deeper or latent linguistic and semantic correlations with deep
learning formalism.
Acknowledgments
This work is supported by the QCRI-MIT collaboration program. We appreciate the helpful discussions
with Regina Barzilay, Karthik Narasimhan and Lanjun Zhou at early stage of the project. Zhongyu Wei
is partially financed by the General Research Fund of Hong Kong (417112) and the Shenzhen Funda-
mental Research Program (JCYJ20130401172046450) of China. We thank anonymous reviewers for
their insightful comments.
References
Regina Barzilay, Michael Elhadad, et al. 1997. Using lexical chains for text summarization. In Proceedings of the
ACL Workshop on Intelligent Scalable Text Summarization, number 1, pages 10?17.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 335?336. ACM.
James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Research, 34:637?674.
Yajuan Duan, Zhimin Chen, Furu Wei, Ming Zhou, and Heung-Yeung Shum. 2012. Twitter topic summarization
by ranking tweets using social influence and content quality. In Proceedings of COLING, pages 763?780.
G?unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research, 22:457?479.
Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for combining
preferences. Journal of Machine Learning Research, 4:933?969.
881
Wei Gao, Peng Li, and Kareem Darwish. 2012. Joint topic modeling for event summarization across news and
social media streams. In Proceedings of the 21st ACM International Conference on Information and Knowledge
Management, pages 1173?1182. ACM.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013. Linking tweets to news: A framework to enrich short text
data in social media. In Proceedings of the ACL, pages 239?249.
Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a# twitter. In
Proceedings of ACL, pages 368?378.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino, Norihito Yasuda, and Masaaki Nagata. 2013. Single-
document summarization as a tree knapsack problem. In Proceedings of EMNLP, pages 1515?1520.
Meishan Hu, Aixin Sun, and Ee-Peng Lim. 2008. Comments-oriented document summarization: understanding
documents with readers? feedback. In Proceedings of the 31st Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 291?298. ACM.
David Inouye and Jugal K Kalita. 2011. Comparing twitter summarization algorithms for multiple post summaries.
In Proceedings of 2011 IEEE Third International Conference on Social Computing (SocialCom), pages 298?
306. IEEE.
Joel Judd and Jugal Kalita. 2013. Better twitter summaries? In Proceedings of NAACL-HLT, pages 445?449.
Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to
sentence compression. Artificial Intelligence, 139(1):91?107.
Alok Kothari, Walid Magdy, Ahmed Mourad Kareem Darwish, and Ahmed Taei. 2013. Detecting comments on
news articles in microblogs. In Proceedings of ICWSM, pages 293?302.
Chen Li, Xian Qian, and Yang Liu. 2013. Using supervised bigram-based ilp for extractive summarization. In
Proceedings of ACL, pages 1004?1013.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?81.
Marina Litvak and Mark Last. 2008. Graph-based keyword extraction for single-document summarization. In
Proceedings of the workshop on multi-source multilingual information extraction and summarization, pages
17?24. Association for Computational Linguistics.
Hans Peter Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development,
2(2):159?165.
Daniel Marcu. 1997. From discourse structures to text summaries. In Proceedings of ACL, pages 82?88.
Ani Nenkova. 2005. Automatic text summarization of newswire: lessons learned from the document under-
standing conference. In Proceedings of the 20th International Conference on Artificial Intelligence, pages
1436?1441. AAAI Press.
Jeffrey Nichols, Jalal Mahmud, and Clemens Drews. 2012. Summarizing sporting events using twitter. In Pro-
ceedings of the 2012 ACM International Conference on Intelligent User Interfaces, pages 189?198. ACM.
Owen Phelan, Kevin McCarthy, Mike Bennett, and Barry Smyth. 2011. Terms of a feather: Content-based news
recommendation and discovery using twitter. In Advances in Information Retrieval, pages 448?459. Springer.
Beaux Sharifi, M-A Hutton, and Jugal K Kalita. 2010. Experiments in microblog summarization. In Proceedings
of 2010 IEEE Second International Conference on Social Computing (SocialCom), pages 49?56. IEEE.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document summarization using conditional
random fields. In Proceeding of IJCAI, pages 2862?2867.
Tadej
?
Stajner, Bart Thomee, Ana-Maria Popescu, Marco Pennacchiotti, and Alejandro Jaimes. 2013. Automatic
selection of social media responses to news. In Proceedings of the 19th ACM SIGKDD international Conference
on Knowledge Discovery and Data Mining, pages 50?58. ACM.
Ilija Suba?si?c and Bettina Berendt. 2011. Peddling or creating? investigating the role of twitter in news reporting.
In Advances in Information Retrieval, pages 207?213. Springer.
882
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang, Yuchang Lu, and Zheng Chen. 2005. Web-page summa-
rization using clickthrough data. In Proceedings of the 28th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 194?201. ACM.
Krysta Marie Svore, Lucy Vanderwende, and Christopher JC Burges. 2007. Enhancing single-document summa-
rization by combining ranknet and third-party sources. In Proceedings of EMNLP-CoNLL, pages 448?457.
Kam-Fai Wong, Mingli Wu, andWenjie Li. 2008. Extractive summarization using supervised and semi-supervised
learning. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 985?992.
Association for Computational Linguistics.
Kristian Woodsend and Mirella Lapata. 2010. Automatic generation of story highlights. In Proceedings of
the 48th Annual Meeting of the Association for Computational Linguistics, pages 565?574. Association for
Computational Linguistics.
Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and Juanzi Li. 2011. Social context summarization. In
Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 255?264. ACM.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic models. In Advances in Information Retrieval, pages
338?349. Springer.
Arkaitz Zubiaga, Damiano Spina, Enrique Amig?o, and Julio Gonzalo. 2012. Towards real-time summarization of
scheduled events from twitter streams. In Proceedings of the 23rd ACM Conference on Hypertext and Social
Media, pages 319?320. ACM.
883
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 162?171,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Discourse Relations for Eliminating
Intra-sentence Polarity Ambiguities
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei, Kam-Fai Wong
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong
Shatin, NT, Hong Kong, China
Key Laboratory of High Confidence Software Technologies
Ministry of Education, China
{ljzhou, byli, wgao, zywei, kfwong}@se.cuhk.edu.hk
Abstract
Polarity classification of opinionated sen-
tences with both positive and negative senti-
ments1 is a key challenge in sentiment anal-
ysis. This paper presents a novel unsuper-
vised method for discovering intra-sentence
level discourse relations for eliminating polar-
ity ambiguities. Firstly, a discourse scheme
with discourse constraints on polarity was de-
fined empirically based on Rhetorical Struc-
ture Theory (RST). Then, a small set of cue-
phrase-based patterns were utilized to collect
a large number of discourse instances which
were later converted to semantic sequential
representations (SSRs). Finally, an unsuper-
vised method was adopted to generate, weigh
and filter new SSRs without cue phrases for
recognizing discourse relations. Experimen-
tal results showed that the proposed methods
not only effectively recognized the defined
discourse relations but also achieved signifi-
cant improvement by integrating discourse in-
formation in sentence-level polarity classifica-
tion.
1 Introduction
As an important task of sentiment analysis, polar-
ity classification is critically affected by discourse
structure (Polanyi and Zaenen, 2006). Previous re-
search developed discourse schema (Asher et al,
2008) (Somasundaran et al, 2008) and proved that
the utilization of discourse relations could improve
the performance of polarity classification on dia-
logues (Somasundaran et al, 2009). However, cur-
1Defined as ambiguous sentences in this paper
rent state-of-the-art methods for sentence-level po-
larity classification are facing difficulties in ascer-
taining the polarity of some sentences. For example:
(a) [Although Fujimori was criticized by the international
community]?[he was loved by the domestic population]?
[because people hated the corrupted ruling class]. (??
??????????????????????
??????????????????????)
Example (a) is a positive sentence holding a Con-
trast relation between first two segments and a
Cause relation between last two segments. The po-
larity of "criticized", "hated" and "corrupted" are rec-
ognized as negative expressions while "loved" is rec-
ognized as a positive expression. Example (a) is dif-
ficult for existing polarity classification methods for
two reasons: (1) the number of positive expressions
is less than negative expressions; (2) the importance
of each sentiment expression is unknown. However,
consider Figure 1, if we know that the polarity of
the first two segments holding a Contrast relation
is determined by the nucleus (Mann and Thompson,
1988) segment and the polarity of the last two seg-
ments holding aCause relation is also determined by
the nucleus segment, the polarity of the sentence will
be determined by the polarity of "[he...population]".
Thus, the polarity of Example (a) is positive.
Statistics showed that 43% of the opinionated
sentences in NTCIR2 MOAT (Multilingual Opinion
Analysis Task) Chinese corpus3 are ambiguous. Ex-
isting sentence-level polarity classification methods
ignoring discourse structure often give wrong results
for these sentences. We implemented state-of-the-
2http://research.nii.ac.jp/ntcir/
3Including simplified Chinese and traditional Chinese cor-
pus from NTCIR-6 MOAT and NTCIR-7 MOAT
162
Figure 1: Discourse relations for Example (a). (n and s
denote nucleus and satellite segment, respectively)
art method (Xu and Kit, 2010) in NTCIR-8 Chinese
MOAT as the baseline polarity classifier (BPC) in
this paper. Error analysis of BPC showed that 49%
errors came from ambiguous sentences.
In this paper, we focused on the automation of
recognizing intra-sentence level discourse relations
for polarity classification. Based on the previous
work of Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988), a discourse scheme with dis-
course constraints on polarity was defined empiri-
cally (see Section 3). The scheme contains 5 rela-
tions: Contrast, Condition, Continuation, Cause and
Purpose. From a raw corpus, a small set of cue-
phrase-based patterns were used to collect discourse
instances. These instances were then converted to
semantic sequential representations (SSRs). Finally,
an unsupervised SSR learner was adopted to gener-
ate, weigh and filter high quality new SSRs with-
out cue phrases. Experimental results showed that
the proposed methods could effectively recognize
the defined discourse relations and achieve signifi-
cant improvement in sentence-level polarity classi-
fication comparing to BPC.
The remainder of this paper is organized as fol-
lows. Section 2 introduces the related work. Sec-
tion 3 presents the discourse scheme with discourse
constraints on polarity. Section 4 gives the detail of
proposed method. Experimental results are reported
and discussed in Section 5 and Section 6 concludes
this paper.
2 Related Work
Research on polarity classification were generally
conducted on 4 levels: document-level (Pang et al,
2002), sentence-level (Riloff et al, 2003), phrase-
level (Wilson et al, 2009) and feature-level (Hu and
Liu, 2004; Xia et al, 2007).
There was little research focusing on the auto-
matic recognition of intra-sentence level discourse
relations for sentiment analysis in the literature.
Polanyi and Zaenen (2006) argued that valence cal-
culation is critically affected by discourse struc-
ture. Asher et al (2008) proposed a shallow se-
mantic representation using a feature structure and
use five types of rhetorical relations to build a fine-
grained corpus for deep contextual sentiment anal-
ysis. Nevertheless, they did not propose a com-
putational model for their discourse scheme. Sny-
der and Barzilay (2007) combined an agreement
model based on contrastive RST relations with a lo-
cal aspect model to make a more informed over-
all decision for sentiment classification. Nonethe-
less, contrastive relations were only one type of dis-
course relations which may help polarity classifica-
tion. Sadamitsu et al (2008) modeled polarity re-
versal using HCRFs integrated with inter-sentence
discourse structures. However, our work is on intra-
sentence level and our purpose is not to find polar-
ity reversals but trying to adapt general discourse
schemes (e.g., RST) to help determine the overall
polarity of ambiguous sentences.
The most closely related works were (Somasun-
daran et al, 2008) and (Somasundaran et al, 2009),
which proposed opinion frames as a representation
of discourse-level associations on dialogue andmod-
eled the scheme to improve opinion polarity clas-
sification. However, opinion frames was difficult
to be implemented because the recognition of opin-
ion target was very challenging in general text. Our
work differs from their approaches in two key as-
pects: (1) we distinguished nucleus and satellite in
discourse but opinion frames did not; (2) our method
for discourse discovery was unsupervised while their
method needed annotated data.
Most research works about discourse classifica-
tion were not related to sentiment analysis. Su-
pervised discourse classification methods (Soricut
and Marcu, 2003; Duverle and Prendinger, 2009)
needed manually annotated data. Marcu and Echi-
habi (2002) presented an unsupervised method to
recognize discourse relations held between arbitrary
spans of text. They showed that lexical pairs ex-
tracted from massive amount of data can have a
major impact on discourse classification. Blair-
Goldensohn et al (2007) extended Marcu's work by
using parameter opitimization, topic segmentation
and syntactic parsing. However, syntactic parsers
163
were usually costly and impractical when dealing
with large scale of text. Thus, in additional to lex-
ical features, we incorporated sequential and seman-
tic information in proposed method for discourse re-
lation classification. Moreover, our method kept the
characteristic of language independent, so it could be
applied to other languages.
3 Discourse Scheme for Eliminating
Polarity Ambiguities
Since not all of the discourse relations in RST
would help eliminate polarity ambiguities, the dis-
course scheme defined in this paper was on a much
coarser level. In order to ascertain which relations
should be included in our scheme, 500 ambigu-
ous sentences were randomly chosen from NTCIR
MOAT Chinese corpus and the most common dis-
course relations for connecting independent clauses
in compound sentences were annotated. We found
that 13 relations from RST occupied about 70% of
the annotated discourse relations which may help
eliminate polarity ambiguities. Inspired by Marcu
and Echihabi (2002), to construct relatively low-
noise discourse instances for unsupervised methods
using cue phrases, we grouped the 13 relations into
the following 5 relations:
Contrast is a union of Antithesis, Concession, Oth-
erwise and Contrast from RST.
Condition is selected from RST.
Continuation is a union of Continuation, Parallel
from RST.
Cause is a union of Evidence, Volitional-Cause,
Nonvolitional-Cause, Volitional-result and
Nonvolitional-result from RST.
Purpose is selected from RST.
The discourse constraints on polarity presented
here were based on the observation of annotated dis-
course instances: (1) discourse instances holding
Contrast relation should contain two segments with
opposite polarities; (2) discourse instances hold-
ing Continuation relation should contain two seg-
ments with the same polarity; (3) the polarity of dis-
course instances holdingContrast,Condition,Cause
or Purpose was determined by the nucleus segment;
(4) the polarity of discourse instances holding Con-
tinuation was determined by either segment.
Relation Cue Phrases(English Translation)
Contrast although1, but2, however2
Condition if1, (if1?then2)
Continuation and, further more,(not only, but also)
Cause because
1, thus2, accordingly2,
as a result2
Purpose in order to
2, in order that2,
so that2
1 means CUE1 and 2 means CUE2
Table 1: Examples of cue phrases
4 Methods
The proposed methods were based on two as-
sumptions: (1) Cue-phrase-based patterns could be
used to find limited number of high quality discourse
instances; (2) discourse relations were determined
by lexical, structural and semantic information be-
tween two segments.
Cue-phrase-based patterns could find only lim-
ited number of discourse instances with high pre-
cision (Marcu and Echihabi, 2002). Therefore, we
could not rely on cue-phrase-based patterns alone.
Moreover, there was no annotated corpus similar to
Penn Discourse TreeBank (Miltsakaki et al, 2004)
in other languages such as Chinese. Thus, we pro-
posed a language independent unsupervised method
to identify discourse relations without cue phrases
while maintaining relatively high precision. For
each discourse relation, we started with several cue-
phrase-based patterns and collected a large number
of discourse instances from raw corpus. Then, dis-
course instances were converted to semantic sequen-
tial representations (SSRs). Finally, an unsupervised
method was adopted to generate, weigh and filter
common SSRswithout cue phrases. Themined com-
mon SSRs could be directly used in our SSR-based
classifier in unsupervised manner or be employed as
effective features for supervised methods.
4.1 Gathering and representing discourse
instances
A discourse instance, denoted by Di, consists of
two successive segments (Di[1], Di[2]) within a sen-
tence. For example:
D1: [Although Boris is very brilliant at math]s, [he
164
BOS... ?[CUE2]...EOS
BOS [CUE1]... ?...EOS
BOS... ?[CUE1]...EOS
BOS [CUE1]... ?[CUE2]...EOS
Table 2: Cue-phrase-based patterns. BOS and EOS de-
noted the beginning and end of two segments.
is a horrible teacher]n
D2: [John is good at basketball]s, [but he lacks team
spirit]n
In D1, "although" indicated the satellite section
while inD2, "but" indicated the nucleus section. Ac-
cordingly, different cue phrases may indicate differ-
ent segment type. Table 1 listed some examples of
cue phrases for each discourse relation. Some cue
phrases were singleton (e.g. "although" and "as a re-
sult") and some were used as a pair (e.g. "not only,
but also"). "CUE1" indicated satellite segments and
"CUE2" indicated nucleus segments. Note that we
did not distinguish satellite from nucleus for Con-
tinuation in this paper because the polarity could be
determined by either segment.
Table 2 listed cue-phrase-based patterns for all re-
lations. To simplify the problem of discourse seg-
mentation, we split compound sentences into dis-
course segments using commas and semicolons. Al-
though we collected discourse instances from com-
pound sentences only, the number of instances for
each discourse relation was large enough for the pro-
posed unsupervised method. Note that we only col-
lected instances containing at least one sentiment
word in each segment.
In order to incorporate lexical and semantic infor-
mation in our method, we represented each word in
a discourse instance using a part-of-speech tag, a se-
mantic label and a sentiment tag. Then, all discourse
instances were converted to SSRs. The rules for con-
verting were as follows:
(1) Cue phrases and punctuations were ingored.
But the information of nucleus(n) and satellite(s)
was preserved.
(2) Adverbs(RB) appearing in sentiment lexicon,
verbs(V ), adjectives(JJ ) and nouns(NN) were repre-
sented by their part-of-speech (pos) tag with seman-
tic label (semlabel) if available.
(3) Named entities (NE; PER: person name;ORG:
organization), pronouns (PRP), and function words
were represented by their corresponding named en-
tity tags and part-of-speech tags, respectively.
(4) Added sentiment tag (P : Positive; N : Nega-
tive) to all sentiment words.
By applying above rules, the SSRs forD1 andD2
would be:
d1: [PERV|Ja01 RB|Ka01 JJ|Ee14|P IN NN|Dk03]s
, [PRP V|Ja01 DT JJ|Ga16|N NN|Ae13 ]n
d2: [PER V|Ja01 JJ|Ee14|P IN NN|Bp12]s, [PRP
V|He15|N NN|Di10 NN|Dd08 ]n
Refer to d1 and d2, "Boris" could match "John"
in SSRs because they were converted to "PER" and
they all appeared at the beginning of discourse in-
stances. "Ja01", "Ee14" etc. were semantic labels
from Chinese synonym list extended version (Che et
al., 2010). There were similar resources in other lan-
guages such asWordnet(Fellbaum, 1998) in English.
The next problem became how to start from current
SSRs and generate new SSRs for recognizing dis-
course relations without cue phrases.
4.2 Mining common SSRs
Recall assumption (2), in order to incorporate lex-
ical, structural and semantic information for the sim-
ilarity calculation of two SSRs holding the same
discourse relation, three types of matches were de-
fined for {(u, v)|u ? di[k], v ? dj[k], k = 1, 2}:
(1)Full match: (i) u = v or (ii) u.pos = v.pos and
u.semlabel=v.semlabel or (iii) u.pos=v.pos and
u had a sentiment tag and v had a sentiment tag or
(iv) u.pos and v.pos?{PRP, PER, ORG} (2) Partial
match: u.pos = v.pos but not Full match; (3) Mis-
match: u.pos ?= v.pos.
Generating common SSRs
Intuitively, a simple way of estimating the simi-
larity between two SSRs was using the number of
mismatches. Therefore, we utilized match(di, dj)
where i ?= j, which integrated the three types of
matches defined above to calculate the number of
mismatches and generate common SSRs. Consider
Table 3, in common SSRs, full matches were pre-
served, partial matches were replaced by part of
speech tags and mismatches were replaced by '*'s.
The common SSRs generated during the calculation
of match(di, dj) consisted of two parts. The first
part was generated by di[1] and dj[1] and the second
part was generated by di[2] and dj[2]. We stipulated
165
d1 d2 mis conf ssr
PER PER 0 0 PER
V|Ja01 V|Ja01 0 0 V|Ja01
RB|Ka01 +1 ?0.298 *
JJ|Ee14|P JJ|Ee14|P 0 0 JJ|Ee14|P
IN IN 0 0 IN
NN|Dk03 NN|Bp12 0 ?0.50 NN
conf(ssr[1]) = ?0.798
PRP PRP 0 0 PRP
V|Ja01 V|He15|N 0 ?0.50 V
DT +1 ?0.184 *
JJ|Ga16|N +1 ?1.0 *
NN|Ae13 NN|Di10 0 ?0.50 NN
NN|Dd08 +1 ?1.0 *
conf(ssr[2]) = ?3.184
Table 3: Calculation of match(d1, d2). ssr denoted
the common SSR between d1 and d2 , conf(ssr[1]) and
conf(ssr[2]) denoted the confidence of ssr.
that di and dj could generate a common SSR if and
only if the orders of nucleus segment and satellite
segment were the same.
In order to guarantee relatively high quality com-
mon SSRs, we empirically set the upper threshold
of the number of mismatches as 0.5 (i.e., ? 1/2 of
the number of words in the generated SSR). It's not
difficult to figure out that the number of mismatches
generated in Table 3 satisfied this requirement. As a
result, for each discourse relation rn, a correspond-
ing common SSR set Sn could be obtained by adopt-
ing match(di, dj) where i ?= j for all discourse in-
stances. An advantage of match(d1, d2) was that
the generated common SSRs preserved the sequen-
tial structure of original discourse instances. And
common SSRs allows us to build high precision dis-
course classifiers (See Section 5).
Weighing and filtering common SSRs
A problem of match(di, dj) was that it ignored
some important information by treating different
mismatches equally. For example, the adverb "very"
in "very brilliant" of D1 was not important for dis-
course recognition. In other words, the number of
mismatches inmatch(di, dj) could not precisely re-
flect the confidence of the generated common SSRs.
Therefore, it was needed to weigh different mis-
matches for the confidence calculation of common
SSRs.
Intuitively, if a partial match or a mismatch (de-
noted by um) occurred very frequently in the gener-
ation of common SSRs, the importance of um tends
to diminish. Inspired by the tf-idf model, given
ssri?Sn, we utilized the following equation to esti-
mate the weight (denoted by wm) of um.
wm = ?ufm ? log (|Sn|/ssrfm )
where ufm denoted the frequency of um during the
generation of ssri, |Sn| denoted the size of Sn and
ssrfm denoted the number of common SSRs in Sn
containing um . All weights were normalized to
[?1, 0).
Nouns (except for named entities) and verbs were
most representative words in discourse recognition
(Marcu and Echihabi, 2002). In addition, adjectives
and adverbs appearing in sentiment lexicons were
important for polarity classification. Therefore, for
these 4 kinds of words, we utilized ?1.0 for a mis-
match and ?0.50 for a partial match.
As we had got the weights for all partial matches
and mismatches, the confidence of ssri?Sn could be
calculated using the cumulation of weights of par-
tial matches and mismatches in ssri[1] and ssri[2].
Recall Table 3, conf(ssr[1]) and conf(ssr[2]) rep-
resented the confidence scores ofmatch(di[1], dj[1])
and match(di[2], dj[2]), respectively. In order to
control the quantity and quality of mined SSRs, a
threshold minconf was introduced. ssri will be
preserved if and only if conf(ssri[1]) ?minconf
and conf(ssri[2]) ? minconf . The value of
minconf was tuned using the development data.
Finally, we combined adjacent '*'s and preserved
SSRs containing at least one notional word and at
least two words in each segment to meet the de-
mand of maintaining high precision (e.g., "[* DT
*]", "[PER *]" will be dropped). Moreover, since
many of the SSRs were duplicated, we ranked all
the generated SSRs according to their occurrences
and dropped those appearing only once in order to
preserve common SSRs. At last, SSRs appearing in
more than one common SSR set were removed for
maintaining the uniqueness of each set. The com-
mon SSR set Sn for each discourse relation rn could
be directly used in SSR-based unsupervised classi-
fiers or be employed as effective features in super-
vised methods.
166
Relation Occurrence
Contrast 86 (8.2%)
Condition 27 (2.6%)
Continuation 445 (42.2%)
Cause 123 (11.7%)
Purpose 55 (5.2%)
Others 318 (30.2%)
Table 4: Distribution of discourse relations on NTC-7.
Others represents discourse relations not included in our
discourse scheme.
5 Experiments
5.1 Annotation work and Data
We extracted all compound sentences which may
contain the defined discourse relations from opinion-
ated sentences (neutral ones were dropped) of NT-
CIR7MOAT simplified Chinese training data. 1,225
discourse instances were extracted and two annota-
tors were trained to annotate discourse relations ac-
cording to the discourse scheme defined in Section 3.
Note that we annotate both explicit and implicit dis-
course relations. The overall inter annotator agree-
ment was 86.05% and the Kappa-value was 0.8031.
Table 4 showed the distribution of annotated dis-
course relations based on the inter-annotator agree-
ment. The proportion of occurrences of each dis-
course relations varied greatly. For example, Con-
tinuation was the most common relation in anno-
tated corpus, but the occurrences of Condition rela-
tion were rare.
The experiments of this paper were performed us-
ing the following data sets:
NTC-7 contained manually annotated discourse
instances (shown in Table 4). The experiments of
discourse identification were performed on this data
set.
NTC-8 contained all opinionated sentences (neu-
tral ones were dropped) extracted from NTCIR8
MOAT simplified Chinese test data. The experi-
ments of polarity ambiguity elimination using the
identified discourse relations were performed on this
data set.
XINHUA contained simplified Chinese raw news
text from Xinhua.com (2002-2005). A word seg-
mentation tool, a part-of-speech tagging tool, a
named entity recognizer and a word sense disam-
biguation tool (Che et al, 2010) were adopted to all
sentences. The common SSRs were mined from this
data set.
5.2 Experimental Settings
Discourse relation identification
In order to systematically justify the effectiveness
of proposed unsupervised method, following exper-
iments were performed on NTC-7:
Baseline used only cue-phrase-based patterns.
M&E proposed by Marcu and Echihabi (2002).
Given a discourse instance Di, the probabilities:
P (rk|(Di[1], Di[2])) for each relation rk were esti-
mated on all text from XINHUA. Then, the most
likely discourse relation was determined by taking
the maximum over argmaxk{P (rk|(Di[1], Di[2])}.
cSSR used both cue-phrase-based patterns to-
gether with common SSRs for recognizing discourse
relations. Common SSRs were mined from dis-
course instances extracted fromXINHUAusing cue-
phrase-based patterns. Development data were ran-
domly selected for tuning minconf .
SVM was trained utilizing cue phrases, probabil-
ities from M&E, topic similarity, structure overlap,
polarity of segments and mined common SSRs (Op-
tional). The parameters of the SVM classifier were
set by a grid search on the training set. We performed
4-fold cross validation on NTC-7 to get an average
performance.
The purposes of introducing SVM in our experi-
ment were: (1) to compare the performance of cSSR
to supervised method; (2) to examine the effective-
ness of integrating common SSRs as features for su-
pervised methods.
Polarity ambiguity elimination
BPC was trained mainly utilizing punctuation,
uni-gram, bi-gram features with confidence score
output. Discourse classifiers such as Baseline, cSSR
or SVM were adopted individually for the post-
processing of BPC. Given an ambiguous sentence
which contained more than one segment, an intuitive
three-step method was adopted to integrated a dis-
course classifier and discourse constraints on polar-
ity for the post-processing of BPC:
(1) Recognize all discourse relations together with
nucleus and satellite information using a discourse
classifier. The nucleus and satellite information is
167
Figure 2: Influences of different values of minconf to
the performance of cSSR
acquired by cSSR if a segment pair could match a
cSSR. Otherwise, we use the annotated nucleus and
satellite information.
(2) Apply discourse constraints on polarity to
ascertain the polarity for each discourse instance.
There may be conflicts between polarities acquired
by BPC and discourse constraints on polarity (e.g.,
Two segments with the same polarity holding a Con-
trast relation). To handle this problem, we chose
the segment with higher polarity confidence and ad-
justed the polarity of the other segment using dis-
course constraints on polarity.
(3) If there was more than one discourse instance
in a single sentence, the overall polarity of the sen-
tence was determined by voting of polarities from
each discourse instance under the majority rule.
5.3 Experimental Results
Refer to Figure 2, the performance of cSSR was
significantly affected by minconf . Note that we
performed the tuning process ofminconf on differ-
ent development data (1/4 instances randomly se-
lected from NTC-7) and Figure 2 showed the av-
erage performance. cSSR became Baseline when
minconf =0. A significant drop of precision was
observed when minconf was less than ?2.5. The
recall remained around 0.495 when minconf ?
?4.0. The best performance was observed when
minconf=?3.5. As a result, ?3.5 was utilized as
the threshold value for cSSR in the following exper-
iments.
Table 5 presented the experimental results for dis-
course relation classification. it showed that:
(1) Cue-phrase-based patterns could find only lim-
ited number of discourse relations (34.1% of average
BPC Baseline cSSR SVM+SSRs
Precision 0.7661 0.7982 0.8059 0.8113
Recall 0.7634 0.7957 0.8038 0.8091
F-score 0.7648 0.7970 0.8048 0.8102
Table 6: Performance of integrating discourse classifiers
and constraints to polarity classification. Note that the
experiments were performed on NTC-8 which contained
only opinionated sentences.
recall) with a very high precision (96.17% of average
precision). This is a proof of assumption (1) given
in Section 4. On the other side, M&E which only
considered word pairs between two segments of dis-
course instances got a higher recall with a large drop
of precision. The drop of precision may be caused
by the neglect of structural and semantic information
of discourse instances. However, M&E still outper-
formed Baseline in average F -score.
(2) cSSR enhanced Baseline by increasing the av-
erage recall by about 15% with only a small drop of
precision. The performance of cSSR demonstrated
that our method could effectively discover high qual-
ity common SSRs. The most remarkable improve-
ment was observed on Continuation in which the re-
call increased by almost 20% with only a minor drop
of precision. Actually, cSSR outperformed Baseline
in all discourse relations except forContrast. In Dis-
course Tree Bank (Carlson et al, 2001) only 26%
of Contrast relations were indicated by cue phrases
while in NTC-7 about 70% of Contrast were indi-
cated by cue phrases. A possible reason was that
we were dealing with Chinese news text which were
usually well written. Another important observation
was that the performance of cSSR was very close to
the result of SVM.
(3) SVM+SSRs achieved the best F -score on
Continuation and average performance. The integra-
tion of SSRs to the feature set of SVM contributed to
a remarkable increase in average F -score. The re-
sults of cSSR and SVM+SSRs demonstrated the ef-
fectiveness of common SSRs mined by the proposed
unsupervised method.
Table 6 presented the performance of integrat-
ing discourse classifiers to polarity classification.
For Baseline and cSSR, the information of nucleus
and satellite could be obtained directly from cue-
168
Relation Baseline M&E cSSR SVM SVM+SSRs
Contrast
P 0.9375 0.4527 0.7531 0.9375 0.9375
R 0.6977 0.7791 0.7093 0.6977 0.6977
F 0.8000 0.5726 0.7305 0.8000 0.8000
Condition
P 1.0000 0.4444 0.6774 1.0000 0.7083
R 0.5556 0.8889 0.7778 0.5185 0.6296
F 0.7143 0.5926 0.7241 0.6829 0.6667
Continuation
P 0.9831 0.6028 0.9761 0.6507 0.7266
R 0.2607 0.5865 0.4584 0.6697 0.6629
F 0.4120 0.5945 0.6239 0.6600 0.6933
Cause
P 1.0000 0.5542 0.9429 1.0000 0.9412
R 0.2114 0.3740 0.2683 0.2114 0.2602
F 0.3489 0.4466 0.4177 0.3489 0.4076
Purpose
P 0.8947 0.3704 0.8163 0.9167 0.7193
R 0.6182 0.7273 0.7273 0.6000 0.7455
F 0.7312 0.4908 0.7692 0.7253 0.7321
Average
P 0.9617 0.5302 0.8864 0.7207 0.7607
R 0.3410 0.5951 0.4878 0.5856 0.6046
F 0.5035 0.5608 0.6293 0.6461 0.6737
Table 5: Performance of recognizing discourse relations. (The evaluation criteria are Precision, Recall and F-score)
phrase-based patterns and SSRs, respectively. For
SVM+cSSR, the nucleus and satellite information
was acquired by cSSR if a segment pair could match
a cSSR. Otherwise, we used manually annotated nu-
cleus and satellite information. It's clear that the
performance of polarity classification was enhanced
with the improvement of discourse relation recogni-
tion. M&E was not included in this experiment be-
cause the performance of polarity classification was
decreased by the mis-classified discourse relations.
SVM+SSRs achieved significant (p<0.01) improve-
ment in polarity classification compared to BPC.
5.4 Discussion
Effect of weighing and filtering
To assess the contribution of weighing and filter-
ing in mining SSRs using a minimum confidence
threshold, i.e. minconf , we implemented cSSR?
without weighing and filtering on the same data set.
Consider Table 7, cSSR achieved obvious improve-
ment in Precision and F -score than cSSR?. More-
over, the total number of SSRs was greatly reduced
in cSSR with only a minor drop of recall. This was
because cSSR? was affected by thousands of low
quality common SSRs which would be filtered in
cSSR. The result in Table 7 proved that weighing and
cSSR? cSSR
Precision 0.6182 0.8864
Recall 0.5014 0.4878
F-score 0.5537 0.6293
NOS > 1 million ? 0.12 million
Table 7: Comparison of cSSR? and cSSR. "NOS" denoted
the number of mined common SSRs.
filtering were essential in our proposed method.
We further analyzed how the improvement was
achieved in cSSR. In our experiment, the most com-
mon mismatches were auxiliary words, named enti-
ties, adjectives or adverbs without sentiments (e.g.,
"green", "very", etc.), prepositions, numbers and
quantifiers. It's straightforward that these words
were insignificant in discourse relation classification
purpose. Moreover, these words did not belong to
the 4 kinds of most representative words. In other
words, the weights of most mismatches were calcu-
lated using the equation presented in Section 4.2 in-
stead of utilizing a unified value, i.e. ?1. Recall
Table 3, the weight of "RB|Ka01" (original: "very")
was ?0.298 and "DT" (original: 'a') was ?0.184.
Comparing to the weights of mismatches for most
representative words (?1.0), the proposed method
successfully down weighed the words which were
169
Figure 3: Improvement from individual discourse rela-
tions. N denoted the number of ambiguities eliminated.
not important for discourse identification. There-
fore, weighing and filtering were able to preserve
high quality SSRs while filter out low quality SSRs
by setting the confidence threshold, i.e. minconf .
Contribution of different discourse relations
We also analyzed the contribution of different dis-
course relations in eliminating polarity ambiguities.
Refer to Figure 3, the improvement of polarity classi-
fication mainly came from three discourse relations:
Contrast, Continuation and Cause. It was straight-
forward that Contrast relation could eliminate po-
larity ambiguities because it held between two seg-
ments with opposite polarities. The contribution of
Cause relation also result from two segments holding
different polarities such as example (a) in Section 1.
However, recall Table 4, although Cause occurred
more often than Contrast, only a part of discourse
instances holding Cause relation contained two seg-
ments with the opposite polarities. Another impor-
tant relation in eliminating ambiguity was Continu-
ation. We investigated sentences with polarities cor-
rected by Continuation relation. Most of them fell
into two categories: (1) sentences with mistakenly
classified sentiments by BPC; (2) sentences with im-
plicit sentiments. For example:
(b) [France and Germany have banned human cloning at
present]?[on 20th, U.S. President George W. Bush called
for regulations of the same content to Congress] (???
????????????????????? 20
???????????????????)
The first segment of example (b) was negative
("banned" expressed a negative sentiment) and a
Continuation relation held between these two seg-
ments. Consequently, the polarity of the second seg-
ment should be negative.
6 Conclusions and Future work
This paper focused on unsupervised discovery
of intra-sentence discourse relations for sentence
level polarity classification. We firstly presented a
discourse scheme based on empirical observations.
Then, an unsupervised method was proposed start-
ing from a small set of cue-phrase-based patterns to
mine high quality common SSRs for each discourse
relation. The performance of discourse classification
was further improved by employing SSRs as features
in supervisedmethods. Experimental results showed
that our methods not only effectively recognized dis-
course relations but also achieved significant im-
provement (p<0.01) in sentence level polarity clas-
sification. Although we were dealing with Chinese
text, the proposed unsupervised method could be
easily generalized to other languages.
The future work will be focused on (1) integrating
more semantic and syntactic information in proposed
unsupervised method; (2) extending our method to
inter-sentence level and then jointly modeling intra-
sentence level and inter-sentence level discourse
constraints on polarity to reach a global optimal in-
ference for polarity classification.
Acknowledgments
This work is partially supported by National 863
program of China (Grant No. 2009AA01Z150),
the Innovation and Technology Fund of Hong Kong
SAR (Project No. GHP/036/09SZ) and 2010/11
CUHK Direct Grants (Project No. EE09743).
References
N. Asher, F. Benamara, and Y.Y. Mathieu. 2008. Distill-
ing opinion in discourse: A preliminary study. Coling
2008: Companion volume: Posters and Demonstra-
tions, pages 5--8.
S. Blair-Goldensohn, K.R. McKeown, and O.C. Ram-
bow. 2007. Building and refining rhetorical-semantic
relationmodels. InProceedings of NAACLHLT, pages
428--435.
L. Carlson, D.Marcu, andM.E. Okurowski. 2001. Build-
ing a discourse-tagged corpus in the framework of
170
rhetorical structure theory. In Proceedings of the Sec-
ond SIGdial Workshop on Discourse and Dialogue-
Volume 16, pages 1--10. Association for Computa-
tional Linguistics.
W. Che, Z. Li, and T. Liu. 2010. Ltp: A chinese language
technology platform. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Demonstrations, pages 13--16. Association for Com-
putational Linguistics.
D.A. Duverle and H. Prendinger. 2009. A novel dis-
course parser based on support vector machine classi-
fication. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2, pages 665--673. Associ-
ation for Computational Linguistics.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. The MIT press.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168--177. ACM.
W.C. Mann and S.A. Thompson. 1988. Rhetorical struc-
ture theory: Toward a functional theory of text organi-
zation. Text-Interdisciplinary Journal for the Study of
Discourse, 8(3):243--281.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 368--375. Associa-
tion for Computational Linguistics.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004.
The penn discourse treebank. InProceedings of the 4th
International Conference on Language Resources and
Evaluation. Citeseer.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing-
Volume 10, pages 79--86. Association for Computa-
tional Linguistics.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: The-
ory and applications, pages 1--10.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning sub-
jective nouns using extraction pattern bootstrapping.
In Proceedings of the seventh conference on Natu-
ral language learning at HLT-NAACL 2003-Volume 4,
pages 25--32. Association for Computational Linguis-
tics.
K. Sadamitsu, S. Sekine, and M. Yamamoto. 2008. Sen-
timent analysis based on probabilistic models using
inter-sentence information.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In Proceedings of
NAACL HLT, pages 300--307.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse level opinion interpretation. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 801--808. Association for
Computational Linguistics.
S. Somasundaran, G. Namata, J. Wiebe, and L. Getoor.
2009. Supervised and unsupervised methods in em-
ploying discourse relations for improving opinion po-
larity classification. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 170--179. Association for Compu-
tational Linguistics.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 149--156. Association for Computational Lin-
guistics.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399--433.
Y.Q. Xia, R.F. Xu, K.F. Wong, and F. Zheng. 2007. The
unified collocation framework for opinion mining. In
International Conference on Machine Learning and
Cybernetics, volume 2, pages 844--850. IEEE.
R. Xu and C. Kit. 2010. Incorporating feature-based and
similarity-based opinion mining--ctl in ntcir-8 moat.
InProceedings of the 8th NTCIRWorkshop, pages 276-
-281.
171
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1137?1146,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Generating Aspect-oriented Multi-Document Summarization with
Event-aspect model
Peng Li1 and Yinglin Wang1 and Wei Gao2and Jing Jiang3
1 Department of Computer Science and Engineering, Shanghai Jiao Tong University
2 Department of Systems Engineering and Engineering Management, Chinese University of Hong Kong
3 School of Information Systems, Singapore Management University
{lipeng, ylwang@sjtu.edu.cn} {wgao@se.cuhk.edu.hk} {jingjiang@smu.edu.sg}
Abstract
In this paper, we propose a novel approach to
automatic generation of aspect-oriented sum-
maries from multiple documents. We first de-
velop an event-aspect LDA model to cluster
sentences into aspects. We then use extend-
ed LexRank algorithm to rank the sentences
in each cluster. We use Integer Linear Pro-
gramming for sentence selection. Key features
of our method include automatic grouping of
semantically related sentences and sentence
ranking based on extension of random walk
model. Also, we implement a new sentence
compression algorithm which use dependency
tree instead of parser tree. We compare our
method with four baseline methods. Quantita-
tive evaluation based on Rouge metric demon-
strates the effectiveness and advantages of our
method.
1 Introduction
In recent years, there has been much interest in
the task of multi-document summarization. In this
paper, we study the task of automatically generat-
ing aspect-oriented summaries from multiple docu-
ments. The goal of aspect-oriented summarization
is to present the most important content to the us-
er in a condensed form and a well-organized struc-
ture to satisfy the user?s needs. A summary should
follow a readable structure and cover all the aspect-
s users are interested in. For example, a summary
about natural disasters should include aspects about
what happened, when/where it happened, reasons,
damages, rescue efforts, etc. and these aspects may
be scattered in multiple articles written by different
news agencies. Our goal is to automatically collect
aspects and construct summaries from multiple doc-
uments.
Aspect-oriented summarization can be used in
many scenarios. First of all, it can be used to gener-
ateWikipedia-like summary articles, especially used
to generate introduction sections that summarizes
the subject of articles before the table of contents
and other elaborate sections. Second, opinionat-
ed text often contains multiple viewpoints about an
issue generated by different people. Summarizing
these multiple opinions can help people easily di-
gest them. Furthermore, combined with search en-
gines and question&answering systems, we can bet-
ter organize the summary content based on aspects
to improve user experience.
Despite its usefulness, the problem of modeling
domain specific aspects for multi-document summa-
rization has not been well studied. The most relevant
work is by (Haghighi and Vanderwende, 2009) on
exploring content models for multi-document sum-
marization. They proposed a HIERSUM model for
finding the subtopics or aspects which are combined
by using KL-divergence criterion for selecting rel-
evant sentences. They introduced a general con-
tent distribution and several specific content distri-
butions to discover the topic and aspects for a s-
ingle document collection. However, the aspects
may be shared not only across documents in a sin-
gle collection, but also across documents in different
topic-related collections. Their model is conceptual-
ly inadequate for simultaneously summarizing mul-
tiple topic-related document collections. Further-
more, their sentence selection method based on KL-
divergence cannot prevent redundancy across differ-
ent aspects.
In this paper, we study how to overcome these
1137
limitations. We hypothesize that comparatively
summarizing topics across similar collections can
improve the effectiveness of aspect-oriented multi-
document summarization. We propose a novel
extraction-based approach which consists of four
main steps listed below:
Sentence Clustering: Our goal in this step is to
automatically identify the different aspects and clus-
ter sentences into aspects (See Section 2). We sub-
stantially extend the entity-aspect model in (Li et al,
2010) for generating general sentence clusters.
Sentence Ranking: In this step, we use an exten-
sion of LexRank algorithm proposed by (Paul et al,
2010) to score representative sentences in each clus-
ter (See Section 3).
Sentence Compression: In this step, we aim to
improve the linguistic quality of the summaries by
simplifying the sentence expressions. We prune sen-
tences using grammatical relations defined on de-
pendency trees for recognizing important clauses
and removing redundant subtrees (See Section 4).
Sentence Selection: Finally, we select one com-
pressed version of the sentences from each aspec-
t cluster. We use Integer Linear Programming
(ILP) algorithm, which optimizes a global objective
function, for sentence selection (McDonald, 2007;
Gillick and Favre, 2009; Sauper and Barzilay, 2009)
(See Section 5).
We evaluate our method using TAC2010 Guided
Summarization task data sets1 (Section 6). Our eval-
uation shows that our method obtains better ROUGE
recall score compared with four baseline methods,
and it also achieve reasonably high-quality aspec-
t clusters in terms of purity.
2 Sentence Clustering
In this step, our goal is to discover event aspects con-
tained in a document set and cluster sentences in-
to aspects. Here we substantially extend the entity-
aspect model in Li et al (2010) and refer to it as
event-aspect model. The main difference between
our event-aspect model and entity-aspect model is
that we introduce an additional layer of event topics
and the separation of general and specific aspects.
1http://www.nist.gov/tac/2010/
Summarization/
Our extension is based upon the following ob-
servations. For example, specific events like
?Columbine Massacre? and ?Malaysia Resort Ab-
duction? can be related to the ?Attack? topic. Each
event consists of multiple articles written by dif-
ferent news agencies. Interesting aspects may in-
clude ?what happened, when, where, perpetrators,
reasons, who affected, damages and countermea-
sures,? etc2. We compared the ?Columbine Mas-
sacre? and ?Malaysia Resort Abduction? data set-
s and found 5 different kinds of words in the text:
(1) stop words that occur frequently in any docu-
ment collection; (2) general content words describ-
ing ?damages? or ?countermeasures? aspect of at-
tacks; (3) specific content words describing ?what
happened?, ?who affected? or ?where? aspect of the
concrete event; (4) background words describing the
general topic of ?Attack?; (5) words that are local to
a single document and do not appear across different
documents. Table 1 shows four sentences related to
two major aspects. We found that the entity-aspect
model does not have enough capacity to cluster sen-
tences into aspects (See Section 6). So we introduce
additional layer to improve the effectiveness of sen-
tence clustering. We also found that their one aspect
per sentence assumption is not very strong in this
scenario. Although a sentence may belong to a sin-
gle general aspect, it still contains multiple specific
aspect words like second sentence in Table 1. There-
fore, We assume that each sentence belongs to both
a general aspect and a specific aspect.
2.1 Event-Aspect Model
Stop words can be ignored by LDA model because
they can be easily identified using a standard stop
word list. Suppose that for a given event topic, there
are in total C specific events for which we need to
simultaneously generate summaries. We can assume
four kinds of unigram language models (i.e. multi-
nomial word distributions). For each event topic,
there is a background model ?B that generates words
commonly used in all documents, and there are AG
general aspect models ?ga (1 ? ga ? AG), where
AG is the number of general aspects. For each spe-
cific event in a topic, there are AS specific aspect
2http://www.nist.gov/tac/2010/
Summarization/Guided-Summ.2010.guidelines.
html
1138
countermeasures
Police/GA are/S close/B to/S identifying/GA someone/B responsible/GA
for/S the/S attack/B .
Investigators/GA do/S not/S know/B how/S many/S suspects/SA
they/S are/S looking/B for/S, but/S reported/B progress/B toward/S
identifying/GA one/S of/S the/S bombers/SA .
what happened, when, where
During/S the/S morning/SA rush/D hour/D on/S July/SA 7/SA terrorists/B
exploded/SA bombs/SA on/S three/D London/SA subway/D trains/SA and/S a/S
double-decker/D bus/SA .
Four/D coordinated/B bombings/SA struck/B central/B London/SA on/SA
July/SA 7/SA, three/D in/S subway/D cars/SA and/S one/D on/S a/S bus/SA .
Table 1: Four sentences on ?COUNTERMEASURES? and ?What, When, Where? aspects from the ?Attack? topic. S:
stop word. B: background word. GA: general aspect word. SA: specific aspect word. D: document word.
models ?sa (1 ? sa ? AS), where AS is the num-
ber of specific aspects, and also there are D doc-
ument models ?d (1 ? d ? D), where D is the
number of documents in this collection. We assume
that these word distributions have a uniform Dirich-
let prior with parameter ?.
We introduce a level distribution ? that control-
s whether we choose a word from ?ga or ?sa. ?
is sampled from Beta(?0, ?1) distribution. We also
introduce an aspect distribution ? that controls how
often a general or a specific aspect occurs in the col-
lection, where ? is sampled from another Dirichlet
prior with parameter ?. There is also a multinomi-
al distribution ? that controls in each sentence how
often we encounter a background word, a document
word, or an aspect word. ? has a Dirichlet prior with
parameter ?.
Let Sd denote the number of sentences in docu-
ment d, Nd,s denote the number of words (after stop
word removal) in sentence s of document d, and
wd,s,n denote the n?th word in this sentence. We
introduce hidden variables zgad,s and zsad,s to indicatethat a sentence s of document d belongs to which
general or specific aspects . We introduce hidden
variables yd,s,n for each word to indicate whether a
word is generated from the background model, the
document model, or the aspect model. We also intro-
duce hidden variables ld,s,n to indicate whether the
n?th word in sentence s of document d is generated
from the general aspect model. Figure 1 describes
the process of generating the whole document col-
lection. The plate notation of the model is shown in
Figure 2. Note that the values of ?0, ?1, ?1, ?2, ?
and ? are fixed. The number of general and specific
aspects AG and AS are also empirically set.
Given a document collection, i.e. the set of all
wd,s,n, our goal is to find the most likely assignmen-
t of zgad,s, zsad,s, yd,s,n and ld,s,n that maximizes dis-tribution p(z,y, l|w;?, ?, ?, ?), where z, y, l and w
represent the set of all z, y, l andw variables, respec-
tively. With the assignment, sentences are naturally
clustered into aspects, and words are labeled as ei-
ther a background word, a document word, a general
aspect word or a specific aspect word.
Inference can be done with Gibbs sampling,
which is commonly used in LDA models (Griffiths
and Steyvers, 2004).
In our experiments, we set ?1 = 5, ?2 = 3,
? = 0.01, ? = 20, ?1 = 10 and ?2 = 10. We
run 100 burn-in iterations through all documents in
a collection to stabilize the distribution of z and y
before collecting samples. We take 10 samples with
a gap of 10 iterations between two samples, and av-
erage over these 10 samples to get the estimation for
the parameters.
After estimating all the distributions, we can find
the values of each zgad,s and zsad,s that gives us sen-tences clustered into general and specific aspects.
3 Sentence Ranking
In this step, we want to order the clustered sen-
tences so that the representative sentences can be
ranked higher in each aspect. Inspired by Paul et
al. (2010), we use an extended LexRank algorithm
to obtain top ranked sentences. LexRank (Erkan and
Radev, 2004) algorithm defines a random walk mod-
1139
1. Draw ?1 ? Dir(?1), ?2 ? Dir(?2), ? ? Dir(?)
Draw ? ? Beta(?0, ?1)
2. For each event topic, there is a background model
?B, and there are general aspect ga, where 1 ?
ga ? AG
(a) draw ?B ? Dir(?)
(b) draw ?ga ? Dir(?)
3. For each document collection, there are specific
aspect sa, where 1 ? sa ? AS
(a) draw ?sa ? Dir(?)
4. For each document d = 1, . . . , D,
(a) draw ?d ? Dir(?)
(b) for each sentence s = 1, . . . , Sd
i. draw zga ? Multi(?1)
ii. draw zsa ? Multi(?2)
iii. for each word n = 1, . . . , Nd,s
A. draw ld,s,n ? Binomial(?)
B. draw yd,s,n ? Multi(?)
C. drawwd,s,n ? Multi(?B) if yd,s,n =
1, wd,s,n ? Multi(?d) if yd,s,n = 2,
wd,s,n ? Multi(?z
sa
d,s) if yd,s,n =
3 and ld,s,n = 1 or wd,s,n ?
Multi(?z
ga
d,s) if yd,s,n = 3 and
ld,s,n = 0
Figure 1: The document generation process.
el on top of a graph that represents sentences to be
summarized as nodes and their similarities as edges.
The LexRank score of a sentence gives the expected
probability that a random walk will visit that sen-
tence in the long run. A variant is called continu-
ous LexRank improved LexRank by making use of
the strength of the similarity links. The continuous
LexRank score can be computed using the following
formula:
L(u) = dN + (1 ? d)
?
v?adj[u]
p(u|v)L(v)
whereL(u) is the LexRank value of sentence u,N is
the total number of nodes in the graph, d is a damp-
ing factor for the convergence of the method, and
p(u|v) is the jumping probability between sentence
u and its neighboring sentence v. p(u|v) is defined
using content similarity function sim(u, v) between
two sentences:
T
yd? d
SD sd ,wSA?
?
C
?
pi ?
?
gaz saz
l
?
GA
B?
1? 2?
1? 2?
Figure 2: The event-aspect model.
p(u|v) = sim(u, v)?
z?adj[v] sim(z, v)
The major extension is to modify this jumping
probability so as to favor visiting representative sen-
tences. More specifically, we scale sim(u, v) by the
likelihood that the two sentences represent the same
general aspect ga or specific aspect sa:
sim?(u, v) = sim(u, v)[
AG?
ga=1
P (ga|u)P (ga|v)
+
AS?
sa=1
P (sa|u)P (sa|v)]
where the value P (ga|u) and P (sa|u) can be
computed by our event-aspect model. We define
sim(u, v) as the tf ? idf weighted cosine similar-
ity between two sentences.
We found that sentence ranking is better con-
ducted before the compression because the pre-
compressed sentences are more informative and the
similarity function in LexRank can be better off with
the complete information.
4 Sentence Compression
It has been shown that sentence compression can
improve linguistic quality of summaries (Zajic et
al., 2007; Gillick et al, 2010). Commonly used
?Syntactic parse and trim? approach may produce
poor compression results. For example, given the
sentence ?We have friends whose children go to
Columbine, the freshman said?, the procedure tries
to remove the clause ?the freshman said? from the
parse tree by using the ?SBAR? label to locate the
1140
clause, and will result in ?whose children go to
Columbine?, which is not adequate. Furthermore,
some important temporal modifier, numeric modifier
and clausal complement need to be retained because
they reflect content aspects of the summary. There-
fore, we propose the ?dependency parse and trim?
approach, which prunes sentences based on depen-
dency tree representations, using English grammati-
cal relations to recognize clauses and remove redun-
dant structures. Table 2 shows two examples by re-
moving redundant auxiliary clauses. Below is the
sentence compression procedure:
1. Select possible subtree root nodes using gram-
matical relations, such as clausal complement,
complementizer, or parataxis 3.
2. Decide which subtree root node can be the root
of clause. If this root contains maximum num-
ber of child nodes and the collection of all child
edges include object or auxiliary relations, it is
selected as the root node.
3. Remove redundant modifiers such as adverbial-
s, relative clause modifiers and abbreviations,
participials and infinitive modifiers.
4. Traverse the subtrees and generate all possible
compression alternatives using the subtree root
node, then keep the top two longest sub sen-
tences.
5. Drop the sub sentences shorter than 5 words.
5 Sentence Selection
After sentence pruning, we prepare for the final
event summary generation process. In this step, we
select one compressed version of the sentence from
each aspect cluster. To avoid redundancy between
aspects, we use Integer Linear Programming to opti-
mize a global objective function for sentence selec-
tion. Inspired by (Sauper and Barzilay, 2009), we
formulate the optimization problem based on sen-
tence ranking information. More specifically, we
3The parataxis relation is a relation between the main verb
of a clause and other sentential elements, such as a sentential
parenthetical, colon, or semicolon
Original Compressed
When rescue workers
arrived, they said, on-
ly one of his limbs was
visible.
When rescue workers
arrived, only one of his
limbs was visible.
Two days earlier, a
massacre by two s-
tudents at Columbine
High, whose teams are
called the Rebels, left
15 people dead and
dozens wounded.
Two days earlier, a
massacre by two stu-
dents at Columbine
High, left 15 peo-
ple dead and dozens
wounded.
Table 2: Example compressed sentences.
would like to select exactly one compressed sen-
tence which receives the highest possible ranking s-
core from each aspect cluster subject to a series of
constraints, such as redundancy and length. We em-
ployed lp solver 4, an efficient mixed integer pro-
gramming solver using the Branch-and-Bound algo-
rithm to select sentences.
Assume that there are in total K aspects in an
event topic. For each aspect j, there are in total R
ranked sentences. The variables Sjl is a binary indi-
cator of the sentence. That is, Sjl= 1 if the sentence
is included in the final summary, and Sjl = 0 other-
wise. l is the ranked position of the sentence in this
aspect cluster.
Objective Function
Top ranked sentences are the most relevant corre-
sponding to the related aspects which we want to in-
clude in the final summary. Thus we try to minimize
the ranks of the sentences to improve the overall re-
sponsiveness.
min(
K?
j=1
Rj?
l=1
l ? Sjl)
Exclusivity Constraints
To prevent redundancy in each aspect, we just
choose one sentence from each general or specific
aspect cluster. The constraint is formulated as fol-
lows:
Rj?
l=1
Sjl = 1 ?j ? {1 . . .K}
4http://lpsolve.sourceforge.net/5.5/
1141
Redundancy Constraints
We also want to prevent redundancy across differ-
ent aspects. If sentence-similarity sim(sjl, sj?l?) be-
tween sentence sjl and sj?l? is above 0.5, then we
drop the pair and choose one sentence ranked higher
from the pair otherwise. This constraint is formulat-
ed as follows:
(Sjl + Sj?l?) ? sim(sjl, sj?l?) ? 0.5
?j, j? ? {1 . . .K}?l ? {1 . . . Rj}?l? ? {1 . . . Rj?}
Length Constraints
We add this constraint to ensure that the length of
the final summary is limited to L words.
K?
j=1
Rj?
l=1
lenjl ? Sjl ? L
where lenjl is the length of Sjl.
6 Evaluation
In order to systematically evaluate our method, we
want to check (1) whether the whole system is effec-
tive, which means to quantitatively evaluate summa-
ry quality, and (2) whether individual components
like clustering and compression algorithms are use-
ful.
6.1 Data
We use TAC2010 Summarization task data set for
the summary content evaluation. This data set pro-
vides 46 events. Each event falls into a predefined
event topic. Each specific event includes an even-
t statement and 20 relevant newswire articles which
have been divided into 2 sets: Document Set A and
Document Set B. Each document set has 10 docu-
ments, and all the documents in Set A chronologi-
cally precede the documents in Set B. We just use
document Set A for our task. Assessors wrote mod-
el summaries for each event, so we can compare
our automatic generated summaries with the model
summaries. We combine topic related data sets to-
gether, then these data sets simultaneously annotated
by our Event-aspect model. After labeling process,
we run sentence ranking, compression and selection
module to get final aspect-oriented summarizations.
6.2 Quality of summary
We use the ROUGE (Lin and Hovy, 2003) metric for
measuring the summarization system performance.
Ideally, a summarization criterion should be more
recall oriented. So the average recall of ROUGE-
1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 and
ROUGE-L were computed by running ROUGE-
1.5.5 with stemming but no removal of stop word-
s. We compare our method with the following four
baseline methods.
Baseline 1
In this baseline, we try to compare different sen-
tence clustering algorithms in the multi-document
summarization scenario. First, we use CLUTO 5 to
do K-means clustering. Then we try entity-aspect
model proposed by Li et al (2010) to do sentence
clustering. Entity-aspect model is similar with ?HI-
ERSUM? content model proposed by Haghighi and
Vanderwende (2009). We use the same ranking,
compression, and selection components to generate
aspect-oriented summaries for comparison.
Baseline 2
In this baseline, we compare our method with
traditional ranking and selection summary genera-
tion framework (Erkan and Radev, 2004; Nenkova
and Vanderwende, 2005) to show that our sentence
clustering component is necessary in aspect-oriented
summarization system. Also we want check whether
sentence ranking combined with greedy based sen-
tence selection can prevent redundancy effective-
ly. We follow LexRank based sentence ranking
combined with greedy sentence selection methods.
We implement two greedy algorithms (Zhang et al,
2008; Paul et al, 2010). One is to select the top
ranked sentence simultaneously by removing 10 re-
dundant neighbor sentences from the sentence sim-
ilarity graph if the summary length is less then 100
words. This is repeated until the graph cannot be
partitioned. The similarity graph building threshold
is 0.3, damping factor is 0.2 and error tolerance for
Power Method in LexRank is 0.1. The other is to se-
lect top ranked sentences as long as the redundancy
score (similarity) between a candidate sentence and
5http://glaros.dtc.umn.edu/gkhome/cluto/
cluto/overview
1142
current summary is under 0.5. This is repeated until
the summary reaches a 100 word length limit.
Baseline 3
In this baseline, we compare our ILP based sen-
tence selection with KL-divergence based sentence
selection. The KL-divergence formula we use is be-
low,
KL(PS ||QD) =
?
w
P (w) log P (w)Q(w)
where P (S) is the empirical unigram distribution of
the candidate summary S, and Q(D) is the unigram
distribution of document collection D. We only re-
placed our selection method with the KL-divergence
selection method. Other parts are the same. After
ranking sentences for each aspect, we add the sen-
tence with the highest ranking score from each as-
pect sentence cluster as long as the KL-divergence
between candidate and current summary does not
decrease. This is repeated until the summary reach-
es a 100 word length limit. To our knowledge, this
is the first work to directly compare Integer Lin-
ear Programming based sentence selection with KL-
divergence based sentence selection in summariza-
tion generation framework.
Baseline 4
In this baseline, we directly compare our method
with ?HIERSUM? proposed by (Haghighi and Van-
derwende, 2009). As in Baseline 1, we use entity-
aspect model to approximate ?HIERSUM? mod-
el. We replace unigram distribution of P (w) in
KL-divergence with learned distribution estimated
by ?HIERSUM? model. The KL-divergence based
greedy sentence selection algorithm is similar to
Baseline 3.
For fair comparison, Baselines 1, 2, 3 and 4 use
the same sentence compression algorithm and have
the summary length no more then 100 words. In
Table 3, we show the average ROUGE recall of 46
summaries generated by our method and four base-
line methods. We can see that our method gives
better Rouge recall measures then the four baseline
methods. For BL-1, we can see that LDA-based sen-
tence clustering is better then k-means. For BL-2,
we can see that traditional ranking plus greedy selec-
tion summary generation framework is not suitable
for the aspect-oriented summarization task. More
specifically, greedy-based sentence selection can not
prevent redundancy effectively. BL-3 evaluation re-
sults showed that ILP-based sentence selection is
better then KL-divergence selection in terms of pre-
venting redundancy across different aspects. The
measurement performance between BL-3 and BL-
4 is close. They use the same KL-divergence based
sentence selection, but topic model they use are d-
ifferent, and also BL-3 has a sentence ranking pro-
cess. The Rouge recall of our method is better than
BL-4. It is expected because our event-aspect mod-
el can better find the aspects and also prove that
our LexRank based sentence ranking combined with
ILP-based sentence selection can prevent redundan-
cy.
Due to TAC2010 summarization community just
compute ROUGE-2 and ROUGE-SU4 metrics for
participants, our ROUGE-2 metric ranked 11 out
of 23, ROUGE-SU4 metric ranked 12 out of 23.
They use MEAD6 as their baseline approach. The
ROUGE-2 score of our approach achieve 0.06508
higher than MEAD?s 0.05929. The ROUGE-SU4 s-
core of our approach achieve 0.10146 higher than
MEAD?s 0.09112. Many systems that get high-
er performances leverage domain knowledge bases
like Wikipedia or training data, but we didn?t. The
advantage of our method is that we generate sum-
maries with totally unsupervised framework and this
approach is domain adaptive.
6.3 Quality of aspect-oriented sentence clusters
To judge the quality of the aspect-oriented sentence
clusters, we ask the human judges to group the
ground truth sentences based on the aspect related-
ness in each event topic. We then compute the pu-
rity of the automatically generated clusters against
the human judged clusters. The results are shown
in Table 4. In our experiments, we set the number
of general aspect clusters AG is 5 and specific as-
pect clusters AS is 3. We can see from Table 4 that
our generated aspect clusters can achieve reasonably
good performance.
6http://www.summarization.com/mead/
1143
Rouge Average Recall
Method ROUGE-1 ROUGE-2 ROUGE-SU4 ROUGE-W-1.2 ROUGE-L
BL-1 k-means 0.21895 0.03689 0.06644 0.06683 0.19208
entity-aspect 0.26082 0.05082 0.08286 0.08055 0.22976
BL-2 greedy 1 0.27802 0.04872 0.08302 0.08488 0.24426
greedy 2 0.27898 0.04723 0.08275 0.08500 0.24430
BL-3 KL-Div 0.29286 0.05369 0.09117 0.08827 0.25100
BL-4 HIERSUM 0.28736 0.05502 0.08932 0.08923 0.25285
Without compression 0.30563 0.05983 0.09513 0.09468 0.25487
Our Method 0.32641 0.06508 0.10146 0.09998 0.28610
Table 3: ROUGE evaluation results on TAC2010 Summarization data sets
Category A Purity
Accidents and Natural Disasters 7 0.613
Attacks 8 0.658
Health and Safety 5 0.724
Endangered Resources 4 0.716
Investigations and Trials 6 0.669
Table 4: The true numbers of aspects as judged by the
human annotator (A), and the purity of the clusters.
Category Average Score
Accidents and Natural Disasters 2.4
Attacks 2.3
Health and Safety 2.6
Endangered Resources 2.5
Investigations and Trials 2.4
Table 5: The average score of each event topic.
6.4 Quality of sentence compression
To judge the quality of the dependency tree based
sentence compression algorithm, we ask the human
judges to choose 20 sentences from each event top-
ic then score them. The judges follow 3-point scale
to score each compressed sentence: 1 means poor,
2 means barely acceptable, and 3 means good. We
then compute the average scores. The results are
shown in Table 5. To evaluate the effectiveness of
sentence compression component, we conduct the
system without sentence compression component,
then compare it with our system. In Table 3, we
can see that sentence compression can improve the
system performance.
7 Related Work
Our event-aspect model is related to a number of
previous extensions of LDA models. Chemudugun-
ta et al (2007) proposed to introduce a background
topic and document-specific topics. Our background
and document language models are similar to theirs.
However, they still treat documents as bags of words
rather then sets of sentences as in our models. Titov
and McDonald (2008) exploited the idea that a short
paragraph within a document is likely to be about
the same aspect. The way we separate words in-
to stop words, background words, document word-
s and aspect words bears similarity to that used
in (Daume? III and Marcu, 2006; Haghighi and Van-
derwende, 2009). Paul and Girju (2010) proposed a
topic-aspect model for simultaneously finding topic-
s and aspects. The most related extension is entity-
aspect model proposed by Li et al (2010). The main
difference between event-aspect model and entity-
aspect model is our model further consider aspect
granularity and add a layer to model topic-related
events.
Filippova and Strube (2008) proposed a depen-
dency tree based sentence compression algorithm.
Their approach need a large corpus to build language
model for compression, whereas we prune depen-
dency tree using grammatical rules.
Paul et al (2010) proposed to modify LexRank
algorithm using their topic-aspect model. But their
task is to summarize contrastive viewpoints in opin-
ionated text. Furthermore, they use a simple greedy
approach for constructing summary.
McDonald (2007) proposed to use Integer Linear
Programming framework in multi-document sum-
1144
marization. And Sauper and Barzilay (2009) use in-
teger linear programming framework to automatical-
ly generate Wikipedia articles. There is a fundamen-
tal difference between their method and ours. They
used trained perceptron algorithm for ranking ex-
cerpts, whereas we give an extended LexRank with
integer linear programming to optimize sentence se-
lection for our aspect-oriented multi-document sum-
marization.
8 Conclusions and Future Work
In this paper, we study the task of automatically
generating aspect-oriented summary from multiple
documents. We proposed an event-aspect model
that can automatically cluster sentences into aspect-
s. We then use an extension of the LexRank algo-
rithm to rank sentences. We took advantage of the
output generated by the event-aspect model to mod-
ify jumping probabilities so as to favor visiting rep-
resentative sentence. We also proposed dependen-
cy tree compression algorithm to prune sentence for
improving linguistic quality of the summaries. Fi-
nally we use Integer Linear Programming Frame-
work to select aspect relevant sentences. We con-
ducted quantitative evaluation using standard test
data sets. We found that our method gave overal-
l better ROUGE scores than four baseline methods,
and the new sentence clustering and compression al-
gorithm are robust.
There are a number of directions we plan to pur-
sue in the future in order to improve our method.
First, we can possibly apply more linguistic knowl-
edge to improve the quality of sentence compres-
sion. Currently the sentence compression algorith-
m may generate meaningless subtrees. It is rela-
tively hard to decide which clause is redundant in
terms of summarization. Second, we may explore
more domain knowledge to improve the quality of
aspect-oriented summaries. For example, we know
that the ?who-affected? aspect is related to person,
and ?when, where? are related to Time and Location.
we can import Name Entity Recognition to anno-
tate these phrases and then help locate relevant sen-
tences. Third, we want to extend our event-aspect
model to simultaneously find topics and aspects.
Acknowledgments
This work was supported by the National Nat-
ural Science Foundation of China (NSFC No.
60773088), the National High-tech R&D Program
of China (863 Program No. 2009AA04Z106), and
the Key Program of Basic Research of Shanghai
Municipal S&T Commission (No. 08JC1411700).
References
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling general and specific aspects
of documents with a probabilistic topic model. In Ad-
vances in Neural Information Processing Systems 19,
pages 241?248.
Hal. Daume? III and Daniel. Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 305?312.
Association for Computational Linguistics.
Gu?nes. Erkan and Dragomir Radev. 2004. LexRank:
Graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search, 22(1):457?479.
K. Filippova and M. Strube. 2008. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference, pages 25?32. Association for Computa-
tional Linguistics.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
gauge Processing, pages 10?18.
Dan Gillick, Benoit Favre, D. Hakkani-Tur, B. Bohnet,
Y. Liu, and S. Xie. 2010. The icsi/utd summarization
system at tac 2009. In Proceedings of the Second Text
Analysis Conference, Gaithersburg, Maryland, USA:
National Institute of Standards and Technology.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National A-
cademy of Sciences of the United States of America,
101(Suppl. 1):5228?5235.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics on
ZZZ, pages 362?370. Association for Computational
Linguistics.
1145
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gen-
erating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the Joint Conference of the 48th Annual Meeting of the
ACL. Association for Computational Linguistics.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 71?78. Association for Computation-
al Linguistics.
RyanMcDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. Advances in
Information Retrieval, pages 557?564.
A. Nenkova and L. Vanderwende. 2005. The impact
of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-101.
Michael J. Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In In AAAI-2010: Twenty-Fourth Con-
ference on Artificial Intelligence.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 66?76, Morristown, NJ, USA.
Association for Computational Linguistics.
Christina Sauper and Regina Barzilay. 2009. Automati-
cally generating wikipedia articles: A structure-aware
approach. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 208?216, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceeding
of the 17th International Conference on World Wide
Web, pages 111?120.
D. Zajic, B.J. Dorr, J. Lin, and R. Schwartz. 2007. Multi-
candidate reduction: Sentence compression as a tool
for document summarization tasks. Information Pro-
cessing & Management, 43(6):1549?1570.
Jin. Zhang, Xueqi. Cheng, and Hongbo. Xu. 2008. GSP-
Summary: a graph-based sub-topic partition algorith-
m for summarization. In Proceedings of the 4th Asi-
a information retrieval conference on Information re-
trieval technology, pages 321?334. Springer-Verlag.
1146
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 112?122,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Query Weighting for Ranking Model Adaptation
Peng Cai1, Wei Gao2, Aoying Zhou1, and Kam-Fai Wong2,3
1East China Normal University, Shanghai, China
pengcai2010@gmail.com, ayzhou@sei.ecnu.edu.cn
2The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
{wgao, kfwong}@se.cuhk.edu.hk
3Key Laboratory of High Confidence Software Technologies, Ministry of Education, China
Abstract
We propose to directly measure the impor-
tance of queries in the source domain to the
target domain where no rank labels of doc-
uments are available, which is referred to
as query weighting. Query weighting is a
key step in ranking model adaptation. As
the learning object of ranking algorithms is
divided by query instances, we argue that
it?s more reasonable to conduct importance
weighting at query level than document level.
We present two query weighting schemes.
The first compresses the query into a query
feature vector, which aggregates all document
instances in the same query, and then con-
ducts query weighting based on the query fea-
ture vector. This method can efficiently esti-
mate query importance by compressing query
data, but the potential risk is information loss
resulted from the compression. The second
measures the similarity between the source
query and each target query, and then com-
bines these fine-grained similarity values for
its importance estimation. Adaptation exper-
iments on LETOR3.0 data set demonstrate
that query weighting significantly outperforms
document instance weighting methods.
1 Introduction
Learning to rank, which aims at ranking documents
in terms of their relevance to user?s query, has been
widely studied in machine learning and information
retrieval communities (Herbrich et al, 2000; Fre-
und et al, 2004; Burges et al, 2005; Yue et al,
2007; Cao et al, 2007; Liu, 2009). In general,
large amount of training data need to be annotated
by domain experts for achieving better ranking per-
formance. In real applications, however, it is time
consuming and expensive to annotate training data
for each search domain. To alleviate the lack of
training data in the target domain, many researchers
have proposed to transfer ranking knowledge from
the source domain with plenty of labeled data to the
target domain where only a few or no labeled data is
available, which is known as ranking model adapta-
tion (Chen et al, 2008a; Chen et al, 2010; Chen et
al., 2008b; Geng et al, 2009; Gao et al, 2009).
Intuitively, the more similar an source instance
is to the target instances, it is expected to be more
useful for cross-domain knowledge transfer. This
motivated the popular domain adaptation solution
based on instance weighting, which assigns larger
weights to those transferable instances so that the
model trained on the source domain can adapt more
effectively to the target domain (Jiang and Zhai,
2007). Existing instance weighting schemes mainly
focus on the adaptation problem for classification
(Zadrozny, 2004; Huang et al, 2007; Jiang and Zhai,
2007; Sugiyama et al, 2008).
Although instance weighting scheme may be ap-
plied to documents for ranking model adaptation,
the difference between classification and learning to
rank should be highlighted to take careful consider-
ation. Compared to classification, the learning ob-
ject for ranking is essentially a query, which con-
tains a list of document instances each with a rel-
evance judgement. Recently, researchers proposed
listwise ranking algorithms (Yue et al, 2007; Cao
et al, 2007) to take the whole query as a learning
object. The benchmark evaluation showed that list-
112
Target domainSource Domain
d1(s1) d2(s1)
d3(s1)
d1(s2)
d2(s2)
d3(s2)
d2(t1)
d1(t2)d2(t2) d3(t2)
d3(t1)
d1(t1)
(a) Instance based weighting
d2(s1)
d1(s1)
d3(s1)
d1(s2)d2(s2)d3(s2)
qs2
qs1
d3(t1)
d2(t1)
d1(t1)
d1(t2)d2(t2)d3(t2)
qt1
qt2
Target domainSource Domain
(b) Query based weighting
Figure 1: The information about which document instances belong to the same query is lost in document instance
weighting scheme. To avoid losing this information, query weighting takes the query as a whole and directly measures
its importance.
wise approach significantly outperformed pointwise
approach, which takes each document instance as in-
dependent learning object, as well as pairwise ap-
proach, which concentrates learning on the order of
a pair of documents (Liu, 2009). Inspired by the
principle of listwise approach, we hypothesize that
the importance weighting for ranking model adapta-
tion could be done better at query level rather than
document level.
Figure 1 demonstrates the difference between in-
stance weighting and query weighting, where there
are two queries qs1 and qs2 in the source domain
and qt1 and qt2 in the target domain, respectively,
and each query has three retrieved documents. In
Figure 1(a), source and target domains are repre-
sented as a bag of document instances. It is worth
noting that the information about which document
instances belong to the same query is lost. To
avoid this information loss, query weighting scheme
shown as Figure 1(b) directly measures importance
weight at query level.
Instance weighting makes the importance estima-
tion of document instances inaccurate when docu-
ments of the same source query are similar to the
documents from different target queries. Take Fig-
ure 2 as a toy example, where the document in-
stance is represented as a feature vector with four
features. No matter what weighting schemes are
used, it makes sense to assign high weights to source
queries qs1 and qs2 because they are similar to tar-
get queries qt1 and qt2, respectively. Meanwhile, the
source query qs3 should be weighted lower because
<d1
s1
>=( 5, 1, 0 ,0 )
<d2
s1
>=( 6, 2, 0 ,0 )
<d1
s2
>=( 0, 0, 5, 1)
<d2
s2
>=( 0, 0, 6, 2)
<d1
s3
>=( 5, 1, 0, 0)
<d2
s3
>=( 0, 0, 6, 2)
<d1
t1
>=(5, 1, 0 ,0 )
<d2
t1
>=(6, 2, 0 ,0 )
<d1
t2
>=( 0, 0, 5, 1)
<d2
t2
>=( 0, 0, 6, 2)
q
s1
q
s2
q
s3
qt1
qt2
Figure 2: A toy example showing the problem of docu-
ment instance weighting scheme.
it?s not quite similar to any of qt1 and qt2 at query
level, meaning that the ranking knowledge from qs3
is different from that of qt1 and qt2 and thus less
useful for the transfer to the target domain. Unfor-
tunately, the three source queries qs1, qs2 and qs3
would be weighted equally by document instance
weighting scheme. The reason is that all of their
documents are similar to the two document instances
in target domain despite the fact that the documents
of qs3 correspond to their counterparts from different
target queries.
Therefore, we should consider the source query
as a whole and directly measure the query impor-
tance. However, it?s not trivial to directly estimate
113
a query?s weight because a query is essentially pro-
vided as a matrix where each row represents a vector
of document features. In this work, we present two
simple but very effective approaches attempting to
resolve the problem from distinct perspectives: (1)
we compress each query into a query feature vec-
tor by aggregating all of its document instances, and
then conduct query weighting on these query feature
vectors; (2) we measure the similarity between the
source query and each target query one by one, and
then combine these fine-grained similarity values to
calculate its importance to the target domain.
2 Instance Weighting Scheme Review
The basic idea of instance weighting is to put larger
weights on source instances which are more simi-
lar to target domain. As a result, the key problem
is how to accurately estimate the instance?s weight
indicating its importance to target domain. (Jiang
and Zhai, 2007) used a small number of labeled data
from target domain to weight source instances. Re-
cently, some researchers proposed to weight source
instance only using unlabeled target instances (Shi-
modaira, 2000; Sugiyama et al, 2008; Huang et al,
2007; Zadrozny, 2004; Gao et al, 2010). In this
work, we also focus on weighting source queries
only using unlabeled target queries.
(Gao et al, 2010; Ben-David et al, 2010) pro-
posed to use a classification hyperplane to separate
source instances from target instances. With the do-
main separator, the probability that a source instance
is classified to target domain can be used as the im-
portance weight. Other instance weighting methods
were proposed for the sample selection bias or co-
variate shift in the more general setting of classifier
learning (Shimodaira, 2000; Sugiyama et al, 2008;
Huang et al, 2007; Zadrozny, 2004). (Sugiyama et
al., 2008) used a natural model selection procedure,
referred to as Kullback-Leibler divergence Impor-
tance Estimation Procedure (KLIEP), for automat-
ically tuning parameters, and showed that its impor-
tance estimation was more accurate. The main idea
is to directly estimate the density function ratio of
target distribution pt(x) to source distribution ps(x),
i.e. w(x) = pt(x)ps(x) . Then model w(x) can be used to
estimate the importance of source instances. Model
parameters were computed with a linear model by
minimizing the KL-divergence from pt(x) to its esti-
mator p?t(x). Since p?t(x) = w?(x)ps(x), the ultimate
objective only contains model w?(x).
For using instance weighting in pairwise rank-
ing algorithms, the weights of document instances
should be transformed into those of document
pairs (Gao et al, 2010). Given a pair of documents
?xi, xj? and their weights wi and wj , the pairwise
weight wij could be estimated probabilistically as
wi ?wj . To consider query factor, query weight was
further estimated as the average value of the weights
over all the pairs, i.e., wq = 1M
?
i,j wij , where M
is the number of pairs in query q. Additionally, to
take the advantage of both query and document in-
formation, a probabilistic weighting for ?xi, xj? was
modeled by wq ? wij . Through the transformation,
instance weighting schemes for classification can be
applied to ranking model adaptation.
3 Query Weighting
In this section, we extend instance weighting to di-
rectly estimate query importance for more effec-
tive ranking model adaptation. We present two
query weighting methods from different perspec-
tives. Note that although our methods are based on
domain separator scheme, other instance weighting
schemes such as KLIEP (Sugiyama et al, 2008) can
also be extended similarly.
3.1 Query Weighting by Document Feature
Aggregation
Our first query weighting method is inspired by the
recent work on local learning for ranking (Geng et
al., 2008; Banerjee et al, 2009). The query can be
compressed into a query feature vector, where each
feature value is obtained by the aggregate of its cor-
responding features of all documents in the query.
We concatenate two types of aggregates to construct
the query feature vector: the mean ?? = 1|q|
?|q|
i=1 f?i
and the variance ?? = 1|q|
?|q|
i=1(f?i ? ??)2, where f?i
is the feature vector of document i and |q| denotes
the number of documents in q . Based on the ag-
gregation of documents within each query, we can
use a domain separator to directly weight the source
queries with the set of queries from both domains.
Given query data sets Ds = {qis}mi=1 and Dt =
{qjt }nj=1 respectively from the source and target do-
114
Algorithm 1 Query Weighting Based on Document Feature Aggregation in the Query
Input:
Queries in the source domain, Ds = {qis}mi=1;
Queries in the target domain, Dt = {qjt }nj=1;
Output:
Importance weights of queries in the source domain, IWs = {Wi}mi=1;
1: ys = ?1, yt = +1;
2: for i = 1; i ? m; i + + do
3: Calculate the mean vector ??i and variance vector ??i for qis;
4: Add query feature vector q?is = (??i, ??i, ys) to D?s ;
5: end for
6: for j = 1; j ? n; j + + do
7: Calculate the mean vector ??j and variance vector ??j for qjt ;
8: Add query feature vector q?jt = (??j , ??j , yt) to D?t;
9: end for
10: Find classification hyperplane Hst which separates D?s from D?t;
11: for i = 1; i ? m; i + + do
12: Calculate the distance of q?is to Hst, denoted as L(q?is);
13: Wi = P (qis ? Dt) = 11+exp(??L(q?is)+?)
14: Add Wi to IWs;
15: end for
16: return IWs;
mains, we use algorithm 1 to estimate the proba-
bility that the query qis can be classified to Dt, i.e.
P (qis ? Dt), which can be used as the importance of
qis relative to the target domain. From step 1 to 9,D?s
and D?t are constructed using query feature vectors
from source and target domains. Then, a classifi-
cation hyperplane Hst is used to separate D?s from
D?t in step 10. The distance of the query feature
vector q?is from Hst are transformed to the probabil-
ity P (qis ? Dt) using a sigmoid function (Platt and
Platt, 1999).
3.2 Query Weighting by Comparing Queries
across Domains
Although the query feature vector in algorithm 1 can
approximate a query by aggregating its documents?
features, it potentially fails to capture important fea-
ture information due to the averaging effect during
the aggregation. For example, the merit of features
in some influential documents may be canceled out
in the mean-variance calculation, resulting in many
distorted feature values in the query feature vector
that hurts the accuracy of query classification hy-
perplane. This urges us to propose another query
weighting method from a different perspective of
query similarity.
Intuitively, the importance of a source query to
the target domain is determined by its overall sim-
ilarity to every target query. Based on this intu-
ition, we leverage domain separator to measure the
similarity between a source query and each one of
the target queries, where an individual domain sep-
arator is created for each pair of queries. We esti-
mate the weight of a source query using algorithm 2.
Note that we assume document instances in the same
query are conditionally independent and all queries
are independent of each other. In step 3, D?qis is con-
structed by all the document instances {x?k} in query
qis with the domain label ys. For each target query
qjt , we use the classification hyperplane Hij to es-
timate P (x?k ? D?qjt
), i.e. the probability that each
document x?k of qis is classified into the document set
of qjt (step 8). Then the similarity between qis and q
j
t
is measured by the probability P (qis ? q
j
t ) at step 9.
Finally, the probability of qis belonging to the target
domain P (qis ? Dt) is calculated at step 11.
It can be expected that algorithm 2 will generate
115
Algorithm 2 Query Weighting by Comparing Source and Target Queries
Input:
Queries in source domain, Ds = {qis}mi=1;
Queries in target domain, Dt = {qjt }nj=1;
Output:
Importance weights of queries in source domain, IWs = {Wi}mi=1;
1: ys = ?1, yt = +1;
2: for i = 1; i ? m; i + + do
3: Set D?qis={x?k, ys)}
|qis|
k=1;
4: for j = 1; j ? n; j + + do
5: Set D?
qjt
={x?k? , yt)}
|qjt |
k?=1;
6: Find a classification hyperplane Hij which separates D?qis from D
?
qjt
;
7: For each k, calculate the distance of x?k to Hij , denoted as L(x?k);
8: For each k, calculate P (x?k ? D?qjt
) = 11+exp(??L(x?k)+?) ;
9: Calculate P (qis ? q
j
t ) = 1|qis|
?|qis|
k=1 P (x?k ? D
?
qjt
);
10: end for
11: Add Wi = P (qis ? Dt) = 1n
?n
j=1 P (qis ? q
j
t ) to IWs;
12: end for
13: return IWs;
more precise measures of query similarity by utiliz-
ing the more fine-grained classification hyperplane
for separating the queries of two domains.
4 Ranking Model Adaptation via Query
Weighting
To adapt the source ranking model to the target do-
main, we need to incorporate query weights into ex-
isting ranking algorithms. Note that query weights
can be integrated with either pairwise or listwise al-
gorithms. For pairwise algorithms, a straightforward
way is to assign the query weight to all the document
pairs associated with this query. However, document
instance weighting cannot be appropriately utilized
in listwise approach. In order to compare query
weighting with document instance weighting, we
need to fairly apply them for the same approach of
ranking. Therefore, we choose pairwise approach to
incorporate query weighting. In this section, we ex-
tend Ranking SVM (RSVM) (Herbrich et al, 2000;
Joachims, 2002) ? one of the typical pairwise algo-
rithms for this.
Let?s assume there are m queries in the data set
of source domain, and for each query qi there are
?(qi) number of meaningful document pairs that can
be constructed based on the ground truth rank labels.
Given ranking function f , the objective of RSVM is
presented as follows:
min
1
2
||w?||2 + C
m
?
i=1
?(qi)
?
j=1
?ij (1)
subject to zij ? f(w?, x?j(1)qi ? x?
j(2)
qi ) ? 1 ? ?ij
?ij ? 0, i = 1, . . . ,m; j = 1, . . . , ?(qi)
where x?j(1)qi and x?
j(2)
qi are two documents with dif-
ferent rank label, and zij = +1 if x?j(1)qi is labeled
more relevant than x?j(2)qi ; or zij = ?1 otherwise.
Let ? = 12C and replace ?ij with Hinge Loss func-
tion (.)+, Equation 1 can be turned to the following
form:
min ?||w?||2+
m
?
i=1
?(qi)
?
j=1
(
1 ? zij ? f(w?, x?j(1)qi ? x?
j(2)
qi )
)+
(2)
Let IW (qi) represent the importance weight of
source query qi. Equation 2 is extended for inte-
grating the query weight into the loss function in a
116
straightforward way:
min ?||w?||2+
m
?
i=1
IW (qi) ?
?(qi)
?
j=1
(
1 ? zij ? f(w?, x?j(1)qi ? x?
j(2)
qi )
)+
where IW (.) takes any one of the weighting
schemes given by algorithm 1 and algorithm 2.
5 Evaluation
We evaluated the proposed two query weighting
methods on TREC-2003 and TREC-2004 web track
datasets, which were released through LETOR3.0 as
a benchmark collection for learning to rank by (Qin
et al, 2010). Originally, different query tasks were
defined on different parts of data in the collection,
which can be considered as different domains for us.
Adaptation takes place when ranking tasks are per-
formed by using the models trained on the domains
in which they were originally defined to rank the
documents in other domains. Our goal is to demon-
strate that query weighting can be more effective
than the state-of-the-art document instance weight-
ing.
5.1 Datasets and Setup
Three query tasks were defined in TREC-2003 and
TREC-2004 web track, which are home page finding
(HP), named page finding (NP) and topic distilla-
tion (TD) (Voorhees, 2003; Voorhees, 2004). In this
dataset, each document instance is represented by 64
features, including low-level features such as term
frequency, inverse document frequency and docu-
ment length, and high-level features such as BM25,
language-modeling, PageRank and HITS. The num-
ber of queries of each task is given in Table 1.
The baseline ranking model is an RSVM directly
trained on the source domain without using any
weighting methods, denoted as no-weight. We im-
plemented two weighting measures based on do-
main separator and Kullback-Leibler divergence, re-
ferred to DS and KL, respectively. In DS measure,
three document instance weighting methods based
on probability principle (Gao et al, 2010) were
implemented for comparison, denoted as doc-pair,
doc-avg and doc-comb (see Section 2). In KL mea-
sure, there is no probabilistic meaning for KLweight
Query Task TREC 2003 TREC 2004
Topic Distillation 50 75
Home Page finding 150 75
Named Page finding 150 75
Table 1: The number of queries in TREC-2003 and
TREC-2004 web track
and the doc-comb based on KL is not interpretable,
and we only present the results of doc-pair and doc-
avg for KL measure. Our proposed query weight-
ing methods are denoted by query-aggr and query-
comp, corresponding to document feature aggrega-
tion in query and query comparison across domains,
respectively. All ranking models above were trained
only on source domain training data and the labeled
data of target domain was just used for testing.
For training the models efficiently, we imple-
mented RSVM with Stochastic Gradient Descent
(SGD) optimizer (Shalev-Shwartz et al, 2007). The
reported performance is obtained by five-fold cross
validation.
5.2 Experimental Results
The task of HP and NP are more similar to
each other whereas HP/NP is rather different from
TD (Voorhees, 2003; Voorhees, 2004). Thus,
we carried out HP/NP to TD and TD to HP/NP
ranking adaptation tasks. Mean Average Precision
(MAP) (Baeza-Yates and Ribeiro-Neto, 1999) is
used as the ranking performance measure.
5.2.1 Adaptation from HP/NP to TD
The first set of experiments performed adaptation
from HP to TD and NP to TD. The results of MAP
are shown in Table 2.
For the DS-based measure, as shown in the table,
query-aggr works mostly better than no-weight,doc-
pair, doc-avg and doc-comb, and query-comp per-
forms the best among the five weighting methods.
T-test on MAP indicates that the improvement of
query-aggr over no-weight is statistically significant
on two adaptation tasks while the improvement of
document instance weighting over no-weight is sta-
tistically significant only on one task. All of the
improvement of query-comp over no-weight, doc-
pair,doc-avg and doc-comb are statistically signifi-
cant. This demonstrates the effectiveness of query
117
Model Weighting method HP03 to TD03 HP04 to TD04 NP03 to TD03 NP04 to TD04
no-weight 0.2508 0.2086 0.1936 0.1756
DS
doc-pair 0.2505 0.2042 0.1982? 0.1708
doc-avg 0.2514 0.2019 0.2122?? 0.1716
doc-comb 0.2562 0.2051 0.2224??? 0.1793
query-aggr 0.2573 0.2106??? 0.2088 0.1808???
query-comp 0.2816??? 0.2147??? 0.2392??? 0.1861???
KL
doc-pair 0.2521 0.2048 0.1901 0.1761
doc-avg 0.2534 0.2127? 0.1904 0.1777
doc-comb - - - -
query-aggr 0.1890 0.1901 0.1870 0.1643
query-comp 0.2548? 0.2142? 0.2313??? 0.1807?
Table 2: Results of MAP for HP/NP to TD adaptation. ?, ?, ? and boldface indicate significantly better than no-weight,
doc-pair, doc-avg and doc-comb, respectively. Confidence level is set at 95%
weighting compared to document instance weight-
ing.
Furthermore, query-comp can perform better than
query-aggr. The reason is that although document
feature aggregation might be a reasonable represen-
tation for a set of document instances, it is possible
that some information could be lost or distorted in
the process of compression. By contrast, more ac-
curate query weights can be achieved by the more
fine-grained similarity measure between the source
query and all target queries in algorithm 2.
For the KL-based measure, similar observation
can be obtained. However, it?s obvious that DS-
based models can work better than the KL-based.
The reason is that KL conducts weighting by density
function ratio which is sensitive to the data scale.
Specifically, after document feature aggregation, the
number of query feature vectors in all adaptation
tasks is no more than 150 in source and target do-
mains. It renders the density estimation in query-
aggr is very inaccurate since the set of samples is
too small. As each query contains 1000 documents,
they seemed to provide query-comp enough samples
for achieving reasonable estimation of the density
functions in both domains.
5.2.2 Adaptation from TD to HP/NP
To further validate the effectiveness of query
weighting, we also conducted adaptation from TD
to HP and TD to NP . MAP results with significant
test are shown in Table 3.
We can see that document instance weighting
schemes including doc-pair, doc-avg and doc-comb
can not outperform no-weight based on MAP mea-
sure. The reason is that each query in TD has 1000
retrieved documents in which 10-15 documents are
relevant whereas each query in HP or NP only con-
sists 1-2 relevant documents. Thus, when TD serves
as the source domain, it leads to the problem that
too many document pairs were generated for train-
ing the RSVM model. In this case, a small number
of documents that were weighted inaccurately can
make significant impact on many number of docu-
ment pairs. Since query weighting method directly
estimates the query importance instead of document
instance importance, both query-aggr and query-
comp can avoid such kind of negative influence that
is inevitable in the three document instance weight-
ing methods.
5.2.3 The Analysis on Source Query Weights
An interesting problem is which queries in the
source domain are assigned high weights and why
it?s the case. Query weighting assigns each source
query with a weight value. Note that it?s not mean-
ingful to directly compare absolute weight values
between query-aggr and query-comp because source
query weights from distinct weighting methods have
different range and scale. However, it is feasible
to compare the weights with the same weighting
method. Intuitively, if the ranking model learned
from a source query can work well in target do-
main, it should get high weight. According to this
intuition, if ranking models fq1s and fq2s are learned
118
model weighting scheme TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04
no-weight 0.6986 0.6158 0.5053 0.5427
DS
doc-pair 0.6588 0.6235? 0.4878 0.5212
doc-avg 0.6654 0.6200 0.4736 0.5035
doc-comb 0.6932 0.6214? 0.4974 0.5077
query-aggr 0.7179??? 0.6292??? 0.5198??? 0.5551???
query-comp 0.7297??? 0.6499??? 0.5203??? 0.6541???
KL
doc-pair 0.6480 0.6107 0.4633 0.5413
doc-avg 0.6472 0.6132 0.4626 0.5406
doc-comb ? ? ? ?
query-aggr 0.6263 0.5929 0.4597 0.4673
query-comp 0.6530?? 0.6358??? 0.4726 0.5559???
Table 3: Results of MAP for TD to HP/NP adaptation. ?, ?, ? and boldface indicate significantly better than no-weight,
doc-pair, doc-avg and doc-comb, respectively. Confidence level is set as 95%.
from queries q1s and q2s respectively, and fq1s per-
forms better than fq2s , then the source query weight
of q1s should be higher than that of q2s .
For further analysis, we compare the weight val-
ues between each source query pair, for which we
trained RSVM on each source query and evaluated
the learned model on test data from target domain.
Then, the source queries are ranked according to the
MAP values obtained by their corresponding rank-
ing models. The order is denoted as Rmap. Mean-
while, the source queries are also ranked with re-
spect to their weights estimated by DS-based mea-
sure, and the order is denoted as Rweight. We hope
Rweight is correlated as positively as possible with
Rmap. For comparison, we also ranked these queries
according to randomly generated query weights,
which is denoted as query-rand in addition to query-
aggr and query-comp. The Kendall?s ? = P?QP+Q
is used to measure the correlation (Kendall, 1970),
where P is the number of concordant query pairs
and Q is the number of discordant pairs. It?s
noted that ? ?s range is from -1 to 1, and the larger
value means the two ranking is better correlated.
The Kendall?s ? by different weighting methods are
given in Table 4 and 5.
We find that Rweight produced by query-aggr and
query-comp are all positively correlated with Rmap
and clearly the orders generated by query-comp are
more positive than those by query-aggr. This is
another explanation why query-comp outperforms
query-aggr. Furthermore, both are far better than
weighting TD03 to HP03 TD04 to HP04
doc-pair 28,835 secs 21,640 secs
query-aggr 182 secs 123 secs
query-comp 15,056 secs 10,081 secs
Table 6: The efficiency of weighting in seconds.
query-rand because theRweight by query-rand is ac-
tually independent of Rmap.
5.2.4 Efficiency
In the situation where there are large scale data in
source and target domains, how to efficiently weight
a source query is another interesting problem. With-
out the loss of generality, we reported the weighting
time of doc-pair, query-aggr and query-comp from
adaptation from TD to HP using DS measure. As
doc-avg and doc-comb are derived from doc-pair,
their efficiency is equivalent to doc-pair.
As shown in table 6, query-aggr can efficiently
weight query using query feature vector. The reason
is two-fold: one is the operation of query document
aggregation can be done very fast, and the other is
there are 1000 documents in each query of TD or HP,
which means that the compression ratio is 1000:1.
Thus, the domain separator can be found quickly. In
addition, query-comp is more efficient than doc-pair
because doc-pair needs too much time to find the
separator using all instances from source and target
domain. And query-comp uses a divide-and-conquer
method to measure the similarity of source query to
each target query, and then efficiently combine these
119
Weighting method HP03 to TD03 HP04 to TD04 NP03 to TD03 NP04 to TD04
query-aggr 0.0906 0.0280 0.0247 0.0525
query-comp 0.1001 0.0804 0.0711 0.1737
query-rand 0.0041 0.0008 -0.0127 0.0163
Table 4: The Kendall?s ? of Rweight and Rmap in HP/NP to TD adaptation.
Weighting method TD03 to HP03 TD04 to HP04 TD03 to NP03 TD04 to NP04
query-aggr 0.1172 0.0121 0.0574 0.0464
query-comp 0.1304 0.1393 0.1586 0.0545
query-rand ?0.0291 0.0022 0.0161 -0.0262
Table 5: The Kendall?s ? of Rweight and Rmap in TD to HP/NP adaptation.
fine-grained similarity values.
6 Related Work
Cross-domain knowledge transfer has became an
important topic in machine learning and natural lan-
guage processing (Ben-David et al, 2010; Jiang
and Zhai, 2007; Blitzer et al, 2006; Daume? III
and Marcu, 2006). (Blitzer et al, 2006) pro-
posed model adaptation using pivot features to build
structural feature correspondence in two domains.
(Pan et al, 2009) proposed to seek a common fea-
tures space to reduce the distribution difference be-
tween the source and target domain. (Daume? III and
Marcu, 2006) assumed training instances were gen-
erated from source domain, target domain and cross-
domain distributions, and estimated the parameter
for the mixture distribution.
Recently, domain adaptation in learning to rank
received more and more attentions due to the lack
of training data in new search domains. Existing
ranking adaptation approaches can be grouped into
feature-based (Geng et al, 2009; Chen et al, 2008b;
Wang et al, 2009; Gao et al, 2009) and instance-
based (Chen et al, 2010; Chen et al, 2008a; Gao et
al., 2010) approaches. In (Geng et al, 2009; Chen et
al., 2008b), the parameters of ranking model trained
on the source domain was adjusted with the small
set of labeled data in the target domain. (Wang et al,
2009) aimed at ranking adaptation in heterogeneous
domains. (Gao et al, 2009) learned ranking mod-
els on the source and target domains independently,
and then constructed a stronger model by interpo-
lating the two models. (Chen et al, 2010; Chen et
al., 2008a) weighted source instances by using small
amount of labeled data in the target domain. (Gao et
al., 2010) studied instance weighting based on do-
main separator for learning to rank by only using
training data from source domain. In this work, we
propose to directly measure the query importance in-
stead of document instance importance by consider-
ing information at both levels.
7 Conclusion
We introduced two simple yet effective query
weighting methods for ranking model adaptation.
The first represents a set of document instances
within the same query as a query feature vector,
and then directly measure the source query impor-
tance to the target domain. The second measures
the similarity between a source query and each tar-
get query, and then combine the fine-grained simi-
larity values to estimate its importance to target do-
main. We evaluated our approaches on LETOR3.0
dataset for ranking adaptation and found that: (1)
the first method efficiently estimate query weights,
and can outperform the document instance weight-
ing but some information is lost during the aggrega-
tion; (2) the second method consistently and signifi-
cantly outperforms document instance weighting.
8 Acknowledgement
P. Cai and A. Zhou are supported by NSFC (No.
60925008) and 973 program (No. 2010CB731402).
W. Gao and K.-F. Wong are supported by national
863 program (No. 2009AA01Z150). We also thank
anonymous reviewers for their helpful comments.
120
References
Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval.
Somnath Banerjee, Avinava Dubey, Jinesh Machchhar,
and Soumen Chakrabarti. 2009. Efficient and accu-
rate local learning for ranking. In SIGIR workshop :
Learning to rank for information retrieval, pages 1?8.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79(1-2):151?175.
John Blitzer, Ryan Mcdonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. 2005. Learning to
rank using gradient descent. In Proceedings of ICML,
pages 89?96.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise ap-
proach to listwise approach. In Proceedings of ICML,
pages 129 ? 136.
Depin Chen, Jun Yan, Gang Wang, Yan Xiong, Weiguo
Fan, and Zheng Chen. 2008a. Transrank: A novel
algorithm for transfer of rank learning. In Proceedings
of ICDM Workshops, pages 106?115.
Keke Chen, Rongqing Lu, C.K. Wong, Gordon Sun,
Larry Heck, and Belle Tseng. 2008b. Trada: Tree
based ranking function adaptation. In Proceedings of
CIKM.
Depin Chen, Yan Xiong, Jun Yan, Gui-Rong Xue, Gang
Wang, and Zheng Chen. 2010. Knowledge transfer
for cross domain learning to rank. Information Re-
trieval, 13(3):236?253.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126.
Y. Freund, R. Iyer, R. Schapire, and Y. Singer. 2004.
An efficient boosting algorithm for combining prefer-
ences. Journal of Machine Learning Research, 4:933?
969.
Jianfeng Gao, Qiang Wu, Chris Burges, Krysta Svore,
Yi Su, Nazan Khan, Shalin Shah, and Hongyan Zhou.
2009. Model adaptation via model interpolation and
boosting for web search ranking. In Proceedings of
EMNLP.
Wei Gao, Peng Cai, Kam Fai Wong, and Aoying Zhou.
2010. Learning to rank only using training data from
related domain. In Proceedings of SIGIR, pages 162?
169.
Xiubo Geng, Tie-Yan Liu, Tao Qin, Andrew Arnold,
Hang Li, and Heung-Yeung Shum. 2008. Query de-
pendent ranking using k-nearest neighbor. In Proceed-
ings of SIGIR, pages 115?122.
Bo Geng, Linjun Yang, Chao Xu, and Xian-Sheng Hua.
2009. Ranking model adaptation for domain-specific
search. In Proceedings of CIKM.
R. Herbrich, T. Graepel, and K. Obermayer. 2000.
Large Margin Rank Boundaries for Ordinal Regres-
sion. MIT Press, Cambridge.
Jiayuan Huang, Alexander J. Smola, Arthur Gretton,
Karsten M. Borgwardt, and Bernhard Scho?lkopf.
2007. Correcting sample selection bias by unlabeled
data. In Proceedings of NIPS, pages 601?608.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
ACL.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of SIGKDD,
pages 133?142.
Maurice Kendall. 1970. Rank Correlation Methods.
Griffin.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information Re-
trieval, 3(3):225?331.
Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and
Qiang Yang. 2009. Domain adaptation via transfer
component analysis. In Proceedings of IJCAI, pages
1187?1192.
John C. Platt and John C. Platt. 1999. Probabilistic out-
puts for support vector machines and comparisons to
regularized likelihood methods. In Advances in Large
Margin Classifiers, pages 61?74. MIT Press.
Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. Letor:
A benchmark collection for research on learning to
rank for information retrieval. Information Retrieval,
13(4):346?374.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: Primal estimated sub-gradient solver for svm.
In Proceedings of the 24th International Conference
on Machine Learning, pages 807?814.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227?244.
Masashi Sugiyama, Shinichi Nakajima, Hisashi
Kashima, Paul von Bu?nau, and Motoaki Kawan-
abe. 2008. Direct importance estimation with
model selection and its application to covariate
shift adaptation. In Proceedings of NIPS, pages
1433?1440.
Ellen M. Voorhees. 2003. Overview of trec 2003. In
Proceedings of TREC-2003, pages 1?13.
Ellen M. Voorhees. 2004. Overview of trec 2004. In
Proceedings of TREC-2004, pages 1?12.
Bo Wang, Jie Tang, Wei Fan, Songcan Chen, Zi Yang,
and Yanzhu Liu. 2009. Heterogeneous cross domain
ranking in latent space. In Proceedings of CIKM.
121
Y. Yue, T. Finley, F. Radlinski, and T. Joachims. 2007.
A support vector method for optimizing average preci-
sion. In Proceedings of SIGIR, pages 271?278.
Bianca Zadrozny Zadrozny. 2004. Learning and evalu-
ating classifiers under sample selection bias. In Pro-
ceedings of ICML, pages 325?332.
122
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270?274,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Information-theoretic Multi-view Domain Adaptation
Pei Yang1,3, Wei Gao2, Qi Tan1, Kam-Fai Wong3
1South China University of Technology, Guangzhou, China
{yangpei,tanqi}@scut.edu.cn
2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
wgao@qf.org.qa
3The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
kfwong@se.cuhk.edu.hk
Abstract
We use multiple views for cross-domain doc-
ument classification. The main idea is to
strengthen the views? consistency for target
data with source training data by identify-
ing the correlations of domain-specific fea-
tures from different domains. We present
an Information-theoretic Multi-view Adapta-
tion Model (IMAM) based on a multi-way
clustering scheme, where word and link clus-
ters can draw together seemingly unrelated
domain-specific features from both sides and
iteratively boost the consistency between doc-
ument clusterings based on word and link
views. Experiments show that IMAM signifi-
cantly outperforms state-of-the-art baselines.
1 Introduction
Domain adaptation has been shown useful to many
natural language processing applications including
document classification (Sarinnapakorn and Kubat,
2007), sentiment classification (Blitzer et al, 2007),
part-of-speech tagging (Jiang and Zhai, 2007) and
entity mention detection (Daume? III and Marcu,
2006).
Documents can be represented by multiple inde-
pendent sets of features such as words and link struc-
tures of the documents. Multi-view learning aims
to improve classifiers by leveraging the redundancy
and consistency among these multiple views (Blum
and Mitchell, 1998; Ru?ping and Scheffer, 2005; Ab-
ney, 2002). Existing methods were designed for
data from single domain, assuming that either view
alone is sufficient to predict the target class accu-
rately. However, this view-consistency assumption
is largely violated in the setting of domain adapta-
tion where training and test data are drawn from dif-
ferent distributions.
Little research was done for multi-view domain
adaptation. In this work, we present an Information-
theoretical Multi-view Adaptation Model (IMAM)
based on co-clustering framework (Dhillon et al,
2003) that combines the two learning paradigms to
transfer class information across domains in multi-
ple transformed feature spaces. IMAM exploits a
multi-way-clustering-based classification scheme to
simultaneously cluster documents, words and links
into their respective clusters. In particular, the word
and link clusterings can automatically associate the
correlated features from different domains. Such
correlations bridge the domain gap and enhance the
consistency of views for clustering (i.e., classifying)
the target data. Results show that IMAM signifi-
cantly outperforms the state-of-the-art baselines.
2 Related Work
The work closely related to ours was done by Dai
et al (2007), where they proposed co-clustering-
based classification (CoCC) for adaptation learning.
CoCC was extended from information-theoretic co-
clustering (Dhillon et al, 2003), where in-domain
constraints were added to word clusters to provide
the class structure and partial categorization knowl-
edge. However, CoCC is a single-view algorithm.
Although multi-view learning (Blum and
Mitchell, 1998; Dasgupta et al, 2001; Abney,
2002; Sridharan and Kakade, 2008) is common
within a single domain, it is not well studied under
cross-domain settings. Chen et al (2011) proposed
270
CODA for adaptation based on co-training (Blum
and Mitchell, 1998), which is however a pseudo
multi-view algorithm where original data has only
one view. Therefore, it is not suitable for the
true multi-view case as ours. Zhang et al (2011)
proposed an instance-level multi-view transfer
algorithm that integrates classification loss and view
consistency terms based on large margin framework.
However, instance-based approach is generally poor
since new target features lack support from source
data (Blitzer et al, 2011). We focus on feature-level
multi-view adaptation.
3 Our Model
Intuitively, source-specific and target-specific fea-
tures can be drawn together by mining their
co-occurrence with domain-independent (common)
features, which helps bridge the distribution gap.
Meanwhile, the view consistency on target data can
be strengthened if target-specific features are appro-
priately bundled with source-specific features. Our
model leverages the complementary cooperation be-
tween different views to yield better adaptation per-
formance.
3.1 Representation
Let DS be the source training documents and DT
be the unlabeled target documents. Let C be the set
of class labels. Each source document ds ? DS is
labeled with a unique class label c ? C. Our goal
is to assign each target document dt ? DT to an
appropriate class as accurately as possible.
Let W be the vocabulary of the entire document
collectionD = DS?DT . LetL be the set of all links
(hyperlinks or citations) among documents. Each
d ? D can be represented by two views, i.e., a bag-
of-words set {w} and a bag-of-links set {l}.
Our model explores multi-way clustering that si-
multaneously clusters documents, words and links.
Let D?, W? and L? be the respective clustering of doc-
uments, words and links. The clustering functions
are defined as CD(d) = d? for document, CW (w) =
w? for word and CL(l) = l? for link, where d?, w? and l?
represent the corresponding clusters.
3.2 Objectives
We extend the information-theoretic co-clustering
framework (Dhillon et al, 2003) to incorporate the
loss frommultiple views. Let I(X,Y ) be mutual in-
formation (MI) of variables X and Y , our objective
is to minimize the MI loss of two different views:
? = ? ??W + (1? ?) ??L (1)
where
?W = I(DT ,W )? I(D?T , W? ) + ? ?
[
I(C,W )? I(C, W? )
]
?L = I(DT , L)? I(D?T , L?) + ? ?
[
I(C,L)? I(C, L?)
]
?W and ?L are the loss terms based on word view
and link view, respectively, traded off by ?. ? bal-
ances the effect of word or link clusters from co-
clustering. When ? = 1, the function relies on text
only that reduces to CoCC (Dai et al, 2007).
For any x ? x?, we define conditional distribution
q(x|y?) = p(x|x?)p(x?|y?) under co-clustering (X?, Y? )
based on Dhillon et al (2003). Therefore, for any
w ? w?, l ? l?, d ? d? and c ? C, we can calculate
a set of conditional distributions: q(w|d?), q(d|w?),
q(l|d?), q(d|l?), q(c|w?), q(c|l?).
Eq. 1 is hard to optimize due to its combinatorial
nature. We transform it to the equivalent form based
on Kullback-Leibler (KL) divergence between two
conditional distributions p(x|y) and q(x|y?), where
D(p(x|y)||q(x|y?)) =
?
x p(x|y)log
p(x|y)
q(x|y?) .
Lemma 1 (Objective functions) Equation 1 can
be turned into the form of alternate minimization:
(i) For document clustering, we minimize
? =
?
d
p(d)?D(d, d?) + ?C(W? , L?),
where ?C(W? , L?) is a constant1 and
?D(d, d?) =? ? D(p(w|d)||q(w|d?))
+ (1? ?) ? D(p(l|d)||q(l|d?)).
(ii) For word and link clustering, we minimize
? = ?
?
w
p(w)?W (w, w?)+(1??)
?
l
p(l)?L(l, l?),
where for any feature v (e.g., w or l) in feature set
V (e.g., W or L), we have
?V (v, v?) =D(p(d|v)||q(d|v?))
+ ? ? D(p(c|v)||q(c|v?)).
1We can obtain that ?C(W? , L?) =
?
[
?(I(C,W )? I(C, W? )) + (1? ?)(I(C,L)? I(C, L?))
]
,
which is constant since word/link clusters keep fixed during the
document clustering step.
271
Lemma 12 allows us to alternately reorder either
documents or both words and links by fixing the
other in such a way that the MI loss in Eq. 1 de-
creases monotonically.
4 Consistency of Multiple Views
In this section, we present how the consistency of
document clustering on target data could be en-
hanced among multiple views, which is the key issue
of our multi-view adaptation method.
According to Lemma 1, minimizing ?D(d, d?) for
each d can reduce the objective function value itera-
tively (t denotes round id):
C(t+1)D (d) = argmin
d?
[
? ? D(p(w|d)||q(t)(w|d?))
+(1? ?) ? D(p(l|d)||q(t)(l|d?))
]
(2)
In each iteration, the optimal document cluster-
ing function C(t+1)D is to minimize the weighted sum
of KL-divergences used in word-view and link-view
document clustering functions as shown above. The
optimal word-view and link-view clustering func-
tions can be denoted as follows:
C(t+1)DW (d) = argmin
d?
D(p(w|d)||q(t)(w|d?)) (3)
C(t+1)DL (d) = argmin
d?
D(p(l|d)||q(t)(l|d?)) (4)
Our central idea is that the document clusterings
C(t+1)DW and C
(t+1)
DL based on the two views are drawn
closer in each iteration due to the word and link
clusterings that bring together seemingly unrelated
source-specific and target-specific features. Mean-
while, C(t+1)D combines the two views and reallo-
cates the documents so that it remains consistent
with the view-based clusterings as much as possi-
ble. The more consistent the views, the better the
document clustering, and then the better the word
and link clustering, which creates a positive cycle.
4.1 Disagreement Rate of Views
For any document, a consistency indicator function
with respect to the two view-based clusterings can
be defined as follows (t is omitted for simplicity):
2Due to space limit, the proof of all lemmas will be given in
a long version of the paper.
Definition 1 (Indicator function) For any d ? D,
?CDW ,CDL (d) =
{
1, if CDW (d) = CDL(d);
0, otherwise
Then we define the disagreement rate between two
view-based clustering functions:
Definition 2 (Disagreement rate)
?(CDW , CDL) = 1?
?
d?D ?CDW ,CDL (d)
|D| (5)
Abney (2002) suggests that the disagreement rate
of two independent hypotheses upper-bounds the er-
ror rate of either hypothesis. By minimizing the dis-
agreement rate on unlabeled data, the error rate of
each view can be minimized (so does the overall er-
ror). However, Eq. 5 is not continuous nor convex,
which is difficult to optimize directly. By using the
optimization based on Lemma 1, we can show em-
pirically that disagreement rate is monotonically de-
creased (see Section 5).
4.2 View Combination
In practice, view-based document clusterings in
Eq. 3 and 4 are not computed explicitly. Instead,
Eq. 2 directly optimizes view combination and pro-
duces the document clustering. Therefore, it is nec-
essary to disclose how consistent it could be with the
view-based clusterings.
Suppose ? = {FD|FD(d) = d?, d? ? D?} is
the set of all document clustering functions. For
any FD ? ?, we obtain the disagreement rate
?(FD, CDW ? CDL), where CDW ? CDL denotes the
clustering resulting from the overlap of the view-
based clusterings.
Lemma 2 CD always minimizes the disagreement
rate for any FD ? ? such that
?(CD, CDW ? CDL) = minFD??
?(FD, CDW ? CDL)
Meanwhile, ?(CD, CDW ? CDL) = ?(CDW , CDL).
Lemma 2 suggests that IMAM always finds the
document clustering with the minimal disagreement
rate to the overlap of view-based clusterings, and the
minimal value of disagreement rate equals to the dis-
agreement rate of the view-based clusterings.
272
Table 1: View disagreement rate ? and error rate ? that
decrease with iterations and their Pearson?s correlation ?.
Round 1 2 3 4 5 ?
DA-EC ? 0.194 0.153 0.149 0.144 0.144 0.998? 0.340 0.132 0.111 0.101 0.095
DA-NT ? 0.147 0.083 0.071 0.065 0.064 0.996? 0.295 0.100 0.076 0.069 0.064
DA-OS ? 0.129 0.064 0.052 0.047 0.041 0.998? 0.252 0.092 0.068 0.060 0.052
DA-ML ? 0.166 0.102 0.071 0.065 0.064 0.984? 0.306 0.107 0.076 0.062 0.054
EC-NT ? 0.311 0.250 0.228 0.219 0.217 0.988? 0.321 0.137 0.112 0.096 0.089
5 Experiments and Results
Data and Setup
Cora (McCallum et al, 2000) is an online archive
of computer science articles. The documents in the
archive are categorized into a hierarchical structure.
We selected a subset of Cora, which contains 5 top
categories and 10 sub-categories. We used a similar
way as Dai et al (2007) to construct our training and
test sets. For each set, we chose two top categories,
one as positive class and the other as the negative.
Different sub-categories were deemed as different
domains. The task is defined as top category classifi-
cation. For example, the dataset denoted as DA-EC
consists of source domain: DA 1(+), EC 1(-); and
target domain: DA 2(+), EC 2(-).
The classification error rate ? is measured as the
proportion of misclassified target documents. In or-
der to avoid the infinity values, we applied Laplacian
smoothing when computing the KL-divergence. We
tuned ?, ? and the number of word/link clusters by
cross-validation on the training data.
Results and Discussions
Table 1 shows the monotonic decrease of view dis-
agreement rate ? and error rate ? with the iterations
and their Pearson?s correlation ? is nearly perfectly
positive. This indicates that IMAM gradually im-
proves adaptation by strengthening the view consis-
tency. This is achieved by the reinforcement of word
and link clusterings that draw together target- and
source-specific features that are originally unrelated
but co-occur with the common features.
We compared IMAM with (1) Transductive SVM
(TSVM) (Joachims, 1999) using both words and
links features; (2) Co-Training (Blum and Mitchell,
Table 2: Comparison of error rate with baselines.
Data TSVM Co-Train CoCC MVTL-LM IMAM
DA-EC 0.214 0.230 0.149 0.192 0.138
DA-NT 0.114 0.163 0.106 0.108 0.069
DA-OS 0.262 0.175 0.075 0.068 0.039
DA-ML 0.107 0.171 0.109 0.183 0.047
EC-NT 0.177 0.296 0.225 0.261 0.192
EC-OS 0.245 0.175 0.137 0.176 0.074
EC-ML 0.168 0.206 0.203 0.264 0.173
NT-OS 0.396 0.220 0.107 0.288 0.070
NT-ML 0.101 0.132 0.054 0.071 0.032
OS-ML 0.179 0.128 0.051 0.126 0.021
Average 0.196 0.190 0.122 0.174 0.085
1998); (3) CoCC (Dai et al, 2007): Co-clustering-
based single-view transfer learner (with text view
only); and (4) MVTL-LM (Zhang et al, 2011):
Large-margin-based multi-view transfer learner.
Table 2 shows the results. Co-Training performed
a little better than TSVM by boosting the confidence
of classifiers built on the distinct views in a comple-
mentary way. But since Co-Training doesn?t con-
sider the distribution gap, it performed clearly worse
than CoCC even though CoCC has only one view.
IMAM significantly outperformed CoCC on all
the datasets. In average, the error rate of IMAM
is 30.3% lower than that of CoCC. This is because
IMAM effectively leverages distinct and comple-
mentary views. Compared to CoCC, using source
training data to improve the view consistency on tar-
get data is the key competency of IMAM.
MVTL-LM performed worse than CoCC. It sug-
gests that instance-based approach is not effective
when the data of different domains are drawn from
different feature spaces. Although MVTL-LM regu-
lates view consistency, it cannot identify the associ-
ations between target- and source-specific features
that is the key to the success of adaptation espe-
cially when domain gap is large and less common-
ality could be found. In contrast, CoCC and IMAM
uses multi-way clustering to find such correlations.
6 Conclusion
We presented a novel feature-level multi-view do-
main adaptation approach. The thrust is to incor-
porate distinct views of document features into the
information-theoretic co-clustering framework and
strengthen the consistency of views on clustering
(i.e., classifying) target documents. The improve-
ments over the state-of-the-arts are significant.
273
References
Steven Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 360-367.
John Blitzer, Mark Dredze and Fernado Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 440-447.
John Blitzer, Sham Kakade and Dean P. Foster. 2011.
Domain Adaptation with Coupled Subspaces. In Pro-
ceedings of the 14th International Conference on Arti-
ficial Intelligence and Statistics (AISTATS), pages 173-
181.
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the 11th Annual Conference on Computa-
tional Learning Theory, pages 92-100.
Minmin Chen, Killian Q. Weinberger and John Blitzer.
2011. Co-Training for Domain Adaptation. In Pro-
ceedings of NIPS, pages 1-9.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang and Yong
Yu. 2007. Co-clustering Based Classification for Out-
of-domain Documents. In Proceedings of the 13th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 210-219.
Sanjoy Dasgupta, Michael L. Littman and David
McAllester. 2001. PAC Generalization Bounds for
Co-Training. In Proceeding of NIPS, pages 375-382.
Hal Daume? III and Daniel Marcu. 2006. Domain Adap-
tation for Statistical Classifiers. Journal of Artificial
Intelligence Research, 26(2006):101-126.
Inderjit S. Dhillon, Subramanyam Mallela and Dharmen-
dra S. Modha. 2003. Information-Theoretic Co-
clustering. In Proceedings of the ninth ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 210-219.
Thorsten Joachims. 1999. Transductive Inference for
Text Classification using Support Vector Machines. In
Proceedings of Sixteenth International Conference on
Machine Learning, pages 200-209.
Jing Jiang and Chengxiang Zhai. 2007. Instance Weight-
ing for Domain Adaptation in NLP. In Proceedings of
the 45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 264-271.
Andrew K. McCallum, Kamal Nigam, Jason Rennie and
Kristie Seymore. 2000. Automating the Construction
of Internet Portals with Machine Learning. Informa-
tion Retrieval, 3(2):127-163.
Stephan Ru?ping and Tobias Scheffer. 2005. Learning
with Multiple Views. In Proceedings of ICML Work-
shop on Learning with Multiple Views.
Kanoksri Sarinnapakorn and Miroslav Kubat. 2007.
Combining Sub-classifiers in Text Categorization: A
DST-Based Solution and a Case Study. IEEE Transac-
tions Knowledge and Data Engineering, 19(12):1638-
1651.
Karthik Sridharan and Sham M. Kakade. 2008. An In-
formation Theoretic Framework for Multi-view Learn-
ing. In Proceedings of the 21st Annual Conference on
Computational Learning Theory, pages 403-414.
Dan Zhang, Jingrui He, Yan Liu, Luo Si and Richard D.
Lawrence. 2011. Multi-view Transfer Learning with
a Large Margin Approach. In Proceedings of the 17th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 1208-1216.
274
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 58?62,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Empirical Study on Uncertainty Identification in Social Media Context
Zhongyu Wei1, Junwen Chen1, Wei Gao2,
Binyang Li1, Lanjun Zhou1, Yulan He3, Kam-Fai Wong1
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
3School of Engineering & Applied Science, Aston University, Birmingham, UK
{zywei,jwchen,byli,ljzhou,kfwong}@se.cuhk.edu.hk
wgao@qf.org.qa, y.he@cantab.net
Abstract
Uncertainty text detection is important
to many social-media-based applications
since more and more users utilize social
media platforms (e.g., Twitter, Facebook,
etc.) as information source to produce
or derive interpretations based on them.
However, existing uncertainty cues are in-
effective in social media context because
of its specific characteristics. In this pa-
per, we propose a variant of annotation
scheme for uncertainty identification and
construct the first uncertainty corpus based
on tweets. We then conduct experiments
on the generated tweets corpus to study the
effectiveness of different types of features
for uncertainty text identification.
1 Introduction
Social media is not only a social network tool for
people to communicate but also plays an important
role as information source with more and more
users searching and browsing news on it. People
also utilize information from social media for de-
veloping various applications, such as earthquake
warning systems (Sakaki et al, 2010) and fresh
webpage discovery (Dong et al, 2010). How-
ever, due to its casual and word-of-mouth pecu-
liarities, the quality of information in social me-
dia in terms of factuality becomes a premier con-
cern. Chances are there for uncertain information
or even rumors flooding in such a context of free
form. We analyzed a tweet dataset which includes
326,747 posts (Details are given in Section 3) col-
lected during 2011 London Riots, and result re-
veals that at least 18.91% of these tweets bear un-
certainty characteristics1. Therefore, distinguish-
ing uncertain statements from factual ones is cru-
cial for users to synthesize social media informa-
tion to produce or derive reliable interpretations,
1The preliminary study was done based on a manually de-
fined uncertainty cue-phrase list. Tweets containing at least
one hedge cue were treated as uncertain.
and this is expected helpful for applications like
credibility analysis (Castillo et al, 2011) and ru-
mor detection (Qazvinian et al, 2011) based on
social media.
Although uncertainty has been studied theoret-
ically for a long time as a grammatical phenom-
ena (Seifert and Welte, 1987), the computational
treatment of uncertainty is a newly emerging area
of research. Szarvas et al (2012) pointed out that
?Uncertainty - in its most general sense - can be
interpreted as lack of information: the receiver of
the information (i.e., the hearer or the reader) can-
not be certain about some pieces of information?.
In recent years, the identification of uncertainty
in formal text, e.g., biomedical text, reviews or
newswire, has attracted lots of attention (Kilicoglu
and Bergler, 2008; Medlock and Briscoe, 2007;
Szarvas, 2008; Light et al, 2004). However, un-
certainty identification in social media context is
rarely explored.
Previous research shows that uncertainty identi-
fication is domain dependent as the usage of hedge
cues varies widely in different domains (Morante
and Sporleder, 2012). Therefore, the employment
of existing out-of-domain corpus to social media
context is ineffective. Furthermore, compared to
the existing uncertainty corpus, the expression of
uncertainty in social media is fairly different from
that in formal text in a sense that people usu-
ally raise questions or refer to external informa-
tion when making uncertain statements. But, nei-
ther of the uncertainty expressions can be repre-
sented based on the existing types of uncertainty
defined in the literature. Therefore, a different un-
certainty classification scheme is needed in social
media context.
In this paper, we propose a novel uncertainty
classification scheme and construct the first uncer-
tainty corpus based on social media data ? tweets
in specific here. And then we conduct experi-
ments for uncertainty post identification and study
the effectiveness of different categories of features
based on the generated corpus.58
2 Related work
We introduce some popular uncertainty corpora
and methods for uncertainty identification.
2.1 Uncertainty corpus
Several text corpora from various domains have
been annotated over the past few years at different
levels (e.g., expression, event, relation, sentence)
with information related to uncertainty.
Sauri and Pustejovsky (2009) presented a cor-
pus annotated with information about the factu-
ality of events, namely Factbank, which is con-
structed based on TimeBank2 containing 3,123 an-
notated sentences from 208 news documents with
8 different levels of uncertainty defined.
Vincze et al (2008) constructed the BioSocpe
corpus, which consists of medical and biological
texts annotated for negation, uncertainty and their
linguistic scope. This corpus contains 20,924 sen-
tences.
Ganter et al (2009) generated Wikipedia
Weasels Corpus, where Weasel tags in Wikipedia
articles is adopted readily as labels for uncertainty
annotation. It contains 168,923 unique sentences
with 437 weasel tags in total.
Although several uncertainty corpora exist,
there is not a uniform set of standard for uncer-
tainty annotation. Szarvas et al (2012) normal-
ized the annotation of the three corpora aforemen-
tioned. However, the context of these corpora
is different from that of social media. Typically,
these documents annotated are grammatically cor-
rect, carefully punctuated, formally structured and
logically expressed.
2.2 Uncertainty identification
Previous work on uncertainty identification fo-
cused on classifying sentences into uncertain
or definite categories. Existing approaches are
mainly based on supervised methods (Light et
al., 2004; Medlock and Briscoe, 2007; Medlock,
2008; Szarvas, 2008) using the annotated corpus
with different types of features including Part-Of-
Speech (POS) tags, stems, n-grams, etc..
Classification of uncertain sentences was con-
solidated as a task in the 2010 edition of CoNLL
shared task on learning to detect hedge cues
and their scope in natural language text (Farkas
et al, 2010). The best system for Wikipedia
data (Georgescul, 2010) employed Support Vector
Machine (SVM), and the best system for biolog-
ical data (Tang et al, 2010) adopted Conditional
2http://www.timeml.org/site/timebank/
timebank.html
Random Fields (CRF).
In our work, we conduct an empirical study of
uncertainty identification on tweets dataset and ex-
plore the effectiveness of different types of fea-
tures (i.e., content-based, user-based and Twitter-
specific) from social media context.
3 Uncertainty corpus for microblogs
3.1 Types of uncertainty in microblogs
Traditionally, uncertainty can be divided into
two categories, namely Epistemic and Hypothet-
ical (Kiefer, 2005). For Epistemic, there are two
sub-classes Possible and Probable. For Hypotheti-
cal, there are four sub-classes including Investiga-
tion, Condition, Doxastic andDynamic. The detail
of the classification is described as below (Kiefer,
2005):
Epistemic: On the basis of our world knowledge
we cannot decide at the moment whether the
statement is true or false.
Hypothetical: This type of uncertainty includes
four sub-classes:
? Doxastic: Expresses the speaker?s be-
liefs and hypotheses.
? Investigation: Proposition under inves-
tigation.
? Condition: Proposition under condi-
tion.
? Dynamic: Contains deontic, disposi-
tional, circumstantial and buletic modal-
ity.
Compared to the existing uncertainty corpora,
social media authors enjoy free form of writing.
In order to study the difference, we annotated a
small set of 827 randomly sampled tweets accord-
ing to the scheme of uncertainty types above, in
which we found 65 uncertain tweets. And then,
we manually identified all the possible uncertain
tweets, and found 246 really uncertain ones out of
these 827 tweets, which means that 181 uncertain
tweets are missing based on this scheme. We have
the following three salient observations:
? Firstly, there is no tweet found with the type of
Investigation. We find people seldom use words
like ?examine? or ?test? (indicative words of In-
vestigation category) when posting tweets. Once
they do this, the statement should be considered
as highly certain. For example, @dobibid I have
tested the link, it is fake!
? Secondly, people frequently raise questions
about some specific topics for confirmation which
expresses uncertainty. For example, @ITVCentral59
Can you confirm that Birmingham children?s hos-
pital has/hasn?t been attacked by rioters?
? Thirdly, people tend to post message with exter-
nal information (e.g., story from friends) which re-
veals uncertainty. For example, Friend who works
at the children?s hospital in Birmingham says the
riot police are protecting it.
Based on these observations, we propose a vari-
ant of uncertainty types in social media context
by eliminating the category of Investigation and
adding the category of Question and External un-
der Hypothetical, as shown in Table 3.1. Note
that our proposed scheme is based on Kiefer?s
work (2005) which was previously extended to
normalize uncertainty corpora in different genres
by Szarvas et al (2012). But we did not try these
extended schema for specific genres since even the
most general one (Kiefer, 2005) was proved un-
suitable for social media context.
3.2 Annotation result
The dataset we annotated was collected from Twit-
ter using Streaming API during summer riots
in London during August 6-13 2011, including
326,747 tweets in total. Search criteria include
hashtags like #ukriots, #londonriots, #prayforlon-
don, and so on. We further extracted the tweets
relating to seven significant events during the riot
identified by UK newspaper The Guardian from
this set of tweets. We annotated all the 4,743 ex-
tracted tweets for the seven events3.
Two annotators were trained to annotate the
dataset independently. Given a collection of
tweets T = {t1, t2, t3...tn}, the annotation task is
to label each tweet ti as either uncertain or cer-
tain. Uncertainty assertions are to be identified
in terms of the judgements about the author?s in-
tended meaning rather than the presence of uncer-
tain cue-phrase. For those tweets annotated as un-
certain, sub-class labels are also required accord-
ing to the classification indicated in Table 3.1 (i.e.,
multi-label is allowed).
The Kappa coefficient (Carletta, 1996) indi-
cating inter-annotator agreement was 0.9073 for
the certain/uncertain binary classification and was
0.8271 for fine-grained annotation. The conflict
labels from the two annotators were resolved by a
third annotator. Annotation result is displayed in
Table 3.2, where 926 out of 4,743 tweets are la-
beled as uncertain accounting for 19.52%. Ques-
tion is the uncertainty category with most tweets,
followed by External. Only 21 tweets are labeled
3http://www.guardian.co.uk/
uk/interactive/2011/dec/07/
london-riots-twitter
Tweet# 4743
Uncertainty# 926
Epistemic Possible# 16Probable# 129
Hypothetical
Condition# 71
Doxastic# 48
Dynamic# 21
External# 208
Question# 488
Table 2: Statistics of annotation result
as Dynamic and all of them are buletic modal-
ity4 which shares similarity with Doxastic. There-
fore, we consider Dynamic together with Domes-
tic in the error analysis for simplicity. During
the preliminary annotation, we found that uncer-
tainty cue-phrase is a good indicator for uncer-
tainty tweets since tweets labeled as uncertain al-
ways contain at least one cue-phrase. Therefore,
annotators are also required identify cue-phrases
which trigger the sense of uncertainty in the tweet.
All cue-phrases appearing more than twice are col-
lected to form a uncertainty cue-phrase list.
4 Experiment and evaluation
We aim to identify those uncertainty tweets from
tweet collection automatically based on machine
learning approaches. In addition to n-gram fea-
tures, we also explore the effectiveness of three
categories of social media specific features includ-
ing content-based, user-based and Twitter-specific
ones. The description of the three categories of
features is shown in Table 4. Since the length of
tweet is relatively short, we therefore did not carry
out stopwords removal or stemming.
Our preliminary experiments showed that com-
bining unigrams with bigrams and trigrams gave
better performance than using any one or two of
these three features. Therefore, we just report the
result based on the combination of them as n-gram
features. Five-fold cross validation is used for
evaluation. Precision, recall and F-1 score of un-
certainty category are used as the metrics.
4.1 Overall performance
The overall performance of different approaches
is shown in Table 4.1. We used uncertainty cue-
phrase matching approach as baseline, denoted
by CP. For CP, we labeled tweets containing at
least one entry in uncertainty cue-phrase list (de-
scribed in Section 3) as uncertain. All the other
approaches are supervised methods using SVM
based on different feature sets. n-gram stands for
n-gram feature set, C means content-based feature
set, U denotes user-based feature set, T represents
4Proposition expresses plans, intentions or desires.60
Category Subtype Cue Phrase Example
Epistemic Possible, etc. may, etc. It may be raining.Probable likely, etc. It is probably raining.
Hypothetical
Condition if, etc. If it rains, we?ll stay in.
Doxastic believe, etc. He believes that the Earth is flat.
Dynamic hope, etc. fake picture of the london eye on fire... i hope
External someone said, etc. Someone said that London zoo was attacked.
Question seriously?, etc. Birmingham riots are moving to the children hospital?! seriously?
Table 1: Classification of uncertainty in social media context
Category Name Description
Content-based
Length Length of the tweet
Cue Phrase Whether the tweet contains a uncertainty cue
OOV Ratio Ratio of words out of vocabulary
Twitter-specific
URL Whether the tweet contains a URL
URL Count Frequency of URLs in corpus
Retweet Count How many times has this tweet been retweeted
Hashtag Whether the tweet contains a hashtag
Hashtag Count Number of Hashtag in tweets
Reply Is the current tweet a reply tweet
Rtweet Is the current tweet a retweet tweet
User-based
Follower Count Number of follower the user owns
List Count Number of list the users owns
Friend Count Number of friends the user owns
Favorites Count Number of favorites the user owns
Tweet Count Number of tweets the user published
Verified Whether the user is verified
Table 3: Feature list for uncertainty classification
Approach Precision Recall F-1
CP 0.3732 0.9589 0.5373
SVMn?gram 0.7278 0.8259 0.7737
SVMn?gram+C 0.8010 0.8260 0.8133
SVMn?gram+U 0.7708 0.8271 0.7979
SVMn?gram+T 0.7578 0.8266 0.7907
SVMn?gram+ALL 0.8162 0.8269 0.8215
SVMn?gram+Cue Phrase 0.7989 0.8266 0.8125SVMn?gram+Length 0.7372 0.8216 0.7715SVMn?gram+OOV Ratio 0.7414 0.8233 0.7802
Table 4: Result of uncertainty tweets identification
Twitter-specific feature set and ALL is the combi-
nation of C, U and T.
Table 4.1 shows that CP achieves the best recall
but its precision is the lowest. The learning based
methods with different feature sets give some sim-
ilar recalls. Compared to CP, SVMn?gram in-
creases the F-1 score by 43.9% due to the salient
improvement on precision and small drop of re-
call. The performance improves in terms of pre-
cision and F-1 score when the feature set is ex-
panded by adding C, U or T onto n-gram, where
+C brings the highest gain, and SVMn?gram+ALL
performs best in terms of precision and F-1 score.
We then study the effectiveness of the three
content-based features, and result shows that the
presence of uncertain cue-phrase is most indica-
tive for uncertainty tweet identification.
4.2 Error analysis
We analyze the prediction errors based on
SVMn?gram+ALL. The distribution of errors in
terms of different types of uncertainty is shown
Type Poss. Prob. D.&D. Cond. Que. Ext.
Total# 16 129 69 71 488 208
Error# 11 20 18 11 84 40
% 0.69 0.16 0.26 0.15 0.17 0.23
Table 5: Error distributions
in Table 4.2. Our method performs worst on the
type of Possible and on the combination of Dy-
namic and Doxastic because these two types have
the least number of samples in the corpus and the
classifier tends to be undertrained without enough
samples.
5 Conclusion and future work
In this paper, we propose a variant of classification
scheme for uncertainty identification in social me-
dia and construct the first uncertainty corpus based
on tweets. We perform uncertainty identification
experiments on the generated dataset to explore
the effectiveness of different types of features. Re-
sult shows that the three categories of social media
specific features can improve uncertainty identifi-
cation. Furthermore, content-based features bring
the highest improvement among the three and the
presence of uncertain cue-phrase contributes most
for content-based features.
In future, we will explore to use uncertainty
identification for social media applications.
6 Acknowledgement
This work is partially supported by General Re-
search Fund of Hong Kong (No. 417112).61
References
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
linguistics, 22(2):249?254.
Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter.
In Proceedings of the 20th International Conference
on World Wide Web, pages 675?684.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: im-
proving recency ranking using twitter data. In Pro-
ceedings of the 19th International Conference on
World Wide Web, pages 331?340. ACM.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The conll-
2010 shared task: learning to detect hedges and their
scope in natural language text. In Proceedings of
the 14th Conference on Computational Natural Lan-
guage Learning?Shared Task, pages 1?12. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009, pages 173?
176. Association for Computational Linguistics.
Maria Georgescul. 2010. A hedgehop over a max-
margin framework using hedge cues. In Proceed-
ings of the 14th Conference on Computational Natu-
ral Language Learning?Shared Task, pages 26?31.
Association for Computational Linguistics.
Ferenc Kiefer. 2005. Lehetoseg es szuk-
segszeruseg[Possibility and necessity]. Tinta Kiado,
Budapest.
H. Kilicoglu and S. Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a
linguistically motivated perspective. BMC bioinfor-
matics, 9(Suppl 11):S10.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings
of BioLink 2004 workshop on linking biological lit-
erature, ontologies and databases: tools for users,
pages 17?24.
B. Medlock and T. Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific litera-
ture. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
992?999.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41(4):636?654.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational Linguistics, 38(2):223?260.
Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it:
Identifying misinformation in microblogs. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1589?1599.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web, pages 851?860. ACM.
R. Saur?? and J. Pustejovsky. 2009. Factbank: A cor-
pus annotated with event factuality. Language Re-
sources and Evaluation, 43(3):227?268.
Stephan Seifert and Werner Welte. 1987. A basic bib-
liography on negation in natural language, volume
313. Gunter Narr Verlag.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Iryna Gurevych. 2012. Cross-
genre and cross-domain detection of semantic uncer-
tainty. Computational Linguistics, 38(2):335?367.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of 46th Annual Meet-
ing of the Association for Computational Linguis-
tics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A cascade method for detect-
ing hedges and their scope in natural language text.
In Proceedings of the 14th Conference on Compu-
tational Natural Language Learning?Shared Task,
pages 13?17. Association for Computational Lin-
guistics.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and
J. Csirik. 2008. The bioscope corpus: biomedical
texts annotated for uncertainty, negation and their
scopes. BMC bioinformatics, 9(Suppl 11):S9.
62
