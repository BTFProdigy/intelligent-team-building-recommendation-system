Supporting temporal question answering:
strategies for offline data collection
David Ahn
ISLA, University of Amsterdam
ahn@ science. uva. nl
Steven Schockaert, Martine De Cock, and Etienne Kerre
Ghent University
Steven. Schockaert,Martine. DeCock,Etienne. Kerre@ UGent. be
Abstract
We pursue two strategies for offline data collection for a temporal question answering
system that uses both quantitative methods and fuzzy methods to reason about time
and events. The first strategy extracts event descriptions from the structured year
entries in the online encyclopedia Wikipedia, yielding clean quantitative temporal
information about a range of events. The second strategy mines the web using
patterns indicating temporal relations between events and times and between events.
Web mining leverages the volume of data available on the web to find qualitative
temporal relations between known events and new, related events and to build fuzzy
time spans for events for which we lack crisp metric temporal information.
1 Introduction
Time structures our world, and the questions we ask reflect that. Not only
do we want to know quantitative information?when did some event happen
or how long did some state of affairs persist?but we also want qualitative
information?what was going on before or during major events, or what hap-
pened afterwards. While the amount of information available to answer such
questions continues to increase, the temporal information needed is not always
fully specified. No information source is obliged to timestamp every referenced
event, so while evidence for qualitative temporal relations abounds, there is
often no quantitative information to verify it. Furthermore, many events, such
as the Cold War or the Great Depression, are inherently vague?with gradual
beginnings or endings?or ill-defined aggregations of smaller events.
In order for a temporal QA system to be able to make use of such limited,
incomplete temporal information, careful consideration must be given both to
the extraction of temporal information it needs and to the temporal reasoning
mechanisms it employs. We are presently at work on a temporal QA system
that provides access to events extracted from Wikipedia and satellite events
mined from the web and that models the time span of vague events as fuzzy sets
and qualitative temporal relations as fuzzy relations. In this paper, we focus
on the creation of the knowledge base of events and temporal information,
which takes place offline, prior to any user interaction. A separate paper [10]
describes the fuzzy reasoning mechanisms the system uses.
In ?2 and ?3, we introduce temporal questions and sketch the architec-
ture of our temporal QA system. In ?4, we describe event extraction from
Wikipedia, an online encyclopedia. In ?5, we describe web mining for fuzzy
and qualitative temporal information. Note that the work described here is
still in progress, so while our extraction methods are in use, we are still ex-
perimenting with them, and the QA system is not yet complete.
2 Temporal questions
There are a variety of question types that fall under the umbrella of temporal
questions, including questions that ask for times as answers, those that ask for
temporal relations, and those that ask for information restricted to a certain
time period [6]. The degree of explicitness of temporal reference in a temporal
question also varies significantly: some temporal questions refer explicitly to a
date or time, while others refer to times only implicitly, by reference to events
or states. Here, we focus on temporally restricted questions, and in particular,
those restricted by events, such as these (from the CLEF 2005 QA track):
(1) Who played the role of Superman before he was paralyzed?
(2) What disease did many American soldiers get after the Gulf War?
The data collection we describe, though, can be used to support the answering
of other types of temporal questions, as well.
Temporally restricted questions consist of two parts: the main clause,
which indicates the information request, and the temporal restriction, which
is a subordinate clause or PP headed by a temporal preposition or connective,
such as before, after, during, etc., which we refer to as temporal signals [7]. The
temporal signal that connects the two parts of a temporally restricted ques-
tion indicates the temporal relation that must hold between the time spans
of the restricting event and the requested events. Since much of the tempo-
ral information we have access to regarding events is vague and incomplete,
we explore the use of fuzzy temporal reasoning for temporal QA, instead of
the standard Allen algebra of temporal interval relations [1]. The model we
use is a generalization of Allen?s algebra that is suitable for vague events and
relations. For crisp events, our reasoning algorithm is equivalent to Allen?s
path-consistency algorithm. For vague events, fuzzy relations can express that
a given qualitative relation is only satisfied to a certain degree [11].
3 Architecture of a temporal QA system
Our temporal QA system follows a strategy of extracting information likely to
be useful in answering questions?in our case, a knowledge base of events and
temporal relations?in a pre-processing stage, before any questions are asked
[3,4]. The system consists of several components: a question analysis module,
the knowledge base, and an answer selection module.
Question analysis: Since we are focusing on temporal questions in which
a temporal relation restricts the information being queried, our question anal-
ysis module must separate the non-temporal part of the question?the actual
information request?from the temporal restriction. Our question analysis
module parses the question and extracts phrases headed by temporal signals
as potential temporal restrictions. It then uses standard pattern-based tech-
niques to extract keyword queries and the expected answer type.
Knowledge base: The knowledge base (KB) consists of two parts: an
XML database containing descriptions of individual events and a temporal
relation network containing inclusion and before/after relations for events in
the KB. Quantitative temporal information about events (i.e., starting and
ending dates for crisp events and fuzzy sets for vague ones) is contained in the
XML database. The rest of this paper describes the construction of the KB.
Answer selection: To answer a temporally restricted question, we must
find events that match the non-temporal part of the question and filter out
those that do not satisfy the restriction. We treat the problem of finding the
events as a retrieval problem, using the keyword queries from question analy-
sis, with event descriptions as target documents. Checking whether an event
satisfies the restriction is a matter of inferring whether an appropriate qualita-
tive temporal relation holds between the event and a time or event matching
the restriction. We use IR techniques to find events matching the restriction,
and we use both quantitative and fuzzy temporal reasoning to make the infer-
ence [10]. From the remaining event descriptions, we use standard techniques
to extract an answer. Typically, the information request is mapped to a named
entity type by the question analysis module, so appropriate named entities are
harvested and scored and the top-scoring entity is returned.
4 Extracting events from Wikipedia
Wikipedia is a free, open-domain, web-based encyclopedia [12]. In addition
to traditional encyclopedia entries, it also has entries for a variety of time
periods, which contain lists of historical and/or current events. We extract
events from the entries for years. The standardized formatting of year entries
in Wikipedia, together with the wiki markup used for this formatting, makes
extracting event descriptions straightforward. A typical year entry contains
sections delimited by the second-level headings ==Events==, ==Births==, and
==Deaths==. Each of these sections is optionally split into subsections de-
limited by third-level headings indicating months (e.g., ===May===) or the
lack of a date (===Unknown date===). Within these subsections are asterisk-
delimited lists, each item of which corresponds to an event (or a date with a
list of events). Event descriptions begin with a date or a date range, if known,
and then continue with one or two sentences describing the event. This text
contains phrases marked up as wiki links (pointers to Wikipedia entries):
(3) [[March 10]] - The [[New Hampshire]] primary is won by [[Henry Cabot
Lodge]], Ambassador to [[South Vietnam]].
Sometimes, an event description begins with a wiki link, set off with a colon,
indicating a larger event (which we call a super-event) of which it is a part:
(4) [[August 8]] - [[Watergate scandal]]: US President [[Richard Nixon]]
announces his resignation (effective [[August 9]])
Given the structured nature of year entries, simple hand-built patterns
can be used to perform what amounts to shallow semantic interpretation,
extracting event descriptions from the entries, including temporal location
information and limited participant and mereological information (via wiki
links and super-events, when present). For each extracted event description,
the date(s) and any embedded wiki links are extracted, using simple pattern-
matching, and the text of the description is parsed. This information is added
to our XML database as an event element with the following sub-elements:
date/start date/end date (normalized dates; which one(s) depends on what
is given in the entry), super event (wiki link to super event, if present),
description (text of the description), and parse (converted to XML).
From the entries for the years from 1600 to 2005, we have extracted about
33,000 events, somewhat over half (about 19,000) birth and death events.
5 Web mining for fuzzy and qualitative information
The basic idea behind web mining is that there is enough information on the
web that if there is a significant connection between two events, we should be
able to find this connection by searching for patterns that typically express it.
We use web mining to build representations of the time span of vague events
and to find both additional events related to events already in the KB and for
qualitative temporal relations between events in the KB.
While most of the smaller-scale events extracted from Wikipedia come
with quantitative temporal information, it is not always fully specified. Fur-
thermore, many of the super-events from Wikipedia, as well as the new events
we mine from the web, lack such information. We cope with this by search-
ing the web for beginning and ending dates using a simple pattern-based
approach?sending patterns to Google and extracting information from the
returned snippets. The patterns we use include, e.g., ?event? began on ?date?
and ?event? lasted until ?date?. If there is sufficient agreement among different
web pages about the beginning and ending date of an event, we represent the
time span of this event as an interval. If not, we use the techniques described
in [9] to construct a suitable fuzzy set [13] to represent the time span, which
is stored in the XML database as part of the event representation. Of course,
for some events, we may fail to find sufficient information about beginning or
ending dates, in which case they remain undated, or ungrounded, events.
We also use a pattern-based approach to mine the web both for new events
and for temporal relations relating ungrounded events to grounded events. Be-
cause new events are only usable if they can be temporally connected to events
already in the KB, we can use a uniform set of hand-crafted patterns that in-
dicate a temporal relation between events. The patterns we use include, e.g.,
?NP1? gave way to ?NP2? and ?NP2? took place after ?NP1?, for before/after
relations, and ?NP1? and other events during ?NP2? and ?NP1? took place
during ?NP2?, for inclusion relations. All our patterns relate NP descriptions
of events, which means that they can only be used with known events that
have NP descriptions. Fortunately, this includes all super-events and newly
mined events, which make up most of the ungrounded events in the KB.
Since we can use the same patterns to mine for new events and for tempo-
ral relations for ungrounded events, we combine the tasks. Our basic proce-
dure for mining with these patterns is as follows. Substitute either ?NP1? or
?NP2? with the NP description of a known event, send the resulting pattern to
Google, and parse and tag the returned snippets with named entities. Extract
NPs in the other NP position, discarding those tagged as person, location,
or organization. For each remaining NP, if it refers to a known event in the
KB, add to the temporal relation network a link between the original known
event and the event referred to by the mined NP. Otherwise, add a new event
for the mined NP and a link between the original event and the new event.
The hardest step is determining whether a mined NP refers to an event
already in the KB. Coreference resolution is clearly necessary to temporal
constraints on coreferring event descriptions, but to maintain consistency in
the KB, we must be careful in asserting coreference. We do not try to solve
the cross-document coreference task completely but instead split mined NPs
into two groups. The first group, which includes all indefinite, demonstrative,
quantificational, and pronominal NPs, is added to the temporal network but is
never considered for coreference. The second group contains NPs that we are
confident refer to a unique event and that are thus candidates for coreference.
To find these NPs, we are experimenting with heuristics to determine
unique reference. Some heuristics are capitalization-based, while others are
based on collocation measures using web hit counts [5,8], such as the ratio
between the number of hits for the entire NP and the product of the hits
for each of the individual words in the NP (similar to pointwise mutual in-
formation [2]). When an NP is determined to be uniquely referring, we use
string matching to determine whether it is coreferent with an existing event
description. If it is, the temporal relation mined is added to the KB for this
existing event. Otherwise, a new event and relation are added to the KB.
6 Conclusion
We have described the construction of a knowledge base of events and temporal
relations for a temporal QA system. We take advantage of a freely available,
structured resource?Wikipedia?to obtain relatively accurate quantitative in-
formation about events. We also mine the web to build fuzzy sets for vague
events and to find events for which we can only get qualitative information.
Acknowledgements The first author was supported by the Netherlands
Organization for Scientific Research (NWO), under project number 612.066.302.
The second author was supported by a PhD grant from the Research Foun-
dation ? Flanders.
References
[1] Allen, J., Maintaining knowledge about temporal intervals, Communications of
the ACM 26 (1983), pp. 832?843.
[2] Church, K. et al, Using statistics in lexical analysis, in: Lexical Acquisition:
Exploiting On-Line Resources to Build a Lexicon, Lawrence Erlbaum, 1991 .
[3] Fleischman, M., E. Hovy and A. Echihabi, Offline strategies for online question
answering: Answering questions before they are asked, in: ACL 2003, 2003.
[4] Jijkoun, V., G. Mishne and M. de Rijke, Preprocessing documents to answer
Dutch questions, in: BNAIC?03, 2003.
[5] Magnini, B., M. Negri, R. Prevete and H. Tanev, Is it the right answer?
Exploiting web redundancy for answer validation, in: ACL-02, 2002.
[6] Pustejovsky, J. et al, TERQAS final report, http://www.cs.brandeis.edu/
?jamesp/arda/time/readings/TERQAS-FINAL-REPORT.pdf (2002).
[7] Saur??, R. et al, TimeML annotation guidelines, http://www.cs.brandeis.
edu/?jamesp/arda/time/timeMLdocs/annguide12wp.pdf (2004).
[8] Schlobach, S., D. Ahn, M. de Rijke and V. Jijkoun, Data-driven type checking
in open domain question answering, Journal of Applied Logic (to appear).
[9] Schockaert, S., Construction of membership functions for fuzzy time periods, in:
J. Gervain, editor, ESSLLI 2005 Student Session, 2005.
[10] Schockaert, S., D. Ahn, M. De Cock and E. E. Kerre, Question answering with
imperfect temporal information, in: FQAS-2006, to appear.
[11] Schockaert, S., M. De Cock and E. E. Kerre, Imprecise temporal interval
relations, in: LNCS 3849, Springer, 2006 .
[12] Wikipedia, Wikipedia, the free encyclopedia, http://en.wikipedia.org/w/
index.php?title=Wikipedia&oldid=35397363, [Accessed 16-January-2006].
[13] Zadeh, L. A., Fuzzy sets, Information and Control 8 (1965), pp. 338?353.
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 317?322,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation
Els Lefever1,2, Ve?ronique Hoste1,2,3 and Martine De Cock2
1LT3, Language and Translation Technology Team, University College Ghent
Groot-Brittannie?laan 45, 9000 Gent, Belgium
2Dept. of Applied Mathematics and Computer Science, Ghent University
Krijgslaan 281 (S9), 9000 Gent, Belgium
3Dept. of Linguistics, Ghent University
Blandijnberg 2, 9000 Gent, Belgium
Abstract
This paper describes a set of exploratory ex-
periments for a multilingual classification-
based approach to Word Sense Disambigua-
tion. Instead of using a predefined monolin-
gual sense-inventory such as WordNet, we use
a language-independent framework where the
word senses are derived automatically from
word alignments on a parallel corpus. We built
five classifiers with English as an input lan-
guage and translations in the five supported
languages (viz. French, Dutch, Italian, Span-
ish and German) as classification output. The
feature vectors incorporate both the more tra-
ditional local context features, as well as bi-
nary bag-of-words features that are extracted
from the aligned translations. Our results
show that the ParaSense multilingual WSD
system shows very competitive results com-
pared to the best systems that were evaluated
on the SemEval-2010 Cross-Lingual Word
Sense Disambiguation task for all five target
languages.
1 Introduction
Word Sense Disambiguation (WSD) is the NLP
task that consists in selecting the correct sense of
a polysemous word in a given context. Most state-
of-the-art WSD systems are supervised classifiers
that are trained on manually sense-tagged corpora,
which are very time-consuming and expensive to
build (Agirre and Edmonds, 2006) . In order to over-
come this acquisition bottleneck (sense-tagged cor-
pora are scarce for languages other than English),
we decided to take a multilingual approach to WSD,
that builds up the sense inventory on the basis of
the Europarl parallel corpus (Koehn, 2005). Using
translations from a parallel corpus implicitly deals
with the granularity problem as finer sense distinc-
tions are only relevant as far as they are lexicalized
in the target translations. It also facilitates the in-
tegration of WSD in multilingual applications such
as multilingual Information Retrieval (IR) or Ma-
chine Translation (MT). Significant improvements
in terms of general MT quality were for the first time
reported by Carpuat and Wu (2007) and Chan et al
(2007). Both papers describe the integration of a
dedicated WSD module in a Chinese-English statis-
tical machine translation framework and report sta-
tistically significant improvements in terms of stan-
dard MT evaluation metrics.
Several studies have already shown the validity
of using parallel corpora for sense discrimination
(e.g. (Ide et al, 2002)), for bilingual WSD mod-
ules (e.g. (Gale and Church, 1993; Ng et al, 2003;
Diab and Resnik, 2002; Chan and Ng, 2005; Da-
gan and Itai, 1994)) and for WSD systems that use
a combination of existing WordNets with multilin-
gual evidence (Tufis? et al, 2004). The research de-
scribed in this paper is novel as it presents a truly
multilingual classification-based approach to WSD
that directly incorporates evidence from four other
languages. To this end, we build further on two
well-known research ideas: (1) the possibility to
use parallel corpora to extract translation labels and
features in an automated way and (2) the assump-
tion that incorporating evidence from multiple lan-
guages into the feature vector will be more infor-
mative than a more restricted set of monolingual or
bilingual features. Furthermore, our WSD system
does not use any information from external lexical
resources such as WordNet (Fellbaum, 1998) or Eu-
roWordNet (Vossen, 1998).
317
2 Experimental Setup
Starting point of the experiments was the six-lingual
sentence-aligned Europarl corpus that was used in
the SemEval-2010 ?Cross-Lingual Word Sense Dis-
ambiguation? (CLWSD) task (Lefever and Hoste,
2010b). The task is a lexical sample task for twenty
English ambiguous nouns that consists in assign-
ing a correct translation in the five supported tar-
get languages (viz. French, Italian, Spanish, Ger-
man and Dutch) for an ambiguous focus word in a
given context. In order to detect the relevant transla-
tions for each of the twenty ambiguous focus words,
we ran GIZA++ (Och and Ney, 2003) with its de-
fault settings for all focus words. This word align-
ment output was then considered to be the label for
the training instances for the corresponding classi-
fier (e.g. the Dutch translation is the label that is used
to train the Dutch classifier). By considering this
word alignment output as oracle information, we re-
defined the CLWSD task as a classification task.
To train our five classifiers (English as input lan-
guage and French, German, Dutch, Italian and Span-
ish as focus languages), we used the memory-based
learning (MBL) algorithm implemented in TIMBL
(Daelemans and Hoste, 2002), which has success-
fully been deployed in previous WSD classification
tasks (Hoste et al, 2002). We performed heuris-
tic experiments to define the parameter settings for
the classifier, leading to the selection of the Jef-
frey Divergence distance metric, Gain Ratio feature
weighting and k = 7 as number of nearest neigh-
bours. In future work, we plan to use an optimized
word-expert approach in which a genetic algorithm
performs joint feature selection and parameter op-
timization per ambiguous word (Daelemans et al,
2003).
For our feature vector creation, we combined a set
of English local context features and a set of binary
bag-of-words features that were extracted from the
aligned translations.
2.1 Training Feature Vector Construction
We created two experimental setups. The first
training set incorporates the automatically generated
word alignments as labels. We applied an automatic
post-processing step on these word alignments in or-
der to remove leading and trailing determiners and
prepositions. In future work, we will investigate
other word alignment strategies and measure the im-
pact on the classification scores. The second training
set uses manually verified word alignments as labels
for the training instances. This second setup is then
to be considered as the upper bound on the current
experimental setup.
All English sentences were preprocessed
by means of a memory-based shallow parser
(MBSP) (Daelemans and van den Bosch, 2005) that
performs tokenization, Part-of-Speech tagging and
text chunking. The preprocessed sentences were
used as input to build a set of commonly used WSD
features related to the English input sentence:
? features related to the focus word itself being
the word form of the focus word, the lemma,
Part-of-Speech and chunk information
? local context features related to a window of
three words preceding and following the focus
word containing for each of these words their
full form, lemma, Part-of-Speech and chunk in-
formation
In addition to these well known monolingual fea-
tures, we extracted a set of binary bag-of-words fea-
tures from the aligned translation that are not the
target language of the classifier (e.g. for the Dutch
classifier, we extract bag-of-words features from the
Italian, Spanish, French and German aligned trans-
lations). In order to extract useful content words,
we first ran Part-of-Speech tagging and lemmatisa-
tion by means of the Treetagger (Schmid, 1994) tool.
Per ambiguous focus word, a list of content words
(nouns, adjectives, verbs and adverbs) was extracted
that occurred in the aligned translations of the En-
glish sentences containing the focus word. One bi-
nary feature per selected content word was then cre-
ated per ambiguous word: ?0? in case the word does
not occur in the aligned translation of this instance,
and ?1? in case the word does occur in the aligned
translation of the training instance.
2.2 Test Feature Vector Construction
For the creation of the feature vectors for the test in-
stances, we follow a similar strategy as the one we
used for the creation of the training instances. The
first part of the feature vector contains the English
318
local context features that were also extracted for
the training instances. For the construction of the
bag-of-words features however, we need to adopt a
different approach as we do not have aligned trans-
lations for the English test instances at our disposal.
We decided to deploy a novel strategy that uses
the Google Translate API1 to automatically gener-
ate a translation for all English test instances in the
five supported languages. Online machine transla-
tions tools have already been used before to create
artificial parallel corpora that were used for NLP
tasks such as for instance Named Entity Recogni-
tion (Shah et al, 2010).
In a next step the automatically generated transla-
tion was preprocessed in the same way as the train-
ing translations (Part-of-Speech-tagged and lemma-
tized). The resulting lemmas were then used to con-
struct the same set of binary bag-of-words features
that were stored for the training instances of the am-
biguous focus word.
3 Evaluation
To evaluate our five classifiers, we used the sense in-
ventory and test set of the SemEval ?Cross-Lingual
Word Sense Disambiguation? task. The sense inven-
tory was built up on the basis of the Europarl corpus:
all retrieved translations of a polysemous word were
manually grouped into clusters, which constitute dif-
ferent senses of that given word. The test instances
were selected from the JRC-ACQUIS Multilingual
Parallel Corpus2 and BNC3. To label the test data,
native speakers provided their top three translations
from the predefined clusters of Europarl translations,
in order to assign frequency weights to the set of
gold standard translations. A more detailed descrip-
tion of the construction of the data set can be found
in Lefever and Hoste (2010a).
As evaluation metrics, we used both the SemEval
BEST precision metric from the CLWSD task as
well as a straightforward accuracy measure. The
SemEval metric takes into account the frequency
weights of the gold standard translations: transla-
tions that were picked by different annotators get
a higher weight. For the BEST evaluation, systems
1http://code.google.com/apis/language/
2http://wt.jrc.it/lt/Acquis/
3http://www.natcorp.ox.ac.uk/
can propose as many guesses as the system believes
are correct, but the resulting score is divided by the
number of guesses. In this way, systems that out-
put a lot of guesses are not favoured. For a more
detailed description of the SemEval scoring scheme,
we refer to McCarthy and Navigli (2007). Follow-
ing variables are used for the SemEval precision for-
mula. Let H be the set of annotators, T the set of test
items and hi the set of responses for an item i ? T
for annotator h ? H . Let A be the set of items from
T where the system provides at least one answer and
ai : i ? A the set of guesses from the system for
item i. For each i, we calculate the multiset union
(Hi) for all hi for all h ? H and for each unique
type (res) in Hi that has an associated frequency
(freqres).
Prec =
?
ai:i?A
P
res?ai
freqres
|ai|
|Hi|
|A|
(1)
The second metric we use is a straightforward ac-
curacy measure, that divides the number of correct
answers by the total amount of test instances.
As a baseline, we selected the most frequent lem-
matized translation that resulted from the automated
word alignment (GIZA++). We also compare our
results with the two winning SemEval-2 systems
for the Cross-Lingual Word Sense Disambiguation
task, UvT-WSD (that only participated for Dutch
and Spanish) and T3-COLEUR. The UvT-WSD sys-
tem (van Gompel, 2010), that also uses a k-nearest
neighbor classifier and a variety of local and global
context features, obtained the best scores for Span-
ish and Dutch in the SemEval CLWSD competi-
tion. Although we also use a memory-based learner,
our method is different from this system in the way
the feature vectors are constructed. Next to the
incorporation of similar local context features, we
also include evidence from multiple languages in
our feature vector. For French, Italian and Ger-
man however, the T3-COLEUR system (Guo and
Diab, 2010) outperformed the other systems in the
SemEval competition. This system adopts a differ-
ent approach: during the training phase a monolin-
gual WSD system processes the English input sen-
tence and a word alignment module is used to ex-
tract the aligned translation. The English senses to-
gether with their aligned translations (and probabil-
319
ity scores) are then stored in a word sense transla-
tion table, in which look-ups are performed during
the testing phase. This system also differs from the
Uvt-WSD and ParaSense systems in the sense that
the word senses are derived from WordNet, whereas
the other systems do not use any external resources.
The results for all five classifiers are listed in two
tables. Table 1 gives an overview of the SemEval-
2010 weighted precision scores, whereas Table 2
shows the more straightforward accuracy figures.
Both tables list the scores averaged over all twenty
test words for the baseline (most frequent word
alignment), the best SemEval system (for a given
language) and the two ParaSense setups: one that ex-
clusively uses automatically generated word align-
ments, and one that uses the verified word alignment
labels. For both setups we trained three flavors of
the ParaSense system (1: local context + translation
features, 2: translation features and 3: local context
features).
The classification results show that for both se-
tups all three flavors of the ParaSense system easily
beat the baseline. Moreover, the ParaSense system
clearly outperforms the winning SemEval systems,
except for Spanish where the scores are similar. As
all systems, viz. the two SemEval systems as well
as the three flavors of the ParaSense system, were
trained on the same Europarl data, the scores illus-
trate the potential advantages of using a multilingual
approach. Although we applied a very basic strategy
for the selection of our bag-of-words translation fea-
tures (we did not perform any filtering on the trans-
lations except for Part-of-Speech information), we
observe that for three languages the full feature vec-
tor outperforms the classifier that uses the more tra-
ditional WSD local context features. For Dutch, the
classifier that merely uses translation features even
outperforms the classifier that uses the local context
features. In previous research (Lefever and Hoste,
2011), we showed that the classifier using evidence
from all different languages was constantly better
than the ones using less or no multilingual evidence.
In addition, the scores also degraded relatively to the
number of translation features that was used. As we
used a different set of translation features for the lat-
ter pilot experiments (we only used the translations
of the ambiguous words instead of the full bag-of-
words features we used for the current setup), we
need to confirm this trend with more experiments
using the current feature sets.
Another important observation is that the classifi-
cation scores degrade when using the automatically
generated word alignments, but only to a minor ex-
tent. This clearly shows the viability of our setup.
Further experiments with different word alignment
settings and symmetrisation methods should allow
us to further improve the results with the automat-
ically generated word alignments. Using the non-
validated labels makes the system very flexible and
language-independent, as all steps in the feature vec-
tor creation can be run automatically.
4 Conclusion
We presented preliminary results for a multilingual
classification-based approach to Word Sense Dis-
ambiguation. In addition to the commonly used
monolingual local context features, we also incor-
porate bag-of-word features that are built from the
aligned translations. Although there is still a lot of
room for improvement on the feature base, our re-
sults show that the ParaSense system clearly outper-
forms state-of-the-art systems for all languages, ex-
cept for Spanish where the results are very similar.
As all steps are run automatically, this multilingual
approach could be an answer for the acquisition bot-
tleneck, as long as there are parallel corpora avail-
able for the targeted languages. Although large mul-
tilingual corpora are still rather scarce, we strongly
believe there will be more parallel corpora available
in the near future (large companies and organiza-
tions disposing of large quantities of parallel text,
internet corpora such as the ever growing Wikipedia
corpus, etc.). Another line of research could be the
exploitation of comparable corpora to acquire addi-
tional training data.
In future work, we want to run additional exper-
iments with different classifiers (SVM) and apply
a genetic algorithm to perform joint feature selec-
tion, parameter optimization and instance selection.
We also plan to expand our feature set by including
global context features (content words from the En-
glish sentence) and to examine the relationship be-
tween the performance and the number (and nature)
of languages that is added to the feature vector. In
addition, we will apply semantic analysis tools (such
320
French Italian Spanish Dutch German
Baseline 20.71 14.03 18.36 15.69 13.16
T3-COLEUR 21.96 15.55 19.78 10.71 13.79
UvT-WSD 23.42 17.70
Non-verified word alignment labels
ParaSense1 (full feature vector) 24.54 18.03 22.80 18.56 16.88
ParaSense2 (translation features) 23.92 16.77 22.58 17.70 15.98
ParaSense3 (local context features) 24.09 19.89 23.21 17.57 16.55
Verified word alignment labels
ParaSense1 (full feature vector) 24.60 19.64 23.10 18.61 17.41
ParaSense2 (translation features) 24.29 19.15 22.94 18.25 16.90
ParaSense3 (local context features) 24.79 21.31 23.56 17.70 17.54
Table 1: SemEval precision scores averaged over all twenty test words
French Italian Spanish Dutch German
Baseline 63.10 47.90 53.70 59.40 52.30
T3-COLEUR 66.88 50.73 59.83 40.01 54.20
UvT-WSD 70.20 64.10
Non-verified word alignment labels
ParaSense1 (full feature vector) 75.20 63.40 68.20 68.10 66.20
ParaSense2 (translation features) 73.20 58.30 67.60 65.90 63.60
ParaSense3 (local context features) 73.50 65.50 69.40 63.90 61.90
Verified word alignment labels
ParaSense1 (full feature vector) 75.70 63.20 68.50 68.20 67.80
ParaSense2 (translation features) 74.70 61.30 68.30 66.80 66.20
ParaSense3 (local context features) 75.20 67.30 70.30 63.30 66.10
Table 2: Accuracy percentages averaged over all twenty test words
as LSA) on our multilingual bag-of-words sets in
order to detect latent semantic topics in the multi-
lingual feature base. Finally, we want to evaluate
to which extent the integration of our WSD output
helps practical applications such as Machine Trans-
lation or Information Retrieval.
Acknowledgments
We thank the anonymous reviewers for their valu-
able remarks. This research was funded by the Uni-
versity College Research Fund.
References
E. Agirre and P. Edmonds, editors. 2006. Word Sense
Disambiguation. Algorithms and Applications. Text,
Speech and Language Technology. Springer, Dor-
drecht.
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambiguation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 61?72, Prague, Czech Republic.
Y.S. Chan and H.T. Ng. 2005. Scaling Up Word Sense
Disambiguation via Parallel Texts. In Proceedings of
the 20th National Conference on Artificial Intelligence
(AAAI 2005), pages 1037?1042, Pittsburgh, Pennsyl-
vania, USA.
Y.S. Chan, H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 33?
40, Prague, Czech Republic.
W. Daelemans and V. Hoste. 2002. Evaluation of Ma-
chine Learning Methods for Natural Language Pro-
cessing Tasks. In Proceedings of the third Interna-
tional Conference on Language Resources and Eval-
uation (LREC?02), pages 755?760.
W. Daelemans and A. van den Bosch. 2005. Memory-
based Language Processing. Cambridge University
Press.
W. Daelemans, V. Hoste, F. De Meulder, and B. Naudts.
2003. Combined optimization of feature selection and
321
algorithm parameters in machine learning of language.
Machine Learning, pages 84?95.
I. Dagan and A. Itai. 1994. Word sense disambiguation
using a second language monolingual corpus. Compu-
tational Linguistics, 20(4):563?596.
M. Diab and P. Resnik. 2002. An Unsupervised Method
for Word Sense Tagging Using Parallel Corpora. In
Proceedings of ACL, pages 255?262.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
W.A. Gale and K.W. Church. 1993. A program for align-
ing sentences in bilingual corpora. Computational
Linguistics, 19(1):75?102.
W. Guo and M. Diab. 2010. COLEPL and COLSLM: An
Unsupervised WSD Approach to Multilingual Lexical
Substitution, Tasks 2 and 3 SemEval 2010. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, pages 129?133, Uppsala, Sweden. As-
sociation for Computational Linguistics.
V. Hoste, I. Hendrickx, W. Daelemans, and A. van den
Bosch. 2002. Parameter Optimization for Machine-
Learning of Word Sense Disambiguation. Natural
Language Engineering, Special Issue on Word Sense
Disambiguation Systems, 8:311?325.
N. Ide, T. Erjavec, and D. Tufis?. 2002. Sense discrimi-
nation with parallel corpora. . In ACL-2002 Workhop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 54?60, Philadelphia.
Ph. Koehn. 2005. Europarl: a parallel corpus for statisti-
cal machine translation. In Tenth Machine Translation
Summit, pages 79?86, Phuket, Thailand.
E. Lefever and V. Hoste. 2010a. Construction
of a Benchmark Data Set for Cross-Lingual Word
Sense Disambiguation. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, May. European Language Resources Associa-
tion (ELRA).
E. Lefever and V. Hoste. 2010b. SemEval-2010 Task
3: Cross-Lingual Word Sense Disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, ACL 2010, pages 15?20, Uppsala,
Sweden.
E. Lefever and V. Hoste. 2011. Examining the Validity
of Cross-Lingual Word Sense Disambiguation. In Pro-
ceedings of the Conference on Computational Linguis-
tics and Intelligent Text Processing (CICLing 2011),
Tokyo, Japan.
D. McCarthy and R. Navigli. 2007. SemEval-2007 Task
10: English Lexical Substitution Task. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations (SemEval-2007), pages 48?53, Prague, Czech
Republic.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting par-
allel texts for word sense disambiguation: An empiri-
cal study. In 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 455?462,
Sapporo, Japan.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on new methods in Language Pro-
cessing, Manchester, UK.
R. Shah, B. Lin, A. Gershman, and R. Frederking. 2010.
SYNERGY: A Named Entity Recognition System for
Resource-scarce Languages such as Swahili using On-
line Machine Translation. In Proceedings of the
Second Workshop on African Language Technology
(AFLAT 2010), Valletta, Malt.
D. Tufis?, R. Ion, and N. Ide. 2004. Fine-Grained
Word Sense Disambiguation Based on Parallel Cor-
pora, Word Alignment, Word Clustering and Aligned
Wordnets. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING
2004), pages 1312?1318, Geneva, Switzerland, Au-
gust. Association for Computational Linguistics.
M. van Gompel. 2010. UvT-WSD1: A Cross-Lingual
Word Sense Disambiguation System. In Proceedings
of the 5th International Workshop on Semantic Evalu-
ation, pages 238?241, Uppsala, Sweden. Association
for Computational Linguistics.
P. Vossen, editor. 1998. EuroWordNet: a multilingual
database with lexical semantic networks. Kluwer Aca-
demic Publishers, Norwell, MA, USA.
322
