TnT - -  A S ta t i s t i ca l  Par t -o f -Speech  Tagger  
Thors ten  Brants  
Saarland University 
Computat iona l  Linguistics 
D-66041 Saarbriicken, Germany 
thors t  en@co l i ,  un i - sb ,  de 
Abst rac t  
Trigrams'n'Tags (TnT) is an efficient statistical 
part-of-speech tagger. Contrary to claims found 
elsewhere in the literature, we argue that a tagger 
based on Markov models performs at least as well as 
other current approaches, including the Maximum 
Entropy framework. A recent comparison has even 
shown that TnT performs ignificantly better for the 
tested corpora. We describe the basic model of TnT, 
the techniques used for smoothing and for handling 
unknown words. Furthermore, we present evalua- 
tions on two corpora. 
1 In t roduct ion  
A large number of current language processing sys- 
tems use a part-of-speech tagger for pre-processing. 
The tagger assigns a (unique or ambiguous) part-of- 
speech tag to each token in the input and passes its 
output o the next processing level, usually a parser. 
Furthermore, there is a large interest in part-of- 
speech tagging for corpus annotation projects, who 
create valuable linguistic resources by a combination 
of automatic processing and human correction. 
For both applications, a tagger with the highest 
possible accuracy is required. The debate about 
which paradigm solves the part-of-speech tagging 
problem best is not finished. Recent comparisons 
of approaches that can be trained on corpora (van 
Halteren et al, 1998; Volk and Schneider, 1998) have 
shown that in most cases statistical aproaches (Cut- 
ting et al, 1992; Schmid, 1995; Ratnaparkhi, 1996) 
yield better results than finite-state, rule-based, or 
memory-based taggers (Brill, 1993; Daelemans etal., 
1996). They are only surpassed by combinations of
different systems, forming a "voting tagger". 
Among the statistical approaches, the Maximum 
Entropy framework has a very strong position. Nev- 
ertheless, a recent independent comparison of 7 tag- 
gets (Zavrel and Daelemans, 1999) has shown that 
another approach even works better: Markov mod- 
els combined with a good smoothing technique and 
with handling of unknown words. This tagger, TnT, 
not only yielded the highest accuracy, it also was the 
fastest both in training and tagging. 
The tagger comparison was organized as a "black- 
box test": set the same task to every tagger and 
compare the outcomes. This paper describes the 
models and techniques used by TnT together with 
the implementation. 
The reader will be surprised how simple the under- 
lying model is. The result of the tagger comparison 
seems to support he maxime "the simplest is the 
best". However, in this paper we clarify a number 
of details that are omitted in major previous pub- 
lications concerning tagging with Markov models. 
As two examples, (Rabiner, 1989) and (Charniak 
et al, 1993) give good overviews of the techniques 
and equations used for Markov models and part-of- 
speech tagging, but they are not very explicit in the 
details that are needed for their application. We ar- 
gue that it is not only the choice of the general model 
that determines the result of the tagger but also the 
various "small" decisions on alternatives. 
The aim of this paper is to give a detailed ac- 
count of the techniques used in TnT. Additionally, 
we present results of the tagger on the NEGRA cor- 
pus (Brants et al, 1999) and the Penn Treebank 
(Marcus et al, 1993). The Penn Treebank results 
reported here for the Markov model approach are 
at least equivalent to those reported for the Maxi- 
mum Entropy approach in (Ratnaparkhi, 1996). For 
a comparison to other taggers, the reader is referred 
to (Zavrel and Daelemans, 1999). 
2 Arch i tec ture  
2.1 The Under ly ing Model  
TnT uses second order Markov models for part-of- 
speech tagging. The states of the model represent 
tags, outputs represent the words. Transition prob- 
abilities depend on the states, thus pairs of tags. 
Output probabilities only depend on the most re- 
cent category. To be explicit, we calculate 
argmax P(tilti-1, ti-2)P(wilti P(tr+l ItT) 
ti...iT 
(i) 
for a given sequence of words w I . . .  W T of length T. 
tl... tT are elements of the tagset, the additional 
224
tags t - l ,  to, and tT+l are beginning-of-sequence 
and end-of-sequence markers. Using these additional 
tags, even if they stem from rudimentary process- 
ing of punctuation marks, slightly improves tagging 
results. This is different from formulas presented 
in other publications, which just stop with a "loose 
end" at the last word. If sentence boundaries are 
not marked in the input, TnT adds these tags if it 
encounters one of \[.!?;\] as a token. 
Transition and output probabilities are estimated 
from a tagged corpus. As a first step, we use the 
maximum likelihood probabilities /5 which are de- 
rived from the relative frequencies: 
Unigrams: /5(t3) = f(t3) (2) 
N 
f(t2, t3) (3) 
Bigrams: P(t31t~)= f(t2) 
f(ta, t2, t3) 
Trigrams: /5(t3ltx,t~) - f ( t l , t2)  (4) 
Lexical: /5(w3 It3) - /(w3, t3) (5) 
f(t3) 
for all tl, t2, t3 in the tagset and w3 in the lexi- 
con. N is the total number of tokens in the training 
corpus. We define a maximum likelihood probabil- 
ity to be zero if the corresponding nominators and 
denominators are zero. As a second step, contex- 
tual frequencies are smoothed and lexical frequences 
are completed by handling words that are not in the 
lexicon (see below). 
2.2 Smooth ing  
Trigram probabilities generated from a corpus usu- 
ally cannot directly be used because of the sparse- 
data problem. This means that there are not enough 
instances for each trigram to reliably estimate the 
probability. Furthermore, setting a probability to 
zero because the corresponding trigram never oc- 
cured in the corpus has an undesired effect. It causes 
the probability of a complete sequence to be set to 
zero if its use is necessary for a new text sequence, 
thus makes it impossible to rank different sequences 
containing a zero probability. 
The smoothing paradigm that delivers the best 
results in TnT is linear interpolation of unigrams, 
bigrams, and trigrams. Therefore, we estimate a 
trigram probability as follows: 
P(t3ltl ,  t2) = AlP(t3) + Ag_/5(t31t2) + A3/5(t3\[t1, t2) 
(6) 
/5 are maximum likelihood estimates of the proba- 
bilities, and A1 + A2 + A3 = 1, so P again represent 
probability distributions. 
We use the context-independent variant of linear 
interpolation, i.e., the values of the As do not depend 
on the particular trigram. Contrary to intuition, 
this yields better esults than the context-dependent 
variant. Due to sparse-data problems, one cannot es- 
timate a different set of As for each trigram. There- 
fore, it is common practice to group trigrams by fre- 
quency and estimate tied sets of As. However, we 
are not aware of any publication that has investi- 
gated frequency groupings for linear interpolation i
part-of-speech tagging. All groupings that we have 
tested yielded at most equivalent results to context- 
independent linear interpolation. Some groupings 
even yielded worse results. The tested groupings 
included a) one set of As for each frequency value 
and b) two classes (low and high frequency) on the 
two ends of the scale, as well as several groupings 
in between and several settings for partitioning the 
classes. 
The values of Ax, A2, and A3 are estimated by 
deleted interpolation. This technique successively 
removes each trigram from the training corpus and 
estimates best values for the As from all other n- 
grams in the corpus. Given the frequency counts 
for uni-, bi-, and trigrams, the weights can be very 
efficiently determined with a processing time linear 
in the number of different rigrams. The algorithm 
is given in figure 1. Note that subtracting 1 means 
taking unseen data into account. Without this sub- 
traction the model would overfit the training data 
and would generally ield worse results. 
2.3 Handling of Unknown Words 
Currently, the method of handling unknown words 
that seems to work best for inflected languages is 
a suffix analysis as proposed in (Samuelsson, 1993). 
Tag probabilities are set according to the word's end- 
ing. The suffix is a strong predictor for word classes, 
e.g., words in the Wall Street Journal part of the 
Penn Treebank ending in able are adjectives (JJ) in 
98% of the cases (e.g. fashionable, variable), the rest 
of 2% are nouns (e.g. cable, variable). 
The probability distribution for a particular suf- 
fix is generated from all words in the training set 
that share the same suffix of some predefined max- 
imum length. The term suffix as used here means 
"final sequence of characters of a word" which is not 
necessarily a linguistically meaningful suffix. 
Probabilities are smoothed by successive abstrac- 
tion. This calculates the probability of a tag t 
given the last m letters li of an n letter word: 
P(t l ln - r ,+l , . . . ln) .  The sequence of increasingly 
more general contexts omits more and more char- 
acters of the suffix, such that P(t l ln_m+2,. . . , ln) ,  
P(t \ [ ln-m+3,. . . , l~) ,  . . . ,  P(t) are used for smooth- 
ing. The recursiou formula is 
P(t l l ,_ i+l,  . . . ln) 
= P( t l ln - i+ l , . . .  In) + O iP ( t l l~- , . . . ,  ln) (7) 
1 +0~ 
225
set  )%1 ---- )%2 = )%3 = 0 
fo reach  t r ig ram tl,t2,t3 with  f(t l ,t2,t3) > 0 
depending on the maximum of the fo l low ing  three  va lues :  
f(tl ,t2,ts)-- 1 case f (h,t2)- I  " increment )%3 by f ( t l , t2 , t3)  
f(t2,t3)-I case f(t2)- I  " increment )%2 by f ( t l , t2 , t s )  
f ( t3 ) - - i  
case N-1  " increment )%1 by f(t l ,t2,t3) 
end 
end 
normalize )%1, )%2, )%3 
Figure 1: Algorithm for calculting the weights for context-independent li ear interpolation )%1, )%2, )%3 when 
the n-gram frequencies are known. N is the size of the corpus? If the denominator in one of the expressions 
is 0, we define the result of that expression to be 0. 
for i = m. . .  0, using the maximum likelihood esti- 
mates/5  from frequencies in the lexicon, weights Oi 
and the initialization 
P(t) =/5(t) .  (8) 
The maximum likelihood estimate for a suffix of 
length i is derived from corpus frequencies by 
P(t i /~- i+l ,  .. l~) = f (t ,  1~-~+1,... l~) ? (9 )  
For the Markov model, we need the inverse condi- 
tional probabilities P ( / , - i+ l , . . .  lnlt) which are ob- 
tained by Bayesian inversion. 
A theoretical motivated argumentation uses the 
standard eviation of the maximum likelihood prob- 
abilities for the weights 0i (Samuelsson, 1993)? 
This leaves room for interpretation. 
1) One has to identify a good value for m, the 
longest suffix used? The approach taken for TnT is 
the following: m depends on the word in question. 
We use the longest suffix that we can find in the 
training set (i.e., for which the frequency is greater 
than or equal to 1), but at most 10 characters. This 
is an empirically determined choice? 
2) We use a context-independent approach for 0i, 
as we did for the contextual weights )%i. It turned 
out to be a good choice to set al 0i to the standard 
deviation of the unconditioned maximum likelihood 
probabilities of the tags in the training corpus, i.e., 
we set 
1 
$ 
Oi = ~--~(/5(tj) - 15)2 (10) 
s 1 j= l  
for all i = 0 . . .  m - 1, using a tagset of s tags and 
the average 
$ 
/5 = 1 ~/5(t j )  (11) 
8 
j----I 
This usually yields values in the range 0.03 .. .  0.10. 
3) We use different estimates for uppercase and 
lowercase words, i.e., we maintain two different suffix 
tries depending on the capitalization of the word. 
This information improves the tagging results? 
4) Another freedom concerns the choice of the 
words in the lexicon that should be used for suf- 
fix handling. Should we use all words, or are some 
of them better suited than others? Accepting that 
unknown words are most probably infrequent, one 
can argue that using suffixes of infrequent words in 
the lexicon is a better approximation for unknown 
words than using suffixes of frequent words. There- 
fore, we restrict the procedure of suffix handling to 
words with a frequency smaller than or equal to some 
threshold value. Empirically, 10 turned out to be a 
good choice for this threshold. 
2.4 Cap i ta l i za t ion  
Additional information that turned out to be use- 
ful for the disambiguation process for several cor- 
pora and tagsets is capitalization information. Tags 
are usually not informative about capitalization, but 
probability distributions of tags around capitalized 
words are different from those not capitalized. The 
effect is larger for English, which only capitalizes 
proper names, and smaller for German, which capi- 
talizes all nouns. 
We use flags ci that are true if wi is a capitalized 
word and false otherwise? These flags are added to 
the contextual probability distributions. Instead of 
P(tsItl,t2) (12) 
we use 
P(t3, c3\[tl , cl, t2, c2) (13) 
and equations (3) to (5) are updated accordingly. 
This is equivalent o doubling the size of the tagset 
and using different ags depending on capitalization. 
226
2.5 Beam Search 
The processing time of the Viterbi algorithm (Ra- 
biner, 1989) can be reduced by introducing a beam 
search. Each state that receives a 5 value smaller 
than the largest 5 divided by some threshold value 
is excluded from further processing. While the 
Viterbi algorithm is guaranteed to find the sequence 
of states with the highest probability, this is no 
longer true when beam search is added. Neverthe- 
less, for practical purposes and the right choice of 
0, there is virtually no difference between the algo- 
rithm with and without a beam. Empirically, a value 
of 0 = 1000 turned out to approximately double the 
speed of the tagger without affecting the accuracy. 
The tagger currently tags between 30~000 and 
60,000 tokens per second (including file I/O) on a 
Pentium 500 running Linux. The speed mainly de- 
pends on the percentage of unknown words and on 
the average ambiguity rate. 
3 Eva luat ion  
We evaluate the tagger's performance under several 
aspects. First of all, we determine the tagging ac- 
curacy averaged over ten iterations. The overall ac- 
curacy, as well as separate accuracies for known and 
unknown words are measured. 
Second, learning curves are presented, that indi- 
cate the performance when using training corpora of 
different sizes, starting with as few as 1,000 tokens 
and ranging to the size of the entire corpus (minus 
the test set). 
An important characteristic of statistical taggers 
is that they not only assign tags to words but also 
probabilities in order to rank different assignments. 
We distinguish reliable from unreliable assignments 
by the quotient of the best and second best assign- 
ments 1. All assignments for which this quotient is 
larger than some threshold are regarded as reliable, 
the others as unreliable. As we will see below, accu- 
racies for reliable assignments are much higher. 
The tests are performed on partitions of the cor- 
pora that use 90% as training set and 10% as test 
set, so that the test data is guaranteed to be unseen 
during training. Each result is obtained by repeat- 
ing the experiment 10 times with different partitions 
and averaging the single outcomes. 
In all experiments, contiguous test sets are used. 
The alternative is a round-robin procedure that puts 
every 10th sentence into the test set. We argue that 
contiguous test sets yield more realistic results be- 
cause completely unseen articles are tagged. Using 
the round-robin procedure, parts of an article are al- 
ready seen, which significantly reduces the percent- 
age of unknown words. Therefore, we expect even 
1 By definition, this quotient is co if there is only one pos- 
sible tag for a given word. 
higher results when testing on every 10th sentence 
instead of a contiguous et of 10%. 
In the following, accuracy denotes the number of 
correctly assigned tags divided by the number of to- 
kens in the corpus processed. The tagger is allowed 
to assign exactly one tag to each token. 
We distinguish the overall accuracy, taking into 
account all tokens in the test corpus, and separate 
accuracies for known and unknown tokens. The lat- 
ter are interesting, since usually unknown tokens are 
much more difficult to process than known tokens, 
for which a list of valid tags can be found in the 
lexicon. 
3.1 Tagging the  NEGRA corpus  
The German NEGRA corpus consists of 20,000 sen- 
tences (355,000 tokens) of newspaper texts (Frank- 
furter Rundschau) that are annotated with parts-of- 
speech and predicate-argument structures (Skut et 
al., 1997). It was developed at the Saarland Univer- 
sity in Saarbrficken 2. Part of it was tagged at the 
IMS Stuttgart. This evaluation only uses the part- 
of-speech annotation and ignores structural annota- 
tions. 
Tagging accuracies for the NEGRA corpus are 
shown in table 2. 
Figure 3 shows the learning curve of the tagger, 
i.e., the accuracy depending on the amount of train- 
ing data. Training length is the nmnber of tokens 
used for training. Each training length was tested 
ten times, training and test sets were randomly cho- 
sen and disjoint, results were averaged. The training 
length is given on a logarithmic scale. 
It is remarkable that tagging accuracy for known 
words is very high even for very small training cot- 
pora. This means that we have a good chance of 
getting the right tag if a word is seen at least once 
during training. Average percentages of unknown 
tokens are shown in the bottom line of each diagram. 
We exploit the fact that the tagger not only de- 
termines tags, but also assigns probabilities. If there 
is an alternative that has a probability "close to" 
that of the best assignment, his alternative can be 
viewed as almost equally well suited. The notion of 
"close to" is expressed by the distance of probabil- 
ities, and this in turn is expressed by the quotient 
of probabilities. So, the distance of the probabili- 
ties of a best tag tbest and an alternative tag tart 
is expressed by P(tbest)/p(tau), which is some value 
greater or equal to 1 since the best tag assignment 
has the highest probability. 
Figure 4 shows the accuracy when separating as- 
signments with quotients larger and smaller than 
the threshold (hence reliable and unreliable assign- 
ments). As expected, we find that accuracies for 
2For availability, please check 
h~tp ://w~. col i. uni-sb, de/s fb378/negra-corpus 
777 227
Table 2: Part-of-speech tagging accuracy for the NEGRA corpus, averaged over 10 test runs, training and 
test set are disjoint. The table shows the percentage of unknown tokens, separate accuracies and standard 
deviations for known and unknown tokens, as well as the overall accuracy. 
percentage 
unknowns 
NEGRA corpus 11.9% 
known 
ace .  
97.7% 0.23 
unknown 
acc. (x 
89.0% 0.72 
overall 
aCE. o" 
96.7% 0.29 
NEGRA Corpus :  POS Learn ing  Curve  
100 
9O 
80 
70 /S  
6O 
50 , i 
1 2 5 
50.8 46.4 41.4 
I i i I I I i 
10 20 50 100 200 320 500 
36.0 30.7 23.0 18.3 14.3 11.9 11).3 
Overall 
min =78.1% 
max=96.7% 
e Known 
rain =95.7% 
max=97.7% 
- - -a- - -  Unknown 
rain =61.2% 
max=89.0% 
I 
1000 x 1000 Training Length 
St  avg. percentage unknown 
Figure 3: Learning curve for tagging the NEGRA corpus. The training sets of variable sizes as well as test 
sets of 30,000 tokens were randomly chosen. Training and test sets were disjoint, the procedure was repeated 
10 times and results were averaged. Percentages of unknowns for 500k and 1000k training are determined 
from an untagged extension. 
NEGRA Corpus: Accuracy of  re l iab le  assignments 
100 
99 
98 
97 
A 
f " / :. Reliable rain =96.7% max=99.4% 
96 I i i i i i i i i i i 
2 5 10 20 50 100 500 2000 10000 threshold 0 
100 97.9 95.1 92.7 90.3 86.8 84.1 81.0 76.1 71.9 68.3 64.1 62.0 % cases reliable 
- 53.5 62.9 69.6 74.5 79.8 82.7 85.2 88.0 89.6 90.8 91.8 92.2 acc. of complement 
Figure 4: Tagging accuracy for the NEGRA corpus when separating reliable and unreliable assignments. The 
curve shows accuracies for reliable assignments. The numbers at the bottom line indicate the percentage of 
reliable assignments and the accuracy of the complement set (i.e., unreliable assignments). 
228
Table 5: Part-of-speech tagging accuracy for the Penn Treebank. The table shows the percentage of unknown 
tokens, separate accuracies and standard deviations for known and unknown tokens, as well as the overall 
accuracy. 
I percentage known 
unknowns acc. a 
Penn Treebank 2.9% 97.0% 0.15 
unknown 
aCC. O" 
85.5% 0.69 
overall 
aCE. O" 
96.7% 0.15 
100 
9O 
80 
70 < 
60 
Penn Treebank: POS Learn ing  Curve  
/ 
50 I I ~ i I I I i I 
1 2 5 10 20 50 100 200 500 
50.3 42.8 33.4 26.8 20.2 13.2 9.8 7.0 4.4 
Overall 
rain =78.6% 
max=96.7% 
Known 
rain =95.2% 
max=97.0% 
Unknown 
min =62.2% 
max=85.5% 
I 
1000 ? 1000 Training Length 
2.9 avg. percentage unknown 
Figure 6: Learning curve for tagging the Penn Treebank. The training sets of variable sizes as well as test sets 
of 100,000 tokens were randomly chosen. Training and test sets were disjoint, the procedure was repeated 
10 times and results were averaged. 
Penn Treebank: Accuracy of reliable assignments 
100 
99 
98 
97 
Overall 
rain =96.6% 
max=99.4% 
96 i i i i t i i i i i i i 
2 5 10 20 50 100 500 2000 10000 threshold 0 
100 97.7 94.6 92.2 89.8 86.3 83.5 80.4 76.6 73.8 71.0 67.2 64.5 % cases reliable 
- 53.5 62.8 68.9 73.9 79.3 82.6 85.2 87.5 88.8 89.8 91.0 91.6 acc. of complement 
Figure 7: Tagging accuracy for the Penn Treebank when separating reliable and unreliable assignments. The 
curve shows accuracies for reliable assignments. The numbers at the bottom line indicate the percentage of 
reliable assignments and the accuracy of the complement set. 
229 
reliable assignments are much higher than for unre- 
liable assignments. This distinction is, e.g., useful 
for annotation projects during the cleaning process, 
or during pre-processing, sothe tagger can emit mul- 
tiple tags if the best tag is classified as unreliable. 
3.2 Tagging the Penn Treebank  
We use the Wall Street Journal as contained in the 
Penn Treebank for our experiments. The annotation 
consists of four parts: 1) a context-free structure 
augmented with traces to mark movement and dis- 
continuous constituents, 2) phrasal categories that 
are annotated as node labels, 3) a small set of gram- 
matical functions that are annotated as extensions to 
the node labels, and 4) part-of-speech tags (Marcus 
et al, 1993). This evaluation only uses the part-of- 
speech annotation. 
The Wall Street Journal part of the Penn Tree- 
bank consists of approx. 50,000 sentences (1.2 mil- 
lion tokens). 
Tagging accuracies for the Penn Treebank are 
shown in table 5. Figure 6 shows the learning curve 
of the tagger, i.e., the accuracy depending on the 
amount of training data. Training length is the num- 
ber of tokens used for training. Each training length 
was tested ten times. Training and test sets were 
disjoint, results are averaged. The training length is 
given on a logarithmic scale. As for the NEGRA cor- 
pus, tagging accuracy is very high for known tokens 
even with small amounts of training data. 
We exploit the fact that the tagger not only de- 
termines tags, but also assigns probabilities. Figure 
7 shows the accuracy when separating assignments 
with quotients larger and smaller than the threshold 
(hence reliable and unreliable assignments). Again, 
we find that accuracies for reliable assignments are 
much higher than for unreliable assignments. 
3.3 Summary of Part-of-Speech Tagging 
Results 
Average part-of-speech tagging accuracy is between 
96% and 97%, depending on language and tagset, 
which is at least on a par with state-of-the-art re- 
sults found in the literature, possibly better. For 
the Penn Treebank, (Ratnaparkhi, 1996) reports an 
accuracy of 96.6% using the Maximum Entropy ap- 
proach, our much simpler and therefore faster HMM 
approach delivers 96.7%. This comparison eeds to 
be re-examined, since we use a ten-fold crossvalida- 
tion and averaging of results while Ratnaparkhi only 
makes one test run. 
The accuracy for known tokens is significantly 
higher than for unknown tokens. For the German 
newspaper data, results are 8.7% better when the 
word was seen before and therefore is in the lexicon, 
than when it was not seen before (97.7% vs. 89.0%). 
Accuracy for known tokens is high even with very 
small amounts of training data. As few as 1000 to- 
kens are sufficient o achieve 95%-96% accuracy for 
them. It is important for the tagger to have seen a 
word at least once during training. 
Stochastic taggers assign probabilities to tags. We 
exploit the probabilities to determine reliability of 
assignments. For a subset hat is determined uring 
processing by the tagger we achieve accuracy rates 
of over 99%. The accuracy of the complement set is 
much lower. This information can, e.g., be exploited 
in an annotation project to give an additional treat- 
ment to the unreliable assignments, or to pass se- 
lected ambiguities to a subsequent processing step. 
4 Conc lus ion  
We have shown that a tagger based on Markov mod- 
els yields state-of-the-art results, despite contrary 
claims found in the literature. For example, the 
Markov model tagger used in the comparison of (van 
Halteren et al, 1998) yielded worse results than all 
other taggers. In our opinion, a reason for the wrong 
claim is that the basic algorithms leave several deci- 
sions to the implementor. The rather large amount 
of freedom was not handled in detail in previous pub- 
lications: handling of start- and end-of-sequence, the 
exact smoothing technique, how to determine the 
weights for context probabilities, details on handling 
unknown words, and how to determine the weights 
for unknown words. Note that the decisions we made 
yield good results for both the German and the En- 
glish Corpus. They do so for several other corpora 
as well. The architecture remains applicable to a 
large variety of languages. 
According to current tagger comparisons (van 
Halteren et al, 1998; Zavrel and Daelemans, 1999), 
and according to a comparsion of the results pre- 
sented here with those in (Ratnaparkhi, 1996), the 
Maximum Entropy framework seems to be the only 
other approach yielding comparable results to the 
one presented here. It is a very interesting future 
research topic to determine the advantages of either 
of these approaches, to find the reason for their high 
accuracies, and to find a good combination of both. 
TnT is freely available to universities and re- 
lated organizations for research purposes (see 
http ://www. coli. uni-sb, de/-thorsten/tnt). 
Acknowledgements  
Many thanks go to Hans Uszkoreit for his sup- 
port during the development of TnT. Most of the 
work on TnT was carried out while the author 
received a grant of the Deutsche Forschungsge- 
meinschaft in the Graduiertenkolleg Kognitionswis- 
senschaft Saarbriicken. Large annotated corpora re 
the pre-requisite for developing and testing part-of- 
speech taggers, and they enable the generation of 
high-quality language models. Therefore, I would 
230 
like to thank all the people who took the effort 
to annotate the Penn Treebank, the Susanne Cor- 
pus, the Stuttgarter Referenzkorpus, the NEGRA 
Corpus, the Verbmobil Corpora, and several others. 
And, last but not least, I would like to thank the 
users of TnT who provided me with bug reports and 
valuable suggestions for improvements. 
Re ferences  
Thorsten Brants, Wojciech Skut, and Hans Uszko- 
reit. 1999. Syntactic annotation of a German 
newspaper corpus. In Proceedings of the ATALA 
Treebank Workshop, pages 69-76, Paris, France. 
Eric Brill. 1993. A Corpus-Based Approach to Lan- 
guage Learning. Ph.D. Dissertation, Department 
of Computer and Information Science, University 
of Pennsylvania. 
Eugene Charniak, Curtis Hendrickson, Neil Ja- 
cobson, and Mike Perkowitz. 1993. Equations 
for part-of-speech tagging. In Proceedings of the 
Eleventh National Con\[erence on Artificial In- 
telligence, pages 784-789, Menlo Park: AAAI 
Press/MIT Press. 
Doug Cutting, Julian Kupiec, Jan Pedersen, and 
Penelope Sibun. 1992. A practical part-of-speech 
tagger. In Proceedings of the 3rd Conference 
on Applied Natural Language Processing (ACL), 
pages 133-140. 
Walter Daelemans, Jakub Zavrel, Peter Berck, and 
Steven Gillis. 1996. Mbt: A memory-based part 
of speech tagger-generator. In Proceedings of the 
Workshop on Very Large Corpora, Copenhagen, 
Denmark. 
Mitchell Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: The Penn Treebank. Compu- 
tational Linguistics, 19(2):313-330. 
Lawrence R. Rabiner. 1989. A tutorial on Hid- 
den Markov Models and selected applications in 
speech recognition. In Proceedings o\] the IEEE, 
volume 77(2), pages 257-285. 
Adwait Ratnaparkhi. 1996. A maximum entropy 
model for part-of-speech tagging. In Proceedings 
o\] the Conference on Empirical Methods in Nat- 
ural Language Processing EMNLP-96, Philadel- 
phia, PA. 
Christer Samuelsson. 1993. Morphological tag- 
ging based entirely on Bayesian inference. In 
9th Nordic Conference on Computational Lin- 
guistics NODALIDA-93, Stockholm University, 
Stockholm, Sweden. 
Helmut Schmid. 1995. Improvements in part-of- 
speech tagging with an application to German. 
In Helmut Feldweg and Erhard Hinrichts, editors, 
Lexikon und Text. Niemeyer, Tfibingen. 
Wojciech Skut, Brigitte Krenn, Thorsten Brants, 
and Hans Uszkoreit. 1997. An annotation scheme 
for free word order languages. In Proceedings of 
the Fifth Conference on Applied Natural Language 
Processing ANLP-97, Washington, DC. 
Hans van Halteren, Jakub Zavrel, and Walter Daele- 
mans. 1998. Improving data driven wordclass tag- 
ging by system combination. In Proceedings of the 
International Conference on Computational Lin- 
guistics COLING-98, pages 491-497, Montreal, 
Canada. 
Martin Volk and Gerold Schneider. 1998. Compar- 
ing a statistical and a rule-based tagger for ger- 
man. In Proceedings of KONVENS-98, pages 125- 
137, Bonn. 
Jakub Zavrel and Walter Daelemans. 1999. Eval- 
uatie van part-of-speech taggers voor bet cor- 
pus gesproken ederlands. CGN technical report, 
Katholieke Universiteit Brabant, Tilburg. 
231 
Probabilistic Parsing and Psychological Plausibility 
Thors ten  Brants  and  Mat thew Crocker  
Saarland University, COlnl)U|;al;ional Linguistics 
D-6G041 Saarbriicken, Germany 
{brants ,  crocker}@coi?,  un?-sb ,  de 
Abst ract  
Given the recent evidence for prot)abilistic 
mechanisms in models of hmnan aml)iguity res- 
olution, this paper investigates the plausibil- 
ity of exl)loiting current wide-coverage, 1)rob- 
al)ilistic 1)arsing techniques to model hmnan 
linguistic t)ert'orman(:e. In l)arl.i(:ulm ', we in- 
vestigate the, t)crforlnance of stan(tar(l stoclms- 
tic parsers when they arc revis(;(l to el)crate 
incrementally, and with reduced nlenlory re- 
sources. We t)resent techniques for ranking 
and filtering mlMyses, together with exl)erimen- 
tal results. Our results confirm that stochas- 
tic parsers which a(lhere to these 1)sy('hologi- 
cally lnotivated constraints achieve goo(l l)er- 
f()rman(:e. Memory cast t)e reduce(t (lown to 
1% ((:Oml)are(l to exhausitve search) without re- 
ducing recall an(l 1)rox:ision. A(lditionally, thes(; 
models exhil)it substamtially faster l)ertbrmance. 
FinMly, we ~rgue that this generM result is likely 
to hold for more sophisticated, nnd i)sycholin- 
guistically plausil)le, probal)ilistic parsing mod- 
els. 
1 I n t roduct ion  
Language engineering and coml)ut~tional psy- 
cholinguistics are often viewed as (list|net re- 
search progrmnmes: engineering sohttions aim 
at practical methods which ('an achieve good 
1)erformance, typically paying little attention 
to linguistic or cognitive modelling. Comlm- 
tational i)sycholing,fistics, on the other hand, 
is often focussed on detailed mo(lelling of hu- 
man lmhaviour tbr a relatively small number 
of well-studied constructions. In this paper we 
suggest hat, broadly, the human sentence pro- 
cessing mechanism (HSPM) and current statis- 
ti(:al parsing technology can be viewed as having 
similar ol)jectives: to optimally (i.e. ral)idly and 
accurately) understand l;he text and utl;erances 
they encounter. 
Our aim is to show that large scale t)robabilis- 
tic t)arsers, when subjected to basic cognitive 
constraints, can still achieve high levels of pars- 
ing accuracy. If successful, this will contribute 
to a t)lausil)h; explanation of the fact th~tt I)(;() -
\])lc, in general, are also extremely accurate and 
rol)llS(;. Sllch a 1'o81111; Wollld also strellgthclt ex- 
isting results showing that related l)robal)ilistic 
lne('hanisms can exl)lain specific psycholinguis- 
tic phenomena. 
To investigate this issue, we construct a stan- 
dard 'l)aseline' stochastic parser, which mir- 
rors t;he pertbrmance of a similar systems (e.g. 
(,lohnson, 1998)). We then consider an incre- 
re(total version of th(', parser, and (;v~,htat(; tim 
etf'c(:ts of several l)rol)al)ilistic filtering strate- 
gies which m'e us(,(l to 1)rune the l)arser's earch 
space, and ther(;l)y r('(lu('(', memory load. 
rio &,,-;sess th(; generMity of oltr resnll;s for 
more Sol)histi(;ate(t prot)al)ilistic models, we also 
conduct experiments using a model in which 
parent-node intbrmation is encoded on the 
(laughters. This increase in contextual informa- 
tion has t)(;(;11 shown 1;o improve t)erforlnance 
(.Johnson, 1998), and the model is also shown 
to be rolmst to the inerementality and memory 
constraints investigated here. 
We present the results of parsing pertbr- 
mance ext)eriments , showing the accuracy of 
these systems with respect to l)oth a parsed 
corpus and the 1)aseline parser. Our experi- 
ments suggest hat a strictly incremental model, 
in which memory resources are substantially 
reduced through filtering, can achieve l)reci- 
sion and recall which equals that of 'unre- 
stricted' systems. Furthermore, implementa- 
tion of these restrictions leads to substantially 
faster 1)(;rtbrmance. In (:onchlsion, we argue 
that such 1)road-coverage probabilistic parsing 
111 
models provide a valuable framework tbr ex- 
plaining the human capacity to rapidly, accu- 
rately, and robustly understand "garden va- 
riety" language. This lends further supt)ort 
to psycholinguistic a counts which posit proba- 
bilistic ambiguity resolution mechanisms to ex- 
plain "garden path" phenomena. 
It is important to reiterate that our intention 
here is only to investigate the performance of 
probabilistic parsers under psycholinguistically 
motivated constraints. We do not argue for the 
psychological plausibility of SCFG parsers (or 
the parent-encoded variant) per se. Our inves- 
tigation of these models was motivated rather 
by our desire to obtain a generalizable result 
for these simple and well-understood models, 
since obtaining similar results for more sophisti- 
cated models (e.g. (Collins, 1996; Ratnaparkhi, 
199711 might have been attributed to special 
properties of these models. Rather, the current 
result should be taken as support br the poten- 
tial scaleability and performance ofprobabilistic 
I)sychological models uch as those proposed by 
(aurafsky, 1996) and (Crocker and Brants, to 
appear). 
2 Psycholinguistic Mot ivat ion  
Theories of human sentence processing have 
largely been shaped by the study of pathologies 
in tnnnan language processing behaviour. Most 
psycholinguistic models seek to explain the d{f- 
ficulty people have in comprehending structures 
that are ambiguous or memory-intensive (see 
(Crocker, 1999) for a recent overview). While 
often insightflfl, this approach diverts attention 
from the fact that people are in fact extremely 
accnrate and effective in understanding the 
vast majority of their "linguistic experience". 
This observation, combined with the mounting 
psycholinguistic evidence for statistically-based 
mechanisms, leads us to investigate the merit of 
exploiting robust, broad coverage, probabilistie 
parsing systems as models of hmnan linguistic 
pertbrmance. 
The view that hmnan language processing 
can be viewed as an optimally adapted sys- 
tem, within a probabilistic fl'amework, is ad- 
vanced by (Chater et al, 19981, while (Juraf- 
sky, 19961 has proposed a specific probabilis- 
tic parsing model of human sentence process- 
ing. In work on human lexical category dis- 
ambiguation, (Crocker and Corley, to appear), 
have demonstrated that a standard (iimrmnen- 
tal) HMM-based part-of-speech tagger mod- 
els the finding from a range of psycholinguis- 
tic experiments. In related research, (Crocker 
and Brants, 19991 present evidence that an 
incremental stochastic parser based oll Cas- 
caded Markov Models (Brants, 1999) can ac- 
count tbr a range of experimentally observed 
local ambiguity preferences. These include 
NP/S complement ambiguities, reduced relative 
clauses, noun-verb category ambiguities, and 
'that'-ambiguities (where 'that' can be either a 
complementizer or a determiner) (Crocker and 
Brants, to appear). 
Crucially, however, there are differences be- 
tween the classes of mechanisms which are psy- 
chologically plausible, and those which prevail 
in current language technology. We suggest that 
two of the most important differences concern 
incrcmentality~ and memory 7vso'urces. There is 
overwhehning experimental evidence that peo- 
ple construct connected (i.e. semantically in- 
terpretable) analyses for each initial substring 
of an utterance, as it is encountered. That is, 
processing takes place incrementally, from left 
to right, on a word by word basis. 
Secondly, it is universally accecpted that peo- 
ple can at most consider a relatively small 
number of competing analyses (indeed, some 
would argue that number is one, i.e. process- 
ing is strictly serial). In contrast, many exist- 
ing stochastic parsers are "unrestricted", in that 
they are optinfised tbr accuracy, and ignore such 
t)sychologically motivated constraints. Thus the 
appropriateness of nsing broad-coverage proba- 
bilistic parsers to model the high level of hu- 
man performance is contingent upon being able 
to maintain these levels of accuracy when the 
constraints of" incrementality and resource limi- 
rations are imposed. 
3 Incremental Stochastic 
Context-Free Parsing 
The fbllowing assumes that the reader is fa- 
miliar with stochastic context-free grammars 
(SCFG) and stochastic chart-parsing tech- 
niques. A good introduction can be found, e.g., 
in (Manning and Schfitze, 19991. We use stan- 
dard abbreviations for terminial nodes, 11051- 
terminal nodes, rules and probabilities. 
112 
This t)tq)er invcsl;igates tochastic (;onl;(;xl;- 
fl'ee parsing l)ascd on ~ grmmmu" (;hat is (tcrivc(l 
from a trcel)ank, start ing with 1)art-ofsl)eech 
ta,gs as t(;rlninals. The gl:;~nllnt~r is (lcriv(;d l)y 
(:olle(:ting M1 rul('.s X -+ c~ th;tt oc(:ur in the tr(',(;- 
bank mM (;heir ffe(lU(m(:i('~s f .  The l)l'()l);tl)ilil;y 
of a rule is set to 
.f(x l ' ( x  - (:l) 
E .f(x 
fl 
\],br ~ descril)l;ion of treebank grammars see 
(Charniak, 1.996). The gr~mmmr does not coii- 
ta.in c-rules, oth(:rwis(: th(:r(: is no restriction 
oll the rules. In particular, w(: do not r(:quir(' 
C homsky-NormM-Form. 
In addit ion to the rult:s tha(; corr(:st)ond 
(;o sl;rucl;ur(:s in th(: corpus, w(: a.dd ;~ new 
st~u:l; sylnl)ol ROOT to l;h(; grnmmar and rules 
ROOT -~ X for all non-t;(;rminals X togel;lwx 
with l)rol)al)iliti('s (h:):iv(:d l'roln th(: root n()(t(:s 
in th(: tort)us I. 
For t)m:sing th(:se gr~unmn)'s, w(: r(:ly upon 
n stan(tard l)oLi;onl-U t) (:ha.rl,-t)arsing t(:(:hniqu(: 
with n modification for in(:rcmental parsing, i.(:., 
tbt" each word, all edges nr(: proc(:ss(:d and l)ossi- 
b\]y 1)run(:d 1)(:ti)r(: \])ro(:e(:(ling to the next word. 
Th(: outlilm of th(: Mgorithm is as follows. 
A (:hart; (:ntry 1~ (:onsists of a sl;;u:I, aim (:n(l 1)o- 
s it ion i ;rod j ,  a (tott(:d rul(: X ~ (~:.'7, tim insi(t(: 
l)rol)nl)ility fl(Xi,.j) thud; X g(:n(:ra.tx:s l;ll(: t(:rmi- 
hal string from t)osi(:ion i to .7, mM information 
M)out th(: most l)robat)\](: ilL~i(t(' stru(:i;ur(:. 1t7 th(: 
dot of th(: dotte(t ruh: is nt th(' r ightmost i)osi- 
tion, the corresl)ondillg (:(lg(: is an inactive edg(:. 
If the (tot is at mty other 1)osition, il; is mt ,,ctivc, 
edge. Imu:l;ivo, e(tgcs repr(',scnt re('ogniz(',d hypo- 
(:heti(:a,1 constituents, whil(; a(:tiv(; (;(lg(',s r(;1)r(;- 
s(:nt 1)r(:lixes of hyl)ol;heticM (:()llsi;it;ll(:lll;s. 
Th(: i th t(:rminal nod(: I,i l;lla, t; (:nt(:rs th(: (:hart 
gencra, tcs an inactive edge for l;\]m span (i - 1, i). 
Ba, sed on this, n(;w active mid inactive (;(lges are 
generated according to the stan(t~tr(t algorithm. 
Sine(: we are ilfl;(:r(:stcd in th(: most i)robM)le 
pars(:, the chart can be minimized in th(: tbl- 
lowing way whik: sti\]l 1)crfi)rming an ('xhaustiv(: 
search. If" ther(: is mor(: l;hm~ one (:(lg(~ that  cov- 
ers a span ( i , j )  having (;h(', sa, me non-t(:rminM 
symbol on th(; lefIAmnd side of th(: (to(,(x:(l rule, 
1The ROOT node is used int;ernally fl)r parsing; it is 
neither emitted nor count,ed for recall and l)recision. 
only the one with the highest inside prol)M)ility 
is k(;1)t ill tit(; (:\]mrt. The others cmmot con- 
trilmt(; to th(; most i)rol)M)le 1)nrse.. 
For an ina('tiv(: edge si)aiming i to j and rei)- 
rcs(mting the rule X --> y1 . . .yq~ the inside 
l)robM)ility/31 is set to 
\[d 
il = 1"(x -+ H (2) 
/=\] 
wher(: il and jl mm'k the start and end t)ostition 
of Yl having i = il nnd j = Jr. The insid(: 
prol)M)ility tbr an active cdg(: fiA with the dot 
after th(: kth syml)ol of th(: right-hmM side is 
sol, to 
k 
I I  <r ' t I t  il,jl} (3) 
l-d 
W(: (lo not use the t)rol)M)i\]ity of th(: rule a.t this 
point. This allows us to ('oral)in(: a.ll (:(Ig(:s with 
(;h(: sam(: st)m~ and th(: dot al; th(: sam(: 1)osition 
but with (liiI'er(:uI; symbols on the l(,ft-hmM side. 
Jntrodu(:ing a distinguish(:(1 M't-hand sid(: only 
for in~mtiv(: (lg('s significantly r(:du(;(:s th(: nun> 
b(:r of a(:(;iv(: (:dg(:s in the (:hm't. This goes one 
st, e t) furth(:r than lint)licitly right-1)inarizing th(: 
grmmnar; not only suilix(:s of right-hmM si(h:s 
are join(:(t, but also l;hc ('orr(:sponding l(:fi;-hand 
sid(:s. 
d Memory  Rest r i c t ions  
\?(: inv(:stig~rt(: th(: (dimin~I;ion (pruning) ()f 
edges from th(: ('hnrt in our in('r(:nl(:n|;a| \])re's- 
ing sch(:m(:. Aft(:r processing a word and b(:fi))'(: 
1)roc(:cding to the n(:xt word during incremental 
1)re:sing, low rnnk(,d edges ~r(: removed. This is 
(:(luivM(:lfl; t;() imposing m(:mory rcsia'ictions on 
the t)ro(:('ssing system. 
The, original algor ithm k('ei)s on(; edge in th(: 
(:hart fi)r each (:oml)ination of span (start and 
cn(l position) ~md non-tcrmimd symbol (for in- 
active edges) or r ight-hand side l)r(:fixcs of (lot;- 
te(t rules (for active edges). With 1)tinting, we 
restric(; the mmfl)cr of edges allowed per span. 
The limit~tion (:an b(: cxi)resscd in two ways: 
1. Va'riable bcam,. Sch:ct a threshold 0 > 1. 
Edg(: c. is removed, ill its 1)rol)ability is p~:, 
I;lm 1)csl; l)rol)M)ility fi)r the span is Pl, and 
v,; < pl_. (~l) 
0 
113 
2. Fixed beam. Select a maximum number of 
edges per span m. An edge e is removed, if 
its prot)ability is not in the first m highest 
probabilities tbr edges with the same span. 
We pertbrmed exl)eriments using both types 
of beauls. Fixed beams yielded consistently bet- 
ter results than variable beams when t)lotting 
chart size vs. F-score. Thereibre, the following 
results are reported tbr fixed t)eams. 
We, compare and rank edges covering the 
same span only, and we rank active and inactive 
edges separately. This is in contrast to (Char- 
niak et al, 1998) who rank all edges. They 
use nornmlization in order to account tbr dif- 
ferent spans since in general, edges for longer 
spans involve more nmltiplications of t)robabil - 
ities, yielding lower probabilities. Charniak et 
al.'s normalization value is calculated by a dil- 
ferent probability model than the inside proba- 
bilities of the edges. So, in addition to the nor- 
malization for different span lengths, they need 
a normalizatio11 constant hat accounts tbr the 
different probability models. 
This investigation is based on a much simt)ler 
ranking tbrmula. We use what can be described 
as the unigram probability of a non-terminal 
node, i.e., the a priori prot)ability of the co l  
resl)onding non-ternlinal symbol(s) times the 
inside t)robat)ility. Thus, fi~r an inactive edge 
(i, j, X --> (~,/31(Xi,j)}, we use the l)rob~fl)ility 
Pm(X i , j )  = P (X)  . P ( tg . . . t j _ I IX )  (5) 
= 
for ranking. This is the prol)ability of the node 
and its yield being present in a parse. The 
higher this value, |;lie better is this node. flI is 
the inside probability for inactive edges as given 
in eqnation 2, P(X)  is the a priori probability 
tbr non-terminal X, (as estimated from the fre- 
quency in the training COrlmS) and Pm is the 
probability of the edge tbr the non-terminal X
spanning positions i to j that is used tbr rank- 
ing. 
For an active edge { i , j ,X  --~ y1 . . . yk .  
yk+l  ym,  y )  k ? " ~ , ,~ )) " ' "  ~A( i l , j l  (the (tot is aI" 
ter the kth symbol of 
llSe: 
the right-hand side) we 
(7) 
= P(Y I . . .Yk ) . f lA (E I~, :h . . .Y i~ , jk )  (9) 
p (y l  ,,, yk)  can be read ()If the corpus. It is 
the a priori probability that the right-hand side 
of a production has the prefix y1 ... y/c, which 
is estilnated by 
f (y l  . . .  yt~ is prefix) 00) 
N 
where N is the total number of productions in 
the corpus 2, i = ij, j = j/~ and flA is the inside 
probability of the pretix. 
5 Exper iments  
5.1 Data  
We use sections 2 - 21 of the Wall Street Jour- 
lYecl)ank (Marcus el; al., nal part of' the Penn ~ " 
1993) to generate a treebank grammar. Traces, 
flmctional tags and other tag extensions that do 
not mark syntactic ategory are removed before 
training 3. No other modifications are made. For 
testing, we use the \] 578 sentences of length 40 
or less of section 22. The input to the parser is 
the sequence of i)art-ofspeech tags. 
5.2 Eva luat ion 
For evaluation, we use the parsewfi measures 
and report labeld F-score (the harmolfiC mean 
of labeled recall and labeled precision). R.eport- 
ing the F-score makes ore" results comt)aral)le to 
those of other previous experinmnts using the 
same data sets. As a n leasure  of  the an lount  
of work done by the parser, we report the size 
of the chart. The mnnl)er of active and imm- 
rive edges that enter the chart is given tbr the 
exhaustive search, not cored;lug those hypothet- 
ical edges theft are replaced or rejected because 
there is an alternative dge with higher t)roba- 
t)ility 4. For t)runed search, we give |:tie percent- 
age of edges required. 
5.3 F ixed Beam 
For our experiments, we define the beam by a 
maximunl number of edges per span. Beams 
for active and inactive edges are set separately. 
The Imams run from 2 to 12, and we test all 
2Here, we use proper prefixes, i.e., all prefixes not 
including the last element. 
aAs an example, PP-TMP=3 is replaced 173, PP. 
4The size of the chart is corot)arable to the "number 
of edges popped" as given in (Chanfiak et al, 1998). 
114 
i 
78 
77 
(D 
8 7o cJ~ 
75 
(1) 
74 z$ 
73 
72 
71 
79 
Resu l ts  w i th  Or ig ina l  and  Parent  Encod ing  
A 
ctive: 8 
/ ma?.:l'~v?", "~I ;t(:l ix'(" (i 
/ j  j~  activ(',: 3 
C 
" ' ' ; "  F l l ldCt  1V( 12  
active: 9 
inactive,: 2 
active: 3 / 
1.0 1.2 
ina(:tiv(',: 6 ilmctive: 8 
a(:l.ive: d . a('tivo,: 7 
I I 1 \[ \] i I I i - -  
\] .d 1.6 1.8 2.0 2.2 2.d 2.6 2.8 3.0 % (:hart; size 
Figure 1: \]!}xt)erimental results tbr increJnelfl;al parsing and t)rmfing. The figm:e shows the percent- 
age of edges relative to (',xhaustiv(; s(;ar(:h mid l;h(', F-s(:()re a(:hieved with this chart size. Exhaustive 
search yiehled 71.21% fin" th(; original en(:o(ting and 7!).28% for the I)arent (m(:o(ting. l/.c, sull;s in the 
grey ar(;as are equiwflent with a (:()nli(l('n(:('~ (tegr(',e of (~ =: 0.99. 
12\] comlfi\]~ati(ms of the, s(~ lmmus for ac:i;ivc and 
illactiw~ edges, l~ach setting results in a lm.ri;ic -
ulm" average size of l;he chart and an F-score, 
which arc tel)erred ill (;he following se(:l;ioll. 
5.4  Exper imenta l  Resu l ts  
The results of our 121 tes(; Hills with (tifl'erent 
settings for active and in;u:tivc \])(~a.ms m'e given 
in figure 1. The (tittgranl shows ch~trt sizes vs. 
labeled F-scores. It sorts char|; sizes across d i f  
ferent sel;l;ings of the beams. If several beam 
sett;ings result in equiwdenfi chart sizes, the di- 
agram cent;tins the one yielding th(', highes|, F- 
SCOI ' (L  
The 111~ill tinding is thai: we can r('xlu(:e the 
size of the chart to l)el;ween 1% and 3% of 
the size required fi)r exhaustive s(,ar(:h without 
affecting the results. Only very small 1)cams 
d(;grad(' t)ertbrmance 5. The eiti;ct occurs for 
both models despite the simple ranking formub~. 
This significantly reduces memory r(,quirements 
'~Givc, n the' amount of test data (26,322 non-terminal 
nod(!s), results within a rang(' of around 0.7% arc cquiv- 
al(mt with a (:onfidcnc(; degr(',(, of (~ = 99%. 
(given as size of the chart) and increases l)m'sing 
qmed. 
i1 t Exhaustive search yields an I -Score of 
71.21 % when using the original Petal %'eel)ank 
cn(:odh~g. ()nly around 1% the edges are re- 
(tuir('.d to yield e.(tuiwdcnt resul(;s with incrcm(,.n- 
tal processing and printing after each word is 
added to the chart;. This result is, among other 
settings, obtained by a tixcd beam of 2 for in- 
active edges and 3 tin" active e(lges ri
1,br the parmtt encoding, exhaustive search 
yields an l,-Scorc of 79.28%. Only 1)etween 2 
mM 3% of the edges are required to yMd an 
equiwflcnt result with incremental t)l'OCcSSillg 
and pruning. As an cXmnl)le, the point at size 
= 3.0% F-score = 79.1% is generated by the 
beam setting of 12 for imml;ive and 9 tbr active 
edges. The parent encoding yields around 8% 
higher F-scores but it also imposes a higher ab- 
solute and relative memory load on t;he process. 
The higher (hw'ee of par~dlelism in l;he inactive 
(;Using variable Imams, wc would nccd \].95% of the 
\[:hart entries 1;o achieve an (Kl l l ivalenI ;  F - scor (x  
115 
chart stems from the parent hytmthesis in each 
node. In terms of pure node categories, the av- 
erage number of parallel nodes at this point is 
3.5 7 . 
Exhaustive search for the base encoding needs 
in average 140,000 edges per sentence, tbr tile 
parent encoding 200,000 edges; equivalent re- 
sults for the base encoding can be achieved with 
around 1% of these edges, equivalent results tbr 
the parent encoding need between 2 and 3%. 
The lower mmlber of edges significantly in- 
creases parsing speed. Using exhaustive search 
tbr the base model, the parser processes 3.0 to- 
kens per second (measured on a Pentium III 
500; no serious efforts of optimization have gone 
into the parser). With a chart size of 1%, speed 
is 630 tokens/second. This is a factor of 210 
without decreasing accuracy. Sl)eed for the par- 
ent model is 0.5 tokens/second (exhaustive) and 
111 tokens/seconds (3.0% chart size), yielding 
an improvement by factor 220. 
6 Related Work 
Probably mostly related to the work reported 
here are (Charniak et al, 1998) and (Roark and 
Johnson, 1999). Both report on significantly 
improved parsing efl:iciency by selecting only 
subset of edges tbr processing. There are three 
main differences to our at)t)roach. One is that 
they use a ranking fbr best-first search while 
we immediately prune hypotheses. They need 
to store a large number edges because it is not 
known in advance how maw of the edges will be 
used until a parse is found. Tile second differ- 
ence is that we proceed strictly incrementally 
without look-ahead. (Chanfiak et al, 1998) 
use a non-incremental procedure, (Roark and 
Johnson, 1999) use a look-ahead of one word. 
Thirdly, we use a much simpler ranking tbnnula. 
Additionally, (Chanfiak et al, 1998) and 
(Roark and Johnson, 1999) do not use the 
original Penntree encoding tbr the context-fl'ee 
structures. Betbre training and parsing, they 
change/remove some of the productions and in- 
troduce new part-of-speech tags tbr auxiliaries. 
The exact effect of these modifications is un- 
known, and it is unclear if these affect compa- 
7For the active chart, lmralellism cannot be given for 
different nodes types since active edges are introduced 
fbr right-hand side prefixes, collapsing all possible left- 
hand sides. 
rability to our results. 
Tile heavy restrictions in our method (imme- 
diate pruning, no look-ahead, very simple rank- 
ing formula) have consequences on the accuracy. 
Using right context and sorting instead of prun- 
ing yields roughly 2% higher results (compared 
to our base encodingS). But our work shows 
that even with these massive restrictions, the 
chart size can be reduced to 1% without a de- 
crease in accuracy when compared to exhaustive 
search. 
7 Conclusions 
A central challenge in computational psycholin- 
guistics is to explaiu how it is that people are 
so accurate and robust in processing language. 
Given the substantial psycholinguistic evidence 
tbr statistical cognitive mechanisms, our objec- 
tive in this paper was to assess the plausibility 
of using wide-coverage probabilistic parsers to 
model lmman linguistic performance. In par- 
ticular, we set out to investigate the effects of 
imposing incremental processing and significant 
memory limitations on such parsers. 
The central finding of our experiments i that 
incremental parsing with massive (97% - 99%) 
pruning of the search space does not impair 
the accuracy of stochastic ontext-free parsers. 
This basic finding was rotmst across different 
settings of the beams and tbr the original Penn 
Treebank encoding as well as the parent encod- 
ing. We did however, observe significantly re- 
duced memory and time requirements when us- 
ing combined active/inactive dge filtering. To 
our knowledge, this is the first investigation on 
tree-bank grammars that systematically varies 
the beam tbr pruning. 
Our ainl in this paper is not to challenge 
state-of-the-art parsing accuracy results. For 
our experiments we used a purely context-ti'ee 
stochastic parser combined with a very sim- 
ple pruning scheme based on simple "unigram" 
probabilities, and no use of right context. We 
do, however suggest hat our result should ap- 
ply to richer, more sophistacted probabilistic 
SComparison of results is not straight-forward since 
(Roark and Johnson, 1999) report accuracies only tbr 
those sentences for which a parse tree was generated (be- 
tween 93 and 98% of the sentences), while our parser 
(except for very small Imams) generates parses for vir- 
tually all sentences, hence we report; accuracies for all 
sentences. 
116 
models, e.g. when adding word st~tistics to the 
model (Charni~d?, 1997). 
We thereibre conclude theft wide-covcr~ge, 
prol)~fl)ilistic pnrsers do not suffer impaired a('- 
curacy when subject to strict cognii;iv(~ meXnOl'y 
limitntions mM incremental processing. Fm'- 
thermore, parse times are sut)stm~ti~fily reduced. 
This sltggt',sts that it; m~y lie fruit;tiff to tlur,sllC 
the use of these models within ?',onlt)utational 
l)sycholinguistics, where it: is necessary to ex- 
plain not Olfly the relatively r~tr(; 'pathologies' of 
the hmmm parser, but also its mor(; fl'e(tuently 
ol)scrved ~u:(:ur~my ~(1 rol)llSiilless. 
References  
Thorst;en \]h'mfl;s. 1999. Cascadt;d Mm'kov mod- 
els. In P'rocecdings Vf 9th, Cm@~'t'.'m:e. of
the EuTvpea'n Chapter of the Association ,fro" 
Com.p'atatiou, al Linguistics EA 6'\])-99, B(;rg(;n, 
Norway. 
Eugene Charni~k, Sharon (\]ohlwater, and Mnrk 
,Johnson. 1998. ltMge-b~sed lmst-tirst (:hart 
pro'sing. In l~'rocec.dings of l, hc. Si:cl, h. l,Vor/,:- 
shop on l/cry LaT~\](: Corpora (WVLC-9S), 
Montreal, K~ma(la. 
Eugene Ch~rni~fl?. 1996. '15:ee-bank grmmm~rs. 
In P'rocecding,~ of t,h,c Th, irtec'nth, National 
Cm@rc'.nce on A'rt'ti/icial lntdlig(:,m:~:, l)a.g(,,s 
1031 1036, Menlo Pnrk: AAA\] Press/M1T 
l)i.ess. 
\]!htgen('. Chm:nia.k. 1997. Sl;a.i;isti(:al \]mrs- 
ing wit;h ~t context-fr(:(~ gl:;41111llVtl' 2.1~11(1 \voF(| 
statistics. In P~'occ.cdings qf the \],b,a'rt,(:enth 
National Co~@'r('.nce o'n A'rt~ificial Intelli- 
gence, pagc.s 1031 1036, Menlo Park: AAAI 
Press/MIT Press. 
Nicholas Chafer, Matthew Crock(;l', ~md Martin 
Pickcring. 1998. The rational analysis of in- 
quiry: The case. for parsign. In Charter and 
O~ksfor(1, editors, Ratio'hal Models o/" Cog'ni- 
tion. Oxford University Press. 
Michael Collins. 1!196. A new st~tistical l)arscr 
b~tse.d on l)igr~un lexical depend(;neies, in 
Proceedings of ACL-96, Sa, llta, Cruz, CA, 
USA. 
Matthew Crocker and Thorsten Br~mts. 1999. 
Incremental probabilisti(: lnodels of lmman 
linguistic perform;race. In The 5th Cm@r- 
cnce on Arc:hitcctu~v.s a'nd Mcch, anism.,s for 
La'nguagc Processing, Edi~flmrgh, U.K. 
Matthew Crocker and Thorst;en Brmfl;s. to ~t)- 
l)car. Wide cover~ge l)rol)~flfilistic sentence 
processing. Journal of Psych, oling'aistic Re- 
search,, November 2000. 
M~tthew Cro('kex mM Steil'an Corley. to ~l)- 
peru:. Modulm" nrchitectures and statisticM 
mcchnnisms: The case. frolli lexical category 
disnmbiguntion. In Merlo and Stevenson, ed- 
itors, The Lczical Basis of Sentence Process- 
in9. John Bcnjamins. 
1VI~tthew Crocker. 1999. Mech~misms for scn- 
|;ellce, tn'oeessing. In Garrod and Pieker- 
ing, editors, Language Proc~ssing. Psychology 
\])ross, London, UK. 
Mm'k ,Johnson. 1998. PCFG models of linguis- 
tic t;rec \]'el)rcse\]fl;~tions. Com, p,utational Lin- 
g'aistic.~, 24(4):613 632. 
\])~mi(;l .\]m:at~ky. \]996. A t)robabilistic n|o(M of 
lexi(:~tl nnd syntactic a(:(:ess and (lisambigua- 
tion. Cognitive Science, 20:137 194. 
Christot)her Mmming mid Him'ieh S(:lfiil;ze_ 
1999. l,b,a.ndatiou, s of Statistical Natural Lan- 
9'uag(: P'roct'.s,si'ng. MKI' Press, Cmnl)ridge, 
Mass~Lc:husetts. 
Mit(:hell IVlarmts, \]{eatrice S~mtorini, and 
Mary Ann M~rcinkiewicz. 1993. Building 
a lm:ge mmotated corl)us of English: The 
P(mn Treet)ank. Computational Linguistics, 
|!)(2):313 330. 
A(twait l/.~ttnat)~trkhi. 11!)!)7. A \]inem" ol)served 
time stnl;isI;ic;d t)m;ser based on m~tximmn en- 
tropy models. In \])'rocc.c:ding.s of the Co',:fcr- 
?:'m:c o'n Empirical Methods in Nat'a'ral La'n- 
g'uafle P'lvccssing \]'?MNLP-gZ Providence, 
11\]. 
\]h'inn Ronrk ~md Mm:k Johnson. 1!199. Efficient 
t)rol)al)ilisl, ic tot)-(lown ;rod left-(:orner pars- 
illg. hi \])'l'occcdi~,.(l,s of the. ,~7111, A~l, t,'ttal Mcc.t- 
i'ng of the A.ssociation for Cou~,p'atation Lin- 
g'aistic.~ A CL- 99, M~rybmd. 
117 
The LinGO Redwoods Treebank
Motivation and Preliminary Applications
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten Brants
{oe |kristina |manning |dan}@csli.stanford.edu,
shieber@deas.harvard.edu, brants@parc.xerox.com
Abstract
The LinGO Redwoods initiative is a seed activity in the de-
sign and development of a new type of treebank. While sev-
eral medium- to large-scale treebanks exist for English (and
for other major languages), pre-existing publicly available re-
sources exhibit the following limitations: (i) annotation is
mono-stratal, either encoding topological (phrase structure) or
tectogrammatical (dependency) information, (ii) the depth of
linguistic information recorded is comparatively shallow, (iii)
the design and format of linguistic representation in the tree-
bank hard-wires a small, predefined range of ways in which
information can be extracted from the treebank, and (iv) rep-
resentations in existing treebanks are static and over the (often
year- or decade-long) evolution of a large-scale treebank tend
to fall behind the development of the field. LinGO Redwoods
aims at the development of a novel treebanking methodology,
rich in nature and dynamic both in the ways linguistic data can
be retrieved from the treebank in varying granularity and in the
constant evolution and regular updating of the treebank itself.
Since October 2001, the project is working to build the foun-
dations for this new type of treebank, to develop a basic set of
tools for treebank construction and maintenance, and to con-
struct an initial set of 10,000 annotated trees to be distributed
together with the tools under an open-source license.
1 Why Another (Type of) Treebank?
For the past decade or more, symbolic, linguistically ori-
ented methods and statistical or machine learning ap-
proaches to NLP have often been perceived as incompat-
ible or even competing paradigms. While shallow and
probabilistic processing techniques have produced use-
ful results in many classes of applications, they have not
met the full range of needs for NLP, particularly where
precise interpretation is important, or where the variety
of linguistic expression is large relative to the amount
of training data available. On the other hand, deep
approaches to NLP have only recently achieved broad
enough grammatical coverage and sufficient processing
efficiency to allow the use of precise linguistic grammars
in certain types of real-world applications.
In particular, applications of broad-coverage analyti-
cal grammars for parsing or generation require the use of
sophisticated statistical techniques for resolving ambigu-
ities; the transfer of Head-Driven Phrase Structure Gram-
mar (HPSG) systems into industry, for example, has am-
plified the need for general parse ranking, disambigua-
tion, and robust recovery techniques. We observe general
consensus on the necessity for bridging activities, com-
bining symbolic and stochastic approaches to NLP. But
although we find promising research in stochastic pars-
ing in a number of frameworks, there is a lack of appro-
priately rich and dynamic language corpora for HPSG.
Likewise, stochastic parsing has so far been focussed on
information-extraction-type applications and lacks any
depth of semantic interpretation. The Redwoods initia-
tive is designed to fill in this gap.
In the next section, we present some of the motivation
for the LinGO Redwoods project as a treebank develop-
ment process. Although construction of the treebank is
in its early stages, we present in Section 3 some prelim-
inary results of using the treebank data already acquired
on concrete applications. We show, for instance, that
even simple statistical models of parse ranking trained
on the Redwoods corpus built so far can disambiguate
parses with close to 80% accuracy.
2 A Rich and Dynamic Treebank
The Redwoods treebank is based on open-source HPSG
resources developed by a broad consortium of re-
search groups including researchers at Stanford (USA),
Saarbru?cken (Germany), Cambridge, Edinburgh, and
Sussex (UK), and Tokyo (Japan). Their wide distribution
and common acceptance make the HPSG framework and
resources an excellent anchor point for the Redwoods
treebanking initiative.
The key innovative aspect of the Redwoods ap-
proach to treebanking is the anchoring of all linguis-
tic data captured in the treebank to the HPSG frame-
work and a generally-available broad-coverage gram-
mar of English, the LinGO English Resource Grammar
(Flickinger, 2000) as implemented with the LKB gram-
mar development environment (Copestake, 2002). Un-
like existing treebanks, there is no need to define a (new)
form of grammatical representation specific to the tree-
bank. Instead, the treebank records complete syntacto-
semantic analyses as defined by the LinGO ERG and pro-
vide tools to extract different types of linguistic informa-
tion at varying granularity.
The treebanking environment, building on the [incr
tsdb()] profiling environment (Oepen & Callmeier,
2000), presents annotators, one sentence at a time, with
the full set of analyses produced by the grammar. Using
a pre-existing tree comparison tool in the LKB (similar
in kind to the SRI Cambridge TreeBanker; Carter, 1997),
annotators can quickly navigate through the parse for-
est and identify the correct or preferred analysis in the
current context (or, in rare cases, reject all analyses pro-
posed by the grammar). The tree selection tool presents
users, who need little expert knowledge of the underly-
ing grammar, with a range of basic properties that distin-
guish competing analyses and that are relatively easy to
judge. All disambiguating decisions made by annotators
are recorded in the [incr tsdb()] database and thus become
available for (i) later dynamic extraction from the anno-
tated profile or (ii) dynamic propagation into a more re-
cent profile obtained from re-running a newer version of
the grammar on the same corpus.
Important innovative research aspects in this approach
to treebanking are (i) enabling users of the treebank to
extract information of the type they need and to trans-
form the available representation into a form suited to
their needs and (ii) the ability to update the treebank with
an enhanced version of the grammar in an automated
fashion, viz. by re-applying the disambiguating decisions
on the corpus with an updated version of the grammar.
Depth of Representation and Transformation of In-
formation Internally, the [incr tsdb()] database records
analyses in three different formats, viz. (i) as a deriva-
tion tree composed of identifiers of lexical items and con-
structions used to build the analysis, (ii) as a traditional
phrase structure tree labeled with an inventory of some
fifty atomic labels (of the type ?S?, ?NP?, ?VP? et al), and
(iii) as an underspecified MRS (Copestake, Lascarides,
& Flickinger, 2001) meaning representation. While rep-
resentation (ii) will in many cases be similar to the rep-
resentation found in the Penn Treebank, representation
(iii) subsumes the functor ? argument (or tectogrammati-
cal) structure advocated in the Prague Dependency Tree-
bank or the German TiGer corpus. Most importantly,
however, representation (i) provides all the information
required to replay the full HPSG analysis (using the orig-
inal grammar and one of the open-source HPSG process-
ing environments, e.g., the LKB or PET, which already
have been interfaced to [incr tsdb()]). Using the latter ap-
proach, users of the treebank are enabled to extract infor-
mation in whatever representation they require, simply
by reconstructing full analyses and adapting the exist-
ing mappings (e.g., the inventory of node labels used for
phrase structure trees) to their needs. Likewise, the ex-
isting [incr tsdb()] facilities for comparing across compe-
tence and performance profiles can be deployed to evalu-
ate results of a (stochastic) parse disambiguation system,
essentially using the preferences recorded in the treebank
as a ?gold standard? target for comparison.
Automating Treebank Construction Although a pre-
cise HPSG grammar like the LinGO ERG will typically
assign a small number of analyses to a given sentence,
choosing among a few or sometimes a few dozen read-
ings is time-consuming and error-prone. The project is
exploring two approaches to automating the disambigua-
tion task, (i) seeding lexical selection from a part-of-
speech (POS) tagger and (ii) automated inter-annotator
comparison and assisted resolution of conflicts.
Treebank Maintenance and Evolution One of the
challenging research aspects of the Redwoods initiative
is about developing a methodology for automated up-
dates of the treebank to reflect the continuous evolution
of the underlying linguistic framework and of the LinGO
grammar. Again building on the notion of elementary
linguistic discriminators, we expect to explore the semi-
automatic propagation of recorded disambiguating deci-
sions into newer versions of the parsed corpus. While
it can be assumed that the basic phrase structure inven-
tory and granularity of lexical distinctions have stabilized
to a certain degree, it is not guaranteed that one set of
discriminators will always fully disambiguate a more re-
cent set of analyses for the same utterance (as the gram-
mar may introduce new ambiguity), nor that re-playing
a history of disambiguating decisions will necessarily
identify the correct, preferred analysis for all sentences.
A better understanding of the nature of discriminators
and relations holding among them is expected to provide
the foundations for an update procedure that, ultimately,
should be mostly automated, with minimal manual in-
spection, and which can become part of the regular re-
gression test cycle for the grammar.
Scope and Current State of Seeding Initiative The
first 10,000 trees to be hand-annotated as part of the
kick-off initiative are taken from a domain for which the
English Resource Grammar is known to exhibit broad
and accurate coverage, viz. transcribed face-to-face dia-
logues in an appointment scheduling and travel arrange-
ment domain.1 For the follow-up phase of the project, it
is expected to move into a second domain and text genre,
presumably more formal, edited text taken from newspa-
per text or another widely available on-line source. As
of June 2002, the seeding initiative is well underway.
The integrated treebanking environment, combining [incr
tsdb()] and the LKB tree selection tool, has been estab-
lished and has been deployed in a first iteration of anno-
tating the VerbMobil utterances. The approach to parse
selection through minimal discriminators turned out to
be not hard to learn for a second-year Stanford under-
graduate in linguistics, and allowed completion of the
first iteration in less than ten weeks. Table 1 summarizes
the current Redwoods status.
1Corpora of some 50,000 such utterances are readily available from
the VerbMobil project (Wahlster, 2000) and have already been studied
extensively among researchers world-wide.
2Of the four data sets only VM32 has been double-checked by
an expert grammarian and (almost) completely disambiguated to date;
therefore it exhibits an interestingly higher degree of phrasal ambiguity
in the ?active = 1? subset.
total active = 0 active = 1 active > 1 unannotated
corpus ] ?  ? ] ?  ? ] ?  ? ] ?  ? ] ?  ?
VM6 2422 7?7 4?2 32?9 218 8?0 4?4 9?7 1910 7?0 4?0 7?5 80 10?0 4?8 23?8 214 14?9 4?3 287?5
VM13 1984 8?5 4?0 37?9 175 8?5 4?1 9?9 1491 7?2 3?9 7?5 85 9?9 4?5 22?1 233 14?1 4?2 22?1
VM31 1726 6?2 4?5 22?4 164 7?9 4?6 8?0 1360 6?6 4?5 5?9 61 10?1 4?2 14?5 141 13?5 4?7 201?5
VM32 608 7?4 4?3 25?6 51 10?7 4?3 54?4 551 7?9 4?4 19?0 5 12?2 3?9 27?2 1 21?0 6?1 2220?0
Table 1: Redwoods development status as of June 2002: four sets of transcribed and hand-segmented VerbMobil dialogues have
been annotated. The columns are, from left to right, the total number of sentences (excluding fragments) for which the LinGO
grammar has at least one analysis (?]?), average length (???), lexical and structural ambiguity (?? and ???, respectively), followed
by the last four metrics broken down for the following subsets: sentences (i) for which the annotator rejected all analyses (no active
trees), (ii) where annotation resulted in exactly one preferred analysis (one active tree), (iii) those where full disambiguation was
not accomplished through the first round of annotation (more than one active tree), and (iv) massively ambiguous sentences that
have yet to be annotated.2
3 Early Experimental Results
Development of the treebank has just started. Nonethe-
less, we have performed some preliminary experiments
on concrete applications to motivate the utility of the re-
source being developed. In this section, we describe ex-
periments using the Redwoods treebank to build and test
systems for parse disambiguation. As a component, we
build a tagger for the HPSG lexical tags in the treebank,
and report results on this application as well.
Any linguistic system that allows multiple parses
of strings must address the problem of selecting from
among the admitted parses the preferred one. A variety
of approaches for building statistical models of parse se-
lection are possible. At the simplest end, we might look
only at the lexical type sequence assigned to the words
by each parse and rank the parse based on the likelihood
of that sequence. These lexical types ? the preterminals
in the derivation ? are essentially part-of-speech tags, but
encode considerably finer-grained information about the
words. Well-understood statistical part-of-speech tag-
ging technology is sufficient for this approach.
In order to use more information about the parse,
we might examine the entire derivation of the string.
Most probabilistic parsing research ? including, for ex-
ample, work by by Collins (1997), and Charniak (1997)
? is based on branching process models (Harris, 1963).
The HPSG derivations that the treebank makes available
can be viewed as just such a branching process, and
a stochastic model of the trees can be built as a prob-
abilistic context-free grammar (PCFG) model. Abney
(1997) notes important problems with the soundness of
the approach when a unification-based grammar is ac-
tually determining the derivations, motivating the use
of log-linear models (Agresti, 1990) for parse ranking
that Johnson and colleagues further developed (Johnson,
Geman, Canon, Chi, & Riezler, 1999). These models
can deal with the many interacting dependencies and
the structural complexity found in constraint-based or
unification-based theories of syntax.
Nevertheless, the naive PCFG approach has the advan-
tage of simplicity, so we pursue it and the tagging ap-
proach to parse ranking in these proof-of-concept exper-
iments (more recently, we have begun work on building
log-linear models over HPSG signs (Toutanova & Man-
ning, 2002)). The learned models were used to rank
possible parses of unseen test sentences according to the
probabilities they assign to them. We report parse se-
lection performance as percentage of test sentences for
which the correct parse was highest ranked by the model.
(We restrict attention in the test corpus to sentences that
are ambiguous according to the grammar, that is, for
which the parse selection task is nontrivial.) We examine
four models: an HMM tagging model, a simple PCFG, a
PCFG with grandparent annotation, and a hybrid model
that combines predictions from the PCFG and the tagger.
These models will be described in more detail presently.
The tagger that we have implemented is a standard tri-
gram HMM tagger, defining a joint probability distribu-
tion over the preterminal sequences and yields of these
trees. Trigram probabilities are smoothed by linear in-
terpolation with lower-order models. For comparison,
we present the performance of a unigram tagger and an
upper-bound oracle tagger that knows the true tag se-
quence and scores highest the parses that have the correct
preterminal sequence.
The PCFG models define probability distributions
over the trees of derivational types corresponding to the
HPSG analyses of sentences. A PCFG model has parame-
ters ?i, j for each rule Ai ? ? j in the corresponding con-
text free grammar.3 In our application, the nonterminals
in the PCFG Ai are rules of the HPSG grammar used to
build the parses (such as HEAD-COMPL or HEAD-ADJ).
We set the parameters to maximize the likelihood of the
set of derivation trees for the preferred parses of the sen-
tences in a training set. As noted above, estimating prob-
abilities from local tree counts in the treebank does not
provide a maximum likelihood estimate of the observed
data, as the grammar rules further constrain the possible
derivations. Essentially, we are making an assumption of
context-freeness of rule application that does not hold in
the case of the HPSG grammar. Nonetheless, we can still
build the model and use it to rank parses.
3For an introduction to PCFG grammars see, for example, Manning
& Schu?tze (1999).
As previously noted by other researchers (Charniak &
Caroll, 1994), extending a PCFG with grandparent an-
notation improves the accuracy of the model. We imple-
mented an extended PCFG that conditions each node?s
expansion on its parent in the phrase structure tree. The
extended PCFG (henceforth PCFG-GP) has parameters
P(Ak Ai ? ? j |Ak, Ai) . The resulting grammar can be
viewed as a PCFG whose nonterminals are pairs of the
nonterminals of the original PCFG.
The combined model scores possible parses using
probabilities from the PCFG-GP model together with the
probability of the preterminal sequence of the parse tree
according to a trigram tag sequence model. More specif-
ically, for a tree T ,
Score(t) = log(PPCFG-GP(T )) + ? log(PTRIG(tags(T ))
where PTRIG(tags(T )) is the probability of the sequence
of preterminals t1 ? ? ? tn in T according to a trigram tag
model:
PTRIG(t1 ? ? ? tn) =
?n
i=1
P(ti |ti?1, ti?2)
with appropriate treatment of boundaries. The trigram
probabilities are smoothed as for the HMM tagger. The
combined model is relatively insensitive to the relative
weights of the two component models, as specified by ?;
in any case, exact optimization of this parameter was not
performed. We refer to this model as Combined. The
Combined model is not a sound probabilistic model as it
does not define a probability distribution over parse trees.
It does however provide a crude way to combine ancestor
and left context information.
The second column in Table 2 shows the accuracy
of parse selection using the models described above.
For comparison, a baseline showing the expected perfor-
mance of choosing parses randomly according to a uni-
form distribution is included as the first row. The accu-
racy results are averaged over a ten-fold cross-validation
on the data set summarized in Table 1. The data we used
for this experiment was the set of disambiguated sen-
tences that have exactly one preferred parse (comprising
a total of 5312 sentences). Often the stochastic models
we are considering give the same score to several differ-
ent parses. When a model ranks a set of m parses highest
with equal scores and one of those parses is the preferred
parse in the treebank, we compute the accuracy on this
sentence as 1/m.
Since our approach of defining the probability of anal-
yses using derivation trees is different from the tradi-
tional approach of learning PCFG grammars from phrase
structure trees, a comparison of the two is probably in
order. We tested the model PCFG-GP defined over the
corresponding phrase structure trees and its average ac-
curacy was 65.65% which is much lower than the accu-
racy of the same model over derivation trees (71.73%).
This result suggests that the information about grammar
constructions is very helpful for parse disambiguation.
Method Task
tag sel. parse sel.
Random 90.13% 25.81%
Tagger unigram 96.75% 44.15%
trigram 97.87% 47.74%
oracle 100.00% 54.59%
PCFG simple 97.40% 66.26%
grandparent 97.43% 71.73%
combined 98.08% 74.03%
Table 2: Performance of the HMM and PCFG models for the
tag and parse selection tasks (accuracy).
The results in Table 2 indicate that high disambigua-
tion accuracy can be achieved using very simple statisti-
cal models. The performance of the perfect tagger shows
that, informally speaking, roughly half of the information
necessary to disambiguate parses is available in the lexi-
cal types alone. About half of the remaining information
is recovered by our best method, Combined.
An alternative (more primitive) task is the tagging task
itself. It is interesting to know how much the tagging
task can be improved by perfecting parse disambigua-
tion. With the availability of a parser, we can examine the
accuracy of the tag sequence of the highest scoring parse,
rather than trying to tag the word sequence directly. We
refer to this problem as the tag selection problem, by
analogy with the relation between the parsing problem
and the parse selection problem. The first column of Ta-
ble 2 presents the performance of the models on the tag
selection problem. The results are averaged accuracies
over 10 cross-validation splits of the same corpus as the
previous experiment, and show that parse disambigua-
tion using information beyond the lexical type sequence
slightly improves tag selection performance. Note that
in these experiments, the models are used to rank the tag
sequences of the possible parses and not to find the most
probable tag sequence. Therefore tagging accuracy re-
sults are higher than they would be in the latter case.
Since our corpus has relatively short sentences and low
ambiguity it is interesting to see how much the perfor-
mance degrades as we move to longer and more highly
ambiguous sentences. For this purpose, we report in Ta-
ble 3 the parse ranking accuracy of the Combined model
as a function of the number of possible analyses for sen-
tences. Each row corresponds to a set of sentences with
number of possible analyses greater or equal to the bound
shown in the first column. For example, the first row con-
tains information for the sentences with ambiguity ? 2,
which is all ambiguous sentences. The columns show the
total number of sentences in the set, the expected accu-
racy of guessing at random, and the accuracy of the Com-
bined model. We can see that the parse ranking accuracy
is decreasing quickly and more powerful models will be
needed to achieve good accuracy for highly ambiguous
sentences.
Despite several differences in corpus size and compo-
Analyses Sentences Random Combined
? 2 3824 25.81% 74.03%
? 5 1789 9.66% 59.64%
? 10 1027 5.33% 51.61%
? 20 525 3.03% 45.33%
Table 3: Parse ranking accuracy by number of possible parses.
sition, it is perhaps nevertheless useful to compare this
work with other work on parse selection for unification-
based grammars. Johnson et al (1999) estimate a
Stochastic Unification Based Grammar (SUBG) using a
log-linear model. The features they include in the model
are not limited to production rule features but also ad-
junct and argument and other linguistically motivated
features. On a dataset of 540 sentences (total training
and test set) from a Verbmobil corpus they report parse
disambiguation accuracy of 58.7% given a baseline accu-
racy for choosing at random of 9.7%. The random base-
line is much lower than ours for the full data set, but it is
comparable for the random baseline for sentences with
more than 5 analyses. The accuracy of our Combined
model for these sentences is 59.64%, so the accuracies
of the two models seem fairly similar.
4 Related Work
To the best of our knowledge, no prior research has
been conducted exploring the linguistic depth, flexibil-
ity in available information, and dynamic nature of tree-
banks that we have proposed. Earlier work on building
corpora of hand-selected analyses relative to an exist-
ing broad-coverage grammar was carried out at Xerox
PARC, SRI Cambridge, and Microsoft Research. As all
these resources are tuned to proprietary grammars and
analysis engines, the resulting treebanks are not publicly
available, nor have reported research results been repro-
ducible. Yet, especially in light of the successful LinGO
open-source repository, it seems vital that both the tree-
bank and associated processing schemes and stochastic
models be available to the general (academic) public. An
on-going initiative at Rijksuniversiteit Groningen (NL) is
developing a treebank of dependency structures (Mullen,
Malouf, & Noord, 2001), derived from an HPSG-like
grammar of Dutch (Bouma, Noord, & Malouf, 2001).
The general approach resembles the Redwoods initiative
(specifically the discriminator-based method of tree se-
lection; the LKB tree comparison tool was originally de-
veloped by Malouf, after all), but it provides only a sin-
gle stratum of representation, and has no provision for
evolving analyses in tandem with the grammar. Dipper
(2000) presents the application of a broad-coverage LFG
grammar for German to constructing tectogrammatical
structures for the TiGer corpus. The approach is similar
to the Groningen framework, and shares its limitations.
References
Abney, S. P. (1997). Stochastic attribute-value grammars.
Computational Linguistics, 23, 597 ? 618.
Agresti, A. (1990). Categorical data analysis. John Wiley &
Sons.
Bouma, G., Noord, G. van, & Malouf, R. (2001).
Alpino. Wide-coverage computational analysis of Dutch. In
W. Daelemans, K. Sima-an, J. Veenstra, & J. Zavrel (Eds.),
Computational linguistics in the Netherlands (pp. 45 ? 59).
Amsterdam, The Netherlands: Rodopi.
Carter, D. (1997). The TreeBanker. A tool for supervised
training of parsed corpora. In Proceedings of the Workshop
on Computational Environments for Grammar Development
and Linguistic Engineering. Madrid, Spain.
Charniak, E. (1997). Statistical parsing with a context-free
grammar and word statistics. In Proceedings of the Four-
teenth National Conference on Artificial Intelligence (pp.
598 ? 603). Providence, RI.
Charniak, E., & Caroll, G. (1994). Context-sensitive statistics
for improved grammatical language models. In Proceedings
of the Twelth National Conference on Artificial Intelligence
(pp. 742 ? 747). Seattle, WA.
Collins, M. J. (1997). Three generative, lexicalised models for
statistical parsing. In Proceedings of the 35th Meeting of
the Association for Computational Linguistics and the 7th
Conference of the European Chapter of the ACL (pp. 16 ?
23). Madrid, Spain.
Copestake, A. (2002). Implementing typed feature structure
grammars. Stanford, CA: CSLI Publications.
Copestake, A., Lascarides, A., & Flickinger, D. (2001). An
algebra for semantic construction in constraint-based gram-
mars. In Proceedings of the 39th Meeting of the Association
for Computational Linguistics. Toulouse, France.
Dipper, S. (2000). Grammar-based corpus annotation. In
Workshop on linguistically interpreted corpora LINC-2000
(pp. 56 ? 64). Luxembourg.
Flickinger, D. (2000). On building a more efficient grammar
by exploiting types. Natural Language Engineering, 6 (1)
(Special Issue on Efficient Processing with HPSG), 15 ? 28.
Harris, T. E. (1963). The theory of branching processes.
Berlin, Germany: Springer.
Johnson, M., Geman, S., Canon, S., Chi, Z., & Riezler, S.
(1999). Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Meeting of the Associa-
tion for Computational Linguistics (pp. 535 ? 541). College
Park, MD.
Manning, C. D., & Schu?tze, H. (1999). Foundations of statis-
tical Natural Language Processing. Cambridge, MA: MIT
Press.
Mullen, T., Malouf, R., & Noord, G. van. (2001). Statistical
parsing of Dutch using Maximum Entropy models with fea-
ture merging. In Proceedings of the Natural Language Pro-
cessing Pacific Rim Symposium. Tokyo, Japan.
Oepen, S., & Callmeier, U. (2000). Measure for mea-
sure: Parser cross-fertilization. Towards increased compo-
nent comparability and exchange. In Proceedings of the 6th
International Workshop on Parsing Technologies (pp. 183 ?
194). Trento, Italy.
Toutanova, K., & Manning, C. D. (2002). Feature selection
for a rich HPSG grammar using decision trees. In Proceed-
ings of the sixth conference on natural language learning
(CoNLL-2002). Taipei.
Wahlster, W. (Ed.). (2000). Verbmobil. Foundations of speech-
to-speech translation. Berlin, Germany: Springer.
Story Link Detection and New Event Detection are Asymmetric
Francine Chen
PARC
3333 Coyote Hill Rd
Palo Alto, CA 94304
fchen@parc.com
Ayman Farahat
PARC
3333 Coyote Hill Rd
Palo Alto, CA 94304
farahat@parc.com
Thorsten Brants
PARC
3333 Coyote Hill Rd
Palo Alto, CA 94304
thorsten@brants.net
Abstract
Story link detection has been regarded as a
core technology for other Topic Detection and
Tracking tasks such as new event detection. In
this paper we analyze story link detection and
new event detection in a retrieval framework
and examine the effect of a number of tech-
niques, including part of speech tagging, new
similarity measures, and an expanded stop list,
on the performance of the two detection tasks.
We present experimental results that show that
the utility of the techniques on the two tasks
differs, as is consistent with our analysis.
1 Introduction
Topic Detection and Tracking (TDT) research is spon-
sored by the DARPA TIDES program. The research has
five tasks related to organizing streams of data such as
newswire and broadcast news (Wayne, 2000). A link
detection (LNK) system detects whether two stories are
?linked?, or discuss the same event. A story about a plane
crash and another story about the funeral of the crash vic-
tims are considered to be linked. In contrast, a story about
hurricane Andrew and a story about hurricane Agnes are
not linked because they are two different events. A new
event detection (NED) system detects when a story dis-
cusses a previously unseen event. Link detection is con-
sidered to be a core technology for new event detection
and the other tasks.
Several groups are performing research on the TDT
tasks of link detection and new event detection (e.g.,
(Carbonell et al, 2001) (Allan et al, 2000)). In this pa-
per, we compare the link detection and new event detec-
tion tasks in an information retrieval framework, examin-
ing the criteria for improving a NED system based on a
LNK system, and give specific directions for improving
each system separately. We also investigate the utility of
a number of techniques for improving the systems.
2 Common Processing and Models
The Link Detection and New Event Detection systems
that we developed for TDT2002 share many process-
ing steps in common. This includes preprocessing
to tokenize the data, recognize abbreviations, normal-
ize abbreviations, remove stop-words, replace spelled-
out numbers by digits, add part-of-speech tags, replace
the tokens by their stems, and then generating term-
frequency vectors. Document frequency counts are in-
crementally updated as new sources of stories are pre-
sented to the system. Additionally, separate source-
specific counts are used, so that, for example, the
term frequencies for the New York Times are com-
puted separately from stories from CNN. The source-
specific, incremental, document frequency counts are
used to compute a TF-IDF term vector for each story.
Stories are compared using either the cosine distance
 
	 ffMultiple Similarity Measures and Source-Pair Information
in Story Link Detection
Francine Chen Ayman Farahat
Palo Alto Research Center
3333 Coyote Hill Rd.
Palo Alto, CA 94304
 
fchen, farahat  @parc.com, thorsten@brants.net
Thorsten Brants
Abstract
State-of-the-art story link detection systems,
that is, systems that determine whether two sto-
ries are about the same event or linked, are usu-
ally based on the cosine-similarity measured
between two stories. This paper presents a
method for improving the performance of a link
detection system by using a variety of simi-
larity measures and using source-pair specific
statistical information. The utility of a num-
ber of different similarity measures, including
cosine, Hellinger, Tanimoto, and clarity, both
alone and in combination, was investigated.
We also compared several machine learning
techniques for combining the different types
of information. The techniques investigated
were SVMs, voting, and decision trees, each
of which makes use of similarity and statisti-
cal information differently. Our experimental
results indicate that the combination of similar-
ity measures and source-pair specific statistical
information using an SVM provides the largest
improvement in estimating whether two stories
are linked; the resulting system was the best-
performing link detection system at TDT-2002.
1 Introduction
Story link detection, as defined in the Topic Detection and
Tracking (TDT) competition sponsored by the DARPA
TIDES program, is the task of determining whether two
stories, such as news articles and/or radio broadcasts, are
about the same event, or linked. In TDT an event is de-
fined as ?something that happens at some specific time
and place? (TDT, 2002). For example, a story about a tor-
nado in Kansas in May and another story about a tornado
in Nebraska in June should not be classified as linked be-
cause they are about different events, although they both
fall under the same general ?topic? of natural disasters.
But a story about damage due to a tornado in Kansas and
a story about the clean-up and repairs due to the same tor-
nado in Kansas are considered linked events.
In the TDT link detection task, a link detection sys-
tem is given a sequence of time-ordered sets of stories,
where each set is from one news source. The system can
?look ahead? N source files from the current source file
being processed when deciding whether the current pair
is linked. Because the TDT link detection task is focused
on streams of news stories, one of the primary differences
between link detection and the more traditional IR catego-
rization task is that new events occur relatively frequently
and comparisons of interest are focused on events that are
not known in advance. One consequence of this is that the
best-performing systems usually adapt to new input. Link
detection is thought of as the basis for other event-based
topic analysis tasks, such as topic tracking, topic detec-
tion, and first-story detection (TDT, 2002).
2 Background and Related Work
The DARPA TDT story link detection task requires iden-
tifying pairs of linked stories. The original language of
the stories are in English, Mandarin and Arabic. The
sources include broadcast news and newswire. For the
required story link detection task, the research groups
tested their systems on a processed version of the data in
which the story boundaries have been manually identified,
the Arabic and Mandarin stories have been automatically
translated to English, and the broadcast news stories have
been converted to text by an automatic speech recognition
(ASR) system.
A number of research groups have developed story
link detection systems. The best current technology for
link detection relies on the use of cosine similarity be-
tween document terms vectors with TF-IDF term weight-
ing. In a TF-IDF model, the frequency of a term in a docu-
ment (TF) is weighted by the inverse document frequency
(IDF), the inverse of the number of documents containing
a term. UMass (Allan et al, 2000) has examined a num-
ber of similarity measures in the link detection task, in-
cluding weighted sum, language modeling and Kullback-
Leibler divergence, and found that the cosine similarity
produced the best results. More recently, in Lavrenko et
al. (2002), UMass found that the clarity similarity mea-
sure performed best for the link detection task. In this
paper, we also examine a number of similarity measures,
both separately, as in Allan et al (2000), and in combina-
tion. In the machine learning field, classifier combination
has been shown to provide accuracy gains (e.g., Belkin et
al.(1995); Kittler et al (1998); Brill and Wu (1998); Di-
etterich (2000)). Motivated by the performance improve-
ment observed in these studies, we explored the combina-
tion of similarity measures for improving Story Link De-
tection.
CMU hypothesized that the similarity between a pair
of stories is influenced by the source of each story. For
example, sources in a language that is translated to En-
glish will consistently use the same terminology, result-
ing in greater similarity between linked documents with
the same native language. In contrast, sources from radio
broadcasts may be transcribed much less consistently than
text sources due to recognition errors, so that the expected
similarity of a radio broadcast and a text source is less than
that of two text sources. They found that similarity thresh-
olds that were dependent on the type of the story-pair
sources (e.g., English/non-English language and broad-
cast news/newswire) improved story-link detection re-
sults by 15% (Carbonell et al, 2001). We also investigate
how to make use of differences in similarity that are de-
pendent on the types of sources composing a story pair.
We refer to the statistics characterizing story pairs with the
same source types as source-pair specific information. In
contrast to the source-specific thresholds used by CMU,
we normalize the similarity measures based on the source-
pair specific information, simultaneously with combining
different similarity measures.
Other researchers have successfully used machine
learning algorithms such as support vector machines
(SVM) (Cristianini and Shawe-Taylor, 2000; Joachims,
1998) and boosted decision stumps (Schapire and Singer,
2000) for text categorization. SVM-based systems, such
as that described in (Joachims, 1998), are typically among
the best performers for the categorization task. However,
attempts to directly apply SVMs to TDT tasks such as
tracking and link detection have not been successful; this
has been attributed in part to the lack of enough data for
training the SVM1. In these systems, the input was the
set of term vectors characterizing each document, similar
to the input used for the categorization task. In this pa-
1http://www.ldc.upenn.edu/Projects/TDT3/email/email 402.
html, accessed Mar 11, 2004.
per, we present a method for using SVMs to improve link
detection performance by combining heterogeneous in-
put features, composed of multiple similarity metrics and
statistical characterization of the story sources. We addi-
tionally examine the utility of the statistical information
by comparing against decision trees, where the statistical
characterization is not utilized. We also examine the util-
ity of the similarity values by comparing against voting,
where the classification based on each similarity measure
is combined.
3 System Description
To determine whether two documents are linked, state-of-
the-art link detection systems perform three primary pro-
cessing steps:
1. preprocessing to create a normalized set of terms
for representing each document as a vector of term
counts, or term vector
2. adapting model parameters (i.e., IDF) as new story
sets are introduced and computing the similarity of
the term vectors
3. determining whether a pair of stories are linked
based on the similarity score.
In this paper, we describe our investigations in improv-
ing the basic story link detection systems by using source
specific information and combining a number of similar-
ity measures. As in the basic story link detection system, a
similarity score between two stories is computed. In con-
trast to the basic story link detection system, a variety of
similarity measures is computed and the prediction mod-
els use source-pair-specific statistics (i.e., median, aver-
age, and variance of the story pair similarity scores). We
do this in a post-processing step using machine learning
classifiers (i.e., SVMs, decision trees, or voting) to pro-
duce a decision with an associated confidence score as to
whether a pair of stories are linked. Source-pair-specific
statistics and multiple similarity measures are used as in-
put features to the machine learning based techniques in
post-processing the similarity scores. In the next sections,
we describe the components and processing performed by
our system.
3.1 Preprocessing
For preprocessing, we tokenize the data, remove stop-
words, replace spelled-out numbers by digits, replace the
tokens by their stems using the Inxight LinguistX mor-
phological analyzer, and then generate a term-frequency
vector to represent each story. For text where the original
source is Mandarin, some of the terms are untranslated.
In our experiments, we retain these terms because many
are content words. Both the training data and test data are
preprocessed in the same way.
3.1.1 Stop Words
Our base stoplist is composed of 577 terms. We extend
the stoplist with terms that are represented differently by
ASR systems and text documents. For example, in the
broadcast news documents in the TDT collection ?30? is
spelled out as ?thirty? and ?CNN? is represented as three
separate tokens ?C?, ?N?, and ?N?. To handle these differ-
ences, an ?ASR stoplist? was automatically created. Chen
et al (2003) found that the use of an enhanced stoplist,
formed from the union of a base stoplist and ASR stoplist,
was very effective in improving performance and empir-
ically better than normalizing ASR abbreviations.
3.1.2 Source-specific Incremental TF-IDF Model
The training data is used to compute the initial docu-
ment frequency over the corpus for each term. The docu-
ment frequency of term   ,   is defined to be:
 
	
 

	



 Optimizing Story Link Detection is not Equivalent to
Optimizing New Event Detection
Ayman Farahat
PARC
3333 Coyote Hill Rd
Palo Alto, CA 94304
farahat@parc.com
Francine Chen
PARC
3333 Coyote Hill Rd
Palo Alto, CA 94304
fchen@parc.com
Thorsten Brants
PARC
3333 Coyote Hill Rd
Palo Alto, CA 94304
thorsten@brants.net
Abstract
Link detection has been regarded as a core
technology for the Topic Detection and
Tracking tasks of new event detection. In
this paper we formulate story link detec-
tion and new event detection as informa-
tion retrieval task and hypothesize on the
impact of precision and recall on both sys-
tems. Motivated by these arguments, we
introduce a number of new performance
enhancing techniques including part of
speech tagging, new similarity measures
and expanded stop lists. Experimental re-
sults validate our hypothesis.
1 Introduction
Topic Detection and Tracking (TDT) research is
sponsored by the DARPA Translingual Information
Detection, Extraction, and Summarization (TIDES)
program. The research has five tasks related to
organizing streams of data such as newswire and
broadcast news (Wayne, 2000): story segmentation,
topic tracking, topic detection, new event detection
(NED), and link detection (LNK). A link detection
system detects whether two stories are ?linked?, or
discuss the same event. A story about a plane crash
and another story about the funeral of the crash vic-
tims are considered to be linked. In contrast, a story
about hurricane Andrew and a story about hurricane
Agnes are not linked because they are two different
events. A new event detection system detects when
a story discusses a previously unseen or ?not linked?
event. Link detection is considered to be a core tech-
nology for new event detection and the other tasks.
Several groups are performing research in the
TDT tasks of link detection and new event detection.
Based on their findings, we incorporated a number
of their ideas into our baseline system. CMU (Yang
et al, 1998) and UMass (Allan et al, 2000a) found
that for new event detection it was better to com-
pare a new story against all previously seen stories
than to cluster previously seen stories and compare
a new story against the clusters. CMU (Carbonell
et al, 2001) found that NED results could be im-
proved by developing separate models for different
news sources to that could capture idiosyncrasies of
different sources, which we also extended to link de-
tection. UMass reported on adapting a tracking sys-
tem for NED detection (Allan et al, 2000b). Allan
et. al , (Allan et al, 2000b) developed a NED system
based upon a tracking technology and showed that
to achieve high-quality first story detection, tracking
effectiveness must improve to a degree that experi-
ence suggests is unlikely. In this paper, while we
reach a similar conclusion as (Allan et al, 2000b) for
LNK and NED systems , we give specific directions
for improving each system separately. We compare
the link detection and new event detection tasks and
discuss ways in which we have observed that tech-
niques developed for one task do not always perform
similarly for the other task.
2 Common Processing and Models
This section describes those parts of the process-
ing steps and the models that are the same for New
Event Detection and for Link Detection.
2.1 Pre-Processing
For pre-processing, we tokenize the data, recog-
nize abbreviations, normalize abbreviations, remove
stop-words, replace spelled-out numbers by digits,
add part-of-speech tags, replace the tokens by their
stems, and then generate term-frequency vectors.
2.2 Incremental TF-IDF Model
Our similarity calculations of documents are based
on an incremental TF-IDF model. In a TF-IDF
model, the frequency of a term in a document (TF) is
weighted by the inverse document frequency (IDF).
In the incremental model, document frequencies
 
are not static but change in time steps 	 . At
time 	 , a new set of test documents 
 is added to
the model by updating the frequencies
 

 

 

 (1)
where
 

denote the document frequencies in the
newly added set of documents 
 . The initial docu-
ment frequencies
 ffProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 858?867, Prague, June 2007. c?2007 Association for Computational Linguistics
Large Language Models in Machine Translation
Thorsten Brants Ashok C. Popat Peng Xu Franz J. Och Jeffrey Dean
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
{brants,popat,xp,och,jeff}@google.com
Abstract
This paper reports on the benefits of large-
scale statistical language modeling in ma-
chine translation. A distributed infrastruc-
ture is proposed which we use to train on
up to 2 trillion tokens, resulting in language
models having up to 300 billion n-grams. It
is capable of providing smoothed probabil-
ities for fast, single-pass decoding. We in-
troduce a new smoothing method, dubbed
Stupid Backoff, that is inexpensive to train
on large data sets and approaches the quality
of Kneser-Ney Smoothing as the amount of
training data increases.
1 Introduction
Given a source-language (e.g., French) sentence f ,
the problem of machine translation is to automati-
cally produce a target-language (e.g., English) trans-
lation e?. The mathematics of the problem were for-
malized by (Brown et al, 1993), and re-formulated
by (Och and Ney, 2004) in terms of the optimization
e? = arg max
e
M
?
m=1
?mhm(e, f) (1)
where {hm(e, f)} is a set of M feature functions and
{?m} a set of weights. One or more feature func-
tions may be of the form h(e, f) = h(e), in which
case it is referred to as a language model.
We focus on n-gram language models, which are
trained on unlabeled monolingual text. As a general
rule, more data tends to yield better language mod-
els. Questions that arise in this context include: (1)
How might one build a language model that allows
scaling to very large amounts of training data? (2)
How much does translation performance improve as
the size of the language model increases? (3) Is there
a point of diminishing returns in performance as a
function of language model size?
This paper proposes one possible answer to the
first question, explores the second by providing
learning curves in the context of a particular statis-
tical machine translation system, and hints that the
third may yet be some time in answering. In particu-
lar, it proposes a distributed language model training
and deployment infrastructure, which allows direct
and efficient integration into the hypothesis-search
algorithm rather than a follow-on re-scoring phase.
While it is generally recognized that two-pass de-
coding can be very effective in practice, single-pass
decoding remains conceptually attractive because it
eliminates a source of potential information loss.
2 N -gram Language Models
Traditionally, statistical language models have been
designed to assign probabilities to strings of words
(or tokens, which may include punctuation, etc.).
Let wL1 = (w1, . . . , wL) denote a string of L tokens
over a fixed vocabulary. An n-gram language model
assigns a probability to wL1 according to
P (wL1 ) =
L
?
i=1
P (wi|wi?11 ) ?
L
?
i=1
P? (wi|wi?1i?n+1)
(2)
where the approximation reflects a Markov assump-
tion that only the most recent n ? 1 tokens are rele-
vant when predicting the next word.
858
For any substring wji of wL1 , let f(w
j
i ) denote the
frequency of occurrence of that substring in another
given, fixed, usually very long target-language string
called the training data. The maximum-likelihood
(ML) probability estimates for the n-grams are given
by their relative frequencies
r(wi|wi?1i?n+1) =
f(wii?n+1)
f(wi?1i?n+1)
. (3)
While intuitively appealing, Eq. (3) is problematic
because the denominator and / or numerator might
be zero, leading to inaccurate or undefined probabil-
ity estimates. This is termed the sparse data prob-
lem. For this reason, the ML estimate must be mod-
ified for use in practice; see (Goodman, 2001) for a
discussion of n-gram models and smoothing.
In principle, the predictive accuracy of the lan-
guage model can be improved by increasing the or-
der of the n-gram. However, doing so further exac-
erbates the sparse data problem. The present work
addresses the challenges of processing an amount
of training data sufficient for higher-order n-gram
models and of storing and managing the resulting
values for efficient use by the decoder.
3 Related Work on Distributed Language
Models
The topic of large, distributed language models is
relatively new. Recently a two-pass approach has
been proposed (Zhang et al, 2006), wherein a lower-
order n-gram is used in a hypothesis-generation
phase, then later the K-best of these hypotheses are
re-scored using a large-scale distributed language
model. The resulting translation performance was
shown to improve appreciably over the hypothesis
deemed best by the first-stage system. The amount
of data used was 3 billion words.
More recently, a large-scale distributed language
model has been proposed in the contexts of speech
recognition and machine translation (Emami et al,
2007). The underlying architecture is similar to
(Zhang et al, 2006). The difference is that they in-
tegrate the distributed language model into their ma-
chine translation decoder. However, they don?t re-
port details of the integration or the efficiency of the
approach. The largest amount of data used in the
experiments is 4 billion words.
Both approaches differ from ours in that they store
corpora in suffix arrays, one sub-corpus per worker,
and serve raw counts. This implies that all work-
ers need to be contacted for each n-gram request.
In our approach, smoothed probabilities are stored
and served, resulting in exactly one worker being
contacted per n-gram for simple smoothing tech-
niques, and in exactly two workers for smoothing
techniques that require context-dependent backoff.
Furthermore, suffix arrays require on the order of 8
bytes per token. Directly storing 5-grams is more
efficient (see Section 7.2) and allows applying count
cutoffs, further reducing the size of the model.
4 Stupid Backoff
State-of-the-art smoothing uses variations of con-
text-dependent backoff with the following scheme:
P (wi|wi?1i?k+1) =
{
?(wii?k+1) if (wii?k+1) is found
?(wi?1i?k+1)P (wii?k+2) otherwise
(4)
where ?(?) are pre-computed and stored probabili-
ties, and ?(?) are back-off weights. As examples,
Kneser-Ney Smoothing (Kneser and Ney, 1995),
Katz Backoff (Katz, 1987) and linear interpola-
tion (Jelinek and Mercer, 1980) can be expressed in
this scheme (Chen and Goodman, 1998). The recur-
sion ends at either unigrams or at the uniform distri-
bution for zero-grams.
We introduce a similar but simpler scheme,
named Stupid Backoff 1 , that does not generate nor-
malized probabilities. The main difference is that
we don?t apply any discounting and instead directly
use the relative frequencies (S is used instead of
P to emphasize that these are not probabilities but
scores):
S(wi|wi?1i?k+1) =
?
?
?
?
?
f(wii?k+1)
f(wi?1i?k+1)
if f(wii?k+1) > 0
?S(wi|wi?1i?k+2) otherwise
(5)
1The name originated at a time when we thought that such
a simple scheme cannot possibly be good. Our view of the
scheme changed, but the name stuck.
859
In general, the backoff factor ? may be made to de-
pend on k. Here, a single value is used and heuris-
tically set to ? = 0.4 in all our experiments2 . The
recursion ends at unigrams:
S(wi) =
f(wi)
N (6)
with N being the size of the training corpus.
Stupid Backoff is inexpensive to calculate in a dis-
tributed environment while approaching the quality
of Kneser-Ney smoothing for large amounts of data.
The lack of normalization in Eq. (5) does not affect
the functioning of the language model in the present
setting, as Eq. (1) depends on relative rather than ab-
solute feature-function values.
5 Distributed Training
We use the MapReduce programming model (Dean
and Ghemawat, 2004) to train on terabytes of data
and to generate terabytes of language models. In this
programming model, a user-specified map function
processes an input key/value pair to generate a set of
intermediate key/value pairs, and a reduce function
aggregates all intermediate values associated with
the same key. Typically, multiple map tasks oper-
ate independently on different machines and on dif-
ferent parts of the input data. Similarly, multiple re-
duce tasks operate independently on a fraction of the
intermediate data, which is partitioned according to
the intermediate keys to ensure that the same reducer
sees all values for a given key. For additional details,
such as communication among machines, data struc-
tures and application examples, the reader is referred
to (Dean and Ghemawat, 2004).
Our system generates language models in three
main steps, as described in the following sections.
5.1 Vocabulary Generation
Vocabulary generation determines a mapping of
terms to integer IDs, so n-grams can be stored us-
ing IDs. This allows better compression than the
original terms. We assign IDs according to term fre-
quency, with frequent terms receiving small IDs for
efficient variable-length encoding. All words that
2The value of 0.4 was chosen empirically based on good
results in earlier experiments. Using multiple values depending
on the n-gram order slightly improves results.
occur less often than a pre-determined threshold are
mapped to a special id marking the unknown word.
The vocabulary generation map function reads
training text as input. Keys are irrelevant; values are
text. It emits intermediate data where keys are terms
and values are their counts in the current section
of the text. A sharding function determines which
shard (chunk of data in the MapReduce framework)
the pair is sent to. This ensures that all pairs with
the same key are sent to the same shard. The re-
duce function receives all pairs that share the same
key and sums up the counts. Simplified, the map,
sharding and reduce functions do the following:
Map(string key, string value) {
// key=docid, ignored; value=document
array words = Tokenize(value);
hash_map<string, int> histo;
for i = 1 .. #words
histo[words[i]]++;
for iter in histo
Emit(iter.first, iter.second);
}
int ShardForKey(string key, int nshards) {
return Hash(key) % nshards;
}
Reduce(string key, iterator values) {
// key=term; values=counts
int sum = 0;
for each v in values
sum += ParseInt(v);
Emit(AsString(sum));
}
Note that the Reduce function emits only the aggre-
gated value. The output key is the same as the inter-
mediate key and automatically written by MapRe-
duce. The computation of counts in the map func-
tion is a minor optimization over the alternative of
simply emitting a count of one for each tokenized
word in the array. Figure 1 shows an example for
3 input documents and 2 reduce shards. Which re-
ducer a particular term is sent to is determined by a
hash function, indicated by text color. The exact par-
titioning of the keys is irrelevant; important is that all
pairs with the same key are sent to the same reducer.
5.2 Generation of n-Grams
The process of n-gram generation is similar to vo-
cabulary generation. The main differences are that
now words are converted to IDs, and we emit n-
grams up to some maximum order instead of single
860
Figure 1: Distributed vocabulary generation.
words. A simplified map function does the follow-
ing:
Map(string key, string value) {
// key=docid, ignored; value=document
array ids = ToIds(Tokenize(value));
for i = 1 .. #ids
for j = 0 .. maxorder-1
Emit(ids[i-j .. i], "1");
}
Again, one may optimize the Map function by first
aggregating counts over some section of the data and
then emit the aggregated counts instead of emitting
?1? each time an n-gram is encountered.
The reduce function is the same as for vocabu-
lary generation. The subsequent step of language
model generation will calculate relative frequencies
r(wi|wi?1i?k+1) (see Eq. 3). In order to make that step
efficient we use a sharding function that places the
values needed for the numerator and denominator
into the same shard.
Computing a hash function on just the first words
of n-grams achieves this goal. The required n-
grams wii?n+1 and wi?1i?n+1 always share the same
first word wi?n+1, except for unigrams. For that we
need to communicate the total count N to all shards.
Unfortunately, sharding based on the first word
only may make the shards very imbalanced. Some
terms can be found at the beginning of a huge num-
ber of n-grams, e.g. stopwords, some punctuation
marks, or the beginning-of-sentence marker. As an
example, the shard receiving n-grams starting with
the beginning-of-sentence marker tends to be several
times the average size. Making the shards evenly
sized is desirable because the total runtime of the
process is determined by the largest shard.
The shards are made more balanced by hashing
based on the first two words:
int ShardForKey(string key, int nshards) {
string prefix = FirstTwoWords(key);
return Hash(prefix) % nshards;
}
This requires redundantly storing unigram counts in
all shards in order to be able to calculate relative fre-
quencies within shards. That is a relatively small
amount of information (a few million entries, com-
pared to up to hundreds of billions of n-grams).
5.3 Language Model Generation
The input to the language model generation step is
the output of the n-gram generation step: n-grams
and their counts. All information necessary to calcu-
late relative frequencies is available within individ-
ual shards because of the sharding function. That is
everything we need to generate models with Stupid
Backoff. More complex smoothing methods require
additional steps (see below).
Backoff operations are needed when the full n-
gram is not found. If r(wi|wi?1i?n+1) is not found,
then we will successively look for r(wi|wi?1i?n+2),
r(wi|wi?1i?n+3), etc. The language model generation
step shards n-grams on their last two words (with
unigrams duplicated), so all backoff operations can
be done within the same shard (note that the required
n-grams all share the same last word wi).
5.4 Other Smoothing Methods
State-of-the-art techniques like Kneser-Ney
Smoothing or Katz Backoff require additional,
more expensive steps. At runtime, the client needs
to additionally request up to 4 backoff factors for
each 5-gram requested from the servers, thereby
multiplying network traffic. We are not aware of
a method that always stores the history backoff
factors on the same shard as the longer n-gram
without duplicating a large fraction of the entries.
This means one needs to contact two shards per
n-gram instead of just one for Stupid Backoff.
Training requires additional iterations over the data.
861
Step 0 Step 1 Step 2
context counting unsmoothed probs and interpol. weights interpolated probabilities
Input key wii?n+1 (same as Step 0 output) (same as Step 1 output)
Input value f(wii?n+1) (same as Step 0 output) (same as Step 1 output)
Intermediate key wii?n+1 wi?1i?n+1 wi?n+1i
Sharding wii?n+1 wi?1i?n+1 w
i?n+2
i?n+1 , unigrams duplicated
Intermediate value fKN (wii?n+1) wi,fKN (wii?n+1)
fKN (wii?n+1)?D
fKN (wi?1i?n+1)
,?(wi?1i?n+1)
Output value fKN (wii?n+1) wi,
fKN (wii?n+1)?D
fKN (wi?1i?n+1)
,?(wi?1i?n+1) PKN (wi|wi?1i?n+1), ?(wi?1i?n+1)
Table 1: Extra steps needed for training Interpolated Kneser-Ney Smoothing
Kneser-Ney Smoothing counts lower-order n-
grams differently. Instead of the frequency of the
(n? 1)-gram, it uses the number of unique single
word contexts the (n?1)-gram appears in. We use
fKN(?) to jointly denote original frequencies for the
highest order and context counts for lower orders.
After the n-gram counting step, we process the n-
grams again to produce these quantities. This can
be done similarly to the n-gram counting using a
MapReduce (Step 0 in Table 1).
The most commonly used variant of Kneser-Ney
smoothing is interpolated Kneser-Ney smoothing,
defined recursively as (Chen and Goodman, 1998):
PKN (wi|wi?1i?n+1) =
max(fKN(wii?n+1) ? D, 0)
fKN(wi?1i?n+1)
+ ?(wi?1i?n+1)PKN (wi|wi?1i?n+2),
where D is a discount constant and {?(wi?1i?n+1)} are
interpolation weights that ensure probabilities sum
to one. Two additional major MapReduces are re-
quired to compute these values efficiently. Table 1
describes their input, intermediate and output keys
and values. Note that output keys are always the
same as intermediate keys.
The map function of MapReduce 1 emits n-gram
histories as intermediate keys, so the reduce func-
tion gets all n-grams with the same history at the
same time, generating unsmoothed probabilities and
interpolation weights. MapReduce 2 computes the
interpolation. Its map function emits reversed n-
grams as intermediate keys (hence we use wi?n+1i
in the table). All unigrams are duplicated in ev-
ery reduce shard. Because the reducer function re-
ceives intermediate keys in sorted order it can com-
pute smoothed probabilities for all n-gram orders
with simple book-keeping.
Katz Backoff requires similar additional steps.
The largest models reported here with Kneser-Ney
Smoothing were trained on 31 billion tokens. For
Stupid Backoff, we were able to use more than 60
times of that amount.
6 Distributed Application
Our goal is to use distributed language models in-
tegrated into the first pass of a decoder. This may
yield better results than n-best list or lattice rescor-
ing (Ney and Ortmanns, 1999). Doing that for lan-
guage models that reside in the same machine as the
decoder is straight-forward. The decoder accesses
n-grams whenever necessary. This is inefficient in a
distributed system because network latency causes a
constant overhead on the order of milliseconds. On-
board memory is around 10,000 times faster.
We therefore implemented a new decoder archi-
tecture. The decoder first queues some number of
requests, e.g. 1,000 or 10,000 n-grams, and then
sends them together to the servers, thereby exploit-
ing the fact that network requests with large numbers
of n-grams take roughly the same time to complete
as requests with single n-grams.
The n-best search of our machine translation de-
coder proceeds as follows. It maintains a graph of
the search space up to some point. It then extends
each hypothesis by advancing one word position in
the source language, resulting in a candidate exten-
sion of the hypothesis of zero, one, or more addi-
tional target-language words (accounting for the fact
that variable-length source-language fragments can
correspond to variable-length target-language frag-
ments). In a traditional setting with a local language
model, the decoder immediately obtains the nec-
essary probabilities and then (together with scores
862
Figure 2: Illustration of decoder graph and batch-
querying of the language model.
from other features) decides which hypotheses to
keep in the search graph. When using a distributed
language model, the decoder first tentatively extends
all current hypotheses, taking note of which n-grams
are required to score them. These are queued up for
transmission as a batch request. When the scores are
returned, the decoder re-visits all of these tentative
hypotheses, assigns scores, and re-prunes the search
graph. It is then ready for the next round of exten-
sions, again involving queuing the n-grams, waiting
for the servers, and pruning.
The process is illustrated in Figure 2 assuming a
trigram model and a decoder policy of pruning to
the four most promising hypotheses. The four ac-
tive hypotheses (indicated by black disks) at time t
are: There is, There may, There are, and There were.
The decoder extends these to form eight new nodes
at time t + 1. Note that one of the arcs is labeled ,
indicating that no target-language word was gener-
ated when the source-language word was consumed.
The n-grams necessary to score these eight hypothe-
ses are There is lots, There is many, There may be,
There are lots, are lots of, etc. These are queued up
and their language-model scores requested in a batch
manner. After scoring, the decoder prunes this set as
indicated by the four black disks at time t + 1, then
extends these to form five new nodes (one is shared)
at time t + 2. The n-grams necessary to score these
hypotheses are lots of people, lots of reasons, There
are onlookers, etc. Again, these are sent to the server
together, and again after scoring the graph is pruned
to four active (most promising) hypotheses.
The alternating processes of queuing, waiting and
scoring/pruning are done once per word position in
a source sentence. The average sentence length in
our test data is 22 words (see section 7.1), thus we
have 23 rounds3 per sentence on average. The num-
ber of n-grams requested per sentence depends on
the decoder settings for beam size, re-ordering win-
dow, etc. As an example for larger runs reported in
the experiments section, we typically request around
150,000 n-grams per sentence. The average net-
work latency per batch is 35 milliseconds, yield-
ing a total latency of 0.8 seconds caused by the dis-
tributed language model for an average sentence of
22 words. If a slight reduction in translation qual-
ity is allowed, then the average network latency per
batch can be brought down to 7 milliseconds by re-
ducing the number of n-grams requested per sen-
tence to around 10,000. As a result, our system can
efficiently use the large distributed language model
at decoding time. There is no need for a second pass
nor for n-best list rescoring.
We focused on machine translation when describ-
ing the queued language model access. However,
it is general enough that it may also be applicable
to speech decoders and optical character recognition
systems.
7 Experiments
We trained 5-gram language models on amounts of
text varying from 13 million to 2 trillion tokens.
The data is divided into four sets; language mod-
els are trained for each set separately4 . For each
training data size, we report the size of the result-
ing language model, the fraction of 5-grams from
the test data that is present in the language model,
and the BLEU score (Papineni et al, 2002) obtained
by the machine translation system. For smaller train-
ing sizes, we have also computed test-set perplexity
using Kneser-Ney Smoothing, and report it for com-
parison.
7.1 Data Sets
We compiled four language model training data sets,
listed in order of increasing size:
3One additional round for the sentence end marker.
4Experience has shown that using multiple, separately
trained language models as feature functions in Eq (1) yields
better results than using a single model trained on all data.
863
 1e+07
 1e+08
 1e+09
 1e+10
 1e+11
 1e+12
 10  100  1000  10000  100000  1e+06
 0.1
 1
 10
 100
 1000
N
um
be
r o
f n
-g
ra
m
s
Ap
pr
ox
. L
M
 s
ize
 in
 G
B
LM training data size in million tokens
x1.8/x2
x1.8/x2
x1.8/x2
x1.6/x2
target
+ldcnews
+webnews
+web
Figure 3: Number of n-grams (sum of unigrams to
5-grams) for varying amounts of training data.
target: The English side of Arabic-English parallel
data provided by LDC5 (237 million tokens).
ldcnews: This is a concatenation of several English
news data sets provided by LDC6 (5 billion tokens).
webnews: Data collected over several years, up to
December 2005, from web pages containing pre-
dominantly English news articles (31 billion to-
kens).
web: General web data, which was collected in Jan-
uary 2006 (2 trillion tokens).
For testing we use the ?NIST? part of the 2006
Arabic-English NIST MT evaluation set, which is
not included in the training data listed above7. It
consists of 1797 sentences of newswire, broadcast
news and newsgroup texts with 4 reference transla-
tions each. The test set is used to calculate transla-
tion BLEU scores. The English side of the set is also
used to calculate perplexities and n-gram coverage.
7.2 Size of the Language Models
We measure the size of language models in total
number of n-grams, summed over all orders from
1 to 5. There is no frequency cutoff on the n-grams.
5http://www.nist.gov/speech/tests/mt/doc/
LDCLicense-mt06.pdf contains a list of parallel resources
provided by LDC.
6The bigger sets included are LDC2005T12 (Gigaword,
2.5B tokens), LDC93T3A (Tipster, 500M tokens) and
LDC2002T31 (Acquaint, 400M tokens), plus many smaller
sets.
7The test data was generated after 1-Feb-2006; all training
data was generated before that date.
target webnews web
# tokens 237M 31G 1.8T
vocab size 200k 5M 16M
# n-grams 257M 21G 300G
LM size (SB) 2G 89G 1.8T
time (SB) 20 min 8 hours 1 day
time (KN) 2.5 hours 2 days ?
# machines 100 400 1500
Table 2: Sizes and approximate training times for
3 language models with Stupid Backoff (SB) and
Kneser-Ney Smoothing (KN).
There is, however, a frequency cutoff on the vocab-
ulary. The minimum frequency for a term to be in-
cluded in the vocabulary is 2 for the target, ldcnews
and webnews data sets, and 200 for the web data set.
All terms below the threshold are mapped to a spe-
cial term UNK, representing the unknown word.
Figure 3 shows the number of n-grams for lan-
guage models trained on 13 million to 2 trillion to-
kens. Both axes are on a logarithmic scale. The
right scale shows the approximate size of the served
language models in gigabytes. The numbers above
the lines indicate the relative increase in language
model size: x1.8/x2 means that the number of n-
grams grows by a factor of 1.8 each time we double
the amount of training data. The values are simi-
lar across all data sets and data sizes, ranging from
1.6 to 1.8. The plots are very close to straight lines
in the log/log space; linear least-squares regression
finds r2 > 0.99 for all four data sets.
The web data set has the smallest relative increase.
This can be at least partially explained by the higher
vocabulary cutoff. The largest language model gen-
erated contains approx. 300 billion n-grams.
Table 2 shows sizes and approximate training
times when training on the full target, webnews, and
web data sets. The processes run on standard current
hardware with the Linux operating system. Gen-
erating models with Kneser-Ney Smoothing takes
6 ? 7 times longer than generating models with
Stupid Backoff. We deemed generation of Kneser-
Ney models on the web data as too expensive and
therefore excluded it from our experiments. The es-
timated runtime for that is approximately one week
on 1500 machines.
864
 50
 100
 150
 200
 250
 300
 350
 10  100  1000  10000  100000  1e+06
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
Pe
rp
le
xit
y
Fr
ac
tio
n 
of
 c
ov
er
ed
 5
-g
ra
m
s
LM training data size in million tokens
+.022/x2
+.035/x2
+.038/x2
+.026/x2
target KN PP
ldcnews KN PP
webnews KN PP
target C5
+ldcnews C5
+webnews C5
+web C5
Figure 4: Perplexities with Kneser-Ney Smoothing
(KN PP) and fraction of covered 5-grams (C5).
7.3 Perplexity and n-Gram Coverage
A standard measure for language model quality is
perplexity. It is measured on test data T = w|T |1 :
PP (T ) = e
? 1|T |
|T |
 
i=1
log p(wi|wi?1i?n+1) (7)
This is the inverse of the average conditional prob-
ability of a next word; lower perplexities are bet-
ter. Figure 4 shows perplexities for models with
Kneser-Ney smoothing. Values range from 280.96
for 13 million to 222.98 for 237 million tokens tar-
get data and drop nearly linearly with data size (r2 =
0.998). Perplexities for ldcnews range from 351.97
to 210.93 and are also close to linear (r2 = 0.987),
while those for webnews data range from 221.85 to
164.15 and flatten out near the end. Perplexities are
generally high and may be explained by the mix-
ture of genres in the test data (newswire, broadcast
news, newsgroups) while our training data is pre-
dominantly written news articles. Other held-out
sets consisting predominantly of newswire texts re-
ceive lower perplexities by the same language mod-
els, e.g., using the full ldcnews model we find per-
plexities of 143.91 for the NIST MT 2005 evaluation
set, and 149.95 for the NIST MT 2004 set.
Note that the perplexities of the different language
models are not directly comparable because they use
different vocabularies. We used a fixed frequency
cutoff, which leads to larger vocabularies as the
training data grows. Perplexities tend to be higher
with larger vocabularies.
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 10  100  1000  10000  100000  1e+06
Te
st
 d
at
a 
BL
EU
LM training data size in million tokens
+0.62BP/x2
+0.56BP/x2
+0.51BP/x2
+0.66BP/x2
+0.70BP/x2
+0.39BP/x2
+0.15BP/x2
target KN
+ldcnews KN
+webnews KN
target SB
+ldcnews SB
+webnews SB
+web SB
Figure 5: BLEU scores for varying amounts of data
using Kneser-Ney (KN) and Stupid Backoff (SB).
Perplexities cannot be calculated for language
models with Stupid Backoff because their scores are
not normalized probabilities. In order to neverthe-
less get an indication of potential quality improve-
ments with increased training sizes we looked at the
5-gram coverage instead. This is the fraction of 5-
grams in the test data set that can be found in the
language model training data. A higher coverage
will result in a better language model if (as we hy-
pothesize) estimates for seen events tend to be bet-
ter than estimates for unseen events. This fraction
grows from 0.06 for 13 million tokens to 0.56 for 2
trillion tokens, meaning 56% of all 5-grams in the
test data are known to the language model.
Increase in coverage depends on the training data
set. Within each set, we observe an almost constant
growth (correlation r2 ? 0.989 for all sets) with
each doubling of the training data as indicated by
numbers next to the lines. The fastest growth oc-
curs for webnews data (+0.038 for each doubling),
the slowest growth for target data (+0.022/x2).
7.4 Machine Translation Results
We use a state-of-the-art machine translation system
for translating from Arabic to English that achieved
a competitive BLEU score of 0.4535 on the Arabic-
English NIST subset in the 2006 NIST machine
translation evaluation8 . Beam size and re-ordering
window were reduced in order to facilitate a large
8See http://www.nist.gov/speech/tests/mt/
mt06eval official results.html for more results.
865
number of experiments. Additionally, our NIST
evaluation system used a mixture of 5, 6, and 7-gram
models with optimized stupid backoff factors for
each order, while the learning curve presented here
uses a fixed order of 5 and a single fixed backoff fac-
tor. Together, these modifications reduce the BLEU
score by 1.49 BLEU points (BP)9 at the largest train-
ing size. We then varied the amount of language
model training data from 13 million to 2 trillion to-
kens. All other parts of the system are kept the same.
Results are shown in Figure 5. The first part
of the curve uses target data for training the lan-
guage model. With Kneser-Ney smoothing (KN),
the BLEU score improves from 0.3559 for 13 mil-
lion tokens to 0.3832 for 237 million tokens. At
such data sizes, Stupid Backoff (SB) with a constant
backoff parameter ? = 0.4 is around 1 BP worse
than KN. On average, one gains 0.62 BP for each
doubling of the training data with KN, and 0.66 BP
per doubling with SB. Differences of more than 0.51
BP are statistically significant at the 0.05 level using
bootstrap resampling (Noreen, 1989; Koehn, 2004).
We then add a second language model using ldc-
news data. The first point for ldcnews shows a large
improvement of around 1.4 BP over the last point
for target for both KN and SB, which is approxi-
mately twice the improvement expected from dou-
bling the amount of data. This seems to be caused
by adding a new domain and combining two models.
After that, we find an improvement of 0.56?0.70 BP
for each doubling of the ldcnews data. The gap be-
tween Kneser-Ney Smoothing and Stupid Backoff
narrows, starting with a difference of 0.85 BP and
ending with a not significant difference of 0.24 BP.
Adding a third language models based on web-
news data does not show a jump at the start of the
curve. We see, however, steady increases of 0.39?
0.51 BP per doubling. The gap between Kneser-Ney
and Stupid Backoff is gone, all results with Stupid
Backoff are actually better than Kneser-Ney, but the
differences are not significant.
We then add a fourth language model based on
web data and Stupid Backoff. Generating Kneser-
Ney models for these data sizes is extremely ex-
pensive and is therefore omitted. The fourth model
91 BP = 0.01 BLEU. We show system scores as BLEU, dif-
ferences as BP.
shows a small but steady increase of 0.15 BP per
doubling, surpassing the best Kneser-Ney model
(trained on less data) by 0.82 BP at the largest
size. Goodman (2001) observed that Kneser-Ney
Smoothing dominates other schemes over a broad
range of conditions. Our experiments confirm this
advantage at smaller language model sizes, but show
the advantage disappears at larger data sizes.
The amount of benefit from doubling the training
size is partly determined by the domains of the data
sets10. The improvements are almost linear on the
log scale within the sets. Linear least-squares regres-
sion shows correlations r2 > 0.96 for all sets and
both smoothing methods, thus we expect to see sim-
ilar improvements when further increasing the sizes.
8 Conclusion
A distributed infrastructure has been described to
train and apply large-scale language models to ma-
chine translation. Experimental results were pre-
sented showing the effect of increasing the amount
of training data to up to 2 trillion tokens, resulting
in a 5-gram language model size of up to 300 billion
n-grams. This represents a gain of about two orders
of magnitude in the amount of training data that can
be handled over that reported previously in the liter-
ature (or three-to-four orders of magnitude, if one
considers only single-pass decoding). The infra-
structure is capable of scaling to larger amounts of
training data and higher n-gram orders.
The technique is made efficient by judicious
batching of score requests by the decoder in a server-
client architecture. A new, simple smoothing tech-
nique well-suited to distributed computation was
proposed, and shown to perform as well as more
sophisticated methods as the size of the language
model increases.
Significantly, we found that translation quality as
indicated by BLEU score continues to improve with
increasing language model size, at even the largest
sizes considered. This finding underscores the value
of being able to train and apply very large language
models, and suggests that further performance gains
may be had by pursuing this direction further.
10There is also an effect of the order in which we add the
models. As an example, web data yields +0.43 BP/x2 when
added as the second model. A discussion of this effect is omit-
ted due to space limitations.
866
References
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard, Cambridge,
MA, USA.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. In Sixth
Symposium on Operating System Design and Imple-
mentation (OSDI-04), San Francisco, CA, USA.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of ICASSP-2007, Honolulu, HI, USA.
Joshua Goodman. 2001. A bit of progress in language
modeling. Technical Report MSR-TR-2001-72, Mi-
crosoft Research, Redmond, WA, USA.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Pattern Recognition in Practice, pages
381?397. North Holland.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3).
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 181?
184.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP-04, Barcelona, Spain.
Hermann Ney and Stefan Ortmanns. 1999. Dynamic
programming search for continuous speech recogni-
tion. IEEE Signal Processing Magazine, 16(5):64?83.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL-
02, pages 311?318, Philadelphia, PA, USA.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In Proceedings of EMNLP-2006, pages
216?223, Sydney, Australia.
867
Proceedings of NAACL HLT 2009: Tutorials, pages 3?4,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Distributed Language Models  Thorsten Brants and Peng Xu, Google Inc.    Language models are used in a wide variety of natural language applications, including machine translation, speech recognition, spelling correction, optical character recognition, etc.  Recent studies have shown that more data is better data, and bigger language models are better language models: the authors found nearly constant machine translation improvements with each doubling of the training data size even at 2 trillion tokens (resulting in 400 billion n-grams). Training and using such large models is a challenge. This tutorial shows efficient methods for distributed training of large language models based on the MapReduce computing model. We also show efficient ways of using distributed models in which requesting individual n-grams is expensive because they require communication between different machines.   Tutorial Outline  1) Training Distributed Models  * N-gram collection    Use of the MapReduce model; compressing intermediate data; minimizing    communication overhead with good sharding functions.  * Smoothing    Challenges of Katz Backoff and Kneser-Ney Smoothing in a distributed system;    Smoothing techniques that are easy to compute in a distributed system:    Stupid Backoff, Linear Interpolation; minimizing communication by sharding    and aggregation.  2) Model Size Reduction  * Pruning    Reducing the size of the model by removing n-grams that don't have much impact.    Entropy pruning is simple to compute for Stupid Backoff, requires some effort for Katz    and Kneser-Ney in a distributed system. Effects of extreme pruning.  * Quantization    Reducing the memory size of the model by storing approximations of the values. We    discuss several quantizers; typically 4 to 8 bits are sufficient to store a floating point    value.  * Randomized Data Structures    Reducing the memory size of the model by changing the set of n-grams that is stored.    This typically lets us store models in 3 bytes per n-gram, independent of the n-gram 
3
   order without significant impact on quality. At the same time it provides very fast    access to the n-grams.  3) Using Distributed Models  * Serving    Requesting a single n-gram in a distributed setup is expensive because it requires    communication between machines. We show how to use a distributed language model    in the first-pass of a decoder by batching up n-gram request.   Target Audience  Target audience are researchers in all areas that focus on or use large n-gram language models.   Presenters  Thorsten Brants received his Ph.D. in 1999 at the Saarland University, Germany, on part-of-speech tagging and parsing. From 2000 to 2003, he worked at the Palo Alto Research Center (PARC) on statistical methods for topic and event detection. Thorsten is now a Research Scientist at Google working on large, distributed language models with focus on applications in machine translation. Other research interests include information retrieval, named entity detection, and speech recognition.  Peng Xu joined Google as a Research Scientist shortly after getting a Ph.D. in April 2005 from the Johns Hopkins University. While his research is focused on statistical machine translation at Google, he is also interested in statistical machine learning, information retrieval, and speech recognition.  
4
Proceedings of ACL-08: HLT, pages 505?513,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Randomized Language Models via Perfect Hash Functions
David Talbot?
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh, UK
d.r.talbot@sms.ed.ac.uk
Thorsten Brants
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
brants@google.com
Abstract
We propose a succinct randomized language
model which employs a perfect hash func-
tion to encode fingerprints of n-grams and
their associated probabilities, backoff weights,
or other parameters. The scheme can repre-
sent any standard n-gram model and is easily
combined with existing model reduction tech-
niques such as entropy-pruning. We demon-
strate the space-savings of the scheme via ma-
chine translation experiments within a dis-
tributed language modeling framework.
1 Introduction
Language models (LMs) are a core component in
statistical machine translation, speech recognition,
optical character recognition and many other areas.
They distinguish plausible word sequences from a
set of candidates. LMs are usually implemented
as n-gram models parameterized for each distinct
sequence of up to n words observed in the train-
ing corpus. Using higher-order models and larger
amounts of training data can significantly improve
performance in applications, however the size of the
resulting LM can become prohibitive.
With large monolingual corpora available in ma-
jor languages, making use of all the available data
is now a fundamental challenge in language mod-
eling. Efficiency is paramount in applications such
as machine translation which make huge numbers
of LM requests per sentence. To scale LMs to larger
corpora with higher-order dependencies, researchers
?Work completed while this author was at Google Inc.
have considered alternative parameterizations such
as class-based models (Brown et al, 1992), model
reduction techniques such as entropy-based pruning
(Stolcke, 1998), novel represention schemes such as
suffix arrays (Emami et al, 2007), Golomb Coding
(Church et al, 2007) and distributed language mod-
els that scale more readily (Brants et al, 2007).
In this paper we propose a novel randomized lan-
guage model. Recent work (Talbot and Osborne,
2007b) has demonstrated that randomized encod-
ings can be used to represent n-gram counts for
LMs with signficant space-savings, circumventing
information-theoretic constraints on lossless data
structures by allowing errors with some small prob-
ability. In contrast the representation scheme used
by our model encodes parameters directly. It can
be combined with any n-gram parameter estimation
method and existing model reduction techniques
such as entropy-based pruning. Parameters that are
stored in the model are retrieved without error; how-
ever, false positives may occur whereby n-grams not
in the model are incorrectly ?found? when requested.
The false positive rate is determined by the space us-
age of the model.
Our randomized language model is based on the
Bloomier filter (Chazelle et al, 2004). We encode
fingerprints (random hashes) of n-grams together
with their associated probabilities using a perfect
hash function generated at random (Majewski et al,
1996). Lookup is very efficient: the values of 3 cells
in a large array are combined with the fingerprint of
an n-gram. This paper focuses on machine transla-
tion. However, many of our findings should transfer
to other applications of language modeling.
505
2 Scaling Language Models
In statistical machine translation (SMT), LMs are
used to score candidate translations in the target lan-
guage. These are typically n-gram models that ap-
proximate the probability of a word sequence by as-
suming each token to be independent of all but n?1
preceding tokens. Parameters are estimated from
monolingual corpora with parameters for each dis-
tinct word sequence of length l ? [n] observed in
the corpus. Since the number of parameters grows
somewhat exponentially with n and linearly with the
size of the training corpus, the resulting models can
be unwieldy even for relatively small corpora.
2.1 Scaling Strategies
Various strategies have been proposed to scale LMs
to larger corpora and higher-order dependencies.
Model-based techniques seek to parameterize the
model more efficiently (e.g. latent variable models,
neural networks) or to reduce the model size directly
by pruning uninformative parameters, e.g. (Stolcke,
1998), (Goodman and Gao, 2000). Representation-
based techniques attempt to reduce space require-
ments by representing the model more efficiently or
in a form that scales more readily, e.g. (Emami et al,
2007), (Brants et al, 2007), (Church et al, 2007).
2.2 Lossy Randomized Encodings
A fundamental result in information theory (Carter
et al, 1978) states that a random set of objects can-
not be stored using constant space per object as the
universe from which the objects are drawn grows
in size: the space required to uniquely identify an
object increases as the set of possible objects from
which it must be distinguished grows. In language
modeling the universe under consideration is the
set of all possible n-grams of length n for given
vocabulary. Although n-grams observed in natu-
ral language corpora are not randomly distributed
within this universe no lossless data structure that we
are aware of can circumvent this space-dependency
on both the n-gram order and the vocabulary size.
Hence as the training corpus and vocabulary grow, a
model will require more space per parameter.
However, if we are willing to accept that occa-
sionally our model will be unable to distinguish be-
tween distinct n-grams, then it is possible to store
each parameter in constant space independent of
both n and the vocabulary size (Carter et al, 1978),
(Talbot and Osborne, 2007a). The space required in
such a lossy encoding depends only on the range of
values associated with the n-grams and the desired
error rate, i.e. the probability with which two dis-
tinct n-grams are assigned the same fingerprint.
2.3 Previous Randomized LMs
Recent work (Talbot and Osborne, 2007b) has used
lossy encodings based on Bloom filters (Bloom,
1970) to represent logarithmically quantized cor-
pus statistics for language modeling. While the ap-
proach results in significant space savings, working
with corpus statistics, rather than n-gram probabil-
ities directly, is computationally less efficient (par-
ticularly in a distributed setting) and introduces a
dependency on the smoothing scheme used. It also
makes it difficult to leverage existing model reduc-
tion strategies such as entropy-based pruning that
are applied to final parameter estimates.
In the next section we describe our randomized
LM scheme based on perfect hash functions. This
scheme can be used to encode any standard n-gram
model which may first be processed using any con-
ventional model reduction technique.
3 Perfect Hash-based Language Models
Our randomized LM is based on the Bloomier filter
(Chazelle et al, 2004). We assume the n-grams and
their associated parameter values have been precom-
puted and stored on disk. We then encode the model
in an array such that each n-gram?s value can be re-
trieved. Storage for this array is the model?s only
significant space requirement once constructed.1
The model uses randomization to map n-grams
to fingerprints and to generate a perfect hash func-
tion that associates n-grams with their values. The
model can erroneously return a value for an n-gram
that was never actually stored, but will always return
the correct value for an n-gram that is in the model.
We will describe the randomized algorithm used to
encode n-gram parameters in the model, analyze the
probability of a false positive, and explain how we
construct and query the model in practice.
1Note that we do not store the n-grams explicitly and there-
fore that the model?s parameter set cannot easily be enumerated.
506
3.1 N -gram Fingerprints
We wish to encode a set of n-gram/value pairs
S = {(x1, v(x1)), (x2, v(x2)), . . . , (xN , v(xN ))}
using an array A of size M and a perfect hash func-
tion. Each n-gram xi is drawn from some set of pos-
sible n-grams U and its associated value v(xi) from
a corresponding set of possible values V .
We do not store the n-grams and their proba-
bilities directly but rather encode a fingerprint of
each n-gram f(xi) together with its associated value
v(xi) in such a way that the value can be retrieved
when the model is queried with the n-gram xi.
A fingerprint hash function f : U ? [0, B ? 1]
maps n-grams to integers between 0 and B ? 1.2
The array A in which we encode n-gram/value pairs
has addresses of size dlog2 Be hence B will deter-
mine the amount of space used per n-gram. There
is a trade-off between space and error rate since the
larger B is, the lower the probability of a false pos-
itive. This is analyzed in detail below. For now we
assume only that B is at least as large as the range
of values stored in the model, i.e. B ? |V|.
3.2 Composite Perfect Hash Functions
The function used to associate n-grams with their
values (Eq. (1)) combines a composite perfect hash
function (Majewski et al, 1996) with the finger-
print function. An example is shown in Fig. 1.
The composite hash function is made up of k in-
dependent hash functions h1, h2, . . . , hk where each
hi : U ? [0,M ? 1] maps n-grams to locations in
the array A. The lookup function is then defined as
g : U ? [0, B ? 1] by3
g(xi) = f(xi)?
(
k?
i=1
A[hi(xi)]
)
(1)
where f(xi) is the fingerprint of n-gram xi and
A[hi(xi)] is the value stored in location hi(xi) of the
array A. Eq. (1) is evaluated to retrieve an n-gram?s
parameter during decoding. To encode our model
correctly we must ensure that g(xi) = v(xi) for all
n-grams in our set S. Generating A to encode this
2The analysis assumes that all hash functions are random.
3We use ? to denote the exclusive bitwise OR operator.
Figure 1: Encoding an n-gram?s value in the array.
function for a given set of n-grams is a significant
challenge described in the following sections.
3.3 Encoding n-grams in the model
All addresses in A are initialized to zero. The proce-
dure we use to ensure g(xi) = v(xi) for all xi ? S
updates a single, unique location in A for each n-
gram xi. This location is chosen from among the k
locations given by hj(xi), j ? [k]. Since the com-
posite function g(xi) depends on the values stored at
all k locations A[h1(xi)], A[h2(xi)], . . . , A[hk(xi)]
in A, we must also ensure that once an n-gram xi
has been encoded in the model, these k locations
are not subsequently changed since this would inval-
idate the encoding; however, n-grams encoded later
may reference earlier entries and therefore locations
in A can effectively be ?shared? among parameters.
In the following section we describe a randomized
algorithm to find a suitable order in which to enter
n-grams in the model and, for each n-gram xi, de-
termine which of the k hash functions, say hj , can
be used to update A without invalidating previous
entries. Given this ordering of the n-grams and the
choice of hash function hj for each xi ? S, it is clear
that the following update rule will encode xi in the
array A so that g(xi) will return v(xi) (cf. Eq.(1))
A[hj(xi)] = v(xi)? f(xi)?
k?
i=1?i6=j
A[hi(xi)]. (2)
3.4 Finding an Ordered Matching
We now describe an algorithm (Algorithm 1; (Ma-
jewski et al, 1996)) that selects one of the k hash
507
functions hj , j ? [k] for each n-gram xi ? S and
an order in which to apply the update rule Eq. (2) so
that g(xi) maps xi to v(xi) for all n-grams in S.
This problem is equivalent to finding an ordered
matching in a bipartite graph whose LHS nodes cor-
respond to n-grams in S and RHS nodes correspond
to locations in A. The graph initially contains edges
from each n-gram to each of the k locations in A
given by h1(xi), h2(xi), . . . , hk(xi) (see Fig. (2)).
The algorithm uses the fact that any RHS node that
has degree one (i.e. a single edge) can be safely
matched with its associated LHS node since no re-
maining LHS nodes can be dependent on it.
We first create the graph using the k hash func-
tions hj , j ? [k] and store a list (degree one)
of those RHS nodes (locations) with degree one.
The algorithm proceeds by removing nodes from
degree one in turn, pairing each RHS node with
the unique LHS node to which it is connected. We
then remove both nodes from the graph and push the
pair (xi, hj(xi)) onto a stack (matched). We also
remove any other edges from the matched LHS node
and add any RHS nodes that now have degree one
to degree one. The algorithm succeeds if, while
there are still n-grams left to match, degree one
is never empty. We then encode n-grams in the order
given by the stack (i.e., first-in-last-out).
Since we remove each location in A (RHS node)
from the graph as it is matched to an n-gram (LHS
node), each location will be associated with at most
one n-gram for updating. Moreover, since we match
an n-gram to a location only once the location has
degree one, we are guaranteed that any other n-
grams that depend on this location are already on
the stack and will therefore only be encoded once
we have updated this location. Hence dependencies
in g are respected and g(xi) = v(xi) will remain
true following the update in Eq. (2) for each xi ? S.
3.5 Choosing Random Hash Functions
The algorithm described above is not guaranteed to
succeed. Its success depends on the size of the array
M , the number of n-grams stored |S| and the choice
of random hash functions hj , j ? [k]. Clearly we
require M ? |S|; in fact, an argument from Majew-
ski et al (1996) implies that if M ? 1.23|S| and
k = 3, the algorithm succeeds with high probabil-
Figure 2: The ordered matching algorithm: matched =
[(a, 1), (b, 2), (d, 4), (c, 5)]
ity. We use 2-universal hash functions (L. Carter
and M. Wegman, 1979) defined for a range of size
M via a prime P ? M and two random numbers
1 ? aj ? P and 0 ? bj ? P for j ? [k] as
hj(x) ? ajx + bj mod P
taken modulo M . We generate a set of k hash
functions by sampling k pairs of random numbers
(aj , bj), j ? [k]. If the algorithm does not find
a matching with the current set of hash functions,
we re-sample these parameters and re-start the algo-
rithm. Since the probability of failure on a single
attempt is low when M ? 1.23|S|, the probability
of failing multiple times is very small.
3.6 Querying the Model and False Positives
The construction we have described above ensures
that for any n-gram xi ? S we have g(xi) = v(xi),
i.e., we retrieve the correct value. To retrieve a value
given an n-gram xi we simply compute the finger-
print f(xi), the hash functions hj(xi), j ? [k] and
then return g(xi) using Eq. (1). Note that unlike the
constructions in (Talbot and Osborne, 2007b) and
(Church et al, 2007) no errors are possible for n-
grams stored in the model. Hence we will not make
errors for common n-grams that are typically in S.
508
Algorithm 1 Ordered Matching
Input : Set of n-grams S; k hash functions hj , j ? [k];
number of available locations M .
Output : Ordered matching matched or FAIL.
matched ? [ ]
for all i ? [0,M ? 1] do
r2li ? ?
end for
for all xi ? S do
l2ri ? ?
for all j ? [k] do
l2ri ? l2ri ? hj(xi)
r2lhj(xi) ? r2lhj(xi) ? xi
end for
end for
degree one ? {i ? [0,M ? 1] | |r2li| = 1}
while |degree one| ? 1 do
rhs ? POP degree one
lhs ? POP r2lrhs
PUSH (lhs, rhs) onto matched
for all rhs? ? l2rlhs do
POP r2lrhs?
if |r2lrhs? | = 1 then
degree one ? degree one ? rhs?
end if
end for
end while
if |matched| = |S| then
return matched
else
return FAIL
end if
On the other hand, querying the model with an n-
gram that was not stored, i.e. with xi ? U \ S we
may erroneously return a value v ? V .
Since the fingerprint f(xi) is assumed to be dis-
tributed uniformly at random (u.a.r.) in [0, B ? 1],
g(xi) is also u.a.r. in [0, B?1] for xi ? U\S . Hence
with |V| values stored in the model, the probability
that xi ? U \ S is assigned a value in v ? V is
Pr{g(xi) ? V|xi ? U \ S} = |V|/B.
We refer to this event as a false positive. If V is fixed,
we can obtain a false positive rate  by setting B as
B ? |V|/.
For example, if |V| is 128 then taking B = 1024
gives an error rate of  = 128/1024 = 0.125 with
each entry inA using dlog2 1024e = 10 bits. Clearly
B must be at least |V| in order to distinguish each
value. We refer to the additional bits allocated to
each location (i.e. dlog2 Be ? log2 |V| or 3 in our
example) as error bits in our experiments below.
3.7 Constructing the Full Model
When encoding a large set of n-gram/value pairs S,
Algorithm 1 will only be practical if the raw data
and graph can be held in memory as the perfect hash
function is generated. This makes it difficult to en-
code an extremely large set S into a single array A.
The solution we adopt is to split S into t smaller
sets S?i, i ? [t] that are arranged in lexicographic or-
der.4 We can then encode each subset in a separate
array A?i, i ? [t] in turn in memory. Querying each
of these arrays for each n-gram requested would be
inefficient and inflate the error rate since a false posi-
tive could occur on each individual array. Instead we
store an index of the final n-gram encoded in each
array and given a request for an n-gram?s value, per-
form a binary search for the appropriate array.
3.8 Sanity Checks
Our models are consistent in the following sense
(w1, w2, . . . , wn) ? S =? (w2, . . . , wn) ? S.
Hence we can infer that an n-gram can not be
present in the model, if the n? 1-gram consisting of
the final n ? 1 words has already tested false. Fol-
lowing (Talbot and Osborne, 2007a) we can avoid
unnecessary false positives by not querying for the
longer n-gram in such cases.
Backoff smoothing algorithms typically request
the longest n-gram supported by the model first, re-
questing shorter n-grams only if this is not found. In
our case, however, if a query is issued for the 5-gram
(w1, w2, w3, w4, w5) when only the unigram (w5) is
present in the model, the probability of a false posi-
tive using such a backoff procedure would not be  as
stated above, but rather the probability that we fail to
avoid an error on any of the four queries performed
prior to requesting the unigram, i.e. 1?(1?)4 ? 4.
We therefore query the model first with the unigram
working up to the full n-gram requested by the de-
coder only if the preceding queries test positive. The
probability of returning a false positive for any n-
gram requested by the decoder (but not in the model)
will then be at most .
4In our system we use subsets of 5 million n-grams which
can easily be encoded using less than 2GB of working space.
509
4 Experimental Set-up
4.1 Distributed LM Framework
We deploy the randomized LM in a distributed
framework which allows it to scale more easily
by distributing it across multiple language model
servers. We encode the model stored on each lan-
guagage model server using the randomized scheme.
The proposed randomized LM can encode param-
eters estimated using any smoothing scheme (e.g.
Kneser-Ney, Katz etc.). Here we choose to work
with stupid backoff smoothing (Brants et al, 2007)
since this is significantly more efficient to train and
deploy in a distributed framework than a context-
dependent smoothing scheme such as Kneser-Ney.
Previous work (Brants et al, 2007) has shown it to
be appropriate to large-scale language modeling.
4.2 LM Data Sets
The language model is trained on four data sets:
target: The English side of Arabic-English parallel
data provided by LDC (132 million tokens).
gigaword: The English Gigaword dataset provided
by LDC (3.7 billion tokens).
webnews: Data collected over several years, up to
January 2006 (34 billion tokens).
web: The Web 1T 5-gram Version 1 corpus provided
by LDC (1 trillion tokens).5
An initial experiment will use the Web 1T 5-gram
corpus only; all other experiments will use a log-
linear combination of models trained on each cor-
pus. The combined model is pre-compiled with
weights trained on development data by our system.
4.3 Machine Translation
The SMT system used is based on the framework
proposed in (Och and Ney, 2004) where translation
is treated as the following optimization problem
e? = argmax
e
M?
i=1
?i?i(e, f). (3)
Here f is the source sentence that we wish to trans-
late, e is a translation in the target language, ?i, i ?
[M ] are feature functions and ?i, i ? [M ] are
weights. (Some features may not depend on f .)
5N -grams with count < 40 are not included in this data set.
Full Set Entropy-Pruned
# 1-grams 13,588,391 13,588,391
# 2-grams 314,843,401 184,541,402
# 3-grams 977,069,902 439,430,328
# 4-grams 1,313,818,354 407,613,274
# 5-grams 1,176,470,663 238,348,867
Total 3,795,790,711 1,283,522,262
Table 1: Num. of n-grams in the Web 1T 5-gram corpus.
5 Experiments
This section describes three sets of experiments:
first, we encode the Web 1T 5-gram corpus as a ran-
domized language model and compare the result-
ing size with other representations; then we mea-
sure false positive rates when requesting n-grams
for a held-out data set; finally we compare transla-
tion quality when using conventional (lossless) lan-
guages models and our randomized language model.
Note that the standard practice of measuring per-
plexity is not meaningful here since (1) for efficient
computation, the language model is not normalized;
and (2) even if this were not the case, quantization
and false positives would render it unnormalized.
5.1 Encoding the Web 1T 5-gram corpus
We build a language model from the Web 1T 5-gram
corpus. Parameters, corresponding to negative loga-
rithms of relative frequencies, are quantized to 8-bits
using a uniform quantizer. More sophisticated quan-
tizers (e.g. (S. Lloyd, 1982)) may yield better results
but are beyond the scope of this paper.
Table 1 provides some statistics about the corpus.
We first encode the full set of n-grams, and then a
version that is reduced to approx. 1/3 of its original
size using entropy pruning (Stolcke, 1998).
Table 2 shows the total space and number of bytes
required per n-gram to encode the model under dif-
ferent schemes: ?LDC gzip?d? is the size of the files
as delivered by LDC; ?Trie? uses a compact trie rep-
resentation (e.g., (Clarkson et al, 1997; Church et
al., 2007)) with 3 byte word ids, 1 byte values, and 3
byte indices; ?Block encoding? is the encoding used
in (Brants et al, 2007); and ?randomized? uses our
novel randomized scheme with 12 error bits. The
latter requires around 60% of the space of the next
best representation and less than half of the com-
510
size (GB) bytes/n-gram
Full Set
LDC gzip?d 24.68 6.98
Trie 21.46 6.07
Block Encoding 18.00 5.14
Randomized 10.87 3.08
Entropy Pruned
Trie 7.70 6.44
Block Encoding 6.20 5.08
Randomized 3.68 3.08
Table 2: Web 1T 5-gram language model sizes with dif-
ferent encodings. ?Randomized? uses 12 error bits.
monly used trie encoding. Our method is the only
one to use the same amount of space per parameter
for both full and entropy-pruned models.
5.2 False Positive Rates
All n-grams explicitly inserted into our randomized
language model are retrieved without error; how-
ever, n-grams not stored may be incorrectly assigned
a value resulting in a false positive. Section (3) an-
alyzed the theoretical error rate; here, we measure
error rates in practice when retrieving n-grams for
approx. 11 million tokens of previously unseen text
(news articles published after the training data had
been collected). We measure this separately for all
n-grams of order 2 to 5 from the same text.
The language model is trained on the four data
sources listed above and contains 24 billion n-
grams. With 8-bit parameter values, the model
requires 55.2/69.0/82.7 GB storage when using
8/12/16 error bits respectively (this corresponds to
2.46/3.08/3.69 bytes/n-gram).
Using such a large language model results in a
large fraction of known n-grams in new text. Table
3 shows, e.g., that almost half of all 5-grams from
the new text were seen in the training data.
Column (1) in Table 4 shows the number of false
positives that occurred for this test data. Column
(2) shows this as a fraction of the number of unseen
n-grams in the data. This number should be close to
2?b where b is the number of error bits (i.e. 0.003906
for 8 bits and 0.000244 for 12 bits). The error rates
for bigrams are close to their expected values. The
numbers are much lower for higher n-gram orders
due to the use of sanity checks (see Section 3.8).
total seen unseen
2gms 11,093,093 98.98% 1.02%
3gms 10,652,693 91.08% 8.92%
4gms 10,212,293 68.39% 31.61%
5gms 9,781,777 45.51% 54.49%
Table 3: Number of n-grams in test set and percentages
of n-grams that were seen/unseen in the training data.
(1) (2) (3)
false pos. false posunseen
false pos
total
8 error bits
2gms 376 0.003339 0.000034
3gms 2839 0.002988 0.000267
4gms 6659 0.002063 0.000652
5gms 6356 0.001192 0.000650
total 16230 0.001687 0.000388
12 error bits
2gms 25 0.000222 0.000002
3gms 182 0.000192 0.000017
4gms 416 0.000129 0.000041
5gms 407 0.000076 0.000042
total 1030 0.000107 0.000025
Table 4: False positive rates with 8 and 12 error bits.
The overall fraction of n-grams requested for
which an error occurs is of most interest in applica-
tions. This is shown in Column (3) and is around a
factor of 4 smaller than the values in Column (2). On
average, we expect to see 1 error in around 2,500 re-
quests when using 8 error bits, and 1 error in 40,000
requests with 12 error bits (see ?total? row).
5.3 Machine Translation
We run an improved version of our 2006 NIST MT
Evaluation entry for the Arabic-English ?Unlimited?
data track.6 The language model is the same one as
in the previous section.
Table 5 shows baseline translation BLEU scores
for a lossless (non-randomized) language model
with parameter values quantized into 5 to 8 bits. We
use MT04 data for system development, with MT05
data and MT06 (?NIST? subset) data for blind test-
ing. As expected, results improve when using more
bits. There seems to be little benefit in going beyond
6See http://www.nist.gov/speech/tests/mt/2006/doc/
511
dev test test
bits MT04 MT05 MT06
5 0.5237 0.5608 0.4636
6 0.5280 0.5671 0.4649
7 0.5299 0.5691 0.4672
8 0.5304 0.5697 0.4663
Table 5: Baseline BLEU scores with lossless n-gram
model and different quantization levels (bits).
 0.554
 0.556
 0.558
 0.56
 0.562
 0.564
 0.566
 0.568
 0.57
 8  9  10  11  12  13  14  15  16
MT
05
 B
LE
U
Number of Error Bits
8 bit values
7 bit values
6 bit values
5 bit values
Figure 3: BLEU scores on the MT05 data set.
8 bits. Overall, our baseline results compare favor-
ably to those reported on the NIST MT06 web site.
We now replace the language model with a ran-
domized version. Fig. 3 shows BLEU scores for the
MT05 evaluation set with parameter values quan-
tized into 5 to 8 bits and 8 to 16 additional ?er-
ror? bits. Figure 4 shows a similar graph for MT06
data. We again see improvements as quantization
uses more bits. There is a large drop in performance
when reducing the number of error bits from 10 to
8, while increasing it beyond 12 bits offers almost
no further gains with scores that are almost identi-
cal to the lossless model. Using 8-bit quantization
and 12 error bits results in an overall requirement of
(8+12)?1.23 = 24.6 bits = 3.08 bytes per n-gram.
All runs use the sanity checks described in Sec-
tion 3.8. Without sanity checks, scores drop, e.g. by
0.002 for 8-bit quantization and 12 error bits.
Randomization and entropy pruning can be com-
bined to achieve further space savings with minimal
loss in quality as shown in Table (6). The BLEU
score drops by between 0.0007 to 0.0018 while the
 0.454
 0.456
 0.458
 0.46
 0.462
 0.464
 0.466
 0.468
 8  9  10  11  12  13  14  15  16
MT
06
 (N
IST) B
LE
U
Number of Error Bits
8 bit values
7 bit values
6 bit values
5 bit values
Figure 4: BLEU scores on MT06 data (?NIST? subset).
size dev test test
LM GB MT04 MT05 MT06
unpruned block 116 0.5304 0.5697 0.4663
unpruned rand 69 0.5299 0.5692 0.4659
pruned block 42 0.5294 0.5683 0.4665
pruned rand 27 0.5289 0.5679 0.4656
Table 6: Combining randomization and entropy pruning.
All models use 8-bit values; ?rand? uses 12 error bits.
model is reduced to approx. 1/4 of its original size.
6 Conclusions
We have presented a novel randomized language
model based on perfect hashing. It can associate
arbitrary parameter types with n-grams. Values ex-
plicitly inserted into the model are retrieved without
error; false positives may occur but are controlled
by the number of bits used per n-gram. The amount
of storage needed is independent of the size of the
vocabulary and the n-gram order. Lookup is very
efficient: the values of 3 cells in a large array are
combined with the fingerprint of an n-gram.
Experiments have shown that this randomized
language model can be combined with entropy prun-
ing to achieve further memory reductions; that error
rates occurring in practice are much lower than those
predicted by theoretical analysis due to the use of
runtime sanity checks; and that the same translation
quality as a lossless language model representation
can be achieved when using 12 ?error? bits, resulting
in approx. 3 bytes per n-gram (this includes one byte
to store parameter values).
512
References
B. Bloom. 1970. Space/time tradeoffs in hash coding
with allowable errors. CACM, 13:422?426.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL 2007, Prague.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Larry Carter, Robert W. Floyd, John Gill, George
Markowsky, and Mark N. Wegman. 1978. Exact and
approximate membership testers. In STOC, pages 59?
65.
L. Carter and M. Wegman. 1979. Universal classes of
hash functions. Journal of Computer and System Sci-
ence, 18:143?154.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The Bloomier Filter: an efficient
data structure for static support lookup tables. In Proc.
15th ACM-SIAM Symposium on Discrete Algoritms,
pages 30?39.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with golomb
coding. In Proceedings of EMNLP-CoNLL 2007,
Prague, Czech Republic, June.
P. Clarkson and R. Rosenfeld. 1997. Statistical language
modeling using the CMU-Cambridge toolkit. In Pro-
ceedings of EUROSPEECH, vol. 1, pages 2707?2710,
Rhodes, Greece.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)
2007, Hawaii, USA.
J. Goodman and J. Gao. 2000. Language model size re-
duction by pruning and clustering. In ICSLP?00, Bei-
jing, China.
S. Lloyd. 1982. Least squares quantization in PCM.
IEEE Transactions on Information Theory, 28(2):129?
137.
B.S. Majewski, N.C. Wormald, G. Havas, and Z.J. Czech.
1996. A family of perfect hashing methods. British
Computer Journal, 39(6):547?554.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270?274.
D. Talbot andM. Osborne. 2007a. Randomised language
modelling for statistical machine translation. In 45th
Annual Meeting of the ACL 2007, Prague.
D. Talbot and M. Osborne. 2007b. Smoothed Bloom
filter language models: Tera-scale LMs on the cheap.
In EMNLP/CoNLL 2007, Prague.
513
Proceedings of ACL-08: HLT, pages 755?762,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Distributed Word Clustering for Large Scale Class-Based
Language Modeling in Machine Translation
Jakob Uszkoreit? Thorsten Brants
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
{uszkoreit,brants}@google.com
Abstract
In statistical language modeling, one technique
to reduce the problematic effects of data spar-
sity is to partition the vocabulary into equiva-
lence classes. In this paper we investigate the
effects of applying such a technique to higher-
order n-gram models trained on large corpora.
We introduce a modification of the exchange
clustering algorithm with improved efficiency
for certain partially class-based models and a
distributed version of this algorithm to effi-
ciently obtain automatic word classifications
for large vocabularies (>1 million words) us-
ing such large training corpora (>30 billion to-
kens). The resulting clusterings are then used
in training partially class-based language mod-
els. We show that combining them with word-
based n-gram models in the log-linear model
of a state-of-the-art statistical machine trans-
lation system leads to improvements in trans-
lation quality as indicated by the BLEU score.
1 Introduction
A statistical language model assigns a probability
P (w) to any given string of words wm1 = w1, ..., wm.
In the case of n-gram language models this is done
by factoring the probability:
P (wm1 ) =
m?
i=1
P (wi|w
i?1
1 )
and making a Markov assumption by approximating
this by:
m?
i=1
P (wi|w
i?1
1 ) ?
m?
i=1
p(wi|w
i?1
i?n+1)
Even after making the Markov assumption and thus
treating all strings of preceding words as equal which
? Parts of this research were conducted while the author
studied at the Berlin Institute of Technology
do not differ in the last n? 1 words, one problem n-
gram language models suffer from is that the training
data is too sparse to reliably estimate all conditional
probabilities P (wi|w
i?1
1 ).
Class-based n-gram models are intended to help
overcome this data sparsity problem by grouping
words into equivalence classes rather than treating
them as distinct words and thus reducing the num-
ber of parameters of the model (Brown et al, 1990).
They have often been shown to improve the per-
formance of speech recognition systems when com-
bined with word-based language models (Martin et
al., 1998; Whittaker and Woodland, 2001). However,
in the area of statistical machine translation, espe-
cially in the context of large training corpora, fewer
experiments with class-based n-gram models have
been performed with mixed success (Raab, 2006).
Class-based n-gram models have also been shown
to benefit from their reduced number of parameters
when scaling to higher-order n-grams (Goodman and
Gao, 2000), and even despite the increasing size and
decreasing sparsity of language model training cor-
pora (Brants et al, 2007), class-based n-gram mod-
els might lead to improvements when increasing the
n-gram order.
When training class-based n-gram models on large
corpora and large vocabularies, one of the prob-
lems arising is the scalability of the typical cluster-
ing algorithms used for obtaining the word classifi-
cation. Most often, variants of the exchange algo-
rithm (Kneser and Ney, 1993; Martin et al, 1998)
or the agglomerative clustering algorithm presented
in (Brown et al, 1990) are used, both of which have
prohibitive runtimes when clustering large vocabu-
laries on the basis of large training corpora with a
sufficiently high number of classes.
In this paper we introduce a modification of the ex-
change algorithm with improved efficiency and then
present a distributed version of the modified algo-
rithm, which makes it feasible to obtain word clas-
755
sifications using billions of tokens of training data.
We then show that using partially class-based lan-
guage models trained using the resulting classifica-
tions together with word-based language models in
a state-of-the-art statistical machine translation sys-
tem yields improvements despite the very large size
of the word-based models used.
2 Class-Based Language Modeling
By partitioning all Nv words of the vocabulary into
Nc sets, with c(w) mapping a word onto its equiva-
lence class and c(wji ) mapping a sequence of words
onto the sequence of their respective equivalence
classes, a typical class-based n-gram model approxi-
mates P (wi|w
i?1
1 ) with the two following component
probabilities:
P (wi|w
i?1
1 ) ? p0(wi|c(wi)) ? p1(c(wi)|c(w
i?1
i?n+1))
(1)
thus greatly reducing the number of parameters in
the model, since usually Nc is much smaller than
Nv.
In the following, we will call this type of model a
two-sided class-based model, as both the history of
each n-gram, the sequence of words conditioned on,
as well as the predicted word are replaced by their
class.
Once a partition of the words in the vocabulary is
obtained, two-sided class-based models can be built
just like word-based n-gram models using existing
infrastructure. In addition, the size of the model is
usually greatly reduced.
2.1 One-Sided Class-Based Models
Two-sided class-based models received most atten-
tion in the literature. However, several different
types of mixed word and class models have been
proposed for the purpose of improving the perfor-
mance of the model (Goodman, 2000), reducing its
size (Goodman and Gao, 2000) as well as lower-
ing the complexity of related clustering algorithms
(Whittaker and Woodland, 2001).
In (Emami and Jelinek, 2005) a clustering algo-
rithm is introduced which outputs a separate clus-
tering for each word position in a trigram model. In
the experimental evaluation, the authors observe the
largest improvements using a specific clustering for
the last word of each trigram but no clustering at
all for the first two word positions. Generalizing this
leads to arbitrary order class-based n-gram models
of the form:
P (wi|w
i?1
1 ) ? p0(wi|c(wi)) ? p1(c(wi)|w
i?1
i?n+1) (2)
which we will call predictive class-based models in the
following sections.
3 Exchange Clustering
One of the frequently used algorithms for automat-
ically obtaining partitions of the vocabulary is the
exchange algorithm (Kneser and Ney, 1993; Martin
et al, 1998). Beginning with an initial clustering,
the algorithm greedily maximizes the log likelihood
of a two-sided class bigram or trigram model as de-
scribed in Eq. (1) on the training data. Let V be
the set of words in the vocabulary and C the set of
classes. This then leads to the following optimization
criterion, where N(w) and N(c) denote the number
of occurrences of a word w or a class c in the training
data and N(c, d) denotes the number of occurrences
of some word in class c followed by a word in class d
in the training data:
C? = argmax
C
?
w?V
N(w) ? logN(w) +
+
?
c?C,d?C
N(c, d) ? logN(c, d)?
?2 ?
?
c?C
N(c) ? logN(c) (3)
The algorithm iterates over all words in the vo-
cabulary and tentatively moves each word to each
cluster. The change in the optimization criterion is
computed for each of these tentative moves and the
exchange leading to the highest increase in the opti-
mization criterion (3) is performed. This procedure
is then repeated until the algorithm reaches a local
optimum.
To be able to efficiently calculate the changes in
the optimization criterion when exchanging a word,
the counts in Eq. (3) are computed once for the ini-
tial clustering, stored, and afterwards updated when
a word is exchanged.
Often only a limited number of iterations are per-
formed, as letting the algorithm terminate in a local
optimum can be computationally impractical.
3.1 Complexity
The implementation described in (Martin et al,
1998) uses a memory saving technique introducing
a binary search into the complexity estimation. For
the sake of simplicity, we omit this detail in the fol-
lowing complexity analysis. We also do not employ
this optimization in our implementation.
The worst case complexity of the exchange algo-
rithm is quadratic in the number of classes. However,
756
Input: The fixed number of clusters Nc
Compute initial clustering
while clustering changed in last iteration do
forall w ? V do
forall c ? C do
move word w tentatively to cluster
c
compute updated optimization
criterion
move word w to cluster maximizing
optimization criterion
Algorithm 1: Exchange Algorithm Outline
the average case complexity can be reduced by up-
dating only the counts which are actually affected by
moving a word from one cluster to another. This can
be done by considering only those sets of clusters for
which N(w, c) > 0 or N(c, w) > 0 for a word w about
to be exchanged, both of which can be calculated ef-
ficiently when exchanging a word. The algorithm
scales linearly in the size of the vocabulary.
With Nprec and N
suc
c denoting the average number
of clusters preceding and succeeding another cluster,
B denoting the number of distinct bigrams in the
training corpus, and I denoting the number of itera-
tions, the worst case complexity of the algorithm is
in:
O(I ? (2 ?B +Nv ?Nc ? (N
pre
c +N
suc
c )))
When using large corpora with large numbers of
bigrams the number of required updates can increase
towards the quadratic upper bound as Nprec and
Nsucc approach Nc. For a more detailed description
and further analysis of the complexity, the reader is
referred to (Martin et al, 1998).
4 Predictive Exchange Clustering
Modifying the exchange algorithm in order to opti-
mize the log likelihood of a predictive class bigram
model, leads to substantial performance improve-
ments, similar to those previously reported for an-
other type of one-sided class model in (Whittaker
and Woodland, 2001).
We use a predictive class bigram model as given
in Eq. (2), for which the maximum-likelihood prob-
ability estimates for the n-grams are given by their
relative frequencies:
P (wi|w
i?1
1 ) ? p0(wi|c(wi)) ? p1(c(wi)|wi?1)(4)
=
N(wi)
N(c(wi))
?
N(wi?1, c(wi))
N(wi?1)
(5)
whereN(w) again denotes the number of occurrences
of the word w in the training corpus and N(v, c)
the number of occurrences of the word v followed by
some word in class c. Then the following optimiza-
tion criterion can be derived, with F (C) being the
log likelihood function of the predictive class bigram
model given a clustering C:
F (C) =
?
w?V
N(w) ? log p(w|c(w))
+
?
v?V,c?C
N(v, c) ? log p(c|v) (6)
=
?
w?V
N(w) ? log
N(w)
N(c(w))
+
?
v?V,c?C
N(v, c) ? log
N(v, c)
N(v)
(7)
=
?
w?V
N(w) ? logN(w)
?
?
w?V
N(w) ? logN(c(w))
+
?
v?V,c?C
N(v, c) ? logN(v, c)
?
?
v?V,c?C
N(v, c) ? logN(v) (8)
The very last summation of Eq. (8) now effectively
sums over all occurrences of all words and thus can-
cels out with the first summation of (8) which leads
to:
F (C) =
?
v?V,c?C
N(v, c) ? logN(v, c)
?
?
w?V
N(w) ? logN(c(w)) (9)
In the first summation of Eq. (9), for a given word v
only the set of classes which contain at least one word
w for whichN(v, w) > 0 must be considered, denoted
by suc(v). The second summation is equivalent to
?
c?C N(c) ? logN(c). Thus the further simplified
criterion is:
F (C) =
?
v?V,c?suc(v)
N(v, c) ? logN(v, c)
?
?
c?C
N(c) ? logN(c) (10)
When exchanging a word w between two classes c
and c?, only two summands of the second summation
of Eq. (10) are affected. The first summation can be
updated by iterating over all bigrams ending in the
exchanged word. Throughout one iteration of the
algorithm, in which for each word in the vocabulary
each possible move to another class is evaluated, this
757
amounts to the number of distinct bigrams in the
training corpus B, times the number of clusters Nc.
Thus the worst case complexity using the modified
optimization criterion is in:
O(I ?Nc ? (B +Nv))
Using this optimization criterion has two effects
on the complexity of the algorithm. The first dif-
ference is that in contrast to the exchange algorithm
using a two sided class-based bigram model in its op-
timization criterion, only two clusters are affected by
moving a word. Thus the algorithm scales linearly
in the number of classes. The second difference is
that B dominates the term B+Nv for most corpora
and scales far less than linearly with the vocabulary
size, providing a significant performance advantage
over the other optimization criterion, especially when
large vocabularies are used (Whittaker and Wood-
land, 2001).
For efficiency reasons, an exchange of a word be-
tween two clusters is separated into a remove and a
move procedure. In each iteration the remove proce-
dure only has to be called once for each word, while
for a given word move is called once for every clus-
ter to compute the consequences of the tentative ex-
changes. An outline of the move procedure is given
below. The remove procedure is similar.
Input: A word w, and a destination cluster c
Result: The change in the optimization
criterion when moving w to cluster c
delta? N(c) ? logN(c)
N ?(c)? N(c)?N(w)
delta? delta?N ?(c) ? logN ?(c)
if not a tentative move then
N(c)? N ?(c)
forall v ? suc(w) do
delta? delta?N(v, c) ? logN(v, c)
N ?(v, c)? N(v, c)?N(v, w)
delta? delta+N ?(v, c) ? logN ?(v, c)
if not a tentative move then
N(v, c)? N ?(v, c)
return delta
Procedure MoveWord
5 Distributed Clustering
When training on large corpora, even the modified
exchange algorithm would still require several days
if not weeks of CPU time for a sufficient number of
iterations.
To overcome this we introduce a novel distributed
exchange algorithm, based on the modified exchange
algorithm described in the previous section. The vo-
cabulary is randomly partitioned into sets of roughly
equal size. With each word w in one of these sets, all
words v preceding w in the corpus are stored with
the respective bigram count N(v, w).
The clusterings generated in each iteration as well
as the initial clustering are stored as the set of words
in each cluster, the total number of occurrences of
each cluster in the training corpus, and the list of
words preceeding each cluster. For each word w in
the predecessor list of a given cluster c, the number
of times w occurs in the training corpus before any
word in c, N(w, c), is also stored.
Together with the counts stored with the vocab-
ulary partitions, this allows for efficient updating of
the terms in Eq. (10).
The initial clustering together with all the required
counts is created in an initial iteration by assigning
the n-th most frequent word to cluster n mod Nc.
While (Martin et al, 1998) and (Emami and Je-
linek, 2005) observe that the initial clustering does
not seem to have a noticeable effect on the quality
of the resulting clustering or the convergence rate,
the intuition behind this method of initialization is
that it is unlikely for the most frequent words to be
clustered together due to their high numbers of oc-
currences.
In each subsequent iteration each one of a num-
ber of workers is assigned one of the partitions of
the words in the vocabulary. After loading the cur-
rent clustering, it then randomly chooses a subset
of these words of a fixed size. For each of the se-
lected words the worker then determines to which
cluster the word is to be moved in order to maxi-
mize the increase in log likelihood, using the count
updating procedures described in the previous sec-
tion. All changes a worker makes to the clustering
are accumulated locally in delta data structures. At
the end of the iteration all deltas are merged and
applied to the previous clustering, resulting in the
complete clustering loaded in the next iteration.
This algorithm fits well into the MapReduce pro-
gramming model (Dean and Ghemawat, 2004) that
we used for our implementation.
5.1 Convergence
While the greedy non-distributed exchange algo-
rithm is guaranteed to converge as each exchange
increases the log likelihood of the assumed bigram
model, this is not necessarily true for the distributed
exchange algorithm. This stems from the fact that
the change in log likelihood is calculated by each
worker under the assumption that no other changes
to the clustering are performed by other workers in
758
this iteration. However, if in each iteration only a
rather small and randomly chosen subset of all words
are considered for exchange, the intuition is that the
remaining words still define the parameters of each
cluster well enough for the algorithm to converge.
In (Emami and Jelinek, 2005) the authors observe
that only considering a subset of the vocabulary of
half the size of the complete vocabulary in each it-
eration does not affect the time required by the ex-
change algorithm to converge. Yet each iteration is
sped up by approximately a factor of two. The qual-
ity of class-based models trained using the result-
ing clusterings did not differ noticeably from those
trained using clusterings for which the full vocabu-
lary was considered in each iteration. Our experi-
ments showed that this also seems to be the case for
the distributed exchange algorithm. While consider-
ing very large subsets of the vocabulary in each iter-
ation can cause the algorithm to not converge at all,
considering only a very small fraction of the words
for exchange will increase the number of iterations
required to converge. In experiments we empirically
determined that choosing a subset of roughly a third
of the size of the full vocabulary is a good balance in
this trade-off. We did not observe the algorithm to
not converge unless we used fractions above half of
the vocabulary size.
We typically ran the clustering for 20 to 30 itera-
tions after which the number of words exchanged in
each iteration starts to stabilize at less than 5 per-
cent of the vocabulary size. Figure 1 shows the num-
ber of words exchanged in each of 34 iterations when
clustering the approximately 300,000 word vocabu-
lary of the Arabic side of the English-Arabic parallel
training data into 512 and 2,048 clusters.
Despite a steady reduction in the number of words
exchanged per iteration, we observed the conver-
gence in regards to log-likelihood to be far from
monotone. In our experiments we were able to
achieve significantly more monotone and faster con-
vergence by employing the following heuristic. As
described in Section 5, we start out the first itera-
tion with a random partition of the vocabulary into
subsets each assigned to a specific worker. However,
instead of keeping this assignment constant through-
out all iterations, after each iteration the vocabu-
lary is partitioned anew so that all words from any
given cluster are considered by the same worker in
the next iteration. The intuition behind this heuris-
tic is that as the clustering becomes more coherent,
the information each worker has about groups of sim-
ilar words is becoming increasingly accurate. In our
experiments this heuristic lead to almost monotone
convergence in log-likelihood. It also reduced the
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 100000
 0  5  10  15  20  25  30  35
w
o
r
d
s
 
e
x
c
h
a
n
g
e
d
iteration
512 clusters2048 clusters
Figure 1: Number of words exchanged per iteration
when clustering the vocabulary of the Arabic side of
the English-Arabic parallel training data (347 million to-
kens).
number of iterations required to converge by up to a
factor of three.
5.2 Resource Requirements
The runtime of the distributed exchange algorithm
depends highly on the number of distinct bigrams in
the training corpus. When clustering the approxi-
mately 1.5 million word vocabulary of a 405 million
token English corpus into 1,000 clusters, one itera-
tion takes approximately 5 minutes using 50 workers
based on standard hardware running the Linux oper-
ating system. When clustering the 0.5 million most
frequent words in the vocabulary of an English cor-
pus with 31 billion tokens into 1,000 clusters, one it-
eration takes approximately 30 minutes on 200 work-
ers.
When scaling up the vocabulary and corpus sizes,
the current bottleneck of our implementation is load-
ing the current clustering into memory. While the
memory requirements decrease with each iteration,
during the first few iterations a worker typically still
needs approximately 2 GB of memory to load the
clustering generated in the previous iteration when
training 1,000 clusters on the 31 billion token corpus.
6 Experiments
We trained a number of predictive class-based lan-
guage models on different Arabic and English cor-
pora using clusterings trained on the complete data
of the same corpus. We use the distributed training
and application infrastructure described in (Brants
et al, 2007) with modifications to allow the training
of predictive class-based models and their application
in the decoder of the machine translation system.
759
For all models used in our experiments, both word-
and class-based, the smoothing method used was
Stupid Backoff (Brants et al, 2007). Models with
Stupid Backoff return scores rather than normalized
probabilities, thus perplexities cannot be calculated
for these models. Instead we report BLEU scores
(Papineni et al, 2002) of the machine translation sys-
tem using different combinations of word- and class-
based models for translation tasks from English to
Arabic and Arabic to English.
6.1 Training Data
For English we used three different training data sets:
en target: The English side of Arabic-English and
Chinese-English parallel data provided by LDC (405
million tokens).
en ldcnews: Consists of several English news data
sets provided by LDC (5 billion tokens).
en webnews: Consists of data collected up to De-
cember 2005 from web pages containing primarily
English news articles (31 billion tokens).
A fourth data set, en web, was used together with
the other three data sets to train the large word-
based model used in the second machine translation
experiment. This set consists of general web data
collected in January 2006 (2 trillion tokens).
For Arabic we used the following two different
training data sets:
ar gigaword: Consists of several Arabic news data
sets provided by LDC (629 million tokens).
ar webnews: Consists of data collected up to
December 2005 from web pages containing primarily
Arabic news articles (approximately 600 million
tokens).
6.2 Machine Translation Results
Given a sentence f in the source language, the ma-
chine translation problem is to automatically pro-
duce a translation e? in the target language. In the
subsequent experiments, we use a phrase-based sta-
tistical machine translation system based on the log-
linear formulation of the problem described in (Och
and Ney, 2002):
e? = argmax
e
p(e|f)
= argmax
e
M?
m=1
?mhm(e, f) (11)
where {hm(e, f)} is a set of M feature functions and
{?m} a set of weights. We use each predictive class-
based language model as well as a word-based model
as separate feature functions in the log-linear com-
bination in Eq. (11). The weights are trained using
minimum error rate training (Och, 2003) with BLEU
score as the objective function.
The dev and test data sets contain parts of the
2003, 2004 and 2005 Arabic NIST MT evaluation
sets among other parallel data. The blind test data
used is the ?NIST? part of the 2006 Arabic-English
NIST MT evaluation set, and is not included in the
training data.
For the first experiment we trained predictive
class-based 5-gram models using clusterings with 64,
128, 256 and 512 clusters1 on the en target data. We
then added these models as additional features to
the log linear model of the Arabic-English machine
translation system. The word-based language model
used by the system in these experiments is a 5-gram
model also trained on the en target data set.
Table 1 shows the BLEU scores reached by the
translation system when combining the different
class-based models with the word-based model in
comparison to the BLEU scores by a system using
only the word-based model on the Arabic-English
translation task.
dev test nist06
word-based only 0.4085 0.3498 0.5088
64 clusters 0.4122 0.3514 0.5114
128 clusters 0.4142 0.3530 0.5109
256 clusters 0.4141 0.3536 0.5076
512 clusters 0.4120 0.3504 0.5140
Table 1: BLEU scores of the Arabic English system using
models trained on the English en target data set
Adding the class-based models leads to small im-
provements in BLEU score, with the highest im-
provements for both dev and nist06 being statisti-
cally significant 2.
In the next experiment we used two predictive
class-based models, a 5-gram model with 512 clusters
trained on the en target data set and a 6-gram model
also using 512 clusters trained on the en ldcnews
data set. We used these models in addition to
a word-based 6-gram model created by combining
models trained on all four English data sets.
Table 2 shows the BLEU scores of the machine
translation system using only this word-based model,
the scores after adding the class-based model trained
on the en target data set and when using all three
models.
1The beginning of sentence, end of sentence and unkown
word tokens were each treated as separate clusters
2Differences of more than 0.0051 are statistically significant
at the 0.05 level using bootstrap resampling (Noreen, 1989;
Koehn, 2004)
760
dev test nist06
word-based only 0.4677 0.4007 0.5672
with en target 0.4682 0.4022 0.5707
all three models 0.4690 0.4059 0.5748
Table 2: BLEU scores of the Arabic English system using
models trained on various data sets
For our experiment with the English Arabic trans-
lation task we trained two 5 -gram predictive class-
based models with 512 clusters on the Arabic
ar gigaword and ar webnews data sets. The word-
based Arabic 5-gram model we used was created
by combining models trained on the Arabic side of
the parallel training data (347 million tokens), the
ar gigaword and ar webnews data sets, and addi-
tional Arabic web data.
dev test nist06
word-based only 0.2207 0.2174 0.3033
with ar webnews 0.2237 0.2136 0.3045
all three models 0.2257 0.2260 0.3318
Table 3: BLEU scores of the English Arabic system using
models trained on various data sets
As shown in Table 3, adding the predictive class-
based model trained on the ar webnews data set
leads to small improvements in dev and nist06
scores but causes the test score to decrease. How-
ever, adding the class-based model trained on the
ar gigaword data set to the other class-based and the
word-based model results in further improvement of
the dev score, but also in large improvements of the
test and nist06 scores.
We performed experiments to eliminate the pos-
sibility of data overlap between the training data
and the machine translation test data as cause for
the large improvements. In addition, our experi-
ments showed that when there is overlap between
the training and test data, the class-based models
lead to lower scores as long as they are trained only
on data also used for training the word-based model.
One explanation could be that the domain of the
ar gigaword corpus is much closer to the domain of
the test data than that of other training data sets
used. However, further investigation is required to
explain the improvements.
6.3 Clusters
The clusters produced by the distributed algorithm
vary in their size and number of occurrences. In
a clustering of the en target data set with 1,024
clusters, the cluster sizes follow a typical long-
tailed distribution with the smallest cluster contain-
Bai Bi Bu Cai Cao Chang Chen Cheng Chou Chuang Cui Dai
Deng Ding Du Duan Fan Fu Gao Ge Geng Gong Gu Guan
Han Hou Hsiao Hsieh Hsu Hu Huang Huo Jiang Jiao Juan
Kang Kuang Kuo Li Liang Liao Lin Liu Lu Luo Mao Meets
Meng Mi Miao Mu Niu Pang Pi Pu Qian Qiao Qiu Qu Ren
Run Shan Shang Shen Si Song Su Sui Sun Tan Tang Tian Tu
Wang Wu Xie Xiong Xu Yang Yao Ye Yin Zeng Zhang Zhao
Zheng Zhou Zhu Zhuang Zou
% PERCENT cents percent
approvals bonus cash concessions cooperatives credit disburse-
ments dividends donations earnings emoluments entitlements
expenditure expenditures fund funding funds grants income
incomes inflation lending liquidity loan loans mortgage mort-
gages overhead payroll pension pensions portfolio profits pro-
tectionism quotas receipts receivables remittances remunera-
tion rent rents returns revenue revenues salaries salary savings
spending subscription subsidies subsidy surplus surpluses tax
taxation taxes tonnage tuition turnover wage wages
Abby Abigail Agnes Alexandra Alice Amanda Amy Andrea
Angela Ann Anna Anne Annette Becky Beth Betsy Bonnie
Brenda Carla Carol Carole Caroline Carolyn Carrie Catherine
Cathy Cheryl Christina Christine Cindy Claire Clare Claudia
Colleen Cristina Cynthia Danielle Daphne Dawn Debbie Deb-
orah Denise Diane Dina Dolores Donna Doris Edna Eileen
Elaine Eleanor Elena Elisabeth Ellen Emily Erica Erin Esther
Evelyn Felicia Felicity Flora Frances Gail Gertrude Gillian
Gina Ginger Gladys Gloria Gwen Harriet Heather Helen Hi-
lary Irene Isabel Jane Janice Jeanne Jennifer Jenny Jessica
Jo Joan Joanna Joanne Jodie Josie Judith Judy Julia Julie
Karen Kate Katherine Kathleen Kathryn Kathy Katie Kim-
berly Kirsten Kristen Kristin Laura Laurie Leah Lena Lil-
lian Linda Lisa Liz Liza Lois Loretta Lori Lorraine Louise
Lynne Marcia Margaret Maria Marian Marianne Marilyn Mar-
jorie Marsha Mary Maureen Meg Melanie Melinda Melissa
Merle Michele Michelle Miriam Molly Nan Nancy Naomi Na-
talie Nina Nora Norma Olivia Pam Pamela Patricia Patti
Paula Pauline Peggy Phyllis Rachel Rebecca Regina Renee
Rita Roberta Rosemary Sabrina Sally Samantha Sarah Selena
Sheila Shelley Sherry Shirley Sonia Stacy Stephanie Sue Su-
sanne Suzanne Suzy Sylvia Tammy Teresa Teri Terri Theresa
Tina Toni Tracey Ursula Valerie Vanessa Veronica Vicki Vi-
vian Wendy Yolanda Yvonne
almonds apple apples asparagus avocado bacon bananas bar-
ley basil bean beans beets berries berry boneless broccoli
cabbage carrot carrots celery cherries cherry chile chiles chili
chilies chives cilantro citrus cranberries cranberry cucumber
cucumbers dill doughnuts egg eggplant eggs elk evergreen fen-
nel figs flowers fruit fruits garlic ginger grapefruit grasses herb
herbs jalapeno Jell-O lemon lemons lettuce lime lions mac-
aroni mango maple melon mint mozzarella mushrooms oak
oaks olives onion onions orange oranges orchids oregano oys-
ter parsley pasta pastries pea peach peaches peanuts pear
pears peas pecan pecans perennials pickles pine pineapple
pines plum pumpkin pumpkins raspberries raspberry rice rose-
mary roses sage salsa scallions scallops seasonings seaweed
shallots shrimp shrubs spaghetti spices spinach strawberries
strawberry thyme tomato tomatoes truffles tulips turtles wal-
nut walnuts watermelon wildflowers zucchini
mid-April mid-August mid-December mid-February mid-
January mid-July mid-June mid-March mid-May mid-
November mid-October mid-September mid-afternoon
midafternoon midmorning midsummer
Table 4: Examples of clusters
761
ing 13 words and the largest cluster containing 20,396
words. Table 4 shows some examples of the gener-
ated clusters. For each cluster we list all words oc-
curring more than 1,000 times in the corpus.
7 Conclusion
In this paper, we have introduced an efficient, dis-
tributed clustering algorithm for obtaining word clas-
sifications for predictive class-based language models
with which we were able to use billions of tokens of
training data to obtain classifications for millions of
words in relatively short amounts of time.
The experiments presented show that predictive
class-based models trained using the obtained word
classifications can improve the quality of a state-of-
the-art machine translation system as indicated by
the BLEU score in both translation tasks. When
using predictive class-based models in combination
with a word-based language model trained on very
large amounts of data, the improvements continue to
be statistically significant on the test and nist06 sets.
We conclude that even despite the large amounts of
data used to train the large word-based model in
our second experiment, class-based language models
are still an effective tool to ease the effects of data
sparsity.
We furthermore expect to be able to increase the
gains resulting from using class-based models by
using more sophisticated techniques for combining
them with word-based models such as linear inter-
polations of word- and class-based models with coef-
ficients depending on the frequency of the history.
Another interesting direction of further research is
to evaluate the use of the presented clustering tech-
nique for language model size reduction.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language models
in machine translation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing and on Computational Natural Language
Learning (EMNLP-CoNLL), pages 858?867, Prague,
Czech Republic.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de
Souza, Jennifer C. Lai, and Robert L. Mercer. 1990.
Class-based n-gram models of natural language. Com-
putational Linguistics, 18(4):467?479.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified data processing on large clusters. In Pro-
ceedings of the Sixth Symposium on Operating System
Design and Implementation (OSDI-04), San Francisco,
CA, USA.
Ahmad Emami and Frederick Jelinek. 2005. Ran-
dom clusterings for language modeling. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), Philadelphia,
PA, USA.
Joshua Goodman and Jianfeng Gao. 2000. Language
model size reduction by pruning and clustering. In
Proceedings of the IEEE International Conference on
Spoken Language Processing (ICSLP), Beijing, China.
Joshua Goodman. 2000. A bit of progress in language
modeling. Technical report, Microsoft Research.
Reinherd Kneser and Hermann Ney. 1993. Improved
clustering techniques for class-based statistical lan-
guage modelling. In Proceedings of the 3rd European
Conference on Speech Communication and Technology,
pages 973?976, Berlin, Germany.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing (EMNLP), Barcelona, Spain.
Sven Martin, Jo?rg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
Speech Communication, 24:19?37.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons, New York.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 295?302, Philadelphia, PA,
USA.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 311?318, Philadelphia,
PA, USA.
Martin Raab. 2006. Language model techniques in ma-
chine translation. Master?s thesis, Universita?t Karl-
sruhe / Carnegie Mellon University.
E. W. D. Whittaker and P. C. Woodland. 2001. Effi-
cient class-based language modelling for very large vo-
cabularies. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 545?548, Salt Lake City, UT, USA.
762
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 141?148, New York City, June 2006. c?2006 Association for Computational Linguistics
A Context Pattern Induction Method for Named Entity Extraction
Partha Pratim Talukdar
CIS Department
University of Pennsylvania
Philadelphia, PA 19104
partha@cis.upenn.edu
Thorsten Brants
Google, Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043
brants@google.com
Mark Liberman Fernando Pereira
CIS Department
University of Pennsylvania
Philadelphia, PA 19104
{myl,pereira}@cis.upenn.edu
Abstract
We present a novel context pattern in-
duction method for information extrac-
tion, specifically named entity extraction.
Using this method, we extended several
classes of seed entity lists into much larger
high-precision lists. Using token member-
ship in these extended lists as additional
features, we improved the accuracy of a
conditional random field-based named en-
tity tagger. In contrast, features derived
from the seed lists decreased extractor ac-
curacy.
1 Introduction
Partial entity lists and massive amounts of unla-
beled data are becoming available with the growth
of the Web as well as the increased availability of
specialized corpora and entity lists. For example,
the primary public resource for biomedical research,
MEDLINE, contains over 13 million entries and is
growing at an accelerating rate. Combined with
these large corpora, the recent availability of entity
lists in those domains has opened up interesting op-
portunities and challenges. Such lists are never com-
plete and suffer from sampling biases, but we would
like to exploit them, in combination with large un-
labeled corpora, to speed up the creation of infor-
mation extraction systems for different domains and
languages. In this paper, we concentrate on explor-
ing utility of such resources for named entity extrac-
tion.
Currently available entity lists contain a small
fraction of named entities, but there are orders of
magnitude more present in the unlabeled data1. In
this paper, we test the following hypotheses:
i. Starting with a few seed entities, it is possible
to induce high-precision context patterns by ex-
ploiting entity context redundancy.
ii. New entity instances of the same category can
be extracted from unlabeled data with the in-
duced patterns to create high-precision exten-
sions of the seed lists.
iii. Features derived from token membership in the
extended lists improve the accuracy of learned
named-entity taggers.
Previous approaches to context pattern induc-
tion were described by Riloff and Jones (1999),
Agichtein and Gravano (2000), Thelen and Riloff
(2002), Lin et al (2003), and Etzioni et al (2005),
among others. The main advance in the present
method is the combination of grammatical induction
and statistical techniques to create high-precision
patterns.
The paper is organized as follows. Section 2 de-
scribes our pattern induction algorithm. Section 3
shows how to extend seed sets with entities extracted
by the patterns from unlabeled data. Section 4 gives
experimental results, and Section 5 compares our
method with previous work.
1For example, based on approximate matching, there is an
overlap of only 22 organizations between the 2403 organiza-
tions present in CoNLL-2003 shared task training data and the
Fortune-500 list.
141
2 Context Pattern Induction
The overall method for inducing entity context pat-
terns and extending entity lists is as follows:
1. Let E = seed set, T = text corpus.
2. Find the contexts C of entities in E in the cor-
pus T (Section 2.1).
3. Select trigger words from C (Section 2.2).
4. For each trigger word, induce a pattern automa-
ton (Section 2.3).
5. Use induced patterns P to extract more entities
E? (Section 3).
6. Rank P and E? (Section 3.1).
7. If needed, add high scoring entities in E? to E
and return to step 2. Otherwise, terminate with
patterns P and extended entity list E ? E? as
results.
2.1 Extracting Context
Starting with the seed list, we first find occurrences
of seed entities in the unlabeled data. For each such
occurrence, we extract a fixed number W (context
window size) of tokens immediately preceding and
immediately following the matched entity. As we
are only interested in modeling the context here, we
replace all entity tokens by the single token -ENT-.
This token now represents a slot in which an entity
can occur. Examples of extracted entity contexts are
shown in Table 1. In the work presented in this pa-
pers, seeds are entity instances (e.g. Google is a seed
for organization category).
increased expression of -ENT- in vad mice
the expression of -ENT- mrna was greater
expression of the -ENT- gene in mouse
Table 1: Extracted contexts of known genes with
W = 3.
The set of extracted contexts is denoted by C . The
next step is to automatically induce high-precision
patterns containing the token -ENT- from such ex-
tracted contexts.
2.2 Trigger Word Selection
To induce patterns, we need to determine their starts.
It is reasonable to assume that some tokens are more
specific to particular entity classes than others. For
example, in the examples shown above, expression
can be one such word for gene names. Whenever
one comes across such a token in text, the proba-
bility of finding an entity (of the corresponding en-
tity class) in its vicinity is high. We call such start-
ing tokens trigger words. Trigger words mark the
beginning of a pattern. It is important to note that
simply selecting the first token of extracted contexts
may not be a good way to select trigger words. In
such a scheme, we would have to vary W to search
for useful pattern starts. Instead of that brute-force
technique, we propose an automatic way of select-
ing trigger words. A good set of trigger words is
very important for the quality of induced patterns.
Ideally, we want a trigger word to satisfy the follow-
ing:
? It is frequent in the set C of extracted contexts.
? It is specific to entities of interest and thereby
to extracted contexts.
We use a term-weighting method to rank candi-
date trigger words from entity contexts. IDF (In-
verse Document Frequency) was used in our experi-
ments but any other suitable term-weighting scheme
may work comparably. The IDF weight fw for a
word w occurring in a corpus is given by:
fw = log
( N
nw
)
where N is the total number of documents in the
corpus and nw is the total number of documents con-
taining w. Now, for each context segment c ? C , we
select a dominating word dc given by
dc = argmaxw?c fw
There is exactly one dominating word for each
c ? C . All dominating words for contexts in C form
multiset M . Let mw be the multiplicity of the dom-
inating word w in M . We sort M by decreasing mw
and select the top n tokens from this list as potential
trigger words.
142
Selection criteria based on dominating word fre-
quency work better than criteria based on simple
term weight because high term weight words may
be rare in the extracted contexts, but would still be
misleadingly selected for pattern induction. This can
be avoided by using instead the frequency of domi-
nating words within contexts, as we did here.
2.3 Automata Induction
Rather than using individual contexts directly, we
summarize them into automata that contain the most
significant regularities of the contexts sharing a
given trigger word. This construction allows us to
determine the relative importance of different con-
text features using a variant of the forward-backward
algorithm from HMMs.
2.3.1 Initial Induction
For each trigger word, we list the contexts start-
ing with the word. For example, with ?expression?
as the trigger word, the contexts in Table 1 are re-
duced to those in Table 2. Since ?expression? is a
left-context trigger word, only one token to the right
of -ENT- is retained. Here, the predictive context
lies to the left of the slot -ENT- and a single to-
ken is retained on the right to mark the slot?s right
boundary. To model predictive right contexts, the to-
ken string can be reversed and the same techniques
as here applied on the reversed string.2
expression of -ENT- in
expression of -ENT- mrna
expression of the -ENT- gene
Table 2: Context segments corresponding to trigger
word ?expression?.
Similar contexts are prepared for each trigger
word. The context set for each trigger word is then
summarized by a pattern automaton with transitions
that match the trigger word and also the wildcard
-ENT- . We expect such automata to model the po-
sition in context of the entity slot and help us extract
more entities of the same class with high precision.
2Experiments reported in this paper use predictive left con-
text only.
10
11
12
of
of
of
the
the
a
...
...
a
Figure 1: Fragment of a 1-reversible automaton
We use a simple form of grammar induction to
learn the pattern automata. Grammar induction tech-
niques have been previously explored for informa-
tion extraction (IE) and related tasks. For instance,
Freitag (1997) used grammatical inference to im-
prove precision in IE tasks.
Context segments are short and typically do not
involve recursive structures. Therefore, we chose to
use 1-reversible automata to represent sets of con-
texts. An automaton A is k-reversible iff (1) A is
deterministic and (2) Ar is deterministic with k to-
kens of lookahead, where Ar is the automaton ob-
tained by reversing the transitions of A. Wrapper in-
duction using k-reversible grammar is discussed by
Chidlovskii (2000).
In the 1-reversible automaton induced for each
trigger word, all transitions labeled by a given token
go to the same state, which is identified with that
token. Figure 1 shows a fragment of a 1-reversible
automaton. Solan et al (2005) describe a similar au-
tomaton construction, but they allow multiple transi-
tions between states to distinguish among sentences.
Each transition e = (v,w) in a 1-reversible au-
tomaton A corresponds to a bigram vw in the con-
texts used to create A. We thus assign each transition
the probability
P (w|v) = C(v,w)?w?C(v,w?)
where C(v,w) is the number of occurrences of the
bigram vw in contexts for W . With this construc-
tion, we ensure words will be credited in proportion
to their frequency in contexts. The automaton may
overgenerate, but that potentially helps generaliza-
tion.
143
2.3.2 Pruning
The initially induced automata need to be pruned
to remove transitions with weak evidence so as to
increase match precision.
The simplest pruning method is to set a count
threshold c below which transitions are removed.
However, this is a poor method. Consider state 10 in
the automaton of Figure 2, with c = 20. Transitions
(10, 11) and (10, 12) will be pruned. C(10, 12)  c
but C(10, 11) just falls short of c. However, from
the transition counts, it looks like the sequence ?the
-ENT-? is very common. In such a case, it is not
desirable to prune (10, 11). Using a local threshold
may lead to overpruning.
We would like instead to keep transitions that are
used in relatively many probable paths through the
automaton. The probability of path p is P (p) =
?
(v,w)?p P (w|v). Then the posterior probability of
edge (v,w) is
P (v,w) =
?
(v,w)?p P (p)
?
p P (p)
,
which can be efficiently computed by the forward-
backward algorithm (Rabiner, 1989). We can now
remove transitions leaving state v whose posterior
probability is lower than pv = k(maxw P (v,w)),
where 0 < k ? 1 controls the degree of pruning,
with higher k forcing more pruning. All induced and
pruned automata are trimmed to remove unreachable
states.
10
11
12
of
of
of
the
the
an
 (98)
13a
an
... (40)
... (7)
(5)
(80)
(18)
(40)(20)
(20)
(20)
(2)
-ENT-
Figure 2: Automaton to be pruned at state 10. Tran-
sition counts are shown in parenthesis.
3 Automata as Extractor
Each automaton induced using the method described
in Sections 2.3-2.3.2 represents high-precision pat-
terns that start with a given trigger word. By scan-
ning unlabeled data using these patterns, we can ex-
tract text segments which can be substituted for the
slot token -ENT-. For example, assume that the in-
duced pattern is ?analyst at -ENT- and? and that
the scanned text is ?He is an analyst at the Univer-
sity of California and ...?. By scanning this text us-
ing the pattern mentioned above, we can figure out
that the text ?the University of California? can sub-
stitute for ?-ENT-?. This extracted segment is a
candidate extracted entity. We now need to decide
whether we should retain all tokens inside a candi-
date extraction or purge some tokens, such as ?the?
in the example.
One way to handle this problem is to build a
language model of content tokens and retain only
the maximum likelihood token sequence. However,
in the current work, the following heuristic which
worked well in practice is used. Each token in the
extracted text segment is labeled either keep (K) or
droppable (D). By default, a token is labeled K. A
token is labeled D if it satisfies one of the droppable
criteria. In the experiments reported in this paper,
droppable criteria were whether the token is present
in a stopword list, whether it is non-capitalized, or
whether it is a number.
Once tokens in a candidate extraction are labeled
using the above heuristic, the longest token sequence
corresponding to the regular expression K[D K]?K is
retained and is considered a final extraction. If there
is only one K token, that token is retained as the fi-
nal extraction. In the example above, the tokens are
labeled ?the/D University/K of/D California/K?, and
the extracted entity will be ?University of Califor-
nia?.
To handle run-away extractions, we can set a
domain-dependent hard limit on the number of to-
kens which can be matched with ?-ENT-?. This
stems from the intuition that useful extractions are
not very long. For example, it is rare that a person
name longer than five tokens.
3.1 Ranking Patterns and Entities
Using the method described above, patterns and
the entities extracted by them from unlabeled data
are paired. But both patterns and extractions vary
in quality, so we need a method for ranking both.
Hence, we need to rank both patterns and entities.
This is difficult given that there we have no nega-
144
tive labeled data. Seed entities are the only positive
instances that are available.
Related previous work tried to address this prob-
lem. Agichtein and Gravano (2000) seek to extract
relations, so their pattern evaluation strategy consid-
ers one of the attributes of an extracted tuple as a
key. They judge the tuple as a positive or a negative
match for the pattern depending on whether there are
other extracted values associated with the same key.
Unfortunately, this method is not applicable to entity
extraction.
The pattern evaluation mechanism used here is
similar in spirit to those of Etzioni et al (2005) and
Lin et al (2003). With seeds for multiple classes
available, we consider seed instances of one class
as negative instances for the other classes. A pat-
tern is penalized if it extracts entities which belong
to the seed lists of the other classes. Let pos(p) and
neg(p) be respectively the number of distinct pos-
itive and negative seeds extracted by pattern p. In
contrast to previous work mentioned above, we do
not combine pos(p) and neg(p) to calculate a single
accuracy value. Instead, we discard all patterns p
with positive neg(p) value, as well as patterns whose
total positive seed (distinct) extraction count is less
than certain threshold ?pattern. This scoring is very
conservative. There are several motivations for such
a conservative scoring. First, we are more interested
in precision than recall. We believe that with mas-
sive corpora, large number of entity instances can
be extracted anyway. High accuracy extractions al-
low us to reliably (without any human evaluation)
use extracted entities in subsequent tasks success-
fully (see Section 4.3). Second, in the absence of
sophisticated pattern evaluation schemes (which we
are investigating ? Section 6), we feel it is best to
heavily penalize any pattern that extracts even a sin-
gle negative instance.
Let G be the set of patterns which are retained
by the filtering scheme described above. Also, let
I(e, p) be an indicator function which takes value 1
when entity e is extracted by pattern p and 0 other-
wise. The score of e, S(e), is given by
S(e) = ?p?GI(e, p)
This whole process can be iterated by includ-
ing extracted entities whose score is greater than or
equal to a certain threshold ?entity to the seed list.
4 Experimental Results
For the experiments described below, we used 18
billion tokens (31 million documents) of news data
as the source of unlabeled data. We experimented
with 500 and 1000 trigger words. The results pre-
sented were obtained after a single iteration of the
Context Pattern Induction algorithm (Section 2).
4.1 English LOC, ORG and PER
For this experiment, we used as seed sets subsets of
the entity lists provided with CoNLL-2003 shared
task data.3 Only multi-token entries were included
in the seed lists of respective categories (location
(LOC), person (PER) & organization (ORG) in this
case). This was done to partially avoid incorrect
context extraction. For example, if the seed entity is
?California?, then the same string present in ?Uni-
versity of California? can be incorrectly considered
as an instance of LOC. A stoplist was used for drop-
ping tokens from candidate extractions, as described
in Section 3. Examples of top ranking induced pat-
terns and extracted entities are shown in Table 9.
Seed list sizes and experimental results are shown
in Table 3. The precision numbers shown in Table 3
were obtained by manually evaluating 100 randomly
selected instances from each of the extended lists.
Category Seed
Size
Patterns
Used
Extended
Size
Precision
LOC 379 29 3001 70%
ORG 1597 276 33369 85%
PER 3616 265 86265 88%
Table 3: Results of LOC, ORG & PER entity list ex-
tension experiment with ?pattern = 10 set manually.
The overlap4 between the induced ORG list and
the Fortune-500 list has 357 organization names,
which is significantly higher than the seed list over-
lap of 22 (see Section 1). This shows that we have
been able to improve coverage considerably.
4.2 Watch Brand Name
A total of 17 watch brand names were used as
seeds. In addition to the pattern scoring scheme
3A few locally available entities in each category were also
added. These seeds are available upon request from the authors.
4Using same matching criteria as in Section 1.
145
of Section 3.1, only patterns containing sequence
?watch? were finally retained. Entities extracted
with ?entity = 2 are shown in Table 5. Extraction
precision is 85.7%.
Corum, Longines, Lorus, Movado, Accutron, Au-
demars Piguet, Cartier, Chopard, Franck Muller,
IWC, Jaeger-LeCoultre, A. Lange & Sohne, Patek
Philippe, Rolex, Ulysse, Nardin, Vacheron Con-
stantin
Table 4: Watch brand name seeds.
Rolex Fossil Swatch
Cartier Tag Heuer Super Bowl
Swiss Chanel SPOT
Movado Tiffany Sekonda
Seiko TechnoMarine Rolexes
Gucci Franck Muller Harry Winston
Patek Philippe Versace Hampton Spirit
Piaget Raymond Weil Girard Perregaux
Omega Guess Frank Mueller
Citizen Croton David Yurman
Armani Audemars Piguet Chopard
DVD DVDs Chinese
Breitling Montres Rolex Armitron
Tourneau CD NFL
Table 5: Extended list of watch brand names after
single iteration of pattern induction algorithm.
This experiment is interesting for several reasons.
First, it shows that the method presented in this pa-
per is effective even with small number of seed in-
stances. From this we conclude that the unambigu-
ous nature of seed instances is much more important
than the size of the seed list. Second, no negative
information was used during pattern ranking in this
experiment. This suggests that for relatively unam-
biguous categories, it is possible to successfully rank
patterns using positive instances only.
4.3 Extended Lists as Features in a Tagger
Supervised models normally outperform unsuper-
vised models in extraction tasks. The downside of
supervised learning is expensive training data. On
the other hand, massive amounts of unlabeled data
are readily available. The goal of semi-supervised
learning to combine the best of both worlds. Recent
research have shown that improvements in super-
vised taggers are possible by including features de-
rived from unlabeled data (Miller et al, 2004; Liang,
2005; Ando and Zhang, 2005). Similarly, automati-
cally generated entity lists can be used as additional
features in a supervised tagger.
System F1 (Precision, Recall)
Florian et al (2003),
best single, no list
89.94 (91.37, 88.56)
Zhang and Johnson
(2003), no list
90.26 (91.00, 89.53)
CRF baseline, no list 89.52 (90.39, 88.66)
Table 6: Baseline comparison on 4 categories (LOC,
ORG, PER, MISC) on Test-a dataset.
For this experiment, we started with a conditional
random field (CRF) (Lafferty et al, 2001) tagger
with a competitive baseline (Table 6). The base-
line tagger was trained5 on the full CoNLL-2003
shared task data. We experimented with the LOC,
ORG and PER lists that were automatically gener-
ated in Section 4.1. In Table 7, we show the accuracy
of the tagger for the entity types for which we had
induced lists. The test conditions are just baseline
features with no list membership, baseline plus seed
list membership features, and baseline plus induced
list membership features. For completeness, we also
show in Table 8 accuracy on the full CoNLL task
(four entity types) without lists, with seed list only,
and with the three induced lists. The seed lists (Sec-
tion 4.1) were prepared from training data itself and
hence with increasing training data size, the model
overfitted as it became completely reliant on these
seed lists. From Tables 7 & 8 we see that incor-
poration of token membership in the extended lists
as additional membership features led to improve-
ments across categories and at all sizes of training
data. This also shows that the extended lists are of
good quality, since the tagger is able to extract useful
evidence from them.
Relatively small sizes of training data pose inter-
esting learning situation and is the case with practi-
cal applications. It is encouraging to observe that the
list features lead to significant improvements in such
cases. Also, as can be seen from Table 7 & 8, these
lists are effective even with mature taggers trained
on large amounts of labeled data.
5Standard orthographic information, such as character n-
grams, capitalization, tokens in immediate context, chunk tags,
and POS were used as features.
146
Training Data Test-a Test-b
(Tokens) No List Seed List Unsup. List No List Seed List Unsup. List
9268 68.16 70.91 72.82 60.30 63.83 65.56
23385 78.36 79.21 81.36 71.44 72.16 75.32
46816 82.08 80.79 83.84 76.44 75.36 79.64
92921 85.34 83.03 87.18 81.32 78.56 83.05
203621 89.71 84.50 91.01 84.03 78.07 85.70
Table 7: CRF tagger F-measure on LOC, ORG, PER extraction.
Training Data Test-a Test-b
(Tokens) No List Seed List Unsup. List No List Seed List Unsup. List
9229 68.27 70.93 72.26 61.03 64.52 65.60
204657 89.52 84.30 90.48 83.17 77.20 84.52
Table 8: CRF tagger F-measure on LOC, ORG, PER and MISC extraction.
5 Related Work
The method presented in this paper is similar in
many respects to some of the previous work on
context pattern induction (Riloff and Jones, 1999;
Agichtein and Gravano, 2000; Lin et al, 2003; Et-
zioni et al, 2005), but there are important differ-
ences. Agichtein and Gravano (2000) focus on rela-
tion extraction while we are interested in entity ex-
traction. Moreover, Agichtein and Gravano (2000)
depend on an entity tagger to initially tag unlabeled
data whereas we do not have such requirement. The
pattern learning methods of Riloff and Jones (1999)
and the generic extraction patterns of Etzioni et al
(2005) use language-specific information (for exam-
ple, chunks). In contrast, the method presented here
is language independent. For instance, the English
pattern induction system presented here was applied
on German data without any change. Also, in the
current method, induced automata compactly repre-
sent all induced patterns. The patterns induced by
Riloff and Jones (1999) extract NPs and that deter-
mines the number of tokens to include in a single
extraction. We avoid using such language dependent
chunk information as the patterns in our case include
right6 boundary tokens thus explicitly specifying the
slot in which an entity can occur. Another interest-
ing deviation here from previous work on context
pattern induction is the fact that on top of extending
6In case of predictive left context.
seed lists at high precision, we have successfully in-
cluded membership in these automatically generated
lexicons as features in a high quality named entity
tagger improving its performance.
6 Conclusion
We have presented a novel language-independent
context pattern induction method. Starting with a
few seed examples, the method induces in an unsu-
pervised way context patterns and extends the seed
list by extracting more instances of the same cat-
egory at fairly high precision from unlabeled data.
We were able to improve a CRF-based high quality
named entity tagger by using membership in these
automatically generated lists as additional features.
Pattern and entity ranking methods need further
investigation. Thorough comparison with previ-
ously proposed methods also needs to be carried out.
Also, it will be interesting to see whether the fea-
tures generated in this paper complement some of
the other methods (Miller et al, 2004; Liang, 2005;
Ando and Zhang, 2005) that also generate features
from unlabeled data.
7 Acknowledgements
We thank the three anonymous reviewers as well as
Wojciech Skut, Vrishali Wagle, Louis Monier, and
Peter Norvig for valuable suggestions. This work is
supported in part by NSF grant EIA-0205448.
147
Induced LOC Patterns
troops in -ENT-to
Cup qualifier against -ENT-in
southern -ENT-town
war - torn -ENT-.
countries including -ENT-.
Bangladesh and -ENT-,
England in -ENT-in
west of -ENT-and
plane crashed in -ENT-.
Cup qualifier against -ENT-,
Extracted LOC Entities
US
United States
Japan
South Africa
China
Pakistan
France
Mexico
Israel
Pacific
Induced PER Patterns
compatriot -ENT-.
compatriot -ENT-in
Rep. -ENT-,
Actor -ENT-is
Sir -ENT-,
Actor -ENT-,
Tiger Woods , -ENT-and
movie starring -ENT-.
compatriot -ENT-and
movie starring -ENT-and
Extracted PER Entities
Tiger Woods
Andre Agassi
Lleyton Hewitt
Ernie Els
Serena Williams
Andy Roddick
Retief Goosen
Vijay Singh
Jennifer Capriati
Roger Federer
Induced ORG Patterns
analyst at -ENT-.
companies such as -ENT-.
analyst with -ENT-in
series against the -ENT-tonight
Today ?s Schaeffer ?s Option Activity Watch features -ENT-(
Cardinals and -ENT-,
sweep of the -ENT-with
joint venture with -ENT-(
rivals -ENT-Inc.
Friday night ?s game against -ENT-.
Extracted ORG Entities
Boston Red Sox
St. Louis Cardinals
Chicago Cubs
Florida Marlins
Montreal Expos
San Francisco Giants
Red Sox
Cleveland Indians
Chicago White Sox
Atlanta Braves
Table 9: Top ranking LOC, PER, ORG induced pattern and extracted entity examples.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM International Con-
ference on Digital Libraries.
Rie Ando and Tong Zhang. 2005. A high-performance
semi-supervised learning method for text chunking. In
Proceedings of ACL-2005. Ann Arbor, USA.
Boris Chidlovskii. 2000. Wrapper generation by k-
reversible grammar induction. ECAI Workshop on
Machine Learning for Information Extraction.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web - an exper-
imental study. Artificial Intelligence Journal.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Proceedings of CoNLL-2003.
Dayne Freitag. 1997. Using grammatical inference to
improve precision in information extraction. In ICML-
97 Workshop on Automata Induction, Grammatical In-
ference, and Language Acquisition, Nashville.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
ICML 2001.
Percy Liang. 2005. Semi-supervised learning for natural
language. MEng. Thesis, MIT.
Winston Lin, Roman Yangarber, and Ralph Grishman.
2003. Bootstrapped learning of semantic classes from
positive and negative examples. In Proceedings of
ICML-2003 Workshop on The Continuum from La-
beled to Unlabeled Data.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proceedings of HLT-NAACL 2004.
L. R. Rabiner. 1989. A tutorial on hidden markov mod-
els and selected applications in speech recognition. In
Proc. of IEEE, 77, 257?286.
Ellen Riloff and Rosie Jones. 1999. Learning Dictio-
naries for Information Extraction by Multi-level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. In Proceedings of National Academy of Sci-
iences. 102:11629-11634.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of EMNLP 2002.
Tong Zhang and David Johnson. 2003. A robust risk
minimization based named entity recognition system.
In Proceedings of CoNLL-2003.
148
