Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1402?1413, Dublin, Ireland, August 23-29 2014.
Deep-Syntactic Parsing
Miguel Ballesteros
1
, Bernd Bohnet
2
, Simon Mille
1
, Leo Wanner
1,3
1
Natural Language Processing Group, Pompeu Fabra University, Barcelona, Spain
2
School of Computer Science, University of Birmingham, United Kingdom
3
Catalan Institute for Research and Advanced Studies (ICREA)
1,3
{name.lastname}@upf.edu
2
bohnetb@cs.bham.ac.uk
Abstract
?Deep-syntactic? dependency structures that capture the argumentative, attributive and coordi-
native relations between full words of a sentence have a great potential for a number of NLP-
applications. The abstraction degree of these structures is in-between the output of a syntactic
dependency parser (connected trees defined over all words of a sentence and language-specific
grammatical functions) and the output of a semantic parser (forests of trees defined over indi-
vidual lexemes or phrasal chunks and abstract semantic role labels which capture the argument
structure of predicative elements, dropping all attributive and coordinative dependencies). We
propose a parser that delivers deep syntactic structures as output.
1 Introduction
Surface-syntactic structures (SSyntSs) as produced by data-driven syntactic dependency parsers are per
force idiosyncratic in that they contain governed prepositions, determiners, support verb constructions
and language-specific grammatical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and
Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simpli-
fication, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the
source and the target structures. Therefore, the use of more abstract ?syntactico-semantic? structures
seems more appropriate. Following Mel??cuk (1988), we call these structures deep-syntactic structures
(DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic
Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to ab-
stract from language-specific grammatical idiosyncrasies. Compared to PropBank and Semantic Frame
stuctures, they have the advantage to be connected and complete, i.e., capture all argumentative, attribu-
tive and coordinative dependencies between the meaningful lexical items of a sentence, while PropBank
and Semantic Frame structures are not always connected, may contain either individual lexical items or
phrasal chunks as nodes, and discard attributive and coordinative relations (be they within the chunks or
sentential). In other words, they constitute incomplete structures that drop not only idiosyncratic, func-
tional but also meaningful elements of a given sentence and often contain dependencies between chunks
rather than individual tokens. Therefore, we propose to put on the research agenda the task of deep-
syntactic parsing and show how a DSyntS is obtained from a SSynt dependency parse using data-driven
tree transduction in a pipeline with a syntactic parser.
1
In Section 2, we introduce SSyntSs and DSyntSs
and discuss the fundamentals of SSyntS?DSyntS transduction. Section 3 describes the experiments that
we carried out on Spanish material, and Section 4 discusses their outcome. Section 5 summarizes the
related work, before in Section 6 some conclusions and plans for future work are presented.
2 Fundamentals of SSyntS?DSyntS transduction
Before we set out to discuss the principles of the SSyntS?DSynt transduction, we must specify the
DSyntSs and SSyntSs as used in our experiments.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The term ?tree transduction? is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension
of finite state transduction (Aho, 1972) to trees.
1402
2.1 Defining SSyntS and DSyntS
SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value
structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels.
The features of the node labels in SSyntSs are lex
ssynt
, and ?syntactic grammemes? of the value of
lex
ssynt
, i.e., number, gender, case, definiteness, person for nouns and tense, aspect, mood and voice for
verbs. The value of lex
ssynt
can be any (either full or functional) lexical item; in graphical representations
of SSyntSs, usually only the value of lex
ssynt
is shown. The edge labels of a SSyntS are grammatical
functions ?subj?, ?dobj?, ?det?, ?modif?, etc. In other words, SSyntSs are syntactic structures of the kind
as encountered in the standard dependency treebanks; cf., e.g., dependency version of the Penn TreeBank
(Johansson and Nugues, 2007) for English, Prague Dependency Treebank for Czech (Haji?c et al., 2006),
Ancora for Spanish (Taul?e et al., 2008), Copenhagen Dependency Treebank for Danish (Buch-Kromann,
2003), etc. In formal terms that we need for the outline of the transduction below, a SSyntS is defined as
follows:
Definition 1 (SSyntS) An SSyntS of a language L is a quintuple T
SS
= ?N,A, ?
l
s
?n
, ?
r
s
?a
, ?
n?g
?
defined over all lexical items L of L, the set of syntactic grammemes G
synt
, and the set of grammatical
functions R
gr
, where
? the set N of nodes and the set A of directed arcs form a connected tree,
? ?
l
s
?n
assigns to each n ? N an l
s
? L,
? ?
r
s
?a
assigns to each a ? A an r ? R
gr
, and
? ?
n?g
assigns to each ?
l
s
?n
(n) a set of grammemes G
t
? G
synt
.
The features of the node labels in DSyntSs as worked with in this paper are lex
dsynt
and ?seman-
tic grammemes? of the value of lex
dsynt
, i.e., number and determination for nouns and tense, aspect,
mood and voice for verbs.
2
In contrast to lex
ssynt
in SSyntS, DSyntS?s lex
dsynt
can be any full, but
not a functional lexeme. In accordance with this restriction, in the case of look after a person, AFTER
will not appear in the corresponding DSyntS; it is a functional (or governed) preposition (so are TO or
BY, in Figure 1).
3
In contrast, AFTER in leave after the meeting is a full lexeme; it will remain in the
DSyntS because there it has its own meaning of ?succession in time?. The edge labels of a DSyntS are
language-independent ?deep-syntactic? relations I,. . . ,VI, ATTR, COORD, APPEND. ?I?,. . . ,?VI? are
argument relations, analogous to A0, A1, etc. in the PropBank annotation. ?ATTR? subsumes all (cir-
cumstantial) ARGM-x PropBank relations as well as the modifier relations not captured by the PropBank
and FrameNet annotations. ?COORD? is the coordinative relation as in: John-COORD?and-II?Mary,
publish-COORD?or-II?perish, and so on. APPEND subsumes all parentheticals, interjections, direct
addresses, etc., as, e.g., in Listen, John!: listen-APPEND?John. DSyntSs thus show a strong similarity
with PropBank structures, with four important differences: (i) their lexical labels are not disambiguated;
(ii) instead of circumstantial thematic roles of the kind ARGM-LOC, ARGM-DIR, etc. they use a unique
ATTR relation; (iii) they capture all existing dependencies between meaningful lexical nodes; and (iv)
they are connected.
4
A number of other annotations have resemblance with DSyntSs; cf. (Ivanova et al.,
2012) for an overview of deep dependency structures. Formally, a DSyntS is defined as follows:
Definition 2 (DSyntS) An DSyntS of a language L is a quintuple T
DS
= ?N,A, ?
l
s
?n
, ?
r
s
?a
, ?
n?g
?
defined over the full lexical items L
d
of L, the set of semantic grammemes G
sem
, and the set of deep-
syntactic relations R
dsynt
, where
? the set N of nodes and the set A of directed arcs form a connected tree,
? ?
l
s
?n
assigns to each n ? N an l
s
? L
d
,
? ?
r
s
?a
assigns to each a ? A an r ? R
dsynt
, and
? ?
n?g
assigns to each ?
l
s
?n
(n) a set of grammemes G
t
? G
sem
.
Consider in Figure 1 an example for an SSyntS and its corresponding DSyntS.
2
Most of the grammemes have a semantic and a surface interpretation; see (Mel??cuk, 2013).
3
Functional lexemes also include auxiliaries (e.g. HAVE, or BE when it is not a copula), and definite and indefinite deter-
miners (THE, A); see Figure 1).
4
Our DSyntSs are thus DSyntSs as used in the Meaning-Text Theory (Mel??cuk, 1988), only that our DSyntSs do not
disambiguate lexical items and do not use lexical functions (Mel??cuk, 1996).
1403
(a) almost 1.2 million jobs have been created by the state thanks to their endeavours
restr
quant
quant
subj
analyt perf
analyt pass
agent
adv
prepos
det
obl obj
prepos
det
(b) almost 1.2 million job create state thanks their endeavour
ATTR
ATTR
ATTR
II
I
ATTR
II
I
Figure 1: An SSyntS (a) and its corresponding DSyntS (b)
2.2 Fleshing out the SSyntS?DSyntS transduction
It is clear that the SSyntS and DSyntS of the same sentence are not isomorphic. The following corre-
spondences between the SSyntS S
ss
and DSyntS S
ds
of a sentence need to be taken into account during
SSyntS?DSyntS transduction:
(i) a node in S
ss
is a node in S
ds
;
(ii) a relation in S
ss
corresponds to a relation in S
ds
;
(iii) a fragment of the S
ss
tree corresponds to a single node in S
ds
;
(iv) a relation with a dependent node in S
ss
is a grammeme in S
ds
;
(v) a grammeme in S
ss
is a grammeme in S
ds
;
(vi) a node in S
ss
is conflated with another node in S
ds
; and
(vii) a node in S
ds
has no correspondence in S
ss
.
The grammeme correspondences (iv) and (v) and the ?pseudo? correspondences in (vi) and (vii)
5
are
few or idiosyncratic and are best handled in a rule-based post-processing stage. The main task of the
SSyntS?DSyntS transducer is thus to cope with the correspondences (i)?(iii). For this purpose, we can
view both SSyntS and DSyntS as vectors indexed in terms of two-dimensional matrices I = N ?N (N
being the set of nodes of a given tree 1, . . . ,m), with I(i, j) = ?(n
i
, n
j
), if n
i
, n
j
? N and (n
i
, n
j
) ? A
and I(i, j) = 0 otherwise (where ??(n
i
, n
j
)? is the function that assigns to an edge a relation label and
i, j = 1, . . . ,m; i 6= j are nodes of the tree). That is, for a given SSyntS, the matrix I(i, j) contains in
the cells (i, j), i, j = 1, . . . ,m, the names of the SSynt-relations between the nodes n
i
and n
j
, and ?0?
otherwise, while for a given DSyntS, the cells of its matrix I
D
contain DSyntS-relations.
Starting from the matrix I
S
of a given SSyntS, the task is therefore to obtain the matrix I
D
of the
corresponding DSyntS, that is, to identify correspondences between i/j, (i, j) and groups of (i, j) of
I
S
with i
?
/j
?
and (i
?
, j
?
) of I
D
; see (i)?(iii) above. In other words, the task consists in identifying and
removing all functional lexemes, and attach correctly the remaining nodes between them.
6
As a ?token chain?surface-syntactic tree? projection, this task can be viewed as a classification task.
However, while the former is isomorphic, we know that the SSyntS?DSyntS projection is not. In order
to approach the task to an isomorphic projection (and thus simplify its modelling), it is convenient to
interpret SSyntS and the targeted DSyntS as collections of hypernodes:
Definition 3 (Hypernode) Given a SSyntS S
s
with its index matrix I
S
(a DSyntS S
d
with its index matrix
I
D
), a node partition p (with |p |? 1) of I
S
(I
D
) is a hypernode h
s
i
(h
d
i
) iff p corresponds to a partition
p
?
(with |p
?
|? 1) of S
d
(S
s
).
In this way, the SSyntS?DSyntS correspondence boils down to a correspondence between individual
hypernodes and between individual arcs, and the transduction embraces the following three (classifica-
tion) subtasks: 1. Hypernode identification, 2. DSynt tree construction, and 3. DSynt arc labeling, which
are completed by a post-processing stage.
5
(vi) covers, e.g., reflexive verb particles such as se in Spanish, which are conflated in the DSyntS with the verb:
se?aux refl dir-conocer vs. CONOCERSE ?know each other?; (vii) covers, e.g., the zero subject in pro-drop languages (which
is absent in the SSyntS and present in the DSyntS).
6
What is particularly challenging is the identification of functional prepositions: based on the information found in the
corpus only, our system must decide if a given preposition is a full or a functional lexeme. That is, we do not resort to any
external lexical resources.
1404
1. Hypernode identification. The hypernode identification consists of a binary classification of the
nodes of a given SSyntS as nodes that form a hypernode of cardinality 1 (i.e., nodes that have a one-
to-one correspondence to a node in the DSyntS) vs. nodes that form part of a hypernode of cardinality
> 1. In practice, hypernodes of type one will be formed by: 1) noun nodes that do not govern determiner
or functional preposition nodes, 2) full verb nodes that are not governed by any auxiliary verb nodes
and that do not govern any functional preposition node, adjective nodes, adverbial nodes, and semantic
preposition nodes. Hypernodes of type two will be formed by: 1) noun nodes + determiner / func-
tional preposition nodes they govern, 2) verb nodes + auxiliary nodes they are governed by + functional
preposition nodes they govern.
2. DSynt tree reconstruction. The outcome of the hypernode identification stage is thus the set H
s
=
H
s
|p|=1
?H
s
|p|>1
of hypernodes of two types. With this set at hand, we can define an isomorphy function
? : H
s
? H
d
|p|=1
(with h
d
? H
d
|p|=1
consisting of n
d
? N
ds
, i.e., the set of nodes of the target DSyntS).
? is the identity function for h
s
? H
s
|p|=1
. For h
s
? H
s
|p|>1
, ? maps the functional nodes in h
s
onto
grammemes (attribute-value pairs) of the lexically meaningful node in h
d
and identifies the lexically
meaningful node as head. Some of the dependencies of the obtained nodes n
d
? N
ds
can be recovered
from the dependencies of their sources. Due to the projection of functional nodes to grammemes (which
can be also seen as node removal), some dependencies will be also missing and must be introduced.
Algorithm 1 recalculates the dependencies for the target DSyntS S
d
, starting from the index matrix I
S
of
SSyntS S
s
to obtain a connected tree.
Algorithm 1: DSyntS tree reconstruction
for ?n
i
? N
d
do
if ?n
j
: (n
j
, n
i
) ? S
s
? ?(n
j
) ? N
d
then
(n
j
, n
i
)? S
d
// the equivalent of the head node of n
i
is included in DSyntS
else if ?n
j
, n
a
: (n
j
, n
i
) ? S
s
? ?(n
j
) 6? N
d
?
?(n
a
) ? N
d
then
//n
a
is the first ancestor of n
j
that has an equivalent in DSyntS
//the equivalent of the head node of n
i
is not included in DSyntS, but the ancestor n
a
is
(n
a
, n
i
)? S
d
else
//the equivalent of the head node of n
i
is not included in DSyntS, but several ancestors of it are
n
b
:= BestHead(n
i
, S
s
, S
d
)
(n
b
, n
i
)? S
d
endfor
BestHead recursively ascends S
s
from a given node n
i
until it encounters one or several head nodes
n
d
? N
ds
. In case of several encountered head nodes, the one which governs the highest frequency
dependency is returned.
3. Label Classification. The tree reconstruction stage produces a ?hybrid? connected dependency tree
S
s?d
with DSynt nodes N
ds
, and arcs A
s
labelled by SSynt relation labels, i.e., an index matrix we
can denote as I
?
, whose cells (i, j) contain SSynt labels for all n
i
, n
j
? N
ds
: (n
i
, n
j
) ? A
s
and
?0? otherwise. The next and last stage of SSynt-to-DSyntS transduction is thus the projection of SSynt
relation labels of S
s?d
to their corresponding DSynt labels, or, in other words, the mapping of I
?
to I
D
of the target DSyntS.
4. Postprocessing. As mentioned in Section 2, there is a limited number of idiosyncratic correspon-
dences between elements of SSyntS and DSyntS (the correspondences (iv?vii) which can be straight-
forwardly handled by a rule-based postprocessor because (a) they are non-ambiguous, i.e., a ? b, c ?
d ? a = b ? c = d, and (b) they are few. Thus, only determiners and auxiliaries in SSyntS map onto a
grammeme in DSyntS, both SSyntS and DSyntS count with less than a dozen grammemes, etc.
3 Experiments
In order to validate the outlined SSyntS?DSyntS transduction and to assess its performance in combi-
nation with a surface dependency parser, i.e., starting from plain sentences, we carried out a number of
1405
experiments in which we implemented the transducer and integrated it into a pipeline shown in Figure 2.
JointPoS TaggerSSynt parser
SSynt?DSyntTransducerPlainSentences
DSyntTreebankSSyntTreebank
SSyntS DSynS
Figure 2: Setup of a deep-syntactic parser
For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et
al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS
treebank the semantically and information structure influenced relation tags to obtain an annotation gran-
ularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)).
Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens
in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and
86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641
tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank).
To obtain the SSyntS, we use Bohnet and Nivre (2012)?s transition-based parser, which combines
lemmatization, PoS tagging, and syntactic dependency parsing?tuned and trained on the respective sets
of the SSyntS treebank. Cf. Table 1 for the performance of the parser on the development set.
POS LEMMA LAS UAS
96.14 91.10 78.64 86.49
Table 1: Results of Bohnet and Nivre?s surface-syntactic parser on the development set
In what follows, we first present the realization of the SSyntS?DSyntS transducer and then the real-
ization of the baseline.
3.1 SSyntS?DSyntS transducer
As outlined in Section 2.2, the SSyntS?DSyntS transducer is composed of three submodules and a post-
processing stage:
1. Hypernode identification. For the hypernode identification, we trained a binary polynomial (degree
2) SVM from LIBSVM (Chang and Lin, 2001). The SVM allows both features related to the processed
node and higher-order features, which can be related to the head node of the processed node or to its
sibling nodes. After several feature selection trials, we chose the following features for each node n:
? lemma or stem of the label of n,
? label of the relation between n and its head,
? surface PoS of n?s label (the SSynt and DSyntS treebanks distinguish between surface and deep
PoS),
? label of the relation between n?s head to its own head,
? surface PoS of the label of n?s head node.
After an optimization round of the parameters available in the SVM implementation, the hypernode
identification achieved over the gold development set 99.78% precision and 99.02% recall (and thus
99.4% F1). That is, only very few hypernodes are not identified correctly. The main error source are
governed prepositions: the classifier has to learn when to assign a preposition an own hypernode (i.e.,
when it is lexically meaningful) and when it should be included into the hypernode of the governor (i.e.,
when it is functional). Our interpretation is that the features we use for this task are appropriate, but
that the training data set is too small. As a result, some prepositions are erroneously left out from or
introduced into the DSyntS.
1406
2. Tree reconstruction. The implementation of the tree reconstruction module shows an unlabelled
dependency attachment precision of 98.18% and an unlabelled dependency attachment recall of 97.43%
over the gold development set. Most of the errors produced by this module have their origin in the
previous module, i.e., hypernode identification. When a node has been incorrectly removed, the module
errs in the attachment because it cannot use the node in question as the destination or the origin of a
dependency, as it is the case in the gold-standard annotation:
Gold-standard: ser como e?ne
be like letter-n
II
II
Predicted: ser e?ne
II
When a node has erroneously not been removed, no dependencies between its governor and its depen-
dent can be established since DSyntS must remain a tree (which gives the same LAS and UAS errors as
when a node has been erroneously removed):
Gold-standard: y Michael Jackson
II
Predicted: y a Michael Jackson
and to Michael Jackson
II
II
3. Relation label classification. For relation label classification, we use a multiclass linear SVM. The
label classification depends on the concrete annotation schemata of the SSyntS and DSyntS treebanks
on which the parser is trained. Depending on the schemata, some DSynt relation labels may be easier to
derive from the original SSyntS relation labels than others. Table 2 lists all SSynt relation labels that have
a straightforward mapping to DSyntS relation labels in the used treebanks, i.e., neither their dependent
nor their governor are removed, and the SSyntS label always maps to the same DSynt label.
SSynt DSynt
abbrev ATTR
abs pred ATTR
adv ATTR
adv mod ATTR
agent I
appos ATTR
attr ATTR
aux phras ?
aux refl dir II
SSynt DSynt
aux refl indir III
bin junct ATTR
compl1 II
compl2 III
compl adnom ATTR
coord COORD
copul II
copul clitic II
copul quot II
SSynt DSynt
dobj clitic II
dobj quot II
elect ATTR
juxtapos APPEND
modal II
modif ATTR
num junct COORD
obj copred ATTR
prepos II
SSynt DSynt
prepos quot II
prolep APPEND
quant ATTR
quasi coord COORD
quasi subj I
relat ATTR
restr ATTR
sequent ATTR
subj I
subj copred ATTR
Table 2: Straightforward SSynt to DSyntS mappings
Table 3 shows SSyntS relation?DSyntS relation label correspondences that are not straightforward.
SSynt DepRel
A
Mapping to DSynt
analyt fut remove Gov and Dep; add tense=FUT
analyt pass remove Gov; invert I and II; add voice=PASS
analyt perf remove Gov; add tense=PAST
analyt progr remove Gov; add tem constituency=PROGR
aux refl lex remove Dep; add se at the end of Gov?s lemma
aux refl pass remove Dep; invert I and II; add voice=PASS
compar remove Dep if conjunction
compar /coord /sub conj remove Dep if governed preposition
det
IF Dep=el?un THEN remove Dep; add definiteness=DEF/INDEF
IF Dep=possessive THEN DepRel ATTR?I?II?III
IF Dep=other THEN DepRel ATTR
dobj remove Dep if governed preposition
iobj remove Dep if governed preposition; DepRel II?III?IV?V?VI
iobj clitic DepRel II?III?IV?V?VI
obl compl remove Dep if governed preposition; DepRel I?II?III?IV?V?VI
obl obj remove Dep if governed preposition; DepRel II?III?IV?V?VI
punc ?
punc init ?
Table 3: Complex SSynt to DSynt mappings
1407
The final set of features selected for label classification includes: (i) lemma of the dependent node, (ii)
dependency relation to the head of the dependent node, (iii) dependency relation label of the head node
to its own head, (iv) dependency relation to the head of the sibling nodes of the dependent node, if any.
After an optimization round of the parameter set of the SVM-model, relation labelling achieved
94.00% label precision and 93.28% label recall on the development set. The recall is calculated con-
sidering all the nodes that are included in the gold standard. The error sources for relation labelling
were mostly the dependencies that involved possessives and the various types of objects (see Table
3) due to their differing valency. For instance, the relation det in su?det?coche ?his/her car? and
su?det?llamada ?his/her phone call? have different correspondences in DSyntS: su?ATTR?coche
vs. su?I?llamada. That is, the DSyntS relation depends on the lexical properties of the governor.
7
Once again, more training data is needed in order to classify better those cases.
4. Postprocessing In the postprocessing stage for Spanish, the following rules capture non-ambiguous
correspondences between elements of the SSynt-index matrix I
S
= N
s
?N
s
and DSyntS index matrix
I
D
= N
d
?N
d
, with n
s
? N
s
and n
d
? N
d
, and n
s
and n
d
corresponding to each other (we do not list
here identity correspondences such as between the number grammemes of n
s
and n
d
):
? if n
s
is dependent of analyt pass or analyt refl pass relation, then the voice grammeme in n
d
is
PASS;
? if n
s
is dependent of analyt progr, then the voice grammeme in n
d
is PROGR;
? if n
s
is dependent of analyt refl lex, then add the particle -SE as suffix of node label (word) of d
d
;
? if any of the children of n
s
is labelled by one of the tokens UN ?a
masc
?, UNA ?a
fem
?, UNOS
?some
masc
? or UNAS ?some
fem
?, then the definiteness grammeme in n
d
INDEF, otherwise it is
DEF;
? if the n
s
label is a finite verb and n
s
does not govern a subject relation, then add to I
?
the relation
n
d
? I?n
?
d
, with n
?
d
being a newly introduced node.
3.2 Baseline
As point of reference for the evaluation of the performance of our SSyntS?DSyntS transducer, we use a
rule-based baseline that carries out the most direct transformations extracted from Tables 2 and 3. The
baseline detects hypernodes by directly removing all the nodes that we are sure need to be removed, i.e.
punctuation and auxiliaries. The nodes that are only potentially to be removed, i.e., all dependents of
DepRels that have a possibly governed preposition or conjunction in Table 3, are left in the DSyntS. The
new relation labels in the DSyntS are obtained by selecting the label that is most likely to substitute the
SSyntS relation label according to classical grammar studies. The rules of the rule-based baseline look
as follows:
1 if (deprel==abbrev) then deep deprel=ATTR
2 if (deprel==obl obj) then deep deprel=II
. . .
n if (deprel==punc) then remove(current node)
4 Results and Discussion
Let us look in this section at the performance figures of the SSyntS parser, the SSyntS?DSyntS trans-
ducer, and the sentence?DSyntS pipeline obtained in the experiments.
4.1 SSyntS?DSyntS transducer results
In Table 4, the performance of the subtasks of the SSyntS?DSyntS transducer is contrasted to the per-
formance of the baselines; the evaluation of the postprocessing subtask is not included because the one-
to-one projection of SSyntS elements to DSyntS guarantees an accuracy of 100% of the operations
performed. The transducer has been applied to the gold standard test set, which is the held-out test set,
with gold standard PoS tags, lemmas and dependency trees. It outputs in total 5610 nodes; the rule-based
baseline outputs 8653 nodes. As mentioned in Section 3, our gold standard includes 5641 nodes.
7
Note that lexemes are not generalized: a verb and its corresponding noun (e.g., construct/construction) are considered
distinct lexemes.
1408
Hyper-Node Detection
Measure Rule-based Baseline Tree Transducer
p 64.31 (5565/8653) 99.79 (5598/5610)
r 98.65 (5565/5641) 99.24 (5598/5641)
F1 77.86 99.51
Attachment and Labelling
Measure Rule-based Baseline Tree Transducer
LAP 50.02 (4328/8653) 91.07 (5109/5610)
UAP 53.05 (4590/8653) 98.32 (5516/5610)
LA-P 57.66 (4989/8653) 92.37 (5182/5610)
LAR 76.72 (4328/5641) 90.57 (5109/5641)
UAR 81.37 (4590/5641) 97.78 (5516/5641)
LA-R 88.44 (4989/5641) 91.86 (5182/5641)
Table 4: Performance of the SSyntS?DSyntS transducer and of the rule-based baseline over the gold-
standard held-out test set (LAP: labelled attachment precision, UAP: unlabelled attachment precision, LA-P: label assign-
ment precision, LAR: labelled attachment recall, UAR: Unlabelled attachment recall and LA-R: Label assignment recall)
Our data-driven SSyntS?DSyntS transducer is much better than the baseline with respect to all eval-
uation measures.
8
The transducer relies on distributional patterns identified in the training data set, and
makes thus use of information that is not available for the rule-based baseline, which studies one node
at a time. However, the rule-based baseline results also show that transduction that would remove a few
nodes would provide results close to a 100% recall for the hypernode detection because a DSynt tree is a
subtree of the SSynt tree (if we ignore the nodes introduced by post-processing). This is also evidenced
by the labeled and attachment recall scores. The results of the transducer on the test and development
sets are quite comparable. The hypernode detection is even better on the test set. The label accuracy
suffers most from using unseen data during the development of the system. The attachment figures are
approximately equivalent on both sets.
4.2 Results of deep-syntactic parsing
Let us consider now the performance of the complete DSynt parsing pipeline (PoS-tagger+surface-
dependency parser? SSyntS?DSyntS transducer) on the held-out test set. Table 5 displays the figures
of the Bohnet and Nivre parser. The figures are in line with the performance of state-of-the-art parsers
for Spanish (Mille et al., 2012).
POS LEMMA LAS UAS
96.05 92.10 81.45 88.09
Table 5: Performance of Bohnet and Nivre?s joint PoS-tagger+dependency parser trained on Ancora-UPF
Table 6 shows the performance of the pipeline when we feed the output of the syntactic parser to the
rule-based baseline SSyntS?DSyntS module and the tree transducer. We observe a clear error propaga-
tion from the dependency parser (which provides 81.45% LAS) to the SSyntS?DSyntS transducer, which
loses in tree quality more than 18%.
Hyper-Node Detection
Measure Baseline Tree Transducer
p 63.87 (5528/8655) 97.07 (5391/5554)
r 98.00 (5528/5641) 95.57 (5391/5641)
F1 77.33 96.31
Labelling and Attachment
Measure Baseline Tree Transducer
LAP 38.75 (3354/8655) 68.31 (3794/5554)
UAP 44.69 (3868/8655) 77.31 (4294/5554)
LA-P 49.66 (4298/8655) 80.47 (4469/5554)
LAR 59.46 (3354/5641) 67.26 (3794/5641)
UAR 68.57 (3868/5641) 76.12 (4294/5641)
LA-R 76.19 (4298/5641) 79.22 (4469/5641)
Table 6: Performance of the deep-syntactic parsing pipeline
5 Related Work
To the best of our knowledge, data-driven deep-syntactic parsing as proposed in this paper is novel. As
semantic role labeling and frame-semantic analysis, it has the goal to obtain more semantically oriented
structures than those delivered by state-of-the-art syntactic parsing. Semantic role labeling received
considerable attention in the CoNLL shared tasks for syntactic dependency parsing in 2006 and 2007
8
We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too
weak to be used as baseline.
1409
(Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and
semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Haji?c et al., 2009).
The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued
with predicate identification, argument identification, argument labeling, and word sense disambigua-
tion; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly
all arguments to select the best combination was applied. Some of the systems were based on integrated
syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu??s et al.,
2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform
structural changes?as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS.
Klime?s (2006)?s parser removes nodes (producing tectogrammatical structures as in the Prague Depen-
dency Treebank), but is based on rules instead of classifiers, as in our case. The same applies to earlier
works in the TAG-framework, as, e.g., in (Rambow and Joshi, 1997).
However, this is not to say that the idea of the surface?surface syntax?deep syntax pipeline is new.
It goes back at least to Curry (1961) and is implemented in a number of more recent works; see, e.g., (de
Groote, 2001; Klime?s, 2006; Bojar et al., 2008).
6 Conclusions and Future Work
We have presented a deep-syntactic parsing pipeline which consists of a state-of-the-art dependency
parser and a novel SSyntS?DSyntS transducer. The obtained DSyntSs can be used in different applica-
tions since they abstract from language-specific grammatical idiosyncrasies of the SSynt structures as
produced by state-of-the art dependency parsers, but still avoid the complexities of genuine semantic
analysis.
9
DSyntS-treebanks needed for data-driven applications can be bootstrapped by the pipeline.
If required, a SSyntS?DSyntS structure pair can be also mapped to a pure predicate-argument graph
such as the DELPH-IN structure (Oepen, 2002) or to an approximation thereof (as the Enju conversion
(Miyao, 2006), which keeps functional nodes), to an DRS (Kamp and Reyle, 1993), or to a PropBank
structure. On the other hand, DSyntS-treebanks can be used for automatic extraction of deep grammars.
As shown by Cahill et al. (2008), automatically obtained resources can be of an even better quality than
manually-crafted resources. In this context, especially research in the context of CCGs (Hockenmeier,
2003; Clark and Curran, 2007) and TAGs (Xia, 1999) should be also mentioned.
To validate our approach with languages other than Spanish, we carried out an experiment on a Chi-
nese SSyntS-DSyntS Treebank (training the DSynt-transducer on the outcome of the SSynt-parser). The
results over predicted input showed an accuracy of about 75%, i.e., an accuracy comparable to the accu-
racy achieved for Spanish. We are also investigating multilingual approaches, such as the one proposed
by McDonald et al. (2013).
In the future, we will carry out further in-depth feature engineering for the task of DSynt-parsing. It
proved to be crucial in semantic role labelling and dependency parsing (Che et al., 2009; Ballesteros and
Nivre, 2012); we expect it be essential for our task as well. Furthermore, we will join surface syntactic
and deep-syntactic parsing we kept so far separate; see, e.g., (Zhang and Clark, 2008; Llu??s et al., 2013;
Bohnet and Nivre, 2012) for analogous proposals. Further research is required here since although joint
models avoid error propagation from the first stage to the second, overall, pipelined models still proved
to be competitive; cf. the outcome of CoNLL shared tasks.
The deep-syntactic parser described in this paper is available for downloading at https://code.
google.com/p/deepsyntacticparsing/.
Acknowledgements
This work has been supported by the European Commission under the contract number FP7-ICT-610411.
Many thanks to the three anonymous COLING reviewers for their very helpful comments and sugges-
tions.
9
The motivation to work with DSyntS instead of SSyntS is thus similiar to the motivation of the authors of the Abstract
Meaning Representation (AMR) for Machine Translation (Banarescu et al., 2013), only that AMRs are considerably more
semantic than DSyntSs.
1410
References
Alfred V. Aho. 1972. The theory of parsing, translation and, compiling. Prentice Hall, Upper Saddle River, NJ.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOptimizer: A System for MaltParser Optimization. In Proceed-
ings of the Eighth International Conference on Language Resources and Evaluation (LREC 12).
L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and
N. Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop & Interoperability with Discourse, pages 178?186, Sofia, Bulgaria.
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In EMNLP-CoNLL.
O. Bojar, S. Cinkov?a, and J. Pt?a?cek. 2008. Towards English-to-Czech MT via Tectogrammatical Layer. The
Prague Bulletin of Mathematical Linguistics, 90:57?68.
Mathias Buch-Kromann. 2003. The Danish dependency treebank and the dtag treebank tool. In 2nd Workshop on
Treebanks and Linguistic Theories (TLT), Sweden, pages 217?220.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceed-
ings of the 10th Conference on Computational Natural Language Learning (CoNLL), pages 149?164.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan Riezler, Josef van Genabith, and Andy Way. 2008. Wide-
coverage deep statistical parsing using automatic dependency structure annotation. Computational Linguistics,
34(1):81?124.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: A Library for Support Vector Machines. Software available
at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL 2009): Shared Task, pages 49?54, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493?552.
R. Curry. 1961. Some logical aspects of grammatical structure. In R. Jakobson, editor, Structure of Language and
Its Mathematical Aspects, pages 56?68. American Mathematical Society, Providence, RI.
Ph. de Groote. 2001. Towards abstract categorial grammar. In Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics (ACL).
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato. 2002. The FrameNet database and software tools. In
Proceedings of the Third International Conference on Language Resources and Evaluation, volume IV, Las
Palmas. LREC, LREC.
A. Gesmundo, J. Henderson, P. Merlo, and I.Titov. 2009. Latent variable model of synchronous syntactic-semantic
parsing for multiple languages. In CoNLL 2009 Shared Task., Conf. on Computational Natural Language
Learning, pages 37?42, Boulder, Colorado, USA.
Jan Haji?c, Jarmila Panevov?a, Eva Haji?cov?a, Petr Sgall, Petr Pajas, Jan
?
St?ep?anek, Ji?r?? Havelka, Marie Mikulov?a,
and Zden
?
k
?
Zabokrtsk?y. 2006. Prague Dependency Treebank 2.0. Linguistic Data Consortium, Philadelphia.
Jan Haji?c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s M`arquez,
Adam Meyers, Joakim Nivre, Sebastian Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu, Nianwen Xue,
and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.
In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared
Task, pages 1?18.
J. Hockenmeier. 2003. Parsing with generative models of predicate-argument structure. In Proceedings of the 41st
Annual Meeting of the Association for Computational Linguistics (ACL), pages 359?366, Sapporo, Japan.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and Dan Flickinger. 2012. Who did what to whom? a contrastive
study of syntacto-semantic dependencies. In Proceedings of the Sixth Linguistic Annotation Workshop, pages
2?11, Jeju, Republic of Korea, July. Association for Computational Linguistics.
1411
R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for english. In J. Nivre, H.-J.
Kaalep, K. Muischnek, and M. Koit, editors, Proceedings of NODALIDA 2007, pages 105?112, Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic?semantic analysis with PropBank and
NomBank. In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning, pages
183?187, Manchester, United Kingdom.
H. Kamp and U. Reyle. 1993. From Discourse to Logic. Kluwer Academic Publishers, Dordrecht, NL.
R.T. Kasper and W.C. Rounds. 1986. A logical semantics for feature structures. In Proceedings of the 24th annual
meeting on Association for Computational Linguistics, pages 257?266.
V?aclav Klime?s. 2006. Analytical and Tectogrammatical Analysis of a Natural Language. Ph.D. thesis, UFAL,
MFF UK, Prague, Czech Republic.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez. 2013. Joint arc-factored parsing of syntactic and semantic
dependencies. Transactions of the Association for Computational Linguistics, pages 219?230.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,
Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu Castell?o, and Jungmee
Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92?97.
Igor Mel??cuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.
Igor Mel??cuk. 1996. Lexical functions: A tool for the description of lexical relations in the lexicon. In L. Wanner,
editor, Lexical functions in lexicography and natural language processing, pages 37?102. Benjamins Academic
Publishers, Amsterdam.
Igor Mel??cuk. 2013. Semantics: From meaning to text, Volume 2. Benjamins Academic Publishers, Amsterdam.
Simon Mille, Alicia Burga, Gabriela Ferraro, and Leo Wanner. 2012. How does the granularity of an annotation
scheme influence dependency parsing performance? In Conference on Computational Linguistics, COLING
2012.
Simon Mille, Alicia Burga, and Leo Wanner. 2013. AnCora-UPF: A Multi-Level Annotation of Spanish . In
Proceedings of the Second International Conference on Dependency Linguistics (DEPLING 2013).
Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-Oriented Grammar Development
and Feature Forest Model. Ph.D. thesis, University of Tokyo.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 Shared Task
on Dependency Parsing. In Proceedings of the CoNLL Shared Task of EMNLP-CoNLL 2007, pages 915?932.
Stephan Oepen. 2002. Collaborative Language Engineering: A Case Study in Efficient Grammar-based Process-
ing. Stanford Univ Center for the Study.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank. Computational Linguistics,
31:71?106.
Owen Rambow and Aravind Joshi. 1997. A formal look at dependency grammar and phrase structure grammars,
with special consideration of word-order phenomena. In L. Wanner, editor, Recent Trends in Meaning-Text
Theory, pages 167?190. Benjamins Academic Publishers, Amsterdam.
W.C. Rounds. 1970. Mappings and grammars on trees. Mathematical Systems Theory, 4(3):257?287.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu??s M`arquez, and Joakim Nivre. 2008. The conll 2008
shared task on joint parsing of syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the
Twelfth Conference on Computational Natural Language Learning, pages 159?177.
M. Taul?e, M. Ant`onia Mart??, and Marta Recasens. 2008. Ancora: Multilevel annotated corpora for Catalan and
Spanish. In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08), Marrakech,
Morocco, may. European Language Resources Association (ELRA).
J.W. Thatcher. 1970. Generalized sequential machine maps. Journal of Computer and System Sciences, 4(4):339?
367.
1412
F. Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of the 5th Natural
Language Processing Pacific Rim Symposium, pages 398?403, Beijing, China.
Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Pro-
ceedings of ACL-08: HLT, pages 888?896, Columbus, Ohio, June. Association for Computational Linguistics.
1413
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 22?30,
Utica, May 2012. c?2012 Association for Computational Linguistics
Towards a Surface Realization-Oriented Corpus Annotation
Leo Wanner
ICREA and
Universitat Pompeu Fabra
Roc Boronat 138
Barcelona, 08018, Spain
leo.wanner@upf.edu
Simon Mille
Universitat Pompeu Fabra
Roc Boronat 138
Barcelona, 08018, Spain
simon.mille@upf.edu
Bernd Bohnet
Universita?t Stuttgart
IMS, Pfaffenwaldring 5b
Stuttgart, 70569, Germany
bohnet@ims.uni-
stuttgart.de
Abstract
Until recently, deep stochastic surface realiza-
tion has been hindered by the lack of seman-
tically annotated corpora. This is about to
change. Such corpora are increasingly avail-
able, e.g., in the context of CoNLL shared
tasks. However, recent experiments with
CoNLL 2009 corpora show that these popu-
lar resources, which serve well for other ap-
plications, may not do so for generation. The
attempts to adapt them for generation resulted
so far in a better performance of the realizers,
but not yet in a genuinely semantic generation-
oriented annotation schema. Our goal is to
initiate a debate on how a generation suit-
able annotation schema should be defined. We
define some general principles of a semantic
generation-oriented annotation and propose an
annotation schema that is based on these prin-
ciples. Experiments shows that making the
semantic corpora comply with the suggested
principles does not need to have a negative im-
pact on the quality of the stochastic generators
trained on them.
1 Introduction
With the increasing interest in data-driven surface
realization, the question on the adequate annota-
tion of corpora for generation also becomes increas-
ingly important. While in the early days of stochas-
tic generation, annotations produced for other ap-
plications were used (Knight and Hatzivassiloglou,
1995; Langkilde and Knight, 1998; Bangalore and
Rambow, 2000; Oh and Rudnicky, 2000; Langkilde-
Geary, 2002), the poor results obtained, e.g., by
(Bohnet et al, 2010) with the original CoNLL 2009
corpora, show that annotations that serve well for
other applications, may not do so for generation and
thus need at least to be adjusted.1 This has also
been acknowledged in the run-up to the surface re-
alization challenge 2011 (Belz et al, 2011), where
a considerable amount of work has been invested
into the conversion of the annotations of the CoNLL
2008 corpora (Surdeanu et al, 2008), i.e., PropBank
(Palmer et al, 2005), which served as the reference
treebank, into a more ?generation friendly? annota-
tion. However, all of the available annotations are to
a certain extent still syntactic. Even PropBank and
its generation-oriented variants contain a significant
number of syntactic features (Bohnet et al, 2011b).
Some previous approaches to data-driven genera-
tion avoid the problem related to the lack of seman-
tic resources in that they use hybrid models that im-
ply a symbolic submodule which derives the syntac-
tic representation that is then used by the stochas-
tic submodule (Knight and Hatzivassiloglou, 1995;
Langkilde and Knight, 1998). (Walker et al, 2002),
(Stent et al, 2004), (Wong and Mooney, 2007), and
(Mairesse et al, 2010) start from deeper structures:
Walker et al and Stent et al from deep-syntactic
structures (Mel?c?uk, 1988), and Wong and Mooney
and Mairesse et al from higher order predicate logic
structures. However, to the best of our knowledge,
1Trained on the original ConLL 2009 corpora, (Bohnet et al,
2010)?s SVM-based generator reached a BLEU score of 0.12 for
Chinese, 0.18 for English, 0.11 for German and 0.14 for Span-
ish. Joining the unconnected parts of the sentence annotations to
connected trees (as required by a stochastic realizer) improved
the performance to a BLEU score of 0.69 for Chinese, 0.66 for
English, 0.61 for German and 0.68 for Spanish.
22
none of them uses corpora annotated with the struc-
tures from which they start.
To deep stochastic generation, the use of hybrid
models is not an option and training a realizer on
syntactically-biased annotations is highly problem-
atic in the case of data-to-text NLG, which starts
from numeric time series or conceptual or seman-
tic structures: the syntactic features will be simply
not available in the input structures at the moment
of application.2. Therefore, it is crucial to define a
theoretically sound semantic annotation that is still
good in practical terms.
Our goal is thus to discuss some general prin-
ciples of a semantic generation-oriented annotation
schema and offer a first evaluation of its possible
impact on stochastic generation. Section2 details
what kind of information is available respectively
not available during data-to-text generation. Sec-
tion 3 states some general principles that constrain
an adequate semantic representation, while Section
4 formally defines their well-formedness. Section 5
reports then on the experiments made with the pro-
posed annotation, and Section6 offers some conclu-
sions.
2 What can we and what we cannot count
on?
In data-to-text or ontology-to-text generation, with
the standard content selection?discourse structur-
ing?surface generation pipeline in place, and no
hard-wired linguistic realization of the individual
chunks of the data or ontology structure, the input
to the surface realization module can only be an ab-
stract structure that does not contain any syntactic
(and even lexical) information. Conceptual graphs
in the sense of Sowa (Sowa, 2000) are structures of
this kind;3 see Figure 1 for illustration (?Cmpl? =
?Completion?, ?Rcpt? = ?Recipient?, ?Strt? = ?Start?,
?Attr? = Attribute, ?Chrc? = ?Characteristic?, and
?Amt? = ?Amount?). Content selection accounts for
the determination of the content units that are to be
communicated and Discourse Structuring for the de-
limitation of Elementary Discourse Units (EDUs)
2Even though in this article we are particularly interested in
data-to-text generation, we are convinced that clean semantic
and syntactic annotations also facilitate text-to-text generation.
3But note that this can be any other content structure.
and their organization and for the discursive rela-
tions between them (e.g., Bcas (Because) in the Fig-
ure).
In particular, such a structure cannot contain:
? non-meaningful nodes: governed prepositions
(BECAUSE of, CONCENTRATION of), auxil-
iaries (passive be), determiners (a, the);
? syntactic connectors (between A and B), rela-
tive pronouns, etc.
? syntactic structure information: A modifies B,
A is the subject of B, etc.
In other words, a deep stochastic generator has
to be able to produce all syntactic phenomena from
generic structures that guarantee a certain flexibil-
ity when it comes to their surface form (i.e., without
encoding directly this type of syntactic information).
For instance, a concentration of NO2 can be realized
as a NO2 concentration, between 23h00 and 00h00
as from 23h00 until 00h00, etc. This implies that
deep annotations as, for instance, have been derived
so far from PennTreeBank/PropBank, in which ei-
ther all syntactic nodes of the annotation are kept
(as in (Bohnet et al, 2010)) or only certain syntac-
tic nodes are removed (as THAT complementizers
and TO infinitives in the shared task 2011 on sur-
face realization (Belz et al, 2011)) still fall short
of a genuine semantic annotation. Both retain a
lot of syntactic information which is not accessible
in genuine data-to-text generation: nodes (relative
pronouns, governed prepositions and conjunctions,
determiners, auxiliaries, etc.) and edges (relative
clause edges, control edges, modifier vs. argumen-
tal edges, etc.).
This lets us raise the question how the annotation
policies should look like to serve generation well
and to what extent existing resources such as Prop-
Bank comply with them already. We believe that
the answer is critical for the future research agenda
in generation and will certainly play an outstanding
role in the shared tasks to come.
In the next section, we assess the minimal princi-
ples which the annotation suitable for (at least) data-
to-text generation must follow in order to lead to
a core semantic structure. This core structure still
ignores such important information as co-reference,
23
Figure 1: Sample conceptual structure as could be produced by text planning (Because of a concentration of NO2 of
13?g/m3, the NO2 threshold value was exceeded between 23h00 and 00h00)
scope, presupposition, etc.: this information is ob-
viously necessary, but it is not absolutely vital for
a sufficient restriction of the possible choices faced
during surface generation. Further efforts will be re-
quired to address its annotation in appropriate depth.
3 The principles of generation-suitable
semantic annotation
Before talking about generation-suitable annotation,
we must make some general assumptions concern-
ing NLG as such. These assumptions are necessary
(but might not always be sufficient) to cover deep
generation in all its subtleties: (i) data-to-text gener-
ation starts from an abstract conceptual or semantic
representation of the content that is to be rendered
into a well-formed sentence; (ii) data-to-text gener-
ation is a series of equivalence mappings from more
abstract to more concrete structures, with a chain of
inflected words as the final structure; (iii) the equiva-
lence between the source structure Ss and the target
structure St is explicit and self-contained, i.e., for
the mapping from Ss to St, only features contained
in Ss and St are used. The first assumption is in
the very nature of the generation task in general; the
second and the third are owed to requirements of sta-
tistical generation (although a number of rule-based
generators show these characteristics as well).
The three basic assumptions give rise to the fol-
lowing four principles.
1. Semanticity: The semantic annotation must cap-
ture the meaning and only the meaning of a given
sentence. Functional nodes (auxiliaries, determin-
ers, governed conjunctions and prepositions), node
duplicates and syntactically-motivated arcs should
not appear in the semantic structure: they re-
flect grammatical and lexical features, and thus al-
ready anticipate how the meaning will be worded.
For example, meet-AGENT?the (directors), meet-
LOCATION?in (Spain), meet-TIME?in (2002)
cited in (Buch-Kromann et al, 2011) as semantic
annotation of the phrase meeting between the direc-
tors in Spain in 2002 in the Copenhagen Depen-
dency Treebank does not meet this criterion: the,
and both ins are functional nodes. Node dupli-
cates such as the relative pronoun that in the Prop-
Bank annotation (But Panama illustrates that their
their substitute is a system) that?R-A0-produces
(an absurd gridlock) equally reflect syntactic fea-
tures, as do syntactically-motivated arc labels of the
kind ?R(elative)-A0?.
The PropBank annotation of the sentence cited
above also intermingles predicate-argument rela-
tions (?Ai?) with syntactico-functional relations
(?AM-MNR?): gridlock?AM-MNR?absurd. The
predicate-argument analysis of modifiers suggests
namely that they are predicative semantemes that
take as argument the node that governs them
in the syntactic structure; in the above struc-
ture: absurd?A1?gridlock. This applies also
to locatives, temporals and other ?circumstan-
tials?, which are most conveniently represented
as two-place semantemes: house?A1?location?
A2?Barcelona, party?A1?time?A2?yes-
terday, and so on. Although not realized at the sur-
face, location, time, etc. are crucial.
24
2. Informativity: A propositional semantic annota-
tion must be enriched by information structure fea-
tures that predetermine the overall syntactic struc-
ture (paratactic, hypotactic, parenthetical, . . . ), the
internal syntactic structure (subject/object, clefted or
not, any element fronted or not, etc.), determiner dis-
tribution, etc. in the sentence. Otherwise, it will be
always underspecified with respect to its syntactic
equivalence in that, as a rule, a single semantic struc-
ture will correspond to a number of syntactic struc-
tures. This is not to say that with the information
structure in place we will always achieve a 1:1 cor-
respondence between the semantic and syntactic an-
notations; further criteria may be needed?including
prosody, style, presupposedness, etc. However, in-
formation structure is crucial.
The most relevant information structure features
are those of Thematicity, Foregroundedness and
Givenness.4
Thematicity specifies what the utterance states
(marked as rheme) and about what it states it
(marked as theme).5 Theme/rheme determines, in
the majority of cases, the subject-object structure
and the topology of the sentence. For instance,6
[John]theme?A1?[see?A2?Maria]rheme may
be said to correspond to John?subject?see?
dir.obj?Maria and [John?A1?see]rheme?A2
?[Maria]theme to John ?obj?seepass?subject
?Maria. For the generation of relative sentence
structures such as John bought a car which was old
and ugly, we need to accommodate for a recursive
definition of thematicity: [John]theme?A1?[buy?
A2?[c1 : car]theme?A1?[old]rheme; c1?A1
?[ugly]rheme]rheme.7 With no recursive (or sec-
ondary in Mel?c?uk?s terms) thematicity, we would
4We use mainly the terminology and definitions (although in
some places significantly simplified) of (Mel?c?uk, 2001), who,
to the best of our knowledge, establishes the most detailed cor-
relation between information structure and syntactic features.
5Similar notions are topic/focus (Sgall et al, 1986) and
topic/comment (Gundel, 1988).
6As in PropBank, we use ?Ai? as argument labels of predica-
tive lexemes, but for us, ?A1? stands for the first argument, ?A2?
for the second argument, etc. That is, in contrast to PropBank,
we do not support the use of ?A0? to refer to a lexeme?s exter-
nal argument since the distinction between external and internal
arguments is syntactic.
7c1 is a ?handle? in the sense of Minimal Recursion Seman-
tics (Copestake et al, 1997).
get John bought an old and ugly car.8
It is quite easy to find some counter-examples
to the default theme/rheme?syntactic feature cor-
relation, in particular in the case of questions
and answers. For instance, the neutral answer
to the question What will John bake tomorrow?,
John will bake a cake, would be split as follows:
[John?A1?bake]theme ?A2?[cake]rheme. In
this case, the main verb at the surface, bake, is in-
cluded in the theme and not in the rheme. Consider
also the sentence In a cross-border transaction, the
buyer is in a different region of the globe from the
target, where the main theme is in a cross-border
transaction, i.e., not the subject of the sentence (with
the subject the buyer being the embedded theme of
the main rheme). In these cases, the correlation is
more complex, but it undoubtedly exists and needs
to be distilled during the training phase.
Foregroundedness captures the ?prominence?
of the individual elements of the utterance for
the speaker or hearer. An element is ?fore-
grounded? if it is prominent and ?backgrounded?
if it is of lesser prominence; elements that are
neither foregrounded nor backgrounded are ?neu-
tral?. A number of correlations can be iden-
tified: (i) a ?foregrounded? A1 argument of a
verb will trigger a clefting construction; e.g.,
[John]foregr;theme?A1?[see?A2?Maria]rheme
will lead to It was John who saw Maria; similarly,
[John?A1?bake]foregr;theme ?A2?[cake]rheme
will lead to What John will bake is a cake; (ii) a
?foregrounded? A2 argument of a verb will corre-
spond to a clefting construction or a dislocation: It
was Maria, whom John saw; (iii) a ?foregrounded?
A1 or A2 argument of a noun will result in an argu-
ment promotion, as, e.g., John?s arrival (instead of
arrival of John); (iv) a ?foregrounded? circumstan-
tial will be fronted: Under this tree he used to rest;
(v) marking a part of the semantic structure as ?back-
grounded? will lead to a parenthetical construction:
John (well known among the students and professors
alike) was invited as guest speaker. If no elements
8We believe that operator scopes (e.g., negations and quan-
tifiers) can, to a large extent, be encoded within the thematic
structure; see (Cook and Payne, 2006) for work in the LFG-
framework on German, which provides some evidence for this.
However, it must be stated that very little work has been done
on the subject until now.
25
are marked as foregrounded/backgrounded, the de-
fault syntactic structure and the default word order
are realized.
Givenness captures to what extent an information
element is present to the hearer. The elementary
givenness markers ?given? and ?new? correlate in
syntax with determiner distribution. Thus, the ?new?
marker of an object node will often correspond to
an indefinite or zero determiner of the correspond-
ing noun: A masked man was seen to enter the
bank (man is newly introduced into the discourse).
The ?given? marker will often correlate with a defi-
nite determiner: The masked man (whom a passer-
by noticed before) was seen to enter the bank. To
distinguish between demonstratives and definite de-
terminers, a gradation of givenness markers as sug-
gested by Gundel et al (Gundel et al, 1989) is nec-
essary: ?given1/2/3?.
As already for Thematicity, numerous examples
can be found where the giveness-syntactic feature
correlation deviates from the default correlation. For
instance, in I have heard a cat, the cat of my neigh-
bour, there would be only one single (given) node
cat in the semantic structure, which does not pre-
vent the first appearance of cat in the sentence to be
indefinite. In A warrant permits a holder that he ac-
quire one share of common stock for $17.50 a share,
warrant is given, even if it is marked by an indefinite
determiner. Again, this only shows the complexity
of the annotation of the information structure, but it
does not call into question the relevance of the infor-
mation structure to NLG.
As one of the few treebanks, the Prague Depen-
dency Treebank (PDT) (Hajic? et al, 2006) accounts
for aspects of the information structure in that it an-
notates Topic-Focus Articulation in terms of various
degrees of contextual boundness, which are corre-
lated with word order and intonation (Mikulova? et
al., 2006, p.152).
3. Connectivity: The semantic annotation must
ensure that the annotation of an utterance forms
a connected structure: without a connected struc-
ture, generation algorithms that imply a traver-
sal of the input structure will fail to generate a
grammatical sentence. For instance, the Prop-
Bank annotation of the sentence But Panama il-
lustrates that their substitute is a system that pro-
duces an absurd gridlock (here shown partially)
does not comply with this principle since it con-
sists of four unconnected meaning-bearing sub-
structures (the single node ?but? and the subtrees
governed by ?illustrate?, ?produce? and ?substi-
tute?): but | Panama?A0?illustrate?A1?that |
system?A0?produce?A1?gridlock?AM-
MNR?absurd | substitute?A0?their.
4 Outline of a Generation-Oriented
Annotation
The definitions below specify the syntactic well-
formedness of the semantic annotation. They do not
intend to and cannot substitute a detailed annotation
manual, which is indispensable to achieve a seman-
tically accurate annotation.
Definition 1: [Semantic Annotation of a sentence
S, SA]
SA of S in the text T in language L is a pair
?Ssem, Sinf ?, where Ssem is the semantic structure
of S (ensuring Semanticity and Connectivity), and
Sinf is the information structure of S (ensuring In-
formativity).
Let us define each of the two structures of the se-
mantic annotation in turn.
Definition 2: [Semantic Structure of a sentence S,
Ssem]
Ssem of S is a labeled acyclic directed connected
graph (V,E, ?, ?) defined over the vertex label al-
phabet L := LS ?MC ?MT ?Mt ?Ma (such that
LS ? (MC ?MT ?Mt ?Ma) = ?) and the edge
label alphabet Rsem ? {A1, A2, A3, A4, A5, A6},
with
? V as the set of vertices;
? E as the set of directed edges;
? ? as the function that assigns each v ? V an ele-
ment l ? L;
? ? as the function that assigns each e ? E an ele-
ment a ? Rsem;
? LS as the meaning bearing lexical units (LUs) of
S;
? MC ? {LOC, TMP, EXT, MNR, CAU, DIR,
SPEC, ELAB, ADDR} as the ?circumstantial meta
semantemes? (with the labels standing for ?locative?,
?temporal?, ?temporal/spatial extension?, ?manner?,
?cause?, ?direction?, ?specification?, ?elaboration?,
and ?addressee?);
? MT ? {TIME, TCST} as the ?temporal meta se-
mantemes? (with the labels standing for ?time? and
26
?time constituency?);
? Mt ? {past?, present?, future?} as the ?time
value semantemes?;
? Ma ? {imperfective?, durative?,
semelfactive?, iterative?, telic?, atelic?,
nil?} as the ?aspectual value semantemes?9
such that the following conditions hold:
(a) the edges in Ssem are in accordance with the va-
lency structure of the lexical units (LUs) in S: If
lp?Ai?lr ? Ssem (lp, lr ? LS , i ? {1, 2, 3, . . .}),
then the semantic valency of lp possesses at least i
slots and lr fulfils the semantic restrictions of the i-
th slot
(b) the edges in Ssem are exhaustive: If ?(nr) =
lr ? L instantiates in S the i-th semantic argument
of ?(np) = lp, then lp?Ai?lr ? Ssem
(c) Ssem does not contain any duplicated argument
edges: If ?(np)?Ai??(nr), ?(np) ?Aj? ?(nq) ?
Ssem (with np, nr, nq ? N ) then Ai 6= Aj and nr 6=
nq
(d) circumstantial LUs in S are represented in Ssem
by two-place meta-semantemes: If lr ? Lsem is
a locative/temporal/ manner/cause/direction/specifi-
cation/elaboration/addressee LU and in the syntac-
tic dependency structure of S, lr modifies lp, then
lr?A2-?-A1?lp ? Ssem (with ? ? LOC, TMP,
MNR, CAU, DIR, SPEC, ELAB, ADDR)
(e) verbal tense is captured by the two-place predi-
cate TIME: If lp ? Lsem is a verbal LU then lr?A2-
TIME-A1?lp ? Ssem, with lr ? Mt
(f) verbal aspect is captured by the two-place predi-
cate TCST: If lp ? Lsem is a verbal LU then lr?A2-
TCST-A1?lp ? Ssem, with lr ? Ma.
(a) implies that no functional node is target of an ar-
gument arc: this would contradict the semantic va-
lency conditions of any lexeme in S. (b) ensures that
no edge in Ssem is missing: if a given LU is an argu-
ment of another LU in the sentence, then there is an
edge from the governor LU to the argument LU. (c)
means that no predicate in Ssem possesses in S two
different instances of the same argument slot. The
circumstantial meta-semantemes in (d) either cap-
ture the semantic role of a circumstantial that would
otherwise get lost or introduce a predicate type for a
name. Most of the circumstantial meta-semantemes
9The aspectual feature names are mainly from (Comrie,
1976).
reflect PropBank?s modifier relations ?AM-X? (but in
semantic, not in syntactico-functional terms), such
that their names are taken from PropBank or are in-
spired by PropBank. LOC takes as A1 a name of a
location of its A2: Barcelona?A1-LOC-A2?live-
A1?John; TMP a temporal expression: yesterday
?A1-TMP-A2?arrive-A1?John; MNR a man-
ner attribute: player?A1-MNR-A2?solo; CAU
the cause: accept?A1-CAU-A2?reason in This
is the reason why they accepted it; DIR a spa-
tial direction: run around?A2-DIR-A1?circles in
I?m running around in circles; SPEC a ?context
specifier?: should?A2-SPEC-A1?thought in You
should leave now, just a thought; ELAB an appos-
itive attribute company?A1-ELAB-A2 ?bank in
This company, a bank, closed; and ADDR direct ad-
dress: come?A1-ADDR-A2?John in John, come
here!
Definition 3: [Information Structure of a sen-
tence S, Sinf ]
Let Ssem of S be defined as above. Sinf of S is
an undirected labeled hypergraph (V, I) with V as
the set of vertices of S and I the set of hyperedges,
with I := {themei (i = 1, 2, . . . ), rhemei (i = 1, 2,
. . . ), givenj (j = 1,. . . ,3), new, foregrounded, back-
grounded}. The following conditions apply:
(a) thematicity is recursive, i.e., a thematic hyper-
edge contains under specific conditions embedded
theme/rheme hyperedges: If ?nk ? themei such that
?(nk) = lp is a verbal lexeme and lp -A1?lr ?
Ssem, then ? themei+1, rhemei+1 ? themei
(b) theme and rheme hyperedges of the same re-
cursion level, given and new hyperedges, and fore-
grounded and backgrounded hyperedges are dis-
joint: themei ? rhemei = ? (i = 1, 2, . . . ), givenj
? new = ? (j = 1,. . . ,3), foregr. ? backgr. = ?
(c) any node in Ssem forms part of either theme or
rheme: ?np ? Ssem : np ? theme1 ? rheme1.
Consider in Figure 2 an example of SA with its
two structures.10 All syntactic nodes have been re-
moved, and all the remaining nodes are connected
in terms of a predicate?argument structure, with no
use of any syntactically motivated edge, so as to en-
sure that the structure complies with the Semantic-
ity and Connectivity principles. Figure 2 illustrates
the three main aspects of Informativity: (i) thematic-
10The meta-semanteme TCST is not shown in the figure.
27
ity, with the two theme/rheme oppositions; (ii) fore-
groundedness, with the backgrounded part of the
primary rheme; and (iii) givenness, with the attribute
givenness and the value 2 on the node program. The
information structure constrains the superficial real-
ization of the sentence in that the primary theme will
be the subject of the sentence, and the main node of
the primary rheme pointing to it will be the main
verb of the same sentence. The secondary theme
and rheme will be realized as an embedded sen-
tence in which you will be the subject, that is, forc-
ing the realization of a relative clause. However, it
does not constrain the appearance of a relative pro-
noun. For instance: we obtained technologies you
do not see anywhere else and we obtained technolo-
gies that you do not see anywhere else are possible
realizations of this structure. Leaving the relative
pronoun in the semantic structure would force one
realization to occur when it does not have to (both
outputs are equally correct and meaning-equivalent
to the other). Similarly, marking the Soviet space
program as backgrounded leaves some doors open
when it comes to surface realization: Cosmos, the
Soviet space program vs. Cosmos (the Soviet space
program) vs. the Soviet space program Cosmos (if
Cosmos is backgrounded too) are possible realiza-
tions of this substructure.
ELABORATION is an example of a meta-node
needed to connect the semantic structure: Cosmos
and program have a semantic relation, but neither is
actually in the semantic frame of the other?which
is why the introduction of an extra node cannot be
avoided. In this case, we could have a node NAME,
but ELABORATION is much more generic and can
actually be automatically introduced without any ad-
ditional information.
5 Experiments
Obviously, the removal of syntactic features from
a given standard annotation, with the goal to ob-
tain an increasingly more semantic annotation, can
only be accepted if the quality of (deep) stochas-
tic generation does not unacceptably decrease. To
assess this aspect, we converted automatically the
PropBank annotation of the WSJ journal as used in
the CoNLL shared task 2009 into an annotation that
complies with all of the principles sketched above
for deep statistical generation and trained (Bohnet
et al, 2010)?s generator on this new annotation.11
For our experiments, we used the usual training,
development and test data split of the WSJ cor-
pus (Langkilde-Geary, 2002; Ringger et al, 2004;
Bohnet et al, 2010); Table 1 provides an overview
of the used data.
set section # sentences
training 2 - 21 39218
development 24 1334
test 23 2400
Table 1: Data split of the used data in the WSJ Corpus
The resulting BLEU score of our experiment was
0.64, which is comparable with the accuracy re-
ported in (Bohnet et al, 2010) (namely, 0.659), who
used an annotation that still contained all functional
nodes (such that their generation task was consider-
ably more syntactic and thus more straightforward).
To assess furthermore whether the automatically
converted PropBank already offers some advantages
to other applications than generation, we used it in a
semantic role labeling (SRL) experiment with (Bjo?
rkelund et al, 2010)?s parser. The achieved overall
accuracy is 0.818, with all analysis stages (including
the predicate identification stage) being automatic,
which is a rather competitive figure. In the original
CoNLL SRL setting with Oracle reading, an accu-
racy of 0.856 is achieved.
Another telling comparison can be made between
the outcomes of the First Surface Realization Shared
Task (Belz et al, 2011), in which two different
input representations were given to the competing
teams: a shallow representation and a deep repre-
sentation. The shallow structures were unordered
syntactic dependency trees, with all the tokens of
the sentence, and the deep structures were predicate-
argument graphs with some nodes removed (see
Section 2). Although the performance of shallow
generators was higher than the performance of the
deep generators (the StuMaBa shallow generator
(Bohnet et al, 2011a) obtained a BLEU score of
0.89, as opposed to 0.79 of the StuMaBa deep gen-
11Obviously, our conversion can be viewed only preliminary.
It does not take into account all the subtleties that need to be
taken account?for instance, with respect to the information
structure; see also Section 6.
28
Figure 2: Illustration of the semantic annotation of the sentence Through the development of Cosmos, the Soviet space
program, we obtained technologies you do not see anywhere else.
erator), the difference is not as striking as one would
expect.12
6 Conclusions
Our experiments and the Surface Realization Shared
Task 2011 suggest that making the deep annotation
more semantic does not necessarily imply an unsur-
mountable problem for stochastic generation. We
can thus conclude that deriving automatically a deep
semantic annotation from PropBank allowed us to
obtain very promising results, both for NLG and
SRL. By sticking to universal predicate-argument
structures, as PropBank does, we maintain the po-
tential of the corpus to be mapped to other, more id-
iosyncratic, annotations. Still, automatic conversion
will always remain deficient. Thus, a flawless iden-
tification of semantic predication cannot be guaran-
teed. For instance, when an actancial arc points to a
preposition, it is not clear how to deduce whether
this preposition is semantic or lexical. Also, the
treatment of phraseological nodes is problematic,
as is the annotation of a comprehensive informa-
12Note that our results mentioned above cannot be directly
compared with the StuMaBa results during the Generation
Challenges 2011 because the realizers are different.
tion structure: the criteria for the automatic deriva-
tion of the information structure from the syntactic
structure and the topology of a sentence can only
be superficial and likely to be even less efficient in
longer and complex sentences. The annotation of
intersentential coreferences and the identification of
gapped elements are further major hurdles for an au-
tomatic derivation of a truly semantic resource. As
a consequence, we believe that new annotation poli-
cies are needed to obtain a high quality semantic re-
source. The best strategy is to start with a conver-
sion of an existing semantically annotated treebank
such as PropBank, revising and extending the result
of this conversion in a manual concerted action?
always following truly semantic annotation policies.
Acknowledgments
We would like to thank the reviewers for their valu-
able comments and suggestions and the Penn Tree-
bank/PropBank/NomBank team, without whom our
experiments would not be possible. Many thanks
also to Mike White for the useful discussions on
some of the topics discussed in the paper. Although
we might still not agree on all of the details, he
made us see the task of generation-oriented annota-
29
tion from another angle and revise some of our ini-
tial assumptions.
References
S. Bangalore and O. Rambow. 2000. Exploiting a Proba-
bilistic Hierarchical Model for Generation. In Proc. of
COLING ?00.
A. Belz, M. White, D. Espinosa, D. Hogan, and A. Stent.
2011. The First Surface Realization Shared Task:
Overview and Evaluation Results. In ENLG11.
A. Bjo?rkelund, B. Bohnet, L. Hafdell, and P. Nugues.
2010. A high-performance syntactic and semantic de-
pendency parser. In Proc. of COLING ?10: Demon-
stration Volume.
B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence generation
with a stochastic multi-level realizer. In Proc. of COL-
ING ?10.
B. Bohnet, S. Mille, B. Favre, and L. Wanner. 2011a.
<STUMABA>: From Deep Representation to Sur-
face. In ENLG11.
B. Bohnet, S. Mille, and L. Wanner. 2011b. Statisti-
cal language generation from semantic structures. In
Proc. of International Conference on Dependency Lin-
guistics.
M. Buch-Kromann, M. Gylling-J?rgensen, L. Jelbech-
Knudsen, I. Korzen, and H. Mu?ller. 2011. The
inventory of linguistic relations used in the Copen-
hagen Dependency Treebanks. www.cbs.dk/ con-
tent/download/149771/1973272/file.
B. Comrie. 1976. Aspect. Cambridge University Press,
Cambridge.
P. Cook and J. Payne. 2006. Information Structure and
Scope in German. In LFG06.
A. Copestake, D. Flickinger, and I. Sag. 1997. Minimal
recursion semantics. Technical report, CSLI, Stanford
University, Stanford.
J. Gundel, N. Hedberg, and R. Zacharski. 1989. Give-
ness, Implicature and Demonstrative Expressions in
English Discourse. In CLS-25, Part II (Parasession
on Language in Context), pages 89?103. Chicago Lin-
guistics Society.
J.K. Gundel. 1988. ?Universals of Topic-Comment
Structure?. In M. Hammond, E. Moravc?ik, and
J. Wirth, editors, Studies in Syntactic Typology. John
Benjamins, Amsterdam & Philadelphia.
J. Hajic?, J. Panevova?, E. Hajic?ova?, P. Sgall, P. Pa-
jas, J. S?te?pa?nek, J. Havelka, M. Mikulova?, and
Z. Z?abokrtsky?. 2006. Prague Dependency Treebank
2.0.
K. Knight and V. Hatzivassiloglou. 1995. Two-level,
many paths generation. In Proc. of ACL ?95.
I. Langkilde and K. Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proc. of
COLING/ACL ?98.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator. In Proc. of 2nd INLG Conference.
F. Mairesse, M Gas?ic?, F. Juric???c?, S Keizer, B. Thomson,
K. Yu, and S. Young. 2010. Phrase-based statistical
language generation using graphical models and active
learning. In Proc. of ACL ?10.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. SUNY Press, Albany.
I.A. Mel?c?uk. 2001. Communicative Organization
in Natural Language (The Semantic-Communicative
Structure of Sentences). Benjamins Academic Pub-
lishers, Amsterdam.
M. Mikulova? et al 2006. Annotation on
the tectogrammatical level in the Prague
Dependency Treebank: Reference book.
www.cbs.dk/content/download/149771/ 1973272/file.
A.H. Oh and A.I. Rudnicky. 2000. Stochastic language
generation for spoken dialogue systems. In Proc. of
ANL/NAACL Workshop on Conversational Systems.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?105.
E. Ringger, M. Gamon, R.C. Moore, D. Rojas, M. Smets,
and S. Corston-Oliver. 2004. Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In Proceedings of COLING,
pages 673?679.
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The Mean-
ing of the Sentence in its Semantic and Pragmatic As-
pects. Reidel Publishing Company, Dordrecht.
J. F. Sowa. 2000. Knowledge Representation: Logi-
cal, Philosophical, and Computational Foundations.
Brooks Cole Publishing Co., Pacific Grove, CA, USA.
A. Stent, R. Prasad, and M. Walker. 2004. Trainable sen-
tence planning for complex information presentation
in spoken dialog systems. In Proc. of ACL ?04.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the 12th
CoNLL-2008.
M.A. Walker, O.C. Rambow, and M. Rogati. 2002.
Training a sentence planner for spoken dialogue using
boosting. Computer Speech and Language, 16:409?
433.
Y.W. Wong and R.J. Mooney. 2007. Generation by in-
verting a semantic parser that uses statistical machine
translation. In Proc. of the HLT Conference.
30
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 136?140,
Utica, May 2012. c?2012 Association for Computational Linguistics
The Surface Realisation Task: Recent Developments and Future Plans
Anja Belz
Computing, Engineering and Maths
University of Brighton
Brighton BN1 4GJ, UK
a.s.belz@brighton.ac.uk
Bernd Bohnet
Institute for Natural Language Processing
University of Stuttgart
70174 Stuttgart
bohnet@ims.uni-stuttgart.de
Simon Mille, Leo Wanner
Information and Communication Technologies
Pompeu Fabra University
08018 Barcelona
<firstname>.<lastname>@upf.edu
Michael White
Department of Linguistics
Ohio State University
Columbus, OH, 43210, US
mwhite@ling.osu.edu
Abstract
The Surface Realisation Shared Task was first
run in 2011. Two common-ground input rep-
resentations were developed and for the first
time several independently developed surface
realisers produced realisations from the same
shared inputs. However, the input representa-
tions had several shortcomings which we have
been aiming to address in the time since. This
paper reports on our work to date on improv-
ing the input representations and on our plans
for the next edition of the SR Task. We also
briefly summarise other related developments
in NLG shared tasks and outline how the dif-
ferent ideas may be usefully brought together
in the future.
1 Introduction
The Surface Realisation (SR) Task was introduced
as a new shared task at Generation Challenges 2011
(Belz et al, 2011). Our aim in developing the SR
Task was to make it possible, for the first time, to
directly compare different, independently developed
surface realisers by developing a ?common-ground?
representation that could be used by all participat-
ing systems as input. In fact, we created two dif-
ferent input representations, one shallow, one deep,
in order to enable more teams to participate. Corre-
spondingly, there were two tracks in SR?11: In the
Shallow Track, the task was to map from shallow
syntax-level input representations to realisations; in
the Deep Track, the task was to map from deep
semantics-level input representations to realisations.
By the time teams submitted their system outputs,
it had become clear that the inputs required by some
types of surface realisers were more easily derived
from the common-ground representation than the in-
puts required by other types. There were other re-
spects in which the representations were not ideal,
e.g. the deep representations retained too many syn-
tactic elements as stopgaps where no deeper infor-
mation had been available. It was clear that the in-
put representations had to be improved for the next
edition of the SR Task. In this paper, we report on
our work in this direction so far and relate it to some
new shared task proposals which have been devel-
oped in part as a response to the above difficulties.
We discuss how these developments might usefully
be integrated, and outline plans for SR?13, the next
edition of the SR Task.
2 SR?11
The SR?11 input representations were created by
post-processing the CoNLL 2008 Shared Task
data (Surdeanu et al, 2008), for the preparation of
which selected sections of the WSJ Treebank were
converted to syntactic dependencies with the Pen-
nconverter (Johansson and Nugues, 2007). The
resulting dependency bank was then merged with
Nombank (Meyers et al, 2004) and Propbank
(Palmer et al, 2005). Named entity information
from the BBN Entity Type corpus was also incorpo-
rated. The SR?11 shallow representation was based
on the Pennconverter dependencies, while the deep
representation was derived from the merged Nom-
bank, Propbank and syntactic dependencies in a pro-
136
cess similar to the graph completion algorithm out-
lined by Bohnet (2010).
Five teams submitted a total of six systems to
SR?11 which we evaluated automatically using a
range of intrinsic metrics. In addition, systems were
assessed by human judges in terms of Clarity, Read-
ability and Meaning Similarity.
The four top-performing systems were all statis-
tical dependency realisers that do not make use of
an explicit, pre-existing grammar. By design, statis-
tical dependency realisers are robust and relatively
easy to adapt to new kinds of dependency inputs
which made them well suited to the SR?11 Task. In
contrast, there were only two systems that employed
a grammar, either hand-crafted or treebank-derived,
and these did not produce competitive results. Both
teams reported substantial difficulties in converting
the common ground inputs into the ?native? inputs
required by their systems.
The SR?11 results report pointed towards two
kinds of possible improvements: (i) introducing (ad-
ditional) tasks where performance would not depend
to the same extent on the relation between common-
ground and native inputs, e.g. a text-to-text shared
task on sentential paraphrasing; and (ii) improving
the representations themselves. In the remainder of
this paper we report on developments in both these
directions.
3 Towards SR?13
As outlined above, the first SR Shared Task turned
up some interesting representational issues that re-
quired some in-depth investigation. In the end, it
was this fact that led to the decision to postpone
the 2nd SR Shared Task until 2013 in order to al-
low enough time to address these issues properly. In
this section, we describe our plans for SR?13 to the
extent to which they have progressed.
3.1 Task definition
As in the first SR task, the participating teams will
be provided with annotated corpora consisting of
common-ground input representations and their cor-
responding outputs. Two kinds of input will be of-
fered: deep representations and surface representa-
tions. The deep input representations will be se-
mantic graphs; the surface representations syntactic
trees. Both will be derived from the Penn Treebank.
The task will consist in the generation of a text start-
ing from either of the input representations.
3.2 Changes to the input representations
During the working group discussions which fol-
lowed SR?11, it became apparent that the CoNLL
syntactic dependency trees overlaid with Prop-
bank/Nombank relations had turned out to be inade-
quate in various respects for the purpose of deriving
a suitable semantic representation. For instance:
? Governed prepositions are not distinguished
from semantically loaded prepositions in the
CoNLL annotation. In SR?11, only strongly
governed prepositions such as give something
TO someone were removed, but in many cases
the meaning of a preposition which introduces
an argument (of a verb, a noun, an adjective
or an adverb) clearly depends on the predicate:
believe IN something, account FOR some-
thing, etc. In those cases, too, the preposition
should be removed from the semantic annota-
tion, since the relisers have to be able to intro-
duce non-semantic features un-aided. On the
contrary, semantically loaded governed prepo-
sitions such as live IN a flat/ON a roof/NEXT
TO the main street etc. should be retained in
the annotation. These prepositions all receive
argumental arcs in PropBank/NomBank, so it
is not easy to distinguish between them. One
possibility would be to target a restricted list of
prepositions which are void of meaning most of
the time, and remove those prepositions when
they introduce arguments.
? The annotation of relative pronouns did not
survive the conversion of the original Penn
Treebank to the CoNLL format unscathed: the
antecedent of the relative pronoun is sometimes
lost or the relative pronoun is not annotated,
predominantly because the predicate which the
relative pronoun is an argument of was not con-
sidered to be a predicate by annotators, as in
the degree TO WHICH companies are irritated.
However, in the original constituency annota-
tion, the traces allow for retrieving antecedents
and semantic governors, hence using this orig-
137
inal annotation could be useful in order to get a
clean annotation of such phenomena.
Agreement has been reached on a range of other is-
sues, although the feasibility of implementing the
corresponding changes might have to be further
evaluated:
? Coordinations should be annotated in the se-
mantic representation with the conjunction as
the head of all the conjuncts. This treatment
would allow e.g. an adequate representation of
sharing of dependents among the conjuncts.
? The inversion of ?modifier? arcs and the intro-
duction of meta-semantemes would avoid an-
ticipating syntactic decisions such as the direc-
tion of non-argumental syntactic edges, and al-
low for connecting unconnected parts of the se-
mantic structures.
? In order to keep the scope of various phenom-
ena intact after inverting non-argumental edges,
we should explicitly mark the scope of e.g.
negations, quantifiers, quotation marks etc. as
attribute values on the nodes.
? Control arcs should be removed from the se-
mantic representation since they do not provide
information relevant at that level.
? Named entities will be further specified adding
a reduced set of named entity types from the
BBN annotations.
Finally, we will perform automatic and manual qual-
ity checks in order to ensure that the proposed
changes are adequately introduced in the annotation.
3.3 Evaluation
We will once again follow the main data set divi-
sions of the CoNLL?08 data (training set = WSJ Sec-
tions 02?21; development set = Section 24; test set =
Section 23), with the proviso that we have removed
300 randomly selected sentences from the develop-
ment set for use in human evaluations. Of these, we
used 100 sentences in SR?11 and will use a different
100 in SR?13.
Evaluation criteria identified as important for
evaluation of surface realisation output in previous
work include Adequacy (preservation of meaning),
Fluency (grammaticality/idiomaticity), Clarity, Hu-
manlikeness and Task Effectiveness. We will aim to
evaluate system outputs submitted by SR?13 partic-
ipants in terms of most of these criteria, using both
automatic and human-assessed methods.
As in SR?11, the automatic evaluation metrics (as-
sessing Humanlikeness) will be BLEU, NIST, TER
and possibly METEOR. We will apply text normal-
isation to system outputs before scoring them with
the automatic metrics. For n-best ranked system
outputs, we will again compute a single score for all
outputs by computing their weighted sum of their
individual scores, where a weight is assigned to a
system output in inverse proportion to its rank. For
a subset of the test data we may obtain additional al-
ternative realisations via Mechanical Turk for use in
the automatic evaluations.
We are planning to expand the range of human-
assessed evaluation experiments (assessing Ade-
quacy, Fluency and Clarity) to the following meth-
ods:
1. Preference Judgement Experiment (C2, C3):
Collect preference judgements using an exist-
ing evaluation interface (Kow and Belz, 2012)
and directly recruited evaluators. We will
present sentences in the context of a chunk of
5 consecutive sentences to the evaluators, and
ask for separate judgements for Clarity, Flu-
ency and Meaning Similarity.
2. HTER (Snover et al, 2006): In this evaluation
method, human evaluators are asked to post-
edit the output of a system, and the edits are
then categorised and counted. Crucial to this
evaluation method is the construction of clear
instructions for evaluators and the categorisa-
tion of edits. We will categorise edits as relat-
ing to Meaning Similarity, Fluency and/or Clar-
ity; we will also consider further subcategorisa-
tions.
We will once again provide evaluation scripts to par-
ticipants so they can perform automatic evaluations
on the development data. These scores serve two
purposes. Firstly, development data scores must be
included in participants? reports. Secondly, partici-
138
pants may wish to use the evaluation scripts in de-
veloping and tuning their systems.
We will report per-system results separately for
the automatic metrics (4 sets of results), and for the
human-assessed measures (2 sets of results). For
each set of results, we will report single-best and
n-best results. For single-best results, we may fur-
thermore report results both with and without miss-
ing outputs. We will rank systems, and report sig-
nificance of pairwise differences using bootstrap re-
sampling where necessary (Koehn, 2004; Zhang and
Vogel, 2010). We will separately report correlation
between human and automatic metrics, and between
different automatic metrics.
3.4 Assessing different aspects of realisation
separately
In addition, we will consider measuring different as-
pects of the realisation performance of participating
systems (syntax, word order, morphology) since a
system can perform well on one and badly on an-
other. For instance, a system might perform well
on morphological realisation while it has poor re-
sults on linearisation. We would like to capture this
fact. This may involve asking participating teams to
submit intermediate representations or identifiers to
identify the reference words. This more fine-grained
approach should help us to obtain a more precise
picture of the state of affairs in the field and could
help to reveal the respective strengths of different
surface realisers more clearly.
4 Related Developments
4.1 Syntactic Paraphrase Ranking
The new shared task on syntactic paraphrase ranking
described elsewhere in this volume (White, 2012) is
intended to run as a follow-on to the main surface
realisation shared task. Taking advantage of the hu-
man judgements collected to evaluate the surface re-
alisations produced by competing systems, the task
is to automatically rank the realisations that differ
from the reference sentence in a way that agrees with
the human judgements as often as possible. The task
is designed to appeal to developers of surface real-
isation systems as well as machine translation eval-
uation metrics. For surface realisation systems, the
task sidesteps the thorny issue of converting inputs
to a common representation. Developers of reali-
sation systems that can generate and optionally rank
multiple outputs for a given input will be encouraged
to participate in the task, which will test the system?s
ability to produce acceptable paraphrases and/or to
rank competing realisations. For MT evaluation
metrics, the task provides a challenging framework
for advancing automatic evaluation, as many of the
paraphrases are expected to be of high quality, dif-
fering only in subtle syntactic choices.
4.2 Content Selection Challenge
The new shared task on content selection has been
put forward (Bouayad-Agha et al, 2012) to initi-
ate work on content selection from a common, stan-
dardised semantic-web format input, and thus pro-
vide the context for an objective assessment of dif-
ferent content selection strategies. The task con-
sists in selecting the contents communicated in ref-
erence biographies of celebrities from a large vol-
ume of RDF-triples. The selected triples will be
evaluated against a gold triple selection set using
standard quality assessment metrics.
The task can be considered complementary to the
surface realisation shared task in that it contributes
to the medium-term goal of setting up a task that
covers all stages of the generation pipeline. In fu-
ture challenges, it can be explored to what extent and
how the output content plans can be mapped onto
semantic representations that serve as input to the
surface realisers.
5 Plans
We are currently working on the new improved
common-ground input representation scheme and
converting the data to the new scheme.
The provisional schedule for SR?13 looks as
follows:
Announcement and call for expres-
sions of interest:
6 July 2012
Preliminary registration and release
of description of new representations:
27 July 2012
Release of data and documentation: 2 Nov 2012
System Submission Deadline: 10 May 2013
Evaluation Period: 10 May?
10 Jul 2013
Provisional dates for results session: 8?9 Aug 2013
139
6 Conclusion
For a large number of NLP applications (among
them, e.g., text generation proper, summarisation,
question answering, and dialogue), surface realisa-
tion (SR) is a key technology. Unfortunately, so
far in nearly all of these applications, idiosyncratic,
custom-made SR implementations prevail. How-
ever, a look over the fence at the language analy-
sis side shows that the broad use of standard de-
pendency treebanks and semantically annotated re-
sources such as PropBank and NomBank that were
created especially with parsing in mind led to stan-
dardised high-quality off-the-shelf parser implemen-
tations. It seems clear that in order to advance the
field of surface realisation, the generation commu-
nity also needs adequate resources on which large-
scale experiments can be run in search of the surface
realiser with the best performance, a surface realiser
which is commonly accepted, follows general trans-
parent principles and is thus usable as plug-in in the
majority of applications.
The SR Shared Task aims to contribute to this
goal. On the one hand, it will lead to the creation
of NLG-suitable resources in that it will convert
the PropBank into a more semantic and more com-
pletely annotated resource. On the other hand, it will
offer a forum for the presentation and evaluation of
various approaches to SR and thus help us to search
for the best solution to the SR task with the greatest
potential to become a widely accepted off-the-shelf
tool.
Acknowledgments
We gratefully acknowledge the contributions to dis-
cussions and development of ideas made by the
other members of the SR working group: Miguel
Ballesteros, Johan Bos, Aoife Cahill, Josef van Gen-
abith, Pablo Gerva?s, Deirdre Hogan and Amanda
Stent.
References
Anja Belz, Michael White, Dominic Espinosa, Deidre
Hogan, Eric Kow, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG?11), pages 217?226. Association for Compu-
tational Linguistics.
Bernd Bohnet, Leo Wanner, Simon Mille, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd International Conference
on Computational Linguistics, Beijing, China.
Nadjet Bouayad-Agha, Gerard Casamayor, Leo Wanner,
and Chris Mellish. 2012. Content selection from
semantic web data. In Proceedings of the 7th In-
ternational Natural Language Generation Conference
(INLG?12).
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,
and Mare Koit, editors, Proceedings of NODALIDA
2007, pages 105?112, Tartu, Estonia.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Eric Kow and Anja Belz. 2012. LG-Eval: A toolkit
for creating online language evaluation experiments.
In Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC?12).
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In NAACL/HLT Workshop Frontiers in Corpus
Annotation.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: A corpus annotated with
semantic roles. In Computational Linguistics Journal,
pages 71?105.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL?08), Manchester, UK.
Michael White. 2012. Shared task proposal: Syntac-
tic paraphrase ranking. In Proceedings of the 7th In-
ternational Natural Language Generation Conference
(INLG?12).
Ying Zhang and Stephan Vogel. 2010. Significance tests
of automatic machine translation evaluation metrics.
Machine Translation, 24:51?65.
140
Proceedings of the 8th International Natural Language Generation Conference, pages 108?112,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Classifiers for data-driven deep sentence generation
Miguel Ballesteros1, Simon Mille1 and Leo Wanner2,1
1NLP Group, Department of Information and Communication Technologies
Pompeu Fabra University, Barcelona
2Catalan Institute for Research and Advanced Studies (ICREA)
<fname>.<lname>@upf.edu
Abstract
State-of-the-art statistical sentence gener-
ators deal with isomorphic structures only.
Therefore, given that semantic and syntac-
tic structures tend to differ in their topol-
ogy and number of nodes, i.e., are not iso-
morphic, statistical generation saw so far
itself confined to shallow, syntactic gener-
ation. In this paper, we present a series
of fine-grained classifiers that are essen-
tial for data-driven deep sentence genera-
tion in that they handle the problem of the
projection of non-isomorphic structures.
1 Introduction
Deep data-driven (or stochastic) sentence gener-
ation needs to be able to map abstract seman-
tic structures onto syntactic structures. This has
been a problem so far since both types of struc-
tures differ in their topology and number of nodes
(i.e., are non-isomorphic). For instance, a truly
semantic structure will not contain any functional
nodes,1 while a surface-syntactic structure or a
chain of tokens in a linearized tree will con-
tain all of them. Some state-of-the-art propos-
als use a rule-based module to handle the projec-
tion between non-isomorphic semantic and syn-
tactic structures/chains of tokens, e.g., (Varges and
Mellish, 2001; Belz, 2008; Bohnet et al., 2011),
and some adapt the semantic structures to be iso-
morphic with syntactic structures (Bohnet et al.,
2010). In this paper, we present two alternative
stochastic approaches to the projection between
non-isomorphic structures, both based on a cas-
cade of Support Vector Machine (SVM) classi-
fiers.2 The first approach addresses the projection
as a generic non-isomorphic graph transduction
1See, for instance, (Bouayad-Agha et al., 2012).
2Obviously, other machine learning techniques could also
be used.
problem in terms of four classifiers for 1. identi-
fication of the (non-isomorphic) correspondences
between fragments of the source and target struc-
ture, 2. generation of the nodes of the target struc-
ture, 3. generation of the dependencies between
corresponding fragments of the source and target
structure, and 4. generation of the internal depen-
dencies in all fragments of the target structure.
The second approach takes advantage of the lin-
guistic knowledge about the projection of the in-
dividual linguistic token types. It replaces each
of the above four classifiers by a set of classifiers,
with each single classifier dealing with only one
individual linguistic token type (verb, noun, ad-
verb, etc.) or with a configuration thereof. As will
be seen, the linguistic knowledge pays off: the sec-
ond approach achieves considerably better results.
Since our goal is to address the challenge of the
projection of non-isomorphic structures, we focus,
in what follows, on this task. That is, we do not
build a complete generation pipeline until the sur-
face. This could be done, for instance, by feed-
ing the output obtained from the projection of a
semantic onto a syntactic structure to the surface
realizer described in (Bohnet et al., 2010).
2 The Task
The difference in the linguistic abstraction of se-
mantic and syntactic structures leads to diver-
gences that impede the isomorphy between the
two and make the mapping between them a chal-
lenge for statistical generation. Let us, before we
come to the implementation, give some theoretical
details on these structures as we picture them and
on the possible approaches to the projection of a
semantic structure to a syntactic one.
2.1 The Notion of semantic and syntactic
structures
As semantic structure, we assume a shallow se-
mantic representation that is very similar to the
108
PropBank (Babko-Malaya, 2005) and deep anno-
tations as used in the Surface Realisation Shared
Task (Belz et al., 2011): the deep-syntactic layer
of the AnCora-UPF corpus (Mille et al., 2013).
Deep-syntactic structures (DSyntSs) do not
contain any punctuation and functional nodes, i.e.,
governed prepositions and conjunctions, auxil-
iaries and determiners.3
As syntactic structure (in the terminology
of Ancora-UPF: surface-syntactic structures,
SSyntSs), we assume dependency trees in which
the nodes are labeled by open or closed class
lexemes and the edges by grammatical function
relations of the type subject, oblique object,
adverbial, modifier, etc.; cf.4 See Figure 1 for a
contrastive illustration of DSyntS and SSyntS.
rumor want new song be successful
I
II
ATTR I II
a rumor wants that the new song will be successful
det subj dobj
conj
detmodif subj analyt fut copul
Figure 1: DSyntS (above) and SSyntS (below) of
an English Sentence.
Note, however, that the proposal outlined be-
low for the projection of non-isomorphic struc-
tures is trainable on any multi-layered treebanks
where different layers are not isomorphic.
2.2 Projection of DSyntSs onto SSyntSs
In order to project a DSyntS onto its correspond-
ing SSyntS in the course of sentence generation,
the following types of actions need to be per-
formed:
1. Project each node in the DSyntS onto its SSynS-
correspondence. This correspondence can be a
single node, as, e.g., successful? successful, or a
subtree (hypernode, known as syntagm in linguis-
tics), as, e.g., song ? the song ?DT NN? (where
?DT? is a determiner and ?NN? a noun) or be
? that will be ?IN VAUX VB? (where ?IN? is a
preposition, ?VAUX? an auxiliary and ?VB? a full
verb). In formal terms, we assume any SSyntS-
correspondence to be a hypernode with a cardinal-
ity ? 1.
2. Generate the correct lemma for the nodes in
3For more details on the SSyntS, see (Mille et al., 2013).
4DSyntSs and their corresponding SSyntSs are stored in
the 14-column CoNLL?08 format.
SSyntS that do not have a 1:1 correspondence in
the SSyntS (as ?DT?, ?IN? and ?VAUX? above).
3. Establish the dependencies within the individ-
ual SSyntS-hypernodes.
4. Establish the dependencies between the
SSyntS-hypernodes (more precisely, between the
nodes of different SSyntS-hypernodes) to obtain a
connected SSyntS-tree.
3 Classifiers
As mentioned in the Introduction, the realization
of the actions 1.? 4. can be approached either in
terms of 4 generic classifiers (Section 3.1) or in
terms of 4 sets of fine-grained (micro) classifiers
(Section 3.2) that map one representation onto an-
other. As also mentioned above, we realize both
approaches as Support Vector Machines (SVMs).
3.1 Generic classifier approach
Each of the generic classifiers deals with one of
the following tasks.
a. Hypernode Identification: Given a deep
syntactic node nd from the DSyntS, the system
must find the shape of the surface hypernode (=
syntagm) that corresponds to nd in the SSyntS.
The hypernode identification SVM uses the fol-
lowing features:
POS of nd, POS of nd?s head, voice,
temp. constituency, finiteness, tense, lemma of
nd, and nd?s dependencies.
In order to simplify the task, we define the shape
of a surface hypernode as a list of surface PoS-
tags. This list contains the PoS of each of the lem-
mas within the hypernode and a tag that signals the
original deep node; for instance:
[ VB(deep), VAUX, IN]
b. Lemma Generation. Once the hypernodes
of the SSyntS under construction have been pro-
duced, the functional nodes that have been newly
introduced in the hypernodes must be assigned a
lemma. The lemma generation SVM uses the fol-
lowing features of the deep nodes nd in the hyper-
nodes:
? finiteness, ? definiteness, ? PoS of nd, ? lemma
of nd, ? PoS of the head of nd
to select the most likely lemma.
c. Intra-hypernode Dependency Generation.
Given a hypernode and its lemmas provided by
the two previous stages, the dependencies (i.e., the
dependency attachments and dependency labels)
between the elements of the hypernode must be
109
determined (and thus also the governor of the hy-
pernode). For this task, the intra-hypernode de-
pendency generation SVM uses the following fea-
tures:
? lemmas included in the hypernode, ? PoS-tags
of the lemmas in the hypernode, ? voice of the
head h of the hypernode, ? deep dependency re-
lation to h.
[ VB(deep), VAUX, IN]
analyt fut prepos
Figure 2: Internal dependency within a hypernode.
d. Inter-hypernode Dependency Generation.
Once the individual hypernodes have been con-
verted into connected dependency subtrees, the
hypernodes must be connected between each
other, such that we obtain a complete SSyntS. The
inter-hypernode dependency generation SVM uses
the following features of a hypernode ss:
? the internal dependencies of ss, ? the head of
ss, ? the lemmas of ss, ? the PoS of the depen-
dent of the head of ss in DSyntS
to determine for each hypernode its governor.
[ VB(deep), VAUX, IN] [ NN(deep), DT]
subj
Figure 3: Surface dependencies between two hy-
pernodes.
3.2 Implementation of sets of micro
classifiers
In this alternative approach, a single classifier is
foreseen for each kind of input. Thus, for the
hypernode identification module, for each deep
PoS tag (which can be one of the following four:
?N? (noun), ?V? (verb), ?Adv? (adverb), ?A? (ad-
jective)), a separate multi-class classifier is de-
fined. For instance, in the case of ?N?, the N-
classifier will use the above features to assign
to the a DSynt-node with PoS ?N? the most ap-
propriate (most likely) hypernode?in this case,
[NN(deep), DT]. In a similar way, in the case of
the lemma generation module, for each surface
PoS tag, a separate classifier is defined. Thus,
the DT-classifier would pick for the hypernode
[NN(deep), DT] the most likely lemma for the DT-
node (optimally, a determiner).
For the intra-hypernode attachments module,
for each kind of hypernode, dynamically a sepa-
rate classifier is generated.5 In the case of the hy-
5This implies that the number of classifiers varies depend-
ing on the training set, in the intra-hypernode dependency
generation there are 108 SVMs.
pernode [ VB(deep), VAUX, IN], the correspond-
ing classifier will create a link between the prepo-
sition and the auxiliary, and between the auxiliary
and the verb, with respectively the preposition and
the auxiliary as heads because it is the best link
that it can find; cf. Figure 2 for illustration.
Finally, for the inter-hypernode attachments
module, for each hypernode with a distinct in-
ternal dependency pattern, a separate classifier is
dynamically derived (for our treebank, we ob-
tained 114 different SVM classifiers because it
also takes into account hypernodes with just one
token). For instance, the classifier for the hypern-
ode [ NN(deep), DT] is most likely to identify as
its governor VAUX in the hypernode [ VB(deep),
VAUX, IN]; cf. Figure 3.
4 Experiments and Results
In this section, we present the performance of the
two approaches to DSyntS?SSyntS projection on
the DSyntS- and SSynt-layers of the AnCora-UPF
treebank (Mille et al., 2013).6 Table 1 displays
the results for the generic classifier for all tasks
on the development and the test set, while Table
2 displays the results obtained through the sets of
micro classifiers.
Dev.set # %
Hypernode identification 3131/3441 90.99
Lemma generation 818/936 87.39
Intra-hypernode dep. generation 545/798 68.30
Inter-hypernode dep. generation 2588/3055 84.71
Test set # %
Hypernode identification 5166/5887 87.75
Lemma generation 1822/2084 87.43
Intra-hypernode dep. generation 1093/1699 64.33
Inter-hypernode dep. generation 4679/5385 86.89
Table 1: Results of the evaluation of the generic
classifiers for the non-isomorphic transduction.
The results show that for hypernode identifica-
tion and inter-hypernode dependency generation,
the results of both types of classifiers are compara-
ble, be it on the development set or on the test set.
However, thanks to the micro classifiers, with the
same features, the lemma generation model based
on micro classifiers improves by 4 points and the
intra-hypernode dependency generation by nearly
6Following a classical machine learning set-up, we di-
vided the treebank into: (i) a development set (219 sen-
tences, 3271 tokens in the DSyntS treebank and 4953 tokens
in the SSyntS treebank); (ii) a training set (3036 sentences,
57665 tokens in the DSyntS treebank and 86984 tokens in
the SSyntS treebank); and a (iii) a held-out test for evalua-
tion (258 sentences, 5641 tokens in the DSyntS treebank and
8955 tokens in the SSyntS treebank).
110
Dev.set # %
Hypernode identification 3133/3441 91.05
Lemma generation 851/936 90.92
Intra-hypernode dep. generation 767/798 96.12
Inter-hypernode dep. generation 2574/3055 84.26
Test set # %
Hypernode identification 5169/5886 87.82
Lemma generation 1913/2084 91.79
Intra-hypernode dep. generation 1630/1699 95.94
Inter-hypernode dep. generation 4648/5385 86.31
Table 2: Results of the evaluation of the micro
classifiers for the non-isomorphic transduction.
30 points. This means that the intra-hypernode de-
pendency generation task is too sparse to be real-
ized as a single classifier. The micro classifiers
are in this case binary, i.e., 2:1, or unary, i.e., 1:1
classifiers, which implies a tremendous reduction
of the search space (and thus higher accuracy). In
contrast, the single classifier is a multi-class clas-
sifier that must decide among more than 60 pos-
sible classes. Although most of these 60 classes
are diferentiated by features, the differentiation
is not perfect. In the case of lemma generation,
we observe a similar phenomenon. In this case,
the micro-classifiers are multi-class classifiers that
normally have to cope with 5 different classes
(lemmas in this case), while the unique classi-
fier has to cope with around 60 different classes
(or lemmas). Hypernode identification and inter-
hypernode dependency generation are completely
guided by the input; thus, it seems that they do not
err in the same way.
Although the micro classifier approach leads
to significantly better results, we believe that it
can still be improved. First, the introduction of
prepositions causes most errors in hypernode de-
tection and lemma generation: when a preposition
should be introduced or not and which preposi-
tion should be introduced depends exclusively on
the sub-categorization frame of the governor of
the deep node. A treebank of a limited size as
used in our experiments simply does not contain
subcategorization patterns of all predicative lexi-
cal items (especially of nouns)?which would be
crucial. Thus, in the test set evaluation, out of the
171 lemma errors 147 are prepositions and out of
the 717 errors on hypernode identification, more
than 500 are due to nouns and preposition. The in-
crease of the size of the treebank would therefore
be an advantage.
Second, in the case of inter-hypernode depen-
dency, errors are due to the labels of the dependen-
cies more than to the attachements, and are quite
distributed over the different types of configura-
tions. The generation of these dependencies suf-
fers from the fact that the SSyntS tag-set is very
fine-grained. For instance, there are 9 different
types of verbal objects in SSyntS,7 which capture
very specific syntactic properties of Spanish, such
as ?can the dependent can be replaced by a clitic
pronoun? Can the dependent be moved away from
its governor? Etc. This kind of information is not
of a high relevance for generation of well-formed
text. Using a more reduced (more coarse-grained)
SSyntS tag set would definitely improve the qual-
ity of the projection.
5 Related work
There is an increasing amount of work on sta-
tistical sentence generation; see, e.g., (Bangalore
and Rambow, 2000; Langkilde-Geary, 2002; Fil-
ippova and Strube, 2008). However, hardly any
addresses the problem of the projection between
non-isomorphic semantic and syntactic structures.
In general, structure prediction approaches use
a single classifier model (Smith, 2011). But
see, e.g., (Carreras et al., 2008), who use dif-
ferent models to predict each part of the triplet
for spinal model pruning, and (Bjo?rkelund et al.,
2010; Johansson and Nugues, 2008), who use
a set of classifiers for predicate identification in
the context of semantic role labelling. Amalgam
(Corston-Oliver et al., 2002), which maps a logi-
cal input onto sentences with intermediate syntac-
tic (phrase-based) representation, uses language-
specific decision trees in order to predict when to
introduce auxiliaries, determiners, cases, etc.
6 Conclusions
We presented two alternative classifier approaches
to deep generation that cope with the projection
of non-isomorphic semantic and syntactic struc-
tures and argued that the micro classifier approach
is more adequate. In spite of possible improve-
ments presented in Section 4, each set of micro
classifiers achieves results above 86% on the test
set. For intra-hypernode dependency generation,
it even reaches 95.94% .
Acknowledgments
This work has been partially funded by the Euro-
pean Commission under the contract number FP7-
ICT-610411.
7There are 47 SSynt dependencies in total, to compare to
the 7 dependencies in the DSyntS.
111
References
Olga Babko-Malaya, 2005. Propbank Annotation
Guidelines.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gener-
ation. In Proceedings of the 18th International Con-
ference on Computational Linguistics (COLING),
pages 42?48, Saarbru?cken, Germany.
Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first Surface Realisation Shared Task: Overview and
evaluation results. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation (ENLG), pages
217?226, Nancy, France.
Anja Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Journal of Natural Lan-
guage Engineering, 14(4):431?455.
A. Bjo?rkelund, B. Bohnet, L. Hafdell, and P. Nugues.
2010. A high-performance syntactic and semantic
dependency parser. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics : Demonstration Volume (COLING), pages 33?
36, Beijing, China.
Bernd Bohnet, Leo Wanner, Simon Mille, and Ali-
cia Burga. 2010. Broad coverage multilingual
deep sentence generation with a stochastic multi-
level realizer. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING), pages 98?106, Beijing, China.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo
Wanner. 2011. StuMaBa: From deep representation
to surface. In Proceedings of the Generation Chal-
lenges Session at the 13th European Workshop on
Natural Language Generation (ENLG), pages 232?
235, Nancy, France.
Nadjet Bouayad-Agha, Gerard Casamayor, Simon
Mille, and Leo Wanner. 2012. Perspective-oriented
generation of football match summaries: Old tasks,
new challenges. ACM Transactions on Speech and
Language Processing, 9(2):3:1?3:31.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the per-
ceptron for efficient, feature-rich parsing. In Pro-
ceedings of the 12th Conference on Computational
Natural Language Learning (CoNLL), pages 9?16,
Manchester, UK.
Simon Corston-Oliver, Michael Gamon, Eric Ringger,
and Robert Moore. 2002. An overview of Amal-
gam: A machine-learned generation module. In
Proceedings of the 2nd International Natural Lan-
guage Generation Conference (INLG), pages 33?40,
New-York, NY, USA.
Katja Filippova and Michael Strube. 2008. Sen-
tence fusion via dependency graph compression.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 177?185, Honolulu, Hawaii.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based Semantic Role Labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 69?78, Honolulu, Hawaii.
Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of
the 2nd International Natural Language Generation
Conference (INLG), pages 17?24, New-York, NY,
USA. Citeseer.
Simon Mille, Alicia Burga, and Leo Wanner. 2013.
AnCora-UPF: A multi-level annotation of Spanish.
In Proceedings of the 2nd International Conference
on Dependency Linguistics (DepLing), pages 217?
226, Prague, Czech Republic.
Noah A. Smith. 2011. Linguistic Structure Prediction.
Synthesis Lectures on Human Language Technolo-
gies. Morgan and Claypool.
Sebastian Varges and Chris Mellish. 2001. Instance-
based Natural Language Generation. In Proceed-
ings of the 2nd Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL), pages 1?8, Pittsburgh, PA, USA.
112
