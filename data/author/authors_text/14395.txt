Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 952?961, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Exploring Topic Coherence over many models and many topics
Keith Stevens1,2 Philip Kegelmeyer3 David Andrzejewski2 David Buttler2
1University of California Los Angeles; Los Angeles , California, USA
2Lawrence Livermore National Lab; Livermore, California, USA
3Sandia National Lab; Livermore, California, USA
{stevens35,andrzejewski1,buttler1}@llnl.gov
wpk@sandia.gov
Abstract
We apply two new automated semantic eval-
uations to three distinct latent topic models.
Both metrics have been shown to align with
human evaluations and provide a balance be-
tween internal measures of information gain
and comparisons to human ratings of coher-
ent topics. We improve upon the measures
by introducing new aggregate measures that
allows for comparing complete topic models.
We further compare the automated measures
to other metrics for topic models, compar-
ison to manually crafted semantic tests and
document classification. Our experiments re-
veal that LDA and LSA each have different
strengths; LDA best learns descriptive topics
while LSA is best at creating a compact se-
mantic representation of documents and words
in a corpus.
1 Introduction
Topic models learn bags of related words from large
corpora without any supervision. Based on the
words used within a document, they mine topic level
relations by assuming that a single document cov-
ers a small set of concise topics. Once learned,
these topics often correlate well with human con-
cepts. For example, one model might produce topics
that cover ideas such as government affairs, sports,
and movies. With these unsupervised methods, we
can extract useful semantic information in a variety
of tasks that depend on identifying unique topics or
concepts, such as distributional semantics (Jurgens
and Stevens, 2010), word sense induction (Van de
Cruys and Apidianaki, 2011; Brody and Lapata,
2009), and information retrieval (Andrzejewski and
Buttler, 2011).
When using a topic model, we are primarily con-
cerned with the degree to which the learned top-
ics match human judgments and help us differen-
tiate between ideas. But until recently, the evalua-
tion of these models has been ad hoc and applica-
tion specific. Evaluations have ranged from fully
automated intrinsic evaluations to manually crafted
extrinsic evaluations. Previous extrinsic evaluations
have used the learned topics to compactly represent
a small fixed vocabulary and compared this distribu-
tional space to human judgments of similarity (Jur-
gens and Stevens, 2010). But these evaluations are
hand constructed and often costly to perform for
domain-specific topics. Conversely, intrinsic mea-
sures have evaluated the amount of information en-
coded by the topics, where perplexity is one com-
mon example(Wallach et al 2009), however, Chang
et al(2009) found that these intrinsic measures do
not always correlate with semantically interpretable
topics. Furthermore, few evaluations have used the
same metrics to compare distinct approaches such
as Latent Dirichlet Allocation (LDA) (Blei et al
2003), Latent Semantic Analysis (LSA) (Landauer
and Dutnais, 1997), and Non-negative Matrix Fac-
torization (NMF) (Lee and Seung, 2000). This has
made it difficult to know which method is most use-
ful for a given application, or in terms of extracting
useful topics.
We now provide a comprehensive and automated
evaluation of these three distinct models (LDA,
LSA, NMF), for automatically learning semantic
topics. While these models have seen significant im-
provements, they still represent the core differences
between each approach to modeling topics. For our
evaluation, we use two recent automated coherence
measures (Mimno et al 2011; Newman et al 2010)
952
originally designed for LDA that bridge the gap be-
tween comparisons to human judgments and intrin-
sic measures such as perplexity. We consider several
key questions:
1. How many topics should be learned?
2. How many learned topics are useful?
3. How do these topics relate to often used semantic tests?
4. How well do these topics identify similar documents?
We begin by summarizing the three topic mod-
els and highlighting their key differences. We then
describe the two metrics. Afterwards, we focus on
a series of experiments that address our four key
questions and finally conclude with some overall re-
marks.
2 Topic Models
We evaluate three latent factor models that have seen
widespread usage:
1. Latent Dirichlet Allocation
2. Latent Semantic Analysis with Singular Value De-
composition
3. Latent Semantic Analysis with Non-negative Ma-
trix Factorization
Each of these models were designed with differ-
ent goals and are supported by different statistical
theories. We consider both LSA models as topic
models as they have been used in a variety of sim-
ilar contexts such as distributional similarity (Jur-
gens and Stevens, 2010) and word sense induction
(Van de Cruys and Apidianaki, 2011; Brody and
Lapata, 2009). We evaluate these distinct models
on two shared tasks (1) grouping together similar
words while separating unrelated words and (2) dis-
tinguishing between documents focusing on differ-
ent concepts.
We distill the different models into a shared repre-
sentation consisting of two sets of learned relations:
how words interact with topics and how topics inter-
act with documents. For a corpus withD documents
and V words, we denote these relations in terms of
T topics as
(1) a V ? T matrix, W , that indicates the strength
each word has in each topic, and
(2) a T ? D matrix, H , that indicates the strength
each topic has in each document.
T serves as a common parameter to each model.
2.1 Latent Dirichlet Allocation
Latent Dirichlet Allocation (Blei et al 2003) learns
the relationships between words, topics, and docu-
ments by assuming documents are generated by a
particular probabilistic model. It first assumes that
there are a fixed set of topics, T used throughout the
corpus, and each topic z is associated with a multi-
nomial distribution over the vocabulary?z , which is
drawn from a Dirichlet prior Dir(?). A given docu-
ment Di is then generated by the following process
1. Choose ?i ? Dir(?), a topic distribution for Di
2. For each word wj ? Di:
(a) Select a topic zj ? ?i
(b) Select the word wj ? ?zj
In this model, the ? distributions represent the
probability of each topic appearing in each docu-
ment and the ? distributions represent the proba-
bility of words being used for each topic. These
two sets of distributions correspond to our H and W
matrices, respectively. The process above defines a
generative model; given the observed corpus, we use
collapsed Gibbs sampling implementation found in
Mallet1 to infer the values of the latent variables ?
and ? (Griffiths and Steyvers, 2004). The model re-
lies only on two additional hyper parameters, ? and
?, that guide the distributions.
2.2 Latent Semantic Analysis
Latent Semantic Analysis (Landauer and Dutnais,
1997; Landauer et al 1998) learns topics by first
forming a traditional term by document matrix used
in information retrieval and then smoothing the
counts to enhance the weight of informative words.
Based on the original LSA model, we use the Log-
Entropy transform. LSA then decomposes this
smoothed, term by document matrix in order to gen-
eralize observed relations between words and docu-
ments. For both LSA models, we used implementa-
tions found in the S-Space package.2
Traditionally, LSA has used the Singular Value
Decomposition, but we also consider Non-negative
Matrix Factorization as we?ve seen NMF applied
in similar situations (Pauca et al 2004) and others
1http://mallet.cs.umass.edu/
2https://github.com/fozziethebeat/S-Space
953
Model Label Top Words UMass UCI
High Quality Topics
LDA interview told asked wanted interview people made thought time called knew -2.52 1.29wine wine wines bottle grapes made winery cabernet grape pinot red -1.97 1.30
NMF grilling grilled sweet spicy fried pork dish shrimp menu dishes sauce -1.01 1.98cloning embryonic cloned embryo human research stem embryos cell cloning cells -1.84 1.46
SVD cooking sauce food restaurant water oil salt chicken pepper wine cup -1.87 -1.21stocks fund funds investors weapons stocks mutual stock movie film show -2.30 -1.88
Low Quality Topics
LDA rates 10-yr rate 3-month percent 6-month bds bd 30-yr funds robot -1.94 -12.32charity fund contributions .com family apartment charities rent 22d children assistance -2.43 -8.88
NMF plants stem fruitful stems trunk fruiting currants branches fence currant espalier -3.12 -12.59farming buzzards groundhog prune hoof pruned pruning vines wheelbarrow tree clematis -1.90 -12.56
SVD city building city area buildings p.m. floors house listed eat-in a.m. -2.70 -8.03time p.m. system study a.m. office political found school night yesterday -1.67 -7.02
Table 1: Top 10 words from several high and low quality topics when ordered by the UCI Coherence
Measure. Topic labels were chosen in an ad hoc manner only to briefly summarize the topic?s focus.
have found a connection between NMF and Proba-
bilistic Latent Semantic Analysis (Ding et al 2008),
an extension to LSA.We later refer to these two LSA
models simply as SVD and NMF to emphasize the
difference in factorization method.
Singular Value Decomposition decomposes M
into three smaller matrices
M = U?V T
and minimizes Frobenius norm of M ?s reconstruc-
tion error with the constraint that the rows of U and
V are orthonormal eigenvectors. Interestingly, the
decomposition is agnostic to the number of desired
dimensions. Instead, the rows and columns in U and
V T are ordered based on their descriptive power, i.e.
how well they remove noise, which is encoded by
the diagonal singular value matrix ?. As such, re-
duction is done by retaining the first T rows and
columns from U and V T . For our generalization,
we use W = U? and H = ?V T . We note that
values in U and V T can be both negative and pos-
itive, preventing a straightforward interpretation as
unnormalized probabilities
Non-negative Matrix Factorization also factor-
izes M by minimizing the reconstruction error, but
with only one constraint: the decomposed matrices
consist of only non-negative values. In this respect,
we can consider it to be learning an unnormalized
probability distributions over topics. We use the
original Euclidean least squares definition of NMF3.
Formally, NMF is defined as
M = WH
where H and W map directly onto our generaliza-
tion. As in the original NMF work, we learn these
unnormalized probabilities by initializing each set of
probabilities at random and update them according
to the following iterative update rules
W = W MHTWHHT H = H
WTM
WTWH
3 Coherence Measures
Topic Coherence measures score a single topic by
measuring the degree of semantic similarity between
high scoring words in the topic. These measure-
ments help distinguish between topics that are se-
mantically interpretable topics and topics that are ar-
tifacts of statistical inference, see Table 1 for exam-
ples ordered by the UCI measure. For our evalua-
tions, we consider two new coherence measures de-
signed for LDA, both of which have been shown to
match well with human judgements of topic quality:
(1) The UCI measure (Newman et al 2010) and (2)
The UMass measure (Mimno et al 2011).
Both measures compute the coherence of a topic
as the sum of pairwise distributional similarity
3We note that the alternative KL-Divergence form of NMF
has been directly linked to PLSA (Ding et al 2008)
954
scores over the set of topic words, V . We generalize
this as
coherence(V ) =
?
(vi,vj)?V
score(vi, vj , )
where V is a set of word describing the topic and 
indicates a smoothing factor which guarantees that
score returns real numbers. (We will be exploring
the effect of the choice of ; the original authors used
 = 1.)
The UCI metric defines a word pair?s score to
be the pointwise mutual information (PMI) between
two words, i.e.
score(vi, vj , ) = log
p(vi, vj) + 
p(vi)p(vj)
The word probabilities are computed by counting
word co-occurrence frequencies in a sliding window
over an external corpus, such as Wikipedia. To some
degree, this metric can be thought of as an external
comparison to known semantic evaluations.
The UMass metric defines the score to be based
on document co-occurrence:
score(vi, vj , ) = log
D(vi, vj) + 
D(vj)
whereD(x, y) counts the number of documents con-
taining words x and y and D(x) counts the num-
ber of documents containing x. Significantly, the
UMass metric computes these counts over the orig-
inal corpus used to train the topic models, rather
than an external corpus. This metric is more intrin-
sic in nature. It attempts to confirm that the models
learned data known to be in the corpus.
4 Evaluation
We evaluate the quality of our three topic models
(LDA, SVD, and NMF) with three experiments. We
focus first on evaluating aggregate coherence meth-
ods for a complete topic model and consider the
differences between each model as we learn an in-
creasing number of topics. Secondly, we compare
coherence scores to previous semantic evaluations.
Lastly, we use the learned topics in a classifica-
tion task and evaluate whether or not coherent top-
ics are equally informative when discriminating be-
tween documents.
For all our experiments, we trained our models on
92,600 New York Times articles from 2003 (Sand-
haus, 2008). For all articles, we removed stop words
and any words occurring less than 200 times in the
corpus, which left 35,836 unique tokens. All doc-
uments were tokenized with OpenNLP?s MaxEnt4
tokenizer. For the UCI measure, we compute the
PMI between words using a 20 word sliding win-
dow passed over the WaCkypedia corpus (Baroni et
al., 2009). In all experiments, we compute the co-
herence with the top 10 words from each topic that
had the highest weight, in terms of LDA and NMF
this corresponds with a high probability of the term
describing the topic but for SVD there is no clear
semantic interpretation.
4.1 Aggregate methods for topic coherence
Before we can compare topic models, we require an
aggregate measure that represents the quality of a
complete model, rather than individual topics. We
consider two aggregates methods: (1) the average
coherence of all topics and (2) the entropy of the co-
herence for all topics. The average coherence pro-
vides a quick summarization of a model?s quality
whereas the entropy provides an alternate summa-
rization that differentiates between two interesting
situations. Since entropy measures the complexity
of a probability distribution, it can easily differenti-
ate between uniform distributions and multimodal,
distributions. This distinction is relevant when users
prefer to have roughly uniform topic quality instead
of a wide gap between high- and low-quality topics,
or vice versa. We compute the entropy by dropping
the log and  factor from each scoring function.
Figure 1 shows the average coherence scores for
each model as we vary the number of topics. These
average scores indicate some simple relationships
between the models: LDA and NMF have approx-
imately the same performance and both models are
consistently better than SVD. All of the models
quickly reach a stable average score at around 100
topics. This initially suggests that learning more
4http://incubator.apache.org/opennlp/
955
Number of topics
Avera
ge To
pic Co
heren
ce
?5
?4
?3
?2
?1
0
100 200 300 400 500
(a) UMass
Number of topics
Avera
ge To
pic Co
heren
ce
?10
?8
?6
?4
?2
0
2
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 1: Average Topic Coherence for each model
Number of topics
Cohe
rence
 Entro
py
0
1
2
3
4
5
6
7
100 200 300 400 500
(a) UMass
Number of topics
Cohe
rence
 Entro
py
0
1
2
3
4
5
6
7
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 2: Entropy of the Topic Coherence for each model
topics neither increases or decreases the quality of
the model, but Figure 2 indicates otherwise. While
the entropy for the UMass score stays stable for all
models, NMF produces erratic entropy results under
the UCI score as we learn more topics. As entropy is
higher for even distributions and lower for all other
distributions, these results suggest that the NMF is
learning topics with drastically different levels of
quality, i.e. some with high quality and some with
very low quality, but the average coherence over all
topics do not account for this.
Low quality topics may be composed of highly
unrelated words that can?t be fit into another topic,
and in this case, our smoothing factor, , may be ar-
tificially increasing the score for unrelated words.
Following the practice of the original use of these
metrics, in Figures 1 and 2 we set  = 1. In Fig-
ure 3, we consider  = 10?12, which should sig-
nificantly reduce the score for completely unrelated
words. Here, we see a significant change in the per-
formance of NMF, the average coherence decreases
dramatically as we learn more topics. Similarly, per-
formance of SVD drops dramatically and well below
the other models. In figure 4 we lastly compute the
average coherence using only the top 10% most co-
herence topics with  = 10?12. Here, NMF again
performs on par with LDA. With the top 10% topics
still having a high average coherence but the full set
956
Number of topics
Avera
ge To
pic Co
heren
ce
?5
?4
?3
?2
?1
0
100 200 300 400 500
(a) UMass
Number of topics
Avera
ge To
pic Co
heren
ce
?10
?8
?6
?4
?2
0
2
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 3: Average Topic Coherence with  = 10?12
Number of topics
Avera
ge Co
heren
ce of 
top 10
%
?5
?4
?3
?2
?1
0
100 200 300 400 500
(a) UMass
Number of topics
Avera
ge Co
heren
ce of 
top 10
%
?10
?8
?6
?4
?2
0
2
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 4: Average Topic Coherence of the top 10% topics with  = 10?12
of topics having a low coherence, NMF appears to
be learning more low quality topics once it?s learned
the first 100 topics, whereas LDA learns fewer low
quality topics in general.
4.2 Word Similarity Tasks
The initial evaluations for each coherence mea-
sure asked human judges to directly evaluate top-
ics (Newman et al 2010; Mimno et al 2011). We
expand upon this comparison to human judgments
by considering word similarity tasks that have of-
ten been used to evaluate distributional semantic
spaces (Jurgens and Stevens, 2010). Here, we use
the learned topics as generalized semantics describ-
ing our knowledge about words. If a model?s topics
generalize the knowledge accurately, we would ex-
pect similar words, such as ?cat? and ?dog?, to be
represented with a similar set of topics. Rather than
evaluating individual topics, this similarity task con-
siders the knowledge within the entire set of topics,
the topics act as more compact representation for the
known words in a corpus.
We use the Rubenstein and Goodenough (1965)
and Finkelstein et al(2002) word similarity tasks.
In each task, human judges were asked to evaluate
the similarity or relatedness between different sets of
word pairs. Fifty-One Evaluators for the Rubenstein
and Goodenough (1965) dataset were given 65 pairs
957
Tscor
e
0.0
0.1
0.2
0.3
0.4
0.5
0.6
100 200 300 400 500
modelLDANMFSVD
(a) Rubenstein & Goodenough
T
scor
e
0.0
0.1
0.2
0.3
0.4
0.5
100 200 300 400 500
modelLDANMFSVD
(b) Wordsim 353/Finklestein et. al.
Figure 5: Word Similarity Evaluations for each model
Topics
Corre
lation
0.0
0.2
0.4
0.6
100 200 300 400 500
(a) UMass
Topics
Corre
lation
0.0
0.2
0.4
0.6
100 200 300 400 500
modelLDANMFSVD
(b) UCI
Figure 7: Correlation between topic coherence and topic ranking in classification
of words and asked to rate their similarity on a scale
from 0 to 4, where a higher score indicates a more
similar word pair. Finkelstein et al(2002) broadens
the word similarity evaluation and asked 13 to 16
different subjects to rate 353 word pairs on a scale
from 0 to 10 based on their relatedness, where relat-
edness includes similarity and other semantic rela-
tions. We can evaluate each topic model by comput-
ing the cosine similarity between each pair of words
in the evaluate set and then compare the model?s
ratings to the human ratings by ranked correlation.
A high correlation signifies that the topics closely
model human judgments.
Figure 5 displays the results. SVD and LDA
both surpass NMF on the Rubenstein & Goode-
nough test while SVD is clearly the best model on
the Finklestein et. al test. While our first experi-
ment showed that SVDwas the worst model in terms
of topic coherence scores, this experiment indicates
that SVD provides an accurate, stable, and reliable
approximation to human judgements of similarity
and relatedness between word pairs in comparison
to other topic models.
4.3 Coherence versus Classification
For our final experiment, we examine the relation-
ship between topic coherence and classification ac-
curacy for each topic model. We suspect that highly
958
score
Corre
lation
0.01
0.02
0.03
0.04
0.05
0.06
0.07
?25 ?20 ?15 ?10 ?5
(a) UMass
score
Corre
lation
0.01
0.02
0.03
0.04
0.05
0.06
0.07
?30 ?20 ?10 0
modelLDANMFSVD
(b) UCI
Figure 8: Comparison between topic coherence and topic rank with 500 topics
Topics
Accu
racy
20
30
40
50
60
70
80
100 200 300 400 500
ModelLDANMFSVD
Figure 6: Classification accuracy for each model
coherent topics, and coherent topic models, will per-
form better for classification. We address this ques-
tion by performing a document classification task
using the topic representations of documents as in-
put features and examine the relationship between
topic coherence and the usefulness of the corre-
sponding feature for classification.
We trained each topic model with all 92,600 New
York Times articles as before. We use the sec-
tion labels provided for each article as class labels,
where each label indicates the on-line section(s) un-
der which the article was published and should thus
be related to the topics contained in each article. To
reduce the noise in our data set we narrow down the
articles to those that have only one label and whose
label is applied to at least 2000 documents. This re-
sults in 57,696 articles with label distributions listed
in Table 2. We then represent each document using
columns in the topic by document matrix H learned
for each topic model.
Label Count Label Count
New York and Region 11219 U.S. 3675
Paid Death Notices 11152 Arts 3437
Opinion 8038 World 3330
Business 7494 Style 2137
Sports 7214
Table 2: Section label counts for New York Times
articles used for classification
For each topic model trained on N topics, we
performed stratified 10-fold cross-validation on the
57,696 labeled articles. In each fold, we build an
automatically-sized bagged ensemble of unpruned
CART-style decision trees(Banfield et al 2007) on
90% of the dataset5, use that ensemble to assign la-
bels to the other 10%, and measure the accuracy of
that assignment. Figure 6 shows the average classifi-
cation accuracy over all ten folds for each model. In-
terestingly, SVD has slightly, but statistically signif-
icantly, higher accuracy results than both NMF and
LDA. Furthermore, performance quickly increases
5The precise choice of the classifier scheme matters little, as
long as it is accurate, speedy, and robust to label noise; all of
which is true of the choice here.
959
and plateaus with well under 50 topics.
Our bagged decision trees can also determine the
importance of each feature during classification. We
evaluate the strength of each topic during classifi-
cation by tracking the number of times each node
in our decision trees observe each topic, please see
(Caruana et al 2006) for more details. Figure 8 plot
the relationship between this feature ranking and the
topic coherence for each topic when training LDA,
SVD, and NMF on 500 topics. Most topics for each
model provide little classification information, but
SVD shows a much higher rank for several topics
with a relatively higher coherence score. Interest-
ingly, for all models, the most coherent topics are not
the most informative. Figure 7 plots a more compact
view of this same relationship: the Spearman rank
correlation between classification feature rank and
topic coherence. NMF shows the highest correlation
between rank and coherence, but none of the mod-
els show a high correlation when using more than
100 topics. SVD has the lowest correlation, which
is probably due to the model?s overall low coherence
yet high classification accuracy.
5 Discussion and Conclusion
Through our experiments, we made several excit-
ing and interesting discoveries. First, we discov-
ered that the coherence metrics depend heavily on
the smoothing factor . The original value, 1.0 cre-
ated a positive bias towards NMF from both met-
rics even when NMF generated incoherent topics.
The high smoothing factor also gave a significant in-
crease to SVD scores. We suspect that this was not
an issue in previous studies with the coherence mea-
sures as LDA prefers to form topics from words that
co-occur frequently, whereas NMF and SVD have
no such preferences and often create low quality top-
ics from completely unrelated words. Therefore, we
suggest a smaller  value in general.
We also found that the UCI measure often agreed
with the UMass measure, but the UCI-entropy ag-
gregate method induced more separation between
LSA, SVD, and NMF in terms of topic coherence.
This measure also revealed the importance of the
smoothing factor for topic coherence measures.
With respects to human judgements, we found
that coherence scores do not always indicate a bet-
ter representation of distributional information. The
SVD model consistently out performed both LDA
and NMF models, which each had higher coherence
scores, when attempting to predict human judge-
ments of similarity.
Lastly, we found all models capable of producing
topics that improved document classification. At the
same time, SVD provided the most information dur-
ing classification and outperformed the other mod-
els, which again had more coherent topics. Our com-
parison between topic coherence scores and feature
importance in classification revealed that relatively
high quality topics, but not the most coherent topics,
drive most of the classification decisions, and most
topics do not affect the accuracy.
Overall, we see that each topic model paradigm
has it?s own strengths and weaknesses. Latent Se-
mantic Analysis with Singular Value Decomposition
fails to form individual topics that aggregate similar
words, but it does remarkably well when consider-
ing all the learned topics as similar words develop
a similar topic representation. These topics simi-
larly perform well during classification. Conversely,
both Non Negative Matrix factorization and Latent
Dirichlet Allocation learn concise and coherent top-
ics and achieved similar performance on our evalua-
tions. However, NMF learns more incoherent topics
than LDA and SVD. For applications in which a hu-
man end-user will interact with learned topics, the
flexibility of LDA and the coherence advantages of
LDA warrant strong consideration. All of code for
this work will be made available through an open
source project.6
6 Acknowledgments
This work was performed under the auspices of
the U.S. Department of Energy by Lawrence Liv-
ermore National Laboratory under Contract DE-
AC52-07NA27344 (LLNL-CONF-522871) and by
Sandia National Laboratory under Contract DE-
AC04-94AL85000.
References
David Andrzejewski and David Buttler. 2011. Latent
topic feedback for information retrieval. In Proceed-
6https://github.com/fozziethebeat/TopicModelComparison
960
ings of the 17th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
?11, pages 600?608, New York, NY, USA. ACM.
Robert E. Banfield, Lawrence O. Hall, Kevin W. Bowyer,
andW. Philip Kegelmeyer. 2007. A comparison of de-
cision tree ensemble creation techniques. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
29(1):173?180, January.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alcation. J. Mach. Learn.
Res., 3:993?1022, March.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?09, pages 103?
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Rich Caruana, Mohamed Elhawary, Art Munson, Mirek
Riedewald, Daria Sorokina, Daniel Fink, Wesley M.
Hochachka, and Steve Kelling. 2006. Mining cit-
izen science data to predict orevalence of wild bird
species. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ?06, pages 909?915, New York,
NY, USA. ACM.
Jonathan Chang, Sean Gerrish, Chong Wang, and
David M Blei. 2009. Reading tea leaves : How hu-
mans interpret topic models. New York, 31:1?9.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Comput. Stat.
Data Anal., 52:3913?3927, April.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Trans. Inf. Syst., 20:116?131, January.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235, April.
David Jurgens and Keith Stevens. 2010. The s-space
package: an open source package for word space mod-
els. In Proceedings of the ACL 2010 System Demon-
strations, ACLDemos ?10, pages 30?35, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Thomas K Landauer and Susan T. Dutnais. 1997. A so-
lution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, pages 211?240.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. An Introduction to Latent Semantic Analysis.
Discourse Processes, (25):259?284.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In In
NIPS, pages 556?562. MIT Press.
David Mimno, Hanna Wallach, Edmund Talley, Miriam
Leenders, and Andrew McCallum. 2011. Optimizing
semantic coherence in topic models. In Proceedings of
the 2011 Conference on Emperical Methods in Natu-
ral Language Processing, pages 262?272, Edinburgh,
Scotland, UK. Association of Computational Linguis-
tics.
David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010. Evaluating topic
models for digital libraries. In Proceedings of the 10th
annual joint conference on Digital libraries, JCDL
?10, pages 215?224, New York, NY, USA. ACM.
V Paul Pauca, Farial Shahnaz, Michael W Berry, and
Robert J Plemmons, 2004. Text mining using nonnega-
tive matrix factorizations, volume 54, pages 452?456.
SIAM.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8:627?633, October.
Evan Sandhaus. 2008. The New York Times Annotated
Corpus.
Tim Van de Cruys and Marianna Apidianaki. 2011. La-
tent semantic word sense induction and disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 1476?1485, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In Proceedings of the 26th International Con-
ference on Machine Learning (ICML). Omnipress.
961
Proceedings of the ACL 2010 System Demonstrations, pages 30?35,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
The S-Space Package: An Open Source Package for Word Space Models
David Jurgens
University of California, Los Angeles,
4732 Boelter Hall
Los Angeles, CA 90095
jurgens@cs.ucla.edu
Keith Stevens
University of California, Los Angeles,
4732 Boelter Hall
Los Angeles, CA 90095
kstevens@cs.ucla.edu
Abstract
We present the S-Space Package, an open
source framework for developing and eval-
uating word space algorithms. The pack-
age implements well-known word space
algorithms, such as LSA, and provides a
comprehensive set of matrix utilities and
data structures for extending new or ex-
isting models. The package also includes
word space benchmarks for evaluation.
Both algorithms and libraries are designed
for high concurrency and scalability. We
demonstrate the efficiency of the reference
implementations and also provide their re-
sults on six benchmarks.
1 Introduction
Word similarity is an essential part of understand-
ing natural language. Similarity enables meaning-
ful comparisons, entailments, and is a bridge to
building and extending rich ontologies for evaluat-
ing word semantics. Word space algorithms have
been proposed as an automated approach for de-
veloping meaningfully comparable semantic rep-
resentations based on word distributions in text.
Many of the well known algorithms, such as
Latent Semantic Analysis (Landauer and Dumais,
1997) and Hyperspace Analogue to Language
(Burgess and Lund, 1997), have been shown to
approximate human judgements of word similar-
ity in addition to providing computational mod-
els for other psychological and linguistic phenom-
ena. More recent approaches have extended this
approach to model phenomena such as child lan-
guage acquisition (Baroni et al, 2007) or seman-
tic priming (Jones et al, 2006). In addition, these
models have provided insight in fields outside of
linguistics, such as information retrieval, natu-
ral language processing and cognitive psychology.
For a recent survey of word space approaches and
applications, see (Turney and Pantel, 2010).
The parallel development of word space models
in different fields has often resulted in duplicated
work. The pace of development presents a need
for a reliable method for accurate comparisons be-
tween new and existing approaches. Furthermore,
given the frequent similarity of approaches, we
argue that the research community would greatly
benefit from a common library and evaluation util-
ities for word spaces. Therefore, we introduce the
S-Space Package, an open source framework with
four main contributions:
1. reference implementations of frequently
cited algorithms
2. a comprehensive, highly concurrent library of
tools for building new models
3. an evaluation framework for testing mod-
els on standard benchmarks, e.g. the TOEFL
Synonym Test (Landauer et al, 1998)
4. a standardized interface for interacting with
all word space models, which facilitates word
space based applications.
The package is written in Java and defines a
standardized Java interface for word space algo-
rithms. While other word space frameworks ex-
ist, e.g. (Widdows and Ferraro, 2008), the focus
of this framework is to ease the development of
new algorithms and the comparison against exist-
ing models. Compared to existing frameworks,
the S-Space Package supports a much wider vari-
ety of algorithms and provides significantly more
reusable developer utilities for word spaces, such
as tokenizing and filtering, sparse vectors and
matrices, specialized data structures, and seam-
less integration with external programs for di-
mensionality reduction and clustering. We hope
that the release of this framework will greatly fa-
cilitate other researchers in their efforts to de-
velop and validate new word space models. The
toolkit is available at http://code.google.com/
p/airhead-research/, which includes a wiki
30
containing detailed information on the algorithms,
code documentation and mailing list archives.
2 Word Space Models
Word space models are based on the contextual
distribution in which a word occurs. This ap-
proach has a long history in linguistics, starting
with Firth (1957) and Harris (1968), the latter
of whom defined this approach as the Distribu-
tional Hypothesis: for two words, their similarity
in meaning is predicted by the similarity of the
distributions of their co-occurring words. Later
models have expanded the notion of co-occurrence
but retain the premise that distributional similarity
can be used to extract meaningful relationships be-
tween words.
Word space algorithms consist of the same core
algorithmic steps: word features are extracted
from a corpus and the distribution of these features
is used as a basis for semantic similarity. Figure 1
illustrates the shared algorithmic structure of all
the approaches, which is divided into four compo-
nents: corpus processing, context selection, fea-
ture extraction and global vector space operations.
Corpus processing normalizes the input to cre-
ate a more uniform set of features on which the al-
gorithm can work. Corpus processing techniques
frequently include stemming and filtering of stop
words or low-frequency words. For web-gathered
corpora, these steps also include removal of non
linguistic tokens, such as html markup, or restrict-
ing documents to a single language.
Context selection determines which tokens in a
document may be considered for features. Com-
mon approaches use a lexical distance, syntac-
tic relation, or document co-occurrence to define
the context. The various decisions for selecting
the context accounts for many differences between
otherwise similar approaches.
Feature extraction determines the dimensions of
the vector space by selecting which tokens in the
context will count as features. Features are com-
monly word co-occurrences, but more advanced
models may perform a statistical analysis to se-
lect only those features that best distinguish word
meanings. Other models approximate the full set
of features to enable better scalability.
Global vector space operations are applied to
the entire space once the initial word features have
been computed. Common operations include al-
tering feature weights and dimensionality reduc-
Document-Based Models
LSA (Landauer and Dumais, 1997)
ESA (Gabrilovich and Markovitch, 2007)
Vector Space Model (Salton et al, 1975)
Co-occurrence Models
HAL (Burgess and Lund, 1997)
COALS (Rohde et al, 2009)
Approximation Models
Random Indexing (Sahlgren et al, 2008)
Reflective Random Indexing (Cohen et al, 2009)
TRI (Jurgens and Stevens, 2009)
BEAGLE (Jones et al, 2006)
Incremental Semantic Analysis (Baroni et al, 2007)
Word Sense Induction Models
Purandare and Pedersen (Purandare and Pedersen, 2004)
HERMIT (Jurgens and Stevens, 2010)
Table 1: Algorithms in the S-Space Package
tion. These operations are designed to improve
word similarity by changing the feature space it-
self.
3 The S-Space Framework
The S-Space framework is designed to be extensi-
ble, simple to use, and scalable. We achieve these
goals through the use of Java interfaces, reusable
word space related data structures, and support for
multi-threading. Each word space algorithm is de-
signed to run as a stand alone program and also to
be used as a library class.
3.1 Reference Algorithms
The package provides reference implementations
for twelve word space algorithms, which are listed
in Table 1. Each algorithm is implemented in its
own Java package, and all commonalities have
been factored out into reusable library classes.
The algorithms implement the same Java interface,
which provides a consistent abstraction of the four
processing stages.
We divide the algorithms into four categories
based on their structural similarity: document-
based, co-occurrence, approximation, and Word
Sense Induction (WSI) models. Document-based
models divide a corpus into discrete documents
and construct the vector space from word fre-
quencies in the documents. The documents are
defined independently of the words that appear
in them. Co-occurrence models build the vector
space using the distribution of co-occurring words
in a context, which is typically defined as a re-
gion around a word or paths rooted in a parse
tree. The third category of models approximate
31
Corpus Processing Context Selection Feature Extraction Global Operations
Vector Space
Token Filtering
Stemming
Bigramming
Dimensionality Reduction
Feature Selection
Matrix Transforms
Lexical Distance
In Same Document
Syntactic Link
Word Co-occurence
Joint Probabilitiy
Approximation
Corpus
Figure 1: A high-level depiction of common algorithmic steps that convert a corpus into a word space
co-occurrence data rather than model it explic-
itly in order to achieve better scalability for larger
data sets. WSI models also use co-occurrence but
also attempt to discover distinct word senses while
building the vector space. For example, these al-
gorithms might represent ?earth? with two vectors
based on its meanings ?planet? and ?dirt.?
3.2 Data Structures and Utilities
The S-Space Package provides efficient imple-
mentations for matrices, vectors, and specialized
data structures such as multi-maps and tries. Im-
plementations are modeled after the java.util li-
brary and offer concurrent implementations when
multi-threading is required. In addition, the li-
braries provide support for converting between
multiple matrix formats, enabling interaction with
external matrix-based programs. The package also
provides support for parsing different corpora for-
mats, such as XML or email threads.
3.3 Global Operation Utilities
Many algorithms incorporate dimensionality re-
duction to smooth their feature data, e.g. (Lan-
dauer and Dumais, 1997; Rohde et al, 2009),
or to improve efficiency, e.g. (Sahlgren et al,
2008; Jones et al, 2006). The S-Space Pack-
age supports two common techniques: the Sin-
gular Value Decomposition (SVD) and random-
ized projections. All matrix data structures are de-
signed to seamlessly integrate with six SVD im-
plementations for maximum portability, including
SVDLIBJ1 , a Java port of SVDLIBC2, a scalable
sparse SVD library. The package also provides
a comprehensive library for randomized projec-
tions, which project high-dimensional feature data
into a lower dimensional space. The library sup-
ports both integer-based projections (Kanerva et
al., 2000) and Gaussian-based (Jones et al, 2006).
The package supports common matrix trans-
formations that have been applied to word
spaces: point wise mutual information (Dekang,
1http://bender.unibe.ch/svn/codemap/Archive/svdlibj/
2http://tedlab.mit.edu/?dr/SVDLIBC/
1998), term frequency-inverse document fre-
quency (Salton and Buckley, 1988), and log en-
tropy (Landauer and Dumais, 1997).
3.4 Measurements
The choice of similarity function for the vector
space is the least standardized across approaches.
Typically the function is empirically chosen based
on a performance benchmark and different func-
tions have been shown to provide application spe-
cific benefits (Weeds et al, 2004). To facili-
tate exploration of the similarity function param-
eter space, the S-Space Package provides sup-
port for multiple similarity functions: cosine sim-
ilarity, Euclidean distance, KL divergence, Jac-
card Index, Pearson product-moment correlation,
Spearman?s rank correlation, and Lin Similarity
(Dekang, 1998)
3.5 Clustering
Clustering serves as a tool for building and refin-
ing word spaces. WSI algorithms, e.g. (Puran-
dare and Pedersen, 2004), use clustering to dis-
cover the different meanings of a word in a cor-
pus. The S-Space Package provides bindings for
using the CLUTO clustering package3. In addi-
tion, the package provides Java implementations
of Hierarchical Agglomerative Clustering, Spec-
tral Clustering (Kannan et al, 2004), and the Gap
Statistic (Tibshirani et al, 2000).
4 Benchmarks
Word space benchmarks assess the semantic con-
tent of the space through analyzing the geomet-
ric properties of the space itself. Currently used
benchmarks assess the semantics by inspecting the
representational similarity of word pairs. Two
types of benchmarks are commonly used: word
choice tests and association tests. The S-Space
Package supports six tests, and has an easily ex-
tensible model for adding new tests.
3http://glaros.dtc.umn.edu/gkhome/views/cluto
32
Word Choice Word Association
Algorithm Corpus TOEFL ESL RDWP R-G WordSim353 Deese
BEAGLE TASA 46.03 35.56 46.99 0.431 0.342 0.235
COALS TASA 65.33 60.42 93.02 0.572 0.478 0.388
HAL TASA 44.00 20.83 50.00 0.173 0.180 0.318
HAL Wiki 50.00 31.11 43.44 0.261 0.195 0.042
ISA TASA 41.33 18.75 33.72 0.245 0.150 0.286
LSA TASA 56.00a 50.00 45.83 0.652 0.519 0.349
LSA Wiki 60.76 54.17 59.20 0.681 0.614 0.206
P&P TASA 34.67 20.83 31.39 0.088 -0.036 0.216
RI TASA 42.67 27.08 34.88 0.224 0.201 0.211
RI Wiki 68.35 31.25 40.80 0.226 0.315 0.090
RI + Perm.b TASA 52.00 33.33 31.39 0.137 0.260 0.268
RRI TASA 36.00 22.92 34.88 0.088 0.138 0.109
VSM TASA 61.33 52.08 84.88 0.496 0.396 0.200
a Landauer et al (1997) report a score of 64.4 for this test, while Rohde et al (2009) report a score of 53.4.
b + Perm indicates that permutations were used with Random Indexing, as described in (Sahlgren et al, 2008)
Table 2: A comparison of the implemented algorithms on common evaluation benchmarks
4.1 Word Choice
Word choice tests provide a target word and a list
of options, one of which has the desired relation to
the target. Word space models solve these tests by
selecting the option whose representation is most
similar. Three word choice benchmarks that mea-
sure synonymy are supported.
The first test is the widely-reported Test of En-
glish as a Foreign Language (TOEFL) synonym
test from (Landauer et al, 1998), which consists
of 80 multiple-choice questions with four options.
The second test comes from the English as a Sec-
ond Language (ESL) exam and consists of 50
question with four choices (Turney, 2001). The
third consists of 200 questions from the Canadian
Reader?s Digest Word Power (RDWP) (Jarmasz
and Szpakowicz, 2003), which unlike the previ-
ous two tests, allows the target and options to be
multi-word phrases.
4.2 Word Association
Word association tests measure the semantic re-
latedness of two words by comparing word space
similarity with human judgements. Frequently,
these tests measure synonymy; however, other
types of word relations such as antonymy (?hot?
and ?cold?) or functional relatedness (?doctor?
and ?hospital?) are also possible. The S-Space
Package supports three association tests.
The first test uses data gathered by Rubenstein
and Goodneough (1965). To measure word simi-
larity, word similarity scores of 51 human review-
ers were gathered a set of 65 noun pairs, scored on
a scale of 0 to 4. The ratings are then correlated
with word space similarity scores.
Finkelstein et al (2002) test for relatedness. 353
word pairs were rated by either 13 or 16 subjects
on a 0 to 10 scale for how related the words are.
This test is notably more challenging for word
space models because human ratings are not tied
to a specific semantic relation.
The third benchmark considers the antonym as-
sociation. Deese (1964) introduced 39 antonym
pairs that Greffenstette (1992) used to assess
whether a word space modeled the antonymy rela-
tionship. We quantify this relationship by measur-
ing the similarity rank of each word in an antonym
pair, w1, w2, i.e. w2 is the kth most-similar word
to w1 in the vector space. The antonym score is
calculated as 2rankw1 (w2)+rankw2 (w1) . The score
ranges from [0, 1], where 1 indicates that the most
similar neighbors in the space are antonyms. We
report the mean score for all 39 antonyms.
5 Algorithm Analysis
The content of a word space is fundamentally
dependent upon the corpus used to construct it.
Moreover, algorithms which use operations such
as the SVD have a limit to the corpora sizes they
33
 0
 5000
 10000
 15000
 20000
 25000
 100000  200000  300000  400000  500000  600000
63.5M 125M 173M 228M 267M 296M
Se
co
nd
s
Number of documents
Tokens in Documents (in millions)
LSA
VSM
COALS
BEAGLE
HAL
RI
Figure 2: Processing time across different corpus
sizes for a word space with the 100,000 most fre-
quent words
 0
 100
 200
 300
 400
 500
 600
 700
 800
2 3 4 5 6 7 8
Pe
rc
en
ta
ge
 im
pr
ov
em
en
t
Number of threads
RRI
BEAGLE
COALS
LSA
HAL
RI
VSM
Figure 3: Run time improvement as a factor of in-
creasing the number of threads
can process. We therefore highlight the differ-
ences in performance using two corpora. TASA
is a collection of 44,486 topical essays introduced
in (Landauer and Dumais, 1997). The second cor-
pus is built from a Nov. 11, 2009 Wikipedia snap-
shot, and filtered to contain only articles with more
than 1000 words. The resulting corpus consists of
387,082 documents and 917 million tokens.
Table 2 reports the scores of reference algo-
rithms on the six benchmarks using cosine simi-
larity. The variation in scoring illustrates that dif-
ferent algorithms are more effective at capturing
certain semantic relations. We note that scores are
likely to change for different parameter configura-
tions of the same algorithm, e.g. token filtering or
changing the number of dimensions.
As a second analysis, we report the efficiency
of reference implementations by varying the cor-
pus size and number of threads. Figure 2 reports
the total amount of time each algorithm needs for
processing increasingly larger segments of a web-
gathered corpus when using 8 threads. In all cases,
only the top 100,000 words were counted as fea-
tures. Figure 3 reports run time improvements due
to multi-threading on the TASA corpus.
Algorithm efficiency is determined by three fac-
tors: contention on global statistics, contention on
disk I/O, and memory limitations. Multi-threading
benefits increase proportionally to the amount of
work done per context. Memory limitations ac-
count for the largest efficiency constraint, espe-
cially as the corpus size and number of features
grow. Several algorithms lack data points for
larger corpora and show a sharp increase in run-
ning time in Figure 2, reflecting the point at which
the models no longer fit into 8GB of memory.
6 Future Work and Conclusion
We have described a framework for developing
and evaluating word space algorithms. Many well
known algorithms are already provided as part of
the framework as reference implementations for
researches in distributional semantics. We have
shown that the provided algorithms and libraries
scale appropriately. Last, we motivate further re-
search by illustrating the significant performance
differences of the algorithms on six benchmarks.
Future work will be focused on providing sup-
port for syntactic features, including dependency
parsing as described by (Pado? and Lapata, 2007),
reference implementations of algorithms that use
this information, non-linear dimensionality reduc-
tion techniques, and more advanced clustering al-
gorithms.
References
Marco Baroni, Alessandro Lenci, and Luca Onnis.
2007. Isa meets lara: A fully incremental word
space model for cognitively plausible simulations of
semantic learning. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguis-
tics.
Curt Burgess and Kevin Lund. 1997. Modeling pars-
ing constraints with high-dimensional context space.
Language and Cognitive Processes, 12:177210.
Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-
dows. 2009. Reflective random indexing and indi-
rect inference: A scalable method for discovery of
implicit connections. Journal of Biomedical Infor-
matics, 43.
J. Deese. 1964. The associative structure of some com-
mon english adjectives. Journal of Verbal Learning
and Verbal Behavior, 3(5):347?357.
34
Lin Dekang. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the Joint An-
nual Meeting of the Association for Computational
Linguistics and International Conference on Com-
putational Linguistics, pages 768?774.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Z. S.
Rivlin, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions of Information Systems, 20(1):116?
131.
J. R. Firth, 1957. A synopsis of linguistic theory 1930-
1955. Oxford: Philological Society. Reprinted in
F. R. Palmer (Ed.), (1968). Selected papers of J. R.
Firth 1952-1959, London: Longman.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI?07: Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, pages 1606?1611.
Gregory Grefenstette. 1992. Finding semantic similar-
ity in raw text: The Deese antonyms. In Working
notes of the AAAI Fall Symposium on Probabilis-
tic Approaches to Natural Language, pages 61?65.
AAAI Press.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Conference on
Recent Advances in Natural Language Processing,
pages 212?219.
Michael N. Jones, Walter Kintsch, and Doughlas J. K.
Mewhort. 2006. High-dimensional semantic space
accounts of priming. Journal of Memory and Lan-
guage, 55:534?552.
David Jurgens and Keith Stevens. 2009. Event detec-
tion in blogs using temporal random indexing. In
Proceedings of RANLP 2009: Events in Emerging
Text Types Workshop.
David Jurgens and Keith Stevens. 2010. HERMIT:
Flexible Clustering for the SemEval-2 WSI Task. In
Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2010). Association
of Computational Linguistics.
P. Kanerva, J. Kristoferson, and A. Holst. 2000. Ran-
dom indexing of text samples for latent semantic
analysis. In L. R. Gleitman and A. K. Josh, editors,
Proceedings of the 22nd Annual Conference of the
Cognitive Science Society, page 1036.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2004. On clusterings: Good, bad and spectral. Jour-
nal of the ACM, 51(3):497?515.
Thomas K. Landauer and Susan T. Dumais. 1997. A
solution to Plato?s problem: The Latent Semantic
Analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review,
104:211?240.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. In-
troduction to Latent Semantic Analysis. Discourse
Processes, (25):259?284.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-Based Construction of Seman-
tic Space Models. Computational Linguistics,
33(2):161?199.
Amruta Purandare and Ted Pedersen. 2004. Word
sense discrimination by clustering contexts in vector
and similarity spaces. In HLT-NAACL 2004 Work-
shop: Eighth Conference on Computational Natu-
ral Language Learning (CoNLL-2004), pages 41?
48. Association for Computational Linguistics.
Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2009. An improved model of
semantic similarity based on lexical co-occurrence.
Cognitive Science. sumitted.
H. Rubenstein and J. B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8:627?633.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proceedings of the 30th Annual Meeting of the Cog-
nitive Science Society (CogSci?08).
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information
Processing & Management, 24:513?523.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM, 18(11):613?620.
Robert Tibshirani, Guenther Walther, and Trevor
Hastie. 2000. Estimating the number of clusters in a
dataset via the gap statistic. Journal Royal Statistics
Society B, 63:411?423.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001), pages 491?502.
Julie Weeds, David Weir, and Diana McCarty. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics
COLING?04, pages 1015?1021.
Dominic Widdows and Kathleen Ferraro. 2008. Se-
mantic vectors: a scalable open source package and
online technology management application. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08).
35
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 359?362,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
HERMIT: Flexible Clustering for the SemEval-2 WSI Task
David Jurgens
University of California, Los Angeles
Los Angeles, California, USA
jurgens@cs.ucla.edu
Keith Stevens
University of California, Los Angeles
Los Angeles, California, USA
kstevens@cs.ucla.edu
Abstract
A single word may have multiple un-
specified meanings in a corpus. Word
sense induction aims to discover these dif-
ferent meanings through word use, and
knowledge-lean algorithms attempt this
without using external lexical resources.
We propose a new method for identify-
ing the different senses that uses a flexi-
ble clustering strategy to automatically de-
termine the number of senses, rather than
predefining it. We demonstrate the effec-
tiveness using the SemEval-2 WSI task,
achieving competitive scores on both the
V-Measure and Recall metrics, depending
on the parameter configuration.
1 Introduction
The Word Sense Induction task of SemEval 2010
compares several sense induction and discrimina-
tion systems that are trained over a common cor-
pus. Systems are provided with an unlabeled train-
ing corpus consisting of 879,807 contexts for 100
polysemous words, with 50 nouns and 50 verbs.
Each context consists of several sentences that use
a single sense of a target word, where at least one
sentence contains the word. Systems must use the
training corpus to induce sense representations for
the many word senses and then use those represen-
tations to produce sense labels for the same 100
words in unseen contexts from a testing corpus.
We perform this task by utilizing a distribu-
tional word space formed using dimensionality
reduction and a hybrid clustering method. Our
model is highly scalable; the dimensionality of the
word space is reduced immediately through a pro-
cess based on random projections. In addition, an
online part of our clustering algorithm maintains
only a centroid that describes an induced word
sense, instead of all observed contexts, which lets
the model scale to much larger corpora than those
used in the SemEval-2 WSI task.
2 The Word Sense Induction Model
We perform word sense induction by modeling
individual contexts in a high dimensional word
space. Word senses are induced by finding con-
texts which are similar and therefore likely to use
the same sense of the target word. We use a hybrid
clustering method to group similar contexts.
2.1 Modeling Context
For a word, each of its contexts are represented by
the words with which it co-occurs. We approx-
imate this high dimensional co-occurrence space
with the Random Indexing (RI) word space model
(Kanerva et al, 2000). RI represents the occur-
rence of a word with an index vector, rather than
a set of dimensions. An index vector is a fixed,
sparse vector that is orthogonal to all other words?
index vectors with a high probability; the total
number of dimensions in the model is fixed at a
small value, e.g. 5,000. Orthogonality is obtained
by setting a small percentage of the vector?s values
to ?1 and setting the rest to 0.
A context is represented by summing the index
vectors corresponding to the n words occurring to
the left and right of the polysemous word. Each
occurrence of the polysemous word in the entire
corpus is treated as a separate context. Contexts
are represented by a compact first-order occur-
rence vector; using index vectors to represent the
occurrences avoids the computational overhead of
other dimensional reduction techniques such as
the SVD.
2.2 Identifying Related Contexts
Clustering separates similar context vectors into
dissimilar clusters that represent the distinct
senses of a word. We use an efficient hybrid of
online K-Means and Hierarchical Agglomerative
359
Clustering (HAC) with a threshold. The thresh-
old allows for the final number of clusters to be
determined by data similarity instead of having to
specify the number of clusters.
The set of context vectors for a word are clus-
tered using K-Means, which assigns a context to
the most similar cluster centroid. If the near-
est centroid has a similarity less than the cluster
threshold and there are not K clusters, the context
forms a new cluster. We define the similarity be-
tween contexts vectors as the cosine similarity.
Once the corpus has been processed, clusters
are repeatedly merged using HAC with the aver-
age link criteria, following (Pedersen and Bruce,
1997). Average link clustering defines cluster sim-
ilarity as the mean cosine similarity of the pair-
wise similarity of all data points from each clus-
ter. Cluster merging stops when the two most sim-
ilar clusters have a similarity less than the clus-
ter threshold. Reaching a similarity lower than the
cluster threshold signifies that each cluster repre-
sents a distinct word sense.
2.3 Applying Sense Labels
Before training and evaluating our model, all
occurrences of the 100 polysemous words were
stemmed in the corpora. Stemming was required
due to a polysemous word being used in multiple
lexical forms, e.g. plural, in the corpora. By stem-
ming, we avoid the need to combine contexts for
each of the distinct word forms during clustering.
After training our WSI model on the training
corpus, we process the test corpus and label the
context for each polysemous word with an induced
sense. Each test context is labeled with the name
of the cluster whose centroid has the highest co-
sine similarity to the context vector. We represent
the test contexts in the same method used for train-
ing; index vectors are re-used from training.
3 Evaluation and Results
The WSI task evaluated the submitted solutions
with two methods of experimentation: an unsuper-
vised method and a supervised method. The unsu-
pervised method is measured according to the V-
Measure and the F-Score. The supervised method
is measured using recall.
3.1 Scoring
The first measure used is the V-Measure (Rosen-
berg and Hirschberg, 2007), which compares the
clusters of target contexts to word classes. This
measure rates the homogeneity and completeness
of a clustering solution. Solutions that have word
clusters formed from one word class are homoge-
neous; completeness measures the degree to which
a word class is composed of target contexts allo-
cated to a single cluster.
The second measure, the F-Score, is an ex-
tension from information retrieval and provides a
contrasting evaluation metric by using a different
interpretation of homogeneity and completeness.
For the F-Score, the precision and recall of all pos-
sible context pairs are measured, where a word
class has the expected context pairs and a provided
solution contains some word pairs that are correct
and others that are unexpected. The F-Score tends
to discount smaller clusters and clusters that can-
not be assigned to a word class (Manandhar et al,
2010).
3.2 Parameter Tuning
Previous WSI evaluations provided a test corpus,
a set of golden sense labels, and a scoring mecha-
nism, which allowed models to do parameter tun-
ing prior to providing a set of sense labels. The
SemEval 2010 task provided a trial corpus that
contains contexts for four verbs that are not in the
evaluation corpus, which can be used for train-
ing and testing. The trial corpus also came with a
set of golden sense assignments. No golden stan-
dard was provided for the training or test corpora,
which limited any parameter tuning.
HERMIT exposes three parameters: cluster
threshold, the maximum number of clusters and
the window size for a context. An initial anal-
ysis from the trial data showed that the window
size most affected the scores; small window sizes
resulted in higher V-Measure scores, while larger
window sizes maximized the F-Score. Because
contexts are represented using only first-order fea-
tures, a smaller window size should have less over-
lap, which potentially results in a higher number
of clusters. We opted to maximize the V-Measure
score by using a window size of ?1.
Due to the limited number of training instances,
our precursory analysis with the trial data did not
show significant differences for the remaining two
parameters; we arbitrarily selected a clustering
threshold of .15 and a maximum of 15 clusters per
word without any parameter tuning.
After the release of the testing key, we per-
360
formed a post-hoc analysis to evaluate the effects
of parameter tuning on the scores. We include two
alternative parameter configurations that were op-
timized for the F-Score (HERMIT-F) and the su-
pervised evaluations (HERMIT-S). The HERMIT-
F variation used a threshold of 0.85 and a win-
dow size of ?10 words. The HERMIT-S variation
used a threshold of 0.85 and a window size of ?1
words. We did not vary the maximum number of
clusters, which was set at 15.
For each evaluation, we provide the scores of
seven systems: the three HERMIT configurations,
the highest and lowest scoring submitted systems,
the Most Frequent Sense (MFS) baseline, and a
Random baseline provided by the evaluation team.
We provide the scores for each experiment when
evaluating all words, nouns, and verbs. We also
include the system?s rank relative to all submitted
systems and the average number of senses gen-
erated for each system; our alternative HERMIT
configurations are given no rank.
3.3 Unsupervised Evaluation
System All Nouns Verbs Rank Senses
HERMIT-S 16.2 16.7 15.3 10.83
HERMIT 16.1 16.7 15.6 1 10.78
Random 4.4 4.6 4.1 18 4.00
HERMIT-F 0.015 0.008 0.025 1.54
MFS 0.0 0.0 0.0 27 1.00
LOW 0.0 0.0 0.1 28 1.01
Table 1: V-Measure for the unsupervised evalua-
tion
System All Nouns Verbs Rank Senses
MFS 63.4 57.0 72.7 1 1.00
HIGH 63.3 57.0 72.4 2 1.02
HERMIT-F 62.1 56.7 69.9 1.54
Random 31.9 30.4 34.1 25 4.00
HERMIT 26.7 30.1 24.4 27 10.78
HERMIT-S 26.5 23.9 30.3 10.83
LOW 16.1 15.8 16.4 28 9.71
Table 2: F-Scores for the unsupervised evaluation
The unsupervised evaluation considers a golden
sense labeling to be word classes and a set of in-
duced word senses as clusters of target contexts
(Manandhar et al, 2010). Tables 1 and 2 display
the results for the unsupervised evaluation when
measured according to the V-Measure and the F-
Score, respectively. Our system provides the best
V-Measure of all submitted systems for this eval-
uation. This is in part due to the average number
of senses our system generated (10.78), which fa-
vors more homogenous clusters. Conversely, this
configuration does poorly when measured by F-
Score, which tends to favor systems that generate
fewer senses per word.
When configured for the F-Score, HERMIT-
F performs well; this configuration would have
ranked third for the F-Score if it had been submit-
ted. However, its performance is also due to the
relatively few senses per word it generates, 1.54.
The inverse performance of both optimized con-
figurations is reflective of the contrasting nature of
the two performance measures.
3.4 Supervised Evaluation
System All Noun Verb Rank
HIGH 62.44 59.43 66.82 1
MFS 58.67 53.22 66.620 15
HERMIT-S 58.48 54.18 64.78
HERMIT 58.34 53.56 65.30 17
Random 57.25 51.45 65.69 19
HERMIT-F 56.44 53.00 61.46
LOW 18.72 1.55 43.76 28
Table 3: Supervised recall for the 80/20 split
System All Noun Verb Rank
HIGH 61.96 58.62 66.82 1
MFS 58.25 52.45 67.11 12
HERMIT 57.27 52.53 64.16 18
HERMIT-S 57.10 52.76 63.46
Random 56.52 50.21 65.73 20
HERMIT-F 56.18 52.26 61.88
LOW 18.91 1.52 44.23 28
Table 4: Supervised recall for the 60/40 split
The supervised evaluation simulates a super-
vised Word Sense Disambiguation (WSD) task.
The induced sense labels for the test corpus are
split such that the first set is used for mapping in-
duced senses to golden senses and the remaining
sense labels are treated as sense labels provided
by a WSD system, which allows for evaluation.
Five splits are done at random to avoid any biases
created due to the separation of the mapping cor-
pus and the evaluation corpus; the resulting score
for this task is the average recall over the five di-
visions. Two sets of splits were used for evalua-
tion: one with 80% of the senses as the mapping
portion and 20% as the evaluation portion and one
with 60% as the mapping portion corpus and 40%
for evaluation.
The results for the 80/20 split and 60/40 split
are displayed in tables 3 and 4, respectively. In
both supervised evaluations, our submitted system
361
 0
 4
 8
 12
Cl
us
te
rs Clusters
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
 2  4  6  8 10 12 14
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
F-
Sc
or
e
V-
M
ea
su
re
Window Size
F-Score
V-Measure
Figure 1: A comparison for F-Score and V-
Measure for different window sizes. Scores are an
average using thresholds of 0.15, 0.55 and 0.75.
does moderately well. In both cases it outperforms
the Random baseline and does almost as well as
the MFS baseline. The submitted system outper-
forms the Random baseline and approaches the
MFS baseline for the 80/20 split. The HERMIT-S
version, which is optimized for this task, provides
similar results.
4 Discussion
The HERMIT system is easily configured to
achieve close to state of the art performance for
either evaluation measure on the unsupervised
benchmark. This reconfigurability allows the al-
gorithm to be tuned for producing a few coarse
senses of a word, or many finer-grained senses.
We further investigated the performance with
respect to the window size parameter on both mea-
sures. Since each score can be effectively opti-
mized individually, we considered whether both
scores could be maximized concurrently. Figure
1 presents the impact of the window size on both
measures using an average of three threshold pa-
rameter configurations.
The analysis of both measures indicates that
reasonable performance can be obtained from us-
ing a slightly larger context window. For ex-
ample, a window size of 4 has an average F-
Score of 52.4 and V-Measure of 7.1. Although
this configuration produces scores lower than the
optimized versions, its performance would have
ranked 12th according to V-Measure and 15th for
F-Score. These scores are consistent with the me-
dian performance of the submitted systems and of-
fer a middle ground should a HERMIT user want
a compromise between many fine-grained word
senses and a few coarse-grained word senses.
5 Conclusion
We have shown that our model is a highly flexi-
ble and tunable Word Sense Induction model. De-
pending on the task, it can be optimized to gen-
erate a set of word senses that range from be-
ing broad and representative to highly refined.
Furthermore, we demonstrated a balanced perfor-
mance setting for both measures for when param-
eter tuning is not possible. The model we sub-
mitted and presented is only one possible config-
uration available, and in the future we will be ex-
ploring the effect of other context features, such
as syntactic structure in the form of word ordering
(Sahlgren et al, 2008) or dependency parse trees,
(Pado? and Lapata, 2007), and other clustering al-
gorithms. Last, this model is provided as part of
the S-Space Package (Jurgens and Stevens, 2010),
an open source toolkit for word space algorithms.
References
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Deonstrations.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In L. R. Gleitman and A. K. Josh,
editors, Proceedings of the 22nd Annual Conference
of the Cognitive Science Society, page 1036.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
Task 14: Word Sense Induction & Disambiguation.
In Proceedings of SemEval-2.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-Based Construction of Seman-
tic Space Models. Computational Linguistics,
33(2):161?199.
Ted Pedersen and Rebecca Bruce. 1997. Distinguish-
ing word senses in untagged text. In Proceedings
of the Second Conference on Empirical Methods in
Natural Language Processing, pages 197?207.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
Measure: A Conditional Entropy-Based External
Cluster Evaluation Measure. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL). ACL.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode or-
der in word space. In Proceedings of the 30th
Annual Meeting of the Cognitive Science Society
(CogSci?08).
362
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 1?6,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Capturing Nonlinear Structure in Word Spaces through Dimensionality
Reduction
David Jurgens
University of California, Los Angeles,
4732 Boelter Hall
Los Angeles, CA 90095
jurgens@cs.ucla.edu
Keith Stevens
University of California, Los Angeles,
4732 Boelter Hall
Los Angeles, CA 90095
kstevens@cs.ucla.edu
Abstract
Dimensionality reduction has been shown
to improve processing and information ex-
traction from high dimensional data. Word
space algorithms typically employ lin-
ear reduction techniques that assume the
space is Euclidean. We investigate the ef-
fects of extracting nonlinear structure in
the word space using Locality Preserv-
ing Projections, a reduction algorithm that
performs manifold learning. We apply
this reduction to two common word space
models and show improved performance
over the original models on benchmarks.
1 Introduction
Vector space models of semantics frequently em-
ploy some form of dimensionality reduction for
improvement in representations or computational
overhead. Many of the dimensionality reduc-
tion algorithms assume that the unreduced word
space is linear. However, word similarities have
been shown to exhibit many non-metric proper-
ties: asymmetry, e.g North Korea is more sim-
ilar to Red China than Red China is to North
Korea, and non-transitivity, e.g. Cuba is similar
the former USSR, Jamaica is similar to Cuba,
but Jamaica is not similar to the USSR (Tversky,
1977). We hypothesize that a non-linear word
space model might more accurately preserve these
non-metric relationships.
To test our hypothesis, we capture the non-
linear structure with dimensionality reduction by
using Locality Preserving Projection (LPP) (He
and Niyogi, 2003), an efficient, linear approxi-
mation of Eigenmaps (Belkin and Niyogi, 2002).
With this reduction, the word space vectors are as-
sumed to exist on a nonlinear manifold that LPP
learns in order to project the vectors into a Eu-
clidean space. We measure the effects of us-
ing LPP on two basic word space models: the
Vector Space Model and a Word Co-occurrence
model. We begin with a brief overview of these
word spaces and common dimensionality reduc-
tion techniques. We then formally introduce LPP.
Following, we use two experiments to demonstrate
LPP?s capacity to accurately dimensionally reduce
word spaces.
2 Word Spaces and Reductions
We consider two common word space models
that have been used with dimensionality reduc-
tion. The first is the Vector Space Model (VSM)
(Salton et al, 1975). Words are represented as
vectors where each dimension corresponds to a
document in the corpus and the dimension?s value
is the number of times the word occurred in the
document. We label the second model the Word
Co-occurrence (WC) model: each dimension cor-
respond to a unique word, with the dimension?s
value indicating the number of times that dimen-
sion?s word co-occurred.
Dimensionality reduction has been applied to
both models for three kinds of benefits: to im-
prove computational efficiency, to capture higher
order relationships between words, and to reduce
noise by smoothing or eliminating noisy features.
We consider three of the most popular reduction
techniques and the general word space models to
which they have been applied: linear projections,
feature elimination and random approximations.
The most frequently applied linear projection
technique is the Singular Value Decomposition
(SVD). The SVD factors a matrix A, which rep-
resents a word space, into three matrices U?V ?
such that ? is a diagonal matrix containing the
singular values of A, ordered descending based on
their effect on the variance in the values of A. The
original matrix can be approximated by using only
the top k singular values, setting all others to 0.
The approximation matrix, A? = Uk?kV ?k , is the
least squares best-fit rank-k approximation of A.
1
The SVD has been used with great success on
both models. Latent Semantic Analysis (LSA)
(Landauer et al, 1998) extends the (VSM) by de-
composing the space using the SVD and mak-
ing the word space the left singular vectors, Uk.
WC models have also utilized the SVD to improve
performance (Schu?tze, 1992; Bullinaria and Levy,
2007; Baroni and Lenci, 2008).
Feature elimination reduces the dimensional-
ity by removing those with low information con-
tent. This approach has been successfully applied
to WC models such as HAL (Lund and Burgess,
1996) by dropping those with low entropy. This
technique effectively removes the feature dimen-
sions of high frequency words, which provide lit-
tle discriminatory content.
Randomized projections have also been suc-
cessfully applied to VSM models, e.g. (Kanerva
et al, 2000) and WC models, e.g. (Sahlgren et al,
2008). This reduction statistically approximates
the original space in a much lower dimensional
space. The projection does not take into account
the structure of data, which provides only a com-
putational benefit from fewer dimensions, unlike
the previous two reductions.
3 Locality Preserving Projection
For a set of vectors, x1, x2, . . . , xn ? Rm, LPP
preserves the distance in the k-dimensional space,
where k ? m, by solving the following minimiza-
tion problem,
min
w
?
ij
(w?xi ?w?xj)2Sij (1)
where w is a transformation vector that projects x
into the lower dimensional space, and S is a ma-
trix that represents the local structure of the origi-
nal space. Minimizing this equation is equivalent
to finding the transformation vector that best pre-
serves the local distances in the original space ac-
cording to S. LPP assumes that the data points xi
exist on a manifold. This is in contrast to the SVD,
which assumes that the space is Euclidean and per-
forms a global, rather than local, minimization. In
treating the space as a manifold, LPP is able to dis-
cover some of the nonlinear structure of the data
from its local structure.
To solve the minimization problem in Equation
1, LPP uses a linear approximation of the Lapla-
cian Eigenmaps procedure (Belkin and Niyogi,
2002) as follows:
1. Let X be a matrix where xi is the ith row vec-
tor. Construct an adjacency matrix, S, which
represents the local structure of the original
vector space, by making an edge between
points xi and xj if xj is locally proximate to
xi. Two variations are available for determin-
ing proximity: either the k-nearest neighbors,
or all the data points with similarity > ?.
2. Weight the edges in S proportional to the
closeness of the data points. Four main op-
tions are available: a Gaussian kernel, a poly-
nomial kernel, cosine similarity, or binary.
3. Construct the diagonal matrix D where entry
Dii =
?
j Sij . Let L = D ? S. Then solve
the generalized eigenvector problem:
XLX?w = ?XDX?w. (2)
He and Niyogi (2003) show that solving this
problem is equivalent to solving Equation 1.
4. Let Wk = [w1, . . . ,wk] denote the matrix of
transformation vectors, sorted in descending
order according to their eigenvalues ?. The
original space is projected into k dimensions
by W?k X ? Xk.
For many applications of LPP, such as doc-
ument clustering (He et al, 2004), the original
data matrix X is transformed by first perform-
ing Principle Component Analysis and discarding
the smallest principle components, which requires
computing the full SVD. However, for large data
sets such as those frequently used in word space
algorithms, performing the full SVD is computa-
tionally infeasible.
To overcome this limitation, Cai et al (2007a)
show how Spectral Regression may be used as
an alternative for solving the same minimization
equation through an iterative process. The princi-
ple idea is that Equation 2 may be recast as
Sy = ?Dy (3)
where y = X?w, which ensures y will be an
eigenvector with the same eigenvalue for the prob-
lem in Equation 2. Finding the transformation
matrix Wk, used in step 4, is done in two steps.
First, Equation 3 is solved to produce eigenvectors
[y0, . . . ,yk], sorted in decreasing order according
to their eigenvalues ?. Second, the set of trans-
formation vectors composing Wk, [w1, . . . ,wk],
is found by a least-squares regression:
wj = argmin
w
n
?
i=1
(w?xi ? yji )2 + ?||w||2 (4)
2
where yji denotes the value of the jth dimension
of yi. The ? parameter penalizes solutions pro-
portionally to their magnitude, which Cai et al
(2007b) note ensures the stability of w as an ap-
proximate eigenproblem solution.
4 Experiments
Two experiments measures the effects of nonlin-
ear dimensionality reduction for word spaces. For
both, we apply LPP to two basic word space mod-
els, the VSM and WC. In the first experiment,
we measure the word spaces? abilities to model
semantic relations, as determined by priming ex-
periments. In the second experiment, we evaluate
the representation capabilities of the LPP-reduced
models on standard word space benchmarks.
4.1 Setup
For the VSM-based word space, we consider three
different weighting schemes: no weighting, TF-
IDF and the log-entropy (LE) used in (Landauer
et al, 1998). For the WC-based word space, we
use a 5 word sliding window. Due to the large pa-
rameter space for LPP models, we performed only
a limited configuration search. An initial analysis
using the 20 nearest neighbors and cosine simi-
larity did not show significant performance differ-
ences when the number of dimensions was varied
between 50 and 1000. We therefore selected 300
dimensions for all tests. Further work is needed to
identify the impact of different parameters. Stop
words were removed only for the WC+LPP model.
We compare the LPP-based spaces to three mod-
els: VSM, HAL, and LSA.
Two corpora are used to train the models in both
experiments. The first corpus, TASA, is a collec-
tion of 44,486 essays that are representative of the
reading a student might see upon entering college,
introduced by (Landauer et al, 1998). The cor-
pus consists of 98,420 unique words; no filtering
is done when processing this corpus. The second
corpus, WIKI, is a 387,082 article subset of a De-
cember 2009 Wikipedia snapshot consisting of all
the articles with more than 1,000 tokens. The cor-
pus is filtered to retain the top 100,000 most fre-
quent tokens in addition to all the tokens used in
each experiment?s data set.
4.2 Experiment 1
Semantic priming measures word association
based on human responses to a provided cue.
Priming studies have been used to evaluate word
spaces by equating vector similarity with an in-
creased priming response. We use data from two
types of priming experiments to measure whether
LPP models better correlate with human perfor-
mance than non-LPP word spaces.
Normed Priming Nelson et al (1998) collected
free association responses to 5,019 prime words.
An average of 149 participants responded to each
prime with the first word that came to mind.
Based on this dataset, we introduce a new
benchmark that correlates word space similarity
with the associative strength of semantic priming
pairs. We use three measures for modeling prime-
target strength, which were inspired by Steyvers
et al (2004). Let Wab be the percentage of partici-
pants who responded to prime a with target b. The
three measures of associative strength are
S1ab = Wab
S2ab = Wab +Wba
S3ab = S2ab +
?
c S2acS2cb
These measure three different levels of semantic
relatedness between words a and b. S1ab measures
the relationship from a to b, which is frequently
asymmetric due to ordering, e.g. ?orange? pro-
duces ?juice? more frequently than ?juice? pro-
duces ?orange.? S2ab measures the symmetric asso-
ciation between a and b; Steyvers et al (2004) note
that this may better model the associative strength
by including weaker associates that may have been
a suitable second response. S3ab further increases
the association by including the indirect associa-
tions between a and b from all cued primes.
For each measure, we rank a prime?s targets
according to their strength and then compute the
Spearman rank correlation with the prime-target
similarities in the word space. The rank compari-
son measures how well word space similarity cor-
responds to the priming association. We report the
average rank correlation of associational strengths
over all primes.
Priming Effect The priming study by Hodgson
(1991), which evaluated how different semantic
relationships affected the strength of priming, pro-
vides the data for our second priming test. Six re-
lationships were examined in the study: antonymy,
synonymy, conceptual association (sleep and bed),
categorical coordinates (mist and rain), phrasal as-
sociates (pony and express), and super- and sub-
ordinates. Each relationship contained an average
3
Antonymy Conceptual Coordinates
Algorithm Rb U E R U E R U E
VSM+LPP+LE 0.103 0.018 0.085 0.197 0.050 0.147 0.071 0.027 0.044
VSM+LPP+TF-IDF 0.348 0.321 0.027 0.408 0.414 -0.005 0.323 0.294 0.029
VSM+LPP 0.247 0.122 0.124 0.312 0.120 0.193 0.230 0.111 0.119
VSM+LPPa 0.298 0.070 0.228 0.284 0.033 0.252 0.321 0.037 0.284
WC+LPP 0.255 0.071 0.185 0.413 0.110 0.303 0.431 0.134 0.298
HAL 0.813 0.716 0.096 0.845 0.814 0.031 0.861 0.809 0.052
HALa 0.915 0.879 0.037 0.867 0.846 0.021 0.913 0.861 0.052
LSA 0.235 0.023 0.213 0.392 0.028 0.364 0.199 0.014 0.185
LSAa 0.287 0.061 0.226 0.362 0.041 0.321 0.316 0.037 0.278
VSM 0.051 0.011 0.040 0.111 0.012 0.099 0.032 0.008 0.024
Phrasal Ordinates Synonymy
Algorithm R U E R U E R U E
VSM+LPP+LE 0.147 0.039 0.108 0.225 0.032 0.193 0.081 0.027 0.053
VSM+LPP+TF-IDF 0.438 0.425 0.013 0.277 0.290 -0.013 0.344 0.328 0.017
VSM+LPP 0.234 0.107 0.127 0.273 0.115 0.158 0.237 0.157 0.080
VSM+LPPa 0.202 0.031 0.171 0.270 0.032 0.238 0.299 0.069 0.230
WC+LPP 0.274 0.087 0.186 0.324 0.076 0.248 0.345 0.111 0.233
HAL 0.805 0.776 0.029 0.825 0.789 0.036 0.757 0.681 0.076
HALa 0.866 0.856 0.010 0.881 0.857 0.024 0.898 0.879 0.019
LSA 0.280 0.021 0.258 0.258 0.018 0.240 0.197 0.019 0.178
LSAa 0.269 0.030 0.238 0.326 0.032 0.294 0.327 0.052 0.275
VSM 0.104 0.013 0.091 0.061 0.008 0.053 0.052 0.009 0.043
a Processed using the WIKI corpus
b R are related primes, U are unrelated primes, E is the priming effect
Table 1: Experiment 1 priming results for the six relation categories from Hodgson (1991)
Word Choice Word Association
Algorithm Corpus TOEFL ESL RDWP F. et al R.&G. Deese
VSM+LPP+le TASA 24.000 50.000 45.313 0.296 0.092 0.034
VSM+LPP+tf-idf TASA 22.667 25.000 37.209 0.023 0.086 0.001
VSM+LPP TASA 41.333 54.167 39.063 0.219 0.136 0.045
VSM+LPP Wiki 33.898 48.780 43.434 0.530 0.503 0.108
WC+LPP TASA 46.032 40.000 45.783 0.423 0.414 0.126
HAL TASA 44.00 20.83 50.00 0.173 0.180 0.318
HAL Wiki 50.00 31.11 43.44 0.261 0.195 0.042
LSA TASA 56.000 50.000 55.814 0.516 0.651 0.349
LSA Wiki 60.759 54.167 59.200 0.614 0.681 0.206
VSM TASA 61.333 52.083 84.884 0.396 0.496 0.200
Table 2: Results from Experiment 2 on six word space benchmarks
of 23 word pairs. Hodgson?s results showed that
priming effects were exhibited by the prime-target
pairs in all six categories.
We use the same methodology as Pado? and La-
pata (2007) for this data set; the prime-target (Re-
lated Primes) cosine similarity is compared with
the average cosine similarity between the prime
and all other targets (Unrelated Primes) within the
semantic category. The priming effect is the dif-
ference between the two similarity values.
4.3 Experiment 2
We use six standard word space benchmarks to
test our hypothesis that LPP can accurately capture
general semantic knowledge and association based
relations. The benchmarks come in two forms:
word association and word choice tests.
Word choice tests provide a target word and a
list of options, one of which has the desired rela-
tion to the target. To answer these questions, we
select the option with the highest cosine similar-
ity with the target. Three word choice synonymy
benchmarks are used: the Test of English as a For-
eign Language (TOEFL) test set from (Landauer
et al, 1998), the English as a Second Language
(ESL) test set from (Turney, 2001), and the Cana-
dian Reader?s Digest Word Power (RDWP) from
(Jarmasz and Szpakowicz, 2003).
4
Algorithm Corpus S1 S2 S3
VSM+LPP+LE TASA 0.457 0.413 0.255
VSM+LPP+TF-IDF TASA 0.464 0.390 0.207
VSM+LPP TASA 0.457 0.427 0.275
VSM+LPP Wiki 0.472 0.440 0.333
WC+LPP TASA 0.469 0.437 0.315
HAL TASA 0.485 0.434 0.310
HAL Wiki 0.462 0.406 0.266
LSA TASA 0.494 0.481 0.414
LSA Wiki 0.489 0.472 0.398
VSM TASA 0.484 0.460 0.407
Table 3: Experiment 1 results for normed priming.
Word association tests measure the semantic re-
latedness of two words by comparing their simi-
larity in the word space with human judgements.
These tests are more precise than word choice tests
because they take into account the specific value
of the word similarity. Three word association
benchmarks are used: the word similarity data set
of Rubenstein and Goodenough (1965), the word-
relatedness data set of Finkelstein et al (2002),
and the antonymy data set of Deese (1964), which
measures the degree to which high similarity cap-
tures the antonymy relationship. The Finkelstein
et al test is notable in that the human judges were
free to score based on any word relationship.
5 Results and Discussion
The LPP-based models show mixed performance
in comparison to existing models on normed prim-
ing tasks, shown in Table 3. Adding LPP to
the VSM decreased performance; however, when
WIKI was used instead of TASA, the VSM+LPP
model increased .15 on all correlations, whereas
LSA?s performance decreased. This suggests that
LPP needs more data than LSA to properly model
the word space manifold. WC+LPP performs
comparably to HAL, which indicates that LPP
is effective in retaining the original WC space?s
structure in significantly fewer dimensions.
For the categorical priming tests shown in Ta-
ble 1, LPP-based models show competitive results.
VSM+LPP with the WIKI corpus performs much
better than other VSM+LPP configurations. Un-
like in the previous priming experiment, adding
LPP to the base models resulted in a significant
performance improvement. We also note that both
HAL models and the VSM+LPP+TF-IDF model
have high similarity ratings for unrelated primes.
We posit that these models? feature weighting re-
sults in poor differentiation between words in the
same semantic category, which causes their de-
creased performance.
For experiment 2, LPP-based spaces showed
mixed results on word choice benchmarks, while
showing notable improvement on the more pre-
cise word association benchmarks. Table 2 lists
the results. Notably, LPP-based spaces performed
well on the ESL synonym benchmark but poorly
on the TOEFL synonym benchmark, even when
the larger WIKI corpus was used. This suggests
that LPP was not effective in retaining the re-
lationship between certain classes of synonyms.
Given that performance did not improve with the
WIKI corpus, further analysis is needed to iden-
tify whether a different representation of the local
structure would improve results or if the poor per-
formance is due to another factor. While LSA and
VSM model performed best on all benchmarks,
LPP-based spaces performed competitively on the
word association tests. In all but two tests, the
WC+LPP model outperformed HAL.
The results from both experiments indicate that
LPP is capable of accurately representing distri-
butional information in a much lower dimensional
space. However, in many cases, applications using
the SVD-reduced representations performed bet-
ter. In addition, application of standard weight-
ing schemes worsened LPP-models? performance,
which suggests that the local neighborhood is ad-
versely distorted. Nevertheless, we view these re-
sults as a promising starting point for further eval-
uation of nonlinear dimensionality reduction.
6 Conclusions and Future Work
We have shown that LPP is an effective dimen-
sionality reduction technique for word space algo-
rithms. In several benchmarks, LPP provided a
significant benefit to the base models and in a few
cases outperformed the SVD. However, it does not
perform consistently better than existing models.
Future work will focus on four themes: identifying
optimal LPP parameter configurations; improving
LPP with weighting; measuring LPP?s capacity to
capture higher order co-occurrence relationships,
as was shown for the SVD (Lemaire et al, 2006);
and investigating whether more computationally
expensive nonlinear reduction algorithms such as
ISOMAP (Tenenbaum et al, 2000) are better for
word space algorithms. We plan to release imple-
mentations of the LPP-based models as a part of
the S-Space Package (Jurgens and Stevens, 2010).
5
References
Marco Baroni and Alessandro Lenci. 2008. Con-
cepts and properties in word spaces. From context to
meaning: Distributional models of the lexicon in lin-
guistics and cognitive science (Special issue of the
Italian Journal of Linguistics), 1(20):55?88.
Mikhail Belkin and Partha Niyogi. 2002. Laplacian
Eigenmaps and Spectral Techniques for Embedding
and Clustering. In Advances in Neural Information
Processing Systems, number 14.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: a computational study. Behav-
ior Research Methods, 39:510?526.
Deng Cai, Xiaofei He, and Jiawei Han. 2007a. Spec-
tral regression for efficient regularized subspace
learning. In IEEE International Conference on
Computer Vision (ICCV?07).
Deng Cai, Xiaofei He, Wei Vivian Zhang, , and Jiawei
Han. 2007b. Regularized Locality Preserving In-
dexing via Spectral Regression. In Proceedings of
the 2007 ACM International Conference on Infor-
mation and Knowledge Management (CIKM?07).
James Deese. 1964. The associative structure of
some common english adjectives. Journal of Verbal
Learning and Verbal Behavior, 3(5):347?357.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Woflman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions of Information
Systems, 20(1):116?131.
Xiaofei He and Partha Niyogi. 2003. Locality preserv-
ing projections. In Advances in Neural Information
Processing Systems 16 (NIPS).
Xiaofei He, Deng Cai, Haifeng Liu, and Wei-Ying
Ma. 2004. Locality preserving indexing for doc-
ument representation. In SIGIR ?04: Proceedings
of the 27th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 96?103.
James M. Hodgson. 1991. Informational constraints
on pre-lexical priming. Language and Cognitive
Processes, 6:169?205.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Conference on
Recent Advances in Natural Language Processing,
pages 212?219.
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In L. R. Gleitman and A. K. Josh,
editors, Proceedings of the 22nd Annual Conference
of the Cognitive Science Society, page 1036.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to Latent Semantic Analy-
sis. Discourse Processes, (25):259?284.
Beno??t Lemaire, , and Guy Henhie?re. 2006. Effects
of High-Order Co-occurrences on Word Semantic
Similarities. Current Psychology Letters, 1(18).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occrrence. Behavoir Research Methods, Instru-
ments & Computers, 28(2):203?208.
Douglas L. Nelson, Cathy L. McEvoy, and
Thomas A. Schreiber. 1998. The Uni-
versity of South Florida word association,
rhyme, and word fragment norms. http:
//www.usf.edu/FreeAssociation/.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-Based Construction of Seman-
tic Space Models. Computational Linguistics,
33(2):161?199.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8:627?633.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode or-
der in word space. In Proceedings of the 30th
Annual Meeting of the Cognitive Science Society
(CogSci?08).
Gerard Salton, A. Wong, and C. S. Yang. 1975. A
vector space model for automatic indexing. Com-
munications of the ACM, 18(11):613?620.
Hinrich Schu?tze. 1992. Dimensions of meaning.
In Proceedings of Supercomputing ?92, pages 787?
796.
Mark Steyvers, Richard M. Shiffrin, and Douglas L.
Nelson, 2004. Word association spaces for predict-
ing semantic similarity effects in episodic memory.
American Psychological Assocation.
Joshua B. Tenenbaum, Vin de Silva, and John C.
Langford. 2000. A global geometric framework
for nonlinear dimensionality reduction. Science,
290(5500):2319?2323.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001), pages 491?502.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84:327?352.
6
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 113?123,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Measuring the Impact of Sense Similarity on Word Sense Induction
David Jurgens1,2
1HRL Laboratories, LLC
Malibu, California, USA
jurgens@cs.ucla.edu
Keith Stevens2
2University of California, Los Angeles
Los Angeles, California, USA
kstevens@cs.ucla.edu
Abstract
Word Sense Induction (WSI) is an unsuper-
vised learning approach to discovering the dif-
ferent senses of a word from its contextual
uses. A core challenge to WSI approaches
is distinguishing between related and possibly
similar senses of a word. Current WSI evalu-
ation techniques have yet to analyze the spe-
cific impact of similarity on accuracy. There-
fore, we present a new WSI evaluation that
quantifies the relationship between the relat-
edness of a word?s senses and the ability of a
WSI algorithm to distinguish between them.
Furthermore, we perform an analysis on sense
confusions in SemEval-2 WSI task according
to sense similarity. Both analyses for a rep-
resentative selection of clustering-based WSI
approaches reveals that performance is most
sensitive to the clustering algorithm and not
the lexical features used.
1 Introduction
Many words in a language have several distinct
meanings. For example, ?earth? may refer to the
planet Earth, dirt, or solid ground, depending on the
context. The goal of Word Sense Induction (WSI) is
to automatically discover the different senses by ex-
amining how a word is used. This unsupervised dis-
covery process produces a sense inventory where the
number of senses is corpus-driven and where senses
may reflect additional usages not present in a pre-
defined sense inventory, such as those for medicine
or law (Dorow and Widdows, 2003). Furthermore,
these discovered senses can be used to automati-
cally expand lexical resources such as WordNet or
FrameNet (Klapaftis and Manandhar, 2010).
Discovering the multiple senses is frequently
confounded by the relationships between a word?s
senses. While homonyms such as ?bass? or ?bank?
have unrelated senses, many polysemous words
have interrelated senses, with lexicographers of-
ten in disagreement for the number of fine-grained
senses (Palmer et al, 2007). For example, the most
frequent four senses for ?law? according to Word-
Net, shown in Table 1, are similar in several aspects
and could be ascribed interchangeably in some con-
texts. The difficulty of automatically distinguishing
two senses is proportional to their similarity because
of the increasing likelihood of the two senses shar-
ing similar contexts.
While the issue distinguishing between related
senses is a recognized issue for Word Sense Dis-
ambiguation (Chugur et al, 2002; McCarthy, 2006),
which uses supervised training to learn sense dis-
tinctions, measuring the impact of sense related-
ness on the harder problem of WSI remains unad-
dressed. The recent SemEval WSI tasks (Agirre and
Soroa, 2007; Manandhar and Klapaftis, 2009) have
provided a standard framework for evaluating WSI
systems, with a controlled training corpus designed
to limit sense ambiguity in the example contexts.
However, given the potential relatedness of a word?s
senses, we view it necessary to consider how WSI
methods perform relative to the degree of contextual
ambiguity. Our goal is therefore to quantify the sim-
ilarity at which a WSI approach is unable to distin-
guish between two senses, which reflects the sense
granularity at which the approach operates.
We propose two new evaluations. The first, de-
scribed in Section 4, uses a similarity-based pseudo-
word discrimination task to measure the discrimi-
nation capability for related senses along a graded
scale of similarity. As a second evaluation, in
113
1 the collection of rules imposed by authority
2 legal document setting forth rules governing a particular
kind of activity
3 a rule or body of rules of conduct inherent in human
nature and essential to or binding upon human society
4 a generalization that describes recurring facts or events
in nature
Table 1: Definitions for the top four senses of ?law?
according to WordNet
Section 5 we perform an error analysis using the
SemEval-2010 WSI task, examining sense confu-
sion relative to the sense similarities. For both evalu-
ations, we examine twenty different WSI clustering-
based models through combining five feature types
and four clustering algorithms. These models were
selected to be representative of a wide class of exist-
ing algorithms as a way of influence future algorith-
mic directions based on the current model?s perfor-
mance.
2 Clustering Contexts to Discover Senses
Frequently, WSI is treated as an unsupervised clus-
tering problem: The contexts in which a word ap-
pears are clustered in order to discover its senses
(Navigli, 2009). We selected four diverse cluster-
ing algorithms for evaluation based on three crite-
ria: (1) the ability to automatically determine the fi-
nal number of clusters given an upper bound or a
set of parameters, (2) an efficient run time, and (3)
high quality results in either WSI or other fields re-
lated to text analysis. The first criteria is essential
for WSI; the final number of senses must be derived
without supervision in order to reflect the true num-
ber of senses present in the corpus.
K-Means K-Means builds clusters based on the
similarity between two data points. Clusters grow
by assigning data points to the cluster with the most
similar centroid. After every data point is assigned,
each cluster?s centroid is recalculated to be the av-
erage of all the data points assigned to the cluster.
This process repeats until the centroids converge to
a fixed point. We choose initial seeds at random and
use the H2 criterion function (Zhao and Karypis,
2001). Although K-Means is efficient and widely
used, it requires the number of clusters to be spec-
ified a priori. Therefore, we follow the WSI model
of Pedersen and Kulkarni (2006) and use the Gap
Statistic (Tibshirani et al, 2000) to automatically de-
termine the number of clusters.
The Gap Statistic runs K-Means repeatedly with
different values of K , ranging from 1 to some sen-
sible maximum. The Gap Statistic first induces a
data model from the feature distributions of the ini-
tial dataset and then for each K , creates a set of arti-
ficial datasets by sampling from the derived model.
K is increased until the ?gap?, i.e. the distance be-
tween the objective function of the original dataset
and the average objective function of the artificial
datasets, is larger then the gap for the previous K
value. We calculate the gap using 10 artificial data
sets sampled from the model.
Spectral Clustering Spectral Clustering inter-
prets a dataset?s elements as vertices in graph with
edges based on their similarity (Ng et al, 2001).
Clusters are found by identifying the graph parti-
tion that produces the minimum conductance be-
tween every partition. This can be thought of as
trying to find small islands that are connected by as
few bridges as possible. We refer the reader to (von
Luxburg, 2007) for further technical details. To our
knowledge, only He et al (2010) have applied spec-
tral clustering to WSI, which was performed on a
Chinese dataset. However, the algorithm used by He
et al requires the number of clusters to be specified.
We instead use a hybrid spectral clustering algo-
rithm, first applied to information retrieval (Cheng
et al, 2006), that automatically selects the number
of clusters. This algorithm recursively partitions a
dataset in half by finding the cut that produces the
minimum conductance, which builds a tree of par-
titions. This split is done until either every data
point is in its own partition or a maximum number of
partitions is found. Partitions are then dynamically
merged, starting at leaf partitions, based on a cluster-
ing criteria. We use the relaxed correlation criteria
(Cheng et al, 2006), which tries to maximize both
inter cluster similarity and intra cluster dissimilarity.
The final cluttering generated is then the best tree-
respecting partition of the original data set.
Clustering By Committee Pantel and Lin (2002)
found that K-Means clustering folded all features
found in a cluster into the centroid, many of which
are not useful for identifying the desired word sense.
114
To overcome this, they proposed a novel cluster-
ing algorithm for WSI, Clustering by Committee
(CBC), which includes only the most distinguishing
features for a cluster into the centroid.
For each context, an initial set of ?committees?
is formed by clustering the most similar contexts to
each context, with the resulting committees ranked
to prefer larger, highly similar clusters. The final
set of committees (sense clusters) are selected by re-
cursively identifying the highest ranking committees
that are dissimilar to each other and then repeating
the process for any contexts not similar to existing
committees. In essence, CBC aims to find the clus-
ters that are similar to the largest set of contexts,
while keeping clusters dissimilar from each other.
CBC?s recursion ensures that contexts dissimilar to
the large committees are still grouped into their own
smaller committees, which enables the discovery of
infrequent senses with distinct contexts. We use a
hard sense assignment for each context, i.e., a con-
text is labeled with only one sense according to the
most similar cluster.
Streaming K-Means As WSI moves into induc-
ing senses from Web-scale amounts of data, exist-
ing clustering algorithms that keep all contexts in
memory become impractical. Jurgens and Stevens
(2010a) proposed an on-line hybrid clustering so-
lution using on-line K-Means and Hierarchical Ag-
glomerative Clustering, which automatically de-
cided the number of clusters without retaining all
the contexts. To the best of our knowledge, theirs
is the only work using an on-line approach. We
extend this work by applying a more theoretically
sound online K-Means algorithm, called Streaming
K-Means (Braverman et al, 2011), to WSI. We use
Streaming K-Means to conduct a direct algorithmic
comparison with K-Means in the hopes that online
approaches can be made just as effective as off-line
approaches.
Streaming K-Means processes each data point
only once, thus reducing the memory overhead dra-
matically. Instead of recording each data point, it
immediately assigns each data point to a cluster and
maintains K?C clusters. C varies as the algorithm
runs, initially being set to 0. When assigning a data
point, it is only assigned to an existing cluster when
their similar is above some threshold, otherwise the
data point becomes the centroid of a new cluster.
Once C reaches a threshold, based on an estimate of
the number of data points, or the overall K-Means
clustering cost reaches some limit, the centroids are
treated as new data points and re-clustered, with the
goal of merging some centroids. We follow (Jur-
gens and Stevens, 2010a) and cluster the final cen-
troids with Hierarchical Agglomerative Clustering,
with the average link criteria as suggested by (Ped-
ersen and Bruce, 1997).
3 Modeling Context
For each clustering algorithm, we consider five con-
text models that represent the types of lexical fea-
tures used by the majority of WSI approaches.
Co-Occurrence Contexts formed from word co-
occurrence are the most common in WSI algorithms.
For each occurrence of a word, those words within
a certain range are counted as features. Prior work
has used a variety of context sizes, e.g. words in
the same sentence (Bordag, 2006), in nearby lexi-
cal positions (Gauch and Futrelle, 1993), or within a
paragraph-sized context window (Pedersen, 2010).
We consider two co-occurrence context models:
a 5-word and a 25-word window. We note that in
co-occurrence-based word space algorithms, smaller
context sizes have shown to better capture paradag-
matic similarity, while larger sizes capture semantic
associativity (Peirsman et al, 2008; Utsumi, 2010).
Dependency-Relations Dependency parsing cre-
ates a syntax tree where words are directly linked
according to their relation. These links refine co-
occurrence based contexts by utilizing syntactic in-
dications of how words are related. Dependency
parsed features have proven highly effective for
word representations in many NLP applications,
e.g., (Pado? and Lapata, 2007; Baroni et al, 2010).
We follow Pantel and Lin (2002) and Dorow and
Widdows (2003) using the sentence as contexts and
all words with a dependency path of length 3 or less,
with the last word and its relation as a feature. We
note that recently Kern et al (2010) achieved good
WSI performance with only a small, manually-tuned
subset of all relations as context.
Word Ordering Word ordering can provide a
mild form of syntactic information (Jones et al,
2006; Sahlgren et al, 2008). While other syntac-
115
tic features may provide significantly more informa-
tion, word ordering is efficient to compute and pro-
vides an alternative source of syntactic information
for knowledge-lean systems or for languages where
NLP tools are not readily available.
Because we treat word ordering as a syntactic fea-
ture, we limit the context to words occurring in the
same sentence. A feature is the combination of a
co-occurring word and its relative position, i.e. the
same word in different positions is treated as two
separate features.
Parts of Speech Part of speech tagging can pro-
vide a preliminary coarse-grained sense disambigua-
tion of a word?s contextual features, where a word
may have as many senses as it does parts of speech.
For example, consider an occurrence of ?house? in
the context of ?address? as a noun and verb: ?I went
to his house address,? and ?I heard the legislator ad-
dress the house.? Labeling ?address? with its part
of speech provides for more semantic information
on its meaning, which further constrains the sense
of ?house.? Prior work (Pedersen and Bruce, 1997)
has suggested that this information can improve per-
formance, but to our knowledge, the impact of POS
features has not been evaluated in isolation.
Each context is formed from the containing sen-
tence; a feature is a combination of each word and its
part of speech, e.g., ?board-NOUN? is distinct from
?board-VERB.?
4 WSI Performance on Related Senses
The proposed methodology measures the ability of a
WSI approach to distinguish between related senses.
However, generating a large corpus with manu-
ally labeled sense assignments and sense similarity
judgements is prohibitively expensive. Therefore,
we employ a pseudo-word discrimination task where
a base word and a second word, its confounder,
are replaced throughout the corpus with a pseudo-
word. The objective is then to determine which of
the words was originally present given the context
of an occurrence of the pseudo-word. Due to not
requiring manual annotation, this type of task was
initially proposed as a substitute for word sense dis-
ambiguation (Schu?tze, 1992; Gale et al, 1992) and
for selectional preferences (Clark and Weir, 2002).
Following the suggestions of Chambers and Ju-
festival laws
offices 0.13660 interests 0.18289
play 0.13751 politics 0.20440
convention 0.20296 governments 0.29125
tournament 0.29007 regulations 0.40761
concerts 0.48348 legislation 0.56112
Table 2: Example confounders for ?festival? and
?laws? and their similarities
rafsky (2010) on designing pseudo-words, pseudo-
words were created from words with the same part
of speech and equal frequency in the training cor-
pus. We selected nouns occurring more than 5,000
times in a 2009 Wikipedia snapshot and then drew
5,000 contexts for each. The snapshot was tagged
with the Stanford Part of Speech Tagger (Toutanova
et al, 2003) and parsed with the Malt Parser (Nivre
et al, 2006).
To evaluate the impact of sense similarity, pseudo-
words were created from word pairs with a broad
range of lexical similarities. We selected lexical
similarity as an approximation of sense similarity
in order to model the hypothesis that similar senses
may appear in similar contexts. Similarity scores
were calculated using cosine similarity on contex-
tual distributions built from a sliding ?2 word win-
dow over the Wikipedia corpus. Table 2 highlights
several example confounders and their similarities
with the base term. In total, we generated 5000 term-
confounder pairs from 98 base terms, with a mean of
51 confounders per term.
All clustering parameters were chosen using the
default values provided in the original papers. K-
means and Streaming K-Means were both set with
a maximum of 15 clusters, with the final number of
clusters being determined by the data itself.
4.1 Evaluation
The pseudo-word?s senses are induced from a train-
ing segment using each feature and clustering com-
bination. Given that both words making up the
pseudo-word may be polysemous, more than two
senses may be induced. Each sense cluster is la-
beled according to which of the original words was
present in the majority of its contexts. For testing,
each instance of the pseudo-word in a previously
unseen context is assigned the label of the cluster
116
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Ps
eu
do
 W
or
d 
Di
sc
rim
in
at
io
n 
Ac
cu
ra
cy
Base word and Confounder Similarity
(a) 5-Word Co-Occurrence
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Ps
eu
do
 W
or
d 
Di
sc
rim
in
at
io
n 
Ac
cu
ra
cy
Base word and Confounder Similarity
(b) 25-Word Co-Occurrence
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Ps
eu
do
 W
or
d 
Di
sc
rim
in
at
io
n 
Ac
cu
ra
cy
Base word and Confounder Similarity
(c) Dependency Relations
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Ps
eu
do
 W
or
d 
Di
sc
rim
in
at
io
n 
Ac
cu
ra
cy
Base word and Confounder Similarity
(d) Word Order
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Ps
eu
do
 W
or
d 
Di
sc
rim
in
at
io
n 
Ac
cu
ra
cy
Base word and Confounder Similarity
(e) Parts of Speech (f)
Figure 1: Pseudo-word discrimination performance
to which it is most similar. We perform five-fold
cross-validation, using 4,000 contexts for training
and 1,000 contexts for testing. Discrimination ac-
curacy is reported as the average of all five runs.
Since an equal number of contexts are used for each
term, the base line accuracy of a most frequent sense
model is 50% for each pseudo-word.
4.2 Results and Discussion
Figure 1 shows the discrimination accuracy relative
to the similarity of a base pair and confounder, for
each feature and clustering algorithm combination.
Similarity values were binned at the 0.01 level with
a mean of 39.0 scores per bin (median=11). Be-
cause most word pairs are not related, the distri-
bution of similarity values is biased towards lower
values. Therefore, we omit similarity ranges above
0.5, as too few confounders occurred in that range to
draw reliable conclusions. The standard error (not
shown) is < 1 for all measurements.
The general trends suggests that the clustering al-
gorithm impacts the sense discriminatory ability far
more than the lexical feature choice. Furthermore,
sense similarity affects most clustering algorithms,
with most systems seeing a noticeable performance
drop when pseudo-word similarity is increased just
beyond 0. Performance at high similarity becomes
more variable for all algorithms and features.
For each clustering algorithm, we see dramati-
cally different trends. Streaming K-Means performs
well with co-occurrence based features and it does
poorly when either contexts have too many features,
as in the 25 Window Co-Occurrence feature space,
or the feature space overall is too sparse, as in the
Parts of Speech and Ordering feature spaces.
K-Means with the gap statistic converges to the
most frequent sense baseline for nearly every con-
founder pair. We note that this behavior significantly
differs from that seen in (Pedersen and Kulkarni,
2006), which clustered second-order co-occurrence
vectors rather than the first-order features that we
use. Our analysis showed that the H2 criterion was
responsible for this behavior. A subsequent analy-
sis revealed that K-Means still converged to MFS
for the E1, E2, I1, and I2 criterion functions (Zhao
and Karypis, 2001) as well as when the number of
artificial datasets was increased up to 100. How-
ever, additional tests using the same features on the
SemEval-1 WSI task did not converge to MFS. Fur-
ther investigation is needed to identify the cause of
convergence and what types of data are appropriate
117
the Gap Statistic.
Clustering by Committee performs well on most
models, but significantly worse on dependency re-
lation features. A subsequent analysis showed that
CBC generates significantly more clusters than all
other models. For the POS, 5 word window, and 25
window Co-Occurrence feature spaces, CBC gener-
ated between 205 and 247 clusters on average, per
word. With the order feature space, CBC generated
1087 clusters per word. However, when paired with
dependency relation features, the number of clusters
drops to only 78 per word.
Spectral Clustering is most affected by sense
similarity, performing competitively for unrelated
senses but dropping significantly when words be-
come even slightly similar. This performance drop
is seen across all features. Performance is therefore
low, with the exception of dependency relations.
Overall, these results suggest that sense related-
ness is a important factor in WSI performance and
its impact should be considered in future WSI eval-
uations. A potential next step is to vary the pro-
portion of contexts from the confounder. The cur-
rent method intentionally uses a uniform distribu-
tion to avoid potential bias; however, word sense dis-
tributions are rarely equal, and a varied distribution
would more closely model real world distributions.
Similarly, the current method tested only two senses,
whereas an n-way disambiguation between multiple
confounders should also provide further insight into
a WSI approach?s discriminatory abilities.
5 Sense Confusion in SemEval-2 Task 14
As a second experiment, we analyze incorrect sense
assignments on SemEval-2 Task 14 (Manandhar et
al., 2010) to measure whether sense-relatedness bi-
ases which sense was incorrectly selected. For WSI
systems, a similarity bias would indicate that similar
senses are more likely to be incorrectly identified as
a single sense.
We summarize Task 14 as follows. Systems are
provided with an unlabeled training corpus con-
sisting of 879,807 multi-sentence contexts for 100
polysemous words, comprised of 50 nouns and 50
verbs. Systems induce sense representations for tar-
get words from the training corpus and then use
those representations to label the senses of the tar-
get words in unseen contexts from a test corpus.
The induced senses are then evaluated against the
gold standard labels OntoNotes (Hovy et al, 2006)
senses labels for the test corpus. For our evaluation,
we use both the two contrasting unsupervised mea-
sures, the paired FScore (Artiles et al, 2009) and the
V-Measure (Rosenberg and Hirschberg, 2007), and
a supervised measure. For each metric, we use the
evaluation framework provided by the organizers of
SemEval-2 Task 14.1
The V-Measure rates the homogeneity and com-
pleteness of a clustering solution. Solutions that
have word clusters formed from one gold-standard
sense are homogeneous; completeness measures the
degree to which a gold-standard sense?s instances
are assigned to a single cluster. The paired FScore
measures two types of overlap of a solution and the
gold standard in cluster assignments for all in pair-
wise combination of instances. This score tends
to penalize solutions with many small clusters and
highly heterogeneous clusters (Manandhar and Kla-
paftis, 2009).
The supervised evaluation measures the recall
when building a Word Sense Disambiguation classi-
fier from the induced senses. The WSI system labels
the entire corpus, which is then divided into train-
ing and test portions. The sense labels in the train-
ing portion are used to construct a mapping from in-
duced senses to the gold standard OntoNotes labels.
This mapping is then evaluated for the induced la-
bels in the test. We report the scores for the 80%
training and 20% testing scenario.
5.1 Evaluation
We expect that if sense similarity is a factor in sense
confusion, the probability of confusion will increase
with sense similarity. Therefore, we measure the
probability of labeling an instance with the incorrect
OntoNotes sense relative to the sense similarity with
the gold standard sense.
In order to calculate the incorrect assignments,
the induced senses must be mapped to OntoNotes
senses. Each induced sense, si, is mapped to the
OntoNotes sense that occurs most frequently among
the instances in the test corpus that are assigned in-
duced sense si. We note that this labeling process
is only an approximate solution to assigning gold
standard labels to induced senses. A more robust
1
http://www.cs.york.ac.uk/semeval2010_WSI/
118
 0
 50
 100
 150
 200
 250
 300
 350
 0.05  0.1  0.15  0.2  0.25  0.3
Fr
eq
ue
nc
y 
of
 S
en
se
 C
on
fu
sio
n
Ontonote Sense Similarity
Actual (avg)
Baseline (avg)
Actual (max)
Baseline (max)
(a) Streaming K-means
 0
 20
 40
 60
 80
 100
 120
 140
 160
 0.05  0.1  0.15  0.2  0.25  0.3
Fr
eq
ue
nc
y 
of
 S
en
se
 C
on
fu
sio
n
Ontonote Sense Similarity
Actual (avg)
Baseline (avg)
Actual (max)
Baseline (max)
(b) CBC
 0
 50
 100
 150
 200
 250
 300
 350
 0.05  0.1  0.15  0.2  0.25  0.3
Fr
eq
ue
nc
y 
of
 S
en
se
 C
on
fu
sio
n
Ontonote Sense Similarity
Actual (avg)
Baseline (avg)
Actual (max)
Baseline (max)
(c) Spectral Clustering
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0.05  0.1  0.15  0.2  0.25  0.3
Fr
eq
ue
nc
y 
of
 S
en
se
 C
on
fu
sio
n
Ontonote Sense Similarity
Actual (avg)
Baseline (avg)
Actual (max)
Baseline (max)
(d) K-means
Figure 2: The error frequency distributions for confusing the correct sense with another sense of the given
similarity when using a 5-word co-occurrence window as context. Dashed lines indicate the null models.
labeling could take into account the distribution of
gold standard senses labels in the corpus from which
the senses are induced; however, such labels are not
available in the Task 14 training corpus.
For each incorrect sense assignment, we mea-
sure the similarity of the confused sense to the
correct sense. To our knowledge, no work has
been done on calculating sense similarity within the
OntoNotes sense hierarchy.2 Therefore, we approxi-
mate OntoNotes sense similarity by using sense sim-
ilarity in the WordNet ontology, on which has many
similarity measures have been defined. Following
Budanitsky and Hirst (2006), we estimate the Word-
Net sense similarity using the method proposed by
Jiang and Conrath (1997).
Each OntoNotes sense si is mapped to a set of
WordNet 3.0 senses Si = {wn1, . . . , wnn} using
2We suspect that this is in part because a word?s OntoNotes
senses have been designed to minimize sense confusion.
the sense mapping provided by the CoNLL shared
task.3 The sense similarity for two OntoNotes
senses is computed using one of two methods:
sim = 1|S1||S2|
?
wni?S1,wnj?S2
JCN(wni, wnj),
(1)
or
sim = argmax
wni?S1,wnj?S2
JCN(wni, wnj), (2)
where JCN indicates the Jiang-Conrath similar-
ity of two WordNet senses, calculated using Word-
Net::Similarity (Pedersen et al, 2004). Eq. 1 com-
putes similarity as the average similarity of all pair-
wise WordNet sense combinations, while Eq. 2 uses
the highest similarity. The resulting OntoNote sense
similarities range from 0 to 1, with 1 being maxi-
mally similar. We excluded 10 words from the test
3
http://conll.bbn.com/index.php/data.html
119
Context Feature Clustering V-Measure F-Score Recall # Clusters Purity GoF p-Value
5-Word Co-Occurrence
Streaming 6.7 55.5 54.8 4.74 0.103 p < 2.07e-37
Spectral 10.8 39.2 54.3 8.41 0.194 p < 1.11e-25
CBC 23.9 8.2 39.5 39.7 0.665 p < 0.916
K-Means 2.5 61.8 55.6 1.68 0.020 p < 1.20e-37
25-Word Co-Occurrence
Streaming 2.6 61.7 55.5 1.7 0.020 p < 1.20e-37
Spectral 5.0 48.6 55.9 3.3 0.083 p < 4.36e-32
CBC 21.3 11.6 45.0 32.2 0.561 p < 0.011
K-Means 2.5 61.8 55.6 1.68 0.020 p < 1.20e-37
Dependency Relations
Streaming 3.0 61.5 55.6 1.9 0.022 p < 7.33e-38
Spectral 8.5 46.8 55.3 5.9 0.134 p < 5.45e-14
CBC 12.9 31.3 52.4 11.4 0.259 p < 4.07e-12
K-Means 2.5 61.8 55.6 1.6 0.020 p < 1.20e-37
Word Order
Streaming 10.8 43.1 54.2 10.8 0.300 p < 4.46e-24
Spectral 12.2 32.4 53.7 10.0 0.26 p < 3.27e-20
CBC 27.2 11.8 30.3 54.9 0.857 p < 0.999
K-Means 2.5 61.8 55.6 1.6 0.020 p < 1.20e-37
Parts of Speech
Streaming 6.6 53.0 54.5 4.7 0.117 p < 1.06e-39
Spectral 10.9 39.4 53.7 8.3 0.201 p < 2.38e-13
CBC 23.8 08.0 40.1 39.7 0.678 p < 1.04e-2
K-Means 2.5 61.8 55.6 1.6 0.020 p < 1.20e-37
SemEval-2 Most Frequent Sense 0.0 63.4 58.6 1.0 0.0 p < 4.244e-23
Best SemEval-2 FScore 0.0 63.3 58.6 1.0 0.0 p < 2.893e-23
Best SemEval-2 VMeasure 16.2 26.7 58.3 10.7 0.367 p < 1.956e-14
Best SemEval-2 Supervised Recall 15.7 49.7 62.4 11.5 0.187 p < 8.910e-19
Table 3: Unsupervised and Supervised scores on the SemEval-2010 WSI Task for each feature and clustering
models, with reference scores for the top performing systems for each evaluation shown below.
set that did not have mappings from OntoNotes to
WordNet 3.0 senses, and additional 23 words that
only had two senses, which prevented testing for
a similarity bias. The remaining 67 words yielded
4,097 test instances for evaluation.
Each instance of the test corpus was tested for
sense confusion, recording the similarity of the in-
correctly assigned sense and the gold standard sense.
The resulting incorrect assignments are transformed
into an error distribution according by accumulating
error counts into similarity bins where each bin has a
range of 0.02. We analyze the WSI systems defined
in section 4 as well as the results of three systems
that participated in Task 14 and scored the highest
on the paired FScore, V-measure, or Supervised Re-
call evaluations.
To quantify the impact, we compare each system?s
error distribution against a null model over the set of
incorrect test instances missed by that system. In
the null model, the incorrect sense for each instance
is selected with uniform probability from the avail-
able senses. This behavior produces a distribution
with no similarity bias. The cumulative error dis-
tribution for the null model is not uniform due to
multiple sense pairings having the same similarity.4
To quantify the difference between a system?s error
distribution and corresponding null model, we cal-
culate the G-test as a measure of Goodness of Fit
(GoF). The resulting p-values reflect the probability
of observing the system?s error distribution if there
was no bias from sense-similarity.
5.2 Results and Discussion
We compare the error analysis against the evalua-
tion measures of Task 14. Table 3 displays the eval-
4Verb senses often have a JCN similarity of 0 due to hav-
ing no shared parent within the WordNet verb sense hierarchy,
which results in high frequency distribution around 0.
120
uation measures. We also report the average num-
ber of clusters per word, the cluster purity, and the
p-value when using Eq. 2 to measure sense similar-
ity. Figure 2 visualizes the error distributions for the
four clustering algorithms on 5-word co-occurrence
features. The distributions in Figure 2 are represen-
tative of those of the other context models, which we
omit due to space. Each plot reflects the frequency
at which a sense with the specified similarity was
confused for the correct sense.
The low p-values in Table 3 indicate a significant
deviation from the null model. Examining the shape
of the error distribution in Figure 2 reveals a no-
ticeable skew towards higher similarity when an in-
correct sense assignment is made. This distribution
skew is also consistent for both similarity measures.
Comparing the Task 14 results in Table 3 to the
sense confusion trends in in Figure 2 highlights an
interesting pattern among the various models: as the
number of induced sense clusters increases, the er-
ror distribution better approximates the null model.
Specifically, the GoF for all models was well corre-
lated with cluster purity (?=0.66), and the number of
clusters (?=0.76). CBC generated the highest num-
ber of clusters and has a sense confusion distribution
that closely matches the null model, indicating that
it is less affected by sense similarity. In compari-
son, all of the Streaming K-Means models, which
have the fewest clusters, differ noticeably from the
null model. Spectral Clustering, which also gener-
ates fewer clusters than CBC, has an observed con-
fusion rate that differs from the baseline. K-Means
again reduces to the MFS baseline.
When comparing along the feature sets, we see
that on average Word Order features generate the
highest V-Measure scores, highest purity, and high-
est p-values for Streaming K-Means and CBC. This
result correlates well with the average number of
features seen per context: Word Order contexts used
0.03% of the feature space while contexts in other
feature spaces used between 0.07% and 0.12% of
the feature space, suggesting that the SemEval mea-
sures are determined in part by feature space den-
sity. Similarly, 25-word co-occurrence features had
the highest percentage of features used per context,
0.12%, and generated the lowest V-Measure, purity
score, and p-value for 3 clustering models.
These scores support another known trend in the
SemEval-2 evaluation: the performance on the V-
Measure is proportional to the number of induced
sense clusters, while the paired FScore is inversely
proportional. But what is surprising is that models
which perform well against the V-Measure also ex-
hibit a smaller sense similarity bias, suggesting that
CBC and similar clustering methods are suitable for
situations where competing senses of a word have a
high degree of overlap.
As a final comparison, we also computed the
sense bias for the top 3 SemEval systems under each
measure. The best of these models are listed in Table
3. We did not find any consistent trends between the
V-Measure, purity, and p-value among these mod-
els. The top F-Scoring models all used either a first
or second order co-occurrence feature space similar
to ours (Kern et al, 2010; Pedersen, 2010), whereas
the top supervised score was achieved by a graph-
based system (Klapaftis and Manandhar, 2008).
6 Future Work and Conclusion
We presented a two evaluation for WSI approaches
and examined the performance of a wide range of
algorithms. The results raise a potential issue for
clustering-based WSI approaches: sense discrimi-
nation degrades notably as the sense relatedness in-
creases. We highlight three potential avenues for
future research. First, this methodology should be
applied to additional WSI models, such as graph-
based (Klapaftis and Manandhar, 2008; Navigli and
Crisafulli, 2010) and probabilistic models (Dinu and
Lapata, 2010; Elshamy et al, 2010). Second, we
plan to extend the analysis to different sense dis-
tributions, varying number of senses, and for hu-
man annotated sense similarity data. Third, this
evaluation makes the simplifying assumption of one
sense per instance; however, Erk et al (2009) note
that the relations between senses may cause a single
word instance to evoke multiple senses within the
same context. Therefore, a future experiment should
consider how WSI systems might address learning
senses given the presence of multiple, similar senses
for a single instance.
All models, associated data sets, testing frame-
work, and scores have been released as a part of the
open-source S-Space Package (Jurgens and Stevens,
2010b).5
5
http://code.google.com/p/airhead-research/
121
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations, pages 7?12.
ACL, June.
Javier Artiles, Enrique Amigo?, and Julio Gonzalo. 2009.
The role of named entities in web people search. In
Proceedings of EMNLP, pages 534?542. ACL.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Stefan Bordag. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. In Pro-
ceedings of the 11th EACL, pages 137?144.
Vladimir Braverman, Adam Meyerson, Rafail Ostrovsky,
Alan Roytman, Michael Shindler, and Brian Tagiku.
2011. Streaming k-means on Well-Clusterable Data.
In Proceedings of SODA 2011.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47, March.
Nathanael Chambers and Dan Jurafsky. 2010. Improving
the Use of Pseudo-Words for Evaluating Selectional
Preferences. In ACL 2010.
David Cheng, Ravi Kannan, Santosh Vempala, and Grant
Wang. 2006. A divide-and-merge methodology for
clustering. ACM Transactions on Database Systems
(TODS), 31(4):1499?1525.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the ACL-02 workshop on
Word sense disambiguation: recent successes and fu-
ture directions - Volume 8, WSD ?02, pages 32?39,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172. Association
for Computational Linguistics.
Beate Dorow and Dominic Widdows. 2003. Discover-
ing corpus-specific word senses. In Proceedings of the
10th EACL, pages 79?82.
Wesam Elshamy, Doina Caragea, and William H. Hsu.
2010. KSU KDD: Word sense induction by cluster-
ing in topic space. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 367?
370. Association for Computational Linguistics.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1, pages 10?18. Association
for Computational Linguistics.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Work on statistical methods for word
sense disambiguation. In AAAI Fall Symposium on
Probabilistic Approaches to Natural Language, pages
54?60.
Susan Gauch and Robert P. Futrelle. 1993. Experiments
in automatic word class and word sense identification
for information retrieval. In Proceedings of the 3rd
Annual Symposium on Document Analysis and Infor-
mation Retrieval, pages 425?434.
Zhengyan He, Yang Song, and Houfeng Wang. 2010.
Applying Spectral Clustering for Chinese Word Sense
Induction. In Proceedings of the CIPS-SIGHAN Joint
Conference on Chinese Language Processing.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages
57?60. Association for Computational Linguistics.
Jay J. Jiang and David W. Conrath. 1997. Semantic Sim-
ilarity Based on Corpus Statistics and Lexical Taxon-
omy. In Proceedings of International Conference on
Research in Computational Linguistics,.
Michael N. Jones, Walter Kintsch, and Doughlas J. K.
Mewhort. 2006. High-dimensional semantic space ac-
counts of priming. Journal of Memory and Language,
55:534?552.
David Jurgens and Keith Stevens. 2010a. HERMIT: Us-
ing word ordering applied to the Sense Induction task
of SemEval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluations. Association for
Computational Linguistics.
David Jurgens and Keith Stevens. 2010b. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations.
Roman Kern, Markus Muhr, and Michael Granitzer.
2010. KCDC: Word sense induction by using gram-
matical dependencies and sentence phrase structure.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 351?354. Association for
Computational Linguistics.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word
sense induction using graphs of collocations. In Pro-
ceeding of the 2008 conference on ECAI 2008: 18th
European Conference on Artificial Intelligence, pages
298?302.
122
Ioannis P. Klapaftis and Suresh Manandhar. 2010. Tax-
onomy learning using word sense induction. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, HLT ?10, pages
82?90, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Suresh Manandhar and Ioannis P. Klapaftis. 2009.
SemEval-2010 Task 14: Evaluation Setting for Word
Sense Induction & Disambiguation Systems. In
NAACL-HLT 2009 Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-
2010 task 14: Word sense induction & disambigua-
tion. In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68. Association for
Computational Linguistics.
Diana McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. Making Sense of Sense:
Bringing Psycholinguistics and Computational Lin-
guistics Together, page 17.
Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing
word senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 116?
126. Association for Computational Linguistics.
Roberto Navigli. 2009. Word sense disambiguation: a
survey. ACM Computing Surveys, 41(2):1?69.
Andrew Ng, Michael Jordan, and Yair Weiss. 2001. On
spectral clustering: Analysis and an algorithm. In Ad-
vances in Neural Information Processing Systems 14:
Proceeding of the 2001 Conference, pages 849?856.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC, pages 2216?2219.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
Based Construction of Semantic Space Models. Com-
putational Linguistics, 33(2):161?199.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
Natural Language Engineering, 13(02):137?163.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM Conference
on Knowledge Discovery and Data Mining (KDD-02),
pages 613?619.
Ted Pedersen and Rebecca Bruce. 1997. Distinguishing
word senses in untagged text. In Proceedings of the
Second Conference on Empirical Methods in Natural
Language Processing, pages 197?207, August.
Ted Pedersen and Anagha Kulkarni. 2006. Automatic
cluster stopping with criterion functions and the gap
statistic. In In Proceedings of the Demo Session of
HLT/NAACL, pages 276?279.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet:: Similarity: measuring the
relatedness of concepts. In Demonstration Papers at
HLT-NAACL 2004 on XX, pages 38?41. Association
for Computational Linguistics.
Ted Pedersen. 2010. Duluth-WSI: SenseClusters Ap-
plied to the Sense Induction Task of SemEval-2. In
Proceedings of the SemEval 2010 Workshop : the
5th International Workshop on Semantic Evaluations,
pages 363?366, July.
Yves Peirsman, Kris Heylen, and Dirk Geeraerts. 2008.
Size matters. Tight and loose context definitions in En-
glish word space models. In ESSLLI Workshop on Dis-
tributional Lexical Semantics, pages 34?41.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL). ACL, June.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order in
word space. In Proceedings of the 30th Annual Meet-
ing of the Cognitive Science Society (CogSci?08).
Hinrich Schu?tze, 1992. Context Space, pages 113?120.
AAAI Press, Menlo Park, CA.
Robert Tibshirani, Guenther Walther, and Trevor Hastie.
2000. Estimating the number of clusters in a dataset
via the gap statistic. Journal Royal Statistics Society
B, 63:411?423.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. pages 252?
259.
Akira Utsumi. 2010. Exploring the Relationship be-
tween Semantic Spaces and Semantic Relations. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation (LREC?2010),
pages 257?262.
Ulrike von Luxburg. 2007. A tutorial on spectral cluster-
ing. Statistics and Computing, 17(4):395?416.
Ying Zhao and George Karypis. 2001. Criterion func-
tions for document clustering: Experiments and analy-
sis. Technical Report UMN CS 01-040, University of
Minnesota.
123
Proceedings of the 2012 Student Research Workshop, pages 25?30,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Evaluating Unsupervised Ensembles when applied to Word Sense Induction
Keith Stevens1,2
1University of California Los Angeles; Los Angeles , California, USA
2Lawrence Livermore National Lab; Livermore, California, USA?
kstevens@cs.ucla.edu
Abstract
Ensembles combine knowledge from distinct
machine learning approaches into a general
flexible system. While supervised ensembles
frequently show great benefit, unsupervised
ensembles prove to be more challenging. We
propose evaluating various unsupervised en-
sembles when applied to the unsupervised task
of Word Sense Induction with a framework for
combining diverse feature spaces and cluster-
ing algorithms. We evaluate our system us-
ing standard shared tasks and also introduce
new automated semantic evaluations and su-
pervised baselines, both of which highlight the
current limitations of existing Word Sense In-
duction evaluations.
1 Introduction
Machine learning problems often benefit from many
differing solutions using ensembles (Dietterich,
2000) and supervised Natural Language Processing
tasks have been no exception. However, use of un-
supervised ensembles in NLP tasks has not yet been
rigorously evaluated. Brody et al (2006) first con-
sidered unsupervised ensembles by combining four
state of the art Word Sense Disambiguation systems
using a simple voting scheme with much success.
Later, Brody and Lapata (2009) combined different
feature sets using a probabilistic Word Sense Induc-
tion model and found that only some combinations
produced an improved system. These early and lim-
ited evaluations show both the promise and draw-
back of combining different unsupervised models:
?This work was performed under the auspices of the U.S.
Department of Energy by Lawrence Livermore National Lab-
oratory under Contract DE-AC52-07NA27344 (LLNL-CONF-
530791).
particular combinations provide a benefit but select-
ing these combinations is non-trivial.
We propose applying a new and more gen-
eral framework for combining unsupervised systems
known as Ensemble Clustering to unsupervised NLP
systems and focus on the fully unsupervised task
of Word Sense Induction. Ensemble Clustering can
combine together multiple and diverse clustering al-
gorithms or feature spaces and has been shown to
noticeably improve clustering accuracy for both text
based datasets and other datasets (Monti et al, 2003;
Strehl et al, 2002). Since Word Sense Induction is
fundamentally a clustering problem, with many vari-
ations, it serves well as a NLP case study for Ensem-
ble Clustering.
The task of Word Sense Induction extends the
problem of Word Sense Disambiguation by simply
assuming that a model must first learn and define a
sense inventory before disambiguating multi-sense
words. This induction step frees the disambiguation
process from any fixed sense inventory and can in-
stead flexibly define senses based on observed pat-
terns within a dataset (Pedersen, 2006). However,
this induction step has proven to be greatly chal-
lenging, in the most recent shared tasks, induction
systems either appear to perform poorly or fail to
outperform the simple Most Frequent Sense baseline
(Agirre and Soroa, 2007a; Manandhar et al, 2010).
In this work, we propose applying Ensemble
Clustering as a general framework for combining
not only different feature spaces but also a variety of
different clustering algorithms. Within this frame-
work we will explore which types of models should
be combined and how to best combine them. In ad-
dition, we propose two new evaluations: (1) new se-
mantic coherence measures that evaluate the seman-
25
Figure 1: The Ensemble Clustering model: individ-
ual clustering algorithms partition perturbations of
the dataset and all partitions are combined via a con-
sensus function to create a final solution, pi?.
tic quality and uniqueness of induced word senses
without referring to an external sense inventory (2)
and a new set of baseline systems based on super-
vised learning algorithms. With the new evaluations
and a framework for combining general induction
models, we intend to find not only improved models
but a better understanding of how to improve later
induction models.
2 Consensus Clustering
Ensemble Clustering presents a new method for
combining together arbitrary clustering algorithms
without any supervision (Monti et al, 2003; Strehl
et al, 2002). The method adapts simple boosting
and voting approaches from supervised ensembles
to merge together diverse clustering partitions into
a single consensus solution. Ensemble Clustering
forms a single consensus partition by processing a
data set in two steps: (1) create a diverse set of en-
sembles that each partition some perturbation of the
full dataset and (2) find the median partition that best
agrees with each ensemble?s partition. Figure 1 vi-
sually displays these two steps.
Variation in these two steps accounts for the wide
variety of Ensemble Clustering approaches. Each
ensemble can be created from either a large col-
lection of distinct clustering algorithms or through
a boosting approach where the same algorithm is
trained on variations of the dataset. Finding the me-
dian partition turns out to be an NP-Complete prob-
lem under most settings (Goder and Filkov, 2008)
and thus must be approximated with one of sev-
eral heuristics. We consider several well tested ap-
proaches to both steps.
Formally, we define Ensemble Clustering to
operate over a dataset of N elements: D =
{d1, . . . , dN}. Ensemble Clustering then creates H
ensembles that each partition a perturbation Dh of D
to create H partitions, ? = {pi1, ? ? ? , piH}. The con-
sensus algorithm then approximates the best consen-
sus partition pi? that satisfies:
argmin
pi?
?
pih??
d(pih, pi?) (1)
according to some distance metric d(pii, pij) between
two partitions. We use the symmetric difference dis-
tance as d(pii, pij). Let Pi be the set of co-cluster
data points in pii. The distance metric is then defined
to be
d(pi1, pi2) = |P1 \ P2|+ |P2 \ P1|
2.1 Forming Ensembles
Ensemble clustering can combine together overlap-
ping decisions from many different clustering algo-
rithms or it can similarly boost the performance of a
single algorithm by using different parameters. We
consider two simple formulations of ensemble cre-
ation: Homogeneous Ensembles and Heterogeneous
Ensembles. We secondly consider approaches for
combining the two creation methods.
Homogeneous Ensembles partition randomly
sampled subsets of the data points from D without
replacement. By sampling without replacement,
each ensemble will likely see different representa-
tions of each cluster and can specialize its partition
the around observed subset. Furthermore, each
ensemble will observe less noise and can better
define each true cluster (Monti et al, 2003). We
note that since each ensemble only observes an
incomplete subset of D, some clusters may not be
represented at all in some partitions.
Heterogeneous Ensembles create diverse parti-
tions by simply using complete partitions over D
from different clustering algorithms, either due to
different parameters or due to completely different
clustering models (Strehl et al, 2002).
26
Combined Heterogeneous and Homogeneous En-
sembles can be created by creating many homo-
geneous variations of each distinct clustering algo-
rithm within a heterogeneous ensemble. In this
framework, each single method can be boosted by
subsampling the data in order to observe the true
clusters and then combined with other algorithms
using differing cluttering criteria.
2.2 Combining data partitions
Given the set of partitions, ? = {pii, ? ? ? , pih}, the
consensus algorithm must find a final partition, pi?
that best minimizes Equation 1. We find an approx-
imation to pi? using the following algorithms.
Agglomerative Clustering first creates a consen-
sus matrix, M that records the aggregate decisions
made by each partition. Formally, M records the
fraction of partitions that observed two data point
and assigned them to the same cluster:
M(i, j) =
?h
k=1 1{di, dj ? pick}
?h
k=1 1{di, dj ? pik}
Where di refers to element i, pick refers to cluster c in
partition pik, and 1{?} is the indicator function. The
consensus partition, pi? is then the result of creat-
ing C partitions with Agglomerative Clustering us-
ing the Average Link criterion and M as the simi-
larity between each data point (Monti et al, 2003).
Best of K simply sets pi? as the partition pih ? ?
that minimizes Equation 1 (Goder and Filkov, 2008).
Best One Element Move begins with an initial
consensus partition p?i? and repeatedly changes the
assignment of a single data point such that Equa-
tion 1 is minimized and repeats until no move can
be found. We initialize this with Best of K.
Filtered Stochastic Best One Element Move
also begins with an initial consensus partition p?i? and
repeatedly finds the best one element move, but does
not compare against every partition in ? for each it-
eration. It instead maintains a history of move costs
and updates that history with a stochastically se-
lected partition from ? for each move iteration and
ends after some fixed number of iterations (Zheng et
al., 2011).
Figure 2: The general Word Sense Induction Model:
models extract distributional data from contexts and
induce senses by clustering the extracted informa-
tion. Models then use representations of each sense
to disambiguate new contexts.
3 Word Sense Induction Models
Word Sense Induction models define word senses in
terms of the distributional hypothesis, whereby the
meaning of a word can be defined by the surround-
ing context (Haris, 1985). Rather than form a single
representation for any word, induction models repre-
sent the distinct contexts surrounding a multi-sense
word and find commonalities between the observed
contexts by clustering. These similar contexts then
define a particular word sense and can be used to
later recognize later instances of the sense, Figure 2.
Models can be roughly categorized based on their
context model and their clustering algorithm into
two categories: feature vector methods and graph
methods. Feature vector methods simply transform
each context into a feature vector that records con-
textual information and then cluster with any algo-
rithm that can partition individual data points. Graph
methods build a large distributional graph that mod-
els lexical features from all contexts and then parti-
tions the graph using a graph-based clustering algo-
rithm. In both cases, models disambiguate new uses
of a word by finding the sense with the most features
in common with the new context.
3.1 Context Models
Context models follow the distributional hypothesis
by encoding various lexical and syntactic features
that frequently occur with a multi-sense word. Each
context model records different levels of informa-
tion, and in different formats, but are limited to fea-
27
tures available from syntactic parsing. Below we
summarize our context models which are based on
previous induction systems:
Word Co-occurence (WoC) acts as the core fea-
ture vector method and has been at the core of nearly
all systems that model distributional semantics (Ped-
ersen, 2006). The WoC model represents each con-
text simply as the words within ?W words from
the multi-sense word. Each co-occurring word is
weighted by the number of times it occurs within
the window.
Parts of Speech (PoS) extends the WoC model
by appending each lexical feature with its part of
speech. This provides a simple disambiguation
of each feature so that words with multiple parts
of speech are not conflated into the same feature.
(Brody et al, 2006).
Dependency Relations (DR) restrains word co-
occurrence to words that are reachable from the
multi-sense word via a syntactic parse composed
of dependency relationships limited by some length
(Pado? and Lapata, 2007). We treat each reachable
word and the last relation in the path as a feature
(Van de Cruys and Apidianaki, 2011).
Second Order Co-occurrence (SndOrd) provides
a rough compositional approach to representing sen-
tences that utilizes word co-occurrence and partially
solves the data sparsity problem observed with the
WoC model. The SndOrd model first builds a large
distributional vector for each word in a corpus and
then forms context vectors by adding the distribu-
tional vector for each co-occurring context word
(Pedersen, 2006).
Graph models encode rich amounts of linguistic
information for all contexts as a large distributional
graph. Each co-occurring context word is assigned
a node in the graph and edges are formed between
any words that co-occur in the same context. The
graph is refined by comparing nodes and edges to a
large representative corpus and dropping some oc-
currences (Klapaftis and Manandhar, 2010).
Latent Factor Models projects co-occurrence in-
formation into a latent feature space that ties to-
gether relationships between otherwise distinct fea-
tures. We consider three latent models: the Singular
Value Decomposition (SVD) (Schu?tze, 1998), Non-
negative Matrix Factorization (NMF) (Van de Cruys
and Apidianaki, 2011), and Latent Dirichlet Alloca-
tion (Brody and Lapata, 2009). We note that SVD
and NMF operate as a second step over any feature
vector model whereas LDA is a standalone model.
3.2 Clustering Algorithms
Distributional clustering serves as the main tool
for detecting distinct word senses. Each algorithm
makes unique assumptions about the distribution of
the dataset and should thus serve well as diverse
models, as needed by supervised ensembles (Diet-
terich, 2000). While many WSI models automat-
ically estimate the number of clusters for a word,
we initially simplify our evaluation by assuming the
number of clusters is known a priori and instead fo-
cus on the distinct underlying clustering algorithms.
Below we briefly summarize each base algorithm:
K-Means operates over feature vectors and iter-
atively refines clusters by associating each context
vector with its most representative centroid and then
reformulating the centroid (Pedersen and Kulkarni,
2006).
Hierarchical Agglomerative Clustering can be
applied to both feature vectors and collocation
graphs. In both cases, each sentences or collocation
vertex is placed in their own clusters and then the
two most similar clusters are merged together into a
new cluster (Schu?tze, 1998).
Spectral Clustering separates an associativity
matrix by finding the cut with the lowest conduc-
tance. We consider two forms of spectral clustering:
EigenCluster (Cheng et al, 2006), a method origi-
nally designed to cluster snippets for search results
into semantically related categories, and GSpec (Ng
et al, 2001), a method that directly clusters a collo-
cation graph.
Random Graph Walks performs a series of ran-
dom walks through a collocation graph in order to
discover nodes that serve as central discriminative
points in the graph and tightly connected compo-
nents in the graph. We consider Chinese Whispers
(Klapaftis and Manandhar, 2010) and a hub selec-
tion algorithm (Agirre and Soroa, 2007b).
28
4 Proposed Evaluation
We first propose evaluating ensemble configurations
of Word Sense Induction models using the standard
shared tasks from SemEval-1 (Agirre and Soroa,
2007a) and SemEval-2 (Manandhar et al, 2010).
We then propose comparing these results, and past
SemEval results, to supervised baselines as a gauge
of how well the algorithms do compared to more in-
formed models. We then finally propose an intrin-
sic evaluation that rates the semantic interpretability
and uniqueness of each induced sense.
Evaluating Ensemble Configurations must be
done to determine which variation of Ensemble
Clustering best applies to the Word Sense Induction
tasks. Preliminary research has shown that Homoge-
neous ensemble combined with the HAC consensus
function typically improve base models while com-
bining heterogeneous induction models greatly re-
duces performance. We thus propose various sets of
ensembles to evaluate whether or not certain context
models or clustering algorithms can be effectively
combined:
1. mixing different feature vector models with the same
clustering algorithm,
2. mixing different clustering algorithms using the same
context model,
3. mixing feature vector context models and graph context
models using matching clustering algorithms,
4. mixing all possible models,
5. and improving each heterogeneous algorithm by first
boosting them with homogeneous ensembles.
SemEval Shared Tasks provide a shared corpus
and evaluations for comparing different WSI Mod-
els. Both shared tasks from SemEval provide a cor-
pus of training data for 100 multi-sense words and
then compare the induced sense labels generated for
a set of test contexts with human annotated sense
using a fixed sense inventory. The task provides two
evaluations: an unsupervised evaluation that treats
each set of induced senses as a clustering solution
and measures accuracy with simple metrics such as
the Paired F-Score, V-Measure, and Adjusted Mu-
tual Information; and a supervised evaluation that
builds a simple supervised word sense disambigua-
tion system using the sense labels (Agirre and Soroa,
2007a; Manandhar et al, 2010).
Supervised Baselines should set an upper limit
on the performance we can expect from most unsu-
pervised algorithms, as has been observed in other
NLP tasks. We train these baselines by using feature
vector models in combination with the SemEval-1
dataset1. We propose several standard supervised
machine learning algorithms as different baselines:
Naive Bayes, Logistic Regression, Decision Trees,
Support Vector Machines, and various ensembles of
each such as simple Bagged Ensembles.
Semantic Coherence evaluations balance the
shared task evaluations by functioning without a
sense inventory. Any evaluation against an existing
inventory cannot accurately measure newly detected
senses, overlapping senses, or different sense gran-
ularities. Therefore, our proposed sense coherence
measures focus on the semantic quality of a sense,
adapted from topic coherence measures (Newman
et al, 2010; Mimno et al, 2011). These evaluate
the degree to which features in an induced sense de-
scribe the meaning of the word sense, where highly
related features constitute a more coherent sense and
unrelated features indicate an incoherent sense. Fur-
thermore, we adapt the coherence metric to evaluate
the amount of semantic overlap between any two in-
duced senses.
5 Concluding Remarks
This research will better establish the benefit of
Ensemble Clustering when applied to unsuper-
vised Natural Language Processing tasks that cen-
ter around clustering by examining which feature
spaces and algorithms can be effectively combined
along with different different ensemble configura-
tions. Furthermore, this work will create new base-
lines that evaluate the inherent challenge of Word
Sense Induction and new automated and knowledge
lean measurements that better evaluate new or over-
lapping senses learned by induction systems. All of
the work will be provided as part of a flexible open
source framework that can later be applied to new
context models and clustering algorithms.
1We cannot use graph context models as they do not model
contexts individually, nor can we use the SemEval-2 dataset be-
cause the training set lacks sense labels needed for training su-
pervised systems
29
References
Eneko Agirre and Aitor Soroa. 2007a. Semeval-2007
task 02: evaluating word sense induction and discrim-
ination systems. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 7?12, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Eneko Agirre and Aitor Soroa. 2007b. Ubc-as: a
graph based unsupervised system for induction and
classification. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 346?349, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?09, pages 103?
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised wsd. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 97?104, Sydney, Australia, July. Association for
Computational Linguistics.
David Cheng, Ravi Kannan, Santosh Vempala, and Grant
Wang. 2006. A divide-and-merge methodology for
clustering. ACM Trans. Database Syst., 31:1499?
1525, December.
Thomas G. Dietterich. 2000. Ensemble methods in ma-
chine learning. In Proceedings of the First Interna-
tional Workshop on Multiple Classifier Systems, MCS
?00, pages 1?15, London, UK. Springer-Verlag.
Andrey Goder and Valdimir Filkov, 2008. Consensus
Clustering Algorithms: Comparison and Refinement.,
pages 109?117.
Zellig Haris, 1985. Distributional Structure, pages 26?
47. Oxford University Press.
Ioannis P. Klapaftis and Suresh Manandhar. 2010. Word
sense induction & disambiguation using hierarchical
random graphs. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?10, pages 745?755, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. Semeval-2010 task 14:
Word sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 63?68, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
David Mimno, Hanna Wallach, Edmund Talley, Miriam
Leenders, and Andrew McCallum. 2011. Optimizing
semantic coherence in topic models. In Proceedings of
the 2011 Conference on Emperical Methods in Natu-
ral Language Processing, pages 262?272, Edinburgh,
Scotland, UK. Association of Computational Linguis-
tics.
Stefano Monti, Pablo Tamayo, Jill Mesirov, and Todd
Golub. 2003. Consensus clustering ? a resampling-
based method for class discovery and visualization of
gene expression microarray data. Machine Learning,
52:91?118, July.
David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010. Evaluating topic
models for digital libraries. In Proceedings of the 10th
annual joint conference on Digital libraries, JCDL
?10, pages 215?224, New York, NY, USA. ACM.
A. Ng, M. Jordan, and Y. Weiss. 2001. On Spectral Clus-
tering: Analysis and an algorithm. In T. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances in
Neural Information Processing Systems, pages 849?
856. MIT Press.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
Based Construction of Semantic Space Models. Com-
putational Linguistics, 33(2):161?199.
Ted Pedersen and Anagha Kulkarni. 2006. Automatic
cluster stopping with criterion functions and the gap
statistic. In Proceedings of the 2006 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy: companion volume: demonstrations, NAACL-
Demonstrations ?06, pages 276?279, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ted Pedersen. 2006. Unsupervised corpus-based meth-
ods for WSD. In Word Sense Disambiguation: Algo-
rithms and Applications, pages 133?166. Springer.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Comput. Linguist., 24:97?123, March.
Alexander Strehl, Joydeep Ghosh, and Claire Cardie.
2002. Cluster ensembles - a knowledge reuse frame-
work for combining multiple partitions. Journal of
Machine Learning Research, 3:583?617.
Tim Van de Cruys and Marianna Apidianaki. 2011. La-
tent semantic word sense induction and disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 1476?1485, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Haipeng Zheng, S.R. Kulkarni, and V.H. Poor. 2011.
Consensus clustering: The filtered stochastic best-
one-element-move algorithm. In Information Sciences
and Systems (CISS), 2011 45th Annual Conference on,
pages 1 ?6, march.
30
