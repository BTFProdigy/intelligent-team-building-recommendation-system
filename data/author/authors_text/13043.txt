Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 65?71,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Combining Morphosyntactic Enriched Representation with
n-best Reranking in Statistical Translation
H. Bonneau-Maynard, A. Allauzen, D. De?chelotte and H. Schwenk
Spoken Language Processing Group
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
{maynard,allauzen,dechelot,schwenk}@limsi.fr
Abstract
The purpose of this work is to explore
the integration of morphosyntactic infor-
mation into the translation model itself, by
enriching words with their morphosyntac-
tic categories. We investigate word dis-
ambiguation using morphosyntactic cate-
gories, n-best hypotheses reranking, and
the combination of both methods with
word or morphosyntactic n-gram lan-
guage model reranking. Experiments
are carried out on the English-to-Spanish
translation task. Using the morphosyn-
tactic language model alone does not
results in any improvement in perfor-
mance. However, combining morphosyn-
tactic word disambiguation with a word
based 4-gram language model results in a
relative improvement in the BLEU score
of 2.3% on the development set and 1.9%
on the test set.
1 Introduction
Recent works in statistical machine translation
(SMT) shows how phrase-based modeling (Och and
Ney, 2000a; Koehn et al, 2003) significantly out-
perform the historical word-based modeling (Brown
et al, 1993). Using phrases, i.e. sequences of
words, as translation units allows the system to pre-
serve local word order constraints and to improve
the consistency of phrases during the translation pro-
cess. Phrase-based models provide some sort of
context information as opposed to word-based mod-
els. Training a phrase-based model typically re-
quires aligning a parallel corpus, extracting phrases
and scoring them using word and phrase counts. The
derived statistics capture the structure of natural lan-
guage to some extent, including implicit syntactic
and semantic relations.
The output of a SMT system may be difficult to
understand by humans, requiring re-ordering words
to recover its syntactic structure. Modeling language
generation as a word-based Markovian source (an n-
gram language model) discards linguistic properties
such as long term word dependency and word-order
or phrase-order syntactic constraints. Therefore, ex-
plicit introduction of structure in the language mod-
els becomes a major and promising focus of atten-
tion.
However, as of today, it seems difficult to outper-
form a 4-gram word language model. Several stud-
ies have attempted to use morphosyntactic informa-
tion (also known as part-of-speech or POS informa-
tion) to improve translation. (Och et al, 2004) have
explored many different feature functions. Rerank-
ing n-best lists using POS has also been explored by
(Hasan et al, 2006). In (Kirchhoff and Yang, 2005),
a factored language model using POS information
showed similar performance to a 4-gram word lan-
guage model. Syntax-based language models have
also been investigated in (Charniak et al, 2003). All
these studies use word phrases as translation units
and POS information in just a post-processing step.
This paper explores the integration of morphosyn-
tactic information into the translation model itself
by enriching words with their morphosyntactic cat-
65
egories. The same idea has already been applied
in (Hwang et al, 2007) to the Basic Travel Ex-
pression Corpus (BTEC). To our knowledge, this
approach has not been evaluated on a large real-
word translation problem. We report results on
the TC-STAR task (public European Parliament Ple-
nary Sessions translation). Furthermore, we pro-
pose to combine this approach with classical n-best
list reranking. Experiments are carried out on the
English-to-Spanish task using a system based on the
publicly available Moses decoder.
This paper is organized as follows: In Section
2 we first describe the baseline statistical machine
translation systems. Section 3 presents the consid-
ered task and the processing of the corpora. The
experimental evaluation is summarized in section 4.
The paper concludes with a discussion of future re-
search directions.
2 System Description
The goal of statistical machine translation is to pro-
duce a target sentence e from a source sentence f .
Among all possible target language sentences the
one with the highest probability is chosen. The use
of a maximum entropy approach simplifies the intro-
duction of several additional models explaining the
translation process:
e? = argmaxPr(e|f)
= argmaxe {exp(
?
i
?ihi(e, f))} (1)
where the feature functions hi are the system
models characterizing the translation process, and
the coefficients ?i act as weights.
2.1 Moses decoder
Moses1 is an open-source, state-of-the-art phrase-
based decoder. It implements an efficient beam-
search algorithm. Scripts are also provided to train a
phrase-based model. The popular Giza++ (Och and
Ney, 2000b) tool is used to align the parallel corpora.
The baseline system uses 8 feature functions hi,
namely phrase translation probabilities in both di-
rections, lexical translation probabilities in both di-
rections, a distortion feature, a word and a phrase
1http://www.statmt.org/moses/
penalty and a trigram target language model. Ad-
ditional features can be added, as described in the
following sections. The weights ?i are typically op-
timized so as to maximize a scoring function on a
development set (Och and Ney, 2002).
The moses decoder can output n-best lists, pro-
ducing either distinct target sentences or not (as
different segmentations may lead to the same sen-
tence). In this work, distinct sentences were always
used.
These n-best lists can be rescored using higher
order language models (word- or syntactic-based).
There are two ways to carry out the rescoring: one,
by replacing the language model score or by adding
a new feature function; two, by performing a log-
linear interpolation of the language model used for
decoding and the new language model. This latter
approach was used in all the experiments described
in this paper. The set of weights is systematically
re-optimized using the algorithm presented below.
2.2 Weight optimization
A common criterion to optimize the coefficients of
the log-linear combination of feature functions is to
maximize the BLEU score (Papineni et al, 2002)
on a development set (Och and Ney, 2002). For
this purpose, the public numerical optimization tool
Condor (Berghen and Bersini, 2005) is integrated in
the following iterative algorithm:
0. Using good general purpose weights, the
Moses decoder is used to generate 1000-best
lists.
1. The 1000-best lists are reranked using the cur-
rent set of weights.
2. The current hypothesis is extracted and scored.
3. This BLEU score is passed to Condor, which
either computes a new set of weights (the al-
gorithm then proceeds to step 1) or detects that
a local maxima has been reached and the algo-
rithm stops iterating.
The solution is usually found after about 100 itera-
tions. It is stressed that the n-best lists are generated
only once and that the whole tuning operates only
on the n-best lists.
66
English: IPP declareV V P resumedV V D theDT sessionNN ofIN theDT EuropeanNP ParliamentNP
Spanish: declaroV Lfin reanudadoV Ladj elART perodoNC dePREP sesionesNC
delPDEL ParlamentoNC EuropeoADJ
Figure 1: Example of POS-tag enriched bi-text used to train the translation models
2.3 POS disambiguation
It is well-known that syntactic structures vary
greatly across languages. Spanish, for example,
can be considered as a highly inflectional language,
whereas inflection plays only a marginal role in En-
glish.
POS language models can be used to rerank the
translation hypothesis, but this requires tagging the
n-best lists generated by the SMT system. This can
be difficult since POS taggers are not well suited for
ill-formed or incorrect sentences. Finding a method
in which morphosyntactic information is used di-
rectly in the translation model could help overcome
this drawback but also takes account for the syntac-
tic specificities of both source and target languages.
It seems likely that the morphosyntactic informa-
tion of each word will be useful to encode linguis-
tic characteristics, resulting in a sort of word disam-
biguation by considering its morphosyntactic cate-
gory. Therefore, in this work we investigate a trans-
lation model which enriches every word with its syn-
tactic category. The enriched translation units are a
combination of the original word and the POS tag, as
shown in Figure 1. The translation system takes a se-
quence of enriched units as inputs and outputs. This
implies that the test data must be POS tagged before
translation. Likewise, the POS tags in the enriched
output are removed at the end of the process to pro-
vide the final translation hypothesis which contain
only a word sequence. This approach also allows
to carry out a n-best reranking step using either a
word-based or a POS-based language model.
3 Task, corpus and tools
The experimental results reported in this article were
obtained in the framework of an international evalu-
ation organized by the European TC-STAR project2
in February 2006. This project is envisaged as a
2http://www.tc-star.org/
long-term effort to advance research in all core tech-
nologies for speech-to-speech translation.
The main goal of this evaluation is to trans-
late public European Parliament Plenary Sessions
(EPPS). The training material consists of the sum-
mary edited by the European Parliament in several
languages, which is also known as the Final Text
Editions (Gollan et al, 2005). These texts were
aligned at the sentence level and they are used to
train the statistical translation models (see Table 1
for some statistics).
Spanish English
Whole parallel corpus
Sentence Pairs 1.2M
Total # Words 34.1M 32.7M
Vocabulary size 129k 74k
Sentence length ? 40
Sentence Pairs 0.91M
Total # Words 18.5M 18.0M
Word vocabulary 104k 71k
POS vocabulary 69 59
Enriched units vocab. 115k 77.6k
Table 1: Statistics of the parallel texts used to train
the statistical machine translation system.
Three different conditions are considered in the
TC-STAR evaluation: translation of the Final Text
Edition (text), translation of the transcriptions of the
acoustic development data (verbatim) and transla-
tion of speech recognizer output (ASR). Here we
only consider the verbatim condition, translating
from English to Spanish. For this task, the develop-
ment and test data consists of about 30k words. The
test data is partially collected in the Spanish parlia-
ment. This results in a small mismatch between de-
velopment and test data. Two reference translations
are provided. The scoring is case sensitive and in-
cludes punctuation symbols.
67
3.1 Text normalization
The training data used for normalization differs sig-
nificantly from the development and test data. The
Final Text Edition corpus follows common ortho-
graphic rules (for instance, the first letter of the word
following a full stop or a column is capitalized) and
represents most of the dates, quantities, article refer-
ences and other numbers in digits. Thus the text had
to be ?true-cased? and all numbers were verbalized
using in-house language-specific tools. Numbers are
not tagged as such at this stage; this is entirely left
to the POS tagger.
3.2 Translation model training corpus
Long sentences (more than 40 words) greatly slow
down the training process, especially at the align-
ment step with Giza++. As shown in Figure 2, the
histogram of the length of Spanish sentences in the
training corpus decreases steadily after a length of
20 to 25 words, and English sentences exhibit a sim-
ilar behavior. Suppressing long sentences from the
corpus reduces the number of aligned sentences by
roughly 25% (see Table 1) but speeds the whole
training procedure by a factor of 3. The impact on
performance is discussed in the next section.
 0
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 0  10  20  30  40  50  60  70  80  90  100
Histogram of Spanish sentences? lengths (training set)
Figure 2: Histogram of the sentence length (Spanish
part of the parallel corpus).
3.3 Language model training corpus
In the experiments reported below, a trigram word
language model is used during decoding. This
model is trained on the Spanish part of the parallel
corpus using only sentences shorter than 40 words
(total of 18.5M of language model training data).
Second pass language models were trained on all
available monolingual data (34.1M words).
3.4 Tools
POS tagging was performed with the TreeTagger
(Schmid, 1994). This software provides resources
for both of the considered languages and it is freely
available. TreeTagger is a Markovian tagger that
uses decision trees to estimate trigram transition
probabilities. The English version is trained on the
PENN treebank corpus3 and the Spanish version on
the CRATER corpus.4
Language models are built using the SRI-LM
toolkit (Stolcke, 2002). Modified Knesser-Ney dis-
counting was used for all models. In (Goodman,
2001), a systematic description and comparison of
the usual smoothing methods is reported. Modified
Knesser-Ney discounting appears to be the most ef-
ficient method.
4 Experiments and Results
Two baseline English-to-Spanish translation mod-
els were created with Moses. The first model was
trained on the whole parallel text ? note that sen-
tences with more than 100 words are excluded by
Giza++. The second model was trained on the cor-
pus using only sentences with at most 40 words. The
BLEU score on the development set using good gen-
eral purpose weights is 48.0 for the first model and
47.0 for the second. Because training on the whole
bi-text is much slower, we decided to perform our
experiments on the bi-texts restricted to the ?short?
sentences.
4.1 Language model generation
The reranking experiments presented below use the
following language models trained on the Spanish
part of the whole training corpus:
? word language models,
? POS language model,
? POS language model, with a stop list used to
remove the 100 most frequent words (POS-
stop100 LM),
? language model of enriched units.
3http://www.cis.upenn.edu/ treebank
4http://www.comp.lancs.ac.uk/linguistics/crater/corpus.html
68
English : you will be aware President that over the last few sessions in Strasbourg. ..
Baseline: usted sabe que el Presidente durante los u?ltimos sesiones en Estrasburgo ...
Enriched units: usted sabe que el Presidente en los u?ltimos per??odos de sesiones en Estrasburgo ...
English : ... in this house there might be some recognition ...
Baseline: ... en esta asamblea no puede ser un cierto reconocimiento ...
Enriched units: ... en esta asamblea existe un cierto reconocimiento ...
Figure 3: Comparative translations using the baseline word system and the enriched unit system.
For each of these four models, various orders
were tested (n = 3, 4, 5), but in this paper we only
report those orders that yielded the greatest improve-
ments. POS language models were obtained by first
extracting POS sequences from the previously POS-
tagged training corpus and then by estimating stan-
dard back-off language models.
As shown in Table 1, the vocabulary size of the
word language model is 104k for Spanish and 74k
for English. The number of POS is small: 69 for
Spanish and 59 for English. We emphasize that
the tagset provided by TreeTagger does include nei-
ther gender nor number distinction. The vocabulary
size of the enriched-unit language model is 115k for
Spanish and 77.6k for English. The syntactical am-
biguity of words is low: the mean ambiguity ratio is
1.14 for Spanish and 1.12 for English.
4.2 Reranking the word n-best lists
The results concerning reranking experiments of the
n-best lists provided by the translation model based
on words as units are summarized in Table 2. The
baseline result, with trigram word LM reranking,
gives a BLEU score of 47.0 (1rst row). From the
n-best lists provided by this translation model, we
compared reranking performances with different tar-
get language models. As observed in the literature,
an improvement can be obtained by reranking with
a 4-gram word language model (47.0 ? 47.5, 2d
row). By post-tagging this n-best list, a POS lan-
guage model reranking can be performed. However,
reranking with a 5-gram POS language model alone
does not give any improvement from the baseline
(BLEU score of 46.9, 3rd row). This result corre-
sponds to known work in the literature (Kirchhoff
and Yang, 2005; Hasan et al, 2006), when using
POS only as a post-processing step during rerank-
ing. As suggested in section 2.3, this lack of per-
formance can be due to the fact that the tagger is
not able to provide a usefull tagging of sentences
included in the n-best lists. This observation is
also available when reranking of the word n-best is
done with a language model based on enriched units
(BLEU score of 47.6, not reported in Table 2).
4.3 POS disambiguation and reranking
The results concerning reranking experiments of the
n-best lists provided by the translation model based
on enriched units are summarized in Table 3. Us-
ing a trigram language model of enriched transla-
tion units leads to a BLEU score of 47.4, a 0.4 in-
crease over the baseline presented in section 4.2.
Figure 3 shows comparative translation examples
from the baseline and the enriched translation sys-
tems. In the first example, the baseline system out-
puts ?durante los u?ltimos sesiones? where the en-
riched translation system produces ?en los u?ltimos
per??odos de sesiones?, a better translation that may
be attributed to the introduction of the masculine
word ?per??odos?, allowing the system to build a
syntactically correct sentence. In the second exam-
ple, the syntactical error ?no puede ser un cierto re-
conocimiento? produced by the baseline system in-
duces an incorrect meaning of the sentence, whereas
the enriched translation system hypothesis ?existe un
cierto reconocimiento? is both syntactically and se-
mantically correct.
Reranking the enriched n-best with POS language
models (either with or without a stop list) does not
seem to be efficient (0.3 BLEU increasing with the
POS-stop100 language model).
A better improvement is obtained when reranking
is performed with the 4-gram word language model.
This results in a BLEU score of 47.9, correspond-
ing to a 0.9 improvement over the word baseline. It
is interesting to observe that reranking a n-best list
69
Dev. Test
3g word LM baseline 47.0 46.0
4g word LM reranking 47.5 46.5
5g POS reranking 46.9 46.1
Table 2: BLEU scores using words as translation
units.
obtained with a translation model based on enriched
units with a word LM results in better performances
than a enriched units LM reranking of a n-best list
obtained with a translation model based on words.
The last two rows of Table 3 give results when
combining word and POS language models to rerank
the enriched n-best lists. In both cases, 10 features
are used for reranking (8 Moses features + word
language model probability + POS language model
probability). The best result is obtained by com-
bining the 5-gram word language model with the 5-
gram POS-stop100 language model. In that case,
the best BLEU score is observed (48.1), with a 2.3%
relative increase over the trigram word baseline.
4.4 Results on the test set
The results on the test set are given in the second
column of Tables 2 and 3. Although the enriched
translation system is only 0.1 BLEU over the base-
line system (46.0 ? 46.1) when using a trigram lan-
guage model, the best condition observed on the de-
velopment set (word and POS-stop100 LMs rerank-
ing) results in a 46.8 BLEU score, corresponding to
a 0.8 increasing.
It can be observed that rescoring with a 4-gram
word language model leads to same score resulting
in a 1.9% relative increase over the trigram word
baseline.
5 Conclusion and future work
Combining word language model reranking of n-
best lists based on syntactically enriched units seems
to produce more consistent hypotheses. Using en-
riched translation units results in a relative 2.3%
improvement in BLEU on the development set and
1.9% on the test over the trigram baseline. Over a
standard translation model with 4-gram rescoring,
the enriched unit translation model leads to an abso-
lute increase in BLEU score of 0.4 both on the devel-
opment and the test sets. These first results are en-
Dev. Test
3g enriched units LM baseline 47.4 46.1
4g enriched units LM reranking 47.8 46.8
4g word LM reranking 47.9 46.9
5g POS LM reranking 47.5 46.2
5g POS-stop100 LM reranking 47.7 46.3
word + POS LMs reranking 47.9 46.9
word + POS-stop100 LMs rerank. 48.1 46.8
Table 3: BLEU scores using enriched translation
units.
couraging enough to further investigate the integra-
tion of syntactic information in the translation model
itself, rather than to restrict it to the post-processing
pass. As follow-up experiments, it is planned to in-
clude gender and number information in the tagset,
as well as the word stems to the enriched units.
This work should be considered as preliminary
experiments for the investigation of factored trans-
lation models, which Moses is able to handle. POS
factorization is indeed a way to add some general-
ization capability to the enriched translation models.
6 Acknowledgments
This work has been partially funded by the European
Union under the integrated project TC-STAR (IST-
2002-FP6-506738), and by the French Government
under the project INSTAR (ANR JCJC06 143038).
We would like to thanks Marc Ferras for his help
concerning the Spanish language.
References
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of powell?s
UOBYQA algorithm: Experimental results and com-
parison with the DFO algorithm. Journal of Computa-
tional and Applied Mathematics, 181:157?175.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proceedings of MT Summit IX.
C. Gollan, M. Bisani, S. Kanthak, R. Schlueter, and ?H.
Ney. 2005. Cross domain automatic transcription on
70
the TC-STAR epps corpus. In Proceedings of ICASSP
2005.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15(4):403?434, October.
S. Hasan, O. Bender, and H. Ney. 2006. Reranking trans-
lation hypothesis using structural properties. In Pro-
ceedings of EACL 2006.
Y.S. Hwang, A. Finch, and Y. Sasaki. 2007. Improving
statistical machine translation using shallow linguistic
knoledge. to be published in Computer, Speech and
Language.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation. In
Proceedings of ACL ?05 workshop on Building and Us-
ing Parallel Text, pages 125?128.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
2003 (HLT-NAACL 2003), Edmonton, Canada, May.
Franz Josef Och and Hermann Ney. 2000a. Improved
statistical alignment models. In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pages 440?447, Hongkong, China, Octo-
ber.
Franz Josef Och and Hermann Ney. 2000b. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics, pages 440?447, Hong Kong, China,
October.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In NAACL,
pages 161?168.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, University of Pennsylva-
nia.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, September.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP, pages II:
901?904.
71
Proceedings of the Third Workshop on Statistical Machine Translation, pages 107?110,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?08
Daniel D?chelotte, Gilles Adda, Alexandre Allauzen, H?l?ne Bonneau-Maynard,
Olivier Galibert, Jean-Luc Gauvain, Philippe Langlais? and Fran?ois Yvon
LIMSI/CNRS
firstname.lastname@limsi.fr
Abstract
This paper describes our statistical machine
translation systems based on the Moses toolkit
for the WMT08 shared task. We address the
Europarl and News conditions for the follow-
ing language pairs: English with French, Ger-
man and Spanish. For Europarl, n-best rescor-
ing is performed using an enhanced n-gram
or a neuronal language model; for the News
condition, language models incorporate extra
training data. We also report unconvincing re-
sults of experiments with factored models.
1 Introduction
This paper describes our statistical machine trans-
lation systems based on the Moses toolkit for the
WMT 08 shared task. We address the Europarl and
News conditions for the following language pairs:
English with French, German and Spanish. For Eu-
roparl, n-best rescoring is performed using an en-
hanced n-gram or a neuronal language model, and
for the News condition, language models are trained
with extra training data. We also report unconvinc-
ing results of experiments with factored models.
2 Base System architecture
LIMSI took part in the evaluations on Europarl data
and on News data, translating French, German and
Spanish from and to English, amounting a total
of twelve evaluation conditions. Figure 1 presents
the generic overall architecture of LIMSI?s transla-
tion systems. They are fairly standard phrase-based
?Univ. Montr?al, felipe@iro.umontreal.ca
Other
Targettext
Targettext
MosestextSource or
Translation model 4g language model 4g language model
and extractionRescoring
$n$?besttranslations
LM InterpolationPhrase pairextraction Neural network
or
or
+ News Co.EuroparlEuroparl EuroparlNews Co.sources
Figure 1: Generic architecture of LIMSI?s SMT systems.
Depending on the condition, the decoder generates ei-
ther the final output or n-best lists. In the latter case,
the rescoring incorporates the same translation features,
except for a better target language model (see text).
translation systems (Och and Ney, 2004; Koehn et
al., 2003) and use Moses (Koehn et al, 2007) to
search for the best target sentence. The search uses
the following models: a phrase table, providing 4
scores and a phrase penalty, a lexicalized reordering
model (7 scores), a language model score and a word
penalty. These fourteen scores are weighted and lin-
early combined (Och and Ney, 2002; Och, 2003);
their respective weights are learned on development
data so as to maximize the BLEU score. In the fol-
lowing, we detail several aspects of our systems.
2.1 Translation models
The translation models deployed in our systems for
the europarl condition were trained on the provided
Europarl parallel data only. For the news condition,
they were trained on the Europarl data merged with
107
the news-commentary parallel data, as depicted on
Figure 1. This setup was found to be more favor-
able than training on Europarl data only (for obvious
mismatching domain reasons) and than training on
news-commentary data only, most probably because
of a lack of coverage. Another, alternative way of
benefitting from the coverage of the Europarl corpus
and the relevance of the news-commentary corpus
is to use two phrase-tables in parallel, an interest-
ing feature of Moses. (Koehn and Schroeder, 2007)
found that this was the best way to ?adapt? a transla-
tion system to the news-commentary task. These re-
sults are corroborated in (D?chelotte, 2007)1 , which
adapts a ?European Parliament? system using a ?Eu-
ropean and Spanish Parliaments? development set.
However, we were not able to reproduce those find-
ings for this evaluation. This might be caused by the
increase of the number of feature functions, from 14
to 26, due to the duplication of the phrase table and
the lexicalized reordering model.
2.2 Language Models
2.2.1 Europarl language models
The training of Europarl language models (LMs)
was rather conventional: for all languages used in
our systems, we used a 4-gram LM based on the
entire Europarl vocabulary and trained only on the
available Europarl training data. For French, for
instance, this yielded a model with a 0.2 out-of-
vocabulary (OOV) rate on our LM development set,
and a perplexity of 44.9 on the development data.
For French also, a more accurate n-gram LM was
used to rescore the first pass translation; this larger
model includes both Europarl and giga word corpus
of newswire text, lowering the perplexity to 41.9 on
the development data.
2.2.2 News language models
For this condition, we took advantage of the a
priori information that the test text would be of
newspaper/newswire genre and from the November-
december 2007 period. We consequently built much
larger LMs for translating both to French and to En-
glish, and optimized their combination on appropri-
1(D?chelotte, 2007) further found that giving an increased
weight to the small in-domain data could out-perform the setup
with two phrase-tables in parallel. We haven?t evaluated this
idea for this evaluation.
ate source of data. For French, we interpolated five
different LMs trained on corpus containing respec-
tively newspapers, newswire, news commentary and
Europarl data, and tuned their combination with text
downloaded from the Internet. Our best LM had an
OOV rate of about 2.1% and a perplexity of 111.26
on the testset. English LMs were built in a similar
manner, our largest model combining 4 LMs from
various sources, which, altogether, represent about
850M words. Its perplexity on the 2008 test set was
approximately 160, with an OOV rate of 2.7%.
2.2.3 Neural network language models
Neural-Network (NN) based continuous space
LMs similar to the ones in (Schwenk, 2007) were
also trained on Europarl data. These networks com-
pute the probabilities of all the words in a 8192 word
output vocabulary given a context in a larger, 65000-
word vocabulary. Each word in the context is first
associated with a numerical vector of dimension 500
by the input layer. The activity of the 500 neurons in
the hidden layer is computed as the hyperbolic tan-
gent of the weighted sum of these vectors, projecting
the context into a [?1, 1] hypercube of dimension
500. Final projection on a set of 8192 output neurons
yields the final probabilities through a softmax-ed,
weighted sum of the coordinates in the hypercube.
The final NN-based model is interpolated with the
main LM model in a 0.4-0.6 ratio, and yields a per-
plexity reduction of 9% relative with respect to the
n-gram LM on development data.
2.3 Tuning procedure
We use MERT, distributed with the Moses decoder,
to tune the first pass of the system. The weights
were adjusted to maximize BLEU on the develop-
ment data. For the baseline system, a dozen Moses
runs are necessary for each MERT optimization, and
several optimization runs were started and compared
during the system?s development. Tuning was per-
formed using dev2006 for the Europarl task and on
News commentary dev2007 for the news task.
2.4 Rescoring and post processing
For the Europarl condition, distinct 100 best trans-
lations from Moses were rescored with improved
LMs: when translating to French, we used the
French model described in section 2.2.1; when
108
Es-En En-Es Fr-En En-Fr
baseline 32.21 31.62 32.41 29.31
Limsi 32.49 31.23 32.62 30.27
Table 1: Comparison of two tokenization policies
All results on Europarl test2007
CI system CS system
En?Fr 27.23 27.55
Fr?En 30.96 30.98
Table 2: Effect of training on true case texts, for English
to French (case INsensitive BLEU scores, untuned sys-
tems, results on test2006 dataset)
translating to English, we used the neuronal LM de-
scribed in section 2.2.3.
For all the ?lowcase? systems (see below), recase-
ing was finally performed using our own recaseing
tool. Case is restored by creating a word graph al-
lowing all possible forms of caseing for each word
and each component of a compound word. This
word graph is then decoded using a cased 4-gram
LM to obtain the most likely form. In a final step,
OOV words (with respect to the source language
word list) are recased to match their original form.
3 Experiments with the base system
3.1 Word tokenization and case
We developed our own tokenizer for English, French
and Spanish, and used the baseline tokenizer for
German. Experiments on the 2007 test dataset for
Europarl task show the impact of the tokenization
on the BLEU scores, with 3-gram LMs. Results are
always improved with our own tokenizer, except for
English to Spanish (Table 1).
Our systems were initially trained on lowercase
texts, similarly to the proposed baseline system.
However, training on true case texts proved bene-
ficial when translating from English to French, even
when scoring in a case insensitive manner. Table 2
shows an approximate gain of 0.3 BLEU for that di-
rection, and no impact on French to English perfor-
mance. Our English-French systems are therefore
case sensitive.
3.2 Language Models
For Europarl, we experimented with LMs of increas-
ing orders: we found that using a 5-gram LM only
yields an insignificant improvement over a 4-gram
LM. As a result, we used 4-gram LMs for all our
first pass decodings. For the second pass, the use
of the Neural Network LMs, if used with an appro-
priate (tuned) weight, yields a small, yet consistent
improvement of BLEU for all pairs.
Performance on the news task are harder to ana-
lyze, due to the lack of development data. Throwing
in large set of in-domain data was obviously helpful,
even though we are currently unable to adequately
measure this effect.
4 Experiments with factored models
Even though these models were not used in our sub-
missions, we feel it useful to comment here our (neg-
ative) experiments with factored models.
4.1 Overview
In this work, factored models (Koehn and Hoang,
2007) are experimented with three factors : the sur-
face form, the lemma and the part of speech (POS).
The translation process is composed of different
mapping steps, which either translate input factors
into output factors, or generate additional output fac-
tors from existing output factors. In this work, four
mapping steps are used with two decoding paths.
The first path corresponds to the standard and di-
rect mapping of surface forms. The second decod-
ing path consists in two translation steps for respec-
tively POS tag and the lemmas, followed by a gener-
ation step which produces the surface form given the
POS-lemma couple. The system also includes three
reordering models.
4.2 Training
Factored models have been built to translate from
English to French for the news task. To estimate the
phrase and generation tables, the training texts are
first processed in order to compute the lemmas and
POS information. The English texts are tagged and
lemmatized using the English version of the Tree-
tagger2. For French, POS-tagging is carried out
with a French version of the Brill?s tagger trained
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
109
on the MULTITAG corpus (Allauzen and Bonneau-
Maynard, 2008). Lemmatization is performed with
a French version of the Treetagger.
Three phrase tables are estimated with the Moses
utilities, one per factor. For the surface forms, the
parallel corpus is the concatenation of the official
training data for the tasks Europarl and News com-
mentary, whereas only the parallel data of news
commentary are used for lemmas and POS. For the
generation step, the table built on the parallel texts of
news commentary is augmented with a French dic-
tionary of 280 000 forms. The LM is the largest LM
available for French (see section 2.2.2).
4.3 Results and lessons learned
On the news test set of 2008, this system obtains a
BLEU score of 20.2, which is worse than our ?stan-
dard? system (20.9). A similar experiment on the
Europarl task proved equally unsuccessful.
Using only models which ignore the surface form
of input words yields a poor system. Therefore, in-
cluding a model based on surface forms, as sug-
gested (Koehn and Hoang, 2007), is also neces-
sary. This indeed improved (+1.6 BLEU for Eu-
roparl) over using one single decoding path, but not
enough to match our baseline system performance.
These results may be explained by the use of auto-
matic tools (POS tagger and lemmatizer) that are not
entirely error free, and also, to a lesser extend, by the
noise in the test data. We also think that more effort
has to be put into the generation step.
Tuning is also a major issue for factored trans-
lation models. Dealing with 38 weights is an op-
timization challenge, which took MERT 129 itera-
tions to converge. The necessary tradeoff between
the huge memory requirements of these techniques
and computation time is also detrimental to their use.
Although quantitative results were unsatisfactory,
it is finally worth mentioning that a manual exami-
nation of the output revealed that the explicit usage
of gender and number in our models (via POS tags)
may actually be helpful when translating to French.
5 Conclusion
In this paper, we presented our statistical MT sys-
tems developed for the WMT 08 shared task. As ex-
pected, regarding the Europarl condition, our BLEU
improvements over the best 2007 results are limited:
paying attention to tokenization and caseing issues
brought us a small pay-off; rescoring with better
language models gave also some reward. The news
condition was new, and more challenging: our satis-
factory results can be attributed to the use of large,
well tuned, language models. In comparison, our ex-
periments with factored models proved disappoint-
ing, for reasons that remain to be clarified. On a
more general note, we feel that the performance of
MT systems for these tasks are somewhat shadowed
by normalization issues (tokenization errors, incon-
sistent use of caseing, typos, etc), making it difficult
to clearly analyze our systems? performance.
References
A. Allauzen and H. Bonneau-Maynard. 2008. Training
and evaluation of POS taggers on the French multitag
corpus. In Proc. LREC?08, To appear.
D. D?chelotte. 2007. Traduction automatique de la pa-
role par m?thodes statistiques. Ph.D. thesis, Univ.
Paris XI, December.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proc. EMNLP-CoNLL, pages 868?876.
P. Koehn and J. Schroeder. 2007. Experiments in domain
adaptation for statistical machine translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 224?227, Prague, Czech Republic.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, pages
127?133, Edmonton, Canada, May.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL, demonstration
session, Prague, Czech Republic.
F.J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. ACL, pages 295?302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, Sapporo, Japan.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492?518.
110
Two Ways to Use a Noisy Parallel News corpus for improving Statistical
Machine Translation
Souhir Gahbiche-Braham He?le`ne Bonneau-Maynard
Universite? Paris-Sud 11
LIMSI-CNRS
91403 Orsay, France
{souhir,hbm,yvon}@limsi.fr
Franc?ois Yvon
Abstract
In this paper, we present two methods to use
a noisy parallel news corpus to improve sta-
tistical machine translation (SMT) systems.
Taking full advantage of the characteristics of
our corpus and of existing resources, we use
a bootstrapping strategy, whereby an existing
SMT engine is used both to detect parallel sen-
tences in comparable data and to provide an
adaptation corpus for translation models. MT
experiments demonstrate the benefits of vari-
ous combinations of these strategies.
1 Introduction
In Statistical Machine Translation (SMT), systems
are created from parallel corpora consisting of a set
of source language texts aligned with its translation
in the target language. Such corpora however only
exist (at least are publicly documented and avail-
able) for a limited number of domains, genres, reg-
isters, and language pairs. In fact, there are a few
language pairs for which parallel corpora can be ac-
cessed, except for very narrow domains such as po-
litical debates or international regulatory texts. An-
other very valuable resource for SMT studies, espe-
cially for under-resource languages, are comparable
corpora, made of pairs of monolingual corpora that
contain texts of similar genres, from similar periods,
and/or about similar topics.
The potential of comparable corpora has long
been established as a useful source from which to
extract bilingual word dictionaries (see eg. (Rapp,
1995; Fung and Yee, 1998)) or to learn multilingual
terms (see e.g. (Lange?, 1995; Smadja et al, 1996)).
More recently, the relative corpus has caused the
usefulness of comparable corpora be reevaluated as
a potential source of parallel fragments, be they
paragraphs, sentences, phrases, terms, chunks, or
isolated words. This tendency is illustrated by the
work of e.g. (Resnik and Smith, 2003; Munteanu
and Marcu, 2005), which combines Information Re-
trieval techniques (to identify parallel documents)
and sentence similarity detection to detect parallel
sentences.
There are many other ways to improve SMT mod-
els with comparable or monolingual data. For in-
stance, the work reported in (Schwenk, 2008) draws
inspiration from recent advances in unsupervised
training of acoustic models for speech recognition
and proposes to use self-training on in-domain data
to adapt and improve a baseline system trained
mostly with out-of-domain data.
As discussed e.g. in (Fung and Cheung, 2004),
comparable corpora are of various nature: there ex-
ists a continuum between truly parallel and com-
pletely unrelated texts. Algorithms for exploiting
comparable corpora should thus be tailored to the
peculiarities of the data on which they are applied.
In this paper, we report on experiments aimed at
using a noisy parallel corpus made out of news sto-
ries in French and Arabic in two different ways: first,
to extract new, in-domain, parallel sentences; sec-
ond, to adapt our translation and language models.
This approach is made possible due to the specifici-
ties of our corpus. In fact, our work is part of a
project aiming at developing a platform for process-
ing multimedia news documents (texts, interviews,
images and videos) in Arabic, so as to streamline the
44
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 44?51,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
work of a major international news agency. As part
as the standard daily work flow, a significant por-
tion of the French news are translated (or adapted)
in Arabic by journalists. Having access to one full
year of the French and Arabic corpus (consisting, to
date, of approximately one million stories (150 mil-
lion words)), we have in our hands an ideal compa-
rable resource to perform large scale experiments.
These experiments aim at comparing various
ways to build an accurate machine translation sys-
tem for the news domain using (i) a baseline system
trained mostly with out-of-domain data (ii) the com-
parable dataset. As will be discussed, given the very
large number of parallel news in the data, our best
option seems to reconstruct an in-domain training
corpus of automatically detected parallel sentences.
The rest of this paper is organized as follows.
In Section 2, we relate our work to some exist-
ing approaches for using comparable corpora. Sec-
tion 3 presents our methodology for extracting par-
allel sentences, while our phrase-table adaptation
strategies are described in Section 4. In Section 5,
we describe our experiments and contrast the results
obtained with several adaptation strategies. Finally,
Section 6 concludes the paper.
2 Related work
From a bird?s eye view, attempts to use comparable
corpora in SMT fall into two main categories: first,
approaches aimed at extracting parallel fragments;
second, approaches aimed at adapting existing re-
sources to a new domain.
2.1 Extracting parallel fragments
Most attempts at automatically extracting parallel
fragments use a two step process (see (Tillmann and
Xu, 2009) for a counter-example): a set of candidate
parallel texts is first identified; within this short list
of possibly paired texts, parallel sentences are then
identified based on some similarity score.
The work reported in (Zhao and Vogel, 2002) con-
centrates on finding parallel sentences in a set of
comparable stories pairs in Chinese/English. Sen-
tence similarity derives from a probabilistic align-
ment model for documents, which enables to recog-
nize parallel sentences based on their length ratio,
as well as on the IBM 1 model score of their word-
to-word alignment. To account for various levels of
parallelism, the model allows some sentences in the
source or target language to remain unaligned.
The work of (Resnik and Smith, 2003) considers
mining a much larger ?corpora? consisting of docu-
ments collected on the Internet. Matched documents
and sentences are primarily detected based on sur-
face and/or formal similarity of the web addresses
or of the page internal structure.
This line of work is developed notably in
(Munteanu and Marcu, 2005): candidate parallel
texts are found using Cross-Lingual Information Re-
trieval (CLIR) techniques; sentence similarity is in-
directly computed using a logistic regression model
aimed at detecting parallel sentences. This formal-
ism allows to enrich baseline features such as the
length ratio, the word-to-word (IBM 1) alignment
scores with supplementary scores aimed at reward-
ing sentences containing identical words, etc. More
recently, (Smith et al, 2010) reported significant im-
provements mining parallel Wikipedia articles us-
ing more sophisticated indicators of sentence par-
allelism, incorporating a richer set of features and
cross-sentence dependencies within a Conditional
Random Fields (CRFs) model. For lack of find
enough parallel sentences, (Munteanu and Marcu,
2006; Kumano and Tokunaga, 2007) consider the
more difficult issue of mining parallel phrases.
In (Abdul-Rauf and Schwenk, 2009), the authors,
rather than computing a similarity score between a
source and a target sentence, propose to use an ex-
isting translation engine to process the source side
of the corpus, thus enabling sentence comparison to
be performed in the target language, using the edit
distance or variants thereof (WER or TER). This
approach is generalized to much larger collections
in (Uszkoreit et al, 2010), which draw advantage
of working in one language to adopt efficient paral-
lelism detection techniques (Broder, 2000).
2.2 Comparable corpora for adaptation
Another very productive use of comparable corpora
is to adapt or specialize existing resources (dictio-
naries, translation models, language models) to spe-
cific domains and/or genres. We will only focus here
on adapting the translation model; a review of the
literature on language model adaptation is in (Bella-
garda, 2001) and the references cited therein.
45
Figure 1: Extraction of parallel corpora
The work in (Snover et al, 2008) is a first step
towards augmenting the translation model with new
translation rules: these rules associate, with a tiny
probability, every phrase in a source document with
the most frequent target phrases found in a compa-
rable corpus specifically built for this document.
The study in (Schwenk, 2008) considers self-
training, which allows to adapt an existing system
to new domains using monolingual (source) data.
The idea is to automatically translate the source side
of an in-domain corpus using a reference translation
system. Then, according to some confidence score,
the best translations are selected to form an adap-
tation corpus, which can serve to retrain the trans-
lation model. The authors of (Cettolo et al, 2010)
follow similar goals with different means: here, the
baseline translation model is used to obtain a phrase
alignment between source and target sentences in
a comparable corpus. These phrase alignments are
further refined, before new phrases not in the origi-
nal phrase-table, can be collected.
The approaches developed below borrow from
both traditions: given (i) the supposed high degree
of parallelism in our data and (ii) the size of the
available comparable data, we are in a position to
apply any of the above described technique. This
is all the easier to do as all stories are timestamped,
which enables to easily spot candidate parallel texts.
In both cases, we will apply a bootstrapping strat-
egy using as baseline a system trained with out-of-
domain data.
3 Extracting Parallel Corpora
This section presents our approach for extracting a
parallel corpus from a comparable in-domain cor-
pora so as to adapt a SMT system to a specific do-
main. Our methodology assumes that both a base-
line out-of-domain translation system and a compa-
rable in-domain corpus are available, two require-
ments that are often met in practice.
As shown in Figure 1, our approach for extracting
an in-domain parallel corpus from the in-domain
comparable corpus consists in 3 steps and closely
follows (Abdul-Rauf and Schwenk, 2009):
translation: translating the source side of the
comparable corpora;
document pairs selection : selecting, in the com-
parable corpus, documents that are similar to the
translated output;
sentence pairs selection : selecting parallel sen-
tences among the selected documents.
The main intuition is that computing document
similarities in one language enables to use simple
and effective comparison procedures, instead of hav-
ing to define ad hoc similarities measures based on
complex underlying alignment models.
The translation step consists here in translating
the source (Arabic) side of the comparable corpus
using a baseline out-of-domain system, which has
been trained on parallel out-of-domain data.
The document selection step consists in trying
to match the automatic translations (source:target)
with the original documents in the target language.
For each (source:target) document, a similarity score
with all the target documents is computed. We con-
tend here with a simple association score, namely
the Dice coefficient, computed as the number of
words in common in both documents, normalized by
the length of the (source:target) document.
A priori knowledge, such as the publication dates
46
of the documents, are used to limit the number of
document pairs to be compared. For each source
document, the target document that has the best
score is then selected as a potential parallel docu-
ment. The resulting pairs of documents are then fil-
tered depending on a threshold Td, so as to avoid
false matches (in the experiments described below,
the threshold has been set so as to favor precision
over recall).
At the end of this step, a set of similar source
and target document pairs has been selected. These
pairs may consist in documents that are exact trans-
lations of each other. In most cases, the documents
are noisy translation and only a subset of their sen-
tences are mutual translation.
The sentence selection step then consists in per-
forming a sentence level alignment of each pair of
documents to select a set of parallel sentences. Sen-
tence alignment is then performed with the hunalign
sentence alignment tool (Varga et al, 2005), which
also provides alignment confidence measures. As
for the document selection step, only sentence pairs
that obtain an alignment score greater than a prede-
fined threshold Ts are selected, where Ts is again
chosen to favor prevision of alignments of recall.
From these, 1 : 1 alignments are retained, yielding
a small, adapted, parallel corpus. This method is
quite different from (Munteanu and Marcu, 2005)?s
work where the sentence selection step is done by a
Maximum Entropy classifier.
4 Domain Adaptation
In the course of mining our comparable corpus, we
have produced a translation into French for all the
source language news stories. This means that we
have three parallel corpora at our disposal:
? The baseline training corpus, which is large
(a hundred million words), delivering a reason-
able translation performance quality of transla-
tion, but out-of-domain;
? The extracted in-domain corpus, which is
much smaller, and potentially noisy;
? The translated in-domain corpus, which is of
medium-size, and much worse in quality than
the others.
Considering these three corpora, different adapta-
tion methods of the translation models are explored.
The first approach is to concatenate the baseline and
in-domain training data (either extracted or trans-
lated) to train a new translation model. Given the
difference in size between the two corpus, this ap-
proach may introduce a bias in the translation model
in favor of out-of-domain.
The second approach is to train separate transla-
tion models with baseline on the one hand, and with
in-domain on the other data and to weight their com-
bination with MERT (Och, 2003). This alleviates
the former problem but increases the number of fea-
tures that need to be trained, running the risk to make
MERT less stable.
A last approach is also considered, which consists
in using only the in-domain data to train the trans-
lation model. In that case, the question is the small
size of the in-domain data.
The comparative experiments on the three ap-
proaches, using the three corpora are described in
next section.
5 Experiments and results
5.1 Context and data
The experiments have been carried out in the con-
text of the Cap Digital SAMAR1 project which aims
at developping a platform for processing multimedia
news in Arabic. Every day, about 250 news in Ara-
bic, 800 in French and in English2 are produced and
accumulated on our disks. News collected from De-
cember 2009 to December 2010 constitute the com-
parable corpora, containing a set of 75,975 news for
the Arabic part and 288,934 news for the French part
(about 1M sentences for Arabic and 5M sentences
for French).
The specificity of this comparable corpus is that
many Arabic stories are known to be translation of
news that were first written in French. The transla-
tions may not be entirely faithful: when translating
a story, the journalist is in fact free to rearrange the
structure, and to some extend, the content of a doc-
ument (see example Figure 2).
In our experiments, the in-domain comparable
corpus then consists in a set of Arabic and French
1http://www.samar.fr
2The English news have not been used in this study.
47
Arabic:
	
?A
	
J

J

?@ 	?? A
	
JK
Y? ?
	
KA? B ?A?g ?


	
? 	?mProceedings of the 6th Workshop on Statistical Machine Translation, pages 309?315,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIMSI @ WMT11
Alexandre Allauzen
He?le`ne Bonneau-Maynard
Hai-Son Le
Aure?lien Max
Guillaume Wisniewski
Franc?ois Yvon
Univ. Paris-Sud and LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Gilles Adda
Josep M. Crego
Adrien Lardilleux
Thomas Lavergne
Artem Sokolov
LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Abstract
This paper describes LIMSI?s submissions to
the Sixth Workshop on Statistical Machine
Translation. We report results for the French-
English and German-English shared transla-
tion tasks in both directions. Our systems
use n-code, an open source Statistical Ma-
chine Translation system based on bilingual
n-grams. For the French-English task, we fo-
cussed on finding efficient ways to take ad-
vantage of the large and heterogeneous train-
ing parallel data. In particular, using a sim-
ple filtering strategy helped to improve both
processing time and translation quality. To
translate from English to French and Ger-
man, we also investigated the use of the
SOUL language model in Machine Trans-
lation and showed significant improvements
with a 10-gram SOUL model. We also briefly
report experiments with several alternatives to
the standard n-best MERT procedure, leading
to a significant speed-up.
1 Introduction
This paper describes LIMSI?s submissions to the
Sixth Workshop on Statistical Machine Translation,
where LIMSI participated in the French-English and
German-English tasks in both directions. For this
evaluation, we used n-code, our in-house Statistical
Machine Translation (SMT) system which is open-
source and based on bilingual n-grams.
This paper is organized as follows. Section 2 pro-
vides an overview of n-code, while the data pre-
processing and filtering steps are described in Sec-
tion 3. Given the large amount of parallel data avail-
able, we proposed a method to filter the French-
English GigaWord corpus (Section 3.2). As in our
previous participations, data cleaning and filtering
constitute a non-negligible part of our work. This
includes detecting and discarding sentences in other
languages; removing sentences which are also in-
cluded in the provided development sets, as well as
parts that are repeated (for the monolingual news
data, this can reduce the amount of data by a fac-
tor 3 or 4, depending on the language and the year);
normalizing the character set (non-utf8 characters
which are aberrant in context, or in the case of the
GigaWord corpus, a lot of non-printable and thus in-
visible control characters such as EOT (end of trans-
mission)1).
For target language modeling (Section 4), a stan-
dard back-off n-gram model is estimated and tuned
as described in Section 4.1. Moreover, we also in-
troduce in Section 4.2 the use of the SOUL lan-
guage model (LM) (Le et al, 2011) in SMT. Based
on neural networks, the SOUL LM can handle an
arbitrary large vocabulary and a high order marko-
vian assumption (up to 10-gram in this work). Fi-
nally, experimental results are reported in Section 5
both in terms of BLEU scores and translation edit
rates (TER) measured on the provided newstest2010
dataset.
2 System Overview
Our in-house n-code SMT system implements the
bilingual n-gram approach to Statistical Machine
Translation (Casacuberta and Vidal, 2004). Given a
1This kind of characters was used for Teletype up to the sev-
enties or early eighties.
309
source sentence sJ1, a translation hypothesis t?
I
1 is de-
fined as the sentence which maximizes a linear com-
bination of feature functions:
t?I1 = argmax
tI1
{
M
?
m=1
?mhm(sJ1, tI1)
}
(1)
where sJ1 and t
I
1 respectively denote the source and
the target sentences, and ?m is the weight associated
with the feature function hm. The translation fea-
ture is the log-score of the translation model based
on bilingual units called tuples. The probability as-
signed to a sentence pair by the translation model is
estimated by using the n-gram assumption:
p(sJ1, t
I
1) =
K
?
k=1
p((s, t)k|(s, t)k?1 . . .(s, t)k?n+1)
where s refers to a source symbol (t for target) and
(s, t)k to the kth tuple of the given bilingual sentence
pair. It is worth noticing that, since both languages
are linked up in tuples, the context information pro-
vided by this translation model is bilingual. In ad-
dition to the translation model, eleven feature func-
tions are combined: a target-language model (see
Section 4 for details); four lexicon models; two lex-
icalized reordering models (Tillmann, 2004) aim-
ing at predicting the orientation of the next transla-
tion unit; a ?weak? distance-based distortion model;
and finally a word-bonus model and a tuple-bonus
model which compensate for the system preference
for short translations. The four lexicon models are
similar to the ones used in a standard phrase-based
system: two scores correspond to the relative fre-
quencies of the tuples and two lexical weights are
estimated from the automatically generated word
alignments. The weights associated to feature func-
tions are optimally combined using a discriminative
training framework (Och, 2003) (Minimum Error
Rate Training (MERT), see details in Section 5.4),
using the provided newstest2009 data as develop-
ment set.
2.1 Training
Our translation model is estimated over a training
corpus composed of tuple sequences using classi-
cal smoothing techniques. Tuples are extracted from
a word-aligned corpus (using MGIZA++2 with de-
fault settings) in such a way that a unique segmenta-
tion of the bilingual corpus is achieved, allowing to
estimate the n-gram model. Figure 1 presents a sim-
ple example illustrating the unique tuple segmenta-
tion for a given word-aligned pair of sentences (top).
Figure 1: Tuple extraction from a sentence pair.
The resulting sequence of tuples (1) is further re-
fined to avoid NULL words in the source side of the
tuples (2). Once the whole bilingual training data is
segmented into tuples, n-gram language model prob-
abilities can be estimated. In this example, note that
the English source words perfect and translations
have been reordered in the final tuple segmentation,
while the French target words are kept in their orig-
inal order.
2.2 Inference
During decoding, source sentences are encoded
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, at decoding time,
only those encoded reordering hypotheses are trans-
lated. Reordering hypotheses are introduced using
a set of reordering rules automatically learned from
the word alignments.
In the previous example, the rule [perfect transla-
tions ; translations perfect] produces the swap of
the English words that is observed for the French
and English pair. Typically, part-of-speech (POS)
information is used to increase the generalization
power of such rules. Hence, rewriting rules are built
using POS rather than surface word forms. Refer
2http://geek.kyloo.net/software
310
to (Crego and Marin?o, 2007) for details on tuple ex-
traction and reordering rules.
3 Data Pre-processing and Selection
We used all the available parallel data allowed in
the constrained task to compute the word align-
ments, except for the French-English tasks where
the United Nation corpus was not used to train our
translation models. To train the target language
models, we also used all provided data and mono-
lingual corpora released by the LDC for French
and English. Moreover, all parallel corpora were
POS-tagged with the TreeTagger (Schmid, 1994).
For German, the fine-grained POS information used
for pre-processing was computed by the RFTag-
ger (Schmid and Laws, 2008).
3.1 Tokenization
We took advantage of our in-house text process-
ing tools for the tokenization and detokenization
steps (De?chelotte et al, 2008). Previous experi-
ments have demonstrated that better normalization
tools provide better BLEU scores (Papineni et al,
2002). Thus all systems are built in ?true-case.?
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which poses a number of diffi-
culties both at training and decoding time. Thus,
to translate from German to English, the German
side was normalized using a specific pre-processing
scheme (described in (Allauzen et al, 2010)), which
aims at reducing the lexical redundancy and splitting
complex compounds.
Using the same pre-processing scheme to trans-
late from English to German would require to post-
process the output to undo the pre-processing. As in
our last year?s experiments (Allauzen et al, 2010),
this pre-processing step could be achieved with a
two-step decoding. However, by stacking two de-
coding steps, we may stack errors as well. Thus, for
this direction, we used the German tokenizer pro-
vided by the organizers.
3.2 Filtering the GigaWord Corpus
The available parallel data for English-French in-
cludes a large Web corpus, referred to as the Giga-
Word parallel corpus. This corpus is very noisy, and
contains large portions that are not useful for trans-
lating news text. The first filter aimed at detecting
foreign languages based on perplexity and lexical
coverage. Then, to select a subset of parallel sen-
tences, trigram LMs were trained for both French
and English languages on a subset of the available
News data: the French (resp. English) LM was used
to rank the French (resp. English) side of the cor-
pus, and only those sentences with perplexity above
a given threshold were selected. Finally, the two se-
lected sets were intersected. In the following exper-
iments, the threshold was set to the median or upper
quartile value of the perplexity. Therefore, half (or
75%) of this corpus was discarded.
4 Target Language Modeling
Neural networks, working on top of conventional
n-gram models, have been introduced in (Bengio
et al, 2003; Schwenk, 2007) as a potential means
to improve conventional n-gram language models
(LMs). However, probably the major bottleneck
with standard NNLMs is the computation of poste-
rior probabilities in the output layer. This layer must
contain one unit for each vocabulary word. Such a
design makes handling of large vocabularies, con-
sisting of hundreds thousand words, infeasible due
to a prohibitive growth in computation time. While
recent work proposed to estimate the n-gram dis-
tributions only for the most frequent words (short-
list) (Schwenk, 2007), we explored the use of the
SOUL (Structured OUtput Layer Neural Network)
language model for SMT in order to handle vocabu-
laries of arbitrary sizes.
Moreover, in our setting, increasing the order of
standard n-gram LM did not show any significant
improvement. This is mainly due to the data spar-
sity issue and to the drastic increase in the number of
parameters that need to be estimated. With NNLM
however, the increase in context length at the input
layer results in only a linear growth in complexity
in the worst case (Schwenk, 2007). Thus, training
longer-context neural network models is still feasi-
ble, and was found to be very effective in our system.
311
4.1 Standard n-gram Back-off Language
Models
To train our language models, we assumed that the
test set consisted in a selection of news texts dat-
ing from the end of 2010 to the beginning of 2011.
This assumption was based on what was done for
the 2010 evaluation. Thus, for each language, we
built a development corpus in order to optimize the
vocabulary and the target language model.
Development set and vocabulary In order to
cover different periods, two development sets were
used. The first one is newstest2008. This corpus is
two years older than the targeted time period; there-
fore, a second development corpus named dev2010-
2011 was collected by randomly sampling bunches
of 5 consecutive sentences from the provided news
data of 2010 and 2011.
To estimate such large LMs, a vocabulary
was first defined for each language by including
all tokens observed in the Europarl and News-
Commentary corpora. For French and English, this
vocabulary was then expanded with all words that
occur more than 5 times in the French-English Gi-
gaWord corpus, and with the most frequent proper
names taken from the monolingual news data of
2010 and 2011. As for German, since the amount
of training data was smaller, the vocabulary was ex-
panded with the most frequent words observed in the
monolingual news data of 2010 and 2011. This pro-
cedure resulted in a vocabulary containing around
500k words in each language.
Language model training All the training data al-
lowed in the constrained task were divided into sev-
eral sets based on dates or genres (resp. 9 and 7
sets for English and French). On each set, a stan-
dard 4-gram LM was estimated from the 500k words
vocabulary using absolute discounting interpolated
with lower order models (Kneser and Ney, 1995;
Chen and Goodman, 1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 were first linearly interpolated.
The associated coefficients were estimated so as to
minimize the perplexity evaluated on dev2010-2011.
The resulting LM and the 2010-2011 LM were fi-
naly interpolated with newstest2008 as development
data. This procedure aims to avoid overestimating
the weight associated to the 2010-2011 LM.
4.2 The SOUL Model
We give here a brief overview of the SOUL LM;
refer to (Le et al, 2011) for the complete training
procedure. Following the classical work on dis-
tributed word representation (Brown et al, 1992),
we assume that the output vocabulary is structured
by a clustering tree, where each word belongs to
only one class and its associated sub-classes. If wi
denotes the i-th word in a sentence, the sequence
c1:D(wi) = c1, . . . ,cD encodes the path for the word
wi in the clustering tree, with D the depth of the tree,
cd(wi) a class or sub-class assigned to wi, and cD(wi)
the leaf associated with wi (the word itself). The
n-gram probability of wi given its history h can then
be estimated as follows using the chain rule:
P(wi|h) = P(c1(wi)|h)
D
?
d=2
P(cd(wi)|h,c1:d?1)
Figure 2 represents the architecture of the NNLM
to estimate this distribution, for a tree of depth
D = 3. The SOUL architecture is the same as for
the standard model up to the output layer. The
main difference lies in the output structure which in-
volves several layers with a softmax activation func-
tion. The first softmax layer (class layer) estimates
the class probability P(c1(wi)|h), while other out-
put sub-class layers estimate the sub-class proba-
bilities P(cd(wi)|h,c1:d?1). Finally, the word layers
estimate the word probabilities P(cD(wi)|h,c1:D?1).
Words in the short-list are a special case since each
of them represents its own class without any sub-
classes (D = 1 in this case).
5 Experimental Results
The experimental results are reported in terms of
BLEU and translation edit rate (TER) using the
newstest2010 corpus as evaluation set. These auto-
matic metrics are computed using the scripts pro-
vided by the NIST after a detokenization step.
5.1 English-French
Compared with last year evaluation, the amount of
available parallel data has drastically increased with
about 33M of sentence pairs. It is worth noticing
312
wi-1
w
i-2
w
i-3
R
R
R
W
ih
 shared context space
input layer
hidden layer:
tanh activation
word layers
sub-class 
layers
}
short list
Figure 2: Architecture of the Structured Output Layer
Neural Network language model.
that the provided corpora are not homogeneous, nei-
ther in terms of genre nor in terms of topics. Never-
theless, the most salient difference is the noise car-
ried by the GigaWord and the United Nation cor-
pora. The former is an automatically collected cor-
pus drawn from different websites, and while some
parts are indeed relevant to translate news texts, us-
ing the whole GigaWord corpus seems to be harm-
ful. The latter (United Nation) is obviously more
homogeneous, but clearly out of domain. As an il-
lustration, discarding the United Nation corpus im-
proves performance slightly.
Table 1 summarizes some of our attempts at deal-
ing with such a large amount of parallel data. As
stated above, translation models are trained with
the news-commentary, Europarl, and GigaWord cor-
pora. For this last data set, results show the reward of
sentence pair selection as described in Section 3.2.
Indeed, filtering out 75% of the corpus yields to
a significant BLEU improvement when translating
from English to French and of 1 point in the other
direction (line upper quartile in Table 1). More-
over, a larger selection (50% in the median line) still
increases the overall performance. This shows the
room left for improvement by a more accurate data
selection process such as a well optimized thresh-
old in our approach, or a more sophisticated filtering
strategy (see for example (Foster et al, 2010)).
Another issue when using such a large amount
System en2fr fr2en
BLEU TER BLEU TER
All 27.4 56.6 26.8 55.0
Upper quartile 27.8 56.3 28.4 53.8
Median 28.1 56.0 28.6 53.5
Table 1: English-French translation results in terms of
BLEU score and TER estimated on newstest2010 with
the NIST script. All means that the translation model is
trained on news-commentary, Europarl, and the whole
GigaWord. The rows upper quartile and median corre-
spond to the use of a filtered version of the GigaWord.
of data is the mismatch between the target vocab-
ulary derived from the translation model and that of
the LM. The translation model may generate words
which are unknown to the LM, and their probabili-
ties could be overestimated. To avoid this behaviour,
the probability of unknown words for the target LM
is penalized during the decoding step.
5.2 English-German
For this translation task, we compare the impact of
two different POS-taggers to process the German
part of the parallel data. The results are reported
in Table 2. Results show that to translate from En-
glish to German, the use of a fine-grained POS infor-
mation (RFTagger) leads to a slight improvement,
whereas it harms the source reordering model in the
other direction. It is worth noticing that to translate
from German to English, the RFTagger is always
used during the data pre-processing step, while a dif-
ferent POS tagger may be involved for the source
reordering model training.
System en2de de2en
BLEU TER BLEU TER
RFTagger 22.8 60.1 16.3 66.0
TreeTagger 23.1 59.4 16.2 66.0
Table 2: Translation results in terms of BLEU score
and translation edit rate (TER) estimated on newstest2010
with the NIST scoring script.
5.3 The SOUL Model
As mentioned in Section 4.2, the order of a con-
tinuous n-gram model such as the SOUL LM can
be raised without a prohibitive increase in complex-
ity. We summarize in Table 3 our experiments with
313
SOUL LMs of orders 4, 6, and 10. The SOUL LM
is introduced in the SMT pipeline by rescoring the
n-best list generated by the decoder, and the asso-
ciated weight is tuned with MERT. We observe for
the English-French task: a BLEU improvement of
0.3, as well as a similar trend in TER, when intro-
ducing a 4-gram SOUL LM; an additional BLEU
improvement of 0.3 when increasing the order from
4 to 6; and a less important gain with the 10-gram
SOUL LM. In the end, the use of a 10-gram SOUL
LM achieves a 0.7 BLEU improvement and a TER
decrease of 0.8. The results on the English-German
task show the same trend with a 0.5 BLEU point
improvement.
SOUL LM en2fr en2de
BLEU TER BLEU TER
without 28.1 56.0 16.3 66.0
4-gram 28.4 55.5 16.5 64.9
6-gram 28.7 55.3 16.7 64.9
10-gram 28.8 55.2 16.8 64.6
Table 3: Translation results from English to French and
English to German measured on newstest2010 using a
100-best rescoring with SOUL LMs of different orders.
5.4 Optimization Issues
Along with MIRA (Margin Infused Relaxed Al-
gorithm) (Watanabe et al, 2007), MERT is the
most widely used algorithm for system optimiza-
tion. However, standard MERT procedure is known
to suffer from instability of results and very slow
training cycle with approximate estimates of one de-
coding cycle for each training parameter. For this
year?s evaluation, we experimented with several al-
ternatives to the standard n-best MERT procedure,
namely, MERT on word lattices (Macherey et al,
2008) and two differentiable variants to the BLEU
objective function optimized during the MERT cy-
cle. We have recast the former in terms of a spe-
cific semiring and implemented it using a general-
purpose finite state automata framework (Sokolov
and Yvon, 2011). The last two approaches, hereafter
referred to as ZHN and BBN, replace the BLEU
objective function, with the usual BLEU score on
expected n-gram counts (Rosti et al, 2010) and
with an expected BLEU score for normal n-gram
counts (Zens et al, 2007), respectively. All expecta-
tions (of the n-gram counts in the first case and the
BLEU score in the second) are taken over all hy-
potheses from n-best lists for each source sentence.
Experiments with the alternative optimization
methods achieved virtually the same performance in
terms of BLEU score, but 2 to 4 times faster. Neither
approach, however, showed any consistent and sig-
nificant improvement for the majority of setups tried
(with the exception of the BBN approach, that had
almost always improved over n-best MERT, but for
the sole French to English translation direction). Ad-
ditional experiments with 9 complementary transla-
tion models as additional features were performed
with lattice-MERT, but neither showed any substan-
tial improvement. In the view of these rather incon-
clusive experiments, we chose to stick to the classi-
cal MERT for the submitted results.
6 Conclusion
In this paper, we described our submissions to
WMT?11 in the French-English and German-
English shared translation tasks, in both directions.
For this year?s participation, we only used n-code,
our open source Statistical Machine Translation sys-
tem based on bilingual n-grams. Our contributions
are threefold. First, we have shown that n-gram
based systems can achieve state-of-the-art perfor-
mance on large scale tasks in terms of automatic
metrics such as BLEU. Then, as already shown by
several sites in the past evaluations, there is a signifi-
cant reward for using data selection algorithms when
dealing with large heterogeneous data sources such
as the GigaWord. Finally, the use of a large vocab-
ulary continuous space language model such as the
SOUL model has enabled to achieve significant and
consistent improvements. For the upcoming evalua-
tion(s), we would like to suggest that the important
work of data cleaning and pre-processing could be
shared among all the participants instead of being
done independently several times by each site. Re-
ducing these differences could indeed help improve
the reliability of SMT systems evaluation.
Acknowledgment
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
314
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
P.F. Brown, P.V. de Souza, R.L. Mercer, V.J. Della Pietra,
and J.C. Lai. 1992. Class-based n-gram models of nat-
ural language. Computational Linguistics, 18(4):467?
479.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
Josep Maria Crego and Jose? Bernardo Marin?o. 2007. Im-
proving statistical MT by coupling reordering and de-
coding. Machine Translation, 20(3):199?215.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP 2011), Prague (Czech Republic),
22-27 May.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. of the Conf. on EMNLP, pages 725?734.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for wmt10 system combination task. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 321?326,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Holger Schwenk. 2007. Continuous space language
models. Computer, Speech & Language, 21(3):492?
518.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the 15th
Annual Conference of the European Association for
Machine Translation, EAMT?2011, May.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 524?
532.
315
