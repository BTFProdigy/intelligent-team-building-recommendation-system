Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 99?103,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Yandex School of Data Analysis machine translation systems for WMT13
Alexey Borisov, Jacob Dlougach, Irina Galinskaya
Yandex School of Data Analysis
16, Leo Tolstoy street, Moscow, Russia
{alborisov,jacob,galinskaya}@yandex-team.ru
Abstract
This paper describes the English-Russian
and Russian-English statistical machine
translation (SMT) systems developed at
Yandex School of Data Analysis for the
shared translation task of the ACL 2013
Eighth Workshop on Statistical Machine
Translation. We adopted phrase-based
SMT approach and evaluated a number
of different techniques, including data fil-
tering, spelling correction, alignment of
lemmatized word forms and translitera-
tion. Altogether they yielded +2.0 and
+1.5 BLEU improvement for ru-en and en-
ru language pairs. We also report on the
experiments that did not have any positive
effect and provide an analysis of the prob-
lems we encountered during the develop-
ment of our systems.
1 Introduction
We participated in the shared translation task of
the ACL 2013 Workshop on Statistical Machine
Translation (WMT13) for ru-en and en-ru lan-
guage pairs. We provide a detailed description of
the experiments carried out for the development of
our systems.
The rest of the paper is organized as follows.
Section 2 describes the tools and data we used.
Our Russian?English and English?Russian se-
tups are discussed in Section 3. In Section 4 we
report on the experiments that did not have any
positive effect despite our expectations. We pro-
vide a thorough analysis of erroneous outputs in
Section 5 and draw conclusions in Section 6.
2 Tools and data
2.1 Tools
We used an open source SMT system Moses
(Koehn et al, 2007) for all our experiments ex-
cluding the one described in Section 4.1 due to its
performance constraints. To overcome the limita-
tion we employed our in-house decoder.
Language models (LM) were created with an
open source IRSTLM toolkit (Federico et al,
2008). We computed 4-gram LMs with modified
Kneser-Ney smoothing (Kneser and Ney, 1995).
We used an open source MGIZA++ tool (Gao
and Vogel, 2008) to compute word alignment.
To obtain part of speech (POS) tags we used
an open source Stanford POS tagger for English
(Toutanova et al, 2003) and an open source suite
of language analyzers, FreeLing 3.0 (Carreras et
al., 2004; Padr? and Stanilovsky, 2012), for Rus-
sian.
We utilized a closed source free for non-
commercial use morphological analyzer, Mystem
(Segalovich, 2003), that used a limited dictionary
to obtain lemmas.
We also made use of the in-house language rec-
ognizer based on (Dunning, 1994) and a spelling
corrector designed on the basis of the work of
Cucerzan and Brill (2004).
We report all results in case-sensitive BLEU
(Papineni et al, 2002) using mt-eval13a script
from Moses distribution.
2.2 Data
Training data
We used News Commentary and News Crawl
monolingual corpora provided by the organizers
of the workshop.
Bilingual training data comprised English-
Russian parallel corpus release by Yandex1, News
Commentary and Common Crawl corpora pro-
vided by the organizers.
We also exploited Wiki Headlines collection of
three parallel corpora provided by CMU2 as a
1https://translate.yandex.ru/corpus
2http://www.statmt.org/wmt13/
wiki-titles.ru-en.tar.gz
99
source of reliable data.
Development set
The newstest2012 test set (Callison-Burch et al,
2012) was divided in the ratio 2:1 into a tuning
set and a test set. The latter is referred to as
newstest2012-test in the rest of the paper.
3 Primary setups
3.1 Baseline
We built the baseline systems according to the in-
structions available at the Moses website3.
3.2 Preprocessing
The first thing we noticed was that some sentences
marked as Russian appeared to be sentences in
other languages (most commonly English). We
applied a language recognizer for both monolin-
gual and bilingual corpora. Results are given in
Table 1.
Corpus Filtered out (%)
Bilingual 3.39
Monolingual (English) 0.41
Monolingual (Russian) 0.58
Table 1: Results of the language recognizer: per-
centage of filtered out sentences.
The next thing we came across was the pres-
ence of a lot of spelling errors in our training data,
so we applied a spelling corrector. Statistics are
presented in Table 2.
Corpus Modified (%)
Bilingual (English) 0.79
Bilingual (Russian) 1.45
Monolingual (English) 0.61
Monolingual (Russian) 0.52
Table 2: Results of the spelling corrector: percent-
age of modified sentences.
3.3 Alignment of lemmatized word forms
Russian is a language with rich morphology. The
diversity of word forms results in data sparse-
ness that makes translation of rare words dif-
ficult. In some cases inflections do not con-
tain any additional information and are used
3http://www.statmt.org/moses/?n=moses.
baseline
only to make an agreement between two words.
E.g. ADJ + NOUN: ?????? ?? ???? (beau-
tiful harp), ?????? ?? ??????? (beautiful pi-
ano), ?????? ?? ????? (beautiful grand piano).
These inflections reflect the gender of the noun
words, that has no equivalent in English.
In this particular case we can drop the inflec-
tions, but for other categories they can still be use-
ful for translation, because the information they
contain appears in function words in English. On
the other hand, most of Russian morphology is
useless for word alignment.
We applied a morphological analyzer Mystem
(Segalovich, 2003) to the Russian text and con-
verted each word to its dictionary form. Next
we computed word alignment between the origi-
nal English text and the lemmatized Russian text.
All the other steps were executed according to the
standard procedure with the original texts.
3.4 Phrase score adjustment
Sometimes phrases occur one or two times in the
training corpus. In this case the corresponding
phrase translation probability would be overesti-
mated. We used Good-Turing technique described
in (Gale, 1994) to decrease it to some more realis-
tic value.
3.5 Decoding
Minimum Bayes-Risk (MBR)
MBR decoding (Kumar and Byrne, 2004) aims
to minimize the expected loss of translation er-
rors. As it is not possible to explore the space of
all possible translations, we approximated it with
the 1,000 most probable translations. A minus
smoothed BLEU score (Lin and Och, 2004) was
used for the loss function.
Reordering constrains
We forbade reordering over punctuation and trans-
lated quoted phrases independently.
3.6 Handling unknown words
The news texts contained a lot of proper names
that did not appear in the training data. E.g. al-
most 25% of our translations contained unknown
words. Dropping the unknown words would lead
to better BLEU scores, but it might had caused
bad effect on human judgement. To leave them
in Cyrillic was not an option, so we exploited two
approaches: incorporating reliable data from Wiki
Headlines and transliteration.
100
newstest2012-test newstest2013
Russian?English
Baseline 28.96 21.82
+ Preprocessing 29.59 22.28
+ Alignment of lemmatized word forms 29.97 22.61
+ Good-Turing 30.31 22.87
+ MBR 30.45 23.21
+ Reordering constraints 30.54 23.33
+ Wiki Headlines 30.68 23.46
+ Transliteration 30.93 23.73
English?Russian
Baseline 21.96 16.24
+ Preprocessing 22.48 16.76
+ Good-Turing 22.84 17.13
+ MBR and Reordering constraints 23.27 17.45
+ Wiki Headlines and Transliteration 23.54 17.80
Table 3: Experimental results in case-sensitive BLEU for Russian?English and English?Russian tasks.
Wiki Headlines
We replaced the names occurring in the text with
their translations, based on the information in
"guessed-names" corpus from Wiki Headlines.
As has been mentioned in Section 3.3, Russian
is a morphologically rich language. This often
makes it hard to find exactly the same phrases,
so we applied lemmatization of Russian language
both for the input text and the Russian side of the
reference corpus.
Russian?English transliteration
We gained considerable improvement from incor-
porating Wiki Headlines, but still 17% of transla-
tions contained Cyrillic symbols.
We applied a transliteration algorithm based on
(Knight and Graehl, 1998). This technique yielded
us a significant improvement, but introduced a lot
of errors. E.g. ?????? ???? (James Bond) was
converted to Dzhejms Bond.
English?Russian transliteration
In Russian, it is a common practice to leave some
foreign words in Latin. E.g. the names of compa-
nies: Apple, Google, Microsoft look inadmissible
when either translated directly or transliterated.
Taking this into account, we applied the
same transliteration algorithm (Knight and Graehl,
1998), but replaced an unknown word with its
transliteration only if we found a sufficient num-
ber of occurrences of its transliterated form in the
monolingual corpus. We used five for such num-
ber.
3.7 Experimental results
We summarized the gains from the de-
scribed techniques for Russian?English and
English?Russian tasks on Table 3.
4 What did not work
4.1 Translation in two stages
Frequently machine translations contain errors
that can be easily corrected by human post-editors.
Since human aided machine translation is cost-
efficient, we decided to address this problem to the
computer.
We propose to translate sentences in two stages.
At the first stage a SMT system is used to trans-
late the input text into a preliminary form (in target
language). At the next stage the preliminary form
is translated again with an auxiliary SMT system
trained on the translated and the target sides of the
parallel corpus.
We encountered a technical challenge, when we
had to build a SMT system for the second stage.
A training corpus with one side generated with
the first stage SMT system was not possible to be
acquired with Moses due to its performance con-
straints. Thereupon we utilized our in-house SMT
decoder and managed to translate 2M sentences in
time.
We applied this technique both for ru-en and en-
ru language pairs. Approximately 20% of the sen-
101
tences had changed, but the BLEU score remained
the same.
4.2 Factored model
We tried to build a factored model for ru-en lan-
guage pair with POS tags produced by Stanford
POS tagger (Toutanova et al, 2003).
Unfortunately, we did not gain any improve-
ments from it.
5 Analysis
We carefully examined the erroneous outputs of
our system and compared it with the outputs of
the other systems participating in ru-en and en-ru
tasks, and with the commercial systems available
online (Bing, Google, Yandex).
5.1 Transliteration
Russian?English
The standard transliteration procedure is not in-
vertible. This means that a Latin word being trans-
fered into Cyrillic and then transliterated back
to Latin produces an artificial word form. E.g.
?????? ?????????? / Havard Halvarsen was
correctly transliterated by only four out of 23
systems, including ours. Twelve systems either
dropped one of the words or left it in Cyrillic.
We provide a list of typical mistakes in order of
their frequency: Khavard Khalvarsen, Khavard
Khal?varsen, Xavard Xaljvarsen. Another exam-
ple: ???? ?????? (Miss Wyatt) ? Miss Uayett
(all the systems failed).
The next issue is the presence of non-null in-
flections that most certainly would result in wrong
translation by any straight-forward algorithm. E.g.
??????????? ? (Heidelberg)? Heidelberga.
English?Russian
In Russian, most words of foreign origin are writ-
ten phonetically. Thereby, in order to obtain the
best quality we should transliterate the transcrip-
tion, not the word itself. E.g. the French derived
name Elsie Monereau [?elsi mon@?r@V] being trans-
lated by letters would result in ???? ????????
while the transliteration of the transcription would
result in the correct form ???? ?????.
5.2 Grammars
English and Russian make use of different gram-
mars. When the difference in their sentence struc-
ture becomes fundamental the phrase-based ap-
proach might get inapplicable.
Word order
Both Russian and English are classified as subject-
verb-object (SOV) languages, but Russian has
rather flexible word order compared to English
and might frequently appear in other forms. This
often results in wrong structure of the translated
sentence. A common mistake made by our sys-
tem and reproduced by the major online services:
?? ?????????? ? ??????? (rules have not been
changed either) ? have not changed and the
rules.
Constructions
? there is / there are is a non-local construc-
tion that has no equivalent in Russian. In
most cases it can not be produced from the
Russian text. E.g. ?? ????? ????? ??????-
?? (there is a matryoshka doll on the table)
? on the table is a matryoshka.
? multiple negatives in Russian are grammati-
cally correct ways to express negation (a sin-
gle negative is sometimes incorrect) while
they are undesirable in standard English. E.g.
??? ????? ??????? ?? ??? (nobody has
ever been there) being translated word by
word would result in there nobody never not
was.
5.3 Idioms
Idiomatic expressions are hard to discover and
dangerous to translate literary. E.g. a Russian
idiom ???? ?? ???? (let come what may) be-
ing translated word by word would result in was
not was. Neither of the commercial systems we
checked managed to collect sufficient statistic to
translate this very popular expression.
6 Conclusion
We have described the primary systems developed
by the team of Yandex School of Data Analysis for
WMT13 shared translation task.
We have reported on the experiments and
demonstrated considerable improvements over the
respective baseline. Among the most notable tech-
niques are data filtering, spelling correction, align-
ment of lemmatized word forms and translitera-
tion. We have analyzed the drawbacks of our sys-
tems and shared the ideas for further research.
102
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT12), pages 10?51, Montr?al, Canada, June.
Association for Computational Linguistics.
Xavier Carreras, Isaac Chao, Llu?s Padr?, and Muntsa
Padr?. 2004. FreeLing: An open-source suite of
language analyzers. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC).
Silviu Cucerzan and Eric Brill. 2004. Spelling cor-
rection as an iterative process that exploits the col-
lective knowledge of web users. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 293?300.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical report, Computing Research Lab
(CRL), New Mexico State University, Las Cruces,
NM, USA.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit
for handling large scale language models. In Pro-
ceedings of 9th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 1618?1621.
William Gale. 1994. Good-Turing smoothing with-
out tears. Journal of Quantitative Linguistics (JQL),
2:217?237.
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 49?57.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), vol-
ume 1, pages 181?184.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-burch, Richard Zens, Rwth Aachen,
Alexandra Constantin, Marcello Federico, Nicola
Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen,
Christine Moran, and Ondr?ej Bojar. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 177?180.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL), pages 163?171.
Chin-Yew Lin and Franz Josef Och. 2004. OR-
ANGE: a method for evaluating automatic evalua-
tion metrics for machine translation. In Proceed-
ings of the 20th international conference on Com-
putational Linguistics (COLING), Stroudsburg, PA,
USA. Association for Computational Linguistics.
Llu?s Padr? and Evgeny Stanilovsky. 2012. FreeLing
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC), Istanbul, Turkey, May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Processings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318.
Ilya Segalovich. 2003. A fast morphological algorithm
with unknown word guessing induced by a dictio-
nary for a web search engine. In Hamid R. Arab-
nia and Elena B. Kozerenko, editors, Proceedings of
the International Conference on Machine Learning;
Models, Technologies and Applications (MLMTA),
pages 273?280, Las Vegas, NV, USA, June. CSREA
Press.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL), pages 252?259.
103
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 66?70,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Yandex School of Data Analysis
Russian-English Machine Translation System for WMT14
Alexey Borisov and Irina Galinskaya
Yandex School of Data Analysis
16, Leo Tolstoy street, Moscow, Russia
{alborisov, galinskaya}@yandex-team.ru
Abstract
This paper describes the Yandex School
of Data Analysis Russian-English system
submitted to the ACL 2014 Ninth Work-
shop on Statistical Machine Translation
shared translation task. We start with the
system that we developed last year and in-
vestigate a few methods that were success-
ful at the previous translation task includ-
ing unpruned language model, operation
sequence model and the new reparameter-
ization of IBM Model 2. Next we propose
a {simple yet practical} algorithm to trans-
form Russian sentence into a more easily
translatable form before decoding. The al-
gorithm is based on the linguistic intuition
of native Russian speakers, also fluent in
English.
1 Introduction
The annual shared translation task organized
within the ACL Workshop on Statistical Machine
Translation (WMT) aims to evaluate the state of
the art in machine translation for a variety of lan-
guages. We participate in the Russian to English
translation direction.
The rest of the paper is organized as follows.
Our baseline system as well as the experiments
concerning the methods already discussed in lit-
erature are described in Section 2. In Section 3 we
present an algorithm we use to transform the Rus-
sian sentence before translation. In Section 4 we
discuss the results and conclude.
2 Initial System Development
We use all the Russian-English parallel data avail-
able in the constraint track and the Common Crawl
English monolingual corpus.
2.1 Baseline
We use the phrase-based Moses statistical ma-
chine translation system (Koehn et al., 2007) with
mostly default settings and a few changes (Borisov
et al., 2013) made in the following steps.
Data Preprocessing includes filtering out non
Russian-English sentence pairs and correction of
spelling errors.
Phrase Table Smoothing uses Good-Turing
scheme (Foster et al., 2006).
Consensus Decoding selects the translation
with minimum Bayes risk (Kumar and Byrne,
2004).
Handling of UnknownWords comprises incor-
poration of proper names from Wiki Headlines
parallel data provided by CMU
1
and translitera-
tion. We improve the transliteration algorithm in
Section 2.4.
Note that unlike last year we do not use word
alignments computed for the lemmatized word
forms.
2.2 Language Model
We use 5-gram unpruned language model with
modified Kneser-Ney discount estimated with
KenLM toolkit (Heafield et al., 2013).
2.3 Word alignment
Word alignments are generated using the
fast_align tool (Dyer et al., 2013), which is much
faster than IBM Model 4 from MGIZA++ (Gao
and Vogel, 2008) and outperforms the latter in
terms of BLEU. Results are given in Table 1.
2.4 Transliteration
We employ machine transliteration to generate ad-
ditional translation options for out-of-vocabulary
1
http://www.statmt.org/wmt14/
wiki-titles.tgz
66
MGIZA++ fast_align
Run Time 22 h 14 m 2h 49 m
Perplexity
? ru?en 97.00 90.37
? en?ru 209.36 216.71
BLEU
? WMT13 25.27 25.49
? WMT14 31.76 31.92
Table 1: Comparison of word alignment tools:
MGIZA++ vs. fast_align. fast_align runs ten
times as fast and outperforms the IBM Model 4
from MGIZA++ in terms of BLEU scores.
words. The transformation model we use is a
transfeme based model (Duan and Hsu, 2011),
which is analogous to translation model in phrase-
based machine translation. Transformation units,
or transfemes, are trained with Moses using the
default settings. Decoding is very similar to beam
search. We build a trie from the words in English
monolingual corpus, and search in it, based on the
transformation model.
2.5 Operation Sequence Model
The Operation Sequence N-gram Model (OSM)
(Durrani et al., 2011) integrates reordering opera-
tions and lexical translations into a heterogeneous
sequence of minimal translation units (MTUs) and
learns a Markov model over it. Reordering deci-
sions influence lexical selections and vice versa
thus improving the translation model. We use
OSM as a feature function in phrase-based SMT.
Please, refer to (Durrani et al., 2013) for imple-
mentation details.
3 Morphological Transformations
Russian is a fusional synthetic language, mean-
ing that the relations between words are redundant
and encoded inside the words. Adjectives alter
their form to reflect the gender, case, number and
in some cases, animacy of the nouns, resulting in
dozens of different word forms matching a single
English word. An example is given in Table 2.
Verbs in Russian are typically constructed from
the morphemes corresponding to functional words
in English (to, shall, will, was, were, has, have,
had, been, etc.). This Russian phenomenon leads
to two problems: data sparsity and high number of
one-to-many alignments, which both may result in
translation quality degradation.
Number
SG PL
Case Gender
NOM MASC ??????
NOM FEM ?????? ??????
NOM NEUT ??????
GEN MASC ???????
GEN FEM ?????? ??????
GEN NEUT ???????
DAT MASC ???????
DAT FEM ?????? ??????
DAT NEUT ???????
ACC MASC, AN ???????
ACC MASC, INAN ?????? ??????
ACC FEM ??????
ACC NEUT ??????
INS MASC ??????
INS FEM ?????? ??????
INS FEM ??????
INS NEUT ??????
ABL MASC ??????
ABL FEM ?????? ??????
ABL NEUT ??????
Table 2: Russian word forms corresponding to the
English word "summer" (adj.).
Hereafter, we propose an algorithm to transform
the original Russian sentence into a more easily
translatable form. The algorithm is based on the
linguistic intuition of native Russian speakers, also
fluent in English.
3.1 Approach
Based on the output from Russian morphological
analyzer we rewrite the input sentence based on
the following principles:
1. the original sentence is restorable
(by a Russian native speaker)
2. redundant information is omitted
3. word alignment is less ambiguous
3.2 Algorithm
The algorithm consists of two steps.
On the first step we employ in-house Rus-
sian morphological analyzer similar to Mys-
tem (Segalovich, 2003) to convert each word
(WORD) into a tuple containing its canonical form
(LEMMA), part of speech tag (POS) and a set
67
Category Abbr. Values
Animacy ANIM AN, INAN
Aspect ASP IMPERF, PERF
Case CASE NOM, GEN, DAT, ACC, INS, ABL
Comparison Type COMP COMP, SURP
Gender GEND MASC, FEM, NEUT
Mood MOOD IND, IMP, COND, SBJV
Number NUM SG, PL
Participle Type PART ACT, PASS
Person PERS PERS1, PERS2, PERS3
Tense TNS PRES, NPST, PST
Table 3: Morphological Categories
of other grammemes associated with the word
(GRAMMEMES). The tuple is later referred to as
LPG. If the canonical form or part of speech are
ambiguous, we set LEMMA to WORD; POS to
"undefined"; and GRAMMEMES to ?. Gram-
memes are grouped into grammatical categories
listed in Table 3.
WORD ?? LEMMA + POS + GRAMMEMES
On the second step, the LPGs are converted into
tokens that, we hope, will better match English
structure. Some grammemes result in separate to-
kens, others stay with the lemma, and the rest get
dropped. The full set of morphological transfor-
mations we use is given in Table 4.
An example of applying the algorithm to a Rus-
sian sentence is given in Figure 1.
3.3 Results
The translation has been improved in several
ways:
Incorrect Use of Tenses happens quite often in
statistical machine translation, which is especially
vexing in simple cases such as asks instead of
asked, explains instead of explain along with more
difficult ones e.g. has increased instead of would
increase. The proposed algorithm achieves con-
siderable improvement, since it explicitly models
tenses and all its relevant properties.
Missing Articles is a common problem of
most Russian-English translation systems, be-
cause there are no articles in Russian. Our model
creates an auxiliary token for each noun, which re-
flects its case and motivates an article.
Use of Simple Vocabulary is not desirable
when the source text is a vocabulary-flourished
?????? ???? 
??????.adj+?   ins  ????.n+sg 
on a summer day 
??????, adj, 
{inan, dat|ins, ?, male|neut, sg|pl} 
????, ?noun, 
{inan, ins, male, sg} 
Figure 1: An illustration of the proposed algorithm
to transform Russian sentence ?????? ???? (let-
nim dnem), meaning on a summer day, into a more
easily translatable form. First, for each word we
extract its canonical form, part of speech tag and a
set of associated morphological properties (gram-
memes). Then we apply hand-crafted rules (Ta-
ble 4) to transform them into separate tokens.
one. News are full of academic, bookish, inkhorn,
and other rare words. Phrase Table smoothing
methods discount the translation probabilities for
rare phrase pairs, preventing them from appearing
in English translation, while many of these rare
phrase pairs are correct. The good thing is that the
phrase pairs containing the transformed Russian
words may not be rare themselves, and thereby are
not discounted so heavily. A more effective use of
English vocabulary has been observed on WMT13
test dataset (see Table 5).
We have demonstrated the improvements on a
qualitative level. The quantitative results are sum-
marized in Table 6 (baseline ? without morpholog-
ical transformations; proposed ? with morpholog-
ical transformations).
68
LPG? tokens
LEMMA, adj,
{ANIM, CASE, COMP, GEND, NUM}
?
LEMMA.adj+COMP
LEMMA, noun,
{ANIM, CASE, GEND, NUM}
?
CASE LEMMA.n+NUM
LEMMA, verb (ger), {ASP, TNS}
?
LEMMA.vg+ASP+TNS
LEMMA, verb (inf), {ASP}
?
LEMMA.vi+ASP
LEMMA, verb (part), {PART, ASP, TNS}
?
LEMMA.vp+PART+ASP+TNS
LEMMA, verb (?),
{PART, ASP, MOOD, TENSE,
NUM, PERS}
?
1. TNS={PRES} | TNS={NPST} & ASP={IMPERF}
a. PERS3 ? PERS & SG ? NUM
LEMMA.v+pres+MOOD+PERS+NUM
b. otherwise
LEMMA.v+pres+MOOD
2. TNS={PST}
ASP LEMMA.v+pst+MOOD
3. TNS={NPST} & ASP={IMPERF}
fut LEMMA.v+MOOD
4. if ambiguous
LEMMA.v+PART+ASP+MOOD
+TNS+NUM+PERS
LEMMA, OTHER, GRAMMEMES
?
LEMMA.POS+GRAMMEMES
Table 4: A set of rules we use to transform
the LPGs (LEMMA, POS, GRAMMEMES), ex-
tracted on the first step, into individual tokens.
4 Discussion and Conclusion
We described the Yandex School of Data Anal-
ysis Russian-English system submitted to the
ACL 2014 Ninth Workshop on Statistical Machine
Translation shared translation task. The main con-
tribution of this work is an algorithm to transform
the Russian sentence into a more easily translat-
Input Translation
??????????? (a) differences
(raznoglasiya) (b) disputes
?????????????? (a) promoter
(propagandistom) (b) propagandist
??????????????? (a) mainly
(preimuschestvenno) (b) predominantly
Table 5: Morphological Transformations lead to
more effective use of English vocabulary. Trans-
lations marked with "a" were produced using the
baseline system; with "b" also use Morphological
Transformations.
Baseline Proposed
Distinct Words 899,992 564,354
OOV Words
? WMT13 829 590
? WMT14 884 660
Perplexity
? ru?en 90.37 99.81
? en?ru 216.71 128.15
BLEU
? WMT13 25.49 25.63
? WMT14 31.92 32.56
Table 6: Results of Morphological Transforma-
tions. We improved the statistical characteristics
of our models by reducing the number of distinct
words by 37% and managed to translate 25% of
previously untranslated words. BLEU scores were
improved by 0.14 and 0.64 points for WMT13 and
WMT14 test sets respectively.
able form before decoding. Significant improve-
ments in human satisfaction and BLEU scores
have been demonstrated from applying this algo-
rithm.
One limitation of the proposed algorithm is that
it does not take into account the relations between
words sharing the same root. E.g. the word ?????-
??? (aistinyh) meaning stork (adj.) is handled in-
dependently from the word ???? (aist) meaning
stork (n.). Our system as well as the major online
services (Bing, Google, Yandex) transliterated this
word, but the word aistinyh does not make much
sense to a non-Russian reader. It might be worth-
while to study this problem in more detail.
Another direction for future work is to apply
the proposed algorithm in reverse direction. We
suggest the following two-step procedure. English
69
sentence is first translated into Russian
?
(Russian
after applying the morphological transformations),
and at the next step it is translated again with an
auxiliary SMT system trained on the (Russian*,
Russian) parallel corpus created from the Russian
monolingual corpus.
References
Alexey Borisov, Jacob Dlougach, and Irina Galinskaya.
2013. Yandex school of data analysis machine trans-
lation systems for wmt13. In Proceedings of the
Eighth Workshop on Statistical Machine Translation
(WMT), pages 97?101. Association for Computa-
tional Linguistics.
Huizhong Duan and Bo-June Paul Hsu. 2011. On-
line spelling correction for query completion. In
Proceedings of the 20th international conference on
World Wide Web (WWW), pages 117?126. ACM.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 1045?1054. Association
for Computational Linguistics.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine trans-
lation systems for european language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation (WMT), pages 112?119. Associa-
tion for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), pages 644?648.
Association for Computational Linguistics.
George Foster, Roland Kuhn, and John Howard John-
son. 2006. Phrasetable smoothing for statistical ma-
chine translation. In Proceedings of the 44th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 53?61. Association for Com-
putational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 49?57. As-
sociation for Computational Linguistics.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
690?696, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-burch, Richard Zens, Rwth Aachen,
Alexandra Constantin, Marcello Federico, Nicola
Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen,
Christine Moran, and Ond?rej Bojar. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 177?180. Association for Compu-
tational Linguistics.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 163?171. Association for
Computational Linguistics.
Ilya Segalovich. 2003. A fast morphological algorithm
with unknown word guessing induced by a dictio-
nary for a web search engine. In Hamid R. Arab-
nia and Elena B. Kozerenko, editors, Proceedings of
the International Conference on Machine Learning;
Models, Technologies and Applications (MLMTA),
pages 273?280, Las Vegas, NV, USA, June. CSREA
Press.
70
