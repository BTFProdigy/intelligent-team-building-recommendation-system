Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 74?78,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Accelerated Estimation of Conditional Random Fields using a
Pseudo-Likelihood-inspired Perceptron Variant
Teemu Ruokolainen
a
Miikka Silfverberg
b
Mikko Kurimo
a
Krister Lind?n
b
a
Department of Signal Processing and Acoustics, Aalto University, firstname.lastname@aalto.fi
b
Department of Modern Languages, University of Helsinki, firstname.lastname@helsinki.fi
Abstract
We discuss a simple estimation approach
for conditional random fields (CRFs). The
approach is derived heuristically by defin-
ing a variant of the classic perceptron al-
gorithm in spirit of pseudo-likelihood for
maximum likelihood estimation. The re-
sulting approximative algorithm has a lin-
ear time complexity in the size of the la-
bel set and contains a minimal amount of
tunable hyper-parameters. Consequently,
the algorithm is suitable for learning CRF-
based part-of-speech (POS) taggers in
presence of large POS label sets. We
present experiments on five languages.
Despite its heuristic nature, the algorithm
provides surprisingly competetive accura-
cies and running times against reference
methods.
1 Introduction
The conditional random field (CRF) model (Laf-
ferty et al., 2001) has been successfully applied
to several sequence labeling tasks in natural lan-
guage processing, including part-of-speech (POS)
tagging. In this work, we discuss accelerating the
CRF model estimation in presence of a large num-
ber of labels, say, hundreds or thousands. Large la-
bel sets occur in POS tagging of morphologically
rich languages (Erjavec, 2010; Haverinen et al.,
2013).
CRF training is most commonly associated with
the (conditional) maximum likelihood (ML) crite-
rion employed in the original work of Lafferty et
al. (2001). In this work, we focus on an alternative
training approach using the averaged perceptron
algorithm of Collins (2002). While yielding com-
petitive accuracy (Collins, 2002; Zhang and Clark,
2011), the perceptron algorithm avoids extensive
tuning of hyper-parameters and regularization re-
quired by the stochastic gradient descent algo-
rithm employed in ML estimation (Vishwanathan
et al., 2006). Additionally, while ML and percep-
tron training share an identical time complexity,
the perceptron is in practice faster due to sparser
parameter updates.
Despite its simplicity, running the perceptron al-
gorithm can be tedious in case the data contains
a large number of labels. Previously, this prob-
lem has been addressed using, for example, k-best
beam search (Collins and Roark, 2004; Zhang and
Clark, 2011; Huang et al., 2012) and paralleliza-
tion (McDonald et al., 2010). In this work, we
explore an alternative strategy, in which we mod-
ify the perceptron algorithm in spirit of the classic
pseudo-likelihood approximation for ML estima-
tion (Besag, 1975). The resulting novel algorithm
has linear complexity w.r.t. the label set size and
contains only a single hyper-parameter, namely,
the number of passes taken over the training data
set.
We evaluate the algorithm, referred to as the
pseudo-perceptron, empirically in POS tagging
on five languages. The results suggest that the
approach can yield competitive accuracy com-
pared to perceptron training accelerated using a
violation-fixed 1-best beam search (Collins and
Roark, 2004; Huang et al., 2012) which also pro-
vides a linear time complexity in label set size.
The rest of the paper is as follows. In Section 2,
we describe the pseudo-perceptron algorithm and
discuss related work. In Sections 3 and 4, we
describe our experiment setup and the results, re-
spectively. Conclusions on the work are presented
in Section 5.
2 Methods
2.1 Pseudo-Perceptron Algorithm
The (unnormalized) CRF model for input and
output sequences x = (x
1
, x
2
, . . . , x
|x|
) and
74
y = (y
1
, y
2
, . . . , y
|x|
), respectively, is written as
p (y |x;w) ? exp
(
w ??(y, x)
)
=
|x|
?
i=n
exp
(
w ? ?(y
i?n
, . . . , y
i
, x, i)
)
,
(1)
where w denotes the model parameter vector, ?
the vector-valued global feature extracting func-
tion, ? the vector-valued local feature extracting
function, and n the model order. We denote the
tag set as Y . The model parameters w are esti-
mated based on training data, and test instances
are decoded using the Viterbi search (Lafferty et
al., 2001).
Given the model definition (1), the param-
eters w can be estimated in a straightforward
manner using the structured perceptron algo-
rithm (Collins, 2002). The algorithm iterates
over the training set a single instance (x, y) at
a time and updates the parameters according
to the rule w
(i)
= w
(i?1)
+ ??(x, y, z), where
??(x, y, z) for the ith iteration is written as
??(x, y, z) = ?(x, y)??(x, z). The predic-
tion z is obtained as
z = arg max
u?Y(x)
w ??(x, u) (2)
by performing the Viterbi search over
Y(x) = Y ? ? ? ? ? Y , a product of |x| copies
of Y . In case the perceptron algorithm yields
a small number of incorrect predictions on the
training data set, the parameters generalize well
to test instances with a high probability (Collins,
2002).
The time complexity of the Viterbi search is
O(|x| ? |Y|
n+1
). Consequently, running the per-
ceptron algorithm can become tedious if the la-
bel set cardinality |Y| and/or the model order n
is large. In order to speed up learning, we define
a variant of the algorithm in the spirit of pseudo-
likelihood (PL) learning (Besag, 1975). In anal-
ogy to PL, the key idea of the pseudo-perceptron
(PP) algorithm is to obtain the required predictions
over single variables y
i
while fixing the remaining
variables to their true values. In other words, in-
stead of using the Viterbi search to find the z as in
(2), we find a z
?
for each position i ? 1..|x| as
z
?
= arg max
u?Y
?
i
(x)
w ??(x, u) , (3)
with Y
?
i
(x) = {y
1
}?? ? ??{y
i?1
}?Y?{y
i+1
}?
? ? ? ? {y
|x|
}. Subsequent to training, test instances
are decoded in a standard manner using the Viterbi
search.
The appeal of PP is that the time complexity
of search is reduced to O(|x| ? |Y|), i.e., linear
in the number of labels in the label set. On the
other hand, we no longer expect the obtained pa-
rameters to necessarily generalize well to test in-
stances.
1
Consequently, we consider PP a heuris-
tic estimation approach motivated by the rather
well-established success of PL (Kor
?
c and F?rstner,
2008; Sutton and McCallum, 2009).
2
Next, we study yet another heuristic pseudo-
variant of the perceptron algorithm referred to as
the piecewise-pseudo-perceptron (PW-PP). This
algorithm is analogous to the piecewise-pseudo-
likelihood (PW-PL) approximation presented by
Sutton and McCallum (2009). In this variant, the
original graph is first split into smaller, possibly
overlapping subgraphs (pieces). Subsequently, we
apply the PP approximation to the pieces. We em-
ploy the approach coined factor-as-piece by Sut-
ton and McCallum (2009), in which each piece
contains n + 1 consecutive variables, where n is
the CRF model order.
The PW-PP approach is motivated by the results
of Sutton and McCallum (2009) who found PW-
PL to increase stability w.r.t. accuracy compared
to plain PL across tasks. Note that the piecewise
approximation in itself is not interesting in chain-
structured CRFs, as it results in same time com-
plexity as standard estimation. Meanwhile, the
PW-PP algorithm has same time complexity as PP.
2.2 Related work
Previously, impractical running times of percep-
tron learning have been addressed most notably
using the k-best beam search method (Collins and
Roark, 2004; Zhang and Clark, 2011; Huang et
al., 2012). Here, we consider the ?greedy? 1-best
beam search variant most relevant as it shares the
time complexity of the pseudo search. Therefore,
in the experimental section of this work, we com-
pare the PP and 1-best beam search.
We are aware of at least two other learning ap-
proaches inspired by PL, namely, the pseudo-max
and piecewise algorithms of Sontag et al. (2010)
and Alahari et al. (2010), respectively. Com-
pared to these approaches, the PP algorithm pro-
vides a simpler estimation tool as it avoids the
1
We leave formal treatment to future work.
2
Meanwhile, note that pseudo-likelihood is a consistent
estimator (Gidas, 1988; Hyv?rinen, 2006).
75
hyper-parameters involved in the stochastic gradi-
ent descent algorithms as well as the regularization
and margin functions inherent to the approaches of
Alahari et al. (2010) and Sontag et al. (2010). On
the other hand, Sontag et al. (2010) show that the
pseudo-max approach achieves consistency given
certain assumptions on the data generating func-
tion. Meanwhile, as discussed in previous section,
we consider PP a heuristic and do not provide any
generalization guarantees. To our understanding,
Alahari et al. (2010) do not provide generalization
guarantees for their algorithm.
3 Experimental Setup
3.1 Data
For a quick overview of the data sets, see Table 1.
Penn Treebank. The first data set we consider
is the classic Penn Treebank. The complete tree-
bank is divided into 25 sections of newswire text
extracted from the Wall Street Journal. We split
the data into training, development, and test sets
using the sections 0-18, 19-21, and 22-24, accord-
ing to the standardly applied division introduced
by Collins (2002).
Multext-East. The second data we consider is
the multilingual Multext-East (Erjavec, 2010) cor-
pus. The corpus contains the novel 1984 by
George Orwell. From the available seven lan-
guages, we utilize the Czech, Estonian and Ro-
manian sections. Since the data does not have a
standard division to training and test sets, we as-
sign the 9th and 10th from each 10 consecutive
sentences to the development and test sets, respec-
tively. The remaining sentences are assigned to the
training sets.
Turku Dependency Treebank. The third data
we consider is the Finnish Turku Dependency
Treebank (Haverinen et al., 2013). The treebank
contains text from 10 different domains. We use
the same data split strategy as for Multext East.
3.2 Reference Methods
We compare the PP and PW-PP algorithms with
perceptron learning accelerated using 1-best beam
search modified using the early update rule
(Huang et al., 2012). While Huang et al. (2012)
experimented with several violation-fixing meth-
ods (early, latest, maximum, hybrid), they ap-
peared to reach termination at the same rate in
lang. train. dev. test tags train. tags
eng 38,219 5,527 5,462 45 45
rom 5,216 652 652 405 391
est 5,183 648 647 413 408
cze 5,402 675 675 955 908
fin 5,043 630 630 2,355 2,141
Table 1: Overview on data. The training (train.),
development (dev.) and test set sizes are given in
sentences. The columns titled tags and train. tags
correspond to total number of tags in the data set
and number of tags in the training set, respectively.
POS tagging. Our preliminary experiments using
the latest violation updates supported this. Conse-
quently, we employ the early updates.
We also provide results using the CRFsuite
toolkit (Okazaki, 2007), which implements a 1st-
order CRF model. To best of our knowledge,
CRFsuite is currently the fastest freely available
CRF implementation.
3
In addition to the averaged
perceptron algorithm (Collins, 2002), the toolkit
implements several training procedures (Nocedal,
1980; Crammer et al., 2006; Andrew and Gao,
2007; Mejer and Crammer, 2010; Shalev-Shwartz
et al., 2011). We run CRFsuite using these algo-
rithms employing their default parameters and the
feature extraction scheme and stopping criterion
described in Section 3.3. We then report results
provided by the most accurate algorithm on each
language.
3.3 Details on CRF Training and Decoding
While the methods discussed in this work are ap-
plicable for nth-order CRFs, we employ 1st-order
CRFs in order to avoid overfitting the relatively
small training sets.
We employ a simple feature set including word
forms at position t? 2, . . . , t+ 2, suffixes of word
at position t up to four letters, and three ortho-
graphic features indicating if the word at position
t contains a hyphen, capital letter, or a digit.
All the perceptron variants (PP, PW-PP, 1-best
beam search) initialize the model parameters with
zero vectors and process the training instances in
the order they appear in the corpus. At the end
of each pass, we apply the CRFs using the latest
averaged parameters (Collins, 2002) to the devel-
opment set. We assume the algorithms have con-
verged when the model accuracy on development
3
See benchmark results at http://www.chokkan.
org/software/crfsuite/benchmark.html
76
has not increased during last three iterations. Af-
ter termination, we apply the averaged parameters
yielding highest performance on the development
set to test instances.
Test and development instances are decoded us-
ing a combination of Viterbi search and the tag
dictionary approach of Ratnaparkhi (1996). In this
approach, candidate tags for known word forms
are limited to those observed in the training data.
Meanwhile, word forms that were unseen during
training consider the full label set.
3.4 Software and Hardware
The experiments are run on a standard desktop
computer. We use our own C++-based implemen-
tation of the methods discussed in Section 2.
4 Results
The obtained training times and test set accuracies
(measured using accuracy and out-of-vocabulary
(OOV) accuracy) are presented in Table 2. The
training CPU times include the time (in minutes)
consumed by running the perceptron algorithm
variants as well as evaluation of the development
set accuracy. The column labeled it. corresponds
to the number of passes over training set made by
the algorithms before termination.
We summarize the results as follows. First, PW-
PP provided higher accuracies compared to PP on
Romanian, Czech, and Finnish. The differences
were statistically significant
4
on Czech. Second,
while yielding similar running times compared
to 1-best beam search, PW-PP provided higher
accuracies on all languages apart from Finnish.
The differences were significant on Estonian and
Czech. Third, while fastest on the Penn Treebank,
the CRFsuite toolkit became substantially slower
compared to PW-PP when the number of labels
were increased (see Czech and Finnish). The dif-
ferences in accuracies between the best perform-
ing CRFsuite algorithm and PP and PW-PP were
significant on Czech.
5 Conclusions
We presented a heuristic perceptron variant for
estimation of CRFs in the spirit of the classic
4
We establish significance (with confidence level 0.95)
using the standard 1-sided Wilcoxon signed-rank test per-
formed on 10 randomly divided, non-overlapping subsets of
the complete test sets.
method it. time (min) acc. OOV
English
PP 9 6 96.99 87.97
PW-PP 10 7 96.98 88.11
1-best beam 17 8 96.91 88.33
Pas.-Agg. 9 1 97.01 88.68
Romanian
PP 9 8 96.81 83.66
PW-PP 8 7 96.91 84.38
1-best beam 17 10 96.88 85.32
Pas.-Agg. 13 9 97.06 84.69
Estonian
PP 10 8 93.39 78.10
PW-PP 8 6 93.35 78.66
1-best beam 23 15 92.95 75.65
Pas.-Agg. 15 12 93.27 77.63
Czech
PP 11 26 89.37 70.67
PW-PP 16 41 89.84 72.52
1-best beam 14 19 88.95 70.90
Pegasos 15 341 90.42 72.59
Finnish
PP 11 58 87.09 58.58
PW-PP 11 56 87.16 58.50
1-best beam 21 94 87.38 59.29
Pas.-Agg. 16 693 87.17 57.58
Table 2: Results. We report CRFsuite results pro-
vided by most accurate algorithm on each lan-
guage: the Pas.-Agg. and Pegasos refer to the al-
gorithms of Crammer et al. (2006) and Shalev-
Shwartz et al. (2011), respectively.
pseudo-likelihood estimator. The resulting ap-
proximative algorithm has a linear time complex-
ity in the label set cardinality and contains only
a single hyper-parameter, namely, the number of
passes taken over the training data set. We eval-
uated the algorithm in POS tagging on five lan-
guages. Despite its heuristic nature, the algo-
rithm provided competetive accuracies and run-
ning times against reference methods.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language stud-
ies) and the Academy of Finland under the grant
no 251170 (Finnish Centre of Excellence Pro-
gram (2012-2017)). We would like to thank Dr.
Onur Dikmen for the helpful discussions during
the work.
77
References
Karteek Alahari, Chris Russell, and Philip H.S. Torr.
2010. Efficient piecewise learning for conditional
random fields. In Computer Vision and Pattern
Recognition (CVPR), 2010 IEEE Conference on,
pages 895?901.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L
1
-regularized log-linear models. In Proceed-
ings of the 24th international conference on Ma-
chine learning, pages 33?40.
Julian Besag. 1975. Statistical analysis of non-lattice
data. The statistician, pages 179?195.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 111.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing, volume 10, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research, 7:551?585.
Toma? Erjavec. 2010. Multext-east version 4: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10).
Basilis Gidas. 1988. Consistency of maximum like-
lihood and pseudo-likelihood estimators for Gibbs
distributions. In Stochastic differential systems,
stochastic control theory and applications, pages
129?145.
Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Missil?,
Stina Ojala, Tapio Salakoski, and Filip Ginter. 2013.
Building the essential resources for Finnish: the
Turku Dependency Treebank. Language Resources
and Evaluation.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151.
Aapo Hyv?rinen. 2006. Consistency of pseudolike-
lihood estimation of fully visible Boltzmann ma-
chines. Neural Computation, 18(10):2283?2292.
Filip Kor
?
c and Wolfgang F?rstner. 2008. Approximate
parameter learning in conditional random fields: An
empirical investigation. Pattern Recognition, pages
11?20.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456?464.
Avihai Mejer and Koby Crammer. 2010. Confidence
in structured-prediction using confidence-weighted
models. In Proceedings of the 2010 conference on
empirical methods in natural language processing,
pages 971?981.
Jorge Nocedal. 1980. Updating quasi-Newton matri-
ces with limited storage. Mathematics of computa-
tion, 35(151):773?782.
Naoaki Okazaki. 2007. CRFsuite: a fast implemen-
tation of conditional random fields (CRFs). URL
http://www.chokkan.org/software/crfsuite.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the conference on empirical methods in natural
language processing, volume 1, pages 133?142.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro,
and Andrew Cotter. 2011. Pegasos: Primal esti-
mated sub-gradient solver for SVM. Mathematical
Programming, 127(1):3?30.
David Sontag, Ofer Meshi, Tommi Jaakkola, and Amir
Globerson. 2010. More data means less inference:
A pseudo-max approach to structured learning. In
Advances in Neural Information Processing Systems
23, pages 2181?2189.
Charles Sutton and Andrew McCallum. 2009. Piece-
wise training for structured prediction. Machine
learning, 77(2):165?194.
S.V.N. Vishwanathan, Nicol Schraudolph, Mark
Schmidt, and Kevin Murphy. 2006. Accelerated
training of conditional random fields with stochas-
tic gradient methods. In Proceedings of the 23rd in-
ternational conference on Machine learning, pages
969?976.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
78
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 259?264,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Part-of-Speech Tagging using Conditional Random Fields: Exploiting
Sub-Label Dependencies for Improved Accuracy
Miikka Silfverberg
a
Teemu Ruokolainen
b
Krister Lind?n
a
Mikko Kurimo
b
a
Department of Modern Languages, University of Helsinki,
firstname.lastname@helsinki.fi
b
Department of Signal Processing and Acoustics, Aalto University,
firstname.lastname@aalto.fi
Abstract
We discuss part-of-speech (POS) tagging
in presence of large, fine-grained la-
bel sets using conditional random fields
(CRFs). We propose improving tagging
accuracy by utilizing dependencies within
sub-components of the fine-grained labels.
These sub-label dependencies are incor-
porated into the CRF model via a (rela-
tively) straightforward feature extraction
scheme. Experiments on five languages
show that the approach can yield signifi-
cant improvement in tagging accuracy in
case the labels have sufficiently rich inner
structure.
1 Introduction
We discuss part-of-speech (POS) tagging using
the well-known conditional random field (CRF)
model introduced originally by Lafferty et al
(2001). Our focus is on scenarios, in which the
POS labels have a rich inner structure. For exam-
ple, consider
PRON+1SG V+NON3SG+PRES N+SG
I like ham
,
where the compound labels PRON+1SG,
V+NON3SG+PRES, and N+SG stand for pro-
noun first person singular, verb non-third singular
present tense, and noun singular, respectively.
Fine-grained labels occur frequently in mor-
phologically complex languages (Erjavec, 2010;
Haverinen et al, 2013).
We propose improving tagging accuracy by uti-
lizing dependencies within the sub-labels (PRON,
1SG, V, NON3SG, N, and SG in the above ex-
ample) of the compound labels. From a technical
perspective, we accomplish this by making use of
the fundamental ability of the CRFs to incorporate
arbitrarily defined feature functions. The newly-
defined features are expected to alleviate data spar-
sity problems caused by the fine-grained labels.
Despite the (relative) simplicity of the approach,
we are unaware of previous work exploiting the
sub-labels to the extent presented here.
We present experiments on five languages (En-
glish, Finnish, Czech, Estonian, and Romanian)
with varying POS annotation granularity. By uti-
lizing the sub-labels, we gain significant improve-
ment in model accuracy given a sufficiently fine-
grained label set. Moreover, our results indi-
cate that exploiting the sub-labels can yield larger
improvements in tagging compared to increasing
model order.
The rest of the paper is organized as follows.
Section 2 describes the methodology. Experimen-
tal setup and results are presented in Section 3.
Section 4 discusses related work. Lastly, we pro-
vide conclusions on the work in Section 5.
2 Methods
2.1 Conditional Random Fields
The (unnormalized) CRF model (Lafferty et al,
2001) for a sentence x = (x
1
, . . . , x
|x|
) and a POS
sequence y = (y
1
, . . . , y
|x|
) is defined as
p (y |x;w) ?
|x|
?
i=n
exp
(
w??(y
i?n
, . . . , y
i
, x, i)
)
,
(1)
where n denotes the model order,w the model pa-
rameter vector, and ? the feature extraction func-
tion. We denote the tag set as Y , that is, y
i
? Y
for i ? 1 . . . |x|.
2.2 Baseline Feature Set
We first describe our baseline feature set
{?
j
(y
i?1
, y
i
, x, i)}
|?|
j=1
by defining emission and
transition features. The emission feature set as-
sociates properties of the sentence position i with
259
the corresponding label as
{?
j
(x, i)1(y
i
= y
?
i
) | j ? 1 . . . |X | , ?y
?
i
? Y} ,
(2)
where the function 1(q) returns one if and only if
the proposition q is true and zero otherwise, that is
1(y
i
= y
?
i
) =
{
1 if y
i
= y
?
i
0 otherwise
, (3)
and X = {?
j
(x, i)}
|X |
j=1
is the set of functions
characterizing the word position i. Following the
classic work of Ratnaparkhi (1996), our X com-
prises simple binary functions:
1. Bias (always active irrespective of input).
2. Word forms x
i?2
, . . . , x
i+2
.
3. Prefixes and suffixes of the word form x
i
up
to length ?
suf
= 4.
4. If the word form x
i
contains (one or more)
capital letter, hyphen, dash, or digit.
Binary functions have a return value of either zero
(inactive) or one (active). Meanwhile, the transi-
tion features
{1(y
i?k
= y
?
i?k
) . . .1(y
i
= y
?
i
) |
y
?
i?k
, . . . , y
?
i
? Y ,?k ? 1 . . . n} (4)
capture dependencies between adjacent labels ir-
respective of the input x.
2.2.1 Expanded Feature Set Leveraging
Sub-Label Dependencies
The baseline feature set described above can yield
a high tagging accuracy given a conveniently sim-
ple label set, exemplified by the tagging results
of Collins (2002) on the Penn Treebank (Mar-
cus et al, 1993). (Note that conditional random
fields correspond to discriminatively trained hid-
den Markov models and Collins (2002) employs
the latter terminology.) However, it does to some
extent overlook some beneficial dependency infor-
mation in case the labels have a rich sub-structure.
In what follows, we describe expanded feature sets
which explicitly model the sub-label dependen-
cies.
We begin by defining a function P(y
i
) which
partitions any label y
i
into its sub-label compo-
nents and returns them in an unordered set. For
example, we could define P(PRON+1+SG) =
{PRON, 1, SG}. (Label partitions employed in
the experiments are described in Section 3.2.) We
denote the set of all sub-label components as S.
Subsequently, instead of defining only (2), we
additionally associate the feature functionsX with
all sub-labels s ? S by defining
{?
j
(x, i)1(s ? P(y
i
)) | ?j ? 1 . . . |X | ,?s ? S} ,
(5)
where 1(s ? P(y
i
)) returns one in case s is in
P(y
i
) and zero otherwise. Second, we exploit sub-
label transitions using features
{1(s
i?k
? P(y
i?k
)) . . .1(s
i
? P(y
i
)) |
?s
i?k
, . . . , s
i
? S ,?k ? 1 . . .m} . (6)
Note that we define the sub-label transitions up
to order m, 1 ? m ? n, that is, an nth-order
CRF model is not obliged to utilize sub-label tran-
sitions all the way up to order n. This is be-
cause employing high-order sub-label transitions
may potentially cause overfitting to training data
due to substantially increased number of features
(equivalent to the number of model parameters,
|w| = |?|). For example, in a second-order
(n = 2) model, it might be beneficial to em-
ploy the sub-label emission feature set (5) and
first-order sub-label transitions while discarding
second-order sub-label transitions. (See the exper-
imental results presented in Section 3.)
In the remainder of this paper, we use the fol-
lowing notations.
1. A standard CRF model incorporating (2) and
(4) is denoted as CRF(n,-).
2. A CRF model incorporating (2), (4), and (5)
is denoted as CRF(n,0).
3. A CRF model incorporating (2), (4), (5), and
(6) is denoted as CRF(n,m).
2.3 On Linguistic Intuition
This section aims to provide some intuition on the
types of linguistic phenomena that can be captured
by the expanded feature set. To this end, we con-
sider an example on the plural number in Finnish.
First, consider the plural nominative word form
kissat (cats) where the plural number is denoted
by the 1-suffix -t. Then, by employing the features
(2), the suffix -t is associated solely with the com-
pound label NOMINATIVE+PLURAL. However,
by incorporating the expanded feature set (5), -t
260
will also be associated to the sub-label PLURAL.
This can be useful because, in Finnish, also adjec-
tives and numerals are inflected according to num-
ber and denote the plural number with the suffix
-t (Hakulinen et al, 2004, ?79). Therefore, one
can exploit -t to predict the plural number also in
words such as mustat (plural of black) with a com-
pound analysis ADJECTIVE+PLURAL.
Second, consider the number agreement (con-
gruence). For example, in the sentence fragment
mustat kissat juoksevat (black cats are running),
the words mustat and kissat share the plural num-
ber. In other words, the analyses of both mustat
and kissat are required to contain the sub-label
PLURAL. This short-span dependency between
sub-labels will be captured by a first-order sub-
label transition feature included in (6).
Lastly, we note that the feature expansion sets
(5) and (6) will, naturally, capture any short-span
dependencies within the sub-labels irrespective if
the dependencies have a clear linguistic interpre-
tation or not.
3 Experiments
3.1 Data
For a quick overview of the data sets, see Table 1.
Penn Treebank. The English Penn Treebank
(Marcus et al, 1993) is divided into 25 sections
of newswire text extracted from the Wall Street
Journal. We split the data into training, develop-
ment, and test sets using the sections 0-18, 19-21,
and 22-24, according to the standardly applied di-
vision introduced by Collins (2002).
Turku Depedency Treebank. The Finnish
Turku Depedendency Treebank (Haverinen et al,
2013) contains text from 10 different domains.
The treebank does not have default partition to
training and test sets. Therefore, from each 10
consecutive sentences, we assign the 9th and 10th
to the development set and the test set, respec-
tively. The remaining sentences are assigned to
the training set.
Multext-East. The third data we consider is the
multilingual Multext-East (Erjavec, 2010) corpus,
from which we utilize the Czech, Estonian and Ro-
manian sections. The corpus corresponds to trans-
lations of the novel 1984 by George Orwell. We
apply the same data splits as for Turku Depen-
dency Treebank.
lang. train. dev. test tags train. tags
Eng 38,219 5,527 5,462 45 45
Rom 5,216 652 652 405 391
Est 5,183 648 647 413 408
Cze 5,402 675 675 955 908
Fin 5,043 630 630 2,355 2,141
Table 1: Overview on data. The training (train.),
development (dev.) and test set sizes are given in
sentences. The columns titled tags and train. tags
correspond to total number of tags in the data set
and number of tags in the training set, respectively.
3.2 Label Partitions
This section describes the employed compound la-
bel splits. The label splits for all data sets are sub-
mitted as data file attachments. All the splits are
performed a priori to model learning, that is, we
do not try to optimize them on the development
sets.
The POS labels in the Penn Treebank are split
in a way which captures relevant inflectional cat-
egories, such as tense and number. Consider, for
example, the split for the present tense third sin-
gular verb label P(VBZ) = {VB, Z}.
In the Turku Dependency Treebank, each
morphological tag consists of sub-labels mark-
ing word-class, relevant inflectional categories,
and their respective values. Each inflec-
tional category, such as case or tense, com-
bined with its value, such as nominative or
present, constitutes one sub-label. Consider,
for example, the split for the singular, adessive
noun P(N+CASE_ADE+NUM_SG) = {POS_N,
CASE_ADE, NUM_SG}.
The labeling scheme employed in the Multext-
East data set represents a considerably different
annotation approach compared to the Penn and
Turku Treebanks. Each morphological analysis is
a sequence of feature markers, for example Pw3?
r. The first feature marker (P) denotes word class
and the rest (w, 3, and r) encode values of inflec-
tional categories relevant for that word class. A
feature marker may correspond to several differ-
ent values depending on word class and its posi-
tion in the analysis. Therefore it becomes rather
difficult to split the labels into similar pairs of in-
flectional category and value as we are able to do
for the Turku Dependency Treebank. Since the in-
terpretation of a feature marker depends on its po-
sition in the analysis and the word class, the mark-
ers have to be numbered and appended with the
261
word class marker. For example, consider the split
P(Pw3?r) = {0 : P, 1 : Pw, 2 : P3, 5 : Pr}.
3.3 CRF Model Specification
We perform experiments using first-order and
second-order CRFs with zeroth-order and first-
order sub-label features. Using the notation
introduced in Section 2, the employed mod-
els are CRF(1,-), CRF(1,1), CRF(2,-), CRF(2,0),
and CRF(2,1). We do not report results us-
ing CRF(2,2) since, based on preliminary exper-
iments, this model overfits on all languages.
The CRF model parameters are estimated using
the averaged perceptron algorithm (Collins, 2002).
The model parameters are initialized with a zero
vector. We evaluate the latest averaged parameters
on the held-out development set after each pass
over the training data and terminate training if no
improvement in accuracy is obtained during three
last passes. The best-performing parameters are
then applied on the test instances.
We accelerate the perceptron learning using
beam search (Zhang and Clark, 2011). The beam
width, b, is optimized separately for each lan-
guage on the development sets by considering b =
1, 2, 4, 8, 16, 32, 64, 128 until the model accuracy
does not improve by at least 0.01 (absolute).
Development and test instances are decoded us-
ing Viterbi search in combination with the tag dic-
tionary approach of Ratnaparkhi (1996). In this
approach, candidate tags for known word forms
are limited to those observed in the training data.
Meanwhile, word forms that were unseen during
training consider the full label set.
3.4 Software and Hardware
The experiments are run on a standard desktop
computer (Intel Xeon E5450 with 3.00 GHz and
64 GB of memory). The methods discussed in
Section 2 are implemented in C++.
3.5 Results
The obtained tagging accuracies and training
times are presented in Table 2. The times in-
clude running the averaged perceptron algorithm
and evaluation of the development sets. The col-
umn labeled it. corresponds to the number of
passes over the training data made by the percep-
tron algorithm before termination. We summarize
the results as follows.
First, compared to standard feature extraction
approach, employing the sub-label transition fea-
tures resulted in improved accuracy on all lan-
guages apart from English. The differences were
statistically significant on Czech, Estonian, and
Finnish. (We establish statistical significance
(with confidence level 0.95) using the standard 1-
sided Wilcoxon signed-rank test performed on 10
randomly divided, non-overlapping subsets of the
complete test sets.) This results supports the in-
tuition that the sub-label features should be most
useful in presence of large, fine-grained label sets,
in which case the learning is most affected by data
sparsity.
Second, on all languages apart from English,
employing a first-order model with sub-label fea-
tures yielded higher accuracy compared to a
second-order model with standard features. The
differences were again statistically significant on
Czech, Estonian, and Finnish. This result suggests
that, compared to increasing model order, exploit-
ing the sub-label dependencies can be a preferable
approach to improve the tagging accuracy.
Third, applying the expanded feature set in-
evitably causes some increase in the computa-
tional cost of model estimation. However, as
shown by the running times, this increase is not
prohibitive.
4 Related Work
In this section, we compare the approach pre-
sented in Section 2 to two prior systems which at-
tempt to utilize sub-label dependencies in a similar
manner.
Smith et al (2005) use a CRF-based system
for tagging Czech, in which they utilize expanded
emission features similar to our (5). However, they
do not utilize the full expanded transition features
(6). More specifically, instead of utilizing a sin-
gle chain as in our approach, Smith et al employ
five parallel structured chains. One of the chains
models the sequence of word-class labels such as
noun and adjective. The other four chains model
gender, number, case, and lemma sequences, re-
spectively. Therefore, in contrast to our approach,
their system does not capture cross-dependencies
between inflectional categories, such as the de-
pendence between the word-class and case of ad-
jacent words. Unsurprisingly, Smith et al fail
to achieve improvement over a generative HMM-
based POS tagger of Haji
?
c (2001). Meanwhile,
our system outperforms the generative trigram tag-
ger HunPos (Hal?csy et al, 2007) which is an im-
262
model it. time (min) acc. OOV.
English
CRF(1, -) 8 9 97.04 88.65
CRF(1, 0) 6 17 97.02 88.44
CRF(1, 1) 8 22 97.02 88.82
CRF(2, -) 9 15 97.18 88.82
CRF(2, 0) 11 36 97.17 89.23
CRF(2, 1) 8 27 97.15 89.04
Romanian
CRF(1, -) 14 29 97.03 85.01
CRF(1, 0) 13 68 96.96 84.59
CRF(1, 1) 16 146 97.24 85.94
CRF(2, -) 7 19 97.08 85.21
CRF(2, 0) 18 99 97.02 85.42
CRF(2, 1) 12 118 97.29 86.25
Estonian
CRF(1, -) 15 28 93.39 78.66
CRF(1, 0) 17 66 93.81 80.44
CRF(1, 1) 13 129 93.77 79.37
CRF(2, -) 15 30 93.48 77.13
CRF(2, 0) 13 53 93.78 79.60
CRF(2, 1) 16 105 94.01 79.53
Czech
CRF(1, -) 6 28 89.28 70.90
CRF(1, 0) 10 112 89.94 74.44
CRF(1, 1) 10 365 90.78 76.83
CRF(2, -) 19 91 89.81 72.44
CRF(2, 0) 13 203 90.35 76.37
CRF(2, 1) 24 936 91.00 77.75
Finnish
CRF(1, -) 10 80 87.37 59.29
CRF(1, 0) 13 249 88.58 63.46
CRF(1, 1) 12 474 88.41 62.63
CRF(2, -) 11 106 86.74 56.96
CRF(2, 0) 13 272 88.52 63.46
CRF(2, 1) 12 331 88.68 63.62
Table 2: Results.
proved open-source implementation of the well-
known TnT tagger of Brants (2000). The obtained
HunPos results are presented in Table 3.
Eng Rom Est Cze Fin
HunPos 96.58 96.96 92.76 89.57 85.77
Table 3: Results using a generative HMM-based
HunPos tagger of Halacsy et al (2007).
Ceaus?u (2006) uses a maximum entropy
Markov model (MEMM) based system for tag-
ging Romanian which utilizes transitional behav-
ior between sub-labels similarly to our feature set
(6). However, in addition to ignoring the most in-
formative emission-type features (5), Ceaus?u em-
beds the MEMMs into the tiered tagging frame-
work of Tufis (1999). In tiered tagging, the full
morphological analyses are mapped into a coarser
tag set and a tagger is trained for this reduced tag
set. Subsequent to decoding, the coarser tags are
mapped into the original fine-grained morpholog-
ical analyses. There are several problems associ-
ated with this tiered tagging approach. First, the
success of the approach is highly dependent on a
well designed coarse label set. Consequently, it
requires intimate knowledge of the tag set and lan-
guage. Meanwhile, our model can be set up with
relatively little prior knowledge of the language
or the tagging scheme (see Section 3.2). More-
over, a conversion to a coarser label set is neces-
sarily lossy (at least for OOV words) and poten-
tially results in reduced accuracy since recovering
the original fine-grained tags from the coarse tags
may induce errors. Indeed, the accuracy 96.56, re-
ported by Ceaus?u on the Romanian section of the
Multext-East data set, is substantially lower than
the accuracy 97.29 we obtain. These accuracies
were obtained using identical sized training and
test sets (although direct comparison is impossible
because Ceaus?u uses a non-documented random
split).
5 Conclusions
We studied improving the accuracy of CRF-based
POS tagging by exploiting sub-label dependency
structure. The dependencies were included in the
CRF model using a relatively straightforward fea-
ture expansion scheme. Experiments on five lan-
guages showed that the approach can yield signif-
icant improvement in tagging accuracy given suf-
ficiently fine-grained label sets.
In future work, we aim to perform a more
fine-grained error analysis to gain a better under-
standing where the improvement in accuracy takes
place. One could also attempt to optimize the
compound label splits to maximize prediction ac-
curacy instead of applying a priori partitions.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the grant no
251170 (Finnish Centre of Excellence Program
(2012-2017)). We would like to thank the anony-
mous reviewers for their useful comments.
263
References
Thorsten Brants. 2000. Tnt: A statistical part-of-
speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing,
pages 224?231.
A. Ceausu. 2006. Maximum entropy tiered tagging.
In The 11th ESSLI Student session, pages 173?179.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002), vol-
ume 10, pages 1?8.
Toma?z Erjavec. 2010. Multext-east version 4: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10).
Jan Haji?c, Pavel Krbec, Pavel Kv?eto?n, Karel Oliva, and
Vladim?r Petkevi?c. 2001. Serial combination of
rules and statistics: A case study in czech tagging.
In Proceedings of the 39th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 268?
275.
Auli Hakulinen, Maria Vilkuna, Riitta Korhonen, Vesa
Koivisto, Tarja Riitta Heinonen, and Irja Alho.
2004. Iso suomen kielioppi. Suomalaisen Kirjal-
lisuuden Seura, Helsinki, Finland.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz.
2007. Hunpos: An open source trigram tagger. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 209?212.
Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Missil?,
Stina Ojala, Tapio Salakoski, and Filip Ginter. 2013.
Building the essential resources for Finnish: the
Turku Dependency Treebank. Language Resources
and Evaluation.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguis-
tics, 19(2):313?330.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the conference on empirical methods in natu-
ral language processing, volume 1, pages 133?142.
Philadelphia, PA.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of the Confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
475?482.
Dan Tufis. 1999. Tiered tagging and combined lan-
guage models classifiers. In Proceedings of the Sec-
ond International Workshop on Text, Speech and Di-
alogue, pages 28?33.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
264
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 38?45,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
A Method for Compiling Two-level Rules with Multiple Contexts 
 
 
Kimmo Koskenniemi 
University of Helsinki 
Helsinki, Finland  
kimmo.koskenniemi@helsinki.fi 
Miikka Silfverberg 
University of Helsinki 
Helsinki, Finland 
miikka.silfverberg@helsinki.fi 
 
  
 
Abstract 
A novel method is presented for compiling 
two-level rules which have multiple context 
parts. The same method can also be applied 
to the resolution of so-called right-arrow rule 
conflicts. The method makes use of the fact 
that one can efficiently compose sets of two-
level rules with a lexicon transducer. By in-
troducing variant characters and using simple 
pre-processing of multi-context rules, all 
rules can be reduced into single-context rules. 
After the modified rules have been combined 
with the lexicon transducer, the variant char-
acters may be reverted back to the original 
surface characters. The proposed method ap-
pears to be efficient but only partial evidence 
is presented yet.   
1 Introduction 
Two-level rules can be compiled into length-
preserving transducers whose intersection effec-
tively reflects the constraints and the correspon-
dences imposed by the two-level grammar. Two-
level rules relate input strings (lexical representa-
tions) with output strings (surface representa-
tions). The pairs of strings are treated as charac-
ter pairs x:z consisting of lexical (input) char-
acters x and surface (output) characters z, and 
regular expressions based on such pairs.  Two-
level rule transducers are made length-preserving 
(epsilon-free) by using a place holder zero (0) 
within the rules and in the representations.  The 
zero is then removed after the rules have been 
combined by (virtual) intersection, before the 
result is composed with the lexicon.  There are 
four kinds of two-level rules:  
1. right-arrow rules or restriction rules, 
(x:z => LC _ RC) saying that the 
correspondence pair is allowed only if 
immediately preceded by left context LC 
and followed by right context RC, 
2. left-arrow rules or surface coercion 
rules,  (x:z <= LC _ RC) which say 
that in this context, the lexical character 
x may  only  correspond  to  the  surface  
character z, 
3. double-arrow rules (<=>), a shorthand 
combining these two requirements, and 
4. exclusion rules (x:z /<= LC _ RC) 
which forbid the pair x:z  to occur in 
this context. 
All types of rules may have more than one 
context part. In particular, the right-arrow rule  
x:z => LC1 _ RC1; LC2 _ RC2 would 
say that the pair x:z (which we call the centre 
of the rule) may occur in either one of these two 
contexts.  For various formulations of two-level 
rules, see e.g. (Koskenniemi, 1983), (Grimley-
Evans et al, 1996), (Black et.al., 1987), (Ruess-
ink, 1989), (Ritchie, 1992), (Kiraz, 2001) and a 
comprehensive survey on their formal interpreta-
tions, see (Vaillette, 2004). 
Compiling two-level rules into transducers is 
easy in all other cases except for right-arrow 
rules with multiple context-parts; see e.g. 
Koskenniemi (1983). Compiling right-arrow 
rules with multiple context parts is more difficult 
because the compilation of the whole rule is not 
in a simple relation to the component expressions 
in the rule; see e.g. Karttunen et al (1987).   
The method proposed here reduces multi-
context rules into a set of separate simple rules, 
one for each context, by introducing some auxil-
iary variant characters.  These auxiliary charac-
ters are then normalized back into the original 
surface characters after the intersecting composi-
tion of the lexicon and the modified rules. The 
method is presented in section 3. The compila-
tion of multiple contexts using the proposed 
scheme appears to be very simple and fast.  Pre-
liminary results and discussion about the compu-
tational complexity are presented in section 4. 
38
1.1 The compilation task with an example 
We make use of a simplified linguistic example 
where a stop k is realized as v between identical 
rounded close vowels (u, y). The example re-
sembles one detail of Finnish consonant grada-
tion but it is grossly simplified. According to the 
rule in the example, the lexical representation 
pukun would be realized as the surface repre-
sentation puvun.  This correspondence is tradi-
tionally represented as: 
p u k u n 
p u v u n 
where the upper tier represents the lexical or 
morphophonemic representation which we inter-
pret as the input, and the lower one corresponds 
to the surface representation which we consider 
as the output. 1   This two-tier representation is 
usually represented on a single line as a sequence 
of input and output character pairs where pairs of 
identical characters, such as p:p are abbreviated 
as a single p.  E.g.  the above pair  of  strings  be-
comes a string of pairs:  
p u k:v u n 
In our example we require that the correspon-
dence k:v may occur only between two identi-
cal rounded close vowels, i.e. either between two 
letters u or between two letters y. Multiple con-
texts are needed in the right-arrow rule which 
expresses this constraint. As a two-level gram-
mar, this would be: 
Alphabet a b ? k ? u v w ? 
 k:v; 
Rules 
k:v => u _ u; 
       y _ y;  
This grammar would permit sequences such as:  
p u k:v u n 
k y k:v y n 
p u k:v u k:v u n 
l u k:v u n k y k:v y n 
t u k k u 
but it would exclude sequences: 
p u k:v y n 
t u k:v a n 
                                               
1 In Xerox terminology, the input or lexical characters 
are called the upper characters, and the output or sur-
face characters are called the lower characters. Other 
orientations are used by some authors. 
Whereas one can always express  multi-
context left-arrow rules (<=) and exclusion rules 
(/<=)  equivalently  as  separate  rules,  this  does  
not  hold  for  right-arrow rules.  The  two  separate  
rules 
k:v => u _ u; 
k:v => y _ y;  
would be in conflict with each other permitting 
no occurrences of k:v at all, (unless we apply 
so-called conflict resolution which would effec-
tively combine the two rules back to a single rule 
with two context parts). 
2 Previous compilation methods 
The first compiler of two-level rules was imple-
mented by the first author in 1985 and it handled 
also multi-context rules (Koskenniemi, 1985). 
The compiler used a finite-state package written 
by Ronald Kaplan and Martin Kay at Xerox 
PARC, and a variant of a formula they used for 
compiling cascaded rewrite rules. Their own 
work was not published until 1994. Kosken-
niemi?s compiler was re-implemented in LISP by 
a student in her master?s thesis (Kinnunen, 
1987).  
Compilation of two-level rules in general re-
quires  some  care  because  the  centres  may  occur  
several times in pair strings, the contexts may 
overlap and the centres may act as part of a con-
text for another occurrence of the same centre.  
For other rules than right-arrow rules, each con-
text is yet another condition for excluding un-
grammatical  strings  of  pairs,  which  is  how  the  
rules are related to each other.  The context parts 
of a right-arrow rule are, however, permissions, 
one of which has to be satisfied.  Expressing un-
ions of context parts was initially a problem 
which required complicated algorithms. 
Some of the earlier compilation methods are 
mentioned below. They all produce a single 
transducer out of each multi-context right-arrow 
rule. 
2.1 Method based on Kaplan and Kay 
Kaplan and Kay (1994) developed a method 
around 1980 for compiling rewriting rules into 
finite-state transducers 2 . The method was 
adapted by Koskenniemi to the compilation of 
two-level rules by modifying the formula 
                                               
2 Douglas Johnson (1972) presented a similar tech-
nique earlier but his work was not well known in 
early 1980s. 
39
slightly. In this method, auxiliary left and right 
bracket characters (<1, >1, <2, >2, ...) 
were freely added in order to facilitate the check-
ing of the context conditions.  A unique left and 
right bracket was dedicated for each context part 
of  the  rule.   For  each  context  part  of  a  rule,  se-
quences with freely added brackets were then 
filtered with the context expressions so that only 
such sequences remained where occurrences of 
the brackets were delimited with the particular 
left or right context (allowing free occurrence of 
brackets for other context parts). Thereafter, it 
was easy to check that all occurrences of the cen-
tre  (i.e.  the  left  hand  part  of  the  rule  before  the  
rule operator) were delimited by some matching 
pair of brackets. As all component transducers in 
this expression were length-preserving (epsilon-
free), the constraints could be intersected with 
each other resulting in a single rule transducer 
for the multi-context rule (and finally the brack-
ets could be removed). 
2.2 Method of Grimley-Evans, Kiraz and 
Pulman 
Grimley-Evans, Kiraz and Pulman presented a 
simpler compilation formula for two-level rules 
(1996).  The method is prepared to handle more 
than two levels of representation, and it does not 
need the freely added brackets in the intermedi-
ate  stages.   Instead,  it  uses  a  marker  for  the rule  
centre and can with it express disjunctions of 
contexts.  Subtracting such a disjunction from all 
strings where the centre occurs expresses all pair 
strings which violate the multi-context rule.  
Thus, the negation of such a transducer is the 
desired result. 
2.3 Yli-Jyr??s method 
Yli-Jyr? (Yli-Jyr? et al, 2006) introduced a 
concept of Generalized Restriction (GR) where 
expressions with auxiliary boundary characters  
made it possible to express context parts of rules 
in a natural way, e.g. as: 
Pi* LC  Pi  RC Pi*   
Here Pi is the set of feasible pairs of characters 
and LC and RC are the left and right contexts. 
The two context parts of our example would cor-
respond to the following two expressions: 
Pi* u  Pi  u Pi* 
Pi* y  Pi  y Pi*     
Using such expressions, it is easy to express dis-
junctions of contexts as unions of the above ex-
pressions. This makes it logically simple to com-
pile multi-context right-arrow rules. The rule 
centre x:z can be expressed simply as: 
Pi*  x:z  Pi* 
The right-arrow rule  can be expressed as  an im-
plication where the expression for the centre im-
plies the union of the context parts.  Thereafter, 
one may just remove the auxiliary boundary 
characters, and the result is the rule-transducer. 
(It is easy to see that only one auxiliary character 
is needed when the length of the centres is one.) 
The compilation of rules with centres whose 
length is one using the GR seems very similar to 
that  of  Grimley-Evans  et  al.   The  nice  thing  
about GR is that one can easily express various 
rule types, including but not limited to the four 
types listed above. 
2.4 Intersecting compose 
It was observed somewhere around 1990 at 
Xerox that the rule sets may be composed with 
the lexicon transducers in an efficient way and 
that the resulting transducer was roughly similar 
in size as the lexicon transducer itself (Karttunen 
et al, 1992). This observation gives room to the 
new approach presented below. 
At that time, it was not practical to intersect 
complete two-level grammars if they contained 
many elaborate rules (and this is still a fairly 
heavy operation).  Another useful observation 
was that the intersection of the rules could be 
done in a joint single operation with the compo-
sition (Karttunen, 1994).  Avoiding the separate 
intersection made the combining of the lexicon 
and rules feasible and faster.  In addition to 
Xerox LEXC program, e.g. the HFST finite-state 
software contains this operation and it is rou-
tinely used when lexicons and two-level gram-
mars are combined into lexicon transducers 
(Lind?n et al, 2009). 
M?ns Huld?n has noted (2009) that the com-
posing of the lexicon and the rules is sometimes 
a heavy operation, but can be optimized if one 
first composes the output side of the lexicon 
transducer with the rules, and thereafter the 
original lexicon with this intermediate result. 
3 Proposed method for compilation 
The  idea  is  to  modify  the  two-level  grammar  so  
that the rules become simpler. The modified 
grammar will contain only simple rules with sin-
gle context parts. This is done at the cost that the 
grammar will transform lexical representations 
into slightly modified surface representations.  
40
The surface representations are, however, fixed 
after the rules have been combined with the lexi-
con so that the resulting lexicon transducer is 
equivalent to the result produced using earlier 
methods. 
3.1 The method through the example 
Let us return to the example in the introduction. 
The modified surface representation differs from 
the ultimate representation by having a slightly 
extended alphabet where some surface characters 
are expressed as their variants, i.e. there might be 
v1 or v2 in addition to v.  In particular, the first 
variant v1 will  be  used  exactly  where  the  first  
context of the original multi-context rule for k:v 
is satisfied, and v2 where the second context is 
satisfied. After extending the alphabet and split-
ting  the  rule,  our  example  grammar  will  be  as  
follows: 
Alphabet a b ? k ? u v w x y ? 
  k:v1 k:v2; 
Rules 
k:v1 => u _ u; 
k:v2 => y _ y;  
These rules would permit sequences such as:  
p u k:v1 u n 
k y k:v2 y n 
p u k:v1 u k:v1 u n  
but exclude a sequence  
p u k:v2 u n    
The output of the modified grammar is now as 
required, except that it includes these variants v1 
and v2 instead of v.   If  we first  perform the in-
tersecting composition of the rules and the lexi-
con, we then can compose the result with a trivial 
transducer which simply transforms both v1 and 
v2 into v. 
It  should  be  noted  that  here  the  context  ex-
pressions of these example rules do not contain v 
on the output side, and therefore the introduction 
of the variants v1 and v2 causes no further 
complications.   In  the  general  case,  the  variants  
should be added as alternatives of v in  the  con-
text expressions, see the explanation below. 
3.2 More general cases 
The strategy is to pre-process the two-level 
grammar in steps by splitting more complex con-
structions into simpler ones until we have units 
whose components are trivial to compile.  The 
intersection of the components will have the de-
sired effect when composed with a lexicon and a 
trivial correction module.   Assume, for the time 
being, that all centres (i.e. the left-hand parts) of 
the rules are of length one. 
(1) Split double-arrow (<=>) rules into one 
right-arrow (=>)  rule  and  one  left-arrow  (<=) 
rule with centres and context parts identical to 
those of the original double-arrow rule. 
(2) Unfold the iterative where clauses in left-
arrow rules by establishing a separate left-arrow 
rule for each value of the iterator variable, e.g. 
V:Vb <= [a | o | u] ?* _; 
 where V in (A O U) 
       Vb in (a o u) matched; 
becomes 
A:a <= [a | o | u] ?* _; 
O:o <= [a | o | u] ?* _; 
U:u <= [a | o | u] ?* _; 
Unfold the where clauses in right-arrow rules 
in  either  of  the  two  ways:  (a)  If  the  where 
clauses create disjoint centres (as above), then 
establish a separate right-arrow rule for each 
value  of  the  variable,  and  (b)  if  the  clause  does  
not affect the centre, then create a single multi-
context right-arrow rule whose contexts consist 
of the context parts of the original rule by replac-
ing the where clause variable by its values, one 
value at a time, e.g. 
k:v => Vu _ Vu; where Vu in (u y); 
becomes 
k:v => u _ u; 
       y _ y; 
If  there  are  set  symbols  or  disjunctions  in  the  
centres  of  a  right-arrow  rule,  then  split  the  rule  
into separate rules where each rule has just a sin-
gle pair as its centre, and the context part is iden-
tical to the context part (after the unfolding of the 
where clauses). 
Note that these two first steps would probably 
be common to any method of compiling multi-
context rules.  After these two steps, we have 
right-arrow, left-arrow and exclusion rules.  The 
right-arrow  rules  have  single  pairs  as  their  cen-
tres. 
(3) Identify the right-arrow rules which, after 
the unfolding, have multiple contexts, and record 
each  pair  which  is  the  centre  of  such  a  rule.   
Suppose that the output character (i.e. the surface 
character) of such a rule is z and there are n con-
text parts in the rule, then create n new auxiliary 
characters z1, z2, ..., zn  and denote the set consist-
ing of them by S(z).  
41
Split the rule into n distinct single-context 
right-arrow rules by replacing the z of the centre 
by each zi in turn. 
Our simple example rule becomes now. 
k:v1 => u _ u; 
k:v2 => y _ y; 
(4) When all rules have been split according to 
the above steps, we need a post-processing phase 
for the whole grammar. We have to extend the 
alphabet by adding the new auxiliary characters 
in it. If original surface characters (which now 
have variants) were referred to in the rules, each 
such reference must be replaced with the union 
of the original character and its variants. This 
replacement has to be done throughout the  
grammar. For any existing pairs x:z listed in the 
alphabet, we add there also the pairs x:z1, ..., x:zn.  
The same is done for all declarations of sets 
where z occurs  (as  an output  character).  Insert  a  
declaration for a new character set corresponding 
to S(z).  In  all  define  clauses  and  in  all  rule-
context expressions where z occurs as an output 
character,  it  is  replaced  by  the  set  S(z).   In  all  
centres of left-arrow rules where z occurs  as  the 
output character, it is replaced by S(z).  
The  purpose  of  this  step  is  just  to  make  the  
modified two-level grammar consistent in terms 
of its alphabet, and to make the modified rules 
treat  the occurrence of  any of  the output  charac-
ters z1,  z2,  ?, zn in the same way as the original 
rule treated z wherever it occurred in its contexts. 
 
After this pre-processing we only have right-
arrow, left-arrow and exclusion rules with a sin-
gle context part.  All rules are independent of 
each  other  in  such  a  way  that  their  intersection  
would have the effect we wish the grammar to 
have.  Thus, we may compile the rule set as such 
and  each  of  these  simple  rules  separately.   Any  
of the existing compilation formulas will do. 
After compiling the individual rules, they have 
to be intersected and composed with the lexicon 
transducer which transforms base forms and in-
flectional feature symbols into the morphopho-
nemic representation of the word-forms. The 
composing and intersecting is efficiently done as 
a single operation because it then avoids the pos-
sible explosion which can occur if intermediate 
result of the intersection is computed in full.   
The rules are mostly independent of each 
other, capable of recurring freely. Therefore 
something near the worst case complexity is 
likely  to  occur,  i.e.  the  size  of  the  intersection  
would have many states, roughly proportional to 
the  product  of  the  numbers  of  the  states  in  the  
individual rule transducers. 
The composition of the lexicon and the logical 
intersection of the modified rules is almost iden-
tical  to  the  composition  of  the  lexicon  and  the  
logical intersection of the original rules. The only 
difference is that the output (i.e. the surface) rep-
resentation contains some auxiliary characters zi 
instead of the original surface characters z. A 
simple transducer will correct this. (The trans-
ducer has just one (final) state and identity transi-
tions for all original surface characters and a re-
duction zi:z for  each of  the auxiliary characters.)   
This composition with the correcting transducer 
can be made only after the rules have been com-
bined with the lexicon.   
3.3 Right-arrow conflicts 
Right-arrow rules are often considered as per-
missions.   A  rule  could  be  interpreted  as  ?this  
correspondence pair may occur if the following 
context condition is met?.  Further permissions 
might  be  stated  in  other  rules.  As  a  whole,  any  
occurrence must get at least one permission in 
order to be allowed.  
The right-arrow conflict resolution scheme 
presented by Karttunen implemented this 
through an extensive pre-processing where the 
conflicts were first detected and then resolved 
(Karttunen et al, 1987). The resolution was done 
by copying context parts among the rules in con-
flict.  Thus, what was compiled was a grammar 
with rules extended with copies of context parts 
from other rules.  
The scenario outlined above could be slightly 
modified in order to implement the simple right-
arrow rule  conflict  resolution  in  a  way  which  is  
equivalent to the solution presented by Kart-
tunen.   All  that  is  needed is  that  one would first  
split the right-arrow rules with multiple context 
parts into separate rules.  Only after that, one 
would consider all right-arrow rules and record 
rules  with identical  centres.   For  groups of  rules  
with identical centres, one would introduce the 
further variants of the surface characters, a sepa-
rate  variant  for  each  rule.   In  this  scheme,  the  
conflict resolution of right-arrow rules is imple-
mented fairly naturally in a way analogous to the 
handling of multi-context rules. 
3.4 Note on longer centres in rules 
In the above discussion, the left-hand parts of 
rules,  i.e.  their  centres,  were  always  of  length  
one.  In fact, one may define rules with longer 
centres by a scheme which reduces them into 
42
rules with length one centres.  It appears that the 
basic rule types (the left and right-arrow rules) 
with longer centres can be expressed in terms of 
length one centres, if we apply conflict resolution 
for the right-arrow rules. 
We replace a right-arrow rule, e.g. 
x1:z1 x2:z2 ... xk:zk => LC _ RC; 
with k separate rules 
x1:z1 => LC _ x2:z2 ... xk:zk RC; 
x2:z2 => LC x1:z1 _ ... xk:zk RC; 
... 
xk:zk => LC x1:z1 x2:z2 ... _ RC; 
Effectively, each input character may be realized 
according  to  the  original  rule  only  if  the  rest  of  
the  centre  will  also  be  realized  according  to  the  
original rule.  
Respectively, we replace a left-arrow rule, e.g. 
x1:z1 x2:z2 ... xk:zk <= LC _ RC; 
with k separate rules 
x1:z1 <= LC _ x2: ... xk: RC; 
x2:z2 <= LC x1: _ ... xk: RC; 
... 
xk:zk <= LC x1: x2: ... _ RC; 
Here the realization of the surface string is forced 
for each of its character of the centre separately, 
without reference to what happens to other char-
acters  in  the  centre.   (Otherwise  the  contexts  of  
the separate rules would be too restrictive, and 
allow the default realization as well.) 
 
4 Complexity and implementation 
In order to implement the proposed method, one 
could write a pre-processor which just transforms 
the  grammar  into  the  simplified  form,  and  then  
use an existing two-level compiler. Alternatively, 
one could modify an existing compiler, or write a 
new compiler which would be somewhat simpler 
than the existing ones. We have not implemented 
the proposed method yet, but rather simulated the 
effects using existing two-level rule compilers. 
Because the pre-processing would be very fast 
anyway, we decided to estimate the efficiency of 
the proposed method through compiling hand-
modified rules with the existing HFST-TWOLC 
(Lind?n et al, 2009) and Xerox TWOLC3 two-
                                               
3 We used an old version 3.4.10 (2.17.7) which we 
thought would make use of the Kaplan and Kay for-
mula.  We suspected that the most recent versions 
might have gone over to the GR formula. 
level rule compilers. The HFST tools are built on 
top of existing open source finite-state packages 
OpenFST (Allauzen et al, 2007) and Helmut 
Schmid?s SFST (2005).  
It appears that all normal morphographemic 
two-level grammars can be compiled with the 
methods of Kaplan and Kay, Grimley-Evans and 
Yli-Jyr?. 
Initial tests of the proposed scheme are prom-
ising.  The  compilation  speed  was  tested  with  a  
grammar of consisting of 12 rules including one 
multi-context rule for Finnish consonant grada-
tion with some 8 contexts and a full Finnish lexi-
con. When the multi-context rule was split into 
separate rules, the compilation was somewhat 
faster (12.4 sec) to than when the rule was com-
piled a multi-context rule using the GR formula 
(13.9 sec).  The gain in the speed by splitting was 
lost at the additional work needed in the inter-
secting compose of the rules and the full lexicon 
and the final fixing of the variants. On the whole, 
the proposed method had no advantage over the 
GR method. 
In  order  to  see  how  the  number  of  context  
parts affects the compilation speed, we made 
tests with an extreme grammar simulating Dutch 
hyphenation rules. The hyphenation logic was 
taken out of TeX hyphenation patterns which had 
been converted into two-level rules. The first 
grammar consisted of a single two-level rule 
which had some 3700 context parts. This gram-
mar could not be compiled using Xerox TWOLC 
which applies the Kaplan and Kay method be-
cause more than 5 days on a dedicated Linux 
machine with 64 GB core memory was not 
enough for completing the computation.  When 
using  of  GR  method  of  HFST-TWOLC,  the  
compilation time was not a problem (34 min-
utes).  The method of Grimley-Evans et al 
would probably have been equally feasible.  
Compiling the grammar after splitting it into 
separate rules as proposed above was also feasi-
ble: about one hour with Xerox TWOLC and 
about 20 hours with HFST-TWOLC. The differ-
ence between these two implementations de-
pends most likely on the way they handle alpha-
bets.  The Xerox tool makes use of a so-called 
'other' symbol which stands for characters not 
mentioned in the rule. It also optimizes the com-
putation by using equivalence classes of charac-
ter pairs.  These make the compilation less sensi-
tive to the 3700 new symbols added to the alpha-
bet than what happens in the HFST routines.    
Another test was made using a 50 pattern sub-
set of the above hyphenation grammar.  Using 
43
the  Xerox  TWOLC,  the  subset  compiled  as  a  
multi-context rule in 28.4 seconds, and when 
split according to the method proposed here, it 
compiled in 0.04 seconds. Using the HFST-
TWOLC, the timings were 3.1 seconds and 5.4 
seconds, respectively.   These results corroborate 
the intuition that the Kaplan and Kay formula is 
sensitive  to  the  number  of  context  parts  in  rules  
whereas  the  GR  formula  is  less  sensitive  to  the  
number of context parts in rules. 
There are factors which affect the speed of 
HFST-TWOLC, including the implementation 
detail including the way of treating characters or 
character pairs which are not specifically men-
tioned in a particular transducer. We anticipate 
that there is much room for improvement in 
treating larger alphabets in HFST internal rou-
tines and there is no inherent reason why it 
should be slower than the Xerox tool. The next 
release of HFST will use Huld?n?s FOMA finite-
state package. FOMA implements the ?other? 
symbol and is expected to improve the process-
ing of larger alphabets. 
Our intuition and observation is that the pro-
posed compilation phase requires linear time 
with  respect  to  the  number  of  context  parts  in  a  
rule. Whether the proposed compilation method 
has an advantage over the compilation using the 
GR  or  Grimley-Evans  formula  remains  to  be  
seen. 
5 Acknowledgements 
Miikka Silfverberg, a PhD student at Finnish graduate 
school Langnet and the author of HFST-TWOLC 
compiler. His contribution consists of making all tests 
used here to estimate and compare the efficiency of 
the compilation methods.   
The current work is part of the FIN-CLARIN infra-
structure project at the University of Helsinki funded 
by the Finnish Ministry of Education. 
References 
Cyril Allauzen, Michael Riley, Johan Schalkwyk, 
Wojciech Skut and Mehryar Mohri. 2007. 
OpenFst: A General and Efficient Weighted Finite-
State Transducer Library. In Implementation and 
Application of Automata, Lecture Notes in Com-
puter Science. Springer, Vol. 4783/2007, 11-23. 
Alan Black, Graeme Ritchie, Steve Pulman, and Gra-
ham Russell. 1987. ?Formalisms for morphogra-
phemic description?. In Proceedings of the Third 
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, 11?18. 
Edmund Grimley-Evans, Georg A. Kiraz, Stephen G. 
Pulman. 1996. Compiling a Partition-Based Two-
Level Formalism. In COLING 1996, Volume 1: 
The 16th International Conference on Computa-
tional Linguistics, pp. 454-459. 
Huld?n, M?ns. 2009. Finite-State Machine Construc-
tion Methods and Algorithms for Phonology and 
Morphology. PhD Thesis, University of Arizona. 
Douglas C. Johnson. 1972. Formal Aspects of Phono-
logical Description. Mouton, The Hague. 
Ronald M. Kaplan and Martin Kay. 1994. Regular 
Models of Phonological Rule Systems. Computa-
tional Linguistics 20(3): 331?378. 
Lauri Karttunen. 1994. Constructing lexical transduc-
ers. In Proceedings of the 15th conference on 
Computational linguistics, Volume 1. pp. 406-411. 
Lauri Karttunen, Ronald M. Kaplan, and Annie Zae-
nen. 1992. Two-Level Morphology with Composi-
tion. Proceedings of the 14th conference on Com-
putational linguistics, August 23-28, 1992, Nantes, 
France. 141-148. 
Lauri Karttunen, Kimmo Koskenniemi, and Ronald. 
M. Kaplan. 1987. A Compiler for Two-level Pho-
nological Rules. In Dalrymple, M. et al Tools for 
Morphological Analysis. Center for the Study of 
Language and Information. Stanford University. 
Palo Alto.  
Maarit Kinnunen. 1987. Morfologisten s??nt?jen 
k??nt?minen ??rellisiksi automaateiksi. (Translat-
ing morphological rules into finite-state automata. 
Master?s thesis.). Department of Computer Sci-
ence, University of Helsinki 
George Anton Kiraz. 2001. Computational Nonlinear 
Morphology: With Emphasis on Semitic Lan-
guages. Studies in Natural Language Processing. 
Cambridge University Press, Cambridge. 
Kimmo Koskenniemi. 1983. Two-Level Morph-
ology: A General Computational Model for 
Word-form Recognition and Production. Uni-
versity of Helsinki, Department of General Lin-
guistics, Publications No. 11. 
Kimmo Koskenniemi. 1985. Compilation of automata 
from morphological two-level rules. In F. Karlsson 
(ed.), Papers from the fifth Scandinavian Confer-
ence of Computational Linguistics, Helsinki, De-
cember 11-12, 1985. pp. 143-149. 
Krister Lind?n, Miikka Silfverberg and Tommi Piri-
nen. 2009. HFST Tools for Morphology ? An Effi-
cient Open-Source Package for Construction of 
Morphological Analyzers. In State of the Art in 
Computational Morphology (Proceedings of Work-
shop on Systems and Frameworks for Computa-
tional Morphology, SFCM 2009). Springer. 
Graeme Ritchie. 1992. Languages generated by two-
level morphological rules?. Computational Lin-
guistics, 18(1):41?59. 
44
H. A. Ruessink. 1989. Two level formalisms. Utrecht 
Working Papers in NLP. Technical Report 5. 
Helmut Schmid. 2005. A Programming Language for 
Finite State Transducers. In Proceedings of the 5th 
International Workshop on Finite State Methods in 
Natural Language Processing (FSMNLP 2005). 
pp. 50-51. 
Nathan Vaillette. 2004. Logical Specification of Fi-
nite-State Transductions for Natural Language 
Processing. PhD Thesis, Ohio State University. 
Anssi Yli-Jyr? and Kimmo Koskenniemi. 2006. 
Compiling Generalized Two-Level Rules and 
Grammars.  International Conference on NLP: Ad-
vances in natural language processing. Springer. 
174 ? 185. 
 
45
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 55?59,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Implementation of replace rules using preference operator 
Senka Drobac, Miikka Silfverberg, and Anssi Yli-Jyr?  University of Helsinki Department of Modern Languages Unioninkatu 40 A FI-00014 Helsingin yliopisto, Finland {senka.drobac, miikka.silfverberg, anssi.yli-jyra}@helsinki.fi  Abstract 
We explain the implementation of replace rules with the .r-glc. operator and preference relations. Our modular approach combines various preference constraints to form differ-ent replace rules. In addition to describing the method, we present illustrative examples. 1 Introduction The idea of HFST - Helsinki Finite-State Technol-ogy (Lind?n et al 2009, 2011) is to provide open-source replicas of well-known tools for building morphologies, including XFST (Beesley and Kart-tunen 2003). HFST's lack of replace rules such as those supported by XFST, prompted us to imple-ment them using the present method, which repli-cates XFST's behavior (with minor differences which will be detailed in later work), but will also allow easy expansion with new functionalities. The semantics of replacement rules mixes con-textual conditions with replacement strategies that are specified by replace rule operators. This paper describes the implementation of replace rules using a preference operator, .r-glc., that disambiguates alternative replacement strategies according to a preference relation. The use of preference relations (Yli-Jyr? 2008b) is similar to the worsener rela-tions used by Gerdemann (2009). The current ap-proach was first described in Yli-Jyr? (2008b), and is closely related to the matching-based finite-state approaches to optimality in OT phonology (Noord and Gerdemann 1999; Eisner 2000). The prefer-ence operator, .r-glc., is the reversal of generalized lenient composition (glc), a preference operator construct proposed by J?ger (2001). The imple-mentation is developed using the HFST library, and is now a part of the same. 
The purpose of this paper is to explain a general method of compiling replace rules with .r-glc. operator and to show how preference constraints described in Yli-Jyr? (2008b) can be combined to form different replace rules. 2 Notation The notation used in this paper is the standard reg-ular expression notation extended with replace rule operators introduced and described in Beesley and Karttunen (2003). In a simple rule ? ??? ?? ???? ??? ?_ ???,? , ?? ?_ ??? op is a replace rule operator such as: ?, ? , ?@?, ?@>, ?, (?), ?; ? ?  ???  is the set of patterns in the input text that are overwritten in the output text by the alternative patterns, which are given as set ? ?  ???, where ?? is a universal language and ?? set of alphabetical symbols; ?? and ?? are left and right contexts and dir is context direction (||, //, \\ and \/). Rules can also be parallel. Then they are divid-ed with double comma (,,), or alternately with sin-gle comma if context is not specified. Operation Name X Y The concatenation of Y after X X | Y The disjunction of X and Y X:Y The cross product of X and Y, where X and Y denote languages X .o. Y The composition of X and Y, where X and Y denote relations X+ The Kleene plus X* The Kleene star proj1(X) The projection of the input lan-guage of the relation X proj2(X) The projection of the output lan-guage of the relation X Table 1 ? List of operations 
55
Operators used in the paper are listed in Table 1, where X and Y stand for regular expressions. Additionally, parenthesis ( ) are used to mark optionality, squared brackets [ ] for precedence and question mark ? is used to denote set ? in regular expressions. 3 Method The general idea for compiling replace rules with the .r-glc. operator and preference constraints is shown in Figure 1. 
 Figure 1: General method of building a replace rule  The method consists of the following steps: 1. Building an Unconstrained Bracketed Transducer (UBT) ? a transducer which applies or skips contextually valid re-placements freely in all possible portions of the inputs.  Every application of the re-placement rule is marked with special brackets. Similar replace rules that differ only with respect to their replacement strategies will use the same UBT. Thus, the compilation of UBT is independent of the replacement strategy, which increases the modularity of the compilation algo-rithm. 2. Implement the functionality of the replace rule operator by constraining the UBT with the respective preference relation. 3. Remove brackets from the transducer.  The major advantage of this method is its mod-ularity. The algorithm is divided into small com-ponents which are combined in the desired way. This approach allows every part of the algorithm to be separately and clearly defined, tested and 
changed. Furthermore, modularity makes it possi-ble to easily integrate new functionalities such as weighted replace rules or two level contexts. 3.1 Unconstrained Bracketed Transducer As mentioned earlier, it is first necessary to build the UBT. This step can be seen as a variant of Yli-Jyr? and Koskenniemi's (2007) method for compil-ing contextually restricted changes in two-level grammars.  The main difference now is that the rule applications cannot overlap because they will be marked with brackets.  Step 1: Bracketed center The first step is to create a bracketed center, ???????  ? the replace relation surrounded by brackets , . For optional replacement, it is nec-essary that ??????? also contains the upper side of the relation bracketed with another pair of brackets ?? = , . This is necessary for filtering out all the results without any brackets (see later filter ???? ?) and getting non optional replacement.  ??????? = ?? ? ?? ? ?????  ? In case of parallel replace rules, bracketed cen-ter is the union of all individual bracketed centers. Like XFST, this implementation requires parallel replace rules to have the same replace operator (and optionality) in all replacements. Step 2: The change centers in free context The second step is to expand bracketed center to be valid in any context. If ? = , , ,  ?, we can define: ? = ? ? ? ? ??????? ? Then, center in free context is: ??????? ? = ? ? ?  ????????  ?? where ? is diamond, which is used to align centers and contexts during compilation. Step 3: Expanded center in context The next step is to compile contexts. The method used for constructing ????????????  depends on whether the context must match on the upper or the lower side. Since it is possible to have multiple contexts, each replacement should be surrounded with all applicable contexts: ?????????? = ??| ??? ?|? | ??? Center surrounded with one context is: ?? = ? ?| ???  ??  ????????  ? ? ?| ??? , 
Remove brackets .r-glc. 
.r-glc. ? .r-glc. 
UBT 
Constraint 1 
Constraint N 
REPLACE RULE 
56
where ? and ? are left and right contexts from the replace rule, and ?? and ?? are expanded contexts, depending on which side the context matches. In the case when context must match on the upper side, ?? and ?? are: ?? = ? ?? ? B ?  ?. o. ? ?? ?? = ? ?? ? B  ?. o. ? ?? If they must match on the lower side: ?? = ?. o. ? ?? ? B  ? ?? = ?. o. ? ?? ? B  where brackets are freely inserted (?) in the con-texts and then composed with ?.  In this example:  ? ? ?  ?? ? ? ? ?_ ?? both contexts should match on the upper side of the replacement, so ??????????  is: ?? = ? ?c ? B ?  ?. o. ? ?? ?? = d ?? ? B  ?. o. ? ?? ?? = ( ?? ?| ???)  ? ??  ? ? ? ? ? ?  ?( ?? ?| ???) ?????????? = ?? This way of compiling contexts allows every rule in a parallel replace rule to have its own con-text direction (||, //, \\, \/). Therefore, rules like the following one are valid in this implementation:  ? ? ?  ?? ?\\ ?? ?_ ?? ?, , ? ? ?  ?? ?// ?? ?_ ?? Steps 4: Final operations Finally, to get the unconstrained replace transducer it is necessary to subtract ??????????  from ??????? ? , remove diamond and do a negation of that relation. Let ? = ? ? ? ? ??  ?? ??????? ?, then: ? ? ? ? ??  ???????? ?  ??  ???????????  ?  where ? ?? denotes removal of diamond. 3.2 Constraints All the preference constraints were defined in Yli-Jyr? (2008), but since they were mostly difficult to interpret and implement, here is the list of the con-straints written with regular expressions over the set of finite binary relations. First, let us define RP ? a regular expression of-ten used in the restraints: ?? =  ?  ?  ??: 0 ?  ?0:? ?  ???? ? ? The left most preference is achieved by: ?? = ? ???<: 0 ? ?: 0 ? ?  ???  ??? ? Right most preference: ?? =  ??? ? ??? ? ? ? ? ? 0 ??? ? ? Longest match left to right: 
?? =  ? ???  ?| ? 0:  ? 0:  ?  ?  ??0 ?  ?? ?] ??? ? ?? ??? = ???  ?  ? ? ? ?  ??? ? ? ? ?0 ?: ?  ? ???  ??? Longest match right to left: ?? =  ? ???  ?| ??? ? ? 0:  ? 0 ?  ?  ? ? ? 0  ?? ?] ????? =  ??? ???  ?0 ?  ? ? ?  ??? ? ? ?  ??? ? Shortest match left to right: ?? =  ? ???  ?| ?  ?0 ?  ?  ? ? ? 0 ?  ? ?0 ?  ?? ?] ??? ? ?? ???? = ???  ?  ? ? ? ?  ??? ? ? ? ? ? ? 0 ? ???  ??? Shortest match right to left: ?? =  ? ???  ?| ? ??? ? ?  ?0 ?  ?  ? ? ? 0 ?  ? ?0 ?  ?? ?] ?????? =  ??? ???  ?  ?: 0 ? ?  ??? ? ?  ? ? ??? For compiling epenthesis rules, to avoid more than one epsilon in the row: ?? =  ?,  ?  ?? =  ? ,  ?    ??  ? ?? ? = ???  ?????????  ???  For non-optional replacements: ???? ? = ???  ?[??: 0 ? ? ??? ? ???: 0 ??? ]? To remove paths containing ??, where ?? =  ? , : ?? ? ? = ??? ?? ??? Since ??  ? ?? ?  and ?? ? ?  are reflexive, they are not preference relation. Instead, they are filters applied after preference relations. 3.3 Applying constraints with .r-glc. operator To apply a preference constraint in order to restrict transducer t, we use .r-glc. operator. The .r-glc. operation between transducer t and a constraint is shown in Figure 2. Input language of a transducer is noted as proj1 and output language as proj2. 
 Figure 2: Breakdown of the operation: t .r-glc. constraint Contraints combinations As shown in Figure 1, in order to achieve desired replace rules, it is often necessary to use several constraints. For example, to achieve left to right longest match, it is necessary to combine ??  and 
.o. 
 
t 
proj1(t)   ? proj2 proj1(t) .o. constraint .o. proj1(t)  
57
?? ??? . If the same longest match contains epen-thesis, ??  ? ??  constraint should also be used. 3.4 Removing brackets Removing brackets is simply achieved by applying ?? ? = ??? ? ??? constraint, where B is set of brack-ets we want to remove. Additionally, in HFST implementation, it is also required to remove the brackets from the transducers alphabets.  4 Examples Let us show how the replace rule is compiled on different examples.  Since it would take too much space to show whole transducers, we will show only output of the intermediate results applied to an input string.  The first example shows how to achieve a non- optional replacement. Intermediate results of the replace rule ? ?  ?? ?|| ?? ?_ ?? is shown in the Table 2. Since the arrow demands non-optional replace-ment, the unconstrained bracketed replace, if ap-plied to the input string ? ?? ?? , contains three possible results. The first result is the input string itself, which would be part of the non-optional replacement. The second result is necessary to filter out the first one. In this example, because of the restricting context, replacement is possible only in the middle, and therefore, it is bracketed with special brackets. Finally, the third result contains the bracketed replace relation. ? ?  ?? ?|| ?? ?_ ?? UBT ???? ? ?? ? ? ? ?? ? ?? ? ? ?  ?? ? ?: ?  ?? ? ?: ?  ?? ? ? ?  ??  ? ?: ?  ??  Table 2: Steps of the non optional replacement  Once when we have the unconstrained bracket-ed replace transducer, we are ready to apply filters. First filter, ???? ? will filter out all results that contain smaller number of brackets in every posi-tion, without making difference to the type of brackets. In this example, it will filter out the first result, the one that does not have any brackets at all. The second filter, ?? ? ? will filter out all the results containing ??brackets because they don?t contain the replace relation. Finally, to get the final 
result, it is necessary to remove brackets from the relation.  Following examples will be shown on the input string ? ?? ?? ??. Table 3 shows steps of building left to right longest match and Table 4 left to right shortest match. Both longest match and shortest match have the same first two steps. After building Unconstrained Bracketed Replace, we apply ??  filter which finds all the results with left most brackets in every position and filters out all the rest. This contraints characteristic filters out the results without the brackets as well, so the result will be non-optional. In order to get the longest match, we apply another filter (?? ??? ) to the result of the left most filter. This filter finds the longest of the bracketed matches with the same starting position. In the final step, if we apply filter ?? ????  instead of ?? ??? , we will get the shortest match (Table 4). ?+ ?@? ?? ?|| ?? ?_ ?? UBT ??  ?? ???  ? ?? ?? ?? ? ?: ?  ?? ?? ? ?: ? ?: ?  ?? ? ?: ? ??: ?  ?? ? ?? ? ?: ?  ?? 
? ?: ?  ?? ?? ? ?: ? ?: ?  ?? ? ?: ? ??: ?  ??  
? ?: ? ??: ?  ??  
Table 3: Left to right longest match  ?+ ?@> ?? ?|| ?? ?_ ?? UBT ??  ?? ????  ? ?? ?? ?? ? ?: ?  ?? ?? ? ?: ? ?: ?  ?? ? ?: ? ??: ?  ?? ? ?? ? ?: ?  ?? 
? ?: ?  ?? ?? ? ?: ? ?: ?  ?? ? ?: ? ??: ?  ??  ? ?: ? ?: ?  ??  Table 4: Left to right shortest match 5 Conclusion The large number of different replace operators makes it quite complicated and error-prone to build a supporting framework for them. However, the .r-glc. operator and preference relations allow split-ting the algorithm into small reusable units which are easy to maintain and upgrade with new func-tionalities. The replace rules are now part of the HFST li-brary and can be used through hfst-regexp2fst command line tool, but there is still some work to 
58
be done to build an interactive interface. Addition-ally, we are planning to add support for two level contexts and parallel weighted rules.  Acknowledgments 
The research leading to these results has received funding from the European Commission?s 7th Framework Program under grant agreement n? 238405 (CLARA). References Beesley, K.R., Karttunen, L.: Finite State Morphology. CSLI publications (2003) Eisner, J.: Directional constraint evaluation in optimali-ty theory. In: 20th COLING 2000, Proceedings of the Conference, Saarbr?cken, Germany (2000) 257?263 Gerdemann, D. (2009). Mix and Match Replacement Rules. Proceedings of the Workshop on RANLP 2009 Workshop on Adaptation of Language Re-sources and Technology to New Domains, Borovets, Bulgaria, 2011, pages 39-47. Gerdemann, D., van Noord, G.: Approximation and exactness in Finite-State Optimality Theory. In Eis-ner, J., Karttunen, L., Th?riault, A., eds.: SIGPHON 2000, Finite State Phonology. (2000) Gerdemann, D., van Noord, G.: Transducers from re-write rules with backreferences. In: 9th EACL 1999, Proceedings of the Conference. (1999) 126?133 J?ger, G.: Gradient constraints in Finite State OT: The unidirectional and the bidirectional case. In: Proceed-ings of FSMNLP 2001, an ESSLLI Workshop, Hel-sinki (2001) (35?40) Karttunen, L.: The replace operator. In: 33th ACL 1995, Proceedings of the Conference, Cambridge, MA, USA (1995) 16?23 Karttunen, L.: Directed replace operator. In Roche, E., Schabes, Y., eds.: Finitestate language processing, Cambridge, Massachusetts, A Bradford Book. The MIT Press (1996) 117?147 Kempe, A., Karttunen, L.: Parallel replacement in finite state calculus. In: 16th COLING 1996, Proc. Conf. Volume 2., Copenhagen, Denmark (1996) 622?627 Lind?n, K., Axelson, E., Hardwick, S., Silfverberg, M., Pirinen, T.: HFST - Framework for Compiling and Applying Morphologies, Communications in Com-puter and Information Science, vol. 100, pp. 67-85. Springer Berlin Heidelberg (2011) Lind?n, K., Silfverberg, M., Pirinen, T.: Hfst tools for morphology - an efficient open-source package for construction of morphological analyzers. In: Mahlow, C., Pietrowski, M. (eds.) State of the Art in Computational Morphology. Communications in 
Computer and Information Science, vol. 41, pp. 28-47. Springer Berlin Heidelberg (2009) Yli-Jyr?, A., Koskenniemi, K.: A new method for com-piling parallel replacement rules. In Holub, J., ?d?rek, J., eds.: Implementation and Application of Automata, 12th International Conference, CIAA 2007, Revised Selected Papers. Volume 4783 of LNCS., Springer (2007) 320?321 Yli-Jyr?, A.: Applications of Diamonded Double Nega-tion. In Finite-state methods and natural language processing. Thomas Hanneforth and Kay-Michael W?rtzner. 6th International Workshop, FSMNLP 2007. Potsdam, Germany, September 14-16. Revised Papers. Universit?tsverlag Potsdam (2008a) 6-30 Yli-Jyr?, A., Transducers from Parallel Replace Rules and Modes with Generalized Lenient Composition. In Finite-state methods and natural language pro-cessing. Thomas Hanneforth and Kay-Michael W?rtzner. 6th International Workshop, FSMNLP 2007. Potsdam, Germany, September 14-16. Revised Papers. Universit?tsverlag Potsdam (2008b) 197-212 
59
