Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 304?313,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Selecting Sentences for Answering Complex Questions
Yllias Chali
University of Lethbridge
4401 University Drive
Lethbridge, Alberta, Canada, T1K 3M4
chali@cs.uleth.ca
Shafiq R. Joty
University of British Columbia
2366 Main Mall
Vancouver, B.C. Canada V6T 1Z4
rjoty@cs.ubc.ca
Abstract
Complex questions that require inferencing
and synthesizing information from multiple
documents can be seen as a kind of topic-
oriented, informative multi-document summa-
rization. In this paper, we have experimented
with one empirical and two unsupervised
statistical machine learning techniques: k-
means and Expectation Maximization (EM),
for computing relative importance of the sen-
tences. However, the performance of these ap-
proaches depends entirely on the feature set
used and the weighting of these features. We
extracted different kinds of features (i.e. lex-
ical, lexical semantic, cosine similarity, ba-
sic element, tree kernel based syntactic and
shallow-semantic) for each of the document
sentences in order to measure its importance
and relevancy to the user query. We used a
local search technique to learn the weights of
the features. For all our methods of generating
summaries, we have shown the effects of syn-
tactic and shallow-semantic features over the
bag of words (BOW) features.
1 Introduction
After having made substantial headway in factoid
and list questions, researchers have turned their at-
tention to more complex information needs that can-
not be answered by simply extracting named enti-
ties (persons, organizations, locations, dates, etc.)
from documents. For example, the question: ?De-
scribe the after-effects of cyclone Sidr-Nov 2007 in
Bangladesh? requires inferencing and synthesizing
information from multiple documents. This infor-
mation synthesis in NLP can be seen as a kind of
topic-oriented, informative multi-document summa-
rization, where the goal is to produce a single text as
a compressed version of a set of documents with a
minimum loss of relevant information.
In this paper, we experimented with one em-
pirical and two well-known unsupervised statisti-
cal machine learning techniques: k-means and EM
and evaluated their performance in generating topic-
oriented summaries. However, the performance of
these approaches depends entirely on the feature set
used and the weighting of these features. We ex-
tracted different kinds of features (i.e. lexical, lexi-
cal semantic, cosine similarity, basic element, tree
kernel based syntactic and shallow-semantic) for
each of the document sentences in order to measure
its importance and relevancy to the user query. We
have used a gradient descent local search technique
to learn the weights of the features. Traditionally,
information extraction techniques are based on the
BOW approach augmented by language modeling.
But when the task requires the use of more com-
plex semantics, the approaches based on only BOW
are often inadequate to perform fine-level textual
analysis. Some improvements on BOW are given
by the use of dependency trees and syntactic parse
trees (Hirao et al, 2004), (Punyakanok et al, 2004),
(Zhang and Lee, 2003), but these, too are not ade-
quate when dealing with complex questions whose
answers are expressed by long and articulated sen-
tences or even paragraphs. Shallow semantic rep-
resentations, bearing a more compact information,
could prevent the sparseness of deep structural ap-
proaches and the weakness of BOW models (Mos-
chitti et al, 2007). Attempting an application of
304
syntactic and semantic information to complex QA
hence seems natural, as pinpointing the answer to a
question relies on a deep understanding of the se-
mantics of both. In more complex tasks such as
computing the relatedness between the query sen-
tences and the document sentences in order to gen-
erate query-focused summaries (or answers to com-
plex questions), to our knowledge no study uses tree
kernel functions to encode syntactic/semantic infor-
mation. For all our methods of generating sum-
maries (i.e. empirical, k-means and EM), we have
shown the effects of syntactic and shallow-semantic
features over the BOW features.
This paper is organized as follows: Section 2 fo-
cuses on the related work, Section 3 describes how
the features are extracted, Section 4 discusses the
scoring approaches, Section 5 discusses how we re-
move the redundant sentences before adding them
to the summary, Section 6 describes our experimen-
tal study. We conclude and give future directions in
Section 7.
2 Related Work
Researchers all over the world working on query-
based summarization are trying different direc-
tions to see which methods provide the best re-
sults. The LexRank method addressed in (Erkan
and Radev, 2004) was very successful in generic
multi-document summarization. A topic-sensitive
LexRank is proposed in (Otterbacher et al, 2005).
As in LexRank, the set of sentences in a document
cluster is represented as a graph, where nodes are
sentences and links between the nodes are induced
by a similarity relation between the sentences. Then
the system ranked the sentences according to a ran-
dom walk model defined in terms of both the inter-
sentence similarities and the similarities of the sen-
tences to the topic description or question.
The summarization methods based on lexical
chain first extract the nouns, compound nouns and
named entities as candidate words (Li et al, 2007).
Then using WordNet, the systems find the semantic
similarity between the nouns and compound nouns.
After that, lexical chains are built in two steps: 1)
Building single document strong chains while dis-
ambiguating the senses of the words and, 2) build-
ing multi-chain by merging the strongest chains of
the single documents into one chain. The systems
rank sentences using a formula that involves a) the
lexical chain, b) keywords from query and c) named
entities.
(Harabagiu et al, 2006) introduce a new paradigm
for processing complex questions that relies on a
combination of (a) question decompositions; (b) fac-
toid QA techniques; and (c) Multi-Document Sum-
marization (MDS) techniques. The question decom-
position procedure operates on a Marcov chain, by
following a random walk with mixture model on a
bipartite graph of relations established between con-
cepts related to the topic of a complex question and
subquestions derived from topic-relevant passages
that manifest these relations. Decomposed questions
are then submitted to a state-of-the-art QA system
in order to retrieve a set of passages that can later be
merged into a comprehensive answer by a MDS sys-
tem. They show that question decompositions using
this method can significantly enhance the relevance
and comprehensiveness of summary-length answers
to complex questions.
There are approaches that are based on probabilis-
tic models (Pingali et al, 2007) (Toutanova et al,
2007). (Pingali et al, 2007) rank the sentences based
on a mixture model where each component of the
model is a statistical model:
Score(s) = ??QIScore(s)+(1??)?QFocus(s,Q)
Where, Score(s) is the score for sentence s. Query-
independent score (QIScore) and query-dependent score
(QFocus) are calculated based on probabilistic models.
(Toutanova et al, 2007) learns a log-linear sentence rank-
ing model by maximizing three metrics of sentence good-
ness: (a) ROUGE oracle, (b) Pyramid-derived, and (c)
Model Frequency. The scoring function is learned by fit-
ting weights for a set of feature functions of sentences
in the document set and is trained to optimize a sentence
pair-wise ranking criterion. The scoring function is fur-
ther adapted to apply to summaries rather than sentences
and to take into account redundancy among sentences.
There are approaches in ?Recognizing Textual Entail-
ment?, ?Sentence Alignment? and ?Question Answering?
that use syntactic and/or semantic information in order to
measure the similarity between two textual units. (Mac-
Cartney et al, 2006) use typed dependency graphs (same
as dependency trees) to represent the text and the hypoth-
esis. Then they try to find a good partial alignment be-
tween the typed dependency graphs representing the hy-
pothesis and the text in a search space of O((m + 1)n)
305
where hypothesis graph contains n nodes and a text graph
contains m nodes. (Hirao et al, 2004) represent the sen-
tences using Dependency Tree Path (DTP) to incorporate
syntactic information. They apply String Subsequence
Kernel (SSK) to measure the similarity between the DTPs
of two sentences. They also introduce Extended String
Subsequence Kernel (ESK) to incorporate semantics in
DTPs. (Kouylekov and Magnini, 2005) use the tree edit
distance algorithms on the dependency trees of the text
and the hypothesis to recognize the textual entailment.
According to this approach, a text T entails a hypothesis
H if there exists a sequence of transformations (i.e. dele-
tion, insertion and substitution) applied to T such that
we can obtain H with an overall cost below a certain
threshold. (Punyakanok et al, 2004) represent the ques-
tion and the sentence containing answer with their depen-
dency trees. They add semantic information (i.e. named
entity, synonyms and other related words) in the depen-
dency trees. They apply the approximate tree matching
in order to decide how similar any given pair of trees are.
They also use the edit distance as the matching criteria in
the approximate tree matching. All these methods show
the improvement over the BOW scoring methods.
Our Basic Element (BE)-based feature used the depen-
dency tree to extract the BEs (i.e. head-modifier-relation)
and ranked the BEs based on their log-likelihood ratios.
For syntactic feature, we extracted the syntactic trees for
the sentence as well as for the query using the Charniak
parser and measured the similarity between the two trees
using the tree kernel function. We used the ASSERT se-
mantic role labeler system to parse the sentence as well
as the query semantically and used the shallow seman-
tic tree kernel to measure the similarity between the two
shallow-semantic trees.
3 Feature Extraction
The sentences in the document collection are analyzed
in various levels and each of the document-sentences is
represented as a vector of feature-values. The features
can be divided into several categories:
3.1 Lexical Features
3.1.1 N-gram Overlap
N-gram overlap measures the overlapping word se-
quences between the candidate sentence and the query
sentence. With the view to measure the N-gram
(N=1,2,3,4) overlap scores, a query pool and a sentence
pool are created. In order to create the query (or sentence)
pool, we took the query (or document) sentence and cre-
ated a set of related sentences by replacing its important
words1 by their first-sense synonyms. For example given
1hence forth important words are the nouns, verbs, adverbs
and adjectives
a stemmed document-sentence: ?John write a poem?, the
sentence pool contains: ?John compose a poem?, ?John
write a verse form? along with the given sentence. We
measured the recall based n-gram scores for a sentence P
using the following formula:
n-gramScore(P) = maxi(maxj N-gram(si, qj))
N-gram(S,Q) =
?
gramn?S
Countmatch(gramn)
?
gramn?S
Count(gramn)
Where, n stands for the length of the n-gram (n =
1, 2, 3, 4) and Countmatch (gramn) is the number
of n-grams co-occurring in the query and the candi-
date sentence, qj is the jth sentence in the query
pool and si is the ith sentence in the sentence pool
of sentence P .
3.1.2 LCS, WLCS and Skip-Bigram
A sequence W = [w1, w2, ..., wn] is a subse-
quence of another sequence X = [x1, x2, ..., xm], if
there exists a strict increasing sequence [i1, i2, ..., ik]
of indices of X such that for all j =
1, 2, ..., k we have xij = wj . Given two sequences,
S1 and S2, the Longest Common Subsequence
(LCS) of S1 and S2 is a common subsequence with
maximum length. The longer the LCS of two sen-
tences is, the more similar the two sentences are.
The basic LCS has a problem that it does not dif-
ferentiate LCSes of different spatial relations within
their embedding sequences (Lin, 2004). To improve
the basic LCS method, we can remember the length
of consecutive matches encountered so far to a reg-
ular two dimensional dynamic program table com-
puting LCS. We call this weighted LCS (WLCS)
and use k to indicate the length of the current con-
secutive matches ending at words xi and yj . Given
two sentences X and Y, the WLCS score of X and
Y can be computed using the similar dynamic pro-
gramming procedure as stated in (Lin, 2004). We
computed the LCS and WLCS-based F-measure fol-
lowing (Lin, 2004) using both the query pool and the
sentence pool as in the previous section.
Skip-bigram is any pair of words in their sentence
order, allowing for arbitrary gaps. Skip-bigram mea-
sures the overlap of skip-bigrams between a candi-
date sentence and a query sentence. Following (Lin,
2004), we computed the skip bi-gram score using
both the sentence pool and the query pool.
306
3.1.3 Head and Head Related-words Overlap
The number of head words common in between
two sentences can indicate how much they are rel-
evant to each other. In order to extract the heads
from the sentence (or query), the sentence (or query)
is parsed by Minipar 2 and from the dependency
tree we extract the heads which we call exact head
words. For example, the head word of the sentence:
?John eats rice? is ?eat?.
We take the synonyms, hyponyms and hyper-
nyms3 of both the query-head words and the
sentence-head words and form a set of words which
we call head-related words. We measured the exact
head score and the head-related score as follows:
ExactHeadScore =
?
w1?HeadSet
Countmatch(w1)
?
w1?HeadSet
Count(w1)
HeadRelatedScore =
?
w1?HeadRelSet
Countmatch(w1)
?
w1?HeadRelSet
Count(w1)
Where HeadSet is the set of head words in the sen-
tence and Countmatch is the number of matches
between the HeadSet of the query and the sen-
tence. HeadRelSet is the set of synonyms, hy-
ponyms and hypernyms of head words in the sen-
tence and Countmatch is the number of matches
between the head-related words of the query and the
sentence.
3.2 Lexical Semantic Features
We form a set of words which we call QueryRe-
latedWords by taking the important words from the
query, their first-sense synonyms, the nouns? hy-
pernyms/hyponyms and important words from the
nouns? gloss definitions.
Synonym overlap measure is the overlap be-
tween the list of synonyms of the important words
extracted from the candidate sentence and the
QueryRelatedWords. Hypernym/hyponym overlap
measure is the overlap between the list of hypernyms
and hyponyms of the nouns extracted from the sen-
tence and the QueryRelatedWords, and gloss overlap
measure is the overlap between the list of important
words that are extracted from the gloss definitions
of the nouns of the sentence and the QueryRelated-
Words.
2http://www.cs.ualberta.ca/ lindek/minipar.htm
3hypernym and hyponym levels are restricted to 2 and 3 re-
spectively
3.3 Statistical Similarity Measures
Statistical similarity measures are based on the
co-occurance of similar words in a corpus. We
have used two statistical similarity measures:
1. Dependency-based similarity measure and 2.
Proximity-based similarity measure.
Dependency-based similarity measure uses the
dependency relations among words in order to mea-
sure the similarity. It extracts the dependency triples
then uses statistical approach to measure the similar-
ity. Proximity-based similarity measure is computed
based on the linear proximity relationship between
words only. It uses the information theoretic defini-
tion of similarity to measure the similarity.
We used the data provided by Dr. Dekang Lin4.
Using the data, one can retrieve most similar words
for a given word. The similar words are grouped into
clusters. Note that, for a word there can be more than
one cluster. Each cluster represents the sense of the
word and its similar words for that sense.
For each query word, we extract all of its clus-
ters from the data. Now, in order to determine the
right cluster for a query word, we measure the over-
lap score between the QueryRelatedWords and the
clusters of words. The hypothesis is that, the cluster
that has more words common with the QueryRelat-
edWords is the right cluster. We chose the cluster for
a word which has the highest overlap score.
Once we get the clusters for the query words, we
measured the overlap between the cluster words and
the sentence words as follows:
Measure =
?
w1?SenWords
Countmatch(w1)
?
w1?SenWords
Count(w1)
Where, SenWords is the set of important words ex-
tracted from the sentence and Countmatch is the number
of matches between the sentence words and the clusters
of similar words of the query words.
3.4 Graph-based Similarity Measure
In LexRank (Erkan and Radev, 2004), the concept of
graph-based centrality is used to rank a set of sentences,
in producing generic multi-document summaries. A sim-
ilarity graph is produced for the sentences in the docu-
ment collection. In the graph, each node represents a
sentence. The edges between the nodes measure the co-
sine similarity between the respective pair of sentences.
The degree of a given node is an indication of how much
important the sentence is. Once the similarity graph is
4http://www.cs.ualberta.ca/ lindek/downloads.htm
307
constructed, the sentences are then ranked according to
their eigenvector centrality. To apply LexRank to query-
focused context, a topic-sensitive version of LexRank is
proposed in (Otterbacher et al, 2005). We followed a
similar approach in order to calculate this feature. The
score of a sentence is determined by a mixture model of
the relevance of the sentence to the query and the similar-
ity of the sentence to other high-scoring sentences.
3.5 Syntactic and Semantic Features:
So far, we have included the features of type Bag of
Words (BOW). The task like query-based summarization
that requires the use of more complex syntactic and se-
mantics, the approaches with only BOW are often inade-
quate to perform fine-level textual analysis. We extracted
three features that incorporate syntactic/semantic infor-
mation.
3.5.1 Basic Element (BE) Overlap Measure
The ?head-modifier-relation? triples, extracted from
the dependency trees are considered as BEs in our exper-
iment. The triples encode some syntactic/semantic infor-
mation and one can quite easily decide whether any two
units match or not- considerably more easily than with
longer units (Zhou et al, 2005). We used the BE package
distributed by ISI5 to extract the BEs for the sentences.
Once we get the BEs for a sentence, we computed the
Likelihood Ratio (LR) for each BE following (Zhou et
al., 2005). Sorting BEs according to their LR scores pro-
duced a BE-ranked list. Our goal is to generate a sum-
mary that will answer the user questions. The ranked
list of BEs in this way contains important BEs at the top
which may or may not be relevant to the user questions.
We filter those BEs by checking whether they contain any
word which is a query word or a QueryRelatedWords (de-
fined in Section 3.2). The score of a sentence is the sum
of its BE scores divided by the number of BEs in the sen-
tence.
3.5.2 Syntactic Feature
Encoding syntactic structure is easier and straight for-
ward. Given a sentence (or query), we first parse it into
a syntactic tree using a syntactic parser (i.e. Charniak
parser) and then we calculate the similarity between the
two trees using the tree kernel defined in (Collins and
Duffy, 2001).
3.5.3 Shallow-semantic Feature
Though introducing BE and syntactic information
gives an improvement on BOW by the use of depen-
dency/syntactic parses, but these, too are not adequate
when dealing with complex questions whose answers
are expressed by long and articulated sentences or even
5BE website:http://www.isi.edu/ cyl/BE
Figure 1: Example of semantic trees
paragraphs. Shallow semantic representations, bearing a
more compact information, could prevent the sparseness
of deep structural approaches and the weakness of BOW
models (Moschitti et al, 2007).
Initiatives such as PropBank (PB) (Kingsbury and
Palmer, 2002) have made possible the design of accurate
automatic Semantic Role Labeling (SRL) systems like
ASSERT (Hacioglu et al, 2003). For example, consider
the PB annotation:
[ARG0 all][TARGET use][ARG1 the french
franc][ARG2 as their currency]
Such annotation can be used to design a shallow se-
mantic representation that can be matched against other
semantically similar sentences, e.g.
[ARG0 the Vatican][TARGET use][ARG1 the Italian
lira][ARG2 as their currency]
In order to calculate the semantic similarity between
the sentences, we first represent the annotated sentence
using the tree structures like Figure 1 which we call Se-
mantic Tree (ST). In the semantic tree, arguments are re-
placed with the most important word-often referred to as
the semantic head.
The sentences may contain one or more subordinate
clauses. For example the sentence, ?the Vatican, located
wholly within Italy uses the Italian lira as their currency.?
gives the STs as in Figure 2. As we can see in Fig-
ure 2(A), when an argument node corresponds to an en-
tire subordinate clause, we label its leaf with ST, e.g.
the leaf of ARG0. Such ST node is actually the root of
the subordinate clause in Figure 2(B). If taken separately,
such STs do not express the whole meaning of the sen-
tence, hence it is more accurate to define a single struc-
ture encoding the dependency between the two predicates
as in Figure 2(C). We refer to this kind of nested STs as
STNs.
Note that, the tree kernel (TK) function defined in
(Collins and Duffy, 2001) computes the number of com-
mon subtrees between two trees. Such subtrees are sub-
ject to the constraint that their nodes are taken with all
or none of the children they have in the original tree.
308
Figure 2: Two STs composing a STN
Though, this definition of subtrees makes the TK func-
tion appropriate for syntactic trees but at the same time
makes it not well suited for the semantic trees (ST) de-
fined above. For instance, although the two STs of Fig-
ure 1 share most of the subtrees rooted in the ST node,
the kernel defined above computes only one match (ST
ARG0 TARGET ARG1 ARG2) which is not useful.
The critical aspect of the TK function is that the pro-
ductions of two evaluated nodes have to be identical to
allow the match of further descendants. This means that
common substructures cannot be composed by a node
with only some of its children as an effective ST represen-
tation would require. (Moschitti et al, 2007) solve this
problem by designing the Shallow Semantic Tree Kernel
(SSTK) which allows to match portions of a ST. We fol-
lowed the similar approach to compute the SSTK.
4 Ranking Sentences
In this section, we describe the scoring techniques in de-
tail.
4.1 Learning Feature-weights: A Local Search
Strategy
In order to fine-tune the weights of the features, we used
a local search technique with simulated annealing to find
the global maximum. Initially, we set al the feature-
weights, w1, ? ? ? , wn, as equal values (i.e. 0.5) (see Al-
gorithm 1). Based on the current weights we score the
sentences and generate summaries accordingly. We eval-
uate the summaries using the automatic evaluation tool
ROUGE (Lin, 2004) (described in Section 6) and the
ROUGE value works as the feedback to our learning
loop. Our learning system tries to maximize the ROUGE
score in every step by changing the weights individually
by a specific step size (i.e. 0.01). That means, to learn
weight wi, we change the value of wi keeping all other
weight values (wj?j 6=i) stagnant. For each weight wi,
the algorithm achieves the local maximum of ROUGE
value. In order to find the global maximum we ran this
algorithm multiple times with different random choices
of initial values (i.e. simulated annealing).
Input: Stepsize l, Weight Initial Value v
Output: A vector ~w of learned weights
Initialize the weight values wi to v.
for i? 1 to n do
rg1 = rg2 = prev = 0
while (true) do
scoreSentences(~w)
generateSummaries()
rg2 = evaluateROUGE()
if rg1 ? rg2 then
prev = wi
wi+ = l
rg1 = rg2
else
break
end
end
end
return ~w
Algorithm 1: Tuning weights using Local Search
technique
Once we have learned the feature-weights, our empir-
ical method computes the final scores for the sentences
using the formula:
scorei = ~xi. ~w (1)
Where, ~xi is the feature vector for i-th sentence, ~w is
the weight vector and scorei is the score of i-th sentence.
4.2 K-means Learning
We start with a set of initial cluster centers and go through
several iterations of assigning each object to the cluster
whose center is closest. After all objects have been as-
signed, we recompute the center of each cluster as the
centroid or mean (?) of its members.
Once we have learned the means of the clusters using
the k-means algorithm, our next task is to rank the sen-
tences according to a probability model. We have used
Bayesian model in order to do so. Bayes? law says:
P (qk|~x,?) =
p(~x|qk,?)P (qk|?)
?K
k=1 p(~x|qk,?)p(qk|?)
(2)
where qk is a class, ~x is a feature vector repre-
senting a sentence and ? is the parameter set of all
class models. We set the weights of the clusters as
equiprobable (i.e. P (qk|?) = 1/K). We calculated
309
p(x|qk,?) using the gaussian probability distribu-
tion. The gaussian probability density function (pdf)
for the d-dimensional random variable ~x is given by:
p(?,?)(~x) =
e
?1
2 (~x??)
T??1(~x??)
?
2pi
d?
det(?)
(3)
where ?, the mean vector and ?, the covariance
matrix are the parameters of the gaussian distribu-
tion. We get the means (?) from the k-means algo-
rithm and we calculate the covariance matrix using
the unbiased covariance estimation:
?? =
1
N ? 1
N?
i=1
(xj ? ?j)(xi ? ?i)
T (4)
4.3 EM Learning
EM is an iterative two step procedure:
1. Expectation-step and 2. Maximization-step.
In the expectation step, we compute expected values
for the hidden variables hi,j which are cluster mem-
bership probabilities. Given the current parameters,
we compute how likely an object belongs to any
of the clusters. The maximization step computes
the most likely parameters of the model given the
cluster membership probabilities. The data-points
are considered to be generated by a mixture model
of k-gaussians of the form:
P (~x) =
k?
i=1
P (C = i)P (~x|?i,?i) (5)
Where the total likelihood of model ? with k
components given the observed data points, X =
x1, ? ? ? , xn is:
L(?|X) =
n?
i=1
k?
j=1
P (C = j)P (xi|?j)
=
n?
i=1
k?
j=1
wjP (xi|?j ,?j)
?
n?
i=1
log
k?
j=1
wjP (xi|?j ,?j)
where P is the probability density function (i.e.
eq 3). ?j and ?j are the mean and covariance ma-
trix of component j, respectively. Each component
contributes a proportion, wj , of the total population,
such that:
?K
j=1wj = 1.
However, a significant problem with the EM al-
gorithm is that it converges to a local maximum
of the likelihood function and hence the quality of
the result depends on the initialization. In order
to get good results from using random starting val-
ues, we can run the EM algorithm several times
and choose the initial configuration for which we
get the maximum log likelihood among all con-
figurations. Choosing the best one among several
runs is very computer intensive process. So, to im-
prove the outcome of the EM algorithm on gaus-
sian mixture models it is necessary to find a better
method of estimating initial means for the compo-
nents. To achieve this aim we explored the widely
used ?k-means? algorithm as a cluster (means) find-
ing method. That means, the means found by k-
means clustering above will be utilized as the initial
means for EM and we calculate the initial covari-
ance matrices using the unbiased covariance estima-
tion procedure (eq:4).
Once the sentences are clustered by EM al-
gorithm, we filter out the sentences which are
not query-relevant by checking their probabilities,
P (qr|xi,?) where, qr denotes the cluster ?query-
relevant?. If for a sentence xi, P (qr|xi,?) > 0.5
then xi is considered to be query-relevant.
Our next task is to rank the query-relevant sen-
tences in order to include them in the summary. This
can be done easily by multiplying the feature vector
~xi with the weight vector ~w that we learned by the
local search technique (eq:1).
5 Redundancy Checking
When many of the competing sentences are included
in the summary, the issue of information overlap be-
tween parts of the output comes up, and a mecha-
nism for addressing redundancy is needed. There-
fore, our summarization systems employ a final level
of analysis: before being added to the final output,
the sentences deemed to be important are compared
to each other and only those that are not too simi-
lar to other candidates are included in the final an-
swer or summary. Following (Zhou et al, 2005), we
modeled this by BE overlap between an intermedi-
ate summary and a to-be-added candidate summary
310
sentence. We call this overlap ratio R, where R is
between 0 and 1 inclusively. Setting R = 0.7 means
that a candidate summary sentence, s, can be added
to an intermediate summary, S, if the sentence has a
BE overlap ratio less than or equal to 0.7.
6 Experimental Evaluation
6.1 Evaluation Setup
We used the main task of Document Understanding
Conference (DUC) 2007 for evaluation. The task
was: ?Given a complex question (topic description)
and a collection of relevant documents, the task is to
synthesize a fluent, well-organized 250-word sum-
mary of the documents that answers the question(s)
in the topic.?
NIST assessors developed topics of interest to
them and choose a set of 25 documents relevant
(document cluster) to each topic. Each topic and its
document cluster were given to 4 different NIST as-
sessors. The assessor created a 250-word summary
of the document cluster that satisfies the information
need expressed in the topic statement. These multi-
ple ?reference summaries? are used in the evaluation
of summary content.
We carried out automatic evaluation of our sum-
maries using ROUGE (Lin, 2004) toolkit, which
has been widely adopted by DUC for automatic
summarization evaluation. It measures summary
quality by counting overlapping units such as the
n-grams (ROUGE-N), word sequences (ROUGE-L
and ROUGE-W) and word pairs (ROUGE-S and
ROUGE-SU) between the candidate summary and
the reference summary. ROUGE parameters were
set as the same as DUC 2007 evaluation setup.
One purpose of our experiments is to study the
impact of different features for complex question
answering task. To accomplish this, we generated
summaries for the topics of DUC 2007 by each of
our seven systems defined as below:
The LEX system generates summaries based on
only lexical features: n-gram (n=1,2,3,4), LCS,
WLCS, skip bi-gram, head, head synonym. The
LSEM system considers only lexical semantic
features: synonym, hypernym/hyponym, gloss,
dependency-based and proximity-based similarity.
The COS system generates summary based on the
graph-based method. The SYS1 system considers
all the features except the BE, syntactic and seman-
tic features. The SYS2 system considers all the fea-
tures except the syntactic and semantic features. The
SYS3 considers all the features except the semantic
and the ALL6 system generates summaries taking
all the features into account.
6.2 Evaluation Results
Table 17 to Table 3, Table 4 to Table 6 and Table 7 to
Table 9 show the evaluation measures for k-means,
EM and empirical approaches respectively. As Ta-
ble 1 shows, in k-means, SYS2 gets 0-21%, SYS3
gets 4-32% and ALL gets 3-36% improvement in
ROUGE-2 scores over the SYS1 system. We get best
ROUGE-W (Table 2) scores for SYS2 (i.e. includ-
ing BE) but SYS3 and ALL do not perform well in
this case. SYS2 improves the ROUGE-W F-score by
1% over SYS1. We do not get any improvement in
ROUGE-SU (Table 3) scores when we include any
kind of syntactic/semantic structures.
The case is different for EM and empirical ap-
proaches. Here, in every case we get a significant
amount of improvement when we include the syn-
tactic and/or semantic features. For EM (Table 4 to
Table 6), the ratio of improvement in F-scores over
SYS1 is: 1-3% for SYS2, 3-15% for SYS3 and 2-
24% for ALL. In our empirical approach (Table 7
to Table 9), SYS2, SYS3 and ALL improve the F-
scores by 3-11%, 7-15% and 8-19% over SYS1 re-
spectively. These results clearly indicate the positive
impact of the syntactic/semantic features for com-
plex question answering task.
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.074 0.077 0.086 0.075 0.075 0.078 0.077
P 0.081 0.084 0.093 0.081 0.098 0.107 0.110
F 0.078 0.080 0.089 0.078 0.085 0.090 0.090
Table 1: ROUGE-2 measures in k-means learning
Table 10 shows the F-scores of the ROUGE mea-
sures for one baseline system, the best system in
DUC 2007 and our three scoring techniques con-
sidering all features. The baseline system gener-
6SYS2, SYS3 and ALL systems show the impact of BE,
syntactic and semantic features respectively
7R stands for Recall, P stands for Precision and F stands for
F-score
311
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.098 0.097 0.101 0.099 0.101 0.097 0.097
P 0.195 0.194 0.200 0.237 0.233 0.241 0.237
F 0.130 0.129 0.134 0.140 0.141 0.139 0.138
Table 2: ROUGE-W measures in k-means learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.131 0.127 0.139 0.136 0.135 0.135 0.135
P 0.155 0.152 0.162 0.176 0.171 0.174 0.174
F 0.142 0.139 0.150 0.153 0.151 0.152 0.152
Table 3: ROUGE-SU in k-means learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.089 0.080 0.087 0.085 0.085 0.089 0.091
P 0.096 0.087 0.094 0.092 0.095 0.116 0.138
F 0.092 0.083 0.090 0.088 0.090 0.101 0.109
Table 4: ROUGE-2 measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.103 0.096 0.101 0.102 0.101 0.102 0.101
P 0.205 0.193 0.200 0.203 0.218 0.222 0.223
F 0.137 0.128 0.134 0.136 0.138 0.139 0.139
Table 5: ROUGE-W measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.146 0.128 0.138 0.143 0.144 0.145 0.144
P 0.171 0.153 0.162 0.168 0.177 0.186 0.185
F 0.157 0.140 0.149 0.154 0.159 0.163 0.162
Table 6: ROUGE-SU measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.086 0.080 0.087 0.087 0.090 0.095 0.099
P 0.093 0.087 0.094 0.094 0.112 0.115 0.116
F 0.089 0.083 0.090 0.090 0.100 0.104 0.107
Table 7: ROUGE-2 in empirical approach
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.102 0.096 0.101 0.102 0.102 0.104 0.105
P 0.203 0.193 0.200 0.204 0.239 0.246 0.247
F 0.135 0.128 0.134 0.137 0.143 0.147 0.148
Table 8: ROUGE-W in empirical approach
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.144 0.129 0.138 0.145 0.146 0.149 0.150
P 0.169 0.153 0.162 0.171 0.182 0.195 0.197
F 0.155 0.140 0.150 0.157 0.162 0.169 0.170
Table 9: ROUGE-SU in empirical approach
ates summaries by returning all the leading sen-
tences (up to 250 words) in the ?TEXT ? field of
the most recent document(s). It shows that the em-
pirical approach outperforms the other two learning
techniques and EM performs better than k-means al-
gorithm. EM improves the F-scores over k-means
by 0.7-22.5%. Empirical approach improves the F-
scores over k-means and EM by 5.9-20.2% and 3.5-
6.5% respectively. Comparing with the DUC 2007
participants our systems achieve top scores and for
some ROUGE measures there is no statistically sig-
nificant difference between our system and the best
DUC 2007 system.
System ROUGE-
1
ROUGE-
2
ROUGE-
W
ROUGE-
SU
Baseline 0.335 0.065 0.114 0.113
Best 0.438 0.122 0.153 0.174
k-means 0.390 0.090 0.138 0.152
EM 0.399 0.109 0.139 0.162
Empirical 0.413 0.107 0.148 0.170
Table 10: F-measures for different systems
7 Conclusion and Future Work
Our experiments show the following: (a) our ap-
proaches achieve promising results, (b) empirical
approach outperforms the other two learning and
EM performs better than the k-means algorithm for
this particular task, and (c) our systems achieve bet-
ter results when we include BE, syntactic and se-
mantic features.
In future, we have the plan to decompose the com-
plex questions into several simple questions before
measuring the similarity between the document sen-
tence and the query sentence. We expect that by de-
composing complex questions into the sets of sub-
questions that they entail, systems can improve the
average quality of answers returned and achieve bet-
ter coverage for the question as a whole.
312
References
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. In Proceedings of Neural Informa-
tion Processing Systems, pages 625?632, Vancouver,
Canada.
G. Erkan and D. R. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457?479.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2003. Shallow Semantic Parsing Using
Support Vector Machines. In Technical Report TR-
CSLR-2003-03, University of Colorado.
S. Harabagiu, F. Lacatusu, and A. Hickl. 2006. Answer-
ing complex questions with random walk models. In
Proceedings of the 29th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 220 ? 227. ACM.
T. Hirao, , J. Suzuki, H. Isozaki, and E. Maeda. 2004.
Dependency-based sentence alignment for multiple
document summarization. In Proceedings of Coling
2004, pages 446?452, Geneva, Switzerland. COLING.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of the international con-
ference on Language Resources and Evaluation, Las
Palmas, Spain.
M. Kouylekov and B. Magnini. 2005. Recognizing
textual entailment with tree edit distance algorithms.
In Proceedings of the PASCAL Challenges Workshop:
Recognising Textual Entailment Challenge.
J. Li, L. Sun, C. Kit, and J. Webster. 2007. A Query-
Focused Multi-Document Summarizer Based on Lex-
ical Chains. In Proceedings of the Document Under-
standing Conference, Rochester. NIST.
C. Y. Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
Workshop on Text Summarization Branches Out, Post-
Conference Workshop of Association for Computa-
tional Linguistics, pages 74?81, Barcelona, Spain.
B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer,
and C. D. Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, page 4148, New
York, USA.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Seman-
tic Kernels for Question/Answer Classificaion. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 776?783, Prague,
Czech Republic. ACL.
J. Otterbacher, G. Erkan, and D. R. Radev. 2005. Us-
ing Random Walks for Question-focused Sentence Re-
trieval. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 915?922,
Vancouver, Canada.
P. Pingali, Rahul K., and V. Varma. 2007. IIIT Hyder-
abad at DUC 2007. In Proceedings of the Document
Understanding Conference, Rochester. NIST.
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping de-
pendencies trees: An application to question answer-
ing. In Proceedings of AI & Math, Florida, USA.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The PYTHY
Summarization System: Microsoft Research at DUC
2007 . In proceedings of the Document Understanding
Conference, Rochester. NIST.
D. Zhang and W. S. Lee. 2003. A Language Mod-
eling Approach to Passage Question Answering. In
Proceedings of the Twelfth Text REtreival Conference,
pages 489?495, Gaithersburg, Maryland.
L. Zhou, C. Y. Lin, and E. Hovy. 2005. A BE-based
Multi-dccument Summarizer with Query Interpreta-
tion. In Proceedings of Document Understanding
Conference, Vancouver, B.C., Canada.
313
Document Clustering with Grouping and
Chaining Algorithms
Yllias Chali and Soufiane Noureddine
Department of Computer Science,
University of Lethbridge
Abstract. Document clustering has many uses in natural language tools
and applications. For instance, summarizing sets of documents that all
describe the same event requires first identifying and grouping those
documents talking about the same event. Document clustering involves
dividing a set of documents into non-overlapping clusters. In this paper,
we present two document clustering algorithms: grouping algorithm, and
chaining algorithm. We compared them with k-means and the EM algo-
rithms. The evaluation results showed that our two algorithms perform
better than the k-means and EM algorithms in different experiments.
1 Introduction
Document clustering has many uses in natural language tools and applications.
For instance, summarizing sets of documents that all describe the same event
requires first identifying and grouping those documents talking about the same
event. Document clustering involves dividing a set of texts into non-overlapping
clusters, where documents in a cluster are more similar to one another than to
documents in other clusters. The term more similar, when applied to clustered
documents, usually means closer by some measure of proximity or similarity.
According to Manning and Schutze [1], there are two types of structures pro-
duced by clustering algorithms, hierarchical clustering and flat or non-
hierarchical clustering. Flat clustering are simply groupings of similar objects.
Hierarchical clustering is a tree of subclasses which represent the cluster that
contains all the objects of its descendants. The leaves of the tree are the individ-
ual objects of the clustered set. In our experiments, we used the non-hierarchical
clustering k-means and EM [2] and our own clustering algorithms.
There are several similarity measures to help find out groups of related doc-
uments in a set of documents [3]. We use identical word method and semantic
relation method to assign a similarity score to each pair of compared texts. For
the identical word method, we use k-means algorithm, the EM algorithm, and
our own grouping algorithm to cluster the documents. For the semantic relation
method, we use our own grouping algorithm and chaining algorithm to do the
clustering job. We choose WordNet 1.6 as our background knowledge. WordNet
consists of synsets gathered in a hypernym/hyponym hierarchy [4]. We use it
to get word senses and to evaluate the semantic relations between word senses.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 280?291, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Document Clustering with Grouping and Chaining Algorithms 281
2 Identical Word Similarity
To prepare the texts for the clustering process using identical word similarity,
we perform the following steps on each of the selected raw texts:
1. Preprocessing which consists in extracting file contents from the raw texts,
stripping special characters and numbers, converting all words to lower cases
and removing stopwords, and converting all plural forms to singular forms.
2. Create document word vectors: each document was processed to record the
unique words and their frequencies. We built the local word vector for each
document, each vector entry will record a single word and its frequency. We
also keep track of the unique words in the whole texts to be tested. After
processing all the documents, we convert each local vector to a global vector
using the overall unique words.
3. Compute the identical word similarity score among documents: given any
two documents, if we have their global vectors x, y, we can use the cosine
measure [5] to calculate the identical word similarity score between these
two texts.
cos(x, y) =
?n
i=1 xiyi
?
?n
i=1 x
2
i
?
?n
i=1 y
2
i
(1)
where x and y are n-dimensional vectors in a real-valued space.
Now, we determined a global vector for each text. We also have the identical
word similarity scores among all texts. We can directly use these global vectors
to run the k-means or the EM algorithms to cluster the texts. We can also use
the identical word similarity scores to run grouping algorithm (defined later) to
do the clustering via a different approach.
3 Semantic Relation Similarity
To prepare the texts for clustering process using semantic relation similarity, the
following steps are performed on each raw texts:
1. Preprocessing which consists in extracting file contents, and removing special
characters and numbers.
2. Extract all the nouns from the text using part-of-speech tagger (i.e. UPenn-
sylvania tagger). The tagger parses each sentence of the input text into
several forms with specific tags. We get four kinds of nouns as the results of
running the tagger: NN, NNS, NNP and NNPS. We then run a process to
group all the nouns into meaningful nouns and non-meaningful nouns. The
basic idea is to construct the largest compound words using the possible ad-
jective and following nouns, then check whether or not the compound words
have a meaning in WordNet. If not, we break the compound words into pos-
sible smaller ones, then check again until we find the ones with meanings in
282 Y. Chali and S. Noureddine
Wordnet. When we get a noun (or a compound noun) existing in WordNet,
we insert it into the meaningful word set, which we call set of regular nouns,
otherwise we insert it into the non-meaningful word set, which we call set of
proper nouns.
During the processing of each document, we save the over-all unique
meaningful nouns in an over-all regular nouns set. Because of the big over-
head related to accessing WordNet, we try to reduce the overall access times
to a minimal level. Our approach is to use these over-all unique nouns to
retrieve the relevant information from WordNet and save them in a global
file. For each sense of each unique noun, we save its synonyms, two level
hypernyms, and one level hyponyms. If any process frequently needs the
WordNet information, it can use the global file to store the information in a
hash and thus provides fast access to its members.
3. Word sense disambiguation.
Similarly to Galley and McKeown [6], we use lexical chain approach
to disambiguate the nouns in the regular nouns for each document [7,8].
A lexical chain is a sequence of related words in the text, spanning short
(adjacent words or sentences) or long distances (entire text). WordNet is
one lexical resource that may be used for the identification of lexical chains.
Lexical chains can be constructed in a source text by grouping (chaining)
sets of word-senses that are semantically related. We designate the following
nine semantic relations between two senses:
(a) Two noun instances are identical, and used in the same sense;
(b) Two noun instances are used in the same sense (i.e., are synonyms );
(c) The senses of two noun instances have a hypernym/hyponym relation
between them;
(d) The senses of two noun instances are siblings in the hypernym/hyponym
tree;
(e) The senses of two noun instances have a grandparent/grandchild relation
in the hypernym/hyponym tree;
(f) The senses of two noun instances have a uncle/nephew relation in the
hypernym/hyponym tree;
(g) The senses of two noun instances are cousins in the hypernym/hyponym
tree (i.e., two senses share the same grandparent in the hypernym tree
of WordNet);
(h) The senses of two noun instances have a great-grandparent/great-grand-
child relation in the hypernym/hyponym tree (i.e., one sense?s grand-
parent is another sense?s hyponym?s great-grandparent in the hypernym
tree of WordNet).
(i) The senses of two noun instances do not have any semantic relation.
To disambiguate all the nouns in the regular nouns of a text, we proceed
with the following major steps:
(a) Evaluate the semantic relation between any two possible senses according
to the hypernym/hyponym tree in WordNet. For our experiments, we use
Document Clustering with Grouping and Chaining Algorithms 283
the following scoring scheme for the relations defined above as shown in
Table 1. The score between Ai (sense i of word A) and Bj (sense j of word
B) is denoted as score(Ai, Bj). These scores are established empirically
and give more weight to closer words according to WordNet hierarchy.
Table 1. Scoring Scheme for Relations
Relation Score(Ai, Bj)
Identical log(16)
Synonyms log(15)
Hypernyms/hyponyms log(14)
Siblings log(13)
Grandparent/grandchild log(12)
Uncle/nephew log(11)
Cousins log(10)
Great-grandparent/great-grandchild log(9)
No relation 0
(b) Build the lexical chains using all possible senses of all nouns. To build
the lexical chains, we assume each noun possesses all the possible senses
from WordNet. For each sense of each noun in a text, if it is related to all
the senses of any existing chain, then we put this sense into this chain,
else we create a new chain and push this sense into the new empty chain.
After this, we will have several lexical chains with their own scores.
(c) Using the lexical chain, try to assign a specific sense to each nouns.
We sort the chains by their scores in a non-increasing order. We select
the chain with the highest score and assign the senses in that chain
to the corresponding words. These words are disambiguated now. Next,
we process the next chain with the next highest score. If it contains a
different sense of any disambiguated words, we skip it to process the
next chain until we reach the chains with a single entry. We mark the
chains which we used to assign senses to words as selected. For the single
entry chains, if the sense is the only sense of the word, we mark it as
disambiguated. For each undisambiguated word, we check each of its
senses against all the selected chains. If it has a relation with all the
senses in a selected chain, we will then remember which sense-chain
pair has the highest relation score, then we assign that sense to the
corresponding word.
After these steps, the leftover nouns will be the undisambiguated words. We
save the disambiguated words and the undisambiguated words with their
frequencies for calculating the semantic relation scores between texts.
4. Compute the similarity score for each pair of texts.
Now, we should have three parts of nouns for each text: disambiguated
nouns, undisambiguated nouns and the non-meaningful nouns (proper
nouns). We will use all of them to calculate the semantic similarity scores
284 Y. Chali and S. Noureddine
between each pair of texts. For the purpose of calculating the semantic sim-
ilarity scores among texts, we use only the first three relations (a), (b), and
(c) and the last relation (i) and their corresponding scores defined in Table 1.
For a given text pair, we proceed as in the following steps to calculate the
similarity scores:
? Using the disambiguated nouns, the score score1 of the similarity be-
tween two texts T1 and T2 is computed as follows:
score1 =
?n
i=1
?m
j=1 score(Ai, Bj) ? freq(Ai) ? freq(Bj)
?
?n
i=1 freq
2(Ai)
?
?m
j=1 freq
2(Bj)
(2)
where Ai is a word sense from T1 and Bj is a word sense from T2;
score(Ai, Bj) is a semantic relation score defined in Table 1; n and m
are the numbers of disambiguated nouns in T1 and T2; freq(x) is the
frequency of a word sense x.
? For the undisambiguated nouns, if two nouns are identical in their word
formats, then the probability that they take the same sense in both
texts is 1/s, where s is the number of their total possible senses. The
similarity score score2 between two texts T1 and T2 according to the
undisambiguated nouns is computed as follows:
score2 =
?n
i=1
log(16)?freq1(Ai)?freq2(Ai)
si
?
?n
i=1 freq
2
1(Ai)
?
?n
j=1 freq
2
2(Aj)
(3)
where Ai is a word common to T1 and T2; n is the number of common
words to T1 and T2; freq1(Ai) is the frequency of Ai in T1; freq2(Ai) is
the frequency of Ai in T2; si is the number of senses of Ai.
? The proper nouns are playing an important role in relating texts to each
other. So, we use a higher score (i.e., log(30)) for the identical proper
nouns. The similarity score score3 between two texts T1 and T2 among
the proper nouns between is computed as follows:
score3 =
?n
i=1 log(30) ? freq1(Ai) ? freq2(Ai)
?
?n
i=1 freq
2
1(Ai)
?
?n
j=1 freq
2
2(Aj)
(4)
where Ai is a proper noun common to T1 and T2; n is the number of
common proper nouns to T1 and T2; freq1(Ai) is the frequency of Ai in
T1; freq2(Ai) is the frequency of Ai in T2.
? Adding all the scores together as the total similarity score of the text
pair:
score = score1 + score2 + score3 (5)
Now we make it ready to use the grouping algorithm or chaining algorithm
defined shortly to cluster the texts.
Document Clustering with Grouping and Chaining Algorithms 285
4 Clustering Algorithms
Generally, every text should have a higher semantic similarity score with the
texts from its group than the texts from a different groups [9]. There are a
few rare cases where this assumption could fail. One case is that the semantic
similarity score does not reflect the relationships among the texts. Another case
is that the groups are not well grouped by common used criteria or the topic is
too broad in that group.By all means, the texts of any well formed clusters should
have stronger relations among its members than the texts in other clusters. Based
on this idea, we developed two text clustering algorithms: grouping algorithm
and chaining algorithm . They share some common features but with different
approaches.
One major issue in partitioning texts into different clusters is choosing the
cutoff on the relation scores. Virtually, all texts are related with each other to
some extent. The problem here is how similar (or close) they should be so that
we can put them into one cluster and how dissimilar (or far away) they should
be so that we can group them into different clusters. Unless the similarity scores
among all the texts can be represented as binary values, we will always face this
problem with any kind of texts. In order to address this problem, we introduce
two reference values in our text clustering algorithms: high-threshold and low-
threshold. The high-threshold means the high standard for bringing two texts
into the same cluster. The low-threshold means the minimal standard for possibly
bringing two texts into the same cluster. If the score between any two texts
reaches or surpasses the high-threshold, then they will go to the same cluster.
If the score reaches the low-threshold but is lower than the high-threshold, then
we will carry out further checking to decide if we should bring two texts into the
same cluster or not, else, the two texts will not go to the same cluster.
We get our high-threshold and low-threshold for our different algorithms by
running some experiments using the grouped text data. The high-threshold we
used for our two algorithms is 1.0 and the low-threshold we used is 0.6. For our
experiment, we always take a number of grouped texts and mix them up to make
a testing text set. So, each text must belong to one cluster with certain number
of texts.
4.1 Grouping Algorithm
The basic idea is that each text could gather its most related texts to form
an initial group, then we decide which groups have more strength over other
groups, make the stronger groups as final clusters, and use them to bring any
possible texts to their clusters. First, we use each text as a leading text (Tl) to
form a cluster. To do this, we put all the texts which have a score greater than
the high-threshold with Tl into one group and add each score to the group?s
total score. By doing this for all texts, we will have N possible different groups
with different entries and group scores, where N is the number of the total texts
in the set. Next, we select the final clusters from those N groups. We arrange
all the groups by their scores in a non-increasing order. We choose the group
286 Y. Chali and S. Noureddine
with the highest score and check if any text in this group has been clustered
to the existing final clusters or not. If not more than 2 texts are overlapping
with the final clusters, then we take this group as a final cluster, and remove the
overlapping texts from other final clusters. We process the group with the next
highest score in the same way until the groups? entries are less than 4. For those
groups, we would first try to insert their texts into the existing final clusters if
they can fit in one of them. Otherwise, we will let them go to the leftover cluster
which holds all the texts that do not belong to any final clusters. The following
is the pseudocode for the grouping algorithm:
Grouping Algorithm
// Get the initial clusters
for each text ti
construct a text cluster including all the texts(tj)
which score(ti, tj) >= high-threshold;
compute the total score of the text cluster;
find out its neighbor with maximum relation score;
end for
// Build the final clusters
sort the clusters by their total score in non-increasing order;
for each cluster gi in the sorted clusters
if member(gi) > 3 and overlap-mem(gi) <= 2
take gi as a final cluster ci;
mark all the texts in ci as clustered;
else
skip to process next cluster;
end if
end for
// Process the leftover texts and insert them into one of the final clusters
for each text tj
if tj has not been clustered
find cluster ci with the highest score(ci, tj);
if the average-score(ci, tj) >= low-threshold
put tj into the cluster ci;
else if the max score neighbor tm of tj is in ck
put tj into cluster ck;
else
put tj into the final leftover cluster;
end if
end if
end for
output the final clusters and the final leftover cluster;
Document Clustering with Grouping and Chaining Algorithms 287
where: member(gi) is the number of members in group gi; overlap-mem(gi) is
the number of members that are overlapped with any final clusters; score(ci,
tj) is the sum of scores between tj and each text in ci; average-score(ci, tj) is
score(ci, tj) divide by the number of texts in ci.
4.2 Chaining Algorithm
This algorithm is based on the observation of the similarities among the texts in
groups.Within a text group, not all texts are always strongly relatedwith any other
texts. Sometimes there are several subgroups existing in a single group, i.e., cer-
tain texts have stronger relations with their subgroup members and have a weaker
relation with other subgroup members. Usually one or more texts have stronger
relation crossing different subgroups to connect them together, otherwise all the
texts in the group could not be grouped together. So, there is a chaining effect in
each group connecting subgroups together to form one entire group.
We use this chaining idea in the chaining algorithm. First, for each text Tj ,
we find all the texts which have similarity scores that are greater or equal than
the high-threshold with Tj and use them to form a closer-text-set. All the texts
in that set are called closer-text of Tj.
Next, for each text which has not been assigned to a final chain, we use its
initial closer-text-set members to form a new chain. For each of the texts in
the chain, if any of its closer-texts are relatively related (i.e., the score >= low-
threshold) to all the texts in the chain, then we add it into the current chain.
One thing needs to be noticed here is that we do not want to simply bring
all the closer-texts of each current chain?s members into the chain. The reason
is to eliminate the unwanted over-chaining effect that could bring many other
texts which are only related to one text in the existing chain. So, we check each
candidate text against all the texts in the chain to prevent the over-chaining
effect. We repeat this until the chain?s size are not increasing. If the chain has
less than 4 members, we will not use this chain for a final cluster and try to
re-assign the chain members to other chains.
After the above process, if any text has not been assigned to a chain we check
it against all existing chains and find the chain which has highest similarity score
between the chain and this text. If the average similarity score with each chain
members is over low-threshold,we insert this text into that chain, else we put it into
the final leftover chain. The following is the pseudocode for the chaining algorithm:
5 Application
We chose as our input data the documents sets used in the Document Under-
standing Conferences [10,11], organized by NIST. We collected 60 test document
directories for our experiments. Each directory is about a specific topic and has
about 10 texts and each text has about 1000 words. Our experiment is to mix
up the 60 directories and try to reconstitute them using one of our clustering
288 Y. Chali and S. Noureddine
Chaining Algorithm
// construct a closer-text-set for each text
for each text ti 0 < i <= N
for each text tj 0 < j <= N
if score(ti, tj) >= high-threshold
put tj into closer-text-set si;
end if
end for
end for
// Build the chains
c = 0;
for each text ti of all the texts
if it has not been chained in
put text ti into chain c and mark it as been chained;
bring all the text in closer text-set si into the new chain c;
mark si as processed;
while (the size of chain c is changing)
for each text tk in chain c
for each text tm in sk of tk
if the score between tm and any text in chain c >= low-threshold
put tm into chain c;
mark tm as been chained to chain c;
end if
end for
end for
end while
if the size of chain c < 4
discard chain c;
remark the texts in chain c as unchained;
end if
c++;
end if
end for
// Process the leftover texts and insert them into one of the existing chains
for each unchained text tj
find chain ci with the highest score(ci, tj);
if the average-score(ci, tj) >= low-threshold
put tj into the chain ci;
else
put tj into the final leftover chain;
end if
end for
output the valid chains and the final leftover chain.
Document Clustering with Grouping and Chaining Algorithms 289
algorithm. Then, we measure how successful are these algorithms in reconstitut-
ing the original directories. We implemented the k-means algorithm and the EM
algorithm to compare them with our algorithms.
In our test, we found out that the chaining algorithm did not work well
for identical method. We tested grouping algorithm, chaining algorithm, and
EM algorithm with semantic method, and k-means algorithm, EM algorithm,
and grouping algorithm with identical methods. We run the k-means and the
EM algorithms 4 times with each experiment texts set and take the average
performance. As we described before, semantic method represents text relations
with scores, so k-means algorithm which needs input data in vector format will
not be applied to semantic method.
6 Evaluation
For our testing, we need to compare the system clusters with the testing clusters
(original text directories) to evaluate the performance of each system. We first
compare each system cluster with all of the testing clusters to find the best
matched cluster pair with the maximum number of identical texts. We then use
recall, precision, and F-value to evaluate each matching pair. Finally, we use the
average F-value to evaluate the whole system performance. For a best matched
pair TCj (testing cluster) and SCi (system cluster), the recall (R), precision (P),
and F-value (F) are defined as follows:
R =
m
t
(6)
P =
m
m + n
(7)
F (TCj, SCi) =
2PR
P + R
(8)
where m is the number of the overlapping texts between TCj and SCi; n is the
number of the non-overlapping texts in SCi; t is the total number of texts in
TCj.
For the whole system evaluation, we use the Average F which is calculated
using the F-values of each matched pair of clusters.
Average F =
?
i,j max(F (SCi, TCj))
max(m, n)
(9)
Where i <= min(m, n), j <= m, m is the number of testing clusters, and n
is the number of system clusters.
290 Y. Chali and S. Noureddine
7 Results
The performance of grouping algorithm and chaining algorithm are very close
using the semantic relation approach and most of their Average F are over
90%. For the identical word approach, the grouping algorithm performance
is much better than the performances of the k-means algorithm and the EM
algorithm. The poor performance of the k-means algorithm results from ran-
domly selected k initial values. Those initial N-dimensional values usually do
not represent the whole data very well. For the semantic relation approach,
both grouping and chaining algorithms performed better than the
EM algorithm.
Table 2 and 3 are the system Average F values for the different algorithms.
The identical word similarity method used grouping algorithm, k-means algo-
rithm, and EM algorithm. The semantic similarity method used grouping algo-
rithm, chaining algorithm and EM algorithm.
Table 2. Comparisons of F-value using Identical Word Similarity
Identical Word Similarity
Grouping EM k-means
0.98 0.81 0.66
Table 3. Comparisons of F-value using Semantic Relation Similarity
Semantic Relation Similarity
Grouping Chaining EM
0.92 0.91 0.76
8 Conclusion
Document clustering is an important tool for natural language applications. We
presented two novel algorithms grouping algorithm and chaining algorithm for
clustering sets of documents, and which can handle a large set of documents
and clusters. The two algorithms use semantic similarity and identical word
measure, and their performance is much better than the performance of the K-
means algorithm and the performance of the EM algorithm, used as a baseline
for our evaluation.
Evaluating the system quality has been always a difficult issue. We presented
an evaluation methodology to assess how the system clusters are related to the
manually generated clusters using precision and recall measures.
The grouping and the chaining algorithm may be used in several natural
language processing applications requiring clustering tasks such as summarizing
set of documents relating the same event.
Document Clustering with Grouping and Chaining Algorithms 291
Acknowledgments
This work was supported by the Natural Sciences and Engineering Research
Council (NSERC) research grant.
References
1. Manning, C.D., Schutze, H.: Foundations of Statistical Natural Language Process-
ing. MIT Press (2000)
2. Berkhin, P.: Survey of clustering data mining techniques. Technical report, Accrue
Software, San Jose, CA (2002)
3. Duda, R., Hart, P.: Pattern Classification and Scene Analysis. John Wiley & Sons,
New York, NY (1973)
4. Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D., Miller, K.: Five papers on
wordnet. CSL Report 43, Cognitive Science Laboratory, Princeton University
(1993)
5. Salton, G.: Automatic Text Processing: The Transformation, Analysis, and Re-
trieval of Information by Computer. Addison-Wesley Series in Computer Sciences
(1989)
6. Galley, M., McKeown, K.: Improving word sense disambiguation in lexical chaining.
In: Proceedings of the 18th International Joint Conference on Artificial Intelligence,
Acapulco, Mexico. (2003)
7. Barzilay, R., Elhadad, M.: Using lexical chains for text summarization. In: Pro-
ceedings of the 35th Annual Meeting of the Association for Computational Linguis-
tics and the 8th European Chapter Meeting of the Association for Computational
Linguistics, Workshop on Intelligent Scalable Text Summarization, Madrid (1997)
10-17
8. Silber, H.G., McCoy, K.F.: Efficiently computed lexical chains as an intermediate
representation for automatic text summarization. Computational Linguistics 28
(2002) 487?496
9. Pantel, P., Lin, D.: Document clustering with committees. In: Proceedings of the
ACM SIGIR?02, Finland (2002)
10. Over, P., ed.: Proceedings of the Document Understanding Conference, NIST
(2003)
11. Over, P., ed.: Proceedings of the Document Understanding Conference, NIST
(2004)
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 9?12,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving the Performance of the Random Walk Model for Answering
Complex Questions
Yllias Chali and Shafiq R. Joty
University of Lethbridge
4401 University Drive
Lethbridge, Alberta, Canada, T1K 3M4
{chali,jotys}@cs.uleth.ca
Abstract
We consider the problem of answering com-
plex questions that require inferencing and
synthesizing information from multiple doc-
uments and can be seen as a kind of topic-
oriented, informative multi-document summa-
rization. The stochastic, graph-based method
for computing the relative importance of tex-
tual units (i.e. sentences) is very successful
in generic summarization. In this method,
a sentence is encoded as a vector in which
each component represents the occurrence fre-
quency (TF*IDF) of a word. However, the
major limitation of the TF*IDF approach is
that it only retains the frequency of the words
and does not take into account the sequence,
syntactic and semantic information. In this pa-
per, we study the impact of syntactic and shal-
low semantic information in the graph-based
method for answering complex questions.
1 Introduction
After having made substantial headway in factoid
and list questions, researchers have turned their at-
tention to more complex information needs that can-
not be answered by simply extracting named en-
tities like persons, organizations, locations, dates,
etc. Unlike informationally-simple factoid ques-
tions, complex questions often seek multiple differ-
ent types of information simultaneously and do not
presupposed that one single answer could meet al
of its information needs. For example, with complex
questions like ?What are the causes of AIDS??, the
wider focus of this question suggests that the sub-
mitter may not have a single or well-defined infor-
mation need and therefore may be amenable to re-
ceiving additional supporting information that is rel-
evant to some (as yet) undefined informational goal.
This type of questions require inferencing and syn-
thesizing information from multiple documents. In
Natural Language Processing (NLP), this informa-
tion synthesis can be seen as a kind of topic-oriented,
informative multi-document summarization, where
the goal is to produce a single text as a compressed
version of a set of documents with a minimum loss
of relevant information.
Recently, the graph-based method (LexRank) is
applied successfully to generic, multi-document
summarization (Erkan and Radev, 2004). A topic-
sensitive LexRank is proposed in (Otterbacher et al,
2005). In this method, a sentence is mapped to a vec-
tor in which each element represents the occurrence
frequency (TF*IDF) of a word. However, the major
limitation of the TF*IDF approach is that it only re-
tains the frequency of the words and does not take
into account the sequence, syntactic and semantic
information thus cannot distinguish between ?The
hero killed the villain? and ?The villain killed the
hero?. The task like answering complex questions
that requires the use of more complex syntactic and
semantics, the approaches with only TF*IDF are of-
ten inadequate to perform fine-level textual analysis.
In this paper, we extensively study the impact
of syntactic and shallow semantic information in
measuring similarity between the sentences in the
random walk model for answering complex ques-
tions. We argue that for this task, similarity mea-
sures based on syntactic and semantic information
performs better and can be used to characterize the
9
relation between a question and a sentence (answer)
in a more effective way than the traditional TF*IDF
based similarity measures.
2 Graph-based Random Walk Model for
Text Summarization
In (Erkan and Radev, 2004), the concept of graph-
based centrality is used to rank a set of sentences,
in producing generic multi-document summaries. A
similarity graph is produced where each node repre-
sents a sentence in the collection and the edges be-
tween nodes measure the cosine similarity between
the respective pair of sentences. Each sentence is
represented as a vector of term specific weights. The
term specific weights in the sentence vectors are
products of term frequency (tf) and inverse docu-
ment frequency (idf). The degree of a given node
is an indication of how much important the sentence
is. To apply LexRank to query-focused context, a
topic-sensitive version of LexRank is proposed in
(Otterbacher et al, 2005). The score of a sentence is
determined by a mixture model:
p(s|q) = d?
rel(s|q)
?
z?C rel(z|q)
+ (1? d)
?
?
v?C
sim(s, v)
?
z?C sim(z, v)
? p(v|q) (1)
Where, p(s|q) is the score of a sentence s given a
question q, is determined as the sum of its relevance
to the question (i.e. rel(s|q)) and the similarity to
other sentences in the collection (i.e. sim(s, v)).
The denominators in both terms are for normaliza-
tion. C is the set of all sentences in the collection.
The value of the parameter d which we call ?bias?,
is a trade-off between two terms in the equation and
is set empirically. We claim that for a complex task
like answering complex questions where the related-
ness between the query sentences and the document
sentences is an important factor, the graph-based
random walk model of ranking sentences would per-
form better if we could encode the syntactic and se-
mantic information instead of just the bag of word
(i.e. TF*IDF) information in calculating the similar-
ity between sentences. Thus, our mixture model for
answering complex questions is:
p(s|q) = d? TREESIM(s, q) + (1? d)
?
?
v?C
TREESIM(s, v)? p(v|q) (2)
Figure 1: Example of semantic trees
Where TREESIM(s,q) is the normalized syntactic
(and/or semantic) similarity between the query (q)
and the document sentence (s) and C is the set of
all sentences in the collection. In cases where the
query is composed of two or more sentences, we
compute the similarity between the document sen-
tence (s) and each of the query-sentences (qi) then
we take the average of the scores.
3 Encoding Syntactic and Shallow
Semantic Structures
Encoding syntactic structure is easier and straight
forward. Given a sentence (or query), we first parse
it into a syntactic tree using a syntactic parser (i.e.
Charniak parser) and then we calculate the similarity
between the two trees using the general tree kernel
function (Section 4.1).
Initiatives such as PropBank (PB) (Kingsbury and
Palmer, 2002) have made possible the design of
accurate automatic Semantic Role Labeling (SRL)
systems like ASSERT (Hacioglu et al, 2003). For
example, consider the PB annotation:
[ARG0 all][TARGET use][ARG1 the french
franc][ARG2 as their currency]
Such annotation can be used to design a shallow
semantic representation that can be matched against
other semantically similar sentences, e.g.
[ARG0 the Vatican][TARGET use][ARG1 the
Italian lira][ARG2 as their currency]
In order to calculate the semantic similarity be-
tween the sentences, we first represent the annotated
sentence using the tree structures like Figure 1 which
we call Semantic Tree (ST). In the semantic tree, ar-
guments are replaced with the most important word-
often referred to as the semantic head.
The sentences may contain one or more subordi-
nate clauses. For example the sentence, ?the Vati-
can, located wholly within Italy uses the Italian lira
10
Figure 2: Two STs composing a STN
as their currency.? gives the STs as in Figure 2. As
we can see in Figure 2(A), when an argument node
corresponds to an entire subordinate clause, we la-
bel its leaf with ST , e.g. the leaf of ARG0. Such ST
node is actually the root of the subordinate clause
in Figure 2(B). If taken separately, such STs do not
express the whole meaning of the sentence, hence it
is more accurate to define a single structure encod-
ing the dependency between the two predicates as in
Figure 2(C). We refer to this kind of nested STs as
STNs.
4 Syntactic and Semantic Kernels for Text
4.1 Tree Kernels
Once we build the trees (syntactic or semantic),
our next task is to measure the similarity be-
tween the trees. For this, every tree T is rep-
resented by an m dimensional vector v(T ) =
(v1(T ), v2(T ), ? ? ? vm(T )), where the i-th element
vi(T ) is the number of occurrences of the i-th tree
fragment in tree T . The tree fragments of a tree are
all of its sub-trees which include at least one produc-
tion with the restriction that no production rules can
be broken into incomplete parts.
Implicitly we enumerate all the possible tree frag-
ments 1, 2, ? ? ? ,m. These fragments are the axis
of this m-dimensional space. Note that this could
be done only implicitly, since the number m is ex-
tremely large. Because of this, (Collins and Duffy,
2001) defines the tree kernel algorithm whose com-
putational complexity does not depend on m. We
followed the similar approach to compute the tree
kernel between two syntactic trees.
4.2 Shallow Semantic Tree Kernel (SSTK)
Note that, the tree kernel (TK) function defined in
(Collins and Duffy, 2001) computes the number of
common subtrees between two trees. Such subtrees
are subject to the constraint that their nodes are taken
with all or none of the children they have in the orig-
inal tree. Though, this definition of subtrees makes
the TK function appropriate for syntactic trees but
at the same time makes it not well suited for the se-
mantic trees (ST) defined in Section 3. For instance,
although the two STs of Figure 1 share most of the
subtrees rooted in the ST node, the kernel defined
above computes no match.
The critical aspect of the TK function is that the
productions of two evaluated nodes have to be iden-
tical to allow the match of further descendants. This
means that common substructures cannot be com-
posed by a node with only some of its children as
an effective ST representation would require. Mos-
chitti et al (2007) solve this problem by designing
the Shallow Semantic Tree Kernel (SSTK) which
allows to match portions of a ST. We followed the
similar approach to compute the SSTK.
5 Experiments
5.1 Evaluation Setup
The Document Understanding Conference (DUC)
series is run by the National Institute of Standards
and Technology (NIST) to further progress in sum-
marization and enable researchers to participate in
large-scale experiments. We used the DUC 2007
datasets for evaluation.
We carried out automatic evaluation of our sum-
maries using ROUGE (Lin, 2004) toolkit, which
has been widely adopted by DUC for automatic
summarization evaluation. It measures summary
quality by counting overlapping units such as the
n-gram (ROUGE-N), word sequences (ROUGE-L
and ROUGE-W) and word pairs (ROUGE-S and
ROUGE-SU) between the candidate summary and
the reference summary. ROUGE parameters were
set as the same as DUC 2007 evaluation setup. All
the ROUGE measures were calculated by running
ROUGE-1.5.5 with stemming but no removal of
stopwords. The ROUGE run-time parameters are:
ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2
-m -l 250 -a
11
The purpose of our experiments is to study the
impact of the syntactic and semantic representation
for complex question answering task. To accomplish
this, we generate summaries for the topics of DUC
2007 by each of our four systems defined as below:
(1) TF*IDF: system is the original topic-sensitive
LexRank described in Section 2 that uses the simi-
larity measures based on tf*idf.
(2) SYN: system measures the similarity between
the sentences using the syntactic tree and the gen-
eral tree kernel function defined in Section 4.1.
(3) SEM: system measures the similarity between
the sentences using the shallow semantic tree and
the shallow semantic tree kernel function defined in
Section 4.2.
(4) SYNSEM: system measures the similarity be-
tween the sentences using both the syntactic and
shallow semantic trees and their associated kernels.
For each sentence it measures the syntactic and se-
mantic similarity with the query and takes the aver-
age of these measures.
5.2 Evaluation Results
The comparison between the systems in terms of
their F-scores is given in Table 1. The SYN system
improves the ROUGE-1, ROUGE-L and ROUGE-
W scores over the TF*IDF system by 2.84%, 0.53%
and 2.14% respectively. The SEM system im-
proves the ROUGE-1, ROUGE-L, ROUGE-W, and
ROUGE-SU scores over the TF*IDF system by
8.46%, 6.54%, 6.56%, and 11.68%, and over the
SYN system by 5.46%, 5.98%, 4.33%, and 12.97%
respectively. The SYNSEM system improves the
ROUGE-1, ROUGE-L, ROUGE-W, and ROUGE-
SU scores over the TF*IDF system by 4.64%,
1.63%, 2.15%, and 4.06%, and over the SYN sys-
tem by 1.74%, 1.09%, 0%, and 5.26% respectively.
The SEM system improves the ROUGE-1, ROUGE-
L, ROUGE-W, and ROUGE-SU scores over the
SYNSEM system by 3.65%, 4.84%, 4.32%, and
7.33% respectively which indicates that including
syntactic feature with the semantic feature degrades
the performance.
6 Conclusion
In this paper, we have introduced the syntactic and
shallow semantic structures and discussed their im-
Systems ROUGE 1 ROUGE L ROUGE W ROUGE SU
TF*IDF 0.359458 0.334882 0.124226 0.130603
SYN 0.369677 0.336673 0.126890 0.129109
SEM 0.389865 0.356792 0.132378 0.145859
SYNSEM 0.376126 0.340330 0.126894 0.135901
Table 1: ROUGE F-scores for different systems
pacts in measuring the similarity between the sen-
tences in the random walk framework for answer-
ing complex questions. Our experiments suggest the
following: (a) similarity measures based on the syn-
tactic tree and/or shallow semantic tree outperforms
the similarity measures based on the TF*IDF and (b)
similarity measures based on the shallow semantic
tree performs best for this problem.
References
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. In Proceedings of Neural Informa-
tion Processing Systems, pages 625?632, Vancouver,
Canada.
G. Erkan and D. R. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457?479.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2003. Shallow Semantic Parsing Using
Support Vector Machines. In Technical Report TR-
CSLR-2003-03, University of Colorado.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of the international con-
ference on Language Resources and Evaluation, Las
Palmas, Spain.
C. Y. Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
Workshop on Text Summarization Branches Out, Post-
Conference Workshop of Association for Computa-
tional Linguistics, pages 74?81, Barcelona, Spain.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Seman-
tic Kernels for Question/Answer Classificaion. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 776?783, Prague,
Czech Republic. ACL.
J. Otterbacher, G. Erkan, and D. R. Radev. 2005. Us-
ing Random Walks for Question-focused Sentence Re-
trieval. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 915?922,
Vancouver, Canada.
12
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 329?332,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Do Automatic Annotation Techniques Have Any Impact on Supervised
Complex Question Answering?
Yllias Chali
University of Lethbridge
Lethbridge, AB, Canada
chali@cs.uleth.ca
Sadid A. Hasan
University of Lethbridge
Lethbridge, AB, Canada
hasan@cs.uleth.ca
Shafiq R. Joty
University of British Columbia
Vancouver, BC, Canada
rjoty@cs.ubc.ca
Abstract
In this paper, we analyze the impact of
different automatic annotation methods on
the performance of supervised approaches
to the complex question answering prob-
lem (defined in the DUC-2007 main task).
Huge amount of annotated or labeled
data is a prerequisite for supervised train-
ing. The task of labeling can be ac-
complished either by humans or by com-
puter programs. When humans are em-
ployed, the whole process becomes time
consuming and expensive. So, in order
to produce a large set of labeled data we
prefer the automatic annotation strategy.
We apply five different automatic anno-
tation techniques to produce labeled data
using ROUGE similarity measure, Ba-
sic Element (BE) overlap, syntactic sim-
ilarity measure, semantic similarity mea-
sure, and Extended String Subsequence
Kernel (ESSK). The representative super-
vised methods we use are Support Vec-
tor Machines (SVM), Conditional Ran-
dom Fields (CRF), Hidden Markov Mod-
els (HMM), and Maximum Entropy (Max-
Ent). Evaluation results are presented to
show the impact.
1 Introduction
In this paper, we consider the complex question
answering problem defined in the DUC-2007 main
task
1
. We focus on an extractive approach of sum-
marization to answer complex questions where a
subset of the sentences in the original documents
are chosen. For supervised learning methods,
huge amount of annotated or labeled data sets are
obviously required as a precondition. The deci-
sion as to whether a sentence is important enough
1
http://www-nlpir.nist.gov/projects/duc/duc2007/
to be annotated can be taken either by humans or
by computer programs. When humans are em-
ployed in the process, producing such a large la-
beled corpora becomes time consuming and ex-
pensive. There comes the necessity of using au-
tomatic methods to align sentences with the in-
tention to build extracts from abstracts. In this
paper, we use ROUGE similarity measure, Basic
Element (BE) overlap, syntactic similarity mea-
sure, semantic similarity measure, and Extended
String Subsequence Kernel (ESSK) to automati-
cally label the corpora of sentences (DUC-2006
data) into extract summary or non-summary cat-
egories in correspondence with the document ab-
stracts. We feed these 5 types of labeled data into
the learners of each of the supervised approaches:
SVM, CRF, HMM, and MaxEnt. Then we exten-
sively investigate the performance of the classi-
fiers to label unseen sentences (from 25 topics of
DUC-2007 data set) as summary or non-summary
sentence. The experimental results clearly show
the impact of different automatic annotation meth-
ods on the performance of the candidate super-
vised techniques.
2 Automatic Annotation Schemes
Using ROUGE Similarity Measures ROUGE
(Recall-Oriented Understudy for Gisting Evalua-
tion) is an automatic tool to determine the qual-
ity of a summary using a collection of measures
ROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-W
and ROUGE-S which count the number of over-
lapping units such as n-gram, word-sequences,
and word-pairs between the extract and the ab-
stract summaries (Lin, 2004). We assume each
individual document sentence as the extract sum-
mary and calculate its ROUGE similarity scores
with the corresponding abstract summaries. Thus
an average ROUGE score is assigned to each sen-
tence in the document. We choose the top N sen-
tences based on ROUGE scores to have the label
329
+1 (summary sentences) and the rest to have the
label ?1 (non-summary sentences).
Basic Element (BE) Overlap Measure We ex-
tract BEs, the ?head-modifier-relation? triples for
the sentences in the document collection using BE
package 1.0 distributed by ISI
2
. The ranked list
of BEs sorted according to their Likelihood Ra-
tio (LR) scores contains important BEs at the top
which may or may not be relevant to the abstract
summary sentences. We filter those BEs by check-
ing possible matches with an abstract sentence
word or a related word. For each abstract sen-
tence, we assign a score to every document sen-
tence as the sum of its filtered BE scores divided
by the number of BEs in the sentence. Thus, ev-
ery abstract sentence contributes to the BE score
of each document sentence and we select the top
N sentences based on average BE scores to have
the label +1 and the rest to have the label ?1.
Syntactic Similarity Measure In order to cal-
culate the syntactic similarity between the abstract
sentence and the document sentence, we first parse
the corresponding sentences into syntactic trees
using Charniak parser
3
(Charniak, 1999) and then
we calculate the similarity between the two trees
using the tree kernel (Collins and Duffy, 2001).
We convert each parenthesis representation gener-
ated by Charniak parser to its corresponding tree
and give the trees as input to the tree kernel func-
tions for measuring the syntactic similarity. The
tree kernel of two syntactic trees T
1
and T
2
is ac-
tually the inner product of the two m-dimensional
vectors, v(T
1
) and v(T
2
):
TK(T
1
, T
2
) = v(T
1
).v(T
2
)
The TK (tree kernel) function gives the simi-
larity score between the abstract sentence and the
document sentence based on the syntactic struc-
ture. Each abstract sentence contributes a score to
the document sentences and the top N sentences
are selected to be annotated as +1 and the rest as
?1 based on the average of similarity scores.
Semantic Similarity Measure Shallow seman-
tic representations, bearing a more compact infor-
mation, can prevent the sparseness of deep struc-
tural approaches and the weakness of BOW mod-
els (Moschitti et al, 2007). To experiment with
semantic structures, we parse the corresponding
2
BE website:http://www.isi.edu/ cyl/BE
3
available at ftp://ftp.cs.brown.edu/pub/nlparser/
sentences semantically using a Semantic Role La-
beling (SRL) system like ASSERT
4
. ASSERT is
an automatic statistical semantic role tagger, that
can annotate naturally occuring text with semantic
arguments. We represent the annotated sentences
using tree structures called semantic trees (ST).
Thus, by calculating the similarity between STs,
each document sentence gets a semantic similarity
score corresponding to each abstract sentence and
then the topN sentences are selected to be labeled
as +1 and the rest as ?1 on the basis of average
similarity scores.
Extended String Subsequence Kernel (ESSK)
Formally, ESSK is defined as follows (Hirao et al,
2004):
K
essk
(T,U) =
d
?
m=1
?
t
i
?T
?
u
j
?U
K
m
(t
i
, u
j
)
K
m
(t
i
, u
j
) =
{
val(t
i
, u
j
) if m = 1
K
?
m?1
(t
i
, u
j
) ? val(t
i
, u
j
)
Here, K
?
m
(t
i
, u
j
) is defined below. t
i
and u
j
are the nodes of T and U , respectively. Each node
includes a word and its disambiguated sense. The
function val(t, u) returns the number of attributes
common to the given nodes t and u.
K
?
m
(t
i
, u
j
) =
{
0 if j = 1
?K
?
m
(t
i
, u
j?1
) +K
??
m
(t
i
, u
j?1
)
Here ? is the decay parameter for the number
of skipped words. We choose ? = 0.5 for this
research. K
??
m
(t
i
, u
j
) is defined as:
K
??
m
(t
i
, u
j
) =
{
0 if i = 1
?K
??
m
(t
i?1
, u
j
) +K
m
(t
i?1
, u
j
)
Finally, the similarity measure is defined after
normalization as below:
sim
essk
(T,U) =
K
essk
(T,U)
?
K
essk
(T, T )K
essk
(U,U)
Indeed, this is the similarity score we assign to
each document sentence for each abstract sentence
and in the end, top N sentences are selected to
be annotated as +1 and the rest as ?1 based on
average similarity scores.
3 Experiments
Task Description The problem definition at
DUC-2007 was: ?Given a complex question (topic
description) and a collection of relevant docu-
ments, the task is to synthesize a fluent, well-
organized 250-word summary of the documents
4
available at http://cemantix.org/assert
330
that answers the question(s) in the topic?. We con-
sider this task and use the five automatic annota-
tion methods to label each sentence of the 50 doc-
ument sets of DUC-2006 to produce five differ-
ent versions of training data for feeding the SVM,
HMM, CRF and MaxEnt learners. We choose the
top 30% sentences (based on the scores assigned
by an annotation scheme) of a document set to
have the label +1 and the rest to have ?1. Unla-
beled sentences of 25 document sets of DUC-2007
data are used for the testing purpose.
Feature Space We represent each of the
document-sentences as a vector of feature-values.
We extract several query-related features and
some other important features from each sen-
tence. We use the features: n-gram overlap,
Longest Common Subsequence (LCS), Weighted
LCS (WLCS), skip-bigram, exact word overlap,
synonym overlap, hypernym/hyponym overlap,
gloss overlap, Basic Element (BE) overlap, syn-
tactic tree similarity measure, position of sen-
tences, length of sentences, Named Entity (NE),
cue word match, and title match (Edmundson,
1969).
Supervised Systems For SVM we use second
order polynomial kernel for the ROUGE and
ESSK labeled training. For the BE, syntactic, and
semantic labeled training third order polynomial
kernel is used. The use of kernel is based on the
accuracy we achieved during training. We apply
3-fold cross validation with randomized local-grid
search for estimating the value of the trade-off pa-
rameter C. We try the value of C in 2
i
following
heuristics, where i ? {?5,?4, ? ? ? , 4, 5} and set
C as the best performed value 0.125 for second
order polynomial kernel and default value is used
for third order kernel. We use SVM
light 5
pack-
age for training and testing in this research. In case
of HMM, we apply the Maximum Likelihood Esti-
mation (MLE) technique by frequency counts with
add-one smoothing to estimate the three HMM
parameters: initial state probabilities, transition
probabilities and emission probabilities. We use
Dr. Dekang Lin?s HMM package
6
to generate
the most probable label sequence given the model
parameters and the observation sequence (unla-
beled DUC-2007 test data). We use MALLET-0.4
NLP toolkit
7
to implement the CRF. We formu-
5
http://svmlight.joachims.org/
6
http://www.cs.ualberta.ca/
?
lindek/hmm.htm
7
http://mallet.cs.umass.edu/
late our problem in terms of MALLET?s Simple-
Tagger class which is a command line interface to
the MALLET CRF class. We modify the Simple-
Tagger class in order to include the provision for
producing corresponding posterior probabilities of
the predicted labels which are used later for rank-
ing sentences. We build the MaxEnt system using
Dr. Dekang Lin?s MaxEnt package
8
. To define the
exponential prior of the ? values in MaxEnt mod-
els, an extra parameter ? is used in the package
during training. We keep the value of ? as default.
Sentence Selection The proportion of important
sentences in the training data will differ from the
one in the test data. A simple strategy is to rank
the sentences in a document, then select the top N
sentences. In SVM systems, we use the normal-
ized distance from the hyperplane to each sample
to rank the sentences. Then, we choose N sen-
tences until the summary length (250 words for
DUC-2007) is reached. For HMM systems, we
use Maximal Marginal Relevance (MMR) based
method to rank the sentences (Carbonell et al,
1997). In CRF systems, we generate posterior
probabilities corresponding to each predicted label
in the label sequence to measure the confidence of
each sentence for summary inclusion. Similarly
for MaxEnt, the corresponding probability values
of the predicted labels are used to rank the sen-
tences.
Evaluation Results The multiple ?reference
summaries? given by DUC-2007 are used in the
evaluation of our summary content. We evalu-
ate the system generated summaries using the au-
tomatic evaluation toolkit ROUGE (Lin, 2004).
We report the three widely adopted important
ROUGE metrics in the results: ROUGE-1 (uni-
gram), ROUGE-2 (bigram) and ROUGE-SU (skip
bi-gram). Figure 1 shows the ROUGE F-measures
for SVM, HMM, CRF and MaxEnt systems. The
X-axis containing ROUGE, BE, Synt (Syntactic),
Sem (Semantic), and ESSK stands for the annota-
tion scheme used. The Y-axis shows the ROUGE-
1 scores at the top, ROUGE-2 scores at the bottom
and ROUGE-SU scores in the middle. The super-
vised systems are distinguished by the line style
used in the figure.
From the figure, we can see that the ESSK la-
beled SVM system is having the poorest ROUGE -
1 score whereas the Sem labeled system performs
8
http://www.cs.ualberta.ca/
?
lindek/downloads.htm
331
Figure 1: ROUGE F-scores for different supervised systems
best. The other annotation methods? impact is al-
most similar here in terms of ROUGE-1. Ana-
lyzing ROUGE-2 scores, we find that the BE per-
forms the best for SVM, on the other hand, Sem
achieves top ROUGE-SU score. As for the two
measures Sem annotation is performing the best,
we can typically conclude that Sem annotation is
the most suitable method for the SVM system.
ESSK works as the best for HMM and Sem la-
beling performs the worst for all ROUGE scores.
Synt and BE labeled HMMs perform almost simi-
lar whereas ROUGE labeled system is pretty close
to that of ESSK. Again, we see that the CRF per-
forms best with the ESSK annotated data in terms
of ROUGE -1 and ROUGE-SU scores and Sem
has the highest ROUGE-2 score. But BE and Synt
labeling work bad for CRF whereas the ROUGE
labeling performs decently. So, we can typically
conclude that ESSK annotation is the best method
for the CRF system. Analyzing further, we find
that ESSK works best for MaxEnt and BE label-
ing is the worst for all ROUGE scores. We can
also see that ROUGE, Synt and Sem labeled Max-
Ent systems perform almost similar. So, from this
discussion we can come to a conclusion that SVM
system performs best if the training data uses se-
mantic annotation scheme and ESSK works best
for HMM, CRF and MaxEnt systems.
4 Conclusion and Future Work
In the work reported in this paper, we have per-
formed an extensive experimental evaluation to
show the impact of five automatic annotation
methods on the performance of different super-
vised machine learning techniques in confronting
the complex question answering problem. Experi-
mental results show that Sem annotation is the best
for SVM whereas ESSK works well for HMM,
CRF and MaxEnt systems. In the near future,
we plan to work on finding more sophisticated ap-
proaches to effective automatic labeling so that we
can experiment with different supervised methods.
References
Jaime Carbonell, Yibing Geng, and Jade Goldstein.
1997. Automated query-relevant summarization and
diversity-based reranking. In IJCAI-97 Workshop on
AI in Digital Libraries, pages 12?19, Japan.
Eugene Charniak. 1999. A Maximum-Entropy-
Inspired Parser. In Technical Report CS-99-12,
Brown University, Computer Science Department.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
Neural Information Processing Systems, pages 625?
632, Vancouver, Canada.
Harold P. Edmundson. 1969. New methods in auto-
matic extracting. Journal of the ACM, 16(2):264?
285.
Tsutomu Hirao, Jun Suzuki, Hideki Isozaki, and Eisaku
Maeda. 2004. Dependency-based sentence align-
ment for multiple document summarization. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics, pages 446?452.
Chin-Yew Lin. 2004. ROUGE: A Package for Au-
tomatic Evaluation of Summaries. In Proceed-
ings of Workshop on Text Summarization Branches
Out, Post-Conference Workshop of Association for
Computational Linguistics, pages 74?81, Barcelona,
Spain.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
Syntactic and Shallow Semantic Kernels for Ques-
tion/Answer Classificaion. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 776?783, Prague, Czech
Republic. ACL.
332
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 476?479,
Prague, June 2007. c?2007 Association for Computational Linguistics
UofL: Word Sense Disambiguation Using Lexical Cohesion 
Yllias Chali  
        Department of Computer Science 
University of Lethbridge  
Lethbridge, Alberta, Canada, T1K 3M4 
chali@cs.uleth.ca 
Shafiq R. Joty 
Department of Computer Science 
University of Lethbridge  
Lethbridge, Alberta, Canada, T1K 3M4 
jotys@cs.uleth.ca 
 
 
Abstract 
One of the main challenges in the applica-
tions (i.e.: text summarization, question an-
swering, information retrieval, etc.) of 
Natural Language Processing is to deter-
mine which of the several senses of a word 
is used in a given context. The problem is 
phrased as ?Word Sense Disambiguation 
(WSD)? in the NLP community. This paper 
presents the dictionary based disambigua-
tion technique that adopts the assumption 
of one sense per discourse in the context of 
SemEval-2007 Task 7: ?Coarse-grained 
English all-words?.  
1 Introduction 
Cohesion can be defined as the way certain words 
or grammatical features of a sentence can connect 
it to its predecessors (and successors) in a text. 
(Halliday and Hasan, 1976) defined cohesion as 
?the set of possibilities that exist in the language 
for making text hang together?. Cohesion occurs 
where the interpretation of some element in the 
discourse is dependent on that of another. For ex-
ample, an understanding of the reference of a pro-
noun (i.e.: he, she, it, etc.) requires to look back to 
something that has been said before. Through this 
cohesion relation, two text clauses are linked to-
gether. 
Cohesion is achieved through the use in the text 
of semantically related terms, reference, ellipse and 
conjunctions (Barzilay and Elhadad, 1997). Among 
the different cohesion-building devices, the most 
easily identifiable and the most frequent type is 
lexical cohesion. Lexical cohesion is created by 
using semantically related words (repetitions, 
synonyms, hypernyms, hyponyms, meronyms and 
holonyms, glosses, etc.)  
Our technique used WordNet (Miller, 1990) as 
the knowledge source to find the semantic relations 
among the words in a text. We assign weights to 
the semantic relations. The technique can be de-
composed into two steps: (1) building a representa-
tion of all possible senses of the words and (2) dis-
ambiguating the words based on the highest score.  
The remainder of this paper is organized as fol-
lows. In the next section, we review previous work. 
In Section 3, we define the semantic relations and 
their weights. Section 4 presents our two step pro-
cedure for WSD. We conclude with the evaluation. 
2 Previous Work 
Lexical Chaining is the process of connecting se-
mantically related words, creating a set of chains 
that represent different threads of cohesion through 
the text (Galley and McKeown, 2003). This inter-
mediate representation of text has been used in 
many natural language processing applications, 
including automatic summarization (Barzilay and 
Elhadad, 1997; Silber and McCoy, 2003), informa-
tion retrieval (Al-Halimi and Kazman, 1998), and 
intelligent spell checking (Hirst and St-Onge, 
1998). 
Morris and Hirst (1991) at first proposed a man-
ual method for computing lexical chains and first 
computational model of lexical chains was intro-
duced by Hirst and St-Onge (1997). This linear-
time algorithm, however, suffers from inaccurate 
WSD, since their greedy strategy immediately dis-
ambiguates a word as it is first encountered. Later 
476
research (Barzilay and Elhadad, 1997) significantly 
alleviated this problem at the cost of a worse run-
ning time (quadratic); computational inefficiency is 
due to their processing of many possible combina-
tions of word senses in the text in order to decide 
which assignment is the most likely. Silber and 
McCoy (2003) presented an efficient linear-time 
algorithm to compute lexical chains, which models 
Barzilay?s approach, but nonetheless has inaccura-
cies in WSD. 
More recently, Galley and McKeown (2003) 
suggested an efficient chaining method that sepa-
rated WSD from the actual chaining. It performs 
the WSD before the construction of the chains. 
They showed that it could achieve more accuracy 
than the earlier ones. Our method follows the simi-
lar technique with some new semantic relations 
(i.e.: gloss, holonym, meronym). 
3 Semantic Relations 
We used WordNet2.11 (Miller, 1990) and eXtended 
WordNet (Moldovan and Mihalcea, 2001) as our 
knowledge source to find the semantic relations 
among the words in a context.  We assigned a 
weight to each semantic relation. The relations and 
their scores are summarized in the table 1. 
4 System Overview 
The global architecture of our system is shown in 
Figure 1. Each of the modules of the system is de-
scribed below. 
4.1 Context Processing 
Context-processing involves preprocessing the con-
texts using several tools.  We have used the follow-
ing tools:  
Extracting the main text: This module extracts 
the context of the target word from the source xml 
document removing the unnecessary tags and 
makes the context ready for further processing. 
 
Sentence Splitting, Text Stemming and 
Chunking: This module splits the context into sen-
tences, then stems out the words and chunks those. 
We used OAK systems 2  (Sekine, 2002) for this 
purpose.  
                                                 
1
 http://wordnet.princeton.edu/ 
2
 http://nlp.cs.nyu.edu/oak/ 
 
Candidate Words Extraction: This module ex-
tracts the candidate words (for task 7: noun, verb, 
adjective and adverb) from the chunked text. 
4.2 All Sense Representation 
Each candidate word is expanded to all of its 
senses. We created a hash representation to identify 
all possible word representations, motivated from 
Galley and McKeown (2003). Each word sense is 
inserted into the hash entry having the index value 
equal to its synsetID. For example, athlete and jock 
are inserted into the same hash entry (Figure 2). 
 
 
 
Figure 2.  Hash indexed by synsetID 
 
On insertion of the candidate sense into the hash 
we check to see if there exists an entry into the in-
dex value, with which the current word sense has 
one of the above mentioned relations. No disam-
biguation is done at this point; the only purpose is 
to build a representation used in the next stage of 
the algorithm. This representation can be shown as 
a disambiguation graph (Galley and McKeown, 
2003) where the nodes represent word instances 
with their WordNet senses and weighted edges 
connecting the senses of two different words repre-
sent semantic relations (Figure: 3). 
 
 
 
Figure 3. Partial Disambiguation graph, Bass has 
two senses, 1. Food related 2. Music instrument 
related sense. The instrument sense dominates over 
the fish sense as it has more relations (score) with 
the other words in the context. 
Athlete Jock 
Gymnast 
09675378 
10002518 
???
 
Hypernym/ 
Hyponym 
    
Bass   
Instrument sense 
sound 
property  
      Food sense 
Pitch 
Fish 
ground 
bass 
477
4.3 Sense Disambiguation 
We use the intermediate representation (disam-
biguation graph) to perform the WSD. We sum the 
weight of all edges leaving the nodes under their 
different senses. The one sense with the highest 
score is considered the most probable sense. For 
example in fig: 3 Bass is connected with three 
words: Pitch, ground bass and sound property by 
its instrument sense and with one word: Fish by its 
Food sense. For this specific example all the se-
mantic relations are of Hyponym/Hypernym type 
(score 0.33). So we get the score as in table 2.  
In case of tie between two or more senses, we 
select the one sense that comes first in WordNet, 
since WordNet orders the senses of a word by de-
creasing order of frequency. 
  
Sense Mne-
monic  
Score Disambigu-
ated Sense 
4928349 Musical 
Instru-
ment 
3*0.33
=0.99 
7672239 Fish or 
Food 
0.33 
Musical In-
strument 
(4928349) 
 
Table 2.  Score of the senses of word ?Bass? 
 
 
 
Relation Definition Example Weight 
Repetition Same occurrences of the word Weather is great in Atlanta. Florida is 
having a really bad weather. 
1 
Synonym Words belonging to the same syn-
set in WordNet 
Not all criminals are outlaws. 1 
Hypernym 
and Hypo-
nym 
Y is a hypernym of X if X is a 
(kind of) Y And 
X is a hyponym of Y if X is a (kind 
of) Y. 
Peter bought a computer. It was a Dell 
machine. 
0.33 
Holonym 
And 
Meronym 
Y is a holonym of X if X is a part 
of Y And  
X is a meronym of Y if X is a part 
of Y 
The keyboard of this computer is not 
working. 
0.33 
Gloss Definition and/or example sen-
tences for a synset. 
Gloss of word ?dormitory? is  
{a college or university building con-
taining living quarters for students} 
0.33 
 
      Table 1: The relations and their associated weights 
 
 
 
 
 
                               
   Figure 1: Overview of WSD System 
 
 
Context  
Processing 
Sense Disam-
biguation 
 
All Sense Represen-
tation 
Disambiguated  
Sense 
Candidate words 
Extraction 
Source Con-
text 
Chunked Text 
478
             
5 Evaluation 
In SemEval-2007, we participated in Task 7: 
?Coarse-grained English all-words?. The evalua-
tion of our system is given below: 
 
Cases Precision Recall F1-measure 
Average 0.52592 0.48744 0.50595 
Best 0.61408 0.59239 0.60304 
Worst 0.44375 0.41159 0.42707 
 
6 Conclusion 
In this paper, we presented briefly our WSD sys-
tem in the context of SemEval 2007 Task 7. Along 
with normal WordNet relations, our method also 
included additional relations such as repetition and 
gloss using semantically enhanced tool, eXtended 
WordNet. After disambiguation, the intermediate 
representation (disambiguation graph) can be used 
to build the lexical chains which in tern can be used 
as an intermediate representation for other NLP 
applications such as text summarization, question 
answering, text clustering. This method (summing 
edge weights in selecting the right sense) of WSD 
before constructing the chain (Gallery and McKe-
own, 2003) outperforms the earlier methods of 
Barzilay and Elhadad (1997) and Silber and 
McCoy (2003) but this method is highly dependent 
on the lexical cohesion among words in a context. 
So the length of context is an important factor for 
our system to achieve good performance. For the 
task the context given for a tagged word was not so 
large to capture the semantic relations among 
words. This may be the one of the reasons for 
which our system could not achieve one of the best 
results. 
 
References 
Barzilay, R. and Elhadad, M.  1997. Using Lexical 
Chains for Text Summarization. In Proceedings 
of the 35th Annual Meeting of the Association 
for Computational Linguistics and the 8th Euro-
pean Chapter Meeting of the Association for 
Computational Linguistics, Workshop on Intel-
ligent Scalable Test Summarization, pages 10-
17, Madrid. 
 
Chali, Y. and Kolla, M. 2004. Summarization 
techniques at DUC 2004. In Proceedings of the 
Document Understanding Conference, pages 
105 -111, Boston. NIST. 
 
Galley, M. and McKeown, K. 2003. Improving 
Word Sense Disambiguation in Lexical Chain-
ing. In Proceedings of the 18th International 
Joint Conference on Artificial Intelligence 
(IJCAI?03), pages 1486-1488, Acapulco, Mex-
ico. 
 
Halliday M. and Hasan R. 1976. Cohesion in Eng-
lish. Longman, London. 
 
Harabagiu S. and Moldovan D. 1998. WordNet: 
An Electronic Lexical Database, chapter Knowl-
edge Processing on an Extended WordNet. MIT 
press. 
 
Hirst G. and St-Onge D.  1997. Lexical Chains as 
representation of context for the detection and 
correction of malapropisms. In Christiane Fell-
baum, editor, WordNet: An Electronic Lexical 
Database and Some of its Applications. MIT 
Press, pages 305-332. 
 
Morris J. and Hirst. G. 1991, Lexical Cohesion 
Computed by Thesaural Relations as an Indica-
tor of the Structure of Text .Computational Lin-
guistics, 17(1):21-48. 
 
Silber H.G. and McCoy K.F. 2002. Efficiently Com-
puted Lexical Chains As an Intermediate Representa-
tion for Automatic Text Summarization. Computa-
tional Linguistics, 28(4):487-496. 
479
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 681?690,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Fear the REAPER: A System for Automatic Multi-Document
Summarization with Reinforcement Learning
Cody Rioux
University of Lethbridge
Lethbridge, AB, Canada
cody.rioux@uleth.ca
Sadid A. Hasan
Philips Research North America
Briarcliff Manor, NY, USA
sadid.hasan@philips.com
Yllias Chali
University of Lethbridge
Lethbridge, AB, Canada
chali@cs.uleth.ca
Abstract
This paper explores alternate algorithms,
reward functions and feature sets for per-
forming multi-document summarization
using reinforcement learning with a high
focus on reproducibility. We show that
ROUGE results can be improved using
a unigram and bigram similarity metric
when training a learner to select sentences
for summarization. Learners are trained
to summarize document clusters based on
various algorithms and reward functions
and then evaluated using ROUGE. Our ex-
periments show a statistically significant
improvement of 1.33%, 1.58%, and 2.25%
for ROUGE-1, ROUGE-2 and ROUGE-
L scores, respectively, when compared
with the performance of the state of the
art in automatic summarization with re-
inforcement learning on the DUC2004
dataset. Furthermore query focused exten-
sions of our approach show an improve-
ment of 1.37% and 2.31% for ROUGE-2
and ROUGE-SU4 respectively over query
focused extensions of the state of the
art with reinforcement learning on the
DUC2006 dataset.
1 Introduction
The multi-document summarization problem has
received much attention recently (Lyngbaek,
2013; Sood, 2013; Qian and Liu, 2013) due to
its ability to reduce large quantities of text to a
human processable amount as well as its appli-
cation in other fields such as question answering
(Liu et al., 2008; Chali et al., 2009a; Chali et al.,
2009b; Chali et al., 2011b). We expect this trend
to further increase as the amount of linguistic data
on the web from sources such as social media,
wikipedia, and online newswire increases. This
paper focuses specifically on utilizing reinforce-
ment learning (Sutton and Barto, 1998; Szepesv,
2009) to create a policy for summarizing clusters
of multiple documents related to the same topic.
The task of extractive automated multi-
document summarization (Mani, 2001) is to se-
lect a subset of textual units, in this case sentences,
from the source document cluster to form a sum-
mary of the cluster in question. This extractive
approach allows the learner to construct a sum-
mary without concern for the linguistic quality of
the sentences generated, as the source documents
are assumed to be of a certain linguistic quality.
This paper aims to expand on the techniques used
in Ryang and Abekawa (2012) which uses a re-
inforcement learner, specifically TD(?), to create
summaries of document clusters. We achieve this
through introducing a new algorithm, varying the
feature space and utilizing alternate reward func-
tions.
The TD(?) learner used in Ryang and
Abekawa (2012) is a very early reinforcement
learning implementation. We explore the option of
leveraging more recent research in reinforcement
learning algorithms to improve results. To this end
we explore the use of SARSA which is a deriva-
tive of TD(?) that models the action space in ad-
dition to the state space modelled by TD(?). Fur-
thermore we explore the use of an algorithm not
based on temporal difference methods, but instead
on policy iteration techniques. Approximate Pol-
icy Iteration (Lagoudakis and Parr, 2003) gener-
ates a policy, then evaluates and iterates until con-
vergence.
The reward function in Ryang and Abekawa
(2012) is a delayed reward based on tf?idf values.
We further explore the reward space by introduc-
ing similarity metric calculations used in ROUGE
(Lin, 2004) and base our ideas on Saggion et al.
(2010). The difference between immediate re-
wards and delayed rewards is that the learner re-
681
ceives immediate feedback at every action in the
former and feedback only at the end of the episode
in the latter. We explore the performance differ-
ence of both reward types. Finally we develop
query focused extensions to both reward functions
and present their results on more recent Document
Understanding Conference (DUC) datasets which
ran a query focused task.
We first evaluate our systems using the
DUC2004 dataset for comparison with the results
in Ryang and Abekawa (2012). We then present
the results of query focused reward functions
against the DUC2006 dataset to provide refer-
ence with a more recent dataset and a more recent
task, specifically a query-focused summarization
task. Evaluations are performed using ROUGE
for ROUGE-1, ROUGE-2 and ROUGE-L values
for general summarization, while ROUGE-2 and
ROUGE-SU4 is used for query-focused summa-
rization. Furthermore we selected a small subset
of query focused summaries to be subjected to hu-
man evaluations and present the results.
Our implementation is named REAPER
(Relatedness-focused Extractive Automatic
summary Preparation Exploiting Reinfocement
learning) thusly for its ability to harvest a docu-
ment cluster for ideal sentences for performing
the automatic summarization task. REAPER is
not just a reward function and feature set, it is a
full framework for implementing summarization
tasks using reinforcement learning and is avail-
able online for experimentation.
1
The primary
contributions of our experiments are as follows:
? Exploration of TD(?), SARSA and Ap-
proximate Policy Iteration.
? Alternate REAPER reward function.
? Alternate REAPER feature set.
? Query focused extensions of automatic sum-
marization using reinforcement learning.
2 Previous Work and Motivation
Previous work using reinforcement learning for
natural language processing tasks (Branavan et
al., 2009; Wan, 2007; Ryang and Abekawa,
2012; Chali et al., 2011a; Chali et al., 2012)
inspired us to use a similar approach in our
experiments. Ryang and Abekawa (2012) im-
plemented a reinforcement learning approach to
1
https://github.com/codyrioux/REAPER
multi-document summarization which they named
Automatic Summarization using Reinforcement
Learning (ASRL). ASRL uses TD(?) to learn and
then execute a policy for summarizing a cluster of
documents. The algorithm performs N summa-
rizations from a blank state to termination, updat-
ing a set of state-value predictions as it does so.
From these N episodes a policy is created using
the estimated state-value pairs, this policy greed-
ily selects the best action until the summary enters
its terminal state. This summary produced is the
output of ASRL and is evaluated using ROUGE-
1, ROUGE-2, and ROUGE-L (Lin, 2004). The
results segment of the paper indicates that ASRL
outperforms greedy and integer linear program-
ming (ILP) techniques for the same task.
There are two notable details that provide the
motivation for our experiments; TD(?) is rela-
tively old as far as reinforcement learning (RL)
algorithms are concerned, and the optimal ILP did
not outperform ASRL using the same reward func-
tion. The intuition gathered from this is that if
the optimal ILP algorithm did not outperform the
suboptimal ASRL on the ROUGE evaluation, us-
ing the same reward function, then there is clearly
room for improvement in the reward function?s
ability to accurately model values in the state
space. Furthermore one may expect to achieve
a performance boost exploiting more recent re-
search by utilizing an algorithm that intends to
improve upon the concepts on which TD(?) is
based. These provide the motivation for the re-
mainder of the research preformed.
Query focused multi-document summarization
(Li and Li, 2013; Chali and Hasan, 2012b; Yin et
al., 2012; Wang et al., 2013) has recently gained
much attention due to increasing amounts of tex-
tual data, as well as increasingly specific user de-
mands for extracting information from said data.
This is reflected in the query focused tasks run in
the Document Understanding Conference (DUC)
and Text Analysis Conference (TAC) over the past
decade. This has motivated us to design and im-
plement query focused extensions to these rein-
forcement learning approaches to summarization.
There has been some research into the effects of
sentence compression on the output of automatic
summarization systems (Chali and Hasan, 2012a),
specifically the evaluation results garnered from
compressing sentences before evaluation (Qian
and Liu, 2013; Lin and Rey, 2003; Ryang and
682
Abekawa, 2012). However Ryang and Abekawa
(2012) found this technique to be ineffective in im-
proving ROUGE metrics using a similar reinforce-
ment learning approach to this paper, as a result
we will not perform any further exploration into
the effects of sentence compression.
3 Problem Definition
We use an identical problem definition to Ryang
and Abekawa (2012). Assume the given cluster
of documents is represented as a set of textual
units D = {x
1
, x
2
, ? ? ? , x
n
} where |D| = n and
x
i
represents a single textual unit. Textual units
for the purposes of this experiment are the indi-
vidual sentences in the document cluster, that is
D = D
1
?D
2
? ? ? ? ?D
m
where m is the number
of documents in the cluster and eachD
i
represents
a document.
The next necessary component is the score
function, which is to be used as the reward for the
learner. The function score(s) can be applied to
any s ? D. s is a summary of the given document
or cluster of documents.
Given these parameters, and a length limitation
k we can define an optimal summary s
?
as:
s
?
= argmax score(s)
where s ? D and length(s) ? k (1)
It is the objective of our learner to create a pol-
icy that produces the optimal summary for its pro-
vided document cluster D. Henceforth the length
limitations used for general summarization will be
665 bytes, and query focused summarization will
use 250 words. These limitations on summary
length match those set by the Document Under-
standing Conferences associated with the dataset
utilized in the respective experiments.
4 Algorithms
TD(?) and SARSA (Sutton and Barto, 1998) are
temporal difference methods in which the primary
difference is that TD(?) models state value pre-
dictions, and SARSA models state-action value
predictions. Approximate Policy Iteration (API)
follows a different paradigm by iteratively improv-
ing a policy for a markov decision process until the
policy converges.
4.1 TD(?)
In the ASRL implementation of TD(?) the learn-
ing rate ?
k
and temperature ?
k
decay as learning
progresses with the following equations with k set
to the number of learning episodes that had taken
place.
?
k
= 0.001 ? 101/(100 + k
1.1
) (2)
?
k
= 1.0 ? 0.987
k?1
(3)
One can infer from the decreasing values of ?
k
that as the number of elapsed episodes increases
the learner adjusts itself at a smaller rate. Simi-
larly as the temperature ?
k
decreases the action se-
lection policy becomes greedier and thus performs
less exploration, this is evident in (5) below.
Note that unlike traditional TD(?) implementa-
tions the eligibility trace e resets on every episode.
The reasons for this will become evident in the
experiments section of the paper in which ? =
1, ? = 1 and thus there is no decay during an
episode and complete decay after an episode. The
same holds true for SARSA below.
The action-value estimation Q(s, a) is approxi-
mated as:
Q(s, a) = r + ?V (s
?
) (4)
The policy is implemented as such:
policy(a|s; ?; ?) =
e
Q(s,a)/?
?
a?A
e
Q(s,a)/?
(5)
Actions are selected probabilistically using soft-
max selection (Sutton and Barto, 1998) from a
Boltzmann distribution. As the value of ? ap-
proaches 0 the distribution becomes greedier.
4.2 SARSA
SARSA is implemented in a very similar manner
and shares ?
k
, ?
k
, ?(s), m, and policy(s) with the
TD(?) implementation above. SARSA is also
a temporal difference algorithm and thus behaves
similarly to TD(?) with the exception that values
are estimated not only on the state s but a state-
action pair [s, a].
4.3 Approximate Policy Iteration
The third algorithm in our experiment uses Ap-
proximate Policy Iteration (Lagoudakis and Parr,
2003) to implement a reinforcement learner. The
683
novelty introduced by (Lagoudakis and Parr,
2003) is that they eschew standard representations
for a policy and instead use a classifier to represent
the current policy pi. Further details on the algo-
rithm can be obtained from Lagoudakis and Parr
(2003).
5 Experiments
Our state space S is represented simply as a three-
tuple [s a f ] in which s is the set of textual units
(sentences) that have been added to the summary,
a is a sequence of actions that have been per-
formed on the summary and f is a boolean with
value 0 representing non-terminal states and 1 rep-
resenting a summary that has been terminated.
The individual units in our action space are de-
fined as [:insert x
i
] where x
i
is a textual unit as
described earlier, let us define D
i
as the set [:in-
sert x
i
] for all x
i
? D where D is the document
set. We also have one additional action [:finish]
and thus we can define our action space.
A = D
i
? {[: finish]} (6)
The actions eligible to be executed on any given
state s is defined by a function actions(A, s):
actions(A, s) =
{
[: finish] if length(s) > k
A? a
t
otherwise
(7)
The state-action transitions are defined below:
[s
t
, a
t
, 0]
a=insertx
i
???????? [s
t
? x
i
, a
t
? a , 0] (8)
[s
t
, a
t
, 0]
:finish
????? [s
t
, a
t
, 1] (9)
[s
t
, a
t+1
, 1]
any
??? [s
t
, a
t
, 1] (10)
Insertion adds both the content of the textual
unit x
i
to the set s as well as the action itself to
set a. Conversely finishing does not alter s or a
but it flips the f bit to on. Notice from (10) that
once a state is terminal any further actions have no
effect.
5.1 Feature Space
We present an alternate feature set called
REAPER feature set based on the ideas presented
in Ryang and Abekawa (2012). Our proposed fea-
ture set follows a similar format to the previous
one but depends on the presence of top bigrams
instead of tf ? idf words.
? One bit b ? 0, 1 for each of the top n bigrams
(Manning and Sch?utze, 1999) present in the
summary.
? Coverage ratio calculated as the sum of the
bits in the previous feature divided by n.
? Redundancy Ratio calculated as the number
of redundant times a bit in the first feature is
flipped on, divided by n.
? Length Ratio calculated as length(s)/k
where k is the length limit.
? Longest common subsequence length.
? Length violation bit. Set to 1 if length(s) >
k
Summaries which exceed the length limitation
k are subject to the same reduction as the ASRL
feature set (Ryang and Abekawa, 2012) to an all
zero vector with the final bit set to one.
5.2 Reward Function
Our reward function (termed as REAPER reward)
is based on the n-gram concurrence score metric,
and the longest-common-subsequence recall met-
ric contained within ROUGE (Lin, 2004).
reward(s) =
?
?
?
?
?
?1, if length(s) > k
score(s) if s is terminal
0 otherwise
(11)
Where score is defined identically to ASRL,
with the exception that Sim is a new equation
based on ROUGE metrics.
score(s) =
?
x
i
?S
?
s
Rel(x
i
)?
?
x
i
,x
j
?S,i<j
(1? ?
s
)Red(x
i
, x
j
)
(12)
Rel(x
i
) = Sim(x
i
, D) + Pos(x
i
)
?1
(13)
Red(x
i
, x
j
) = Sim(x
i
, x
j
) (14)
684
Ryang and Abekawa (2012) experimentally de-
termined a value of 0.9 for the ?
s
parameter. That
value is used herein unless otherwise specified.
Sim has been redefined as:
Sim(s) =1 ? ngco(1, D, s)+
4 ? ngco(2, D, s)+
1 ? ngco(3, D, s)+
1 ? ngco(4, D, s)+
1 ? rlcs(D, s)
(15)
and ngco is the ngram co-occurence score met-
ric as defined by Lin (2004).
ngco(n,D, s) =
?
r?D
?
ngram?r
Count
match
(ngram)
?
S
r
?D
?
ngram?r
Count(ngram)
(16)
Where n is the n-gram count for example 2 for
bigrams, D is the set of documents, and s is the
summary in question. Count
match
is the maxi-
mum number of times the ngram occurred in either
D or s.
The rlcs(R,S) is also a recall oriented mea-
sure based on longest common subsequence
(Hirschberg, 1977). Recall was selected as
DUC2004 tasks favoured a ? value for F-Measure
(Lin, 2004) high enough that only recall would
be considered. lcs is the longest common sub-
sequence, and length(D) is the total number of
tokens in the reference set D.
rlcs(D, s) =
lcs(D, s)
length(D)
(17)
We are measuring similarity between sentences
and our entire reference set, and thusly our D is
the set of documents defined in section 3. This
is also a delayed reward as the provided reward is
zero until the summary is terminal.
5.2.1 Query Focused Rewards
We have proposed an extension to both reward
functions to allow for query focused (QF) summa-
rization. We define a function score
?
which aims
to balance the summarization abilities of the re-
ward with a preference for selecting textual units
related to the provided query q. Both ASRL and
REAPER score functions have been extended in
the following manner where Sim is the same sim-
ilarity functions used in equation (13) and (15).
score
?
(q, s) = ?Sim(q, s) + (1? ?)score(s)
(18)
The parameter ? is a balancing factor between
query similarity and overall summary score in
which 0 <= ? <= 1, we used an arbitrarily cho-
sen value of 0.9 in these experiments. In the case
of ASRL the parameter q is the vectorized version
of the query function with tf ? idf values, and for
Sim q is a sequence of tokens which make up the
query, stemmed and stop-words removed.
5.2.2 Immediate Rewards
Finally we also employ immediate versions of the
reward functions which behave similarly to their
delayed counterparts with the exception that the
score is always provided to the caller regardless of
the terminal status of state s.
reward(s) =
{
?1, if length(s) > k
score(s) otherwise
(19)
6 Results
We first present results
2
of our experiments, spec-
ifying parameters, and withholding discussion un-
til the following section. We establish a bench-
mark using ASRL and other top-scoring sum-
marization systems compared with REAPER us-
ing ROUGE. For generic multi-document sum-
marization we run experiments on all 50 docu-
ment clusters, each containing 10 documents, of
DUC2004 task 2 with parameters for REAPER
and ASRL fixed at ? = 1, ? = 1, and k = 665.
Sentences were stemmed using a Porter Stemmer
(Porter, 1980) and had the ROUGE stop word set
removed. All summaries were processed in this
manner and then projected back into their original
(unstemmed, with stop-words) state and output to
disk.
Config R-1 R-2 R-L
REAPER 0.40339 0.11397 0.36574
ASRL 0.39013 0.09479 0.33769
MCKP 0.39033 0.09613 0.34225
PEER65 0.38279 0.09217 0.33099
ILP 0.34712 0.07528 0.31241
GREEDY 0.30618 0.06400 0.27507
Table 1: Experimental results with ROUGE-1,
ROUGE-2 and ROUGE-L scores on DUC2004.
2
ROUGE-1.5.5 run with -m -s -p 0
685
Table 1 presents results for REAPER, ASRL
(Ryang and Abekawa, 2012), MCKP (Takamura
and Okumura, 2009), PEER65 (Conroy et al.,
2004) , and GREEDY (Ryang and Abekawa,
2012) algorithms on the same task. This allows
us to make a direct comparison with the results of
Ryang and Abekawa (2012).
REAPER results are shown using the TD(?)
algorithm, REAPER reward function, and ASRL
feature set. This is to establish the validity of the
reward function holding all other factors constant.
REAPER results for ROUGE-1, ROUGE-2 and
ROUGE-L are statistically significant compared to
the result set presented in Table 2 of Ryang and
Abekawa (2012) using p < 0.01.
Run R-1 R-2 R-L
REAPER 0 0.39536 0.10679 0.35654
REAPER 1 0.40176 0.11048 0.36450
REAPER 2 0.39272 0.11171 0.35766
REAPER 3 0.39505 0.11021 0.35972
REAPER 4 0.40259 0.11396 0.36539
REAPER 5 0.40184 0.11306 0.36391
REAPER 6 0.39311 0.10873 0.35481
REAPER 7 0.39814 0.11001 0.35786
REAPER 8 0.39443 0.10740 0.35586
REAPER 9 0.40233 0.11397 0.36483
Average 0.39773 0.11063 0.36018
Table 2: REAPER run 10 times on the DUC2004.
We present the results of 10 runs of REAPER,
with REAPER feature set.. As with ASRL,
REAPER does not converge on a stable solution
which is attributable to the random elements of
TD(?). Results in all three metrics are again
statistically significant compared to ASRL results
presented in the Ryang and Abekawa (2012) pa-
per. All further REAPER experiments use the bi-
gram oriented feature space.
Reward R-1 R-2 R-L
Delayed 0.39773 0.11397 0.36018
Immediate 0.32981 0.07709 0.30003
Table 3: REAPER with delayed and immediate re-
wards on DUC2004.
Table 3 shows the performance difference of
REAPER when using a delayed and immediate re-
ward. The immediate version of REAPER pro-
vides feedback on every learning step, unlike the
delayed version which only provides score at the
end of the episode.
Features R-1 R-2 R-L
ASRL 0.40339 0.11397 0.36574
REAPER 0.40259 0.11396 0.36539
Table 4: REAPER with alternate feature spaces on
DUC2004.
We can observe the results of using REAPER
with various feature sets in Table 4. Experiments
were run using REAPER reward, TD(?), and the
specified feature set.
Algorithm R-1 R-2 R-L
TD(?) 0.39773 0.11063 0.36018
SARSA 0.28287 0.04858 0.26172
API 0.29163 0.06570 0.26542
Table 5: REAPER with alternate algorithms on
DUC2004.
Table 5 displays the performance of REAPER
with alternate algorithms. TD(?) and SARSA
are run using the delayed reward feature, while
API requires an immediate reward and was thus
run with the immediate reward.
System R-2 R-SU4
REAPER 0.07008 0.11689
ASRL 0.05639 0.09379
S24 0.09505 0.15464
Baseline 0.04947 0.09788
Table 6: QF-REAPER on DUC2006.
For query-focused multi-document summariza-
tion we experimented with the DUC2006 system
task, which contained 50 document clusters con-
sisting of 25 documents each. Parameters were
fixed to ? = 1, ? = 1 and k = 250 words. In
Table 6 we can observe the results
3
of our query
focused systems against DUC2006?s top scorer
(S24) for ROUGE-2, and a baseline. The baseline
was generated by taking the most recent document
in the cluster and outputting the first 250 words.
Human Evaluations: We had three native
English-speaking human annotators evaluate a set
of four randomly chosen summaries produced by
REAPER on the DUC2004 dataset.
3
ROUGE-1.5.5 run with -n 2 -x -m -2 4 -u -c 95 -r 1000
-f A -p 0.5 -t 0 -l 250
686
Metric A1 A2 A3 AVG
Grammaticality 3.00 4.00 4.00 3.67
Redundancy 4.75 4.25 2.75 3.92
Referential Clarity 4.00 4.50 3.50 4.00
Focus 4.50 3.50 2.25 3.42
Structure 3.50 4.00 3.00 3.50
Responsiveness 4.25 3.75 3.00 3.67
Table 7: Human evaluation scores on DUC2004.
Table 7 shows the evaluation results accord-
ing to the DUC2006 human evaluation guidelines.
The first five metrics are related entirely to the lin-
guistic quality of the summary in question and the
final metric, Responsiveness, rates the summary
on its relevance to the source documents. Columns
represent the average provided by a given annota-
tor over the four summaries, and the AVG column
represents the average score for all three annota-
tors over all four summaries. Score values are an
integer between 1 and 5 inclusive.
7 Discussion
First we present a sample of a summary generated
from a randomly selected cluster. The following
summary was generated from cluster D30017 of
the DUC 2004 dataset using REAPER reward with
TD(?) and REAPER feature space.
A congressman who visited remote parts of
North Korea last week said Saturday that the food
and health situation there was desperate and de-
teriorating, and that millions of North Koreans
might have starved to death in the last few years.
North Korea is entering its fourth winter of chronic
food shortages with its people malnourished and
at risk of dying from normally curable illnesses,
senior Red Cross officials said Tuesday. More than
five years of severe food shortages and a near-total
breakdown in the public health system have led to
devastating malnutrition in North Korea and prob-
ably left an entire generation of children physi-
cally and mentally impaired, a new study by in-
ternational aid groups has found. Years of food
shortages have stunted the growth of millions of
North Korean children, with two-thirds of children
under age seven suffering malnourishment, U.N.
experts said Wednesday. The founder of South Ko-
rea?s largest conglomerate plans to visit his native
North Korea again next week with a gift of 501
cattle, company officials said Thursday. ?There is
enough rice.
We can observe that the summary is both syn-
tactically sound, and elegantly summarizes the
source documents.
Our baseline results table (Table 1) shows
REAPER outperforming ASRL in a statistically
significant manner on all three ROUGE metrics
in question. However we can see from the abso-
lute differences in score that very few additional
important words were extracted (ROUGE-1) how-
ever REAPER showed a significant improvement
in the structuring and ordering of those words
(ROUGE-2, and ROUGE-L).
The balancing factors used in the REAPER re-
ward function are responsible for the behaviour of
the reward function, and thus largely responsible
for the behaviour of the reinforcement learner. In
equation 15 we can see balance numbers of 1, 4, 1,
1, 1 for 1-grams, 2-grams, 3-grams, 4-grams, and
LCS respectively. In adjusting these values a user
can express a preference for a single metric or a
specific mixture of these metrics. Given that the
magnitude of scores for n-grams decrease as n in-
creases and given that the magnitude of scores for
1-grams is generally 3 to 4 times larger, in our ex-
perience, we can see that this specific reward func-
tion favours bigram similarity over unigram simi-
larity. These balance values can be adjusted to suit
the specific needs of a given situation, however we
leave exploration of this concept for future work.
We can observe in Figure 1 that ASRL does not
converge on a stable value, and dips towards the
300
th
episode while in Figure 2 REAPER does
not take nearly such a dramatic dip. These fig-
ures display average normalized reward for all 50
document clusters on a single run. Furthermore
we can observe that ASRL reaches it?s peak re-
ward around episode 225 while REAPER does so
around episode 175 suggesting that REAPER con-
verges faster.
7.1 Delayed vs. Immediate Rewards
The delayed vs. immediate rewards results in Ta-
ble 3 clearly show that delaying the reward pro-
vides a significant improvement in globally opti-
mizing the summary for ROUGE score. This can
be attributed to the ? = 1 and ? = 1 parame-
ter values being suboptimal for the immediate re-
ward situation. This has the added benefit of be-
ing much more performant computationally as far
fewer reward calculations need be done.
687
Figure 1: ASRL normalized reward.
7.2 Feature Space
The feature space experiments in Table 4 seem to
imply that REAPER performs similarly with both
feature sets. We are confident that an improve-
ment could be made through further experimenta-
tion. Feature engineering, however, is a very broad
field and we plan to pursue this topic in depth in
the future.
7.3 Algorithms
TD(?) significantly outperformed both SARSA
and API in the algorithm comparison. Ryang and
Abekawa (2012) conclude that the feature space is
largely responsible for the algorithm performance.
This is due to the fact that poor states such as those
that are too long, or those that contain few impor-
tant words will reduce to the same feature set and
receive negative rewards collectively. SARSA
loses this benefit as a result of its modelling of
state-action pairs.
API on the other hand may have suffered a per-
formance loss due to its requirements of an imme-
diate reward, this is because when using a delayed
reward if the trajectory of a rollout does not reach
a terminal state the algorithm will not be able to
make any estimations about the value of the state
in question. We propose altering the policy iter-
ation algorithm to use a trajectory length of one
episode instead of a fixed number of actions in or-
der to counter the need for an immediate reward
function.
7.4 Query Focused Rewards
From the ROUGE results in Table 6 we can infer
that while REAPER outperformed ASRL on the
query focused task, however it is notable that both
Figure 2: REAPER normalized reward.
systems under performed when compared to the
top system from the DUC2006 conference.
We can gather from these results that it is not
enough to simply naively calculate similarity with
the provided query in order to produce a query-
focused result. Given that the results produced by
the generic summarization task is rather accept-
able according to our human evaluations we sug-
gest that further research be focused on a proper
similarity metric between the query and summary
to improve the reward function?s overall ability to
score summaries in a query-focused setting.
8 Conclusion and Future Work
We have explored alternate reward functions, fea-
ture sets, and algorithms for the task of automatic
summarization using reinforcement learning. We
have shown that REAPER outperforms ASRL on
both generic summarization and the query focused
tasks. This suggests the effectiveness of our re-
ward function and feature space. Our results also
confirm that TD(?) performs best for this task
compared to SARSA and API .
Due to the acceptable human evaluation scores
on the general summarization task it is clear that
the algorithm produces acceptable summaries of
newswire data. Given that we have a framework
for generating general summaries, and the cur-
rent popularity of the query-focused summariza-
tion task, we propose that the bulk of future work
in this area be focused on the query-focused task
specifically in assessing the relevance of a sum-
mary to a provided query. Therefore we intend to
pursue future research in utilizing word-sense dis-
ambiguation and synonyms, as well as other tech-
niques for furthering REAPER?s query similarity
688
metrics in order to improve its ROUGE and human
evaluation scores on query-focused tasks.
Acknowledgments
We would like to thank the anonymous review-
ers for their useful comments. The research re-
ported in this paper was supported by the Nat-
ural Sciences and Engineering Research Council
(NSERC) of Canada - discovery grant and the Uni-
versity of Lethbridge. This work was done when
the second author was at the University of Leth-
bridge.
References
S . Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement Learning for Mapping
Instructions to Actions. In Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP of
the AFNLP, pages 82?90.
Y. Chali and S. A. Hasan. 2012a. On the Effectiveness
of Using Sentence Compression Models for Query-
Focused Multi-Document Summarization. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
457?474. Mumbai, India.
Y. Chali and S. A. Hasan. 2012b. Query-
focused Multi-document Summarization: Auto-
matic Data Annotations and Supervised Learning
Approaches. Journal of Natural Language Engi-
neering, 18(1):109?145.
Y. Chali, S. A. Hasan, and S. R. Joty. 2009a. Do Auto-
matic Annotation Techniques Have Any Impact on
Supervised Complex Question Answering? Pro-
ceedings of the Joint conference of the 47th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-IJCNLP 2009), pages 329?332.
Y. Chali, S. R. Joty, and S. A. Hasan. 2009b. Com-
plex Question Answering: Unsupervised Learning
Approaches and Experiments. Journal of Artificial
Intelligence Research, 35:1?47.
Y. Chali, S. A. Hasan, and K. Imam. 2011a. A
Reinforcement Learning Framework for Answering
Complex Questions. In Proceedings of the 16th
International Conference on Intelligent User Inter-
faces, pages 307?310. ACM, Palo Alto, CA, USA.
Y. Chali, S. A. Hasan, and S. R. Joty. 2011b. Improv-
ing Graph-based Random Walks for Complex Ques-
tion Answering Using Syntactic, Shallow Semantic
and Extended String Subsequence Kernels. Infor-
mation Processing and Management (IPM), Special
Issue on Question Answering, 47(6):843?855.
Y. Chali, S. A. Hasan, and K. Imam. 2012. Improv-
ing the Performance of the Reinforcement Learn-
ing Model for Answering Complex Questions. In
Proceedings of the 21st ACM Conference on Infor-
mation and Knowledge Management (CIKM 2012),
pages 2499?2502. ACM, Maui, Hawaii, USA.
J. Conroy, J. Goldstein, and D. Leary. 2004. Left-Brain
/ Right-Brain Multi-Document Summarization. In
Proceedings of the Document Un- derstanding Con-
ference (DUC 2004).
D. S. Hirschberg. 1977. Algorithms for the Longest
Common Subsequence Problem. In Journal of the
ACM, 24(4):664?675, October.
M. Lagoudakis and R. Parr. 2003. Reinforcement
learning as classification: Leveraging modern classi-
fiers. In Proceedings of the Twentieth International
Conference on Machine Learning, 20(1):424.
J. Li and S. Li. 2013. A Novel Feature-based Bayesian
Model for Query Focused Multi-document Summa-
rization. In Transactions of the Association for
Computational Linguistics, 1:89?98.
C. Lin and M. Rey. 2003. Improving Summariza-
tion Performance by Sentence Compression A Pi-
lot Study. In Proceedings of the Sixth International
Workshop on Information Retrieval with Asian Lan-
guages.
C. Lin. 2004. ROUGE : A Package for Automatic
Evaluation of Summaries. In Information Sciences,
16(1):25?26.
Y. Liu, S. Li, Y. Cao, C. Lin, D. Han, and Y. Yu.
2008. Understanding and Summarizing Answers
in Community-Based Question Answering Services.
In COLING ?08 Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
1(August):497?504.
S. Lyngbaek. 2013. SPORK: A Summarization
Pipeline for Online Repositories of Knowledge.
M.sc. thesis, California Polytechnic State Univer-
sity.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins Publishing.
C. D. Manning and H. Sch?utze. 1999. Foundations of
Statistical Natural Language Processing, volume 26
of . MIT Press.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
X. Qian and Y. Liu. 2013. Fast Joint Compression and
Summarization via Graph Cuts. In Conference on
Empirical Methods in Natural Language Process-
ing.
S. Ryang and T. Abekawa. 2012. Framework of Au-
tomatic Text Summarization Using Reinforcement
Learning. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, 1:256?265.
689
H. Saggion, C. Iria, T. M. Juan-Manuel, and E. San-
Juan. 2010. Multilingual Summarization Evalua-
tion without Human Models. In COLING ?10 Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, 1:1059?1067.
A. Sood. 2013. Towards Summarization of Written
Text Conversations. M.sc. thesis, International Insti-
tute of Information Technology, Hyderabad, India.
R. S. Sutton and A. G. Barto. 1998. Reinforcement
Learning: An Introduction. MIT Press.
C. A. Szepesv. 2009. Algorithms for Reinforcement
Learning. Morgan & Claypool Publishers.
H. Takamura and M. Okumura. 2009. Text Summa-
rization Model based on Maximum Coverage Prob-
lem and its Variant. In EACL ?09 Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
(April):781?789.
X Wan. 2007. Towards an Iterative Reinforcement Ap-
proach for Simultaneous Document Summarization
and Keyword Extraction. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 552?559.
L. Wang, H. Raghavan, V. Castelli, R. Florian, and
C. Cardie. 2013. A Sentence Compression Based
Framework to Query-Focused. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, 1:1384?1394.
W. Yin, Y. Pei, F. Zhang, and L. Huang. 2012. Query-
focused multi-document summarization based on
query-sensitive feature space. In Proceedings of the
21st ACM international conference on Information
and knowledge management - CIKM ?12, page 1652.
690
