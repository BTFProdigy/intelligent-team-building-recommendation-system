Synset Assignment for Bi-lingual Dictionary with Limited Resource 
 
       Virach Sornlertlamvanich  
Thatsanee Charoenporn  
Chumpol Mokarat 
Thai Computational Linguistics Lab.  
NICT Asia Research Center, 
Thailand Science Park,  
Pathumthani, Thailand 
{virach,thatsanee,chumpol}@tcllab.org 
Hitoshi Isahara 
National Institute of Information 
and Communications Technology 
3-5 Hikaridai, Seika-cho, soraku-gaun, 
Kyoto, Japan 619-0289 
isahara@nict.go.jp 
 
Hammam Riza  
IPTEKNET, Agency for the Assess-
ment and Application of Technology,     
Jakarta Pusat 10340, Indonesia  
hammam@iptek.net.id 
 
Purev Jaimai  
Center for Research on Language 
Processing, National University of 
Mongolia, Ulaanbaatar, Mongolia  
purev@num.edu.mn 
 
Abstract 
This paper explores an automatic WordNet 
synset assignment to the bi-lingual diction-
aries of languages having limited lexicon 
information. Generally, a term in a bi-
lingual dictionary is provided with very 
limited information such as part-of-speech, 
a set of synonyms, and a set of English 
equivalents. This type of dictionary is 
comparatively reliable and can be found in 
an electronic form from various publishers. 
In this paper, we propose an algorithm for 
applying a set of criteria to assign a synset 
with an appropriate degree of confidence to 
the existing bi-lingual dictionary. We show 
the efficiency in nominating the synset 
candidate by using the most common lexi-
cal information. The algorithm is evaluated 
against the implementation of Thai-
English, Indonesian-English, and Mongo-
lian-English bi-lingual dictionaries. The 
experiment also shows the effectiveness of 
using the same type of dictionary from dif-
ferent sources.  
1 Introduction 
The Princeton WordNet (PWN) (Fellbaum, 1998) 
is one of the most semantically rich English lexical 
databases that are widely used as a lexical knowl-
edge resource in many research and development 
topics. The database is divided by part of speech 
into noun, verb, adjective and adverb, organized in 
sets of synonyms, called synset, each of which 
represents ?meaning? of the word entry.  
Though WordNet was already used as a starting 
resource for developing many language WordNets, 
the construction of the WordNet for any languages 
can be varied according to the availability of the 
language resources. Some were developed from 
scratch, and some were developed from the combi-
nation of various existing lexical resources. Span-
ish and Catalan WordNets, for instance, are auto-
matically constructed using hyponym relation, 
monolingual dictionary, bilingual dictionary and 
taxonomy (Atserias et al, 1997). Italian WordNet 
(Magnini et al, 1994) is semi-automatically con-
structed from definition in monolingual dictionary, 
bilingual dictionary, and WordNet glosses. Hun-
garian WordNet uses bilingual dictionary, mono-
lingual explanatory dictionary, and Hungarian the-
saurus in the construction (Proszeky et al, 2002), 
etc. 
673
This paper presents a new method particularly to 
facilitate the WordNet construction by using the 
existing resources having only English equivalents 
and the lexical synonyms. Our proposed criteria 
and algorithm for application are evaluated by im-
plementing to Asian languages which occupy quite 
different language phenomena in terms of gram-
mars and word unit. 
To evaluate our criteria and algorithm, we use 
the PWN version 2.1 containing 207,010 senses 
classified into adjective, adverb, verb, and noun. 
The basic building block is a ?synset? which is 
essentially a context-sensitive grouping of syno-
nyms which are linked by various types of relation 
such as hyponym, hypernymy, meronymy, anto-
nym, attributes, and modification. Our approach is 
conducted to assign a synset to a lexical entry by 
considering its English equivalent and lexical 
synonyms. The degree of reliability of the assign-
ment is defined in terms of confidence score (CS) 
based on our assumption of the membership of the 
English equivalent in the synset. A dictionary from 
different source is also a reliable source to increase 
the accuracy of the assignment because it can ful-
fill the thoroughness of the list of English equiva-
lent and the lexical synonyms. 
The rest of this paper is organized as follows: 
Section 2 describes our criteria for synset assign-
ment. Section 3 provides the results of the experi-
ments and error analysis on Thai, Indonesian, and 
Mongolian. Section 4 evaluates the accuracy of the 
assignment result, and the effectiveness of the 
complimentary use of a dictionary from different 
sources. Section 5 shows a collaborative interface 
for revising the result of synset assignment. And 
Section 6 concludes our work. 
2 Synset Assignment 
A set of synonyms determines the meaning of a 
concept. Under the situation of limited resources 
on a language, English equivalent word in a bi-
lingual dictionary is a crucial key to find an 
appropriate synset for the entry word in question. 
The synset assignment criteria described in this 
Section relies on the information of English 
equivalent and synonym of a lexical entry, which 
is most commonly encoded in a bi-lingual 
dictionary. 
Synset Assignment Criteria 
Applying the nature of WordNet which introduces 
a set of synonyms to define the concept, we set up 
four criteria for assigning a synset to a lexical entry. 
The confidence score (CS) is introduced to 
annotate the likelihood of the assignment. The 
highest score, CS=4, is assigned to the synset that 
is evident to include more than one English 
equivalent of the lexical entry in question. On the 
contrary, the lowest score, CS=1, is assigned to 
any synset that occupies only one of the English 
equivalents of the lexical entry in question when 
multiple English equivalents exist. 
The details of assignment criteria are elaborated 
as in the followings. Li denotes the lexical entry, Ej 
denotes the English equivalent, Sk denotes the syn-
set, and ? denotes the member of a set: 
Case 1: Accept the synset that includes more 
than one English equivalent with confidence score 
of 4. 
Figure 1 simulates that a lexical entry L0 has two 
English equivalents of E0 and E1. Both E0 and E1 
are included in a synset of S1. The criterion implies 
that both E0 and E1 are the synset for L0 which can 
be defined by a greater set of synonyms in S1. 
Therefore the relatively high confidence score, 
CS=4, is assigned for this synset to the lexical en-
try. 
 
Figure 1. Synset assignment with SC=4 
Example: 
L0:  
E0: aim  E1: target 
S0: purpose, intent, intention, aim, design 
S1: aim, object, objective, target 
S2: aim 
In the above example, the synset, S1, is assigned 
to the lexical entry, L0, with CS=4. 
Case 2: Accept the synset that includes more 
than one English equivalent of the synonym of the 
lexical entry in question with confidence score of 3.  
In case that Case 1 fails in finding a synset that 
includes more than one English equivalent, the 
English equivalent of a synonym of the lexical en-
try is picked up to investigate. 
L0 
E0 
S0 ?
 
S1 
?
 
E1 
?
 
S2 
?
 
674
 Figure 2. Synset assignment with SC=3 
Figure 2 simulates that an English equivalent of 
a lexical entry L0 and its synonym L1 are included 
in a synset S1. In this case the synset S1 is assigned 
to both L0 and L1 with CS=3. The score in this case 
is lower than the one assigned in Case 1 because 
the synonym of the English equivalent of the lexi-
cal entry is indirectly implied from the English 
equivalent of the synonym of the lexical entry. The 
newly retrieved English equivalent may not be dis-
torted. 
Example: 
L0: 	
  L1: 
 
E0: stare  E1: gaze 
S0: gaze, stare S1: stare 
In the above example, the synset, S0, is assigned 
to the lexical entry, L0, with CS=3. 
Case 3: Accept the only synset that includes the 
only one English equivalent with confidence score 
of 2. 
 
Figure 3. Synset assignment with SC=2 
Figure 3 simulates the assignment of CS-2 when 
there is only one English equivalent and there is no 
synonym of the lexical entry. Though there is no 
any English equivalent to increase the reliability of 
the assignment, in the same time there is no 
synonym of the lexical entry to distort the relation. 
In this case, the only one English equivalent shows 
it uniqueness in the translation that can maintain a 
degree of the confidence. 
Example: 
L0:           E0: obstetrician     
S0: obstetrician, accoucheur 
In the above example, the synset, S0, is assigned 
to the lexical entry, L0, with CS=2. 
Case 4: Accept more than one synset that in-
cludes each of the English Equivalent with confi-
dence score of 1. 
Case 4 is the most relax rule to provide some re-
lation information between the lexical entry and a 
synset. Figure 4 simulates the assignment of CS=1 
to any relations that do not meet the previous crite-
ria but the synsets that include one of the English 
equivalent of the lexical entry. 
 
Figure 4. Synset assignment with SC=1 
Example: 
L0: 
 
E0: hole  E1: canal 
S0: hole, hollow   
S1: hole, trap, cakehole, maw, yap, gop 
S2: canal, duct, epithelial duct, channel 
In the above example, each synset, S0, S1, and S2 
is assigned to lexical entry L0, with CS=1. 
3 Experiment results 
We applied the synset assignment criteria to a 
Thai-English dictionary (MMT dictionary) (CICC, 
1995) with the synset from WordNet 2.1. To com-
pare the ratio of assignment for Thai-English dic-
tionary, we also investigate the synset assignment 
of Indonesian-English and Mongolian-English dic-
tionaries. 
 WordNet (synset) T-E Dict (entry) 
 total assigned total assigned 
Noun 145,103 18,353 (13%) 43,072
11,867 
(28%)
Verb 24,884 1,333 (5%) 17,669
2,298 
(13%)
Adjective 31,302 4,034 (13%) 18,448
3,722 
(20%)
Adverb 5,721 737 (13%) 3,008
1,519 
(51%)
total 207,010 24,457 (12%) 82,197
19,406 
(24%)
Table 1. Synset assignment to T-E dictionary 
In our experiment, there are only 24,457 synsets 
from 207,010 synsets, which is 12% of the total 
number of the synset that can be assigned to Thai 
lexical entries. Table 1 shows the successful rate in 
assigning synset to Thai-English dictionary. About 
24 % of Thai lexical entries are found with the 
English equivalents that meet one of our criteria.  
Going through the list of unmapped lexical en-
try, we can classify the errors into three groups:- 
1. Compound 
The English equivalent is assigned in a com-
L0 E0 
S0 ?
 
S1 
?
 
E1 
?
 
S2 
?
 
L1 
L0 E0 S0 
?
 
L0 
E0 
S0 ?
 
S1 
?
 
E1 
S2 
?
 
675
pound, especially in case that there is no an 
appropriate translation to represent exactly 
the same sense. For example, 
L: 		Corpus Building for Mongolian Language 
Purev Jaimai 
Center for Research on Language Processing, 
National University of Mongolia, Mongolia 
purev@num.edu.mn 
Odbayar Chimeddorj 
Center for Research on Language Processing, 
National University of Mongolia, Mongolia 
odbayar@num.edu.mn 
 
 
Abstract 
This paper presents an ongoing research 
aimed to build the first corpus, 5 million 
words, for Mongolian language by focus-
ing on annotating and tagging corpus texts 
according to TEI XML (McQueen, 2004) 
format. Also, a tool, MCBuilder, which 
provides support for flexibly and manually 
annotating and manipulating the corpus 
texts with XML structure, is presented. 
1 Introduction 
Mongolian researchers quite recently have begun 
to be involved in the research area of Natural Lan-
guage Processing. All necessary linguistic re-
sources, which are required for Mongolian lan-
guage processing, have to be built from scratch, 
and then they should be shared in public research 
for the rapid development of Mongolian language 
processing. 
This ongoing research aims to build a tagged 
and parsed 5 million words corpus for Mongolian 
by developing a spell-checker, tagger, sentence-
parser and others (see Figure 1 and 2). Also, we 
needed to develop a tagset for the corpus because 
there was not any tagset for Mongolian and the 
traditional words categories are not appropriate to 
it. Thus, we designed a high level tagset, which 
consists of 20 tags, and are further classifying them. 
Currently, we have collected and populated 500 
thousand words, 50 thousand of which have been 
manually tagged, into the corpus (see Figure 1). 
 
 
Figure 1. Current and future states of building a 
Mongolian corpus. 
 
And, we manually build the corpus until collect-
ing and annotating 1 million words and tagging 
100 thousand words of them for semi-
automatically building the corpus in the future. 
2 Corpus Building Design 
We are building the corpus as sub-corpora, which 
are a raw corpus, a cleaned corpus, a tagged corpus 
and a parsed corpus, separately for various kinds of 
studying and use on Mongolian language (Figure 
2). 
 
Figure 2. Schema of building a Mongolian  
corpus. 
At first, we are collecting the editorial articles of 
Unen newspaper (Unen publish), which is one of 
The 6th Workshop on Asian Languae Resources, 2008
97
the best written newspapers in Mongolia, by using 
OCR application. We will also collect laws, school 
book, and literary text (see Figure 3). 
 
 
Figure 3. Text sizes included in the corpus. 
 
The corpus annotation follows TEI XML stan-
dard. According to the work scope, the annotating 
part is divided into two parts that are structural an-
notation such as paragraphs, sentences, and so on, 
and POS tagging. 
The structure of the text annotation is presented 
in Figure 4. 
 
<tei> 
  <teiHeader> 
    <fileDesc/ > 
  </teiHeader> 
  <text> 
    <body> 
    <s> 
    <word id="" pos="tag">WORD</word> 
    </s> 
    </body> 
  </text> 
</tei> 
Figure 4. XML Structure of corpus text. 
 
For annotating two parts, once a manual corpus 
builder, called MCBuilder, were planned to de-
velop, we have developed the first version and 
used to annotating 500 thousand word texts and 
tagging 50 of them (see Figure 5). 
 
 
Figure 5. Screenshot of the corpus organizer and 
its main view. 
MCBuilder has three main windows that are (1) 
manipulating and organizing the corpus, (2) anno-
tating sample texts and (3) manipulating tagset as 
shown in Figure 5. 
3 Conclusion 
Mongolian language has hardly studied by com-
puter, and its traditional rules such as inflectional, 
derivational, part of speech, sentence constituents, 
etc are extremely difficult to computerize. Our re-
search works in the last few years showed it 
(Purev, 2006). Therefore, we are revising them by 
creating a corpus for computer processing. 
The proposals of this ongoing research are the 
first Mongolian 5 million words corpus, and tools 
that are spell-checker, tagger and parser. 
Currently, we have done followings: 
? Defined the corpus design, XML structure 
of the corpus text, and  the high level tagset 
? Collected and annotated 500 thousand 
words text 
? Tagged 50 thousand words 
? Released the first version of a Mongolian 
corpus building tool called MCBuilder 
? First versions of Syllable-parser and 
Morph-analyzer for Mongolian 
We are planning to complete the corpus in the 
next two years. 
4 Acknowledgement 
Here described work was carried out by support of 
PAN Localization Project (PANL10n). 
References 
PANL10n: PANLocalization Project. National Univer-
sity of Computer and Emerging Sciences, Pakistan.  
Purev J. 2006. Corpus for Mongolian Language, Re-
search Project, Mongolia. 
Purev J. and Odbayar Ch.. 2006. Towards Constructing 
the Corpus of Mongolian Language, Proceeding of 
ICEIC. 
Sperberg-McQueen, C. M. and Burnard, L.. 2004. Text 
Encoding Initiative. The XML version of the TEI 
Guidelines, Website.  
Unen press. 1984-1989. Editorial Articles. Mongolia 
The 6th Workshop on Asian Languae Resources, 2008
98
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 103?106,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Part of Speech Tagging for Mongolian Corpus  
 
 
Purev Jaimai and Odbayar Chimeddorj 
Center for Research on Language Processing 
National University of Mongolia 
{purev, odbayar}@num.edu.mn 
 
  
 
Abstract 
 
This paper introduces the current result of a 
research work which aims to build a 5 million 
tagged word corpus for Mongolian. Currently, 
around 1 million words have been auto-
matically tagged by developing a POS tagset 
and a bigram POS tagger.  
1 Introduction 
In the information era, language technologies 
and language processing have become a crucial 
issue to our social development which should 
benefit from the information technology. 
However, there are many communities whose 
languages have been less studied and developed 
for such need.   
Mongolian is one of the Altaic family 
languages. It has a great, long history. 
Nonetheless, till now, there are no corpora for 
the Mongolian language processing (Purev, 
2008). Two years ago, a research project to build 
a tagged corpus for Mongolian began at the 
Center for Research on Language Processing, 
National University of Mongolia. In the last year 
of this project, we developed a POS tagset and a 
POS tagger, and tagged around 1 million words 
by using them.  
Currently, we have manually checked 260 
thousand automatically tagged words. The rest of 
the tagged words have not checked yet because 
checking the tagged corpus needs more time and 
effort without any automatic or semi-automatic 
tool and method.  
The statistical method is used in our POS 
tagger. The rule based method requires the 
Mongolian language description which is 
appropriate to NLP techniques such as POS 
tagger. But, the current description of Mongolian 
is quite difficult to model for the computer 
processing. The tagger is based on a bigram 
method using HMM. The tagger is trained on 
around 250 thousand manually tagged words, 
and its accuracy is around 81 percent on tagging 
around 1 million words. 
2 POS Tagset Design  
We designed a POS tagset for Mongolian corpus 
by studying the main materials in Mongolia 
(PANLocalization, 2007). According to the 
agglutinative characteristics of Mongolian, the 
number of tags is not fixed, and it is possible to 
be created a lot of combinations of tags. 
The POS tagset consists of two parts that are a 
high-level tagset and a low-level tagset. The 
high-level tagset is similar to English tags such 
as noun, verb, adword etc. It consists of 29 tags 
(see Table 1), while the low-level tagset consists 
of 22 sub tags (see Table 2). The annotation of 
our tagset mainly follows the tagsets of 
PennTreebank (Beatrice, 1990) and BNC 
(Geoffrey, 2000).  
 
No. Description Tag 
Noun 
1. Noun N 
2. Pronoun PN 
3. Proper noun RN 
4. Adjective JJ 
5. Pro-adjective PJ 
6. Ad-adjective JJA 
7. Superlative JJS 
8. Number CD 
9. Preposition PR 
10. Postposition PT 
11. Abbreviation ABR 
12. Determiner DT 
13. Morph for possessive  POS 
Verb 
14. Verb V 
103
15.  Proverb PV 
16.  Adverb RB 
17.  Ya pro-word PY 
18.  Ad-adverb RBA 
19.  Modal MD 
20.  Auxiliary AUX
21.  Clausal adverb SRB 
22.  Ge-rooted verb GV 
23.  Co-conjunction CC 
24.  Sub-conjunction CS 
Others 
25.  Interjection INTJ 
26.  Question QN 
27.  Punctuation PUN 
28.  Foreign word FW 
29.  Negative NEG 
 
Table 1. High-Level Tagset for Mongolian 
 
No. Description Tag
Noun 
1.  Genitive G 
2.  Locative L 
3.  Accusative C 
4.  Ablative B 
5.  Instrumental I 
6.  Commutative M 
7.  Plural P 
8.  Possessive S 
9.  Approximate A 
10.  Abbreviated possessive H 
11.  Direction D 
Verb 
12.  Past D 
13.  Present P 
14.  Serial verb S 
15.  Future F 
16.  Infinitive/Base B 
17.  Coordination C 
18.  Subordination S 
19.  1st person 1 
20.  2nd person 2 
21.  3rd person 3 
22.  Negative X 
 
Table 2. Low-Level Tagset for Mongolian 
 
The high-level tags are classified into noun, verb 
and others as shown in Table 1. In the noun 
column, parts of speech in the noun phrase such 
as adjective, number, abbreviation and so on are 
included. In the verb column, parts of speech in 
the verb phrase are included. In the other 
column, the parts of speech except those of the 
noun and verb phrases are included. 
The low-level tagset is divided into two 
general types: noun phrase and verb phrase. It 
also consists of sub tags for inflectional suffixes 
such as cases, verb tenses etc. These tags are 
used mainly in combination with high-level tags. 
Currently, around 198 combination tags have 
been created. Most of them are for noun and verb 
inflections. Tag marking length is 1 ? 5 letters. 
Below we show some tagged sentences (see 
Figure 1). 
 
?? 
PN
???? 
N 
????? 
VP 
I horse ride 
I ride a horse 
 
?? 
PN
???????? 
NB 
????? 
VP 
I from horse fear 
I fear horses 
 
?? 
PN
?????????? 
NBS 
?????? 
VD 
I from my horse got off 
I got off my horse 
 
Figure 1. Mongolian Tagged Sentences 
 
Three example sentences are shown in Figure 1. 
Mongolian sentence is placed in the first line, 
and the following lines, second, third and fourth 
are POS tags, English parts of speech translation 
and English translation, respectively. A word 
?????? (horse) is used with different 
morphological forms such as nominative case in 
the first sentence, ablative case in the second 
sentence and ablative case followed by 
possessive in the last sentence.  The noun 
inflected with nominative case is tagged N, the 
noun inflected with ablative case is tagged NB, 
and the noun inflected with ablative case and 
possessive is tagged NBS according to the two 
level tagset. 
3 Bigram-POS Tagger 
Although the statistical method needs a tagged 
corpus which takes a lot of time and effort, it is 
more reliable for languages whose linguistic 
descriptions have difficulties in NLP and CL 
purposes. Thus, we are developing a statistical 
POS tagger for the project. 
104
The statistical method has been used on POS 
taggers since 1960s (Christopher, 2000). Some of 
these kinds of methods use HMM (Hidden 
Markov Model). The main principle of HMM is 
to assign the most possible tag to an input word 
in a sentence by using the probabilities of 
training data (Brian, 2007 and Daniel, 2000).  
 
 
Figure 2. Overview of Mongolian Bigram tagger 
 
The probabilities for the bigram tagger are 
calculated with the uni-tag frequency, the bi-tag 
frequency and the tokens from the training data 
(see Figure 2 for more detail). 
4 Automatic POS Tagging  
One million words of the Mongolian corpus have 
been tagged as the current result of the project. 
The tagging procedure is shown in Figure 3. 
 
 
 
Figure 3. Automatic Tagging Procedure for 
Mongolian Corpus 
When using the statistical POS tagger, the corpus 
tagging needs a training data. We have manually 
tagged around 110 thousand words. These 110 
thousand words are used as the first training data. 
The statistical information on the first training 
data is shown in Table 3. 
 
Words Word type Texts Tags
112,754 21,867 200 185 
 
Table 3. First Training Data 
 
As shown in Table 3, the training data consists of 
112,754 words. These words are divided into 
21,867 types. This training data can be a good 
representative of the corpus because the texts in 
which distinct to total word ratio is higher are 
chosen (see Table 4). 
 
No. Texts (Files) Distinct Words 
Total 
Words Percent 
1. MNCPR00320 113 125 0.9 
2. MNCPR00312 157 179 0.87 
3. MNCPR00118 118 136 0.86 
4. MNCPR00384 162 187 0.86 
5. MNCPR00122 238 279 0.85 
6. MNCPR00085 190 224 0.84 
7. MNCPR01190 320 379 0.84 
8. MNCPR00300 159 189 0.84 
9. MNCPR00497 241 288 0.83 
10. MNCPR00362 251 300 0.83 
 
Table 4. Some Texts Chosen for Training Data 
 
In Table 4, some of the texts that are chosen for 
the training data are shown. The most 
appropriate text that should be tagged at first is 
MNCPR00320 because its total words are 125 
and distinct words are 113. Consequently, its 
equality of words types and total word is almost 
the same, 0.9. The first 200 texts from the corpus 
are manually tagged for the training data.   
After training the bigram POS tagger, the 
corpus is tagged with it by 100 texts by 100 
texts. After that, we manually checked the 
automatically tagged texts, and corrected the 
incorrectly tagged words and tagged the 
untagged words, in fact, new words to the 
training data. After manually checking and 
tagging, the automatically tagged texts are added 
to the training data for improving the tagger 
accuracy. Then, this whole process is done again 
and again. After each cycle, the training data is 
increased, and the accuracy of the tagger is also 
105
improved. The statistics of automatic tagging the 
first 100 texts is shown in Table 5. 
 
Words Word type Texts Tags 
Untagged 
word 
73,552 9,854 100 108 16,322 
 
Untagged 
word type 
Mistagged 
words Accuracy
3,195 310 76.5 
 
Table 5. First 100 Texts Automatically Tagged 
 
As shown in Table 5, the untagged words are 22 
percent of the total words, and 0.5 percent is 
tagged incorrectly. Incorrectly tagged words are 
manually checked. The mistagged words are 
caused from the insufficient training data. In the 
result of the first automatic tagging, the tagger 
that is trained on around 110 thousand words can 
tag 76.5 percent of around 73 thousand words 
correctly.  
In tagging the second 100 texts, the accuracy is 
almost the same to the previous one because the 
training data is collected from texts containing 
more word types. The correctly tagged words are 
78 percent. After checking and tagging the 
automatically tagged 400 texts, the training data 
is around 260 thousand words as shown in Table 
6. 
 
Words Word types Texts Tags
260,312 27,212 400 198
 
Table 6. Current Training Data 
 
We tagged another 900 texts based on the 
training data in Table 6. They consist of around 
860 thousand words, and 81 percent is tagged. 
The statistics is shown in Table 7.  
 
Words Word type Texts
868,258 41,939 900
 
Untagged 
words 
Untagged word 
types Accuracy 
168,090 19,643 81 
 
Table 7. Automatically tagged words 
 
As shown in Table 7, the bigram POS tagger 
trained on 260 thousand words has tagged 
around 700 thousand of 868 thousand words. The 
accuracy is nearly the same to the previous 
tagging accuracy. That means the first training 
data is well selected, and includes main usage 
words. Therefore the accuracy of the first tagged 
200 texts is very close to that of 900 texts tagged 
later. 
5 Conclusion 
A research project building a 5 million word 
corpus is in its last phase. We have automatically 
tagged 1 million words of the corpus by 
developing a POS tagset and a bigram-POS 
tagger for Mongolian. The tagging accuracy is 
around 81 percent depending on the training 
data. Currently, the training data is around 260 
thousand words. As increasing the training data, 
the accuracy of the tagger can be up to 90 
percent. However, the increasing training data 
takes a lot of time and effort. The tagset currently 
consists of 198 tags. It may increase in the 
further tagging. In this year, we are planning to 
tag and check the 5 million word corpus. 
 
Acknowledgments 
Here described work was carried out by support 
of PAN Localization Project (PANL10n). 
References 
Brian Roark and Richard Sproat. 2007. 
Computational Approaches to Morphology 
and Syntax. Oxford University Press.  
Christopher D. Manning and Hinrich Schutze. 1999. 
Foundations of Statistical NLP. MIT Press. 
Daniel Jurafsky, James H. Martin. 2000. Speech and 
Language Processing. Singapore. 
PANLocalization Project. 2007. Research Report on 
Tagset for Mongolian. Center for Research on 
Language Processing, National University of 
Mongolia. 
Purev Jaimai and Odbayar Chimeddorj. 2008. Corpus 
Building for Mongolian. The Third International 
Joint Conference on Natural Language Processing, 
Hyderabad, India.  
106
