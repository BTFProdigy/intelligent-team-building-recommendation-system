Summarizing Encyclopedic Term Descriptions on the Web
Atsushi Fujii and Tetsuya Ishikawa
Graduate School of Library, Information and Media Studies
University of Tsukuba
1-2 Kasuga, Tsukuba, 305-8550, Japan
{fujii,ishikawa}@slis.tsukuba.ac.jp
Abstract
We are developing an automatic method to
compile an encyclopedic corpus from the
Web. In our previous work, paragraph-style
descriptions for a term are extracted from
Web pages and organized based on domains.
However, these descriptions are independent
and do not comprise a condensed text as
in hand-crafted encyclopedias. To resolve
this problem, we propose a summarization
method, which produces a single text from
multiple descriptions. The resultant sum-
mary concisely describes a term from dif-
ferent viewpoints. We also show the effec-
tiveness of our method by means of experi-
ments.
1 Introduction
Term descriptions, which have been carefully orga-
nized in hand-crafted encyclopedias, are valuable
linguistic knowledge for human usage and compu-
tational linguistics research. However, due to the
limitation of manual compilation, existing encyclo-
pedias often lack new terms and new definitions for
existing terms.
The World Wide Web (the Web), which contains
an enormous volume of up-to-date information, is a
promising source to obtain new term descriptions.
It has become fairly common to consult the Web for
descriptions of a specific term. However, the use of
existing search engines is associated with the follow-
ing problems:
(a) search engines often retrieve extraneous pages
not describing a submitted term,
(b) even if desired pages are retrieved, a user has to
identify page fragments describing the term,
(c) word senses are not distinguished for polyse-
mous terms, such as ?hub (device and center)?,
(d) descriptions in multiple pages are independent
and do not comprise a condensed and coherent
text as in existing encyclopedias.
The authors of this paper have been resolving
these problems progressively. For problems (a) and
(b), Fujii and Ishikawa (2000) proposed an auto-
matic method to extract term descriptions from the
Web. For problem (c), Fujii and Ishikawa (2001)
improved the previous method, so that the multiple
descriptions extracted for a single term are catego-
rized into domains and consequently word senses are
distinguished.
Using these methods, we have compiled an ency-
clopedic corpus for approximately 600,000 Japanese
terms. We have also built a Web site called ?Cy-
clone?1 to utilize this corpus, in which one or more
paragraph-style descriptions extracted from differ-
ent pages can be retrieved in response to a user
input. In Figure 1, three paragraphs describing
?XML? are presented with the titles of their source
pages.
However, the above-mentioned problem (d) re-
mains unresolved and this is exactly what we intend
to address in this paper.
In hand-crafted encyclopedias, a single term is de-
scribed concisely from different ?viewpoints?, such
as the definition, exemplification, and purpose. In
contrast, if the first paragraph in Figure 1 is not
described from a sufficient number of viewpoints
for XML, a user has to read remaining paragraphs.
However, this is inefficient, because the descriptions
are extracted from independent pages and usually
include redundant contents.
To resolve this problem, we propose a summariza-
tion method that produces a concise and condensed
term description from multiple paragraphs. As a re-
sult, a user can obtain sufficient information about a
term with a minimal cost. Additionally, by reducing
the size of descriptions, Cyclone can be used with
mobile devices, such as PDAs.
However, while Cyclone includes various types
of terms, such as technical terms, events, and an-
imals, the required set of viewpoints can vary de-
pending the type of target terms. For example, the
definition and exemplification are necessary for tech-
nical terms, but the family and habitat are necessary
for animals. In this paper, we target Japanese tech-
nical terms in the computer domain.
Section 2 outlines Cyclone. Sections 3 and 4 ex-
plain our summarization method and its evaluation,
respectively. In Section 5, we discuss related work
and the scalability of our method.
1http://cyclone.slis.tsukuba.ac.jp/
Figure 1: Example descriptions for ?XML?.
2 Overview of Cyclone
Figure 2 depicts the overall design of Cyclone,
which produces an encyclopedic corpus by means of
five modules: ?term recognition?, ?extraction?, ?re-
trieval?, ?organization?, and ?related term extrac-
tion?. While Cyclone produces a corpus off-line,
users search the resultant corpus for specific descrip-
tions on-line.
It should be noted that the summarization
method proposed in this paper is not included in
Figure 2 and that the concept of viewpoint has not
been used in the modules in Figure 2.
In the off-line process, the input terms can be ei-
ther submitted manually or collected by the term
recognition module automatically. The term recog-
nition module periodically searches the Web for mor-
pheme sequences not included in the corpus, which
are used as input terms.
The retrieval module exhaustively searches the
Web for pages including an input term, as performed
in existing Web search engines.
The extraction module analyzes the layout (i.e.,
the structure of HTML tags) of each retrieved page
and identifies the paragraphs that potentially de-
scribe the target term. While promising descrip-
tions can be extracted from pages resembling on-line
dictionaries, descriptions can also be extracted from
general pages.
The organization module classifies the multiple
paragraphs for a single term into predefined domains
(e.g., computers, medicine, and sports) and sorts
them according to the score. The score is computed
by the reliability determined by hyper-links as in
Google2 and the linguistic validity determined by a
language model produced from an existing machine-
readable encyclopedia. Thus, different word senses,
which are often associated with different domains,
can be distinguished and high-quality descriptions
can be selected for each domain.
Finally, the related term extraction module
searches top-ranked descriptions for terms strongly
related to the target term (e.g., ?cable? and ?LAN?
for ?hub?). Existing encyclopedias often provide re-
lated terms for each headword, which are effective
to understand the headword. In Cyclone, related
terms can also be used as feedback terms to nar-
row down the user focus. However, this module is
beyond the scope of this paper.
3 Summarization Method
3.1 Overview
Given a set of paragraph-style descriptions for a sin-
gle term in a specific domain (e.g., descriptions for
?hub? in the computer domain), our summarization
2http://www.google.com/
Web
organization
retrieval
extraction
term(s)
encyclopedic
corpus
term recognition
related term extraction
descriptions related terms
Figure 2: Overall design of Cyclone.
method produces a concise text describing the term
from different viewpoints.
These descriptions are obtained by the organiza-
tion module in Figure 2. Thus, the related term
extraction module is independent of our summariza-
tion method.
Our method is multi-document summarization
(MDS) (Mani, 2001). Because a set of input docu-
ments (in our case, the paragraphs for a single term)
were written by different authors and/or different
time, the redundancy and divergence of the topics in
the input are greater than that for single document
summarization. Thus, the recognition of similarity
and difference among multiple contents is crucial.
The following two questions have to be answered:
? by which language unit (e.g., words, phrases, or
sentences) should two contents be compared?
? by which criterion should two contents be re-
garded as ?similar? or ?different??
The answers for these questions can be different de-
pending on the application and the type of input
documents.
Our purpose is to include as many viewpoints as
possible in a concise description. Thus, we com-
pare two contents on a viewpoint-by-viewpoint basis.
In addition, if two contents are associated with the
same viewpoint, we determine that those contents
are similar and that they should not be repeated in
the summary.
Our viewpoint-based summarization (VBS)
method consists of the following four steps:
1. identification, which recognizes the language
unit associated with a viewpoint,
2. classification, which merges the identified units
associated with the same viewpoint into a single
group,
3. selection, which determines one or more repre-
sentative units for each group,
4. presentation, which produces a summary in a
specific format.
The model is similar to those in existing MDS meth-
ods. However, the implementation of each step
varies depending on the application. We elaborate
on the four steps in Sections 3.2-3.5, respectively.
3.2 Identification
The identification module recognizes the language
units, each of which describes a target term from a
specific viewpoint. However, a compound or com-
plex sentence is often associated with multiple view-
points. The following example is an English trans-
lation of a Japanese compound sentence in a Web
page.
XML is an abbreviation for eXtensible
Markup Language, and is a markup lan-
guage.
The first and second clauses describe XML from the
abbreviation and definition viewpoints, respectively.
It should be noted that because ?XML? and ?eX-
tensible Markup Language? are spelled out by the
Roman alphabet in the original sentence, the first
clause does not provide Japanese readers with the
definition of XML.
To extract the language units on a viewpoint-by-
viewpoint basis, we segment Japanese sentences into
simple sentences. However, sentence segmentation
remains a difficult problem and the accuracy is not
100%. First, we analyze the syntactic dependency
structure of an input sentence by CaboCha3. Sec-
ond, we use hand-crafted rules to extract simple sen-
tences using the dependency structure.
The simple sentences excepting the first clause of-
ten lack the subject. To resolve this problem, zero
pronoun detection and anaphora resolution can be
used. However, due to the rudimentary nature of
existing methods, we use hand-crafted rules to com-
plement simple sentences with the subject.
As a result, we can obtain the following two simple
sentences from the above-mentioned input sentence,
in which the complement subject is in parentheses.
? XML is an abbreviation for eXtensible
Markup Language.
? (XML) is a markup language.
3.3 Classification
The classification module merges the simple sen-
tences related to the same viewpoint into a single
group. An existing encyclopedia for technical terms
uses approximately 30 obligatory and optional view-
points. We selected the following 12 viewpoints for
which typical expressions can be coded manually:
3
http://cl.aist-nara.ac.jp/?taku-ku/software/cabocha/
definition, abbreviation, exemplification,
purpose, synonym, reference, product, ad-
vantage, drawback, history, component,
function.
We manually produced 36 linguistic patterns used
to describe terms from a specific viewpoint. These
patterns are regular expressions, in which specific
morphemes are generalized into parts-of-speech or
the special symbol representing the target term.
We use a two-stage classification method. First,
the simple sentences that match with a pattern
are classified into the associated viewpoint group.
A simple sentence that matches with patterns for
multiple viewpoints is classified into every possible
group.
However, the pattern-based method fails to clas-
sify the sentences that do not match with any prede-
fined patterns. Thus, second we classify the remain-
ing sentences into the group in which the most simi-
lar sentence has already been classified. In practice,
we compute the similarity between an unclassified
sentence and each of the classified sentences. The
similarity between two sentences is determined by
the Dice coefficient, i.e., the ratio of content words
commonly included in those sentences. The sen-
tences unclassified through the above method are
classified into the ?miscellaneous? group.
In summary, our two-stage method uses prede-
fined linguistic patterns and statistics of words.
The following examples are English translations
of Japanese sentences extracted in the identification
module. These sentences can be classified into a spe-
cific group on the ground of the underlined expres-
sions, excepting sentence (e). However, in the second
stage, sentence (e) can be classified into the history
group, because sentence (e) is most similar to sen-
tence (c).
(a) XML is an extensible markup language.
? definition
(b) an abbreviation for eXtensible Markup Lan-
guage
? abbreviation
(c) was advised as a standard by W3C in 1998
? history
(d) XML is an abbreviation for Extensible Markup
Language
? abbreviation
(e) the standard of XML was advised by W3C
? ??? ? history
3.4 Selection
The selection module determines one or more rep-
resentative sentences for each viewpoint group. The
number of sentences selected from each group can
vary depending on the desired size of the resultant
summary.
We consider the following factors to compute the
score for each sentence and select sentences with
greater scores in each group.
? the number of common words included (W)
The representative sentences should contain
many words that are common in the group. We
collect the frequencies of words for each group,
and sentences including frequent words are pre-
ferred.
? the rank in Cyclone (R)
As depicted in Figure 2, Cyclone sorts the re-
trieved paragraphs according to the plausibility
as the description. Sentences in highly-ranked
paragraphs are preferred.
? the number of characters included (C)
To minimize the size of a summary, short sen-
tences are preferred.
Because these factors are different in terms of
the dimension, range, and polarity, we normalize
each factor in [0,1] and compute the final score as a
weighed average of the three factors. The weight of
each factor was determined by a preliminary study.
In brief, the relative importance among the three
factors is W>R>C.
However, because the miscellaneous group in-
cludes various viewpoints, we use a different method
from that for the regular groups. First, we select rep-
resentative sentences from the regular groups. Sec-
ond, from the miscellaneous group, we select the sen-
tence that is most dissimilar to the sentences already
selected as representatives. We use the Dice-based
similarity used in Section 3.3 to measure the dis-
similarity between two sentences. If we select more
than one sentence from the miscellaneous group, the
second process is repeated recursively.
3.5 Presentation
The presentation module lists the selected sentences
without any post-editing. Ideally, natural language
generation is required to produce a coherent text by,
for example, complementing conjunctions and gen-
erating anaphoric expressions. However, a simple
list of sentences is also useful to obtain knowledge
about a target term.
Figure 3 depicts an example summary produced
from the top 50 paragraphs for the term ?XML?. In
this figure, six viewpoint groups and the miscella-
neous group were formed and only one sentence was
selected from each group. The order of sentences
presented was determined by the score computed in
the selection module.
While the source paragraphs consist of 11,224
characters, the summary consists of 397 characters,
which is almost the same length as an abstract for a
technical paper.
The following is an English translation of the sen-
tences in Figure 3. Here, the words spelled out by
the Roman alphabet in the original sentences are in
italics.
Figure 3: Example summary for ?XML?.
? definition: XML is an extensible markup lan-
guage (eXtensible Markup Language).
? abbreviation: an abbreviation for Extensible
Markup Language (an extensible markup lan-
guage).
? purpose: Because XML is a standard specifi-
cation for data representation, the data defined
by XML can be reusable, irrespective of the up-
per application.
? advantage: XML is advantageous to develop-
ers of the file maker Pro, which needs to receive
data from the client.
? history: was advised as a standard by W3C
(World Wide Web Consortium: a group stan-
dardizing WWW technologies) in 1998,
? reference: This book is an introduction for
XML, which has recently been paid much at-
tention as the next generation Internet standard
format, and related technologies.
? miscellaneous: In XML, the tags are enclosed
in ?<? and ?>?.
Each viewpoint label or sentence is hyper-linked to
the associated group or the source paragraph, re-
spectively, so that a user can easily obtain more in-
formation on a specific viewpoint. For example, by
the reference sentence, a catalogue page of the book
in question can be retrieved.
Although the resultant summary describes XML
from multiple viewpoints, there is a room for im-
provement. For example, the sentences classified
into the definition and abbreviation viewpoints in-
clude almost the same content.
4 Evaluation
4.1 Methodology
Existing methods for evaluating summarization
techniques can be classified into intrinsic and extrin-
sic approaches.
In the intrinsic approach, the content of a sum-
mary is evaluated with respect to the quality of a
text (e.g., coherence) and the informativeness (i.e.,
the extent to which important contents are in the
summary). In the extrinsic approach, the evaluation
measure is the extent to which a summary improves
the efficiency of a specific task (e.g., relevance judg-
ment in text retrieval).
In DUC4 and NTCIR5, both approaches have
been used to evaluate summarization methods tar-
geting newspaper articles. However, because there
was no public test collections targeting term descrip-
tions in Web pages, we produced our test collection.
4http://duc.nist.gov/
5http://research.nii.ac.jp/ntcir/index-en.html
As the first step of our summarization research, we
addressed only the intrinsic evaluation.
In this paper, we focused on including as many
viewpoints (i.e., contents) as possible in a summary,
but did not address the text coherence. Thus, we
used the informativeness of a summary as the evalu-
ation criterion. We used the following two measures,
which are in the trade-off relation.
? compression ratio
#characters in summary
#characters in Cyclone result
? coverage
#viewpoints in summary
#viewpoints in Cyclone result
Here, ?#viewpoints? denotes the number of view-
point types. Even if a summary contains multiple
sentences related to the same viewpoint, the numer-
ator is increased by 1.
We used 15 Japanese term in an existing computer
dictionary as test inputs. English translations of the
test inputs are as follows:
10BASE-T, ASCII, SQL, XML, accumu-
lator, assembler, binary number, crossing
cable, data warehouse, macro virus, main
memory unit, parallel processing, resolu-
tion, search time, thesaurus.
To calculate the coverage, the simple sentences
in the Cyclone results have to be associated with
viewpoints. To reduce the subjectivity in the evalu-
ation, for each of the 15 terms, we asked two college
students (excluding the authors of this paper) to an-
notate each simple sentence in the top 50 paragraphs
with one or more viewpoints. The two annotators
performed the annotation task independently. The
denominators of the compression ratio and coverage
were calculated by the top 50 paragraphs.
During a preliminary study, the authors and anno-
tators defined 28 viewpoints, including the 12 view-
points targeted in our method. We also defined the
following three categories, which were not considered
as a viewpoint:
? non-description, which were also used to anno-
tate non-sentence fragments caused by errors in
the identification module,
? description for a word sense independent of the
computer domain (e.g., ?hub? as a center, in-
stead of a network device),
? miscellaneous.
It may be argued that an existing hand-crafted
encyclopedia can be used as the standard sum-
mary. However, paragraphs in Cyclone often con-
tain viewpoints not described in existing encyclope-
dias. Thus, we did not use existing encyclopedias in
our experiments.
4.2 Results
Table 1 shows the compression ratio and coverage for
different methods, in which ?#Reps? and ?#Chars?
denote the number of representative sentences se-
lected from each viewpoint group and the number
of characters in a summary, respectively. We always
selected five sentences from the miscellaneous group.
The third column denotes the compression ratio.
The remaining columns denote the coverage on
a annotator-by-annotator basis. The columns ?12
Viewpoints? and ?28 Viewpoints? denote the case
in which we focused only on the 12 viewpoints tar-
geted in our method and the case in which all the
28 viewpoints were considered, respectively.
The columns ?VBS? and ?Lead? denote the cover-
age obtained with our viewpoint-based summariza-
tion method and the lead method. The lead method,
which has often been used as a baseline method in
past literature, systematically extracted the top N
characters from the Cyclone result. Here, N is the
same number in the second column.
In other words, the compression ratio of the VBS
and lead methods was standardized, and we com-
pared the coverage of both methods. The compres-
sion ratio and coverage were averaged over the 15
test terms.
Suggestions which can be derived from Table 1 are
as follows.
First, in the case of ?#Reps=1?, the average
size of a summary was 616 characters, which is
marginally longer than an abstract for a techni-
cal paper. In the case of ?#Reps=3?, the average
summary size was 1309 characters, which is almost
the maximum size for a single description in hand-
crafted encyclopedias. A summary obtained with
four sentences in each group is perhaps too long as
term descriptions.
Second, the compression ratio was roughly 10%,
which is fairly good performance. It may be argued
that the compression ratio is exaggerated. That is,
although paragraphs ranked higher than 50 can po-
tentially provide the sufficient viewpoints, the top 50
paragraphs were always used to calculate the domi-
nator of the compression ratio.
We found that the top 38 paragraphs, on average,
contained all viewpoint types in the top 50 para-
graphs. Thus, the remaining 12 paragraphs did not
provide additional information. However, it is dif-
ficult for a user to determine when to stop reading
a retrieval result. In existing evaluation workshops,
such as NTCIR, the compression ratio is also calcu-
lated using the total size of the input documents.
Third, the VBS method outperformed the lead
method in terms of the coverage, excepting the case
of ?#Reps=1? focusing on the 12 viewpoints by an-
notator B. However, in general the VBS method pro-
duced more informative summaries than the lead
method, irrespective of the compression ratio and
the annotator.
It should be noted that although the VBS method
Table 1: Results of summarization experiments.
Coverage by annotator A (%) Coverage by annotator B (%)
Compression 12 Viewpoints 28 Viewpoints 12 Viewpoints 28 Viewpoints
#Reps #Chars ratio (%) VBS Lead VBS Lead VBS Lead VBS Lead
1 616 5.97 56.62 52.84 49.49 44.84 50.00 53.61 49.49 47.56
2 998 9.61 73.43 57.23 59.26 53.70 64.50 62.96 60.75 57.37
3 1309 12.61 76.04 59.29 63.13 56.44 67.83 64.81 65.22 60.84
targets 12 viewpoints, the sentences selected from
the miscellaneous group can be related to the re-
maining 16 viewpoints. Thus, even if we focus on
the 28 viewpoints, the coverage of the VBS method
can potentially increase.
It should also be noted that all viewpoints are not
equally important. For example, in an existing en-
cyclopedia (Nagao and others, 1990) the definition,
exemplification, and synonym are regarded as the
obligatory viewpoints, and the remaining viewpoints
are optional.
We investigated the coverage for the three obliga-
tory viewpoints. We found that while the coverage
for the definition and exemplification ranged from
60% to 90%, the coverage for the synonym was 50%
or less.
A low coverage for the synonym is partially due
to the fact that synonyms are often described with
parentheses. However, because parentheses are used
for various purposes, it is difficult to identify only
synonyms expressed with parentheses. This problem
needs to be further explored.
5 Discussion
The goal of our research is to automatically compile
a high-quality large encyclopedic corpus using the
Web. Hand-crafted encyclopedias lack new terms
and new definitions for existing terms, and thus the
quantity problem is crucial. The Web contains un-
reliable and unorganized information and thus the
quality problem is crucial. We intend to alleviate
both problems. To the best of our knowledge, no
attempt has been made to intend similar purposes.
Our research is related to question answering
(QA). For example, in TREC QA track, definition
questions are intended to provide a user with the def-
inition of a target item or person (Voorhees, 2003).
However, while the expected answer for a TREC
question is short definition sentences as in a dic-
tionary, we intend to produce an encyclopedic text
describing a target term from multiple viewpoints.
The summarization method proposed in this pa-
per is related to multi-document summarization
(MDS) (Mani, 2001; Radev and McKeown, 1998;
Schiffman et al, 2001). The novelty of our research
is that we applied MDS to producing a condensed
term description from unorganized Web pages, while
existing MDS methods used newspaper articles to
produce an outline of an event and a biography of
a specific person. We also proposed the concept of
viewpoint for MDS purposes.
While we targeted Japanese technical terms in the
computer domain, our method can also be applied to
other types of terms in different languages, without
modifying the model. However, a set of viewpoints
and patterns typically used to describe each view-
point need to be modified or replaced depending the
application. Given annotated data, such as those
used in our experiments, machine learning methods
can potentially be used to produce a set of view-
points and patterns for a specific application.
6 Conclusion
To compile encyclopedic term descriptions from the
Web, we introduced a summarization method to our
previous work. Future work includes generating a
coherent text instead of a simple list of sentences
and performing extensive experiments including an
extrinsic evaluation method.
References
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: Extract-
ing term descriptions from semi-structured texts.
In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pages
488?495.
Atsushi Fujii and Tetsuya Ishikawa. 2001. Organiz-
ing encyclopedic knowledge based on the Web and
its application to question answering. In Proceed-
ings of the 39th Annual Meeting of the Association
for Computational Linguistics, pages 196?203.
Inderjeet Mani, 2001. Automatic Summarization,
chapter 7, pages 169?208. John Benjamins.
Makoto Nagao et al, editors. 1990. Encyclope-
dic Dictoinary of Computer Science. Iwanami
Shoten. (In Japanese).
Dragomir R. Radev and Kathleen R. McKeown.
1998. Generating natural language summaries
from multiple on-line sources. Computational Lin-
guistics, 24(3):469?500.
Barry Schiffman, Inderjeet Mani, and Kristian J.
Concepcion. 2001. Producing biographical sum-
maries: Combining linguistic knowledge with cor-
pus statistics. In Proceedings of the 39th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 450?457.
Ellen M. Voorhees. 2003. Evaluating answers to def-
inition questions. In Companion Volume of the
Proceedings of HLT-NAACL 2003, pages 109?111.
  
A Lemmatization Method for Modern Mongolian and           
its Application to Information Retrieval 
Badam-Osor Khaltar           Atsushi Fujii 
Graduate School of Library, Information and Media Studies 
University of Tsukuba 
1-2 Kasuga Tsukuba, 305-8550, Japan 
{khab23, fujii}@slis.tsukuba.ac.jp 
 
 
 
Abstract 
In Modern Mongolian, a content word can 
be inflected when concatenated with suf-
fixes. Identifying the original forms of 
content words is crucial for natural lan-
guage processing and information retrieval. 
We propose a lemmatization method for 
Modern Mongolian and apply our method 
to indexing for information retrieval. We 
use technical abstracts to show the effec-
tiveness of our method experimentally. 
1 Introduction 
The Mongolian language is divided into Tradition-
al Mongolian, which uses the Mongolian alphabet, 
and Modern Mongolian, which uses the Cyrillic 
alphabet. In this paper, we focus solely on the lat-
ter and use the word ?Mongolian? to refer to Mod-
ern Mongolian. 
In Mongolian, which is an agglutinative lan-
guage, each sentence is segmented on a phrase-by-
phrase basis. A phrase consists of a content word, 
such as a noun or a verb, and one or more suffixes, 
such as postpositional participles. A content word 
can potentially be inflected when concatenated 
with suffixes. 
Identifying the original forms of content words 
in Mongolian text is crucial for natural language 
processing and information retrieval. In informa-
tion retrieval, the process of normalizing index 
terms is important, and can be divided into lemma-
tization and stemming. Lemmatization identifies 
the original form of an inflected word, whereas 
stemming identifies a stem, which is not necessari-
ly a word.  
Existing search engines, such as Google and 
Yahoo!, do not perform lemmatization or stem-
ming for indexing Web pages in Mongolian. 
Therefore, Web pages that include only inflected 
forms of a query cannot be retrieved. 
In this paper, we propose a lemmatization me-
thod for Mongolian and apply our method to in-
dexing for information retrieval. 
2 Inflection types in Mongolian phrases 
Nouns, adjectives, numerals, and verbs can be 
concatenated with suffixes. Nouns and adjectives 
are usually concatenated with a sequence of a 
plural suffix, case suffix, and reflexive possessive 
suffix. Numerals are concatenated with either a 
case suffix or a reflexive possessive suffix. Verbs 
are concatenated with various suffixes, such as an 
aspect suffix, a participle suffix, and a mood suffix. 
Figure 1 shows the inflection types of content 
words in Mongolian phrases. In (a), there is no in-
flection in the content word ???? (book)?, conca-
tenated with the suffix ??? (the genitive case)?. 
The content words are inflected in (b)-(e). 
 
Type Example 
(a) No inflection ??? + ?? ? ????? 
book + genitive case 
(b) Vowel insertion ?? + ? ? ???? 
brother + dative case 
(c) Consonant insertion ?????? + ???? ?????????? 
building + genitive case 
(d) The letters ??? or ??? 
are eliminated, and the 
vowel converts to ???  
???? + ??? ? ?????? 
return + ablative case 
(e) Vowel elimination ???? + ??? ? ?????? 
work + ablative case  
Figure 1: Inflection types of content words in 
Mongolian phrases. 
1
  
Loanwords, which can be nouns, adjectives, or 
verbs in Mongolian, can also be concatenated with 
suffixes. In this paper, we define a loanword as a 
word imported from a Western language. 
Because loanwords are linguistically different 
from conventional Mongolian words, the suffix 
concatenation is also different from that for con-
ventional Mongolian words. Thus, exception rules 
are required for loanwords. 
For example, if the loanword ?????? (station)? 
is to be concatenated with a genitive case suffix, 
???? should be selected from the five genitive 
case suffixes (i.e., ??, ???, ?, ??, and ?) based 
on the Mongolian grammar. However, because 
?????? (station)? is a loanword, the genitive case 
????? is selected instead of ????, resulting in the 
noun phrase ????????? (station?s)?. 
Additionally, the inflection (e) in Figure 1 never 
occurs for noun and adjective loanwords.  
3 Related work 
Sanduijav et al (2005) proposed a lemmatization 
method for noun and verb phrases in Mongolian. 
They manually produced inflection rules and con-
catenation rules for nouns and verbs. Then, they 
automatically produced a dictionary by aligning 
nouns or verbs with suffixes. Lemmatization for 
phrases is performed by consulting this dictionary. 
Ehara et al (2004) proposed a morphological 
analysis method for Mongolian, for which they 
manually produced rules for inflections and conca-
tenations. However, because the lemmatization 
methods proposed by Sanduijav et al (2005) and 
Ehara et al (2004) rely on dictionaries, these me-
thods cannot lemmatize new words that are not in 
dictionaries, such as loanwords and technical terms. 
Khaltar et al (2006) proposed a lemmatization 
method for Mongolian noun phrases that does not 
use a noun dictionary. Their method can be used 
for nouns, adjectives, and numerals, because the 
suffixes that are concatenated with these are almost 
the same and the inflection types are also the same. 
However, they were not aware of the applicability 
of their method to adjectives and numerals. 
The method proposed by Khaltar et al (2006) 
mistakenly extracts loanwords with endings that 
are different from conventional Mongolian words. 
For example, if the phrase ?????????? 
(ecology?s)? is lemmatized, the resulting content 
word will be ????????, which is incorrect. The 
correct word is ???????? (ecology)?. This error 
occurs because the ending ?-????? (-ology)? does 
not appear in conventional Mongolian words.  
In addition, Khaltar et al (2006)?s method 
applies (e) in Figure 1 to loanwords, whereas in-
flection (e) never occurs in noun and adjective 
loanwords. 
Lemmatization and stemming are arguably ef-
fective for indexing in information retrieval (Hull, 
1996; Porter, 1980). Stemmers have been devel-
oped for a number of agglutinative languages, in-
cluding Malay (Tai et al, 2000), Indonesian (Ber-
lian Vega and Bressan, 2001), Finnish (Korenius et 
al., 2004), Arabic (Larkey et al, 2002), Swedish 
(Carlberger et al, 2001), Slovene (Popovi? and 
Willett, 1992) and Turkish (Ekmek?ioglu et al, 
1996). 
Xu and Croft (1998) and Melucci and Orio 
(2003) independently proposed a language-
independent method for stemming, which analyzes 
a corpus in a target language and identifies an 
equivalent class consisting of an original form, 
inflected forms, and derivations. However, their 
method, which cannot identify the original form in 
each class, cannot be used for natural language 
applications where word occurrences must be stan-
dardized by their original forms.  
Finite State Transducers (FSTs) have been ap-
plied to lemmatization. Although Karttunen and 
Beesley (2003) suggested the applicability of FSTs 
to various languages, no rule has actually been 
proposed for Mongolian. The rules proposed in this 
paper can potentially be used for FSTs. 
To the best of our knowledge, no attempt has 
been made to apply lemmatization or stemming to 
information retrieval for Mongolian. Our research 
is the first serious effort to address this problem. 
4 Methodology 
4.1 Overview 
In view of the discussion in Section 3, we en-
hanced the lemmatization method proposed by 
Khaltar et al (2006). The strength of this method is 
that noun dictionaries are not required. 
Figure 2 shows the overview of our lemmatiza-
tion method for Mongolian. Our method consists 
of two segments, which are identified with dashed 
lines in Figure 2: ?lemmatization for verb phrases? 
and ?lemmatization for noun phrases?. 
2
  
 
 
In Figure 2, we enhanced the method proposed 
by Khaltar et al (2006) from three perspectives. 
First, we introduced ?lemmatization for verb 
phrases?. There is a problem to be solved when we 
target both noun and verb phrases. There are a 
number of suffixes that can concatenate with both 
verbs and nouns, but the inflection type can be dif-
ferent depending on the part of speech. As a result, 
verb phrases can incorrectly be lemmatized as 
noun phrases and vice versa. 
Because new verbs are not created as frequently 
as nouns, we predefine a verb dictionary, but do 
not use a noun dictionary. We first lemmatize an 
entered phrase as a verb phrase and then check 
whether the extracted content word is defined in 
our verb dictionary. If the content word is not de-
fined in our verb dictionary, we lemmatize the in-
put phrase as a noun phrase.  
Second, we introduced a ?loanword identifica-
tion rule? in ?lemmatization for noun phrases?. We 
identify a loanword phrase before applying a 
?noun suffix segmentation rule? and ?vowel inser-
tion rule?. Because segmentation rules are different 
for conventional Mongolian words and loanwords, 
we enhance the noun suffix segmentation rule that 
was originally proposed by Khaltar et al (2006). 
Additionally, we do not use the vowel insertion 
rule, if the entered phrase is detected as a loanword 
phrase. The reason is that vowel elimination never 
occurs in noun loanwords.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Third, unlike Khaltar et al (2006), we targeted 
adjective and numeral phrases. Because the suffix-
es concatenated with nouns, adjectives, and num-
erals are almost the same, the lemmatization me-
thod for noun phrases can also be used for adjec-
tive and numeral phrases without any modifica-
tions. We use ?lemmatization for noun phrases? to 
refer to the lemmatization for noun, adjective, and 
numeral phrases. 
We briefly explain our lemmatization process 
using Figure 2. 
We consult a ?verb suffix dictionary? and per-
form backward partial matching to determine 
whether a suffix is concatenated at the end of a 
phrase. If a suffix is detected, we use a ?verb suffix 
segmentation rule? to remove the suffix and extract 
the content word. This process will be repeated 
until the residue of the phrase does not match any 
of the entries in the verb suffix dictionary. 
We use a ?vowel insertion rule? to check wheth-
er vowel elimination occurred in the content word 
and insert the eliminated vowel.  
If the content word is defined in a ?verb dictio-
nary?, we output the content word as a verb and 
terminate the lemmatization process. If not, we use 
the entered phrase and perform lemmatization for 
noun phrases. We consult a ?noun suffix dictio-
nary? to determine whether one or more suffixes 
are concatenated at the end of the target phrase. 
Yes 
No 
Detect a suffix in the phrase 
Remove suffixes and extract a content word 
Check if the content word is a verb 
Detect a suffix in the phrase 
Remove suffixes and extract a content word
Input a 
phrase 
Output the 
content word 
Process the inputted phrase as a noun phrase
Loanword identification rule 
Verb dictionary 
Verb suffix segmentation rule  
Noun suffix dictionary 
Noun suffix segmentation rule 
Verb suffix dictionary  
Figure 2: Overview of our lemmatization method for Mongolian.   
Lemmatization for noun phrases 
Vowel insertion rule Insert an eliminated vowel  
Insert an eliminated vowel  
Identify loanword 
Vowel insertion rule 
Lemmatization for verb phrases 
3
  
We use a ?loanword identification rule? to iden-
tify whether the phrase is a loanword phrase. We 
use a ?noun suffix segmentation rule? to remove 
the suffixes and extract the content word. If the 
phrase is identified as a loanword phrase we use 
different segmentation rules. 
We use the ?vowel insertion rule? which is also 
used for verb phrases to check whether vowel eli-
mination occurred in the content word and insert 
the eliminated vowel. However, if the phrase is 
identified as a loanword phrase, we do not use the 
vowel insertion rule. 
If the target phrase does not match any of the 
entries in the noun suffix dictionary, we determine 
that a suffix is not concatenated and we output the 
phrase as it is. 
The inflection types (b)?(d) in Figure 1 are 
processed by the verb suffix segmentation rule and 
noun suffix segmentation rule. The inflection (e) in 
Figure 1 is processed by the vowel insertion rule. 
We elaborate on the dictionaries and rules in 
Sections 4.2?4.8. 
4.2 Verb suffix dictionary 
We produced a verb suffix dictionary, which con-
sists of 126 suffixes that can concatenate with 
verbs. These suffixes include aspect suffixes, parti-
ciple suffixes, and mood suffixes. 
Figure 3 shows a fragment of our verb suffix 
dictionary, in which inflected forms of suffixes are 
shown in parentheses. All suffixes corresponding 
to the same suffix type represent the same meaning. 
4.3 Verb suffix segmentation rule 
For the verb suffix segmentation rule, we produced 
179 rules. There are one or more segmentation 
rules for each of the 126 verb suffixes mentioned 
in Section 4.2. 
Figure 4 shows a fragment of the verb suffix 
segmentation rule for suffix ?? (past)?. In the 
column ?Segmentation rule?, the condition of each 
?if? sentence is a phrase ending. ?V? refers to a 
vowel and ?*? refers to any strings. ?C9? refers to 
any of the nine consonants ???, ???, ???, ???, ???, 
???, ???, ???, or ???, and ?C7? refers to any of the 
seven consonants ???, ???, ???, ???, ???, ???, or 
???. If a condition is satisfied, we remove one or 
more corresponding characters. 
For example, because the verb phrase 
????????? (renew + past)? satisfies condition (ii),  
Suffix type Suffix 
Appeal 
Complete 
Perfect 
Progressive-perfect 
????, ???? 
??? 
??? (???), ??? (???), ???, ??? 
????, ????, ????, ???? 
Figure 3: Fragment of verb suffix dictionary. 
 
Suffix Segmentation rule   
 
? 
Past 
(i)  If ( *+ V + V +  ? )   
Remove ? 
(ii) If ( * + C9 + C7 + V + ? )  
Remove V + ? 
Figure 4: Fragment of verb suffix segmentation 
rule. 
 
we remove the suffix ??? and the preceding vowel 
??? to extract ????????. 
4.4 Verb dictionary 
We use the verb dictionary produced by Sanduijav 
et al (2005), which includes 1254 verbs. 
4.5 Noun suffix dictionary 
We use the noun suffix dictionary produced by 
Khaltar et al (2006), which contains 35 suffixes 
that can be concatenated with nouns. These suffix-
es are postpositional particles. Figure 5 shows a 
fragment of the dictionary, in which inflected 
forms of suffixes are shown in parentheses. 
4.6 Noun suffix segmentation rule 
There are 196 noun suffix segmentation rules, of 
which 173 were proposed by Khaltar et al (2006). 
As we explained in Section 3, these 173 rules often 
incorrectly lemmatize loanwords with different 
endings from conventional Mongolian words. 
We analyzed the list of English suffixes and 
found that English suffixes ?-ation? and ?-ology? 
are incorrectly lemmatized by Khaltar et al (2006). 
In Mongolian, ?-ation? is transliterated into ????? 
or ????? and ?-ology? is transliterated into 
???????. Thus, we produced 23 rules for 
loanwords that end with ?????, ?????, or ???????.  
Figure 6 shows a fragment of our suffix segmen-
tation rule for loanwords. For example, for the 
loanword phrase ?????????? (ecology + geni-
tive)?, we use the segmentation rule for suffix 
???? (genitive)? in Figure 6. We remove the suffix 
???? (genitive)? and add ??? to the end of the 
content word. As a result, the noun ???????? 
(ecology)? is correctly extracted. 
 
4
  
 Case Suffix 
Genitive 
Accusative 
Dative 
Ablative 
?, ?, ??, ??, ??? 
??, ???, ? 
?, ? 
??? (???), ??? (???), ???, ??? 
Figure 5: Fragment of noun suffix dictionary. 
 
Suffix Segmentation rule for loanwords  
??? 
Genitive 
If (* + ??????) 
     Remove (???) , Add (?) 
??? 
Accusative 
If (* + ??????) 
     Remove (???), Add (?) 
Figure 6: Fragment of suffix segmentation rules 
for loanwords. 
4.7 Vowel insertion rule 
To insert an eliminated vowel and extract the orig-
inal form of a content word, we check the last two 
characters of the content word. If they are both 
consonants, we determine that a vowel was elimi-
nated. However, a number of Mongolian words 
end with two consonants inherently and, therefore, 
Khaltar et al (2006) referred to a textbook on the 
Mongolian grammar (Ts, 2002) to produce 12 rules 
to determine when to insert a vowel between two 
consecutive consonants. We also use these rules as 
our vowel insertion rule. 
4.8 Loanword identification rule 
Khaltar et al (2006) proposed rules for extracting 
loanwords from Mongolian corpora. Words that 
satisfy one of seven conditions are extracted as 
loanwords. Of the seven conditions, we do not use 
the condition that extracts a word ending with 
?consonants + ?? as a loanword because it was not 
effective for lemmatization purposes in prelimi-
nary study. 
5 Experiments 
5.1 Evaluation method 
We collected 1102 technical abstracts from the 
?Mongolian IT Park? 1 and used them for experi-
ments. There were 178,448 phrase tokens and 
17,709 phrase types in the 1102 technical abstracts. 
We evaluated the accuracy of our lemmatization 
method (Section 5.2) and the effectiveness of our 
method in information retrieval (Section 5.3) expe-
rimentally. 
                                                 
1 http://www.itpark.mn/ (October, 2007)  
5.2 Evaluating lemmatization 
Two Mongolian graduate students served as asses-
sors. Neither of the assessors was an author of this 
paper. The assessors provided the correct answers 
for lemmatization. The assessors also tagged each 
word with its part of speech. 
The two assessors performed the same task in-
dependently. Differences can occur between two 
assessors on this task. We measured the agreement 
of the two assessors by the Kappa coefficient, 
which ranges from 0 to 1. The Kappa coefficients 
for performing lemmatization and tagging of parts 
of speech were 0.96 and 0.94, respectively, which 
represents almost perfect agreement (Landis and 
Koch, 1977). However, to enhance the objectivity 
of the evaluation, we used only the phrases for 
which the two assessors agreed with respect to the 
part of speech and lemmatization. 
We were able to use the noun and verb dictiona-
ries of Sanduijav et al (2005). Therefore, we com-
pared our lemmatization method with Sanduijav et 
al. (2005) and Khaltar et al (2006) in terms of ac-
curacy.  
Accuracy is the ratio of the number of phrases 
correctly lemmatized by the method under evalua-
tion to the total number of target phrases. Here, the 
target phrases are noun, verb, adjective, and num-
eral phrases. 
Table 1 shows the results of lemmatization. We 
targeted 15,478 phrase types in the technical ab-
stracts. Our experiment is the largest evaluation for 
Mongolian lemmatization in the literature. In con-
trast, Sanduijav et al (2005) and Khaltar et al 
(2006) used only 680 and 1167 phrase types, re-
spectively, for evaluation purposes. 
In Table 1, the accuracy of our method for 
nouns, which were targeted in all three methods, 
was higher than those of Sanduijav et al (2005) 
and Khaltar et al (2006). Because our method and 
that of Sanduijav et al (2005) used the same verb 
dictionary, the accuracy for verbs is principally the 
same for both methods. The accuracy for verbs 
was low, because a number of verbs were not in-
cluded in the verb dictionary and were mistakenly 
lemmatized as noun phrases. However, this prob-
lem will be solved by enhancing the verb dictio-
nary in the future. In total, the accuracy of our me-
thod was higher than those of Sanduijav et al 
(2005) and Khaltar et al (2006). 
 
5
  
Table 1: Accuracy of lemmatization (%). 
 #Phrase 
types 
Sanduijav 
et al 
(2005) 
Khaltar 
et al 
(2006) 
Our 
method 
Noun 13,016 57.6 87.7 92.5 
Verb 1,797 24.5 23.8 24.5 
Adjective 609 82.6 83.5 83.9 
Numeral 56 41.1 80.4 81.2 
Total 15,478 63.2 72.3 78.2 
 
We analyzed the errors caused by our method in 
Figure 7. In the column ?Example?, the left side 
and the right side of an arrow denote an error and 
the correct answer, respectively.  
The error (a) occurred to nouns, adjectives, and 
numerals, in which the ending of a content word 
was mistakenly recognized as a suffix and was re-
moved. The error (b) occurred because we did not 
consider irregular nouns. The error (c) occurred to 
loanword nouns because the loanword identifica-
tion rule was not sufficient. The error (d) occurred 
because we relied on a verb dictionary. The error 
(e) occurred because a number of nouns were in-
correctly lemmatized as verbs.  
For the errors (a)-(c), we have not found solu-
tions. The error (d) can be solved by enhancing the 
verb dictionary in the future. If we are able to use 
part of speech information, we can solve the error 
(e). There are a number of automatic methods for 
tagging parts of speech (Brill, 1997), which have 
promise for alleviating the error (e). 
5.3 Evaluating the effectiveness of lemmatiza-
tion in information retrieval 
We evaluated the effectiveness of lemmatization 
methods in indexing for information retrieval. No 
test collection for Mongolian information retrieval 
is available to the public. We used the 1102 tech-
nical abstracts to produce our test collection. 
Figure 8 shows an example technical abstract, in 
which the title is ?Advanced Albumin Fusion 
Technology? in English. Each technical abstract 
contains one or more keywords. In Figure 8, key-
words, such as ?????? ?????? (blood serum)? 
and ????? (placenta)? are annotated. 
We used two different types of queries for our 
evaluation. First, we used each keyword as a query, 
which we call ?keyword query (KQ)?. Second, we 
used each keyword list as a query, which we call 
?list query (LQ)?. The average number for key-
words in the keywords list was 6.1. For each query,  
 
Reasons of errors #Errors Example 
(a) Word ending is 
the same as a suffix. 274 
???? ? ??? 
sort 
(b) Noun plural 
tense is irregular. 244 
?????? ? ???? 
animal 
(c) Noun loanword 
ends with two con-
sonants. 
94 
???????? ? ????????? 
dinosaur 
(d) Verb does not 
exist in our verb 
dictionary.  
689 
????? ? ??????
to code  
(e) Word corres-
ponds to multiple 
part of speech. 
853 
???? ? ?? 
country   inter 
 
Figure 7: Errors of our lemmatization method. 
 
we used as the relevant documents the abstracts 
that were annotated with the query keyword in the 
keywords field. Thus, we were able to avoid the 
cost of relevance judgments. 
The target documents are the 1102 technical ab-
stracts, from which we extracted content words in 
the title, abstract, and result fields as index terms. 
However, we did not use the keywords field for 
indexing purposes. We used Okapi BM25 (Robert-
son et al, 1995) as the retrieval model. 
We used the lemmatization methods in Table 2 
to extract content words and compared the Mean 
Average Precision (MAP) of each method using 
KQ and LQ. MAP has commonly been used to 
evaluate the effectiveness of information retrieval. 
Because there were many queries for which the 
average precision was zero in all methods, we dis-
carded those queries. There were 686 remaining 
KQs and 273 remaining LQs. 
The average number of relevant documents for 
each query was 2.1. Although this number is small, 
the number of queries is large. Therefore, our eval-
uation result can be stable, as in evaluations for 
question answering (Voorhees and Tice, 2000).  
We can derive the following points from Table 2. 
First, to clarify the effectiveness of the lemmatiza-
tion in information retrieval, we compare ?no 
lemmatization? with the other methods. Any lem-
matization method improved the MAP for both KQ 
and LQ. Thus, lemmatization was effective for 
information retrieval in Mongolian. Second, we 
compare the MAP of our method with those of 
Sanduijav et al (2005) and Khaltar et al (2006). 
Our method was more effective than the method of 
Sanduijav et al (2005) for both KQ and LQ. How-
ever, the difference between Khaltar et al (2006) 
and our method was small for KQ and our method  
6
  
Figure 8: Example of technical abstract. 
 
Table 2: MAP of lemmatization methods.  
 Keyword query List query 
No lemmatization 0.2312 0.2766 
Sanduijav et al (2005) 0.2882 0.2834 
Khaltar et al (2006) 0.3134 0.3127 
Our method 0.3149 0.3114 
Correct lemmatization 0.3268 0.3187 
 
was less effective than Khaltar et al(2006) for LQ. 
This is because although we enhanced the lemma-
tization for verbs, adjectives, numerals, and loan-
words, the effects were overshadowed by a large 
number of queries comprising conventional Mon-
golian nouns. Finally, our method did not outper-
form the method using the correct lemmatization.  
We used the paired t-test for statistical testing, 
which investigates whether the difference in per-
formance is meaningful or simply because of 
chance (Keen, 1992). Table 3 shows the results, in 
which ?<? and ?<<? indicate that the difference of 
two results was significant at the 5% and 1% levels, 
respectively, and ??? indicates that the difference 
of two results was not significant.  
Looking at Table 3, the differences between no 
lemmatization and any lemmatization method, 
such as Sanduijav et al (2005), Khaltar et al 
(2006), our method, and correct lemmatization, 
were statistically significant in MAP for KQ. 
However, because the MAP value of no lemmati-
zation was improved for LQ, the differences be-
tween no lemmatization and the lemmatization me-
thods were less significant than those for KQ. The 
difference between Sanduijav et al (2005) and our 
method was statistically significant in MAP for 
both KQ and LQ. However, the difference between 
Khaltar et al (2006) and our method was not sig-
nificant in MAP for both KQ and LQ. Although, 
the difference between our method and correct 
lemmatization was statistically significant in MAP 
for KQ, the difference was not significant in MAP 
for LQ.  
 
 
 
Table 3: t-test result of the differences between 
lemmatization methods. 
 Keyword query List query 
No lemmatization vs. 
Correct lemmatization << < 
No lemmatization vs. 
Sanduijav et al (2005) << ? 
No lemmatization vs. 
Khaltar et al (2006) << < 
No lemmatization vs. 
Our method << < 
Sanduijav et al (2005) 
vs. Our method << < 
Khaltar et al (2006) vs. 
Our method ? ? 
Our method vs. Correct 
lemmatization < ? 
6 Conclusion 
In Modern Mongolian, a content word can poten-
tially be inflected when concatenated with suffixes. 
Identifying the original forms of content words is 
crucial for natural language processing and infor-
mation retrieval.  
In this paper, we proposed a lemmatization me-
thod for Modern Mongolian. We enhanced the 
lemmatization method proposed by Khaltar et al 
(2006). We targeted nouns, verbs, adjectives, and 
numerals. We also improved the lemmatization for 
loanwords.  
We evaluated our lemmatization method expe-
rimentally. The accuracy of our method was higher 
than those of existing methods. We also applied 
our lemmatization method to information retrieval 
and improved the retrieval accuracy. 
Future work includes using a part of speech tag-
ger because the part of speech information is effec-
tive for lemmatization.  
References 
Vinsensius Berlian Vega S N and St?phane Bressan. 
2001. Indexing the Indonesian Web: Language iden-
tification and miscellaneous issues. Tenth Interna-
tional World Wide Web Conference, Hong Kong.  
Eric Brill. 1997. Natural Language Processing Using 
Very Large Corpora. Kluwer Academic Press. 
Johan Carlberger, Hercules Dalianis, Martin Hassel, and 
Ola Knutsson. 2001. Improving Precision in Informa-
tion Retrieval for Swedish using Stemming. Proceed-
ings of NODALIDA ?01 - 13th Nordic Conference on 
Computational Linguistics. 
Title: ???????? ????????? ????????? ????????? 
Author?s name: ???? ?????? 
Keywords: ????? ??????, ???? ? 
Abstract: ?????????? ????? ?????? 5, 10% ???? 
Result: ????????? ?????? ??????????, ?????? ? 
7
  
Terumasa Ehara, Suzushi Hayata, and Nobuyuki Kimu-
ra. 2004. Mongolian morphological analysis using 
ChaSen. Proceedings of the 10th Annual Meeting of 
the Association for Natural Language Processing, 
pp. 709-712. (In Japanese). 
?una F. Ekmek?ioglu, Michael F. Lynch, and Peter 
Willett. 1996. Stemming and n-gram matching for 
term conflation in Turkish texts. Information Re-
search News, Vol. 7, No. 1, pp. 2-6. 
David A. Hull. 1996. Stemming algorithms ? a case 
study for detailed evaluation. Journal of the Ameri-
can Society for Information Science and Technology, 
Vol. 47, No. 1, pp. 70-84. 
Lauri Karttunen and Kenneth R. Beesley. 2003. Finite 
State Morphology. CSLI Publications. Stanford. 
Micheal E. Keen. 1992. Presenting results of experi-
mental retrieval comparisons. Information 
Processing and Management, Vol. 28, No. 4, pp. 
491-502. 
Badam-Osor Khaltar, Atsushi Fujii, and Tetsuya Ishi-
kawa. 2006. Extracting loanwords from Mongolian 
corpora and producing a Japanese-Mongolian bilin-
gual dictionary. Proceedings of the 21st International 
Conference on Computational Linguistics and 44th 
Annual Meeting of the Association for Computational 
Linguistics, pp. 657-664. 
Tuomo Korenius, Jorma Laurikkala, Kalervo J?rvelin, 
and Martti Juhola. 2004. Stemming and 
Lemmatization in the Clustering of Finnish Text 
Documents. Proceedings of the thirteenth Associa-
tion for Computing Machinery international confe-
rence on Information and knowledge management. 
pp. 625-633. 
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data. 
Biometrics, Vol. 33, No. 1, pp. 159-174. 
Leah S. Larkey, Lisa Ballesteros, and Margaret E. Con-
nel. 2002. Improving Stemming for Arabic Informa-
tion Retrieval: Light Stemming and Co-occurrence 
Analysis. Proceedings of the 25th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pp. 275-282. 
Massimo Melucci and Nicola Orio. 2003. A Novel Me-
thod for Stemmer Generation Based on Hidden Mar-
kov Models. Proceedings of the twelfth international 
conference on Information and knowledge manage-
ment, pp. 131-138. 
 
 
Mirko Popovi? and Peter Willett. 1992. The effective-
ness of stemming for natural-language access to Slo-
vene textual data. Journal of the American Society 
for Information Science and Technology, Vol. 43, No. 
5, pp. 384-390. 
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, Vol. 14, No. 3, pp. 130-137.  
Stephen E. Robertson, Steve Walker, Susan Jones, 
Micheline Hancock-Beaulieu, and Mike Gatford. 
1995. Okapi at TREC-3. Proceedings of the Third 
Text REtrieval Conference, NIST Special Publication 
500-226. pp. 109-126.   
Enkhbayar Sanduijav, Takehito Utsuro, and Satoshi 
Sato. 2005. Mongolian phrase generation and mor-
phological analysis based on phonological and mor-
phological constraints. Journal of Natural Language 
Processing, Vol. 12, No. 5, pp. 185-205. (In Japa-
nese) . 
Sock Y. Tai, Cheng O. Ong, and Noor A. Abdullah. 
2000. On designing an automated Malaysian stem-
mer for the Malay language. Proceedings of the fifth 
international workshop on information retrieval with 
Asian languages, Hong Kong, pp. 207-208. 
Bayarmaa Ts. 2002. Mongolian grammar for grades I-
IV. (In Mongolian). 
Ellen M. Voorhees and Dawn M. Tice. 2000. Building a 
Question Answering Test Collection. Proceedings of 
the 23rd Annual International ACM SIGIR Confe-
rence on Research and Development in Information 
Retrieval, pp. 200-207. 
Jinxi Xu and Bruce W. Croft. 1998. Corpus-based 
stemming using co-occurrence of word variants. 
ACM Transactions on Information Systems, Vol. 16, 
No. 1, pp. 61-81. 
8
Statistical Machine Translation based Passage Retrieval
for Cross-Lingual Question Answering
Tomoyosi Akiba Kei Shimizu
Dept. of Information and Computer Sciences,
Toyohashi University of Technology
1-1 Hibarigaoka, Tenpaku-cho, Toyohashi-shi,
441-8580, JAPAN
akiba@cl.ics.tut.ac.jp
Atsushi Fujii
Graduate School of Library,
Information and Media Studies,
University of Tsukuba
1-2 Kasuga, Tsukuba, 305-8550, JAPAN
fujii@slis.tsukuba.ac.jp
Abstract
In this paper, we propose a novel ap-
proach for Cross-Lingual Question Answer-
ing (CLQA). In the proposed method, the
statistical machine translation (SMT) is
deeply incorporated into the question an-
swering process, instead of using it as the
pre-processing of the mono-lingual QA pro-
cess as in the previous work. The proposed
method can be considered as exploiting the
SMT-based passage retrieval for CLQA task.
We applied our method to the English-to-
Japanese CLQA system and evaluated the
performance by using NTCIR CLQA 1 and
2 test collections. The result showed that the
proposed method outperformed the previous
pre-translation approach.
1 Introduction
Open-domain Question Answering (QA) was first
evaluated extensively at TREC-8 (Voorhees and
Tice, 1999). The goal in the factoid QA task is to
extract words or phrases as the answer to a question
from an unorganized document collection, rather
than the document lists obtained by traditional infor-
mation retrieval (IR) systems. The cross-lingual QA
task, which has been evaluated at CLEF (Magnini et
al., 2003) and NTCIR (Sasaki et al, 2005), gener-
alizes the factoid QA task by allowing the different
languages pair between the question and the answer.
Basically, the CLQA system can be constructed
simply by translating either the question sentence
or the target documents into the language of the
other side, and applying a mono-lingual QA system.
For example, after the English question sentence is
translated into Japanese, a Japanese mono-lingual
QA system can be applied to extract the answer from
the Japanese target documents. Depending on the
translation techniques used for the pre-processing,
the previous CLQA approach can be classified into
the machine translation based approach (Shimizu et
al., 2005; Mori and Kawagishi, 2005) and the dic-
tionary based approach (Isozaki et al, 2005).
In this paper, we propose a novel approach for
CLQA task. In the proposed method, the statisti-
cal machine translation (SMT) (Brown et al, 1993)
is deeply incorporated into the question answer-
ing process, instead of using the SMT as the pre-
processing before the mono-lingual QA process as
in the previous work. Though the proposed method
can be applied to any language pairs in principle, we
focus on the English-to-Japanese (EJ) CLQA task,
where a question sentence is given in English and its
answer is extracted from a document collection in
Japanese.
Recently, language modeling approach for infor-
mation retrieval has been widely studied (Croft and
Lafferty, 2003). Among them, statistical transla-
tion model has been applied for mono-lingual IR
(Berger and Lafferty, 1999), cross-lingual IR (Xu
et al, 2001), and mono-lingual QA (Murdock and
Croft, 2004). Our method can be considered as that
applying the translation model to cross-lingual QA.
In the rest of this paper, Section 2 summarizes the
previous approach for CLQA. Section 3 describes
our proposed method in detail. Section 4 describes
the experimental evaluation conducted to see the
performance of the proposed method by comparing
it to some reference methods. Section 5 describes
our conclusion and future works.
2 Previous CLQA Systems
Figure 1 shows the configuration of our previ-
ous English-to-Japanese cross-lingual QA system,
which has almost the same configuration to the con-
ventional CLQA systems. Firstly, the input En-
glish question is translated into the corresponding
Japanese question by using a machine translation.
Alternatively, the machine translation can be re-
751
English
Question
Japanese
Question
Document Retrieval
Japanese
Documents
Answer Extraction
Answer
Candidates
Passage Similarity
Calculation
Type Matching Score
Calculation
Answer Rescoring
Japanese
Answer
Japanese
Document
Collection
Expected Answer
Type Detection
Expected
Answer Type
Machine Translation
(or Dictionary-based
Translation)
Figure 1: The configuration of the conventional
CLQA system.
placed by the dictionary-based term-by-term transla-
tion. Then, either the English question or the trans-
lated Japanese question is analyzed to get the ex-
pected answer type.
After that, the mono-lingual QA process is in-
voked. The translated Japanese question is used as
the query of the document retrieval to get the doc-
uments that include the query terms. From the re-
trieved documents, the answer candidates that match
with the expected answer type are extracted with
their location in the documents. Next, the extracted
candidates are rescored by the two points of views;
the passage similarity and the type matching. The
passage similarity is calculated between the trans-
lated Japanese question and the Japanese passage
that surrounds the answer candidate, while the type
matching score is calculated as the likelihood that
the candidate is matched with the expected answer
type. Finally the reordered candidates are outputted
as the answers of the given question.
3 Proposed CLQA System
On the other hand, Figure 2 shows the configuration
of our proposed cross-lingual QA system. It does
not use the machine translation (nor the dictionary-
based translation) as the pre-processing of the input
English question. The original English question is
English
Question
Document Retrieval
Japanese
Documents
Answer Extraction
Answer
Candidates
Passage Similarity
Calculation
Type Matching Score
Calculation
Answer Rescoring
Japanese
Answer
Japanese
Document
Collection
Expected Answer
Type Detection
Expected
Answer Type
Figure 2: The configuration of the proposed CLQA
system.
used directly in the QA process. In order to make
this approach possible, the two subsystems, the doc-
ument retrieval subsystem and the passage similar-
ity calculation subsystem, which are pointed by the
direct arrow from the English question and are em-
phasized by the thick frames in Figure 2, are cross-
lingualized to accept the English question directly
instead of the Japanese question, by means of incor-
porating the statistical machine translation (SMT)
process deeply into them.
In the following two subsections, we will explain
how these two subsystems can deal with the En-
glish question directly. The document retrieval sub-
system is modified so that the Japanese documents
are indexed by English terms. The word transla-
tion probability used in the SMT is used to index the
Japanese document with the corresponding English
terms without losing the consistency. The passage
similarity calculation subsystem calculates the sim-
ilarity between an English question and a Japanese
passage in terms of the probability that the Japanese
passage is translated into the English question.
3.1 Document Retrieval
Given an English question sentence, the document
retrieval subsystem of our proposed CLQA system
retrieves Japanese documents directly. In order to do
so, each Japanese document in the target collection
752
How much did the Japan Bank for International Cooperation
decide to loan to the Taiwan High-Speed Corporation?
Q
?????????? ??????????????
???????????
...?????????????????????? ?
?????????? ?????????
???????????????????????
?????? ???????????????????
??????????????????????
??????????????? ?...
????
article
headline
previous sentence
target sentence (including an answer candidate)
next sentence
an answer candidate
H(S) = S{ } SHS{ } S?1S{ } SS+1{ } SHS?1S{ } SHSS+1{ } S?1SS+1{ } SHS?1SS+1{ }{ }
SH
S
?1
S
S+1
Figure 3: An examples of a question and the corre-
sponding passage candidates.
has been indexed by English terms by using the word
translation probability used in the SMT framework.
The expected term frequency tf(e,D) of an En-
glish term e that would be used as an index to a
Japanese document D can be estimated by the fol-
lowing equation.
tf(e,D) =
?
j?D
t(e|j)tf(j,D) (1)
where tf(j,D) is the term frequency of a Japanese
term j in D and t(e|j) is the word translation prob-
ability that j is translated into e. The probability
t(e|j) is trained by using a large parallel corpus as
the SMT framework. Because the expected term fre-
quency tf(e,D) is consistent with tf(j,D) that is
calculated from the statistics of D, the conventional
vector space IR model based on the TF-IDF term
weighting can be used for implementing our IR sub-
system. We used GETA 1 as the IR engine in our
CLQA system.
3.2 SMT based Passage Retrieval
In order to enable the direct passage retrieval, where
the query and the passage are in different languages,
the statistical machine translation is utilized to cal-
culate the similarity between them. In order words,
we calculate the similarity between them as the
probability that the Japanese passage is translated
into the English question.
The similarity sim(Q,S|A) between a question
Q and a sentence S including an answer candidate
A is calculated by the following equation.
sim(Q,S|A) = max
D?H(S)
P (Q|D ? A) (2)
1http://geta.ex.nii.ac.jp
where P (Q|D ? A) is the probability that a word
sequence D except A is translated into a question
sentence Q, and H(S) is the set of the candidate
passage (term sequences) that are related to a sen-
tence S. The set consists of S and the power set of
S
H
, S
?1
, and S
+1
, where S
H
is the headline of the
article that S belongs, S
?1
is the previous sentence
of S, and S
+1
is the next sentence of S (Figure 3).
In this paper, we use IBM model 1 (Brown et al,
1993) in order to get the probability P (Q|D?A) as
follows.
P (Q|D ? A) =
1
(n+ 1)
m
m
?
j=1
?
i=1,???,k?1,k+l+1,???,n
t(q
j
|d
i
)(3)
where q
1
? ? ? q
m
is a English term sequence of
a question Q, d
1
? ? ? d
n
is a Japanese term se-
quence of a candidate passage D, d
k
? ? ? d
k+l
is
a Japanese term sequence of an answer candi-
date A. Therefore, the Japanese term sequence
d
1
, ? ? ? , d
k?1
, d
k+l+1
, ? ? ? , d
n
(= D - A) is just D ex-
cept A. We exclude the answer term sequence A
from the calculation of the translation probability,
because the English terms that corresponds to the
answer should not be appeared in the question sen-
tence as the nature of question answering.
4 Experimental Evaluation
The experimental evaluation was conducted to see
the total performance of cross language question an-
swering by using our proposed method.
4.1 Test collections
The NTCIR-5 CLQA1 test collection (Sasaki et
al., 2005) and the NTCIR-6 CLQA2 test collection
(Sasaki et al, 2007) for English-to-Japanese task
were used for the evaluation. Each collection con-
tains 200 factoid questions in English. The target
documents for CLQA1 are two years newspaper ar-
ticles from ?YOMIURI SHINBUN? (2000-2001),
while those for CLQA2 are two years articles from
?MAINICHI SHINBUN? (1998-1999).
In the test collections, the answer candidates are
judged with three categories; Right, Unsupported,
and Wrong. The answer labeled Right is correct
and supported by the document that it is from. The
answer labeled Unsupported is correct but not sup-
ported by the document that it is from. The answer
labeled Wrong is incorrect. We used two kind of
golden set for our evaluation: the set including only
753
Right answers (referred as to R) and the set includ-
ing Right and Unsupported answers (referred as to
R+U).
Note that the evaluation results obtained from
CLQA2 are more reliable than that from CLQA1,
because we participated in CLQA2 formal run with
our proposed method (and our reference method la-
beled DICT) and most of the answers by the system
were manually checked for the pooling.
4.2 Translation Model
The translation model used for our method was
trained from the following English-Japanese paral-
lel corpus.
? 170,379 example sentence pairs from the
Japanese-English and English-Japanese dictio-
naries.
? 171,186 sentence pairs from newspaper articles
obtained by the automatic sentence alignment
(Utiyama and hitoshi Isahara, 2003).
A part of the latter sentence pairs were ob-
tained from the paired newspapers that are ?YOMI-
URI SHINBUN? and its English translation ?Daily
Yomiuri?. Because the target documents of CLQA1
are the articles from ?YOMIURI SHINBUN? as
described above, the corresponding sentence pairs,
which are extracted from the articles from 2000 to
2001, were removed from the training corpus for
CLQA1.
Before training the translation model, both En-
glish and Japanese sides of the sentence pairs in par-
allel corpus were normalized. For the sentences of
Japanese side, the inflectional words were normal-
ized to their basic forms by using a Japanese mor-
phological analyzer. For the sentences of English
side, the inflectional words were also normalized
to their basic forms by using a Part-of-Speech tag-
ger and all the words were lowercased. GIZA++
(Och and Ney, 2003) was used for training the IBM
model 4 from the normalized parallel corpus. The
vocabulary sizes were about 58K words for Japanese
side and 74K words for English side. The trained
Japanese-to-English word translation model t(e|j)
was used for our proposed document retrieval (Sec-
tion 3.1) and passage similarity calculation (Section
3.2).
4.3 Compared methods
The proposed method was compared with the sev-
eral reference methods. As the methods from pre-
vious works, three pre-translation methods were in-
vestigated.
The first two methods translate the question by us-
ing machine translation. One of them used a com-
mercial off-the-shell machine translation software 2
(referred to as RMT). The other used the statisti-
cal machine translation that had been created by us-
ing the IBM model 4 obtained from the same par-
allel corpus and tools described in Section 4.2, the
tri-gram language model constructed by using the
target documents of CLQA1, and the existing SMT
decoder (Germann, 2003) (referred to as SMT).
The two methods, RMT and SMT, differ only in
the translation methods, while their backend mono-
lingual QA systems are common.
The third method translates the question by us-
ing translation dictionary (referred to as DICT).
The cross-lingual IR system described in (Fujii and
Ishikawa, 2001) was used for our ?document re-
trieval? subsystem in Figure 2. The CLIR system
enhances the basic translation dictionary, which has
about 1,000,000 entries, with the compound words
obtained by using the statistics of the target doc-
uments and with the borrowed words by using the
transliteration method. Note that, as the other parts
of the system than the document retrieval, includ-
ing proposed SMT based passage retrieval, are all
identical to the proposed method, this comparison is
focused only on the difference in the document re-
trieval methods.
In order to investigate the performance if the ideal
translation is made, the reference Japanese transla-
tions of the English questions included in the test
collections were used as the input of the mono-
lingual QA system (referred to as JJ).
As the variations of the proposed method, the fol-
lowing four methods were compared.
Proposed The same method as described in Section
3.
Proposed +r The document retrieval score is also
used to rescore the answer candidates in
?Rescoring? subsystem in Figure 2, in addi-
tion to the passage similarity score and the type
matching score.
Proposed -p For the passage similarity calculation,
the passage is always fixed only the central sen-
tence S, i.e. the equation (2) is replaced by the
following.
sim(Q,S|A) = P (Q|S ? A) (4)
Proposed -p+r Combination of above two modifi-
cations.
2?IBM Japan, honyaku-no-oosama ver. 5?
754
Table 1: Comparison of the JJ results between the test collections.
test collection R R+U
Top1 Acc. Top5 Acc. MRR Top1 Acc. Top5 Acc. MRR
CLQA1 0.140 0.300 0.196 0.260 0.535 0.354
CLQA2 0.245 0.410 0.307 0.270 0.530 0.366
Table 2: The performances of the proposed and ref-
erence CLQA systems with respect to CLQA1 test
collection.
method Top1 Acc. Top5 Acc. MRR
RMT 0.065 0.175 0.099
SMT 0.060 0.175 0.098
Dict 0.095 0.195 0.134
Proposed 0.090 0.225 0.146
Table 3: The performances among the proposed
methods with respect to CLQA1 test collection.
method Top1 Acc. Top5 Acc. MRR
Proposed 0.090 0.225 0.146
Proposed +r 0.105 0.285 0.173
Proposed -p 0.105 0.245 0.155
Proposed -p+r 0.120 0.280 0.178
JJ 0.260 0.535 0.354
4.4 Evaluation Metrics
Each system outputted five ranked answers a
1
? ? ? a
5
for each question q. We investigated the perfor-
mance of the systems in terms of three evaluation
metrics that are obtained by averaging over all the
questions: the accuracy of the top ranked answers
(referred to as Top 1 Acc.), the accuracy of up-to
fifth ranked answers (referred to as Top 5 Acc.), and
the reciprocal rank (referred to asMRR)RR(q) cal-
culated by the following equation.
rr(a
i
) =
{
1/i if a
i
is a correct answer
0 otherwise (5)
RR(q) = max
ai
rr(a
i
) (6)
4.5 Results
Firstly, we compared the results obtained by using
CLQA1 test collection with that obtained by using
CLQA2. Table 1 shows the results for JJ system.
By using the R judgment, the JJ results of CLQA1
was much worse than that of CLQA2, while the re-
sults were almost same by using the R+U judgment.
Because the difference with respect to the difficul-
ties between the two test collections seems small and
the results from CLQA2 are more reliable, we con-
cluded that the R judgment of CLQA1 was unreli-
able. Therefore, for CLQA1 test collection, we only
investigated the result by using R+U judgment.
Secondly, we compared the proposed method
(Proposed) with the previous methods (RMT,
SMT, and Dict). Table 2 shows the results with re-
spect to CLQA1 test collection. The two methods
based on the machine translation (RMT and SMT)
indicated almost same performance, while the per-
formance of the proposed method was about 1.3 to
1.5 times better for CLQA1. Especially, because the
same training data was used to build the translation
models both in SMT and Proposed, it was shown
that the method to build the SMT model in the QA
process was better than that to use the same SMT
model for pre-processing (pre-translating) the input
sentence.
The DICT performed almost same as the Pro-
posed for CLQA1, while Proposed was 1.7 to 1.9
times better than DICT for CLQA2 as shown in Ta-
ble 4. Note again that this comparison was focused
on the document retrieval subsystem, because the
passage retrieval subsystems of these two methods
were same.
Thirdly, the variations between the proposed
methods were compared. Table 3 shows the results
with respect to CLQA1 test collection. For CLQA1,
both the additional use of the document retrieval
score (+r) and the use of the fixed central sentence
for passage similarity calculation (-p) improved the
performance. However, for CLQA2, the document
retrieval score (+r) did not contribute to improve the
performance, as shown in Table 4.
Finally, seeing from the comparison between JJ
and Proposed, it was shown that the performance of
the proposed CLQA system was about half of that of
the ideal CLQA system.
755
Table 4: The performances of the proposed and reference CLQA systems with respect to CLQA2 test
collection.
methods R R+U
Top1 Acc. Top5 Acc. MRR Top1 Acc. Top5 Acc. MRR
Dict 0.070 0.155 0.102 0.100 0.275 0.163
Proposed 0.130 0.200 0.155 0.165 0.295 0.210
Proposed +r 0.120 0.220 0.153 0.155 0.325 0.211
JJ 0.245 0.410 0.307 0.270 0.530 0.366
5 Conclusion
In this paper, a novel approach for CLQA was pro-
posed. The proposed method did not translate the
input question in source language into the target
language as the preprocessing of QA process. In-
stead, the statistical machine translation was deeply
incorporated into the two QA subsystems in order
to deal with the question in source language directly
in the QA process. Especially, SMT-based passage
retrieval was explored.
For the passage similarity calculation in this pa-
per, the simple IBM model 1 was used. In the future
work, we will investigate if the more sophisticated
translation model or that specialized for CLQA task
can improve the performance further.
References
Adam Berger and John Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of the 22nd Annual
Conference on Research and Development in Information
Retrieval (ACM SIGIR), pages 222?229.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 18(4):263?311.
W. Bruce Croft and John Lafferty, editors. 2003. Language
Modeling for Information Retrieval. Kluwer Academic Pub-
lishers.
Atsushi Fujii and Tetsuya Ishikawa. 2001. Japanese/english
cross-language information retrieval: Exploration of query
translation and transliteration. Computers and the Humani-
ties, 35(4):389?420.
Ulrich Germann. 2003. Greedy decoding for statistical ma-
chine translatioin in slmost linear time. In Proceedings of
HLT-NAACL.
Hideki Isozaki, Katsuhito Sudoh, and Hajime Tsukada. 2005.
NTT?s japanese-english cross-language question answering
system. In Proceedings of The Fifth NTCIRWorkshop, pages
186?193.
Bernardo Magnini, Alessandro Vallin, Christelle Ayache, Gre-
gor Erbach, Anselmo Pe nas, Maarten de Rijke, Paulo
Rocha, Kiril Simov, and Richard Sutcliffe. 2003. Overview
of the CLEF 2004 multilingual question answering track. In
Multilingual Information Access for Text, Speech and Im-
ages, pages 371?391.
Tatsunori Mori and Masami Kawagishi. 2005. A method of
cross language question-answering based on machine trans-
lation and transliteration. In Proceedings of The Fifth NTCIR
Workshop, pages 215?222.
Vanessa Murdock and W. Bruce Croft. 2004. Simple trans-
lation models for sentence retrieval in factoid question an-
swering. In Proceedings of the Workshop on Information
Retrieval for Question Answering.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Yutaka Sasaki, Hsin-Hsi Chen, Kuang hua Chen, and Chuan-Jie
Lin. 2005. Overview of the NTCIR-5 cross-lingual question
answering task (clqa1). In Proceedings of The Fifth NTCIR
Workshop, pages 175?185.
Yutaka Sasaki, Chuan-Jie Lin, Kuang hua Chen, and Hsin-Hsi
Chen. 2007. Overview of the NTCIR-6 cross-lingual ques-
tion answering (clqa) task. In Proceedings of The NTCIR-6
Workshop Meeting.
Kei Shimizu, Tomoyosi Akiba, Atsushi Fujii, and Katunobu
Itou. 2005. Bi-directional cross language question answer-
ing using a single monolingual QA system. In Proceedings
of The Fifth NTCIR Workshop, pages 236?237.
Masao Utiyama and hitoshi Isahara. 2003. Reliable measures
for aligning japanese-english news articles and sentences. In
Proceedings of Annual Meeting of the Association for Com-
putational Linguistics, pages 72?79.
E. Voorhees and D. Tice. 1999. The TREC-8 question answer-
ing track evaluation. In Proceedings of the 8th Text Retrieval
Conference, pages 83?106, Gaithersburg, Maryland.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001. Evalu-
ating a probabilistic model for cross-lingual information re-
trieval. In Proceedings of the 24th Annual Conference on
Research and Development in Information Retrieval (ACM
SIGIR), pages 105?110.
756
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 657?664,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extracting loanwords from Mongolian corpora and producing a 
Japanese-Mongolian bilingual dictionary
 
Badam-Osor Khaltar 
Graduate School of Library, 
Information and Media Studies 
University of Tsukuba 
1-2 Kasuga Tsukuba, 305-8550 
Japan 
khab23@slis.tsukuba.ac.jp 
 
 
 
Atsushi Fujii 
Graduate School of Library, 
Information and Media Studies 
University of Tsukuba 
1-2 Kasuga Tsukuba, 305-8550 
Japan 
fujii@slis.tsukuba.ac.jp 
 
 
 
Tetsuya Ishikawa 
The Historiographical Institute 
The University of Tokyo 
3-1 Hongo 7-chome, Bunkyo-ku 
Tokyo, 133-0033 
Japan 
ishikawa@hi.u-tokyo.ac.jp 
 
 
 
Abstract 
This paper proposes methods for extracting 
loanwords from Cyrillic Mongolian corpora 
and producing a Japanese?Mongolian 
bilingual dictionary. We extract loanwords 
from Mongolian corpora using our own 
handcrafted rules. To complement the 
rule-based extraction, we also extract words 
in Mongolian corpora that are phonetically 
similar to Japanese Katakana words as 
loanwords. In addition, we correspond the 
extracted loanwords to Japanese words and 
produce a bilingual dictionary. We propose a 
stemming method for Mongolian to extract 
loanwords correctly. We verify the 
effectiveness of our methods experimentally. 
 
1 Introduction  
Reflecting the rapid growth in science and 
technology, new words and technical terms are being 
progressively created, and these words and terms are 
often transliterated when imported as loanwords in 
another language. 
Loanwords are often not included in dictionaries, 
and decrease the quality of natural language 
processing, information retrieval, machine 
translation, and speech recognition. At the same time, 
compiling dictionaries is expensive, because it relies 
on human introspection and supervision. Thus, a 
number of automatic methods have been proposed to 
extract loanwords and their translations from corpora, 
targeting various languages. 
In this paper, we focus on extracting loanwords in 
Mongolian. The Mongolian language is divided into 
Traditional Mongolian, written using the Mongolian 
alphabet, and Modern Mongolian, written using the 
Cyrillic alphabet. We focused solely on Modern 
Mongolian, and use the word ?Mongolian? to refer 
to Modern Mongolian in this paper. 
There are two major problems in extracting 
loanwords from Mongolian corpora. 
The first problem is that Mongolian uses the 
Cyrillic alphabet to represent both conventional 
words and loanwords, and so the automatic 
extraction of loanwords is difficult. This feature 
provides a salient contrast to Japanese, where the 
Katakana alphabet is mainly used for loanwords and 
proper nouns, but not used for conventional words. 
The second problem is that content words, such as 
nouns and verbs, are inflected in sentences in 
Mongolian. Each sentence in Mongolian is 
segmented on a phrase-by-phase basis. A phrase 
consists of a content word and one or more suffixes, 
such as postpositional particles. Because loanwords 
are content words, then to extract loanwords 
correctly, we have to identify the original form using 
stemming. 
In this paper, we propose methods for extracting 
loanwords from Cyrillic Mongolian and producing a 
Japanese?Mongolian bilingual dictionary. We also 
propose a stemming method to identify the original 
forms of content words in Mongolian phrases. 
657
2 Related work 
To the best of our knowledge, no attempt has been 
made to extract loanwords and their translations 
targeting Mongolian. Thus, we will discuss existing 
methods targeting other languages.  
In Korean, both loanwords and conventional 
words are spelled out using the Korean alphabet, 
called Hangul. Thus, the automatic extraction of 
loanwords in Korean is difficult, as it is in 
Mongolian. Existing methods that are used to extract 
loanwords from Korean corpora (Myaeng and Jeong, 
1999; Oh and Choi, 2001) use the phonetic 
differences between conventional Korean words and 
loanwords. However, these methods require 
manually tagged training corpora, and are expensive. 
A number of corpus-based methods are used to 
extract bilingual lexicons (Fung and McKeown, 
1996; Smadja, 1996). These methods use statistics 
obtained from a parallel or comparable bilingual 
corpus, and extract word or phrase pairs that are 
strongly associated with each other. However, these 
methods cannot be applied to a language pair where 
a large parallel or comparable corpus is not available, 
such as Mongolian and Japanese. 
Fujii et al (2004) proposed a method that does not 
require tagged corpora or parallel corpora to extract 
loanwords and their translations. They used a 
monolingual corpus in Korean and a dictionary 
consisting of Japanese Katakana words. They 
assumed that loanwords in multiple countries 
corresponding to the same source word are 
phonetically similar. For example, the English word 
?system? has been imported into Korean, Mongolian, 
and Japanese. In these languages, the romanized 
words are ?siseutem?, ?sistem?, and ?shisutemu?, 
respectively. 
It is often the case that new terms have been 
imported into multiple languages simultaneously, 
because the source words are usually influential 
across cultures. It is feasible that a large number of 
loanwords in Korean can also be loanwords in 
Japanese. Additionally, Katakana words can be 
extracted from Japanese corpora with a high 
accuracy. Thus, Fujii et al (2004) extracted the 
loanwords in Korean corpora that were phonetically 
similar to Japanese Katakana words. Because each 
of the extracted loanwords also corresponded to a 
Japanese word during the extraction process, a 
Japanese?Korean bilingual dictionary was produced 
in a single framework. 
However, a number of open questions remain 
from Fujii et al?s research. First, their stemming 
method can only be used for Korean. Second, their 
accuracy in extracting loanwords was low, and thus, 
an additional extraction method was required. Third, 
they did not report on the accuracy of extracting 
translations, and finally, because they used Dynamic 
Programming (DP) matching for computing the 
phonetic similarities between Korean and Japanese 
words, the computational cost was prohibitive. 
In an attempt to extract Chinese?English 
translations from corpora, Lam et al (2004) 
proposed a similar method to Fujii et al (2004). 
However, they searched the Web for 
Chinese?English bilingual comparable corpora, and 
matched named entities in each language corpus if 
they were similar to each other. Thus, Lam et al?s 
method cannot be used for a language pair where 
comparable corpora do not exist. In contrast, using 
Fujii et al?s (2004) method, the Katakana dictionary 
and a Korean corpus can be independent. 
In addition, Lam et al?s method requires 
Chinese?English named entity pairs to train the 
similarity computation. Because the accuracy of 
extracting named entities was not reported, it is not 
clear to what extent this method is effective in 
extracting loanwords from corpora. 
 
3 Methodology 
3.1 Overview 
In view of the discussion outlined in Section 2, we 
enhanced the method proposed by Fujii et al (2004) 
for our purpose. Figure 1 shows the method that we 
used to extract loanwords from a Mongolian corpus 
and to produce a Japanese?Mongolian bilingual 
dictionary. Although the basis of our method is 
similar to that used by Fujii et al (2004), 
?Stemming?, ?Extracting loanwords based on rules?, 
and ?N-gram retrieval? are introduced in this paper. 
First, we perform stemming on a Mongolian 
corpus to segment phrases into a content word and  
one or more suffixes. 
658
 
Second, we discard segmented content words if 
they are in an existing dictionary, and extract the 
remaining words as candidate loanwords. 
Third, we use our own handcrafted rules to extract 
loanwords from the candidate loanwords. While the 
rule-based method can extract loanwords with a high 
accuracy, a number of loanwords cannot be extracted 
using predefined rules. 
Fourth, as performed by Fujii et al (2004), we use 
a Japanese Katakana dictionary and extract a 
candidate loanword that is phonetically similar to a 
Katakana word as a loanword. We romanize the 
candidate loanwords that were not extracted using 
the rules. We also romanize all words in the 
Katakana dictionary.  
However, unlike Fujii et al (2004), we use 
N-gram retrieval to limit the number of Katakana 
words that are similar to the candidate loanwords. 
Then, we compute the phonetic similarities between 
each candidate loanword and each retrieved 
Katakana word using DP matching, and select a pair 
whose score is above a predefined threshold. As a 
result, we can extract loanwords in Mongolian and 
their translations in Japanese simultaneously. 
Finally, to identify Japanese translations for the 
loanwords extracted using the rules defined in the 
third step above, we perform N-gram retrieval and 
DP matching.  
We will elaborate further on each step in Sections 
3.2?3.7. 
3.2 Stemming 
A phrase in Mongolian consists of a content word 
and one or more suffixes. A content word can 
potentially be inflected in a phrase. Figure 2 shows 
 
 
Mongolian corpus Katakana dictionary
 Stemming 
 
 Extracting candidate loanwords Romanization 
 
Japanese-Mongolian bilingual dictionaryExtracting loanwords based on rules 
 
 
 
Romanization N-gram retrieval 
 
 
Mongolian loanword dictionary 
High Similarity
Computing phonetic similarity 
Fig  ure 1: Overview of our extraction method.
Type Example 
(a) No inflection. ??? + ?? ? ????? 
Book + Genitive Case 
(b) Vowel elimination. ???? +???+ ??? ???????? 
Work + Ablative Case +Reflexive
(c) Vowel insertion. ?? + ? ? ???? 
Brother + Dative Case 
(d) Consonant insertion. ?????? + ???? ??????????
Building + Genitive Case 
(e) The letter ??? is 
converted to ???, and 
the vowel is eliminated. 
????????+ ???? ?????????? 
School + Ablative Case 
Figure 2: Inflection types of nouns in Mongolian. 
the inflection types of content words in phrases. In 
phrase (a), there is no inflection in the content word 
???? (book)? concatenated with the suffix ??? 
(genitive case)?. 
However, in phrases (b)?(e) in Figure 2, the 
content words are inflected. Loanwords are also 
inflected in all of these types, except for phrase (b). 
Thus, we have to identify the original form of a 
content word using stemming. While most 
loanwords are nouns, a number of loanwords can 
also be verbs. In this paper, we propose a stemming 
method for nouns. Figure 3 shows our stemming 
method. We will explain our stemming method 
further, based on Figure 3. 
First, we consult a ?Suffix dictionary? and 
perform backward partial matching to determine 
whether or not one or more suffixes are concatenated 
at the end of a target phrase. 
Second, if a suffix is detected, we use a ?Suffix 
segmentation rule? to segment the suffix and extract 
659
 
Figure 3: Overview of our noun stemming method. 
 
the noun. The inflection type in phrases (c)?(e) in 
Figure 2 is also determined. 
Third, we investigate whether or not the vowel 
elimination in phrase (b) in Figure 2 occurred in the 
extracted noun. Because the vowel elimination 
occurs only in the last vowel of a noun, we check the 
last two characters of the extracted noun. If both of 
the characters are consonants, the eliminated vowel 
is inserted using a ?Vowel insertion rule? and the 
noun is converted into its original form. 
Existing Mongolian stemming methods (Ehara et 
al., 2004; Sanduijav et al, 2005) use noun 
dictionaries. Because we intend to extract loanwords 
that are not in existing dictionaries, the above 
methods cannot be used. Noun dictionaries have to 
be updated as new words are created. 
Our stemming method does not require a noun 
dictionary. Instead, we manually produced a suffix 
dictionary, suffix segmentation rule, and vowel 
insertion rule. However, once these resources are 
produced, almost no further compilation is required. 
The suffix dictionary consists of 37 suffixes that 
can concatenate with nouns. These suffixes are 
postpositional particles. Table 1 shows the dictionary 
entries, in which the inflection forms of the 
postpositional particles are shown in parentheses. 
The suffix segmentation rule consists of 173 rules. 
We show examples of these rules in Figure 4. Even 
if suffixes are identical in their phrases, the 
segmentation rules can be different, depending on  
the counterpart noun. 
In Figure 4, the suffix ????? matches both the 
noun phrases (a) and (b) by backward partial 
matching. However, each phrase is segmented by a        
Table 1: Entries of the suffix dictionary. 
detect a suffix in
the phrase 
Suffix dictionary Suffix segmentation rule
phrase 
noun 
segment a suffix 
and extract a noun
Yes 
 
insert a vowel 
check if the last two characters of the 
noun are both consonants 
Vowel insertion rule
No 
Case Suffix 
Genitive 
Accusative 
Dative 
Ablative 
Instrumental 
Cooperative 
Reflexive 
Plural 
?, ?, ??, ??, ??, ???, ??? 
??, ???, ? 
?, ? 
??? (???), ??? (???), ???, ??? 
??? (???), ??? (???), ???, ??? 
???, ???, ??? 
?? (??), ?? (??), ??, ?? 
??? (???), ??? (???) 
 
Suffix Noun phrase Noun 
(a) ?????? 
mother?s 
??? 
mother 
 
??? 
Genitive 
 
(b) ????????? 
Haraa?(river name)s 
????? 
Haraa 
Figure 4: Examples of the suffix segmentation rule. 
 
deferent rule independently. The underlined suffixes 
are segmented in each phrase, respectively. In phrase 
(a), there is no inflection, and the suffix is easily 
segmented. However, in phrase (b), a consonant 
insertion has occurred. Thus, both the inserted 
consonant, ???, and the suffix have to be removed. 
 The vowel insertion rule consists of 12 rules. To 
insert an eliminated vowel and extract the original 
form of the noun, we check the last two characters of 
a target noun. If both of these are consonants, we 
determine that a vowel was eliminated. 
However, a number of nouns end with two 
consonants inherently, and therefore, we referred to a 
textbook on Mongolian grammar (Bayarmaa, 2002) 
to produce 12 rules to determine when to insert a 
vowel between two consecutive consonants. 
For example, if any of ???, ???, ???, ???, ???, or 
??? are at the end of a noun, a vowel is inserted. 
However, if any of ???, ???, ???, ???, ???, ???, ???, 
???, or ??? are the second to last consonant in a noun, 
a vowel is not inserted. 
The Mongolian vowel harmony rule is a 
phonological rule in which female vowels and male 
vowels are prohibited from occurring in a single 
word together (with the exception of proper nouns). 
We used this rule to determine which vowel should 
be inserted. The appropriate vowel is determined by 
the first vowel of the first syllable in the target noun. 
660
For example, if there are ??? and ??? in the first 
syllable, the vowel ??? is inserted between the last 
two consonants. 
3.3 Extracting candidate loanwords 
After collecting nouns using our stemming method, 
we discard the conventional Mongolian nouns. We 
discard nouns defined in a noun dictionary 
(Sanduijav et al, 2005), which includes 1,926 nouns. 
We also discard proper nouns and abbreviations. The 
first characters of proper nouns, such as ?????????? 
(Erdenebat)?, and all the characters of abbreviations, 
such as ????? (Nuclear research centre)?, are 
written using capital letters in Mongolian. Thus, we 
discard words that are written using capital 
characters, except those occurring at the beginning of 
sentences. In addition, because ??? and ??? are not 
used to spell out Western languages, words including 
those characters are also discarded. 
3.4 Extracting loanwords based on rules 
We manually produced seven rules to identify 
loanwords in Mongolian. Words that match with one 
of the following rules are extracted as loanwords. 
(a) A word including the consonants ???, ???, ???, 
or ???. 
These consonants are usually used to spell out 
foreign words. 
(b) A word that violated the Mongolian vowel 
harmony rule. 
Because of the vowel harmony rule, a word 
that includes female and male vowels, which is 
not based on the Mongolian phonetic system, is 
probably a loanword. 
(c) A word beginning with two consonants. 
A conventional Mongolian word does not 
begin with two consonants. 
(d) A word ending with two particular consonants. 
A word whose penultimate character is any 
of: ???, ???, ???, ???, ???, ???, or ??? and 
whose last character is a consonant violates 
Mongolian grammar, and is probably a 
loanword. 
(e) A word beginning with the consonant ???. 
In a modern Mongolian dictionary (Ozawa, 
2000), there are 54 words beginning with ???, 
of which 31 are loanwords. Therefore, a word 
beginning with ??? is probably a loanword. 
(f) A word beginning with the consonant ???. 
In a modern Mongolian dictionary (Ozawa, 
2000), there are 49 words beginning with ???, 
of which only four words are conventional 
Mongolian words. Therefore, a word beginning 
with ??? is probably a loanword. 
(g) A word ending with ?<consonant> + ??. 
We discovered this rule empirically. 
3.5 Romanization  
We manually aligned each Mongolian Cyrillic 
alphabet to its Roman representation1. 
In Japanese, the Hepburn and Kunrei systems are 
commonly used for romanization proposes. We used 
the Hepburn system, because its representation is 
similar to that used in Mongolian, compared to the 
Kunrei system. 
However, we adapted 11 Mongolian romanization 
expressions to the Japanese Hepburn romanization. 
For example, the sound of the letter ?L? does not 
exist in Japanese, and thus, we converted ?L? to ?R? 
in Mongolian. 
3.6 N-gram retrieval 
By using a document retrieval method, we efficiently 
identify Katakana words that are phonetically similar 
to a candidate loanword. In other words, we use a 
candidate loanword, and each Katakana word as a 
query and a document, respectively. We call this 
method ?N-gram retrieval?. 
Because the N-gram retrieval method does not 
consider the order of the characters in a target word, 
the accuracy of matching two words is low, but the 
computation time is fast. On the other hand, because 
DP matching considers the order of the characters in 
a target word, the accuracy of matching two words is 
high, but the computation time is slow. We combined 
these two methods to achieve a high matching 
accuracy with a reasonable computation time. 
First, we extract Katakana words that are 
phonetically similar to a candidate loanword using 
N-gram retrieval. Second, we compute the similarity 
between the candidate loanword and each of the 
retrieved Katakana words using DP matching to 
improve the accuracy. 
We romanize all the Katakana words in the 
dictionary and index them using consecutive N 
                                                         
1 http://badaa.mngl.net/docs.php?p=trans_table (May, 2006) 
661
characters. We also romanize each candidate 
loanword when use as a query. We experimentally 
set N = 2, and use the Okapi BM25 (Robertson et al, 
1995) for the retrieval model. 
3.7 Computing phonetic similarity 
Given the romanized Katakana words and the 
romanized candidate loanwords, we compute the 
similarity between the two strings, and select the 
pairs associated with a score above a predefined 
threshold as translations. We use DP matching to 
identify the number of differences (i.e., insertion, 
deletion, and substitution) between two strings on an 
alphabet-by-alphabet basis. 
While consonants in transliteration are usually the 
same across languages, vowels can vary depending 
on the language. The difference in consonants 
between two strings should be penalized more than 
the difference in vowels. We compute the similarity 
between two romanized words using Equation (1). 
         
vc
dvdc
+?
+??? ?
? )(2
1           (1) 
Here, dc and dv denote the number of differences in 
consonants and vowels, respectively, and ? is a 
parametric consonant used to control the importance 
of the consonants. We experimentally set ? = 2. 
Additionally, c and v denote the number of all the 
consonants and vowels in the two strings, 
respectively. The similarity ranges from 0 to 1. 
 
4 Experiments  
4.1 Method 
We collected 1,118 technical reports published in 
Mongolian from the ?Mongolian IT Park?2 and used 
them as a Mongolian corpus. The number of phrase 
types and phrase tokens in our corpus were 110,458 
and 263,512, respectively. 
We collected 111,116 Katakana words from 
multiple Japanese dictionaries, most of which were 
technical term dictionaries. 
We evaluated our method from four perspectives: 
?stemming?, ?loanword extraction?, ?translation 
extraction?, and ?computational cost.? We will 
discuss these further in Sections 4.2-4.5, respectively. 
4.2 Evaluating stemming  
We randomly selected 50 Mongolian technical 
                                                         
2 http://www.itpark.mn/ (May, 2006) 
reports from our corpus, and used them to evaluate 
the accuracy of our stemming method. These 
technical reports were related to: medical 
science (17), geology (10), light industry (14), 
agriculture (6), and sociology (3). In these 50 reports, 
the number of phrase types including conventional 
Mongolian nouns and loanword nouns was 961 and 
206, respectively. We also found six phrases 
including loanword verbs, which were not used in 
the evaluation.  
Table 2 shows the results of our stemming 
experiment, in which the accuracy for conventional 
Mongolian nouns was 98.7% and the accuracy for 
loanwords was 94.6%. Our stemming method is 
practical, and can also be used for morphological 
analysis of Mongolian corpora. 
We analyzed the reasons for any failures, and 
found that for 12 conventional nouns and 11 
loanwords, the suffixes were incorrectly segmented. 
4.3 Evaluating loanword extraction 
We used our stemming method on our corpus and 
selected the most frequently used 1,300 words. We 
used these words to evaluate the accuracy of our 
loanword extraction method. Of these 1,300 words, 
165 were loanwords. We varied the threshold for the 
similarity, and investigated the relationship between 
precision and recall. Recall is the ratio of the number 
of correct loanwords extracted by our method to the 
total number of correct loanwords. Precision is the 
ratio of the number of correct loanwords extracted 
by our method to the total number of words 
extracted by our method. We extracted loanwords 
using rules (a)?(g) defined in Section 3.4. As a result, 
139 words were extracted. 
Table 3 shows the precision and recall of each rule. 
The precision and recall showed high values using 
?All rules?, which combined the words extracted by 
rules (a)?(g) independently. 
We also extracted loanwords using the phonetic 
similarity, as discussed in Sections 3.6 and 3.7. 
 
Table 2: Results of our noun stemming method. 
 No. of each phrase type Accuracy (%) 
Conventional 
nouns 
961 98.7
Loanwords 206 94.6
662
 
 
 
 
 
 
 
We used the N-gram retrieval method to obtain up to 
the top 500 Katakana words that were similar to each 
candidate loanword. Then, we selected up to the top 
five pairs of a loanword and a Katakana word whose 
similarity computed using Equation (1) was greater 
than 0.6. Table 4 shows the results of our 
similarity-based extraction. 
Both the precision and the recall for the 
similarity-based loanword extraction were lower 
than those for the ?All rules? data listed in Table 3. 
 
Table 4: Precision and recall for our similarity-based 
loanword extraction. 
Words extracted 
automatically 
Extracted correct 
loanwords 
Precision 
(%) 
Recall
(%) 
3,479 109 3.1 66.1
 
We also evaluated the effectiveness of a 
combination of the N-gram and DP matching 
methods. We performed similarity-based extraction 
after rule-based extraction. Table 5 shows the results, 
in which the data of the ?Rule? are identical to those 
of the ?All rules? data listed in Table 3. However, the 
?Similarity? data are not identical to those listed in 
Table 4, because we performed similarity-based 
extraction using only the words that were not 
extracted by rule-based extraction.  
When we combined the rule-based and 
similarity-based methods, the recall improved from 
84.2% to 91.5%. The recall value should be high 
when a human expert modifies or verifies the 
resultant dictionary. 
Figure 5 shows example of extracted loanwords in 
Mongolian and their English glosses. 
4.4 Evaluating Translation extraction  
In the row ?Both? shown in Table 5, 151 loanwords 
were extracted, for each of which we selected up to 
the top five Katakana words whose similarity 
computed using Equation (1) was greater than 0.6 as 
 
 
Table 3: Precision and recall for rule-based loanword extraction. 
Rules (a) (b) 
 
(c) (d) (e) (f) (g) All rules 
Words extracted automatically 102 63
 
21 6 4 5 24 150
Extracted correct loanwords 101 60
 
20 5 4
 
5 19 139
Precision (%) 99.0 95.2 95.2 83.3
 
Table 5: Precision and recall of different loanword 
extraction methods. 
 No. of 
words
No. that 
were correct 
Precision 
(%)  
Recall 
(%) 
Rule 150 139 92.7 84.2
Similarity 60 12 20.0 46.2
Both 210 151 71.2 91.5
 
Mongolian English gloss 
???????? 
????????? 
???????? 
????????? 
albumin 
laboratory 
mechanism 
mitochondria 
Figure 5: Example of extracted loanwords. 
 
translations. As a result, Japanese translations were 
extracted for 109 loanwords. Table 6 shows the 
results, in which the precision and recall of 
extracting Japanese?Mongolian translations were 
56.2% and 72.2%, respectively. 
We analyzed the data and identified the reasons 
for any failures. For five loanwords, the N-gram 
retrieval failed to search for the similar Katakana 
words. For three loanwords, the phonetic similarity 
computed using Equation (1) was not high enough 
for a correct translation. For 27 loanwords, the 
Japanese translations did not exist inherently. For 
seven loanwords, the Japanese translations existed, 
but were not included in our Katakana dictionary.  
Figure 6 shows the Japanese translations extracted 
for the loanwords shown in Figure 5. 
 
Table 6: Precision and recall for translation 
extraction.  
No. of translations 
extracted 
automatically 
No. of extracted 
correct 
translations 
Precision 
(%) 
 
Recall 
(%) 
194 109 56.2 72.2
 
100 100 79.2 92.7
Recall (%) 61.2 36.4 12.1 3.0 2.4 3.03 11.5 84.2
663
Japanese Mongolian English gloss 
????? 
?????  ?
????? 
??????? 
???????? 
????????? 
???????? 
????????? 
albumin 
laboratory 
mechanism 
mitochondria 
Figure 6: Japanese translations extracted for the 
loanwords shown in Figure 5. 
 
4.5 Evaluating computational cost 
We randomly selected 100 loanwords from our 
corpus, and used them to evaluate the computational 
cost of the different extraction methods. We 
compared the computation time and the accuracy of 
?N-gram?, ?DP matching?, and ?N-gram + DP 
matching? methods. The experiments were 
performed using the same PC (CPU = Pentium III 1 
GHz dual, Memory = 2 GB). 
Table 7 shows the improvement in computation 
time by ?N-gram + DP matching? on ?DP matching?, 
and the average rank of the correct translations for 
?N-gram?. We improved the efficiency, while 
maintaining the sorting accuracy of the translations. 
 
Table 7: Evaluation of the computational cost. 
Method N-gram DP N-gram + DP
Loanwords 100 
Computation time (sec.) 95 136,815 293
Extracted correct 
translations 
66 66 66
Average rank of correct 
translations 
44.8 2.7 2.7
 
5 Conclusion 
We proposed methods for extracting loanwords from 
Cyrillic Mongolian corpora and producing a 
Japanese?Mongolian bilingual dictionary. Our 
research is the first serious effort in producing 
dictionaries of loanwords and their translations 
targeting Mongolian. We devised our own rules to 
extract loanwords from Mongolian corpora. We also 
extracted words in Mongolian corpora that are 
phonetically similar to Japanese Katakana words as 
loanwords. We also corresponded the extracted 
loanwords to Japanese words, and produced a 
Japanese?Mongolian bilingual dictionary. A noun 
stemming method that does not require noun 
dictionaries was also proposed. Finally, we evaluated 
the effectiveness of the components experimentally. 
 
References  
Terumasa Ehara, Suzushi Hayata, and Nobuyuki Kimura. 2004. 
Mongolian morphological analysis using ChaSen. Proceedings 
of the 10th Annual Meeting of the Association for Natural 
Language Processing, pp. 709-712. (In Japanese). 
Atsushi Fujii, Tetsuya Ishikawa, and Jong-Hyeok Lee. 2004. 
Term extraction from Korean corpora via Japanese. 
Proceedings of the 3rd International Workshop on 
Computational Terminology, pp. 71-74. 
Pascal Fung and Kathleen McKeown. 1996. Finding terminology 
translations from non-parallel corpora. Proceedings of the 5th 
Annual Workshop on Very Large Corpora, pp. 53-87. 
Wai Lam, Ruizhang Huang, and Pik-Shan Cheung. 2004. 
Learning phonetic similarity for matching named entity 
translations and mining new translations. Proceedings of the 
27th Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, pp. 
289-296. 
Sung Hyun Myaeng and Kil-Soon Jeong. 1999. 
Back-Transliteration of foreign words for information retrieval. 
Information Processing and Management, Vol. 35, No. 4, pp. 
523 -540. 
Jong-Hooh Oh and Key-Sun Choi. 2001. Automatic extraction of 
transliterated foreign words using hidden markov model. 
Proceedings of the International Conference on Computer 
Processing of Oriental Languages, 2001, pp. 433-438. 
Shigeo Ozawa. Modern Mongolian Dictionary. Daigakushorin. 
2000. 
Stephen E. Robertson, Steve Walker, Susan Jones, Micheline 
Hancock-Beaulieu, and Mike Gatford. 1995. Okapi at TREC-3,  
Proceedings of the Third Text REtrieval Conference (TREC-3), 
NIST Special Publication 500-226. pp. 109-126. 
Enkhbayar Sanduijav, Takehito Utsuro, and Satoshi Sato. 2005. 
Mongolian phrase generation and morphological analysis 
based on phonological and morphological constraints. Journal 
of Natural Language Processing, Vol. 12, No. 5, pp. 185-205. 
(In Japanese) . 
Frank Smadja, Vasileios Hatzivassiloglou, Kathleen R. McKeown. 
1996. Translating collocations for bilingual lexicons: A 
statistical approach. Computational Linguistics, Vol. 22, No. 1, 
pp. 1-38.  
Bayarmaa Ts. 2002. Mongolian grammar in I-IV grades. (In 
Mongolian). 
664
Overview of Patent Retrieval Task at NTCIR-3 
Makoto Iwayama 
Tokyo Institute of 
Technology/Hitachi Ltd. 
iwayama@crl.hitachi.co.jp 
Atsushi Fujii 
University of Tsukuba/Japan 
Science and Technology Corp.
fujii@slis.tsukuba.ac.jp 
Noriko Kando 
National Institute of 
Informatics 
kando@nii.ac.jp 
Akihiko Takano 
National Institute of 
Informatics 
aki@acm.org 
 
 
Abstract 
We describe the overview of patent re-
trieval task at NTCIR-3. The main task was 
the technical survey task, where participants 
tried to retrieve relevant patents to news ar-
ticles. In this paper, we introduce the task 
design, the patent collections, the character-
istics of the submitted systems, and the re-
sults overview. We also arranged the free-
styled task, where participants could try 
anything they want as far as the patent col-
lections were used. We describe the brief 
summaries of the proposals submitted to the 
free-styled task. 
1 Introduction 
In the field of information retrieval, there have 
been held successive evaluation workshops, such 
as TREC [8], CREF [1], and NTCIR [5], to build 
and utilize various kinds of test collections. In the 
Third NTCIR Workshop (NTCIR-3), which was 
held from June 2001 to December 2003, a serious 
effort was first made in the ?Patent Retrieval Task? 
to explore information retrieval targeting patent 
documents. 
The goal of Patent Retrieval Task is to provide 
test collections for enhancing research on patent 
information processing, from patent retrieval to 
patent mining. Although there exist many com-
mercial patent retrieval systems and services, pat-
ent retrieval has not been paid much attention in 
the research field of information retrieval. One of 
the reasons is the lack of test collection on patent. 
TREC used patent documents as a part of the 
document collections, but there was no treatment 
specially applied to the patent collection. 
In SIGIR2000, the first workshop on patent re-
trieval was held [4] and there were many fruitful 
discussions on the current status and future direc-
tions of patent retrieval. The workshop convinced 
us that there was the need of test collections spe-
cifically for patents. 
We then asked for PATOLIS Co. [7] to provide 
patent collections for the patent retrieval task. Con-
sequently, we could release three kinds of patent 
collections; those were two years? Japanese full 
texts, five years? Japanese abstracts, and five 
years? English abstracts. At the same time, we 
could fortunately have cooperation with JIPA (Ja-
pan Intellectual Property Association) [3] in creat-
ing search topics and assessing the relevance. 
Since each member of JIPA belongs to the intellec-
tual property division in her/his company, they are 
all experts in patent searching. All the above 
contributions enabled us to kick off the first 
evaluation workshop designed for patent 
information processing. 
There are various phases and aspects in patent 
information processing. For example, various 
kinds of users (researchers, patent searchers, busi-
ness managers, and so on) search patents for vari-
ous purposes (technical survey, finding conflicting 
applications, buying/selling patents, and so on). 
Corresponding to each situation, an appropriate 
search model should be developed. The standard of 
the relevance judgments may also depend on each 
situation. In some cases, retrieving relevant patents 
is not enough but further analysis on the retrieved 
patents might be necessary. For example, creating 
a patent map of a product would clarify the patent 
relations between the techniques used to make the 
product. Cross-lingual patent retrieval is also im-
portant when applying patents to foreign countries. 
All of these are within scope of our project and this 
task was the first step toward our goal. 
2 Task Design 
In this workshop, we focused on a simple task of 
technical survey. End-users we assumed in the task 
were novice users, for example, business managers. 
The major reason of adopting such general task 
was that we could only use the two years? full texts 
that were not enough for trying more patent-
oriented task like finding conflicting applications 
from patents. 
 
 
Figure 1: Scenario of technology survey 
 
To fit the task to a real situation, we used Japa-
nese news articles as the original sources of search 
topics, so the task was conducting cross-database 
retrieval, searching patents by news articles. The 
task assumed the following situation that is de-
picted in Figure 1. When a business manager looks 
through news articles and is interested in one of 
them, she/he clips it out and asks a searcher to find 
related patents to the clipping. The manager passes 
the clipping to the searcher along with her/his 
memorandum, and this clipping with memorandum 
became the search topic in this task. The memo-
randum helps the searcher to have the exact infor-
mation need the manager has, when the clipping 
contains non-relevant topics or the clipping has 
little description on the information need. Task 
participants played the role of the searcher and 
tried to retrieve relevant patents to the clipping. 
Since the purpose of the searching was technical 
survey, the claim part in patent was not treated 
specifically in assessing the relevance. Patent 
documents were treated as if those were technical 
papers. 
Cross-database retrieval itself is so general that 
techniques investigated in the task can be applied 
to various combinations of databases. This is an-
other purpose of the task. 
We prepared search topics in four languages, 
Japanese, English, Korean, and Chinese (both tra-
ditional and simplified). Participants could try 
cross-lingual patent retrieval by using one of the 
non-Japanese topics. Unfortunately, only two 
groups submitted cross-lingual results and both of 
them used English topics. 
In addition to the technical survey task ex-
plained so far, we arranged the optional task, 
where participants could try anything they want as 
far as they used the patent collections provided. 
One of the purposes of this free-styled task is to 
explore next official tasks. 
3 Characteristics of Patent Applications 
In this section, we briefly review the characteristics 
of patent applications (patent documents). 
? There are structures, for example, claims, 
purposes, effects, and embodiments of the 
invention. 
? Although the claim part is the most impor-
tant in patent, it is written in an unusual style 
especially for Japanese patent; all the sub-
topics are written in single sentence. 
? To enlarge the scope of invention, vague or 
general terms are often used in claims. 
? Patents include much technical terminology. 
Applicants may define and use their original 
terms not used in other patents. 
? There are large variations in length. The 
longest patent in our collections contains 
about 30,000 Japanese words! 
? The search models would be significantly 
different between industries, for example, 
between chemical / pharmaceutical indus-
tries and computers / machinery / electric 
industries. 
? Classification exists. IPC (International Pat-
ent Classification) is the most popular one. 
? The criterion of evaluation depends on the 
purpose of searching. For example, high re-
call is required for finding conflicting appli-
cations. 
? In some industries, images are important to 
judge the relevance. 
Our task focused on few of the above character-
istics. We treated patent documents as technical 
documents rather than legal statements, so we did 
not distinguish between the claim part and the oth-
ers in assessing the relevance. High recall was not 
necessary, so we used the standard averaged preci-
sion to evaluate the results. Few groups used struc-
tures and classifications. Images were not included 
in the patent collections provided. 
4 Patent Collections 
PATOLIS Co. provided and we released the fol-
lowing patent collections. 
? kkh: Publication of unexamined patent ap-
plications (1998, 1999) (in Japanese) 
? jsh: JAPIO Patent Abstracts (1995?1999) 
(in Japanese) 
? paj: Patent Abstracts Japan (1995? 1999) (in 
English) 
?Kkh? contains full texts of unexamined patent 
applications in Japanese. Images were eliminated. 
?Jsh? contains human edited abstracts in Japanese. 
Although all the texts in ?kkh? have the abstracts 
written by the applicants, experts in JAPIO (Japan 
Patent Information Organization) [2] short-
ened/lengthened about half of them to fit the length 
within about 400 Japanese characters. They also 
normalized technical terms if necessary. ?Paj? is 
English translation of ?jsh?. 
translation by  
human experts 
 
modif ication of the original abstracts by 
human experts (JAPIO) 
 
kkh: (98,99) 
Publication of 
unexamined patent 
applications  
(in Japanese) 
 
jsh: (95-99) 
JAPIO Patent 
Abstracts 
(in Japanese) 
 
paj: (95-99) 
Patent Abstracts 
Japan 
(in English) 
 
 
Figure 2: Relationships between the patent col-
lections 
 
Figure 2 shows the relationships between these 
three collections. Here, we see parallel relations, 
for example, full texts vs. abstracts, original ab-
stracts vs. edited abstracts, and Japanese abstracts 
vs. English abstracts. Researchers can use these 
parallel collections for various purposes, for exam-
ple, finding rules of abstracting, creating a term 
normalization dictionary, acquiring translation 
knowledge, and so on. 
Table 1 summarizes the characteristics of the 
three collections. 
 
 kkh jsh paj 
Type Full text Abstract Abstract 
Language Japanese Japanese English 
Years 98,99 95-99 95-99 
Number of 
documents
697,262 1,706,154 1,701,339
Bytes 18139M 1883M 2711M 
 
Table 1: Characteristics of the patent collections 
5 Topics 
JIPA members created topics, six for the dry run 
and 25 for the formal run. Since the topics for the 
dry run were substantially revised after the dry run, 
we decided to re-use those in the formal run. In 
consequence, we had the total 31 topics for the 
formal run. 
Figure 3 is an example of the topics in English 
and Table 2 shows the explanations of the fields in 
the topics. In our task, <ARTICLE> and 
<SUPPLEMENT> correspond to the news clipping 
and the memorandum respectively. 
The topics also contain <DESCRIPTION> and 
<NARRATIVE> fields we are familiar with. Since 
many NTCIR tasks already have the results for 
using <DESCRIPTION> and <NARRATIVE> 
fields, we can compare our results of using these 
fields with the results of other tasks. 
Along with the grade of relevance (i.e., ?A?, 
?B?, ?C?, or ?D?), each judged patent has a mark 
(?S?, ?J?, or ?U?) representing the origin from 
which the patent was retrieved. Table 3 explains 
about the marks. For example, a document with 
?BJ? means that the document was judged as ?par-
tially relevant? (i.e. ?B-?) and only found by ex-
perts in their preliminary search (i.e., ?-J?). 
Here, note that all the submitted runs contrib-
uted to collecting the ?S? patents, but only the top 
30 patents for each run were used. Note also that 
we can restore the patent set retrieved by the man-
ual search (i.e., ?PJ? set) by collecting ?J? and ?U? 
patents. 
 
 
<TOPIC><NUM>P004</NUM><LANG>EN</LANG> 
<PURPOSE>technology survey</PURPOSE> 
<TITLE>Device to judge relative merits by comparing 
codes such as barcodes with each other</TITLE> 
<ARTICLE> 
<A-DOC> 
<A-DOCNO>JA-981031179</A-DOCNO> 
<A-LANG>JA</A-LANG> 
<A-SECTION>Society</A-SECTION> 
<A-AE>No</A-AE> 
<A-WORDS>189</A-WORDS> 
<A-HEADLINE>BANDAI lost a lawsuit for piracy filed by 
EPOCH at Tokyo District Court</A-HEADLINE> 
<A-DATE>1998-10-31</A-DATE> 
<A-TEXT>In settlement of the lawsuit filed by EPOCH 
INC., the toy manufacturer, against BANDAI CO., LTD. As 
compensation of 264 million for damages for infringement 
of a card game patent, the Tokyo District Court ordered 
BANDAI to pay about 114 million on the 30th. The presid-
ing judge, Mr. Yoshiyuki Mori, indicated that some func-
tions including key operation for the "Super Barcode 
Wars" mini game machine manufactured and sold by BANDAI 
CO., LTD. in July, 1992 to March, 1993 fell under the 
"technical range of a patent licensed to EPOCH 
INC.".</A-TEXT> 
</A-DOC> 
</ARTICLE> 
<SUPPLEMENT>Determination of victory or defeat by com-
paring each other's values based on codes from barcode 
readings does not conflict with the patent.</SUPPLEMENT> 
<DESCRIPTION>What kind of devices determines leaders or 
victors by reading several codes such as barcodes and 
comparing the values corresponding to these 
codes?</DESCRIPTION> 
<NARRATIVE>"Super Barcode Wars" is a type of mini game 
machine where recorded barcodes are read in cards fea-
turing characters and the game proceeds in semi-real 
time by operating offence and defense keys. Sample codes 
include barcodes and magnetic codes, but shall not be 
defined as limited only to these.</NARRATIVE> 
<CONCEPT>Sign, barcode, code, superiority or inferior-
ity, victory or defeat, comparison, judgment</CONCEPT> 
<PI>PATENT-KKH-G-H01-333373</PI> 
</TOPIC> 
 
Figure 3: Example of the topics 
 
 
Field Explanation 
<LANG> Language code 
<PURPOSE> Purpose of search 
<TITLE> Concise representation 
of search topic 
<ARTICLE> MAINICHI news article 
in NTCIR format 
<SUPPLEMENT> Supplemental informa-
tion of news article 
<DESCRIPTION> Short description of 
search topic 
<NARRATIVE> Long description of 
search topic 
<CONCEPT> List of keywords 
<PI> Original patents of news 
article 
 
Table 2: Explanations of the fields in topics 
6 
6.1 
Results Overview 
Participants 
Eight groups submitted the 36 runs. One group 
submitted runs only for pooling. We briefly de-
scribe the characteristics of each group. Refer to 
the proceedings of Patent Retrieval Task [6] for 
each detail. 
LAPIN: This group focused on the ?term distil-
lation? in cross-database retrieval, where the dif-
ference between the term frequency in source 
database and that in target database was integrated 
into the overall term weighting. 
SRGDU: This group tried several pseudo rele-
vance feedback methods in the context of patent 
retrieval. The proposed method using Taylor for-
mula was compared with the traditional Rocchio 
method. 
daikyo: This group made long gram-based in-
dex from the patent collections. Compared with the 
traditional gram-based indexing, proposed method 
produce more compact index. 
DTEC: This group searched various kinds of 
abstracts rather than full texts, and compared the 
effectiveness of those. The abstracts were JAPIO 
patent abstracts and the combinations of ?title?, 
?applicant?s abstract?, and ?claims?. Manual and 
automatic runs were compared. 
DOVE: This group also submitted manual and 
automatic runs. In the manual runs, non-relevant 
passages in <ARTICLE> were eliminated manu-
ally. 
IFLAB: This group evaluated their cross-
lingual IR system PRIME through several mono-
lingual runs. They also evaluated their translation 
extraction method by using Japanese-US patent 
families, which were not provided in this task. 
brkly: This group submitted both monolingual 
and cross-lingual runs. In the cross-lingual runs, 
words in English topics were translated into Japa-
nese words by using English-Japanese dictionary 
automatically created by the aligned bilingual cor-
pus (i.e., ?paj? and ?jsh?). Their method of creating 
the dictionary is based on word co-occurrence with 
the association measure. 
sics: This group also submitted cross-lingual 
runs, where they automatically created a cross-
lingual thesaurus form the aligned bilingual corpus, 
?paj? and ?jsh?, and used the thesaurus for word-
based query translation. The Random Indexing 
vector-space technique was used to extract the 
cross-lingual thesaurus. Note that, in both the 
?sics? and the ?brkly? groups, there was no mem-
ber who understands Japanese. 
6.2 
6.3 
6.4 
7 
Recall/Precision 
The recall/precision graphs of the mandatory runs 
are shown in Figure 4, and those of the optional 
runs in Figure 5. In each figure, there are both re-
sults for the strict relevance (?A?) and the relaxed 
relevance (?A? + ?B?). For each run in the figures, 
brief system description is specified; the descrip-
tion includes the searching mode (automatic or 
manual), the topic fields used in query construction, 
and the topic language. 
Topic-by-topic Results 
Figure 6 shows the median of the average preci-
sions for each topic. Figure 7 shows the breakdown 
of the relevance judgments. Detailed analysis on 
each topic will be given by JIPA, where it will be 
discussed about the reasons why systems could not 
find some patents human experts found and vise 
versa. 
Recall of the relevant patents retrieved in 
the preliminary human search 
Figure 8 shows the recall of the relevant patents 
retrieved in the preliminary human search. In the 
process of making pool, we used only the top 30 
documents for each run. Here, we extracted more 
documents from each run and investigated how 
many human retrieving relevant patents could be 
covered by the systems. 
Optional (Free-styled) Task 
The following two groups applied to the optional 
task. Refer to the proceedings of Patent Retrieval 
Task [6] for each detail. 
CRL: This group investigated the method of 
extracting various rules from the existing align-
ments in patents. The ?diff? command of UNIX 
was used to find the alignments between JAPIO 
patent abstracts and the original abstracts by appli-
cants, between claims and embodiments, and be-
tween different claims in an application. 
TIT: This group focused on the unusual style of 
Japanese claims, and tried to automatically struc-
ture the claims to raise the readability of claims. 
Rhetorical structure analysis was applied for this 
purpose. 
8 Summary and Future Directions 
In this paper, we described the overview of patent 
retrieval task at NTCIR-3. We are planning to con-
tinue our effort for the next patent retrieval task 
along with the following directions. 
? Longer range of years will be covered. 
? Purpose of search would shift to more real 
one, for example, searching conflicting ap-
plications.  
Acknowledgements 
 
We are grateful to PATOLIS Co. for providing the 
patent collections of this task. We also thank all the 
members of JIPA who created the topics and as-
sessed the relevance. Without their expertise in 
patent, this task would not be realized. Lastly, we 
thank all the participants for their contributions to 
this task. 
References 
[1] CLEF (Cross Language Evaluation Forum) 
(http://clef.iei.pi.cnr.it/) 
[2] JAPIO (Japan Patent Information Organization) 
(http://www.japio.or.jp/) 
[3] JIPA (Japan Intellectual Property Association) 
(http://www.jipa.or.jp/) 
[4] ACM-SIGIR Workshop on Patent Retrieval, or-
ganized by Mun-Kew Leong and Noriko Kando, 
2000. 
(http://research.nii.ac.jp/ntcir/sigir2000ws/) 
[5] NTCIR (NII-NACSIS Test Collection for IR Sys-
tems) 
(http://research.nii.ac.jp/ntcir/index-en.html) 
[6] Proceedings of the Third NTCIR Workshop on 
Research in Information Retrieval, Automatic Text 
Summarization and Question Answering, 2003. 
[7] PATOLIS Co.                                       
 (http://www.patolis.co.jp/e-index.html) 
[8] TREC (Text Retrieval Conference) 
(http://trec.nist.gov/) 
A, mandatory
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
recall
pr
ec
is
io
n
LAPIN4(A)
DTEC1(M)
DOVE4(M)
brklypat1(A)
daikyo(M)
SRGDU5(A)
IFLAB6(A)
brklypat3(A,E)
A: auto
M: manual
E: English topics
A+B, mandatory
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
recall
pr
ec
is
io
n
LAPIN4(A)
DOVE4(M)
DTEC1(M)
daikyo(M)
brklypat1(A)
SRGDU3(A)
IFLAB6(A)
brklypat3(A,E)
A: auto
M: manual
E: English topics
 
Figure 4: Recall/Precision of mandatory runs
A, optional
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
recall
pr
ec
is
io
n
LAPIN1(A,TDNC)
brklypat2(A,DN)
DOVE3(A,DN)
DOVE2(A,D)
SRGDU6(A,DN)
IFLAB2(A,D)
IFLAB3(A,DN)
IFLAB7(A,T)
brklypat4(A,DN,E)
A: auto
M : manual
T: TITLE
D: DESCRIPTION
N: NARRATIVE
C: CONCEPT
E: English Topics
 
 A+B, optional
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
recall
pr
ec
is
io
n
LAPIN1(A,TDNC)
DOVE3(A,DN)
brklypat2(A,DN)
DOVE2(A,D)
IFLAB2(A,D)
SRGDU4(A,DN)
IFLAB4(A,DN)
IFLAB7(A,T)
brklypat4(A,DN,E)
A: auto
M : manual
T: TITLE
D: DESCRIPTION
N: NARRATIVE
C: CONCEPT
E: English Topics
 
Figure 5: Recall/Precision of optional runs 
00.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1 2 3 4 5 6 7 8 9 1011 12 1314 15 16 1718 19 202122 23 24 25 26 272829 30 31
topic ID
m
ed
ia
n o
f 
a
v
e
ra
ge
 
pre
ci
si
on
s
A
A+B
 
Figure 6: Median of average precisions (all runs) 
 
0
50
10
150
20
250
30
350
400
450
BS
AS
BJ
AJ
BU
AU
BS 1 34 44 2 1543 9 29 7 3 6 15 0 6 2 2 15 5 4 0 1 0 1 7 151726 0 0 1 16
AS 0 5 27 2 0 0 55 0 1 0 1 57 6 37 0 2 0 6 9 1 0 1 1 10 49 11 5 35 0 5 15
BJ 10 7 2 2 2 0 9 42 10 3 0 47 22 2 8 4 9 36 0 5 0 0 2 3 3 16 18 3 0 2 19
AJ 4 0 0 0 0 0 23101711 1 17312 10 1 1 2 11 0 0 2 4 2 10812 6 7 7 0 1 16
BU 7 13 4 4 6 0 5 33 7 4 3 29 5 10 3 1 8 18 2 1 1 0 4 16 6 16 38 7 5 7 4
AU 22 13 6 10 15 12 27 23 18 15 4 101 22 17 5 15 5 17 36 4 8 4 4 72 49 12 19 40 6 3 16
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
topic ID
n
um
be
r 
of 
d
oc
um
en
ts
 
Figure 7: Breakdown of relevance judgments  
Recall of A J+AU
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 100 200 300 400 500 600 700 800 900 1000
ranking
re
ca
ll a ll
m andatory
m andatory (au to)
Recall  of AJ+AU+BJ+BU
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 100 200 300 400 500 600 700 800 900 1000
ranking
re
ca
ll a ll
m andatory
m andatory (au to)
 
Figure 8: Recall of the relevant patents retrieved in the preliminary human search 
Term Extraction from Korean Corpora via Japanese
Atsushi Fujii, Tetsuya Ishikawa
Graduate School of Library,
Information and Media Studies
University of Tsukuba
1-2 Kasuga, Tsukuba
305-8550, Japan
{fujii,ishikawa}@slis.tsukuba.ac.jp
Jong-Hyeok Lee
Division of Electrical and
Computer Engineering,
Pohang University of Science and Technology,
Advanced Information Technology Research Center
San 31 Hyoja-dong Nam-gu,
Pohang 790-784, Republic of Korea
jhlee@postech.ac.kr
Abstract
This paper proposes a method to extract foreign
words, such as technical terms and proper nouns,
from Korean corpora and produce a Japanese-
Korean bilingual dictionary. Specific words have
been imported into multiple countries simultane-
ously, if they are influential across cultures. The
pronunciation of a source word is similar in different
languages. Our method extracts words in Korean
corpora that are phonetically similar to Katakana
words, which can easily be identified in Japanese cor-
pora. We also show the effectiveness of our method
by means of experiments.
1 Introduction
Reflecting the rapid growth in science and tech-
nology, new words have progressively been created.
However, due to the limitation of manual compila-
tion, new words are often out-of-dictionary words
and decrease the quality of human language tech-
nology, such as natural language processing, infor-
mation retrieval, machine translation, and speech
recognition. To resolve this problem, a number
of automatic methods to extract monolingual and
bilingual lexicons from corpora have been proposed
for various languages.
In this paper, we focus on extracting foreign words
(or loanwords) in Korean. Technical terms and
proper nouns are often imported from foreign lan-
guages and are spelled out (or transliterated) by the
Korean alphabet system called Hangul . The similar
trend can be observable in Japanese and Chinese. In
Japanese, foreign words are spelled out by its special
phonetic alphabet (or phonogram) called Katakana.
Thus, foreign words can be extracted from Japanese
corpora with a high accuracy, because the Katakana
characters are seldom used to describe the conven-
tional Japanese words, excepting proper nouns.
However, extracting foreign words from Korean
corpora is more difficult, because in Korean both
the conventional and foreign words are written with
Hangul characters. This problem remains a chal-
lenging issue in computational linguistic research.
It is often the case that specific words have been
imported into multiple countries simultaneously, be-
cause the source words (or concepts) are usually in-
fluential across cultures. Thus, it is feasible that a
large number of foreign words in Korean can also be
foreign words in Japanese.
In addition, the foreign words in Korean and
Japanese corresponding to the same source word are
phonetically similar. For example, the English word
?system? has been imported into both Japanese and
Korean. The romanized words are /sisutemu/ and
/siseutem/ in both countries, respectively.
Motivated by these assumptions, we propose a
method to extract foreign words in Korean corpora
by means of Japanese. In brief, our method per-
forms as follows. First, foreign words in Japanese
are collected, for which Katakana words in corpora
and existing lexicons can be used. Second, from Ko-
rean corpora the words that are phonetically similar
to Katakana words are extracted. Finally, extracted
Korean words are compiled in a lexicon with the cor-
responding Japanese words.
In summary, our method can extract foreign words
in Korean and produce a Japanese-Korean bilingual
lexicon in a single framework.
2 Methodology
2.1 Overview
Figure 1 exemplifies our extraction method, which
produces a Japanese-Korean bilingual lexicon using
a Korean corpus and Japanese corpus and/or lexi-
con. The Japanese and Korean corpora do not have
to be parallel or comparable. However, it is desir-
able that both corpora are associated with the same
domain. For the Japanese resource, the corpus and
lexicon can alternatively be used or can be used to-
gether. Note that compiling Japanese monolingual
lexicon is less expensive than that for a bilingual lex-
icon. In addition, new Katakana words can easily be
extracted from a number of on-line resources, such
as the World Wide Web. Thus, the use of Japanese
lexicons does not decrease the utility of our method.
First, we collect Katakana words from Japanese
resources. This can systematically be performed by
means of a Japanese character code, such as EUC-
JP and SJIS.
Second, we represent the Korean corpus and
Japanese Katakana words by the Roman alphabet
(i.e., romanization), so that the phonetic similarity
can easily be computed. However, we use different
romanization methods for Japanese and Korean.
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 71
Third, we extract candidates of foreign words
from the romanized Korean corpus. An alternative
method is to first perform morphological analysis
on the corpus, extract candidate words based on
morphemes and parts-of-speech, and romanize the
extracted words. Our general model does not con-
strain as to which method should be used in the
third step. However, because the accuracy of anal-
ysis often decreases for new words to be extracted,
we experimentally adopt the former method.
Finally, we compute the phonetic similarity be-
tween each combination of the romanized Hangul
and Katakana words, and select the combinations
whose score is above a predefined threshold. As a
result, we can obtain a Japanese-Korean bilingual
lexicon consisting of foreign words.
It may be argued that English lexicons or cor-
pora can be used as source information, instead of
Japanese resources. However, because not all En-
glish words have been imported into Korean, the
extraction accuracy will decrease due to extraneous
words.
Figure 1: Overview of our extraction method.
2.2 Romanizing Japanese
Because the number of phones consisting of Japanese
Katakana characters is limited, we manually pro-
duced the correspondence between each phone
and its Roman representation. The numbers of
Katakana characters and combined phones are 73
and 109, respectively. We also defined a symbol to
represent a long vowel. In Japanese, the Hepbern
and Kunrei systems are commonly used for roman-
ization purposes. We use the Hepburn system, be-
cause its representation is similar to that in Korean,
compared with the Kunrei system.
However, specific Japanese phones, such as /ti/,
do not exist in Korean. Thus, to adapt the Hepburn
system to Korean, /ti/ and /tu/ are converted to
/chi/ and /chu/, respectively.
2.3 Romanizing Korean
The number of Korean Hangul characters is much
greater than that of Japanese Katakana characters.
Each Hangul character is a combination of more
than one consonant. The pronunciation of each char-
acter is determined by its component consonants.
In Korean, there are types of consonant, i.e., the
first consonant, vowel, and last consonant. The
numbers of these consonants are 19, 21, and 27, re-
spectively. The last consonant is optional. Thus, the
number of combined characters is 11,172. However,
to transliterate imported words, the official guide-
line suggests that only seven consonants be used as
the last consonant. In EUC-KR, which is a stan-
dard coding system for Korean text, 2,350 common
characters are coded independent of the pronunci-
ation. Therefore, if we target corpora represented
by EUC-KR, each of the 2,350 characters has to be
corresponded to its Roman representation.
We use Unicode, in which Hangul characters are
sorted according to the pronunciation. Figure 2 de-
picts a fragment of the Unicode table for Korean,
in which each line corresponds to a combination
of the first consonant and vowel and each column
corresponds to the last consonant. The number of
columns is 28, i.e., the number of the last consonants
and the case in which the last consonant is not used.
From this figure, the following rules can be found:
 the first consonant changes every 21 lines, which
corresponds to the number of vowels,
 the vowel changes every line (i.e., 28 characters)
and repeats every 21 lines,
 the last consonant changes every column.
Based on these rules, each character and its pro-
nunciation can be identified by the three consonant
types. Thus, we manually corresponded only the 68
consonants to Roman alphabets.
Figure 2: A fragment of the Unicode table for Ko-
rean Hangul characters.
We use the official romanization system for Ko-
rean, but specific Korean phones are adapted to
Japanese. For example, /j/ and /l/ are converted
to /z/ and /r/, respectively.
It should be noted that the adaptation is not in-
vertible and thus is needed for both J-to-K and K-
to-J directions.
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology72
For example, the English word ?cheese?, which
has been imported to both Korean and Japanese as
a foreign word, is romanized as /chiseu/ in Korean
and /ti:zu/ in Japanese. Here, /:/ is the symbol
representing a Japanese long vowel. Using the adap-
tation, these expressions are converted to /chizu/
and /chi:zu/, respectively, which look more similar
to each other, compared with the original strings.
2.4 Extracting term candidates from
Korean corpora
To extract candidates of foreign words from a Ko-
rean corpus, we first extract phrases. This can be
performed systematically, because Korean sentences
are segmented on a phrase-by-phrase basis.
Second, because foreign words are usually nouns,
we use hand-crafted rules to remove post-position
suffixes (e.g., Josa) and extract nouns from phrases.
Third, we discard nouns including the last con-
sonants that are not recommended for translitera-
tion purposes in the official guideline. Although the
guideline suggests other rules for transliteration, ex-
isting foreign words in Korean are not necessarily
regulated by these rules.
Finally, we consult a dictionary to discard exist-
ing Korean words, because our purpose is to extract
new words. For this purpose, we experimentally
use the dictionary for SuperMorph-K morphologi-
cal analyzer1, which includes approximately 50,000
Korean words.
2.5 Computing Similarity
Given romanized Japanese and Korean words, we
compute the similarity between the two strings and
select the pairs associated with the score above a
threshold as translations. We use a DP (dynamic
programming) matching method to identify the
number of differences (i.e., insertion, deletion, and
substitution) between two strings, on a alphabet-
by-alphabet basis.
In principle, if two strings are associated with a
smaller number of differences, the similarity between
them becomes greater. For this purpose, a Dice-style
coefficient can be used.
However, while the use of consonants in translit-
eration is usually the same across languages, the
use of vowels can vary significantly depending on
the language. For example, the English word ?sys-
tem? is romanized as /sisutemu/ and /siseutem/
in Japanese and Korean, respectively. Thus, the dif-
ferences in consonants between two strings should
be penalized more than the differences in vowels.
In view of the above discussion, we compute the
similarity between two romanized words by Equa-
tion (1).
1 ?
2 ? (? ? dc + dv)
? ? c + v (1)
Here, dc and dv denote the numbers of differences
in consonants and vowels, respectively, and ? is a
1http://www.omronsoft.com/
parametric constant used to control the importance
of the consonants. We experimentally set ? = 2. In
addition, c and v denote the numbers of all conso-
nants and vowels in the two strings. The similarity
ranges from 0 to 1.
3 Experimentation
3.1 Evaluating Extraction Accuracy
We collected 111,166 Katakana words (word types)
from multiple Japanese lexicons, most of which were
technical term dictionaries.
We used the Korean document set in the NTCIR-3
Cross-lingual Information Retrieval test collection2.
This document set consists of 66,146 newspaper ar-
ticles of Korean Economic Daily published in 1994.
We randomly selected 50 newspaper articles and
used them for our experiment. We asked a grad-
uate student excluding the authors of this paper to
identify foreign words in the target text. As a result,
124 foreign word types (205 word tokens) were iden-
tified, which were less than we had expected. This
was partially due to the fact that newspaper articles
generally do not contain a large number of foreign
words, compared with technical publications.
We manually classified the extracted words and
used only the words that were imported to both
Japan and Korea from other languages. We dis-
carded foreign words in Korea imported from Japan,
because these words were often spelled out by non-
Katakana characters, such as Kanji (Chinese charac-
ter). A sample of these words includes ?Tokyo (the
capital of Japan)?, ?Heisei (the current Japanese
era name)?, and ?enko (personal connection)?. In
addition, we discarded the foreign proper nouns for
which the human subject was not able to identify
the source word. As a result, we obtained 67 target
word types. Examples of original English words for
these words are as follows:
digital, group, dollar, re-engineering, line,
polyester, Asia, service, class, card, com-
puter, brand, liter, hotel.
Thus, our method can potentially be applied to
roughly a half of the foreign words in Korean text.
We used the Japanese words to extract plausi-
ble foreign words from the target Korean corpus.
We first romanized the corpus and extracted nouns
by removing post-position suffixes. As a result, we
obtained 3,106 words including all the 67 target
words. By discarding the words in the dictionary
for SuperMorph-K, 958 words including 59 target
words were remained.
For each of the remaining 958 words, we computed
the similarity between each of the 111,166 Japanese
words. For evaluation purposes, we varied a thresh-
old for the similarity and investigated the relation
between precision and recall. Recall is the ratio
of the number of target foreign words extracted by
our method and the total number of target foreign
2http://research.nii.ac.jp/ntcir/index-en.html
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 73
words. Precision is the ratio of the number of target
foreign words extracted by our method and the total
number of words obtained by our method.
Table 1 shows the precision and recall for differ-
ent methods. While we varied a threshold of a sim-
ilarity, we also varied the number of Korean words
corresponded to a single Katakana word (N). By
decreasing the value of the threshold and increasing
the number of words extracted, the recall can be im-
proved but the precision decreases. In Table 1, the
precision and recall are in an extreme trade-off rela-
tion. For example, when the recall was 69.5%, the
precision was only 1.2%.
We manually analyzed the words that were not ex-
tracted by our method. Out of the 59 target words,
12 compound words consisting of both conventional
and foreign words were not extracted. However,
our method extracted compound words consisting
of only foreign words. In addition, the three words
that did not have counterparts in the input Japanese
words were not extracted.
Table 1: Precision/Recall for term extraction.
Threshold for similarity
>0.9 >0.7 >0.5
N=1 50.0/8.5 12.7/40.7 4.1/47.5
N=10 50.0/8.5 7.4/47.5 1.2/69.5
3.2 Application-Oriented Evaluation
During the first experiment, we determined a specific
threshold value for the similarity between Katakana
and Hangul words and selected the pairs whose sim-
ilarity was above the threshold. As a result, we ob-
tained 667 Korean words, which were used to en-
hance the dictionary for the SuperMorph-K morpho-
logical analyzer.
We performed morphological analysis on the 50
articles used in the first experiment, which included
1,213 sentences and 9,557 word tokens. We also in-
vestigated the degree to which the analytical accu-
racy is improved by means of the additional dictio-
nary. Here, accuracy is the ratio of the number of
correct word segmentations and the total segmenta-
tions generated by SuperMorph-K. The same human
subject as in the first experiment identified the cor-
rect word segmentations for the input articles.
First, we focused on the accuracy of segmenting
foreign words. The accuracy was improved from
75.8% to 79.8% by means of the additional dictio-
nary. The accuracy for all words was changed from
94.6% to 94.8% by the additional dictionary.
In summary, the additional dictionary was effec-
tive for analyzing foreign words and was not asso-
ciated with side effect for the overall accuracy. At
the same time, we concede that we need larger-scale
experiments to draw firmer conclusions.
4 Related Work
A number of corpus-based methods to extract bilin-
gual lexicons have been proposed (Smadja et al,
1996). In general, these methods use statistics ob-
tained from a parallel or comparable bilingual corpus
and extract word or phrase pairs that are strongly
associated with each other. However, our method
uses a monolingual Korean corpus and a Japanese
lexicon independent of the corpus, which can easily
be obtained, compared with parallel or comparable
bilingual corpora.
Jeong et al (1999) and Oh and Choi (2001) in-
dependently explored a statistical approach to de-
tect foreign words in Korean text. Although the de-
tection accuracy is reasonably high, these methods
require a training corpus in which conventional and
foreign words are annotated. Our approach does not
require annotated corpora, but the detection accu-
racy is not high enough as shown in Section 3.1. A
combination of both approaches is expected to com-
pensate the drawbacks of each approach.
5 Conclusion
We proposed a method to extract foreign words,
such as technical terms and proper nouns, from Ko-
rean corpora and produce a Japanese-Korean bilin-
gual dictionary. Specific words, which have been
imported into multiple countries, are usually spelled
out by special phonetic alphabets, such as Katakana
in Japanese and Hangul in Korean.
Because extracting foreign words spelled out by
Katakana in Japanese lexicons and corpora can be
performed with a high accuracy, our method ex-
tracts words in Korean corpora that are phonetically
similar to Japanese Katakana words. Our method
does not require parallel or comparable bilingual cor-
pora and human annotation for these corpora.
We also performed experiments in which we ex-
tracted foreign words from Korean newspaper arti-
cles and used the resultant dictionary for morpho-
logical analysis. We found that our method did not
correctly extract compound Korean words consist-
ing of both conventional and foreign words. Future
work includes larger-scale experiments to further in-
vestigate the effectiveness of our method.
References
Kil Soon Jeong, Sung Hyon Myaeng, Jae Sung Lee,
and Key-Sun Choi. 1999. Automatic identification
and back-transliteration of foreign words for informa-
tion retrieval. Information Processing & Management,
35:523?540.
Jong-Hoon Oh and Key sun Choi. 2001. Automatic
extraction of transliterated foreign words using hid-
den markov model. In Proceedings of ICCPOL-2001,
pages 433?438.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Computa-
tional Linguistics, 22(1):1?38.
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology74
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 15?22,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A System for Summarizing and Visualizing Arguments in Subjective
Documents: Toward Supporting Decision Making
Atsushi Fujii
Graduate School of Library,
Information and Media Studies
University of Tsukuba
1-2 Kasuga, Tsukuba, 305-8550, Japan
fujii@slis.tsukuba.ac.jp
Tetsuya Ishikawa
The Historiographical Institute
The University of Tokyo
3-1 Hongo 7-chome, Bunkyo-ku
Tokyo, 133-0033, Japan
ishikawa@hi.u-tokyo.ac.jp
Abstract
On the World Wide Web, the volume of
subjective information, such as opinions
and reviews, has been increasing rapidly.
The trends and rules latent in a large set of
subjective descriptions can potentially be
useful for decision-making purposes. In
this paper, we propose a method for sum-
marizing subjective descriptions, specifi-
cally opinions in Japanese. We visual-
ize the pro and con arguments for a target
topic, such as ?Should Japan introduce the
summertime system?? Users can summa-
rize the arguments about the topic in order
to choose a more reasonable standpoint for
decision making. We evaluate our system,
called ?OpinionReader?, experimentally.
1 Introduction
On the World Wide Web, users can easily dissem-
inate information irrespective of their own spe-
cialty. Thus, natural language information on the
Web is not restricted to objective and authorized
information, such as news stories and technical
publications. The volume of subjective informa-
tion, such as opinions and reviews, has also been
increasing rapidly.
Although a single subjective description by
an anonymous author is not always reliable, the
trends and rules latent in a large set of subjective
descriptions can potentially be useful for decision-
making purposes.
In one scenario, a user may read customer re-
views before choosing a product. In another sce-
nario, a user may assess the pros and cons of a po-
litical issue before determining their own attitude
on the issue.
The decision making in the above scenarios is
performed according to the following processes:
(1) collecting documents related to a specific
topic from the Web;
(2) extracting subjective descriptions from the
documents;
(3) classifying the subjective descriptions ac-
cording to their polarity, such as posi-
tive/negative or pro/con;
(4) organizing (e.g., summarizing and/or visual-
izing) the classified descriptions so that users
can view important points selectively;
(5) making the decision.
Because it is expensive to perform all of the above
processes manually, a number of automatic meth-
ods have been explored. Specifically, a large num-
ber of methods have been proposed to facilitate
processes (2) and (3).
In this paper, we focus on process (4), and pro-
pose a method for summarizing subjective infor-
mation, specifically opinions in Japanese. Our
method visualizes the pro and con arguments for
a target topic, such as ?Should Japan introduce the
summertime system??
By process (4), users can summarize the argu-
ments about the topic in order to choose a more
reasonable standpoint on it. Consequently, our
system supports decision making by users.
However, process (5) is beyond the scope of this
paper, and remains an intellectual activity for hu-
man beings.
We describe and demonstrate our prototype sys-
tem, called ?OpinionReader?. We also evaluate
the components of our system experimentally.
Section 2 surveys previous research on the pro-
cessing of subjective information. Section 3 pro-
vides an overview of OpinionReader, and Sec-
15
tion 4 describes the methodologies of its compo-
nents. Section 5 describes the experiments and
discusses the results obtained.
2 Related Work
For process (1) in Section 1, existing search en-
gines can be used to search the Web for documents
related to a specific topic. However, not all re-
trieved documents include subjective descriptions
for the topic.
A solution to this problem is to automatically
identify diaries and blogs (Nanno et al, 2004),
which usually include opinionated subjective de-
scriptions.
For process (2), existing methods aim to dis-
tinguish between subjective and objective descrip-
tions in texts (Kim and Hovy, 2004; Pang and Lee,
2004; Riloff and Wiebe, 2003).
For process (3), machine-learning methods are
usually used to classify subjective descriptions
into bipolar categories (Dave et al, 2003; Beineke
et al, 2004; Hu and Liu, 2004; Pang and Lee,
2004) or multipoint scale categories (Kim and
Hovy, 2004; Pang and Lee, 2005).
For process (4), which is the subject of this pa-
per, Ku et al (2005) selected documents that in-
clude a large number of positive or negative sen-
tences about a target topic, and used their head-
lines as a summary of the topic. This is the appli-
cation of an existing extraction-based summariza-
tion method to subjective descriptions.
Hu and Liu (2004) summarized customer re-
views of a product such as a digital camera. Their
summarization method extracts nouns and noun
phrases as features of the target product, (e.g.,
?picture? for a digital camera), and lists positive
and negative reviews on a feature-by-feature basis.
The extracted features are sorted according to
the frequency with which each feature appears in
the reviews. This method allows users to browse
the reviews in terms of important features of the
target product.
Liu et al (2005) enhanced the above method to
allow users to compare different products within a
specific category, on a feature-by-feature basis.
3 Overview of OpinionReader
Figure 1 depicts the process flow in Opinion-
Reader. The input is a set of subjective descrip-
tions for a specific topic, classified according to
their polarity. We assume that processes (1)?(3) in
Section 1 are completed, either manually or auto-
matically, prior to the use of our system. It is of-
ten the case that users post their opinions and state
their standpoints, as exemplified by the websites
used in our experiments (see Section 5).
While our primarily target is a set of opinions
for a debatable issue classified into pros and cons,
a set of customer reviews for a product, classified
as positive or negative, can also be submitted.
extracting points at issue
arranging points at issue
ranking opinions
opinions about a topic
pros cons
Figure 1: Process flow in OpinionReader.
Our purpose is to visualize the pro and con ar-
guments about a target topic, so that a user can de-
termine which standpoint is the more reasonable.
We extract ?points at issue? from the opinions
and arrange them in a two-dimensional space. We
also rank the opinions that include each point at
issue according to their importance, so that a user
can selectively read representative opinions on a
point-by-point basis.
The output is presented via a graphical inter-
face as shown in Figure 2, which is an example
output for the topic ?privatization of hospitals by
joint-stock companies?. The opinions used for this
example are extracted from the website for ?BS
debate?1. This interface is accessible via existing
Web browsers.
In Figure 2, the x and y axes correspond to
the polarity and importance respectively, and each
oval denotes an extracted point at issue, such as
?information disclosure?, ?health insurance?, or
?medical corporation?.
Users can easily see which points at issue are
most important from each standpoint. Points at
issue that are important and closely related to one
particular standpoint are usually the most useful in
users? decision making.
By clicking on an oval in Figure 2, users can
read representative opinions corresponding to that
1http://www.nhk.or.jp/bsdebate/
16
point at issue. In Figure 3, two opinions that in-
clude ?information disclosure? are presented. The
opinions on the right and left sides are selected
from the pros and cons, respectively. While the
pros support information disclosure, the cons in-
sist that they have not recognized its necessity.
As a result, users can browse the pro and con
arguments about the topic in detail. However, for
some points at issue, only opinions from a single
standpoint are presented, because the other side
has no argument about that point.
Given the above functions, users can easily
summarize the main points and how they are used
in arguing about the topic in support of one stand-
point or the other.
If subjective descriptions are classified into
more than two categories with a single axis, we
can incorporate these descriptions into our system
by reclassifying them into just two categories. Fig-
ure 4 is an example of summarizing reviews with a
multipoint scale rating. We used reviews with five-
point star rating for the movie ?Star Wars: Episode
III?2. We reclassified reviews with 1?3 stars as
cons, and reviews with 4?5 stars as pros.
In Figure 4, the points at issue are typical
words used in the movie reviews (e.g. ?story?),
the names of characters (e.g. ?Anakin?, ?Obi-
Wan?, and ?Palpatine?), concepts related to Star
Wars (e.g. ?battle scene? and ?Dark Side?), and
comparisons with other movies (e.g., ?War of the
Worlds?).
Existing methods for summarizing opin-
ions (Hu and Liu, 2004; Liu et al, 2005). extract
the features of a product, which corresponds to
the points at issue in our system, and arrange them
along a single dimension representing the impor-
tance of features. The reviews corresponding to
each feature are not ranked.
However, in our system, features are arranged to
show how the feature relates to each polarity. The
opinions addressing a feature are ranked according
to their importance. We target both opinions and
reviews, as shown in Figures 2 and 4, respectively.
4 Methodology
4.1 Extracting Points at Issue
In a preliminary investigation of political opin-
ions on the Web, we identified that points at issue
can be different language units: words, phrases,
2http://moviessearch.yahoo.co.jp/detail?ty=mv&id=321602
sentences, and combinations of sentences. We
currently target nouns, noun phrases, and verb
phrases, whereas existing summarization meth-
ods (Hu and Liu, 2004; Liu et al, 2005) extract
only nouns and noun phrases.
Because Japanese sentences lack lexical seg-
mentation, we first use ChaSen3 to perform a mor-
phological analysis of each input sentence. As a
result, we can identify the words in the input and
their parts of speech.
To extract nouns and noun phrases, we use
handcrafted rules that rely on the word and part-of-
speech information. We extract words and word
sequences that match these rules. To standard-
ize among the different noun phrases that describe
the same content, we paraphrase specific types of
noun phrases.
To extract verb phrases, we analyze the syntac-
tic dependency structure of each input sentence,
by using CaboCha4. We then use handcrafted rules
to extract verb phrases comprising a noun and a
verb from the dependency structure.
It is desirable that the case of a noun (i.e., post-
positional particles) and the modality of a verb
(i.e., auxiliaries) are maintained. However, if we
were to allow variations of case and modality, verb
phrases related to almost the same meaning would
be regarded as different points at issue and thus the
output of our system would contain redundancy.
Therefore, for the sake of conciseness, we cur-
rently discard postpositional particles and auxil-
iaries in verb phrases.
4.2 Arranging Points at Issue
In our system, the points at issue extracted as
described in Section 4.1 are arranged in a two-
dimensional space, as shown in Figure 2. The x-
axis corresponds to the polarity of the points at is-
sue, that is the degree to which a point is related
to each standpoint. The y-axis corresponds to the
importance of the points at issue.
For a point at issue A, which can be a noun,
noun phrase, or verb phrase, the x-coordinate, xA,
is calculated by Equation (1):
xA = P (pro|A)? P (con|A) (1)
P (S|A), in which S denotes either the pro or con
standpoint, is the probability that an opinion ran-
domly selected from a set of opinions addressing
3http://chasen.naist.jp/hiki/ChaSen/
4http://cl.aist-nara.ac.jp/?taku-ku/software/cabocha/
17
JGCNVJKPUWTCPEG
KPHQTOCVKQPFKUENQUWTG
OGFKECNEQTRQTCVKQP
RTQHKV
EQP RTQ
KORTQXGOGPV
EQUOGVKEUWTIGT[
EWUVQOGTPGGFU
OGFKECNVTGCVOGPV
KPHQTOCVKQP
Figure 2: Example of visualizing points at issue for ?privatization of hospitals by joint-stock companies?.
1RKPKQPUQH2TQ1RKPKQPQH%QP
Figure 3: Example of presenting representative opinions for ?information disclosure?.
1DK9CP
#PCMKP
2CNRCVKPGUVQT[
EQP RTQ DCVVNGUEGPG
&CTM5KFG
9CTQHVJG9QTNFU
Figure 4: Example of summarizing reviews with multipoint scale rating for ?Star Wars: Episode III?.
18
A supports S. We calculate P (S|A) as the num-
ber of opinions that are classified into S and that
include A, divided by the number of opinions that
include A.
xA ranges from ?1 to 1. A is classified into one
of the following three categories depending on the
value of xA:
? if A appears in the pros more frequently than
in the cons, xA is a positive number,
? if A appears in the pros and cons equally of-
ten, xA is zero,
? if A appears in the cons more frequently than
in the pros, xA is a negative number.
The calculation of the y-coordinate of A, yA de-
pends on which of the above categories applies to
A. If A appears in standpoint S more frequently
than in its opposite, we define yA as the probabil-
ity that a point at issue randomly selected from the
opinions classified into S is A.
We calculate yA as the frequency of A in the
opinions classified into S, divided by the total fre-
quencies of points at issue in the opinions classi-
fied into S. Thus, yA ranges from 0 to 1.
However, if A appears in the pros and cons
equally often, we use the average of the values of
yA for both standpoints.
General words, which are usually high fre-
quency words, tend to have high values for yA.
Therefore, we discard the words whose yA is
above a predefined threshold. We empirically set
the threshold at 0.02.
Table 1 shows example points at issue for the
topic ?privatization of hospitals by joint-stock
companies? and their values of xA and yA. In Ta-
ble 1, points at issue, which have been translated
into English, are classified into the three categories
(i.e., pro, neutral, and con) according to xA and
are sorted according to yA in descending order, for
each category.
In Table 1, ?improvement? is the most impor-
tant in the pro category, and ?medical corporation?
is the most important in the con category. In the
pro category, many people expect that the qual-
ity of medical treatment will be improved if joint-
stock companies make inroads into the medical in-
dustry. However, in the con category, many people
are concerned about the future of existing medical
corporations.
Table 1: Examples of points at issue and their co-
ordinates for ?privatization of hospitals by joint-
stock companies?.
Point at issue xA yA
improvement 0.33 9.2?10?3
information disclosure 0.33 7.9?10?3
health insurance 0.60 5.3?10?3
customer needs 0.50 3.9?10?3
cosmetic surgery 0.00 2.6?10?3
medical corporation ?0.69 4.4?10?3
medical institution ?0.64 3.6?10?3
medical cost ?0.60 3.2?10?3
profit seeking ?0.78 3.2?10?3
4.3 Ranking Opinions
Given a set of opinions from which a point at is-
sue has been extracted, our purpose now is to rank
the opinions in order of importance. We assume
that representative opinions contain many content
words that occur frequently in the opinion set. In
our case, content words are nouns, verbs, and ad-
jectives identified by morphological analysis.
We calculate the score of a content word w,
s(w), as the frequency of w in the opinion set. We
calculate the importance of an opinion by the sum
of s(w) for the words in the opinion. However,
we normalize the importance of the opinion by the
number of words in the opinion because long opin-
ions usually include many words.
5 Experiments
5.1 Method
The effectiveness of our system should be evalu-
ated from different perspectives. First, the effec-
tiveness of each component of our system should
be evaluated. Second, the effectiveness of the sys-
tem as a whole should be evaluated. In this second
evaluation, the evaluation measure is the extent to
which the decisions of users can be made correctly
and efficiently.
As a first step in our research, in this paper
we perform only the first evaluation and evaluate
the effectiveness of the methods described in Sec-
tion 4. We used the following Japanese websites
as the source of opinions, in which pros and cons
are posted for specific topics.
(a) BS debate5
(b) ewoman6
5http://www.nhk.or.jp/bsdebate/
6http://www.ewoman.co.jp/
19
(c) Official website of the prime minister of
Japan and his cabinet7
(d) Yomiuri online8
For evaluation purposes, we collected the pros and
cons for five topics. Table 2 shows the five top-
ics, the number of opinions, and the sources. For
topic #4, we used the opinions collected from two
sources to increase the number of opinions.
In Table 2, the background of topic #5 should
perhaps be explained. When using escalators, it
is often customary for passengers to stand on one
side (either left or right) to allow other passen-
gers to walk past them. However, some people
insist that walking on escalators, which are mov-
ing stairs, is dangerous.
Graduate students, none of who was an author
of this paper, served as assessors, and produced
reference data. The output of a method under eval-
uation was compared with the reference data.
For each topic, two assessors were assigned to
enhance the degree of objectivity of the results. Fi-
nal results were obtained by averaging the results
over the assessors and the topics.
5.2 Evaluation of Extracting Points at Issue
For each topic used in the experiments, the asses-
sors read the opinions from both standpoints and
extracted the points at issue. We defined the point
at issue as the grounds for an argument. We did not
restrict the form of the points at issue. Thus, the
assessors were allowed to extract any continuous
language units, such as words, phrases, sentences,
and paragraphs, as points at issue.
Because our method is intended to extract
points at issue exhaustively and accurately, we
used recall and precision as evaluation measures
for the extraction.
Recall is the ratio of the number of correct an-
swers extracted automatically to the total number
of correct answers. Precision is the ratio of the
number of correct answers extracted automatically
to the total number of points at issue extracted au-
tomatically.
Table 3 shows the results for each topic, in
which ?System? denotes the number of points at
issue extracted automatically. In Table 3, ?C?,
?R?, and ?P? denote the number of correct an-
swers, recall, and precision, respectively, on an
assessor-by-assessor basis.
7http://www.kantei.go.jp/
8http://www.yomiuri.co.jp/komachi/forum/
Looking at Table 3, we see that the results
can vary depending on the topic and the assessor.
However, recall and precision were approximately
50% and 4%, respectively, on average.
The ratio of agreement between assessors was
low. When we used the points at issue extracted
by one assessor as correct answers and evaluated
the effectiveness of the other assessor in the ex-
traction, the recall and precision ranged from 10%
to 20% depending on the topic. To increase the ra-
tio of agreement between assessors, the instruction
for assessors needs to be revised for future work.
This was mainly because the viewpoint for a tar-
get topic and the language units to be extracted
were different, depending on the assessor. Be-
cause our automatic method extracted points at is-
sue exhaustively, the recall was high and the pre-
cision was low, irrespective of the assessor.
The ratios of noun phrases (including nouns)
and verb phrases to the number of manually ex-
tracted points at issue were 78.5% and 2.0%, re-
spectively. Although the ratio for verb phrases
is relatively low, extracting both noun and verb
phrases is meaningful.
The recalls of our method for noun phrases and
verb phrases were 60.0% and 44.3%, respectively.
Errors were mainly due to noun phrases that were
not modeled in our method, such as noun phrases
that include a relative clause.
5.3 Evaluation of Arranging Points at Issue
As explained in Section 4.2, in our system the
points at issue are arranged in a two-dimensional
space. The x and y axes correspond to the polarity
and the importance of points at issue, respectively.
Because it is difficult for the assessors to judge
the correctness of coordinate values in the two-
dimensional space, we evaluated the effectiveness
of arranging points at issue indirectly.
First, we evaluated the effectiveness of the cal-
culation for the y-axis. We sorted the points at is-
sue, which were extracted automatically (see Sec-
tion 5.2), according to their importance. We eval-
uated the trade-off between recall and precision
by varying the threshold of yA. We discarded the
points at issue whose yA is below the threshold.
Note that while this threshold was used to de-
termine the lower bound of yA, the threshold ex-
plained in Section 4.2 (i.e., 0.02) was used to de-
termine the upper bound of yA and was used con-
sistently irrespective of the lower bound threshold.
20
Table 2: Topics used for experiments.
#Opinions
Topic ID Topic Pro Con Source
#1 principle of result in private companies 57 29 (a)
#2 privatization of hospitals by joint-stock companies 27 44 (a)
#3 the summertime system in Japan 14 17 (b)
#4 privatization of postal services 28 20 (b), (c)
#5 one side walk on an escalator 29 42 (d)
Table 3: Recall and precision of extracting points at issue (C: # of correct answers, R: recall (%), P:
precision (%)).
Assessor A Assessor B
Topic ID System C R P C R P
#1 1968 194 58.2 5.7 101 44.6 2.3
#2 1864 66 50.0 1.8 194 60.8 6.3
#3 508 43 48.8 4.1 43 60.5 5.1
#4 949 77 64.9 5.3 96 36.5 3.7
#5 711 91 30.0 3.8 75 18.7 2.0
Table 4 shows the results, in which the precision
was improved to 50% by increasing the threshold.
In Figure 2, users can change the threshold of im-
portance by using the panel on the right side to
control the number of points at issue presented in
the interface. As a result, users can choose appro-
priate points at issue precisely.
Second, we evaluated the effectiveness of the
calculation for the x-axis. We evaluated the effec-
tiveness of our method in a binary classification.
For each point at issue extracted by an assessor,
the assessor judged which of the two standpoints
the point supports.
If a point at issue whose x-coordinate calculated
by our method is positive (or negative), it was clas-
sified as pro (or con) automatically. We did not use
the points at issue whose x-coordinate was zero for
evaluation purposes.
Table 5 shows the results. While the number of
target points at issue was different depending on
the topic and the assessor, the difference in classi-
fication accuracy was marginal.
For each topic, we averaged the accuracy deter-
mined by each assessor and averaged the accura-
cies over the topic, which gave 95.6%. Overall,
our method performs the binary classification for
points at issue with a high accuracy.
Errors were mainly due to opinions that in-
cluded arguments for both standpoints. For exam-
ple, a person supporting a standpoint might sug-
gest that he/she would support the other side un-
der a specific condition. Points at issue classified
incorrectly had usually been extracted from such
contradictory opinions.
5.4 Evaluation of Ranking Opinions
To evaluate the effectiveness of our method in
ranking opinions on a point-by-point basis, we
used a method that sorts the opinions randomly
as a control. We compared the accuracy of our
method and that of the control. The accuracy is
the ratio of the number of correct answers to the
number of opinions presented by the method un-
der evaluation.
For each point at issue extracted by an assessor,
the assessor assigned the opinions to one of the
following degrees:
? A: the opinion argues about the point at issue
and is represented,
? B: the opinion argues about the point at issue
but is not represented,
? C: the opinion includes the point at issue but
does not argue about it.
We varied the number of top opinions presented
by changing the threshold for the rank of opinions.
Table 6 shows the results, in which N denotes
the number of top opinions presented. The column
?Answer? refers to two cases: the case in which
only the opinions assigned to ?A? were regarded
as correct answers, and the case in which the opin-
ions assigned to ?A? or ?B? were regarded as cor-
rect answers. In either case, our method outper-
formed the control in ranking accuracy.
Although the accuracy of our method for ?A?
opinions was low, the accuracy for ?A? and ?B?
21
Table 4: Trade-off between recall and precision in extracting points at issue.
Threshold 0 0.002 0.004 0.006 0.008 0.010
Recall 0.48 0.17 0.11 0.04 0.03 0.02
Precision 0.04 0.14 0.21 0.31 0.33 0.50
Table 5: Accuracy for classifying points at issue.
Assessor A Assessor B
Topic ID #Points Accuracy (%) #Points Accuracy (%)
#1 113 98.2 45 97.7
#2 33 91.0 118 94.1
#3 21 95.2 26 100
#4 50 92.0 35 91.4
#5 27 96.3 14 100
Table 6: Accuracy of ranking opinions.
Answer Method N = 1 N = 2 N = 3
A Random 19% 28% 19%
Ours 38% 32% 23%
A+B Random 81% 83% 75%
Ours 87% 87% 83%
opinions was high. This suggests that our method
is effective in distinguishing opinions that argue
about a specific point and opinions that include the
point but do not argue about it.
6 Conclusion
In aiming to support users? decision making, we
have proposed a method for summarizing and vi-
sualizing the pro and con arguments about a topic.
Our prototype system, called ?OpinionReader?,
extracts points at issue from the opinions for both
pro and con standpoints, arranges the points in a
two-dimensional space, and allows users to read
important opinions on a point-by-point basis. We
have experimentally evaluated the effectiveness of
the components of our system.
Future work will include evaluating our system
as a whole, and summarizing opinions that change
over time.
References
Philip Beineke, Trevor Hastie, and Shivakumar
Vaithyanathan. 2004. The sentimental factor: Im-
proving review classification via human-provided
information. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 264?271.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In
Proceedings of the 12th International World Wide
Web Conference.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 1367?1373.
Lun-Wei Ku, Li-Ying Lee, Tung-Ho Wu, and Hsin-Hsi
Chen. 2005. Major topic detection and its appli-
cation to opinion summarization. In Proceedings of
the 28th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 627?628.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opin-
ions on the Web. In Proceedings of the 14th Interna-
tional World Wide Web Conference, pages 324?351.
Tomoyuki Nanno, Toshiaki Fujiki, Yasuhiro Suzuki,
and Manabu Okumura. 2004. Automatically col-
lecting, monitoring, and mining Japanese weblogs.
In The 13th International World Wide Web Confer-
ence, pages 320?321. (poster session).
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics, pages 264?271.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 115?124.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the 2003 Conference on Empirical Methods
in Natural Language Processing, pages 105?112.
22
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 242?249,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Modeling Impression in Probabilistic Transliteration into Chinese
LiLi Xu? Atsushi Fujii
Graduate School of Library,
Information and Media Studies
University of Tsukuba
1-2 Kasuga, Tsukuba, 305-8550, Japan
fujii@slis.tsukuba.ac.jp
Tetsuya Ishikawa
The Historiographical Institute
The University of Tokyo
3-1 Hongo 7-chome, Bunkyo-ku
Tokyo, 133-0033, Japan
ishikawa@hi.u-tokyo.ac.jp
Abstract
For transliterating foreign words into Chi-
nese, the pronunciation of a source word
is spelled out with Kanji characters. Be-
cause Kanji comprises ideograms, an indi-
vidual pronunciation may be represented
by more than one character. However,
because different Kanji characters convey
different meanings and impressions, char-
acters must be selected carefully. In this
paper, we propose a transliteration method
that models both pronunciation and im-
pression, whereas existing methods do not
model impression. Given a source word
and impression keywords related to the
source word, our method derives possible
transliteration candidates and sorts them
according to their probability. We evalu-
ate our method experimentally.
1 Introduction
Reflecting the rapid growth of science, technology,
and economies, new technical terms and product
names have progressively been created. These
new words have also been imported into different
languages. There are three fundamental methods
for importing foreign words into a language.
In the first method?translation?the meaning
of the source word in question is represented by
an existing or new word in the target language.
In the second method?transliteration?the
pronunciation of the source word is represented by
using the phonetic alphabet of the target language,
such as Katakana in Japanese and Hangul in Ko-
rean.
? This work was done when the first author was a grad-
uate student at University of Tsukuba, who currently works
for Hitachi Construction Machinery Co., Ltd.
In the third method, the source word is spelled
out as it is. However, the misuse of this method
decreases the understandability and readability of
the target language.
While translation is time-consuming, requiring
selection of an existing word or generation of a
new word that correctly represents the meaning of
the source word, transliteration can be performed
rapidly. However, the situation is complicated for
Chinese, where a phonetic alphabet is not used and
Kanji is used to spell out both conventional Chi-
nese words and foreign words.
Because Kanji comprises ideograms, an in-
dividual pronunciation can potentially be repre-
sented by more than one character. However, if
several Kanji strings are related to the same pro-
nunciation of the source word, their meanings will
be different and will therefore convey different im-
pressions.
For example, ?Coca-Cola? can be represented
by different Kanji strings in Chinese. The offi-
cial transliteration is ??????, which comprises
??? (tasty)? and ??? (pleasant)?, and is there-
fore associated with a positive connotation.
However, there are a number of Kanji strings
that represent similar pronunciations to that of
?Coca-Cola?, but which are associated with in-
appropriate impressions for a beverage, such as
??????. This word includes ????, which is
associated with choking.
Therefore, Kanji characters must be selected
carefully during transliteration into Chinese. This
is especially important when foreign companies
intend to introduce their names and products into
China.
In this paper, we propose a method that models
both impression and pronunciation for translitera-
tion into Chinese.
242
Section 2 surveys previous research into auto-
matic transliteration, in order to clarify the mean-
ing and contribution of our research. Section 3
elaborates on our transliteration method. Section 4
evaluates the effectiveness of our method.
2 Related Work
In a broad sense, the term ?transliteration? has
been used to refer to two tasks.
The first task is transliteration in the strict
sense, which creates new words in a target lan-
guage (Haizhou et al, 2004; Wan and Verspoor,
1998).
The second task is back-transliteration (Fujii
and Ishikawa, 2001; Jeong et al, 1999; Knight
and Graehl, 1998; Qu et al, 2003), which iden-
tifies the source word corresponding to an exist-
ing transliterated word. Back-transliteration is in-
tended mainly for cross-lingual information re-
trieval and machine translation.
Both transliteration tasks require methods that
model pronunciation in the source and target lan-
guages.
However, by definition, in back-transliteration,
the word in question has already been transliter-
ated and the meaning or impression of the source
word does not have to be considered. Thus, back-
transliteration is outside the scope of this paper.
In the following, we use the term ?translitera-
tion? to refer to transliteration in the strict sense.
Existing transliteration methods for Chi-
nese (Haizhou et al, 2004; Wan and Verspoor,
1998) aim to spell out foreign names of people
and places, and do not model impression.
However, as exemplified by ?Coca-Cola? in
Section 1, the impression of words needs to be
modeled in the transliteration of proper names,
such as companies and products. The contribu-
tion of our research is to incorporate a model of
impression into automatic transliteration.
3 Methodology
3.1 Overview
Figure 1 shows our transliteration method, which
models both pronunciation and impression when
transliterating foreign words into Chinese. We
will explain the entire process of our translitera-
tion method in terms of Figure 1.
The input for our method is twofold. First, a
source word to be transliterated into Chinese is re-
quested. Second, one or more words that describe
source word Impression keyword(s)
pronunciation model impression model
ranked list of transliteration candidates
ranking candidates
?? (safeguard)?? (another person)?? (live)?? (nutrition)
????(bitamin)
?, ?, ?, ?, ?, ??????????? ?
Transliteration candidates Kanji characters
Figure 1: Overview of our transliteration method
for Chinese.
the impression of the source word, which we call
?impression keywords?, are requested. Currently,
impression keywords must be provided manually
in Chinese. The output of our method is one or
more Kanji strings.
In an example scenario using our method, a user
has a good command of Chinese and intends to
introduce something (e.g., a company or product)
into China. It is reasonable to assume that this user
can provide one or more Chinese impression key-
words to associate with the target object.
Using the pronunciation model, the source word
is converted into a set of Kanji strings whose pro-
nunciation is similar to that of the source word.
Each of these Kanji strings is a transliteration can-
didate.
Currently, we use Japanese Katakana words as
source words, because Katakana words can easily
be converted into pronunciations using the Latin
alphabet. However, in principle, any language that
uses phonetic script can be a source language for
our method. In Figure 1, the Katakana word ?bita-
min (vitamin)? is used as an example source word.
Using the impression model, impression key-
words are converted into a set of Kanji characters.
A simple implementation is to segment each im-
pression keyword into characters.
However, because it is difficult for a user to pro-
vide an exhaustive list of appropriate keywords
and characters, our impression model derives char-
acters that are not included in the impression key-
words.
Because of the potentially large number of se-
lected candidates, we need to rank the candidates.
We model both pronunciation and impression in
243
a probabilistic framework, so that transliteration
candidates are sorted according to their probabil-
ity score.
Transliteration candidates that include many
characters derived from the impression model are
preferred. In other words, the Kanji characters
derived via the impression model are used to re-
rank the candidates derived via the pronunciation
model.
We elaborate on our probabilistic transliteration
model in Section 3.2. We then discuss the pronun-
ciation and impression models in Sections 3.3 and
3.4, respectively.
3.2 Probabilistic Transliteration Model
Given a romanized Japanese Katakana word R
and a set of impression keywords W , our pur-
pose is to select the Kanji string K that maxi-
mizes P (K|R,W ), which is evaluated as shown
in Equation (1), using Bayes? theorem.
P (K|R,W ) = P (R,W |K) ? P (K)P (R,W )
? P (R|K) ? P (W |K) ? P (K)P (R,W )
? P (R|K) ? P (W |K) ? P (K)
(1)
In the second line of Equation (1), we assume the
conditional independence of R and W given K.
In the third line, we omit P (R,W ), which is in-
dependent of K. This does not affect the rela-
tive rank of Kanji strings, when ranked in terms
of P (K|R,W ).
In Figure 1, R and W are ?bitamin? and
?????????????, respectively, and a K
candidate is ?????.
If a user intends to select more than one Kanji
string, those Ks associated with higher probabili-
ties should be selected.
As shown in Equation (1), P (K|R,W ) can
be approximated by the product of P (R|K),
P (W |K), and P (K). We call these three factors
the pronunciation, impression, and language mod-
els, respectively.
The language model, P (K), models the proba-
bility of K irrespective of R and W . In probabilis-
tic natural language processing, P (K) is usually
realized by a word or character N-gram model, and
therefore a K that appears frequently in a corpus
is assigned a high probability.
However, because our purpose is to generate
new words, the use of statistics obtained from ex-
isting corpora is not effective. Therefore, we con-
sider P (K) to be constant for every K.
In summary, P (K|R,W ) is approximated by a
product of P (R|K) and P (W |K). The quality of
our transliteration method will depend on the im-
plementation of the pronunciation and impression
models.
3.3 Pronunciation Model
The pronunciation model, P (R|K), models the
probability that a roman representation R is se-
lected, given a Kanji string K.
In Japanese, the Hepburn and Kunrei systems
are commonly used for romanization purposes.
We use the Hepburn system. We use Pinyin as
a representation for Kanji characters. We decom-
pose K into Kanji characters and associate K with
R on a character-by-character basis. We calculate
P (R|K) as shown in Equation (2).
P (R|K) ? P (R|Y ) ? P (Y |K)
?
N?
i=1
P (ri|yi) ?
N?
j=1
P (yj |kj)
(2)
Y denotes the Pinyin strings representing the pro-
nunciation of K. ki denotes a single Kanji char-
acter. ri and yi denote substrings of R and Y ,
respectively. R, Y , and K are decomposed into
the same number of elements, namely N . We cal-
culate P (ri|yi) and P (yi|ki) as shown in Equa-
tion (3).
P (ri|yi) = F (ri, yi)?
r
F (r, yi)
P (yi|ki) = F (yi, ki)?
y
F (y, ki)
(3)
F (x, y) denotes the co-occurrence frequency of x
and y. We need the co-occurrence frequencies of
ri and yi and the co-occurrence frequencies of yi
and ki in order to calculate P (R|K).
We used a bilingual dictionary comprising 1 140
Katakana words, most of which are technical
terms and proper nouns, and their transliterations
into Chinese, which are annotated with Pinyin. We
manually corresponded 151 pairs of Katakana and
roman characters on a mora-by-mora basis, and
romanized Katakana characters in the dictionary
automatically.
We obtained 1 140 tuples, of the form
< R, Y,K >. Because the number of tuples was
244
manageable, we obtained the element-by-element
R, Y , and K correspondences manually. Finally,
we calculated F (ri, yi) and F (yi, ki).
If there are many tuples, and the process of man-
ual correspondence is expensive, we can automate
the process as performed in existing transliteration
methods, such as the EM algorithm (Knight and
Graehl, 1998) or DP matching (Fujii and Ishikawa,
2001).
The above calculations are performed off-line.
In the online process, we consider all possible seg-
mentations of a single Katakana word. For exam-
ple, the romanized Katakana word ?bitamin (vi-
tamin)? corresponds to two Pinyin strings and is
segmented differently, as follows:
? bi-ta-min: wei-ta-ming,
? bi-ta-mi-n: wei-ta-mi-an.
3.4 Impression Model
The impression model, P (W |K), models the
probability that W is selected as a set of impres-
sion keywords, given Kanji string K. As in the
calculation of P (R|K) in Equation (2), we de-
compose W and K into elements, in calculating
P (W |K).
W is decomposed into a set of words, wi, and
K is decomposed into a set of Kanji characters, kj .
We calculate P (W |K) as a product of P (wi|kj),
which is the probability that wi is selected as an
impression keyword given kj .
However, unlike Equation (2), the numbers of
wi and kj derived from W and K are not always
the same, because users are allowed to provide an
arbitrary number of impression keywords. There-
fore, for each kj we select the wi that maximizes
P (wi|kj) and approximate P (W |K) as shown in
Equation (4).
P (W |K) ?
?
j
maxwi P (wi|kj) (4)
Figure 2 shows an example in which the four Chi-
nese words in the ?wi? column are also used in
Figure 1.
We calculate P (wi|kj) by Equation (5).
P (wi|kj) = F (wi, kj)?
w
F (w, kj)
(5)
As in Equation (3), F (x, y) denotes the co-
occurrence frequency of x and y.
0.6?????? ????0.1??
??0.40.3?? ????0.5??
???iw jk
3?? ?? ?? ??_??? 3??_?h3??_?h3??_? hh
Figure 2: Example calculation of P (W |K).
In summary, we need co-occurrences of each
word and character in Chinese.
These co-occurrences can potentially be col-
lected from existing language resources, such as
corpora in Chinese.
However, it is desirable to collect an association
between a word and a character, not simply their
co-occurrence in corpora. Therefore, we used
a dictionary of Kanji in Chinese, in which each
Kanji character entry is explained via sentences,
and often exemplified by one or more words that
include that character.
We selected 599 entry characters that are often
used to spell out foreign words. Then we collected
the frequencies with which each word is used to
explain each entry character.
Because Chinese sentences lack lexical seg-
mentation, we used SuperMorpho1 to perform a
morphological analysis of explanation sentences
and example words. As a result, 16 943 word types
were extracted. We used all of these words to cal-
culate the co-occurrence frequencies, irrespective
of the parts of speech.
Table 1 shows examples of Kanji characters,
Chinese words, and their co-occurrence frequen-
cies in the dictionary.
However, P (wi|kj) cannot be calculated for the
Kanji characters not modeled in our method (i.e.,
the Kanji characters not included in the 599 entry
characters). Thus, for smoothing purposes, we ex-
perimentally set P (wi|kj) at 0.001 for those kj not
modeled.
4 Experiments
4.1 Method
We evaluated our transliteration method experi-
mentally. Because the contribution of our research
is the incorporation of the impression model in a
transliteration method, we used a method that uses
only the pronunciation model as a control.
1http://www.omronsoft.com/
245
Table 1: Example of characters, words, and their
co-occurrence frequencies.
jk  iw  ),( ji kwF  jk  iw  ),( ji kwF  jk  iw  ),( ji kwF  
? ? 39 ? ? 3 ? ?? 2 
? ?? 8 ? ?? 2 ? ?? 1 
? ? 4 ? ? 43 ? ?? 5 
? ? 4 ? ?? 2 ? ?? 2 
? ?? 2 ? ?? 2 ? ? 51 
? ? 1 ? ?? 2 ? ? 5 
? ? 2 ? ?? 2 ? ? 3 
? ?? 2 ? ?? 4 ? ?? 11 
? ?? 2 ? ?? 2 ? ?? 2 
? ?? 3 ? ?? 1 ? ?? 7 
? ?? 1 ? ?? 2 ? ?? 5 
 
From a Japanese?Chinese dictionary, we se-
lected 210 Katakana words that had been translit-
erated into Chinese, and used these Katakana
words as test words. Each test word can be clas-
sified into one of the following five categories:
products, companies, places, persons, or general
words. Details of the categories of test inputs are
shown in Table 2.
Three Chinese graduate students who had a
good command of Japanese served as assessors
and produced reference data. None of the asses-
sors was an author of this paper. The assessors
performed the same task for the same test words
independently, in order to enhance the objectivity
of the results.
We produced the reference data via the follow-
ing procedure.
First, for each test word, each assessor pro-
vided one or more impression keywords in Chi-
nese. We did not restrict the number of impression
keywords per test word, which was determined by
each assessor.
If an assessor provided more than one impres-
sion keyword for a single test word, he/she was
requested to sort them in order of preference, so
that we could investigate the effect of the number
of impression keywords on the evaluation results,
by changing the number of top keywords used for
transliteration purposes.
We provided the assessors with the descriptions
for the test words from the source dictionary, so
that the assessors could understand the meaning
of each test word.
Second, for each test word, we applied the con-
trol method and our method independently, which
produced two lists of ranked transliteration candi-
dates. Because the impression keywords provided
by the assessors were used only in our method, the
Table 2: Categories of test words.
 
Example word 
Category # Words 
Japanese Chinese English 
Product 63 ???? ?? Audi 
Company 49 ???? ??? Epson 
Place 36 ???? ??? Ohio 
Person 21 ???? ?? Chopin 
General 41 ????? ??? angel 
ranked list produced by the control was the same
for all assessors.
Third, for each test word, each assessor identi-
fied one or more correct transliterations, according
to their impression of the test word. It was impor-
tant not to reveal to the assessors which method
produced which candidates.
By these means, we selected the top 100
transliteration candidates from the two ranked lists
for the control and our method. We merged these
candidates, removed duplications, and sorted the
remaining candidates by the character code.
As a result, the assessors judged the correctness
of up to 200 candidates for each test word. How-
ever, for some test words, assessors were not able
to find correct transliterations in the candidate list.
The resultant reference data was used to eval-
uate the accuracy of a test method in ranking
transliteration candidates. We used the average
rank of correct answers in the list as the evalua-
tion measure. If more than one correct answer was
found for a single test word, we first averaged the
ranks of these answers and then averaged the ranks
over the test words.
Although we used the top 100 candidates for
judgment purposes, the entire ranked list was used
to evaluate each method. Therefore, the average
rank of correct answers can potentially be over
100. The average number of candidates per test
word was 31 779.
Because our method uses the impression model
to re-rank the candidates produced by the pronun-
ciation model, the lists for the control and our
method comprise the same candidates. Therefore,
it is fair to compare these two methods by the av-
erage rank of the correct answers.
For each test word, there is more than one type
of ?correct answer?, as follows:
(a) transliteration candidates judged as correct
by the assessors independently (translitera-
246
tion candidates judged as correct by at least
one assessor);
(b) transliteration candidates judged as correct
by all assessors;
(c) transliterations defined in the source dictio-
nary.
In (a), the coverage of correct answers is the
largest, whereas the objectivity of the judgment is
the lowest.
In (c), the objectivity of the judgment is the
largest, whereas the coverage of correct answers
is the lowest. Although for each Katakana word
the source dictionary gives only one transliteration
that is commonly used, there are a number of ap-
propriate out-of-dictionary transliterations.
In (b), where the assessors did not disagree
about the correctness, the coverage of correctness
and the objectivity are both middle ranked.
Because none of the above answer types is per-
fect, we used all three types independently.
4.2 Results and Analyses
Tables 3?5 show the results of comparative exper-
iments using the answer types (a)?(c) above, re-
spectively.
In Tables 3?5, the column ?# of test words? de-
notes the number of test words for which at least
one correct answer exists. While the values in the
second column of Table 3 are different depending
on the assessor, in Tables 4 and 5 the values of the
second column are the same for all assessors.
The columns ?Avg. # of KW? and ?Avg. # of
answers? denote the number of impression key-
words and the number of correct answers per test
word, respectively. While the values in the fourth
column of Table 3 are different depending on the
assessor, in Tables 4 and 5 the values of the fourth
column are the same for all assessors.
In Tables 4 and 5, the average rank of correct an-
swers for the control is the same for all assessors.
However, the average rank of correct answers for
our method is different depending on the assessor,
because the impression keywords used depended
on the assessor.
The two columns in ?Avg. rank? denote the av-
erage ranks of correct answers for the control and
for our method, respectively. Looking at Tables 3?
5, it can be seen that our method outperformed the
control in ranking transliteration candidates, irre-
spective of the assessor and the answer type.
The average rank of correct answers for our
method in Table 5 was lower than those in Tables 3
and 4. One reason is that the correct answers in the
source dictionary are not always related to the im-
pression keywords provided by the assessors.
Table 6 presents the results in Table 3 on a
category-by-category basis. Because the results
were similar for answer types (b) and (c), we show
only the answer type (a) results, for the sake of
conciseness. Looking at Table 6, it can be seen
that our method outperformed the control in rank-
ing transliteration candidates, irrespective of the
category of test words.
Our method was effective for transliterating
names of places and people, although these types
of words are usually transliterated independently
of their impressions, compared with the names of
products and companies.
One reason is that, in the dictionary of Kanji
used to produce the impression model, the expla-
nation of an entry sometimes includes a phrase,
such as ?this character is often used for a person?s
name?. Assessors provided the word ?person? in
Chinese as an impression keyword for a number
of person names. As a result, transliteration can-
didates that included characters typically used for
a person?s name were highly ranked.
It may be argued that, because the impression
model was produced using Kanji characters that
are often used for transliteration purposes, the im-
pression model could possibly rank correct an-
swers better than the pronunciation model. How-
ever, the pronunciation model was also produced
from Kanji characters used for transliteration pur-
poses.
Figure 3 shows the distribution of correct an-
swers for different ranges of ranks, using answer
type (a). The number of correct answers in the top
10 for our method is approximately twice that of
the control. In addition, by our method, most of
the correct answers can be found in the top 100
candidates. Because the results were similar for
answer types (b) and (c), we show only the answer
type (a) results, for the sake of conciseness.
As explained in Section 4.1, for each test word,
the assessors were requested to sort the impression
keywords in order of preference. We analyzed the
relation between the number of impression key-
words used for the transliteration and the average
rank of correct answers, by varying the threshold
for the number of top impression keywords used.
247
Table 3: Results obtained with answer type (a).
Avg. rank
Assessor # of test words Avg. # of KW Avg. # of answers Control Our method
A 205 5.1 3.8 706 82
B 204 5.8 3.8 728 44
C 199 3.5 2.6 1 130 28
Avg. 203 4.8 3.4 855 51
Table 4: Results obtained with answer type (b).
Avg. rank
Assessor # of test words Avg. # of KW Avg. # of answers Control Our method
A 108 5.1 1.1 297 22
B 108 5.8 1.1 297 23
C 108 3.5 1.1 297 18
Avg. 108 4.8 1.1 297 21
Table 5: Results obtained with answer type (c).
Avg. rank
Assessor # of test words Avg. # of KW Avg. # of answers Control Our method
A 210 5.1 1 1 738 260
B 210 5.8 1 1 738 249
C 210 3.5 1 1 738 103
Avg. 210 4.8 1 1 738 204
Table 6: Results obtained with answer type (a) on a category-by-category basis.
Avg. rank
Category # of test words Avg. # of KW Avg. # of answers Control Our method
Product 144 4.8 3.5 1 527 64
Company 186 4.7 3.6 742 54
Place 102 4.8 3.7 777 46
Person 61 5.0 3.4 766 51
General 115 4.7 2.6 280 38
Avg. 122 4.8 3.4 818 51
?
???
???
???
???
???
???
???
???
???
???? ????
?
????
??
????
???
????
????
????
????
?
????
????
??
????
??
????
???
???
???
???
???
???
??
???????
??????????
Figure 3: Distribution of average rank for correct answers.
248
Table 7 shows the average rank of correct an-
swers for different numbers of impression key-
words, on an assessor-by-assessor basis. By com-
paring Tables 3 and 7, we see that even if a sin-
gle impression keyword was provided, the average
rank of correct answers was higher than that for
the control. In addition, the average rank of correct
answers was generally improved by increasing the
number of impression keywords.
Finally, we investigated changes in the rank of
correct answers caused by our method. Table 8
shows the results, in which ?Higher? and ?Lower?
denote the number of correct answers whose ranks
determined by our method were higher or lower,
respectively, than those determined by the control.
For approximately 30% of the correct answers,
our method decreased the control?s rank. Errors
were mainly caused by correct answers containing
Kanji characters that were not modeled in the im-
pression model. Although we used a smoothing
technique for characters not in the model, the re-
sult was not satisfactory. To resolve this problem,
the number of characters in the impression model
should be increased.
In summary, our method, which uses both the
impression and pronunciation models, ranked cor-
rect transliterations more highly than a method
that used only the pronunciation model. We con-
clude that the impression model is effective for
transliterating foreign words into Chinese. At the
same time, we concede that there is room for im-
provement in the impression model.
5 Conclusion
For transliterating foreign words into Chinese, the
pronunciation of a source word is spelled out with
Kanji characters. Because Kanji characters are
ideograms, a single pronunciation can be repre-
sented by more than one character. However, be-
cause different Kanji characters convey different
meanings and impressions, characters must be se-
lected carefully.
In this paper, we proposed a transliteration
method that models both pronunciation and im-
pression, compared to existing methods that do
not model impression. Given a source word and
impression keywords related to the source word,
our method derives possible transliteration candi-
dates, and sorts them according to their probabil-
ity. We showed the effectiveness of our method
experimentally.
Table 7: Relation between the number of impres-
sion keywords and average rank of correct answers
with answer type (a).
# of KW
Assessor 1 2 3
A 103 94 92
B 64 60 52
C 113 73 34
Table 8: Changes in ranks of correct answers
caused by our method.
Avg. rank
Answer type # of answers Higher Lower
(a) 2 070 1 431 639
(b) 360 250 110
(c) 630 422 208
Future work will include collecting impression
keywords automatically, and adapting the lan-
guage model to the category of source words.
References
Atsushi Fujii and Tetsuya Ishikawa. 2001.
Japanese/English cross-language information
retrieval: Exploration of query translation and
transliteration. Computers and the Humanities,
35(4):389?420.
Li Haizhou, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, pages
160?167.
Kil Soon Jeong, Sung Hyon Myaeng, Jae Sung Lee,
and Key-Sun Choi. 1999. Automatic identification
and back-transliteration of foreign words for infor-
mation retrieval. Information Processing & Man-
agement, 35:523?540.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Yan Qu, Gregory Grefenstette, and David A. Evans.
2003. Automatic transliteration for Japanese-to-
English text retrieval. In Proceedings of the 26th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 353?360.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and the 17th Inter-
national Conference on Computational Linguistics,
pages 1352?1356.
249
 	
  A Method for Open-Vocabulary Speech-Driven Text Retrieval
Atsushi Fujii  
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
fujii@ulis.ac.jp
Katunobu Itou
National Institute of
Advanced Industrial
Science and Technology
1-1-1 Chuuou Daini Umezono
Tsukuba, 305-8568, Japan
itou@ni.aist.go.jp
Tetsuya Ishikawa
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
ishikawa@ulis.ac.jp
Abstract
While recent retrieval techniques do not
limit the number of index terms, out-of-
vocabulary (OOV) words are crucial in
speech recognition. Aiming at retrieving
information with spoken queries, we fill
the gap between speech recognition and
text retrieval in terms of the vocabulary
size. Given a spoken query, we gener-
ate a transcription and detect OOV words
through speech recognition. We then cor-
respond detected OOV words to terms in-
dexed in a target collection to complete the
transcription, and search the collection for
documents relevant to the completed tran-
scription. We show the effectiveness of
our method by way of experiments.
1 Introduction
Automatic speech recognition, which decodes hu-
man voice to generate transcriptions, has of late
become a practical technology. It is feasible that
speech recognition is used in real-world human lan-
guage applications, such as information retrieval.
Initiated partially by TREC-6, various methods
have been proposed for ?spoken document retrieval
(SDR),? in which written queries are used to search
speech archives for relevant information (Garo-
folo et al, 1997). State-of-the-art SDR methods,
where speech recognition error rate is 20-30%, are

The first and second authors are also members of CREST,
Japan Science and Technology Corporation.
comparable with text retrieval methods in perfor-
mance (Jourlin et al, 2000), and thus are already
practical. Possible rationales include that recogni-
tion errors are overshadowed by a large number of
words correctly transcribed in target documents.
However, ?speech-driven retrieval,? where spo-
ken queries are used to retrieve (textual) informa-
tion, has not fully been explored, although it is re-
lated to numerous keyboard-less applications, such
as telephone-based retrieval, car navigation systems,
and user-friendly interfaces.
Unlike spoken document retrieval, speech-driven
retrieval is still a challenging task, because recogni-
tion errors in short queries considerably decrease re-
trieval accuracy. A number of references addressing
this issue can be found in past research literature.
Barnett et al (1997) and Crestani (2000) indepen-
dently performed comparative experiments related
to speech-driven retrieval, where the DRAGON
speech recognition system was used as an input in-
terface for the INQUERY text retrieval system. They
used as test queries 35 topics in the TREC col-
lection, dictated by a single male speaker. How-
ever, these cases focused on improving text retrieval
methods and did not address problems in improv-
ing speech recognition. As a result, errors in recog-
nizing spoken queries (error rate was approximately
30%) considerably decreased the retrieval accuracy.
Although we showed that the use of target docu-
ment collections in producing language models for
speech recognition significantly improved the per-
formance of speech-driven retrieval (Fujii et al,
2002; Itou et al, 2001), a number of issues still re-
main open questions.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 188-195.
                         Proceedings of the Conference on Empirical Methods in Natural
Section 2 clarifies problems addressed in this pa-
per. Section 3 overviews our speech-driven text
retrieval system. Sections 4-6 elaborate on our
methodology. Section 7 describes comparative ex-
periments, in which an existing IR test collection
was used to evaluate the effectiveness of our method.
Section 8 discusses related research literature.
2 Problem Statement
One major problem in speech-driven retrieval is re-
lated to out-of-vocabulary (OOV) words.
On the one hand, recent IR systems do not limit
the vocabulary size (i.e., the number of index terms),
and can be seen as open-vocabulary systems, which
allow users to input any keywords contained in a tar-
get collection. It is often the case that a couple of
million terms are indexed for a single IR system.
On the other hand, state-of-the-art speech recog-
nition systems still need to limit the vocabulary size
(i.e., the number of words in a dictionary), due
to problems in estimating statistical language mod-
els (Young, 1996) and constraints associated with
hardware, such as memories. In addition, compu-
tation time is crucial for a real-time usage, including
speech-driven retrieval. In view of these problems,
for many languages the vocabulary size is limited to
a couple of ten thousands (Itou et al, 1999; Paul and
Baker, 1992; Steeneken and van Leeuwen, 1995),
which is incomparably smaller than the size of in-
dexes for practical IR systems.
In addition, high-frequency words, such as func-
tional words and common nouns, are usually in-
cluded in dictionaries and recognized with a high
accuracy. However, those words are not necessarily
useful for retrieval. On the contrary, low-frequency
words appearing in specific documents are often ef-
fective query terms.
To sum up, the OOV problem is inherent in
speech-driven retrieval, and we need to fill the gap
between speech recognition and text retrieval in
terms of the vocabulary size. In this paper, we pro-
pose a method to resolve this problem aiming at
open-vocabulary speech-driven retrieval.
3 System Overview
Figure 1 depicts the overall design of our speech-
driven text retrieval system, which consists of
speech recognition, text retrieval and query com-
pletion modules. Although our system is cur-
rently implemented for Japanese, our methodology
is language-independent. We explain the retrieval
process based on this figure.
Given a query spoken by a user, the speech
recognition module uses a dictionary and acous-
tic/language models to generate a transcription of
the user speech. During this process, OOV words,
which are not listed in the dictionary, are also de-
tected. For this purpose, our language model in-
cludes both words and syllables so that OOV words
are transcribed as sequences of syllables.
For example, in the case where ?kankitsu (cit-
rus)? is not listed in the dictionary, this word
should be transcribed as /ka N ki tsu/. How-
ever, it is possible that this word is mistak-
enly transcribed, such as /ka N ke tsu/ and
/ka N ke tsu ke ko/.
To improve the quality of our system, these sylla-
ble sequences have to be transcribed as words, which
is one of the central issues in this paper. In the case
of speech-driven retrieval, where users usually have
specific information needs, it is feasible that users
utter contents related to a target collection. In other
words, there is a great possibility that detected OOV
words can be identified as index terms that are pho-
netically identical or similar.
However, since a) a single sound can potentially
correspond to more than one word (i.e., homonyms)
and b) searching the entire collection for phoneti-
cally identical/similar terms is prohibitive, we need
an efficient disambiguation method. Specifically, in
the case of Japanese, the homonym problem is mul-
tiply crucial because words consist of different char-
acter types, i.e., ?kanji,? ?katakana,? ?hiragana,? al-
phabets and other characters like numerals1.
To resolve this problem, we use a two-stage re-
trieval method. In the first stage, we delete OOV
words from the transcription, and perform text re-
trieval using remaining words, to obtain a specific
number of top-ranked documents according to the
degree of relevance. Even if speech recognition is
not perfect, these documents are potentially associ-
ated with the user speech more than the entire col-
1In Japanese, kanji (or Chinese character) is the idiogram,
and katakana and hiragana are phonograms.
lection. Thus, we search only these documents for
index terms corresponding to detected OOV words.
Then, in the second stage, we replace detected
OOV words with identified index terms so as to
complete the transcription, and re-perform text re-
trieval to obtain final outputs. However, we do not
re-perform speech recognition in the second stage.
In the above example, let us assume that the user
also utters words related to ?kankitsu (citrus),? such
as ?orenji (orange)? and ?remon (lemon),? and that
these words are correctly recognized as words. In
this case, it is possible that retrieved documents
contain the word ?kankitsu (citrus).? Thus, we re-
place the syllable sequence /ka N ke tsu/ in the
query with ?kankitsu,? which is additionally used as
a query term in the second stage.
It may be argued that our method resembles the
notion of pseudo-relevance feedback (or local feed-
back) for IR, where documents obtained in the first
stage are used to expand query terms, and final out-
puts are refined in the second stage (Kwok and Chan,
1998). However, while relevance feedback is used to
improve only the retrieval accuracy, our method im-
proves the speech recognition and retrieval accuracy.
Dictionary
Text retrieval Collection
Acoustic
model
Language
model
Speech recognition
user speech
transcription
top-ranked documents
Query completion
completed
transcription
Figure 1: The overall design of our speech-driven
text retrieval system.
4 Speech Recognition
The speech recognition module generates word se-
quence

, given phone sequence  . In a stochastic
speech recognition framework (Bahl et al, 1983),
the task is to select the

maximizing 


	 ,
which is transformed as in Equation (1) through the
Bayesian theorem.





	



 
	ffA Probabilistic Method for Analyzing Japanese Anaphora
Integrating Zero Pronoun Detection and Resolution
Kazuhiro Seki ?, Atsushi Fujii ??, ??? and Tetsuya Ishikawa ??
?National Institute of Advanced Industrial Science and Technology
1-1-1, Chuuou Daini Umezono, Tsukuba 305-8568, Japan
??University of Library and Information Science
1-2, Kasuga, Tsukuba, 305-8550, Japan
???CREST, Japan Science & Technology Corporation
k.seki@aist.go.jp fujii@ulis.ac.jp ishikawa@ulis.ac.jp
Abstract
This paper proposes a method to analyze
Japanese anaphora, in which zero pronouns
(omitted obligatory cases) are used to refer to
preceding entities (antecedents). Unlike the
case of general coreference resolution, zero pro-
nouns have to be detected prior to resolution
because they are not expressed in discourse.
Our method integrates two probability param-
eters to perform zero pronoun detection and
resolution in a single framework. The first pa-
rameter quantifies the degree to which a given
case is a zero pronoun. The second parame-
ter quantifies the degree to which a given entity
is the antecedent for a detected zero pronoun.
To compute these parameters efficiently, we use
corpora with/without annotations of anaphoric
relations. We show the effectiveness of our
method by way of experiments.
1 Introduction
Anaphora resolution is crucial in natural lan-
guage processing (NLP), specifically, discourse
analysis. In the case of English, partially mo-
tivated by Message Understanding Conferences
(MUCs) (Grishman and Sundheim, 1996), a
number of coreference resolution methods have
been proposed.
In other languages such as Japanese and
Spanish, anaphoric expressions are often omit-
ted. Ellipses related to obligatory cases are usu-
ally termed zero pronouns. Since zero pronouns
are not expressed in discourse, they have to be
detected prior to identifying their antecedents.
Thus, although in English pleonastic pronouns
have to be determined whether or not they are
anaphoric expressions prior to resolution, the
process of analyzing Japanese zero pronouns is
different from general coreference resolution in
English.
For identifying anaphoric relations, existing
methods are classified into two fundamental ap-
proaches: rule-based and statistical approaches.
In rule-based approaches (Grosz et al, 1995;
Hobbs, 1978; Mitkov et al, 1998; Nakaiwa
and Shirai, 1996; Okumura and Tamura, 1996;
Palomar et al, 2001; Walker et al, 1994),
anaphoric relations between anaphors and their
antecedents are identified by way of hand-
crafted rules, which typically rely on syntactic
structures, gender/number agreement, and se-
lectional restrictions. However, it is difficult to
produce rules exhaustively, and rules that are
developed for a specific language are not neces-
sarily effective for other languages. For exam-
ple, gender/number agreement in English can-
not be applied to Japanese.
Statistical approaches (Aone and Bennett,
1995; Ge et al, 1998; Kim and Ehara,
1995; Soon et al, 2001) use statistical mod-
els produced based on corpora annotated with
anaphoric relations. However, only a few
attempts have been made in corpus-based
anaphora resolution for Japanese zero pro-
nouns. One of the reasons is that it is costly
to produce a sufficient volume of training cor-
pora annotated with anaphoric relations.
In addition, those above methods focused
mainly on identifying antecedents, and few at-
tempts have been made to detect zero pronouns.
Motivated by the above background, we
propose a probabilistic model for analyzing
Japanese zero pronouns combined with a detec-
tion method. In brief, our model consists of two
parameters associated with zero pronoun detec-
tion and antecedent identification. We focus on
zero pronouns whose antecedents exist in pre-
ceding sentences to zero pronouns because they
are major referential expressions in Japanese.
Section 2 explains our proposed method (sys-
tem) for analyzing Japanese zero pronouns.
Section 3 evaluates our method by way of ex-
periments using newspaper articles. Section 4
discusses related research literature.
2 A System for Analyzing Japanese
Zero Pronouns
2.1 Overview
Figure 1 depicts the overall design of our system
to analyze Japanese zero pronouns. We explain
the entire process based on this figure.
First, given an input Japanese text, our sys-
tem performs morphological and syntactic anal-
yses. In the case of Japanese, morphological
analysis involves word segmentation and part-
of-speech tagging because Japanese sentences
lack lexical segmentation, for which we use
the JUMAN morphological analyzer (Kurohashi
and Nagao, 1998b). Then, we use the KNP
parser (Kurohashi, 1998) to identify syntactic
relations between segmented words.
Second, in a zero pronoun detection phase,
the system uses syntactic relations to detect
omitted cases (nominative, accusative, and da-
tive) as zero pronoun candidates. To avoid zero
pronouns overdetected, we use the IPAL verb
dictionary (Information-technology Promotion
Agency, 1987) including case frames associated
with 911 Japanese verbs. We discard zero pro-
noun candidates unlisted in the case frames as-
sociated with a verb in question.
For verbs unlisted in the IPAL dictionary,
only nominative cases are regarded as obliga-
tory. The system also computes a probability
that case c related to target verb v is a zero
pronoun, P
zero
(c|v), to select plausible zero pro-
noun candidates.
Ideally, in the case where a verb in ques-
tion is polysemous, word sense disambiguation
is needed to select the appropriate case frame,
because different verb senses often correspond
to different case frames. However, we currently
merge multiple case frames for a verb into a sin-
gle frame so as to avoid the polysemous prob-
lem. This issue needs to be further explored.
Third, in a zero pronoun resolution (i.e., an-
tecedent identification) phase, for each zero pro-
noun the system extracts antecedent candidates
from the preceding contexts, which are ordered
according to the extent to which they can be the
antecedent for the target zero pronoun. From
input text
morphological and
sytactic analyses
output text
case frame
dictionary
annotated
corpora
unannotated
corpora
semantic
model
syntactic
model
zero pronoun
detection
zero pronoun
resolution
Figure 1: The overall design of our system to
analyze Japanese zero pronouns.
the viewpoint of probability theory, our task
here is to compute a probability that zero pro-
noun ? refers to antecedent a
i
, P (a
i
|?), and se-
lect the candidate that maximizes the probabil-
ity score. For the purpose of computing this
score, we model zero pronouns and antecedents
in Section 2.2.
Finally, the system outputs texts containing
anaphoric relations. In addition, the number
of zero pronouns analyzed by the system can
optionally be controlled based on the certainty
score described in Section 2.4.
2.2 Modeling Zero Pronouns and
Antecedents
According to past literature associated with
zero pronoun resolution and our preliminary
study, we use the following six features to model
zero pronouns and antecedents.
? Features for zero pronouns
? Verbs that govern zero pronouns (v), which
denote verbs whose cases are omitted.
? Surface cases related to zero pronouns (c),
for which possible values are Japanese case
marker suffixes, ga (nominative), wo (ac-
cusative), and ni (dative). Those values
indicate which cases are omitted.
? Features for antecedents
? Post-positional particles (p), which play
crucial roles in resolving Japanese zero pro-
nouns (Kameyama, 1986; Walker et al,
1994).
? Distance (d), which denotes the distance
(proximity) between a zero pronoun and an
antecedent candidate in an input text. In
the case where they occur in the same sen-
tence, its value takes 0. In the case where
an antecedent occurs in n sentences previ-
ous to the sentence including a zero pro-
noun, its value takes n.
? Constraint related to relative clauses (r),
which denotes whether an antecedent is in-
cluded in a relative clause or not. In the
case where it is included, the value of r
takes true, otherwise false. The rationale
behind this feature is that Japanese zero
pronouns tend not to refer to noun phrases
in relative clauses.
? Semantic classes (n), which represent se-
mantic classes associated with antecedents.
We use 544 semantic classes defined in the
Japanese Bunruigoihyou thesaurus (Na-
tional Language Research Institute, 1964),
which contains 55,443 Japanese nouns.
2.3 Our Probabilistic Model for Zero
Pronoun Detection and Resolution
We consider probabilities that unsatisfied case
c related to verb v is a zero pronoun, P
zero
(c|v),
and that zero pronoun ?
c
refers to antecedent
a
i
, P (a
i
|?
c
). Thus, a probability that case c (?
c
)
is zero-pronominalized and refers to candidate
a
i
is formalized as in Equation (1).
P (a
i
|?
c
) ? P
zero
(c|v) (1)
Here, P
zero
(c|v) and P (a
i
|?
c
) are computed in
the detection and resolution phases, respec-
tively (see Figure 1).
Since zero pronouns are omitted obligatory
cases, whether or not case c is a zero pronoun
depends on the extent to which case c is oblig-
atory for verb v. Case c is likely to be oblig-
atory for verb v if c frequently co-occurs with
v. Thus, we compute P
zero
(c|v) based on the
co-occurrence frequency of ?v, c? pairs, which
can be extracted from unannotated corpora.
P
zero
(c|v) takes 1 in the case where c is ga (nom-
inative) regardless of the target verb, because ga
is obligatory for most Japanese verbs.
Given the formal representation for zero pro-
nouns and antecedents in Section 2.2, the prob-
ability, P (a|?), is expressed as in Equation (2).
P (a
i
|?) = P (p
i
, d
i
, r
i
, n
i
|v, c) (2)
To improve the efficiency of probability estima-
tion, we decompose the right-hand side of Equa-
tion (2) as follows.
Since a preliminary study showed that d
i
and
r
i
were relatively independent of the other fea-
tures, we approximate Equation (2) as in Equa-
tion (3).
P (a
i
|?) ? P (p
i
, n
i
|v, c) ? P (d
i
) ? P (r
i
)
= P (p
i
|n
i
, v, c) ? P (n
i
|v, c)
? P (d
i
) ? P (r
i
)
(3)
Given that p
i
is independent of v and n
i
, we
can further approximate Equation (3) to derive
Equation (4).
P (a
i
|?
c
) ? P (p
i
|c)?P (d
i
)?P (r
i
)?P (n
i
|v, c) (4)
Here, the first three factors, P (p
i
|c) ? P (d
i
) ?
P (r
i
), are related to syntactic properties, and
P (n
i
|v, c) is a semantic property associated with
zero pronouns and antecedents. We shall call
the former and latter ?syntactic? and ?seman-
tic? models, respectively.
Each parameter in Equation (4) is com-
puted as in Equations (5), where F (x) denotes
the frequency of x in corpora annotated with
anaphoric relations.
P (p
i
|c) =
F (p
i
, c)
?
j
F (p
j
, c)
P (d
i
) =
F (d
i
)
?
j
F (d
j
)
P (r
i
) =
F (r
i
)
?
j
F (r
j
)
P (n
i
|v, c) =
F (n
i
, v, c)
?
j
F (n
j
, v, c)
(5)
However, since estimating a semantic model,
P (n
i
|v, c), needs large-scale annotated corpora,
the data sparseness problem is crucial. Thus,
we explore the use of unannotated corpora.
For P (n
i
|v, c), v and c are features for a zero
pronoun, and n
i
is a feature for an antecedent.
However, we can regard v, c, and n
i
as features
for a verb and its case noun because zero pro-
nouns are omitted case nouns. Thus, it is pos-
sible to estimate the probability based on co-
occurrences of verbs and their case nouns, which
can be extracted automatically from large-scale
unannotated corpora.
2.4 Computing Certainty Score
Since zero pronoun analysis is not a stand-alone
application, our system is used as a module in
other NLP applications, such as machine trans-
lation. In those applications, it is desirable that
erroneous anaphoric relations are not generated.
Thus, we propose a notion of certainty to out-
put only zero pronouns that are detected and
resolved with a high certainty score.
We formalize the certainty score, C(?
c
), for
each zero pronoun as in Equation (6), where
P
1
(?
c
) and P
2
(?
c
) denote probabilities com-
puted by Equation (1) for the first and second
ranked candidates, respectively. In addition, t is
a parametric constant, which is experimentally
set to 0.5.
C(?
c
) = t?P
1
(?
c
) + (1?t)(P
1
(?
c
)?P
2
(?
c
)) (6)
The certainty score becomes great in the case
where P
1
(?
c
) is sufficiently great and signifi-
cantly greater than P
2
(?
c
).
3 Evaluation
3.1 Methodology
To investigate the performance of our system,
we used Kyotodaigaku Text Corpus version
2.0 (Kurohashi and Nagao, 1998a), in which
20,000 articles in Mainichi Shimbun newspaper
articles in 1995 were analyzed by JUMAN and
KNP (i.e., the morph/syntax analyzers used in
our system) and revised manually. From this
corpus, we randomly selected 30 general articles
(e.g., politics and sports) and manually anno-
tated those articles with anaphoric relations for
zero pronouns. The number of zero pronouns
contained in those articles was 449.
We used a leave-one-out cross-validation eval-
uation method: we conducted 30 trials in each
of which one article was used as a test input
and the remaining 29 articles were used for pro-
ducing a syntactic model. We used six years
worth of Mainichi Shimbun newspaper arti-
cles (Mainichi Shimbunsha, 1994?1999) to pro-
duce a semantic model based on co-occurrences
of verbs and their case nouns.
To extract verbs and their case noun pairs
from newspaper articles, we performed a mor-
phological analysis by JUMAN and extracted
dependency relations using a relatively simple
rule: we assumed that each noun modifies the
verb of highest proximity. As a result, we
obtained 12 million co-occurrences associated
with 6,194 verb types. Then, we generalized
the extracted nouns into semantic classes in
the Japanese Bunruigoihyou thesaurus. In the
case where a noun was associated with multiple
classes, the noun was assigned to all possible
classes. In the case where a noun was not listed
in the thesaurus, the noun itself was regarded
as a single semantic class.
3.2 Comparative Experiments
Fundamentally, our evaluation is two-fold: we
evaluated only zero pronoun resolution (an-
tecedent identification) and a combination of
detection and resolution. In the former case,
we assumed that all the zero pronouns are cor-
rectly detected, and investigated the effective-
ness of the resolution model, P (a
i
|?). In the
latter case, we investigated the effectiveness of
the combined model, P (a
i
|?
c
) ? P
zero
(c|v).
First, we compared the performance of the
following different models for zero pronoun res-
olution, P (a
i
|?):
? a semantic model produced based on anno-
tated corpora (Sem1),
? a semantic model produced based on unan-
notated corpora, using co-occurrences of
verbs and their case nouns (Sem2),
? a syntactic model (Syn),
? a combination of Syn and Sem1 (Both1),
? a combination of Syn and Sem2 (Both2),
which is our complete model for zero pro-
noun resolution,
? a rule-based model (Rule).
As a control (baseline) model, we took approxi-
mately two man-months to develop a rule-based
model (Rule) through an analysis on ten articles
in Kyotodaigaku Text Corpus. This model uses
rules typically used in existing rule-based meth-
ods: 1) post-positional particles that follow an-
tecedent candidates, 2) proximity between zero
pronouns and antecedent candidates, and 3)
conjunctive particles. We did not use seman-
tic properties in the rule-based method because
they decreased the system accuracy in a prelim-
inary study.
Table 1: Experimental results for zero pronoun resolution.
# of Correct cases (Accuracy)
k Sem1 Sem2 Syn Both1 Both2 Rule
1 25 (6.2%) 119 (29.5%) 185 (45.8%) 30 (7.4%) 205 (50.7%) 162 (40.1%)
2 46 (11.4%) 193 (47.8%) 227 (56.2%) 49 (12.1%) 250 (61.9%) 213 (52.7%)
3 72 (17.8%) 230 (56.9%) 262 (64.9%) 75 (18.6%) 280 (69.3%) 237 (58.6%)
Table 1 shows the results, where we regarded
the k-best antecedent candidates as the final
output and compared results for different values
of k. In the case where the correct answer was
included in the k-best candidates, we judged it
correct. In addition, ?Accuracy? is the ratio be-
tween the number of zero pronouns whose an-
tecedents were correctly identified and the num-
ber of zero pronouns correctly detected by the
system (404 for all the models). Bold figures
denote the highest performance for each value
of k across different models. Here, the average
number of antecedent candidates per zero pro-
noun was 27 regardless of the model, and thus
the accuracy was 3.7% in the case where the
system randomly selected antecedents.
Looking at the results for two different seman-
tic models, Sem2 outperformed Sem1, which
indicates that the use of co-occurrences of verbs
and their case nouns was effective to identify
antecedents and avoid the data sparseness prob-
lem in producing a semantic model.
The syntactic model, Syn, outperformed the
two semantic models independently, and there-
fore the syntactic features used in our model
were more effective than the semantic features
to identify antecedents. When both syntactic
and semantic models were used in Both2, the
accuracy was further improved. While the rule-
based method, Rule, achieved a relatively high
accuracy, our complete model, Both2, outper-
formed Rule irrespective of the value of k. To
sum up, we conclude that both syntactic and
semantic models were effective to identify ap-
propriate anaphoric relations.
At the same time, since our method requires
annotated corpora, the relation between the
corpus size and accuracy is crucial. Thus, we
performed two additional experiments associ-
ated with Both2.
In the first experiment, we varied the number
of annotated articles used to produce a syntactic
model, where a semantic model was produced
25
30
35
40
45
50
55
0 5 10 15 20 25
0 1 2 3 4 5 6
a
cc
u
ra
cy
 (%
)
annotated corpus size for producing a syntactic model (#articles)
unannotated corpus size for producing a semantic model (year)
unannotated
annotated
Figure 2: The relation between the corpus size
and accuracy for a combination of syntactic and
semantic models (Both2).
based on six years worth of newspaper articles.
In the second experiment, we varied the num-
ber of unannotated articles used to produce a
semantic model, where a syntactic model was
produced based on 29 annotated articles. In
Figure 2, we show two independent results as
space is limited: the dashed and solid graphs
correspond to the results of the first and second
experiments, respectively. Given all the articles
for modeling, the resultant accuracy for each ex-
periment was 50.7%, which corresponds to that
for Both2 with k = 1 in Table 1.
In the case where the number of articles was
varied in producing a syntactic model, the ac-
curacy improved rapidly in the first five arti-
cles. This indicates that a high accuracy can
be obtained by a relatively small number of su-
pervised articles. In the case where the amount
of unannotated corpora was varied in produc-
ing a semantic model, the accuracy marginally
improved as the corpus size increases. However,
note that we do not need human supervision to
produce a semantic model.
Finally, we evaluated the effectiveness of the
25
30
35
40
45
50
55
60
65
70
10 20 30 40 50 60 70 80 90
a
cc
u
ra
cy
 (%
)
coverage (%)
P(ai|?c)?Pzero(c|v)
P(ai|?c)
Figure 3: The relation between coverage and
accuracy for zero pronoun detection (Both2).
50
55
60
65
70
75
80
0 10 20 30 40 50 60 70 80 90 100
a
cc
u
ra
cy
 (%
)
coverage (%)
P(ai|?c)?Pzero(c|v)
P(ai|?c)
Figure 4: The relation between coverage and
accuracy for antecedent identification (Both2).
combination of zero pronoun detection and res-
olution in Equation (1). To investigate the con-
tribution of the detection model, P
zero
(c|v), we
used P (a
i
|?
c
) for comparison. Both cases used
Both2 to compute the probability for zero pro-
noun resolution. We varied a threshold for the
certainty score to plot coverage-accuracy graphs
for zero pronoun detection (Figure 3) and an-
tecedent identification (Figure 4).
In Figure 3, ?coverage? is the ratio between
the number of zero pronouns correctly detected
by the system and the total number of zero pro-
nouns in input texts, and ?accuracy? is the ratio
between the number of zero pronouns correctly
detected and the total number of zero pronouns
detected by the system. Note that since our sys-
tem failed to detect a number of zero pronouns,
the coverage could not be 100%.
Figure 3 shows that as the coverage decreases,
the accuracy improved irrespective of the model
used. When compared with the case of P (a
i
|?),
our model, P (a
i
|?)?P
zero
(c|v), achieved a higher
accuracy regardless of the coverage.
In Figure 4, ?coverage? is the ratio between
the number of zero pronouns whose antecedents
were generated and the number of zero pro-
nouns correctly detected by the system. The
accuracy was improved by decreasing the cov-
erage, and our model marginally improved the
accuracy for P (a
i
|?).
According to those above results, our model
was effective to improve the accuracy for zero
pronoun detection and did not have side effect
on the antecedent identification process. As a
result, the overall accuracy of zero pronoun de-
tection and resolution was improved.
4 Related Work
Kim and Ehara (1995) proposed a probabilis-
tic model to resolve subjective zero pronouns
for the purpose of Japanese/English machine
translation. In their model, the search scope
for possible antecedents was limited to the sen-
tence containing zero pronouns. In contrast,
our method can resolve zero pronouns in both
intra/inter-sentential anaphora types.
Aone and Bennett (1995) used a decision tree
to determine appropriate antecedents for zero
pronouns. They focused on proper and definite
nouns used in anaphoric expressions as well as
zero pronouns. However, their method resolves
only anaphors that refer to organization names
(e.g., private companies), which are generally
easier to resolve than our case.
Both above existing methods require anno-
tated corpora for statistical modeling, while we
used corpora with/without annotations related
to anaphoric relations, and thus we can eas-
ily obtain large-scale corpora to avoid the data
sparseness problem.
Nakaiwa (2000) used Japanese/English bilin-
gual corpora to identify anaphoric relations of
Japanese zero pronouns by comparing J/E sen-
tence pairs. The rationale behind this method
is that obligatory cases zero-pronominalized
in Japanese are usually expressed in English.
However, in the case where corresponding En-
glish expressions are pronouns and anaphors,
their method is not effective. Additionally,
bilingual corpora are more expensive to obtain
than monolingual corpora used in our method.
Finally, our method integrates a parameter
for zero pronoun detection in computing the cer-
tainty score. Thus, we can improve the accuracy
of our system by discarding extraneous outputs
with a small certainty score.
5 Conclusion
We proposed a probabilistic model to ana-
lyze Japanese zero pronouns that refer to an-
tecedents in the previous context. Our model
consists of two probabilistic parameters corre-
sponding to detecting zero pronouns and iden-
tifying their antecedents, respectively. The lat-
ter is decomposed into syntactic and semantic
properties. To estimate those parameters ef-
ficiently, we used annotated/unannotated cor-
pora. In addition, we formalized the certainty
score to improve the accuracy. Through exper-
iments, we showed that the use of unannotated
corpora was effective to avoid the data sparse-
ness problem and that the certainty score fur-
ther improved the accuracy.
Future work would include word sense disam-
biguation for polysemous predicate verbs to se-
lect appropriate case frames in the zero pronoun
detection process.
References
Chinatsu Aone and Scott William Bennett. 1995.
Evaluating automated and manual acquisition of
anaphora resolution strategies. In Proceedings of
33th Annual Meeting of the Association for Com-
putational Linguistics, pages 122?129.
Niyu Ge, John Hale, and Eugene Charniak. 1998.
A statistical approach to anaphora resolution. In
Proceedings of the Sixth Workshop on Very Large
Corpora, pages 161?170.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage Understanding Conference - 6: A brief his-
tory. In Proceedings of the 16th International
Conference on Computational Linguistics, pages
466?471.
Barbara J. Grosz, Aravind K. Joshi, and Scott We-
instein. 1995. Centering: A framework for mod-
eling the local coherence of discourse. Computa-
tional Linguistics, 21(2):203?226.
Jerry R. Hobbs. 1978. Resolving pronoun refer-
ences. Lingua, 44:311?338.
Information-technology Promotion Agency, 1987.
IPA Lexicon of the Japanese language for com-
puters (Basic Verbs). (in Japanese).
Megumi Kameyama. 1986. A property-sharing con-
straint in centering. In Proceedings of the 24th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 200?206.
Yeun-Bae Kim and Terumasa Ehara. 1995. Zero-
subject resolution method based on probabilistic
inference with evaluation function. In Proceedings
of the 3rd Natural Language Processing Pacific-
Rim Symposium, pages 721?727.
Sadao Kurohashi and Makoto Nagao. 1998a. Build-
ing a Japanese parsed corpus while improving the
parsing system. In Proceedings of The 1st In-
ternational Conference on Language Resources &
Evaluation, pages 719?724.
Sadao Kurohashi and Makoto Nagao, 1998b.
Japanese morphological analysis system JUMAN
version 3.6 manual. Department of Informatics,
Kyoto University. (in Japanese).
Sadao Kurohashi, 1998. Japanese Dependency/Case
Structure Analyzer KNP version 2.0b6. De-
partment of Informatics, Kyoto University. (in
Japanese).
Mainichi Shimbunsha. 1994?1999. Mainichi Shim-
bun CD-ROM.
Ruslan Mitkov, Lamia Belguith, and Malgorzata
Stys. 1998. Multilingual robust anaphora reso-
lution. In Proceedings of the 3rd Conference on
Empirical Methods in Natural Language Process-
ing, pages 7?16.
Hiromi Nakaiwa and Satoshi Shirai. 1996.
Anaphora resolution of Japanese zero pronouns
with deictic reference. In Proceedings of the
16th International Conference on Computational
Linguistics, pages 812?817.
Hiromi Nakaiwa. 2000. An environment for extract-
ing resolution rules of zero pronouns from corpora.
In COLING-2000 Workshop on Semantic Anno-
tation and Intelligent Content, pages 44?52.
National Language Research Institute. 1964. Bun-
ruigoihyou. Shuei publisher. (in Japanese).
Manabu Okumura and Kouji Tamura. 1996. Zero
pronoun resolution in Japanese discourse based
on centering theory. In Proceedings of the 16th
International Conference on Computational Lin-
guistics, pages 871?876.
Manuel Palomar, Antonio Ferra?ndez, Lidia Moreno,
Patricio Mart??nez-Barco, Jesu?s Peral, Maximil-
iano Saiz-Noeda, and Rafael Mu noz. 2001. An al-
gorithm for anaphora resolution in Spanish texts.
Computational Linguistics, 27(4):545?568.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Marilyn Walker, Masayo Iida, and Sharon Cote.
1994. Japanese discourse and the process of cen-
tering. Computational Linguistics, 20(2):193?233.
Question Answering Using Encyclopedic Knowledge
Generated from the Web
Atsushi Fujii
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
CREST, Japan Science and
Technology Corporation
fujii@ulis.ac.jp
Tetsuya Ishikawa
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
ishikawa@ulis.ac.jp
Abstract
We propose a question answering sys-
tem which uses an encyclopedia as a
knowledge base. However, since ex-
isting encyclopedias lack technical/new
terms, we use an encyclopedia automat-
ically generated from the World Wide
Web. For this purpose, we first search
the Web for pages containing a term
in question. Then linguistic patterns
and HTML structures are used to ex-
tract text fragments describing the term.
Finally, extracted term descriptions are
organized based on word senses and
domains. We also evaluate our sys-
tem by way of experiments, where the
Japanese Information-Technology En-
gineers Examination is used as a test
collection.
1 Introduction
Motivated partially by the TREC-8 QA collec-
tion (Voorhees and Tice, 2000), question answer-
ing has of late become one of the major topics
within the natural language processing and infor-
mation retrieval communities, and a number of
QA systems targeting the TREC collection have
been proposed (Harabagiu et al, 2000; Moldovan
and Harabagiu, 2000; Prager et al, 2000).
Although Harabagiu et al (2000) proposed a
knowledge-based QA system, most existing sys-
tems rely on conventional IR and shallow NLP
methods. However, question answering is inher-
ently a more complicated procedure that usually
requires explicit knowledge bases.
In this paper, we propose a question answering
system which uses an encyclopedia as a knowl-
edge base. However, since existing (published)
encyclopedias usually lack technical/new terms,
we generate one based on the World Wide Web,
which includes a number of technical and recent
information. For this purpose, we use a modified
version of our method to extract term descriptions
from Web pages (Fujii and Ishikawa, 2000).
Intuitively, our system answers interrogative
questions like ?What is X?? in which a QA sys-
tem searches an encyclopedia database for one or
more descriptions related to term X.
The performance of QA systems can be evalu-
ated based on coverage and accuracy. Coverage
is the ratio between the number of questions an-
swered (disregarding their correctness) and the to-
tal number of questions. Accuracy is the ratio be-
tween the number of correct answers and the total
number of answers made by the system. While
coverage can be estimated objectively and sys-
tematically, estimating accuracy relies on human
subjects (because it is difficult to define the abso-
lute description for term X), and thus is expensive.
In view of this problem, we use as a test col-
lection Information Technology Engineers Exam-
inations1, which are biannual examinations nec-
essary for candidates to qualify to be IT engineers
in Japan.
Among a number of classes, we focus on the
?Class II? examination, which requires funda-
1Japan Information-Technology Engineers Examination
Center. http://www.jitec.jipdec.or.jp/
mental and general knowledge related to informa-
tion technology. Approximately half of questions
are associated with IT technical terms. Since past
examinations and answers are open to the pub-
lic, we can objectively evaluate the performance
of our QA system with minimal cost.
Our system is not categorized into ?open-
domain? systems, where questions expressed in
natural language are not limited to explicit axes
including who, what, when, where, how and why.
However, Moldovan and Harabagiu (2000)
found that each of the TREC questions can be re-
cast as either a single axis or a combination of
axes. They also found that out of the 200 TREC
questions, 64 questions (approximately one third)
were associated with the what axis, for which
our encyclopedia-based system is expected to im-
prove the quality of answers.
Section 2 analyzes the Japanese IT Engineers
Examination, and Section 3 explains our question
answering system. Then, Sections 4 and 5 elab-
orate on our Web-base method for encyclopedia
generation. Finally, Section 6 evaluates our sys-
tem by way of experiments.
2 IT Engineers Examinations
The Class II examination consists of quadruple-
choice questions, among which technical term
questions can be subdivided into two types.
In the first type of question, examinees choose
the most appropriate description for a given tech-
nical term, such as ?memory interleave? and
?router.?
In the second type of question, examinees
choose the most appropriate term for a given
question, for which we show examples collected
from the examination in the autumn of 1999
(translated into English by one of the authors) as
follows:
1. Which data structure is most appropriate for
FIFO (First-In First-Out)?
a) binary trees, b) queues, c) stacks, d) heaps
2. Choose a LAN access method where mul-
tiple terminals transmit data simultaneously
and thus they potentially collide.
a) ATM, b) CSM/CD, c) FDDI, d) token ring
In the autumn of 1999, out of 80 question, the
number of the first and second types were 22 and
18, respectively.
3 Overview of our QA system
For the first type of question (see Section 2),
human examinees would search their knowledge
base (i.e., memory) for the description of a given
term, and compare that description with four can-
didates. Then they would choose the candidate
that is most similar to the description.
For the second type of question, human exam-
inees would search their knowledge base for the
description of each of four candidate terms. Then
they would choose the candidate term whose de-
scription is most similar to the question.
The mechanism of our QA system is analogous
to the above human methods. However, our sys-
tem uses as a knowledge base an encyclopedia
generated from the Web.
To compute the similarity between two de-
scriptions, we use techniques developed in IR re-
search, in which the similarity between a user
query and each document in a collection is usu-
ally quantified based on word frequencies. In our
case, a question and four possible answers corre-
spond to query and document collection, respec-
tively. We use one of the major probabilistic IR
method (Robertson and Walker, 1994).
To sum up, given a question, its type and four
choices, our QA system chooses as the answer
one of four candidates, in which resolution algo-
rithm varies depending on the question type.
4 Encyclopedia Generation
4.1 Overview
Figure 1 depicts the overall design of our method
to generate an encyclopedia for input terms. This
figure consists of three modules: ?retrieval,? ?ex-
traction? and ?organization,? among which the
organization module is newly introduced in this
paper. In principle, the remaining two modules
(?retrieval? and ?extraction?) are the same as pro-
posed by Fujii and Ishikawa (2000).
In Figure 1, terms can be submitted either on-
line or off-line. A reasonable method is that while
the system periodically updates the encyclopedia
off-line, terms unindexed in the encyclopedia are
dynamically processed in real-time usage. In ei-
ther case, our system processes input terms one
by one. We briefly explain each module in the
following three sections, respectively.
domain
model
Web
extraction
rules
organization
encyclopedia
retrieval
extraction
term(s)
description
model
Figure 1: The overview of our Web-based ency-
clopedia generation process.
4.2 Retrieval
The retrieval module searches the Web for pages
containing an input term, for which existing Web
search engines can be used, and those with broad
coverage are desirable.
However, search engines performing query ex-
pansion are not always desirable, because they
usually retrieve a number of pages which do not
contain a query keyword. Since the extraction
module (see Section 4.3) analyzes the usage of
the input term in retrieved pages, pages not con-
taining the term are of no use for our purpose.
Thus, we use as the retrieval module ?Google,?
which is one of the major search engines and does
not conduct query expansion2.
4.3 Extraction
In the extraction module, given Web pages con-
taining an input term, newline codes, redundant
white spaces and HTML tags that are not used in
the following process are discarded so as to stan-
dardize the page format.
Second, we (approximately) identify a region
describing the term in the page, for which two
rules are used.
2http://www.google.com/
The first rule is based on Japanese linguis-
tic patterns typically used for term descrip-
tions, such as ?X toha Y dearu (X is Y).?
Following the method proposed by Fujii and
Ishikawa (2000), we semi-automatically pro-
duced 20 patterns based on the Japanese CD-
ROM World Encyclopedia (Heibonsha, 1998),
which includes approximately 80,000 entries re-
lated to various fields.
It is expected that a region including the sen-
tence that matched with one of those patterns can
be a term description.
The second rule is based on HTML layout. In
a typical case, a term in question is highlighted
as a heading with tags such as <DT>, <B> and
<Hx> (?x? denotes a digit), followed by its de-
scription. In some cases, terms are marked with
the anchor <A> tag, providing hyperlinks to pages
where they are described.
Finally, based on the region briefly identified
by the above method, we extract a page frag-
ment as a term description. Since term descrip-
tions usually consist of a logical segment (such
as a paragraph) rather than a single sentence, we
extract a fragment that matched with one of the
following patterns, which are sorted according to
preference in descending order:
1. description tagged with <DD> in the case
where the term is tagged with <DT>3,
2. paragraph tagged with <P>,
3. itemization tagged with <UL>,
4. N sentences, where we empirically set
N = 3.
4.4 Organization
For the purpose of organization, we classify ex-
tracted term descriptions based on word senses
and domains.
Although a number of methods have been pro-
posed to generate word senses (for example, one
based on the vector space model (Schu?tze, 1998)),
it is still difficult to accurately identify word
senses without explicit dictionaries that predefine
sense candidates.
3<DT> and <DD> are inherently provided to describe
terms in HTML.
Since word senses are often associated with
domains (Yarowsky, 1995), word senses can be
consequently distinguished by way of determin-
ing the domain of each description. For ex-
ample, different senses for ?pipeline (processing
method/transportation pipe)? are associated with
computer and construction domains (fields), re-
spectively.
To sum up, the organization module classifies
term descriptions based on domains, for which we
use domain and description models. In Section 5,
we elaborate on the organization model.
5 Statistical Organization Model
5.1 Overview
Given one or more (in most cases more than one)
descriptions for a single term, the organization
module selects appropriate description(s) for each
domain related to the term.
We do not need all the extracted descriptions
as final outputs, because they are usually similar
to one another, and thus are redundant. For the
moment, we assume that we know a priori which
domains are related to the input term.
From the viewpoint of probability theory, our
task here is to select descriptions with greater
probability for given domains. The probability
for description d given domain c, P (d|c), is com-
monly transformed as in Equation (1), through
use of the Bayesian theorem.
P (d|c) = P (c|d) ? P (d)P (c) (1)
In practice, P (c) can be omitted because this fac-
tor is a constant, and thus does not affect the rela-
tive probability for different descriptions.
In Equation (1), P (c|d) models a probability
that d corresponds to domain c. P (d) models a
probability that d can be a description for the term
in question, disregarding the domain. We shall
call them domain and description models, respec-
tively.
To sum up, in principle we select d?s that are
strongly associated with a certain domain, and are
likely to be descriptions themselves.
Extracted descriptions are not linguistically un-
derstandable in the case where the extraction pro-
cess is unsuccessful and retrieved pages inher-
ently contain non-linguistic information (such as
special characters and e-mail addresses).
To resolve this problem, we previously used
a language model to filter out descriptions with
low perplexity (Fujii and Ishikawa, 2000). How-
ever, in this paper we integrated a description
model, which is practically the same as a lan-
guage model, with an organization model. The
new framework is more understandable with re-
spect to probability theory.
In practice, we first use Equation (1) to com-
pute P (d|c) for all the c?s predefined in the do-
main model. Then we discard such c whose
P (d|c) is below a specific threshold. As a result,
for the input term, related domains and descrip-
tions are simultaneously selected. Thus, we do
not have to know a priori which domains are re-
lated to each term.
In the following two sections, we explain meth-
ods to realize the domain and description models,
respectively.
5.2 Domain Model
The domain model quantifies the extent to which
description d is associated with domain c, which
is fundamentally a categorization task.
Among a number of existing categorization
methods, we experimentally used one proposed
by Iwayama and Tokunaga (1994), which formu-
lates P (c|d) as in Equation (2).
P (c|d) = P (c) ?
?
t
P (t|c) ? P (t|d)
P (t) (2)
Here, P (t|d), P (t|c) and P (t) denote probabili-
ties that word t appears in d, c and all the domains,
respectively. We regard P (c) as a constant. While
P (t|d) is simply a relative frequency of t in d, we
need predefined domains to compute P (t|c) and
P (t). For this purpose, the use of large-scale cor-
pora annotated with domains is desirable.
However, since those resources are pro-
hibitively expensive, we used the ?Nova? dic-
tionary for Japanese/English machine translation
systems4, which includes approximately one mil-
lion entries related to 19 technical fields as listed
below:
4Produced by NOVA, Inc.
aeronautics, biotechnology, business,
chemistry, computers, construction, de-
fense, ecology, electricity, energy, fi-
nance, law, mathematics, mechan-
ics, medicine, metals, oceanography,
plants, trade.
We extracted words from dictionary entries
to estimate P (t|c) and P (t). For Japanese en-
tries, we used the ChaSen morphological ana-
lyzer (Matsumoto et al, 1997) to extract words.
We also used English entries because Japanese
descriptions often contain English words.
It may be argued that statistics extracted from
dictionaries are unreliable, because word frequen-
cies in real word usage are missing. However,
words that are representative for a domain tend
to be frequently used in compound word entries
associated with the domain, and thus our method
is a practical approximation.
5.3 Description Model
The description model quantifies the extent to
which a given page fragment is feasible as a de-
scription for the input term. In principle, we de-
compose the description model into language and
quality properties, as shown in Equation (3).
P (d) = P
L
(d) ? P
Q
(d) (3)
Here, P
L
(d) and P
Q
(d) denote language and
quality models, respectively.
It is expected that the quality model discards
incorrect or misleading information contained in
Web pages. For this purpose, a number of qual-
ity rating methods for Web pages (Amento et al,
2000; Zhu and Gauch, 2000) can be used.
However, since Google (i.e., the search engine
we used in the retrieval module) rates the quality
of pages based on hyperlink information, and se-
lectively retrieves those with higher quality (Brin
and Page, 1998), we tentatively regarded P
Q
(d)
as a constant. Thus, in practice the description
model is approximated solely with the language
model as in Equation (4).
P (d) ? P
L
(d) (4)
Statistical approaches to language modeling
have been used in much NLP research, such
as machine translation (Brown et al, 1993) and
speech recognition (Bahl et al, 1983). Our lan-
guage model is almost the same as existing mod-
els, but is different in two respects.
First, while general language models quantify
the extent to which a given word sequence is lin-
guistically acceptable, our model also quantifies
the extent to which the input is acceptable as a
term description. Thus, we trained the model
based on an existing machine readable encyclo-
pedia.
We used the ChaSen morphological analyzer
to segment the Japanese CD-ROM World Ency-
clopedia (Heibonsha, 1998) into words (we re-
placed headwords with a common symbol), and
then used the CMU-Cambridge toolkit (Clark-
son and Rosenfeld, 1997) to model a word-based
trigram. Consequently, descriptions in which
word sequences are more similar to those in the
World Encyclopedia are assigned greater proba-
bility scores through our language model.
Second, P (d), which is generally a product
of probabilities for N -grams in d, is quite sen-
sitive to the length of d. In the cases of machine
translation and speech recognition, this problem
is less crucial because multiple candidates com-
pared based on the language model are almost
equivalent in terms of length. For example, in the
case of machine translation, candidates are trans-
lations for a single input, which are usually com-
parable with respect to length.
However, since in our case length of descrip-
tions are significantly different, shorter descrip-
tions are more likely to be selected, regardless of
the quality. To avoid this problem, we normalize
P (d) by the number of words contained in d.
6 Experimentation
6.1 Methodology
We evaluated the performance of our question an-
swering system, for which we used as test in-
puts 40 technical term questions collected from
the Class II examination (the autumn of 1999).
First, we generated an encyclopedia including
96 terms that are associated with those 40 ques-
tions. For all the 96 test terms, Google retrieved a
positive number of pages, and the average num-
ber of pages for one term was 196,503. Since
Google practically outputs contents of the top
1,000 pages, the remaining pages were not used
in our experiments.
For each test term, we computed P (d|c) us-
ing Equation (1) and discarded domains whose
P (d|c) was below 0.05. Then, for each remain-
ing domain, the top three descriptions with higher
P (d|c) values were selected as the final outputs,
because a preliminary experiment showed that a
correct description was generally found in the top
three candidates.
In addition, to estimate a baseline perfor-
mance, we used the ?Nichigai? computer dictio-
nary (Nichigai Associates, 1996). This dictio-
nary lists approximately 30,000 Japanese techni-
cal terms related to the computer field, and con-
tains descriptions for 13,588 terms. In this dictio-
nary 42 out of 96 test terms were described.
We compared the following three different re-
sources as a knowledge base:
? the Nichigai dictionary (?Nichigai?),
? the descriptions generated in the first experi-
ment (?Web?),
? combination of both resources (?Nichigai +
Web?).
6.2 Results
Table 1 shows the result of our comparative ex-
periment, in which ?C? and ?A? denote coverage
and accuracy, respectively, for variations of our
QA system.
Since all the questions we used are quadruple-
choice, in case the system cannot answer the
question, random choice can be performed to im-
prove the coverage to 100%.
Thus, for each knowledge resource we com-
pared cases without/with random choice, which
are denoted ?w/o Random? and ?w/ Random? in
Table 1, respectively.
Table 1: Coverage and accuracy (%) for different
question answering methods.
w/o Random w/ Random
Resource C A C A
Nichigai 50.0 65.0 100 45.0
Web 92.5 48.6 100 46.9
Nichigai + Web 95.0 63.2 100 61.3
In the case where random choice was not per-
formed, the Web-based encyclopedia noticeably
improved the coverage for the Nichigai dictio-
nary, but decreased the accuracy. However, by
combining both resources, the accuracy was no-
ticeably improved, and the coverage was compa-
rable with that for the Nichigai dictionary.
On the other hand, in the case where random
choice was performed, the Nichigai dictionary
and the Web-based encyclopedia were compara-
ble in terms of both the coverage and accuracy.
Additionally, by combining both resources, the
accuracy was further improved.
We also investigated the performance of our
QA system where descriptions related to the com-
puter domain are solely used. For example, the
description of ?pipeline (transportation pipe)? is
in principle irrelevant or misleading to answer
questions associated with ?pipeline (processing
method).?
However, coverage/accuracy did not change,
because approximately one third of the resultant
descriptions were inherently related to the com-
puter domain, and thus those related to minor do-
mains did not affect the result.
7 Conclusion
In this paper, we proposed a question answering
system which uses an encyclopedia as a knowl-
edge base. For this purpose, we reformalized
our Web-based extraction method, and proposed
a new statistical organization model to improve
the quality of extracted data.
Given a term for which encyclopedic knowl-
edge (i.e., descriptions) is to be generated, our
method sequentially performs a) retrieval of Web
pages containing the term, b) extraction of page
fragments describing the term, and c) organiz-
ing extracted descriptions based on domains (and
consequently word senses).
For the purpose of evaluation, we used as test
questions the Japanese Information-Technology
Engineers Examination, and found that our Web-
based encyclopedia was comparable with an ex-
isting dictionary in terms of the application to
question answering. In addition, by using the both
resources the performance of question answering
was further improved.
Acknowledgments
The authors would like to thank NOVA, Inc.
for their support with the Nova dictionary and
Katunobu Itou (The National Institute of Ad-
vanced Industrial Science and Technology, Japan)
for his insightful comments on this paper.
References
Brian Amento, Loren Terveen, and Will Hill. 2000.
Does ?authority? mean quality? predicting expert
quality ratings of Web documents. In Proceedings
of the 23rd Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 296?303.
Lalit. R. Bahl, Frederick Jelinek, and Robert L. Mer-
cer. 1983. A maximum linklihood approach to
continuous speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
5(2):179?190.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1?7):107?117.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Philip Clarkson and Ronald Rosenfeld. 1997. Statisti-
cal language modeling using the CMU-Cambridge
toolkit. In Proceedings of EuroSpeech?97, pages
2707?2710.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: Extract-
ing term descriptions from semi-structured texts.
In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pages
488?495.
Sanda M. Harabagiu, Marius A. Pas?ca, and Steven J.
Maiorano. 2000. Experiments with open-domain
textual question answering. In Proceedings of the
18th International Conference on Computational
Linguistics, pages 292?298.
Hitachi Digital Heibonsha. 1998. CD-ROM World
Encyclopedia. (In Japanese).
Makoto Iwayama and Takenobu Tokunaga. 1994. A
probabilistic model for text categorization: Based
on a single random variable with multiple values.
In Proceedings of the 4th Conference on Applied
Natural Language Processing, pages 162?167.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Osamu Imaichi, and Tomoaki
Imamura. 1997. Japanese morphological analysis
system ChaSen manual. Technical Report NAIST-
IS-TR97007, NAIST. (In Japanese).
Dan Moldovan and Sanda Harabagiu. 2000. The
structure and performance of an open-domain ques-
tion answering system. In Proceedings of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 563?570.
Nichigai Associates. 1996. English-Japanese com-
puter terminology dictionary. (In Japanese).
John Prager, Eric Brown, and Anni Coden. 2000.
Question-answering by predictive annotation. In
Proceedings of the 23rd Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 184?191.
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of
the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 232?241.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceed-
ings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 200?207.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 189?
196.
Xiaolan Zhu and Susan Gauch. 2000. Incorporating
quality metrics in centralized/distributed informa-
tion retrieval on the World Wide Web. In Proceed-
ings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 288?295.
