Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 974?979,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
POS Tagging of English-Hindi Code-Mixed Social Media Content
Yogarshi Vyas
?
University of Maryland
yogarshi@cs.umd.edu
Spandana Gella
?
Xerox Research Centre Europe
spandanagella@gmail.com
Jatin Sharma Kalika Bali Monojit Choudhury
Microsoft Research India
{jatin.sharma,kalikab,monojitc}@microsoft.com
Abstract
Code-mixing is frequently observed in
user generated content on social media,
especially from multilingual users. The
linguistic complexity of such content is
compounded by presence of spelling vari-
ations, transliteration and non-adherance
to formal grammar. We describe our
initial efforts to create a multi-level an-
notated corpus of Hindi-English code-
mixed text collated from Facebook fo-
rums, and explore language identifica-
tion, back-transliteration, normalization
and POS tagging of this data. Our re-
sults show that language identification and
transliteration for Hindi are two major
challenges that impact POS tagging accu-
racy.
1 Introduction
Code-Switching and Code-Mixing are typical
and well-studied phenomena of multilingual so-
cieties (Gumperz, 1964; Auer, 1984; Myers-
Scotton, 1993; Danet and Herring, 2007;
Cardenas-Claros and Isharyanti, 2009). Lin-
guists differentiate between the two, where
Code-Switching is juxtaposition within the same
speech exchange of passages of speech be-
longing to two different grammatical systems
or sub-systems (Gumperz, 1982), and Code-
Mixing (CM) refers to the embedding of linguis-
tic units such as phrases, words and morphemes
of one language into an utterance of another lan-
guage (Myers-Scotton, 1993). The first exam-
ple in Fig. 1 features CM where English words
are embedded in a Hindi sentence, whereas the
second example shows codeswitching. Here, we
will use CM to imply both. Work on computa-
?
This work was done during authors? internship at Mi-
crosoft Research India.
tional models of CM have been few and far be-
tween (Solorio and Liu, 2008a; Solorio and Liu,
2008b; Nguyen and Dogruoz, 2013), primarily
due to the paucity of CM data in conventional
text-corpora which makes data-intensive methods
hard to apply. Solorio and Liu (2008a) in their
work on English-Spanish CM use models built on
smaller datasets to predict valid switching points
to synthetically generate data from monolingual
corpora, and in another work (2008b) describe
parts-of-speech (POS) tagging of CM text.
CM though typically observed in spoken lan-
guage is now increasingly more common in text,
thanks to the proliferation of the Computer Me-
diated Communication channels, especially so-
cial media like Twitter and Facebook (Crys-
tal, 2001; Herring, 2003; Danet and Herring,
2007; Cardenas-Claros and Isharyanti, 2009).
Social media content is tremendously important
for studying trends, reviews, events, human-
behaviour as well as linguistic analysis, and there-
fore in recent times has spurred a lot of interest
in automatic processing of such data. Neverthe-
less, CM on social media has not been studied
from a computational aspect. Moreover, social
media content presents additional challenges due
to contractions, non-standard spellings and non-
grammatical constructions. Furthermore, for lan-
guages written in scripts other than Roman, like
Hindi, Bangla, Japanese, Chinese and Arabic, Ro-
man transliterations are typically used for repre-
senting the words (Sowmya et al., 2010). This can
prove a challenge for language identification and
segregation of the two languages.
In this paper, we describe our initial efforts to
POS tag social media content from English-Hindi
(henceforth En-Hi) bilinguals while trying to ad-
dress the challenges of CM, transliteration and
non-standard spelling, as well as lack of anno-
tated data. POS tagging is one of the fundamen-
tal pre-processing steps for NLP, and while there
974
have been works on POS tagging of social media
data (Gimpel et al., 2011; Owoputi et al., 2013)
and of CM (Solorio and Liu, 2008b), but we do
not know of any work on POS tagging of CM
text from social media that involves transliteration.
The salient contributions of this work are in for-
malizing the problem and related challenges for
processing of En-Hi social media data, creation
of an annotated dataset and some initial experi-
ments for language identification, transliteration,
normalization and POS tagging of this data.
2 Corpus Creation
For this study, we collected data from Face-
book public pages of three celebrities: Amitabh
Bachchan, Shahrukh Khan, Narendra Modi, and
the BBC Hindi news page. All these pages are
very popular with 1.8 to 15.5 million ?likes?. A to-
tal of 40 posts were manually selected from these
pages, which were published between 22nd ? 28th
October 2013. The posts having a long thread of
comments (50+) were preferred, because CM and
non-standard usage of language is more common
in the comments. We shall use the term post to re-
fer to either a post or a comment. The corpus thus
created has 6,983 posts and 113,578 words. The
data was semi-automatically cleaned and format-
ted. The user names were removed for anonymity,
but the names appearing in comments, which are
mostly of celebrities, were retained.
2.1 Annotation
There are various interesting linguistic as well as
socio-pragmatic features (e.g., user demograph-
ics, presence of sarcasm or humor, polarity) for
which this corpus could be annotated because CM
is influenced by both linguistic as well as extra-
linguistic features. However, initial attempts at
such detailed and layered annotation soon revealed
the resource-intensiveness of the task. We, thus,
scaled down the annotation to the following four
layers:
Matrix: The posts are split into contiguous
fragments of words such that each fragment has
a unique matrix language (either En or Hi). The
matrix language is defined as the language which
governs the grammatical relation between the con-
stituents of the utterance. Any other language
words that are nested into the matrix constitute the
embedded language(s). Usually, matrix language
can be assigned to clauses or sentences.
Word origin: Every word is marked for its ori-
gin or source language, En or Hi, depending on
whether it is an English or Hindi word. Words that
are of neither Hindi nor English origin are marked
as Ot or Other. Here, we assume that code-mixing
does not happen at sublexical levels, as it is un-
common in this data; Hi and En have a sim-
pler inflectional morphology and thus, sub-lexical
mixing though present (e.g., computeron has
a En root - computer and a Hi plural marker
on) is relatively less common. In languages with
richer morphology and agglutination, like Bangla
and most Dravidian languages, more frequent sub-
lexical mixing may be observed. Also note that
words are borrowed extensively between Hi and
En such that certain English words (e.g., bus,
party, vote etc) are no longer perceived as English
words by the Hindi speakers. However, here we
will not distinguish between CM and borrowing,
and such borrowed English words have also been
labeled as En words.
Normalization/Transliteration: Whenever the
word is in a transliterated form, which is often the
case for the Hi words, it is labeled with the in-
tended word in the native script (e.g., Devanagari
for Hi). If the word is in native script, but uses
a non-standard spelling, it is labeled with the cor-
rect standard spelling. We call this the spelling
normalization layer.
Parts-of-Speech (POS): Finally, each word is
also labeled with its POS. We use the Universal
POS tagset proposed by Petrov et al. (2011) which
has 12 POS tags that are applicable to both En
and Hi. The POS labels are decided based on the
function of a word in the context, rather than a
decontextualized lexical category. This is an im-
portant notion, especially for CM text, because of-
ten the original lexical category of an embedded
word is lost in the context of the matrix language,
and it plays the role of a different lexical category.
Though the Universal POS tagset does not pre-
scribe a separate tag for Named Entities, we felt
the necessity of marking three different kinds of
NEs - people, location and organization, because
almost every comment has one or more NEs and
strictly speaking word origin does not make sense
for these words.
Annotation Scheme: Fig. 1 illustrates the an-
notation scheme through two examples. Each
post is enclosed within <s></s> tags. The
matrices within a post are separated by the
<matrix></matrix> tags which take the matrix
language as an argument. Each word is anno-
975
Figure 1: Two example annotations.
tated for POS, and the language (/E or /H for En
or Hi respectively) only if it is different from the
language of the matrix. In case of non-standard
spelling in English, the correct spelling is ap-
pended as ?sol NOUN=soul?, while for the
Hindi words, the correct Devanagari translitera-
tion is appended. The NEs are marked with the
tags P (person), L (location) or O (organization)
and multiword NEs are enclosed within square
brackets ?[]?.
A random subsample of 1062 posts consisting
of 10171 words were annotated by a linguist who
is a native speaker of Hi and proficient in En. The
annotations were reviewed and corrected by two
experts linguists. During this phase, it was also
observed that a large number of comments were
very short, typically an eulogism of their favorite
celebrity and hence were not interesting from a
linguistic point of view. For our experiments, we
removed all posts that had fewer than 5 words.
The resulting corpus had 381 comments/posts and
4135 words.
2.2 CM Distribution
Most of the posts (93.17%) are in Roman script,
and only 2.93% were in Devanagari. Around 3.5%
of the posts contain words in both the scripts (typ-
ically a post in Devanagari with hashtags or urls in
Roman script), and a very small fraction of the text
(0.4% of comments/posts and 0.6% words) was in
some other script. The fraction of words present
in Roman and Devanagri scripts are 80.76% and
15.32% respectively, which shows that the De-
vanagari posts are relatively longer than the Ro-
man posts. Due to their relative rarity, the posts
containing words in Devanagari or any other script
were not considered for annotation.
In the annotated data, 1102 sentences are in a
single matrix (398 Hi, 698 En and 6 Ot) and in
45 posts there is at least one switch of matrix
(mostly between Hi and En. Thus, 4.2% of the
data shows code-switching. This is a strict defi-
nition of code-switching; if we consider a change
in matrix within a conversation thread as a code-
switch, then in this data all the threads exhibit
code-switching. However, out of the 398 com-
ments in Hi-matrix, 23.37% feature CM (i.e., they
have at least one or more non-Hi (or rather, al-
most always En) words embedded. On the other
hand, only 7.34% En-matrix comments feature
CM (again almost always with Hi). Thus, a total
of 17.2% comments/posts, which contains a quar-
ter of all the words in the annotated corpus, fea-
ture either CM or code-switching or both. We also
note that more than 40% words in the corpus are
in Hi or other Indian languages, but written in Ro-
man script; hence, they are in transliterated form.
See (Bali et al., 2014) for an in-depth discussion
on the characteristics of the CM data.
This analysis demonstrates the necessity of CM
and transliterated text processing in the context of
Indian user-generated social media content. Per-
haps, the numbers are not too different for such
content generated by the users of any other bilin-
gual and multilingual societies.
3 Models and Experiments
POS tagging of En-Hi code-mixed data requires
language identification at both word and matrix
level as well back-transliteration of the text into
976
Actual Predicted Label Recall
Label Hi En
Hi 1057 515 0.672
En 45 2023 0.978
Precision 0.959 0.797
Table 1: Confusion matrix, precision and recall of
the language identification module.
the native script. Additionaly, since we are work-
ing with content from social media, the usage of
non-standard spelling is rampant and thus, nor-
malization of text into some standard form is re-
quired. Ideally, these tasks should be performed
jointly since they are interdependent. However,
due to lack of resources, we implement a pipelined
approach in which the tasks - language identifica-
tion, text normalization and POS tagging - are per-
formed sequentially, in that order. This pipelined
approach also allows us to use various off-the-
shelf tools for solving these subtasks and quickly
create a baseline system. The baseline results can
also provide useful insight into the inherent hard-
ness of POS tagging of code-mixed social media
text. In this section, we first describe our approach
to solve these three tasks, and then discuss the ex-
periments and results.
3.1 Language identification
Langauge identification is a well studied prob-
lem (King and Abney, 2013; Carter et al., 2013;
Goldszmidt et al., 2013; Nguyen and Dogruoz,
2013), though for CM text, especially those in-
volving transliterations and orthographic varia-
tion, this is far from a solved problem (Nguyen and
Dogruoz, 2013). There was a shared task in FIRE
2013 (Saha Roy et al., 2013) on language iden-
tification and back transliteration for En mixed
with Hi, Bangla and Gujarati. Along the lines
of Gella et al (Gella et al., 2013), which was the
best performing system in this shared task, we
used the word-level logistic regression classifier
built by King and Abney (2013). This system pro-
vides a source language with a confidence prob-
ability for each word in the test set. We trained
the classifier on 3201 English words extracted
from the SMS corpus developed by Choudhury
et al (2007), while the Hindi data was obtained
by sampling 3218 Hindi transliterations out of the
En-Hi transliteration pairs developed by Sowmya
et al. (Sowmya et al., 2010). Ideally, the context of
a token is important for identifying the language.
Again, following (Gella et al., 2013) we incorpo-
rate context information through a code-switching
probability, P
s
. A higher value of P
s
implies a
lower probability of code-switching, i.e., adjacent
words are more likely to be in the same language.
Table 1 shows the token (word) level confusion
matrix for the language identification task on our
dataset. The language labels of 84.6% of the to-
kens were correctly predicted by the system. As
can be seen from the Table, the precision for pre-
dicting Hi is high, whereas that for En is low. This
is mainly due to the presence of a large number of
contracted and distorted Hi words in the dataset,
e.g. h for hai (Fig. 1), which were tagged as
En by our system because the training examples
had no contracted Hi words, but short and non-
conventional spellings were in plenty in the En
training examples as those were extracted from the
SMS corpus.
3.2 Normalization
In our dataset, if a word is identified as Hi, then
it must be back-transliterated to Devanagari script
so that any off-the-shelf Hindi POS tagger can be
used. We used the system by Gella et al. (Gella
et al., 2013) for this task, which is part rule-based
and part statistical. The system was trained on the
35000 unique transliteration pairs extracted from
Hindi song lyrics (Gupta et al., 2012). This corpus
has a reasonably wide coverage of Hindi words,
and past researchers have also shown that translit-
eration does not require a very large amount of
training data. Normalization of the En text was
not needed because the POS tagger (Owoputi et
al., 2013) could handle unnormalized text.
3.3 POS tagging
Solorio and Liu (2008b) describes a few ap-
proaches to POS-tagging of code-switched Span-
glish text, all of which primarily relies on two
monolingual taggers and certain heuristics to com-
bine the output from the two. One of the sim-
pler heuristics is based on language identification,
where the POS tag of a word is the output of the
monolingual tagger of the language in which the
word is. In this initial study, we apply this ba-
sic idea for POS tagging of CM data. We divide
the text (which is already sentence-separated) into
contiguous maximal chunks of words which are in
the same language. Then we apply a Hi POS tag-
ger to the Hi chunks, and an En POS tagger to the
En chunks.
977
Model LI HN Tagger Hi Acc. En Acc. Total Acc. Hi CA En CA Total CA
1a K K Standard 75.14 81.91 79.02 27.34 39.67 34.05
1b K K Twitter 75.14 82.66 79.02 27.34 35.74 31.91
2 K NK Twitter 65.61 81.73 74.87 17.58 33.77 26.38
3 NK NK Twitter 44.74 80.68 65.39 40.00 13.17 25.00
Table 2: POS Tagging accuracies for the different models. K=Known, NK = Not Known. LI = Language
labels, HN = Hindi normalized forms, Acc. = Token level accuracy, CA = Chunk level accuracy.
We use a CRF++ based POS tagger for Hi,
which is freely available from http://nltr.
org/snltr-software/. For En, we use the
Twitter POS tagger (Owoputi et al., 2013). It
also has an inbuilt tokenizer and can work di-
rectly on unnormalized text. This tagger has been
chosen because Facebook posts and comments
are more Twitter-like. We also use the Stanford
POS Tagger (Toutanova et al., 2003) which, un-
like the Twitter POS Tagger, has not been tuned
for Twitter-like text. These taggers use different
tagsets - the ILPOST for Hi (Sankaran et al., 2008)
and Penn-TreeBank for En (Marcus et al., 1993).
The output tags are appropriately mapped to the
smaller Universal tagset (Petrov et al., 2011).
3.4 Experiments and Results
We conducted three different experiments as fol-
lows. In the first experiment, we assume that
we know the language identities and normal-
ized/transliterated forms of the words, and only do
the POS tagging. This experiment gives us an idea
of the accuracy of POS tagging task, if normal-
ization, transliteration and language identification
could be done perfectly. We conduct this exper-
iments with two different En POS taggers: the
Stanford POS tagger which is trained on formal
English text (Model 1a) and the Twitter POS tag-
ger (Model 1b). In the next experiment (Model
2), we assume that only the language identity of
the words are known, but for Hindi we apply our
model to generate the back transliterations. For
English, we apply the Twitter POS tagger directly
because it can handle unnormalized social media
text. The third experiment (Model 3) assumes that
nothing is known. So language identifier is first
applied, and based on the language detected, we
apply the Hi translitertaion module, and Hi POS
tagger, or the En tagger. This is the most chal-
lenging and realistic setting. Note that the matrix
information is not used in any of our experiments,
though it could be potentially useful for POS tag-
ging and could be explored in future.
Table 2 gives a summary of the four models
along with the POS tagging accuracies (in %). It
shows token level as well as chunk leve accuracies
(CA), i.e., what percentage of chunks have been
correctly POS tagged. As can be seen, Hi POS
tagging has relatively low accuracies than En POS
tagging at word level for all cases. This is primar-
ily due to the errors of the transliteration module,
which in turn, is because the transliteration does
not address spelling contractions. This is also re-
flected in the drop in the accuracies for the case
where LI is unknown. The very low CA for En
for model 3 is primarily because some of the Hi
chunks are incorrectly identified as En by the lan-
guage identification module (see Table 1). How-
ever, the gradual drop of token and chunk level
accuracies from model 1 to model 3 clearly shows
the effect of gradual error accumulation from each
of the modules. We observe that Nouns were
usually confused most with Verbs and vice versa,
while the Adj were mostly confused with Nouns,
Pronouns with Determiners, and Adpositions with
Conjunctions.
4 Conclusion
This is a work in progress. We have identified
normalization and transliteration as two very chal-
lenging problems for En-Hi CM text. Joint mod-
elling of language identification, normalization,
transliteration as well as POS tagging is expected
to yield better results. We plan to continue our
work in that direction, specifically for conversa-
tional text in social media in a multilingual con-
text. CM is a common phenomenon found in all
bilingual and multilingual societies. The issue of
transliteration exist for most of the South Asian
languages as well as many other languages such as
Arabic and Greek, which use a non-Roman based
script (Gupta et al., 2014). The challenges and is-
sues identified in this study are likely to hold for
many other languages as well, which makes this a
very important and globally prevalent problem.
978
References
Peter Auer. 1984. The Pragmatics of Code-Switching:
A Sequential Approach. Cambridge University
Press.
Kalika Bali, Yogarshi Vyas, Jatin Sharma, and Monojit
Choudhury. 2014. ??i am borrowing ya mixing?? an
analysis of English-Hindi code mixing in Facebook.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code Switching, EMNLP.
M?onica Stella Cardenas-Claros and Neny Isharyanti.
2009. Code-switching and code-mixing in internet
chatting: Between yes, ya, and si a case study. In
The JALT CALL Journal, 5.
Simon Carter, Wouter Weerkamp, and Manos
Tsagkias. 2013. Microblog language identification:
Overcoming the limitations of short, unedited and
idiomatic text. Language Resources and Evaluation
Journal, 47:195?215.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR, 10(3-4):157?174.
David Crystal. 2001. Language and the Internet.
Cambridge University Press.
Brenda Danet and Susan Herring. 2007. The Multilin-
gual Internet: Language, Culture, and Communica-
tion Online. Oxford University Press., New York.
Spandana Gella, Jatin Sharma, and Kalika Bali. 2013.
Query word labeling and back transliteration for in-
dian languages: Shared task system description. In
FIRE Working Notes.
Kevin Gimpel, N. Schneider, B. O?Connor, D. Das,
D. Mills, J. Eisenstein, M. Heilman, D. Yogatama,
J. Flanigan, and N. A. Smith. 2011. Part-of-speech
tagging for twitter: Annotation, features, and exper-
iments. In Proceedings of ACL.
Moises Goldszmidt, Marc Najork, and Stelios Papari-
zos. 2013. Boot-strapping language identifiers for
short colloquial postings. In Machine Learning and
Knowledge Discovery in Databases, volume 8189 of
Lecture Notes in Computer Science, pages 95?111.
John J. Gumperz. 1964. Hindi-punjabi code-switching
in Delhi. In Proceedings of the Ninth International
Congress of Linguistics. Mouton:The Hague.
John J. Gumperz. 1982. Discourse Strategies. Oxford
University Press.
Kanika Gupta, Monojit Choudhury, and Kalika Bali.
2012. Mining Hindi-English transliteration pairs
from online Hindi lyrics. In Proceedings of LREC.
Parth Gupta, Kalika Bali, Rafael E. Banchs, Monojit
Choudhury, and Paolo Rosso. 2014. Query ex-
pansion for mixed-script information retrieval. In
Proc. of SIGIR, pages 677?686. ACM Association
for Computing Machinery.
Susan Herring, editor. 2003. Media and Language
Change. Special issue of Journal of Historical Prag-
matics 4:1.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
NAACL-HLT, pages 1110?1119.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313?330.
Carol Myers-Scotton. 1993. Dueling Languages:
Grammatical Structure in Code-Switching. Clare-
don, Oxford.
Dong Nguyen and A. Seza Dogruoz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 857?862.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
Rishiraj Saha Roy, Monojit Choudhury, Prasenjit Ma-
jumder, and Komal Agarwal. 2013. Overview and
datasets of fire 2013 track on transliterated search.
In FIRE Working Notes.
Bhaskaran Sankaran, Kalika Bali, Monojit Choudhury,
Tanmoy Bhattacharya, Pushpak Bhattacharyya,
Girish Nath Jha, S. Rajendran, K. Saravanan,
L. Sobha, and K. V. Subbarao. 2008. A com-
mon parts-of-speech tagset framework for indian
languages. In Proceedings of LREC.
Thamar Solorio and Yang Liu. 2008a. Learning to
predict code-switching points. In Proceedings of the
Empirical Methods in natural Language Processing.
Thamar Solorio and Yang Liu. 2008b. Parts-of-speech
tagging for English-Spanish code-switched text. In
Proceedings of the Empirical Methods in natural
Language Processing.
V. B. Sowmya, Monojit Choudhury, Kalika Bali,
Tirthankar Dasgupta, and Anupam Basu. 2010. Re-
source creation for training and testing of translitera-
tion systems for indian languages. In Proceedings of
the Language Resource and Evaluation Conference
(LREC).
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL.
979
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215?220,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
One Sense per Tweeter ... and Other Lexical Semantic Tales of Twitter
Spandana Gella, Paul Cook and Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
sgella@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
In recent years, microblogs such as Twit-
ter have emerged as a new communication
channel. Twitter in particular has become
the target of a myriad of content-based
applications including trend analysis and
event detection, but there has been little
fundamental work on the analysis of word
usage patterns in this text type. In this
paper ? inspired by the one-sense-per-
discourse heuristic of Gale et al. (1992)
? we investigate user-level sense distri-
butions, and detect strong support for ?one
sense per tweeter?. As part of this, we con-
struct a novel sense-tagged lexical sample
dataset based on Twitter and a web corpus.
1 Introduction
Social media applications such as Twitter enable
users from all over the world to create and share
web content spontaneously. The resulting user-
generated content has been identified as having
potential in a myriad of applications including
real-time event detection (Petrovi?c et al., 2010),
trend analysis (Lau et al., 2012) and natural dis-
aster response co-ordination (Earle et al., 2010).
However, the dynamism and conversational na-
ture of the text contained in social media can
cause problems for traditional NLP approaches
such as parsing (Baldwin et al., 2013), mean-
ing that most content-based approaches use sim-
ple keyword search or a bag-of-words representa-
tion of the text. This paper is a first step towards
full lexical semantic analysis of social media text,
in investigating the sense distribution of a range
of polysemous words in Twitter and a general-
purpose web corpus.
The primary finding of this paper is that there
are strong user-level lexical semantic priors in
Twitter, equivalent in strength to document-level
lexical semantic priors, popularly termed the ?one
sense per discourse? heuristic (Gale et al., 1992).
This has potential implications for future applica-
tions over Twitter which attempt to move beyond a
simple string-based meaning representation to ex-
plicit lexical semantic analysis.
2 Related Work
The traditional approach to the analysis of word-
level lexical semantics is via word sense dis-
ambiguation (WSD), where usages of a given
word are mapped onto discrete ?senses? in a pre-
existing sense inventory (Navigli, 2009). The most
popular sense inventory used in WSD research has
been WordNet (Fellbaum, 1998), although its fine-
grained sense distinctions have proven to be diffi-
cult to make for human annotators and WSD sys-
tems alike. This has resulted in a move towards
more coarse-grained sense inventories (Palmer et
al., 2004; Hovy et al., 2006; Navigli et al., 2007),
or alternatively away from pre-existing sense in-
ventories altogether, towards joint word sense in-
duction (WSI) and disambiguation (Navigli and
Vannella, 2013; Jurgens and Klapaftis, 2013).
Two heuristics that have proven highly powerful
in WSD and WSI research are: (1) first sense tag-
ging, and (2) one sense per discourse. First sense
tagging is based on the observation that sense dis-
tributions tend to be Zipfian, such that if the pre-
dominant or ?first? sense can be identified, simply
tagging all occurrences of a given word with this
sense can achieve high WSD accuracy (McCarthy
et al., 2007). Unsurprisingly, there are significant
differences in sense distributions across domains
(cf. cloud in the COMPUTING and METEOROLOG-
ICAL domains), motivating the need for unsuper-
vised first sense learning over domain-specific cor-
pora (Koeling et al., 2005).
One sense per discourse is the observation that
a given word will often occur with a single sense
across multiple usages in a single document (Gale
215
et al., 1992). Gale et al. established the heuristic
on the basis of 9 ambiguous words using a coarse-
grained sense inventory, finding that the probabil-
ity of a given pair of usages of a word taken from a
given document having the same sense was 94%.
However, Krovetz (1998) found that for a fine-
grained sense inventory, only 67% of words exhib-
ited the single-sense-per-discourse property for all
documents in a corpus.
A radically different view on WSD is word us-
age similarity, whereby two usages of a given
word are rated on a continuous scale for similar-
ity, in isolation of any sense inventory (Erk et al.,
2009). Gella et al. (2013) constructed a word us-
age similarity dataset for Twitter messages, and
developed a topic modelling approach to the task,
building on the work of Lui et al. (2012). To the
best of our knowledge, this has been the only at-
tempt to carry out explicit word-level lexical se-
mantic analysis of Twitter text.
3 Dataset Construction
In order to study sense distributions of words in
Twitter, we need a sense inventory to annotate
against, and also a set of Twitter messages to an-
notate. Further, as a point of comparison for the
sense distributions in Twitter, we require a second
corpus; here we use the ukWaC (Ferraresi et al.,
2008), a corpus built from web documents.
For the sense inventory, we chose the Macmil-
lan English Dictionary Online
1
(MACMILLAN,
hereafter), on the basis of: (1) its coarse-grained
general-purpose sense distinctions, and (2) its reg-
ular update cycle (i.e. it contains many recently-
emerged senses). These criteria are important
in terms of inter-annotator agreement (especially
as we crowdsourced the sense annotation, as de-
scribed below) and also sense coverage. The
other obvious candidate sense inventory which po-
tentially satisfied these criteria was ONTONOTES
(Hovy et al., 2006), but a preliminary sense-
tagging exercise indicated that MACMILLAN bet-
ter captured Twitter-specific usages.
Rather than annotating all words, we opted for
a lexical sample of 20 polysemous nouns, as listed
in Table 1. Our target nouns were selected to span
the high- to mid-frequency range in both Twitter
and the web corpus, and have at least 3 MACMIL-
LAN senses. The average sense ambiguity is 5.5.
1
http://www.macmillandictionary.com
band bar case charge deal
degree field form function issue
job light match panel paper
position post rule sign track
Table 1: The 20 target nouns used in this research
3.1 Data Sampling
We sampled tweets from a crawl made using the
Twitter Streaming API from January 3, 2012 to
February 29, 2012. The web corpus was built from
ukWaC (Ferraresi et al., 2008), which was based
on a crawl of the .uk domain from 2007. In con-
trast to ukWaC, the tweets are not restricted to doc-
uments from any particular country.
For both corpora, we first selected only the
English documents using langid.py, an off-the-
shelf language identification tool (Lui and Bald-
win, 2012). We next identified documents which
contained nominal usages of the target words,
based on the POS tags supplied with the corpus
in the case of ukWaC, and the output of the CMU
ARK Twitter POS tagger v2.0 (Owoputi et al.,
2012) in the case of Twitter.
For Twitter, we are interested in not just the
overall lexical distribution of each target noun,
but also per-user lexical distributions. As such,
we construct two Twitter-based datasets: (1)
TWITTER
RAND
, a random sample of 100 usages of
each target noun; and (2) TWITTER
USER
, 5 usages
of each target noun from each member of a ran-
dom sample of 20 Twitter users. Naively select-
ing users for TWITTER
USER
without filtering re-
sulted in a preponderance of messages from ac-
counts that were clearly bots, e.g. from commer-
cial sites with a single post per item advertised for
sale, with artificially-skewed sense distributions.
In order to obtain a more natural set of messages
from ?real? people, we introduced a number of
user-level filters, including removing users who
posted the same message with different user men-
tions or hashtags, and users who used the target
nouns more than 50 times over a 2-week period.
From the remaining users, we randomly selected
20 users per target noun, resulting in 20 nouns ?
20 users ? 5 messages = 2000 messages.
For ukWaC, we similarly constructed two
datasets: (1) UKWAC
RAND
, a random sample
of 100 usages of each target noun; and (2)
UKWAC
DOC
, 5 usages of each target noun from 20
documents which contained that noun in at least
216
Figure 1: Screenshot of a sense annotation HIT for position
5 sentences. 5 such sentences were selected for
annotation, resulting in a total of 20 nouns ? 20
documents ? 5 sentences = 2000 sentences.
3.2 Annotation Settings
We sense-tagged each of the four datasets using
Amazon Mechanical Turk (AMT). Each Human
Intelligence Task (HIT) comprised 5 occurrences
of a given target noun, with the target noun high-
lighted in each. Sense definitions and an exam-
ple sentence (where available) were provided from
MACMILLAN. Turkers were free to select multi-
ple sense labels where applicable, in line with best
practice in sense labelling (Mihalcea et al., 2004).
We also provided an ?Other? sense option, in cases
where none of the MACMILLAN senses were ap-
plicable to the current usage of the target noun. A
screenshot of the annotation interface for a single
usage is provided in Figure 1.
Of the five sentences in each HIT, one was a
heldout example sentence for one of the senses of
the target noun, taken from MACMILLAN. This
gold-standard example was used exclusively for
quality assurance purposes, and used to filter the
annotations as follows:
1. Accept all HITs from Turkers whose gold-
standard tagging accuracy was ? 80%;
2. Reject all HITs from Turkers whose gold-
standard tagging accuracy was ? 20%;
3. Otherwise, accept single HITs with correct
gold-standard sense tags, or at least 2/4 (non-
gold-standard) annotations in common with
Turkers who correctly annotated the gold-
standard usage; reject any other HITs.
This style of quality assurance has been shown
to be successful for sense tagging tasks on AMT
(Bentivogli et al., 2011; Vuurens et al., 2011), and
resulted in us accepting around 95% of HITs.
In total, the annotation was made up of 500
HITs (= 2000/4 usages per HIT) for each of the
four datasets, each of which was annotated by
5 Turkers. Our analysis of sense distribution is
based on only those HITs which were accepted in
accordance with the above methodology, exclud-
ing the gold-standard items. We arrive at a single
sense label per usage by unweighted voting across
the annotations, allowing multiple votes from a
single Turker in the case of multiple sense annota-
tions. In this, the ?Other? sense label is considered
as a discrete sense label.
Relative to the majority sense, inter-annotator
agreement post-filtering was respectably high in
terms of Fleiss? kappa at ? = 0.64 for both
UKWAC
RAND
and UKWAC
DOC
. For TWITTER
USER
,
the agreement was actually higher at ? = 0.71, but
for TWITTER
RAND
it was much weaker, ? = 0.47.
All four datasets have been released for pub-
lic use: http://www.csse.unimelb.edu.au/
~
tim/etc/twitter_sense.tgz.
4 Analysis
In TWITTER
USER
, the proportion of users who used
a target noun with one sense across all 5 usages
ranged from 7/20 for form to 20/20 for degree, at
an average of 65%. That is, for 65% of users, a
given noun (with average polysemy = 5.5 senses)
is used with the same sense across 5 separate mes-
sages. For UKWAC
DOC
the proportion of docu-
ments with a single sense of a given target noun
217
Partition Agreement (%)
Gale et al. (1992) document 94.4
TWITTER
USER
user 95.4
TWITTER
USER
? 62.9
TWITTER
RAND
? 55.1
UKWAC
DOC
document 94.2
UKWAC
DOC
? 65.9
UKWAC
RAND
? 60.2
Table 2: Pairwise agreement for each dataset,
based on different partitions of the data (??? indi-
cates no partitioning, and exhaustive comparison)
across all usages ranged from 1/20 for case to
20/20 for band, at an average of 63%. As such,
the one sense per tweeter heuristic is at least as
strong as the one sense per discourse heuristic in
UKWAC
DOC
.
Looking back to the original work of Gale et
al. (1992), it is important to realise that their re-
ported agreement of 94% was calculated pairwise
between usages in a given document. When we
recalculate the agreement in TWITTER
USER
and
UKWAC
DOC
using this methodology, as detailed
in Table 2 (calculating pairwise agreement within
partitions of the data based on ?user? and ?docu-
ment?, respectively), we see that the numbers for
our datasets are very close to those of Gale et al.
on the basis of more than twice as many nouns,
and many more instances per noun. Moreover, the
one sense per tweeter trend again appears to be
slightly stronger than the one sense per discourse
heuristic in UKWAC
DOC
.
One possible interpretation of these results is
that they are due to a single predominant sense,
common to all users/documents rather than user-
specific predominant senses. To test this hy-
pothesis, we calculate the pairwise agreement for
TWITTER
USER
and UKWAC
DOC
across all anno-
tations (without partitioning on user/document),
and also for TWITTER
RAND
and UKWAC
RAND
.
The results are, once again, presented in Ta-
ble 2 (with partition indicated as ??? for the
respective datasets), and are substantially lower
in all cases (< 66%). This indicates that the
first sense preference varies considerably between
users/documents. Note that the agreement is
slightly lower for TWITTER
RAND
and UKWAC
RAND
simply because of the absence of the biasing effect
for users/documents.
Comparing TWITTER
RAND
and UKWAC
RAND
,
there were marked differences in first sense pref-
erences, with 8/20 of the target nouns having a
different first sense across the two corpora. One
surprising observation was that the sense distri-
butions in UKWAC
RAND
were in general more
skewed than in TWITTER
RAND
, with the entropy of
the sense distribution being lower (= more biased)
in UKWAC
RAND
for 15/20 of the target nouns.
All datasets included instances of ?Other?
senses (i.e. usages which didn?t conform to any
of the MACMILLAN senses), with the highest rel-
ative such occurrence being in TWITTER
RAND
at
12.3%, as compared to 6.6% for UKWAC
RAND
.
Interestingly, the number of such usages in
the user/document-biased datasets was around
half these numbers, at 7.4% and 3.6% for
TWITTER
USER
and UKWAC
DOC
, respectively.
5 Discussion
It is worthwhile speculating why Twitter users
would have such a strong tendency to use a given
word with only one sense. This could arise in
part due to patterns of user behaviour, in a given
Twitter account being used predominantly to com-
ment on a favourite sports team or political events,
and as such is domain-driven. Alternatively, it can
perhaps be explained by the ?reactive? nature of
Twitter, in that posts are often emotive responses
to happenings in a user?s life, and while different
things excite different individuals, a given individ-
ual will tend to be excited by events of similar
kinds. Clearly more research is required to test
these hypotheses.
One highly promising direction for this research
would be to overlay analysis of sense distributions
with analysis of user profiles (e.g. Bergsma et al.
(2013)), and test the impact of geospatial and soci-
olinguistic factors on sense preferences. We would
also like to consider the impact of time on the one
sense per tweeter heuristic, and consider whether
?one sense per Twitter conversation? also holds.
To summarise, we have investigated sense dis-
tributions in Twitter and a general web corpus,
over both a random sample of usages and a sample
of usages from a single user/document. We found
strong evidence for Twitter users to use a given
word with a single sense, and also that individual
first sense preferences differ between users, sug-
gesting that methods for determining first senses
on a per user basis could be valuable for lexical se-
mantic analysis of tweets. Furthermore, we found
that sense distributions in Twitter are overall less
skewed than in a web corpus.
218
References
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy so-
cial media text, how diffrnt social media sources?
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing (IJCNLP
2013), pages 356?364, Nagoya, Japan.
Luisa Bentivogli, Marcello Federico, Giovanni
Moretti, and Michael Paul. 2011. Getting expert
quality from the crowd for machine translation
evaluation. Proceedings of the MT Summmit,
13:521?528.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on Twitter. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL HLT 2013), pages
1010?1019, Atlanta, USA.
Paul Earle, Michelle Guy, Richard Buckmaster, Chris
Ostrum, Scott Horvath, and Amy Vaughan. 2010.
OMG earthquake! can Twitter improve earth-
quake response? Seismological Research Letters,
81(2):246?251.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint conference of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing
(ACL-IJCNLP 2009), pages 10?18, Singapore.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop:
Can we beat Google, pages 47?54, Marrakech, Mo-
rocco.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural
Language, pages 233?237.
Spandana Gella, Paul Cook, and Bo Han. 2013. Unsu-
pervised word usage similarity in social media texts.
In Proceedings of the Second Joint Conference on
Lexical and Computational Semantics (*SEM 2013),
pages 248?253, Atlanta, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57?60, New York City, USA.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), pages 290?299, Atlanta, USA.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of the
2005 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 419?
426, Vancouver, Canada.
Robert Krovetz. 1998. More than one sense per dis-
course. NEC Princeton NJ Labs., Research Memo-
randum.
Jey Han Lau, Nigel Collier, and Timothy Baldwin.
2012. On-line trend analysis with topic models:
#twitter trends detection topic model online. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
1519?1534, Mumbai, India.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012)
Demo Session, pages 25?30, Jeju, Republic of Ko-
rea.
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Workshop 2012 (ALTW 2012), pages
33?41, Dunedin, New Zealand.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
4(33):553?590.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 25?28, Barcelona,
Spain.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?
201, Atlanta, USA.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 30?35, Prague, Czech Republic.
219
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Machine Learning Department, Carnegie Mel-
lon University.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the HLT-NAACL
2004 Workshop: 2nd Workshop on Scalable Natu-
ral Language Understanding, pages 49?56, Boston,
USA.
Sasa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with appli-
cation to twitter. In Proceedings of Human Lan-
guage Technologies: The 11th Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL HLT 2010),
pages 181?189, Los Angeles, USA.
Jeroen Vuurens, Arjen P de Vries, and Carsten Eick-
hoff. 2011. How much spam can you take? an anal-
ysis of crowdsourcing results to increase accuracy.
In Proc. ACM SIGIR Workshop on Crowdsourcing
for Information Retrieval (CIR 2011), pages 21?26.
220
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259?270,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Word Sense Distributions, Detecting Unattested Senses and
Identifying Novel Senses Using Topic Models
Jey Han Lau,
?
Paul Cook,
?
Diana McCarthy,
?
Spandana Gella,
?
and Timothy Baldwin
?
? Dept of Philosophy, King?s College London
? Dept of Computing and Information Systems, The University of Melbourne
? University of Cambridge
jeyhan.lau@gmail.com, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net
Abstract
Unsupervised word sense disambiguation
(WSD) methods are an attractive approach
to all-words WSD due to their non-reliance
on expensive annotated data. Unsuper-
vised estimates of sense frequency have
been shown to be very useful for WSD due
to the skewed nature of word sense distri-
butions. This paper presents a fully unsu-
pervised topic modelling-based approach
to sense frequency estimation, which is
highly portable to different corpora and
sense inventories, in being applicable to
any part of speech, and not requiring a hi-
erarchical sense inventory, parsing or par-
allel text. We demonstrate the effective-
ness of the method over the tasks of pre-
dominant sense learning and sense distri-
bution acquisition, and also the novel tasks
of detecting senses which aren?t attested
in the corpus, and identifying novel senses
in the corpus which aren?t captured in the
sense inventory.
1 Introduction
The automatic determination of word sense infor-
mation has been a long-term pursuit of the NLP
community (Agirre and Edmonds, 2006; Navigli,
2009). Word sense distributions tend to be Zip-
fian, and as such, a simple but surprisingly high-
accuracy back-off heuristic for word sense dis-
ambiguation (WSD) is to tag each instance of a
given word with its predominant sense (McCarthy
et al, 2007). Such an approach requires knowl-
edge of predominant senses; however, word sense
distributions ? and predominant senses too ?
vary from corpus to corpus. Therefore, meth-
ods for automatically learning predominant senses
and sense distributions for specific corpora are re-
quired (Koeling et al, 2005; Lapata and Brew,
2004).
In this paper, we propose a method which uses
topic models to estimate word sense distributions.
This method is in principle applicable to all parts
of speech, and moreover does not require a parser,
a hierarchical sense representation or parallel text.
Topic models have been used for WSD in a num-
ber of studies (Boyd-Graber et al, 2007; Li et
al., 2010; Lau et al, 2012; Preiss and Stevenson,
2013; Cai et al, 2007; Knopp et al, 2013), but
our work extends significantly on this earlier work
in focusing on the acquisition of prior word sense
distributions (and predominant senses).
Because of domain differences and the skewed
nature of word sense distributions, it is often the
case that some senses in a sense inventory will
not be attested in a given corpus. A system ca-
pable of automatically finding such senses could
reduce ambiguity, particularly in domain adapta-
tion settings, while retaining rare but nevertheless
viable senses. We further propose a method for ap-
plying our sense distribution acquisition system to
the task of finding unattested senses ? i.e., senses
that are in the sense inventory but not attested in
a given corpus. In contrast to the previous work
of McCarthy et al (2004a) on this topic which
uses the sense ranking score from McCarthy et
al. (2004b) to remove low-frequency senses from
WordNet, we focus on finding senses that are unat-
tested in the corpus on the premise that, given ac-
curate disambiguation, rare senses in a corpus con-
tribute to correct interpretation.
Corpus instances of a word can also correspond
to senses that are not present in a given sense in-
ventory. This can be due to, for example, words
taking on new meanings over time (e.g. the rela-
259
tively recent senses of tablet and swipe related to
touchscreen computers) or domain-specific terms
not being included in a more general-purpose
sense inventory. A system for automatically iden-
tifying such novel senses ? i.e. senses that are
attested in the corpus but not in the sense inven-
tory ? would be a very valuable lexicographi-
cal tool for keeping sense inventories up-to-date
(Cook et al, 2013). We further propose an appli-
cation of our proposed method to the identification
of such novel senses. In contrast to McCarthy et al
(2004b), the use of topic models makes this possi-
ble, using topics as a proxy for sense (Brody and
Lapata, 2009; Yao and Durme, 2011; Lau et al,
2012). Earlier work on identifying novel senses
focused on individual tokens (Erk, 2006), whereas
our approach goes further in identifying groups of
tokens exhibiting the same novel sense.
2 Background and Related Work
There has been a considerable amount of research
on representing word senses and disambiguating
usages of words in context (WSD) as, in order
to produce computational systems that understand
and produce natural language, it is essential to
have a means of representing and disambiguat-
ing word sense. WSD algorithms require word
sense information to disambiguate token instances
of a given ambiguous word, e.g. in the form of
sense definitions (Lesk, 1986), semantic relation-
ships (Navigli and Velardi, 2005) or annotated
data (Zhong and Ng, 2010). One extremely use-
ful piece of information is the word sense prior
or expected word sense frequency distribution.
This is important because word sense distributions
are typically skewed (Kilgarriff, 2004), and sys-
tems do far better when they take bias into ac-
count (Agirre and Martinez, 2004).
Typically, word frequency distributions are esti-
mated with respect to a sense-tagged corpus such
as SemCor (Miller et al, 1993), a 220,000 word
corpus tagged with WordNet (Fellbaum, 1998)
senses. Due to the expense of hand tagging, and
sense distributions being sensitive to domain and
genre, there has been some work on trying to
estimate sense frequency information automati-
cally (McCarthy et al, 2004b; Chan and Ng, 2005;
Mohammad and Hirst, 2006; Chan and Ng, 2006).
Much of this work has been focused on ranking
word senses to find the predominant sense in a
given corpus (McCarthy et al, 2004b; Mohammad
and Hirst, 2006), which is a very powerful heuris-
tic approach to WSD. Most WSD systems rely upon
this heuristic for back-off in the absence of strong
contextual evidence (McCarthy et al, 2007). Mc-
Carthy et al (2004b) proposed a method which
relies on distributionally similar words (nearest
neighbours) associated with the target word in
an automatically acquired thesaurus (Lin, 1998).
The distributional similarity scores of the nearest
neighbours are associated with the respective tar-
get word senses using a WordNet similarity mea-
sure, such as those proposed by Jiang and Conrath
(1997) and Banerjee and Pedersen (2002). The
word senses are ranked based on these similar-
ity scores, and the most frequent sense is selected
for the corpus that the distributional similarity the-
saurus was trained over.
As well as sense ranking for predominant sense
acquisition, automatic estimates of sense fre-
quency distribution can be very useful for WSD
for training data sampling purposes (Agirre and
Martinez, 2004), entropy estimation (Jin et al,
2009), and prior probability estimates, all of which
can be integrated within a WSD system (Chan and
Ng, 2005; Chan and Ng, 2006; Lapata and Brew,
2004). Various approaches have been adopted,
such as normalizing sense ranking scores to ob-
tain a probability distribution (Jin et al, 2009), us-
ing subcategorisation information as an indication
of verb sense (Lapata and Brew, 2004) or alter-
natively using parallel text (Chan and Ng, 2005;
Chan and Ng, 2006; Agirre and Martinez, 2004).
The work of Boyd-Graber and Blei (2007) is
highly related in that it extends the method of Mc-
Carthy et al (2004b) to provide a generative model
which assumes the words in a given document are
generated according to the topic distribution ap-
propriate for that document. They then predict the
most likely sense for each word in the document
based on the topic distribution and the words in
context (?corroborators?), each of which, in turn,
depends on the document?s topic distribution. Us-
ing this approach, they get comparable results to
McCarthy et al when context is ignored (i.e. us-
ing a model with one topic), and at most a 1% im-
provement on SemCor when they use more topics
in order to take context into account. Since the
results do not improve on McCarthy et al as re-
gards sense distribution acquisition irrespective of
context, we will compare our model with that pro-
posed by McCarthy et al
260
Recent work on finding novel senses has tended
to focus on comparing diachronic corpora (Sagi
et al, 2009; Cook and Stevenson, 2010; Gulor-
dava and Baroni, 2011) and has also considered
topic models (Lau et al, 2012). In a similar vein,
Peirsman et al (2010) considered the identifica-
tion of words having a sense particular to one
language variety with respect to another (specif-
ically Belgian and Netherlandic Dutch). In con-
trast to these studies, we propose a model for com-
paring a corpus with a sense inventory. Carpuat
et al (2013) exploit parallel corpora to identify
words in domain-specific monolingual corpora
with previously-unseen translations; the method
we propose does not require parallel data.
3 Methodology
Our methodology is based on the WSI system
described in Lau et al (2012),
1
which has been
shown (Lau et al, 2012; Lau et al, 2013a; Lau et
al., 2013b) to achieve state-of-the-art results over
the WSI tasks from SemEval-2007 (Agirre and
Soroa, 2007), SemEval-2010 (Manandhar et al,
2010) and SemEval-2013 (Navigli and Vannella,
2013; Jurgens and Klapaftis, 2013). The system
is built around a Hierarchical Dirichlet Process
(HDP: Teh et al (2006)), a non-parametric variant
of a Latent Dirichlet Allocation topic model (Blei
et al, 2003) where the model automatically opti-
mises the number of topics in a fully-unsupervised
fashion over the training data.
To learn the senses of a target lemma, we train
a single topic model per target lemma. The sys-
tem reads in a collection of usages of that lemma,
and automatically induces topics (= senses) in the
form of a multinomial distribution over words, and
per-usage topic assignments (= probabilistic sense
assignments) in the form of a multinomial distri-
bution over topics. Following Lau et al (2012),
we assign one topic to each usage by selecting the
topic that has the highest cumulative probability
density, based on the topic allocations of all words
in the context window for that usage.
2
Note that in
their original work, Lau et al (2012) experimented
with the use of features extracted from a depen-
dency parser. Due to the computational overhead
associated with these features, and the fact that the
empirical impact of the features was found to be
1
Based on the implementation available at: https://
github.com/jhlau/hdp-wsi
2
This includes all words in the usage sentence except
stopwords, which were filtered in the preprocessing step.
marginal, we make no use of parser-based features
in this paper.
3
The induced topics take the form of word multi-
nomials, and are often represented by the top-N
words in descending order of conditional probabil-
ity. We interpret each topic as a sense of the target
lemma.
4
To illustrate this, we give the example of
topics induced by the HDP model for network in
Table 1.
We refer to this method as HDP-WSI hence-
forth.
5
In predominant sense acquisition, the task is to
learn, for each target lemma, the most frequently
occurring word sense in a particular domain or
corpus, relative to a predefined sense inventory.
The WSI system provides us with a topic alloca-
tion per usage of a given word, from which we can
derive a distribution of topics over usages and a
predominant topic. In order to map this onto the
predominant sense, we need to have some way of
aligning a topic with a sense. We design our topic?
sense alignment methodology with portability in
mind ? it should be applicable to any sense in-
ventory. As such, our alignment methodology as-
sumes only that we have access to a conventional
sense gloss or definition for each sense, and does
not rely on ontological/structural knowledge (e.g.
the WordNet hierarchy).
To compute the similarity between a sense
and a topic, we first convert the words in the
gloss/definition into a multinomial distribution
over words, based on simple maximum likeli-
hood estimation.
6
We then calculate the Jensen?
Shannon divergence between the multinomial dis-
tribution (over words) of the gloss and that of the
topic, and convert the divergence value into a sim-
ilarity score by subtracting it from 1. Formally, the
similarity sense s
i
and topic t
j
is:
sim(s
i
, t
j
) = 1? JS(S?T ) (1)
where S and T are the multinomial distributions
3
For hyper-parameters ? and ?, we used 0.1 for both. We
did not tune the parameters, and opted to use the default pa-
rameters introduced in Teh et al (2006).
4
To avoid confusion, we will refer to the HDP-induced
topics as topics, and reserve the term sense to denote senses
in a sense inventory.
5
The code used to learn predominant sense and run all
experiments described in this paper is available at: https:
//github.com/jhlau/predom_sense.
6
Words are tokenised using OpenNLP and lemmatised
with Morpha (Minnen et al, 2001). We additionally remove
the target lemma, stopwords and words that are less than 3
characters in length.
261
Topic Num Top-10 Terms
1 network support @card@ information research service group development community member
2 service @card@ road company transport rail area government network public
3 network social model system family structure analysis form relationship neural
4 network @card@ computer system service user access internet datum server
5 system network management software support corp company service application product
6 @card@ radio news television show bbc programme call think film
7 police drug criminal terrorist intelligence network vodafone iraq attack cell
8 network atm manager performance craigavon group conference working modelling assistant
9 root panos comenius etd unipalm lse brazil telephone xxx discuss
Table 1: An example to illustrate the topics induced for network by the HDP model. The top-10 highest
probability terms are displayed to represent each topic (@card@ denotes a tokenised cardinal number).
over words for sense s
i
and topic t
j
, respectively,
and JS(X?Y ) is the Jensen?Shannon divergence
for distribution X and Y .
To learn the predominant sense, we compute the
prevalence score of each sense and take the sense
with the highest prevalence score as the predom-
inant sense. The prevalence score for a sense is
computed by summing the product of its similar-
ity scores with each topic (i.e. sim(s
i
, t
j
)) and the
prior probability of the topic in question (based
on maximum likelihood estimation). Formally, the
prevalence score of sense s
i
is given as follows:
prevalence(s
i
) =
T
?
j
(sim(s
i
, t
j
)? P (t
j
)) (2)
=
T
?
j
(
sim(s
i
, t
j
)?
f(t
j
)
?
T
k
f(t
k
)
)
where f(t
j
) is the frequency of topic t
j
(i.e. the
number of usages assigned to topic t
j
), and T is
the number of topics.
The intuition behind the approach is that the
predominant sense should be the sense that has rel-
atively high similarity (in terms of lexical overlap)
with high-probability topic(s).
4 WordNet Experiments
We first test the proposed method over the tasks
of predominant sense learning and sense distribu-
tion induction, using the WordNet-tagged dataset
of Koeling et al (2005), which is made up of
3 collections of documents: a domain-neutral
corpus (BNC), and two domain-specific corpora
(SPORTS and FINANCE). For each domain,
annotators were asked to sense-annotate a ran-
dom selection of sentences for each of 40 target
nouns, based on WordNet v1.7. The predominant
sense and distribution across senses for each target
lemma was obtained by aggregating over the sense
annotations. The authors evaluated their method in
terms of WSD accuracy over a given corpus, based
on assigning all instances of a target word with the
predominant sense learned from that corpus. For
the remainder of the paper, we denote their system
as MKWC.
To compare our system (HDP-WSI) with
MKWC, we apply it to the three datasets of Koel-
ing et al (2005). For each dataset, we use HDP
to induce topics for each target lemma, compute
the similarity between the topics and the WordNet
senses (Equation (1)), and rank the senses based
on the prevalence scores (Equation (2)). In addi-
tion to the WSD accuracy based on the predomi-
nant sense inferred from a particular corpus, we
additionally compute: (1) Acc
UB
, the upper bound
for the first sense-based WSD accuracy (using the
gold standard predominant sense for disambigua-
tion);
7
and (2) ERR, the error rate reduction be-
tween the accuracy for a given system (Acc) and
the upper bound (Acc
UB
), calculated as follows:
ERR = 1?
Acc
UB
? Acc
Acc
UB
Looking at the results in Table 2, we see lit-
tle difference in the results for the two methods,
with MKWC performing better over two of the
datasets (BNC and SPORTS) and HDP-WSI per-
forming better over the third (FINANCE), but all
differences are small. Based on the McNemar?s
Test with Yates correction for continuity, MKWC
is significantly better over BNC and HDP-WSI is
significantly better over FINANCE (p < 0.0001
in both cases), but the difference over SPORTS
is not statistically significance (p > 0.1). Note
that there is still much room for improvement with
7
The upper bound for a WSD approach which tags all to-
ken occurrences of a given word with the same sense, as a
first step towards context-sensitive unsupervised WSD.
262
Dataset
FS
CORPUS
MKWC HDP-WSI
Acc
UB
Acc ERR Acc ERR
BNC 0.524 0.407 (0.777) 0.376 (0.718)
FINANCE 0.801 0.499 (0.623) 0.555 (0.693)
SPORTS 0.774 0.437 (0.565) 0.422 (0.545)
Table 2: WSD accuracy for MKWC and HDP-WSI
on the WordNet-annotated datasets, as compared
to the upper-bound based on actual first sense in
the corpus (higher values indicate better perfor-
mance; the best system in each row [other than the
FS
CORPUS
upper bound] is indicated in boldface).
Dataset MKWC HDP-WSI
BNC 0.226 0.214
FINANCE 0.426 0.375
SPORTS 0.420 0.363
Table 3: Sense distribution evaluation of MKWC
and HDP-WSI on the WordNet-annotated datasets,
evaluated using JS divergence (lower values indi-
cate better performance; the best system in each
row is indicated in boldface).
both systems, as we see in the gap between the up-
per bound (based on perfect determination of the
first sense) and the respective system accuracies.
Given that both systems compute a continuous-
valued prevalence score for each sense of a tar-
get lemma, a distribution of senses can be ob-
tained by normalising the prevalence scores across
all senses. The predominant sense learning task
of McCarthy et al (2007) evaluates the ability of
a method to identify only the head of this dis-
tribution, but it is also important to evaluate the
full sense distribution (Jin et al, 2009). To this
end, we introduce a second evaluation metric:
the Jensen?Shannon (JS) divergence between the
inferred sense distribution and the gold-standard
sense distribution, noting that smaller values are
better in this case, and that it is now theoretically
possible to obtain a JS divergence of 0 in the case
of a perfect estimate of the sense distribution. Re-
sults are presented in Table 3.
HDP-WSI consistently achieves lower JS diver-
gence, indicating that the distribution of senses
that it finds is closer to the gold standard distri-
bution. Testing for statistical significance over the
paired JS divergence values for each lemma using
the Wilcoxon signed-rank test, the result for FI-
NANCE is significant (p < 0.05) but the results
for the other two datasets are not (p > 0.1 in each
case).
Dataset
FS
CORPUS
FS
DICT
HDP-WSI
Acc
UB
Acc ERR Acc ERR
UKWAC 0.574 0.387 (0.674) 0.514 (0.895)
TWITTER 0.468 0.297 (0.635) 0.335 (0.716)
Table 4: WSD accuracy for HDP-WSI on the
Macmillan-annotated datasets, as compared to the
upper-bound based on actual first sense in the cor-
pus (higher values indicate better performance; the
best system in each row [other than the FS
CORPUS
upper bound] is indicated in boldface).
Dataset FS
CORPUS
FS
DICT
HDP-WSI
UKWAC 0.210 0.393 0.156
TWITTER 0.259 0.472 0.171
Table 5: Sense distribution evaluation of HDP-
WSI on the Macmillan-annotated datasets as com-
pared to corpus- and dictionary-based first sense
methods, evaluated using JS divergence (lower
values indicate better performance; the best sys-
tem in each row is indicated in boldface).
To summarise, the results for MKWC and HDP-
WSI are fairly even for predominant sense learn-
ing (each outperforms the other at a level of statis-
tical significance over one dataset), but HDP-WSI
is better at inducing the overall sense distribution.
It is important to bear in mind that MKWC in
these experiments makes use of full-text parsing in
calculating the distributional similarity thesaurus,
and the WordNet graph structure in calculating the
similarity between associated words and different
senses. Our method, on the other hand, uses no
parsing, and only the synset definitions (and not
the graph structure) of WordNet.
8
The non-reliance
on parsing is significant in terms of portability to
text sources which are less amenable to parsing
(such as Twitter: (Baldwin et al, 2013)), and the
non-reliance on the graph structure of WordNet is
significant in terms of portability to conventional
?flat? sense inventories. While comparable results
on a different dataset have been achieved with a
proximity thesaurus (McCarthy et al, 2007) com-
pared to a dependency one,
9
it is not stated how
8
McCarthy et al (2004b) obtained good results with def-
inition overlap, but their implementation uses the relation
structure alongside the definitions (Banerjee and Pedersen,
2002). Iida et al (2008) demonstrate that further exten-
sions using distributional data are required when applying the
method to resources without hierarchical relations.
9
The thesauri used in the reimplementation of MKWC
in this paper were obtained from http://webdocs.cs.
ualberta.ca/
?
lindek/downloads.htm.
263
wide a window is needed for the proximity the-
saurus. This could be a significant issue with Twit-
ter data, where context tends to be limited. In the
next section, we demonstrate the robustness of the
method in experimenting with two new datasets,
based on Twitter and a web corpus, and the Macmil-
lan English Dictionary.
5 Macmillan Experiments
In our second set of experiments, we move to a
new dataset (Gella et al, to appear) based on text
from ukWaC (Ferraresi et al, 2008) and Twit-
ter, and annotated using the Macmillan English Dic-
tionary
10
(henceforth ?Macmillan?). For the pur-
poses of this research, the choice of Macmillan is
significant in that it is a conventional dictionary
with sense definitions and examples, but no link-
ing between senses.
11
In terms of the original re-
search which gave rise to the sense-tagged dataset,
Macmillan was chosen over WordNet for reasons in-
cluding: (1) the well-documented difficulties of
sense tagging with fine-grained WordNet senses
(Palmer et al, 2004; Navigli et al, 2007); (2) the
regular update cycle of Macmillan (meaning it con-
tains many recently-emerged senses); and (3) the
finding in a preliminary sense-tagging task that it
better captured Twitter usages than WordNet (and
also OntoNotes: Hovy et al (2006)).
The dataset is made up of 20 target nouns which
were selected to span the high- to mid-frequency
range in both Twitter and the ukWaC corpus, and
have at least 3 Macmillan senses. The average sense
ambiguity of the 20 target nouns in Macmillan is 5.6
(but 12.3 in WordNet). 100 usages of each target
noun were sampled from each of Twitter (from a
crawl over the time period Jan 3?Feb 28, 2013 us-
ing the Twitter Streaming API) and ukWaC, after
language identification using langid.py (Lui
and Baldwin, 2012) and POS tagging (based on
the CMU ARK Twitter POS tagger v2.0 (Owoputi
et al, 2012) for Twitter, and the POS tags provided
with the corpus for ukWaC). Amazon Mechani-
cal Turk (AMT) was then used to 5-way sense-tag
each usage relative to Macmillan, including allow-
ing the annotators the option to label a usage as
?Other? in instances where the usage was not cap-
tured by any of the Macmillan senses. After qual-
ity control over the annotators/annotations (see
10
http://www.macmillandictionary.com/
11
Strictly speaking, there is limited linking in the form of
sets of synonyms in Macmillan, but we choose to not use this
information in our research.
Gella et al (to appear) for details), and aggregation
of the annotations into a single sense per usage
(possibly ?Other?), there were 2000 sense-tagged
ukWaC sentences and Twitter messages over the
20 target nouns. We refer to these two datasets as
UKWAC and TWITTER henceforth.
To apply our method to the two datasets, we use
HDP-WSI to train a model for each target noun,
based on the combined set of usages of that lemma
in each of the two background corpora, namely the
original Twitter crawl that gave rise to the TWIT-
TER dataset, and all of ukWaC.
5.1 Learning Sense Distributions
As in Section 4, we evaluate in terms of WSD
accuracy (Table 4) and JS divergence over the
gold-standard sense distribution (Table 5). We
also present the results for: (a) a supervised base-
line (?FS
CORPUS
?), based on the most frequent
sense in the corpus; and (b) an unsupervised base-
line (?FS
DICT
?), based on the first-listed sense in
Macmillan. In each case, the sense distribution is
based on allocating all probability mass for a given
word to the single sense identified by the respec-
tive method.
We first notice that, despite the coarser-grained
senses of Macmillan as compared to WordNet, the
upper bound WSD accuracy using Macmillan is
comparable to that of the WordNet-based datasets
over the balanced BNC, and quite a bit lower than
that of the two domain corpora of Koeling et al
(2005). This suggests that both datasets are di-
verse in domain and content.
In terms of WSD accuracy, the results over
UKWAC (ERR = 0.895) are substantially higher
than those for BNC, while those over TWITTER
(ERR = 0.716) are comparable. The accuracy is
significantly higher than the dictionary-based first
sense baseline (FS
DICT
) over both datasets (McNe-
mar?s test; p < 0.0001), and the ERR is also con-
siderably higher than for the two domain datasets
in Section 4 (FINANCE and SPORTS). One
cause of difficulty in sense-modelling TWITTER
is large numbers of missing senses, with 12.3%
of usages in TWITTER and 6.6% in UKWAC hav-
ing no corresponding Macmillan sense.
12
This chal-
lenges the assumption built into the sense preva-
lence calculation that all topics will align to a pre-
existing sense, a point we return to in Section 5.2.
12
The relative occurrence of unlisted/unclear senses in the
datasets of Koeling et al (2005) is comparable to UKWAC.
264
Dataset P R F
UKWAC 0.73 0.85 0.74
TWITTER 0.56 0.88 0.65
Table 6: Evaluation of our method for identify-
ing unattested senses, averaged over 10 runs of 10-
fold cross validation
The JS divergence results for both datasets are
well below (= better than) the results for all three
WordNet-based datasets, and also superior to both
the supervised and unsupervised first-sense base-
lines. Part of the reason for this improvement is
simply that the average polysemy in Macmillan (5.6
senses per target lemma) is slightly less than in
WordNet (6.7 senses per target lemma),
13
making
the task slightly easier in the Macmillan case.
5.2 Identification of Unattested Senses
We observed in Section 5.1 that there are rela-
tively frequent occurrences of usages (e.g. 12.3%
for TWITTER) which aren?t captured by Macmil-
lan. Conversely, there are also senses in Macmillan
which aren?t attested in the annotated sample of
usages. Specifically, of the 112 senses defined for
the 20 target lemmas, 25 (= 22.3%) of the senses
are not attested in the 2000 usages in either cor-
pora. Given that our methodology computes a
prevalence score for each sense, it can equally be
applied to the detection of these unattested senses,
and it is this task that we address in this section:
the identification of senses that are defined in the
sense inventory but not attested in a given corpus.
Intuitively, an unused sense should have low
similarity with the HDP induced topics. As such,
we introduce sense-to-topic affinity, a measure
that estimates how likely a sense is not attested in
the corpus:
st-affinity(s
i
) =
?
T
j
sim(s
i
, t
j
)
?
S
k
?
T
l
sim(s
k
, t
l
)
(3)
where sim(s
i
, t
j
) is carried over from Equa-
tion (1), and T and S represent the number of top-
ics and senses, respectively.
We treat the task of identification of unused
senses as a binary classification problem, where
the goal is to find a sense-to-topic affinity thresh-
old below which a sense will be considered to
13
Note that the set of lemmas differs between the respec-
tive datasets, so this isn?t an accurate reflection of the relative
granularity of the two dictionaries.
be unused. We pool together all the senses and
run 10-fold cross validation to learn the threshold
for identifying unused senses,
14
evaluated using
sense-level precision (P ), recall (R) and F-score
(F ) at detecting unattested senses. We repeat the
experiment 10 times (partitioning the items ran-
domly into folds) and collect the mean precision,
recall and F-scores across the 10 runs. We found
encouraging results for the task, as detailed in Ta-
ble 6. For the threshold, the average value with
standard deviation is 0.092? 0.044 over UKWAC
and 0.125?0.052 over TWITTER, indicating rela-
tive stability in the value of the threshold both in-
ternally within a dataset, and also across datasets.
5.3 Identification of Novel Senses
In both TWITTER and UKWAC, we observed fre-
quent occurrences of usages of our target nouns
which didn?t map onto a pre-existing Macmillan
sense. A natural question to ask is whether our
method can be used to predict word senses that are
missing from our sense inventory, and identify us-
ages associated with each such missing sense. We
will term these ?novel senses?, and define ?novel
sense identification? to be the task of identifying
new senses that are not recorded in the inventory
but are seen in the corpus.
An immediate complication in evaluating novel
sense identification is that we are attempting to
identify senses which explicitly aren?t in our sense
inventory. This contrasts with the identification of
unattested senses, e.g., where we were attempting
to identify which of the known senses wasn?t ob-
served in the corpus. Also, while we have annota-
tions of ?Other? usages in TWITTER and UKWAC,
there is no real expectation that all such usages
will correspond to the same sense: in practice,
they are attributable to a myriad of effects such as
incorporation in a non-compositional multiword
expression, and errors in POS tagging (i.e. the us-
age not being nominal). As such, we can?t use the
?Other? annotations to evaluate novel sense iden-
tification. The evaluation of systems for this task
is a known challenge, which we address similarly
to Erk (2006) by artificially synthesising novel
senses through removal of senses from the sense
inventory. In this way, even if we remove multi-
ple senses for a given word, we still have access
to information about which usages correspond to
14
We used a fixed step and increment at steps of 0.001, up
to the max value of st-affinity when optimising the threshold.
265
No. Lemmas with Relative Freq Threshold
P R F
a Removed Sense of Removed Sense Mean?stdev
20 0.0?0.2 0.052?0.009 0.35 0.42 0.36
9 0.2?0.4 0.089?0.024 0.24 0.59 0.29
6 0.4?0.6 0.061?0.004 0.63 0.64 0.63
Table 7: Classification of usages with novel sense for all target lemmas.
No. Lemmas with Relative Freq Threshold
P R F
a Removed Sense of Removed Sense Mean?stdev
9 0.2?0.4 0.093?0.023 0.50 0.66 0.52
6 0.4?0.6 0.099?0.018 0.73 0.90 0.80
Table 8: Classification of usages with novel sense for target lemmas with a removed sense.
which novel sense. An additional advantage of
this procedure is that it allows us to control an im-
portant property of novel senses: their frequency
of occurrence.
In the experiments that follow, we randomly
select senses for removal from three frequency
bands: low, medium and high frequency senses.
Frequency is defined by relative occurrence in the
annotated usages: low = 0.0?0.2; medium = 0.2?
0.4; and high = 0.4?0.6. Note that we do not con-
sider high-frequency senses with frequency higher
than 0.6, as it is rare for a medium- to high-
frequency word to take on a novel sense which
is then the predominant sense in a given corpus.
Note also that not all target lemmas will have a
novel sense through synthesis, as they may have
no senses that fall within the indicated bounds of
relative occurrence (e.g. if > 60% of usages are a
single sense). For example, only 6 of our 20 target
nouns have senses which are candidates for high-
frequency novel senses.
As before, we treat the novel sense identifica-
tion task as a classification problem, although with
a significantly different formulation: we are no
longer attempting to identify pre-existing senses,
as novel senses are by definition not included in
the sense inventory. Instead, we are seeking to
identify clusters of usages which are instances of
a novel sense, e.g. for presentation to a lexicogra-
pher as part of a dictionary update process (Run-
dell and Kilgarriff, 2011; Cook et al, 2013). That
is, for each usage, we want to classify whether it
is an instance of a given novel sense.
A usage that corresponds to a novel sense
should have a topic that does not align well with
any of the pre-existing senses in the sense inven-
tory. Based on this intuition, we introduce topic-
to-sense affinity to estimate the similarity of a
topic to the set of senses, as follows:
ts-affinity(t
j
) =
?
S
i
sim(s
i
, t
j
)
?
T
l
?
S
k
sim(s
k
, t
l
)
(4)
where, once again, sim(s
i
, t
j
) is defined as in
Equation (1), and T and S represent the number
of topics and senses, respectively.
Using topic-to-sense affinity as the sole fea-
ture, we pool together all instances and optimise
the affinity feature to classify instances that have
novel senses. Evaluation is done by computing the
mean precision, recall and F-score across 10 sepa-
rate runs; results are summarised in Table 7. Note
that we evaluate only over UKWAC in this section,
for ease of presentation.
The results show that instances with high-
frequency novel senses are more easily identifi-
able than instances with medium/low-frequency
novel senses. This is unsurprising given that high-
frequency senses have a higher probability of gen-
erating related topics (sense-related words are ob-
served more frequently in the corpus), and as such
are more easily identifiable.
We are interested in understanding whether
pooling all instances ? instances from target lem-
mas that have a sense artificially removed and
those that do not ? impacted the results (re-
call that not all target lemmas have a removed
sense). To that end, we chose to include only
instances from lemmas with a removed sense,
and repeated the experiment for the medium- and
high-frequency novel sense condition (for the low-
frequency condition, all target lemmas have a
novel sense). In other words, we are assuming
knowledge of which words have novel sense, and
the task is to identify specifically what the novel
sense is, as represented by novel usages. Results
are presented in Table 8.
266
No. of Lemmas with No. of Lemmas without Relative Freq Wilcoxon Rank Sum
a Removed Sense a Removed Sense of Removed Sense p-value
10 0 0.0?0.2 0.4543
9 11 0.2?0.4 0.0391
6 14 0.4?0.6 0.0247
Table 9: Wilcoxon Rank Sum p-value results for testing target lemmas with removed sense vs. target
lemmas without removed sense using novelty.
From the results, we see that the F-scores im-
proved notably. This reveals that an additional step
is necessary to determine whether a target lemma
has a potential novel sense before feeding its in-
stances to learn which of them contains the usage
of the novel sense.
In the last experiment, we propose a new mea-
sure to tackle this: the identification of target lem-
mas that have a novel sense. We introduce novelty,
a measure of the likelihood of a target lemma w
having a novel sense:
novelty(w) = min
t
j
(
max
s
i
sim(s
i
, t
j
)
f(t
j
)
)
(5)
where f(t
j
) is the frequency of topic t
j
in the
corpus. The intuition behind novelty is that a
target lemma with a novel sense should have a
(somewhat-)frequent topic that has low associa-
tion with any sense. That we use the frequency
rather than the probability of the topic here is de-
liberate, as topics with a higher raw number of oc-
currences (whether as a low-probability topic for
a high-frequency word, or a high-probability topic
for a low-frequency word) are indicative of a novel
word sense.
For each of our three datasets (with low-,
medium- and high-frequency novel senses, respec-
tively), we compute the novelty of the target lem-
mas and the p-value of a one-tailed Wilcoxon rank
sum test to test if the two groups of lemmas (i.e.
lemmas with a novel sense vs. lemmas without a
novel sense) are statistically different.
15
Results
are presented in Table 9. We see that the nov-
elty measure can readily identify target lemmas
with high- and medium-frequency novel senses
(p < 0.05), but the results are less promising for
the low-frequency novel senses.
6 Discussion
Our methodologies for the two proposed tasks of
identifying unused and novel senses are simple
15
Note that the number of words with low-frequency novel
senses here is restricted to 10 (cf. 20 in Table 7) to ensure we
have both positive and negative lemmas in the dataset.
extensions to demonstrate the flexibility and ro-
bustness of our methodology. Future work could
pursue a more sophisticated methodology, using
non-linear combinations of sim(s
i
, t
j
) for com-
puting the affinity measures or multiple features
in a supervised context. We contend, however,
that these extensions are ultimately a preliminary
demonstration to the flexibility and robustness of
our methodology.
A natural next step for this research would be to
couple sense distribution estimation and the detec-
tion of unattested senses with evidence from the
context, using topics or other information about
the local context (e.g. Agirre and Soroa (2009))
to carry out unsupervised WSD of individual token
occurrences of a given word.
In summary, we have proposed a topic
modelling-based method for estimating word
sense distributions, based on Hierarchical Dirich-
let Processes and the earlier work of Lau et al
(2012) on word sense induction, in probabilisti-
cally mapping the automatically-learned topics to
senses in a sense inventory. We evaluated the abil-
ity of the method to learn predominant senses and
induce word sense distributions, based on a broad
range of datasets and two separate sense invento-
ries. In doing so, we established that our method
is comparable to the approach of McCarthy et al
(2007) at predominant sense learning, and supe-
rior at inducing word sense distributions. We fur-
ther demonstrated the applicability of the method
to the novel tasks of detecting word senses which
are unattested in a corpus, and identifying novel
senses which are found in a corpus but not cap-
tured in a word sense inventory.
Acknowledgements
We wish to thank the anonymous reviewers for
their valuable comments. This research was sup-
ported in part by funding from the Australian Re-
search Council.
267
References
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer, Dordrecht, Netherlands.
Eneko Agirre and David Martinez. 2004. Unsuper-
vised WSD based on automatically retrieved exam-
ples: The importance of bias. In Proceedings of
EMNLP 2004, pages 25?32, Barcelona, Spain.
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 7?12, Prague, Czech Republic.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the EACL (EACL
2009), pages 33?41, Athens, Greece.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy so-
cial media text, how diffrnt social media sources?
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing (IJCNLP
2013), pages 356?364, Nagoya, Japan.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using WordNet. In Proceedings of the 3rd In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing-2002),
pages 136?145, Mexico City, Mexico.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David Blei. 2007. Putop:
Turning predominant senses into a topic model for
word sense disambiguation. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 277?281, Prague, Czech Re-
public.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Proc. of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024?1033, Prague, Czech
Republic.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103?
111, Athens, Greece.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh.
2007. NUS-ML: Improving word sense disam-
biguation using topic features. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 249?252, Prague, Czech Re-
public.
Marine Carpuat, Hal Daum?e III, Katharine Henry,
Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. SenseSpotting: Never let your par-
allel data tie you to an old domain. In Proc. of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013), pages 1435?1445,
Sofia, Bulgaria.
Yee Seng Chan and Hwee Tou Ng. 2005. Word
sense disambiguation with distribution estimation.
In Proc. of the 19th International Joint Conference
on Artificial Intelligence (IJCAI 2005), pages 1010?
1015, Edinburgh, UK.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proc. of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 89?96, Sydney, Australia.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally identifying changes in the semantic orientation
of words. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC 2010), pages 28?34, Valletta, Malta.
Paul Cook, Jey Han Lau, Michael Rundell, Diana Mc-
Carthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for de-
tecting new word senses. In Proceedings of eLex
2013, pages 49?65, Tallinn, Estonia.
Katrin Erk. 2006. Unknown word sense detection as
outlier detection. In Proc. of the Main Conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 128?135, New York
City, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proc. of the 4th Web as Corpus Workshop: Can
we beat Google, pages 47?54, Marrakech, Morocco.
Spandana Gella, Paul Cook, and Timothy Baldwin. to
appear. One sense per tweeter ... and other lexical
semantic tales of Twitter. In Proceedings of the 14th
Conference of the EACL (EACL 2014), Gothenburg,
Sweden.
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the Google Books Ngram corpus.
In Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67?71, Edinburgh, UK.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
268
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57?60, New York City, USA.
Ryu Iida, Diana McCarthy, and Rob Koeling. 2008.
Gloss-based semantic similarity metrics for predom-
inant sense acquisition. In Proc. of the Third In-
ternational Joint Conference on Natural Language
Processing, pages 561?568.
Jay Jiang and David Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings on International Conference on Re-
search in Computational Linguistics, pages 19?33,
Taipei, Taiwan.
Peng Jin, Diana McCarthy, Rob Koeling, and John Car-
roll. 2009. Estimating and exploiting the entropy
of sense distributions. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics ? Human Language Technologies
2009 (NAACL HLT 2009): Short Papers, pages 233?
236, Boulder, USA.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), pages 290?299, Atlanta, USA.
Adam Kilgarriff. 2004. How dominant is the common-
est sense of a word? Technical Report ITRI-04-10,
Information Technology Research Institute, Univer-
sity of Brighton.
Johannes Knopp, Johanna V?olker, and Simone Paolo
Ponzetto. 2013. Topic modeling for word sense in-
duction. In Proc. of the International Conference of
the German Society for Computational Linguistics
and Language Technology, pages 97?103, Darm-
stadt, Germany.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of the
2005 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 419?
426, Vancouver, Canada.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45?75.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings
of the 13th Conference of the EACL (EACL 2012),
pages 591?601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a.
unimelb: Topic modelling-based word sense induc-
tion. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013), pages
307?311, Atlanta, USA.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b.
unimelb: Topic modelling-based word sense induc-
tion for web snippet clustering. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 217?221, Atlanta, USA.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 1986 SIGDOC Conference, pages 24?26, On-
tario, Canada.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proc. of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1138?1147, Uppsala,
Sweden.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the ACL and 17th International Confer-
ence on Computational Linguistics (COLING/ACL-
98), pages 768?774, Montreal, Canada.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012)
Demo Session, pages 25?30, Jeju, Republic of Ko-
rea.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task
14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 63?68, Uppsala, Swe-
den.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004a. Automatic identification of infre-
quent word senses. In Proc. of the 20th International
Conference of Computational Linguistics, COLING-
2004, pages 1220?1226, Geneva, Switzerland.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004b. Finding predominant senses in
untagged text. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2004), pages 280?287, Barcelona,
Spain.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
4(33):553?590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of the ARPA Workshop on Human Language
Technology, pages 303?308.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
269
Saif Mohammad and Graeme Hirst. 2006. Determin-
ing word sense dominance using a thesaurus. In
Proc. of EACL-2006, pages 121?128, Trento, Italy.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?
201, Atlanta, USA.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 27(7):1075?1088.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 30?35, Prague, Czech Republic.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Machine Learning Department, Carnegie Mel-
lon University.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the HLT-NAACL
2004 Workshop: 2nd Workshop on Scalable Natu-
ral Language Understanding, pages 49?56, Boston,
USA.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?491.
Judita Preiss and Mark Stevenson. 2013. Unsuper-
vised domain tuning to improve word sense dis-
ambiguation. In Proc. of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 680?684, Atlanta, USA.
Michael Rundell and Adam Kilgarriff. 2011. Au-
tomating the creation of dictionaries: where will
it all end? In Fanny Meunier, Sylvie De
Cock, Ga?etanelle Gilquin, and Magali Paquot, ed-
itors, A Taste for Corpora. In honour of Sylviane
Granger, pages 257?282. John Benjamins, Amster-
dam, Netherlands.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Asso-
ciation, 101:1566?1581.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric Bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14,
Portland, USA.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proc. of the ACL 2010 System
Demonstrations, pages 78?83, Uppsala, Sweden.
270
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 557?564,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DSS: Text Similarity Using Lexical Alignments of Form, Distributional
Semantics and Grammatical Relations
Diana McCarthy
Saarland University?
diana@dianamccarthy.co.uk
Spandana Gella
University of Malta
spandanagella@gmail.com
Siva Reddy
Lexical Computing Ltd.
siva@sivareddy.in
Abstract
In this paper we present our systems for the
STS task. Our systems are all based on a
simple process of identifying the components
that correspond between two sentences. Cur-
rently we use words (that is word forms), lem-
mas, distributional similar words and gram-
matical relations identified with a dependency
parser. We submitted three systems. All sys-
tems only use open class words. Our first sys-
tem (alignheuristic) tries to obtain a map-
ping between every open class token using all
the above sources of information. Our second
system (wordsim) uses a different algorithm
and unlike alignheuristic, it does not use
the dependency information. The third sys-
tem (average) simply takes the average of
the scores for each item from the other two
systems to take advantage of the merits of
both systems. For this reason we only pro-
vide a brief description of that. The results
are promising, with Pearson?s coefficients on
each individual dataset ranging from .3765
to .7761 for our relatively simple heuristics
based systems that do not require training on
different datasets. We provide some analy-
sis of the results and also provide results for
our data using Spearman?s, which as a non-
parametric measure which we argue is better
able to reflect the merits of the different sys-
tems (average is ranked between the others).
1 Introduction
Our motivation for the systems entered in the STS
task (Agirre et al, 2012) was to model the contribu-
? The first author is a visiting scholar on the Erasmus
Mundus Masters Program in ?Language and Communication
Technologies? (LCT, 2007?0060).
tion of each linguistic component of both sentences
to the similarity of the texts by finding an align-
ment. Ultimately such a system could be exploited
for ranking candidate paraphrases of a chunk of text
of any length. We envisage a system as outlined in
the future work section. The systems reported are
simple baselines to such a system. We have two
main systems (alignheuristic and wordsim) and
also a system which simply uses the average score
for each item from the two main systems (average).
In our systems we:
? only deal with open class words as to-
kens i.e. nouns, verbs, adjectives, adverbs.
alignheuristic and average also use num-
bers
? assume that tokens have a 1:1 mapping
? match:
? word forms
? lemmas
? distributionally similar lemmas
? (alignheuristic and average only) ar-
gument or head in a matched grammatical
relation with a word that already has a lex-
ical mapping
? score the sentence pair based on the size of the
overlap. Different formulations of the score are
used by our methods
The paper is structured as follows. In the next
section we make a brief mention of related work
though of course there will be more pertinent related
work presented and published at SemEval 2012. In
section 3 we give a detailed account of the systems
557
and in section 4 we provide the results obtained on
the training data on developing our systems. In sec-
tion 5 we present the results on the test data, along
with a little analysis using the gold standard data. In
section 6 we conclude our findings and discuss our
ideas for future work.
2 Related Work
Semantic textual similarity relates to textual entail-
ment (Dagan et al, 2005), lexical substitution (Mc-
Carthy and Navigli, 2009) and paraphrasing (Hirst,
2003). The key issue for semantic textual similarity
is that the task is to determine similarity, where sim-
ilarity is cast as meaning equivalence. 1 In textual
entailment the relation under question is the more
specific relation of entailment, where the meaning
of one sentence is entailed by another and a sys-
tem needs to determine the direction of the entail-
ment. Lexical substitution relates to semantic tex-
tual similarity though the task involves a lemma in
the context of a sentence, candidate substitutes are
not provided, and the relation at question in the task
is one of substitutability. 2 Paraphrase recognition
is a highly related task, for example using compa-
rable corpora (Barzilay and Elhadad, 2003), and it
is likely that semantic textual similarity measures
might be useful for ranking candidates in paraphrase
acquisition.
In addition to various works related to textual
entailment, lexical substitution and paraphrasing,
there has been some prior work explicitly on se-
mantic text similarity. Semantic textual similarity
has been explored in various works. Mihalcea et al
(2006) extend earlier work on word similarity us-
ing various WordNet similarity measures (Patward-
han et al, 2003) and a couple of corpus-based dis-
tributional measures: PMI-IR (Turney, 2002) and
LSA (Berry, 1992). They use a measure which
takes a summation over all tokens in both sen-
tences. For each token they find the maximum sim-
ilarity (WordNet or distributional) weighted by the
inverse document frequency of that word. The dis-
1See the guidelines given to the annotators at
http://www.cs.columbia.edu/?weiwei/workshop/
instructions.pdf
2This is more or less semantic equivalence since the an-
notators were instructed to focus on meaning http://www.
dianamccarthy.co.uk/files/instructions.pdf.
tributional similarity measures perform at a simi-
lar level to the knowledge-based measures that use
WordNet. Mohler and Mihalcea (2009) adapt this
work for automatic short answer grading, that is
matching a candidate answer to one supplied by
the tutor. Mohler et al (2011) take this applica-
tion forward, combining lexical semantic similarity
measures with a graph-alignment which considers
dependency graphs using the Stanford dependency
parser (de Marneffe et al, 2006) in terms of lexical,
semantic and syntactic features. A score is then pro-
vided for each node in the graph. The features are
combined using machine learning.
The systems we propose likewise use lexical sim-
ilarity and dependency relations, but in a simple
heuristic formulation without a man-made thesaurus
such as WordNet and without machine learning.
3 Systems
We lemmatize and part-of-speech tag the data using
TreeTagger (Schmid, 1994). We process the tagged
data with default settings of the Malt Parser (Nivre
et al, 2007) to dependency parse the data. All sys-
tems make use of a distributional thesaurus which
lists distributionally similar lemmas (?neighbours?)
for a given lemma. This is a thesaurus constructed
using log-dice (Rychly?, 2008) and UkWaC (Fer-
raresi et al, 2008). 3 Note that we use only the
top 20 neighbours for any word in all the methods
described below. We have not experimented with
varying this threshold.
In the following descriptions, we refer to our sen-
tences as s1 and s2 and these open classed tokens
within those sentences as ti ? s1 and t j ? s2 where
each token in either sentence is represented by a
word (w), lemma (l), part-of-speech (p) and gram-
matical relation (gr), identified by the Malt parser,
to its dependency head at a given position (hp) in
the same sentence.
3.1 alignheuristic
This method uses nouns, verbs, adjectives, adverbs
and numbers. The algorithm aligns words (w), or
lemmas (l) from left to right from s1 to s2 and vice
3This is the ukWaC distributional thesaurus avail-
able in Sketch Engine (Kilgarriff et al, 2004) at
http://the.sketchengine.co.uk/bonito/run.cgi/
first\_form?corpname=preloaded/ukwac2
558
versa (wmtch). If there is no alignment for words or
lemmas then it does the same matching process (s1
given s2 and vice versa) for distributionally similar
neighbours using the distributional thesaurus men-
tioned above (tmtch) and also another matching pro-
cess looking for a corresponding grammatical rela-
tion identified with the Malt parser in the other sen-
tence where the head (or argument) already has a
match in both sentences (rmtch).
A fuller and more formal description of the algo-
rithm follows:
1. retain nouns, verbs (not be), adjectives, adverbs
and numbers in both sentences s1 and s2.
2. wmtch:
(a) look for word matches
? wi ? s1 to w j ? s2, left to right i.e. the
first matching w j ? s2 is selected as a
match for wi.
? w j ? s2 to wi ? s1, left to right i.e. the
first matching wi ? s1 is selected as a
match for w j
(b) and then lemma matches for any ti ? s1
and t j ? s1 not matched in steps 2a
? li ? s1 to l j ? s2 , left to right i.e. the
first matching l j ? s2 is selected as a
match for li.
? l j ? s2 to li ? s1 , left to right i.e. the
first matching li ? s1 is selected as a
match for l j
3. using only ti ? s1 and t j ? s2 not matched in
the above steps:
? tmtch: match lemma and PoS (l + p) with
the distributional thesaurus against the top
20 most similar lemma-pos entries. That
is:
(a) For l + pi ? s1, if not already matched
at step 2 above, find the most similar
words in the thesaurus, and match if
one of these is in l + p j ? s2, left to
right i.e. the first matching l + p j ? s2
to any of the most similar words to
l + pi according to the thesaurus is se-
lected as a match for l + pi ? s1.
(b) For l + p j ? s2, if not already matched
at step 2 above, find the most similar
words in the thesaurus, and match if
one of these is in l + pi ? s1, left to
right
? rmtch: match the tokens, if not already
matched at step 2 above, by looking for
a head or argument relation with a token
that has been matched at step 2 to a token
with the inverse relation. That is:
i For ti ? s1, if not already matched at
step 2 above, if hpi ? s1 (the pointer
to the head, i.e. parent, of ti) refers to
a token tx ? s1 which has wmtch at tk
in s2, and there exists a tq ? s2 with
grq = gri and hpq = tk, then match ti
with tq
ii For ti ? s1 , if not already matched
at step 2 or the preceding step (rmtch
3i) and if there exists another tx ? s1
with a hpx which refers to ti (i.e. ti is
the parent, or head, of tx) with a match
between tx and tk ? s2 from step 2, 4
and where tk has grk = grx with hpk
which refers to tq in s2, then match ti
with tq 5
iii we do likewise in reverse for s2 to s1
and then check all matches are recip-
rocated with the same 1:1 mapping
Finally, we calculate the score sim(s1, s2):
|wmtch| + (wt ? |tmtch + rmtch|)
|s1| + |s2|
? 5 (1)
where wt is a weight of 0.5 or less (see below).
The sim score gives a score of 5 where two
sentences have the same open class tokens, since
matches in both directions are included and the de-
nominator is the number of open class tokens in both
sentences. The score is 0 if there are no matches.
The thesaurus and grammatical relation matches are
counted equally and are considered less important
4In the example illustrated in figure 1 and discussed below,
ti could be rose in the upper sentence (s1) and Nasdaq would be
tx and tk.
5So in our example, from figure 1, ti (rose) is matched with tq
(climb) as climb is the counterpart head to rose for the matched
arguments (Nasdaq).
559
NasdaqThe tech?loaded composite rose 20.96 points to 1595.91, ending at its highest level for 12 months.
thesaurus
malt
malt
points, or 1.2 percent, to 1,615.02.The technology?laced climbed 19.11 Index <.IXIC>CompositeNasdaq
Figure 1: Example of matching with alignheuristic
for the score as the exact matches. We set wt to 0.4
for the official run, though we could improve perfor-
mance by perhaps setting a bit lower as shown below
in section 4.1.
Figure 1 shows an example pair of sentences from
the training data in MSRpar. The solid lines show
alignments between words. Composite and compos-
ite are not matched because the lemmatizer assumes
that the former is a proper noun and does not decap-
italise; we could decapitalise all proper nouns. The
dotted arcs show parallel dependency relations in the
sentences where the argument (Nasdaq) is matched
by wmtch. The rmtch process therefore assumes the
corresponding heads (rise and climb) align. In addi-
tion, tmtch finds a match from climb to rise as rise is
in the top 20 most similar words (neighbours) in the
distributional thesaurus. climb is not however in the
top 20 for rise and so a link is not found in the other
direction. We have not yet experimented with val-
idating the thesaurus and grammatical relation pro-
cesses together, though that would be worthwhile in
future.
3.2 wordsim
In this method, we first choose the shortest sentence
based on the number of open words. Let s1 and s2
be the shortest and longest sentences respectively.
For every lemma li ? s1, we find its best matching
lemma l j ? s2 using the following heuristics and
assigning an alignment score as follows:
1. if li=l j, then the alignment score of li
(algnscr(li)) is one.
2. else li ? s1 is matched with a lemma l j ? s2
with which it has the highest distributional sim-
ilarity. 6 The alignment score, algnscr(li) is
the distributional similarity between li and l j
(which is always less than one).
The final sentence similarity score between the
pair s1 and s2 is computed as
sim(s1, s2) =
?
li?s1 algnscr(li)
|s1|
(2)
3.3 average
This system simple uses the average score for each
item from alignheuristic and wordsim. This is
simply so we can make a compromise between the
merits of the two systems.
4 Experiments on the Training Data
Table 1 displays the results on training data for the
system settings as they were for the final test run. We
conducted further analysis for the alignheuristic
system and that is reported in the following subsec-
tion. We can see that while the alignheuristic
is better on the MSRpar and SMT-eur datasets, the
wordsim outperforms it on the MSRvid dataset,
which contains shorter, simpler sentences. One rea-
son might be that the wordsim credits alignments
in one direction only and this works well when sen-
tences are of a similar length but can loose out on the
longer paraphrase and SMT data. This behaviour is
6Provided this is within the top 20 most similar words in the
thesaurus.
560
MSRpar MSRvid SMT-eur
alignheuristic 0.6015 0.6994 0.5222
wordsim 0.4134 0.7658 0.4479
average 0.5337 0.7535 0.5061
Table 1: Results on training data
confirmed by the results on the test data reported be-
low in section 5, though we cannot rule out that other
factors play a part.
4.1 alignheuristic
We developed the system on the training data for the
purpose of finding bugs, and setting the weight in
equation 1. During development we found the opti-
mal weight for wt to be 0.4. 7 Unfortunately we did
not leave ourselves sufficient time to set the weights
after resolving the bugs. In table 1 we report the
results on the training data for the system that we
uploaded, however in table 2 we report more recent
results for the final system but with different values
of wt. From table 2 it seems that results may have
been improved if we had determined the final value
of wt after debugging our system fully, however this
depends on the type of data as 0.4 was optimal for
the datasets with more complex sentences (MSRpar
and SMT-eur).
In table 3, we report results for alignheuristic
with and without the distributional similarity
thesaurus (tmtch) and the dependency relations
(rmtch). In table 4 we show the total number of to-
ken alignments made by the different matching pro-
cesses on the training data. We see, from table 4
that the MSRvid data relies on the thesaurus and de-
pendency relations to a far greater extent than the
other datasets, presumably because of the shorter
sentences where many have a few contrasting words
in similar syntactic relations, for example s1 Some-
one is drawing. s2 Someone is dancing. 8 We see
from table 3 that the use of these matching processes
is less accurate for MSRvid and that while tmtch
improves performance, rmtch seems to degrade per-
formance. From table 2 it would seem that on this
type of data we would get the best results by reduc-
7We have not yet attempted setting the weight on alignment
by relation and alignment by distributional similarity separately.
8Note that the alignheuristic algorithm is symmetrical
with respect to s1 and s2 so it does not matter which is which.
wt MSRpar MSRvid SMT-eur
0.5 0.5998 0.6518 0.5290
0.4 0.6015 0.6994 0.5222
0.3 0.6020 0.7352 0.5146
0.2 0.6016 0.7577 0.5059
0.1 0.6003 0.7673 0.4964
0 0.5981 0.7661 0.4863
Table 2: Results for the alignheuristic algorithm on
the training data: varying wt
MSR MSR SMT
par vid -eur
-tmtch+rmtch 0.6008 0.7245 0.5129
+tmtch-rmtch 0.5989 0.7699 0.4959
-tmtch-rmtch 0.5981 0.7661 0.4863
+tmtch+rmtch 0.6015 0.6994 0.5222
Table 3: Results for the alignheuristic algorithm on
the training data: with and without tmtch and rmtch
ing wt to a minimum, and perhaps it would make
sense to drop rmtch. Meanwhile, on the longer more
complex MSRpar and SMT-eur data, the less precise
rmtch and tmtch are used less frequently (relative to
the wmtch) but can be seen from table 3 to improve
performance on both training datasets. Moreover, as
we mention above, from table 2 the parameter set-
ting of 0.4 used for our final test run was optimal for
these datasets.
MSRpar MSRvid SMT-eur
wmtch 10960 2349 12155
tmtch 378 1221 964
rmtch 1008 965 1755
Table 4: Number of token alignments for the different
matching processes
561
run ALL MSRpar MSRvid SMT-eur On-WN SMT-news
alignheuristic .5253 (60) .5735 (24) .7123 (53) .4781 (25) .6984 (7) .4177 (38)
average .5490 (58) .5020 (48) .7645 (41) .4875 (16) .6677(14) .4324 (31)
wordsim .5130 (61) .3765 (75) .7761 (34) .4161 (58) .5728 (59) .3964 (48)
Table 5: Official results: Rank (out of 89) is shown in brackets
run ALL MSRpar MSRvid SMT-eur On-WN SMT-news average ?
alignheuristic 0.5216 0.5539 0.7125 0.5404 0.6928 0.3655 0.5645
average 0.5087 0.4818 0.7653 0.5415 0.6302 0.3835 0.5518
wordsim 0.4279 0.3608 0.7799 0.4487 0.4976 0.3388 0.4756
Table 7: Spearman?s ? for the 5 datasets, ?all? and the average coefficient across the datasets
run mean Allnrm
alignheuristic 0.6030 (21) 0.7962 (42)
average 0.5943 (26) 0.8047 (35)
wordsim 0.5287 (55) 0.7895 (49)
Table 6: Official results: Further metrics suggested in dis-
cussion. Rank (out of 89) is shown in brackets
5 Results
Table 5 provides the official results for our submitted
systems, along with the rank on each dataset. The re-
sults in the ?all? column which combine all datasets
together are at odds with our intuitions. Our sys-
tems were ranked higher in every individual dataset
compared to the ?all? ranking, with the exception of
wordsim and the MSRpar dataset. This ?all? met-
ric is anticipated to impact systems that have dif-
ferent settings for different types of data however
we did not train our systems to run differently on
different data. We used exactly the same parame-
ter settings for each system on every dataset. We
believe Pearson?s measure has a significant impact
on results because it is a parametric measure and
as such the shape of the distribution (the distribu-
tion of scores) is assumed to be normal. We present
the results in table 6 from new metrics proposed by
participants during the post-results discussion: All-
nrm (normalised) and mean (this score is weighted
by the number of sentence pairs in each dataset). 9
The Allnrm score, proposed by a participant during
the discussion phase to try and combat issues with
9Post-results discussion is archived at http://groups.
google.com/group/sts-semeval/topics
the ?all? score, also does not accord with our intu-
ition given the ranks of our systems on the individ-
ual datasets. The mean score, proposed by another
participant, however does reflect performance on the
individual datasets. Our average system is ranked
between alignheuristic and wordsim which is
in line with our expectations given results on the
training data and individual datasets.
As mentioned above, an issue with the use of
Pearson?s correlation coefficient is that it is para-
metric and assumes that the scores are normally dis-
tributed. We calculated Spearman?s ? which is the
non-parametric equivalent of Pearson?s and uses the
ranks of the scores, rather than the actual scores. 10
The results are presented in table 7. We cannot cal-
culate the results for other systems, and therefore the
ranks for our system, since we do not have the other
system?s outputs but we do see that the relative per-
formance of our system on ?all? is more in line with
our expectations: average, which simply uses the
average of the other two systems for each item, is
usually ranked between the other two systems, de-
pending on the dataset. Spearman?s ?all? gives a sim-
ilar ranking of our three systems as the mean score.
We also show average ?. This is a macro average
of the Spearman?s value for the 5 datasets without
weighting by the number of sentence pairs. 11
10Note that Spearman?s ? is often a little lower than Pear-
son?s (Mitchell and Lapata, 2008)
11We do recognise the difficulty in determining metrics on a
new pilot study. The task organisers are making every effort to
make it clear that this enterprise is a pilot, not a competition and
that they welcome feedback.
562
6 Conclusions
The systems were developed in less than a week
including the time with the test data. There are
many trivial fixes that may improve the basic algo-
rithm, such as decapitalising proper nouns. There
are many things we would like to try, such as val-
idating the dependency matching process with the
thesaurus matching. We would like to match larger
units rather than tokens, with preferences towards
the longer matching blocks. In parallel to the devel-
opment of alignheuristic, we developed a sys-
tem which measures the similarity between a node
in the dependency tree of s1 and a node in the de-
pendency tree of s2 as the sum of the lexical sim-
ilarity of the lemmas at the nodes and the simi-
larity of its children nodes. We did not submit a
run for the system as it did not perform as well as
alignheuristic, probably because the score fo-
cused on structure too much. We hope to spend time
developing this system in future.
Ultimately, we envisage a system that:
? can have non 1:1 mappings between tokens, i.e.
a phrase may be paraphrased as a word for ex-
ample blow up may be paraphrased as explode
? can map between sequences of non-contiguous
words for example the words in the phrase blow
up may be separated but mapped to the word
explode as in the bomb exploded ? They blew
it up
? has categories (such as equivalence, entailment,
negation, omission . . . ) associated with each
mapping. Speculation, modality and sentiment
should be indicated on any relevant chunk so
that differences can be detected between candi-
date and referent
? scores the candidate using a function of the
scores of the mapped units (as in the systems
described above) but alters the score to reflect
the category as well as the source of the map-
ping, for example entailment without equiva-
lence should reduce the similarity score, in con-
trast to equivalence, and negation should re-
duce this still further
Crucially we would welcome a task where anno-
tators would also provide a score on sub chunks of
the sentences (or arbitrary blocks of text) that align
along with a category for the mapping (equivalence,
entailment, negation etc..). This would allow us to
look under the hood at the text similarity task and de-
termine the reason behind the similarity judgments.
7 Acknowledgements
We thank the task organisers for their efforts in or-
ganising the task and their readiness to take on board
discussions on this as a pilot. We also thank the
SemEval-2012 co-ordinators.
References
Agirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,
A. (2012). Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evalua-
tion (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational
Semantics (*SEM 2012).
Barzilay, R. and Elhadad, N. (2003). Sentence align-
ment for monolingual comparable corpora. In
Collins, M. and Steedman, M., editors, Proceed-
ings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, pages 25?
32.
Berry, M. (1992). Large scale singular value compu-
tations. International Journal of Supercomputer
Applications, 6(1):13?49.
Dagan, I., Glickman, O., and Magnini, B. (2005).
The pascal recognising textual entailment chal-
lenge. In Proceedings of the PASCAL First Chal-
lenge Workshop, pages 1?8, Southampton, UK.
de Marneffe, M.-C., MacCartney, B., and Man-
ning, C. D. (2006). Generating typed dependency
parses from phrase structure parses. In To appear
at LREC-06.
Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the Sixth International
Conference on Language Resources and Evalua-
tion (LREC 2008), Marrakech, Morocco.
Hirst, G. (2003). Paraphrasing paraphrased. In-
vited talk at the Second International Workshop
563
on Paraphrasing, 41st Annual Meeting of the As-
sociation for Computational Linguistics.
Kilgarriff, A., Rychly?, P., Smrz, P., and Tugwell, D.
(2004). The sketch engine. In Proceedings of Eu-
ralex, pages 105?116, Lorient, France. Reprinted
in Patrick Hanks (ed.). 2007. Lexicology: Critical
concepts in Linguistics. London: Routledge.
McCarthy, D. and Navigli, R. (2009). The English
lexical substitution task. Language Resources and
Evaluation Special Issue on Computational Se-
mantic Analysis of Language: SemEval-2007 and
Beyond, 43(2):139?159.
Mihalcea, R., Corley, C., and Strapparava, C.
(2006). Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings
of the American Association for Artificial Intelli-
gence (AAAI 2006), Boston, MA.
Mitchell, J. and Lapata, M. (2008). Vector-based
models of semantic composition. In Proceed-
ings of ACL-08: HLT, pages 236?244, Columbus,
Ohio. Association for Computational Linguistics.
Mohler, M., Bunescu, R., and Mihalcea, R. (2011).
Learning to grade short answer questions us-
ing semantic similarity measures and dependency
graph alignments. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 752?762, Portland, Oregon, USA. As-
sociation for Computational Linguistics.
Mohler, M. and Mihalcea, R. (2009). Text-to-text se-
mantic similarity for automatic short answer grad-
ing. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009),
pages 567?575, Athens, Greece. Association for
Computational Linguistics.
Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit,
G., Ku?bler, S., Marinov, S., and Marsi, E. (2007).
Maltparser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering, 13(2):95?135.
Patwardhan, S., Banerjee, S., and Pedersen, T.
(2003). Using measures of semantic relatedness
for word sense disambiguation. In Proceedings
of the Fourth International Conference on Intelli-
gent Text Processing and Computational Linguis-
tics (CICLing 2003), Mexico City.
Rychly?, P. (2008). A lexicographer-friendly associ-
ation score. In Proceedings of 2nd Workshop on
Recent Advances in Slavonic Natural Languages
Processing, RASLAN 2008, Brno.
Schmid, H. (1994). Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44?49, Manchester,
UK.
Turney, P. D. (2002). Mining the web for synonyms:
Pmi-ir versus lsa on toefl. CoRR, cs.LG/0212033.
564
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 207?215, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UniMelb NLP-CORE: Integrating predictions from multiple domains and
feature sets for estimating semantic textual similarity
Spandana Gella,? Bahar Salehi,?? Marco Lui,??
Karl Grieser,? Paul Cook,? and Timothy Baldwin,??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
sgella@student.unimelb.edu.au, bsalehi@student.unimelb.edu.au
mhlui@unimelb.edu.au, kgrieser@student.unimelb.edu.au
paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
In this paper we present our systems for cal-
culating the degree of semantic similarity be-
tween two texts that we submitted to the Se-
mantic Textual Similarity task at SemEval-
2013. Our systems predict similarity using
a regression over features based on the fol-
lowing sources of information: string similar-
ity, topic distributions of the texts based on
latent Dirichlet alocation, and similarity be-
tween the documents returned by an informa-
tion retrieval engine when the target texts are
used as queries. We also explore methods for
integrating predictions using different training
datasets and feature sets. Our best system was
ranked 17th out of 89 participating systems.
In our post-task analysis, we identify simple
changes to our system that further improve our
results.
1 Introduction
Semantic Textual Similarity (STS) measures the de-
gree of semantic similarity or equivalence between
a pair of short texts. STS is related to many natural
language processing applications such as text sum-
marisation (Aliguliyev, 2009), machine translation,
word sense disambiguation, and question answering
(De Boni and Manandhar, 2003; Jeon et al, 2005).
Two short texts are considered similar if they both
convey similar messages. Often it is the case that
similar texts will have a high degree of lexical over-
lap, although this isn?t always so. For example,
SC dismissed government?s review plea in Vodafone
tax case and SC dismisses govt?s review petition on
Vodafone tax verdict are semantically similar. These
texts have matches in terms of exact words (SC,
Vodafone, tax), morphologically-related words (dis-
missed and dismisses), and abbreviations (govern-
ment?s and govt?s). However, the usages (senses) of
plea and petition, and case and verdict are also sim-
ilar.
One straightforward way of estimating semantic
similarity of two texts is by using approaches based
on the similarity of the surface forms of the words
they contain. However, such methods are not capa-
ble of capturing similarity or relatedness at the lexi-
cal level, and moreover, they do not exploit the con-
text in which individual words are used in a target
text. Nevertheless, a variety of knowledge sources
? including part-of-speech, collocations, syntax,
and domain ? can be used to identify the usage or
sense of words in context (McRoy, 1992; Agirre and
Martinez, 2001; Agirre and Stevenson, 2006) to ad-
dress these issues.
Despite their limitations, string similarity mea-
sures have been widely used in previous seman-
tic similarity tasks (Agirre et al, 2012; Islam and
Inkpen, 2008). Latent variable models have also
been used to estimate the semantic similarity be-
tween words, word usages, and texts (Steyvers and
Griffiths, 2007; Lui et al, 2012; Guo and Diab,
2012; Dinu and Lapata, 2010).
In this paper, we consider three different ways of
measuring semantic similarity based on word and
word usage similarity:
1. String-based similarity to measure surface-
level lexical similarity, taking into account
morphology and abbreviations (e.g., dismisses
and dismissed, and government?s and govt?s);
207
2. Latent variable models of similarity to cap-
ture words that have different surface forms,
but that have similar meanings or that can be
used in similar contexts (e.g., petition and plea,
verdict and case); and
3. Topical/domain similarity of the texts with re-
spect to the similarity of documents in an ex-
ternal corpus (based on information-retrieval
methods) that are relevant to the target texts.
We develop features based on all three of these
knowledge sources to capture semantic similarity
from a variety of perspectives. We build a regres-
sion model, trained on STS training data which has
semantic similarity scores for pairs of texts, to learn
weights for the features and rate the similarity of test
instances. Our approach to the task is to explore the
utility of novel features or features that have not per-
formed well in previous research, rather than com-
bine these features with the myriad of features that
have been proposed by others for the task.
2 Text Similarity Measures
In this section we describe the various features used
in our system.
2.1 String Similarity Measures (SS)
Our first set of features contains various string simi-
larity measures (SS), which compare the target texts
in terms of the words they contain and the order
of the words (Islam and Inkpen, 2008). In the Se-
mEval 2012 STS task (Agirre et al, 2012) such
features were used by several participants (Biggins
et al, 2012; Ba?r et al, 2012; Heilman and Mad-
nani, 2012), including the first-ranked team (Ba?r et
al., 2012) who considered string similarity measures
alongside a wide range of other features.
For our string similarity features, the texts were
lemmatized using the implementation of Lancaster
Stemming in NLTK 2.0 (Bird, 2006), and all punc-
tuation was removed. Limited stopword removal
was carried out by eliminating the words a, and, and
the. The output of each string similarity measure
is normalized to the range of [0, 1], where 0 indi-
cates that the texts are completely different, while 1
means they are identical. The normalization method
for each feature is described in Salehi and Cook (to
appear), wherein the authors applied string similar-
ity measures successfully to the task of predicting
the compositionality of multiword expressions.
Identical Unigrams (IU): This feature measures
the number of words shared between the two texts,
irrespective of word order.
Longest Common Substring (LCS): This mea-
sures the longest sequence of words shared between
the two texts. For example, the longest common
substring between the following sentences is bolded:
A woman and man are dancing in the
rain.
A couple are dancing in the street.
Levenshtein (LEV1): Levenshtein distance (also
known as edit distance) calculates the number of
basic word-level edit operations (insertion, deletion
and substitution) to transform one text into the other:
Levenshtein with substitution penalty (LEV2):
This feature is a variant of LEV1 in which substi-
tution is considered as two edit operations: an inser-
tion and a deletion (Baldwin, 2009).
Smith Waterman (SW): This method is designed
to locally align two sequences of amino acids (Smith
and Waterman, 1981). The algorithm looks for
the longest similar regions by maximizing the num-
ber of matches and minimizing the number of in-
sertion/deletion/substitution operations necessary to
align the two sequences. In other words, it finds the
longest common sequence while tolerating a small
number of differences. We call this sequence, the
?aligned sequence?. It has length equal to or greater
than the longest common sequence.
Not Aligned Words (NAW): As mentioned
above, SW looks for similar regions in the given
texts. Our last string similarity feature shows the
number of identical words not aligned by the SW al-
gorithm. We used this feature to examine how simi-
lar the unaligned words are.
These six features (IU, LCS, LEV1, LEV2, SW,
and NAW) form our string similarity (SS) features.
LEV2, SW, and NAW have not been previously con-
sidered for STS.
208
2.2 Topic Modelling Similarity Measures (TM)
The topic modelling features (TM) are based on La-
tent Dirichlet Allocation (LDA), a generative prob-
abilistic model in which each document is mod-
eled as a distribution over a finite set of topics, and
each topic is represented as a distribution over words
(Blei et al, 2003). We build a topic model on a back-
ground corpus, and then for each target text we cre-
ate a topic vector based on the topic allocations of
its content words, based on the method developed
by Lui et al (2012) for predicting word usage simi-
larity.
The choice of the number of topics, T , can
have a big impact on the performance of this
method. Choosing a small T might give overly-
broad topics, while a large T might lead to un-
interpretable topics (Steyvers and Griffiths, 2007).
Moreover smaller numbers of topics have been
shown to perform poorly on both sentence simi-
larity (Guo and Diab, 2012) and word usage sim-
ilarity tasks (Lui et al, 2012). We therefore build
topic models for 33 values of T in the range
2, 3, 5, 8, 10, 50, 80, 100, 150, 200, ...1350.
The background corpus used for generating the
topic models is similar to the COL-WTMF sys-
tem (Guo and Diab, 2012) from the STS-2012 task,
which outperformed LDA. In particular, we use
sense definitions from WordNet, Wiktionary and all
sentences from the Brown corpus. Similarity be-
tween two texts is measured on the basis of the simi-
larity between their topic distributions. We consider
three vector-based similarity measures here: Cosine
similarity, Jensen-Shannon divergence and KL di-
vergence. Thus for each target text pair we extract
99 features corresponding to the 3 similarity mea-
sures for each of the 33 T settings. These features
are used as the TM feature set in the systems de-
scribed below.
2.3 IR Similarity Measures (IR)
The information retrieval?based features (IR) were
based on a dump of English Wikipedia from Novem-
ber 2009. The entire dump was stripped of markup
and tokenised using the OpenNLP tokeniser. The
tokenised documents were then parsed into TREC
format, with each article forming an individual doc-
ument. These documents were indexed using the
Indri IR engine1 with stopword removal. Each
of the two target texts was issued as a full text
query (without any phrases) to Indri, and the first
1000 documents for each text were returned, based
on Okapi term weighting (Robertson and Walker,
1994). These resultant document lists were then
converted into features using a number of set- and
rank-based measures: Dice?s coefficient, Jaccard in-
dex, average overlap, and rank-biased overlap (the
latter two are described in Webber et al (2010)).
The first two are based on simple set overlap and
ignore the ranks; average overlap takes into account
the rank, but equally weights high- and low-ranking
documents; and rank-biased overlap weights higher-
ranked items higher.
In addition to comparisons of the document rank-
ings for a given target text pair, we also consid-
ered a method that compared the top-ranking doc-
uments themselves. To compare two texts, we ob-
tain the top-100 documents using each text as a
query as above. We then calculate the similarity be-
tween these two sets of resultant documents using
the ?2-based corpus similarity measure of Kilgarriff
(2001). In this method the ?2 statistic is calculated
for the 500 most frequent words in the union of the
two sets of documents (corpora), and is interpreted
as the similarity between the sets of documents.
These 5 IR features (4 rank-based, and 1
document-based) are novel in the context of STS,
and are used in the compound systems described be-
low.
3 Compound systems
3.1 Ridge regression
Each of our features represents a (potentially noisy)
measurement of the semantic textual similarity be-
tween two texts. However, the scale of our fea-
tures varies, e.g., [0, 1] for the string similarity fea-
tures vs. unbounded for KL divergence (one of the
topic modelling features). To learn the mapping be-
tween these features and the graded [0, 5] scale of
the shared task, we made use of a statistical tech-
nique known as ridge regression, as implemented in
scikit-learn.2 Ridge regression is a form of
linear regression where the loss function is the ordi-
1http://www.lemurproject.org/indri/
2http://scikit-learn.org
209
nary least squares, but with an additional L2 regular-
ization term. In our empirical evaluation, we found
that ridge regression outperformed linear regression
on our feature set. For brevity, we only present re-
sults from ridge regression.
3.2 Domain Adaptation
Domain adaptation (Daume? and Marcu, 2006) is the
general term applied to techniques for using labelled
data from a related distribution to label data from a
target distribution. For the 2013 Shared Task, no
training data was provided for the target datasets,
making domain adaptation an important considera-
tion. In this work, we assume that each dataset rep-
resents a different domain, and on this basis develop
approaches that are sensitive to inter-domain differ-
ences.
We tested two simple approaches to including do-
main information in our trained model. The first ap-
proach, which we will refer to as flagging, simply in-
volves appending a boolean vector to each training
instance to indicate which training dataset it came
from. The vector has length D, equal to the number
of training datasets (3 for this task, because we train
on the STS 2012 training data). All the values of the
vector are 0, except for a single 1 according to the
dataset that the training instance is drawn from. For
test data, the entire vector consists of 0s.
The second approach we considered is based on
metalearning, and we will refer to it as domain
stacking. In domain stacking, we train a regressor
for each domain (the level 0 regressors (Wolpert,
1992)). Each of these regressors is then applied
to a test instance to produce a predicted value (the
level 0 prediction). These predictions are then com-
bined using a second regressor (the level 1 regres-
sor), to produce a final prediction for each instance
(the level 1 prediction). This approach is closely
related to feature stacking (Lui, 2012) and stacked
generalization (Wolpert, 1992). A general princi-
ple of metalearning is to combine multiple weaker
(?less accurate?) predictors ? termed level 0 pre-
dictors ? to produce a stronger (?more accurate?)
predictor ? the level 1 predictor. In stacked gener-
alization, the level 0 predictors are different learning
algorithms. In feature stacking, they are the same
algorithm trained on different subsets of features, in
this work corresponding to different methods for es-
timating STS (Section 2). In domain stacking, the
level 0 predictions are obtained from subsets of the
training data, where each subset corresponds to all
the instances from a single dataset (e.g. MSRpar or
SMTeuroparl). In terms of subsampling the training
data, this technique is related to bagging (Breiman,
1996). However, rather than generating new train-
ing sets by uniform sampling across the whole pool
of training data, we treat each domain in the train-
ing dataset as a unique sample. Finally, we also ex-
periment with feature-domain stacking, in which the
level 0 predictions are obtained from the cross prod-
uct of subsets of the training data (as per domain
stacking) and subsets of the feature set (as per fea-
ture stacking). We report results for all 3 variants in
Section 5.
This framework of feature-domain stacking can
be applied with any regression or classification al-
gorithm (indeed, the level 0 and level 1 predictors
could be trained using different algorithms). In this
work, all our regressors are trained using ridge re-
gression (Section 3.1).
4 Submitted Runs
In this section we describe the three official runs we
submitted to the shared task.
4.1 Run1 ? Bahar
For this run we used just the SS feature set, aug-
mented with flagging for domain adaptation. Ridge
regression was used to train a regressor across the
three training datasets (MSRvid, MSRpar, SMTeu-
roparl). Each instance was then labelled using the
output of the regressor, and the output range was lin-
early re-scaled to [0, 5] as it occasionally produced
values outside of this range. Although this approach
approximates STS using only lexical textual similar-
ity, it was our best-performing system on the training
data (Table 1). Furthermore the SS features are ap-
pealing because of their simplicity and because they
do not make use of any external resources.
4.2 Run2 ? Concat
In this run, we concatenated the feature vectors
from all three of our feature sets (SS, TM and
IR), and again trained a regressor on the union of
the MSRvid, MSRpar and SMTeuroparl training
datasets. As in Run1, the output of the regression
210
FSet FL FS DS MSRpar MSRvid SMTeuroparl Ave
SS 0.522 0.537 0.526 0.528
(*) SS X 0.552 0.533 0.562 0.549
TM 0.270 0.479 0.425 0.391
TM X 0.250 0.580 0.427 0.419
IR 0.264 0.759 0.407 0.477
IR X 0.291 0.754 0.400 0.482
(+) ALL 0.401 0.543 0.513 0.485
ALL X 0.377 0.595 0.516 0.496
ALL X 0.385 0.587 0.520 0.497
ALL X 0.452 0.637 0.472 0.521
ALL X X 0.429 0.619 0.526 0.524
ALL X X 0.429 0.627 0.526 0.527
(?) ALL X X X 0.441 0.645 0.527 0.538
Table 1: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each training dataset, and the
micro-average over all training datasets. (*), (+),
and (?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
was also linearly re-scaled to the [0, 5] range. Un-
like the previous run, the flagging approach to do-
main adaptation was not used. This approach re-
flects a simple application of machine learning to in-
tegrating data from multiple feature sets and training
datasets, and provides a useful point of comparison
against more sophisticated approaches (i.e., Run3).
4.3 Run3 ? Stacking
In this run, we focused on an alternative method
to integrating information from multiple feature sets
and training datasets, namely feature-domain stack-
ing, as discussed in Section 3.2. In this approach, we
train nine regressors using ridge regression on each
combination of the three training datasets and three
feature sets. Thus, the level 1 representation for each
instance is a vector of nine predictions. For the train-
ing data, when computing the level 1 features for the
same training dataset from which a given instance is
drawn, 10-fold cross-validation is used. Ridge re-
gression is again used to combine the level 1 repre-
sentations and produce the final prediction for each
instance. In addition to this, we also simultaneously
apply the flagging approach to domain adaptation.
This approach incorporates all of our domain adap-
tation efforts, and in initial experiments on the train-
ing data (Table 1) it was our second-best system.
FSet FL FS DS OnWN FNWN Headlines SMT Ave
SS 0.340 0.366 0.688 0.325 0.453
(*) SS X 0.349 0.381 0.711 0.350 0.473
TM 0.648 0.358 0.516 0.209 0.433
TM X 0.701 0.368 0.614 0.287 0.506
IR 0.561 -0.006 0.610 0.228 0.419
IR X 0.596 0.002 0.621 0.256 0.441
(+) ALL 0.679 0.337 0.709 0.323 0.542
ALL X 0.704 0.365 0.718 0.344 0.560
ALL X 0.673 0.298 0.714 0.324 0.539
ALL X 0.618 0.264 0.717 0.357 0.534
ALL X X 0.658 0.309 0.721 0.330 0.540
ALL X X 0.557 0.142 0.694 0.280 0.475
(?) ALL X X X 0.614 0.186 0.706 0314 0.509
Table 2: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each test dataset, and the
micro-average over all test datasets. (*), (+), and
(?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
5 Results
For the STS 2013 task, the organisers advised par-
ticipants to make use of the STS 2012 data; we took
this to mean only the training data. In our post-task
analysis, we realised that the entire 2012 dataset, in-
cluding the testing data, could be used. All our of-
ficial runs were trained only on the training data for
the 2012 task (made up of MSRpar, MSRvid and
SMTeuroparl). We first discuss preliminary find-
ings training and testing on the (STS 2012) training
data, and then present results for the (2013) test data.
Post-submission, we re-trained our systems includ-
ing the 2012 test data.
5.1 Experiments on Training Data
We evaluated our models based on a leave-one-out
cross-validation across the 3 training datasets. Thus,
for each of the training datasets, we trained a sep-
arate model using features from the other two. We
considered approaches based on each individual fea-
ture set, with and without flagging. We further con-
sidered combinations of feature sets using feature
concatenation, as well as feature and domain stack-
ing, again with and without flagging.3 Results are
3We did not consider domain stacking with flagging.
211
FSet FL FS DS OnWN (?) FNWN (?) Headlines (?) SMT (?) Ave (?)
SS 0.3566 (+.0157) 0.3741 (+.0071) 0.6994 (+.0111) 0.3386 (+.0131) 0.4663 (+.0133)
(*) SS X 0.3532 (+.0042) 0.3809 (?.0004) 0.7122 (+.0003) 0.3417 (?.0090) 0.4714 (?.0016)
TM 0.6748 (+.0265) 0.3939 (+.0349) 0.5930 (+.0770) 0.2563 (+.0472) 0.4844 (+.0514)
TM X 0.6269 (?.0743) 0.3519 (?.0162) 0.5999 (?.0142) 0.2653 (?.0223) 0.4743 (?.0317)
IR 0.6632 (+.1015) 0.1026 (+.1093) 0.6383 (?.0281) 0.2987 (+.0701) 0.4863 (+.0673)
IR X 0.6720 (+.0755) 0.0861 (+.0841) 0.6316 (+.0097) 0.2811 (+.0244) 0.4790 (+.0680)
(+) ALL 0.6976 (+.0006) 0.4350 (+.0976) 0.7071 (?.0014) 0.3329 (+.0099) 0.5571 (+.0151)
ALL X 0.6667 (?.0373) 0.4138 (+.0490) 0.7210 (+.0029) 0.3335 (?.0105) 0.5524 (?.0076)
ALL X 0.6889 (+.0149) 0.4620 (+.1636) 0.7309 (+.0167) 0.3538 (+.0295) 0.5721 (+.0331)
ALL X 0.6765 (?.0185) 0.4675 (+.1578) 0.7337 (+.0126) 0.3552 (+.0252) 0.5709 (+.0369)
ALL X X 0.6369 (+.0208) 0.3615 (+.0970) 0.7233 (+.0060) 0.3736 (+.0157) 0.5554 (+.0154)
ALL X X 0.6736 (+.1165) 0.4250 (+.2821) 0.7237 (+0.0297) 0.3404 (+0.0603) 0.5583(+.0833)
(?) ALL X X X 0.6772 (+.0632) 0.3992 (+.2127) 0.7315 (+.0251) 0.3300 (+0.0186) 0.5572 (+.0482)
Table 3: Pearson?s ? for each feature set (FSet), as well as combinations of feature sets and adaptation
strategies, on each test dataset, and the micro-average over all test datasets, using features from all 2012
data (test + train). (*), (+), and (?) denote Run1, Run2, and Run3, respectively, our submissions to the
shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the difference in system
performance after adding the additional training data.
reported in Table 1.
The best results on the training data were achieved
using only our SS feature set with flagging (Run1),
with an average Pearson?s ? of 0.549. This fea-
ture set alo gave the best performance on MSR-
par and SMTeuroparl, although the IR feature set
was substantially better on MSRvid. On the training
datasets, our approaches that combine feature sets
did not give an improvement over the best individ-
ual feature set on any dataset, or overall.
5.2 Test Set Results
STS 2013 included four different test sets. Table 2
presents the Pearson?s ? for the same methods as
Section 5.1 ? including our submitted runs ? on
the test data. Run1 drops in performance on the test
set as compared to the training set, where the other
two runs are more consistent, suggesting that lexi-
cal similarity does not generalise well cross-domain.
Table 4 shows that all of our systems performed
above the baseline on each dataset, except Run3 on
FNWN. Table 4 also shows that Run2 consistently
performed well on all the datasets when compared
to the median of all the systems submitted to the task
(Agirre et al, to appear).
Run2, which was based on the concatenation of
all the feature sets, performed well compared to the
stacking-based approaches on the test set, whereas
the stacking approaches all outperformed Run2 on
the training datasets. This is likely due to the
SS features being more effective for STS predic-
tion in the training datasets as compared to the test
datasets. Based on the training datasets, the stack-
ing approaches placed greater weight on the pre-
dictions from the SS feature set. This hypothe-
sis is supported by the result on Headlines, where
the SS feature set does relatively well, and thus the
stacking approaches tend to outperform the simple
concatenation-based method. Finally, an extension
of Run2 with flagging (not submitted to the shared
task) was the best of our methods on the test data.
5.3 Error Analysis
To better understand the behaviour of our systems,
we examined test instances and made the following
observations. Systems based entirely on the TM fea-
tures and domain adaptation consistently performed
well on sentence pairs for which all of our other sys-
tems performed poorly. One example is the follow-
ing OnWN pair, which corresponds to definitions of
newspaper: an enterprise or company that publishes
newsprint and a business firm that publishes news-
papers. Because these texts do not share many com-
mon words, the SS features cannot capture their se-
mantic similarity.
Stacking based approaches performed well on text
pairs which are complex to comprehend, e.g., Two
German tourists, two pilots killed in Kenya air crash
and Senator Reid involved in Las Vegas car crash,
where the individual methods tend to score lower
212
System Headlines OnWN FNWN SMT Ave
(+) Run1 .711 (15) .349 (71) .381 (23) .351 (18) .473 (49)
(+) Run2 .709 (17) .679 (18) .337 (33) .323 (43) .542 (17)
(+) Run3 .706 (18) .614 (28) .187 (71) .314 (47) .509 (29)
Best .718 (14) .704 (15) .365 (28) .344 (24) .560 (7)
(?) Run1 .712 (14) .353 (70) .381 (23) .341 (25) .471 (54)
(?) Run2 .707 (18) .697 (14) .435 (9) .332 (35) .557 (9)
(?) Run3 .731 (11) .677 (19) .399 (17) .330 (38) .557 (8)
(?) Best .730 (11) .688 (17) .462 (7) .353 (18) .572 (4)
Baseline .540 (67) .283 (81) .215 (67) .286 (65) .364 (73)
Median .640 (45) .528 (45) .327 (45) .318 (45) .480 (45)
Best-Score .783 (1) .843 (1) .581 (1) .403 (1) .618 (1)
Table 4: Pearson?s ? (and projected ranking) of runs.
The upper 4 runs are trained only on STS 2012 train-
ing data. (+) denotes runs that were submitted for
evaluation. (?) denotes systems trained on STS 2012
training and test data. For comparison, we include
?Best?, the highest-scoring parametrization of our
system from our post-task analysis (Table 3). We
also include the organiser?s baseline, as well as the
median and best systems for each dataset across all
competitors.
than the human rating, but stacking was able to pre-
dict a higher score (presumably based on the fact
that no method predicted the text pair to be strongly
dissimilar; rather, all methods predicted there to be
somewhat low similarity).
In some cases, the texts are on a similar topic,
but semantically different, e.g., Nigeria mourns over
193 people killed in plane crash and Nigeria opens
probe into deadly air crash. In such cases, systems
based on SS features and stacking perform well.
Systems based on TM and IR features, on the other
hand, tend to predict overly-high scores because the
texts relate to similar topics and tend to have similar
relevant documents in an external corpus.
5.4 Results with the Full Training dataset
We re-trained all the above systems by extending the
training data to include the 2012 test data. Scores on
the 2013 test datasets and the change in Pearson?s ?
after adding the extra training data (denoted ?) are
presented in Table 3.
In general, the addition of the 2012 test data to
the training dataset improves the performance of the
system, though this is often not the case for the flag-
ging approach to domain adaptation, which in some
instances drops in performance after adding the ad-
ditional training data. The biggest improvements
were seen for feature-domain stacking, particularly
on FNWN. This suggests that feature-domain stack-
ing is more sensitive to the similarity between train-
ing data and test data than flagging, but also that it
is better able to cope with variety in training do-
mains than flagging. Given that the pool of anno-
tated data for the STS task continues to increase,
feature-domain stacking is a promising approach to
exploiting the differences between domains to im-
prove overall STS performance.
To facilitate comparison with the published re-
sults for the 2013 STS task, we present a condensed
summary of our results in Table 4, which shows the
absolute score as well as the projected ranking of
each of our systems. It also includes the median and
baseline results for comparison.
6 Conclusions and Future Work
In this paper we described our approach to the
STS SemEval-2013 shared task. While we did not
achieve high scores relative to the other submit-
ted systems on any of the datasets or overall, we
have identified some novel feature sets which we
show to have utility for the STS task. We have
also compared our proposed method?s performance
with a larger training dataset. In future work, we
intend to consider alternative ways for combining
features learned from different domains and training
datasets. Given the strong performance of our string
similarity features on particular datasets, we also in-
tend to consider combining string and distributional
similarity to capture elements of the texts that are not
currently captured by our string similarity features.
Acknowledgments
This work was supported by the European Erasmus
Mundus Masters Program in Language and Commu-
nication Technologies from the European Commis-
sion.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excel-
lence program.
213
References
Eneko Agirre and David Martinez. 2001. Knowl-
edge sources for word sense disambiguation. In Text,
Speech and Dialogue, pages 1?10. Springer.
Eneko Agirre and Mark Stevenson. 2006. Knowledge
sources for wsd. In Eneko Agirre and Philip Edmonds,
editors, Word Sense Disambiguation, volume 33 of
Text, Speech and Language Technology, pages 217?
251. Springer Netherlands.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. to appear. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics,
Atlana, USA. Association for Computational Linguis-
tics.
Ramiz M Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764?7772.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Sam Biggins, Shaabi Mohammed, Sam Oakley, Luke
Stringer, Mark Stevenson, and Judita Preiss. 2012.
University of sheffield: Two approaches to semantic
text similarity. In *SEM 2012: The First Joint Confer-
ence on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and the
shared task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation (Se-
mEval 2012), pages 655?661, Montre?al, Canada, 7-8
June. Association for Computational Linguistics.
Steven Bird. 2006. NLTK: The Natural Language
Toolkit. In Proceedings of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 69?72, Sydney,
Australia, July. Association for Computational Lin-
guistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Leo Breiman. 1996. Bagging predictors. Machine learn-
ing, 24(2):123?140.
Hal Daume?, III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126, May.
Marco De Boni and Suresh Manandhar. 2003. The use
of sentence similarity as a semantic relevance metric
for question answering. In Proceedings of the AAAI
Symposium on New Directions in Question Answering,
Stanford, USA.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Weiwei Guo and Mona Diab. 2012. Weiwei: A sim-
ple unsupervised latent semantics based approach for
sentence similarity. In *SEM 2012: The First Joint
Conference on Lexical and Computational Semantics
? Volume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 586?590, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Michael Heilman and Nitin Madnani. 2012. Ets: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics ? Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529?535, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management, CIKM ?05, pages 84?90, New York, NY,
USA. ACM.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics, 6(1):97?133.
214
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Association Workshop 2012, pages 33?41,
Dunedin, New Zealand, December.
Marco Lui. 2012. Feature stacking for sentence clas-
sification in evidence-based medicine. In Proceed-
ings of the Australasian Language Technology Associ-
ation Workshop 2012, pages 134?138, Dunedin, New
Zealand, December.
Susan W McRoy. 1992. Using multiple knowledge
sources for word sense discrimination. Computational
Linguistics, 18(1):1?30.
Stephen E Robertson and Steve Walker. 1994. Some
simple effective approximations to the 2-poisson
model for probabilistic weighted retrieval. In Proceed-
ings of the 17th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?94, pages 232?241, Dublin, Ireland.
Bahar Salehi and Paul Cook. to appear. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In *SEM 2013:
The Second Joint Conference on Lexical and Com-
putational Semantics, Atlana, USA. Association for
Computational Linguistics.
TF Smith and MS Waterman. 1981. Identification of
common molecular subsequences. Molecular Biology,
147:195?197.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of latent semantic analysis,
427(7):424?440.
William Webber, Alistair Moffat, and Justin Zobel.
2010. A similarity measure for indefinite rankings.
ACM Transactions on Information Systems (TOIS),
28(4):20.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
215
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 248?253, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Unsupervised Word Usage Similarity in Social Media Texts
Spandana Gella,? Paul Cook,? and Bo Han??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
sgella@student.unimelb.edu.au, paulcook@unimelb.edu.au,
hanb@student.unimelb.edu.au
Abstract
We propose an unsupervised method for au-
tomatically calculating word usage similar-
ity in social media data based on topic mod-
elling, which we contrast with a baseline dis-
tributional method and Weighted Textual Ma-
trix Factorization. We evaluate these meth-
ods against a novel dataset made up of human
ratings over 550 Twitter message pairs anno-
tated for usage similarity for a set of 10 nouns.
The results show that our topic modelling ap-
proach outperforms the other two methods.
1 Introduction
In recent years, with the growing popularity of so-
cial media applications, there has been a steep rise
in the amount of ?post?-based user-generated text
(including microblog posts, status updates and com-
ments) (Bennett, 2012). This data has been iden-
tified as having potential for applications ranging
from trend analysis (Lau et al, 2012a) and event de-
tection (Osborne et al, 2012) to election outcome
prediction (O?Connor et al, 2010). However, given
that posts are generally very short, noisy and lack-
ing in context, traditional NLP approaches tend to
perform poorly over social media data (Hong and
Davison, 2010; Ritter et al, 2011; Han et al, 2012).
This is the first paper to address the task of lexi-
cal semantic interpretation in microblog data based
on word usage similarity. Word usage similar-
ity (USIM: Erk et al (2009)) is a relatively new
paradigm for capturing similarity in the usages of
a given word independently of any lexicon or sense
inventory. The task is to rate on an ordinal scale the
similarity in usage between two different usages of
the same word. In doing so, it avoids common issues
in conventional word sense disambiguation, relating
to sense underspecification, the appropriateness of a
static sense inventory to a given domain, and the in-
ability to capture similarities/overlaps between word
senses. As an example of USIM, consider the fol-
lowing pairing of Twitter posts containing the target
word paper:
1. Deportation of Afghan Asylum Seekers from
Australia : This paper aims to critically evalu-
ate a newly signed agree.
2. @USER has his number on a piece of paper
and I walkd off!
The task is to predict a real-valued number in the
range [1, 5] for the similarity in the respective us-
ages of paper, where 1 indicates the usages are com-
pletely different and 5 indicates they are identical.
In this paper we develop a new USIM dataset
based on Twitter data. In experiments on this dataset
we demonstrate that an LDA-based topic modelling
approach outperforms a baseline distributional se-
mantic approach and Weighted Textual Matrix Fac-
torization (WTMF: Guo and Diab (2012a)). We
further show that context expansion using a novel
hashtag-based strategy improves both the LDA-
based method and WTMF.
2 Related Work
Word sense disambiguation (WSD) is the task of
determining the particular sense of a word from a
given set of pre-defined senses (Navigli, 2009). It
248
contrasts with word sense induction (WSI), where
the senses of a given target word are induced from
an unannotated corpus of usages, and the induced
senses are then used to disambiguate each token us-
age of the word (Manandhar et al, 2010; Lau et
al., 2012b). WSD and WSI have been the predomi-
nant paradigms for capturing and evaluating lexical
semantics, and both assume that each usage corre-
sponds to exactly one of a set of discrete senses of
the target word, and that any prediction other than
the ?correct? sense is equally wrong.
Erk et al (2009) showed that, given a sense in-
ventory, there is a high likelihood of multiple senses
being compatible with a given usage, and proposed
USIM as a means of capturing the similarity in us-
age between a pairing of usages of a given word.
As part of their work, they released a dataset, which
Lui et al (2012) recently developed a topic mod-
elling approach over. Based on extensive experi-
mentation, they demonstrated the best results with
a single topic model for all target words based on
full document context. Our topic modelling-based
approach to USIM builds off the approach of Lui
et al (2012). Guo and Diab (2012a) observed that,
when applied to short texts, the effectiveness of la-
tent semantic approaches can be boosted by expand-
ing the text to include ?missing? words. Based on
this, they proposed Weighted Textual Matrix Factor-
ization (WTMF), based on weighted matrix factor-
ization (Srebro and Jaakkola, 2003). Here we ex-
periment with both LDA based topic modeling and
WTMF to estimate word similarities in twitter data.
LDA based topic modeling has been earlier studied
on Twitter data for tweet classification (Ramage et
al., 2010) and tweet clustering (Jin et al, 2011).
3 Data Preparation
This section describes the construction of the USIM-
tweet dataset based on microblog posts (?tweets?)
from Twitter. We describe the pre-processing steps
taken to sample the tweets in our datasets, outline
the annotation process, and then describe the back-
ground corpora used in our experiments.
3.1 Data preprocessing
Around half of Twitter is non-English (Hong et al,
2011), so our first step was to automatically identify
English tweets using langid.py (Lui and Bald-
win, 2012). We next performed lexical normaliza-
tion using the dictionary of Han et al (2012) to con-
vert lexical variants (e.g., tmrw) to their standard
forms (e.g., tomorrow) and reduce data sparseness.
As our target words, we chose the 10 nouns from
the original USIM dataset of Erk et al (2009) (bar,
charge, execution, field, figure, function, investiga-
tor, match, paper, post), and identified tweets con-
taining the target words as nouns using the CMU
Twitter POS tagger (Owoputi et al, 2012).
3.2 Annotation Settings and Data
To collect word usage similarity scores for Twitter
message pairs, we used a setup similar to that of
Erk et al (2009) using Amazon Mechanical Turk:
we asked the annotators to rate each sentence pair
with an integer score in the range [1, 5] using sim-
ilar annotation guidelines to Erk et al We ran-
domly sampled twitter messages from the TREC
2011 microblog dataset,1 and for each of our 10
nouns, we collected 55 pairs of messages satisfying
the preprocessing described in Section 3.1. These
55 pairs are chosen such that each tweet has at least
4 content words (nouns, verbs, adjectives and ad-
verbs) and at least 70+% of its post-normalized to-
kens in the Aspell dictionary (v6.06)2; these restric-
tions were included in an effort to ensure the tweets
would contain sufficient linguistic content to be in-
terpretable.3 We created 110 Mechanical Turk jobs
(referred to as HITs), with each HIT containing 5
randomly-selected message pairs. For this annota-
tion the tweets were presented in their original form,
i.e., without lexical normalisation applied. Each HIT
was completed by 10 ?turkers?, resulting in a total
of 5500 annotations. The annotation was restricted
to turkers based in the United States having had at
least 95% of their previous HITs accepted. In total,
the annotation was carried out by 68 turkers, each
completing between 1 and 100 HITs.
To detect outlier annotators, we calculated the av-
erage Spearman correlation score (?) of every an-
notator by correlating their annotation values with
every other annotator and taking the average. We
1http://trec.nist.gov/data/tweets/
2http://aspell.net/
3In future analyses we intend to explore the potential impact
of these restrictions on the resulting dataset.
249
Word Orig Exp Word Orig Exp
bar 180k 186k function 26k 27k
charge 41k 43k investigator 17k 19k
execution 28k 30k field 72k 75k
figure 28k 29k match 126k 133k
paper 210k 218k post 299k 310k
Table 1: The number of tweets for each word in each
background corpus (?Orig? = ORIGINAL; ?Exp?
= EXPANDED; RANDEXPANDED, not shown, con-
tains the same number of tweets as EXPANDED).
accepted all the annotations of annotators whose av-
erage ? is greater than 0.6; this corresponded to 95%
of the annotators. Two annotators had a negative
average ? and their annotations (only 4 HITs to-
tal) were discarded. For the other annotators (i.e.,
0 ? ? ? 0.6), we accepted each of their HITs on
a case by case basis; a HIT was accepted only if
at least 2 out of 5 of the annotations for that HIT
were within ?2.0 of the mean for that annotation
based on the judgments of the other turkers. (21
HITS were discarded using this heuristic.) We fur-
ther eliminated 7 HITS which have incomplete judg-
ments. In total only 32 HITs (of the 1100 HITs com-
pleted) were discarded through these heuristics. The
weighted average Spearman correlation over all an-
notators after this filtering is 0.681, which is some-
what higher than the inter-annotator agreement of
0.548 reported by Erk et al (2009). This dataset is
available for download.
3.3 Background Corpus
We created three background corpora based on data
from the Twitter Streaming API in February 2012
(only tweets satisfying the preprocessing steps in
Section 3.1 were chosen).
ORIGINAL: 1 million tweets which contain at least
one of the 10 target nouns;
EXPANDED: ORIGINAL plus an additional 40k
tweets containing at least 1 hashtag attested in
ORIGINAL with an average frequency of use of
10?35 times/hour (medium frequency);
RANDEXPANDED: ORIGINAL plus 40k randomly
sampled tweets containing the same target
nouns.
We select medium-frequency hashtags because low-
frequency hashtags tend to be ad hoc and non-
thematic in nature, while high-frequency hash-
tags are potentially too general to capture us-
age similarity. Statistics for ORIGINAL and EX-
PANDED/RANDEXPANDED are shown in Table 1.
RANDEXPANDED is sampled such that it has the
same number of tweets as EXPANDED.
4 Methodology
We propose an LDA topic modelling-based ap-
proach to the USIM task, which we contrast with
a baseline distributional model and WTMF. In all
these methods, the similarity between two word us-
ages is measured using cosine similarity between the
vector representation of each word usage.
4.1 Baseline
We represent each target word usage in a tweet as a
second-order co-occurrence vector (Schu?tze, 1998).
A second-order co-occurrence vector is built from
the centroid (summation) of all the first-order co-
occurrence vectors of the context words in the same
tweet as the target word.
The first-order co-occurrence vector for a given
target word represents the frequency with which that
word co-occurs in a tweet with other context words.
Each first-order vector is built from all tweets which
contain a context word and the target word catego-
rized as noun in the background corpus, thus sensi-
tizing the first-order vector to the target word. We
use the most frequent 10000 words (excluding stop-
words) in the background corpus as our first-order
vector dimensions/context words. Context words
(dimensions) in the first-order vectors are weighted
by mutual information.
Second-order co-occurrence is used as the context
representation to reduce the effects of data sparse-
ness in the tweets (which cannot be more than 140
codepoints in length).
4.2 Weighted Textual Matrix Factorization
WTMF (Guo and Diab, 2012b) addresses the data
sparsity problem suffered by many latent variable
250
Model ORIGINAL EXPANDED RANDEXPANDED
Baseline 0.09 0.08 0.09
WTMF 0.02 0.09 0.06
LDA 0.20 0.29 0.18
Table 2: Spearman rank correlation (?) for each
method based on each background corpus. The best
result for each corpus is shown in bold.
models by predicting ?missing? words on the ba-
sis of the message content, and including them in
the vector representation. Guo and Diab showed
WTMF to outperform LDA on the SemEval-2012
semantic textual similarity task (STS) (Agirre et al,
2012). The semantic space required for this model
as applied here is built from the background tweets
corresponding to the target word. We experimented
with the missing weight parameter wm of WTMF
in the range [0.05, 0.01, 0.005, 0.0005] and with di-
mensions K=100 and report the best results (wm =
0.0005).
4.3 Topic Modelling
Latent Dirichlet Allocation (LDA) (Blei et al, 2003)
is a generative model in which a document is mod-
eled as a finite mixture of topics, where each topic is
represented as a multinomial distribution of words.
We treat each tweet as a document. Topics sensi-
tive to each target word are generated from its corre-
sponding background tweets. We topic model each
target word individually,4 and create a topic vector
for each word usage based on the topic allocations of
the context words in that usage. We use Gibbs sam-
pling in Mallet (McCallum, 2002) for training and
inference of the LDA model. We experimented with
the number of topics T for each target word ranging
from 2 to 500. We optimized the hyper parameters
by choosing those which best fit the data every 20 it-
erations over a total of 800 iterations, following 200
burn-in iterations.
4Unlike Lui et al (2012) we found a single topic model for
all target words to perform very poorly.
l
l l
l
l l l
l
l l
l l l
l
l l l
2 3 5 8 10 20 30 50 100 150 200 250 300 350 400 450 500
T
?
0.1
0.0
0.1
0.2
0.3
Sp
ea
rm
an
 
co
rre
lati
on
 
? l Original
Expanded
RandExpanded
Figure 1: Spearman rank correlation (?) for LDA for
varying numbers of topics (T ) using different back-
ground corpora.
5 Results
We evaluate the above methods for word usage sim-
ilarity on the dataset constructed in Section 3.2. We
evaluate our models against the mean human ratings
using Spearman?s rank correlation. Table 2 presents
results for each method using each background cor-
pus. The results for LDA are for the optimal set-
ting for T (8, 5, and 20 for ORIGINAL, EXPANDED,
and RANDEXPANDED, respectively). LDA is su-
perior to both the baseline and WTMF using each
background corpus. The performance of LDA im-
proves for EXPANDED but not RANDEXPANDED,
over ORIGINAL, demonstrating the effectiveness of
our hashtag based corpus expansion strategy.
In Figure 1 we plot the rank correlation of LDA
across all words against the number of topics (T ).
As the number of topics increases beyond a certain
number, the rank correlation decreases. LDA trained
on EXPANDED consistently outperforms ORIGINAL
and RANDEXPANDED for lower values of T (i.e.,
T <= 20).
In Table 3, we show results for LDA over each tar-
get word, for ORIGINAL and EXPANDED. (Results
for RANDEXPANDED are not shown but are similar
to ORIGINAL.) Results are shown for the optimal
T for each lemma, and the optimal T over all lem-
mas. Optimizing T for each lemma gives an indica-
tion of the upperbound of the performance of LDA,
and unsurprisingly gives better performance than us-
251
Lemma
ORIGINAL EXPANDED
Per lemma Global Per lemma Global
? (T ) ? (T=8) ? (T ) ? (T=5)
bar 0.39 (10) 0.28 0.35 (50) 0.1
charge 0.27 (30) 0.04 0.33 (20) ?0.08
execution 0.43 (8) 0.43 0.58 (5) 0.58
field 0.46 (5) 0.33 0.53 (10) 0.32
figure 0.24 (150) 0.06 0.24 (250) 0.14
function 0.44 (8) 0.44 0.40 (10) 0.27
investigator 0.3 (30) 0.05 0.50 (5) 0.50
match 0.28 (5) 0.26 0.45 (5) 0.45
paper 0.29 (30) 0.20 0.32 (30) 0.22
post 0.1 (3) ?0.13 0.2 (30) ?0.01
Table 3: Spearman?s ? using LDA for the optimal T
for each lemma (Per lemma) and the best T over all
lemmas (Global) using ORIGINAL and EXPANDED.
? values that are significant at the 0.05 level are
shown in bold.
ing a fixed T for all lemmas. This suggests that ap-
proaches that learn an appropriate number of topics
(e.g., HDP, (Teh et al, 2006)) could give further im-
provements; however, given the size of the dataset,
the computational cost of HDP could be a limitation.
Contrasting our results with a fixed number of
topics to those of Lui et al (2012), our highest rank
correlation of 0.29 (T = 5 using EXPANDED) is
higher than the 0.11 they achieved over the origi-
nal USIM dataset (where the documents offer an or-
der of magnitude more context). The higher inter-
annotator agreement for USIM-tweet compared to
the original USIM dataset (Section 3.2), combined
with this finding, demonstrates that USIM over mi-
croblog data is indeed a viable task.
Returning to the performance of LDA relative
to WTMF in Table 2, the poor performance of
WTMF is somewhat surprising here given WTMF?s
encouraging performance on the somewhat similar
SemEval-2012 STS task. This difference is possi-
bly due to the differences in the tasks: usage simi-
larity measures the similarity of the usage of a tar-
get word while STS measures the similarity of two
texts. Differences in domain ? i.e., Twitter here
and more standard text for STS ? could also be a
factor. WTMF attempts to alleviate the data spar-
sity problem by adding information from ?missing?
words in a text by assigning a small weight to these
missing words. Because of the prevalence of lexical
variation on Twitter, some missing words might be
counted multiple times (e.g., coool, kool, and kewl
all meaning roughly cool) thus indirectly assigning
higher weights to the missing words leading to the
lower performance of WTMF compared to LDA.
6 Summary
We have analysed word usage similarity in mi-
croblog data. We developed a new dataset (USIM-
tweet) for usage similarity of nouns over Twitter.
We applied a topic modelling approach to this task,
and contrasted it with baseline and benchmark meth-
ods. Our results show that the LDA-based approach
outperforms the other methods over microblog data.
Moreover, our novel hashtag-based corpus expan-
sion strategy substantially improves the results.
In future work, we plan to expand our annotated
dataset, experiment with larger background corpora,
and explore alternative corpus expansion strategies.
We also intend to further analyse the difference in
performance LDA and WTMF on similar data.
Acknowledgements
We are very grateful to Timothy Baldwin for his
tremendous help with this work. We additionally
thank Diana McCarthy for her insightful comments
on this paper. We also acknowledge the European
Erasmus Mundus Masters Program in Language and
Communication Technologies from the European
Commission.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excel-
lence programme.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 385?393, Montreal, Canada.
Shea Bennett. 2012. Twitter on track for
500 million total users by March, 250 mil-
lion active users by end of 2012. http:
//www.mediabistro.com/alltwitter/
twitter-active-total-users_b17655.
252
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word usages.
In Proceedings of the Joint conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference
on Natural Language Processing of the Asian Feder-
ation of Natural Language Processing (ACL-IJCNLP
2009), pages 10?18, Singapore.
Weiwei Guo and Mona Diab. 2012a. Modeling sen-
tences in the latent space. In Proc. of the 50th Annual
Meeting of the Association for Computational Linguis-
tics, pages 864?872, Jeju, Republic of Korea.
Weiwei Guo and Mona Diab. 2012b. Weiwei: A sim-
ple unsupervised latent semantics based approach for
sentence similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 586?590, Montreal, Canada.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary for
microblogs. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning 2012,
pages 421?432, Jeju, Republic of Korea.
Liangjie Hong and Brian D Davison. 2010. Empirical
study of topic modeling in twitter. In Proc. of the First
Workshop on Social Media Analytics, pages 80?88.
Lichan Hong, Gregoria Convertino, and Ed H. Chi. 2011.
Language matters in Twitter: A large scale study. In
Proceedings of the 5th International Conference on
Weblogs and Social Media (ICWSM 2011), pages 518?
521, Barcelona, Spain.
Ou Jin, Nathan N Liu, Kai Zhao, Yong Yu, and Qiang
Yang. 2011. Transferring topical knowledge from
auxiliary long texts for short text clustering. In Proc.
of the 20th ACM International Conference on Informa-
tion and Knowledge Management, pages 775?784.
Jey Han Lau, Nigel Collier, and Timothy Baldwin.
2012a. On-line trend analysis with topic models:
#twitter trends detection topic model online. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
1519?1534, Mumbai, India.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012b. Word sense in-
duction for novel sense detection. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics (EACL
2012), pages 591?601, Avignon, France.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL 2012) Demo Session,
pages 25?30, Jeju, Republic of Korea.
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Workshop 2012 (ALTW 2012), pages 33?
41, Dunedin, New Zealand.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task 14:
Word sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 63?68, Uppsala, Sweden.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
4th International Conference on Weblogs and Social
Media, pages 122?129, Washington, USA.
Miles Osborne, Sasa Petrovic?, Richard McCreadie, Craig
Macdonald, and Iadh Ounis. 2012. Bieber no more:
First story detection using Twitter and Wikipedia. In
Proceedings of the SIGIR 2012 Workshop on Time-
aware Information Access, Portland, USA.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, and Nathan Schneider. 2012. Part-of-speech
tagging for Twitter: Word clusters and other advances.
Technical Report CMU-ML-12-107, Carnegie Mellon
University.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In In-
ternational AAAI Conference on Weblogs and Social
Media, volume 5, pages 130?137.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1524?1534, Edinburgh, UK.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the
20th International Conference on Machine Learning,
Washington, USA.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
253
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 54?60,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Exemplar-based Word-Space Model for Compositionality Detection: Shared
task system description
Siva Reddy
University of York, UK
siva@cs.york.ac.uk
Suresh Manandhar
University of York, UK
suresh@cs.york.ac.uk
Diana McCarthy
Lexical Computing Ltd, UK
diana@dianamccarthy.co.uk
Spandana Gella
University of York, UK
spandana@cs.york.ac.uk
Abstract
In this paper, we highlight the problems of
polysemy in word space models of compo-
sitionality detection. Most models represent
each word as a single prototype-based vec-
tor without addressing polysemy. We propose
an exemplar-based model which is designed
to handle polysemy. This model is tested for
compositionality detection and it is found to
outperform existing prototype-based models.
We have participated in the shared task (Bie-
mann and Giesbrecht, 2011) and our best per-
forming exemplar-model is ranked first in two
types of evaluations and second in two other
evaluations.
1 Introduction
In the field of computational semantics, to represent
the meaning of a compound word, two mechanisms
are commonly used. One is based on the distribu-
tional hypothesis (Harris, 1954) and the other is on
the principle of semantic compositionality (Partee,
1995, p. 313).
The distributional hypothesis (DH) states that
words that occur in similar contexts tend to have
similar meanings. Using this hypothesis, distribu-
tional models like the Word-space model (WSM,
Sahlgren, 2006) represent a target word?s meaning
as a context vector (location in space). The simi-
larity between two meanings is the closeness (prox-
imity) between the vectors. The context vector of a
target word is built from its distributional behaviour
observed in a corpus. Similarly, the context vector of
a compound word can be built by treating the com-
pound as a single word. We refer to such a vector as
a DH-based vector.
The other mechanism is based on the principle of
semantic compositionality (PSC) which states that
the meaning of a compound word is a function of,
and only of, the meaning of its parts and the way
in which the parts are combined. If the meaning of
a part is represented in a WSM using the distribu-
tional hypothesis, then the principle can be applied
to compose the distributional behaviour of a com-
pound word from its parts without actually using the
corpus instances of the compound. We refer to this
as a PSC-based vector. So a PSC-based is composed
of component DH-based vectors.
Both of these two mechanisms are capable of de-
termining the meaning vector of a compound word.
For a given compound, if a DH-based vector and
a PSC-based vector of the compound are projected
into an identical space, one would expect the vec-
tors to occupy the same location i.e. both the vectors
should be nearly the same. However the principle
of semantic compositionality does not hold for non-
compositional compounds, which is actually what
the existing WSMs of compositionality detection ex-
ploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006;
Schone and Jurafsky, 2001). The DH-based and
PSC-based vectors are expected to have high simi-
larity when a compound is compositional and low
similarity for non-compositional compounds.
Most methods in WSM (Turney and Pantel, 2010)
represent a word as a single context vector built from
merging all its corpus instances. Such a representa-
tion is called the prototype-based modelling (Mur-
phy, 2002). These prototype-based vectors do not
54
distinguish the instances according to the senses of
a target word. Since most compounds are less am-
biguous than single words, there is less need for dis-
tinguishing instances in a DH-based prototype vec-
tor of a compound and we do not address that here
but leave ambiguity of compounds for future work.
However the constituent words of the compound are
more ambiguous. When DH-based vectors of the
constituent words are used for composing the PSC-
based vector of the compound, the resulting vec-
tor may contain instances, and therefore contexts,
that are not relevant for the given compound. These
noisy contexts effect the similarity between the PSC-
based vector and the DH-based vector of the com-
pound. Basing compositionality judgements on a
such a noisy similarity value is no longer reliable.
In this paper, we address this problem of pol-
ysemy of constituent words of a compound using
an exemplar-based modelling (Smith and Medin,
1981). In exemplar-based modelling of WSM (Erk
and Pado?, 2010), each word is represented by all its
corpus instances (exemplars) without merging them
into a single vector. Depending upon the purpose,
only relevant exemplars of the target word are acti-
vated and then these are merged to form a refined
prototype-vector which is less-noisy compared to
the original prototype-vector. Exemplar-based mod-
els are more powerful than prototype-based ones be-
cause they retain specific instance information.
We have evaluated our models on the validation
data released in the shared task (Biemann and Gies-
brecht, 2011). Based on the validation results, we
have chosen three systems for public evaluation and
participated in the shared task (Biemann and Gies-
brecht, 2011).
2 Word Space Model
In this section, construction of WSM for all our ex-
periments is described. We use Sketch Engine1 (Kil-
garriff et al, 2004) to retrieve all the exemplars for
a target word or a pattern using corpus query lan-
guage. Let w1 w2 be a compound word with con-
stituent words w1 and w2. Ew denotes the set of
exemplars of w. Vw is the prototype vector of the
word w, which is built by merging all the exemplars
in Ew
1Sketch Engine http://www.sketchengine.co.uk
For the purposes of producing a PSC-based vector
for a compound, a vector of a constituent word is
built using only the exemplars which do not contain
the compound. Note that the vectors are sensitive
to a compound?s word-order since the exemplars of
w1 w2 are not the same as w2 w1.
We use other WSM settings following Mitchell
and Lapata (2008). The dimensions of the WSM
are the top 2000 content words in the given corpus
(along with their coarse-grained part-of-speech in-
formation). Cosine similarity (sim) is used to mea-
sure the similarity between two vectors. Values at
the specific positions in the vector representing con-
text words are set to the ratio of the probability of
the context word given the target word to the overall
probability of the context word. The context window
of a target word?s exemplar is the whole sentence of
the target word excluding the target word. Our lan-
guage of interest is English. We use the ukWaC cor-
pus (Ferraresi et al, 2008) for producing out WSMs.
3 Related Work
As described in Section 1, most WSM models for
compositionality detection measure the similarity
between the true distributional vector Vw1w2 of the
compound and the composed vector Vw1?w2 , where
? denotes a compositionality function. If the simi-
larity is high, the compound is treated as composi-
tional or else non-compositional.
Giesbrecht (2009); Katz and Giesbrecht (2006);
Schone and Jurafsky (2001) obtained the compo-
sitionality vector of w1 w2 using vector addition
Vw1?w2 = aVw1 + bVw2 . In this approach, if
sim(Vw1?w2 , Vw1w2) > ?, the compound is clas-
sified as compositional, where ? is a threshold for
deciding compositionality. Global values of a and b
were chosen by optimizing the performance on the
development set. It was found that no single thresh-
old value ? held for all compounds. Changing the
threshold alters performance arbitrarily. This might
be due to the polysemous nature of the constituent
words which makes the composed vector Vw1?w2
filled with noisy contexts and thus making the judge-
ment unpredictable.
In the above model, if a=0 and b=1, the result-
ing model is similar to that of Baldwin et al (2003).
They also observe similar behaviour of the thresh-
55
old ?. We try to address this problem by addressing
the polysemy in WSMs using exemplar-based mod-
elling.
The above models use a simple addition based
compositionality function. Mitchell and Lapata
(2008) observed that a simple multiplication func-
tion modelled compositionality better than addi-
tion. Contrary to that, Guevara (2011) observed
additive models worked well for building composi-
tional vectors. In our work, we try using evidence
from both compositionality functions, simple addi-
tion and simple multiplication.
Bannard et al (2003); McCarthy et al (2003) ob-
served that methods based on distributional similar-
ities between a phrase and its constituent words help
when determining the compositionality behaviour of
phrases. We therefore also use evidence from the
similarities between each constituent word and the
compound.
4 Our Approach: Exemplar-based Model
Our approach works as follows. Firstly, given a
compound w1 w2, we build its DH-based proto-
type vector Vw1w2 from all its exemplars Ew1w2 .
Secondly, we remove irrelevant exemplars in Ew1
and Ew2 of constituent words and build the refined
prototype vectors Vwr1 and Vwr2 of the constituent
words w1 and w2 respectively. These refined vec-
tors are used to compose the PSC-based vectors 2 of
the compound. Related work to ours is (Reisinger
and Mooney, 2010) where exemplars of a word are
first clustered and then prototype vectors are built.
This work does not relate to compositionality but to
measuring semantic similarity of single words. As
such, their clusters are not influenced by other words
whereas in our approach for detecting composition-
ality, the other constituent word plays a major role.
We use the compositionality functions, sim-
ple addition and simple multiplication to build
Vwr1+wr2 and Vwr1?wr2 respectively. Based on
the similarities sim(Vw1w2 , Vwr1), sim(Vw1w2 , Vwr2),
sim(Vw1w2 , Vwr1+wr2) and sim(Vw1w2 , Vwr1?wr2), we
decide if the compound is compositional or non-
compositional. These steps are described in a little
more detail below.
2Note that we use two PSC-based vectors for representing a
compound.
4.1 Building Refined Prototype Vectors
We aim to remove irrelevant exemplars of one con-
stituent word with the help of the other constituent
word?s distributional behaviour. For example, let
us take the compound traffic light. Light occurs
in many contexts such as quantum theory, optics,
lamps and spiritual theory. In ukWaC, light has
316,126 instances. Not all these exemplars are rel-
evant to compose the PSC-based vector of traffic
light. These irrelevant exemplars increases the se-
mantic differences between traffic light and light and
thus increase the differences between Vtraffic?light
and Vtraffic light. sim(Vlight, Vtraffic light) is found to be
0.27.
Our intuition and motivation for exemplar re-
moval is that it is beneficiary to choose only the
exemplars of light which share similar contexts of
traffic since traffic light should have contexts sim-
ilar to both traffic and light if it is compositional.
We rank each exemplar of light based on common
co-occurrences of traffic and also words which are
distributionally similar to traffic. Co-occurrences of
traffic are the context words which frequently occur
with traffic, e.g. car, road etc. Using these, the
exemplar from a sentence such as ?Cameras capture
cars running red lights . . .? will be ranked higher
than one which does not have contexts related to
traffic. The distributionally similar words to traffic
are the words (like synonyms, antonyms) which are
similar to traffic in that they occur in similar con-
texts, e.g. transport, flow etc. Using these distri-
butionally similar words helps reduce the impact of
data sparseness and helps prioritise contexts of traf-
fic which are semantically related. We use Sketch
Engine to compute the scores of a word observed
in a given corpus. Sketch Engine scores the co-
occurrences (collocations) using logDice motivated
by (Curran, 2003) and distributionally related words
using (Rychly? and Kilgarriff, 2007; Lexical Com-
puting Ltd., 2007). For a given word, both of these
scores are normalised in the range (0,1)
All the exemplars of light are ranked based on
the co-occurrences of these collocations and distri-
butionally related words of traffic using
strafficE ? Elight =
?
c ? E
xEc ? y
traffic
c (1)
where strafficE ? Elight stands for the relevance score of the
56
exemplar E w.r.t. traffic, c for context word in the
exemplar E, xEc is the coordinate value (contextual
score) of the context word c in the exemplar E and
ytrafficc is the score of the context word c w.r.t. traffic.
A refined prototype vector of light is then built by
merging the top n exemplars of light
Vlightr =
n?
ei?Etrafficlight ;i=0
ei (2)
where Etrafficlight are the set of exemplars of light
ranked using co-occurrence information from the
other constituent word traffic. n is chosen such that
sim(Vlightr , Vtraffic light) is maximised. This similar-
ity is observed to be greatest using just 2286 (less
than 1%) of the total exemplars of light. After ex-
emplar removal, sim(Vlightr , Vtraffic light) increased to
0.47 from the initial value of 0.27. Though n is cho-
sen by maximising similarity, which is not desirable
for non-compositional compounds, the lack of simi-
larity will give the strongest possible indication that
a compound is not compositional.
4.2 Building Compositional Vectors
We use the compositionality functions, simple ad-
dition and simple multiplication to build composi-
tional vectors Vwr1+wr2 and Vwr1?wr2 . These are as de-
scribed in (Mitchell and Lapata, 2008). In model ad-
dition, Vw1?w2 = aVw1 + bVw2 , all the previous ap-
proaches use static values of a and b. Instead, we use
dynamic weights computed from the participating
vectors using a =
sim(Vw1w2 ,Vw1 )
sim(Vw1w2 ,Vw1 )+sim(Vw1w2 ,Vw2 )
and b = 1?a. These weights differ from compound
to compound.
4.3 Compositionality Judgement
To judge if a compound is compositional or non-
compositional, previous approaches (see Section 3)
base their judgement on a single similarity value. As
discussed, we base our judgement based on the col-
lective evidences from all the similarity values using
a linear equation of the form
?(Vwr1 , Vwr2) = a0 + a1.sim(Vw1w2 , Vwr1)
+ a2.sim(Vw1w2 , Vwr2) (3)
+ a3.sim(Vw1w2 , Vwr1+wr2)
+ a4.sim(Vw1w2 , Vwr1?wr2)
Model APD Acc.
Exm-Best 13.09 88.0
Pro-Addn 15.42 76.0
Pro-Mult 17.52 80.0
Pro-Best 15.12 80.0
Table 1: Average Point Difference (APD) and Av-
erage Accuracy (Acc.) of Compositionality Judge-
ments
where the value of ? denotes the compositionality
score. The range of ? is in between 0-100. If ? ?
34, the compound is treated as non-compositional,
34 < ? < 67 as medium compositional and ? ?
67 as highly compositional. The parameters ai?s
are estimated using ordinary least square regression
by training over the training data released in the
shared task (Biemann and Giesbrecht, 2011). For
the three categories ? adjective-noun, verb-object
and subject-verb ? the parameters are estimated sep-
arately.
Note that if a1 = a2 = a4 = 0, the model bases
its judgement only on addition. Similarly if a1 =
a2 = a3 = 0, the model bases its judgement only on
multiplication.
We also experimented with combinations such as
?(Vwr1 , Vw2) and ?(Vw1 , Vwr2) i.e. using refined vec-
tor for one of the constituent word and the unrefined
prototype vector for the other constituent word.
4.4 Selecting the best model
To participate in the shared task, we have selected
the best performing model by evaluating the mod-
els on the validation data released in the shared task
(Biemann and Giesbrecht, 2011). Table 1 displays
the results on the validation data. The average point
difference is calculated by taking the average of the
difference in a model?s score ? and the gold score
annotated by humans, over all compounds. Table 1
also displays the overall accuracy of coarse grained
labels ? low, medium and high.
Best performance for verb(v)-object(o) com-
pounds is found for the combination ?(Vvr , Vor) of
Equation 3. For subject(s)-verb(v) compounds, it is
for ?(Vsr , Vvr) and a3 = a4 = 0. For adjective(j)-
noun(n) compounds, it is ?(Vjr , Vn). We are not
certain of the reason for this difference, perhaps
there may be less ambiguity of words within specific
grammatical relationships or it may be simply due to
57
TotPrd Spearman ? Kendalls ?
Rand-Base 174 0.02 0.02
Exm-Best 169 0.35 0.24
Pro-Best 169 0.33 0.23
Exm 169 0.26 0.18
SharedTaskNextBest 174 0.33 0.23
Table 2: Correlation Scores
the actual compounds in those categories. We leave
analysis of this for future work. We combined the
outputs of these category-specific models to build
the best model Exm-Best.
For comparison, results of standard mod-
els prototype addition (Pro-Addn) and prototype-
multiplication (Pro-Mult) are also displayed in Table
1. Pro-Addn can be represented as ?(Vw1 , Vw2) with
a1 = a2 = a4 = 0. Pro-Mult can be represented as
?(Vw1 , Vw2) with a1 = a2 = a3 = 0. Pro-Best is
the best performing model in prototype-based mod-
elling. It is found to be ?(Vw1 , Vw2). (Note: De-
pending upon the compound type, some of the ai?s
in Pro-Best may be 0).
Overall, exemplar-based modelling excelled in
both the evaluations, average point difference and
coarse-grained label accuracies. The systems Exm-
Best, Pro-Best and Exm ?(Vwr1 , Vwr2) were submit-
ted for the public evaluation in the shared task. All
the model parameters were estimated by regression
on the task?s training data separately for the 3 com-
pound types as described in Section 4.3. We perform
the regression separately for these classes to max-
imise performance. In the future, we will investigate
whether these settings gave us better results on the
test data compared to setting the values the same re-
gardless of the category of compound.
5 Shared Task Results
Table 2 displays Spearman ? and Kendalls ? corre-
lation scores of all the models. TotPrd stands for
the total number of predictions. Rand-Base is the
baseline system which randomly assigns a compo-
sitionality score for a compound. Our model Exm-
Best was the best performing system compared to
all other systems in this evaluation criteria. Shared-
TaskNextBest is the next best performing system
apart from our models. Due to lemmatization er-
rors in the test data, our models could only predict
judgements for 169 out of 174 compounds.
All ADJ-NN V-SUBJ V-OBJ
Rand-Base 32.82 34.57 29.83 32.34
Zero-Base 23.42 24.67 17.03 25.47
Exm-Best 16.51 15.19 15.72 18.6
Pro-Best 16.79 14.62 18.89 18.31
Exm 17.28 15.82 18.18 18.6
SharedTaskBest 16.19 14.93 21.64 14.66
Table 3: Average Point Difference Scores
All ADJ-NN V-SUBJ V-OBJ
Rand-Base 0.297 0.288 0.308 0.30
Zero-Base 0.356 0.288 0.654 0.25
Most-Freq-Base 0.593 0.673 0.346 0.65
Exm-Best 0.576 0.692 0.5 0.475
Pro-Best 0.567 0.731 0.346 0.5
Exm 0.542 0.692 0.346 0.475
SharedTaskBest 0.585 0.654 0.385 0.625
Table 4: Coarse Grained Accuracy
Table 3 displays average point difference scores.
Zero-Base is a baseline system which assigns a score
of 50 to all compounds. SharedTaskBest is the over-
all best performing system. Exm-Best was ranked
second best among all the systems. For ADJ-NN
and V-SUBJ compounds, the best performing sys-
tems in the shared task are Pro-Best and Exm-Best
respectively. Our models did less well on V-OBJ
compounds and we will explore the reasons for this
in future work.
Table 4 displays coarse grained scores. As above,
similar behaviour is observed for coarse grained ac-
curacies. Most-Freq-Base is the baseline system
which assigns the most frequent coarse-grained la-
bel for a compound based on its type (ADJ-NN, V-
SUBJ, V-OBJ) as observed in training data. Most-
Freq-Base outperforms all other systems.
6 Conclusions
In this paper, we examined the effect of polysemy
in word space models for compositionality detec-
tion. We showed exemplar-based WSM is effective
in dealing with polysemy. Also, we use multiple
evidences for compositionality detection rather than
basing our judgement on a single evidence. Over-
all, performance of the Exemplar-based models of
compositionality detection is found to be superior to
prototype-based models.
58
References
Baldwin, T., Bannard, C., Tanaka, T., and Widdows,
D. (2003). An empirical model of multiword ex-
pression decomposability. In Proceedings of the
ACL 2003 workshop on Multiword expressions:
analysis, acquisition and treatment - Volume 18,
MWE ?03, pages 89?96, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Bannard, C., Baldwin, T., and Lascarides, A. (2003).
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 work-
shop on Multiword expressions: analysis, ac-
quisition and treatment - Volume 18, MWE ?03,
pages 65?72, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Biemann, C. and Giesbrecht, E. (2011). Distri-
butional semantics and compositionality 2011:
Shared task description and results. In Pro-
ceedings of DISCo-2011 in conjunction with ACL
2011.
Curran, J. R. (2003). From distributional to semantic
similarity. Technical report, PhD Thesis, Univer-
sity of Edinburgh.
Erk, K. and Pado?, S. (2010). Exemplar-based mod-
els for word meaning in context. In Proceed-
ings of the ACL 2010 Conference Short Papers,
ACLShort ?10, pages 92?97, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the WAC4 Workshop at
LREC 2008, Marrakesh, Morocco.
Giesbrecht, E. (2009). In search of semantic com-
positionality in vector spaces. In Proceedings
of the 17th International Conference on Concep-
tual Structures: Conceptual Structures: Leverag-
ing Semantic Technologies, ICCS ?09, pages 173?
184, Berlin, Heidelberg. Springer-Verlag.
Guevara, E. R. (2011). Computing semantic com-
positionality in distributional semantics. In Pro-
ceedings of the Ninth International Conference on
Computational Semantics, IWCS ?2011.
Harris, Z. S. (1954). Distributional structure. Word,
10:146?162.
Katz, G. and Giesbrecht, E. (2006). Automatic
identification of non-compositional multi-word
expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underly-
ing Properties, MWE ?06, pages 12?19, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Kilgarriff, A., Rychly, P., Smrz, P., and Tugwell, D.
(2004). The sketch engine. In Proceedings of EU-
RALEX.
Lexical Computing Ltd. (2007). Statistics used in
the sketch engine.
McCarthy, D., Keller, B., and Carroll, J. (2003).
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions: analysis,
acquisition and treatment - Volume 18, MWE ?03,
pages 73?80, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceed-
ings of ACL-08: HLT, pages 236?244, Columbus,
Ohio. Association for Computational Linguistics.
Murphy, G. L. (2002). The Big Book of Concepts.
The MIT Press.
Partee, B. (1995). Lexical semantics and compo-
sitionality. L. Gleitman and M. Liberman (eds.)
Language, which is Volume 1 of D. Osherson (ed.)
An Invitation to Cognitive Science (2nd Edition),
pages 311?360.
Reisinger, J. and Mooney, R. J. (2010). Multi-
prototype vector-space models of word mean-
ing. In Proceedings of the 11th Annual Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-
2010), pages 109?117.
Rychly?, P. and Kilgarriff, A. (2007). An efficient
algorithm for building a distributional thesaurus
(and other sketch engine developments). In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Ses-
sions, ACL ?07, pages 41?44, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Sahlgren, M. (2006). The Word-Space Model: Us-
ing distributional analysis to represent syntag-
59
matic and paradigmatic relations between words
in high-dimensional vector spaces. PhD thesis,
Stockholm University.
Schone, P. and Jurafsky, D. (2001). Is knowledge-
free induction of multiword unit dictionary head-
words a solved problem? In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?01.
Smith, E. E. and Medin, D. L. (1981). Categories
and concepts / Edward E. Smith and Douglas L.
Medin. Harvard University Press, Cambridge,
Mass. :.
Turney, P. D. and Pantel, P. (2010). From frequency
to meaning: vector space models of semantics. J.
Artif. Int. Res., 37:141?188.
60
