Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 753?760,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Machine Learning of Temporal Relations 
Inderjeet Mani??, Marc Verhagen?, Ben Wellner?? 
Chong Min Lee? and James Pustejovsky? 
?The MITRE Corporation 
202 Burlington Road, Bedford, MA 01730, USA 
?Department of Linguistics, Georgetown University 
37th and O Streets, Washington, DC 20036, USA 
?Department of Computer Science, Brandeis University 
415 South St., Waltham, MA 02254, USA 
{imani, wellner}@mitre.org, {marc, jamesp}@cs.brandeis.edu, cml54@georgetown.edu 
Abstract 
This paper investigates a machine learn-
ing approach for temporally ordering and 
anchoring events in natural language 
texts. To address data sparseness, we 
used temporal reasoning as an over-
sampling method to dramatically expand 
the amount of training data, resulting in 
predictive accuracy on link labeling as 
high as 93% using a Maximum Entropy 
classifier on human annotated data. This 
method compared favorably against a se-
ries of increasingly sophisticated base-
lines involving expansion of rules de-
rived from human intuitions. 
1 Introduction 
The growing interest in practical NLP applica-
tions such as question-answering and text sum-
marization places increasing demands on the 
processing of temporal information. In multi-
document summarization of news articles, it can 
be useful to know the relative order of events so 
as to merge and present information from multi-
ple news sources correctly. In question-
answering, one would like to be able to ask when 
an event occurs, or what events occurred prior to 
a particular event.  
A wealth of prior research by (Passoneau 
1988), (Webber 1988), (Hwang and Schubert 
1992), (Kamp and Reyle 1993), (Lascarides and 
Asher 1993), (Hitzeman et al 1995), (Kehler 
2000) and others, has explored the different 
knowledge sources used in inferring the temporal 
ordering of events, including temporal adver-
bials, tense, aspect, rhetorical relations, prag-
matic conventions, and background knowledge. 
For example, the narrative convention of events 
being described in the order in which they occur 
is followed in (1), but overridden by means of a 
discourse relation, Explanation in (2).  
(1) Max stood up. John greeted him.  
(2) Max fell. John pushed him.  
In addition to discourse relations, which often 
require inferences based on world knowledge, 
the ordering decisions humans carry out appear 
to involve a variety of knowledge sources, in-
cluding tense and grammatical aspect (3a), lexi-
cal aspect (3b), and temporal adverbials (3c): 
(3a) Max entered the room. He had drunk a lot 
of wine.  
(3b) Max entered the room. Mary was seated 
behind the desk.  
(3c) The company announced Tuesday that 
third-quarter sales had fallen.  
Clearly, substantial linguistic processing may 
be required for a system to make these infer-
ences, and world knowledge is hard to make 
available to a domain-independent program. An 
important strategy in this area is of course the 
development of annotated corpora than can fa-
cilitate the machine learning of such ordering 
inferences. 
This paper 1  investigates a machine learning 
approach for temporally ordering events in natu-
ral language texts. In Section 2, we describe the 
annotation scheme and annotated corpora, and 
the challenges posed by them. A basic learning 
approach is described in Section 3. To address 
data sparseness, we used temporal reasoning as 
an over-sampling method to dramatically expand 
the amount of training data.  
As we will discuss in Section 5, there are no 
standard algorithms for making these inferences 
that we can compare against. We believe 
strongly that in such situations, it?s worthwhile 
for computational linguists to devote consider-
                                                 
1Research at Georgetown and Brandeis on this prob-
lem was funded in part by a grant from the ARDA 
AQUAINT Program, Phase II.  
753
able effort to developing insightful baselines. 
Our work is, accordingly, evaluated in compari-
son against four baselines: (i) the usual majority 
class statistical baseline, shown along with each 
result, (ii) a more sophisticated baseline that uses 
hand-coded rules (Section 4.1), (iii) a hybrid 
baseline based on hand-coded rules expanded 
with Google-induced rules (Section 4.2), and (iv) 
a machine learning version that learns from im-
perfect annotation produced by (ii) (Section 4.3).  
2 Annotation Scheme and Corpora 
2.1 TimeML 
TimeML (Pustejovsky et al 2005) 
(www.timeml.org) is an annotation scheme for 
markup of events, times, and their temporal rela-
tions in news articles. The TimeML scheme flags 
tensed verbs, adjectives, and nominals with 
EVENT tags with various attributes, including 
the class of event, tense, grammatical aspect, po-
larity (negative or positive), any modal operators 
which govern the event being tagged, and cardi-
nality of the event if it?s mentioned more than 
once. Likewise, time expressions are flagged and 
their values normalized, based on TIMEX3, an 
extension of the ACE (2004) (tern.mitre.org) 
TIMEX2 annotation scheme.  
For temporal relations, TimeML defines a 
TLINK tag that links tagged events to other 
events and/or times. For example, given (3a), a 
TLINK tag orders an instance of the event of 
entering to an instance of the drinking with the 
relation type AFTER. Likewise, given the sen-
tence (3c), a TLINK tag will anchor the event 
instance of announcing to the time expression 
Tuesday (whose normalized value will be in-
ferred from context), with the relation 
IS_INCLUDED. These inferences are shown (in 
slightly abbreviated form) in the annotations in 
(4) and (5). 
(4) Max <EVENT eventID=?e1? 
class=?occurrence? tense=?past? as-
pect=?none?>entered</EVENT> the room. 
He <EVENT eventID=?e2? 
class=?occurrence? tense=?past? as-
pect=?perfect?>had drunk</EVENT>a 
lot of wine.  
<TLINK eventID=?e1? relatedToEven-
tID=?e2? relType=?AFTER?/> 
 (5) The company <EVENT even-
tID=?e1? class=?reporting? 
tense=?past? as-
pect=?none?>announced</EVENT> 
<TIMEX3 tid=?t2? type=?DATE? tempo-
ralFunction=?false? value=?1998-01-
08?>Tuesday </TIMEX3> that third-
quarter sales <EVENT eventID=?e2? 
class=?occurrence? tense=?past? as-
pect=?perfect?> had fallen</EVENT>.  
<TLINK eventID=?e1? relatedToEven-
tID=?e2? relType=?AFTER?/> 
<TLINK eventID=?e1? relatedTo-
TimeID=?t2? relType=?IS_INCLUDED?/> 
 
The anchor relation is an Event-Time TLINK, 
and the order relation is an Event-Event TLINK. 
TimeML uses 14 temporal relations in the 
TLINK RelTypes, which reduce to a disjunctive 
classification of 6 temporal relations RelTypes = 
{SIMULTANEOUS, IBEFORE, BEFORE, BE-
GINS, ENDS, INCLUDES}. An event or time is 
SIMULTANEOUS with another event or time if 
they occupy the same time interval. An event or 
time INCLUDES another event or time if the 
latter occupies a proper subinterval of the former. 
These 6 relations and their inverses map one-to-
one to 12 of Allen?s 13 basic relations (Allen 
1984)2. There has been a considerable amount of 
activity related to this scheme; we focus here on 
some of the challenges posed by the TLINK an-
notation, the part that is directly relevant to the 
temporal ordering and anchoring problems. 
2.2 Challenges 
The annotation of TimeML information is on a 
par with other challenging semantic annotation 
schemes, like PropBank, RST annotation, etc., 
where high inter-annotator reliability is crucial 
but not always achievable without massive pre-
processing to reduce the user?s workload. In Ti-
meML, inter-annotator agreement for time ex-
pressions and events is 0.83 and 0.78 (average of 
Precision and Recall) respectively, but on 
TLINKs it is 0.55 (P&R average), due to the 
large number of event pairs that can be selected 
for comparison. The time complexity of the hu-
man TLINK annotation task is quadratic in the 
number of events and times in the document. 
Two corpora have been released based on Ti-
meML: the TimeBank (Pustejovsky et al 2003) 
(we use version 1.2.a) with 186 documents and 
                                                 
2Of the 14 TLINK relations, the 6 inverse relations are re-
dundant. In order to have a disjunctive classification, SI-
MULTANEOUS and IDENTITY are collapsed, since 
IDENTITY is a subtype of SIMULTANEOUS. (Specifi-
cally, X and Y are identical if they are simultaneous and 
coreferential.) DURING and IS_INCLUDED are collapsed 
since DURING is a subtype of IS_INCLUDED that anchors 
events to times that are durations. IBEFORE (immediately 
before) corresponds to Allen?s MEETS. Allen?s OVER-
LAPS relation is not represented in TimeML. More details 
can be found at timeml.org. 
754
64,077 words of text, and the Opinion Corpus 
(www.timeml.org), with 73 documents and 
38,709 words. The TimeBank was developed in 
the early stages of TimeML development, and 
was partitioned across five annotators with dif-
ferent levels of expertise. The Opinion Corpus 
was developed very recently, and was partitioned 
across just two highly trained annotators, and 
could therefore be expected to be less noisy. In 
our experiments, we merged the two datasets to 
produce a single corpus, called OTC. 
Table 1 shows the distribution of EVENTs and 
TIMES, and TLINK RelTypes3 in the OTC. The 
majority class percentages are shown in paren-
theses. It can be seen that BEFORE and SI-
MULTANEOUS together form a majority of 
event-ordering (Event-Event) links, whereas 
most of the event anchoring (Event-Time) links 
are INCLUDES.  
 
12750 Events, 2114 Times 
Relation Event-Event Event-Time 
IBEFORE 131 15 
BEGINS 160 112 
ENDS 208 159 
SIMULTANEOUS 1528 77 
INCLUDES 950 3001 (65.3%) 
BEFORE 3170 (51.6%) 1229 
TOTAL 6147 4593 
Table 1. TLINK Class Distributions in OTC 
Corpus 
 
The lack of TLINK coverage in human anno-
tation could be helped by preprocessing, pro-
vided it meets some threshold of accuracy. Given 
the availability of a corpus like OTC, it is natural 
to try a machine learning approach to see if it can 
be used to provide that preprocessing. However, 
the noise in the corpus and the sparseness of 
links present challenges to a learning approach. 
3 Machine Learning Approach 
3.1 Initial Learner 
There are several sub-problems related to in-
ferring event anchoring and event ordering. Once 
a tagger has tagged the events and times, the first 
task (A) is to link events and/or times, and the 
second task (B) is to label the links. Task A is 
hard to evaluate since, in the absence of massive 
preprocessing, many links are ignored by the 
human in creating the annotated corpora. In addi-
                                                 
3The number of TLINKs shown is based on the number of 
TLINK vectors extracted from the OTC. 
tion, a program, as a baseline, can trivially link 
all tagged events and times, getting 100% recall 
on Task A. We focus here on Task B, the label-
ing task. In the case of humans, in fact, when a 
TLINK is posited by both annotators between the 
same pairs of events or times, the inter-annotator 
agreement on the labels is a .77 average of P&R. 
To ensure replicability of results, we assume per-
fect (i.e., OTC-supplied) events, times, and links.  
Thus, we can consider TLINK inference as the 
following classification problem: given an or-
dered pair of elements X and Y, where X and Y 
are events or times which the human has related 
temporally via a TLINK, the classifier has to as-
sign a label in RelTypes. Using RelTypes instead 
of RelTypes ?  {NONE} also avoids the prob-
lem of heavily skewing the data towards the 
NONE class.  
To construct feature vectors for machine 
learning, we took each TLINK in the corpus and 
used the given TimeML features, with the 
TLINK class being the vector?s class feature.  
For replicability by other users of these corpora, 
and to be able to isolate the effect of components, 
we used ?perfect? features; no feature engineer-
ing was attempted. The features were, for each 
event in an event-ordering pair, the event-class, 
aspect, modality, tense and negation (all nominal 
features); event string, and signal (a preposi-
tion/adverb, e.g., reported on Tuesday), which 
are string features, and contextual features indi-
cating whether the same tense and same aspect 
are true of both elements in the event pair. For 
event-time links, we used the above event and 
signal features along with TIMEX3 time features. 
For learning, we used an off-the-shelf Maxi-
mum Entropy (ME) classifier (from Carafe, 
available at sourceforge.net/projects/carafe). As 
shown in the UNCLOSED (ME) column in Ta-
ble 24, accuracy of the unclosed ME classifier 
does not go above 77%, though it?s always better 
than the majority class (in parentheses). We also 
tried a variety of other classifiers, including the 
SMO support-vector machine and the na?ve 
Bayes tools in WEKA (www.weka.net.nz). SMO 
performance (but not na?ve Bayes) was compa-
rable with ME, with SMO trailing it in a few 
cases (to save space, we report just ME perform-
ance). It?s possible that feature engineering could 
improve performance, but since this is ?perfect? 
data, the result is not encouraging.  
                                                 
4All machine learning results, except for ME-C in Table 4, 
use 10-fold cross-validation. ?Accuracy? in tables is Predic-
tive Accuracy. 
755
 
 
 UNCLOSED (ME) CLOSED (ME-C) 
 Event-Event Event-Time Event-Event Event-Time 
Accuracy: 62.5 (51.6) 76.13 (65.3) 93.1 (75.2) 88.25 (62.3) 
Relation Prec Rec F Prec Rec F Prec Rec F Prec Rec F 
IBEFORE 50.00 27.27 35.39 0 0 0 77.78 60.86 68.29 0 0 0 
BEGINS 50.00 41.18 45.16 60.00 50.00 54.54 85.25 82.54 83.87 76.47 74.28 75.36 
ENDS 94.74 66.67 78.26 41.67 27.78 33.33 87.83 94.20 90.90 79.31 77.97 78.62 
SIMULTANEOUS 50.35 50.00 50.17 33.33 20.00 25.00 62.50 38.60 47.72 73.68 56.00 63.63 
INCLUDES 47.88 34.34 40.00 80.92 62.72 84.29 90.41 88.23 89.30 86.07 80.78 83.34 
BEFORE 68.85 79.24 73.68 70.47 62.72 66.37 94.95 97.26 96.09 90.16 93.56 91.83 
 
Table 2. Machine learning results using unclosed and closed data
 
3.2 Expanding Training Data using Tem-
poral Reasoning 
To expand our training set, we use a temporal  
closure component SputLink (Verhagen 2004), 
that takes known temporal relations in a text and  
derives new implied relations from them, in ef-
fect making explicit what was implicit. SputLink 
was inspired by (Setzer and Gaizauskas 2000) 
and is based on Allen?s interval algebra, taking 
into account the limitations on that algebra that 
were pointed out by (Vilain et al 1990). It is ba-
sically a constraint propagation algorithm that 
uses a transitivity table to model the composi-
tional behavior of all pairs of relations in a 
document. SputLink?s transitivity table is repre-
sented by 745 axioms. An example axiom:  
 
If relation(A, B) = BEFORE && 
   relation(B, C) = INCLUDES 
then infer relation(A, C) = BEFORE 
 
Once the TLINKs in each document in the 
corpus are closed using SputLink, the same vec-
tor generation procedure and feature representa-
tion described in Section 3.1 are used. The effect 
of closing the TLINKs on the corpus has a dra-
matic impact on learning. Table 2, in the 
CLOSED (ME-C) column shows that accura-
cies for this method (called ME-C, for Maximum 
Entropy learning with closure) are now in the 
high 80?s and low 90?s, and still outperform the 
closed majority class (shown in parentheses).  
What is the reason for the improvement?5 One 
reason is the dramatic increase in the amount of 
training data. The more connected the initial un-
                                                 
5Interestingly, performance does not improve for SIMUL-
TANEOUS.  The reason for this might be due to the rela-
tively modest increase in SIMULTANEOUS relations from 
applying closure (roughly factor of 2). 
closed graph for a document is in TLINKs, the 
greater the impact in terms of closure. When the 
OTC is closed, the number of TLINKs goes up 
by more than 11 times, from 6147 Event-Event 
and 4593 Event-Time TLINKs to 91,157 Event-
Event and 29,963 Event-Time TLINKs. The 
number of BEFORE links goes up from 3170 
(51.6%) Event-Event and 1229 Event-Time 
TLINKs (26.75%) to 68585 (75.2%) Event-
Event and 18665 (62.3%) Event-Time TLINKs, 
making BEFORE the majority class in the closed 
data for both Event-Event and Event-Time 
TLINKs. There are only an average of 0.84 
TLINKs per event before closure, but after clo-
sure it shoots up to 9.49 TLINKs per event. 
(Note that as a result, the majority class percent-
ages for the closed data have changed from the 
unclosed data.) 
Being able to bootstrap more training data is 
of course very useful. However, we need to dig 
deeper to investigate how the increase in data 
affected the machine learning. The improvement 
provided by temporal closure can be explained 
by three factors:  (1) closure effectively creates a 
new classification problem with many more in-
stances, providing more data to train on; (2) the 
class distribution is further skewed which results 
in a higher majority class baseline (3) closure 
produces additional data in such a way as to in-
crease the frequencies and statistical power of 
existing features in the unclosed data, as opposed 
to adding new features.  For example, with un-
closed data, given A BEFORE B and B BE-
FORE C, closure generates A BEFORE C which 
provides more significance for the features re-
lated to A and C appearing as first and second 
arguments, respectively, in a BEFORE relation.  
In order to help determine the effects of the 
above factors, we carried out two experiments in 
which we sampled 6145 vectors from the closed 
756
data ? i.e. approximately the number of Event-
Event vectors in the unclosed data.  This effec-
tively removed the contribution of factor (1) 
above. The first experiment (Closed Class Dis-
tribution) simply sampled 6145 instances uni-
formly from the closed instances, while the sec-
ond experiment (Unclosed Class Distribution) 
sampled instances according to the same distri-
bution as the unclosed data. Table 3 shows these 
results.  The greater class distribution skew in the 
closed data clearly contributes to improved accu-
racy. However, when using the same class distri-
bution as the unclosed data (removing factor (2) 
from above), the accuracy, 76%, is higher than 
using the full unclosed data.  This indicates that 
closure does indeed help according to factor (3). 
4 Comparison against Baselines 
4.1 Hand-Coded Rules 
Humans have strong intuitions about rules for 
temporal ordering, as we indicated in discussing 
sentences (1) to (3). Such intuitions led to the 
development of pattern matching rules incorpo-
rated in a TLINK tagger called GTag. GTag 
takes a document with TimeML tags, along with 
syntactic information from part-of-speech tag-
ging and chunking from Carafe, and then uses 
187 syntactic and lexical rules to infer and label 
TLINKs between tagged events and other tagged 
events or times. The tagger takes pairs of 
TLINKable items (event and/or time) and 
searches for the single most-confident rule to 
apply to it, if any, to produce a labeled TLINK 
between those items. Each (if-then) rule has a 
left-hand side which consists of a conjunction of 
tests based on TimeML-related feature combina-
tions (TimeML features along with part-of-
speech and chunk-related features), and a right-
hand side which is an assignment to one of the 
TimeML TLINK classes.  
The rule patterns are grouped into several dif-
ferent classes: (i) the event is anchored with or 
without a signal to a time expression within the 
same clause, e.g., (3c), (ii) the event is anchored 
without a signal to the document date (as is often 
the case for reporting verbs in news), (iii) an 
event is linked to another event in the same sen-
tence, e.g., (3c), and (iv) the event in a main 
clause of one sentence is anchored with a signal 
or tense/aspect cue to an event in the main clause 
of the previous sentence, e.g., (1-2), (3a-b). 
The performance of this baseline is shown in 
Table 4 (line GTag). The top most accurate rule 
(87% accuracy) was GTag Rule 6.6, which links 
a past-tense event verb joined by a conjunction to 
another past-tense event verb as being BEFORE 
the latter (e.g., they traveled and slept the 
night ..): 
 
If sameSentence=YES && 
 sentenceType=ANY && 
 conjBetweenEvents=YES && 
 arg1.class=EVENT && 
 arg2.class=EVENT && 
 arg1.tense=PAST && 
 arg2.tense=PAST && 
 arg1.aspect=NONE && 
 arg2.aspect=NONE && 
 arg1.pos=VB && 
 arg2.pos=VB && 
 arg1.firstVbEvent=ANY && 
 arg2.firstVbEvent=ANY  
then infer relation=BEFORE 
 
The vast majority of the intuition-bred rules 
have very low accuracy compared to ME-C, with 
intuitions failing for various feature combina-
tions and relations (for relations, for example, 
GTag lacks rules for IBEFORE, STARTS, and 
ENDS). The bottom-line here is that even when 
heuristic preferences are intuited, those prefer-
ences need to be guided by empirical data, 
whereas hand-coded rules are relatively ignorant 
of the distributions that are found in data. 
4.2 Adding Google-Induced Lexical Rules 
One might argue that the above baseline is too 
weak, since it doesn?t allow for a rich set of lexi-
cal relations. For example, pushing can result in 
falling, killing always results in death, and so 
forth. These kinds of defeasible rules have been 
investigated in the semantics literature, including 
the work of Lascarides and Asher cited in Sec-
tion 1.  
However, rather than hand-creating lexical 
rules and running into the same limitations as 
with GTag?s rules, we used an empirically-
derived resource called VerbOcean (Chklovski 
and Pantel 2004), available at 
http://semantics.isi.edu/ocean. This resource con-
sists of lexical relations mined from Google 
searches. The mining uses a set of lexical and 
syntactic patterns to test for pairs of verb 
strongly associated on the Web in an asymmetric 
?happens-before? relation. For example, the sys-
tem discovers that marriage happens-before di-
vorce, and that tie happens-before untie.  
We automatically extracted all the ?happens-
before? relations from the VerbOcean resource at 
the above web site, and then automatically con-
verted those relations to GTag format, producing 
4,199 rules. Here is one such converted rule: 
757
 
If arg1.class=EVENT && 
   arg2.class=EVENT && 
   arg1.word=learn && 
   arg2.word=forget && 
then infer relation=BEFORE 
 
Adding these lexical rules to GTag (with mor-
phological normalization being added for rule 
matching on word features) amounts to a consid-
erable augmentation of the rule-set, by a factor of 
22. GTag with this augmented rule-set might be 
a useful baseline to consider, since one would 
expect the gigantic size of the Google ?corpus? to 
yield fairly robust, broad-coverage rules.  
What if both a core GTag rule and a VerbO-
cean-derived rule could both apply? We assume 
the one with the higher confidence is chosen. 
However, we don?t have enough data to reliably 
estimate rule confidences for the original GTag 
rules; so, for the purposes of VerbOcean rule 
integration, we assigned either the original Ver-
bOcean rules as having greater confidence than 
the original GTag rules in case of a conflict (i.e., 
a preference for the more specific rule), or vice-
versa.  
 The results are shown in Table 4 (lines 
GTag+VerbOcean). The combined rule set, un-
der both voting schemes, had no statistically sig-
nificant difference in accuracy from the original 
GTag rule set. So, ME-C beat this baseline as 
well.  
The reason VerbOcean didn?t help is again 
one of data sparseness, due to most verbs occur-
ring rarely in the OTC. There were only 19 occa-
sions when a happens-before pair from VerbO-
cean correctly matched a human BEFORE 
TLINK, of which 6 involved the same rule being 
right twice (including learn happens-before for-
get, a rule which students are especially familiar 
with!), with the rest being right just once. There 
were only 5 occasions when a VerbOcean rule 
incorrectly matched a human BEFORE TLINK, 
involving just three rules. 
 
 
 Closed Class Distribution UnClosed Class Distribution 
Relation Prec Rec F Accuracy Prec Rec F Accuracy 
IBEFORE 100.0 100.0 100.0 83.33 58.82 68.96 
BEGINS 0 0 0 72.72 50.0 59.25 
ENDS 66.66 57.14 61.53 62.50 50.0 55.55 
SIMULTANEOUS 14.28 6.66 9.09 60.54 66.41 63.34 
INCLUDES 73.91 77.98 75.89 75.75 77.31 76.53 
BEFORE 90.68 92.60 91.63 
87.20  
(72.03) 
84.09 84.61 84.35 
76.0 
(40.95)  
Table 3. Machine Learning from subsamples of the closed data 
 
Accuracy Baseline 
Event-Event Event-Time 
GTag 63.43 72.46 
GTag+VerbOcean - GTag overriding VerbOcean 64.80 74.02 
GTag+VerbOcean - VerbOcean overriding GTag 64.22 73.37 
GTag+closure+ME-C 53.84 (57.00) 67.37 (67.59) 
Table 4. Accuracy of ?Intuition? Derived Baselines 
 
4.3 Learning from Hand-Coded Rules 
Baseline 
The previous baseline was a hybrid confi-
dence-based combination of corpus-induced 
lexical relations with hand-created rules for tem-
poral ordering. One could consider another obvi-
ous hybrid, namely learning from annotations 
created by GTag-annotated corpora. Since the 
intuitive baseline fares badly, this may not be 
that attractive. However, the dramatic impact of 
closure could help offset the limited coverage 
provided by human intuitions.   
Table 4 (line GTag+closure+ME-C) shows the 
results of closing the TLINKs produced by 
GTag?s annotation and then training ME from 
the resulting data. The results here are evaluated 
against a held-out test set. We can see that even 
after closure, the baseline of learning from un-
closed human annotations is much poorer than 
ME-C, and is in fact substantially worse than the  
majority class on event ordering.  
This means that for preprocessing new data 
sets to produce noisily annotated data for this 
classification task, it is far better to use machine-
learning from closed human annotations rather 
758
than machine-learning from closed annotations 
produced by an intuitive baseline. 
5 Related Work 
Our approach of classifying pairs independ-
ently during learning does not take into account 
dependencies between pairs.  For example, a 
classifier may label <X, Y> as BEFORE. Given 
the pair <X, Z>,  such a classifier has no idea if 
<Y, Z> has been classified as BEFORE, in 
which case, through closure, <X, Z> should be 
classified as BEFORE. This can result in the 
classifier producing an inconsistently annotated 
text. The machine learning approach of (Cohen 
et al 1999) addresses this, but their approach is 
limited to total orderings involving BEFORE, 
whereas TLINKs introduce partial orderings in-
volving BEFORE and five other relations. Future 
research will investigate methods for tighter in-
tegration of temporal reasoning and statistical 
classification. 
The only closely comparable machine-
learning approach to the problem of TLINK ex-
traction was that of (Boguraev and Ando 2005), 
who trained a classifier on Timebank 1.1 for 
event anchoring for events and times within the 
same sentence, obtaining an F-measure (for tasks 
A and B together) of 53.1. Other work in ma-
chine-learning and hand-coded approaches, 
while interesting, is harder to compare in terms 
of accuracy since they do not use common task 
definitions, annotation standards, and evaluation 
measures. (Li et al 2004) obtained 78-88% accu-
racy on ordering within-sentence temporal rela-
tions in Chinese texts. (Mani et al 2003) ob-
tained 80.2 F-measure training a decision tree on 
2069 clauses in anchoring events to reference 
times that were inferred for each clause. (Ber-
glund et al 2006) use a document-level evalua-
tion approach pioneered by (Setzer and Gai-
zauskas 2000), which uses a distinct evaluation 
metric. Finally, (Lapata and Lascarides 2004) use 
found data to successfully learn which (possibly 
ambiguous) temporal markers connect a main 
and subordinate clause, without inferring under-
lying temporal relations. 
In terms of hand-coded approaches, (Mani and 
Wilson 2000) used a baseline method of blindly 
propagating TempEx time values to events based 
on proximity, obtaining 59.4% on a small sample 
of 8,505 words of text. (Filatova and Hovy 2001) 
obtained 82% accuracy on ?timestamping? 
clauses for a single type of event/topic on a data 
set of 172 clauses. (Schilder and Habel 2001) 
report 84% accuracy inferring temporal relations 
in German data, and (Li et al 2001) report 93% 
accuracy on extracting temporal relations in Chi-
nese. Because these accuracies are on different 
data sets and metrics, they cannot be compared 
directly with our methods. 
Recently, researchers have developed other 
tools for automatically tagging aspects of Ti-
meML, including EVENT (Sauri et al 2005) at 
0.80 F-measure and TIMEX36 tags at 0.82-0.85 
F-measure. In addition, the TERN competition 
(tern.mitre.org) has shown very high (close to .95  
F-measures) for TIMEX2 tagging, which is fairly 
similar to TIMEX3. These results suggest the 
time is ripe for exploiting ?imperfect? features in 
our machine learning approach. 
6 Conclusion 
Our research has uncovered one new finding: 
semantic reasoning (in this case, logical axioms 
for temporal closure), can be extremely valuable 
in addressing data sparseness. Without it, per-
formance on this task of learning temporal rela-
tions is poor; with it, it is excellent. We showed 
that temporal reasoning can be used as an over-
sampling method to dramatically expand the 
amount of training data for TLINK labeling, re-
sulting in labeling predictive accuracy as high as 
93% using an off-the-shelf Maximum Entropy 
classifier. Future research will investigate this 
effect further, as well as examine factors that 
enhance or mitigate this effect in different cor-
pora. 
The paper showed that ME-C performed sig-
nificantly better than a series of increasingly so-
phisticated baselines involving expansion of 
rules derived from human intuitions. Our results 
in these comparisons confirm the lessons learned 
from the corpus-based revolution, namely that 
rules based on intuition alone are prone to in-
completeness and are hard to tune without access 
to the distributions found in empirical data.  
Clearly, lexical rules have a role to play in se-
mantic and pragmatic reasoning from language, 
as in the discussion of example (2) in Section 1. 
Such rules, when mined by robust, large corpus-
based methods, as in the Google-derived VerbO-
cean, are clearly relevant, but too specific to ap-
ply more than a few times in the OTC corpus.  
It may be possible to acquire confidence 
weights for at least some of the intuitive rules in 
GTag from Google searches, so that we have a 
                                                 
6http://complingone.georgetown.edu/~linguist/GU_TIME_
DOWNLOAD.HTML 
759
level field for integrating confidence weights 
from the fairly general GTag rules and the fairly 
specific VerbOcean-like lexical rules. Further, 
the GTag and VerbOcean rules could be incorpo-
rated as features for machine learning, along with 
features from automatic preprocessing.  
We have taken pains to use freely download-
able resources like Carafe, VerbOcean, and 
WEKA to help others easily replicate and 
quickly ramp up a system. To further facilitate 
further research, our tools as well as labeled vec-
tors (unclosed as well as closed) are available for 
others to experiment with. 
References 
James Allen. 1984. Towards a General Theory of Ac-
tion and Time.  Artificial Intelligence, 23, 2, 123-
154. 
Anders Berglund, Richard Johansson and Pierre 
Nugues. 2006. A Machine Learning Approach to 
Extract Temporal Information from Texts in Swed-
ish and Generate Animated 3D Scenes.  Proceed-
ings of EACL-2006. 
Branimir Boguraev and Rie Kubota Ando. 2005. Ti-
meML-Compliant Text Analysis for Temporal 
Reasoning. Proceedings of IJCAI-05, 997-1003. 
Timothy Chklovski and Patrick Pantel. 
2004.VerbOcean: Mining the Web for Fine-
Grained Semantic Verb Relations. Proceedings of 
EMNLP-04. http://semantics.isi.edu/ocean 
W. Cohen, R. Schapire, and Y. Singer. 1999. Learn-
ing to order things. Journal of Artificial Intelli-
gence Research, 10:243?270, 1999. 
Janet Hitzeman, Marc Moens and Clare Grover. 1995. 
Algorithms for Analyzing the Temporal Structure 
of Discourse. Proceedings of  EACL?95, Dublin, 
Ireland, 253-260. 
C.H. Hwang and L. K. Schubert. 1992. Tense Trees as 
the fine structure of discourse. Proceedings of 
ACL?1992, 232-240. 
Hans Kamp and Uwe Ryle. 1993. From Discourse to 
Logic (Part 2). Dordrecht: Kluwer. 
Andrew Kehler. 2000. Resolving Temporal Relations 
using Tense Meaning and Discourse Interpretation, 
in M. Faller, S. Kaufmann, and M. Pauly, (eds.), 
Formalizing the Dynamics of Information, CSLI 
Publications, Stanford. 
Mirella Lapata and Alex Lascarides. 2004. Inferring 
Sentence-internal Temporal Relations. In Proceed-
ings of the North American Chapter of the Assoca-
tion of Computational Linguistics, 153-160.  
Alex Lascarides and Nicholas Asher. 1993. Temporal 
Relations, Discourse Structure, and Commonsense 
Entailment. Linguistics and Philosophy 16, 437-
494. 
Wenjie Li, Kam-Fai Wong, Guihong Cao and Chunfa 
Yuan. 2004. Applying Machine Learning to Chi-
nese Temporal Relation Resolution. Proceedings of 
ACL?2004, 582-588. 
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.  
2003.  Inferring Temporal Ordering of Events in 
News. Short Paper. Proceedings of HLT-
NAACL'03, 55-57.  
Inderjeet Mani and George Wilson. 2000. Robust 
Temporal Processing of News.  Proceedings of 
ACL?2000. 
Rebecca J. Passonneau. A Computational Model of 
the Semantics of Tense and Aspect. Computational 
Linguistics, 14, 2, 1988, 44-60. 
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, David Day, Lisa Ferro, Robert Gai-
zauskas, Marcia Lazo, Andrea Setzer, and Beth 
Sundheim. 2003. The TimeBank Corpus. Corpus 
Linguistics, 647-656. 
James Pustejovsky, Bob Ingria, Roser Sauri, Jose 
Castano, Jessica Littman, Rob Gaizauskas, Andrea 
Setzer, G. Katz,  and I. Mani. 2005. The Specifica-
tion Language TimeML. In I. Mani, J. Pustejovsky, 
and R. Gaizauskas, (eds.), The Language of Time: 
A Reader. Oxford University Press.  
Roser Saur?, Robert Knippen, Marc Verhagen and 
James Pustejovsky. 2005. Evita: A Robust Event 
Recognizer for QA Systems. Short Paper. Proceed-
ings of HLT/EMNLP 2005: 700-707. 
Frank Schilder and Christof Habel. 2005. From tem-
poral expressions to temporal information: seman-
tic tagging of news messages. In I. Mani, J. Puste-
jovsky, and R. Gaizauskas, (eds.), The Language of 
Time: A Reader. Oxford University Press.  
Andrea Setzer and Robert Gaizauskas. 2000. Annotat-
ing Events and Temporal Information in Newswire 
Texts. Proceedings of LREC-2000, 1287-1294.  
Marc Verhagen. 2004. Times Between The Lines. 
Ph.D. Dissertation, Department of Computer Sci-
ence, Brandeis University. 
Marc Vilain, Henry Kautz, and Peter Van Beek. 1989. 
Constraint propagation algorithms for temporal 
reasoning: A revised report. In D. S. Weld and J. 
de Kleer (eds.), Readings in Qualitative Reasoning 
about Physical Systems, Morgan-Kaufman, 373-
381. 
Bonnie Webber. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14, 2, 1988, 61-73. 
760
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 138?145,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Error Analysis of the TempEval Temporal Relation Identification Task
Chong Min Lee
Linguistics Department
Georgetown University
Washington, DC 20057, USA
cml54@georgetown.edu
Graham Katz
Linguistics Department
Georgetown University
Washington, DC 20057, USA
egk7@georgetown.edu
Abstract
The task to classify a temporal relation be-
tween temporal entities has proven to be dif-
ficult with unsatisfactory results of previous
research. In TempEval07 that was a first at-
tempt to standardize the task, six teams com-
peted with each other for three simple relation-
identification tasks and their results were com-
parably poor. In this paper we provide an anal-
ysis of the TempEval07 competition results,
identifying aspects of the tasks which pre-
sented the systems with particular challenges
and those that were accomplished with relative
ease.
1 Introduction
The automatic temporal interpretation of a text has
long been an important area computational linguis-
tics research (Bennett and Partee, 1972; Kamp and
Reyle, 1993). In recent years, with the advent of
the TimeML markup language (Pustejovsky et al,
2003) and the creation of the TimeBank resource
(Pustejovsky et al, 2003) interest has focussed on
the application of a variety of automatic techniques
to this task (Boguraev and Ando, 2005; Mani et al,
2006; Bramsen et al, 2006; Chambers et al, 2007;
Lee and Katz, 2008). The task of identifying the
events and times described in a text and classifying
the relations that hold among them has proven to be
difficult, however, with reported results for relation
classification tasks ranging in F-score from 0.52 to
0.60.
Variation in the specifics has made comparison
among research methods difficult, however. A first
attempt to standardize this task was the 2007 Tem-
pEval competition(Verhagen et al, 2007). This
competition provided a standardized training and
evaluation scheme for automatic temporal interpre-
tation systems. Systems were pitted against one an-
other on three simple relation-identification tasks.
The competing systems made use of a variety of
techniques but their results were comparable, but
poor, with average system performance on the tasks
ranging in F-score from 0.74 on the easiest task to
0.51 on the most difficult. In this paper we provide
an analysis of the TempEval 07 competition, identi-
fying aspects of the tasks which presented the sys-
tems with particular challenges and those that were
accomplished with relative ease.
2 TempEval
The TempEval competition consisted of three tasks,
each attempting to model an important subpart of the
task of general temporal interpretation of texts. Each
of these tasks involved identifying in running text
the temporal relationships that hold among events
and times referred to in the text.
? Task A was to identify the temporal relation
holding between an event expressions and a
temporal expression occurring in the same sen-
tence.
? Task B was to identify the temporal relations
holding between an event expressions and the
Document Creation Time (DCT) for the text.
? Task C was to identify which temporal relation
held between main events of described by sen-
138
tences adjacent in text.
For the competition, training and development
data?newswire files from the TimeBank corpus
(Pustejovsky et al, 2003) ?was made available in
which the events and temporal expressions of in-
terest were identified, and the gold-standard tempo-
ral relation was specified (a simplified set of tem-
poral relations was used: BEFORE, AFTER, OVER-
LAP, OVERLAP-OR-BEFORE,AFTER-OR-OVERLAP
and VAGUE.1). For evaluation, a set of newswire
texts was provided in which the event and temporal
expressions to be related were identified (with full
and annotated in TimeML markup) but the temporal
relations holding among them withheld. The task in
was to identify these relations.
The text below allows illustrates the features of
the TimeML markup that were made available as
part of the training texts and which will serve as the
basis for our analysis below:
<TIMEX3 tid="t13" type="DATE"
value="1989-11-02"
temporalFunction="false"
functionInDocument="CREATION TIME">11/02/89
</TIMEX3> <s> Italian chemical giant
Montedison S.p.A. <TIMEX3 tid="t19"
type="DATE" value="1989-11-01"
temporalFunction="true"
functionInDocument="NONE"
anchorTimeID="t13">yesterday</TIMEX3
<EVENT eid="e2" class="OCCURRENCE"
stem="offer" aspect="NONE"
tense="PAST" polarity="POS"
pos="NOUN">offered</EVENT>
$37-a-share for all the common shares
outstanding of Erbamont N.V.</s>
<s>Montedison <TIMEX3 tid="t17"
type="DATE" value="PRESENT REF"
temporalFunction="true"
functionInDocument="NONE"
anchorTimeID="t13">currently</TIMEX3>
<EVENT eid="e20" class="STATE"
stem="own" aspect="NONE"
tense="PRESENT" polarity="POS"
pos="VERB">owns</EVENT> about
72%of Erbamont?s common shares
outstanding.</s>
TimeML annotation associates with temporal ex-
pression and event expression identifiers (tid and
eid, respectively). Task A was to identify the tem-
poral relationships holding between time t19 and
event e2 and between t17 and e20 (OVERLAP was
1This contrasts with the 13 temporal relations supported by
TimeML. The full TimeML markup of event and temporal ex-
pressions was maintained.
Task A Task B Task C
CU-TMP 60.9 75.2 53.5
LCC-TE 57.4 71.3 54.7
NAIST 60.9 74.9 49.2
TimeBandits 58.6 72.5 54.3
WVALI 61.5 79.5 53.9
XRCE-T 24.9 57.4 42.2
average 54.0 71.8 51.3
Table 2: TempEval Accuracy (%)
the gold-standard answer for both). Task B was to
identify the relationship between the events and the
document creation time t13 (BEFORE for e2 and
OVERLAP for e20). Task C was to identify the
relationship between e2 and e20 (OVERLAP-OR-
BEFORE). The TempEval07 training data consisted
of a total of 162 document. This amounted to a total
of 1490 total relations for Task A, 2556 for task B,
and 1744 for Task C. The 20 documents of testing
data had 169 Task A relations, 337 Task B relations,
and 258 Task C relations. The distribution of items
by relation type in the training and test data is given
in Table 1.
Six teams participated in the TempEval compe-
tition. They made use of a variety of techniques,
from the application of off-the shelf machine learn-
ing tools to ?deep? NLP. As indicated in Table 22,
while the tasks varied in difficulty, within each task
the results of the teams were, for the most part, com-
parable.3
The systems (other than XRCE-T) did somewhat
to quite a bit better than baseline on the tasks.
Our focus here is on identifying features of the
task that gave rise to difficult, using overall per-
formance of the different systems as a metric. Of
the 764 test items, a large portion were either
?easy??meaning that all the systems provided cor-
rect output?or ?hard??meaning none did.
Task A Task B Task C
All systems correct 24 (14%) 160 (45%) 35 (14%)
No systems correct 33 (20%) 36 (11%) 40 (16%)
In task A, the cases (24/14%) that all participants
make correct prediction are when the target relation
is overlap. And, the part-of-speeches of most events
2TempEval was scored in a number of ways; we report accu-
racy of relation identification here as we will use this measure,
and ones related to it below
3The XRCE-T team, which made use of the deep analysis
engine XIP lightly modified for the competition, was a clear
outlier.
139
Task A Task B Task C
BEFORE 276(19%)/21(12%) 1588(62%)/186(56%) 434(25%)/59(23%)
AFTER 369(25%)/30(18%) 360(14%)/48(15%) 306(18%)/42(16%)
OVERLAP 742(50%)/97(57%) 487(19%)/81(25%) 732(42%)/122(47%)
BEFORE-OR-OVERLAP 32(2%)/2(1%) 47(2%)/8(2%) 66(4%)/12(5%)
OVERLAP-OR-AFTER 35(2%)/5(3%) 35(1%)/2(1%) 54(3%)/7(3%)
VAGUE 36(2%)/14(8%) 39(2%)/5(2%) 152(9%)/16(6%)
Table 1: Relation distribution of training/test sets
in the cases are verbs (19 cases), and their tenses are
past (13 cases). In task B, among 160 cases for that
every participant predicts correct temporal relation,
159 cases are verbs, 122 cases have before as target
relation, and 112 cases are simple past tenses. In
task C, we find that 22 cases among 35 cases are
reporting:reporting with overlap as target relation.
In what follows we will identify aspects of the tasks
that make some items difficult and some not so much
so.
3 Analysis
In order to make fine-grained distinctions and to
compare arbitrary classes of items, our analysis will
be stated in terms of a summary statistic: the success
measure (SM).
(1) Success measure
?
k=0 6kCk
6(
?
k=0 6Ck )
where Ck is the number of items k systems got
correct. This simply the proportion of total correct
responses to items in a class (for all systems) divided
by the total number of items in that class (a success
measure of 1.0 is easy and of 0.0 is hard). For exam-
ple, let?s suppose before relation have 10 instances.
Among the instances, three cases are correct by all
teams, four by three teams, two by two teams, and
one by no teams. Then, SM of before relation is
0.567 ( (3?6)+(4?3)+(2?2)+(1?0)6?(1+2+4+3) ).
In addition, we would like to keep track of how
important each class of errors is to the total evalu-
ation. To indicate this, we compute the error pro-
portion (ER) for each class: the proportion of total
errors attributable to that class.
(2) Error proportion
?
k=0 6 (6? k)Ck
AllErrorsInTask ?NumberOfTeams
TaskA TaskB TaskC
BEFORE 0.26/21% 0.89/23% 0.47/25%
AFTER 0.42/24% 0.56/23% 0.48/17%
OVERLAP 0.75/33% 0.56/39% 0.68/31%
BEFORE-OR-OVERLAP 0.08/9% 0/3% 0.06/9%
OVERLAP-OR-AFTER 0.03/2% 0/1% 0.10/5%
VAGUE 0/19% 0/5% 0.02/12%
Table 3: Overall performance by relation type (SM/ER)
When a case shows high SM and high ER, we can
guess that the case has lots of instances. With low
SM and low ER, it says there is little instances. With
high SM and low ER, we don?t need to focus on the
case because the case show very good performance.
Of particular interest are classes in which the SM is
low and the ER is high because it has a room for the
improvement.
3.1 Overall analysis
Table 3 provides the overall analysis by relation
type. This shows that (as might be expected) the
systems did best on the relations that were the ma-
jority class for each task: overlap in Task A, before
in Task B, and overlap in Task C.
Furthermore systems do poorly on all of the dis-
junctive classes, with this accounting for between
1% and 9% of the task error. In what follows we will
ignore the disjunctive relations. Performance on the
before relation is low for Task A but very good for
Task B and moderate for Task C. For more detailed
analysis we treat each task separately.
3.2 Task A
For Task A we analyze the results with respect to the
attribute information of the EVENT and TIMEX3
TimeML tags. These are the event class (aspectual,
i action, i state, occurrence, perception, reporting,
and state)4 part-of-speech (basically noun and verb),
4The detailed explanations on the event classes
can be found in the TimeML annotation guideline at
140
NOUN VERB
BEFORE 0/5% 0.324/15%
AFTER 0.119/8% 0.507/15%
OVERLAP 0.771/7% 0.747/24%
VAGUE 0/8% 0/10%
Table 4: POS of EVENT in Task A
and tense&aspect marking for event expressions. In-
formation about the temporal expression turned out
not to be a relevant dimension of analysis.
As we seen in Table 4, verbal event expressions
make for easier classification for before and after
(there is a 75%/25% verb/noun split in the data).
When the target relation is overlap, nouns and verbs
have similar SMs.
One reason for this difference, of course, is
that verbal event expressions have tense and aspect
marking (the tense and aspect marking for nouns is
simply none).
In Table 5 we show the detailed error analy-
sis with respect to tense and aspect values of the
event expression. The combination of tense and
aspect values of verbs generates 10 possible val-
ues: future, infinitive, past, past-perfective, past-
progressive (pastprog), past-participle (pastpart),
present, present-perfective (presperf), present-
progressive (presprog), and present-participle (pres-
part). Among them, only five cases (infinitive, past,
present, presperf, and prespart) have more than 2
examples in test data. Past takes the biggest por-
tions (40%) in test data and in errors (33%). Over-
lap seems less influenced with the values of tense
and aspect than before and after when the five cases
are considered. Before and after show 0.444 and
0.278 differences between infinitive and present and
between infinitive and present. But, overlap scores
0.136 differences between present and past. And a
problem case is before with past tense that shows
0.317 SM and 9% EP.
When we consider simultaneously SM and EP of
the semantic class of events in Table 6, we can find
three noticeable cases: occurrence and reporting of
before, and occurrence of after. All of them have
over 5% EP and under 0.4 SM. In case of reporting
of after, its SM is over 0.5 but its EP shows some
room for the improvement.
http://www.timeml.org/.
BEFORE AFTER OVERLAP VAGUE
FUTURE 0/0% 0.333/1% 0.833/0% 0/0%
INFINITIVE 0/3% 0.333/3% 0.667/2% 0/1%
NONE 0/5% 0.119/8% 0.765/7% 0/8%
PAST 0.317/9% 0.544/9% 0.782/10% 0/5%
PASTPERF 0/0% 0.333/1% 0.833/0% 0/0%
PASTPROG 0/0% 0/0% 0.500/1% 0/0%
PRESENT 0.444/2% 0.611/2% 0.646/4% 0/1%
PRESPERF 0.833/0% 0/0% 0.690/3% 0/0%
PRESPROG 0/0% 0/0% 0.833/0% 0/0%
PRESPART 0/0% 0/0% 0.774/4% 0/1%
Table 5: Tense & Aspect of EVENT in Task A
? 4 ? 16 > 16
BEFORE 0/1% 0.322/13% 0.133/6%
AFTER 0.306/5% 0.422/13% 0.500/5%
OVERLAP 0.846/10% 0.654/17% 0.619/3%
VAGUE 0/0% 0/5% 0/13%
Table 7: Distance in Task A
Boguraev and Ando (2005) report a slight in-
crease in performance in relation identification
based on proximity of the event expression to the
temporal expression. We investigated this in Table 7,
looking at the distance in word tokens.
We can see noticeable cases in before and after of
? 16 row. Both cases show over 13% EP and under
0.5 SM. The participants show good SM in overlap
of ? 4. Overlap of ? 16 has the biggest EP (17%).
When its less satisfactory SM (0.654) is considered,
it seems to have a room for the improvement. One of
the cases that have 13% EP is vague of ? 16. It says
that it is difficult even for humans to make a decision
on a temporal relation when the distance between an
event and a temporal expression is greater than and
equal to 16 words.
3.3 Task B
Task B is to identify a temporal relation between an
EVENT and DCT. We analyze the participants per-
formance with part-of-speech. This analysis shows
how poor the participants are on after and overlap of
nouns (0.167 and 0.115 SM). And the EM of over-
lap of verbs (26%) shows that the improvement is
needed on it.
In test data, occurrence and reporting have simi-
lar number of examples: 135 (41%) and 106 (32%)
in 330 examples. In spite of the similar distribu-
tion, their error rates show difference. It suggests
that reporting is easier than occurrence. Moreover,
141
ASPECTUAL I ACTION I STATE OCCURRENCE PERCEPTION REPORTING STATE
BEFORE 0.167/1% 0/0% 0.333/3% 0.067/6% 0/0% 0.364/9% 0/1%
AFTER 0.111/3% 0/0% 0/0% 0.317/9% 0/0% 0.578/8% 0.167/2%
OVERLAP 0.917/0% 0.778/1% 0.583/3% 0.787/15% 0.750/1% 0.667/9% 0.815/2%
VAGUE 0/1% 0/1% 0/0% 0/9% 0/0% 0/6% 0/0%
Table 6: EVENT Class in Task A
ASPECTUAL I ACTION I STATE OCCURRENCE PERCEPTION REPORTING STATE
BEFORE 1/0% 0.905/1% 0.875/1% 0.818/13% 0.556/1% 0.949/5% 0.750/1%
AFTER 0.500/3% 0.500/1% 0/0% 0.578/15% 0.778/1% 0.333/1% 0.444/2%
OVERLAP 0.625/2% 0.405/5% 0.927/1% 0.367/17% 0.500/1% 0.542/6% 0.567/7%
VAGUE 0/1% 0/0% 0/0% 0/4% 0/0% 0/0% 0/0%
Table 9: EVENT Class in Task B
NOUN VERB
BEFORE 0.735/6% 0.908/16%
AFTER 0.167/8% 0.667/14%
OVERLAP 0.115/13% 0.645/26%
VAGUE 0/4% 0/1%
Table 8: POS of EVENT in Task B
Table 9 shows most errors in after occur with oc-
currence class 65% (15%/23%) when we consider
23% EP in Table 3. Occurrence and reporting of be-
fore show noticeably good performance (0.818 and
0.949). And occurrence of overlap has the biggest
error rate (17%) with 0.367 of SM.
In case of state, it has 22 examples (7%) but takes
10% of errors. And it is interesting that the most
errors are concentrated in state. In our intuition, it
is not a difficult task to identify overlap relation of
state class.
Table 9 does not clearly show what causes the
poor performance of nouns in after and overlap.
In the additional analysis of nouns with class in-
formation, occurrence shows poor performance in
after and overlap: 0.111/6% and 0.083/8%. And
other noticeable case in nouns is state of overlap:
0.125/4%. We can see the low performance of nouns
in overlap is due to the poor performance of state
and occurrence, but only occurrence is a cause of
the poor performance in after.
DCT can be considered as speech time. Then,
tense and aspect of verb events can be a cue in pre-
dicting temporal relations between verb events and
DCT. The better performance of the participants in
verbs can be an indirect evidence. The analysis with
tense & aspect can tell us which tense & aspect in-
formation is more useful. A problem with the in-
formation is sparsity. Most cases appear less than
3 times. The cases that have more than or equal
to three instances are 13 cases among the possible
combinations of 7 tenses and 4 aspects in TimeML.
Moreover, only two cases are over 5% of the whole
data: past with before (45%) and present with over-
lap (15%). In Table 10, tense and aspect information
seems valuable in judging a relation between a verb
event and DCT. The participants show good perfor-
mances in the cases that seem easy intuitively: past
with before, future with after, and present with over-
lap. Among intuitively obvious cases that are past,
present, or future tense, present tense makes large
errors (20% of verb errors). And present shows 7%
EP in before.
When events has no cue to infer a relation like
infinitive, none, pastpart, and prespart, their SMs
are lower than 0.500 except infinitive and none of
after. infinitive of overlap shows poor performance
with the biggest error rate (0.125/12%).
3.4 Task C
The task is to identify the relation between consec-
utive main events. There are four part-of-speeches
in Task C: adjective, noun, other, and verb. Among
eight possible pairs of part-of-speeches, only three
pairs have over 1% in 258 TLINKs: noun and verb
(4%), verb and noun (4%), and verb and verb (85%).
When we see the distribution of verb and verb by
three relations (before, after, and overlap), the rela-
tions show 19%, 14%, and 41% distribution each.
In Table 11, the best SM is verb:verb of overlap
(0.690). And verb:verb shows around 0.5 SM in be-
fore and after.
Tense & aspect pairs of main event pairs show
142
BEFORE AFTER OVERLAP VAGUE
FUTURE 0/0% 0.963/1% 0.333/2% 0/0%
FUTURE-PROGRESSIVE 0/0% 0/0% 0.167/1% 0/0%
INFINITIVE 0.367/5% 0.621/7% 0.125/12% 0/2%
NONE 0/0% 0.653/7% 0/2% 0/0%
PAST 0.984/3% 0.333/1% 0.083/3% 0/0%
PASTPERF 1.000/0% 0/0% 0/0% 0/0%
PASTPROG 1.000/0% 0/0% 0/0% 0/0%
PASTPART 0.583/1% 0/0% 0/0% 0/0%
PRESENT 0.429/7% 0.167/3% 0.850/10% 0/0%
PRESPERP 0.861/3% 0/0% 0/2% 0/0%
PRESENT-PROGRESIVE 0/0% 0/0% 0.967/0% 0/0%
PRESPART 0/0% 0.444/3% 0.310/8% 0/0%
Table 10: Tense & Aspect of EVENT in Task B
BEFORE AFTER OVERLAP VAGUE
NOUN:VERB 0.250/2% 0/0% 0.625/1% 0/0%
VERB:NOUN 0.583/1% 0.500/2% 0.333/1% 0/1%
VERB:VERB 0.500/20% 0.491/15% 0.690/26% 0.220/12%
Table 11: POS pairs in Task C
skewed distribution, too. The cases that have
over 1% data are eight: past:none, past:past,
past:present, present:past, present:present,
present:past, presperf:present, and pres-
perf:presperf. Among them, past tense pairs
show the biggest portion (40%). The performance
of the eight cases is reported in Table 12. As we can
guess with the distribution of tense&aspect, most
errors are from past:past (40%). When the target re-
lation of past:past is overlap, the participants show
reasonable SM (0.723). But, their performances are
unsatisfactory in before and after.
When we consider cases over 1% of test data in
main event class pairs, we can see eleven cases as
Table 13. Among the eleven cases, four pairs have
over 5% data: occurrence:occurrence (13%), occur-
rence:reporting (14%), reporting:occurrence (9%),
and reporting:reporting (17%). Reporting:reporting
shows the best performance (0.934/2%) in over-
lap. Two class pairs have over 10% EP: occur-
rence:occurrence (15%), and occurrence:reporting
(14%). In addition, occurrence pairs seem difficult
tasks when target relations are before and after be-
cause they show low SMs (0.317 and 0.200) with
5% and 3% error rates.
4 Discussion and Conclusion
Our analysis shows that the participants have the dif-
ficulty in predicting a relation of a noun event when
its target relation is before and after in Task A, and
after and overlap in Task B. When the distance is in
the range from 5 to 16 in Task A, more effort seems
to be needed.
In Task B, tense and aspect information seems
valuable. Six teams show good performance when
simple tenses such as past, present, and future ap-
pear with intuitively relevant target relations such as
before, overlap, and after. Their poor performance
with none and infinitive tenses, and nouns can be an-
other indirect evidence.
A difficulty in analyzing Task C is sparsity. So,
this analysis is focused on verb:verb pair. When we
can see in (12), past pairs still show the margin for
the improvement. But, a lot of reporting events are
used as main events. When we consider that im-
portant events in news paper are cited, the current
TempEval task can miss useful information.
Six participants make very little correct predic-
tions on before-or-overlap, overlap-or-after, and
vague. A reason on the poor prediction can be small
distribution in the training data as we can see in Ta-
ble 1. Data sparsity problem is a bottleneck in nat-
ural language processing. The addition of the dis-
junctive relations and vague to the target labels can
make the sparsity problem worse. When we con-
sider the participants? poor performance on the la-
bels, we suggest to use three labels (before, overlap,
and after) as the target labels.
143
BEFORE AFTER OVERLAP VAGUE
PAST:NONE 0.750/1% 0.167/1% 0.167/3% 0/0%
PAST:PAST 0.451/12% 0.429/10% 0.723/11% 0.037/7%
PAST:PRESENT 0.667/1% 0/0% 0.708/2% 0/0%
PRESENT:PAST 0/0% 0.292/2% 0.619/2% 0/1%
PRESENT:PRESENT 0.056/2% 0/0% 0.939/1% 0/1%
PRESPERF:PAST 0.500/0% 0/0% 0.542/1% 0/0%
PRESPERF:PRESENT 0/1% 0/0% 0.583/1% 0/0%
PRESPERF:PRESPERF 0/0% 0/0% 0.600/2% 0/0%
Table 12: Tense&Aspect Performance in Task C
BEFORE AFTER OVERLAP VAGUE
I ACTION:OCCURRENCE 0.524/1% 0.400/2% 0.500/1% 0/0%
I STATE:OCCURRENCE 0.250/1% 0.500/1% 0.833/0% 0/0%
I STATE:ASPECTUAL 0/0% 0.333/1% 0.500/0% 0/0%
OCCURRENCE:I ACTION 0.583/1% 0.417/1% 0.300/3% 0/0%
OCCURRENCE:OCCURRENCE 0.317/5% 0.200/3% 0.600/5% 0/2%
OCCURRENCE:REPORTING 0.569/4% 0.367/3% 0.594/5% 0.111/2%
OCCURRENCE:STATE 0.333/1% 0/0% 0.583/1% 0/0%
REPORTING:I STATE 0.167/1% 0.583/1% 0.867/1% 0/0%
REPORTING:OCCURRENCE 0.625/1% 0.611/3% 0.542/3 0/2%
REPORTING:REPORTING 0.167/1% 0.167/2% 0.934/2% 0/4%
Table 13: Event class in Task C
Our analysis can be used as a cue in adding an
additional module for weak points. When a pair of
a noun event and a temporal expression appears in a
sentence, a module can be added based on our study.
References
Branimir Boguraev and Rie Kubota Ando. 2005.
TimeML-Compliant Text Analysis for Temporal Rea-
soning. Preceedings of IJCAI-05, 997?1003.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, David Day, Lisa Ferro, Robert Gaizauska, Marcia
Lazo, Andrea Setzer, and Beth Sundheim. 2003. The
TIMEBANK corpus. Proceedings of Corpus Linguis-
tics 2003, 647?656.
Michael Bennett and Barbara Partee. 1972. Toward the
logic of tense and aspect in English. Technical report,
System Development Corporation. Santa Monica, CA
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Inducing Temporal
Graphs Proceedings of EMNLP 2006, 189?198.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying Temporal Relations Between
Events Proceedings of ACL 2007, 173?176.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learn-
ing of Temporal Relations. Proceedings of ACL-2006,
753?760.
Chong Min Lee and Graham Katz. 2008. Toward an
Automated Time-Event Anchoring System. The Fifth
Midwest Computational Linguistics Colloquium.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic: Introduction to modeltheoretic semantics of
natural language. Kluwer Academic, Boston.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. TimeML: Robust Specification of Event
and Temporal Expressions in Text. IWCS-5, Fifth In-
ternational Workshop on Computational Semantics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, James Pustejovsky.
2007. SemEval-2007 Task 15: TempEval Tempo-
ral Relation Identification. Proceedings of SemEval-
2007, 75?80.
Caroline Hage`ge and Xavier Tannier. 2007 XRCE-T:
XIP Temporal Module for TempEval campaign. Pro-
ceedings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), 492?495.
Steven Bethard and James H. Martin. 2007. CU-TMP:
Temporal Relation Classification Using Syntactic and
Semantic Features. Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), 129?132.
Congmin Min, Munirathnam Srikanth, and Abraham
Fowler. 2007. LCC-TE: A Hybrid Approach to Tem-
poral Relation Identification in News Text. Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), 219?222.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. NAIST.Japan: Temporal Relation
144
Identification Using Dependency Parsed Tree. Pro-
ceedings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), 245?248.
Georgiana Pus?cas?u. 2007. WVALI: Temporal Relation
Identification by Syntactico-Semantic Analysis Pro-
ceedings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), 484?487.
Mark Hepple, Andrea Setzer, and Robert Gaizauskas.
2007. USFD: Preliminary Exploration of Features
and Classifiers for the TempEval-2007 Task. Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), 438?441.
145
Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 40?45,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Temporal Relation Identification with Endpoints
Chong Min Lee
Georgetown University
3700 O Street, NW
Washington, D.C. 20057, USA
cml54@georgetown.edu
Abstract
Temporal relation classification task has is-
sues of fourteen target relations, skewed dis-
tribution of the target relations, and relatively
small amount of data. To overcome the is-
sues, methods such as merging target relations
and increasing data size with closure algo-
rithm have been used. However, the method
using merged relations has a problem on how
to recover original relations. In this paper,
a new reduced-relation method is proposed.
The method decomposes a target relation into
four pairs of endpoints with three target rela-
tions. After classifying a relation of each end-
point pair, four classified relations are com-
bined into a relation of original fourteen target
relations. In the combining step, two heuris-
tics are examined.
1 Introduction
An interesting task in temporal information process-
ing is how to identify a temporal relation between
a pair of temporal entities such as events (EVENT)
and time expressions (TIMEX) in a narrative. Af-
ter the publication of TimeBank (Pustejovsky et al,
2003b) annotated in TimeML (Pustejovsky et al,
2003a), supervised learning techniques have been
tested in the temporal relation identification task
with different types of temporal entity pairs (Cham-
bers et al, 2007; Boguraev and Ando, 2005; Verha-
gen et al, 2007).
There are three issues in applying supervised ma-
chine learning methods to this task. The first issue
is that a temporal entity pair that is defined as a di-
rected temporal link (TLINK) in TimeML should be
classified into a relation among fourteen relations.
The second issue is that the number of TLINKs is
relatively small in spite of the fourteen targets. The
third issue is skewed distributions of the relations.
Without the solutions of the issues, it is impossi-
ble to achieve good performance in temporal relation
identification through machine learning techniques.
Several solutions have been suggested such as in-
creased number of TLINKs with a transitivity clo-
sure algorithm (Mani et al, 2007; Chambers et
al., 2007) and decreased target relations into six
(Mani et al, 2006; Chambers et al, 2007; Tatu and
Srikanth, 2008) or three (Verhagen et al, 2007). An
issue of the reduced-relation method is how to re-
cover original relations. A module for the recovery
can cause performance degeneration and seems in-
tuitively inappropriate.
In this paper, a new reduced-relation method is
presented. The method uses endpoints of tempo-
ral entities. A TimeML relation can be represented
into four endpoint pairs with three relations: before,
equal, and after. This method requires four rela-
tion identification classifiers among endpoints for a
TLINK and each classifier has only three target re-
lations instead of fourteen. The four classified re-
lations need to be combined in order to restore an
interval-based relation. In this study, the perfor-
mance of the proposed method will be evaluated in
identifying TLINK relations between temporal enti-
ties empirically.
Firstly, related studies are described in section 2.
Secondly, the identification of four pointwise rela-
tions is described. Thirdly, methods for the combi-
nation of pointwise relations are explained. Finally,
40
the outlook of the proposed method is proposed.
2 Background
Temporal relation identification has three problems:
sparse data, fourteen target relations, and skewed
distribution. To reduce the problems, previous stud-
ies have used techniques such as increasing data size
with closure algorithm and merging target relations.
Mani et al (2006) used closure algorithm to in-
crease training data size and merged inverse rela-
tions into six main relations. Their study applied
the methods to classify relations of all TLINKs and
showed the benefit of the methods in temporal re-
lation identification. Chambers et al (2007) re-
ported 67.0% accuracy on the relation identification
task among EVENT-EVENT (EE) TLINKs using
the merged relations. And, the accuracy is the best
performance with EE TLINKs.
The merging method assumes that target relations
of TLINKs is already known. When a TLINK re-
lation from an anchor to a target is AFTER, it can
be changed into BEFORE by conversing the anchor
and the target each other. When unknown instance
is given, the merging process is impossible. When
six merged relations were used as target relations,
we assumes the conversion is already done. And the
assumption is inappropriate.
TempEval07 (Verhagen et al, 2007) integrated
14 TLINK relations into three: before, after, and
overlap. overlap is an extended relation that cov-
ers 12 relations except BEFORE and AFTER. This
approach has a burden to recover 12 relations from
the extensive one.
In this study, a TLINK is decomposed into four
pairs of endpoint links in the step of applying ma-
chine learning approaches. Then, four classified
endpoint relations are combined into a TimeML re-
lation. Allen (1983) showed a relative order between
intervals can be decomposed into relative orders of
four endpoint pairs. In TimeML, temporal entities,
EVENT and TIMEX, are intervals. An interval has
a pair of endpoints: start and end. A relation be-
tween two intervals can be represented into relations
of four pairs of starts and ends as in Table 2. A
relative order between endpoints can be represented
with three relations: before, equal, and after. The
proposed method will be empirically investigated in
this study.
3 Resources and Data Preparation
3.1 Temporal Corpora
TimeBank and Opinion corpora consist of 183 and
73 documents respectively. Among the documents,
it is found that 42 documents have inconsistent
TLINKs. The inconsistencies make it impossible to
apply closure algorithm to the documents. There-
fore, the 42 documents with inconsistent TLINKs
are excluded. This study focuses on classifying re-
lations of three types of TLINKs: TLINKs between
EVENTs (EE), between an EVENT and a TIMEX
(ET), and between an EVENT and Document Cre-
ation Time (ED).
As a preparation step, fourteen relations are
merged into eleven relations (TimeML relations).
SIMULTANEOUS, IDENTITY, DURING, and DU-
RUNG BY relations are identical in relative order
between entities. Therfore, the relations are inte-
grated into SIMULTANEOUS1. Then, closure algo-
rithm is run on the documents to increase the num-
ber of TLINKs. The distribution of relations of three
types is given in Table 1.
A document with merged relations is divided into
four documents with endpoint relations: start of an-
chor and start of target, start of anchor and end of
target, end of anchor and start of target, and end of
anchor and end of target documents. The conversion
table of a TimeML relation into four endpoint rela-
tions is given in Table 2 and the distribution of three
relations after the conversion is given in 3.
4 Relation identification with end points
In endpoint relation identification experiment, sup-
port vector machine (SVM) and maximum entropy
classifiers are built to classify three relations: be-
fore, equal, and after. First, feature vectors are
constructed. When four endpoint links are from a
TLINK, their feature vectors are identical except tar-
get endpoint relations.
1Mani et al (2006) said DURING was merged into
IS INCLUSED. However, DURING, SIMULTANEOUS, and
IDENTITY are converted into = of Allen?s relations in
Tarski Toolkit (Verhagen et al, 2005). In this paper, the
implementation is followed.
41
Relation EVENT-EVENT EVENT-TIMEX EVENT-DCT
Original Closed Original Closed Original Closed
AFTER 735 11083 86 2016 169 259
BEFORE 1239 12445 160 1603 721 1291
BEGINS 35 75 23 36 0 0
BEGUN BY 38 74 51 58 10 11
ENDS 15 64 65 128 0 0
ENDED BY 87 132 43 61 6 6
IAFTER 38 138 3 8 1 1
IBEFORE 49 132 2 9 0 0
INCLUDES 246 3987 122 166 417 469
IS INCLUDED 327 4360 1495 2741 435 467
SIMULTANEOUS 1370 2348 201 321 75 90
Table 1: Distribution of TimeML relations
TimeML Relation Inverse Endpoint Relations
x BEFORE y y AFTER x x? < y?, x? < y+,
x+ < y?, x+ < y+
x SIMULTANEOUS y y SIMULTANEOUS x x? = y?, x? < y+,
x+ > y?, x+ = y+
x IBEFORE y y IAFTER x x? < y?, x? < y+,
x+ = y?, x+ < y+
x BEGINS y y BEGUN BY x x? = y?, x? < y+,
x+ > y?, x+ < y+
x ENDS y y ENDED BY x x? > y?, x? < y+,
x+ > y?, x+ = y+
x INCLUDES y y IS INCLUDED x x? < y?, x? < y+,
x+ > y?, x+ > y+
Table 2: Relation conversion table
End pairs EVENT-EVENT EVENT-TIMEX EVENT-DCT
before equal after before equal after before equal after
start-start 1621 (39%) 1443 (35%) 1115 (27%) 327 (15%) 275 (12%) 1649 (73%) 1144 (62%) 85 (5%) 605 (33%)
start-end 3406 (82%) 38 (1%) 735 (18%) 2162 (96%) 3 86 (4%) 1664 (91%) 1 169 (9%)
end-start 1239 (30%) 49 (1%) 2891 (69%) 160 (7%) 2 2089 (93%) 721 (39%) 0 1113 (61%)
end-end 1650 (39%) 1472 (35%) 1057 (25%) 1680 (75%) 309 (14%) 262 (12%) 1156 (63%) 81 (4%) 597 (33%)
Table 3: Distribution of end point relations.
42
10-fold cross validation is applied at document-
level. In some previous studies, all temporal links
were collected into a set and the set was split into
training and test data without the distinction on
sources. However, the approach could boost system
performance as shown in Tatu and Srikanth (2008).
When TLINKs in a file are split in training and
test data, links in training data can be composed of
similar words in test data. In that case, the links in
training can play a role of background knowledge.
Therefore, document-level 10-fold cross validation
is exploited.
4.1 Features
In constructing feature vectors of three TLINK
types, features that were used in order to identify
TimeML relations in previous studies are adopted.
The features have been proved useful in identifying
a TimeML relation in the studies. Moreover, the fea-
tures still seem helpful for endpoint relation identifi-
cation task. For example, past and present tenses of
two EVENTs could be a clue to make a prediction
that present tensed EVENT is probably after past
tensed EVENT.
Annotated information of EVENT and TIMEX in
the temporal corpora is used in the feature vector
construction. This proposed approach to use end-
point conversion in relation identification task is the
first attempt. Therefore, the annotated values are
used as features in order to see the effect of this ap-
proach. However, state-of-the-arts natural language
processing programs such as Charniak parser and
Porter Stemmer are sometimes used to extract ad-
ditional features such as stems of event words, the
existence of both entities in the same phrase, and
etc.
The company has reported declines in op-
erating profit in the past three years
Features for EVENT TENSE, ASPECT,
MODAL, POS, and CLASS annotations are bor-
rowed from temporal corpora as features. And,
a stem of an EVENT word is added as a feature
instead of a word itself in order to normalize it.
reported is represented as <(TENSE:present),
(ASPECT:perferce), (MODAL:none), (POS: verb),
(CLASS: reporting), (STEM:report)>.
Features for TIMEX In the extraction of TIMEX
features, it tries to capture if specific words are in a
time expression to normalize temporal expressions.
The time point of an expression can be inferred
through the specific words such as ago, coming, cur-
rent, earlier and etc. Additionally, the existence of
plural words such as seconds, minutes, hours, days,
months, and years is added as a feature. The specific
words are:
? ago, coming, current, currently, earlier, early,
every, following, future, last, later, latest, next,
now, once, past, previously, recent, recently,
soon, that, the, then, these, this, today, tomor-
row, within, yesterday, and yet
the past three years are represented as <(AGO:0),
(COMING:0), (CURRENT:0), (CURRENTLY:0),
(EARLIER:0), (EARLY:0), (EVERY:0), (FOL-
LOWING:0), (FUTURE:0), (LAST:1), (LATER:0),
(LASTED:0), (NEXT:0), (NOW:0), (ONCE:0),
(PAST:1), (PREVIOUSLY:0), (RECENT:0), (RE-
CENTLY:0), (SOON:0), (THAT:0), (THE:1),
(THEN:0), (THESE:0), (THIS:0), (TODAY:0),
(TOMORRWO:0), (WITHIN:0), (YESTERDAY:0),
(YET:0), (PLURAL:1)>.
Relational features between entities In addition,
relational information between two entities is used
as features. It is represented if two entities are in the
same sentence. To get the other relational informa-
tion, a sentence is parsed with Charniak parser. Syn-
tactic path from an anchor to a target is calculated
from the parsed tree. A syntactic path from reported
to the past three years is ?VBN?VP?PP?NP?. It is
represented if two entities are in the same phrase
and clause with the path. When only one clause
or phrase exists in the path except part-of-speeches
of both entities, the features are marked as 1s. The
counts of words, phrases, and clauses between tem-
poral entities are also used as features. When two
entities are not in the same sentence, 0s are given
as the values of the features except the word count.
Some prepositions and conjunctions are used as fea-
tures when the words are used as a head word of
syntactic path from an entity to the other entity. In
the example of ?VBN?VP?PP?NP?, ?in? in ?in the
past three years? is the head word of PP. So, in is
marked 1. The head words that are used as features
are:
43
? after, as, at, before, between, by, during, for, in,
once, on, over, since, then, through, throughout,
until, when, and while
EE and ET types have feature vectors that consist
of features of both entities and relational features.
ED type has only features of EVENT.
5 Restoration of original relations
Four endpoint relations of a TLINK are classified in
the previous section. The combination of the clas-
sified relations needs to be restored into a relation
among the eleven merged TimeML relations. How-
ever, due to the independence of four classifiers, it is
not guaranteed that a TimeML relation can be gener-
ated from four endpoint relations. When the restora-
tion fails, the existence of errors in the four predic-
tions is implied. In this step, two methods to restore
a TimeML relation are investigated: Minimum Edit
Distance (MED) and Highest Score (HS).
MED checks how many substitutions are needed
to restore a TimeML relation. A TimeML relation
with the minimum changes is defined as the restored
relation. Let?s suppose four endpoint relations are
given such as x? before y?, x? after y+, x+ be-
fore y?, and x+ before y+. Among other possible
ways to get a TimeML relation, BEFORE could be
recovered with a change of before in x? after y+
into before. Therefore, BEFORE is chosen as a re-
stored TimeML relation. When several candidates
are available, a method is examined in selecting one.
The method is to give weight on classifiers that show
better performance. If two candidates are available
by changing before of start-start or before of start-
end in ET type, this method selects a candidate by
changing before when before of start-end shows bet-
ter performance.
HS uses the sum of confidence scores from clas-
sifiers. Each classifier of the four endpoint pairs
generates confidence scores of three relations (be-
fore, equal, and after). Among 81 possible com-
binations of four classifiers with three target rela-
tions, the highest-scored one that can be restored
into a TimeML relation is chosen as a prediction.
When several candidates exist, the selection method
of MED is also adopted.
6 Expectations and future plans
First, I will show how beneficial four endpoint
systems are at identifying endpoint relations. F-
measure will be used to show the performance of an
endpoint relation classifier in identifying each end-
point relation. And, accuracy is used to report over-
all performance of the classifier. Second, I will show
how effective the endpoint method is in identifying
TLINK relations. I will build a base classifier with
eleven TimeML relations and feature vectors that
are identical with the endpoint systems. The perfor-
mance difference in identifying TimeML relations
between this proposed system and the base system
will be presented to show whether this proposed ap-
proach is successful.
Previous research such as Verhagen et al (2007)
using three reltions as target relations showed from
60% to 80% performance according to TLINK
types. Moreover, some distributions of endpoint re-
lations show over 90% such as before of end-start
in ET and ED TLINKs, and after of end-start in ET
TLINK in Table 3. Therefore, we can expect each
endpoint identification system will perform well in
classifying endpoint relations.
The success of this new approach will depend on
the restoration step. The excessively skewed dis-
tributions can make similar predicted sequences of
endpoint relations. It can weaken the advantage of
this endpoint approach that every TimeML relation
can be generated through combining endpoint rela-
tions. For example, equal shows very small dis-
tributions in start-end and end-start endpoint pairs.
Therefore, it is probable that TimeML relations such
as IAFTER and IBEFORE cannot be classified cor-
rectly. It can be a challenge how to correctly classify
endpoint relations with small distribution.
One possible solution for the challenge is to check
global consistency among classified relations such
as Bramsen et al (2006) and Chambers and Juraf-
sky (2008). The global consistency restoration can
give a chance to replace excessively distributed rela-
tions with sparse relations. However, equal is used
additionally in this study. Therefore, modifications
in the method of Bramsen et al (2006) and Cham-
bers and Jurafsky (2008) are needed before applying
their method.
44
References
James Allen. 1983. Maintaining knowledge about tem-
poral intervals. Communications of the Association for
Computing Machinery, 26(1):832?843.
Branimir Boguraev and Rie Kubota Ando. 2005.
TimeML-compliant text analysis for temporal reason-
ing. In Proceedings of the 2005 International Joint
Conference on Artificial Intelligence, pages 997?1003.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Inducing temporal graphs.
In Proceedings of the 2006 Conference on Empirical
Methods on Natural Language Processing, pages 189?
198.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In EMNLP ?08: Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 698?706, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of 45th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 173?176.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics.
Inderjeet Mani, Ben Wellner, Marc Verhagen, and James
Pustejovsky. 2007. Three approaches to learning
tlinks in timeml. Technical Report CS-07-268, Bran-
deis University, Waltham, MA, USA.
James Pustejovsky, Jose Castao, Robert Ingria, Roser
Saur, Robert Gaizauskas, and Andrea Setzer. 2003a.
TimeML: robust specification of event and temporal
expressions in text. In IWCS-5 Fifth International
Workshop on Computational Semantics.
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, David Day, Lisa Ferro, Robert Gaizauskas, Mar-
cia Lazo, Andrea Setzer, and Beth Sundheim. 2003b.
The TimeBank corpus. In Proceedings of Corpus Lin-
guistics 2003, pages 647?656, Lancaster, UK.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING ?08: Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 857?864, Morristown, NJ, USA. Association for
Computational Linguistics.
Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert
Knippen, Seok Bae Jang, Jessica Littman, Anna
Rumshisky, John Phillips, and James Pustejovsky.
2005. Automating temporal annotation with tarsqi. In
ACL ?05: Proceedings of the ACL 2005 on Interac-
tive poster and demonstration sessions, pages 81?84,
Morristown, NJ, USA. Association for Computational
Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal re-
lation identification. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 75?80, Prague.
45
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 134?142,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automated Scoring of Speaking Items in an Assessment for Teachers of
English as a Foreign Language
Klaus Zechner, Keelan Evanini, Su-Youn Yoon, Lawrence Davis,
Xinhao Wang, Lei Chen, Chong Min Lee, Chee Wee Leong
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
{kzechner,kevanini,syoon,ldavis,xwang002,lchen,clee001,cleong}@ets.org
Abstract
This paper describes an end-to-end proto-
type system for automated scoring of spo-
ken responses in a novel assessment for
teachers of English as a Foreign Language
who are not native speakers of English.
The 21 speaking items contained in the as-
sessment elicit both restricted and moder-
ately restricted responses, and their aim is
to assess the essential speaking skills that
English teachers need in order to be effec-
tive communicators in their classrooms.
Our system consists of a state-of-the-art
automatic speech recognizer; multiple fea-
ture generation modules addressing di-
verse aspects of speaking proficiency, such
as fluency, pronunciation, prosody, gram-
matical accuracy, and content accuracy; a
filter that identifies and flags problematic
responses; and linear regression models
that predict response scores based on sub-
sets of the features. The automated speech
scoring system was trained and evaluated
on a data set involving about 1,400 test
takers, and achieved a speaker-level cor-
relation (when scores for all 21 responses
of a speaker are aggregated) with human
expert scores of 0.73.
1 Introduction
As English has become increasingly important as a
language of international business, trade, science,
and communication, efforts to promote teaching
English as a Foreign Language (EFL) have seen
substantially more emphasis in many non-English-
speaking countries worldwide in recent years. In
addition, the prevailing trend in English pedagogy
has been to promote the use of spoken English in
the classroom, as opposed to the respective native
languages of the EFL learners. However, due to
the high demand for EFL teachers in many coun-
tries, the training of these teachers has not always
caught up with these high expectations, so there is
a need for both governmental and private institu-
tions involved in the employment and training of
EFL teachers to assess their competence in the En-
glish language, as well as in English pedagogy.
Against this background, we developed a lan-
guage assessment for EFL teachers who are not
native speakers of English that addresses the four
basic English language skills of Reading, Listen-
ing, Writing and Speaking. This paper focuses
only on the speaking portion of the English assess-
ment, and, in particular, on the system that we de-
veloped to automatically compute scores for test
takers? spoken responses.
Several significant challenges needed to be ad-
dressed during the course of building this auto-
mated speech scoring system, including, but not
limited to:
? The 21 Speaking items belong to 8 differ-
ent task types with different characteristics;
therefore, we had to select features and build
scoring models for each task type separately.
? The test takers speak a variety of native lan-
guages, and thus have very different non-
native accents in their spoken English. Fur-
thermore, the test takers also exhibit a wide
range of speaking proficiency levels, which
contributes to the diversity of their spoken re-
sponses. Our speech recognizer therefore had
to be trained and adapted to a large database
of non-native speech.
? Since content accuracy is very important for
the types of tasks contained in the test, even
small error rates by the automatic speech
recognition (ASR) system can lead to a no-
ticeable impact on feature performance. This
fact motivated the development of a set of
134
features that are robust to speech recognition
errors.
? A significant amount of responses (more than
7%) exhibit issues that make them hard or
impossible to score automatically, e.g., high
noise levels, background speech, etc. We
therefore implemented a filter to identify
these non-scorable responses automatically.
The paper is organized as follows: Section 2
discusses related work; in Section 3, we present
the data used for system training and evaluation;
Section 4 describes the system architecture of the
automated speech scoring system. We detail the
methods we used to build our system in Section 5,
followed by an overview of the results in Section
6. Section 7 discusses our findings; finally, Sec-
tion 8 concludes the paper.
2 Related Work
Automated speech processing and scoring tech-
nology has been applied to a variety of domains
over the course of the past two decades, includ-
ing evaluation and tutoring of children?s literacy
skills (Mostow et al., 1994), preparation for high
stakes English proficiency tests for institutions of
higher education (Zechner et al., 2009), evalua-
tion of English skills of foreign-based call center
agents (Chandel et al., 2007), and evaluation of
aviation English (Pearson Education, Inc., 2011),
to name a few (for a comprehensive overview, see
(Eskenazi, 2009)).
Most of these applications elicit restricted
speech from the participants, and the most com-
mon item type by far is the Read Aloud, in which
the speaker reads a sentence or collection of sen-
tences out loud. Due to the constrained nature
of this task, it is possible to develop ASR sys-
tems that are relatively accurate, even with heav-
ily accented non-native speech. Several types of
features related to a non-native speaker?s ability
to produce English sounds and speech patterns
effectively have been extracted from these types
of responses. Some of the best performing of
these types of features include pronunciation fea-
tures, such as a phone?s spectral match to na-
tive speaker acoustic models (Witt, 1999) and a
phone?s duration compared to native speaker mod-
els (Neumeyer et al., 2000); fluency features, such
as the rate of speech, mean pause length, and num-
ber of disfluencies (Cucchiarini et al., 2000); and
prosody features, such as F0 and intensity slope
(Hoenig, 2002).
In addition to the large majority of applications
that elicit restricted speech, a small number of ap-
plications have also investigated automated scor-
ing of non-native spontaneous speech, in order
to more fully evaluate a speaker?s communicative
competence (e.g., (Cucchiarini et al., 2002) and
(Zechner et al., 2009)). In these systems, the same
types of pronunciation, fluency, and prosody fea-
tures can be extracted; furthermore, features re-
lated to additional aspects of a speaker?s profi-
ciency in the non-native language can be extracted,
such as vocabulary usage (Yoon et al., 2012), syn-
tactic complexity (Bernstein et al., 2010a; Chen
and Zechner, 2011), and topical content (Xie et al.,
2012).
As described in Section 1, the domain for the
automated speaking assessment investigated in
this study is teachers of EFL around the world.
Based on the fact that many of the item types are
designed to assess the test taker?s ability to pro-
ductively use English constructions and linguis-
tic units that commonly recur in English teach-
ing environments, several of the item types elicit
semi-restricted speech (see Table 1 below for a de-
scription of the different item types). These types
of responses fall somewhere between the heavily
restricted speech elicited by a Read Aloud task
and unconstrained spontaneous speech. In these
semi-restricted responses, the test taker may be
provided with a set of lexical items that should
be used to form a sentence; in addition, the test
taker is often asked to make the sentence conform
to a given grammatical template. Thus, the re-
sponses provided for a given prompt of this type
by multiple different speakers will often overlap
with each other; however, it is not possible to
specify a complete list of all possible responses.
These types of items have only infrequently been
examined in the context of automated speech scor-
ing. Some related item types that have been
explored previously include the Sentence Build
and Short item types described in (Bernstein et
al., 2010b); however, those item types typically
elicited a much narrower range of responses than
the semi-restricted ones in this study.
3 Data
The data used in this study was drawn from a pilot
administration of a language assessment for teach-
135
ers of English as a Foreign Language. This test
is designed to assess the ability of a non-native
teacher of English to use English in classroom set-
tings. The language forms and functions included
in this test are based on the materials included in a
curriculum that the test takers studied prior to tak-
ing the assessment. The assessment includes items
that cover the four language skills: Reading, Lis-
tening, Writing, and Speaking. There are a total of
8 different types of Speaking items included in the
assessment. These can be divided into the follow-
ing two categories, depending on how constrained
the test taker?s response is:
? Restricted Speech: In these item types, all
of the linguistic content expected in the
test taker?s response is presented in the test
prompt, and the test taker is asked to read or
repeat it aloud.
? Semi-restricted Speech: In these item types, a
portion of the linguistic content is presented
in the prompt, and the test taker is required to
provide the remaining content to formulate a
complete response.
Sets of 7 Speaking items are presented to the
test taker in thematic units, called ?lessons?, based
on their instructional goals; in total, each test taker
completed three lessons, and thus responded to 21
Speaking items. Table 1 presents descriptions of
the 8 different item types included in the assess-
ment.
The numbers of responses provided by the test
takers to each type (along with their respective re-
sponse durations) are as follows: four Multiple
Choice (10 seconds each), six Read Aloud (four 40
second responses and two 60 second responses),
two Repeat Aloud (15 seconds each), one Incom-
plete Sentence (20 seconds), one Key Words (15
seconds), five Chart (four 20 seconds and one 40
seconds), one Keyword Chart (15 seconds), and
one Visuals (15 seconds). Thus, each test taker
provided a total of approximately 9 minutes of au-
dio.
The responses were all double-scored by trained
human raters on a three-point scale (1 - 3). For
the Restricted Speech items, the raters assessed
the test taker?s pronunciation, pacing, and intona-
tion. For the Semi-restricted Speech items, the re-
sponses were also scored holistically on a 3-point
scale, but raters were also asked to take into ac-
count the appropriateness of the language used
Restricted Speech
Type Description
Multiple
Choice
(MC)
The test taker selects the correct
option and reads it aloud
Read Aloud
(RA)
The test taker reads aloud a set
of classroom instructions
Repeat
Aloud (RP)
The test taker listens to a student
utterance twice and then repeats
it
Semi-restricted Speech
Type Description
Incomplete
Sentence
(IS)
The test taker is given a sentence
fragment and completes the sen-
tence according to the instruc-
tions
Key Words
(KW)
The test taker uses the key words
provided to speak a sentence as
instructed
Chart (CH) The test taker uses an example
from a language chart and then
formulates a similar sentence us-
ing a given grammatical pattern
Keyword
Chart (KC)
The test taker constructs a sen-
tence using keywords provided
and information in a chart
Visuals (VI) The test taker is given two visu-
als and is asked to give instruc-
tions to students based on the
graphical information
Table 1: Types of speaking items included in the
assessment
(e.g., grammatical accuracy and content correct-
ness) in addition to aspects of fluency and pronun-
ciation. For some responses, the raters were not
able to provide a score on the 1 - 3 scale, e.g.,
because the audio response contained no speech
input, the test taker responded in their native lan-
guage, etc. These responses are labeled NS for
Non-Scoreable.
After receiving scores, all of the responses
were transcribed using standard English orthogra-
phy (disfluencies, such as filled pauses and par-
tial words are also included in the transcriptions).
Then, the responses were partitioned (with no
speaker overlap) into five sets for the training and
evaluation of the ASR system and the linear re-
gression scoring models. The amount of data and
136
human score distributions in each of these parti-
tions are displayed in Table 2.
4 System Architecture
The automated scoring system used for the teach-
ers? spoken language assessment consists of the
following four components, which are invoked
one after the other in a pipeline fashion (ETS
SpeechRater
SM
, (Zechner et al., 2009; Higgins et
al., 2011)):
? an automated speech recognizer, generating
word hypotheses from input audio recordings
of the test takers? responses
? a feature computation module that generates
features based on the ASR output, e.g., mea-
suring fluency, pronunciation, prosody, and
content accuracy
? a filtering model that flags responses that
should not be scored automatically due to is-
sues with audio quality, empty responses, etc.
? linear regression scoring models that predict
the score for each response based on a set of
selected features
Furthermore, we use Praat (Boersma and
Weenick, 2012) to extract power and pitch from
the speech signal; this information is used for
some of the feature computation modules, as well
as for the filtering model.
The ASR is an HMM-based triphone system
trained on approximately 800 hours of non-native
speech from a different data set; a background
Language Model (LM) was also trained on the
same data set. Subsequently, 8 adapted LMs were
trained (with an interpolation weight of 0.9 for the
in-domain data) using the responses in the ASR
Training partition for the 8 different item types
listed in Table 1. The ASR system obtained an
overall word error rate (WER) of 13.0% on the
ASR Evaluation partition and 15.6% on the Model
Evaluation partition. As would be expected, the
ASR system performed best on the responses that
were most restricted by the test item and per-
formed worse on the responses that were less re-
stricted. The WER ranged from 11.4% for the
RA responses to 41.4% for the IS responses in the
Model Evaluation partition.
5 Methodology
5.1 Speech features
The feature computation components of our
speech scoring system compute more than 100
features based on a speaker?s response. They be-
long to the following broad dimensions of speak-
ing proficiency: fluency, pronunciation, prosody,
vocabulary usage, grammatical complexity and
accuracy, and content accuracy (Zechner et al.,
2009; Chen and Yoon, 2012; Chen et al., 2009;
Zechner et al., 2011; Yoon et al., 2012; Yoon and
Bhat, 2012; Zechner and Wang, 2013).
After initial feature generation, we selected a set
of about 10 features for each of the 8 item types,
based on the following considerations
1
(Zechner
et al., 2009; Xi et al., 2008):
? empirical performance, i.e., feature correla-
tion with human scores
? construct
2
relevance, i.e., to what extent the
feature measures aspects of speaking profi-
ciency that are considered to be relevant and
important by content experts
? overall construct coverage, i.e., the feature set
should include features from all relevant con-
struct dimensions
? feature independence, i.e., the inter-
correlation between any two features of the
set should be low
Furthermore, some features were transformed
(e.g., by applying the inverse or log function), in
order to increase the normality of their distribu-
tions (an assumption of linear regression classi-
fiers). All feature values that exceeded a thresh-
old of 4 standard deviations from the mean were
replaced by the respective threshold (outlier trun-
cation).
The composition of feature sets is slightly dif-
ferent for the two item type categories: for the 3
restricted item types, features related to fluency,
pronunciation, prosody and read/repeat accuracy
were chosen, whereas for the 5 semi-restricted
item types, vocabulary and grammar features were
also added to the set. Further, while accuracy
1
While automated feature selection is conceivable in prin-
ciple, in our experience it typically does not result in a feature
set that meets all of these criteria well.
2
A construct is the set of knowledge, skills, and abilities
measured by a test.
137
Partition Spk. Resp. Dur. 1 2 3 NS
ASR Training 773 16,049 116.7 1,587 (9.9) 4,086 (25.5) 8,796 (54.8) 1,580 (9.8)
ASR Development 25 525 3.8 53 (10.1) 133 (25.3) 327 (62.3) 12 (2.3)
ASR Evaluation 25 525 3.8 31 (5.9) 114 (21.7) 326 (62.1) 54 (10.3)
Model Training 300 6,300 45.8 675 (10.7) 1,715 (27.2) 3,577 (56.8) 333 (5.3)
Model Evaluation 300 6,300 45.7 647 (10.3) 1,637 (26.0) 3,487 (55.3) 529 (8.4)
Total 1,423 29,699 215.8 2,993 (9.38) 7,685 (25.14) 16,513 (58.26) 2,508 (7.22)
Table 2: Amount of data contained in each partition (speakers, responses, hours of speech) and distribu-
tion of human scores (percentages of scores per partition in brackets).
features for the restricted items were based only
on string alignment measures, content accuracy
features for the semi-restricted items were more
diverse, e.g., based on regular expressions, key-
words, and language model scores (Zechner and
Wang, 2013). Table 3 lists the features that were
used in the scoring models for restricted and semi-
restricted item types, along with sub-constructs
they measure and their description.
5.2 Filtering model
In order to automatically identify responses that
have technical issues (e.g., loud background noise)
or are otherwise not scorable (e.g., empty re-
sponses), a decision tree-based filtering model was
developed using a combination of features derived
from ASR output and from pitch and energy in-
formation (Yoon et al., 2011; Jeon and Yoon,
2012). The filtering model was tested on the scor-
ing model evaluation data, and obtained an ac-
curacy rate (the exact agreement between the fil-
tering model and a human rater concerning the
distinction between scorable and non-scorable re-
sponses) of 97%; it correctly identified 90% of the
non-scorable responses in the data set with a false
positive rate of 21% (recall=0.90, precision=0.79,
F-score=0.84).
5.3 Scoring models
We used the Model Training set to train 8 linear
regression models for the 8 different item types,
using the previously determined feature sets. We
used the features as independent variables in these
models and the summed scores of two human
raters as the dependent variable. These trained
scoring models were then employed to score re-
sponses of the Model Evaluation data (exclud-
ing responses marked as non-scorable by human
raters) and rounded to the nearest integer to predict
the final scores for each response. These scores
were then evaluated against the first human rater
score (H1).
Item N S-H1 H1-H2 WER (%)
RA 1653 0.34 0.51 11.4
RP 543 0.41 0.73 21.8
MC 1036 0.67 0.83 17.1
CH 1372 0.44 0.67 26.3
KW 275 0.45 0.67 28.7
KC 274 0.57 0.74 28.8
IS 260 0.46 0.69 41.4
VI 272 0.43 0.80 30.4
Table 4: Correlations between system and first hu-
man rater (S-H1) and between two human raters
(H1-H2), for all responses of each item type in the
Model Evaluation partition (N). The last column
provides the average ASR word error rate (WER)
in percent.
Additionally, for responses flagged as non-
scorable by the automatic filtering model, the sec-
ond human rater score (H2) was used as final
item score in order to mimic the operational sce-
nario where human raters score responses that are
flagged by the filtering model.
We also compute the agreement between sys-
tem and human raters based on a set of all 21 re-
sponses of a speaker. Score imputation was used
for responses that were labeled as non-scorable by
both the system and H2; in this case, the response
was given the mean score of the total scorable
responses from the same speaker. Similarly, the
same score imputation rule was applied to the H1
scores.
6 Results
Table 4 presents the Pearson correlation coeffi-
cients between human and automated scores for
the responses from the 8 different item types along
with the human-human correlation for each item
type. Furthermore, we also provide the word error
rates of the ASR system for the same 8 item types
in the last column of the table.
138
Feature Sub-construct Description
Content Ed1 Read/repeat accu-
racy / Fluency
Correctly read words per minute
Content Ed2 Read/repeat accu-
racy
Read/repeat word error rate
Content RegEx Content accuracy Matching of regular expressions
Content WER Content accuracy Response discrepancy from high scoring responses
Content NGram Content accuracy N-grams in response matching high scoring response n-
grams
Fluency Rate Fluency Speaking rate
Fluency Chunk Fluency Average length of contiguous word chunks
Fluency Sil1 Fluency Frequency of long silences
Fluency Sil2 Fluency / Grammar Proportion of long within-clause-silences to all within-
clause-silences
Fluency Sil3 Fluency Mean length of silences within a clause
Fluency Disfl1 Fluency Frequency of interruption points (repair, repetition, false
start)
Fluency Disfl2 Fluency Number of disfluencies per second
Fluency Disfl3 Fluency Frequency of repetitions
Pron Vowels Pronunciation Average vowel duration differences relative to a native-
speaker model
Prosody1 Prosody Percentage of stressed syllables
Prosody2 Prosody Mean deviation of time intervals between stressed syllables
Prosody3 Prosody Mean distance between stressed syllables
Vocab1 Vocabulary / Flu-
ency
Number of word types divided by utterance duration
Grammar POS Grammar Part-of-speech based distributional similarity score be-
tween a response and responses with different score levels
Grammar LM Grammar Global language model score (normalized by response
length)
Table 3: List of features used for item type scoring models, with the sub-constructs they represent and
descriptions.
139
Comparison Pearson r
S-H1 0.725
S-H2 0.742
H1-H2 0.934
Table 5: Speaker-level performance (Pearson r
correlations) computed over the sum of all 21
scores from each speaker, N=272
Sub-construct Restricted Semi-restricted
Content 0.33?0.67 0.34?0.61
Fluency 0.19?0.33 0.20?0.33
Pronunciation 0.20?0.22 0.13?0.31
Prosody 0.18?0.24 0.12?0.27
Grammar ? 0.23?0.49
Vocabulary ? 0.21?0.32
Table 6: Range of Pearson r correlations for dif-
ferent features with human scores (H1) by sub-
construct for restricted and semi-restricted item
types.
Table 5 presents the Pearson correlation coeffi-
cients between the speaker-level scores produced
by the automated scoring system (S) and the two
sets of human scores (H1 and H2). These speaker-
level scores were computed based on the sum of
all 21 scores from each speaker in the Model Eval-
uation partition. Responses that received a non-
scorable rating from the human raters were im-
puted, as described above. Furthermore, 28 speak-
ers were excluded from this analysis because they
had more than 7 non-scorable responses each.
3
Finally, Table 6 provides an overview of Pear-
son correlation ranges with human rater scores
(H1) for the different features used in the scoring
models, summarized by the sub-constructs that the
features represent.
7 Discussion
When looking at Table 4, we see that the inter-rater
reliability for human raters ranges between 0.51
(for RA items) and 0.83 (for MC items). Inter-
rater reliability varies less for the 5 semi-restricted
item types (0.67?0.80), compared to the 3 re-
stricted item types (0.51?0.83). As for automated
score correlations with human raters, the Pearson
r coefficients range from 0.34 (RA) to 0.67 (MC).
3
In an operational setting, these test takers would not re-
ceive a test score; instead, they would have the opportunity to
take the test again.
Again, the variability of Pearson r coefficients is
larger for the 3 restricted item types (0.34?0.67)
than for the 5 semi-restricted item types (0.43?
0.57). The degradation in correlation between the
inter-human results and the machine-human re-
sults varies from 0.16 (MC) to 0.37 (VI).
Speech recognition word error rate does not
seem to have a strong influence on model perfor-
mance (RA items have the lowest WER with S-
H1 r=0.34, but r=0.46 for IS items that have the
highest WER). However, we found other factors
that affect model performance negatively; for ex-
ample, multiple repeats of responses by test tak-
ers contribute to the large performance difference
between S-H1 and H1-H2 for the RP items. In
general, we conjecture that using features for a
larger set of sub-construct areas?in the case of
semi-restricted item types?may contribute to the
lower variation of scoring model performance for
this subset of the data.
As for speaker-level results (Table 5), the over-
all degradation between the inter-human correla-
tion and the system-human correlations is of a
similar magnitude (around 0.2) as observed for
most of the individual item types. Still, the
speaker-level correlation of 0.73 is 0.26 higher
than the average item type correlation between the
system and H1.
When we look into more detail at the Pearson
r correlations between individual features used in
the item type scoring models and human scores
(Table 6), we can see that features related to con-
tent accuracy exhibit a substantially stronger per-
formance (r=0.33?0.67) than features related to
most other sub-constructs of speaking proficiency,
namely fluency, pronunciation, prosody, and vo-
cabulary (r ? 0.2). One exception is features
related to grammar, where correlations with hu-
man scores are as high as 0.49. Since related work
on scoring speech using features indicative of flu-
ency, pronunciation, etc. showed higher correla-
tions (e.g., (Cucchiarini et al., 1997; Franco et al.,
2000; Zechner et al., 2009)), we conjecture that
the reason behind this difference is likely to be
found in the fact that the responses in this assess-
ment for teachers of English are quite short (6?
14 words on average for all items except for Read
Aloud items that are about 46 words on average).
Since content features are less reliant on longer
stretches of speech, they still work fairly well for
most items in our corpus.
140
Finally, while the proportion of words contained
in responses in restricted items is much larger than
those contained in responses in semi-restricted
items, these two item type categories are more
evenly distributed over the whole test, i.e., each
test taker responds to 9 semi-restricted and 12 re-
stricted items, and the item scores are then aggre-
gated for a final score with equal weight given to
each item score.
8 Conclusion
This paper presented an overview of an automated
speech scoring system that was developed for a
language assessment for teachers of English as a
Foreign Language (EFL) whose native language
is not English. We described the main compo-
nents of this prototype system and their perfor-
mance: the ASR system, features generated from
ASR output, a filtering model to flag non-scorable
responses, and finally a set of linear regression
models, one for each of 8 different types of test
items.
We found that overall, the correlation between
our speech scoring system?s predicted scores and
human rater scores range between 0.34 and 0.67,
evaluated on responses from 8 item types. Further-
more, we found that correlations based on com-
plete sets of 21 spoken responses per test taker im-
prove to around r = 0.73.
Given the many significant challenges of this
work, including 8 different item types in the as-
sessment, responses from speakers from different
native languages and speaking proficiency levels,
sub-optimal audio conditions for a part of the data,
and a relatively small data set for both ASR system
adaptation and linear regression model training,
we find that the overall performance achieved by
our automated speech scoring system was a good
starting point for an eventual deployment in a low-
stakes assessment context.
Future work will aim at improving the perfor-
mance of the prediction models by the addition of
more features addressing different aspects of the
construct as well as an improved filtering model
for flagging the different types of problematic re-
sponses. Furthermore, agreement between human
raters, in particular for read-aloud items, could be
improved by refining rater rubrics and additional
rater training and monitoring.
Acknowledgments
The authors would like to thank Anastassia Louk-
ina and Jidong Tao for their comments on an ear-
lier version of this paper, and are also indebted
to the anonymous reviewers of BEA-9 and ASRU
2013 for their valuable comments and suggestions.
References
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as pre-
dictors of L2 oral proficiency. In Proceedings of In-
terspeech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355?377.
Paul Boersma and David Weenick. 2012. Praat: Doing
phonetics by computer, version 5.3.32. http://
www.praat.org.
Abhishek Chandel, Abhinav Parate, Maymon Ma-
dathingal, Himanshu Pant, Nitendra Rajput, Shajith
Ikbal, Om Deshmuck, and Ashish Verma. 2007.
Sensei: Spoken language assessment for call cen-
ter agents. In Proceedings of the IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU).
Lei Chen and Su-Youn Yoon. 2012. Application of
structural events detected on ASR outputs for auto-
mated speaking assessment. In Proceedings of In-
terspeech.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722?
731.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by us-
ing speech recognition technology. In Proceedings
of the IEEE Workshop on Auotmatic Speech Recog-
nition and Understanding (ASRU).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers? fluency by means of automatic speech recogni-
tion technology. Journal of the Acoustical Society of
America, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers? fluency: Comparisons between read and spon-
taneous speech. Journal of the Acoustical Society of
America, 111(6):2862?2873.
141
Maxine Eskenazi. 2009. An overview of spoken lan-
guage technology for education. Speech Communi-
cation, 51(10):832?844.
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of
machine scores for automatic grading of pronuncia-
tion quality. Speech Communication, 30(1-2):121?
130.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David M. Williamson. 2011. A three-stage ap-
proach to the automated scoring of spontaneous spo-
ken responses. Computer Speech and Language,
25(2):282?306.
Florian Hoenig. 2002. Automatic assessment of non-
native prosody ? Annotation, modelling, and evalu-
ation. In Proceedings of the International Sympo-
sium on Automatic Detection of Errors in Pronun-
ciation Training (ISADEPT), pages 21?30, Stock-
holm, Sweden.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of Interspeech.
Jack Mostow, Steven F. Roth, Alexander G. Haupt-
mann, and Matthew Kane. 1994. A prototype read-
ing coach that listens. In Proceedings of the Twelfth
National Conference on Artificial Intelligence.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, 30:83?93.
Pearson Education, Inc. 2011. Versant
TM
Aviation English Test. http://www.
versanttest.com/technology/
VersantAviationEnglishTestValidation.
pdf.
Silke Witt. 1999. Use of speech recognition in
computer-assisted language learning. Ph.D. thesis,
Cambridge University.
Xiaoming Xi, Derrick Higgins, Klaus Zechner, and
David M. Williamson. 2008. Automated scoring of
spontaneous speech using SpeechRater v1.0. Edu-
cational Testing Service Research Report RR-08-62.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103?111, Montr?eal, Canada. Asso-
ciation for Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners? syntactic competence based on sim-
ilarity measures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 600?608, Jeju Island, Korea.
Association for Computational Linguistics.
Su-Youn Yoon, Keelan Evanini, and Klaus Zechner.
2011. Non-scorable response detection for auto-
mated speaking proficiency assessment. In Proceed-
ings of NAACL-HLT Workshop on Innovative Use of
NLP for Building Educational Applications.
Su-Youn Yoon, Suma Bhat, and Klaus Zechner. 2012.
Vocabulary profile as a measure of vocabulary so-
phistication. In Proceedings of the 7th Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, NAACL-HLT, Montr?eal, Canada. Associ-
ation for Computational Linguistics.
Klaus Zechner and Xinhao Wang. 2013. Automated
content scoring of spoken responses in an assess-
ment for teachers of english. In Proceedings of
the 8th Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL-HLT,
Atlanta. Association for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011.
Evaluating prosodic features for automated scoring
of non-native read speech. In Proceedings of the
IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU).
142
