First Joint Conference on Lexical and Computational Semantics (*SEM), pages 449?453,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Soft Cardinality: A Parameterized Similarity Function for Text Comparison
Sergio Jimenez
Universidad Nacional
de Colombia, Bogota,
Ciudad Universitaria
edificio 453, oficina 220
sgjimenezv@unal.edu.co
Claudia Becerra
Universidad Nacional
de Colombia, Bogota
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz,
Av. Mendiz?bal, Col.
Nueva Industrial Vallejo,
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
We present an approach for the construction of text
similarity functions using a parameterized resem-
blance coefficient in combination with a softened
cardinality function called soft cardinality. Our ap-
proach provides a consistent and recursive model,
varying levels of granularity from sentences to char-
acters. Therefore, our model was used to compare
sentences divided into words, and in turn, words di-
vided into q-grams of characters. Experimentally,
we observed that a performance correlation func-
tion in a space defined by all parameters was rel-
atively smooth and had a single maximum achiev-
able by ?hill climbing.? Our approach used only sur-
face text information, a stop-word remover, and a
stemmer to tackle the semantic text similarity task
6 at SEMEVAL 2012. The proposed method ranked
3rd (average), 5th (normalized correlation), and 15th
(aggregated correlation) among 89 systems submit-
ted by 31 teams.
1 Introduction
Similarity is the intrinsic ability of humans and some
animals to balance commonalities and differences when
comparing objects that are not identical. Although there
is no direct evidence of how this process works in liv-
ing organisms, some models have been proposed from
the cognitive perspective (Sj?berg, 1972; Tversky, 1977;
Navarro and Lee, 2004). On the other hand, several simi-
larity models have been proposed in mathematics, statis-
tics, and computer science among other fields. Particu-
larly in AI, similarity measures play an important role in
the construction of intelligent systems that are required
to exhibit behavior similar to humans. For instance, in
the field of natural language processing, text similarity
functions provide estimates of the human similarity judg-
ments related to language. In this paper, we combine el-
ements from the perspective of cognitive psychology and
computer science to propose a model for building simi-
larity functions suitable for the task of semantic text sim-
ilarity.
We identify four main families of text similarity func-
tions: i) resemblance coefficients based on sets (e.g. Jac-
card?s (1901) and Dice?s (1945) coefficients) ii) functions
in metric spaces (e.g. cosine tf-idf similarity (Salton et
al., 1975)); iii) the edit distance family of measures (e.g.
Levenstein (1966) distance, LCS (Hirschberg, 1977));
and iv) hybrid approaches ((Monge and Elkan, 1996; Co-
hen et al, 2003; Corley and Mihalcea, 2005; Jimenez et
al., 2010)). All of these measures use a subdivision of
the texts in different granularity levels, such as q-grams
of words, words, q-grams of characters, syllables, and
characters. Among hybrid approaches, Monge-Elkan?s
measure and soft cardinality methods are recursive and
can be used to build similarity functions at any arbitrary
range of granularity. For instance, it is possible to con-
struct a similarity function to compare sentences based
on a function that compares words, which in turn can be
constructed based on a function that compares bigrams of
characters. Furthermore, hybrid approaches can integrate
similarity functions that are not based on the representa-
tion of the surface of text, such as semantic relatedness
measures (Pedersen et al, 2004).
Text similarity measures can be static or adaptive
whether they are binary functions using only surface in-
formation of the two texts, or are functions that suit
to a wider set of texts. For instance, measures using
tf-idf weights adapt their results to the set of texts in
which those weights were obtained. Other approaches
learn parameters of the similarity function from a set of
texts to optimize a particular task. For instance, Ris-
tad and Yianilos (1998) and Bikenko and Mooney (2003)
learned the costs of edit operations for all characters for
an edit-distance function in a name-matching task. Other
machine-learning approaches have also been proposed to
build adaptive measures in name-matching (Bilenko and
449
Mooney, 2003) and textual-entailment tasks.
However, those machine-learning-based methods for
adaptive similarity suffer from sparseness and the ?curse
of dimensionality?. For example, the method of Ristad
and Yianilos learns n2 + 2n parameters, where n is the
size of the character set. Similarly, dimensionality in the
method of Bilenko and Mooney is the size of the data
set vocabulary. This issue is addressed primarily through
machine-learning algorithms, which reduce the dimen-
sionality of the problem regularizating to achieve enough
generalization to get an acceptable performance differ-
ence between training and test data. Although machine-
learning solutions have proven effective for many appli-
cations, the principle of Occam?s razor suggests that it
should be preferable to have a model that explains the
data with a smaller number of significant parameters. In
this paper, we seek a simpler adaptive similarity model
with few meaningful parameters.
Our proposed similarity model starts with a
cardinality-based resemblance coefficient (i.e. Dice?s
coefficient 2|A?B|/|A|+|B|) and generalizes it to model
the effect of asymmetric selection of the referent. This
effect is a human factor discovered by Tversky (1977)
that affects judgments of similarity, i.e. humans tends
to select the more prominent stimulus as the referent
and the less salient stimulus as the object. Some of
Tversky?s examples are ?the son resembles the father?
rather than ?the father resembles the son?, ?an ellipse is
like a circle? not ?a circle is like an ellipse?, and ?North
Korea is like Red China? rather than ?Red China is like
North Korea?. Generally speaking, ?the variant is more
similar to the prototype than vice versa?. In the previous
example, stimulus salience is associated with the promi-
nence of the country; for text comparison we associate
word salience with tf-idf weights. At the text level, we
associate salience with a combination of word-salience,
inter-word similarity, and text length provided by soft
cardinality. Experimentally, we observed that this effect
also occurs when comparing texts, but not necessarily
in the same direction suggested by Tversky. We used
this effect to improve the performance of our similarity
model. In addition, we proposed a parameter that biases
the function to generate greater or lower similarity
scores.
Finally, in our model we used a soft cardinality func-
tion (Jimenez et al, 2010) instead of the classical set car-
dinality. Just as classical cardinality counts the number
of elements which are not identical in a set, soft cardi-
nality uses an auxiliary inter-element similarity function
to make a soft count. For instance, the soft cardinality of
a set with two very similar (but not identical) elements
should be a real number closer to 1.0 instead of 2.0.
The rest of the paper is organized as follows. In Sec-
tion 2 we briefly present soft cardinality. In Section 3 the
proposed parameterized similarity model is presented. In
Section 4 experimental validation is provided using 8 data
sets annotated with human similarity judgments from the
?Semantic-Text-Similarity? task at SEMEVAL-2012. Fi-
nally, a brief discussion is provided in Section 5 and con-
clusions are presented in Section 6.
2 Soft Cardinality
Let A =
{
a1, a2, . . . , a|A|
}
and B =
{
b1, b2, . . . , b|B|
}
be two sets being compared. When each element of ai
or bj has an associated weight wai or wbj the problem
of comparing those sets becomes a weighted similarity
problem. This means that such model has to take into
account not only the commonalities and diferences, but
also their weights. Also, if an (|A ? B|) ? (|A ? B|)
similarity matrix S is available, the problem becomes a
weighted soft similarity problem because the common-
ality between A and B has to be computed not only
with identical elements, but also with elements with a
degree of similarity. The values of S can be obtained
from an auxiliary similarity function sim(a, b) that sat-
isfies at least non-negativity (?a, b, sim(a, b) ? 0) and
reflexivity (?a, sim(a, a) = 1). Other postulates such as
symmetry (?a, b, sim(a, b) = sim(b, a)) and triangle in-
equality1 (?a, b, c, sim(a, c) ? sim(a, b) + sim(b, c)?
1) are not strictly necessary.
Jimenez et al (2010) proposed a set-based weighted
soft-similarity model using resemblance coefficients and
the soft cardinality function instead of classical set car-
dinality. The idea of calculating the soft cardinality is
to treat elements ai in set the A as sets themselves and
to treat inter-element similarities as the intersections be-
tween the elements sim(ai, aj) = |ai ? aj |. Therefore,
the soft cardinality of set A becomes |A|
?
=
?
?
?
?|A|
i=1ai
?
?
?.
Since it is not feasible to calculate this union, they pro-
posed the following weighted approximation using |ai| =
wai :
|A|
?
sim '
|A|?
i
wai
?
?
|A|?
j
sim(ai, aj)
p
?
?
?1
(1)
Parameter p ? 0 in eq.1 controls the ?softeness? of
the cardinality, taking p = 1 its no-effect value and leav-
ing element similarities unchanged for the calculation of
soft cardinality. When p is large, all sim(?, ?) results
lower than 1 are transformed into a number approaching
0. As a result, the soft cardinality behaves like the clas-
sical cardinality, returning the addition of all the weights
of the elements, i.e |A|
?
sim '
?|A|
i wai . When p is close
to 0, all sim(?.?) results are transformed approaching
1triangle inequality postulate for similarity is derived from its coun-
terpart for dissimilarity (distance) distance(a, b) = 1? sim(a, b).
450
into a number approaching 1, making the soft cardinal-
ity returns the average of the weights of the elements, i.e.
|A|
?
sim '
1
|A|
?|A|
i wai . Jimenez et al used p = 2 and
idf weights in the same name-matching task proposed by
Cohen et al (Cohen et al, 2003).
3 A Parameterized Similarity Model
As we mentioned above, Tvesky proposed that humans
tends to select more salient stimulus as referent and less
salient stimulus as object when comparing two objects A
and B. Based on the idea of Tvesrky, the similarity be-
tween two objects can be measured as the ratio between
the salience of commonalities and the salience of the less
salient object. Drawing an analogy between objects as
sets and salience as the cardinality of a set, the salience
of commonalities is |A ? B|, and the salience of the less
salient object is min(|A|, |B|). This ratio is known as the
overlap coefficient Overlap(A,B) = |A?B|min(|A|,|B|) . How-
ever, whether |A| < |B| or whether |A|  |B|, the sim-
ilarity obtained by Overlap(A,B) is the same. Hence,
we propose to model the selecction of the referent using
a parameter ? that makes a weighted average between
min(|A|, |B|) and max(|A|, |B|), controling the degree
to which the asymmetric referent-selection effect is con-
sidered in the similarity measure.
SIM(A,B) =
|A ?B|+ bias
?max (|A|, |B|) + (1? ?)min (|A|, |B|)
(2)
The parameter ? controls the degree to which the
asymmetric referent-selection effect is considered in the
similarity measure. Its no-effect value is ? = 0.5, so
the eq.2 becomes the Dice coefficient. Moreover, when
? = 0 the eq.2 becomes the overlap coefficient, other-
wise when ? = 1 the opposite effect is modeled.
In addition, we introduced a bias parameter in eq. 2
that increases the commonalities of each object pair by
the same amount, and so it measures the degree to which
all of the objects have commonalities among each other.
Clearly, the non-effect value for the bias parameter is 0.
Besides, the bias parameter has the effect of biasing
SIM(A,B) by considering any pair ?A,B? more sim-
ilar if bias > 0 and their cardinalities are small. Con-
versely, the similarity between pairs with large cardinal-
ities is promoted if bias < 0. However, as higher values
of biasmay result in similarity scores outside the interval
[0, 1], additional post-procesing to limit the similarities in
this interval may be required.
The proposed parameterized text similarity measure is
constructed by combining the proposed resemblance co-
efficient in eq.2 and the soft cardinality in eq.1. The
resulting measure has three parameters: ?, bias, and p.
Weights wai can be idf weights. This measure takes two
? Asymetric referent selection at text level
bias Bias parameter at text level
p Soft cardinality exponent at word level
wai Element weights at word level
q1, q2 q1-grams or [q1 : q2]spectra word division
?sim Asymetric referent selection at q-gram level
biassim Bias parameter q-gram level
Table 1: Parameters of the proposed similarity model
texts represented as sets of words and returns their simi-
larity. The auxiliary similarity function sim(a, b) neces-
sary for calculating the soft cardinality is another param-
eter of the model. This auxiliary function is any function
that can compare two words and return a similarity score
in [0, 1].
To build this sim(a, b) function, we chose to reuse the
eq.2 but representing words as sets of q-grams or ranges
of q-grams of different sizes, i.e. [q1 : q2] spectra. Q-
grams are consecutive overlapped substrings of size q.
For instance, the word ?saturday? divided into trigrams
is {/sa, sat, atu, tur, urd, rda, day, ay.}. The character
?.? is a padding character added to differenciate q-grams
at the begining or end of the string. A [2 : 4]spectra
is the combined representation of a word using ?in this
example? bigrams, trigrams and quadgrams (Jimenez and
Gelbukh, 2011). The cardinality function for sim() was
the classical set cardinality. Clearly, the soft cardinal-
ity could be used again if an auxiliary similarity func-
tion for character comparison and a q-gram weighting
mechanism are provided to allow another level of recur-
sion. Therefore, the parameters of sim(a, b) are: ?sim,
biassim. Finally, the entire set of parameters of the pro-
posed similarity model is shown in Table 1.
4 Experimental Setup and Results
The aim of these experiments is to observe the behavior
of the parameters of our similarity model and verify if the
hypothesis that motivated these parameters can be con-
firmed experimentally. The experimental data are 8 data
sets (3 for training and 5 for test) proposed in the ?Seman-
tic Text Similarity? task at SEMEVAL-2012. Each data
set consist of a set of pairs of text annotated with human-
similarity judgments on a scale of 0 to 5. Each similarity
judgment is the average of the judgments provided by 5
human judges. For a comprehensible description of the
task see(Agirre et al, 2012).
For the experiments, all data sets were pre-processed
by converting to lowercase characters, English stop-
words removal and stemming using Porter stemmer
(Porter, 1980). The performance measure used for all ex-
periments was the Pearson correlation r.
451
4.1 Model Parameters
In order to make an initial exploration of the parame-
ters in Table 1, we set q1 = 2 (i.e. bigrams) and used
wai = idf(ai). For other parameters, we started with all
the non-effect values, i.e. ? = 0.5, bias = 0, p = 1,
?sim = 0.5 and biassim = 0. Plots in Figure 1 show
the Pearson correlation measured in each of the data sets.
For each graph, the non-effect configuration was used and
each parameter varies in the range indicated in each hor-
izontal axis. For best viewing, the non-effect values on
each graph are represented by a vertical line.
In this exploration of the parameters it was noted that
each parameter defines a function for the performance
measure that is smooth and with an unique global maxi-
mum. Therefore, we assumed that the join performance
function in the space defined by the 5 parameters also
had the same properties. The parameters for each data set
shown in Table 2 were found using a simple hill-climbing
algorithm. Different q-gram and spectra configurations
were tested manually.
5 Discussion
It is possible to observe from the results in Figure 1 and
Table 2 that the behavior of the parameters is similar in
pairs of data sets that have training and test parts. This
behavior is evident in both MSRvid and MSRpar data
sets, but it is less evident in SMTeuroparl. Furthermore,
the optimal parameters for training data sets MSRvid and
MSRpar were similar to those of their test data sets. In
conclusion, the proposed set of parameters provides a set
of features that characterize a data set for the text similar-
ity task.
Regarding the effect of asymmetry in referent selecc-
tion proposed by Tvesrky, it was observed that ?at text
level? the MSRvid data sets were the only ones that sup-
ported this hypothesis (? = 0.32, 0.42). The remaining
data sets showed the opposite effect (? > 0.5). That is,
annotators chose the most salient document (the longer)
as the referent when a pair of texts is being compared.
The Table 2 also shows that the optimal parameters
for all data sets were different from the no-effect values
combination. This result can also be seen in Figure 1,
where curves crossed the vertical line of no-effect value
?in most of the cases? in values different to the optimum.
Clearly, the proposed set of parameters is useful for ad-
justing the similarity function for a particular data set and
task.
6 Conclusions
We have proposed a new parameterized similarity func-
tion for text comparison and a method for finding the op-
timal values of the parameter set when training data is
available. In addition, the parameter ?, which was moti-
vated by the similarity model of Tversky, proved effective
in obtaining better performance, but we could not con-
firm the Tvesky?s hypothesis that humans tends to select
the object (text) with less stimulus salience (text length)
as the referent. This result might have occurred because
either the stimulus salience is not properly represented by
the length of the text, or Tversky?s hypothesis cannot be
extended to text comparison.
The proposed similarity function proved effective in
the task of ?Semantic Text Similarity? in SEMEVAL
2012. Our method obtained the third best average cor-
relation on the 5 test data sets. This result is remarkable
because our method only used data from the surface of
the texts, a stop-word remover, and a stemmer, which can
be even be considered as a baseline method.
Acknowledgments
This research was funded by the Systems and Industrial
Engineering Department, the Office of Student Welfare
of the National University of Colombia, Bogot?, and
throught a grant from the Colombian Department for
Science, Technology and Innovation Colciencias, proj.
110152128465. The second author recognizes the sup-
port from Mexican Government (SNI, COFAA-IPN, SIP
20113295, CONACYT 50206-H) and CONACYT?DST
India (proj. ?Answer Validation through Textual Entail-
ment?).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-Agirre
Aitor. 2012. SemEval-2012 task 6: A pilot on semantic
textual similarity. In Proc. of the 6th International Workshop
on Semantic Evaluation (SemEval 2012), in conjunction with
the First Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012)., Montreal,Canada.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive du-
plicate detection using learnable string similarity measures.
In Proc. of the ninth ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, pages 39?48,
Washington, D.C. ACM.
William W Cohen, Pradeep Ravikumar, and Stephen E Fien-
berg. 2003. A comparison of string distance metrics for
Name-Matching tasks. In Proc. of the IJCAI2003 Workshop
on Information Integration on the Web II Web03.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and Entail-
ment, Stroudsburg, PA.
Lee R. Dice. 1945. Measures of the amount of ecologic associ-
ation between species. Ecology, pages 297?302.
Daniel S. Hirschberg. 1977. Algorithms for the longest com-
mon subsequence problem. J. ACM, 24(4):664?675.
452
0.6
0.7
0.8
correlati
on
p
0.3
0.4
0.5
0 1 2 3 4 5 6 7 8
Pearson
correlati
on
?
0 0.5 1 1.5
bias
-15 -5 5 15
?sim
-0.5 0.5 1.5
biassim
-4 -2 0 2 4
MSRvid(tr) MSRvid(te) MSRpar(tr) MSRpar(te) SMTeur(tr) SMTeur(te) OnWN SMTnews no effect
-5 -3 -1 1 3 5
Figure 1: Exploring similarity model parameters around their no-effect values (tr=training, te=test)
Parameters correl. Official Results
Data set [q1 : q2] ? bias p ?sim biassim r SoftCard Best
MSRpar.training [4] 0.62 1.14 0.77 -0.04 -0.38 0.6598 n/a n/a
MSR.par.test [4] 0.60 1.02 0.9 -0.02 -0.4 0.6335 0.64051 0.7343
MSRvid.training [1:4] 0.42 -0.80 2.28 0.18 0.08 0.8323 n/a n/a
MSRvid.test [1:4] 0.32 -0.80 1.88 1.08 0.08 0.8579 0.8562 0.8803
SMTeuroparl.training [2:4] 0.74 -0.06 0.91 1.88 2.90 0.6193 n/a n/a
SMTeuroparl.test [2:4] 0.84 -0.16 0.71 1.78 3.00 0.5178 0.51522 0.5666
OnWN.test [2:5] 0.88 -0.62 1.36 -0.02 -0.70 0.7202 0.71091 0.7273
SMTnews.test [1:4] 0.88 0.88 1.57 0.80 3.21 0.5344 0.48331 0.6085
1Result obtained using Jaro-Winkler (Winkler, 1990) measure as sim(a, b) function between words.
2Result obtained using generalized Monge-Elkan measure p = 4, no stop-words removal and no term weights
(Jimenez et al, 2009).
Table 2: Results with optimized parameters and official SEMEVAL 2012 results
Paul Jaccard. 1901. Etude comparative de la distribution florare
dans une portion des alpes et des jura. Bulletin de la Soci?t?
Vaudoise des Sciences Naturelles, pages 547?579.
Sergio Jimenez and Alexander Gelbukh. 2011. SC spectra: a
linear-time soft cardinality approximation for text compari-
son. In Proc. of the 10th international conference on Artifi-
cial Intelligence, MICAI?11, Puebla, Mexico.
Sergio Jimenez, Claudia Becerra, Alexander Gelbukh, and
Fabio Gonzalez. 2009. Generalized Monge-Elkan method
for approximate text string comparison. In Computational
Linguistics and Intelligent Text Processing, volume 5449 of
LNCS, pages 559?570.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gelbukh.
2010. Text comparison using soft cardinality. In String Pro-
cessing and Information Retrieval, volume 6393 of LNCS,
pages 297?302.
Vladimir I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Alvaro E. Monge and Charles Elkan. 1996. The field matching
problem: Algorithms and applications. In Proc. KDD-96,
pages 267?270, Portland, OR.
Daniel Navarro and Michael D. Lee. 2004. Common and dis-
tinctive features in stimulis representation: A modified ver-
sion of the contrast model. Psychonomic Bulletin & Review,
11:961?974.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. WordNet::Similarity: measuring the relatedness of
concepts. In Proc. HLT-NAACL?Demonstration Papers,
Stroudsburg, PA.
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 3(14):130?137.
Eric S. Ristad and Peter N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(5):522?532.
Gerard Salton, A. Wong, and C.S. Yang. 1975. A vector space
model for automatic indexing. Com. ACM, 18(11):613?620.
L. Sj?berg. 1972. A cognitive theory of similarity. G?teborg
Psychological Reports.
Amos Tversky. 1977. Features of similarity. Psychological
Review, 84(4):327?352.
William E. Winkler. 1990. String comparator metrics and en-
hanced decision rules in the Fellegi-Sunter model of record
linkage. In Proc. of the Section on Survey Research Methods.
453
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 684?688,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Soft Cardinality + ML: Learning Adaptive Similarity Functions
for Cross-lingual Textual Entailment
Sergio Jimenez
Universidad Nacional
de Colombia, Bogota,
Ciudad Universitaria
edificio 453, oficina 220
sgjimenezv@unal.edu.co
Claudia Becerra
Universidad Nacional
de Colombia, Bogota
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz,
Av. Mendiz?bal, Col.
Nueva Industrial Vallejo,
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
This paper presents a novel approach for building
adaptive similarity functions based on cardinality us-
ing machine learning. Unlike current approaches
that build feature sets using similarity scores, we
have developed these feature sets with the cardinal-
ities of the commonalities and differences between
pairs of objects being compared. This approach al-
lows the machine-learning algorithm to obtain an
asymmetric similarity function suitable for direc-
tional judgments. Besides using the classic set cardi-
nality, we used soft cardinality to allow flexibility in
the comparison between words. Our approach used
only the information from the surface of the text,
a stop-word remover and a stemmer to address the
cross-lingual textual entailment task 8 at SEMEVAL
2012. We have the third best result among the 29
systems submitted by 10 teams. Additionally, this
paper presents better results compared with the best
official score.
1 Introduction
Adaptive similarity functions are those functions that, be-
yond using the information of two objects being com-
pared, use information from a broader set of objects
(Bilenko and Mooney, 2003). Therefore, the same sim-
ilarity function may return different results for the same
pair of objects, depending on the context of where the
objects are. Adaptability is intended to improve the per-
formance of the similarity function in relation to the task
in question associated with the entire set of objects. For
example, adaptiveness improves relevance of documents
retrieved for a query in an information retrieval task for a
particular document collection.
In text applications there are mainly three methods
to provide adaptiveness to similarity functions: term
weighting, adjustment or learning the parameters of the
similarity function, and machine learning. Term weight-
ing is a common practice that assigns a degree of im-
portance to each occurrence of a term in a text collec-
tion (Salton and Buckley, 1988; Lan et al, 2005). Sec-
ondly, if a similarity function has parameters, these can
be adjusted or learned to adapt to a particular data set.
Depending on the size of the search space defined by
these parameters, they can be adjusted either manually
or using a technique of AI. For instance, Jimenez et
al. manually adjusted a single parameter in the gener-
alized measure of Monge-Elkan (1996) (Jimenez et al,
2009) and Ristrad and Yanilios (1998) learned the costs
of editing operations between particular characters for
the Levenshtein distance (1966) using HMMs. Thirdly,
the machine-learning approach aims to learn a similar-
ity function based on a vector representation of texts us-
ing a subset of texts for training and a learning func-
tion (Bilenko and Mooney, 2003). The three methods
of adaptability can also be used in a variety of combina-
tions, e.g. term weighting in combination with machine
learning (Debole and Sebastiani, 2003; Lan et al, 2005).
Finally, to achieve adaptability, other approaches use data
sets considerably larger, such as large corpora or the Web,
e.g. distributional similarity (Lee, 1999).
In the machine-learning approach, a vector representa-
tion of texts is used in conjunction with an algorithm of
classification or regression (Alpaydin, 2004). Each vec-
tor of features ?f1, f2, . . . , fm? is associated to each pair
?Ti, Tj? of texts. Thus, Bilenko et al (2003) proposed a
set of features indexed by the data set vocabulary, simi-
lar to Zanzotto et al, (2009) who used fragments of parse
trees. However, a more common approach is to select as
features the scores of different similarity functions. Using
these features, the machine-learning algorithm discovers
the relative importance of each feature and a combina-
tion mechanism that maximizes the alignment of the final
result with a gold standard for the particular task.
In this paper, we propose a novel approach to extract
feature sets for a machine-learning algorithm using car-
684
dinalities rather than scores of similarity functions. For
instance, instead of using as a feature the score obtained
by the Dice?s coefficient (i.e. 2?|Ti?Tj |/|Ti|+|Tj |), we use
|Ti|, |Tj | and |Ti ? Tj | as features. The rationale behind
this idea is that despite the similarity scores being suitable
for learning a combined function of similarity, they hide
the information imbalance between the original pair of
texts. Our hypothesis is that the information coded in this
imbalance could provide the machine-learning algorithm
with better information to generate a combined similar-
ity score. For instance, consider these pairs of texts: ?
?The beach house is white.?, ?The house was completely
empty.? ? and ? ?The house?, ?The beach house was com-
pletely empty and isolated? ?. Both pairs have the same
similarity score using the Dice coefficient, but it is evi-
dent that the latter has an imbalance of information lost in
that single score. This imbalance of information is even
more important if the task requires to identify directional
similarities, such as ?T1 is more similar to T2, than T2 is
to T1?.
However, unlike the similarity functions, which are
numerous, there is only one set cardinality. This issue
can be addressed using the soft cardinality proposed by
Jimenez et al (2010), which uses an auxiliary function of
similarity between elements to make a soft count of the
elements in a set. For instance, the classic cardinality of
the set A = { ?Sunday?, ?Saturday? } is |A| = 2; and the
soft cardinality of the same set, using a normalized edit-
distance as auxiliary similarity function, is |A|
?
sim = 1.23
because of the commonalities between both words. Fur-
thermore, soft cardinality allows weighting of elements
giving it additional capacity to adapt.
We used the proposed approach to participate in the
cross-lingual textual-entailment task 8 at SEMEVAL
2012. The task was to recognize bidirectional, forward,
backward or lack of entailment in pairs of texts written
in five languages. We built a system based on the pro-
posed method and the use of surface information of the
text, a stop-word remover and a stemmer. Our system
achieved the third best result in official classification and,
after some debugging, we are reporting better results than
the best official scores.
This paper is structured as follows. Section 2 briefly
describes soft cardinality and other cardinalities for text
applications. Section 3 presents the proposed method.
Experimental validation is presented in Section 4. A brief
discussion is presented in Section 5. Finally, conclusions
are drawn in Section 6.
2 Cardinalities for text
Cardinality is a measure of counting the number of el-
ements in a set. The cardinality of classical set theory
represents the number of non-repeated elements in a set.
However, this cardinality is rigid because it counts in the
same manner very similar or highly differentiated ele-
ments. In text applications, text can be modeled as a
set of words and a desirable cardinality function should
take into account the similarities between words. In this
section, we present some methods to soften the classical
concept of cardinality.
2.1 Lemmatizer Cardinality
The simplest approach is to use a stemmer that collapses
words with common roots in a single lemma. Consider
the sentence: ?I loved, I am loving and I will love you?.
The plain word counting of this sentence is 10 words. The
classical cardinality collapses the three occurrences of the
pronoun ?I? giving a count of 8. However, a lemmatizer
such as Porter?s stemmer (1980) also collapses the words
?loved?, ?loving? and ?love? in a single lemma ?love? for
a count of 6. Thus, when a text is lemmatized, it induces
a relaxation of the classical cardinality of a text. In ad-
dition, to provide corpus adaptability, a weighted version
of this cardinality can add weights associated with each
word occurrence instead of adding 1 for each word (e.g.
tf-idf).
2.2 LCS cardinality
Longest common subsequence (LCS) length is a measure
of the commonalities between two texts, unlike set in-
tersection, taking into account the order. Therefore, a
cardinality function of a pair of texts A and B could
be |A ? B| = len(LCS(A,B)), |A| = len(A) and
|B| = len(B). Functions len(?) and LCS(?, ?) calcu-
late length and LCS respectively, either in character or
word granularity.
2.3 Soft Cardinality
Soft cardinality is a function that uses an auxiliary simi-
larity function to make a soft count of the elements (i.e.
words) in a set (i.e. text) (Jimenez et al, 2010). The aux-
iliary similarity function can be any measure or metric
that returns scores in the interval [0, 1], with 0 being the
lowest degree of similarity and 1 the highest (i.e. identi-
cal words). Clearly, if the auxiliary similarity function is
a rigid comparator that returns 1 for identical words and
0 otherwise, the soft cardinality becomes the classic set
cardinality.
The soft cardinality of a set A = {a1, a2, . . . , a|A|}
can be calculated by the following expression: |A|
?
sim '
?|A|
i wai
(?|A|
j sim(ai, aj)
p
)?1
. Where sim(?, ?) is
the auxiliary similarity function for approximate word
comparison, wai are weights associated with each word
ai, and p is a tuning parameter that controls the degree
of smoothness of the cardinality, i.e. if 0 ? p all ele-
ments in a set are considered identical and if p?? soft
cardinality becomes classic cardinality.
685
2.4 Dot-product VSM ?Cardinality?
Resemblance coefficients are cardinality-based simi-
larity functions. For instance, the Dice coefficient
is the ratio between the cardinality of the intersec-
tion divided by the arithmetic mean of individual
cardinalities:2?|A?B|/|A|+|B|. The cosine coefficient is
similar but instead of using the arithmetic mean it uses
the geometric mean: |A?B|/
?
|A|?
?
|B|. Furthermore, the
cosine similarity is a well known metric used in the vec-
tor space model (VSM) proposed by Salton et al (1975)
cosine(A,B) =
?
wai?wbi??
w2ai?
??
w2bi
. Clearly, this expres-
sion can be compared with the cosine coefficient inter-
preting the dot-product operation in the cosine similar-
ity as a cardinality. Thus, the obtained cardinalities are:
|A ? B|vsm =
?
wpai ? w
p
bi
, |A|vsm =
?
w2pai and
|B|vsm =
?
w2pbi . The exponent p controls the effect
of weighting providing no effect if 0? p or emphasising
the weights if p > 0. In a similar application, Gonza-
lez and Caicedo (2011) used p = 0.5 and normalization
justified by the quantum information retrieval theory.
3 Learning Similarity Functions from
Cardinalities
Different similarity measures use different knowledge,
identify different types of commonalities, and compare
objects with different granularity. In many of the auto-
matic text-processing applications, the qualities of sev-
eral similarity functions may be required to achieve the
final task. The combination of similarity scores with a
machine-learning algorithm to obtain a unified effect for
a particular task is a common practice (Bilenko et al,
2003; Malakasiotis and Androutsopoulos, 2007; Malaka-
siotis, 2009). For each pair of texts for comparison, there
is provided a vector representation based on multiple sim-
ilarity scores as a set of features. In addition, a class at-
tribute is associated with each vector which contains the
objective of the task or the gold standard to be learned by
the machine-learning algorithm.
However, the similarity scores conceal important in-
formation when the task requires dealing with directional
problems, i.e. whenever the order of comparing each pair
of texts is related with the class attribute. For instance,
textual entailment is a directional task since it is neces-
sary to recognize whether the first text entails the second
text or vice versa. This problem can be addressed us-
ing asymmetric similarity functions and including scores
for sim(A,B) and sim(B,A) in the resulting vector for
each pair ?A,B?. Nevertheless, the similarity measures
that are more commonly used are symmetric, e.g. edit-
distance (Levenshtein, 1966), LCS (Hirschberg, 1977),
cosine similarity, and many of the current semantic re-
latedness measures (Pedersen et al, 2004). Although,
there are asymmetric measures such as the Monge-Elkan
measure (1996) and the measure proposed by Corley and
Mihalcea (Corley and Mihalcea, 2005), they are outnum-
bered by the symmetric measures. Clearly, this situation
restricts the use of the machine learning as a method of
combination for directional problems.
Alternatively, we propose the construction of a vector
for each pair of texts using cardinalities instead of sim-
ilarity scores. Moreover, using cardinalities rather than
similarity scores allows the machine-learning algorithm
to discover patterns to cope with directional tasks.
Basically, we propose to use a set with six features for
each cardinality function: |A|, |B|, |A ? B|, |A ? B|,
|A?B| and |B ?A|.
4 Experimental Setup
4.1 Cross-lingual Textual Entailment (CLTE) Task
This task consist of recognizing in a pair of topically re-
lated text fragments T1 and T2 in different languages, one
of the following possible entailment relations: i) bidi-
rectional T1 ? T2 ? T1 ? T2, i.e. semantic equiv-
alence; ii) forward T1 ? T2 ? T1 : T2; iii) back-
ward T1 ; T2 ? T1 ? T2; and iv) no entailment
T1 ; T2 ? T1 : T2. Besides, both T1 and T2 are as-
sumed to be true statements; hence contradictory pairs
are not allowed.
Data sets consist of a collection of 1,000 text pairs
(500 for training and 500 for testing) each one labeled
with one of the possible entailment types. Four balanced
data sets were provided using the following language
pairs: German-English (deu-eng), French-English (fra-
eng), Italian-English (ita-eng) and Spanish-English (spa-
eng). The evaluation measure for experiments was accu-
racy, i.e. the ratio of correctly predicted pairs by the total
number of predictions. For a comprehensive description
of the task see (Negri et al, 2012).
4.2 Experiments
Given that each pair of texts ?T1, T2? are in different lan-
guages, a pair of translations ?T t1 , T
t
2? were provided us-
ing Google Translate service. Thus, each one of the text
pairs ?T1, T t2? and ?T
t
1 , T2? were in the same language.
Then, all produced pairs were pre-processed by remov-
ing stop-words in their respective languages. Finally, all
texts were lemmatized using Porter?s stemmer (1980) for
English and Snowball stemmers for other languages us-
ing an implementation provided by the NLTK (Loper and
Bird, 2002).
Then, different set of features were generated using
similarity scores or cardinalities. While each symmet-
ric similarity function generates 2 features i)sim(T1, T t2)
and ii)sim(T t1 , T2), asymmetric functions generate two
additional features iii)sim(T t2 , T1) and iv)sim(T2, T
t
1).
686
On the other hand, each cardinality function generates
12 features: i) |T1|, ii) |T t2 |, iii) |T1 ? T
t
2 |, iv) |T1 ? T
t
2 |,
v) |T1 ? T t2 |, vi) |T
t
2 ? T1|, vii) |T
t
1 |, viii) |T2|, ix)
|T t1 ? T2|, x) |T
t
1 ? T2|, xi) |T
t
1 ? T2|, and xii) |T2 ? T
t
1 |.
Various combinations of cardinalities, symmetric and
asymmetric functions were used to generate the follow-
ing feature sets:
Sym.simScores: scores of the following symmetric
similarity functions: Jaccard, Dice, and cosine coef-
ficients using classical cardinality and soft cardinality
(edit-distance as auxiliar sim. function). In addition, co-
sine similarity, softTFIDF (Cohen et al, 2003) and edit-
distance (total 18 features).
Asym.LCS.sim: scores of the following asymmetric
similarity functions: sim(T1, T2) = lcs(T1,T2)/len(T1)
and sim(T1, T2) = lcs(T1,T2)/len(T2) at character level (4
features).
Classic.card: cardinalities using classical set cardinal-
ity (12 features).
Dot.card.w: dot-product cardinality using idf weights
as described in Section 2.4, using p = 1 (12 features).
LCS.card: LCS cardinality at word-level using idf
weights as described in Section 2.1 (12 features).
SimScores: combined features sets from
Sym.SimScores, Asym.LCS.sim and the general-
ized Monge-Elkan measure (Jimenez et al, 2009) using
p = 1, 2, 3 (30 features).
Dot.card.w.0.5: same as Dot.card.w using p = 0.5.
Classic.card.w: classical cardinality using idf weights
(12 features).
Soft.card.w: soft cardinality using idf weights as de-
scribed in Section 2.3 using p = 1, 2, 3, 4, 5 (60 features).
The machine-learning classification algorithm for all
feature sets was SVM (Cortes and Vapnik, 1995) with the
complexity parameter C = 1.5 and a linear polynomial
kernel. All experiments were conducted using WEKA
(Hall et al, 2009).
4.3 Results
In Semeval 2012 exercise, participants were given a par-
ticular subdivision into training and test subsets for each
data set. For official results, participants received only the
gold-standard labels for the subset of training, and accu-
racies of each system in the test subset was measured by
the organizers. In Table 1, the results for that particular
division are shown. At the bottom of that table, the of-
ficial results for the first three systems are shown. Our
system, ?3rd.Softcard? was configured using soft cardi-
nality with edit-distance as auxiliary similarity function
and p = 2. Erroneously, at the time of the submission,
all texts in the 5 languages were lemmatized using an En-
glish stemmer and stop-words in all languages were ag-
gregated into a single set before the withdrawal. In spite
of these bugs, our system was the third best score.
FEATURES SPA ITA FRA DEU avg.
Sym.simScores 0.404 0.410 0.410 0.410 0.409
Asym.LCS.sim 0.490 0.492 0.482 0.474 0.485
Classic.card 0.560 0.534 0.570 0.542 0.552
Dot.card.w 0.562 0.568 0.550 0.548 0.557
LCS.card 0.606 0.566 0.568 0.558 0.575
SimScores 0.600 0.562 0.568 0.572 0.576
Dot.card.w.0.5 0.584 0.574 0.586 0.572 0.579
Classic.card.w 0.584 0.576 0.588 0.590 0.585
Soft.card.w 0.598 0.602 0.624 0.604 0.607
SEMEVAL 2012 OFFICIAL RESULTS
1st.HDU.run2 0.632 0.562 0.570 0.552 0.579
2nd.HDU.run1 0.630 0.554 0.564 0.558 0.577
3rd.Softcard 0.552 0.566 0.570 0.550 0.560
Table 1: Accuracy results for Semeval2012 task 8
Soft.card.w 60.174(1.917)% imprv. Sign.
Sym.simScore 39.802(1.783)% 51.2% <0.001
Asym.LCS.sim 48.669(1.820)% 23.6% <0.001
Classic.card 55.278(2.422)% 8.9% 0.010
Dot.card.w 54.906(2.024)% 9.6% 0.004
LCS.card 55.131(2.471) % 9.1% 0.015
SimScores 56.889(2.412) % 5.8% 0.124
Dot.card.w.0.5 57.114(2.141)% 5.4% 0.059
Classic.card.w 56.708(2.008)% 6.1% 0.017
Table 2: Average accuracy comparison vs. Soft.card.w in 100
runs
To compare our approach of using feature sets based
on soft cardinality versus other approaches, we gener-
ated 100 random training-test subdivisions (50%-50%) of
each data set. The average results were compared and
tested statistically with the paired T-tested corrected test.
Results, deviations, the percentage of improvement, and
its significance in comparison with the Soft.card.w sys-
tem are shown in Table2.
5 Discusion
Results in Table 2 show that our hypothesis that fea-
ture sets obtained from cardinalities should outperform
features sets obtained from similarity scores was de-
mostrated when compared versus similarity functions al-
ternatively symmetrical or asymetrical. However, when
our approach is compared with a feature set obtained by
combining symmetric and asymmetric functions, we ob-
tained an improvement of 5.8% but only with a signif-
icance of 0.124. Regarding soft cardinality compared
to alternative cardinalities, soft cardinality outperformed
others in all cases with significance <0.059.
687
6 Conclusions
We have proposed a new method to compose feature sets
using cardinalities rather than similarity scores. Our ap-
proach proved to be effective for directional text compar-
ison tasks such as textual entailment. Furthermore, the
soft cardinality function proved to be the best for obtain-
ing such sets of features.
Acknowledgments
This research was funded by the Systems and Industrial
Engineering Department, the Office of Student Welfare
of the National University of Colombia, Bogot?, and
throught a grant from the Colombian Department for
Science, Technology and Innovation Colciencias, proj.
110152128465. The second author recognizes the sup-
port from Mexican Government (SNI, COFAA-IPN, SIP
20113295, CONACYT 50206-H) and CONACYT?DST
India (proj. ?Answer Validation through Textual Entail-
ment?).
References
Ethem Alpaydin. 2004. Introduction to Machine Learning.
MIT press.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive du-
plicate detection using learnable string similarity measures.
In Proc. of the ninth ACM SIGKDD international conference
on Knowledge discovery and data mining, Washington, D.C.
Mikhail Bilenko, Raymond Mooney, William Cohen, Pradeep
Ravikumar, and Stephen Fienberg. 2003. Adaptive name
matching in information integration. IEEE Intelligent Sys-
tems, 18(5):16?23.
William W Cohen, Pradeep Ravikumar, and Stephen E Fien-
berg. 2003. A comparison of string distance metrics for
Name-Matching tasks. In Proc. of the IJCAI2003 Workshop
on Information Integration on the Web II Web03.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proceedings of the ACL Work-
shop on Empirical Modeling of Semantic Equivalence and
Entailment, Stroudsburg, PA.
Corinna Cortes and Vladimir N. Vapnik. 1995. Support-Vector
networks. Machine Learning, 20(3):273?297.
Franca Debole and Fabrizio Sebastiani. 2003. Supervised term
weighting for automated text categorization. In Proc. of the
2003 ACM symposium on applied computing, New York,
NY.
Fabio A. Gonzalez and Juan C. Caicedo. 2011. Quantum la-
tent semantic analysis. In Proc. of the Third international
conference on Advances in information retrieval theory.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software: An
update. SIGKDD Explorations, 11(1):10?18.
Daniel S. Hirschberg. 1977. Algorithms for the longest com-
mon subsequence problem. J. ACM, 24(4):664?675.
Sergio Jimenez, Claudia Becerra, Alexander Gelbukh, and
Fabio Gonzalez. 2009. Generalized Monge-Elkan method
for approximate text string comparison. In Computational
Linguistics and Intelligent Text Processing, volume 5449 of
LNCS, pages 559?570.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gelbukh.
2010. Text comparison using soft cardinality. In String Pro-
cessing and Information Retrieval, volume 6393 of LNCS,
pages 297?302.
Man Lan, Chew-Lim Tan, Hwee-Boon Low, and Sam-Yuan
Sung. 2005. A comprehensive comparative study on term
weighting schemes for text categorization with support vec-
tor machines. In Special interest tracks and posters of the
14th international conference on World Wide Web, New
York, NY.
Lillian Lee. 1999. Measures of distributional similarity. In
Proc. of the 37th annual meeting of the Association for Com-
putational Linguistics on Computational Linguistics, Col-
lege Park, Maryland.
Vladimir I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Edward Loper and Steven Bird. 2002. NLTK: the natural lan-
guage toolkit. In In Proceedings of the ACL Workshop on
Effective Tools andMethodologies for Teaching Natural Lan-
guage Processing and Computational Linguistics, Philadel-
phia, PA.
Prodromos Malakasiotis and Ion Androutsopoulos. 2007.
Learning textual entailment using SVMs and string similarity
measures. In Proc. of the ACL-PASCALWorkshop on Textual
Entailment and Paraphrasing, Stroudsburg, PA.
Prodromos Malakasiotis. 2009. Paraphrase recognition using
machine learning to combine similarity measures. In Proc. of
the ACL-IJCNLP 2009 Student Research Workshop, Strouds-
burg, PA.
Alvaro E. Monge and Charles Elkan. 1996. The field matching
problem: Algorithms and applications. In Proc. KDD-96,
Portland, OR.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa
Bentivogli, and Danilo Giampiccolo. 2012. 2012. semeval-
2012 task 8: Cross-lingual textual entailment for content syn-
chronization. In In Proc. of the 6th International Workshop
on Semantic Evaluation (SemEval 2012), Montreal, Canada.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. WordNet::Similarity: measuring the relatedness of
concepts. In Proc. HLT-NAACL?Demonstration Papers,
Stroudsburg, PA.
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 3(14):130?137.
Eric S. Ristad and Peter N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(5):522?532.
Gerard Salton and Christopher Buckley. 1988. Term-weighting
approaches in automatic text retrieval. Information Process-
ing & Management, 24(5):513?523.
Gerard Salton, Andrew K. C. Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing. Com-
mun. ACM, 18(11):613?620.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessan-
dro Moschitti. 2009. A machine learning approach to tex-
tual entailment recognition. Natural Language Engineering,
15(Special Issue 04):551?582.
688
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 194?201, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
SOFTCARDINALITY-CORE: Improving Text Overlap with
Distributional Measures for Semantic Textual Similarity
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, esq. Av. Mendiz?bal,
Col. Nueva Industrial Vallejo,
CP 07738, DF, M?xico
www.gelbukh.com
Abstract
Soft cardinality has been shown to be a very
strong text-overlapping baseline for the task of
measuring semantic textual similarity (STS),
obtaining 3rd place in SemEval-2012. At
*SEM-2013 shared task, beside the plain text-
overlapping approach, we tested within soft
cardinality two distributional word-similarity
functions derived from the ukWack corpus.
Unfortunately, we combined these measures
with other features using regression, obtain-
ing positions 18th, 22nd and 23rd among the
90 participants systems in the official rank-
ing. Already after the release of the gold stan-
dard annotations of the test data, we observed
that using only the similarity measures with-
out combining them with other features would
have obtained positions 6th, 7th and 8th; more-
over, an arithmetic average of these similarity
measures would have been 4th(mean=0.5747).
This paper describes both the 3 systems as
they were submitted and the similarity mea-
sures that would obtained those better results.
1 Introduction
The task of textual semantic similarity (STS) con-
sists in providing a similarity function on pairs of
texts that correlates with human judgments. Such
a function has many practical applications in NLP
tasks (e.g. summarization, question answering, tex-
tual entailment, paraphrasing, machine translation
evaluation, among others), which makes this task
particularly important. Numerous efforts have been
devoted to this task (Lee et al, 2005; Mihalcea et al,
2006) and major evaluation campaigns have been
held at SemEval-2012 (Agirre et al, 2012) and in
*SEM-2013 (Agirre et al, 2013).
The experimental setup of STS in 2012 consisted
of three data sets, roughly divided in 50% for train-
ing and for testing, which contained text pairs manu-
ally annotated as a gold standard. Furthermore, two
data sets were provided for surprise testing. The
measure of performance was the average of the cor-
relations per data set weighted by the number of
pairs in each data set (mean). The best performing
systems were UKP (B?r et al, 2012) mean=0.6773,
TakeLab (?aric et al, 2012) mean=0.6753 and soft
cardinality (Jimenez et al, 2012) mean=0.6708.
UKP and TakeLab systems used a large number of
resources (see (Agirre et al, 2012)) such as dictio-
naries, a distributional thesaurus, monolingual cor-
pora, Wikipedia, WordNet, distributional similar-
ity measures, KB similarity, POS tagger, machine
learning and others. Unlike those systems, the soft
cardinality approach used mainly text overlapping
and conventional text preprocessing such as remov-
ing of stop words, stemming and idf term weighting.
This shows that the additional gain in performance
from using external resources is small and that the
soft cardinality approach is a very challenging base-
line for the STS task. Soft cardinality has been
previously shown (Jimenez and Gelbukh, 2012) to
be also a good baseline for other applications such
as information retrieval, entity matching, paraphrase
detection and recognizing textual entailment.
Soft cardinality approach to constructing similar-
ity functions (Jimenez et al, 2010) consists in using
any cardinality-based resemblance coefficient (such
as Jaccard or Dice) but substituting the classical set
194
cardinality with a softened counting function called
soft cardinality. For example, the soft cardinality of
a set containing three very similar elements is close
to (though larger than) 1, while for three very dif-
ferent elements it is close to (though less than) 3.
To use the soft cardinality with texts, they are repre-
sented as sets of words, and a word-similarity func-
tion is used for the soft counting of the words. For
the sake of completeness, we give a brief overview
of the soft-cardinality method in Section 3.
The resemblance coefficient used in our participa-
tion is a modified version of Tversky?s ratio model
(Tversky, 1977). Apart from the two parameters of
this coefficient, a new parameter was included and
functions max and min were used to make it sym-
metrical. The rationale for this new coefficient is
given in Section 2.
Three word similarity features used in our sys-
tems are described in Section 4. The one is a mea-
sure of character q-gram overlapping, which reuses
the coefficient proposed in Section 2; this measure is
described in subsection 4.1. The other two ones are
distributional measures obtained from the ukWack
corpus (Baroni et al, 2009), which is a collection of
web-crawled documents containing about 1.9 billion
words in English. The second measure is, again, a
reuse of the coefficient specified in Section 2, but us-
ing instead sets of occurrences (and co-occurrences)
of words in sentences in the ukWack corpus; this
measure is described in subsection 4.2. Finally, the
third one, which is a normalized version of point-
wise mutual information (PMI), is described in sub-
section 4.3.
The parameters of the three text-similarity func-
tions derived from the combination of the proposed
coefficient of resemblance (Section 2), the soft car-
dinality (Section 3) and the three word-similarity
measures (Section 4) were adjusted to maximize the
correlation with the 2012 STS gold standard data.
At this point, these soft-cardinality similarity func-
tions can provide predictions for the test data. How-
ever, we decided to test the approach of learning a
resemblance function from the training data instead
of using a preset resemblance coefficient. Basically,
most resemblance coefficients are ternary functions
F (x, y, z) where x = |A|, y = |B| and z = |A?B|:
e.g. Dice coefficient is F (x, y, z) = 2z/x+y and Jac-
card is F (x, y, z) = z/x+y?z. Thus, this function
can be learned using a regression model, providing
cardinalities x, y and z as features and the gold stan-
dard value as the target function. The results ob-
tained for the text-similarity functions and the re-
gression approach are presented in Section 7.
Unfortunately, when using a regressor trained
with 2012 STS data and tested with 2013 surprise
data we observed that the results worsened rather
than improved. A short explanation of this is over-
fitting. A more detailed discussion of this, together
with an assessment of the performance gain obtained
by the use of distributional measures is provided in
Section 8.
Finally, in Section 9 the conclusions of our partic-
ipation in this evaluation campaign are presented.
2 Symmetrical Tversky?s Ratio Model
In the field of mathematical psychology Tversky
proposed the ratio model (TRM) (Tversky, 1977)
motivated by the imbalance that humans have on
the selection of the referent to compare things. This
model is a parameterized resemblance coefficient to
compare two sets A and B given by the following
expression:
trm(A,B) =
|A ?B|
?|A \B|+ ?|B \A|+ |A ?B|
,
Having ?, ? ? 0. The numerator represents the
commonality between A and B, and the denomina-
tor represents the referent for comparison. Parame-
ters ? and ? represent the preference in the selection
of A or B as referent. Tversky associated the set
cardinality, to the stimuli of the objects being com-
pared. Let us consider a Tversky?s example of the
70s: A is North Corea, B is red China and stimuli
is the prominence of the country. When subjects as-
sessed the similarity between A and B, they tended
to select the country with less prominence as ref-
erent. Tversky observed that ? was larger than ?
when subjects compared countries, symbols, texts
and sounds. Our motivation is to use this model by
adjusting the parameters ? and ? for better modeling
human similarity judgments for short texts.
However, this is not a symmetric model and the
parameters ? and ?, have the dual interpretation of
modeling the asymmetry in the referent selection,
while controlling the balance between |A ? B| and
195
|A?B|+ |B ?A| as well. The following reformu-
lation, called symmetric TRM (strm), is intended to
address these issues:
strm(A,B) =
c
? (?a+ (1? ?) b) + c
, (1)
a = min(|A ? B|, |B ? A|), b = max(|A ?
B|, |B ? A|) and c = |A ? B| + bias. In strm, ?
models only the balance between the differences in
the cardinalities of A and B, and ? models the bal-
ance between |A?B| and |A?B|+|B?A|. Further-
more, the use of functions min and max makes the
measure to be symmetric. Although the motivation
for the bias parameter is empirical, we believe that
this reduces the effect of the common features that
are frequent and therefore less informative, e.g. stop
words. Note that for ? = 0.5,? = 1 and bias = 0,
strm is equivalent to Dice?s coefficient. Similarity,
for ? = 0.5,? = 2 and bias = 0, strm is equivalent
to the Jaccard?s coefficient.
3 Soft Cardinality
The cardinality of a set is its number of elements. By
definition, the sets do not allow repeated elements,
so if a collection of elements contains repetitions its
cardinality is the number of different elements. The
classical set cardinality does not take into account
similar elements, i.e. only the identical elements
in a collection counted once. The soft cardinality
(Jimenez et al, 2010) considers not only identical
elements but also similar using an auxiliary similar-
ity function sim, which compares pairs of elements.
This cardinality can be calculated for a collection of
elements A with the following expression:
|A|? =
n?
i=1
wi
?
?
n?
j=1
sim(ai, aj)p
?
?
?1
(2)
A ={a1, a2, . . . , an}; wi ? 0; p ? 0; 1 >
sim(x, y) ? 0, x 6= y; and sim(x, x) = 1. The
parameter p controls the degree of "softness" of
the cardinality. This formulation has the property
of reproducing classical cardinality when p is large
and/or when sim is a rigid function that returns 1
only for identical elements and 0 otherwise. The co-
efficients wi are the weights associated with each el-
ement. In text applications elements ai are words
and weights wi represent the importance or infor-
mative character of each word (e.g. idf weights).
The apostrophe is used to differentiate soft cardinal-
ity from the classic set cardinality.
4 Word Similarity
Analogous to the STS, the word similarity is the task
of measuring the relationship of a couple of words
in a way correlated with human judgments. Since
when Rubenstein and Goodenough (1965) provided
the first data set, this task has been addressed pri-
marily through semantic networks (Resnik, 1999;
Pedersen et al, 2004) and distributional measures
(Agirre et al, 2009). However, other simpler ap-
proaches such as edit-distance (Levenshtein, 1966)
and stemming (Porter, 1980) can also be used. For
instance, the former identifies the similarity between
"song" and "sing", and later that between "sing" and
"singing". This section presents three approaches
for word similarity that can be plugged into the soft
cardinality expression in eq. 2.
4.1 Q-grams similarity
Q-grams are the collection of consecutive-
overlapped sub-strings of length q obtained
from the character string in a word. For instance,
the 2-grams (bi-grams) and 3-grams (trigrams) rep-
resentation of the word ?sing? are {?#s?, ?si?, ?in?,
?ng?, ?g#?} and {?#si?, ?sin?, ?ing?, ?ng#?} respec-
tively. The character ?#? is a padding character that
distinguishes q-grams at the beginning and ending
of a word. If the number of characters in a word is
greater or equal than q its representation in q-grams
is the word itself (e.g. the 6-grams in ?sing? are
{?sing?}). Moreover, the 1-grams (unigrams) and
0-grams representations of ?sing? are {?s?, ?i?, ?n?,
?g?} and {?sing?}. A word can also be represented
by combining multiple representations of q-grams.
For instance, the combined representation of ?sing?
using 0-grams, unigrams, and bi-grams is {?sing?,
?s?, ?i?, ?n?, ?g?, ?#s?, ?si?, ?in?, ?ng?, ?g#?}, denoted
by [0:2]-grams. In practice a range [q1 : q2] of
q-grams can be used having 0 ? q1 < q2.
The proposed word-similarity function (named
qgrams) first represents a pair of words using
[q1 : q2]-grams and then compares them reusing
the strm coefficient (eq.1). The parameters of the
196
qgrams function are q1, q2, ?qgrams, ?qgrams, and
biasqgrams. These parameters are sub-scripted to
distinguish them from their counterparts at the text-
similarity functions.
4.2 Context-Set Distributional Similarity
The hypothesis of this measure is that the co-
occurrence of two words in a sentence is a hint of
the possible relationship between them. Let us de-
fine sf(t) as the sentence frequency of a word t in
a corpus. The sentence frequency is equivalent to
the well known document frequency but uses sen-
tences instead of documents. Similarly sf(tA ? tB)
is the number of sentences where words tA and tB
co-occur. The idea is to compute a similarity func-
tion between tA and tB representing them as A and
B, which are sets of the sentences where tA and tB
occur. Similarly, A?B is the set of sentences where
both words co-occur. The required cardinalities can
be obtained from the sentence frequencies by: |A| =
sf(tA); |B| = sf(tB) and |A ? B| = sf(tA ? tB).
These cardinalities are combined reusing again the
strm coefficient (eq. 1) to obtain a word-similarity
function. The parameters of this function, which we
refer to it as csds, are ?csds, ?csds and biascsds.
4.3 Normalized Point-wise Mutual Information
The pointwise mutual information (PMI) is a mea-
sure of relationship between two random variables.
PMI is calculated by the following expression:
pmi(tA, tB) = log2
(
P (tA ? tB)
P (tA) ? P (tB)
)
PMI has been used to measure the relatedness of
pairs of words using the number of the hits returned
by a search engine (Turney, 2001; Bollegala et al,
2007). However, PMI cannot be used directly as
sim function in eq.2. The alternative is to normal-
ize it dividing it by log2(P (tA ? tB)) obtaining a
value in the [1,?1] interval. This measure returns
1 for complete co-occurrence, 0 for independence
and -1 for ?never? co-occurring. Given that the re-
sults in the interval (0,-1] are not relevant, the final
normalized-trimmed expression is:
npmi(tA, tB) = max
[
pmi(tA, tB)
log2(P (tA ? tB))
, 0
]
(3)
The probabilities required by PMI can be obtained
by MLE using sentence frequencies in a large cor-
pus: P (tA) ?
sf(tA)
S , P (tB) ?
sf(tB)
S ,and P (tA ?
tB) ?
sf(tA?tB)
S . Where S is the total number of
sentences in the corpus.
5 Text-similarity Functions
The ?building blocks? proposed in sections 2,
3 and 4, are assembled to build three text-
similarity functions, namely STSqgrams, STScsds
and STSnpmi. The first component is the strm re-
semblance coefficient (eq. 1), which takes as argu-
ments a pair of texts represented as bags of words
with importance weights associated with each word.
In the following subsection 5.1 a detailed descrip-
tion of the procedure for obtaining such weighted
bag-of-words is provided.
The strm coefficient is enhanced by replac-
ing the classical cardinality by the soft cardinality,
which exploits two resources: importance weights
associated with each word (weights wi) and pair-
wise comparisons among words (sim). Unlike
STSqgrams measure, STScsds and STSnpmi mea-
sures require statistics from a large corpus. A brief
description of the used corpus and the method for
obtaining such statistics is described in subsection
5.2. Finally, the three proposed text-similarity func-
tions contain free parameters that need to be ad-
justed. The method used to get those parameters is
described in subsection 5.3.
5.1 Preprocessing and Term Weighting
All training and test texts were preprocessed with
the following sequence of actions: i) text strings
were tokenized, ii) uppercase characters are con-
verted into lower-cased equivalents, iii) stop-words
were removed, iv) punctuation marks were removed,
and v) words were stemmed using Porter?s algorithm
(1980). Then each stemmed word was weighted
with idf (Jones, 2004) calculated using the entire
collection of texts.
5.2 Sentence Frequencies from Corpus
The sentence frequencies sf(t) and sf(tA ? tB) re-
quired by csds and npmi word-similarity func-
tions were obtained from the ukWack corpus (Ba-
roni et al, 2009). This corpus has roughly 1.9 bil-
197
lion words, 87.8 millions of sentences and 2.7 mil-
lions of documents. The corpus was iterated sen-
tence by sentence with the same preprocessing that
was described in the previous section, looking for
all occurrences of words and word pairs from the
full training and test texts. The target words were
stored in a trie, making the entire corpus iteration
took about 90 minutes in a laptop with 4GB and a
1.3Ghz processor.
5.3 Parameter optimization
The three proposed text-similarity functions have
several parameters: p exponent in the soft car-
dinality; ?, ?, and bias in strm coefficient;
their sub-scripted versions in qgrams and csds
word-similarity functions; and finally q1and q2 for
qgrams function. Parameter sets for each of the
three text-similarity functions were optimized us-
ing the full STS-SemEval-2012 data. The function
to maximize was the correlation between similar-
ity scores against the gold standard in the training
data. The set of parameters for each similarity func-
tion were optimized using a greedy hill-climbing ap-
proach by using steps of 0.01 for all parameters ex-
cept q1 and q2 that used 1 as step. The initial values
were p = 1, ? = 0.5, ? = 1, bias = 0, q1 = 2 and
q2 = 3. All parameters were optimized until im-
provement in the function to maximize was below
0.0001. The obtained values are :
STSqgrams p = 1.32,? = 0.52, ? = 0.64, bias =
?0.45, q1 = 0, q2 = 2, ?qgrams = 0.95,
?qgrams = 1.44, biasqgrams = ?0.44.
STScsds p = 0.5, ? = 0.63, ? = 0.69, bias =
?2.05, ?csds = 1.34, ?csds = 2.57, biascsds =
?1.22 .
STSnpmi p = 6.17,? = 0.83, ? = 0.64, bias =
?2.11.
6 Regression for STS
The use of regression is motivated by the follow-
ing experiment. First, a synthetic data set with
1,000 instances was generated with the following
three features: |A| = RandomBetween(1, 100),
|B| = RandomBetween(1, 100) and |A ? B| =
RandomBetween(0,min[|A|, |B|]). Secondly, a
#1 STSsim #11 |A?B|
?/|A|?
#2 |A|? #12 |A?B|?/|B|?
#3 |B|? #13 |A|? ? |B|?
#4 |A ?B|? #14 |A?B|?/|A?B|?
#5 |A ?B|? #15 2?|A?B|?/|A|?+|B|?
#6 |A \B|? #16 |A?B|/min[|A|,|B|]
#7 |B \A|? #17 |A?B|?/max[|A|?,|B|?]
#8 |A ?B ?A ?B|? #18 |A?B|?/
?
|A|??|B|?
#9 |A?B|?/|A|? #19 |A?B|
?+|A|?+|B|?
2?|A|??|B|?
#10 |B?A|?/|B|? #20 gold standard
Table 1: Feature set for regression
linear regressor was trained using the Dice?s coef-
ficient (i.e. 2|A ? B|/|A| + |B|) as target function.
The Pearson correlation obtained using 4-fold cross-
validation as method of evaluation was r = 0.93.
Besides, a Reduced Error Pruning (REP) tree (Wit-
ten and Frank, 2005) boosted with 30 iterations of
Bagging (Breiman, 1996) was used instead of the
linear regressor obtaining r = 0.99. We concluded
that a particular resemblance coefficient can be ac-
curately approximated using a nonlinear regression
algorithm and training data.
This approach can be used for replacing the strm
coefficient by a similarity function learned from STS
training data. The three features used in the previ-
ous experiment were extended to a total of 19 (see
table 1) plus the gold standard as target. The feature
#1 is the score of the corresponding text-similarity
function described in the previous section. Three
sets of features were constructed, each with 19 fea-
tures using the soft cardinality in combination with
the word-similarity functions qgrams, csds and
npmi. Let us name these feature sets as fs:qgrams,
fs:csds and fs:npmi. The submission labeled run1
was obtained using the feature set fs:qgrams (19 fea-
tures). The submission labeled run2 was obtained
using the aggregation of fs:qgrams and fs:csds (19?
2 = 38 features). Finally, run3 was the aggregation
of fs:grams, fs:csds and fs:npmi (19 ? 3 = 57 fea-
tures).
7 Results in *SEM 2013 Shared Task
In this section three groups of systems are described
by using the functions and models proposed in the
previous sections. The first group (and simplest)
198
Data set STSqgrams STScsds STSnpmi average
headlines 0.7625 0.7243 0.7379 0.7562
OnWN 0.7022 0.7050 0.6832 0.7063
FNWM 0.2704 0.3713 0.4215 0.3940
SMT 0.3151 0.3325 0.3408 0.3402
mean 0.5570 0.5592 0.5653 0.5747
rank 8 7 6 4
Table 2: Unofficial results using text-similarity functions
Data set run1 run2 run3
headlines 0.7591 0.7632 0.7640
OnWN 0.7159 0.7239 0.7485
FNWM 0.2806 0.3679 0.3487
SMT 0.2820 0.2786 0.2952
mean 0.5491 0.5586 0.5690
rank 14 8 4
Table 3: Unofficial results using linear regression
of systems consist in using the scores of the three
text-similarity functions STSqgrams, STScsds and
STSnpmi. Table 2 shows the unofficial results of
these three systems. The bottom row shows the posi-
tions that these systems would have obtained if they
had been submitted to the *SEM shared task 2013.
The last column shows the results of a system that
combines the scores of three measures on a single
score calculating the arithmetic mean. This is the
best performing system obtained with the methods
described in this paper.
Tables 3 and 4 show unofficial and official re-
sults of the method described in section 6 using
linear regression and Bagging (30 iterations)+REP
tree respectively. These results were obtained using
WEKA (Hall et al, 2009).
8 Discussion
Contrary to the observation we made in training
data, the methods that used regression to predict the
gold standard performed poorly compared with the
text similarity functions proposed in Section 5. That
is, the results in Table 2 overcome those in Tables 3
and 4. Also in training data, Bagging+REP tree sur-
passed linear regression, but, as can be seen in tables
3 and 4 the opposite happened in test data. This is
a clear symptom of overfitting. However, the OnWN
Data set run1 run2 run3
headlines 0.6410 0.6713 0.6603
OnWN 0.7360 0.7412 0.7401
FNWM 0.3442 0.3838 0.3347
SMT 0.3035 0.2981 0.2900
mean 0.5273 0.5402 0.5294
rank 23 18 22
Table 4: Official results of the submitted runs to STS
*SEM 2013 shared task using Bagging + REP tree for
regression
data set was an exception, which obtained the best
results using linear regression. OnWN was the only
one among the 2013 data sets that was not a sur-
prise data set. Probably the 5.97% relative improve-
ment obtained in run3 by the linear regression versus
the best result in Table 2 may be justified owing to
some patterns discovered by the linear regressor in
the OnWN?2012 training data which are projected
on the OnWN?2013 test data.
It is worth noting that in all three sets of results,
the lowest mean was consistently obtained by the
text-overlapping methods, namely STSqgrams and
run1. The relative improvement in mean due to
the use of distributional measures against the text-
overlapping methods was 3.18%, 3.62% and 2.45%
in each set of results (see Tables 2, 3 and 4). In
FNWM data set, the biggest improvements achieved
55.88%, 31.11% and 11.50% respectively in the
three groups of results, followed by SMT data set.
Both in FNWN data set as in SMT, the texts are sys-
tematically longer than those found in OnWN and
headlines. This result suggests that the improvement
due to distributional measures is more significant in
longer texts than in the shorter ones.
Lastly, it is also important to notice that
the STSqgrams text-similarity function obtained
mean = 0.5570, which proved again to be a very
strong text-overlapping baseline for the STS task.
9 Conclusions
We participated in the CORE-STS shared task in
*SEM 2013 with satisfactory results obtaining po-
sitions 18th, 22nd, and 23rd in the official ranking.
Our systems were based on a new parameterized
resemblance coefficient derived from the Tversky?s
199
ratio model in combination with the soft cardinal-
ity. The three proposed text-similarity functions
used q-grams overlapping and distributional mea-
sures obtained from the ukWack corpus. These text-
similarity functions would have been attained posi-
tions 6th, 7th and 8th in the official ranking, besides
a simple average of them would have reached the
4thplace. Another important conclusion was that the
plain text-overlapping method was consistently im-
proved by the incremental use of the proposed distri-
butional measures. This result was most noticeable
in long texts.
In conclusion, the proposed text-similarity func-
tions proved to be competitive despite their simplic-
ity and the few resources used.
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The Section 2 was proposed during the first author?s
internship at Microsoft Research in 2012. The third
author recognizes the support from Mexican Gov-
ernment (SNI, COFAA-IPN, SIP 20131702, CONA-
CYT 50206-H) and CONACYT?DST India (proj.
122030 ?Answer Validation through Textual Entail-
ment?). Entailment?).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, NAACL ?09,
pages 19?27, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre Aitor. 2012. SemEval-2012 task 6: A pilot on
semantic textual similarity. In Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval@*SEM 2012), Montreal,Canada. Association
for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. Atlanta, Georgia, USA. Association
for Computational Linguistics.
Daniel B?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: computing semantic textual
similarity by combining multiple content similarity
measures. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval *SEM
2012), Montreal, Canada. Association for Computa-
tional Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language resources and evaluation,
43(3):209?226.
Danushka Bollegala, Yutaka Matsuto, and Mitsuru
Ishizuka. 2007. Measuring semantic similarity be-
tween words using web search engines. In Proceed-
ings of the 16th international conference on World
Wide Web, WWW ?07, pages 757?766, New York,
NY, USA. ACM.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Sergio Jimenez and Alexander Gelbukh. 2012. Baselines
for natural language processing tasks. Appl. Comput.
Math., 11(2):180?199.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardinality.
In Edgar Chavez and Stefano Lonardi, editors, String
Processing and Information Retrieval, volume 6393 of
LNCS, pages 297?302. Springer, Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality: A parameterized sim-
ilarity function for text comparison. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval *SEM 2012), Montreal, Canada.
Karen Sp?rck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493?502, October.
Michael D Lee, B.M. Pincombe, and Matthew Welsh.
2005. An empirical evaluation of models of text docu-
ment similarity. IN COGSCI2005, pages 1254?1259.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
200
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In In AAAI?06, pages 775?
780.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity: measuring the re-
latedness of concepts. In Proceedings HLT-NAACL?
Demonstration Papers, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
Phillip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95?130.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
Frane ?aric, Goran Glava?, Mladen Karan, Jan ?najder,
and Bojana Dalbelo Ba?ic. 2012. TakeLab: systems
for measuring semantic text similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval *SEM 2012), Montreal, Canada. As-
sociation for Computational Linguistics.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Luc De Raedt and
Peter Flach, editors, Machine Learning: ECML 2001,
number 2167 in Lecture Notes in Computer Science,
pages 491?502. Springer Berlin Heidelberg, January.
Amos Tversky. 1977. Features of similarity. Psycholog-
ical Review, 84(4):327?352, July.
I.H. Witten and E. Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann Publishers Inc., San Francisto, CA, 2nd
edition.
201
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 34?38, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SOFTCARDINALITY: Learning to Identify Directional
Cross-Lingual Entailment from Cardinalities and SMT
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, Av. Mendiz?bal,
Col. Nueva Industrial Vallejo
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
In this paper we describe our system submit-
ted for evaluation in the CLTE-SemEval-2013
task, which achieved the best results in two
of the four data sets, and finished third in av-
erage. This system consists of a SVM clas-
sifier with features extracted from texts (and
their translations SMT) based on a cardinality
function. Such function was the soft cardinal-
ity. Furthermore, this system was simplified
by providing a single model for the 4 pairs
of languages obtaining better (unofficial) re-
sults than separate models for each language
pair. We also evaluated the use of additional
circular-pivoting translations achieving results
6.14% above the best official results.
1 Introduction
The Cross-Lingual Textual Entailment (CLTE) task
consists in determining the type of directional en-
tailment (i.e. forward, backward, bidirectional or
no-entailment) between a pair of texts T1 and T2,
each one written in different languages (Negri et al,
2013). The texts and reference annotations for this
task were obtained through crowdsourcing applied
to simpler sub-tasks (Negri et al, 2011). CLTE has
as main applications content synchronization and
aggregation in different languages (Mehdad et al,
2012; Duh et al, 2013). We participated in the first
evaluation of this task in 2012 (Negri et al, 2012),
achieving third place on average among 29 partici-
pating systems (Jimenez et al, 2012).
Since in the CLTE task text pairs are in different
languages, in our system, all comparisons made be-
tween two texts imply that one of them was written
by a human and the other is a translation provided by
statistical machine translation (SMT). Our approach
is based on an SVM classifier (Cortes and Vapnik,
1995) whose features were cardinalities combined
with similarity scores. That system was motivated
by the fact that most text similarity functions are
symmetric, e.g. Edit Distance (Levenshtein, 1966),
longest common sub-sequence (Hirschberg, 1977),
Jaro-Winkler similarity (Winkler, 1990), cosine sim-
ilarity (Salton et al, 1975). Thus, the use of these
functions as only resource seems counter-intuitive
since CLTE task is asymmetric for the forward and
backward entailment classes.
Moreover, cardinality is the central component of
the resemblance coefficients such as Jaccard, Dice,
overlap, etc. For instance, if T1 and T2 are texts
represented as bag of words, it is only necessary to
know the cardinalities |T1|, |T2| and |T1 ? T2| to ob-
tain a similarity score using a resemblance coeffi-
cient such as the Dice?s coefficient (i.e. 2 ? |T1 ?
T2|/(|T1| + |T2|)). Therefore, the idea is to use the
individual cardinalities to enrich a set of features ex-
tracted from texts.
Cardinality gives a rough idea of the amount of
information in a collection of elements (i.e. words)
providing the number of different elements therein.
That is, in a collection of elements whose majority
are repetitions contains less information than a col-
lection whose elements are mostly different. How-
ever, the classical sets cardinality is a rigid mea-
sure as do not take account the degree of similarity
among the elements. Unlike the sets cardinality, soft
cardinality (Jimenez et al, 2010) uses the similari-
ties among the elements providing a more flexible
34
measurement of the amount of information in a col-
lection. In the 2012 CLTE evaluation campaign, it
was noted that the soft cardinality overcame classi-
cal cardinality in the task at hand. All the models
used in our participation and proposed in this paper
are based on the soft cardinality. A brief descrip-
tion of the soft cardinality is presented in Section 2,
along with a description of the functions used to pro-
vide the similarities between words. Besides, the set
of features that are derived from all pairs of texts and
their cardinalities are presented in Section 3.
Section 4 provides a detailed description for each
of the 4 models (one for each language pair) used
to get the predictions submitted for evaluation. In
Section 5 a simplified-multilingual model is tested
with several word-similarity functions and circular-
pivoting translations.
In sections 6 and 7 a brief discussion of the results
and conclusions of our participation in this evalua-
tion campaign are presented.
2 Soft Cardinality
The soft cardinality (Jimenez et al, 2010) of a col-
lection of words T is calculated with the following
expression:
|T |? =
n?
i=1
wi
?
?
n?
j=1
sim(ti, tj)p
?
?
?1
(1)
Having T ={t1, t2, . . . , tn}; wi ? 0; p ? 0; 1 >
sim(x, y) ? 0, x 6= y; and sim(x, x) = 1. The
parameter p controls the degree of "softness" of the
cardinality (the larger the ?harder?). The coefficients
wi are weights associated with each word (or term)
t, which can represent the importance or informative
character of each word (e.g. idf weights). The func-
tion sim is a word-similarity function. Three such
functions are considered in this paper:
Q-grams: each word ai is represented as a col-
lection of character q-grams (Kukich, 1992). In-
stead of single length q-grams, a combination of
a range of lengths q1 to q2 was used. Next,
a couple of words are compared with the fol-
lowing resemblance coefficient: sim(ti, tj) =
|ti?tj |+bias
??max(|ti|,|tj |)+(1??)?min(|ti|,|tj |)
. The parameters of
this word-similarity function are q1, q2, ? and bias.
Group 1: basic cardinalities
#1 |T1|? #4 |T1 ? T2|?
#2 |T2|? #5 |T1 ? T2|?
#3 |T1 ? T2|? #6 |T2 ? T1|?
Group 2: asymmetrical ratios
#7 |T1?T2|
?/|T1|? #8 |T1?T2|
?/|T2|?
Group 3: similarity and arithmetical* scores
#9 |T1?T2|
?/|T1?T2|? #10
2?|T1?T2|
?
|T1|?+|T2|?
#11 |T1?T2|
?/
?
|T1|??|T2|? #12
|T!?T2|
?
min[|T1|?,|T2|?]
#13 |T1?T2|
?+|T1|
?+|T2|
?
2?|T1|??|T2|?
#14* |T1|? ? |T2|?
Table 1: Set of features derived from texts T1 and T2
Edit-Distance: a similarity score for a pair of
words can be obtained from their Edit Distance
(Levenshtein, 1966) by normalizing and converting
distance to similarity with the following expression:
sim(ti, tj) = 1?
EditDistance(ti,tj)
max[len(ti),len(tj)]
.
Jaro-Winkler: this measure is based on the Jaro
(1989) similarity, which is given by this expression
Jaro(ti, tj) = 13
(
c
len(ti)
+ clen(tj) +
c?m
c
)
, where c
is the number of characters in common within a slid-
ing window of length max[len(ti),len(tj)]2 ?1. To avoid
division by 0, when c = 0 then Jaro(ti, tj) = 0. The
number of transpositions m is obtained sorting the
common characters according to their occurrence
in each of the words and counting the number of
non-matching characters. Winkler (1990) proposed
an extension to this measure taking into account
the common prefix length l through this expression:
sim(ti, tj) = Jaro(ti, tj) + l10 (1? Jaro(ti, tj)).
3 Features from Cardinalities
For a pair of texts T1 and T2 represented as bags
of words three basic soft cardinalities can be cal-
culated: |T1|?, |T2|? and |T1 ? T2|?. The soft car-
dinality of their union is calculated using the con-
catenation of T1 and T2. More additional features
can be derived from these three basic features, e.g.
|T1?T2|? = |T1|?+|T2|??|T1?T2|? and |T1?T2|? =
|T1|?? |T1 ? T2|?. The complete set of features clas-
sified into three groups are shown in Table 1.
4 Submitted Runs Description
The data for the 2013 CLTE task consists of 4 data
sets (spa-eng, ita-eng, fra-eng and deu-eng) each
35
Data set q1 q2 ? bias
deu-eng 2 2 0.5 0.0
fra-eng 2 3 0.5 0.0
ita-eng 2 4 0.6 0.0
spa-eng 1 3 0.5 0.1
Table 2: Parameters of the q-grams word-similarity func-
tion for each language pair
with 1,000 pairs of texts for training and 500 for
testing. For each pair of texts T1 and T2 written
in two different languages, two translations are pro-
vided using the Google?s translator1. Thus, T t1 is a
translation of T1 into the language of T2 and T t2 is
a translation of T2 into the language of T1. Using
these pivoting translations, two pairs of texts can be
compared: T1 with T t2 and T
t
1 with T2.
Then all training and testing texts and their trans-
lations were pre-processed with the following se-
quence of actions: i) text strings were tokenized,
ii) uppercase characters are converted into lower-
case equivalents, iii) stop words were removed, iv)
punctuation marks were removed, and v) words were
stemmed using the Snowball2 multilingual stem-
mers provided by the NLTK Toolkit (Loper and
Bird, 2002). Then every stemmed word is tagged
with its idf weight (Jones, 2004) calculated with the
complete collection of texts and translations in the
same language.
Five instances of the soft cardinality are provided
using 1, 2, 3, 4 and 5 as values of the parameter
p. Therefore, the total number of features for each
pair of texts is the multiplication of the number of
features in the feature set (i.e. 14, see Table 1) by
the number of soft cardinality functions (5) and by 2,
corresponding to the two pairs of comparable texts.
That is, 14? 5? 2 = 140 features.
The sim function used was q-grams, whose pa-
rameters were adjusted for each language pair.
These parameters, which are shown in Table 2, were
obtained by manual exploration using the training
data.
Four vector data sets for training (one for each
language pair) were built by extracting the 140 fea-
tures from the 1,000 training instances and using
1https://translate.google.com
2http://snowball.tartarus.org
ECNUCS-team?s system
spa-eng ita-eng fra-eng deu-eng average
run4 0.422 0.416 0.436 0.452 0.432
run3 0.408 0.426 0.458 0.432 0.431
SOFTCARDINALITY-team?s system
spa-eng ita-eng fra-eng deu-eng average
run1 0.434 0.454 0.416 0.414 0.430
run2 0.432 0.448 0.426 0.402 0.427
Table 3: Official results for our system and the top per-
forming system ECNUCS (accuracies)
their gold-standard annotations as class attribute.
Predictions for the 500 test cases were obtained
through a SVM classifier trained with each data set.
For the submitted run1, this SVM classifier used a
linear kernel with its complexity parameter set to its
default value C = 1. For the run2, this parameter
was adjusted for each pair of languages with the fol-
lowing values: Cspa?eng = 2.0, Cita?eng = 1.5,
Cfra?eng = 2.3 and Cdeu?eng = 2.0. The imple-
mentation of the SVM used is that which is available
in WEKA v.3.6.9 (SMO) (Hall et al, 2009). Official
results for run1, run2 and best accuracies obtained
among all participant systems are shown in Table 3.
5 A Single Multilingual Model
This section presents the results of our additional ex-
periments in search for a simplified model and in
turn to respond to the following questions: i) Can
one simplified-multilingual model overcome the ap-
proach presented in Section 4? ii) Does using addi-
tional circular-pivoting translations improve perfor-
mance? and iii) Do other word-similarity functions
work better than the q-grams measure?
First, it is important to note that the approach
described in Section 4 used only patterns discov-
ered in cardinalities. This means, that no language-
dependent features was used, with the exception of
the stemmers. Therefore, we wonder whether the
patterns discovered in a pair of languages can be use-
ful in other language pairs. To answer this question,
a single prediction model was built by aggregating
instances from each of the vector data sets into one
data set with 4,000 training instances. Afterward,
this model was used to provide predictions for the
2,000 test cases.
36
Moreover, customization for each pair of lan-
guages in the word-similarity function, which is
show in Table 2, was set on the following unique set
of parameters: q1 = 1, q2 = 3, ? = 0.5, bias = 0.0.
Thus, the words are compared using q-grams and
the Dice coefficient. In addition to the measure of
q-grams, two "off-the-shelf" measures were used as
nonparametric alternatives, namely: Edit Distance
(Levenshtein, 1966) and the Jaro-Winkler similarity
(Winkler, 1990).
In another attempt to simplify this model, we
evaluated the predictive ability of each of the three
groups of features shown in Table 1. The combi-
nation of groups 2 and 3, consistently obtained bet-
ter results when the evaluation with 10 fold cross-
validation was used in the training data. This result
was consistent with the simple training versus test
data evaluation. The sum of all previous simplifica-
tions significantly reduced the number of parameters
and features in comparison with the model described
in Section 4. That is, only one SVM and 4 parame-
ters, namely: ?, bias, q1 and q2.
Besides, the additional use of circular-pivoting
translations was tested. In the original model, for
every pair of texts (T1, T2) their pivot translations
(T t1 , T
t
2) were provided allowing the calculation of
|T1 ? T t2| and |T
t
1 ? T2|. Translations T
t
1 and T
t
2 can
also be translated back to their original languages
obtaining T tt1 and T
tt
2 . These additional transla-
tions in turn allows the calculation of |T tt1 ? T
t
2|
and |T t1 ? T
tt
2 |. This procedure can be repeated
again to obtain T ttt1 and T
ttt
2 , which in turn provides
|T1 ? T ttt2 |, |T
ttt
1 ? T2|, |T
tt
1 ? T
ttt
2 | and |T
ttt
1 ? T
tt
2 |.
The original feature set is denoted as t. The extended
feature sets using double-pivoting translations and
triple-pivot translations are denoted respectively as
tt and ttt.
The results obtained with this simplified model
using single, double and triple pivot translations are
shown in Table 4. The first column indicates the
word-similarity function used by the soft cardinal-
ity and the second column indicates the number of
pivoting translations.
6 Discussion
In spite of the customization of the parameter C in
the run2, the run1 obtained better results than run2
Soft C. #t spa-e ita-e fra-e deu-e avg.
Ed.Dist. t 0.444 0.450 0.440 0.410 0.436
Ed.Dist. tt 0.452 0.464 0.434 0.432 0.446
Ed.Dist. ttt 0.464 0.468 0.440 0.424 0.449
Jaro-W. t 0.422 0.450 0.426 0.406 0.426
Jaro-W. tt 0.430 0.456 0.444 0.400 0.433
Jaro-W. ttt 0.426 0.458 0.430 0.430 0.436
q-grams t 0.428 0.456 0.456 0.432 0.443
q-grams tt 0.436 0.478 0.444 0.430 0.447
q-grams ttt 0.452 0.474 0.464 0.442 0.458
Table 4: Single-multilingual model results (accuracies)
(see Table 3). This result indicates that the simpler
model produced better predictions in unseen data.
It is also important to note that two of the three
multilingual systems proposed in Section 5 achieved
higher scores than the best official results (see rows
containing ?t? in Table 4). This indicates that the
proposed simplified model is able to discover pat-
terns in the cardinalities of a pair of languages and
project them into the other language pairs.
Regarding the use of additional circular-pivoting
translations, Table 4 shows that t was overcome on
average by tt and tt by ttt in all cases of the three
sets of results. The relative improvement obtained
by comparing t versus ttt for each group was 3.0% in
Edit Distance, 2.3% for Jaro-Winkler and 3.4% for
the q-gram measure. This same trend holds roughly
for each language pair.
7 Conclusions
We described the SOFTCARDINALITY system
that participated in the SemEval CLTE evaluation
campaign in 2013, obtaining the best results in data
sets spa-eng and ita-eng, and achieving the third
place on average. This result was obtained using
separate models for each language pair. It was also
concluded that a single-multilingual model outper-
forms that approach. Besides, we found that the
use of additional pivoting translations provide bet-
ter results. Finally, the measure based on q-grams of
characters, used within the soft cardinality, resulted
to be the best option among other measures of word
similarity. In conclusion, the soft cardinality method
used in combination with SMT and SVM classifiers
is a competitive method for the CLTE task.
37
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT?DST India
(proj. 122030 ?Answer Validation through Textual
Entailment?).
References
Corinna Cortes and Vladimir N. Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Kevin Duh, Ching-Man Au Yeung, Tomoharu Iwata, and
Masaaki Nagata. 2013. Managing information dispar-
ity in multilingual document collections. ACM Trans.
Speech Lang. Process., 10(1):1:1?1:28, March.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Daniel S. Hirschberg. 1977. Algorithms for the longest
common subsequence problem. J. ACM, 24(4):664?
675, October.
M.A. Jaro. 1989. Advances in record-linkage methodol-
ogy as applied to matching the 1985 census of tampa,
florida. Journal of the American Statistical Associa-
tion, pages 414?420, June.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardinality.
In Edgar Chavez and Stefano Lonardi, editors, String
Processing and Information Retrieval, volume 6393 of
LNCS, pages 297?302. Springer, Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality+ ML: learning adaptive
similarity functions for cross-lingual textual entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval, *SEM 2012),
Montreal, Canada. ACL.
Karen Sp?rck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493?502, October.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys,
24:377?439, December.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia. Association for Computa-
tional Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting semantic equivalence and informa-
tion disparity in cross-lingual documents. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume 2,
ACL ?12, page 120?124, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, page 670?679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
2012. semeval-2012 task 8: Cross-lingual textual en-
tailment for content synchronization. In Proceedings
of the 6th International Workshop on Semantic Evalu-
ation (SemEval 2012), Montreal, Canada.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
and Luisa Bentivogli. 2013. Semeval-2013 task
8: Cross-lingual textual entailment for content syn-
chronization. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Gerard Salton, Andrew K. C. Wong, and Chung-Shu
Yang. 1975. A vector space model for automatic in-
dexing. Commun. ACM, 18(11):613?620.
William E. Winkler. 1990. String comparator metrics
and enhanced decision rules in the fellegi-sunter model
of record linkage. In Proceedings of the Section on
Survey Research Methods, pages 354?359. American
Statistical Association.
38
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 114?117, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UNAL: Discriminating between Literal and Figurative
Phrasal Usage Using Distributional Statistics and POS tags
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, Av. Mendiz?bal,
Col. Nueva Industrial Vallejo
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
In this paper we describe the system used to
participate in the sub task 5b in the Phrasal Se-
mantics challenge (task 5) in SemEval 2013.
This sub task consists in discriminating lit-
eral and figurative usage of phrases with
compositional and non-compositional mean-
ings in context. The proposed approach is
based on part-of-speech tags, stylistic features
and distributional statistics gathered from the
same development-training-test text collec-
tion. The system obtained a relative improve-
ment in accuracy against the most-frequent-
class baseline of 49.8% in the ?unseen con-
texts? (LexSample) setting and 8.5% in ?un-
seen phrases? (AllWords).
1 Introduction
The Phrasal Semantics task-5b in SemEval 2013
consisted in the discrimination of literal of figura-
tive usage of phrases in context (Korkontzelos et al,
2013). For instance, the occurrence in a text of the
phrase ?a piece of cake? can be used whether to re-
fer to something that is pretty easy or to an actual
piece of cake. The motivation for this task is that
such discrimination could improve the quality and
performance of other tasks like machine translation
and information retrieval.
This problem has been studied in the past. Lin
(1999) observed that the distributional characteris-
tics of the literal and figurative usage are different.
Katz and Giesbrecht (2006) showed that the similar-
ities among contexts are correlated with their literal
or figurative usage. Birke and Sarkar (2006) clus-
tered literal and figurative contexts using a word-
sense-disambiguation approach. Fazly et al (2009)
showed that literal and figurative usages are related
to particular syntactical forms. Sporleder and Li
(2009) showed that for a particular phrase the con-
texts of its literal usages are more cohesive than
those of its figurative usages. Inspired by these
works and in a new observation, we proposed a set or
features based on cohesiveness, syntax and stylom-
etry (Section 2), which are used to train a machine
learning classifier.
The cohesiveness between a phrase an its context
can be measured aggregating the relatedness of the
context words against the target phrase. This cohe-
siveness should be high for phrases used literally.
Conversely, figurative usages can occur in a large
variety of contexts implying low cohesiveness. For
instance, the cohesiveness of the phrase ?a piece of
cake? against context words such as ?coffee?, ?birth-
day? and ?bakery? should be high. The distribu-
tional measures used to obtain the needed related-
ness scores and the proposed measures of cohesive-
ness are presented in subsection 2.1.
Moreover, we observed a stylistic trend in the
training data set. That is, figurative usage tends to
occur later in the document in comparison with the
literal usage. Consequently, a small set of features
that exploits this particular observation is proposed
in subsection 2.2.
Fazly et al (2009) showed that idiomatic phrases
composed of a verb and a noun (e.g. ?break a leg?)
differ from their literal usages in the use of some
syntactic structures. For instance, idiomatic phrases
are less flexible in the use of determiners, pluraliza-
114
tion and passivization. In order to capture that no-
tion in a simple way, a set of features form a part-
of-speech tagger was included in the feature set (see
subsection 2.3).
In Section, additional details of the proposed sys-
tem are provided jointly with the obtained official
results. Finally, in sections 4 and 5 a brief discus-
sion of the results and some concluding remarks are
presented.
2 Features
Each instance of the training and test sets consist of a
short document d where one or more occurrences of
its target phase pd are annotated. For each particular
phrase p, several instances are provided correspond-
ing to literal or figurative usages. In this section, the
set of features that was extracted from each instance
to provide a vectorial representation is presented.
2.1 Cohesiveness Features
Let?s start with some definitions borrowed from the
information retrieval field: D is a collection of doc-
uments, df(w) is the number of documents in D
where the word w occurs (document frequency),
df(w ? pd) is the number of documents where w
and a target phrase pd co-occur, tf(w, d) is the num-
ber of occurrences of w in a document d ? D (term
frequency), and idf(w) = log2
df(w)
|D| is the inverse
document frequency of w (Jones, 2004).
A simple distributional measure of relatedness be-
tween w and p can be obtained with the following
ratio:
R(w, p) =
df(w ? pd)
df(w)
(1)
Pointwise mutual information (PMI) (Church and
Hanks, 1990) is another distributional measure that
can be used for measuring the relatedness of w and
p. The probabilities needed for its calculation can be
obtained by maximum likelihood estimation (MLE):
P (w) ? df(w)|D| , P (pd) ?
df(pd)
|D| and P (w ? pd) ?
df(w?pd)
|D| .
Thus, PMI is given by this expression:
PMI(w, pd) = log2
(
P (w ? pd)
P (w) ? P (pd)
)
(2)
F1:
?
w?d? R(w, pd)
F2:
?
w?d? tf(w, d)
F3:
?
w?d? idf(w)
F4:
?
w?d? PMI(w, pd)
F5:
?
w?d? NPMI(w, pd)
F6:
?
w?d? (tf(w,d) ? R(w, pd))
F7:
?
w?d? (idf(w) ? R(w, pd))
F8
?
w?d? (R(w, pd) ? PMI(w, pd))
F9:
?
w?d? (R(w, pd) ?NPMI(w, pd))
F10:
?
w?d? (tf(w, d) ? idf(w))
F11:
?
w?d? (tf(w, pd) ? PMI(w, pd))
F12:
?
w?d? (tf(w, pd) ?NPMI(w, pd))
F13:
?
w?d? (idf(w) ? PMI(w, pd))
F14:
?
w?d? (idf(w) ?NPMI(w, pd))
F15:
?
w?d? (PMI(w, pd) ?NPMI(w, pd))
F16:
?
w?d? (tf(w, d) ? idf(w) ? R(w,pd))
F17:
?
w?d? (tf(w, d) ? R(w, pd) ? PMI(w, pd))
F18:
?
w?d? (tf(w, d) ? R(w, pd) ?NPMI(w, pd))
F19:
?
w?d? (tf(w, d) ? idf(w) ? PMI(w,pd))
F20:
?
w?d? (tf(w, d) ? idf(w) ?NPMI(w,pd))
Table 1: Cohesiveness features
Furthermore, the scores obtained through eq. 2
can be normalized in the interval [+2,0] with the fol-
lowing expression:
NPMI(w, pd) =
PMI(w, pd)
? log2(P (w ? pd))
+ 1 (3)
A measure of the cohesiveness between a docu-
ment d against its target phrase pd, can be obtained
by aggregating the pairwise relatedness scores be-
tween all the words in d and pd. For instance, us-
ing eq. 1 that measure is
?
w?d? R(w, pd), where d
?
is the set of different words in d. The equations 1,
2 and 3 can be used as weights associated to each
word, which can also be combined among them and
with tf and idf weights. Such weight combinations
produce measures that can be used as cohesiveness
features for a document. The set of 20 features ob-
tained using this approach is shown in Table 1.
2.2 Stylistic Features
The set of stylistic features related to the document
length, vocabulary size and relative position of the
occurrence of the target phrase in a document is
shown in Table 2.
115
F21: Relative position of pd in d
F22: Document length in characters
F23: Document length in tokens
F24: Number of different words
Table 2: Stylistic features
2.3 Syntactic Features
The features F25 to F67 correspond to the set of 43
part-of-speech tags of the NLTK English POS tag-
ger (Loper and Bird, 2002). Each feature contains
the frequency of occurrence of each POS-tag in a
document d.
3 Experimental Setup and Results
The data provided for this task consists of two data
sets LexSample and AllWords, which are divided
into development, training and test sets. Neverthe-
less, we considered a single training set aggregat-
ing the development and training parts from both
data sets for a total of 3,230 instances. Each train-
ing instance has a class label whether ?literally? or
?figuratively? depending on the usage or the tar-
get phrase. Similarly, the aggregated test set con-
tains 1,112 instances, but with unknown values in
the class attribute.
Firstly, the syntactic features for each text were
obtained using the POS tagger included in the NLTK
v.2.0.4 (Loper and Bird, 2002). Secondly, all texts
were preprocessed by tokenizing, lowecasing, stop-
word removing, punctuation removing and stem-
ming using the Porter?s algorithm (1980). This pre-
processed version of the texts was used to obtain the
remaining cohesiveness and stylistic features. The
resulting vectorial data set was used to produce the
predictions labeled ?UNAL.RUN1? through a Lo-
gistic classifier (Cessie and Houwelingen, 1992).
The implementation used for this classifier was the
included in WEKA v.3.6.9 (Hall et al, 2009). The
accuracies obtained by the different feature groups
in the training set using 10-fold cross validation are
shown in Table 3. The last column shows the per-
centage of relative improvement of different feature
sets combinations from the most frequent class base-
line to our best system using all features.
The predictions labeled ?UNAL.RUN2? were ob-
tained with the same vectorial data set but adding
Features Accuracy % improv.
All features 0.7272 100.0%
Cohesiveness+Syntactic 0.7034 87.1%
Cohesiveness 0.6833 76.2%
Syntactic 0.6229 43.5%
Stylistic 0.5492 3.5%
Baseline MFC 0.5427 0.0%
Table 3: Results by group of features in the training set
using 10-fold cross validation
System LexSample AllWords Both
UNAL.RUN1 0.7222 0.6680 0.6970
UNAL.RUN2 0.7542 0.6448 0.7032
Baseline MFC 0.5034 0.6158 0.5558
Best SemEval?13 0.7795 0.6680 0.7276
# test instances 594 518 1,112
Table 4: Official results in the test set (accuracy)
as a nominal feature the target phrase of each in-
stance. The official results obtained by both sub-
mitted runs are shown in Table 4. Note that official
results in the test set are reported separately for the
data sets LexSample and AllWords. The LexSample
test set contains instances whose target phrases were
seen in the training set (i.e. unseen contexts). Un-
like LexSample, AllWords contains instances whose
target phrases were unseen in the training set (i.e.
unseen phrases).
4 Discussion
As it was expected, the results obtained in the ?un-
seen context? setting were consistently better than
in ?unseen phrases?. This result suggests that the
discrimination of literal and figurative usage heavily
depends on particular idiomatic phrases. This can
also be confirmed by the best accuracy obtained by
RUN2 compared with RUN1 in LexSample. Clearly,
the classifier used in RUN2 exploited the identifica-
tion of the phrase to leverage a priori information
about the phrase such as the most frequent usage.
Another factor that could undermine the results in
the ?unseen phrases? setting is the low number of in-
stances per phrase in the AllWords test set, roughly a
third in comparison with LexSample. Given that the
effectiveness of the cohesiveness features depends
116
on the number of documents where the idiomatic
phrase occurs, the predictions for this test set relied
mainly on the less effective features, namely syn-
tactic and stylistic features (see Table 3). However,
this problem could be alleviated obtaining the distri-
butional statistics from a large corpus with enough
occurrences of the unseen phrases.
Besides it is important to note, that in spite of the
low individual contribution of the stylistic features
to the overall accuracy (3.5%), when these are com-
bined with the remaining features they provide an
improvement of 12.9% (see Table 3).
5 Conclusions
We participated in the Phrasal Semantics sub task 5b
in SemEval 2013. Our system proved the effective-
ness of the use of cohesiveness, stylistic and syn-
tactic features for discriminating literal from figura-
tive usage of idiomatic phrases. The most-frequent-
class baseline was overcame by 49.8% in the ?un-
seen contexts? setting (LexSample) and 8.5% in ?un-
seen phrases? (AllWords).
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT?DST India
(proj. 122030 ?Answer Validation through Textual
Entailment?).
References
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of nonlit-
eral language. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, Trento, Italy.
S. Le Cessie and J. C. Van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16(1):22?29, March.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identifica-
tion of idiomatic expressions. Comput. Linguist.,
35(1):61?103, March.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Karen Sp?rck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493?502, October.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
?06, pages 12?19, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ioannis Korkontzelos, Torsten Zesch, Fabio Massimo
Zanzotto, and Chris Biemann. 2013. SemEval-2013
task 5: Evaluating phrasal semantics. In Proceedings
of the 7th International Workshop on Semantic Evalu-
ation (SemEval 2013).
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, ACL ?99,
page 317?324, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia. Association for Computa-
tional Linguistics.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. In Proceedings of the 12th Conference of
the European Chapter of the Association for Computa-
tional Linguistics, EACL ?09, page 754?762, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
117
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 280?284, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SOFTCARDINALITY: Hierarchical Text Overlap
for Student Response Analysis
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, Av. Mendiz?bal,
Col. Nueva Industrial Vallejo
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
In this paper we describe our system used to
participate in the Student-Response-Analysis
task-7 at SemEval 2013. This system is based
on text overlap through the soft cardinality and
a new mechanism for weight propagation. Al-
though there are several official performance
measures, taking into account the overall ac-
curacy throughout the two availabe data sets
(Beetle and SciEntsBank), our system ranked
first in the 2 way classification task and sec-
ond in the others. Furthermore, our sys-
tem performs particularly well with ?unseen-
domains? instances, which was the more chal-
lenging test set. This paper also describes an-
other system that integrates this method with
the lexical-overlap baseline provided by the
task organizers obtaining better results than
the best official results. We concluded that the
soft cardinality method is a very competitive
baseline for the automatic evaluation of stu-
dent responses.
1 Introduction
The Student-Response-Analysis (SRA) task consist
in provide assessments of the correctness of student
answers (A), considering their corresponding ques-
tions (Q) and reference answers (RA) (Dzikovska
et al, 2012). SRA is the task-7 in the SemEval
2013 evaluation campaign (Dzikovska et al, 2013).
The method used in our participation was basically
text overlap based on the soft cardinality (Jimenez
et al, 2010) plus a machine learning classifier. This
method did not use any information external to the
data sets except for a stemmer and a list of stop
words.
The soft cardinality is a general model for object
comparison that has been tested at text applications.
Particularly, this text overlap approach has provided
strong baselines for several applications, i.e. entity
resolution (Jimenez et al, 2010), semantic textual
similarity (Jimenez et al, 2012a), cross-lingual tex-
tual entailment (Jimenez et al, 2012b), information
retrieval, textual entailment and paraphrase detec-
tion (Jimenez and Gelbukh, 2012). A brief descrip-
tion of the soft cardinality is presented in the next
section.
The data for SRA consist of two data sets Bee-
tle (5,199 instances) and SciEntsBank (10,804 in-
stances) divided into training and test sets (76%-
24% for Beetle and 46%-54% SciEntsBank). In ad-
dition, the test part of Beetle data set was divided
into two test sets: ?unseen answers? (35%) and ?un-
seen questions? (65%). Similarity, SciEntsBank test
part is divided into ?unseen answers? (9%), ?unseen
questions? (13%) and ?unseen domains? (78%). All
texts are in English.
The challenge consists in predicting for each in-
stance triple (Q, A, RA) an assessment of correct-
ness for the student?s answer. Three levels of detail
are considered for this assessment: 2 way (correct
and incorrect), 3 way (correct, contradictory and in-
correct) and 5 way (correct, incomplete, contradic-
tory, irrelevant and non-in-the-domain).
Section 3 presents the method used for the extrac-
tion of features from texts using the soft cardinal-
ity to provide a vector representation. In Section 4,
the details of the system used to produce our predic-
280
tions are presented. Besides, in that section a system
that integrates our system with the lexical-overlap
baseline proposed by the task organizers is also pre-
sented. This combined system was motivated by the
observation that our system performed well in the
SciEntsBank data set but poorly in Beetle in compar-
ison with the lexical-overlap baseline. The results
obtained by both systems are also presented in that
section.
Finally in Section 5 the conclusions of our partic-
ipation in this evaluation campaign are presented.
2 Soft Cardinality
The soft cardinality (Jimenez et al, 2010) of a col-
lection of elements S is calculated with the follow-
ing expression:
|S|? =
n?
i=1
wi ?
?
?
n?
j=1
sim(si, sj)p
?
?
?1
(1)
Having S ={s1, s2, . . . , sn}; wi ? 0; p ? 0;
1 > sim(x, y) ? 0, x 6= y; and sim(x, x) = 1.
The parameter p controls the degree of "softness"
of the cardinality (the larger the ?harder?). In fact,
when p ? ? the soft cardinality is equivalent to
classical set cardinality. The default value for this
parameter is p = 1. The coefficients wi are weights
associated with each element, which can represent
the importance or informative character of each ele-
ment. The function sim is a similarity function that
compares pairs of elements in the collection S.
3 Features from Cardinalities
It is commonly accepted that it is possible to make
a fair comparison of two objects if they are of the
same nature. If the objects are instances of a com-
positional hierarchy, they should belong to the same
class to be comparable. Clearly, a house is compa-
rable with another house, a wall with another wall
and a brick with another brick, but walls and bricks
are not comparable (at least not directly). Similarly,
in text applications documents should be compared
with documents, sentences with sentences, words
with words, and so on.
However, a comparison measure between a sen-
tence and a document can be obtained with different
approaches. First, using the information retrieval ap-
proach, the document is considered like a very long
sentence and the comparison is then straight for-
ward. Another approach is to make pairwise com-
parisons between the sentence and each sentence in
the document. Then, the similarity scores of these
comparisons can be aggregated in a single score
using average, max or min functions. These ap-
proaches have issues, the former ignores the sen-
tence subdivision of the document and the later ig-
nores the similarities among the sentences in the
document.
In the task at hand, each instance is composed of
a question Q, a student answer A, which are sen-
tences, and a collection of reference answers RA,
which could be considered as a multi-sentence doc-
ument. The soft cardinality can be used to provide
values for |Q|?, |A|?, |RA|?, |Q?A|?, |A?RA|? and
|Q?RA|?. The intersections that involve RA require
a special treatment to tackle the aforementioned is-
sues.
Let?s start defining a word-similarity function.
Two words (or terms) t1 and t2 can be compared di-
viding them into character q-grams (Kukich, 1992).
The representation in q-grams of ti can be denoted
as t[q]i . Similarly, a combined representation us-
ing a range of q-grams of different length can be
denoted as t[q1:q2]i . For instance, if t1 =?home?
then t[2:3]1 ={?ho?,?om?,?me?,?hom?,?ome?}. Thus,
t[q1:q2]1 and t
[q1:q2]
2 representations can be com-
pared using the Dice?s coefficient to build a word-
similarity function:
simwords(t1, t2) =
2 ?
?
?
?t[q1:q2]1 ? t
[q1:q2]
2
?
?
?
?
?
?t[q1:q2]1
?
?
?+
?
?
?t[q1:q2]1
?
?
?
(2)
Note that in eq. 2 the classical set cardinality was
used, i.e |x| means classical cardinality and |x|? soft
cardinality.
The function simwords can be plugged in eq.1 to
obtain the soft cardinality of a sentence S (using uni-
tary weights wi = 1 and p = 1):
|S|? =
|S|?
i=1
?
?
|S|?
j=1
simword(ti, tj)
?
?
?1
(3)
281
|X| |Y | |X ? Y |
BF1: |Q|? BF2: |A|? BF3: |Q ?A|?
BF2: |A|? BF4: |RA|?? BF5: |RA ?A|??
BF1: |Q|? BF4: |RA|?? BF6: |RA ?Q|??
Table 1: Basic feature set
Where ti are the words in the sentence S .
The sentence-soft-cardinality function can be
used to build a sentence-similarity function to com-
pare two sentences S1 and S2 using again the Dice?s
coefficient:
simsent.(S1, S2) =
2 ? (|S1|? + |S2|? ? |S1 ? S2|?)
|S1|+ |S2|
(4)
In this formulation S1?S2 is the concatenation of
both sentences.
The eq. 4 can be plugged again into eq. 1 to obtain
the soft cardinality of a ?document? RA, which is a
collection of sentences RA = {S1, S2. . . . , S|RA|}:
|RA|?? =
|RA|?
i=1
|Si|
? ?
?
?
|RA|?
j=1
sim(Si, Sj)
?
?
?1
(5)
Note that the soft cardinalities of the sentences
|Si|? were re-used as importance weights wi in eq.
1. These weights are propagations of the unitary
weights assigned to the words, which in turn were
aggregated by the soft cardinality at sentence level
(eq. 3). This soft cardinality is denoted with double
apostrophe because is a function recursively based
in the single-apostrophized soft cardinality.
The proposed soft cardinality expressions are
used to obtain the basic feature set presented in Ta-
ble 1. The soft cardinalities of |Q|?, |A|? and |Q?A|?
are calculated with eq. 3. The soft cardinalities
|RA|??, |RA?A|?? and |RA?Q|?? are calculated with
eq. 5. Recall that Q ? A is the concatenation of the
question and answer sentences. Similarly, RA ? A
and RA ?Q are the collection of reference answers
adding A xor Q .
Starting from the basic feature set, an extended
set, showed in Table 2, can be obtained from each
one of the three rows in Table 1. Recall that |X ?
Y | = |X|+ |Y |?|X?Y | and |X \Y | = |X|?|X?
EF1: |X ? Y | EF2: |X \ Y |
EF3: |Y \X| EF4: |X?Y ||X|
EF5:
|X?Y |
|Y | EF6:
|X?Y |
|X?Y |
EF7:
2?|X?Y |
|X|+|Y | EF8:
|X?Y |?
|X|?|Y |
EF9:
|X?Y |
min(|X|,|Y |) EF10:
|X?Y |
max(|X|,|Y |)
EF11:
|X?Y |?(|X|+|Y |)
2?|X|?|Y | EF12: |X ? Y | ? |X ? Y |
Table 2: Extended feature set
Y |. Consequently, the total number of features is 6
basic features plus 12 extended features multiplied
by 3, i.e. 42 features.
4 Systems Description
4.1 Submitted System
First, each text in the SRA data was preprocessed by
tokenizing, lowercasing, stop-words1 removing and
stemming with the Porter?s algorithm (Porter, 1980).
Second, each stemmed word t was represented in
q-grams: t[3:4] for Beetle and t[4] for SciEntsBank.
These representations obtained the best accuracies
in the training data sets.
Two vector data sets were obtained extracting the
42 features?described in Section 3?for each instance
in Beetle and SciEntsBank separately. Then, three
classification models (2 way, 3way and 5 way) were
learned from the training partitions on each vector
data set using a J48 graft tree (Webb, 1999). All
6 resulting classification models were boosted with
15 iterations of bagging (Breiman, 1996). The used
implementation of this classifier was that included
in WEKA v.3.6.9 (Hall et al, 2009). The results
obtained by this system are shown in Table 3 in the
rows labeled with ?Soft Cardinality-run1?.
4.2 An Improved System
At the time when the official results were released,
we observed that our submitted system performed
pretty well in SciEntsBank but poorly in Beetle.
Moreover, the lexical-overlap baseline outperformed
our system in Beetle. Firstly, we decided to include
in our feature set the 8 features of the lexical over-
lap baseline described by Dzikovska et al (2012)
1those provided by nltk.org
282
Beetle SciEntsBank
Task System UA1 UQ2 All UA1 UQ2 UD3 All All Rank
2 way
Soft Cardinality-unofficial 0.797 0.725 0.750 0.717 0.733 0.726 0.726 0.730 -
Soft Cardinality-run1 0.781 0.667 0.707 0.724 0.745 0.711 0.716 0.715 1
ETS-run1 0.811 0.741 0.765 0.722 0.711 0.698 0.702 0.713 2
CU-run1 0.786 0.718 0.742 0.656 0.674 0.693 0.687 0.697 3
Lexical overlap baseline 0.797 0.740 0.760 0.661 0.674 0.676 0.674 0.690 6
3 way
Soft Cardinality-unofficial 0.608 0.532 0.559 0.656 0.671 0.646 0.650 0.634 -
ETS-run1 0.633 0.551 0.580 0.626 0.663 0.632 0.635 0.625 1
Soft Cardinality-run1 0.624 0.453 0.513 0.659 0.652 0.637 0.641 0.618 2
CoMeT-run1 0.731 0.518 0.592 0.713 0.546 0.579 0.587 0.588 3
Lexical overlap baseline 0.595 0.512 0.541 0.556 0.540 0.577 0.570 0.565 8
5way
Soft Cardinality-unofficial 0.572 0.476 0.510 0.552 0.520 0.534 0.534 0.530 -
ETS-run1 0.574 0.560 0.565 0.543 0.532 0.501 0.509 0.519 1
Soft Cardinality-run1 0.576 0.451 0.495 0.544 0.525 0.512 0.517 0.513 2
ETS-run2 0.715 0.621 0.654 0.631 0.401 0.476 0.481 0.512 3
Lexical overlap baseline 0.519 0.480 0.494 0.437 0.413 0.415 0.417 0.430 11
Total number of test instances 439 819 1,258 540 733 4,562 5,835 7,093
TEST SETS: unseen answers1, unseen questions2, unseen domains3.
Table 3: Official results for the top-3 performing systems (among 15), the lexical overlap baseline in the SRA task
SemEval 2013 and unofficial results of the soft cardinality system combined with the lexical overlap (in italics).
Performance measure used: overall accuracy.
(see Text::Similarity::Overlaps2 package for more
details).
Secondly, the lexical overlap baseline aggregates
the pairwise scores between each reference answer
and the student answer by taking the maximum
value of the pairwise scores. So, we decided to use
this aggregation mechanism instead of the aggrega-
tion proposed through eq. 3.
Thirdly, only at that time we realized that, unlike
Beetle, in SciEntsBank all instances have only one
reference answer. Consequently, the only effect of
eq. 5 in SciEntsBank was in the calculation of |RA?
A|?? (and |RA?Q|??) by |X?Y |?? = |X|
?+|Y |?
1+simsent.(X,Y )
.
As a result, this transformation induced a boosting
effect in X?Y making |X?Y |?? ? |X?Y |? for any
X , Y . We decided to use this intersection-boosting
effect not only in RA ? A, RA ? Q, but in Q ?
A. This intersecton boosting effect works similarly
to the Lesk?s measure (Lesk, 1986) included in the
lexical overlap baseline.
The individual effect in the performance of each
2http://search.cpan.org/dist/Text-
Similarity/lib/Text/Similarity/Overlaps.pm
of the previous decisions was positive in all three
cases. The results obtained using an improved
system that implemented those three decisions are
shown in Table 3?in italics. This system would have
obtained the best general overall accuracy in the of-
ficial ranking.
5 Conclusions
We participated in the Student-Response-Analysis
task-7 in SemEval 2013 with a text overlap system
based on the soft cardinality. This system obtained
places 1st (2 way task) and 2nd (3 way and 5 way)
considering the overall accuracy across all data sets
and test sets. Particularly, our system was the best
in the largest and more challenging test set, namely
?unseen domains?. Moreover, we integrated the lex-
ical overlap baseline to our system obtaining even
better results.
As a conclusion, the text overlap method based on
the soft cardinality is very challenging base line for
the SRA task.
283
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT?DST India
(proj. 122030 ?Answer Validation through Textual
Entailment?).
References
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: a dataset and baselines. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL
HLT ?12, page 200?210, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Myroslava O. Dzikovska, Rodney D. Nielsen, Chris
Brew, Claudia Leacock, Danilo Giampiccolo, Luisa
Bentivogli, Peter Clark, Ido Dagan, and Hoa Trang
Dang. 2013. SemEval-2013 task 7: The joint stu-
dent response analysis and 8th recognizing textual en-
tailment challenge. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013), in conjunction with the Second Joint Confer-
ence on Lexical and Computational Semantcis (*SEM
2013), Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Sergio Jimenez and Alexander Gelbukh. 2012. Baselines
for natural language processing tasks. Appl. Comput.
Math., 11(2):180?199.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardinality.
In Edgar Chavez and Stefano Lonardi, editors, String
Processing and Information Retrieval, volume 6393 of
LNCS, pages 297?302. Springer, Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized simi-
larity function for text comparison. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval, *SEM 2012), Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: learning adaptive
similarity functions for cross-lingual textual entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval, *SEM 2012),
Montreal, Canada. ACL.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys,
24:377?439, December.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ?86, page 24?26, New York, NY,
USA. ACM.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
Geoffrey I. Webb. 1999. Decision tree grafting from the
all-tests-but-one partition. In Proceedings of the 16th
international joint conference on Artificial intelligence
- Volume 2, IJCAI?99, pages 702?707, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
284
