Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 59?68,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Discriminative Learning of Selectional Preference from Unlabeled Text
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View
California, 94301
lindek@google.com
Randy Goebel
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
goebel@cs.ualberta.ca
Abstract
We present a discriminative method for learn-
ing selectional preferences from unlabeled
text. Positive examples are taken from ob-
served predicate-argument pairs, while nega-
tives are constructed from unobserved combi-
nations. We train a Support Vector Machine
classifier to distinguish the positive from the
negative instances. We show how to parti-
tion the examples for efficient training with
57 thousand features and 6.5 million training
instances. The model outperforms other re-
cent approaches, achieving excellent correla-
tion with human plausibility judgments. Com-
pared to Mutual Information, it identifies 66%
more verb-object pairs in unseen text, and re-
solves 37% more pronouns correctly in a pro-
noun resolution experiment.
1 Introduction
Selectional preferences (SPs) tell us which argu-
ments are plausible for a particular predicate. For
example, Table 2 (Section 4.4) lists plausible and
implausible direct objects (arguments) for particu-
lar verbs (predicates). SPs can help resolve syntac-
tic, word sense, and reference ambiguity (Clark and
Weir, 2002), and so gathering them has received a
lot of attention in the NLP community.
One way to determine SPs is from co-occurrences
of predicates and arguments in text. Unfortunately,
no matter how much text we use, many acceptable
pairs will be missing. Bikel (2004) found that only
1.49% of the bilexical dependencies considered by
Collins? parser during decoding were observed dur-
ing training. In our parsed corpus (Section 4.1),
for example, we find eat with nachos, burritos, and
tacos, but not with the equally tasty quesadillas,
chimichangas, or tostadas. Rather than solely re-
lying on co-occurrence counts, we would like to use
them to generalize to unseen pairs.
In particular, we would like to exploit a number
of arbitrary and potentially overlapping properties
of predicates and arguments when we assign SPs.
We do this by representing these properties as fea-
tures in a linear classifier, and training the weights
using discriminative learning. Positive examples
are taken from observed predicate-argument pairs,
while pseudo-negatives are constructed from unob-
served combinations. We train a Support Vector Ma-
chine (SVM) classifier to distinguish the positives
from the negatives. We refer to our model?s scores
as Discriminative Selectional Preference (DSP). By
creating training vectors automatically, DSP enjoys
all the advantages of supervised learning, but with-
out the need for manual annotation of examples.
We evaluate DSP on the task of assigning verb-
object selectional preference. We encode a noun?s
textual distribution as feature information. The
learned feature weights are linguistically interesting,
yielding high-quality similar-word lists as latent in-
formation. Despite its representational power, DSP
scales to real-world data sizes: examples are parti-
tioned by predicate, and a separate SVM is trained
for each partition. This allows us to efficiently learn
with over 57 thousand features and 6.5 million ex-
amples. DSP outperforms recently proposed alterna-
tives in a range of experiments, and better correlates
with human plausibility judgments. It also shows
strong gains over a Mutual Information-based co-
59
occurrence model on two tasks: identifying objects
of verbs in an unseen corpus and finding pronominal
antecedents in coreference data.
2 Related Work
Most approaches to SPs generalize from observed
predicate-argument pairs to semantically similar
ones by modeling the semantic class of the argu-
ment, following Resnik (1996). For example, we
might have a class Mexican Food and learn that the
entire class is suitable for eating. Usually, the classes
are from WordNet (Miller et al, 1990), although
they can also be inferred from clustering (Rooth et
al., 1999). Brockmann and Lapata (2003) compare
a number of WordNet-based approaches, including
Resnik (1996), Li and Abe (1998), and Clark and
Weir (2002), and found that the more sophisticated
class-based approaches do not always outperform
simple frequency-based models.
Another line of research generalizes using simi-
lar words. Suppose we are calculating the proba-
bility of a particular noun, n, occurring as the ob-
ject argument of a given verbal predicate, v. Let
Pr(n|v) be the empirical maximum-likelihood esti-
mate from observed text. Dagan et al (1999) define
the similarity-weighted probability, PrSIM, to be:
PrSIM(n|v) =
?
v??SIMS(v)
Sim(v?, v)Pr(n|v?) (1)
where Sim(v?, v) returns a real-valued similarity be-
tween two verbs v? and v (normalized over all pair
similarities in the sum). In contrast, Erk (2007)
generalizes by substituting similar arguments, while
Wang et al (2005) use the cross-product of simi-
lar pairs. One key issue is how to define the set
of similar words, SIMS(w). Erk (2007) compared a
number of techniques for creating similar-word sets
and found that both the Jaccard coefficient and Lin
(1998a)?s information-theoretic metric work best.
Similarity-smoothed models are simple to compute,
potentially adaptable to new domains, and require
no manually-compiled resources such as WordNet.
Selectional Preferences have also been a recent
focus of researchers investigating the learning of
paraphrases and inference rules (Pantel et al, 2007;
Roberto et al, 2007). Inferences such as ?[X wins
Y] ? [X plays Y]? are only valid for certain argu-
ments X and Y. We follow Pantel et al (2007) in us-
ing automatically-extracted semantic classes to help
characterize plausible arguments.
Discriminative techniques are widely used in NLP
and have been applied to the related tasks of word
prediction and language modeling. Even-Zohar and
Roth (2000) use a classifier to predict the most likely
word to fill a position in a sentence (in their ex-
periments: a verb) from a set of candidates (sets
of verbs), by inspecting the context of the target
token (e.g., the presence or absence of a particu-
lar nearby word in the sentence). This approach
can therefore learn which specific arguments occur
with a particular predicate. In comparison, our fea-
tures are second-order: we learn what kinds of argu-
ments occur with a predicate by encoding features
of the arguments. Recent distributed and latent-
variable models also represent words with feature
vectors (Bengio et al, 2003; Blitzer et al, 2005).
Many of these approaches learn both the feature
weights and the feature representation. Vectors must
be kept low-dimensional for tractability, while learn-
ing and inference on larger scales is impractical. By
partitioning our examples by predicate, we can effi-
ciently use high-dimensional, sparse vectors.
Our technique of generating negative examples
is similar to the approach of Okanohara and Tsujii
(2007). They learn a classifier to disambiguate ac-
tual sentences from pseudo-negative examples sam-
pled from an N-gram language model. Smith and
Eisner (2005) also automatically generate negative
examples. They perturb their input sequence (e.g.
the sentence word order) to create a neighborhood of
implicit negative evidence. We create negatives by
substitution rather than perturbation, and use corpus-
wide statistics to choose our negative instances.
3 Methodology
3.1 Creating Examples
To learn a discriminative model of selectional pref-
erence, we create positive and negative training ex-
amples automatically from raw text. To create the
positives, we automatically parse a large corpus, and
then extract the predicate-argument pairs that have
a statistical association in this data. We measure
this association using pointwise Mutual Information
(MI) (Church and Hanks, 1990). The MI between a
60
verb predicate, v, and its object argument, n, is:
MI(v, n) = log Pr(v, n)
Pr(v)Pr(n) = log
Pr(n|v)
Pr(n) (2)
If MI>0, the probability v and n occur together is
greater than if they were independently distributed.
We create sets of positive and negative examples
separately for each predicate, v. First, we extract all
pairs where MI(v, n)>? as positives. For each pos-
itive, we create pseudo-negative examples, (v, n?),
by pairing v with a new argument, n?, that either has
MI below the threshold or did not occur with v in the
corpus. We require each negative n? to have a similar
frequency to its corresponding n. This prevents our
learning algorithm from focusing on any accidental
frequency-based bias. We mix in K negatives for
each positive, sampling without replacement to cre-
ate all the negatives for a particular predicate. For
each v, 1K+1 of its examples will be positive. The
threshold ? represents a trade-off between capturing
a large number of positive pairs and ensuring these
pairs have good association. Similarly, K is a trade-
off between the number of examples and the com-
putational efficiency. Ultimately, these parameters
should be optimized for task performance.
Of course, some negatives will actually be plau-
sible arguments that were unobserved due to sparse-
ness. Fortunately, modern discriminative methods
like soft-margin SVMs can learn in the face of label
error by allowing slack, subject to a tunable regular-
ization penalty (Cortes and Vapnik, 1995).
If MI is a sparse and imperfect model of SP, what
can DSP gain by training on MI?s scores? We can
regard DSP as learning a view of SP that is or-
thogonal to MI, in a co-training sense (Blum and
Mitchell, 1998). MI labels the data based solely
on co-occurrence; DSP uses these labels to iden-
tify other regularities ? ones that extend beyond co-
occurring words. For example, many instances of
n where MI(eat, n)>? also have MI(buy, n)>? and
MI(cook, n)>? . Also, compared to other nouns,
a disproportionate number of eat-nouns are lower-
case, single-token words, and they rarely contain
digits, hyphens, or begin with a human first name
like Bob. DSP encodes these interdependent prop-
erties as features in a linear classifier. This classi-
fier can score any noun as a plausible argument of
eat if indicative features are present; MI can only
assign high plausibility to observed (eat,n) pairs.
Similarity-smoothed models can make use of the
regularities across similar verbs, but not the finer-
grained string- and token-based features.
Our training examples are similar to the data cre-
ated for pseudodisambiguation, the usual evalua-
tion task for SP models (Erk, 2007; Keller and La-
pata, 2003; Rooth et al, 1999). This data con-
sists of triples (v, n, n?) where v, n is a predicate-
argument pair observed in the corpus and v, n? has
not been observed. The models score correctly
if they rank observed (and thus plausible) argu-
ments above corresponding unobserved (and thus
likely implausible) ones. We refer to this as Pair-
wise Disambiguation. Unlike this task, we classify
each predicate-argument pair independently as plau-
sible/implausible. We also use MI rather than fre-
quency to define the positive pairs, ensuring that the
positive pairs truly have a statistical association, and
are not simply the result of parser error or noise.1
3.2 Partitioning for Efficient Training
After creating our positive and negative training
pairs, we must select a feature representation for our
examples. Let ? be a mapping from a predicate-
argument pair (v, n) to a feature vector, ? :
(v, n) ? ??1...?k?. Predictions are made based
on a weighted combination of the features, y =
? ??(v, n), where ? is our learned weight vector.
We can make training significantly more efficient
by using a special form of attribute-value features.
Let every feature ?i be of the form ?i(v, n) = ?v =
v??f(n)?. That is, every feature is an intersection of
the occurrence of a particular predicate, v?, and some
feature of the argument f(n). For example, a fea-
ture for a verb-object pair might be, ?the verb is eat
and the object is lower-case.? In this representation,
features for one predicate will be completely inde-
pendent from those for every other predicate. Thus
rather than a single training procedure, we can actu-
ally partition the examples by predicate, and train a
1For a fixed verb, MI is proportional to Keller and Lapata
(2003)?s conditional probability scores for pseudodisambigua-
tion of (v, n, n?) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was
shown to be a better measure of association than co-occurrence
frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows
us to use a constant threshold across all verbs. MI was also
recently used for inference-rule SPs by Pantel et al (2007).
61
classifier for each predicate independently. The pre-
diction becomes yv = ?v ??v(n), where ?v are the
learned weights corresponding to predicate v and all
features ?v(n)=f(n) depend on the argument only.
Some predicate partitions may have insufficient
examples for training. Also, a predicate may oc-
cur in test data that was unseen during training. To
handle these instances, we decided to cluster low-
frequency predicates. In our experiments assigning
SP to verb-object pairs, we cluster all verbs that have
less than 250 positive examples, using clusters gen-
erated by the CBC algorithm (Pantel and Lin, 2002).
For example, the low-frequency verbs incarcerate,
parole, and court-martial are all mapped to the same
partition, while more-frequent verbs like arrest and
execute each have their own partition. About 5.5%
of examples are clustered, corresponding to 30% of
the 7367 total verbs. 40% of verbs (but only 0.6% of
examples) were not in any CBC cluster; these were
mapped to a single backoff partition.
The parameters for each partition, ?v, can be
trained with any supervised learning technique. We
use SVM (Section 4.1) because it is effective in simi-
lar high-dimensional, sparse-vector settings, and has
an efficient implementation (Joachims, 1999). In
SVM, the sign of yv gives the classification. We can
also use the scalar yv as our DSP score (i.e. the posi-
tive distance from the separating SVM hyperplane).
3.3 Features
This section details our argument features, f(n), for
assigning verb-object selectional preference. For a
verb predicate (or partition) v and object argument
n, the form of our classifier is yv = ?i ?vi fi(n).
3.3.1 Verb co-occurrence
We provide features for the empirical probability
of the noun occurring as the object argument of other
verbs, Pr(n|v?). If we were to only use these features
(indexing the feature weights by each verb v?), the
form of our classifier would be:
yv =
?
v?
?vv?Pr(n|v?) (3)
Note the similarity between Equation (3) and Equa-
tion (1). Now the feature weights, ?vv? , take the role
of the similarity function, Sim(v?, v). Unlike Equa-
tion (1), however, these weights are not set by an
external similarity algorithm, but are optimized to
discriminate the positive and negative training ex-
amples. We need not restrict ourselves to a short list
of similar verbs; we include Probj(n|v?) features for
every verb that occurs more than 10 times in our cor-
pus. ?vv? may be positive or negative, depending on
the relation between v? and v. We also include fea-
tures for the probability of the noun occurring as the
subject of other verbs, Prsubj(n|v?). For example,
nouns that can be the object of eat will also occur as
the subject of taste and contain. Other contexts, such
as adjectival and nominal predicates, could also aid
the prediction, but have not yet been investigated.
The advantage of tuning similarity to the appli-
cation of interest has been shown previously by
Weeds and Weir (2005). They optimize a few meta-
parameters separately for the tasks of thesaurus gen-
eration and pseudodisambiguation. Our approach,
on the other hand, discriminatively sets millions of
individual similarity values. Like Weeds and Weir
(2005), our similarity values are asymmetric.
3.3.2 String-based
We include several simple character-based fea-
tures of the noun string: the number of tokens, the
case, and whether it contains digits, hyphens, an
apostrophe, or other punctuation. We also include a
feature for the first and last token, and fire indicator
features if any token in the noun occurs on in-house
lists of given names, family names, cities, provinces,
countries, corporations, languages, etc. We also fire
a feature if a token is a corporate designation (like
inc. or ltd.) or a human one (like Mr. or Sheik).
3.3.3 Semantic classes
Motivated by previous SP models that make use
of semantic classes, we generated word clusters us-
ing CBC (Pantel and Lin, 2002) on a 10 GB corpus,
giving 3620 clusters. If a noun belongs in a cluster,
a corresponding feature fires. If a noun is in none of
the clusters, a no-class feature fires.
As an example, CBC cluster 1891 contains:
sidewalk, driveway, roadway, footpath,
bridge, highway, road, runway, street, alley,
path, Interstate, . . .
In our training data, we have examples like widen
highway, widen road and widen motorway. If we
62
see that we can widen a highway, we learn that we
can also widen a sidewalk, bridge, runway, etc.
We also made use of the person-name/instance
pairs automatically extracted by Fleischman et al
(2003).2 This data provides counts for pairs such
as ?Edwin Moses, hurdler? and ?William Farley, in-
dustrialist.? We have features for all concepts and
therefore learn their association with each verb.
4 Experiments and Results
4.1 Set up
We parsed the 3 GB AQUAINT corpus (Voorhees,
2002) using Minipar (Lin, 1998b), and collected
verb-object and verb-subject frequencies, building
an empirical MI model from this data. Verbs and
nouns were converted to their (possibly multi-token)
root, and string case was preserved. Passive sub-
jects (the car was bought) were converted to objects
(bought car). We set the MI-threshold, ? , to be 0,
and the negative-to-positive ratio, K, to be 2.
Numerous previous pseudodisambiguation evalu-
ations only include arguments that occur between 30
and 3000 times (Erk, 2007; Keller and Lapata, 2003;
Rooth et al, 1999). Presumably the lower bound is
to help ensure the negative argument is unobserved
because it is unsuitable, not because of data sparse-
ness. We wish to use our model on arguments of
any frequency, including those that never occurred
in the training corpus (and therefore have empty co-
occurrence features (Section 3.3.1)). We proceed as
follows: first, we exclude pairs whenever the noun
occurs less than 3 times in our corpus, removing
many misspellings and other noun noise. Next, we
omit verb co-occurrence features for nouns that oc-
cur less than 10 times, and instead fire a low-count
feature. When we move to a new corpus, previously-
unseen nouns are treated like these low-count train-
ing nouns.
This processing results in a set of 6.8 million
pairs, divided into 2318 partitions (192 of which
are verb clusters (Section 3.2)). For each parti-
tion, we take 95% of the examples for training,
2.5% for development and 2.5% for a final unseen
test set. We provide full results for two models:
DSPcooc which only uses the verb co-occurrence fea-
tures, and DSPall which uses all the features men-
2Available at http://www.mit.edu/?mbf/instances.txt.gz
tioned in Section 3.3. Feature values are normalized
within each feature type. We train our (linear kernel)
discriminative models using SVMlight (Joachims,
1999) on each partition, but set meta-parameters C
(regularization) and j (cost of positive vs. nega-
tive misclassifications: max at j=2) on the macro-
averaged score across all development partitions.
Note that we can not use the development set to op-
timize ? and K because the development examples
are obtained after setting these values.
4.2 Feature weights
It is interesting to inspect the feature weights re-
turned by our system. In particular, the weights
on the verb co-occurrence features (Section 3.3.1)
provide a high-quality, argument-specific similarity-
ranking of other verb contexts. The DSP parameters
for eat, for example, place high weight on features
like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).
Lin (1998a)?s similar word list for eat misses these
but includes sleep (ranked 6) and sit (ranked 14), be-
cause these have similar subjects to eat. Discrimina-
tive, context-specific training seems to yield a bet-
ter set of similar predicates, e.g. the highest-ranked
contexts for DSPcooc on the verb join,3
lead 1.42, rejoin 1.39, form 1.34, belong to
1.31, found 1.31, quit 1.29, guide 1.19, induct
1.19, launch (subj) 1.18, work at 1.14
give a better SIMS(join) for Equation (1) than the
top similarities returned by (Lin, 1998a):
participate 0.164, lead 0.150, return to 0.148,
say 0.143, rejoin 0.142, sign 0.142, meet
0.142, include 0.141, leave 0.140, work 0.137
Other features are also weighted intuitively. Note
that case is a strong indicator for some arguments,
for example the weight on being lower-case is high
for become (0.972) and eat (0.505), but highly nega-
tive for accuse (-0.675) and embroil (-0.573) which
often take names of people and organizations.
4.3 Pseudodisambiguation
We first evaluate DSP on disambiguating posi-
tives from pseudo-negatives, comparing to recently-
3Which all correspond to nouns occurring in the object po-
sition of the verb (e.g. Probj(n|lead)), except ?launch (subj)?
which corresponds to Prsubj(n|launch).
63
System MacroAvg MicroAvg PairwiseP R F P R F Acc Cov
Dagan et al (1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98
Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83
Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57
DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00
DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00
Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (Mi-
croAvg), plus coverage and accuracy of pairwise competition (Pairwise).
proposed systems that also require no manually-
compiled resources like WordNet. We convert Da-
gan et al (1999)?s similarity-smoothed probability
to MI by replacing the empirical Pr(n|v) in Equa-
tion (2) with the smoothed PrSIM from Equation (1).
We also test an MI model inspired by Erk (2007):
MISIM(n, v) = log
?
n??SIMS(n)
Sim(n?, n) Pr(v, n
?)
Pr(v)Pr(n?)
We gather similar words using Lin (1998a), mining
similar verbs from a comparable-sized parsed cor-
pus, and collecting similar nouns from a broader 10
GB corpus of English text.4
We also use Keller and Lapata (2003)?s approach
to obtaining web-counts. Rather than mining parse
trees, this technique retrieves counts for the pattern
?V Det N? in raw online text, where V is any in-
flection of the verb, Det is the, a, or the empty
string, and N is the singular or plural form of the
noun. We compute a web-based MI by collecting
Pr(n, v), Pr(n), and Pr(v) using all inflections, ex-
cept we only use the root form of the noun. Rather
than using a search engine, we obtain counts from
the Google Web 5-gram Corpus.5
All systems are thresholded at zero to make a clas-
sification. Unlike DSP, the comparison systems may
4For both the similar-noun and similar-verb smoothing, we
only smooth over similar pairs that occurred in the corpus.
While averaging over all similar pairs tends to underestimate
the probability, averaging over only the observed pairs tends to
overestimate it. We tested both and adopt the latter because it
resulted in better performance on our development set.
5Available from the LDC as LDC2006T13. This collection
was generated from approximately 1 trillion tokens of online
text. Unfortunately, tokens appearing less than 200 times have
been mapped to the ?UNK? symbol, and only N-grams appear-
ing more than 40 times are included. Unlike results from search
engines, however, experiments with this corpus are replicable.
not be able to provide a score for each example.
The similarity-smoothed examples will be undefined
if SIMS(w) is empty. Also, the Keller and Lapata
(2003) approach will be undefined if the pair is un-
observed on the web. As a reasonable default for
these cases, we assign them a negative decision.
We evaluate disambiguation using precision (P),
recall (R), and their harmonic mean, F-Score (F).
Table 1 gives the results of our comparison. In the
MacroAvg results, we weight each example equally.
For MicroAvg, we weight each example by the fre-
quency of the noun. To more directly compare with
previous work, we also reproduced Pairwise Disam-
biguation by randomly pairing each positive with
one of the negatives and then evaluating each system
by the percentage it ranks correctly (Acc). For the
comparison approaches, if one score is undefined,
we choose the other one. If both are undefined, we
abstain from a decision. Coverage (Cov) is the per-
cent of pairs where a decision was made.6
Our simple system with only verb co-occurrence
features, DSPcooc, outperforms all comparison ap-
proaches. Using the richer feature set in DSPall
results in a statistically significant gain in perfor-
mance, up to an F-Score of 0.65 and a pairwise
disambiguation accuracy of 0.81.7 DSPall has both
broader coverage and better accuracy than all com-
peting approaches. In the remainder of the experi-
ments, we use DSPall and refer to it simply as DSP.
Some errors are because of plausible but unseen
arguments being used as test-set pseudo-negatives.
For example, for the verb damage, DSP?s three most
high-scoring false positives are the nouns jetliner,
carpet, and gear. While none occur with damage in
6I.e. we use the ?half coverage? condition from Erk (2007).
7The differences between DSPall and all comparison sys-
tems are statistically significant (McNemar?s test, p<0.01).
64
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10  100  1000  10000  100000  1e+06
F-
Sc
or
e
Noun Frequency
DSPall
Erk (2007)
Keller and Lapata (2003)
Figure 1: Disambiguation results by noun frequency.
our corpus, all intuitively satisfy the verb?s SPs.
MacroAvg performance is worse than MicroAvg
because all systems perform better on frequent
nouns. When we plot F-Score by noun frequency
(Figure 1), we see that DSP outperforms comparison
approaches across all frequencies, but achieves its
biggest gains on the low-frequency nouns. A richer
feature set alows DSP to make correct inferences on
examples that provide minimal co-occurrence data.
These are also the examples for which we would ex-
pect co-occurrence models like MI to fail.
As a further experiment, we re-trained DSP but
with only the string-based features removed. Overall
macro-averaged F-score dropped from 0.65 to 0.64
(a statistically significant reduction in performance).
The system scored nearly identically to DSP on the
high-frequency nouns, but performed roughly 15%
worse on the nouns that occurred less than ten times.
This shows that the string-based features are impor-
tant for selectional preference, and particularly help-
ful for low-frequency nouns.
4.4 Human Plausibility
Table 2 compares some of our systems on data used
by Resnik (1996) (also Appendix 2 in Holmes et al
(1989)). The plausibility of these pairs was initially
judged based on the experimenters? intuitions, and
later confirmed in a human experiment. We include
the scores of Resnik?s system, and note that its errors
were attributed to sense ambiguity and other limi-
tations of class-based approaches (Resnik, 1996).8
8For example, warn-engine scores highly because engines
are in the class entity, and physical entities (e.g. people) are
often objects of warn. Unlike DSP, Resnik?s approach cannot
learn that for warn, ?the property of being a person is more
Seen Criteria Unseen Verb-Object Freq.All = 1 = 2 = 3 > 3
MI > 0 0.44 0.33 0.57 0.70 0.82
Freq. > 0 0.57 0.45 0.76 0.89 0.96
DSP > 0 0.73 0.69 0.80 0.85 0.88
Table 3: Recall on identification of Verb-Object pairs
from an unseen corpus (divided by pair frequency).
The other comparison approaches also make a num-
ber of mistakes, which can often be traced to a mis-
guided choice of similar word to smooth with.
We also compare to our empirical MI model,
trained on our parsed corpus. Although Resnik
(1996) reported that 10 of the 16 plausible pairs did
not occur in his training corpus, all of them occurred
in ours and hence MI gives very reasonable scores
on the plausible objects. It has no statistics, however,
for many of the implausible ones. DSP can make
finer decisions than MI, recognizing that ?warning
an engine? is more absurd than ?judging a climate.?
4.5 Unseen Verb-Object Identification
We next compare MI and DSP on a much larger set
of plausible examples, and also test how well the
models generalize across data sets. We took the MI
and DSP systems trained on AQUAINT and asked
them to rate observed (and thus likely plausible)
verb-object pairs taken from an unseen corpus. We
extracted the pairs by parsing the San Jose Mercury
News (SJM) section of the TIPSTER corpus (Har-
man, 1992). Each unique verb-object pair is a single
instance in this evaluation.
Table 3 gives recall across all pairs (All) and
grouped by pair-frequency in the unseen corpus (1,
2, 3, >3). DSP accepts far more pairs than MI
(73% vs. 44%), even far more than a system that
accepts any previously observed verb-object combi-
nation as plausible (57%). Recall is higher on more
frequent verb-object pairs, but 70% of the pairs oc-
curred only once in the corpus. Even if we smooth
MI by smoothing Pr(n|v) in Equation 2 using modi-
fied KN-smoothing (Chen and Goodman, 1998), the
recall of MI>0 on SJM only increases from 44.1%
to 44.9%, still far below DSP. Frequency-based
models have fundamentally low coverage. As fur-
important than the property of being an entity? (Resnik, 1996).
65
Verb Plaus./Implaus. Resnik Dagan et al Erk MI DSP
see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02
read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/? 2.12/-0.65
find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81
hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67
write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31
urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/? -0.34/-0.62
warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/? 2.00/-0.99
judge contest/climate 1.30/0.28 1.50/1.90* 1.70/1.70* 3.90/? 1.00/0.51
teach language/distance 1.87/1.86 2.50/1.30 3.60/2.70 3.53/? 1.86/0.19
show sample/travel 1.44/0.41 1.60/0.14 0.40/-0.82 0.53/-0.49 1.00/-0.83
expect visit/mouth 0.59/5.93* 1.40/1.50* 1.40/0.37 1.05/-0.65 1.44/-0.15
answer request/tragedy 4.49/3.88 2.70/1.50 3.10/-0.64 2.93/? 1.00/0.01
recognize author/pocket 0.50/0.50* 0.03/0.37* 0.77/1.30* 0.48/? 1.00/0.00
repeat comment/journal 1.23/1.23* 2.30/1.40 2.90/? 2.59/? 1.00/-0.48
understand concept/session 1.52/1.51 2.70/0.25 2.00/-0.28 3.96/? 2.23/-0.46
remember reply/smoke 1.31/0.20 2.10/1.20 0.54/2.60* 1.13/-0.06 1.00/-0.42
Table 2: Selectional ratings for plausible/implausible direct objects (Holmes et al, 1989). Mistakes are marked with
an asterisk (*), undefined scores are marked with a dash (?). Only DSP is completely defined and completely correct.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.2  0.4  0.6  0.8  1
In
te
rp
ol
at
ed
 P
re
cis
io
n
Recall
DSP>T
MI>T
DSP>0
MI>0
Figure 2: Pronoun resolution precision-recall on MUC.
ther evidence, if we build a model of MI on the SJM
corpus and use it in our pseudodisambiguation ex-
periment (Section 4.3), MI>0 gets a MacroAvg pre-
cision of 86% but a MacroAvg recall of only 12%.9
4.6 Pronoun Resolution
Finally, we evaluate DSP on a common application
of selectional preferences: choosing the correct an-
tecedent for pronouns in text (Dagan and Itai, 1990;
Kehler et al, 2004). We study the cases where a
9Recall that even the Keller and Lapata (2003) system, built
on the world?s largest corpus, achieves only 34% recall (Table 1)
(with only 48% of positives and 27% of all pairs previously
observed, but see Footnote 5).
pronoun is the direct object of a verb predicate, v. A
pronoun?s antecedent must obey v?s selectional pref-
erences. If we have a better model of SP, we should
be able to better select pronoun antecedents.
We parsed the MUC-7 (1997) coreference corpus
and extracted all pronouns in a direct object rela-
tion. For each pronoun, p, modified by a verb, v, we
extracted all preceding nouns within the current or
previous sentence. Thirty-nine anaphoric pronouns
had an antecedent in this window and are used in
the evaluation. For each p, let N(p)+ by the set of
preceding nouns coreferent with p, and let N(p)?
be the remaining non-coreferent nouns. We take
all (v, n+) where n+ ? N(p)+ as positive, and all
other pairs (v, n?), n? ? N(p)? as negative.
We compare MI and DSP on this set, classifying
every (v, n) with MI>T (or DSP>T ) as positive.
By varying T , we get a precision-recall curve (Fig-
ure 2). Precision is low because, of course, there
are many nouns that satisfy the predicate?s SPs that
are not coreferent. DSP>0 has both a higher recall
and higher precision than accepting every pair pre-
viously seen in text (the right-most point on MI>T ).
The DSP>T system achieves higher precision than
MI>T for points where recall is greater than 60%
(where MI<0). Interestingly, the recall of MI>0 is
66
System Acc
Most-Recent Noun 17.9%
Maximum MI 28.2%
Maximum DSP 38.5%
Table 4: Pronoun resolution accuracy on nouns in current
or previous sentence in MUC.
higher here than it is for general verb-objects (Sec-
tion 4.5). On the subset of pairs with strong empir-
ical association (MI>0), MI generally outperforms
DSP at equivalent recall values.
We next compare MI and DSP as stand-alone pro-
noun resolution systems (Table 4). As a standard
baseline, for each pronoun, we choose the most
recent noun in text as the pronoun?s antecedent,
achieving 17.9% resolution accuracy. This baseline
is quite low because many of the most-recent nouns
are subjects of the pronoun?s verb phrase, and there-
fore resolution violates syntactic coreference con-
straints. If instead we choose the previous noun with
the highest MI as antecedent, we get an accuracy of
28.2%, while choosing the previous noun with the
highest DSP achieves 38.5%. DSP resolves 37%
more pronouns correctly than MI. We leave as fu-
ture work a full-scale pronoun resolution system that
incorporates both MI and DSP as backed-off, inter-
polated, or separate semantic features.
5 Conclusions and Future Work
We have presented a simple, effective model of se-
lectional preference based on discriminative train-
ing. Supervised techniques typically achieve higher
performance than unsupervised models, and we du-
plicate these gains with DSP. Here, however, these
gains come at no additional labeling cost, as train-
ing examples are generated automatically from un-
labeled text. DSP allows an arbitrary combination of
features, including verb co-occurrence features that
yield high-quality similar-word lists as latent output.
This work only scratches the surface of possible fea-
ture mining; information from WordNet relations,
Wikipedia categories, or parallel corpora could also
provide valuable clues to SP. Also, if any other sys-
tem were to exceed DSP?s performance, it could also
be included as one of DSP?s features.
It would be interesting to expand our co-
occurrence features, including co-occurrence counts
across more grammatical relations and using counts
from external, unparsed corpora like the world wide
web. We could also reverse the role of noun and verb
in our training, having verb-specific features and
discriminating separately for each argument noun.
The latent information would then be lists of similar
nouns.
Finally, note that while we focused on word-word
co-occurrences, sense-sense SPs can also be learned
with our algorithm. If our training corpus was sense-
labeled, we could run our algorithm over the senses
rather than the words. The resulting model would
then require sense-tagged input if it were to be used
within an application like parsing or coreference res-
olution. Also, like other models of SP, our technique
can also be used for sense disambiguations: the
weightings on our semantic class features indicate,
for a particular noun, which of its senses (classes) is
most compatible with each verb.
Acknowledgments
We gratefully acknowledge support from the Natu-
ral Sciences and Engineering Research Council of
Canada, the Alberta Ingenuity Fund, and the Alberta
Informatics Circle of Research Excellence.
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
John Blitzer, Amir Globerson, and Fernando Pereira.
2005. Distributed latent variable models of lexical co-
occurrences. In AISTATS.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT, pages 92?100.
Carsten Brockmann and Mirella Lapata. 2003. Evalu-
ating and combining approaches to selectional prefer-
ence acquisition. In EACL, pages 27?34.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. TR-10-98, Harvard University.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
67
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Ido Dagan and Alan Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora references.
In COLING, volume 3, pages 330?332.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preference. In ACL, pages 216?223.
Yair Even-Zohar and Dan Roth. 2000. A classification
approach to word prediction. In NAACL, pages 124?
131.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In ACL, pages 1?7.
Donna Harman. 1992. The DARPA TIPSTER project.
ACM SIGIR Forum, 26(2):26?28.
Virginia M. Holmes, Laurie Stowe, and Linda Cupples.
1989. Lexical expectations in parsing complement-
verb sentences. Journal of Memory and Language,
28:668?689.
Thorsten Joachims. 1999. Making large-scale Support
Vector Machine learning practical. In B. Scho?lkopf
and C. Burges, editors, Advances in Kernel Methods:
Support Vector Machines, pages 169?184. MIT-Press.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. The (non)utility of predicate-
argument frequencies for pronoun interpretation. In
HLT/NAACL, pages 289?296.
Frank Keller and Mirella Lapata. 2003. Using the web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?773.
Dekang Lin. 1998b. Dependency-based evaluation of
MINIPAR. In LREC Workshop on the Evaluation of
Parsing Systems.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: an on-line lexical database.
International Journal of Lexicography, 3(4):235?244.
MUC-7. 1997. Coreference task definition (v3.0, 13 Jul
97). In Proceedings of the Seventh Message Under-
standing Conference (MUC-7).
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In ACL, pages 73?80.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In KDD, pages 613?619.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In
NAACL-HLT, pages 564?571.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational re-
alization. Cognition, 61:127?159.
Basili Roberto, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
RANLP.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In ACL,
pages 104?111.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data.
In ACL, pages 354?362.
Ellen Voorhees. 2002. Overview of the TREC 2002
question answering track. In Proceedings of the
Eleventh Text REtrieval Conference (TREC).
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2005.
Strictly lexical dependency parsing. In International
Workshop on Parsing Technologies, pages 152?159.
Julie Weeds and David Weir. 2005. Co-occurrence re-
trieval: a flexible framework for lexical distributional
similarity. Computational Linguistics, 31(4):439?475.
68
A Comparison of Syntactically Motivated Word Alignment Spaces
Colin Cherry
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
colinc@cs.ualberta.ca
Dekang Lin
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA, USA, 94043
lindek@google.com
Abstract
This work is concerned with the space of
alignments searched by word alignment
systems. We focus on situations where
word re-ordering is limited by syntax. We
present two new alignment spaces that
limit an ITG according to a given depen-
dency parse. We provide D-ITG grammars
to search these spaces completely and
without redundancy. We conduct a care-
ful comparison of five alignment spaces,
and show that limiting search with an ITG
reduces error rate by 10%, while a D-ITG
produces a 31% reduction.
1 Introduction
Bilingual word alignment finds word-level corre-
spondences between parallel sentences. The task
originally emerged as an intermediate result of
training the IBM translation models (Brown et
al., 1993). These models use minimal linguistic
intuitions; they essentially treat sentences as flat
strings. They remain the dominant method for
word alignment (Och and Ney, 2003). There have
been several proposals to introduce syntax into
word alignment. Some work within the framework
of synchronous grammars (Wu, 1997; Melamed,
2003), while others create a generative story that
includes a parse tree provided for one of the sen-
tences (Yamada and Knight, 2001).
There are three primary reasons to add syntax to
word alignment. First, one can incorporate syntac-
tic features, such as grammar productions, into the
models that guide the alignment search. Second,
movement can be modeled more naturally; when a
three-word noun phrase moves during translation,
it can be modeled as one movement operation in-
stead of three. Finally, one can restrict the type of
movement that is considered, shrinking the num-
ber of alignments that are attempted. We investi-
gate this last advantage of syntactic alignment. We
fix an alignment scoring model that works equally
well on flat strings as on parse trees, but we vary
the space of alignments evaluated with that model.
These spaces become smaller as more linguistic
guidance is added. We measure the benefits and
detriments of these constrained searches.
Several of the spaces we investigate draw guid-
ance from a dependency tree for one of the
sentences. We will refer to the parsed lan-
guage as English and the other as Foreign. Lin
and Cherry (2003) have shown that adding a
dependency-based cohesion constraint to an align-
ment search can improve alignment quality. Un-
fortunately, the usefulness of their beam search
solution is limited: potential alignments are con-
structed explicitly, which prevents a perfect search
of alignment space and the use of algorithms like
EM. However, the cohesion constraint is based
on a tree, which should make it amenable to dy-
namic programming solutions. To enable such
techniques, we bring the cohesion constraint in-
side the ITG framework (Wu, 1997).
Zhang and Gildea (2004) compared Yamada
and Knight?s (2001) tree-to-string alignment
model to ITGs. They concluded that methods like
ITGs, which create a tree during alignment, per-
form better than methods with a fixed tree estab-
lished before alignment begins. However, the use
of a fixed tree is not the only difference between
(Yamada and Knight, 2001) and ITGs; the proba-
bility models are also very different. By using a
fixed dependency tree inside an ITG, we can re-
visit the question of whether using a fixed tree is
harmful, but in a controlled environment.
2 Alignment Spaces
Let an alignment be the entire structure that con-
nects a sentence pair, and let a link be the in-
dividual word-to-word connections that make up
an alignment. An alignment space determines
the set of all possible alignments that can ex-
145
ist for a given sentence pair. Alignment spaces
can emerge from generative stories (Brown et al,
1993), from syntactic notions (Wu, 1997), or they
can be imposed to create competition between
links (Melamed, 2000). They can generally be de-
scribed in terms of how links interact.
For the sake of describing the size of alignment
spaces, we will assume that both sentences have n
tokens. The largest alignment space for a sentence
pair has 2n2 possible alignments. This describes
the case where each of the n2 potential links can
be either on or off with no restrictions.
2.1 Permutation Space
A straight-forward way to limit the space of pos-
sible alignments is to enforce a one-to-one con-
straint (Melamed, 2000). Under such a constraint,
each token in the sentence pair can participate in
at most one link. Each token in the English sen-
tence picks a token from the Foreign sentence to
link to, which is then removed from competition.
This allows for n! possible alignments1, a substan-
tial reduction from 2n2 .
Note that n! is also the number of possi-
ble permutations of the n tokens in either one
of the two sentences. Permutation space en-
forces the one-to-one constraint, but allows any re-
ordering of tokens as they are translated. Permu-
tation space methods include weighted maximum
matching (Taskar et al, 2005), and approxima-
tions to maximum matching like competitive link-
ing (Melamed, 2000). The IBM models (Brown
et al, 1993) search a version of permutation space
with a one-to-many constraint.
2.2 ITG Space
Inversion Transduction Grammars, or ITGs (Wu,
1997) provide an efficient formalism to syn-
chronously parse bitext. This produces a parse tree
that decomposes both sentences and also implies
a word alignment. ITGs are transduction gram-
mars because their terminal symbols can produce
tokens in both the English and Foreign sentences.
Inversions occur when the order of constituents is
reversed in one of the two sentences.
In this paper, we consider the alignment space
induced by parsing with a binary bracketing ITG,
such as:
A ? [AA] | ?AA? | e/f (1)
1This is a simplification that ignores null links. The actual
number of possible alignments lies between n! and (n+1)n.
The terminal symbol e/f represents tokens output
to the English and Foreign sentences respectively.
Square brackets indicate a straight combination of
non-terminals, while angle brackets indicate an in-
verted combination: ?A1A2?means that A1A2 ap-
pears in the English sentence, whileA2A1 appears
in the Foreign sentence.
Used as a word aligner, an ITG parser searches
a subspace of permutation space: the ITG requires
that any movement that occurs during translation
be explained by a binary tree with inversions.
Alignments that allow no phrases to be formed in
bitext are not attempted. This results in two for-
bidden alignment structures, shown in Figure 1,
called ?inside-out? transpositions in (Wu, 1997).
Note that no pair of contiguous tokens in the top
    
       
    
Figure 1: Forbidden alignments in ITG
sentence remain contiguous when projected onto
the bottom sentence. Zens and Ney (2003) explore
the re-orderings allowed by ITGs, and provide a
formulation for the number of structures that can
be built for a sentence pair of size n. ITGs explore
almost all of permutation space when n is small,
but their coverage of permutation space falls off
quickly for n > 5 (Wu, 1997).
2.3 Dependency Space
Dependency space defines the set of all align-
ments that maintain phrasal cohesion with respect
to a dependency tree provided for the English sen-
tence. The space is constrained so that the phrases
in the dependency tree always move together.
Fox (2002) introduced the notion of head-
modifier and modifier-modifier crossings. These
occur when a phrase?s image in the Foreign sen-
tence overlaps with the image of its head, or one of
its siblings. An alignment with no crossings main-
tains phrasal cohesion. Figure 2 shows a head-
modifier crossing: the image c of a head 2 overlaps
with the image (b, d) of 2?s modifier, (3, 4). Lin
 	 
 
   
Figure 2: A phrasal cohesion violation.
and Cherry (2003) used the notion of phrasal cohe-
146
sion to constrain a beam search aligner, conduct-
ing a heuristic search of the dependency space.
The number of alignments in dependency space
depends largely on the provided dependency tree.
Because all permutations of a head and its modi-
fiers are possible, a tree that has a single head with
n ? 1 modifiers provides no guidance; the align-
ment space is the same as permutation space. If
the tree is a chain (where every head has exactly
one modifier), alignment space has only 2n per-
mutations, which is by far the smallest space we
have seen. In general, there are
?
? [(m? + 1)!]
permutations for a given tree, where ? stands for a
head node in the tree, andm? counts ??s modifiers.
Dependency space is not a subspace of ITG space,
as it can create both the forbidden alignments in
Figure 1 when given a single-headed tree.
3 Dependency constrained ITG
In this section, we introduce a new alignment
space defined by a dependency constrained ITG,
or D-ITG. The set of possible alignments in this
space is the intersection of the dependency space
for a given dependency tree and ITG space. Our
goal is an alignment search that respects the
phrases specified by the dependency tree, but at-
tempts all ITG orderings of those phrases, rather
than all permutations. The intuition is that most
ordering decisions involve only a small number
of phrases, so the search should still cover a large
portion of dependency space.
This new space has several attractive computa-
tional properties. Since it is a subspace of ITG
space, we will be able to search the space com-
pletely using a polynomial time ITG parser. This
places an upper bound on the search complexity
equal to ITG complexity. This upper bound is
very loose, as the ITG will often be drastically
constrained by the phrasal structure of the depen-
dency tree. Also, by working in the ITG frame-
work, we will be able to take advantage of ad-
vances in ITG parsing, and we will have access
to the forward-backward algorithm to implicitly
count events over all alignments.
3.1 A simple solution
Wu (1997) suggests that in order to have an ITG
take advantage of a known partial structure, one
can simply stop the parser from using any spans
that would violate the structure. In a chart parsing
framework, this can be accomplished by assigning
the invalid spans a value of ?? before parsing
begins. Our English dependency tree qualifies as a
partial structure, as it does not specify a complete
binary decomposition of the English sentence. In
this case, any ITG span that would contain part,
but not all, of two adjacent dependency phrases
can be invalidated. The sentence pair can then be
parsed normally, automatically respecting phrases
specified by the dependency tree.
For example, Figure 3a shows an alignment for
the sentence pair, ?His house in Canada, Sa mai-
son au Canada? and the dependency tree provided
for the English sentence. The spans disallowed by
the tree are shown using underlines. Note that the
illegal spans are those that would break up the ?in
Canada? subtree. After invalidating these spans in
the chart, parsing the sentence pair with the brack-
eting ITG in (1) will produce the two structures
shown in Figure 3b, both of which correspond to
the correct alignment.
This solution is sufficient to create a D-ITG that
obeys the phrase structure specified by a depen-
dency tree. This allows us to conduct a complete
search of a well-defined subspace of the depen-
dency space described in Section 2.3.
3.2 Avoiding redundant derivations with a
recursive ITG
The above solution can derive two structures for
the same alignment. It is often desirable to
eliminate redundant structures when working with
ITGs. Having a single, canonical tree structure for
each possible alignment can help when flattening
binary trees, as it indicates arbitrary binarization
decisions (Wu, 1997). Canonical structures also
eliminate double counting when performing tasks
like EM (Zhang and Gildea, 2004). The nature of
null link handling in ITGs makes eliminating all
redundancies difficult, but we can at least elimi-
nate them in the absence of nulls.
Normally, one would eliminate the redundant
structures produced by the grammar in (1) by re-
placing it with the canonical form grammar (Wu,
1997), which has the following form:
S ? A | B | C
A ? [AB] | [BB] | [CB] |
[AC] | [BC] | [CC]
B ? ?AA? | ?BA? | ?CA? |
?AC? | ?BC? | ?CC?
C ? e/f
(2)
By design, this grammar allows only one struc-
147
                	 
  
            	 
   
                	                  	 
                	 


Figure 3: An example of how dependency trees interact with ITGs. (a) shows the input, dependency
tree, and alignment. Invalidated spans are underlined. (b) shows valid binary structures. (c) shows the
canonical ITG structure for this alignment.
         
       
  A Probabilistic Answer Type Model
Christopher Pinchak
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada
pinchak@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA
lindek@google.com
Abstract
All questions are implicitly associated
with an expected answer type. Unlike
previous approaches that require a prede-
fined set of question types, we present
a method for dynamically constructing
a probability-based answer type model
for each different question. Our model
evaluates the appropriateness of a poten-
tial answer by the probability that it fits
into the question contexts. Evaluation
is performed against manual and semi-
automatic methods using a fixed set of an-
swer labels. Results show our approach to
be superior for those questions classified
as having a miscellaneous answer type.
1 Introduction
Given a question, people are usually able to form
an expectation about the type of the answer, even
if they do not know the actual answer. An accu-
rate expectation of the answer type makes it much
easier to select the answer from a sentence that
contains the query words. Consider the question
?What is the capital of Norway?? We would ex-
pect the answer to be a city and could filter out
most of the words in the following sentence:
The landed aristocracy was virtually crushed
by Hakon V, who reigned from 1299 to 1319,
and Oslo became the capital of Norway, re-
placing Bergen as the principal city of the
kingdom.
The goal of answer typing is to determine
whether a word?s semantic type is appropriate as
an answer for a question. Many previous ap-
proaches to answer typing, e.g., (Ittycheriah et al,
2001; Li and Roth, 2002; Krishnan et al, 2005),
employ a predefined set of answer types and use
supervised learning or manually constructed rules
to classify a question according to expected an-
swer type. A disadvantage of this approach is that
there will always be questions whose answers do
not belong to any of the predefined types.
Consider the question: ?What are tourist attrac-
tions in Reims?? The answer may be many things:
a church, a historic residence, a park, a famous
intersection, a statue, etc. A common method to
deal with this problem is to define a catch-all class.
This class, however, tends not to be as effective as
other answer types.
Another disadvantage of predefined answer
types is with regard to granularity. If the types
are too specific, they are more difficult to tag. If
they are too general, too many candidates may be
identified as having the appropriate type.
In contrast to previous approaches that use a su-
pervised classifier to categorize questions into a
predefined set of types, we propose an unsuper-
vised method to dynamically construct a proba-
bilistic answer type model for each question. Such
a model can be used to evaluate whether or not
a word fits into the question context. For exam-
ple, given the question ?What are tourist attrac-
tions in Reims??, we would expect the appropriate
answers to fit into the context ?X is a tourist attrac-
tion.? From a corpus, we can find the words that
appeared in this context, such as:
A-Ama Temple, Aborigine, addition, Anak
Krakatau, archipelago, area, baseball,
Bletchley Park, brewery, cabaret, Cairo,
Cape Town, capital, center, ...
Using the frequency counts of these words in
the context, we construct a probabilistic model
to compute P (in(w,?)|w), the probability for a
word w to occur in a set of contexts ?, given an
occurrence of w. The parameters in this model are
obtained from a large, automatically parsed, un-
labeled corpus. By asking whether a word would
occur in a particular context extracted from a ques-
393
tion, we avoid explicitly specifying a list of pos-
sible answer types. This has the added benefit
of being easily adapted to different domains and
corpora in which a list of explicit possible answer
types may be difficult to enumerate and/or identify
within the text.
The remainder of this paper is organized as fol-
lows. Section 2 discusses the work related to an-
swer typing. Section 3 discusses some of the key
concepts employed by our probabilistic model, in-
cluding word clusters and the contexts of a ques-
tion and a word. Section 4 presents our probabilis-
tic model for answer typing. Section 5 compares
the performance of our model with that of an or-
acle and a semi-automatic system performing the
same task. Finally, the concluding remarks in are
made in Section 6.
2 Related Work
Light et al (2001) performed an analysis of the
effect of multiple answer type occurrences in a
sentence. When multiple words of the same type
appear in a sentence, answer typing with fixed
types must assign each the same score. Light et
al. found that even with perfect answer sentence
identification, question typing, and semantic tag-
ging, a system could only achieve 59% accuracy
over the TREC-9 questions when using their set of
24 non-overlapping answer types. By computing
the probability of an answer candidate occurring
in the question contexts directly, we avoid having
multiple candidates with the same level of appro-
priateness as answers.
There have been a variety of approaches to de-
termine the answer types, which are also known
as Qtargets (Echihabi et al, 2003). Most previous
approaches classify the answer type of a question
as one of a set of predefined types.
Many systems construct the classification rules
manually (Cui et al, 2004; Greenwood, 2004;
Hermjakob, 2001). The rules are usually triggered
by the presence of certain words in the question.
For example, if a question contains ?author? then
the expected answer type is Person.
The number of answer types as well as the num-
ber of rules can vary a great deal. For example,
(Hermjakob, 2001) used 276 rules for 122 answer
types. Greenwood (2004), on the other hand, used
46 answer types with unspecified number of rules.
The classification rules can also be acquired
with supervised learning. Ittycheriah, et al (2001)
describe a maximum entropy based question clas-
sification scheme to classify each question as hav-
ing one of the MUC answer types. In a similar ex-
periment, Li & Roth (2002) train a question clas-
sifier based on a modified version of SNoW using
a richer set of answer types than Ittycheriah et al
The LCC system (Harabagiu et al, 2003) com-
bines fixed types with a novel loop-back strategy.
In the event that a question cannot be classified as
one of the fixed entity types or semantic concepts
derived from WordNet (Fellbaum, 1998), the an-
swer type model backs off to a logic prover that
uses axioms derived form WordNet, along with
logic rules, to justify phrases as answers. Thus, the
LCC system is able to avoid the use of a miscel-
laneous type that often exhibits poor performance.
However, the logic prover must have sufficient ev-
idence to link the question to the answer, and gen-
eral knowledge must be encoded as axioms into
the system. In contrast, our answer type model
derives all of its information automatically from
unannotated text.
Answer types are often used as filters. It was
noted in (Radev et al, 2002) that a wrong guess
about the answer type reduces the chance for the
system to answer the question correctly by as
much as 17 times. The approach presented here
is less brittle. Even if the correct candidate does
not have the highest likelihood according to the
model, it may still be selected when the answer
extraction module takes into account other factors
such as the proximity to the matched keywords.
Furthermore, a probabilistic model makes it eas-
ier to integrate the answer type scores with scores
computed by other components in a question an-
swering system in a principled fashion.
3 Resources
Before introducing our model, we first describe
the resources used in the model.
3.1 Word Clusters
Natural language data is extremely sparse. Word
clusters are a way of coping with data sparseness
by abstracting a given word to a class of related
words. Clusters, as used by our probabilistic an-
swer typing system, play a role similar to that of
named entity types. Many methods exist for clus-
tering, e.g., (Brown et al, 1990; Cutting et al,
1992; Pereira et al, 1993; Karypis et al, 1999).
We used the Clustering By Committee (CBC)
394
Table 1: Words and their clusters
Word Clusters
suite software, network, wireless, ...
rooms, bathrooms, restrooms, ...
meeting room, conference room, ...
ghost rabbit, squirrel, duck, elephant, frog, ...
goblins, ghosts, vampires, ghouls, ...
punk, reggae, folk, pop, hip-pop, ...
huge, larger, vast, significant, ...
coming-of-age, true-life, ...
clouds, cloud, fog, haze, mist, ...
algorithm (Pantel and Lin, 2002) on a 10 GB En-
glish text corpus to obtain 3607 clusters. The fol-
lowing is an example cluster generated by CBC:
tension, anger, anxiety, tensions, frustration,
resentment, uncertainty, confusion, conflict,
discontent, insecurity, controversy, unease,
bitterness, dispute, disagreement, nervous-
ness, sadness, despair, animosity, hostility,
outrage, discord, pessimism, anguish, ...
In the clustering generated by CBC, a word may
belong to multiple clusters. The clusters to which
a word belongs often represent the senses of the
word. Table 1 shows two example words and their
clusters.
3.2 Contexts
The context in which a word appears often im-
poses constraints on the semantic type of the word.
This basic idea has been exploited by many pro-
posals for distributional similarity and clustering,
e.g., (Church and Hanks, 1989; Lin, 1998; Pereira
et al, 1993).
Similar to Lin and Pantel (2001), we define
the contexts of a word to be the undirected paths
in dependency trees involving that word at either
the beginning or the end. The following diagram
shows an example dependency tree:
Which city hosted the 1988 Winter Olympics?
det subj
obj
NN
NN
det
The links in the tree represent dependency rela-
tionships. The direction of a link is from the head
to the modifier in the relationship. Labels associ-
ated with the links represent types of relations.
In a context, the word itself is replaced with a
variable X. We say a word is the filler of a context
if it replaces X. For example, the contexts for the
word ?Olympics? in the above sentence include
the following paths:
Context of ?Olympics? Explanation
X Winter
NN
Winter X
X 1988
NN
1988 X
X host
obj
host X
X host
obj
city
subj
city hosted X
In these paths, words are reduced to their root
forms and proper names are reduced to their entity
tags (we used MUC7 named entity tags).
Paths allow us to balance the specificity of con-
texts and the sparseness of data. Longer paths typ-
ically impose stricter constraints on the slot fillers.
However, they tend to have fewer occurrences,
making them more prone to errors arising from
data sparseness. We have restricted the path length
to two (involving at most three words) and require
the two ends of the path to be nouns.
We parsed the AQUAINT corpus (3GB) with
Minipar (Lin, 2001) and collected the frequency
counts of words appearing in various contexts.
Parsing and database construction is performed
off-line as the database is identical for all ques-
tions. We extracted 527,768 contexts that ap-
peared at least 25 times in the corpus. An example
context and its fillers are shown in Figure 1.
X host Olympics
subj obj
Africa 2 grant 1 readiness 2
AP 1 he 2 Rio de Janeiro 1
Argentina 1 homeland 3 Rome 1
Athens 16 IOC 1 Salt Lake City 2
Atlanta 3 Iran 2 school 1
Bangkok 1 Jakarta 1 S. Africa 1
. . . . . . . . .
decades 1 president 2 Zakopane 4
facility 1 Pusan 1
government 1 race 1
Figure 1: An example context and its fillers
3.2.1 Question Contexts
To build a probabilistic model for answer typ-
ing, we extract a set of contexts, called question
contexts, from a question. An answer is expected
to be a plausible filler of the question contexts.
Question contexts are extracted from a question
with two rules. First, if the wh-word in a ques-
tion has a trace in the parse tree, the question con-
texts are the contexts of the trace. For example, the
395
question ?What do most tourists visit in Reims??
is parsed as:
Whati do most tourists visit ei in Reims?
det
i
subjdet obj
in
The symbol ei is the trace of whati. Minipar
generates the trace to indicate that the word what
is the object of visit in the deep structure of the
sentence. The following question contexts are ex-
tracted from the above question:
Context Explanation
X visit tourist
obj subj
tourist visits X
X visit Reims
obj in
visit X in Reims
The second rule deals with situations where
the wh-word is a determiner, as in the question
?Which city hosted the 1988 Winter Olympics??
(the parse tree for which is shown in section 3.2).
In such cases, the question contexts consist of a
single context involving the noun that is modified
by the determiner. The context for the above sen-
tence is X city
subj
, corresponding to the sentence
?X is a city.? This context is used because the
question explicitly states that the desired answer is
a city. The context overrides the other contexts be-
cause the question explicitly states the desired an-
swer type. Experimental results have shown that
using this context in conjunction with other con-
texts extracted from the question produces lower
performance than using this context alone.
In the event that a context extracted from a ques-
tion is not found in the database, we shorten the
context in one of two ways. We start by replac-
ing the word at the end of the path with a wildcard
that matches any word. If this fails to yield en-
tries in the context database, we shorten the con-
text to length one and replace the end word with
automatically determined similar words instead of
a wildcard.
3.2.2 Candidate Contexts
Candidate contexts are very similar in form to
question contexts, save for one important differ-
ence. Candidate contexts are extracted from the
parse trees of the answer candidates rather than the
question. In natural language, some words may
be polysemous. For example, Washington may re-
fer to a person, a city, or a state. The occurrences
of Washington in ?Washington?s descendants? and
?suburban Washington? should not be given the
same score when the question is seeking a loca-
tion. Given that the sense of a word is largely de-
termined by its local context (Choueka and Lusig-
nan, 1985), candidate contexts allow the model to
take into account the candidate answers? senses
implicitly.
4 Probabilistic Model
The goal of an answer typing model is to evalu-
ate the appropriateness of a candidate word as an
answer to the question. If we assume that a set
of answer candidates is provided to our model by
some means (e.g., words comprising documents
extracted by an information retrieval engine), we
wish to compute the value P (in(w,?Q)|w). That
is, the appropriateness of a candidate answer w is
proportional to the probability that it will occur in
the question contexts ?Q extracted from the ques-
tion.
To mitigate data sparseness, we can introduce
a hidden variable C that represents the clusters to
which the candidate answer may belong. As a can-
didate may belong to multiple clusters, we obtain:
P (in(w,?Q)|w) =
X
C
P (in(w,?Q), C|w) (1)
=
X
C
P (C|w)P (in(w,?Q)|C,w) (2)
Given that a word appears, we assume that it has
the same probability to appear in a context as all
other words in the same cluster. Therefore:
P (in(w,?Q)|C,w) ? P (in(C,?Q)|C) (3)
We can now rewrite the equation in (2) as:
P (in(w,?Q)|w) ?
X
C
P (C|w)P (in(C,?Q)|C) (4)
This equation splits our model into two parts:
one models which clusters a word belongs to and
the other models how appropriate a cluster is to
the question contexts. When ?Q consists of multi-
ple contexts, we make the na??ve Bayes assumption
that each individual context ?Q ? ?Q is indepen-
dent of all other contexts given the cluster C.
P (in(w,?Q)|w) ?
X
C
P (C|w)
Y
?Q??Q
P (in(C, ?Q)|C) (5)
Equation (5) needs the parameters P (C|w) and
P (in(C, ?Q)|C), neither of which are directly
available from the context-filler database. We will
discuss the estimation of these parameters in Sec-
tion 4.2.
396
4.1 Using Candidate Contexts
The previous model assigns the same likelihood to
every instance of a given word. As we noted in
section 3.2.2, a word may be polysemous. To take
into account a word?s context, we can instead com-
pute P (in(w,?Q)|w, in(w,?w)), where ?w is the
set of contexts for the candidate word w in a re-
trieved passage.
By introducing word clusters as intermediate
variables as before and making a similar assump-
tion as in equation (3), we obtain:
P (in(w,?Q)|w, in(w,?w))
=
X
C
P (in(w,?Q), C|w, in(w,?w)) (6)
?
X
C
P (C|w, in(w,?w))P (in(C,?Q)|C) (7)
Like equation (4), equation (7) partitions the
model into two parts. Unlike P (C|w) in equation
(4), the probability of the cluster is now based on
the particular occurrence of the word in the candi-
date contexts. It can be estimated by:
P (C|w, in(w,?w))
= P (in(w,?w)|w,C)P (w,C)P (in(w,?w)|w)P (w)
(8)
?
Y
?w??w
P (in(w, ?w)|w,C)
Y
?w??w
P (in(w, ?w)|w)
? P (C|w) (9)
=
Y
?w??w
?
P (C|w, in(w, ?w))
P (C|w)
?
? P (C|w) (10)
4.2 Estimating Parameters
Our probabilistic model requires the parameters
P (C|w), P (C|w, in(w, ?)), and P (in(C, ?)|C),
wherew is a word,C is a cluster thatw belongs to,
and ? is a question or candidate context. This sec-
tion explains how these parameters are estimated
without using labeled data.
The context-filler database described in Sec-
tion 3.2 provides the joint and marginal fre-
quency counts of contexts and words (|in(?, w)|,
|in(?, ?)| and |in(w, ?)|). These counts al-
low us to compute the probabilities P (in(w, ?)),
P (in(w, ?)), and P (in(?, ?)). We can also com-
pute P (in(w, ?)|w), which is smoothed with add-
one smoothing (see equation (11) in Figure 2).
The estimation of P (C|w) presents a challenge.
We have no corpus from which we can directly
measure P (C|w) because word instances are not
labeled with their clusters.
P (in(w, ?)|w) = |in(w, ?)|+ P (in(?, ?))|in(w, ?)|+ 1 (11)
Pu(C|w) =
(
1
|{C?|w?C?}| if w ? C,
0 otherwise (12)
P (C|w) =
X
w??S(w)
sim(w,w?)? Pu(C|w?)
X
{C?|w?C?},
w??S(w)
sim(w,w?)? Pu(C?|w?)
(13)
P (in(C, ?)|C) =
X
w??C
P (C|w?)? |in(w?, ?)|+ P (in(?, ?))
X
w??C
P (C|w?)? |in(w?, ?)|+ 1
(14)
Figure 2: Probability estimation
We use the average weighted ?guesses? of the
top similar words of w to compute P (C|w) (see
equation 13). The intuition is that if w? and w
are similar words, P (C|w?) and P (C|w) tend
to have similar values. Since we do not know
P (C|w?) either, we substitute it with uniform dis-
tribution Pu(C|w?) as in equation (12) of Fig-
ure 2. Although Pu(C|w?) is a very crude guess,
the weighted average of a set of such guesses can
often be quite accurate.
The similarities between words are obtained as
a byproduct of the CBC algorithm. For each word,
we use S(w) to denote the top-n most similar
words (n=50 in our experiments) and sim(w,w?)
to denote the similarity between words w and w?.
The following is a sample similar word list for the
word suit:
S(suit) = {lawsuit 0.49, suits 0.47, com-
plaint 0.29, lawsuits 0.27, jacket 0.25, coun-
tersuit 0.24, counterclaim 0.24, pants 0.24,
trousers 0.22, shirt 0.21, slacks 0.21, case
0.21, pantsuit 0.21, shirts 0.20, sweater 0.20,
coat 0.20, ...}
The estimation for P (C|w, in(w, ?w)) is sim-
ilar to that of P (C|w) except that instead of all
w? ? S(w), we instead use {w?|w? ? S(w) ?
in(w?, ?w)}. By only looking at a particular con-
text ?w, we may obtain a different distribution over
C than P (C|w) specifies. In the event that the data
are too sparse to estimate P (C|w, in(w, ?w)), we
fall back to using P (C|w).
P (in(C, ?)|C) is computed in (14) by assum-
ing each instance of w contains a fractional in-
stance of C and the fractional count is P (C|w).
Again, add-one smoothing is used.
397
System Median % Top 1% Top 5% Top 10% Top 50%
Oracle 0.7% 89 (57%) 123 (79%) 131 (85%) 154 (99%)
Frequency 7.7% 31 (20%) 67 (44%) 86 (56%) 112 (73%)
Our model 1.2% 71 (46%) 106 (69%) 119 (77%) 146 (95%)
no cand. contexts 2.2% 58 (38%) 102 (66%) 113 (73%) 145 (94%)
ANNIE 4.0% 54 (35%) 79 (51%) 93 (60%) 123 (80%)
Table 2: Summary of Results
5 Experimental Setup & Results
We evaluate our answer typing system by using
it to filter the contents of documents retrieved by
the information retrieval portion of a question an-
swering system. Each answer candidate in the set
of documents is scored by the answer typing sys-
tem and the list is sorted in descending order of
score. We treat the system as a filter and observe
the proportion of candidates that must be accepted
by the filter so that at least one correct answer is
accepted. A model that allows a low percentage
of candidates to pass while still allowing at least
one correct answer through is favorable to a model
in which a high number of candidates must pass.
This represents an intrinsic rather than extrinsic
evaluation (Molla? and Hutchinson, 2003) that we
believe illustrates the usefulness of our model.
The evaluation data consist of 154 questions
from the TREC-2003 QA Track (Voorhees, 2003)
satisfying the following criteria, along with the top
10 documents returned for each question as iden-
tified by NIST using the PRISE1 search engine.
? the question begins with What, Which, or
Who. We restricted the evaluation such ques-
tions because our system is designed to deal
with questions whose answer types are often
semantically open-ended noun phrases.
? There exists entry for the question in the an-
swer patterns provided by Ken Litkowski2.
? One of the top-10 documents returned by
PRISE contains a correct answer.
We compare the performance of our prob-
abilistic model with that of two other sys-
tems. Both comparison systems make use of a
small, predefined set of manually-assigned MUC-
7 named-entity types (location, person, organiza-
tion, cardinal, percent, date, time, duration, mea-
sure, money) augmented with thing-name (proper
1www.itl.nist.gov/iad/894.02/works/papers/zp2/zp2.html
2trec.nist.gov/data/qa/2003 qadata/03QA.tasks/t12.pats.txt
names of inanimate objects) and miscellaneous
(a catch-all answer type of all other candidates).
Some examples of thing-name are Guinness Book
of World Records, Thriller, Mars Pathfinder, and
Grey Cup. Examples of miscellaneous answers are
copper, oil, red, and iris.
The differences in the comparison systems is
with respect to how entity types are assigned to the
words in the candidate documents. We make use
of the ANNIE (Maynard et al, 2002) named entity
recognition system, along with a manual assigned
?oracle? strategy, to assign types to candidate an-
swers. In each case, the score for a candidate is
either 1 if it is tagged as the same type as the ques-
tion or 0 otherwise. With this scoring scheme pro-
ducing a sorted list we can compute the probability
of the first correct answer appearing at rankR = k
as follows:
P (R = k) =
k?2
Y
i=0
?
t? c? i
t? i
?
c
t? k + 1 (15)
where t is the number of unique candidate answers
that are of the appropriate type and c is the number
of unique candidate answers that are correct.
Using the probabilities in equation (15), we
compute the expected rank, E(R), of the first cor-
rect answer of a given question in the system as:
E(R) =
t?c+1
X
k=1
kP (R = k) (16)
Answer candidates are the set of ANNIE-
identified tokens with stop words and punctuation
removed. This yields between 900 and 8000 can-
didates for each question, depending on the top 10
documents returned by PRISE. The oracle system
represents an upper bound on using the predefined
set of answer types. The ANNIE system repre-
sents a more realistic expectation of performance.
The median percentage of candidates that are
accepted by a filter over the questions of our eval-
uation data provides one measure of performance
and is preferred to the average because of the ef-
fect of large values on the average. In QA, a sys-
tem accepting 60% of the candidates is not signif-
icantly better or worse than one accepting 100%,
398
System Measure
Question Type
All Location Person Organization Thing-Name Misc Other
(154) (57) (17) (19) (17) (37) (7)
Our model
Median 1.2% 0.8% 2.0% 1.3% 3.7% 3.5% 12.2%
Top 1% 71 34 6 9 7 13 2
Top 5% 106 53 11 11 10 19 2
Top 10% 119 55 12 17 10 22 3
Top 50% 146 56 16 18 17 34 5
Oracle
Median 0.7% 0.4% 1.0% 0.3% 0.4% 16.0% 0.3%
Top 1% 89 44 8 16 14 1 6
Top 5% 123 57 17 19 17 6 7
Top 10% 131 57 17 19 17 14 7
Top 50% 154 57 17 19 17 37 7
ANNIE
Median 4.0% 0.6% 1.4% 6.1% 100% 16.7% 50.0%
Top 1% 54 39 5 7 0 0 3
Top 5% 79 53 12 9 0 2 3
Top 10% 93 54 13 11 0 12 3
Top 50% 123 56 16 15 5 28 3
Table 3: Detailed breakdown of performance
but the effect on average is quite high. Another
measure is to observe the number of questions
with at least one correct answer in the top N% for
various values of N . By examining the number of
correct answers found in the topN%we can better
understand what an effective cutoff would be.
The overall results of our comparison can be
found in Table 2. We have added the results of
a system that scores candidates based on their fre-
quency within the document as a comparison with
a simple, yet effective, strategy. The second col-
umn is the median percentage of where the highest
scored correct answer appears in the sorted candi-
date list. Low percentage values mean the answer
is usually found high in the sorted list. The re-
maining columns list the number of questions that
have a correct answer somewhere in the top N%
of their sorted lists. This is meant to show the ef-
fects of imposing a strict cutoff prior to running
the answer type model.
The oracle system performs best, as it bene-
fits from both manual question classification and
manual entity tagging. If entity assignment is
performed by an automatic system (as it is for
ANNIE), the performance drops noticeably. Our
probabilistic model performs better than ANNIE
and achieves approximately 2/3 of the perfor-
mance of the oracle system. Table 2 also shows
that the use of candidate contexts increases the
performance of our answer type model.
Table 3 shows the performance of the oracle
system, our model, and the ANNIE system broken
down by manually-assigned answer types. Due
to insufficient numbers of questions, the cardinal,
percent, time, duration, measure, and money types
are combined into an ?Other? category. When
compared with the oracle system, our model per-
forms worse overall for questions of all types ex-
cept for those seeking miscellaneous answers. For
miscellaneous questions, the oracle identifies all
tokens that do not belong to one of the other
known categories as possible answers. For all
questions of non-miscellaneous type, only a small
subset of the candidates are marked appropriate.
In particular, our model performs worse than the
oracle for questions seeking persons and thing-
names. Person questions often seek rare person
names, which occur in few contexts and are diffi-
cult to reliably cluster. Thing-name questions are
easy for a human to identify but difficult for au-
tomatic system to identify. Thing-names are a di-
verse category and are not strongly associated with
any identifying contexts.
Our model outperforms the ANNIE system in
general, and for questions seeking organizations,
thing-names, and miscellaneous targets in partic-
ular. ANNIE may have low coverage on organi-
zation names, resulting in reduced performance.
Like the oracle, ANNIE treats all candidates not
assigned one of the categories as appropriate for
miscellaneous questions. Because ANNIE cannot
identify thing-names, they are treated as miscella-
neous. ANNIE shows low performance on thing-
names because words incorrectly assigned types
are sorted to the bottom of the list for miscella-
neous and thing-name questions. If a correct an-
swer is incorrectly assigned a type it will be sorted
near the bottom, resulting in a poor score.
399
6 Conclusions
We have presented an unsupervised probabilistic
answer type model. Our model uses contexts de-
rived from the question and the candidate answer
to calculate the appropriateness of a candidate an-
swer. Statistics gathered from a large corpus of
text are used in the calculation, and the model is
constructed to exploit these statistics without be-
ing overly specific or overly general.
The method presented here avoids the use of an
explicit list of answer types. Explicit answer types
can exhibit poor performance, especially for those
questions not fitting one of the types. They must
also be redefined when either the domain or corpus
substantially changes. By avoiding their use, our
answer typing method may be easier to adapt to
different corpora and question answering domains
(such as bioinformatics).
In addition to operating as a stand-alone answer
typing component, our system can be combined
with other existing answer typing strategies, es-
pecially in situations in which a catch-all answer
type is used. Our experimental results show that
our probabilistic model outperforms the oracle and
a system using automatic named entity recognition
under such circumstances. The performance of
our model is better than that of the semi-automatic
system, which is a better indication of the expected
performance of a comparable real-world answer
typing system.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their helpful comments on improving
the paper. The first author is supported by the Nat-
ural Sciences and Engineering Research Council
of Canada, the Alberta Ingenuity Fund, and the Al-
berta Informatics Circle of Research Excellence.
References
P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai, and R.L.
Mercer. 1990. Class-based n-gram Models of Natural
Language. Computational Linguistics, 16(2):79?85.
Y. Choueka and S. Lusignan. 1985. Disambiguation by Short
Contexts. Computer and the Humanities, 19:147?157.
K. Church and P. Hanks. 1989. Word Association Norms,
Mutual Information, and Lexicography. In Proceedings
of ACL-89, pages 76?83, Vancouver, British Columbia,
Canada.
H. Cui, K. Li, R. Sun, T-S. Chua, and M-K. Kan. 2004. Na-
tional University of Singapore at the TREC-13 Question
Answering Main Task. In Notebook of TREC 2004, pages
34?42, Gaithersburg, Maryland.
D.R. Cutting, D. Karger, J. Pedersen, and J.W. Tukey. 1992.
Scatter/Gather: A Cluster-based Approach to Browsing
Large Document Collections. In Proceedings of SIGIR-
92, pages 318?329, Copenhagen, Denmark.
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. Melz,
and D. Ravichandran. 2003. Multiple-Engine Question
Answering in TextMap. In Proceedings of TREC 2003,
pages 772?781, Gaithersburg, Maryland.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, Massachusetts.
M.A. Greenwood. 2004. AnswerFinder: Question Answer-
ing from your Desktop. In Proceedings of the Seventh
Annual Colloquium for the UK Special Interest Group
for Computational Linguistics (CLUK ?04), University of
Birmingham, UK.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden,
J. Williams, and J. Bensley. 2003. Answer Mining by
Combining Extraction Techniques with Abductive Rea-
soning. In Proceedings of TREC 2003, pages 375?382,
Gaithersburg, Maryland.
U. Hermjakob. 2001. Parsing and Question Classification for
Question Answering. In Proceedings of the ACL Work-
shop on Open-Domain Question Answering, Toulouse,
France.
A. Ittycheriah, M. Franz, W-J. Zhu, and A. Ratnaparkhi.
2001. Question Answering Using Maximum Entropy
Components. In Proceedings of NAACL 2001, Pittsburgh,
Pennsylvania.
G. Karypis, E.-H. Han, and V. Kumar. 1999. Chameleon: A
Hierarchical Clustering Algorithm using Dynamic Model-
ing. IEEE Computer: Special Issue on Data Analysis and
Mining, 32(8):68?75.
V. Krishnan, S. Das, and S. Chakrabarti. 2005. Enhanced
Answer Type Inference from Questions using Sequential
Models. In Proceedings of HLT/EMNLP 2005, pages
315?322, Vancouver, British Columbia, Canada.
X. Li and D. Roth. 2002. Learning Question Classifiers.
In Proceedings of COLING 2002, pages 556?562, Taipei,
Taiwan.
M. Light, G. Mann, E. Riloff, and E. Breck. 2001. Analyses
for Elucidating Current Question Answering Technology.
Natural Language Engineering, 7(4):325?342.
D. Lin and P. Pantel. 2001. Discovery of Inference Rules
for Question Answering. Natural Language Engineering,
7(4):343?360.
D. Lin. 1998. Automatic Retrieval and Clustering of Similar
Words. In Proceedings of COLING-ACL 1998, Montreal,
Que?bec, Canada.
D. Lin. 2001. Language and Text Analysis Tools. In Pro-
ceedings of HLT 2001, pages 222?227, San Diego, Cali-
fornia.
D. Maynard, V. Tablan, H. Cunningham, C. Ursu, H. Sag-
gion, K. Bontcheva, and Y. Wilks. 2002. Architectural
Elements of Language Engineering Robustness. Natural
Language Engineering, 8(2/3):257?274.
D. Molla? and B. Hutchinson. 2003. Intrinsic versus Extrinsic
Evaluations of Parsing Systems. In Proceedings of EACL
Workshop on Evaluation Initiatives in Natural Language
Processing, pages 43?50, Budapest, Hungary.
P. Pantel and D. Lin. 2002. Document Clustering with Com-
mittees. In Proceedings of SIGIR 2002, pages 199?206,
Tampere, Finland.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional Clus-
tering of English Words. In Proceedings of ACL 1992,
pages 183?190.
D. Radev, W. Fan, H. Qi, H. Wu, and A. Grewal. 2002. Prob-
ablistic Question Answering on the Web. In Proceedings
of the Eleventh International World Wide Web Conference.
E.M. Voorhees. 2003. Overview of the TREC 2003 Ques-
tion Answering Track. In Proceedings of TREC 2003,
Gaithersburg, Maryland.
400
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 666?674,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Flexible Answer Typing with Discriminative Preference Ranking
Christopher Pinchak? Dekang Lin? Davood Rafiei?
?Department of Computing Science ?Google Inc.
University of Alberta 1600 Amphitheatre Parkway
Edmonton, Alberta, Canada Mountain View, CA, USA
{pinchak,drafiei}@cs.ualberta.ca lindek@google.com
Abstract
An important part of question answering
is ensuring a candidate answer is plausi-
ble as a response. We present a flexible
approach based on discriminative prefer-
ence ranking to determine which of a set
of candidate answers are appropriate. Dis-
criminative methods provide superior per-
formance while at the same time allow the
flexibility of adding new and diverse fea-
tures. Experimental results on a set of fo-
cused What ...? and Which ...? questions
show that our learned preference ranking
methods perform better than alternative
solutions to the task of answer typing. A
gain of almost 0.2 in MRR for both the
first appropriate and first correct answers
is observed along with an increase in pre-
cision over the entire range of recall.
1 Introduction
Question answering (QA) systems have received a
great deal of attention because they provide both
a natural means of querying via questions and be-
cause they return short, concise answers. These
two advantages simplify the task of finding in-
formation relevant to a topic of interest. Ques-
tions convey more than simply a natural language
query; an implicit expectation of answer type is
provided along with the question words. The dis-
covery and exploitation of this implicit expected
type is called answer typing.
We introduce an answer typing method that is
sufficiently flexible to use a wide variety of fea-
tures while at the same time providing a high level
of performance. Our answer typing method avoids
the use of pre-determined classes that are often
lacking for unanticipated answer types. Because
answer typing is only part of the QA task, a flexi-
ble answer typing model ensures that answer typ-
ing can be easily and usefully incorporated into a
complete QA system. A discriminative preference
ranking model with a preference for appropriate
answers is trained and applied to unseen ques-
tions. In terms of Mean Reciprocal Rank (MRR),
we observe improvements over existing systems of
around 0.2 both in terms of the correct answer and
in terms of appropriate responses. This increase
in MRR brings the performance of our model to
near the level of a full QA system on a subset of
questions, despite the fact that we rely on answer
typing features alone.
The amount of information given about the ex-
pected answer can vary by question. If the ques-
tion contains a question focus, which we define
to be the head noun following the wh-word such
as city in ?What city hosted the 1988 Winter
Olympics??, some of the typing information is ex-
plicitly stated. In this instance, the answer is re-
quired to be a city. However, there is often addi-
tional information available about the type. In our
example, the answer must plausibly host a Winter
Olympic Games. The focus, along with the ad-
ditional information, give strong clues about what
are appropriate as responses.
We define an appropriate candidate answer as
one that a user, who does not necessarily know
the correct answer, would identify as a plausible
answer to a given question. For most questions,
there exist plausible responses that are not correct
answers to the question. For our above question,
the city of Vancouver is plausible even though it
is not correct. For the purposes of this paper, we
assume correct answers are a subset of appropri-
ate candidates. Because answer typing is only in-
tended to be a component of a full QA system, we
rely on other components to help establish the true
correctness of a candidate answer.
The remainder of the paper is organized as fol-
lows. Section 2 presents the application of dis-
criminative preference rank learning to answer
typing. Section 3 introduces the models we use
666
for learning appropriate answer preferences. Sec-
tions 4 and 5 discuss our experiments and their re-
sults, respectively. Section 6 presents prior work
on answer typing and the use of discriminative
methods in QA. Finally, concluding remarks and
ideas for future work are presented in Section 7.
2 Preference Ranking
Preference ranking naturally lends itself to any
problem in which the relative ordering between
examples is more important than labels or values
assigned to those examples. The classic exam-
ple application of preference ranking (Joachims,
2002) is that of information retrieval results rank-
ing. Generally, information retrieval results are
presented in some ordering such that those higher
on the list are either more relevant to the query or
would be of greater interest to the user.
In a preference ranking task we have a set of
candidates c1, c2, ..., cn, and a ranking r such that
the relation ci <r cj holds if and only if can-
didate ci should be ranked higher than cj , for
1 ? i, j ? n and i 6= j. The ranking r can form
a total ordering, as in information retrieval, or a
partial ordering in which we have both ci ?r cj
and cj ?r ci. Partial orderings are useful for our
task of answer typing because they can be used to
specify candidates that are of an equivalent rank.
Given some ci <r cj , preference ranking only
considers the difference between the feature rep-
resentations of ci and cj (?(ci) and ?(cj), respec-
tively) as evidence. We want to learn some weight
vector ~w such that ~w ??(ci) > ~w ??(cj) holds for
all pairs ci and cj that have the relation ci <r cj . In
other words, we want ~w ? (?(ci)??(cj)) > 0 and
we can use some margin in the place of 0. In the
context of Support Vector Machines (Joachims,
2002), we are trying to minimize the function:
V (~w, ~?) =
1
2
~w ? ~w + C
?
?i,j (1)
subject to the constraints:
?(ci <r cj) : ~w ? (?(ci)? ?(cj)) ? 1? ?i,j (2)
?i, j : ?i,j ? 0 (3)
The margin incorporates slack variables ?i,j for
problems that are not linearly separable. This
ranking task is analogous to the SVM classi-
fication task on the pairwise difference vectors
(?(ci) ? ?(cj)), known as rank constraints. Un-
like classification, no explicit negative evidence is
required as ~w?(?(ci)??(cj)) = (?1)~w?(?(cj)?
?(ci)). It is also important to note that no rank
constraints are generated for candidates for which
no order relation exists under the ranking r.
Support Vector Machines (SVMs) have previ-
ously been used for preference ranking in the
context of information retrieval (Joachims, 2002).
We adopt the same framework for answer typing
by preference ranking. The SVMlight package
(Joachims, 1999) implements the preference rank-
ing of Joachims (2002) and is used here for learn-
ing answer types.
2.1 Application to Answer Typing
Assigning meaningful scores for answer typing is
a difficult task. For example, given the question
?What city hosted the 1988 Winter Olympics??
and the candidates New York, Calgary, and the
word blue, how can we identify New York and
Calgary as appropriate and the word blue as inap-
propriate? Scoring answer candidates is compli-
cated by the fact that a gold standard for appropri-
ateness scores does not exist. Therefore, we have
no a priori notion that New York is better than the
word blue by some amount v. Because of this, we
approach the problem of answer typing as one of
preference ranking in which the relative appropri-
ateness is more important than the absolute scores.
Preference ranking stands in contrast to classifi-
cation, in which a candidate is classified as appro-
priate or inappropriate depending on the values in
its feature representation. Unfortunately, simple
classification does not work well in the face of a
large imbalance in positive and negative examples.
In answer typing we typically have far more inap-
propriate candidates than appropriate candidates,
and this is especially true for the experiments de-
scribed in Section 4. This is indeed a problem for
our system, as neither re-weighting nor attempt-
ing to balance the set of examples with the use
of random negative examples were shown to give
better performance on development data. This is
not to say that some means of balancing the data
would not provide comparable or superior perfor-
mance, but rather that such a weighting or sam-
pling scheme is not obvious.
An additional benefit of preference ranking over
classification is that preference ranking models the
better-than relationship between candidates. Typ-
ically a set of candidate answers are all related to a
question in some way, and we wish to know which
667
of the candidates are better than others. In con-
trast, binary classification simply deals with the
is/is-not relationship and will have difficulty when
two responses with similar feature values are clas-
sified differently. With preference ranking, viola-
tions of some rank constraints will affect the re-
sulting order of candidates, but sufficient ordering
information may still be present to correctly iden-
tify appropriate candidates.
To apply preference ranking to answer typing,
we learn a model over a set of questions q1, ..., qn.
Each question qi has a list of appropriate candidate
answers a(i,1), ..., a(i,u) and a list of inappropriate
candidate answers b(i,1), ..., b(i,v). The partial or-
dering r is simply the set
?i, j, k : {a(i,j) <r b(i,k)} (4)
This means that rank constraints are only gen-
erated for candidate answers a(i,j) and b(i,k) for
question qi and not between candidates a(i,j) and
b(l,k) where i 6= l. For example, the candidate an-
swers for the question ?What city hosted the 1988
Winter Olympics?? are not compared with those
for ?What colour is the sky?? because our partial
ordering r does not attempt to rank candidates for
one question in relation to candidates for another.
Moreover, no rank constraints are generated be-
tween a(i,j) and a(i,k) nor b(i,j) and b(i,k) because
the partial ordering does not include orderings be-
tween two candidates of the same class. Given two
appropriate candidates to the question ?What city
hosted the 1988 Winter Olympics??, New York
and Calgary, rank constraints will not be created
for the pair (New York, Calgary).
3 Methods
We begin with the work of Pinchak and Lin (2006)
in which question contexts (dependency tree paths
involving the wh-word) are extracted from the
question and matched against those found in a cor-
pus of text. The basic idea is that words that are
appropriate as answers will appear in place of the
wh-word in these contexts when found in the cor-
pus. For example, the question ?What city hosted
the 1988 Winter Olympics?? will have as one of
the question contexts ?X hosted Olympics.? We
then consult a corpus to discover what replace-
ments for X were actually mentioned and smooth
the resulting distribution.
We use the model of Pinchak and Lin (2006)
to produce features for our discriminative model.
Table 1: Feature templates
Pattern Description
E(t, c)
Estimated count of term t
in context c
C(t, c)
Observed count of term t in
context c
?
t? C(t
?, c)
Count of all terms appearing
in context c
?
c? C(t, c
?)
Count of term t in all
contexts
S(t)
Count of the times t occurs
in the candidate list
These features are mostly based on question con-
texts, and are briefly summarized in Table 1. Fol-
lowing Pinchak and Lin (2006), all of our features
are derived from a limited corpus (AQUAINT);
large-scale text resources are not required for our
model to perform well. By restricting ourselves
to relatively small corpora, we believe that our ap-
proach will easily transfer to other domains or lan-
guages (provided parsing resources are available).
To address the sparseness of question contexts,
we remove lexical elements from question context
paths. This removal is performed after feature val-
ues are obtained for the fully lexicalized path; the
removal of lexical elements simply allows many
similar paths to share a single learned weight. For
example, the term Calgary in context X ? sub-
ject ? host ? object ? Olympics (X hosted
Olympics) is used to obtain a feature value v that
is assigned to a feature such as C(Calgary, X ?
subject ? ? ? object ? ?) = v. Removal of
lexical elements results in a space of 73 possible
question contexts. To facilitate learning, all counts
are log values and feature vectors are normalized
to unit length.
The estimated count of term t in context c,
E(t, c), is a component of the model of Pinchak
and Lin (2006) and is calculated according to:
E(t, c) =
?
?
Pr(?|t)C(?, c) (5)
Essentially, this equation computes an expected
count for term t in question c by observing how
likely t is to be part of a cluster ? (Pr(?|t)) and
then observing how often terms of cluster ? oc-
cur in context c (C(?, c)). Although the model
of Pinchak and Lin (2006) is significantly more
668
complex, we use their core idea of cluster-based
smoothing to decide how often a term t will oc-
cur in a context c, regardless of whether or not t
was actually observed in c within our corpus. The
Pinchak and Lin (2006) system is unable to as-
sign individual weights to different question con-
texts, even though not all question contexts are
equally important. For example, the Pinchak and
Lin (2006) model is forced to consider a question
focus context (such as ?X is a city?) to be of equal
importance to non-focus contexts (such as ?X host
Olympics?). However, we have observed that it is
more important that candidate X is a city than it
hosted an Olympics in this instance. Appropriate
answers are required to be cities even though not
all cities have hosted Olympics. We wish to ad-
dress this problem with the use of discriminative
methods.
The observed count features of term t in con-
text c, C(t, c), are included to allow for combina-
tion with the estimated values from the model of
Pinchak and Lin (2006). Because Pinchak and Lin
(2006) make use of cluster-based smoothing, er-
rors may occur. By including the observed counts
of term t in context c, we hope to allow for the
use of more accurate statistics whenever they are
available, and for the smoothed counts in cases for
which they are not.
Finally, we include the frequency of a term t in
the list of candidates, S(t). The idea here is that
the correct and/or appropriate answers are likely
to be repeated many times in a list of candidate
answers. Terms that are strongly associated with
the question and appear often in results are likely
to be what the question is looking for.
Both the C(t, c) and S(t) features are exten-
sions to the Pinchak and Lin (2006) model and can
be incorporated into the Pinchak and Lin (2006)
model with varying degrees of difficulty. The
value of S(t) in particular is highly dependent on
the means used to obtain the candidate list, and the
distribution of words over the candidate list is of-
ten very different from the distribution of words in
the corpus. Because this feature value comes from
a different source than our other features, it would
be difficult to use in a non-discriminative model.
Correct answers to our set of questions are
obtained from the TREC 2002-2006 results
(Voorhees, 2002). For appropriateness labels we
turn to human annotators. Two annotators were in-
structed to label a candidate as appropriate if that
candidate was believable as an answer, even if that
candidate was not correct. For a question such as
?What city hosted the 1988 Winter Olympics??,
all cities should be labeled as appropriate even
though only Calgary is correct. This task comes
with a moderate degree of difficulty, especially
when dealing with questions for which appropriate
answers are less obvious (such as ?What kind of a
community is a Kibbutz??). We observed an inter-
annotator (kappa) agreement of 0.73, which indi-
cates substantial agreement. This value of kappa
conveys the difficulty that even human annotators
have when trying to decide which candidates are
appropriate for a given question. Because of this
value of kappa, we adopt strict gold standard ap-
propriateness labels that are the intersection of the
two annotators? labels (i.e., a candidate is only ap-
propriate if both annotators label it as such, other-
wise it is inappropriate).
We introduce four different models for the rank-
ing of appropriate answers, each of which makes
use of appropriateness labels in different ways:
Correctness Model: Although appropriateness
and correctness are not equivalent, this model
deals with distinguishing correct from incorrect
candidates in the hopes that the resulting model
will be able to perform well on finding both cor-
rect and appropriate answers. For learning, cor-
rect answers are placed at a rank above that of
incorrect candidates, regardless of whether or not
those candidates are appropriate. This represents
the strictest definition of appropriateness and re-
quires no human annotation.
Appropriateness Model: The correctness model
assumes only correct answers are appropriate. In
reality, this is seldom the case. For example,
documents or snipppets returned for the question
?What country did Catherine the Great rule?? will
contain not only Russia (the correct answer), but
also Germany (the nationality of her parents) and
Poland (her modern-day birthplace). To better ad-
dress this overly strict definition of appropriate-
ness, we rank all candidates labeled as appropri-
ate above those labeled as inappropriate, without
regards to correctness. Because we want to learn
a model for appropriateness, training on appropri-
ateness rather than correctness information should
produce a model closer to what we desire.
Combined Model: Discriminative preference
ranking is not limited to only two ranks. We
combine the ideas of correctness and appropri-
669
ateness together to form a three-rank combined
model. This model places correct answers above
appropriate-but-incorrect candidates, which are
in turn placed above inappropriate-and-incorrect
candidates.
Reduced Model: Both the appropriateness model
and the combined model incorporate a large num-
ber of rank constraints. We can reduce the number
of rank constraints generated by simply remov-
ing all appropriate, but incorrect, candidates from
consideration and otherwise following the correct-
ness model. The main difference is that some ap-
propriate candidates are no longer assigned a low
rank. By removing appropriate, but incorrect, can-
didates from the generation of rank constraints, we
no longer rank correct answers above appropriate
candidates.
4 Experiments
To compare with the prior approach of Pinchak
and Lin (2006), we use a set of what and which
questions with question focus (questions with a
noun phrase following the wh-word). These are
a subset of the more general what, which, and who
questions dealt with by Pinchak and Lin (2006).
Although our model can accommodate a wide
range of what, which, when, and who questions,
the focused what and which questions are an easily
identifiable subclass that are rarely definitional or
otherwise complex in terms of the desired answer.
We take the set of focused what and which ques-
tions from TREC 2002-2006 (Voorhees, 2002)
comprising a total of 385 questions and performed
9-fold cross-validation, with one dedicated devel-
opment partition (the tenth partition). The devel-
opment partition was used to tune the regulariza-
tion parameter of the SVM used for testing.
Candidates are obtained by submitting the ques-
tion as-is to the Google search engine and chunk-
ing the top 20 snippets returned, resulting in an
average of 140 candidates per question. Google
snippets create a better confusion set than simply
random words for appropriate and inappropriate
candidates; many of the terms found in Google
snippets are related in some way to the question.
To ensure a correct answer is present (where pos-
sible), we append the list of correct answers to the
list of candidates.
As a measure of performance, we adopt Mean
Reciprocal Rank (MRR) for both correct and ap-
propriate answers, as well as precision-recall for
appropriate answers. MRR is useful as a mea-
sure of overall QA system performance (Voorhees,
2002), but is based only on the top correct or
appropriate answer encountered in a ranked list.
For this reason, we also show the precision-recall
curve to better understand how our models per-
form.
We compare our models with three alternative
approaches, the simplest of which is random. For
random, the candidate answers are randomly shuf-
fled and performance is averaged over a number
of runs (100). The snippet frequency approach
orders candidates based on their frequency of oc-
currence in the Google snippets, and is simply the
S(t) feature of our discriminative models in isola-
tion. We remove terms comprised solely of ques-
tion words from all approaches to prevent question
words (which tend to be very frequent in the snip-
pets) from being selected as answers. The last of
our alternative systems is an implementation of the
work of Pinchak and Lin (2006) in which the out-
put probabilities of their model are used to rank
candidates.
4.1 Results
Figures 1 and 2 show the MRR results and
precision-recall curve of our correctness model
against the alternative approaches. In comparison
to these alternative systems, we show two versions
of our correctness model. The first uses a linear
kernel and is able to outperform the alternative ap-
proaches. The second uses a radial basis function
(RBF) kernel and exhibits performance superior to
that of the linear kernel. This suggests a degree
of non-linearity present in the data that cannot be
captured by the linear kernel alone. Both the train-
ing and running times of the RBF kernel are con-
siderably larger than that of the linear kernel. The
accuracy gain of the RBF kernel must therefore be
weighed against the increased time required to use
the model.
Figures 3 and 4 give the MRR results and
precision-recall curves for our additional mod-
els in comparison with that of the correctness
model. Although losses in MRR and precision
are observed for both the appropriate and com-
bined model using the RBF kernel, the linear ker-
nel versions of these models show slight perfor-
mance gains.
670
Figure 1: MRR results for the correctness model
First Correct Answer First Appropriate Candidate
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Random
Snippet Frequency
Pinchak and Lin (2006)
Linear Kernel
RBF Kernel
5 Discussion of Results
The results of our correctness model, found in Fig-
ures 1 and 2 show considerable gains over our al-
ternative systems, including that of Pinchak and
Lin (2006). The Pinchak and Lin (2006) system
was specifically designed with answer typing in
mind, although it makes use of a brittle generative
model that does not account for ranking of answer
candidates nor for the variable strength of various
question contexts. These results show that our dis-
criminative preference ranking approach creates a
better model of both correctness and appropriate-
ness via weighting of contexts, preference rank
learning, and with the incorporation of additional
related features (Table 1). The last feature, snippet
frequency, is not particularly strong on its own, but
can be easily incorporated into our discriminative
model. The ability to add a wide variety of po-
tentially helpful features is one of the strengths of
discriminative techniques in general.
By moving away from simply correct answers
in the correctness model and incorporating labeled
appropriate examples in various ways, we are able
to further improve upon the performance of our
approach. Training on appropriateness labels in-
stead of correct answers results in a loss in MRR
for the first correct answer, but a gain in MRR for
the first appropriate candidate. Unfortunately, this
does not carry over to the entire range of precision
over recall. For the linear kernel, our three ad-
Figure 2: Precision-recall of appropriate candi-
dates under the correctness model
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
P
r
e
c
i
s
i
o
n
RBF Kernel
Linear Kernel
Pinchak & Lin (2006)
Snippet Frequency
Random
ditional models (appropriateness, combined, and
reduced) show consistent improvements over the
correctness model, but with the RBF kernel only
the reduced model produces a meaningful change.
The precision-recall curves of Figures 2 and 4
show remarkable consistency across the full range
of recall, despite the fact that candidates exist for
which feature values cannot easily be obtained.
Due to tagging and chunking errors, ill-formed
candidates may exist that are judged appropriate
by the annotators. For example, ?explorer Her-
nando Soto? is a candidate marked appropriate
by both annotators to the question ?What Span-
ish explorer discovered the Mississippi River??
However, our context database does not include
the phrase ?explorer Hernando Soto? meaning that
only a few features will have non-zero values. De-
spite these occasional problems, our models are
able to rank most correct and appropriate candi-
dates high in a ranked list.
Finally, we examine the effects of training set
size on MRR. The learning curve for a single par-
titioning under the correctness model is presented
in Figure 5. Although the model trained with
the RBF kernel exhibits some degree of instabil-
ity below 100 training questions, both the linear
and RBF models gain little benefit from additional
training questions beyond 100. This may be due
to the fact that the most common unlexicalized
question contexts have been observed in the first
671
Figure 3: MRR results (RBF kernel)
First Correct Answer First Appropriate Candidate
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Correctness Model
Appropriateness Model
Combined Model
Reduced Model
100 training examples and so therefore additional
questions simply repeat the same information. Re-
quiring only a relatively small number of training
examples means that an effective model can be
learned with relatively little input in the form of
question-answer pairs or annotated candidate lists.
6 Prior Work
The expected answer type can be captured in a
number of possible ways. By far the most com-
mon is the assignment of one or more prede-
fined types to a question during a question anal-
ysis phase. Although the vast majority of the ap-
proaches to answer type detection make use of
rules (either partly or wholly) (Harabagiu et al,
2005; Sun et al, 2005; Wu et al, 2005; Molla? and
Gardiner, 2004), a few notable learned methods
for answer type detection exist.
One of the first attempts at learning a model for
answer type detection was made by Ittycheriah et
al. (2000; 2001) who learn a maximum entropy
classifier over the Message Understanding Confer-
ence (MUC) types. Those same MUC types are
then assigned by a named-entity tagger to iden-
tify appropriate candidate answers. Because of the
potential for unanticipated types, Ittycheriah et al
(2000; 2001) include a Phrase type as a catch-all
class that is used when no other class is appropri-
ate. Although the classifier and named-entity tag-
ger are shown to be among the components with
Figure 4: Precision-recall of appropriate (RBF
kernel)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
P
r
e
c
i
s
i
o
n
Correctness Model
Appropriateness Model
Combined Model
Reduced Model
the lowest error rate in their QA system, it is not
clear how much benefit is obtained from using a
relatively coarse-grained set of classes.
The approach of Li and Roth (2002) is sim-
ilar in that it uses learning for answer type de-
tection. They make use of multi-class learning
with a Sparse Network of Winnows (SNoW) and
a two-layer class hierarchy comprising a total of
fifty possible answer types. These finer-grained
classes are of more use when computing a notion
of appropriateness, although one major drawback
is that no entity tagger is discussed that can iden-
tify these types in text. Li and Roth (2002) also
rely on a rigid set of classes and so run the risk of
encountering a new question of an unseen type.
Pinchak and Lin (2006) present an alternative in
which the probability of a term being appropriate
to a question is computed directly. Instead of as-
signing an answer type to a question, the question
is broken down into a number of possibly overlap-
ping contexts. A candidate is then evaluated as to
how likely it is to appear in these contexts. Un-
fortunately, Pinchak and Lin (2006) use a brittle
generative model when combining question con-
texts that assumes all contexts are equally impor-
tant. This assumption was dealt with by Pinchak
and Lin (2006) by discarding all non-focus con-
texts with a focus context is present, but this is not
an ideal solution.
Learning methods are abundant in QA research
672
Figure 5: Learning curve for MRR of the first cor-
rect answer under the correctness model
10 25 50 100 150 200 310
Training Set Size
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
RBF Kernel
Linear Kernel
Snippet Frequency
Pinchak & Lin (2006)
Random
and have been applied in a number of different
ways. Ittycheriah et al (2000) created an en-
tire QA system based on maximum entropy com-
ponents in addition to the question classifier dis-
cussed above. Ittycheriah et al (2000) were able
to obtain reasonable performance from learned
components alone, although future versions of the
system use non-learned components in addition to
learned components (Prager et al, 2003). The
JAVELIN I system (Nyberg et al, 2005) uses
a SVM during the answer/information extraction
phase. Although learning is applied in many QA
tasks, very few QA systems rely solely on learn-
ing. Compositional approaches, in which multiple
distinct QA techniques are combined, also show
promise for improving QA performance. Echihabi
et al (2003) use three separate answer extraction
agents and combine the output scores with a max-
imum entropy re-ranker.
Surdeanu et al (2008) explore preference rank-
ing for advice or ?how to? questions in which a
unique correct answer is preferred over all other
candidates. Their focus is on complex-answer
questions in addition to the use of a collection of
user-generated answers rather than answer typing.
However, their use of preference ranking mirrors
the techniques we describe here in which the rela-
tive difference between two candidates at different
ranks is more important than the individual candi-
dates.
7 Conclusions and Future Work
We have introduced a means of flexible answer
typing with discriminative preference rank learn-
ing. Although answer typing does not represent a
complete QA system, it is an important component
to ensure that those candidates selected as answers
are indeed appropriate to the question being asked.
By casting the problem of evaluating appropriate-
ness as one of preference ranking, we allow for
the learning of what differentiates an appropriate
candidate from an inappropriate one.
Experimental results on focused what and
which questions show that a discriminatively
trained preference rank model is able to outper-
form alternative approaches designed for the same
task. This increase in performance comes from
both the flexibility to easily combine a number of
weighted features and because comparisons only
need to be made between appropriate and inappro-
priate candidates. A preference ranking model can
be trained from a relatively small set of example
questions, meaning that only a small number of
question/answer pairs or annotated candidate lists
are required.
The power of an answer typing system lies
in its ability to identify, in terms of some given
query, appropriate candidates. Applying the flexi-
ble model described here to a domain other than
question answering could allow for a more fo-
cused set of results. One straight-forward appli-
cation is to apply our model to the process of in-
formation or document retrieval itself. Ensuring
that there are terms present in the document ap-
propriate to the query could allow for the intel-
ligent expansion of the query. In a related vein,
queries are occasionally comprised of natural lan-
guage text fragments that can be treated similarly
to questions. Rarely are users searching for sim-
ple mentions of the query in pages; we wish to
provide them with something more useful. Our
model achieves the goal of finding those appropri-
ate related concepts.
Acknowledgments
We would like to thank Debra Shiau for her as-
sistance annotating training and test data and the
anonymous reviewers for their insightful com-
ments. We would also like to thank the Alberta
Informatics Circle of Research Excellence and the
Alberta Ingenuity Fund for their support in devel-
oping this work.
673
References
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu,
E. Melz, and D. Ravichandran. 2003. Multiple-
Engine Question Answering in TextMap. In Pro-
ceedings of the Twelfth Text REtrieval Conference
(TREC-2003), Gaithersburg, Maryland.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden,
A. Hickl, and P. Wang. 2005. Employing Two
Question Answering Systems in TREC-2005. In
Proceedings of the Fourteenth Text REtrieval Con-
ference (TREC-2005), Gaithersburg, Maryland.
A. Ittycheriah, M. Franz, W-J. Zhu, A. Ratnaparkhi,
and R. Mammone. 2000. IBM?s Statistical Ques-
tion Answering System. In Proceedings of the 9th
Text REtrieval Conference (TREC-9), Gaithersburg,
Maryland.
A. Ittycheriah, M. Franz, and S. Roukos. 2001. IBM?s
Statistical Question Answering System ? TREC-10.
In Proceedings of the 10th Text REtrieval Confer-
ence (TREC-10), Gaithersburg, Maryland.
T. Joachims. 1999. Making Large-Scale SVM Learn-
ing Practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT-Press.
T. Joachims. 2002. Optimizing Search Engines Us-
ing Clickthrough Data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing (KDD). ACM.
X. Li and D. Roth. 2002. Learning Question Clas-
sifiers. In Proceedings of the International Confer-
ence on Computational Linguistics (COLING 2002),
pages 556?562.
D. Molla? and M. Gardiner. 2004. AnswerFinder -
Question Answering by Combining Lexical, Syntac-
tic and Semantic Information. In Proceedings of the
Australian Language Technology Workshop (ALTW
2004, pages 9?16, Sydney, December.
E. Nyberg, R. Frederking, T. Mitamura, M. Bilotti,
K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, L. Lita,
V. Pedro, and A. Schlaikjer. 2005. JAVELIN I and
II Systems at TREC 2005. In Proceedings of the
Fourteenth Text REtrieval Conference (TREC-2005),
Gaithersburg, Maryland.
C. Pinchak and D. Lin. 2006. A Probabilistic Answer
Type Model. In Proceedings of the Eleventh Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL 2006), Trento,
Italy, April.
J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Itty-
cheriah, and R. Mahindru. 2003. IBM?s PIQUANT
in TREC2003. In Proceedings of the Twelfth Text
REtrieval Conference (TREC-2003), Gaithersburg,
Maryland.
R. Sun, J. Jiang, Y.F. Tan, H. Cui, T-S. Chua, and M-Y.
Kan. 2005. Using Syntactic and Semantic Relation
Analysis in Question Answering. In Proceedings
of the Fourteenth Text REtrieval Conference (TREC-
2005), Gaithersburg, Maryland.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of the 46th Annual Meeting for
the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-08: HLT), pages
719?727, Columbus, Ohio, June. Association for
Computational Linguistics.
E.M. Voorhees. 2002. Overview of the TREC 2002
Question Answering Track. In Proceedings of
TREC 2002, Gaithersburg, Maryland.
M. Wu, M. Duan, S. Shaikh, S. Small, and T. Strza-
lkowski. 2005. ILQUA ? An IE-Driven Ques-
tion Answering System. In Proceedings of the
Fourteenth Text REtrieval Conference (TREC-2005),
Gaithersburg, Maryland.
674
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 33?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Bootstrapping Path-Based Pronoun Resolution
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway,
Mountain View, California, 94301
lindek@google.com
Abstract
We present an approach to pronoun reso-
lution based on syntactic paths. Through a
simple bootstrapping procedure, we learn
the likelihood of coreference between a
pronoun and a candidate noun based on the
path in the parse tree between the two en-
tities. This path information enables us to
handle previously challenging resolution
instances, and also robustly addresses tra-
ditional syntactic coreference constraints.
Highly coreferent paths also allow mining
of precise probabilistic gender/number in-
formation. We combine statistical knowl-
edge with well known features in a Sup-
port Vector Machine pronoun resolution
classifier. Significant gains in performance
are observed on several datasets.
1 Introduction
Pronoun resolution is a difficult but vital part of the
overall coreference resolution task. In each of the
following sentences, a pronoun resolution system
must determine what the pronoun his refers to:
(1) John needs his friend.
(2) John needs his support.
In (1), John and his corefer. In (2), his refers
to some other, perhaps previously evoked entity.
Traditional pronoun resolution systems are not de-
signed to distinguish between these cases. They
lack the specific world knowledge required in the
second instance ? the knowledge that a person
does not usually explicitly need his own support.
We collect statistical path-coreference informa-
tion from a large, automatically-parsed corpus to
address this limitation. A dependency path is de-
fined as the sequence of dependency links between
two potentially coreferent entities in a parse tree.
A path does not include the terminal entities; for
example, ?John needs his support? and ?He needs
their support? have the same syntactic path. Our
algorithm determines that the dependency path
linking the Noun and pronoun is very likely to con-
nect coreferent entities for the path ?Noun needs
pronoun?s friend,? while it is rarely coreferent for
the path ?Noun needs pronoun?s support.?
This likelihood can be learned by simply count-
ing how often we see a given path in text with
an initial Noun and a final pronoun that are from
the same/different gender/number classes. Cases
such as ?John needs her support? or ?They need
his support? are much more frequent in text than
cases where the subject noun and pronoun termi-
nals agree in gender/number. When there is agree-
ment, the terminal nouns are likely to be corefer-
ent. When they disagree, they refer to different en-
tities. After a sufficient number of occurrences of
agreement or disagreement, there is a strong sta-
tistical indication of whether the path is coreferent
(terminal nouns tend to refer to the same entity) or
non-coreferent (nouns refer to different entities).
We show that including path coreference in-
formation enables significant performance gains
on three third-person pronoun resolution experi-
ments. We also show that coreferent paths can pro-
vide the seed information for bootstrapping other,
even more important information, such as the gen-
der/number of noun phrases.
2 Related Work
Coreference resolution is generally conducted as
a pairwise classification task, using various con-
straints and preferences to determine whether two
33
expressions corefer. Coreference is typically only
allowed between nouns matching in gender and
number, and not violating any intrasentential syn-
tactic principles. Constraints can be applied as a
preprocessing step to scoring candidates based on
distance, grammatical role, etc., with scores devel-
oped either manually (Lappin and Leass, 1994), or
through a machine-learning algorithm (Kehler et
al., 2004). Constraints and preferences have also
been applied together as decision nodes on a deci-
sion tree (Aone and Bennett, 1995).
When previous resolution systems handle cases
like (1) and (2), where no disagreement or syntac-
tic violation occurs, coreference is therefore de-
termined by the weighting of features or learned
decisions of the resolution classifier. Without
path coreference knowledge, a resolution process
would resolve the pronouns in (1) and (2) the
same way. Indeed, coreference resolution research
has focused on the importance of the strategy
for combining well known constraints and prefer-
ences (Mitkov, 1997; Ng and Cardie, 2002), devot-
ing little attention to the development of new fea-
tures for these difficult cases. The application of
world knowledge to pronoun resolution has been
limited to the semantic compatibility between a
candidate noun and the pronoun?s context (Yang
et al, 2005). We show semantic compatibility can
be effectively combined with path coreference in-
formation in our experiments below.
Our method for determining path coreference
is similar to an algorithm for discovering para-
phrases in text (Lin and Pantel, 2001). In that
work, the beginning and end nodes in the paths
are collected, and two paths are said to be similar
(and thus likely paraphrases of each other) if they
have similar terminals (i.e. the paths occur with a
similar distribution). Our work does not need to
store the terminals themselves, only whether they
are from the same pronoun group. Different paths
are not compared in any way; each path is individ-
ually assigned a coreference likelihood.
3 Path Coreference
We define a dependency path as the sequence of
nodes and dependency labels between two poten-
tially coreferent entities in a dependency parse
tree. We use the structure induced by the minimal-
ist parser Minipar (Lin, 1998) on sentences from
the news corpus described in Section 4. Figure 1
gives the parse tree of (2). As a short-form, we
Johnneedshissupport
subj gen
obj
Figure 1: Example dependency tree.
write the dependency path in this case as ?Noun
needs pronoun?s support.? The path itself does not
include the terminal nouns ?John? and ?his.?
Our algorithm finds the likelihood of coref-
erence along dependency paths by counting the
number of times they occur with terminals that
are either likely coreferent or non-coreferent. In
the simplest version, we count paths with termi-
nals that are both pronouns. We partition pronouns
into seven groups of matching gender, number,
and person; for example, the first person singular
group contains I, me, my, mine, and myself. If the
two terminal pronouns are from the same group,
coreference along the path is likely. If they are
from different groups, like I and his, then they are
non-coreferent. Let NS(p) be the number of timesthe two terminal pronouns of a path, p, are from
the same pronoun group, and let ND(p) be thenumber of times they are from different groups.
We define the coreference of p as:
C(p) = NS(p)NS(p) + ND(p)
Our statistics indicate the example path, ?Noun
needs pronoun?s support,? has a low C(p) value.
We could use this fact to prevent us from resolv-
ing ?his? to ?John? when ?John needs his support?
is presented to a pronoun resolution system.
To mitigate data sparsity, we represent the path
with the root form of the verbs and nouns. Also,
we use Minipar?s named-entity recognition to re-
place named-entity nouns by the semantic cate-
gory of their named-entity, when available. All
modifiers not on the direct path, such as adjectives,
determiners and adverbs, are not considered. We
limit the maximum path length to eight nodes.
Tables 1 and 2 give examples of coreferent and
non-coreferent paths learned by our algorithm and
identified in our test sets. Coreferent paths are
defined as paths with a C(p) value (and overall
number of occurrences) above a certain threshold,
indicating the terminal entities are highly likely
34
Table 1: Example coreferent paths: Italicized entities generally corefer.
Pattern Example
1. Noun left ... to pronoun?s wife Buffett will leave the stock to his wife.
2. Noun says pronoun intends... The newspaper says it intends to file a lawsuit.
3. Noun was punished for pronoun?s crime. The criminal was punished for his crime.
4. ... left Noun to fend for pronoun-self They left Jane to fend for herself.
5. Noun lost pronoun?s job. Dick lost his job.
6. ... created Noun and populated pronoun. Nzame created the earth and populated it
7. Noun consolidated pronoun?s power. The revolutionaries consolidated their power.
8. Noun suffered ... in pronoun?s knee ligament. The leopard suffered pain in its knee ligament.
to corefer. Non-coreferent paths have a C(p) be-
low a certain cutoff; the terminals are highly un-
likely to corefer. Especially note the challenge of
resolving most of the examples in Table 2 with-
out path coreference information. Although these
paths encompass some cases previously covered
by Binding Theory (e.g. ?Mary suspended her,?
her cannot refer to Mary by Principle B (Haege-
man, 1994)), most have no syntactic justification
for non-coreference per se. Likewise, although
Binding Theory (Principle A) could identify the
reflexive pronominal relationship of Example 4 in
Table 1, most cases cannot be resolved through
syntax alone. Our analysis shows that successfully
handling cases that may have been handled with
Binding Theory constitutes only a small portion of
the total performance gain using path coreference.
In any case, Binding Theory remains a chal-
lenge with a noisy parser. Consider: ?Alex gave
her money.? Minipar parses her as a possessive,
when it is more likely an object, ?Alex gave money
to her.? Without a correct parse, we cannot rule
out the link between her and Alex through Bind-
ing Theory. Our algorithm, however, learns that
the path ?Noun gave pronoun?s money,? is non-
coreferent. In a sense, it corrects for parser errors
by learning when coreference should be blocked,
given any consistent parse of the sentence.
We obtain path coreference for millions of paths
from our parsed news corpus (Section 4). While
Tables 1 and 2 give test set examples, many other
interesting paths are obtained. We learn corefer-
ence is unlikely between the nouns in ?Bob mar-
ried his mother,? or ?Sue wrote her obituary.? The
fact you don?t marry your own mother or write
your own obituary is perhaps obvious, but this
is the first time this kind of knowledge has been
made available computationally. Naturally, ex-
ceptions to the coreference or non-coreference of
some of these paths can be found; our patterns
represent general trends only. And, as mentioned
above, reliable path coreference is somewhat de-
pendent on consistent parsing.
Paths connecting pronouns to pronouns are dif-
ferent than paths connecting both nouns and pro-
nouns to pronouns ? the case we are ultimately in-
terested in resolving. Consider ?Company A gave
its data on its website.? The pronoun-pronoun
path coreference algorithm described above would
learn the terminals in ?Noun?s data on pronoun?s
website? are often coreferent. But if we see the
phrase ?Company A gave Company B?s data on
its website,? then ?its? is not likely to refer to
?Company B,? even though we identified this as
a coreferent path! We address this problem with a
two-stage extraction procedure. We first bootstrap
gender/number information using the pronoun-
pronoun paths as described in Section 4.1. We
then use this gender/number information to count
paths where an initial noun (with probabilistically-
assigned gender/number) and following pronoun
are connected by the dependency path, record-
ing the agreement or disagreement of their gen-
der/number category.1 These superior paths are
then used to re-bootstrap our final gender/number
information used in the evaluation (Section 6).
We also bootstrap paths where the nodes in
the path are replaced by their grammatical cate-
gory. This allows us to learn general syntactic con-
straints not dependent on the surface forms of the
words (including, but not limited to, the Binding
Theory principles). A separate set of these non-
coreferent paths is also used as a feature in our sys-
1As desired, this modification allows the first example to
provide two instances of noun-pronoun paths with terminals
from the same gender/number group, linking each ?its? to the
subject noun ?Company A?, rather than to each other.
35
Table 2: Example non-coreferent paths: Italicized entities do not generally corefer
Pattern Example
1. Noun thanked ... for pronoun?s assistance John thanked him for his assistance.
2. Noun wanted pronoun to lie. The president wanted her to lie.
3. ... Noun into pronoun?s pool Max put the floaties into their pool.
4. ... use Noun to pronoun?s advantage The company used the delay to its advantage.
5. Noun suspended pronoun Mary suspended her.
6. Noun was pronoun?s relative. The Smiths were their relatives.
7. Noun met pronoun?s demands The players? association met its demands.
8. ... put Noun at the top of pronoun?s list. The government put safety at the top of its list.
tem. We also tried expanding our coverage by us-
ing paths similar to paths with known path coref-
erence (based on distributionally similar words),
but this did not generally increase performance.
4 Bootstrapping in Pronoun Resolution
Our determination of path coreference can be con-
sidered a bootstrapping procedure. Furthermore,
the coreferent paths themselves can serve as the
seed for bootstrapping additional coreference in-
formation. In this section, we sketch previous ap-
proaches to bootstrapping in coreference resolu-
tion and explain our new ideas.
Coreference bootstrapping works by assuming
resolutions in unlabelled text, acquiring informa-
tion from the putative resolutions, and then mak-
ing inferences from the aggregate statistical data.
For example, we assumed two pronouns from the
same pronoun group were coreferent, and deduced
path coreference from the accumulated counts.
The potential of the bootstrapping approach can
best be appreciated by imagining millions of doc-
uments with coreference annotations. With such a
set, we could extract fine-grained features, perhaps
tied to individual words or paths. For example, we
could estimate the likelihood each noun belongs to
a particular gender/number class by the proportion
of times this noun was labelled as the antecedent
for a pronoun of this particular gender/number.
Since no such corpus exists, researchers have
used coarser features learned from smaller sets
through supervised learning (Soon et al, 2001;
Ng and Cardie, 2002), manually-defined corefer-
ence patterns to mine specific kinds of data (Bean
and Riloff, 2004; Bergsma, 2005), or accepted the
noise inherent in unsupervised schemes (Ge et al,
1998; Cherry and Bergsma, 2005).
We address the drawbacks of these approaches
Table 3: Gender classification performance (%)
Classifier F-Score
Bergsma (2005) Corpus-based 85.4
Bergsma (2005) Web-based 90.4
Bergsma (2005) Combined 92.2
Duplicated Corpus-based 88.0
Coreferent Path-based 90.3
by using coreferent paths as the assumed resolu-
tions in the bootstrapping. Because we can vary
the threshold for defining a coreferent path, we can
trade-off coverage for precision. We now outline
two potential uses of bootstrapping with coref-
erent paths: learning gender/number information
(Section 4.1) and augmenting a semantic compat-
ibility model (Section 4.2). We bootstrap this data
on our automatically-parsed news corpus. The
corpus comprises 85 GB of news articles taken
from the world wide web over a 1-year period.
4.1 Probabilistic Gender/Number
Bergsma (2005) learns noun gender (and num-
ber) from two principal sources: 1) mining it
from manually-defined lexico-syntactic patterns in
parsed corpora, and 2) acquiring it on the fly by
counting the number of pages returned for various
gender-indicating patterns by the Google search
engine. The web-based approach outperformed
the corpus-based approach, while a system that
combined the two sets of information resulted in
the highest performance (Table 3). The combined
gender-classifying system is a machine-learned
classifier with 20 features.
The time delay of using an Internet search en-
gine within a large-scale anaphora resolution ef-
fort is currently impractical. Thus we attempted
36
Table 4: Example gender/number probability (%)
Word masc fem neut plur
company 0.6 0.1 98.1 1.2
condoleeza rice 4.0 92.7 0.0 3.2
pat 58.3 30.6 6.2 4.9
president 94.1 3.0 1.5 1.4
wife 9.9 83.3 0.8 6.1
to duplicate Bergsma?s corpus-based extraction of
gender and number, where the information can be
stored in advance in a table, but using a much
larger data set. Bergsma ran his extraction on
roughly 6 GB of text; we used roughly 85 GB.
Using the test set from Bergsma (2005), we
were only able to boost performance from an F-
Score of 85.4% to one of 88.0% (Table 3). This
result led us to re-examine the high performance
of Bergsma?s web-based approach. We realized
that the corpus-based and web-based approaches
are not exactly symmetric. The corpus-based ap-
proaches, for example, would not pick out gender
from a pattern such as ?John and his friends...? be-
cause ?Noun and pronoun?s NP? is not one of the
manually-defined gender extraction patterns. The
web-based approach, however, would catch this
instance with the ?John * his/her/its/their? tem-
plate, where ?*? is the Google wild-card opera-
tor. Clearly, there are patterns useful for capturing
gender and number information beyond the pre-
defined set used in the corpus-based extraction.
We thus decided to capture gender/number in-
formation from coreferent paths. If a noun is con-
nected to a pronoun of a particular gender along a
coreferent path, we count this as an instance of that
noun being that gender. In the end, the probability
that the noun is a particular gender is the propor-
tion of times it was connected to a pronoun of that
gender along a coreferent path. Gender informa-
tion becomes a single intuitive, accessible feature
(i.e. the probability of the noun being that gender)
rather than Bergsma?s 20-dimensional feature vec-
tor requiring search-engine queries to instantiate.
We acquire gender and number data for over 3
million nouns. We use add-one smoothing for data
sparsity. Some example gender/number probabil-
ities are given in Table 4 (cf. (Ge et al, 1998;
Cherry and Bergsma, 2005)). We get a perfor-
mance of 90.3% (Table 3), again meeting our re-
quirements of high performance and allowing for
a fast, practical implementation. This is lower
than Bergsma?s top score of 92.2% (Figure 3),
but again, Bergsma?s top system relies on Google
search queries for each new word, while ours are
all pre-stored in a table for fast access.
We are pleased to be able to share our gender
and number data with the NLP community.2 In
Section 6, we show the benefit of this data as a
probabilistic feature in our pronoun resolution sys-
tem. Probabilistic data is useful because it allows
us to rapidly prototype resolution systems with-
out incurring the overhead of large-scale lexical
databases such as WordNet (Miller et al, 1990).
4.2 Semantic Compatibility
Researchers since Dagan and Itai (1990) have var-
iously argued for and against the utility of col-
location statistics between nouns and parents for
improving the performance of pronoun resolution.
For example, can the verb parent of a pronoun be
used to select antecedents that satisfy the verb?s se-
lectional restrictions? If the verb phrase was shat-
ter it, we would expect it to refer to some kind
of brittle entity. Like path coreference, semantic
compatibility can be considered a form of world
knowledge needed for more challenging pronoun
resolution instances.
We encode the semantic compatibility between
a noun and its parse tree parent (and grammatical
relationship with the parent) using mutual infor-
mation (MI) (Church and Hanks, 1989). Suppose
we are determining whether ham is a suitable an-
tecedent for the pronoun it in eat it. We calculate
the MI as:
MI(eat:obj, ham) = log Pr(eat:obj:ham)Pr(eat:obj)Pr(ham)
Although semantic compatibility is usually only
computed for possessive-noun, subject-verb, and
verb-object relationships, we include 121 differ-
ent kinds of syntactic relationships as parsed in
our news corpus.3 We collected 4.88 billion par-
ent:rel:node triples, including over 327 million
possessive-noun values, 1.29 billion subject-verb
and 877 million verb-direct object. We use small
probability values for unseen Pr(parent:rel:node),
Pr(parent:rel), and Pr(node) cases, as well as a de-
fault MI when no relationship is parsed, roughly
optimized for performance on the training set. We
2Available at http://www.cs.ualberta.ca/?bergsma/Gender/
3We convert prepositions to relationships to enhance our
model?s semantics, e.g. Joan:of:Arc rather than Joan:prep:of
37
include both the MI between the noun and the pro-
noun?s parent as well as the MI between the pro-
noun and the noun?s parent as features in our pro-
noun resolution classifier.
Kehler et al (2004) saw no apparent gain from
using semantic compatibility information, while
Yang et al (2005) saw about a 3% improvement
with compatibility data acquired by searching on
the world wide web. Section 6 analyzes the con-
tribution of MI to our system.
Bean and Riloff (2004) used bootstrapping to
extend their semantic compatibility model, which
they called contextual-role knowledge, by identi-
fying certain cases of easily-resolved anaphors and
antecedents. They give the example ?Mr. Bush
disclosed the policy by reading it.? Once we iden-
tify that it and policy are coreferent, we include
read:obj:policy as part of the compatibility model.
Rather than using manually-defined heuristics
to bootstrap additional semantic compatibility in-
formation, we wanted to enhance our MI statistics
automatically with coreferent paths. Consider the
phrase, ?Saddam?s wife got a Jordanian lawyer for
her husband.? It is unlikely we would see ?wife?s
husband? in text; in other words, we would not
know that husband:gen:wife is, in fact, semanti-
cally compatible and thereby we would discour-
age selection of ?wife? as the antecedent at res-
olution time. However, because ?Noun gets ...
for pronoun?s husband? is a coreferent path, we
could capture the above relationship by adding a
parent:rel:node for every pronoun connected to a
noun phrase along a coreferent path in text.
We developed context models with and with-
out these path enhancements, but ultimately we
could find no subset of coreferent paths that im-
prove the semantic compatibility?s contribution to
training set accuracy. A mutual information model
trained on 85 GB of text is fairly robust on its own,
and any kind of bootstrapped extension seems to
cause more damage by increased noise than can be
compensated by increased coverage. Although we
like knowing audiences have noses, e.g. ?the audi-
ence turned up its nose at the performance,? such
phrases are apparently quite rare in actual test sets.
5 Experimental Design
The noun-pronoun path coreference can be used
directly as a feature in a pronoun resolution sys-
tem. However, path coreference is undefined for
cases where there is no path between the pro-
noun and the candidate noun ? for example, when
the candidate is in the previous sentence. There-
fore, rather than using path coreference directly,
we have features that are true if C(p) is above or
below certain thresholds. The features are thus set
when coreference between the pronoun and candi-
date noun is likely (a coreferent path) or unlikely
(a non-coreferent path).
We now evaluate the utility of path coreference
within a state-of-the-art machine-learned resolu-
tion system for third-person pronouns with nom-
inal antecedents. A standard set of features is used
along with the bootstrapped gender/number, se-
mantic compatibility, and path coreference infor-
mation. We refer to these features as our ?proba-
bilistic features? (Prob. Features) and run experi-
ments using the full system trained and tested with
each absent, in turn (Table 5). We have 29 features
in total, including measures of candidate distance,
frequency, grammatical role, and different kinds
of parallelism between the pronoun and the can-
didate noun. Several reliable features are used as
hard constraints, removing candidates before con-
sideration by the scoring algorithm.
All of the parsing, noun-phrase identification,
and named-entity recognition are done automat-
ically with Minipar. Candidate antecedents are
considered in the current and previous sentence
only. We use SVMlight (Joachims, 1999) to learn
a linear-kernel classifier on pairwise examples in
the training set. When resolving pronouns, we
select the candidate with the farthest positive dis-
tance from the SVM classification hyperplane.
Our training set is the anaphora-annotated por-
tion of the American National Corpus (ANC) used
in Bergsma (2005), containing 1270 anaphoric
pronouns4 . We test on the ANC Test set (1291 in-
stances) also used in Bergsma (2005) (highest res-
olution accuracy reported: 73.3%), the anaphora-
labelled portion of AQUAINT used in Cherry and
Bergsma (2005) (1078 instances, highest accu-
racy: 71.4%), and the anaphoric pronoun subset
of the MUC7 (1997) coreference evaluation for-
mal test set (169 instances, highest precision of
62.1 reported on all pronouns in (Ng and Cardie,
2002)). These particular corpora were chosen so
we could test our approach using the same data
as comparable machine-learned systems exploit-
ing probabilistic information sources. Parameters
4See http://www.cs.ualberta.ca/?bergsma/CorefTags/ for
instructions on acquiring annotations
38
Table 5: Resolution accuracy (%)
Dataset ANC AQT MUC
1 Previous noun 36.7 34.5 30.8
2 No Prob. Features 58.1 60.9 49.7
3 No Prob. Gender 65.8 71.0 68.6
4 No MI 71.3 73.5 69.2
5 No C(p) 72.3 73.7 69.8
6 Full System 73.9 75.0 71.6
7 Upper Bound 93.2 92.3 91.1
were set using cross-validation on the training set;
test sets were used only once to obtain the final
performance values.
Evaluation Metric: We report results in terms of
accuracy: Of all the anaphoric pronouns in the test
set, the proportion we resolve correctly.
6 Results and Discussion
We compare the accuracy of various configura-
tions of our system on the ANC, AQT and MUC
datasets (Table 5). We include the score from pick-
ing the noun immediately preceding the pronoun
(after our hard filters are applied). Due to the hard
filters and limited search window, it is not possi-
ble for our system to resolve every noun to a cor-
rect antecedent. We thus provide the performance
upper bound (i.e. the proportion of cases with a
correct answer in the filtered candidate list). On
ANC and AQT, each of the probabilistic features
results in a statistically significant gain in perfor-
mance over a model trained and tested with that
feature absent.5 On the smaller MUC set, none of
the differences in 3-6 are statistically significant,
however, the relative contribution of the various
features remains reassuringly constant.
Aside from missing antecedents due to the hard
filters, the main sources of error include inaccurate
statistical data and a classifier bias toward preced-
ing pronouns of the same gender/number. It would
be interesting to see whether performance could be
improved by adding WordNet and web-mined fea-
tures. Path coreference itself could conceivably be
determined with a search engine.
Gender is our most powerful probabilistic fea-
ture. In fact, inspecting our system?s decisions,
gender often rules out coreference regardless of
path coreference. This is not surprising, since we
based the acquisition of C(p) on gender. That is,
5We calculate significance with McNemar?s test, p=0.05.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Recall
Pr
ec
is
io
n
Top-1
Pr
ec
is
io
n
Top-2
Pr
ec
is
io
n
Top-3
Figure 2: ANC pronoun resolution accuracy for
varying SVM-thresholds.
our bootstrapping assumption was that the major-
ity of times these paths occur, gender indicates
coreference or lack thereof. Thus when they oc-
cur in our test sets, gender should often sufficiently
indicate coreference. Improving the orthogonality
of our features remains a future challenge.
Nevertheless, note the decrease in performance
on each of the datasets when C(p) is excluded
(#5). This is compelling evidence that path coref-
erence is valuable in its own right, beyond its abil-
ity to bootstrap extensive and reliable gender data.
Finally, we can add ourselves to the camp of
people claiming semantic compatibility is useful
for pronoun resolution. Both the MI from the pro-
noun in the antecedent?s context and vice-versa
result in improvement. Building a model from
enough text may be the key.
The primary goal of our evaluation was to as-
sess the benefit of path coreference within a com-
petitive pronoun resolution system. Our system
does, however, outperform previously published
results on these datasets. Direct comparison of
our scoring system to other current top approaches
is made difficult by differences in preprocessing.
Ideally we would assess the benefit of our prob-
abilistic features using the same state-of-the-art
preprocessing modules employed by others such
as (Yang et al, 2005) (who additionally use a
search engine for compatibility scoring). Clearly,
promoting competitive evaluation of pronoun res-
olution scoring systems by giving competitors
equivalent real-world preprocessing output along
the lines of (Barbu and Mitkov, 2001) remains the
best way to isolate areas for system improvement.
Our pronoun resolution system is part of a larger
information retrieval project where resolution ac-
39
curacy is not necessarily the most pertinent mea-
sure of classifier performance. More than one can-
didate can be useful in ambiguous cases, and not
every resolution need be used. Since the SVM
ranks antecedent candidates, we can test this rank-
ing by selecting more than the top candidate (Top-
n) and evaluating coverage of the true antecedents.
We can also resolve only those instances where the
most likely candidate is above a certain distance
from the SVM threshold. Varying this distance
varies the precision-recall (PR) of the overall res-
olution. A representative PR curve for the Top-n
classifiers is provided (Figure 2). The correspond-
ing information retrieval performance can now be
evaluated along the Top-n / PR configurations.
7 Conclusion
We have introduced a novel feature for pronoun
resolution called path coreference, and demon-
strated its significant contribution to a state-of-the-
art pronoun resolution system. This feature aids
coreference decisions in many situations not han-
dled by traditional coreference systems. Also, by
bootstrapping with the coreferent paths, we are
able to build the most complete and accurate ta-
ble of probabilistic gender information yet avail-
able. Preliminary experiments show path coref-
erence bootstrapping can also provide a means of
identifying pleonastic pronouns, where pleonastic
neutral pronouns are often followed in a depen-
dency path by a terminal noun of different gender,
and cataphoric constructions, where the pronouns
are often followed by nouns of matching gender.
References
Chinatsu Aone and Scott William Bennett. 1995. Evaluating
automated and manual acquisition of anaphora resolution
strategies. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics, pages 122?
129.
Catalina Barbu and Ruslan Mitkov. 2001. Evaluation tool for
rule-based anaphora resolution methods. In Proceedings
of the 39th Annual Meeting of the Association for Compu-
tational Linguistics, pages 34?41.
David L. Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference resolu-
tion. In HLT-NAACL, pages 297?304.
Shane Bergsma. 2005. Automatic acquisition of gender in-
formation for anaphora resolution. In Proceedings of the
Eighteenth Canadian Conference on Artificial Intelligence
(Canadian AI?2005), pages 342?353.
Colin Cherry and Shane Bergsma. 2005. An expectation
maximization approach to pronoun resolution. In Pro-
ceedings of the Ninth Conference on Natural Language
Learning (CoNLL-2005), pages 88?95.
Kenneth Ward Church and Patrick Hanks. 1989. Word asso-
ciation norms, mutual information, and lexicography. In
Proceedings of the 27th Annual Meeting of the Association
for Computational Linguistics (ACL?89), pages 76?83.
Ido Dagan and Alan Itai. 1990. Automatic processing
of large corpora for the resolution of anaphora refer-
ences. In Proceedings of the 13th International Con-
ference on Computational Linguistics (COLING-90), vol-
ume 3, pages 330?332, Helsinki, Finland.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A statisti-
cal approach to anaphora resolution. In Proceedings of the
Sixth Workshop on Very Large Corpora, pages 161?171.
Liliane Haegeman. 1994. Introduction to Government &
Binding theory: Second Edition. Basil Blackwell, Cam-
bridge, UK.
Thorsten Joachims. 1999. Making large-scale SVM learn-
ing practical. In B. Scho?lkopf and C. Burges, editors, Ad-
vances in Kernel Methods. MIT-Press.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Aleksandr
Simma. 2004. The (non)utility of predicate-argument fre-
quencies for pronoun interpretation. In Proceedings of
HLT/NAACL-04, pages 289?296.
Shalom Lappin and Herbert J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational Linguis-
tics, 20(4):535?561.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Proceedings of the Workshop on the Evalua-
tion of Parsing Systems, First International Conference on
Language Resources and Evaluation.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduction
to WordNet: an on-line lexical database. International
Journal of Lexicography, 3(4):235?244.
Ruslan Mitkov. 1997. Factors in anaphora resolution: they
are not the only things that matter. a case study based on
two different approaches. In Proceedings of the ACL ?97 /
EACL ?97 Workshop on Operational Factors in Practical,
Robust Anaphora Resolution, pages 14?21.
MUC-7. 1997. Coreference task definition (v3.0, 13 Jul
97). In Proceedings of the Seventh Message Understand-
ing Conference (MUC-7).
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics,
27(4):521?544.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based seman-
tic compatibility information. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 165?172, June.
40
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809?816,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Names and Similarities on the Web: Fact Extraction in the Fast Lane
Marius Pas?ca
Google Inc.
Mountain View, CA 94043
mars@google.com
Dekang Lin
Google Inc.
Mountain View, CA 94043
lindek@google.com
Jeffrey Bigham?
University of Washington
Seattle, WA 98195
jbigham@cs.washington.edu
Andrei Lifchits?
University of British Columbia
Vancouver, BC V6T 1Z4
alifchit@cs.ubc.ca
Alpa Jain?
Columbia University
New York, NY 10027
alpa@cs.columbia.edu
Abstract
In a new approach to large-scale extrac-
tion of facts from unstructured text, dis-
tributional similarities become an integral
part of both the iterative acquisition of
high-coverage contextual extraction pat-
terns, and the validation and ranking of
candidate facts. The evaluation mea-
sures the quality and coverage of facts
extracted from one hundred million Web
documents, starting from ten seed facts
and using no additional knowledge, lexi-
cons or complex tools.
1 Introduction
1.1 Background
The potential impact of structured fact reposito-
ries containing billions of relations among named
entities on Web search is enormous. They en-
able the pursuit of new search paradigms, the pro-
cessing of database-like queries, and alternative
methods of presenting search results. The prepa-
ration of exhaustive lists of hand-written extrac-
tion rules is impractical given the need for domain-
independent extraction of many types of facts from
unstructured text. In contrast, the idea of boot-
strapping for relation and information extraction
was first proposed in (Riloff and Jones, 1999), and
successfully applied to the construction of seman-
tic lexicons (Thelen and Riloff, 2002), named en-
tity recognition (Collins and Singer, 1999), extrac-
tion of binary relations (Agichtein and Gravano,
2000), and acquisition of structured data for tasks
such as Question Answering (Lita and Carbonell,
2004; Fleischman et al, 2003). In the context of
fact extraction, the resulting iterative acquisition
?Work done during internships at Google Inc.
framework starts from a small set of seed facts,
finds contextual patterns that extract the seed facts
from the underlying text collection, identifies a
larger set of candidate facts that are extracted by
the patterns, and adds the best candidate facts to
the previous seed set.
1.2 Contributions
Figure 1 describes an architecture geared towards
large-scale fact extraction. The architecture is sim-
ilar to other instances of bootstrapping for infor-
mation extraction. The main processing stages are
the acquisition of contextual extraction patterns
given the seed facts, acquisition of candidate facts
given the extraction patterns, scoring and ranking
of the patterns, and scoring and ranking of the can-
didate facts, a subset of which is added to the seed
set of the next round.
Within the existing iterative acquisition frame-
work, our first contribution is a method for au-
tomatically generating generalized contextual ex-
traction patterns, based on dynamically-computed
classes of similar words. Traditionally, the ac-
quisition of contextual extraction patterns requires
hundreds or thousands of consecutive iterations
over the entire text collection (Lita and Carbonell,
2004), often using relatively expensive or restric-
tive tools such as shallow syntactic parsers (Riloff
and Jones, 1999; Thelen and Riloff, 2002) or
named entity recognizers (Agichtein and Gravano,
2000). Comparatively, generalized extraction pat-
terns achieve exponentially higher coverage in
early iterations. The extraction of large sets of can-
didate facts opens the possibility of fast-growth it-
erative extraction, as opposed to the de-facto strat-
egy of conservatively growing the seed set by as
few as five items (Thelen and Riloff, 2002) after
each iteration.
809
Acquisition of contextual extraction patterns
Distributional similaritiesText collection
Candidate facts
Acquisition of candidate facts
Occurrences of extraction patterns
Validation of candidate facts
Scored extraction patternsScored candidate facts
Scoring and ranking
Validated candidate facts
Seed facts
Occurrences of seed facts Extraction patterns
Validated extraction patterns
Validation of patterns
Generalized extraction patterns
Figure 1: Large-scale fact extraction architecture
The second contribution of the paper is a
method for domain-independent validation and
ranking of candidate facts, based on a similar-
ity measure of each candidate fact relative to the
set of seed facts. Whereas previous studies as-
sume clean text collections such as news cor-
pora (Thelen and Riloff, 2002; Agichtein and Gra-
vano, 2000; Hasegawa et al, 2004), the valida-
tion is essential for low-quality sets of candidate
facts collected from noisy Web documents. With-
out it, the addition of spurious candidate facts to
the seed set would result in a quick divergence of
the iterative acquisition towards irrelevant infor-
mation (Agichtein and Gravano, 2000). Further-
more, the finer-grained ranking induced by simi-
larities is necessary in fast-growth iterative acqui-
sition, whereas previously proposed ranking crite-
ria (Thelen and Riloff, 2002; Lita and Carbonell,
2004) are implicitly designed for slow growth of
the seed set.
2 Similarities for Pattern Acquisition
2.1 Generalization via Word Similarities
The extraction patterns are acquired by matching
the pairs of phrases from the seed set into docu-
ment sentences. The patterns consist of contigu-
ous sequences of sentence terms, but otherwise
differ from the types of patterns proposed in earlier
work in two respects. First, the terms of a pattern
are either regular words or, for higher generality,
any word from a class of similar words. Second,
the amount of textual context encoded in a pat-
tern is limited to the sequence of terms between
(i.e., infix) the pair of phrases from a seed fact that
could be matched in a document sentence, thus ex-
cluding any context to the left (i.e., prefix) and to
the right (i.e., postfix) of the seed.
The pattern shown at the top of Figure 2, which
(Irving Berlin, 1888)
    NNP       NNP       CD
Infix
Aurelio de la Vega was born November 28 , 1925 , in Havana , Cuba .
    FW       FW FW  NNP VBD  VBN      NNP           CD  ,    CD    ,  IN    NNP      ,   NNP    .
foundnot found
Infix
not found
Prefix PostfixInfix
Matching on sentences
Seed fact Infix?only pattern
The poet was born Jan. 13 , several years after the revolution .
not found
British ? native Glenn Cornick of Jethro Tull was born April 23 , 1947 .
   NNP     :      JJ         NNP        NNP       IN    NNP     NNP  VBD  VBN   NNP   CD  ,   CD     .
Infix
foundfound
Chester Burton Atkins was born June 20 , 1924 , on a farm near Luttrell .
   NNP          NNP       NNP     VBD  VBN  NNP  CD  ,   CD     ,  IN DT  NN     IN       NNP       .
Infix
Infix
found
The youngest child of three siblings , Mariah Carey was born March 27 ,
1970 in Huntington , Long Island in New York .
  DT       JJS            NN     IN   CD        NNS       ,    NNP        NNP    VBD  VBN    NNP     CD  ,
  CD    IN       NNP             ,    JJ         NN      IN  NNP    NNP   .
found
foundfound
(S1)
(S2)
(S3)
(S4)
(S5)
(Jethro Tull, 1947)  (Mariah Carey, 1970)  (Chester Burton Atkins, 1924)
Candidate facts
  DT    NN   VBD  VBN  NNP CD ,       JJ           NNS     IN     DT        NN           .
N/A          CL1 born CL2 00 ,              N/A
Figure 2: Extraction via infix-only patterns
contains the sequence [CL1 born CL2 00 .], illus-
trates the use of classes of distributionally similar
words within extraction patterns. The first word
class in the sequence, CL1, consists of words such
as {was, is, could}, whereas the second class in-
cludes {February, April, June, Aug., November}
and other similar words. The classes of words are
computed on the fly over all sequences of terms
in the extracted patterns, on top of a large set of
pairwise similarities among words (Lin, 1998) ex-
tracted in advance from around 50 million news
articles indexed by the Google search engine over
three years. All digits in both patterns and sen-
tences are replaced with a common marker, such
810
that any two numerical values with the same num-
ber of digits will overlap during matching.
Many methods have been proposed to compute
distributional similarity between words, e.g., (Hin-
dle, 1990), (Pereira et al, 1993), (Grefenstette,
1994) and (Lin, 1998). Almost all of the methods
represent a word by a feature vector, where each
feature corresponds to a type of context in which
the word appeared. They differ in how the feature
vectors are constructed and how the similarity be-
tween two feature vectors is computed.
In our approach, we define the features of a
word w to be the set of words that occurred within
a small window of w in a large corpus. The context
window of an instance of w consists of the clos-
est non-stopword on each side of w and the stop-
words in between. The value of a feature w? is de-
fined as the pointwise mutual information between
w? and w: PMI(w?, w) = ? log( P (w,w?)P (w)P (w?)). The
similarity between two different words w1 and w2,
S(w1, w2), is then computed as the cosine of the
angle between their feature vectors.
While the previous approaches to distributional
similarity have only applied to words, we applied
the same technique to proper names as well as
words. The following are some example similar
words and phrases with their similarities, as ob-
tained from the Google News corpus:
? Carey: Higgins 0.39, Lambert 0.39, Payne
0.38, Kelley 0.38, Hayes 0.38, Goodwin 0.38,
Griffin 0.38, Cummings 0.38, Hansen 0.38,
Williamson 0.38, Peters 0.38, Walsh 0.38, Burke
0.38, Boyd 0.38, Andrews 0.38, Cunningham
0.38, Freeman 0.37, Stephens 0.37, Flynn 0.37,
Ellis 0.37, Bowers 0.37, Bennett 0.37, Matthews
0.37, Johnston 0.37, Richards 0.37, Hoffman
0.37, Schultz 0.37, Steele 0.37, Dunn 0.37, Rowe
0.37, Swanson 0.37, Hawkins 0.37, Wheeler 0.37,
Porter 0.37, Watkins 0.37, Meyer 0.37 [..];
? Mariah Carey: Shania Twain 0.38, Christina
Aguilera 0.35, Sheryl Crow 0.35, Britney Spears
0.33, Celine Dion 0.33, Whitney Houston 0.32,
Justin Timberlake 0.32, Beyonce Knowles 0.32,
Bruce Springsteen 0.30, Faith Hill 0.30, LeAnn
Rimes 0.30, Missy Elliott 0.30, Aretha Franklin
0.29, Jennifer Lopez 0.29, Gloria Estefan 0.29,
Elton John 0.29, Norah Jones 0.29, Missy
Elliot 0.29, Alicia Keys 0.29, Avril Lavigne
0.29, Kid Rock 0.28, Janet Jackson 0.28, Kylie
Minogue 0.28, Beyonce 0.27, Enrique Iglesias
0.27, Michelle Branch 0.27 [..];
? Jethro Tull: Motley Crue 0.28, Black Crowes
0.26, Pearl Jam 0.26, Silverchair 0.26, Black Sab-
bath 0.26, Doobie Brothers 0.26, Judas Priest 0.26,
Van Halen 0.25, Midnight Oil 0.25, Pere Ubu 0.24,
Black Flag 0.24, Godsmack 0.24, Grateful Dead
0.24, Grand Funk Railroad 0.24, Smashing Pump-
kins 0.24, Led Zeppelin 0.24, Aerosmith 0.24,
Limp Bizkit 0.24, Counting Crows 0.24, Echo
And The Bunnymen 0.24, Cold Chisel 0.24, Thin
Lizzy 0.24 [..].
To our knowledge, the only previous study that
embeds similarities into the acquisition of extrac-
tion patterns is (Stevenson and Greenwood, 2005).
The authors present a method for computing pair-
wise similarity scores among large sets of poten-
tial syntactic (subject-verb-object) patterns, to de-
tect centroids of mutually similar patterns. By as-
suming the syntactic parsing of the underlying text
collection to generate the potential patterns in the
first place, the method is impractical on Web-scale
collections. Two patterns, e.g. chairman-resign
and CEO-quit, are similar to each other if their
components are present in an external hand-built
ontology (i.e., WordNet), and the similarity among
the components is high over the ontology. Since
general-purpose ontologies, and WordNet in par-
ticular, contain many classes (e.g., chairman and
CEO) but very few instances such as Osasuna,
Crewe etc., the patterns containing an instance
rather than a class will not be found to be simi-
lar to one another. In comparison, the classes and
instances are equally useful in our method for gen-
eralizing patterns for fact extraction. We merge
basic patterns into generalized patterns, regardless
of whether the similar words belong, as classes or
instances, in any external ontology.
2.2 Generalization via Infix-Only Patterns
By giving up the contextual constraints imposed
by the prefix and postfix, infix-only patterns rep-
resent the most aggressive type of extraction pat-
terns that still use contiguous sequences of terms.
In the absence of the prefix and postfix, the outer
boundaries of the fact are computed separately for
the beginning of the first (left) and end of the sec-
ond (right) phrases of the candidate fact. For gen-
erality, the computation relies only on the part-
of-speech tags of the current seed set. Starting
forward from the right extremity of the infix, we
collect a growing sequence of terms whose part-
of-speech tags are [P1+ P2+ .. Pn+], where the
811
notation Pi+ represents one or more consecutive
occurrences of the part-of-speech tag Pi. The se-
quence [P1 P2 .. Pn] must be exactly the sequence
of part of speech tags from the right side of one of
the seed facts. The point where the sequence can-
not be grown anymore defines the boundary of the
fact. A similar procedure is applied backwards,
starting from the left extremity of the infix. An
infix-only pattern produces a candidate fact from
a sentence only if an acceptable sequence is found
to the left and also to the right of the infix.
Figure 2 illustrates the process on the infix-
only pattern mentioned earlier, and one seed fact.
The part-of-speech tags for the seed fact are [NNP
NNP] and [CD] for the left and right sides respec-
tively. The infix occurs in all sentences. How-
ever, the matching of the part-of-speech tags of the
sentence sequences to the left and right of the in-
fix, against the part-of-speech tags of the seed fact,
only succeeds for the last three sentences. It fails
for the first sentence S1 to the left of the infix, be-
cause [.. NNP] (for Vega) does not match [NNP
NNP]. It also fails for the second sentence S2 to
both the left and the right side of the infix, since [..
NN] (for poet) does not match [NNP NNP], and
[JJ ..] (for several) does not match [CD].
3 Similarities for Validation and Ranking
3.1 Revisiting Standard Ranking Criteria
Because some of the acquired extraction patterns
are too generic or wrong, all approaches to iter-
ative acquisition place a strong emphasis on the
choice of criteria for ranking. Previous literature
quasi-unanimously assesses the quality of each
candidate fact based on the number and qual-
ity of the patterns that extract the candidate fact
(more is better); and the number of seed facts ex-
tracted by the same patterns (again, more is bet-
ter) (Agichtein and Gravano, 2000; Thelen and
Riloff, 2002; Lita and Carbonell, 2004). However,
our experiments using many variations of previ-
ously proposed scoring functions suggest that they
have limited applicability in large-scale fact ex-
traction, for two main reasons. The first is that
it is impractical to perform hundreds of acquisi-
tion iterations on terabytes of text. Instead, one
needs to grow the seed set aggressively in each
iteration. Previous scoring functions were im-
plicitly designed for cautious acquisition strate-
gies (Collins and Singer, 1999), which expand the
seed set very slowly across consecutive iterations.
In that case, it makes sense to single out a small
number of best candidates, among the other avail-
able candidates. Comparatively, when 10,000 can-
didate facts or more need to be added to a seed set
of 10 seeds as early as after the first iteration, it
is difficult to distinguish the quality of extraction
patterns based, for instance, only on the percent-
age of the seed set that they extract. The second
reason is the noisy nature of the Web. A substan-
tial number of factors can and will concur towards
the worst-case extraction scenarios on the Web.
Patterns of apparently high quality turn out to pro-
duce a large quantity of erroneous ?facts? such as
(A-League, 1997), but also the more interesting
(Jethro Tull, 1947) as shown earlier in Figure 2, or
(Web Site David, 1960) or (New York, 1831). As
for extraction patterns of average or lower quality,
they will naturally lead to even more spurious ex-
tractions.
3.2 Ranking of Extraction Patterns
The intuition behind our criteria for ranking gen-
eralized pattern is that patterns of higher preci-
sion tend to contain words that are indicative of
the relation being mined. Thus, a pattern is more
likely to produce good candidate facts if its in-
fix contains the words language or spoken if ex-
tracting Language-SpokenIn-Country facts, or the
word capital if extracting City-CapitalOf-Country
relations. In each acquisition iteration, the scor-
ing of patterns is a two-pass procedure. The first
pass computes the normalized frequencies of all
words excluding stopwords, over the entire set of
extraction patterns. The computation applies sep-
arately to the prefix, infix and postfix of the pat-
terns. In the second pass, the score of an extraction
pattern is determined by the words with the high-
est frequency score in its prefix, infix and postfix,
as computed in the first pass and adjusted for the
relative distance to the start and end of the infix.
3.3 Ranking of Candidate Facts
Figure 3 introduces a new scheme for assessing the
quality of the candidate facts, based on the compu-
tation of similarity scores for each candidate rela-
tive to the set of seed facts. A candidate fact, e.g.,
(Richard Steele, 1672), is similar to the seed set if
both its phrases, i.e., Richard Steele and 1672, are
similar to the corresponding phrases (John Lennon
or Stephen Foster in the case of Richard Steele)
from the seed facts. For a phrase of a candidate
fact to be assigned a non-default (non-minimum)
812
...
Lennon
Lambert
McFadden
Bateson
McNamara
Costello
Cronin
Wooley
Baker
...
Foster
Hansen
Hawkins
Fisher
Holloway
Steele
Sweeney
Chris
John
James
Andrew
Mike
Matt
Brian
Christopher
...
John Lennon         1940
Seed facts
Stephen Foster      1826
Brian McFadden           1980
(4)(3)
Robert S. McNamara    1916
(6)(5)
Barbara Steele               1937
(7) (2)
Stan Hansen                  1949
(9)(8)
Similar wordsSimilar words
for: John
Similar words
for: Stephen
for: Lennon
Similar words
for: Foster
...
Stephen
Robert
Michael
Peter
William
Stan
Richard(1)
Barbara
(3)
(5)
(7) (2)
(8)
(9)
(4)
(6)
(2)(1)
Candidate facts
Jethro Tull                     1947
Richard Steele               1672
Figure 3: The role of similarities in estimating the
quality of candidate facts
similarity score, the words at its extremities must
be similar to one or more words situated at the
same positions in the seed facts. This is the case
for the first five candidate facts in Figure 3. For ex-
ample, the first word Richard from one of the can-
didate facts is similar to the first word John from
one of the seed facts. Concurrently, the last word
Steele from the same phrase is similar to Foster
from another seed fact. Therefore Robert Foster
is similar to the seed facts. The score of a phrase
containing N words is:
{
C1 +
?N
i=1 log(1 + Simi) , if Sim1,N > 0
C2 , otherwise.
where Simi is the similarity of the component
word at position i in the phrase, and C1 and C2
are scaling constants such that C2C1. Thus,
the similarity score of a candidate fact aggregates
individual word-to-word similarity scores, for the
left side and then for the right side of a candidate
fact. In turn, the similarity score of a component
word Simi is higher if: a) the computed word-to-
word similarity scores are higher relative to words
at the same position i in the seeds; and b) the com-
ponent word is similar to words from more than
one seed fact.
The similarity scores are one of a linear com-
bination of features that induce a ranking over the
candidate facts. Three other domain-independent
features contribute to the final ranking: a) a phrase
completeness score computed statistically over the
entire set of candidate facts, which demotes candi-
date facts if any of their two sides is likely to be
incomplete (e.g., Mary Lou vs. Mary Lou Retton,
or John F. vs. John F. Kennedy); b) the average
PageRank value over all documents from which
the candidate fact is extracted; and c) the pattern-
based scores of the candidate fact. The latter fea-
ture converts the scores of the patterns extracting
the candidate fact into a score for the candidate
fact. For this purpose, it considers a fixed-length
window of words around each match of a candi-
date fact in some sentence from the text collection.
This is equivalent to analyzing all sentence con-
texts from which a candidate fact can be extracted.
For each window, the word with the highest fre-
quency score, as computed in the first pass of the
procedure for scoring the patterns, determines the
score of the candidate fact in that context. The
overall pattern-based score of a candidate fact is
the sum of the scores over all its contexts of occur-
rence, normalized by the frequency of occurrence
of the candidate over all sentences.
Besides inducing a ranking over the candidate
facts, the similarity scores also serve as a valida-
tion filter over the candidate facts. Indeed, any
candidates that are not similar to the seed set can
be filtered out. For instance, the elimination of
(Jethro Tull, 1947) is a side effect of verifying that
Tull is not similar to any of the last-position words
from phrases in the seed set.
4 Evaluation
4.1 Data
The source text collection consists of three chunks
W1, W2, W3 of approximately 100 million doc-
uments each. The documents are part of a larger
snapshot of the Web taken in 2003 by the Google
search engine. All documents are in English.
The textual portion of the documents is cleaned
of Html, tokenized, split into sentences and part-
of-speech tagged using the TnT tagger (Brants,
2000).
The evaluation involves facts of type Person-
BornIn-Year. The reasons behind the choice of
this particular type are threefold. First, many
Person-BornIn-Year facts are probably available
on the Web (as opposed to, e.g., City-CapitalOf-
Country facts), to allow for a good stress test
for large-scale extraction. Second, either side of
the facts (Person and Year) may be involved in
many other types of facts, such that the extrac-
tion would easily divergence unless it performs
correctly. Third, the phrases from one side (Per-
son) have an utility in their own right, for lexicon
813
Table 1: Set of seed Person-BornIn-Year facts
Name Year Name Year
Paul McCartney 1942 John Lennon 1940
Vincenzo Bellini 1801 Stephen Foster 1826
Hoagy Carmichael 1899 Irving Berlin 1888
Johann Sebastian Bach 1685 Bela Bartok 1881
Ludwig van Beethoven 1770 Bob Dylan 1941
construction or detection of person names.
The Person-BornIn-Year type is specified
through an initial set of 10 seed facts shown in Ta-
ble 1. Similarly to source documents, the facts are
also part-of-speech tagged.
4.2 System Settings
In each iteration, the case-insensitive matching of
the current set of seed facts onto the sentences pro-
duces basic patterns. The patterns are converted
into generalized patterns. The length of the infix
may vary between 1 and 6 words. Potential pat-
terns are discarded if the infix contains only stop-
words.
When a pattern is retained, it is used as an
infix-only pattern, and allowed to generate at most
600,000 candidate facts. At the end of an itera-
tion, approximately one third of the validated can-
didate facts are added to the current seed set. Con-
sequently, the acquisition expands the initial seed
set of 10 facts to 100,000 facts (after iteration 1)
and then to one million facts (after iteration 2) us-
ing chunk W1.
4.3 Precision
A separate baseline run extracts candidate facts
from the text collection following the traditional
iterative acquisition approach. Pattern general-
ization is disabled, and the ranking of patterns
and facts follows strictly the criteria and scoring
functions from (Thelen and Riloff, 2002), which
are also used in slightly different form in (Lita
and Carbonell, 2004) and (Agichtein and Gravano,
2000). The theoretical option of running thou-
sands of iterations over the text collection is not
viable, since it would imply a non-justifiable ex-
pense of our computational resources. As a more
realistic compromise over overly-cautious acqui-
sition, the baseline run retains as many of the top
candidate facts as the size of the current seed,
whereas (Thelen and Riloff, 2002) only add the
top five candidate facts to the seed set after each it-
eration. The evaluation considers all 80, a sample
of the 320, and another sample of the 10,240 facts
retained after iterations 3, 5 and 10 respectively.
The correctness assessment of each fact consists
in manually finding some Web page that contains
clear evidence that the fact is correct. If no such
page exists, the fact is marked as incorrect. The
corresponding precision values after the three iter-
ations are 91.2%, 83.8% and 72.9%.
For the purpose of evaluating the precision of
our system, we select a sample of facts from
the entire list of one million facts extracted from
chunk W1, ranked in decreasing order of their
computed scores. The sample is generated auto-
matically from the top of the list to the bottom, by
retaining a fact and skipping the following consec-
utive N facts, where N is incremented at each step.
The resulting list, which preserves the relative or-
der of the facts, contains 1414 facts. The 115 facts
for which a Web search engine does not return any
documents, when the name (as a phrase) and the
year are submitted together in a conjunctive query,
are discarded from the sample of 1414 facts. In
those cases, the facts were acquired from the 2003
snapshot of the Web, but queries are submitted to
a search engine with access to current Web doc-
uments, hence the difference when some of the
2003 documents are no longer available or index-
able.
Based on the sample set, the average preci-
sion of the list of one million facts extracted from
chunk W1 is 98.5% over the top 1/100 of the list,
93.1% over the top half of the list, and 88.3% over
the entire list of one million facts. Table 2 shows
examples of erroneous facts extracted from chunk
W1. Causes of errors include incorrect approxima-
tions of the name boundaries (e.g., Alma in Alma
Theresa Rausch is incorrectly tagged as an adjec-
tive), and selection of the wrong year as birth year
(e.g., for Henry Lumbar).
In the case of famous people, the extracted facts
tend to capture the correct birth year for several
variations of the names, as shown in Table 3. Con-
versely, it is not necessary that a fact occur with
high frequency in order for it to be extracted,
which is an advantage over previous approaches
that rely strongly on redundancy (cf. (Cafarella et
al., 2005)). Table 4 illustrates a few of the cor-
rectly extracted facts that occur rarely on the Web.
4.4 Recall
In contrast to the assessment of precision, recall
can be evaluated automatically, based on external
814
Table 2: Incorrect facts extracted from the Web
Spurious Fact Context in Source Sentence
(Theresa Rausch, Alma Theresa Rausch was born
1912) on 9 March 1912
(Henry Lumbar, Henry Lumbar was born 1861
1937) and died 1937
(Concepcion Paxety, Maria de la Concepcion Paxety
1817) b. 08 Dec. 1817 St. Aug., FL.
(Mae Yaeger, Ella May/Mae Yaeger was born
1872) 20 May 1872 in Mt.
(Charles Whatley, Long, Charles Whatley b. 16
1821) FEB 1821 d. 29 AUG
(HOLT George W. HOLT (new line) George W. Holt
Holt, 1845) was born in Alabama in 1845
(David Morrish David Morrish (new line)
Canadian, 1953) Canadian, b. 1953
(Mary Ann, 1838) had a daughter, Mary Ann, who
was born in Tennessee in 1838
(Mrs. Blackmore, Mrs. Blackmore was born April
1918) 28, 1918, in Labaddiey
Table 3: Birth years extracted for both
pseudonyms and corresponding real names
Pseudonym Real Name Year
Gloria Estefan Gloria Fajardo 1957
Nicolas Cage Nicolas Kim Coppola 1964
Ozzy Osbourne John Osbourne 1948
Ringo Starr Richard Starkey 1940
Tina Turner Anna Bullock 1939
Tom Cruise Thomas Cruise Mapother IV 1962
Woody Allen Allen Stewart Konigsberg 1935
lists of birth dates of various people. We start by
collecting two gold standard sets of facts. The first
set is a random set of 609 actors and their birth
years from a Web compilation (GoldA). The sec-
ond set is derived from the set of questions used
in the Question Answering track (Voorhees and
Tice, 2000) of the Text REtrieval Conference from
1999 through 2002. Each question asking for the
birth date of a person (e.g., ?What year was Robert
Frost born??) results in a pair containing the per-
son?s name and the birth year specified in the an-
swer keys. Thus, the second gold standard set
contains 17 pairs of people and their birth years
(GoldT ). Table 5 shows examples of facts in each
of the gold standard sets.
Table 6 shows two types of recall scores com-
puted against the gold standard sets. The recall
scores over ?Gold take into consideration only the
set of person names from the gold standard with
some extracted year(s). More precisely, given that
some years were extracted for a person name, it
verifies whether they include the year specified in
the gold standard for that person name. Compar-
atively, the recall score denoted AllGold is com-
Table 4: Extracted facts that occur infrequently
Fact Source Domain
(Irvine J Forcier, 1912) geocities.com
(Marie Louise Azelie Chabert, 1861) vienici.com
(Jacob Shalles, 1750) selfhost.com
(Robert Chester Claggett, 1898) rootsweb.com
(Charoltte Mollett, 1843) rootsweb.com
(Nora Elizabeth Curran, 1979) jimtravis.com
Table 5: Composition of gold standard sets
Gold Set Composition and Examples of Facts
GoldA Actors (Web compilation) Nr. facts: 609
(Andie MacDowell, 1958), (Doris Day,
1924), (Diahann Carroll, 1935)
GoldT People (TREC QA track) Nr. facts: 17
(Davy Crockett, 1786), (Julius Caesar,
100 B.C.), (King Louis XIV, 1638)
puted over the entire set of names from the gold
standard.
For the GoldA set, the size of the ?Gold set of
person names changes little when the facts are ex-
tracted from chunk W1 vs. W2 vs. W3. The re-
call scores over ?Gold exhibit little variation from
one Web chunk to another, whereas the AllGold
score is slightly higher on the W3 chunk, prob-
ably due to a higher number of documents that
are relevant to the extraction task. When the facts
are extracted from a combination of two or three
of the available Web chunks, the recall scores
computed over AllGold are significantly higher as
the size of the ?Gold set increases. In compar-
ison, the recall scores over the growing ?Gold
set increases slightly with larger evaluation sets.
The highest value of the recall score for GoldA
is 89.9% over the ?Gold set, and 70.7% over
AllGold. The smaller size of the second gold stan-
dard set, GoldT , explains the higher variation of
the values shown in the lower portion of Table 6.
4.5 Comparison to Previous Results
Another recent approach specifically addresses the
problem of extracting facts from a similarly-sized
collection of Web documents. In (Cafarella et al,
2005), manually-prepared extraction rules are ap-
plied to a collection of 60 million Web documents
to extract entities of types Company and Country,
as well as facts of type Person-CeoOf-Company
and City-CapitalOf-Country. Based on manual
evaluation of precision and recall, a total of 23,128
company names are extracted at precision of 80%;
the number decreases to 1,116 at precision of 90%.
In addition, 2,402 Person-CeoOf-Company facts
815
Table 6: Automatic evaluation of recall, over two
gold standard sets GoldA (609 person names) and
GoldT (17 person names)
Gold Set Input Data Recall (%)
(Web Chunk) ?Gold AllGold
GoldA W1 86.4 49.4
W2 85.0 50.5
W3 86.3 54.1
W1+W2 88.5 64.5
W1+W2+W3 89.9 70.7
GoldT W1 81.8 52.9
W2 90.0 52.9
W3 100.0 64.7
W1+W2 81.8 52.9
W1+W2+W3 91.6 64.7
are extracted at precision 80%. The recall value is
80% at precision 90%. Recall is evaluated against
the set of company names extracted by the system,
rather than an external gold standard with pairs of
a CEO and a company name. As such, the result-
ing metric for evaluating recall used in (Cafarella
et al, 2005) is somewhat similar to, though more
relaxed than, the recall score over the ?Gold set
introduced in the previous section.
5 Conclusion
The combination of generalized extraction pat-
terns and similarity-driven ranking criteria results
in a fast-growth iterative approach for large-scale
fact extraction. From 10 Person-BornIn-Year facts
and no additional knowledge, a set of one million
facts of the same type is extracted from a collec-
tion of 100 million Web documents of arbitrary
quality, with a precision around 90%. This cor-
responds to a growth ratio of 100,000:1 between
the size of the extracted set of facts and the size
of the initial set of seed facts. To our knowledge,
the growth ratio and the number of extracted facts
are several orders of magnitude higher than in any
of the previous studies on fact extraction based on
either hand-written extraction rules (Cafarella et
al., 2005), or bootstrapping for relation and infor-
mation extraction (Agichtein and Gravano, 2000;
Lita and Carbonell, 2004). The next research steps
converge towards the automatic construction of a
searchable repository containing billions of facts
regarding people.
References
E. Agichtein and L. Gravano. 2000. Snowball: Extracting
relations from large plaintext collections. In Proceedings
of the 5th ACM International Conference on Digital Li-
braries (DL-00), pages 85?94, San Antonio, Texas.
T. Brants. 2000. TnT - a statistical part of speech tagger.
In Proceedings of the 6th Conference on Applied Natural
Language Processing (ANLP-00), pages 224?231, Seattle,
Washington.
M. Cafarella, D. Downey, S. Soderland, and O. Etzioni.
2005. KnowItNow: Fast, scalable information extrac-
tion from the web. In Proceedings of the Human Lan-
guage Technology Conference (HLT-EMNLP-05), pages
563?570, Vancouver, Canada.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of the 1999
Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP/VLC-99),
pages 189?196, College Park, Maryland.
M. Fleischman, E. Hovy, and A. Echihabi. 2003. Offline
strategies for online question answering: Answering ques-
tions before they are asked. In Proceedings of the 41st
Annual Meeting of the Association for Computational Lin-
guistics (ACL-03), pages 1?7, Sapporo, Japan.
G. Grefenstette. 1994. Explorations in Automatic Thesaurus
Discovery. Kluwer Academic Publishers, Boston, Mas-
sachusetts.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discover-
ing relations among named entities from large corpora. In
Proceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-04), pages 415?
422, Barcelona, Spain.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of the 28th Annual
Meeting of the Association for Computational Linguistics
(ACL-90), pages 268?275, Pittsburgh, Pennsylvania.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of the 17th International Confer-
ence on Computational Linguistics and the 36th Annual
Meeting of the Association for Computational Linguistics
(COLING-ACL-98), pages 768?774, Montreal, Quebec.
L. Lita and J. Carbonell. 2004. Instance-based ques-
tion answering: A data driven approach. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-04), pages 396?403,
Barcelona, Spain.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clus-
tering of english words. In Proceedings of the 31st Annual
Meeting of the Association for Computational Linguistics
(ACL-93), pages 183?190, Columbus, Ohio.
E. Riloff and R. Jones. 1999. Learning dictionaries for in-
formation extraction by multi-level bootstrapping. In Pro-
ceedings of the 16th National Conference on Artificial In-
telligence (AAAI-99), pages 474?479, Orlando, Florida.
M. Stevenson and M. Greenwood. 2005. A semantic ap-
proach to IE pattern induction. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL-05), pages 379?386, Ann Arbor, Michigan.
M. Thelen and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-02),
pages 214?221, Philadelphia, Pennsylvania.
E.M. Voorhees and D.M. Tice. 2000. Building a question-
answering test collection. In Proceedings of the 23rd
International Conference on Research and Development
in Information Retrieval (SIGIR-00), pages 200?207,
Athens, Greece.
816
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 105?112,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Soft Syntactic Constraints for Word Alignment
through Discriminative Training
Colin Cherry
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
colinc@cs.ualberta.ca
Dekang Lin
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA, USA, 94043
lindek@google.com
Abstract
Word alignment methods can gain valu-
able guidance by ensuring that their align-
ments maintain cohesion with respect to
the phrases specified by a monolingual de-
pendency tree. However, this hard con-
straint can also rule out correct alignments,
and its utility decreases as alignment mod-
els become more complex. We use a pub-
licly available structured output SVM to
create a max-margin syntactic aligner with
a soft cohesion constraint. The resulting
aligner is the first, to our knowledge, to use
a discriminative learning method to train
an ITG bitext parser.
1 Introduction
Given a parallel sentence pair, or bitext, bilin-
gual word alignment finds word-to-word connec-
tions across languages. Originally introduced as a
byproduct of training statistical translation models
in (Brown et al, 1993), word alignment has be-
come the first step in training most statistical trans-
lation systems, and alignments are useful to a host
of other tasks. The dominant IBM alignment mod-
els (Och and Ney, 2003) use minimal linguistic in-
tuitions: sentences are treated as flat strings. These
carefully designed generative models are difficult
to extend, and have resisted the incorporation of
intuitively useful features, such as morphology.
There have been many attempts to incorporate
syntax into alignment; we will not present a com-
plete list here. Some methods parse two flat strings
at once using a bitext grammar (Wu, 1997). Others
parse one of the two strings before alignment be-
gins, and align the resulting tree to the remaining
string (Yamada and Knight, 2001). The statisti-
cal models associated with syntactic aligners tend
to be very different from their IBM counterparts.
They model operations that are meaningful at a
syntax level, like re-ordering children, but ignore
features that have proven useful in IBM models,
such as the preference to align words with simi-
lar positions, and the HMM preference for links to
appear near one another (Vogel et al, 1996).
Recently, discriminative learning technology
for structured output spaces has enabled several
discriminative word alignment solutions (Liu et
al., 2005; Moore, 2005; Taskar et al, 2005). Dis-
criminative learning allows easy incorporation of
any feature one might have access to during the
alignment search. Because the features are han-
dled so easily, discriminative methods use features
that are not tied directly to the search: the search
and the model become decoupled.
In this work, we view synchronous parsing only
as a vehicle to expose syntactic features to a dis-
criminative model. This allows us to include the
constraints that would usually be imposed by a
tree-to-string alignment method as a feature in our
model, creating a powerful soft constraint. We
add our syntactic features to an already strong
flat-string discriminative solution, and we show
that they provide new information resulting in im-
proved alignments.
2 Constrained Alignment
Let an alignment be the complete structure that
connects two parallel sentences, and a link be
one of the word-to-word connections that make
up an alignment. All word alignment methods
benefit from some set of constraints. These limit
the alignment search space and encourage com-
petition between potential links. The IBM mod-
els (Brown et al, 1993) benefit from a one-to-
many constraint, where each target word has ex-
105
the tax causes unrest
l' imp?t cause le malaise
Figure 1: A cohesion constraint violation.
actly one generator in the source. Methods like
competitive linking (Melamed, 2000) and maxi-
mum matching (Taskar et al, 2005) use a one-to-
one constraint, where words in either sentence can
participate in at most one link. Throughout this pa-
per we assume a one-to-one constraint in addition
to any syntax constraints.
2.1 Cohesion Constraint
Suppose we are given a parse tree for one of the
two sentences in our sentence pair. We will re-
fer to the parsed language as English, and the
unparsed language as Foreign. Given this infor-
mation, a reasonable expectation is that English
phrases will move together when projected onto
Foreign. When this occurs, the alignment is said
to maintain phrasal cohesion.
Fox (2002) measured phrasal cohesion in gold
standard alignments by counting crossings. Cross-
ings occur when the projections of two disjoint
phrases overlap. For example, Figure 1 shows a
head-modifier crossing: the projection of the the
tax subtree, impo?t . . . le, is interrupted by the pro-
jection of its head, cause. Alignments with no
crossings maintain phrasal cohesion. Fox?s exper-
iments show that cohesion is generally maintained
for French-English, and that dependency trees pro-
duce the highest degree of cohesion among the
tested structures.
Cherry and Lin (2003) use the phrasal cohesion
of a dependency tree as a constraint on a beam
search aligner. This constraint produces a sig-
nificant reduction in alignment error rate. How-
ever, as Fox (2002) showed, even in a language
pair as close as French-English, there are situa-
tions where phrasal cohesion should not be main-
tained. These include incorrect parses, systematic
violations such as not ? ne . . . pas, paraphrases,
and linguistic exceptions.
We aim to create an alignment system that
obeys cohesion constraints most of the time, but
can violate them when necessary. Unfortunately,
Cherry and Lin?s beam search solution does not
lend itself to a soft cohesion constraint. The im-
perfect beam search may not be able to find the
optimal alignment under a soft constraint. Further-
more, it is not clear what penalty to assign to cross-
ings, or how to learn such a penalty from an iter-
ative training process. The remainder of this pa-
per will develop a complete alignment search that
is aware of cohesion violations, and use discrimi-
native learning technology to assign a meaningful
penalty to those violations.
3 Syntax-aware Alignment Search
We require an alignment search that can find the
globally best alignment under its current objective
function, and can account for phrasal cohesion in
this objective. IBM Models 1 and 2, HMM (Vo-
gel et al, 1996), and weighted maximum matching
alignment all conduct complete searches, but they
would not be amenable to monitoring the syntac-
tic interactions of links. The tree-to-string models
of (Yamada and Knight, 2001) naturally consider
syntax, but special modeling considerations are
needed to allow any deviations from the provided
tree (Gildea, 2003). The Inversion Transduction
Grammar or ITG formalism, described in (Wu,
1997), is well suited for our purposes. ITGs per-
form string-to-string alignment, but do so through
a parsing algorithm that will allow us to inform the
objective function of our dependency tree.
3.1 Inversion Transduction Grammar
An ITG aligns bitext through synchronous pars-
ing. Both sentences are decomposed into con-
stituent phrases simultaneously, producing a word
alignment as a byproduct. Viewed generatively, an
ITG writes to two streams at once. Terminal pro-
ductions produce a token in each stream, or a token
in one stream with the null symbol ? in the other.
We will use standard ITG notation: A ? e/f in-
dicates that the token e is produced on the English
stream, while f is produced on the Foreign stream.
To allow for some degree of movement during
translation, non-terminal productions are allowed
to be either straight or inverted. Straight pro-
ductions, with their non-terminals inside square
brackets [. . .], produce their symbols in the same
order on both streams. Inverted productions, in-
dicated by angled brackets ?. . .?, have their non-
terminals produced in the given order on the En-
glish stream, but this order is reversed in the For-
eign stream.
106
the Canadian agriculture industry
l' industrie agricole Canadienne
Figure 2: An example of an ITG alignment. A
horizontal bar across an arc indicates an inversion.
An ITG chart parser provides a polynomial-
time algorithm to conduct a complete enumeration
of all alignments that are possible according to its
grammar. We will use a binary bracketing ITG, the
simplest interesting grammar in this formalism:
A? [AA] | ?AA? | e/f
This grammar enforces its own weak cohesion
constraint: for every possible alignment, a corre-
sponding binary constituency tree must exist for
which the alignment maintains phrasal cohesion.
Figure 2 shows a word alignment and the corre-
sponding tree found by an ITG parser. Wu (1997)
provides anecdotal evidence that only incorrect
alignments are eliminated by ITG constraints. In
our French-English data set, an ITG rules out
only 0.3% of necessary links beyond those already
eliminated by the one-to-one constraint (Cherry
and Lin, 2006).
3.2 Dependency-augmented ITG
An ITG will search all alignments that conform
to a possible binary constituency tree. We wish
to confine that search to a specific n-array depen-
dency tree. Fortunately, Wu (1997) provides a
method to have an ITG respect a known partial
structure. One can seed the ITG parse chart so that
spans that do not agree with the provided structure
are assigned a value of?? before parsing begins.
The result is that no constituent is ever constructed
with any of these invalid spans.
In the case of phrasal cohesion, the invalid spans
correspond to spans of the English sentence that
interrupt the phrases established by the provided
dependency tree. To put this notion formally, we
first define some terms: given a subtree T[i,k],
where i is the left index of the leftmost leaf in T[i,k]
and k is the right index of its rightmost leaf, we say
any index j ? (i, k) is internal to T[i,k]. Similarly,
any index x /? [i, k] is external to T[i,k]. An in-
valid span is any span for which our provided tree
T[i,k]
x1 i j k x2j'
T
Figure 3: Illustration of invalid spans. [j?, j] and
[j, k] are legal, while [x1, j] and [j, x2] are not.
the tax causes unrest
Figure 4: The invalid spans induced by a depen-
dency tree.
has a subtree T[i,k] such that one endpoint of the
span is internal to T[i,k] while the other is external
to it. Figure 3 illustrates this definition, while Fig-
ure 4 shows the invalid spans induced by a simple
dependency tree.
With these invalid spans in place, the ITG can
no longer merge part of a dependency subtree with
anything other than another part of the same sub-
tree. Since all ITG movement can be explained
by inversions, this constrained ITG cannot in-
terrupt one dependency phrase with part of an-
other. Therefore, the phrasal cohesion of the in-
put dependency tree is maintained. Note that this
will not search the exact same alignment space
as a cohesion-constrained beam search; instead it
uses the union of the cohesion constraint and the
weaker ITG constraints (Cherry and Lin, 2006).
Transforming this form of the cohesion con-
straint into a soft constraint is straight-forward.
Instead of overriding the parser so it cannot use
invalid English spans, we will note the invalid
spans and assign the parser a penalty should it
use them. The value of this penalty will be de-
termined through discriminative training, as de-
scribed in Section 4. Since the penalty is avail-
able within the dynamic programming algorithm,
the parser will be able to incorporate it to find a
globally optimal alignment.
4 Discriminative Training
To discriminatively train our alignment systems,
we adopt the Support Vector Machine (SVM) for
107
Structured Output (Tsochantaridis et al, 2004).
We have selected this system for its high degree of
modularity, and because it has an API freely avail-
able1. We will summarize the learning mechanism
briefly in this section, but readers should refer to
(Tsochantaridis et al, 2004) for more details.
SVM learning is most easily expressed as a con-
strained numerical optimization problem. All con-
straints mentioned in this section are constraints
on this optimizer, and have nothing to do with the
cohesion constraint from Section 2.
4.1 SVM for Structured Output
Traditional SVMs attempt to find a linear sepa-
rator that creates the largest possible margin be-
tween two classes of vectors. Structured output
SVMs attempt to separate the correct structure
from all incorrect structures by the largest possible
margin, for all training instances. This may sound
like a much more difficult problem, but with a few
assumptions in place, the task begins to look very
similar to a traditional SVM.
As in most discriminative training methods, we
begin by assuming that a candidate structure y,
built for an input instance x, can be adequately de-
scribed using a feature vector ?(x, y). We also as-
sume that our ?(x, y) decomposes in such a way
that the features can guide a search to recover the
structure y from x. That is:
struct(x; ~w) = argmaxy?Y ?~w,?(x, y)? (1)
is computable, where Y is the set of all possible
structures, and ~w is a vector that assigns weights
to each component of ?(x, y). ~w is the parameter
vector we will learn using our SVM.
Now the learning task begins to look straight-
forward: we are working with vectors, and the
task of building a structure y has been recast as
an argmax operator. Our learning goal is to find a
~w so that the correct structure is found:
?i, ?y ? Y \ yi : ?~w,?i(yi)? > ?~w,?i(y)? (2)
where xi is the ith training example, yi is its
correct structure, and ?i(y) is short-hand for
?(xi, y). As several ~w will fulfill (2) in a linearly
separable training set, the unique max-margin ob-
jective is defined to be the ~w that maximizes the
minimum distance between yi and the incorrect
structures in Y .
1At http://svmlight.joachims.org/svm struct.html
This learning framework also incorporates a no-
tion of structured loss. In a standard vector clas-
sification problem, there is 0-1 loss: a vector is
either classified correctly or it is not. In the struc-
tured case, some incorrect structures can be bet-
ter than others. For example, having the argmax
select an alignment missing only one link is bet-
ter than selecting one with no correct links and a
dozen wrong ones. A loss function ?(yi, y) quan-
tifies just how incorrect a particular structure y is.
Though Tsochantaridis et al (2004) provide sev-
eral ways to incorporate loss into the SVM ob-
jective, we will use margin re-scaling, as it corre-
sponds to loss usage in another max-margin align-
ment approach (Taskar et al, 2005). In margin
re-scaling, high loss structures must be separated
from the correct structure by a larger margin than
low loss structures.
To allow some misclassifications during train-
ing, a soft-margin requirement replaces our max-
margin objective. A slack variable ?i is introduced
for each training example xi, to allow the learner
to violate the margin at a penalty. The magnitude
of this penalty to determined by a hand-tuned pa-
rameter C. After a few transformations (Tsochan-
taridis et al, 2004), the soft-margin learning ob-
jective can be formulated as a quadratic program:
min~w,?
1
2
||~w||2 +
C
n
n?
i=1
?i, s.t. ?i?i ? 0 (3)
?i, ?y ? Y \ yi : (4)
?~w,?i(yi)??i(y)? ? ?(yi, y)? ?i
Note how the slack variables ?i allow some in-
correct structures to be built. Also note that the
loss ?(yi, y) determines the size of the margin be-
tween structures.
Unfortunately, (4) provides one constraint for
every possible structure for every training exam-
ple. Enumerating these constraints explicitly is in-
feasible, but in reality, only a subset of these con-
straints are necessary to achieve the same objec-
tive. Re-organizing (4) produces:
?i,?y ? Y \ yi :
?i ? ?(yi, y)? ?~w,?i(yi)??i(y)?
(5)
which is equivalent to:
?i : ?i ? max
y?Y\yi
costi(y; ~w) (6)
where costi is defined as:
costi(y; ~w) = ?(yi, y)? ?~w,?i(yi)??i(y)?
108
Provided that the max cost structure can be found
in polynomial time, we have all the components
needed for a constraint generation approach to this
optimization problem.
Constraint generation places an outer loop
around an optimizer that minimizes (3) repeatedly
for a growing set of constraints. It begins by min-
imizing (3) with an empty constraint set in place
of (4). This provides values for ~w and ~?. The max
cost structure
y? = argmaxy?Y\yicosti(y; ~w)
is found for i = 1 with the current ~w. If the re-
sulting costi(y?; ~w) is greater than the current value
of ?i, then this represents a violated constraint2 in
our complete objective, and a new constraint of
the form ?i ? costi(y?; ~w) is added to the con-
straint set. The algorithm then iterates: the opti-
mizer minimizes (3) again with the new constraint
set, and solves the max cost problem for i = i+ 1
with the new ~w, growing the constraint set if nec-
essary. Note that the constraints on ? change with
~w, as cost is a function of ~w. Once the end of
the training set is reached, the learner loops back
to the beginning. Learning ends when the entire
training set can be processed without needing to
add any constraints. It can be shown that this
will occur within a polynomial number of itera-
tions (Tsochantaridis et al, 2004).
With this framework in place, one need only fill
in the details to create an SVM for a new struc-
tured output space:
1. A ?(x, y) function to transform instance-
structure pairs into feature vectors
2. A search to find the best structure given a
weight vector: argmaxy ?~w,?(x, y)?. This
has no role in training, but it is necessary to
use the learned weights.
3. A structured loss function ?(y, y?)
4. A search to find the max cost structure:
argmaxycosti(y;w)
4.2 SVMs for Alignment
Using the Structured SVM API, we have created
two SVM word aligners: a baseline that uses
weighted maximum matching for its argmax op-
erator, and a dependency-augmented ITG that will
2Generally the test to see if ?i > costi(y?; ~w) is approxi-
mated as ?i > costi(y?; ~w) + ? for a small constant ?.
satisfy our requirements for an aligner with a soft
cohesion constraint. Our x becomes a bilingual
sentence-pair, while our y becomes an alignment,
represented by a set of links.
4.2.1 Weighed Maximum Matching
Given a bipartite graph with edge values, the
weighted maximum matching algorithm (West,
2001) will find the matching with maximum
summed edge values. To create a matching align-
ment solution, we reproduce the approach of
(Taskar et al, 2005) within the framework de-
scribed in Section 4.1:
1. We define a feature vector ? for each poten-
tial link l in x, and ? in terms of y?s compo-
nent links: ?(x, y) =
?
l?y ?(l).
2. Our structure search is the matching algo-
rithm. The input bipartite graph has an edge
for each l. Each edge is given the value
v(l)? ?~w, ?(l)?.
3. We adopt the weighted Hamming loss in de-
scribed (Taskar et al, 2005):
?(y, y?) = co|y ? y?|+ cc|y? ? y|
where co is an omission penalty and cc is a
commission penalty.
4. Our max cost search corresponds to their
loss-augmented matching problem. The in-
put graph is modified to prefer costly links:
?l /? y : v(l)? ?~w, ?(l)?+ cc
?l ? y : v(l)? ?~w, ?(l)? ? co
Note that our max cost search could not have been
implemented as loss-augmented matching had we
selected one of the other loss objectives presented
in (Tsochantaridis et al, 2004) in place of margin
rescaling.
We use the same feature representation ?(l) as
(Taskar et al, 2005), with some small exceptions.
Let l = (Ej , Fk) be a potential link between the
jth word of English sentence E and the kth word
of Foreign sentence F . To measure correlation be-
tween Ej and Fk we use conditional link proba-
bility (Cherry and Lin, 2003) in place of the Dice
coefficient:
cor(Ej , Fk) =
#links(Ej , Fk)? d
#cooccurrences(Ej , Fk)
where the link counts are determined by word-
aligning 50K sentence pairs with another match-
ing SVM that uses the ?2 measure (Gale and
109
Church, 1991) in place of Dice. The ?2 measure
requires only co-occurrence counts. d is an abso-
lute discount parameter as in (Moore, 2005). Also,
we omit the IBM Model 4 Prediction features, as
we wish to know how well we can do without re-
sorting to traditional word alignment techniques.
Otherwise, the features remain the same,
including distance features that measure
abs
(
j
|E| ?
k
|F |
)
; orthographic features; word
frequencies; common-word features; a bias term
set alays to 1; and an HMM approximation
cor(Ej+1, Fk+1).
4.2.2 Soft Dependency-augmented ITG
Because of the modularity of the structured out-
put SVM, our SVM ITG re-uses a large amount
infrastructure from the matching solution. We
essentially plug an ITG parser in the place of
the matching algorithm, and add features to take
advantage of information made available by the
parser. x remains a sentence pair, and y becomes
an ITG parse tree that decomposes x and speci-
fies an alignment. Our required components are as
follows:
1. We define a feature vector ?T on instances
of production rules, r. ? is a function of
the decomposition specified by y: ?(x, y) =
?
r?y ?T (r).
2. The structure search is a weighted ITG parser
that maximizes summed production scores.
Each instance of a production rule r is as-
signed a score of ?~w, ?T (r)?
3. Loss is unchanged, defined in terms of the
alignment induced by y.
4. A loss-augmented ITG is used to find the max
cost. Productions of the form A ? e/f
that correspond to links have their scores aug-
mented as in the matching system.
The ?T vector has two new features in addition to
those present in the matching system?s ?. These
features can be active only for non-terminal pro-
ductions, which have the formA? [AA] | ?AA?.
One feature indicates an inverted production A?
?AA?, while the other indicates the use of an in-
valid span according to a provided English depen-
dency tree, as described in Section 3.2. These
are the only features that can be active for non-
terminal productions.
A terminal production rl that corresponds to a
link l is given that link?s features from the match-
ing system: ?T (rl) = ?(l). Terminal productions
r? corresponding to unaligned tokens are given
blank feature vectors: ?T (r?) = ~0.
The SVM requires complete ? vectors for the
correct training structures. Unfortunately, our
training set contains gold standard alignments, not
ITG parse trees. The gold standard is divided into
sure and possible link sets S and P (Och and Ney,
2003). Links in S must be included in a correct
alignment, while P links are optional. We create
ITG trees from the gold standard using the follow-
ing sorted priorities during tree construction:
? maximize the number of links from S
? minimize the number of English dependency
span violations
? maximize the number of links from P
? minimize the number of inversions
This creates trees that represent high scoring align-
ments, using a minimal number of invalid spans.
Only the span and inversion counts of these trees
will be used in training, so we need not achieve a
perfect tree structure. We still evaluate all methods
with the original alignment gold standard.
5 Experiments and Results
We conduct two experiments. The first tests
the dependency-augmented ITG described in Sec-
tion 3.2 as an aligner with hard cohesion con-
straints. The second tests our discriminative ITG
with soft cohesion constraints against two strong
baselines.
5.1 Experimental setup
We conduct our experiments using French-English
Hansard data. Our ?2 scores, link probabilities
and word frequency counts are determined using a
sentence-aligned bitext consisting of 50K sentence
pairs. Our training set for the discriminative align-
ers is the first 100 sentence pairs from the French-
English gold standard provided for the 2003 WPT
workshop (Mihalcea and Pedersen, 2003). For
evaluation we compare to the remaining 347 gold
standard pairs using the alignment evaluation met-
rics: precision, recall and alignment error rate or
AER (Och and Ney, 2003). SVM learning param-
eters are tuned using the 37-pair development set
provided with this data. English dependency trees
are provided by Minipar (Lin, 1994).
110
Table 1: The effect of hard cohesion constraints on
a simple unsupervised link score.
Search Prec Rec AER
Matching 0.723 0.845 0.231
ITG 0.764 0.860 0.200
D-ITG 0.830 0.873 0.153
5.2 Hard Constraint Performance
The goal of this experiment is to empirically con-
firm that the English spans marked invalid by
Section 3.2?s dependency-augmented ITG provide
useful guidance to an aligner. To do so, we
compare an ITG with hard cohesion constraints,
an unconstrained ITG, and a weighted maximum
matching aligner. All aligners use the same sim-
ple objective function. They maximize summed
link values v(l), where v(l) is defined as follows
for an l = (Ej , Fk):
v(l) = ?2(Ej , Fk)? 10
?5abs
(
j
|E|
?
k
|F |
)
All three aligners link based on ?2 correlation
scores, breaking ties in favor of closer pairs. This
allows us to evaluate the hard constraints outside
the context of supervised learning.
Table 1 shows the results of this experiment.
We can see that switching the search method
from weighted maximum matching to a cohesion-
constrained ITG (D-ITG) has produced a 34% rel-
ative reduction in alignment error rate. The bulk
of this improvement results from a substantial in-
crease in precision, though recall has also gone up.
This indicates that these cohesion constraints are a
strong alignment feature. The ITG row shows that
the weaker ITG constraints are also valuable, but
the cohesion constraint still improves on them.
5.3 Soft Constraint Performance
We now test the performance of our SVM ITG
with soft cohesion constraint, or SD-ITG, which
is described in Section 4.2.2. We will test against
two strong baselines. The first baseline, matching
is the matching SVM described in Section 4.2.1,
which is a re-implementation of the state-of-the-
art work in (Taskar et al, 2005)3. The second
baseline, D-ITG is an ITG aligner with hard co-
hesion constraints, but which uses the weights
3Though it is arguably lacking one of its strongest fea-
tures: the output of GIZA++ (Och and Ney, 2003)
Table 2: The performance of SVM-trained align-
ers with various degrees of cohesion constraint.
Method Prec Rec AER
Matching 0.916 0.860 0.110
D-ITG 0.940 0.854 0.100
SD-ITG 0.944 0.878 0.086
trained by the matching SVM to assign link val-
ues. This is the most straight-forward way to com-
bine discriminative training with the hard syntactic
constraints.
The results are shown in Table 2. The first thing
to note is that our Matching baseline is achieving
scores in line with (Taskar et al, 2005), which re-
ports an AER of 0.107 using similar features and
the same training and test sets.
The effect of the hard cohesion constraint has
been greatly diminished after discriminative train-
ing. Matching and D-ITG correspond to the the
entries of the same name in Table 1, only with a
much stronger, learned value function v(l). How-
ever, in place of a 34% relative error reduction, the
hard constraints in the D-ITG produce only a 9%
reduction from 0.110 to 0.100. Also note that this
time the hard constraints result in a reduction in
recall. This indicates that the hard cohesion con-
straint is providing little guidance not provided by
other features, and that it is actually eliminating
more sure links than it is helping to find.
The soft-constrained SD-ITG, which has access
to the D-ITG?s invalid spans as a feature during
SVM training, is fairing substantially better. Its
AER of 0.086 represents a 22% relative error re-
duction compared to the matching system. The
improved error rate is caused by gains in both pre-
cision and recall. This indicates that the invalid
span feature is doing more than just ruling out
links; perhaps it is de-emphasizing another, less
accurate feature?s role. The SD-ITG overrides the
cohesion constraint in only 41 of the 347 test sen-
tences, so we can see that it is indeed a soft con-
straint: it is obeyed nearly all the time, but it can be
broken when necessary. The SD-ITG achieves by
far the strongest ITG alignment result reported on
this French-English set; surpassing the 0.16 AER
reported in (Zhang and Gildea, 2004).
Training times for this system are quite low; un-
supervised statistics can be collected quickly over
a large set, while only the 100-sentence training
111
set needs to be iteratively aligned. Our match-
ing SVM trains in minutes on a single-processor
machine, while the SD-ITG trains in roughly one
hour. The ITG is the bottleneck, so training time
could be improved by optimizing the parser.
6 Related Work
Several other aligners have used discriminative
training. Our work borrows heavily from (Taskar
et al, 2005), which uses a max-margin approach
with a weighted maximum matching aligner.
(Moore, 2005) uses an averaged perceptron for
training with a customized beam search. (Liu et
al., 2005) uses a log-linear model with a greedy
search. To our knowledge, ours is the first align-
ment approach to use this highly modular struc-
tured SVM, and the first discriminative method to
use an ITG for the base aligner.
(Gildea, 2003) presents another aligner with a
soft syntactic constraint. This work adds a cloning
operation to the tree-to-string generative model in
(Yamada and Knight, 2001). This allows subtrees
to move during translation. As the model is gen-
erative, it is much more difficult to incorporate a
wide variety of features as we do here. In (Zhang
and Gildea, 2004), this model was tested on the
same annotated French-English sentence pairs that
we divided into training and test sets for our exper-
iments; it achieved an AER of 0.15.
7 Conclusion
We have presented a discriminative, syntactic
word alignment method. Discriminative training
is conducted using a highly modular SVM for
structured output, which allows code reuse be-
tween the syntactic aligner and a maximum match-
ing baseline. An ITG parser is used for the align-
ment search, exposing two syntactic features: the
use of inverted productions, and the use of spans
that would not be available in a tree-to-string sys-
tem. This second feature creates a soft phrasal co-
hesion constraint. Discriminative training allows
us to maintain all of the features that are useful to
the maximum matching baseline in addition to the
new syntactic features. We have shown that these
features produce a 22% relative reduction in error
rate with respect to a strong flat-string model.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?312.
C. Cherry and D. Lin. 2003. A probability model to improve
word alignment. In Meeting of the Association for Com-
putational Linguistics, pages 88?95, Sapporo, Japan, July.
C. Cherry and D. Lin. 2006. A comparison of syntacti-
cally motivated word alignment spaces. In Proceedings
of EACL, pages 145?152, Trento, Italy, April.
H. J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of EMNLP, pages 304?311.
W. A. Gale and K. W. Church. 1991. Identifying word cor-
respondences in parallel texts. In 4th Speech and Natural
Language Workshop, pages 152?157. DARPA.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In Meeting of the Association for Computa-
tional Linguistics, pages 80?87, Sapporo, Japan.
D. Lin. 1994. Principar - an efficient, broad-coverage,
principle-based parser. In Proceedings of COLING, pages
42?48, Kyoto, Japan.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models for word
alignment. In Meeting of the Association for Computa-
tional Linguistics, pages 459?466, Ann Arbor, USA.
I. D. Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?
249.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In HLT-NAACL Workshop on
Building and Using Parallel Texts, pages 1?10, Edmon-
ton, Canada.
R. Moore. 2005. A discriminative framework for bilingual
word alignment. In Proceedings of HLT-EMNLP, pages
81?88, Vancouver, Canada, October.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational Lin-
guistics, 29(1):19?52, March.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discrimi-
native matching approach to word alignment. In Proceed-
ings of HLT-EMNLP, pages 73?80, Vancouver, Canada.
I. Tsochantaridis, T. Hofman, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdependent
and structured output spaces. In Proceedings of ICML,
pages 823?830.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceedings
of COLING, pages 836?841, Copenhagen, Denmark.
D. West. 2001. Introduction to Graph Theory. Prentice Hall,
2nd edition.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
K. Yamada and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Meeting of the Association for
Computational Linguistics, pages 523?530.
H. Zhang and D. Gildea. 2004. Syntax-based alignment:
Supervised or unsupervised? In Proceedings of COLING,
Geneva, Switzerland, August.
112
Proceedings of ACL-08: HLT, pages 10?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Distributional Identification of Non-Referential Pronouns
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View
California, 94301
lindek@google.com
Randy Goebel
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
goebel@cs.ualberta.ca
Abstract
We present an automatic approach to deter-
mining whether a pronoun in text refers to
a preceding noun phrase or is instead non-
referential. We extract the surrounding tex-
tual context of the pronoun and gather, from
a large corpus, the distribution of words that
occur within that context. We learn to reliably
classify these distributions as representing ei-
ther referential or non-referential pronoun in-
stances. Despite its simplicity, experimental
results on classifying the English pronoun it
show the system achieves the highest perfor-
mance yet attained on this important task.
1 Introduction
The goal of coreference resolution is to determine
which noun phrases in a document refer to the same
real-world entity. As part of this task, coreference
resolution systems must decide which pronouns re-
fer to preceding noun phrases (called antecedents)
and which do not. In particular, a long-standing
challenge has been to correctly classify instances of
the English pronoun it. Consider the sentences:
(1) You can make it in advance.
(2) You can make it in Hollywood.
In sentence (1), it is an anaphoric pronoun refer-
ring to some previous noun phrase, like ?the sauce?
or ?an appointment.? In sentence (2), it is part of the
idiomatic expression ?make it? meaning ?succeed.?
A coreference resolution system should find an an-
tecedent for the first it but not the second. Pronouns
that do not refer to preceding noun phrases are called
non-anaphoric or non-referential pronouns.
The word it is one of the most frequent words in
the English language, accounting for about 1% of
tokens in text and over a quarter of all third-person
pronouns.1 Usually between a quarter and a half of
it instances are non-referential (e.g. Section 4, Ta-
ble 3). As with other pronouns, the preceding dis-
course can affect it?s interpretation. For example,
sentence (2) can be interpreted as referential if the
preceding sentence is ?You want to make a movie??
We show, however, that we can reliably classify a
pronoun as being referential or non-referential based
solely on the local context surrounding the pronoun.
We do this by turning the context into patterns and
enumerating all the words that can take the place of
it in these patterns. For sentence (1), we can ex-
tract the context pattern ?make * in advance? and
for sentence (2) ?make * in Hollywood,? where ?*?
is a wildcard that can be filled by any token. Non-
referential distributions tend to have the word it fill-
ing the wildcard position. Referential distributions
occur with many other noun phrase fillers. For ex-
ample, in our n-gram collection (Section 3.4), ?make
it in advance? and ?make them in advance? occur
roughly the same number of times (442 vs. 449), in-
dicating a referential pattern. In contrast, ?make it in
Hollywood? occurs 3421 times while ?make them in
Hollywood? does not occur at all.
These simple counts strongly indicate whether an-
other noun can replace the pronoun. Thus we can
computationally distinguish between a) pronouns
that refer to nouns, and b) all other instances: includ-
ing those that have no antecedent, like sentence (2),
1e.g. http://ucrel.lancs.ac.uk/bncfreq/flists.html
10
and those that refer to sentences, clauses, or implied
topics of discourse. Beyond the practical value of
this distinction, Section 3 provides some theoretical
justification for our binary classification.
Section 3 also shows how to automatically extract
and collect counts for context patterns, and how to
combine the information using a machine learned
classifier. Section 4 describes our data for learning
and evaluation, It-Bank: a set of over three thousand
labelled instances of the pronoun it from a variety
of text sources. Section 4 also explains our com-
parison approaches and experimental methodology.
Section 5 presents our results, including an interest-
ing comparison of our system to human classifica-
tion given equivalent segments of context.
2 Related Work
The difficulty of non-referential pronouns has been
acknowledged since the beginning of computational
resolution of anaphora. Hobbs (1978) notes his algo-
rithm does not handle pronominal references to sen-
tences nor cases where it occurs in time or weather
expressions. Hirst (1981, page 17) emphasizes the
importance of detecting non-referential pronouns,
?lest precious hours be lost in bootless searches
for textual referents.? Mu?ller (2006) summarizes
the evolution of computational approaches to non-
referential it detection. In particular, note the pio-
neering work of Paice and Husk (1987), the inclu-
sion of non-referential it detection in a full anaphora
resolution system by Lappin and Leass (1994), and
the machine learning approach of Evans (2001).
There has recently been renewed interest in
non-referential pronouns, driven by three primary
sources. First of all, research in coreference resolu-
tion has shown the benefits of modules for general
noun anaphoricity determination (Ng and Cardie,
2002; Denis and Baldridge, 2007). Unfortunately,
these studies handle pronouns inadequately; judg-
ing from the decision trees and performance fig-
ures, Ng and Cardie (2002)?s system treats all pro-
nouns as anaphoric by default. Secondly, while
most pronoun resolution evaluations simply exclude
non-referential pronouns, recent unsupervised ap-
proaches (Cherry and Bergsma, 2005; Haghighi and
Klein, 2007) must deal with all pronouns in unre-
stricted text, and therefore need robust modules to
automatically handle non-referential instances. Fi-
nally, reference resolution has moved beyond writ-
ten text into in spoken dialog. Here, non-referential
pronouns are pervasive. Eckert and Strube (2000)
report that in the Switchboard corpus, only 45%
of demonstratives and third-person pronouns have a
noun phrase antecedent. Handling the common non-
referential instances is thus especially vital.
One issue with systems for non-referential detec-
tion is the amount of language-specific knowledge
that must be encoded. Consider a system that jointly
performs anaphora resolution and word alignment
in parallel corpora for machine translation. For this
task, we need to identify non-referential anaphora in
multiple languages. It is not always clear to what
extent the features and modules developed for En-
glish systems apply to other languages. For exam-
ple, the detector of Lappin and Leass (1994) labels a
pronoun as non-referential if it matches one of sev-
eral syntactic patterns, including: ?It is Cogv-ed that
Sentence,? where Cogv is a ?cognitive verb? such
as recommend, think, believe, know, anticipate, etc.
Porting this approach to a new language would re-
quire not only access to a syntactic parser and a list
of cognitive verbs in that language, but the devel-
opment of new patterns to catch non-referential pro-
noun uses that do not exist in English.
Moreover, writing a set of rules to capture this
phenomenon is likely to miss many less-common
uses. Alternatively, recent machine-learning ap-
proaches leverage a more general representation of
a pronoun instance. For example, Mu?ller (2006)
has a feature for ?distance to next complementizer
(that, if, whether)? and features for the tokens and
part-of-speech tags of the context words. Unfor-
tunately, there is still a lot of implicit and explicit
English-specific knowledge needed to develop these
features, including, for example, lists of ?seem?
verbs such as appear, look, mean, happen. Sim-
ilarly, the machine-learned system of Boyd et al
(2005) uses a set of ?idiom patterns? like ?on the
face of it? that trigger binary features if detected in
the pronoun context. Although machine learned sys-
tems can flexibly balance the various indicators and
contra-indicators of non-referentiality, a particular
feature is only useful if it is relevant to an example
in limited labelled training data.
Our approach avoids hand-crafting a set of spe-
11
cific indicator features; we simply use the distribu-
tion of the pronoun?s context. Our method is thus
related to previous work based on Harris (1985)?s
distributional hypothesis.2 It has been used to deter-
mine both word and syntactic path similarity (Hin-
dle, 1990; Lin, 1998a; Lin and Pantel, 2001). Our
work is part of a trend of extracting other important
information from statistical distributions. Dagan and
Itai (1990) use the distribution of a pronoun?s con-
text to determine which candidate antecedents can fit
the context. Bergsma and Lin (2006) determine the
likelihood of coreference along the syntactic path
connecting a pronoun to a possible antecedent, by
looking at the distribution of the path in text. These
approaches, like ours, are ways to inject sophisti-
cated ?world knowledge? into anaphora resolution.
3 Methodology
3.1 Definition
Our approach distinguishes contexts where pro-
nouns cannot be replaced by a preceding noun
phrase (non-noun-referential) from those where
nouns can occur (noun-referential). Although coref-
erence evaluations, such as the MUC (1997) tasks,
also make this distinction, it is not necessarily
used by all researchers. Evans (2001), for exam-
ple, distinguishes between ?clause anaphoric? and
?pleonastic? as in the following two instances:
(3) The paper reported that it had snowed. It was
obvious. (clause anaphoric)
(4) It was obvious that it had snowed. (pleonastic)
The word It in sentence (3) is considered referen-
tial, while the word It in sentence (4) is considered
non-referential.3 From our perspective, this inter-
pretation is somewhat arbitrary. One could also say
that the It in both cases refers to the clause ?that it
had snowed.? Indeed, annotation experiments using
very fine-grained categories show low annotation re-
liability (Mu?ller, 2006). On the other hand, there
is no debate over the importance nor the definition
of distinguishing pronouns that refer to nouns from
those that do not. We adopt this distinction for our
2Words occurring in similar contexts have similar meanings
3The it in ?it had snowed? is, of course, non-referential.
work, and show it has good inter-annotator reliabil-
ity (Section 4.1). We henceforth refer to non-noun-
referential simply as non-referential, and thus con-
sider the word It in both sentences (3) and (4) as
non-referential.
Non-referential pronouns are widespread in nat-
ural language. The es in the German ?Wie geht es
Ihnen? and the il in the French ?S?il vous pla??t? are
both non-referential. In pro-drop languages that may
omit subject pronouns, there remains the question
of whether an omitted pronoun is referential (Zhao
and Ng, 2007). Although we focus on the English
pronoun it, our approach should differentiate any
words that have both a structural and a referential
role in language, e.g. words like this, there and
that (Mu?ller, 2007). We believe a distributional ap-
proach could also help in related tasks like identify-
ing the generic use of you (Gupta et al, 2007).
3.2 Context Distribution
Our method extracts the context surrounding a pro-
noun and determines which other words can take the
place of the pronoun in the context. The extracted
segments of context are called context patterns. The
words that take the place of the pronoun are called
pattern fillers. We gather pattern fillers from a large
collection of n-gram frequencies. The maximum
size of a context pattern depends on the size of n-
grams available in the data. In our n-gram collection
(Section 3.4), the lengths of the n-grams range from
unigrams to 5-grams, so our maximum pattern size
is five. For a particular pronoun in text, there are five
possible 5-grams that span the pronoun. For exam-
ple, in the following instance of it:
... said here Thursday that it is unnecessary to continue ...
We can extract the following 5-gram patterns:
said here Thursday that *
here Thursday that * is
Thursday that * is unnecessary
that * is unnecessary to
* is unnecessary to continue
Similarly, we extract the four 4-gram patterns.
Shorter n-grams were not found to improve perfor-
mance on development data and hence are not ex-
tracted. We only use context within the current sen-
tence (including the beginning-of-sentence and end-
of-sentence tokens) so if a pronoun occurs near a
sentence boundary, some patterns may be missing.
12
Pattern Filler Type String
#1: 3rd-person pron. sing. it/its
#2: 3rd-person pron. plur. they/them/their
#3: any other pronoun he/him/his/,
I/me/my, etc.
#4: infrequent word token ?UNK?
#5: any other token *
Table 1: Pattern filler types
We take a few steps to improve generality. We
change the patterns to lower-case, convert sequences
of digits to the # symbol, and run the Porter stem-
mer4 (Porter, 1980). To generalize rare names, we
convert capitalized words longer than five charac-
ters to a special NE tag. We also added a few simple
rules to stem the irregular verbs be, have, do, and
said, and convert the common contractions ?nt, ?s,
?m, ?re, ?ve, ?d, and ?ll to their most likely stem.
We do the same processing to our n-gram corpus.
We then find all n-grams matching our patterns, al-
lowing any token to match the wildcard in place of
it. Also, other pronouns in the pattern are allowed
to match a corresponding pronoun in an n-gram, re-
gardless of differences in inflection and class.
We now discuss how to use the distribution of pat-
tern fillers. For identifying non-referential it in En-
glish, we are interested in how often it occurs as a
pattern filler versus other nouns. However, deter-
mining part-of-speech in a large n-gram corpus is
not simple, nor would it easily extend to other lan-
guages. Instead, we gather counts for five differ-
ent classes of words that fill the wildcard position,
easily determined by string match (Table 1). The
third-person plural they (#2) reliably occurs in pat-
terns where referential it also resides. The occur-
rence of any other pronoun (#3) guarantees that at
the very least the pattern filler is a noun. A match
with the infrequent word token ?UNK? (#4) (ex-
plained in Section 3.4) will likely be a noun because
nouns account for a large proportion of rare words in
a corpus. Gathering any other token (#5) also mostly
finds nouns; inserting another part-of-speech usually
4Adapted from the Bow-toolkit (McCallum, 1996). Our
method also works without the stemmer; we simply truncate
the words in the pattern at a given maximum length (see Sec-
tion 5.1). With simple truncation, all the pattern processing can
be easily applied to other languages.
Pattern Filler Counts#1 #2 #3 #5
sai here NE that * 84 0 291 3985
here NE that * be 0 0 0 93
NE that * be unnecessari 0 0 0 0
that * be unnecessari to 16726 56 0 228
* be unnecessari to continu 258 0 0 0
Table 2: 5-gram context patterns and pattern-filler counts
for the Section 3.2 example.
results in an unlikely, ungrammatical pattern.
Table 2 gives the stemmed context patterns for our
running example. It also gives the n-gram counts
of pattern fillers matching the first four filler types
(there were no matches of the ?UNK? type, #4).
3.3 Feature Vector Representation
There are many possible ways to use the above
counts. Intuitively, our method should identify as
non-referential those instances that have a high pro-
portion of fillers of type #1 (i.e., the word it), while
labelling as referential those with high counts for
other types of fillers. We would also like to lever-
age the possibility that some of the patterns may be
more predictive than others, depending on where the
wildcard lies in the pattern. For example, in Table 2,
the cases where the it-position is near the beginning
of the pattern best reflect the non-referential nature
of this instance. We can achieve these aims by or-
dering the counts in a feature vector, and using a la-
belled set of training examples to learn a classifier
that optimally weights the counts.
For classification, we define non-referential as
positive and referential as negative. Our feature rep-
resentation very much resembles Table 2. For each
of the five 5-gram patterns, ordered by the position
of the wildcard, we have features for the logarithm
of counts for filler types #1, #2, ... #5. Similarly,
for each of the four 4-gram patterns, we provide the
log-counts corresponding to types #1, #2, ... #5 as
well. Before taking the logarithm, we smooth the
counts by adding a fixed number to all observed val-
ues. We also provide, for each pattern, a feature that
indicates if the pattern is not available because the
it-position would cause the pattern to span beyond
the current sentence. There are twenty-five 5-gram,
twenty 4-gram, and nine indicator features in total.
13
Our classifier should learn positive weights on the
type #1 counts and negative weights on the other
types, with higher absolute weights on the more pre-
dictive filler types and pattern positions. Note that
leaving the pattern counts unnormalized automati-
cally allows patterns with higher counts to contribute
more to the prediction of their associated instances.
3.4 N-Gram Data
We now describe the collection of n-grams and their
counts used in our implementation. We use, to our
knowledge, the largest publicly available collection:
the Google Web 1T 5-gram Corpus Version 1.1.5
This collection was generated from approximately 1
trillion tokens of online text. In this data, tokens ap-
pearing less than 200 times have been mapped to the
?UNK? symbol. Also, only n-grams appearing more
than 40 times are included. For languages where
such an extensive n-gram resource is not available,
the n-gram counts could also be taken from the page-
counts returned by an Internet search engine.
4 Evaluation
4.1 Labelled It Data
We need labelled data for training and evaluation of
our system. This data indicates, for every occurrence
of the pronoun it, whether it refers to a preceding
noun phrase or not. Standard coreference resolution
data sets annotate all noun phrases that have an an-
tecedent noun phrase in the text. Therefore, we can
extract labelled instances of it from these sets. We
do this for the dry-run and formal sets from MUC-7
(1997), and merge them into a single data set.
Of course, full coreference-annotated data is a
precious resource, with the pronoun it making up
only a small portion of the marked-up noun phrases.
We thus created annotated data specifically for the
pronoun it. We annotated 1020 instances in a col-
lection of Science News articles (from 1995-2000),
downloaded from the Science News website. We
also annotated 709 instances in the WSJ portion of
the DARPA TIPSTER Project (Harman, 1992), and
279 instances in the English portion of the Europarl
Corpus (Koehn, 2005).
A single annotator (A1) labelled all three datasets, while two additional annotators not connected
5Available from the LDC as LDC2006T13
Data Set Number of It % Non-Referential
Europarl 279 50.9
Sci-News 1020 32.6
WSJ 709 25.1
MUC 129 31.8
Train 1069 33.2
Test 1067 31.7
Test-200 200 30.0
Table 3: Data sets used in experiments.
with the project (A2 and A3) were asked to sepa-rately re-annotate a portion of each, so that inter-
annotator agreement could be calculated. A1 and
A2 agreed on 96% of annotation decisions, while
A1-A3, and A2-A3, agreed on 91% and 93% of de-cisions, respectively. The Kappa statistic (Jurafsky
and Martin, 2000, page 315), with P(E) computed
from the confusion matrices, was a high 0.90 for A1-
A2, and 0.79 and 0.81 for the other pairs, around the0.80 considered to be good reliability. These are,
perhaps surprisingly, the only known it-annotation-
agreement statistics available for written text. They
contrast favourably with the low agreement seen on
categorizing it in spoken dialog (Mu?ller, 2006).
We make all the annotations available in It-Bank,
an online repository for annotated it-instances.6
It-Bank also allows other researchers to distribute
their it annotations. Often, the full text of articles
containing annotations cannot be shared because of
copyright. However, sharing just the sentences con-
taining the word it, randomly-ordered, is permissible
under fair-use guidelines. The original annotators
retain their copyright on the annotations.
We use our annotated data in two ways. First
of all, we perform cross-validation experiments on
each of the data sets individually, to help gauge the
difficulty of resolution on particular domains and
volumes of training data. Secondly, we randomly
distribute all instances into two main sets, a training
set and a test set. We also construct a smaller test
set, Test-200, containing only the first 200 instances
in the Test set. We use Test-200 for human experi-
ments and error analysis (Section 5.2). Table 3 sum-
marizes all the sets used in the experiments.
6www.cs.ualberta.ca/?bergsma/ItBank/. It-Bank also con-
tains an additional 1,077 examples used as development data.
14
4.2 Comparison Approaches
We represent feature vectors exactly as described
in Section 3.3. We smooth by adding 40 to all
counts, equal to the minimum count in the n-gram
data. For classification, we use a maximum entropy
model (Berger et al, 1996), from the logistic re-
gression package in Weka (Witten and Frank, 2005),
with all default parameter settings. Results with
our distributional approach are labelled as DISTRIB.
Note that our maximum entropy classifier actually
produces a probability of non-referentiality, which
is thresholded at 50% to make a classification.
As a baseline, we implemented the non-referential
it detector of Lappin and Leass (1994), labelled as
LL in the results. This is a syntactic detector, a
point missed by Evans (2001) in his criticism: the
patterns are robust to intervening words and modi-
fiers (e.g. ?it was never thought by the committee
that...?) provided the sentence is parsed correctly.7
We automatically parse sentences with Minipar, a
broad-coverage dependency parser (Lin, 1998b).
We also use a separate, extended version of
the LL detector, implemented for large-scale non-
referential detection by Cherry and Bergsma (2005).
This system, also for Minipar, additionally detects
instances of it labelled with Minipar?s pleonastic cat-
egory Subj. It uses Minipar?s named-entity recog-
nition to identify time expressions, such as ?it was
midnight,? and provides a number of other patterns
to match common non-referential it uses, such as
in expressions like ?darn it,? ?don?t overdo it,? etc.
This extended detector is labelled as MINIPL (for
Minipar pleonasticity) in our results.
Finally, we tested a system that combines the
above three approaches. We simply add the LL and
MINIPL decisions as binary features in the DISTRIB
system. This system is called COMBO in our results.
4.3 Evaluation Criteria
We follow Mu?ller (2006)?s evaluation criteria. Pre-
cision (P) is the proportion of instances that we la-
bel as non-referential that are indeed non-referential.
Recall (R) is the proportion of true non-referentials
that we detect, and is thus a measure of the coverage
7Our approach, on the other hand, would seem to be suscep-
tible to such intervening material, if it pushes indicative context
tokens out of the 5-token window.
System P R F Acc
LL 93.4 21.0 34.3 74.5
MINIPL 66.4 49.7 56.9 76.1
DISTRIB 81.4 71.0 75.8 85.7
COMBO 81.3 73.4 77.1 86.2
Table 4: Train/Test-split performance (%).
of the system. F-Score (F) is the geometric average
of precision and recall; it is the most common non-
referential detection metric. Accuracy (Acc) is the
percentage of instances labelled correctly.
5 Results
5.1 System Comparison
Table 4 gives precision, recall, F-score, and accu-
racy on the Train/Test split. Note that while the LL
system has high detection precision, it has very low
recall, sharply reducing F-score. The MINIPL ap-
proach sacrifices some precision for much higher
recall, but again has fairly low F-score. To our
knowledge, our COMBO system, with an F-Score
of 77.1%, achieves the highest performance of any
non-referential system yet implemented. Even more
importantly, DISTRIB, which requires only minimal
linguistic processing and no encoding of specific in-
dicator patterns, achieves 75.8% F-Score. The dif-
ference between COMBO and DISTRIB is not statis-
tically significant, while both are significantly bet-
ter than the rule-based approaches.8 This provides
strong motivation for a ?light-weight? approach to
non-referential it detection ? one that does not re-
quire parsing or hand-crafted rules and ? is easily
ported to new languages and text domains.
Since applying an English stemmer to the con-
text words (Section 3.2) reduces the portability of
the distributional technique, we investigated the use
of more portable pattern abstraction. Figure 1 com-
pares the use of the stemmer to simply truncating the
words in the patterns at a certain maximum length.
Using no truncation (Unaltered) drops the F-Score
by 4.3%, while truncating the patterns to a length of
four only drops the F-Score by 1.4%, a difference
which is not statistically significant. Simple trunca-
tion may be a good option for other languages where
stemmers are not readily available. The optimum
8All significance testing uses McNemar?s test, p<0.05
15
 68
 70
 72
 74
 76
 78
 80
 1  2  3  4  5  6  7  8  9  10
F-
Sc
or
e
Truncated word length
Stemmed patterns
Truncated patterns
Unaltered patterns
Figure 1: Effect of pattern-word truncation on non-
referential it detection (COMBO system, Train/Test split).
System Europl. Sci-News WSJ MUC
LL 44.0 39.3 21.5 13.3
MINIPL 70.3 61.8 22.0 50.7
DISTRIB 79.7 77.2 69.5 68.2
COMBO 76.2 78.7 68.1 65.9
COMBO4 83.6 76.5 67.1 74.7
Table 5: 10-fold cross validation F-Score (%).
truncation size will likely depend on the length of
the base forms of words in that language. For real-
world application of our approach, truncation also
reduces the table sizes (and thus storage and look-
up costs) of any pre-compiled it-pattern database.
Table 5 compares the 10-fold cross-validation F-
score of our systems on the four data sets. The
performance of COMBO on Europarl and MUC is
affected by the small number of instances in these
sets (Section 4, Table 3). We can reduce data frag-
mentation by removing features. For example, if we
only use the length-4 patterns in COMBO (labelled as
COMBO4), performance increases dramatically on
Europarl and MUC, while dipping slightly for the
larger Sci-News and WSJ sets. Furthermore, select-
ing just the three most useful filler type counts as
features (#1,#2,#5), boosts F-Score on Europarl to
86.5%, 10% above the full COMBO system.
5.2 Analysis and Discussion
In light of these strong results, it is worth consid-
ering where further gains in performance might yet
be found. One key question is to what extent a lim-
ited context restricts identification performance. We
first tested the importance of the pattern length by
System P R F Acc
DISTRIB 80.0 73.3 76.5 86.5
COMBO 80.7 76.7 78.6 87.5
Human-1 92.7 63.3 75.2 87.5
Human-2 84.0 70.0 76.4 87.0
Human-3 72.2 86.7 78.8 86.0
Table 6: Evaluation on Test-200 (%).
using only the length-4 counts in the DISTRIB sys-
tem (Train/Test split). Surprisingly, the drop in F-
Score was only one percent, to 74.8%. Using only
the length-5 counts drops F-Score to 71.4%. Neither
are statistically significant; however there seems to
be diminishing returns from longer context patterns.
Another way to view the limited context is to ask,
given the amount of context we have, are we mak-
ing optimum use of it? We answer this by seeing
how well humans can do with the same information.
As explained in Section 3.2, our system uses 5-gram
context patterns that together span from four-to-the-
left to four-to-the-right of the pronoun. We thus pro-
vide these same nine-token windows to our human
subjects, and ask them to decide whether the pro-
nouns refer to previous noun phrases or not, based
on these contexts. Subjects first performed a dry-
run experiment on separate development data. They
were shown their errors and sources of confusion
were clarified. They then made the judgments unas-
sisted on the final Test-200 data. Three humans per-
formed the experiment. Their results show a range
of preferences for precision versus recall, with both
F-Score and Accuracy on average below the perfor-
mance of COMBO (Table 6). Foremost, these results
show that our distributional approach is already get-
ting good leverage from the limited context informa-
tion, around that achieved by our best human.
It is instructive to inspect the twenty-five Test-200
instances that the COMBO system classified incor-
rectly, given human performance on this same set.
Seventeen of the twenty-five COMBO errors were
also made by one or more human subjects, suggest-
ing system errors are also mostly due to limited con-
text. For example, one of these errors was for the
context: ?it takes an astounding amount...? Here, the
non-referential nature of the instance is not apparent
without the infinitive clause that ends the sentence:
?... of time to compare very long DNA sequences
16
with each other.?
Six of the eight errors unique to the COMBO sys-
tem were cases where the system falsely said the
pronoun was non-referential. Four of these could
have referred to entire sentences or clauses rather
than nouns. These confusing cases, for both hu-
mans and our system, result from our definition
of a referential pronoun: pronouns with verbal or
clause antecedents are considered non-referential
(Section 3.1). If an antecedent verb or clause is
replaced by a nominalization (Smith researched...
to Smith?s research), a referring pronoun, in the
same context, becomes referential. When we inspect
the probabilities produced by the maximum entropy
classifier (Section 4.2), we see only a weak bias for
the non-referential class on these examples, reflect-
ing our classifier?s uncertainty. It would likely be
possible to improve accuracy on these cases by en-
coding the presence or absence of preceding nomi-
nalizations as a feature of our classifier.
Another false non-referential decision is for the
phrase ?... machine he had installed it on.? The it is
actually referential, but the extracted patterns (e.g.
?he had install * on?) are nevertheless usually filled
with it.9 Again, it might be possible to fix such ex-
amples by leveraging the preceding discourse. No-
tably, the first noun-phrase before the context is the
word ?software.? There is strong compatibility be-
tween the pronoun-parent ?install? and the candidate
antecedent ?software.? In a full coreference resolu-
tion system, when the anaphora resolution module
has a strong preference to link it to an antecedent
(which it should when the pronoun is indeed refer-
ential), we can override a weak non-referential prob-
ability. Non-referential it detection should not be
a pre-processing step, but rather part of a globally-
optimal configuration, as was done for general noun
phrase anaphoricity by Denis and Baldridge (2007).
The suitability of this kind of approach to correct-
ing some of our system?s errors is especially obvious
when we inspect the probabilities of the maximum
entropy model?s output decisions on the Test-200
set. Where the maximum entropy classifier makes
mistakes, it does so with less confidence than when
it classifies correct examples. The average predicted
9This example also suggests using filler counts for the word
?the? as a feature when it is the last word in the pattern.
probability of the incorrect classifications is 76.0%
while the average probability of the correct classi-
fications is 90.3%. Many incorrect decisions are
ready to switch sides; our next step will be to use
features of the preceding discourse and the candi-
date antecedents to help give them a push.
6 Conclusion
We have presented an approach to detecting non-
referential pronouns in text based on the distribu-
tion of the pronoun?s context. The approach is sim-
ple to implement, attains state-of-the-art results, and
should be easily ported to other languages. Our tech-
nique demonstrates how large volumes of data can
be used to gather world knowledge for natural lan-
guage processing. A consequence of this research
was the creation of It-Bank, a collection of thou-
sands of labelled examples of the pronoun it, which
will benefit other coreference resolution researchers.
Error analysis reveals that our system is getting
good leverage out of the pronoun context, achiev-
ing results comparable to human performance given
equivalent information. To boost performance fur-
ther, we will need to incorporate information from
preceding discourse. Future research will also test
the distributional classification of other ambiguous
pronouns, like this, you, there, and that. Another
avenue of study will look at the interaction between
coreference resolution and machine translation. For
example, if a single form in English (e.g. that)
is separated into different meanings in another lan-
guage (e.g., Spanish demonstrative ese, nominal ref-
erence e?se, abstract or statement reference eso, and
complementizer que), then aligned examples pro-
vide automatically-disambiguated English data. We
could extract context patterns and collect statistics
from these examples like in our current approach.
In general, jointly optimizing translation and coref-
erence is an exciting and largely unexplored re-
search area, now partly enabled by our portable non-
referential detection methodology.
Acknowledgments
We thank Kristin Musselman and Christopher Pinchak for as-
sistance preparing the data, and we thank Google Inc. for shar-
ing their 5-gram corpus. We gratefully acknowledge support
from the Natural Sciences and Engineering Research Council
of Canada, the Alberta Ingenuity Fund, and the Alberta Infor-
matics Circle of Research Excellence.
17
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In COLING-
ACL, pages 33?40.
Adrianne Boyd, Whitney Gegg-Harrison, and Donna By-
ron. 2005. Identifying non-referential it: a machine
learning approach incorporating linguistically moti-
vated patterns. In ACL Workshop on Feature Engi-
neering for Machine Learning in NLP, pages 40?47.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution. In
CoNLL, pages 88?95.
Ido Dagan and Alan Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora references.
In COLING, volume 3, pages 330?332.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference using integer
programming. In NAACL-HLT, pages 236?243.
Miriam Eckert and Michael Strube. 2000. Dialogue acts,
synchronizing units, and anaphora resolution. Journal
of Semantics, 17(1):51?89.
Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of it. Literary and
Linguistic Computing, 16(1):45?57.
Surabhi Gupta, Matthew Purver, and Dan Jurafsky. 2007.
Disambiguating between generic and referential ?you?
in dialog. In ACL Demo and Poster Sessions, pages
105?108.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In ACL, pages 848?855.
Donna Harman. 1992. The DARPA TIPSTER project.
ACM SIGIR Forum, 26(2):26?28.
Zellig Harris. 1985. Distributional structure. In J.J.
Katz, editor, The Philosophy of Linguistics, pages 26?
47. Oxford University Press, New York.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In ACL, pages 268?
275.
Graeme Hirst. 1981. Anaphora in Natural Language
Understanding: A Survey. Springer Verlag.
Jerry Hobbs. 1978. Resolving pronoun references. Lin-
gua, 44(311):339?352.
Daniel Jurafsky and James H. Martin. 2000. Speech and
language processing. Prentice Hall.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit X, pages
79?86.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):535?561.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?773.
Dekang Lin. 1998b. Dependency-based evaluation of
MINIPAR. In LREC Workshop on the Evaluation of
Parsing Systems.
Andrew Kachites McCallum. 1996. Bow:
A toolkit for statistical language modeling,
text retrieval, classification and clustering.
http://www.cs.cmu.edu/?mccallum/bow.
MUC-7. 1997. Coreference task definition (v3.0, 13 Jul
97). In Proceedings of the Seventh Message Under-
standing Conference (MUC-7).
Christoph Mu?ller. 2006. Automatic detection of non-
referential It in spoken multi-party dialog. In EACL,
pages 49?56.
Christoph Mu?ller. 2007. Resolving It, This, and That in
unrestricted multi-party dialog. In ACL, pages 816?
823.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In COLING, pages 730?736.
Chris D. Paice and Gareth D. Husk. 1987. Towards the
automatic recognition of anaphoric features in English
text: the impersonal pronoun ?it?. Computer Speech
and Language, 2:109?132.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, second edition.
Shanheng Zhao and Hwee Tou Ng. 2007. Identification
and resolution of Chinese zero pronouns: A machine
learning approach. In EMNLP, pages 541?550.
18
Proceedings of ACL-08: HLT, pages 532?540,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semi-supervised Convex Training for Dependency Parsing
Qin Iris Wang
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
wqin@cs.ualberta.ca
Dale Schuurmans
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
dale@cs.ualberta.ca
Dekang Lin
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA, USA, 94043
lindek@google.com
Abstract
We present a novel semi-supervised training
algorithm for learning dependency parsers.
By combining a supervised large margin loss
with an unsupervised least squares loss, a dis-
criminative, convex, semi-supervised learning
algorithm can be obtained that is applicable
to large-scale problems. To demonstrate the
benefits of this approach, we apply the tech-
nique to learning dependency parsers from
combined labeled and unlabeled corpora. Us-
ing a stochastic gradient descent algorithm, a
parsing model can be efficiently learned from
semi-supervised data that significantly outper-
forms corresponding supervised methods.
1 Introduction
Supervised learning algorithms still represent the
state of the art approach for inferring dependency
parsers from data (McDonald et al, 2005a; McDon-
ald and Pereira, 2006; Wang et al, 2007). How-
ever, a key drawback of supervised training algo-
rithms is their dependence on labeled data, which
is usually very difficult to obtain. Perceiving the
limitation of supervised learning?in particular, the
heavy dependence on annotated corpora?many re-
searchers have investigated semi-supervised learn-
ing techniques that can take both labeled and unla-
beled training data as input. Following the common
theme of ?more data is better data? we also use both
a limited labeled corpora and a plentiful unlabeled
data resource. Our goal is to obtain better perfor-
mance than a purely supervised approach without
unreasonable computational effort. Unfortunately,
although significant recent progress has been made
in the area of semi-supervised learning, the perfor-
mance of semi-supervised learning algorithms still
fall far short of expectations, particularly in chal-
lenging real-world tasks such as natural language
parsing or machine translation.
A large number of distinct approaches to semi-
supervised training algorithms have been investi-
gated in the literature (Bennett and Demiriz, 1998;
Zhu et al, 2003; Altun et al, 2005; Mann and
McCallum, 2007). Among the most prominent ap-
proaches are self-training, generative models, semi-
supervised support vector machines (S3VM), graph-
based algorithms and multi-view algorithms (Zhu,
2005).
Self-training is a commonly used technique
for semi-supervised learning that has been ap-
532
plied to several natural language processing tasks
(Yarowsky, 1995; Charniak, 1997; Steedman et al,
2003). The basic idea is to bootstrap a supervised
learning algorithm by alternating between inferring
the missing label information and retraining. Re-
cently, McClosky et al (2006a) successfully applied
self-training to parsing by exploiting available un-
labeled data, and obtained remarkable results when
the same technique was applied to parser adaptation
(McClosky et al, 2006b). More recently, Haffari
and Sarkar (2007) have extended the work of Abney
(2004) and given a better mathematical understand-
ing of self-training algorithms. They also show con-
nections between these algorithms and other related
machine learning algorithms.
Another approach, generative probabilistic mod-
els, are a well-studied framework that can be ex-
tremely effective. However, generative models use
the EM algorithm for parameter estimation in the
presence of missing labels, which is notoriously
prone to getting stuck in poor local optima. More-
over, EM optimizes a marginal likelihood score that
is not discriminative. Consequently, most previous
work that has attempted semi-supervised or unsu-
pervised approaches to parsing have not produced
results beyond the state of the art supervised results
(Klein and Manning, 2002; Klein and Manning,
2004). Subsequently, alternative estimation strate-
gies for unsupervised learning have been proposed,
such as Contrastive Estimation (CE) by Smith and
Eisner (2005). Contrastive Estimation is a general-
ization of EM, by defining a notion of learner guid-
ance. It makes use of a set of examples (its neighbor-
hood) that are similar in some way to an observed
example, requiring the learner to move probability
mass to a given example, taking only from the ex-
ample?s neighborhood. Nevertheless, CE still suf-
fers from shortcomings, including local minima.
In recent years, SVMs have demonstrated state
of the art results in many supervised learning tasks.
As a result, many researchers have put effort on
developing algorithms for semi-supervised SVMs
(S3VMs) (Bennett and Demiriz, 1998; Altun et
al., 2005). However, the standard objective of an
S3VM is non-convex on the unlabeled data, thus
requiring sophisticated global optimization heuris-
tics to obtain reasonable solutions. A number of
researchers have proposed several efficient approx-
imation algorithms for S3VMs (Bennett and Demi-
riz, 1998; Chapelle and Zien, 2005; Xu and Schu-
urmans, 2005). For example, Chapelle and Zien
(2005) propose an algorithm that smoothes the ob-
jective with a Gaussian function, and then performs
a gradient descent search in the primal space to
achieve a local solution. An alternative approach is
proposed by Xu and Schuurmans (2005) who formu-
late a semi-definite programming (SDP) approach.
In particular, they present an algorithm for multi-
class unsupervised and semi-supervised SVM learn-
ing, which relaxes the original non-convex objective
into a close convex approximation, thereby allowing
a global solution to be obtained. However, the com-
putational cost of SDP is still quite expensive.
Instead of devising various techniques for cop-
ing with non-convex loss functions, we approach the
problem from a different perspective. We simply re-
place the non-convex loss on unlabeled data with an
alternative loss that is jointly convex with respect
to both the model parameters and (the encoding of)
the self-trained prediction targets. More specifically,
for the loss on the unlabeled data part, we substi-
tute the original unsupervised structured SVM loss
with a least squares loss, but keep constraints on
the inferred prediction targets, which avoids trivial-
ization. Although using a least squares loss func-
tion for classification appears misguided, there is
a precedent for just this approach in the early pat-
tern recognition literature (Duda et al, 2000). This
loss function has the advantage that the entire train-
ing objective on both the labeled and unlabeled data
now becomes convex, since it consists of a convex
structured large margin loss on labeled data and a
convex least squares loss on unlabeled data. As
we will demonstrate below, this approach admits an
efficient training procedure that can find a global
minimum, and, perhaps surprisingly, can systemat-
ically improve the accuracy of supervised training
approaches for learning dependency parsers.
Thus, in this paper, we focus on semi-supervised
language learning, where we can make use of both
labeled and unlabeled data. In particular, we in-
vestigate a semi-supervised approach for structured
large margin training, where the objective is a com-
bination of two convex functions, the structured
large margin loss on labeled data and the least
squares loss on unlabeled data. We apply the result-
533
 fundsInvestors continue to  pour cash into money
Figure 1: A dependency tree
ing semi-supervised convex objective to dependency
parsing, and obtain significant improvement over
the corresponding supervised structured SVM. Note
that our approach is different from the self-training
technique proposed in (McClosky et al, 2006a),
although both methods belong to semi-supervised
training category.
In the remainder of this paper, we first review
the supervised structured large margin training tech-
nique. Then we introduce the standard semi-
supervised structured large margin objective, which
is non-convex and difficult to optimize. Next we
present a new semi-supervised training algorithm for
structured SVMs which is convex optimization. Fi-
nally, we apply this algorithm to dependency pars-
ing and show improved dependency parsing accu-
racy for both Chinese and English.
2 Dependency Parsing Model
Given a sentence X = (x1, ..., xn) (xi denotes
each word in the sentence), we are interested in
computing a directed dependency tree, Y , over X.
As shown in Figure 1, in a dependency structure,
the basic units of a sentence are the syntactic re-
lationships (aka. head-child or governor-dependent
or regent-subordinate relations) between two indi-
vidual words, where the relationships are expressed
by drawing links connecting individual words (Man-
ning and Schutze, 1999). The direction of each link
points from a head word to a child word, and each
word has one and only one head, except for the head
of the sentence. Thus a dependency structure is ac-
tually a rooted, directed tree. We assume that a di-
rected dependency tree Y consists of ordered pairs
(xi ? xj) of words in X such that each word ap-
pears in at least one pair and each word has in-degree
at most one. Dependency trees are assumed to be
projective here, which means that if there is an arc
(xi ? xj), then xi is an ancestor of all the words
between xi and xj .1 Let ?(X) denote the set of all
the directed, projective trees that span on X. The
parser?s goal is then to find the most preferred parse;
that is, a projective tree, Y ? ?(X), that obtains
the highest ?score?. In particular, one would assume
that the score of a complete spanning tree Y for a
given sentence, whether probabilistically motivated
or not, can be decomposed as a sum of local scores
for each link (a word pair) (Eisner, 1996; Eisner and
Satta, 1999; McDonald et al, 2005a). Given this
assumption, the parsing problem reduces to find
Y ? = arg max
Y ??(X)
score(Y |X) (1)
= arg max
Y ??(X)
?
(xi?xj)?Y
score(xi ? xj)
where the score(xi ? xj) can depend on any mea-
surable property of xi and xj within the sentence X.
This formulation is sufficiently general to capture
most dependency parsing models, including proba-
bilistic dependency models (Eisner, 1996; Wang et
al., 2005) as well as non-probabilistic models (Mc-
Donald et al, 2005a).
For standard scoring functions, particularly those
used in non-generative models, we further assume
that the score of each link in (1) can be decomposed
into a weighted linear combination of features
score(xi ? xj) = ? ? f(xi ? xj) (2)
where f(xi ? xj) is a feature vector for the link
(xi ? xj), and ? are the weight parameters to be
estimated during training.
3 Supervised Structured Large Margin
Training
Supervised structured large margin training ap-
proaches have been applied to parsing and produce
promising results (Taskar et al, 2004; McDonald et
al., 2005a; Wang et al, 2006). In particular, struc-
tured large margin training can be expressed as min-
imizing a regularized loss (Hastie et al, 2004), as
shown below:
1We assume all the dependency trees are projective in our
work (just as some other researchers do), although in the real
word, most languages are non-projective.
534
min
?
?
2 ?
?? + (3)
?
i
max
Li,k
(?(Li,k, Yi)? diff(?, Yi, Li,k))
where Yi is the target tree for sentence Xi; Li,k
ranges over all possible alternative k trees in ?(Xi);
diff(?, Yi, Li,k) = score(?, Yi) ? score(?, Li,k);
score(?, Yi) =
?
(xm?xn)?Yi ? ? f(xm ? xn), as
shown in Section 2; and ?(Li,k, Yi) is a measure of
distance between the two trees Li,k and Yi. This is
an application of the structured large margin training
approach first proposed in (Taskar et al, 2003) and
(Tsochantaridis et al, 2004).
Using the techniques of Hastie et al (2004) one
can show that minimizing the objective (3) is equiv-
alent to solving the quadratic program
min
?,?
?
2 ?
?? + e?? subject to
?i,k ? ?(Li,k, Yi)? diff(?, Yi, Li,k)
?i,k ? 0
for all i, Li,k ? ?(Xi) (4)
where e denotes the vector of all 1?s and ? represents
slack variables. This approach corresponds to the
training problem posed in (McDonald et al, 2005a)
and has yielded the best published results for En-
glish dependency parsing.
To compare with the new semi-supervised ap-
proach we will present in Section 5 below, we re-
implemented the supervised structured large margin
training approach in the experiments in Section 7.
More specifically, we solve the following quadratic
program, which is based on Equation (3)
min
?
?
2 ?
?? +
?
i
max
L
k
?
m=1
k
?
n=1
?(Li,m,n, Yi,m,n)
? diff(?, Yi,m,n, Li,m,n) (5)
where diff(?, Yi,m,n, Li,m,n) = score(?, Yi,m,n) ?
score(?, Li,m,n) and k is the sentence length. We
represent a dependency tree as a k ? k adjacency
matrix. In the adjacency matrix, the value of Yi,m,n
is 1 if the word m is the head of the word n, 0 oth-
erwise. Since both the distance function ?(Li, Yi)
and the score function decompose over links, solv-
ing (5) is equivalent to solve the original constrained
quadratic program shown in (4).
4 Semi-supervised Structured Large
Margin Objective
The objective of standard semi-supervised struc-
tured SVM is a combination of structured large mar-
gin losses on both labeled and unlabeled data. It has
the following form:
min
?
?
2 ?
?? +
N
?
i=1
structured loss (?,Xi, Yi)
+ min
Yj
U
?
j=1
structured loss (?,Xj , Yj) (6)
where
structured loss (?,Xi, Yi)
= max
L
k
?
m=1
k
?
n=1
?(Li,m,n, Yi,m,n) (7)
?diff(?, Yi,m,n, Li,m,n)
N and U are the number of labeled and unlabeled
training sentences respectively, and Yj ranges over
guessed targets on the unsupervised data.
In the second term of the above objective shown in
(6), both ? and Yj are variables. The resulting loss
function has a hat shape (usually called hat-loss),
which is non-convex. Therefore the objective as a
whole is non-convex, making the search for global
optimal difficult. Note that the root of the optimiza-
tion difficulty for S3VMs is the non-convex property
of the second term in the objective function. We will
propose a novel approach which can deal with this
problem. We introduce an efficient approximation?
least squares loss?for the structured large margin
loss on unlabeled data below.
5 Semi-supervised Convex Training for
Structured SVM
Although semi-supervised structured SVM learning
has been an active research area, semi-supervised
structured SVMs have not been used in many real
applications to date. The main reason is that most
available semi-supervised large margin learning ap-
proaches are non-convex or computationally expen-
sive (e.g. (Xu and Schuurmans, 2005)). These tech-
niques are difficult to implement and extremely hard
to scale up. We present a semi-supervised algorithm
535
for structured large margin training, whose objective
is a combination of two convex terms: the super-
vised structured large margin loss on labeled data
and the cheap least squares loss on unlabeled data.
The combined objective is still convex, easy to opti-
mize and much cheaper to implement.
5.1 Least Squares Convex Objective
Before we introduce the new algorithm, we first in-
troduce a convex loss which we apply it to unlabeled
training data for the semi-supervised structured large
margin objective which we will introduce in Sec-
tion 5.2 below. More specifically, we use a struc-
tured least squares loss to approximate the struc-
tured large margin loss on unlabeled data. The cor-
responding objective is:
min
?,Yj
?
2 ?
?? + (8)
?
2
U
?
j=1
k
?
m=1
k
?
n=1
(
??f(Xj,m ? Xj,n)? Yj,m,n
)2
subject to constraints on Y (explained below).
The idea behind this objective is that for each pos-
sible link (Xj,m ? Xj,n), we intend to minimize the
difference between the link and the corresponding
estimated link based on the learned weight vector.
Since this is conducted on unlabeled data, we need
to estimate both ? and Yj to solve the optimization
problem. As mentioned in Section 3, a dependency
tree Yj is represented as an adjacency matrix. Thus
we need to enforce some constraints in the adjacency
matrix to make sure that each Yj satisfies the depen-
dency tree constraints. These constraints are critical
because they prevent (8) from having a trivial solu-
tion in Y. More concretely, suppose we use rows to
denote heads and columns to denote children. Then
we have the following constraints on the adjacency
matrix:
? (1) All entries in Yj are between 0 and 1
(convex relaxation of discrete directed edge in-
dicators);
? (2) The sum over all the entries on each col-
umn is equal to one (one-head rule);
? (3) All the entries on the diagonal are zeros
(no self-link rule);
? (4) Yj,m,n + Yj,n,m ? 1 (anti-symmetric
rule), which enforces directedness.
One final constraint that is sufficient to ensure that
a directed tree is obtained, is connectedness (i.e.
acyclicity), which can be enforced with an addi-
tional semidefinite constraint. Although convex, this
constraint is more expensive to enforce, therefore we
drop it in our experiments below. (However, adding
the semidefinite connectedness constraint appears to
be feasible on a sentence by sentence level.)
Critically, the objective (8) is jointly convex in
both the weights ? and the edge indicator variables
Y. This means, for example, that there are no local
minima in (8)?any iterative improvement strategy,
if it converges at all, must converge to a global min-
imum.
5.2 Semi-supervised Convex Objective
By combining the convex structured SVM loss on
labeled data (shown in Equation (5)) and the con-
vex least squares loss on unlabeled data (shown in
Equation (8)), we obtain a semi-supervised struc-
tured large margin loss
min
?,Yj
?
2 ?
?? +
N
?
i=1
structured loss (?,Xi, Yi) +
U
?
j=1
least squares loss (?,Xj , Yj) (9)
subject to constraints on Y (explained above).
Since the summation of two convex functions is
also convex, so is (9). Replacing the two losses with
the terms shown in Equation (5) and Equation (8),
we obtain the final convex objective as follows:
min
?,Yj
?
2N ?
?? +
N
?
i=1
max
L
k
?
m=1
k
?
n=1
?(Li,m,n, Yi,m,n)?
diff(?, Yi,m,n, Li,m,n) + ?2U ?
?? + (10)
?
2
U
?
j=1
k
?
m=1
k
?
n=1
(
??f(Xj,m ? Xj,n)? Yj,m,n
)2
subject to constraints on Y (explained above),
where diff(?, Yi,m,n, Li,m,n) = score(?, Yi,m,n) ?
536
score(?, Li,m,n), N and U are the number of labeled
and unlabeled training sentences respectively, as we
mentioned before. Note that in (10) we have split
the regularizer into two parts; one for the supervised
component of the objective, and the other for the
unsupervised component. Thus the semi-supervised
convex objective is regularized proportionally to the
number of labeled and unlabeled training sentences.
6 Efficient Optimization Strategy
To solve the convex optimization problem shown in
Equation (10), we used a gradient descent approach
which simply uses stochastic gradient steps. The
procedure is as follows.
? Step 0, initialize the Yj variables of each
unlabeled sentence as a right-branching (left-
headed) chain model, i.e. the head of each word
is its left neighbor.
? Step 1, pass through all the labeled training sen-
tences one by one. The parameters ? are up-
dated based on each labeled sentence.
? Step 2, based on the learned parameter weights
from the labeled data, update ? and Yj on each
unlabeled sentence alternatively:
? treat Yj as a constant, update ? on each
unlabeled sentence by taking a local gra-
dient step;
? treat ? as a constant, update Yj by call-
ing the optimization software package
CPLEX to solve for an optimal local so-
lution.
? Repeat the procedure of step 1 and step 2 until
maximum iteration number has reached.
This procedure works efficiently on the task of
training a dependency parser. Although ? and
Yj are updated locally on each sentence, progress
in minimizing the total objective shown in Equa-
tion (10) is made in each iteration. In our experi-
ments, the objective usually converges within 30 it-
erations.
7 Experimental Results
Given a convex approach to semi-supervised struc-
tured large margin training, and an efficient training
algorithm for achieving a global optimum, we now
investigate its effectiveness for dependency parsing.
In particular, we investigate the accuracy of the re-
sults it produces. We applied the resulting algorithm
to learn dependency parsers for both English and
Chinese.
7.1 Experimental Design
Data Sets
Since we use a semi-supervised approach, both la-
beled and unlabeled training data are needed. For
experiment on English, we used the English Penn
Treebank (PTB) (Marcus et al, 1993) and the con-
stituency structures were converted to dependency
trees using the same rules as (Yamada and Mat-
sumoto, 2003). The standard training set of PTB
was spit into 2 parts: labeled training data?the
first 30k sentences in section 2-21, and unlabeled
training data?the remaining sentences in section
2-21. For Chinese, we experimented on the Penn
Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004)
and we used the rules in (Bikel, 2004) for conver-
sion. We also divided the standard training set into
2 parts: sentences in section 400-931 and sentences
in section 1-270 are used as labeled and unlabeled
data respectively. For both English and Chinese,
we adopted the standard development and test sets
throughout the literature.
As listed in Table 1 with greater detail, we
experimented with sets of data with different sen-
tence length: PTB-10/CTB4-10, PTB-15/CTB4-15,
PTB-20/CTB4-20, CTB4-40 and CTB4, which
contain sentences with up to 10, 15, 20, 40 and all
words respectively.
Features
For simplicity, in current work, we only used two
sets of features?word-pair and tag-pair indicator
features, which are a subset of features used by
other researchers on dependency parsing (McDon-
ald et al, 2005a; Wang et al, 2007). Although
our algorithms can take arbitrary features, by only
using these simple features, we already obtained
very promising results on dependency parsing
using both the supervised and semi-supervised
approaches. Using the full set of features described
in (McDonald et al, 2005a; Wang et al, 2007) and
comparing the corresponding dependency parsing
537
English
PTB-10
Training(l/ul) 3026/1016
Dev 163
Test 270
PTB-15
Training 7303/2370
Dev 421
Test 603
PTB-20
Training 12519/4003
Dev 725
Test 1034
Chinese
CTB4-10
Training(l/ul) 642/347
Dev 61
Test 40
CTB4-15
Training 1262/727
Dev 112
Test 83
CTB4-20
Training 2038/1150
Dev 163
Test 118
CTB4-40
Training 4400/2452
Dev 274
Test 240
CTB4
Training 5314/2977
Dev 300
Test 289
Table 1: Size of Experimental Data (# of sentences)
results with previous work remains a direction for
future work.
Dependency Parsing Algorithms
For simplicity of implementation, we use a stan-
dard CKY parser in the experiments, although
Eisner?s algorithm (Eisner, 1996) and the Spanning
Tree algorithm (McDonald et al, 2005b) are also
applicable.
7.2 Results
We evaluate parsing accuracy by comparing the di-
rected dependency links in the parser output against
the directed links in the treebank. The parameters
? and ? which appear in Equation (10) were tuned
on the development set. Note that, during training,
we only used the raw sentences of the unlabeled
data. As shown in Table 2 and Table 3, for each
data set, the semi-supervised approach achieves a
significant improvement over the supervised one in
dependency parsing accuracy on both Chinese and
English. These positive results are somewhat sur-
prising since a very simple loss function was used on
Training Test length Supervised Semi-sup
Train-10 ? 10 82.98 84.50
Train-15 ? 10 84.80 86.93? 15 76.96 80.79
Train-20
? 10 84.50 86.32
? 15 78.77 80.57
? 20 74.89 77.85
Train-40
? 10 84.19 85.71
? 15 78.03 81.21
? 20 76.25 77.79
? 40 68.17 70.90
Train-all
? 10 82.67 84.80
? 15 77.92 79.30
? 20 77.30 77.24
? 40 70.11 71.90
all 66.30 67.35
Table 2: Supervised and Semi-supervised Dependency
Parsing Accuracy on Chinese (%)
Training Test length Supervised Semi-sup
Train-10 ? 10 87.77 89.17
Train-15 ? 10 88.06 89.31? 15 81.10 83.37
Train-20
? 10 88.78 90.61
? 15 83.00 83.87
? 20 77.70 79.09
Table 3: Supervised and Semi-supervised Dependency
Parsing Accuracy on English (%)
538
the unlabeled data. A key benefit of the approach is
that a straightforward training algorithm can be used
to obtain global solutions. Note that the results of
our model are not directly comparable with previous
parsing results shown in (McClosky et al, 2006a),
since the parsing accuracy is measured in terms of
dependency relations while their results are f -score
of the bracketings implied in the phrase structure.
8 Conclusion and Future Work
In this paper, we have presented a novel algorithm
for semi-supervised structured large margin training.
Unlike previous proposed approaches, we introduce
a convex objective for the semi-supervised learning
algorithm by combining a convex structured SVM
loss and a convex least square loss. This new semi-
supervised algorithm is much more computationally
efficient and can easily scale up. We have proved our
hypothesis by applying the algorithm to the signifi-
cant task of dependency parsing. The experimental
results show that the proposed semi-supervised large
margin training algorithm outperforms the super-
vised one, without much additional computational
cost.
There remain many directions for future work.
One obvious direction is to use the whole Penn Tree-
bank as labeled data and use some other unannotated
data source as unlabeled data for semi-supervised
training. Next, as we mentioned before, a much
richer feature set can be used in our model to get
better dependency parsing results. Another direc-
tion is to apply the semi-supervised algorithm to
other natural language problems, such as machine
translation, topic segmentation and chunking. In
these areas, there are only limited annotated data
available. Therefore semi-supervised approaches
are necessary to achieve better performance. The
proposed semi-supervised convex training approach
can be easily applied to these tasks.
Acknowledgments
We thank the anonymous reviewers for their useful
comments. Research is supported by the Alberta In-
genuity Center for Machine Learning, NSERC, MI-
TACS, CFI and the Canada Research Chairs pro-
gram. The first author was also funded by the Queen
Elizabeth II Graduate Scholarship.
References
S. Abney. 2004. Understanding the yarowsky algorithm.
Computational Linguistics, 30(3):365?395.
Y. Altun, D. McAllester, and M. Belkin. 2005. Max-
imum margin semi-supervised learning for structured
variables. In Proceedings of Advances in Neural In-
formation Processing Systems 18.
K. Bennett and A. Demiriz. 1998. Semi-supervised sup-
port vector machines. In Proceedings of Advances in
Neural Information Processing Systems 11.
D. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4).
O. Chapelle and A. Zien. 2005. Semi-supervised clas-
sification by low density separation. In Proceedings
of the Tenth International Workshop on Artificial In-
teligence and Statistics.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
the Association for the Advancement of Artificial In-
telligence, pages 598?603.
R. Duda, P. Hart, and D. Stork. 2000. Pattern Classifica-
tion. Wiley, second edition.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexi-
cal context-free grammars and head-automaton gram-
mars. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
the International Conference on Computational Lin-
guistics.
G. Haffari and A. Sarkar. 2007. Analysis of semi-
supervised learning with the yarowsky algorithm. In
Proceedings of the Conference on Uncertainty in Arti-
ficial Intelligence.
T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. 2004.
The entire regularization path for the support vector
machine. Journal of Machine Learning Research,
5:1391?1415.
D. Klein and C. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
D. Klein and C. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proceedingsof the Annual Meeting of the
Association for Computational Linguistics.
G. S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In Proceedings of International Confer-
ence on Machine Learning.
C. Manning and H. Schutze. 1999. Foundations of Sta-
tistical Natural Language Processing. MIT Press.
539
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006a. Ef-
fective self-training for parsing. In Proceedings of the
Human Language Technology: the Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
Proceedings of the International Conference on Com-
putational Linguistics and the Annual Meeting of the
Association for Computational Linguistics.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of European Chapter of the Annual Meeting
of the Association for Computational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of Human Language
Technologies and Conference on Empirical Methods
in Natural Language Processing.
M. Palmer et al 2004. Chinese Treebank 4.0. Linguistic
Data Consortium.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the European Chapter of
the Annual Meeting of the Association for Computa-
tional Linguistics, pages 331?338.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-
margin Markov networks. In Proceedings of Advances
in Neural Information Processing Systems 16.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
International Conference on Machine Learning.
Q. Wang, D. Schuurmans, and D. Lin. 2005. Strictly
lexical dependency parsing. In Proceedings of the In-
ternational Workshop on Parsing Technologies, pages
152?159.
Q. Wang, C. Cherry, D. Lizotte, and D. Schuurmans.
2006. Improved large margin dependency parsing via
local constraints and Laplacian regularization. In Pro-
ceedings of The Conference on Computational Natural
Language Learning, pages 21?28.
Q. Wang, D. Lin, and D. Schuurmans. 2007. Simple
training of dependency parsers via structured boosting.
In Proceedings of the International Joint Conference
on Artificial Intelligence, pages 1756?1762.
L. Xu and D. Schuurmans. 2005. Unsupervised and
semi-supervised multi-class support vector machines.
In Proceedings the Association for the Advancement of
Artificial Intelligence.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of the International Workshop on Parsing
Technologies.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the Annual Meeting of the Association for Com-
putational Linguistics, pages 189?196, Cambridge,
Massachusetts.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In Proceedings of International Con-
ference on Machine Learning.
X. Zhu. 2005. Semi-supervised learning literature sur-
vey. Technical report, Computer Sciences, University
of Wisconsin-Madison.
540
Proceedings of ACL-08: HLT, pages 994?1002,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mining Parenthetical Translations from the Web by Word Alignment   Dekang Lin Shaojun Zhao? Benjamin Van Durme? Marius Pa?ca Google, Inc. University of Rochester University of Rochester Google, Inc. Mountain View Rochester Rochester Mountain View CA, 94043 NY, 14627 NY, 14627 CA, 94043 lindek@google.com zhao@cs.rochester.edu vandurme@cs.rochester.edu mars@google.com   Abstract 
Documents in languages such as Chinese, Japanese and Korean sometimes annotate terms with their translations in English inside a pair of parentheses. We present a method to extract such translations from a large collec-tion of web documents by building a partially parallel corpus and use a word alignment al-gorithm to identify the terms being translated. The method is able to generalize across the translations for different terms and can relia-bly extract translations that occurred only once in the entire web. Our experiment on Chinese web pages produced more than 26 million pairs of translations, which is over two orders of magnitude more than previous re-sults. We show that the addition of the ex-tracted translation pairs as training data provides significant increase in the BLEU score for a statistical machine translation sys-tem.  
1 Introduction In natural language documents, a term (word or phrase) is sometimes followed by its translation in another language in a pair of parentheses. We call these parenthetical translations. The following examples are from Chinese web pages  (we added underlines to indicate what is being translated): (1) ???????????Brookings Institution??? ??????????????????????????Jeremy Shapiro?????...  (2) ????????????????indigestion?????gastritis????????????. (3) ???????????not going to fly?????? (4) ?????????????(linear programming).                                                              ?Contributions made during an internship at Google 
The parenthetically translated terms are typically new words, technical terminologies, idioms, prod-ucts, titles of movies, books, songs, and names of persons, organizations locations, etc. Commonly, an author might use such a parenthetical when a given term has no standard translation (or translit-eration), and does not appear in conventional dic-tionaries.  That is, an author might expect a term to be an out-of-vocabulary item for the target reader, and thus helpfully provides a reference translation in situ. For example, in (1), the name Shapiro was transliterated as ???. The name has many other transliterations in web documents, such as ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ??? ..., where the three Chinese characters corresponds to the three sylla-bles in Sha-pi-ro respectively. Each syllable may be mapped into different characters: 'Sha' into ? or ?, 'pi' into ?, ?, ?, and 'ro' into ?, ?, ?, .... Variation is not limited to the effects of phonetic similarity.  Story titles, for instance, are commonly translated semantically, often leading to a number of translations that have similar meaning, yet differ greatly in lexicographic form. For example, while the movie title Syriana is sometimes phonetically transliterated as ???, ???, it may also be trans-lated semantically according to the plot of the movie, e.g., ??? (mystery in mystery), ?? (real log), ???  (spy against spy), ????  (oil-triggered secret war), ??? (Syria), ?? (mystery journey), ...  The parenthetical translations are extremely valuable both as a stand-alone on-line dictionary and as training data for statistical machine transla-tion systems. They provide fresh data (new words) and cover a much wider range of topics than typi-cal parallel training data for statistical machine translation systems. 
994
The main contribution of this paper is a method for mining parenthetical translations by treating text snippets containing candidate pairs as a par-tially parallel corpus and using a word alignment algorithm to establish the correspondences be-tween in-parenthesis and pre-parenthesis words.  This technique allows us to identify translation pairs even if they only appeared once on the entire web. As a result, we were able to obtain 26.7 mil-lion Chinese-English translation pairs from web documents in Chinese. This is over two orders of magnitude more than the number of extracted translation pairs in the previously reported results (Cao, et al 2007). The next section presents an overview of our al-gorithm, which is then detailed in Sections 3 and 4. We evaluate our results in Section 5 by comparison with bilingually linked Wikipedia titles and by us-ing the extracted pairs as additional training data in a statistical machine translation system. 2 Mining Parenthetical Translations A parenthetical translation matches the pattern:  (4)             f1f2?fm (e1e2?en) which is a sequence of m non-English words fol-lowed by a sequence of n English words in paren-theses. In the remainder of the paper, we assume the non-English text is Chinese, but our technique works for other languages as well.  There have been two approaches to finding such parenthetical translations. One is to assume that the English term e1e2?en is given and use a search en-gine to retrieve text snippets containing e1e2?en from predominately non-English web pages (Na-gata et al 2001, Kwok et al 2005). Another method (Cao et al 2007) is to go through a non-English corpus and collect all instances that match the parenthetical pattern in (4). We followed the second approach since it does not require a prede-fined list of English terms and is amendable for extraction at large scale. In both cases, one can obtain a list of candidate pairs, where the translation of the in-parenthesis terms is a suffix of the pre-parenthesis text. The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al 2005). For example, Table 1 lists a set of Chinese segments (with word-to-word translation underneath) that 
precede the English term Lower Egypt. Owing to the frequency with which ??? appears as a can-didate, and in varying contexts, one has a good reason to believe???is the correct translation of Lower Egypt. ?   ??   ??  ?   ?  ?? downstream  region is  down Egypt ?   ??            ??       ?     ?? center located-at  down Egypt ?   ??   ??     ?       ?     ?? and  so-called of  down Egypt ?   ??       ?     ?? called down Egypt Table 1: Chinese text preceding Lower Egypt Unfortunately, this heuristic does not hold as of-ten as one might imagine.  Consider the candidates for Channel Spacing in Table 2.  The suffix?? (gap) has the highest frequency count. It is none-theless an incomplete translation of Channel Spac-ing. The correct translations in rows c to h occurred with Channel Spacing only once. a ?     ?   ??    ?? ?  is   channel distance b ?  ?        ??          ?? its   channel distance c ?    ??                  ??                    ??          ?? in-addition-to  reducing  wave-passage  distance d ?  ?     ??      ?                 ??   ?? also showed have wave-passage  gap e ?      ?          ?      ?      ??  ?? also  therefore is   channel   gap f ? ?       ??      ?  ?? and  channel   ?s    gap g ?  ??      ??      ??      ?                 ??   ?? an   important property is signal-passage  gap h ?   ??       ??    ??     ??  ?? already  able   reach passage  gap Table 2: Text preceding Channel Spacing The crucial observation we make here is that al-though the words like ?? (in row g) co-occurred with Channel Spacing only once, there are many co-occurrences of ??and Channel in other candi-date pairs, such as: ? ? ? ? ?? ?? (Speech Channel) ? ? ?? ?? ?? (Block Flat Fading Channel) ? ?? B (Channel B) ? ?? ?? ?? (Fiber Channel Probes) 
995
? ?? ?? (Reverse Channel) ? ?? ?? ?? ?? (Reverse Channel) Unlike previous approaches that rely solely on the preceding text of a single English term to de-termine its translation, we treat the entire collection of candidate pairs as a partially parallel corpus and establish the correspondences between the words using a word alignment algorithm.  At first glance, word alignment appears to be a more difficult problem than the extraction of par-enthetical translations. Extraction of parenthetical translations need only determine the first pre-parenthesis word aligned with an in-parenthesis word, whereas word alignment requires the respec-tive linking of all such (pre,in)-parenthesis word pairs. However, by casting the problem as word alignment, we are able to generalize across in-stances involving different in-parenthesis terms, giving us a larger number of, and more varied, ex-ample contexts per word. For the examples in Table 2, the words??(channel), ?? (wave passage), ?? (signal pas-sage), and ?? (passage) are aligned with Channel, and the words??(distance) and ??  (gap) are aligned with Spacing. Given these alignments, the left boundary of the translated Chinese term is simply the leftmost word that is linked to one of the English words.  Our algorithm consists of two steps: Step 1 constructs a partially parallel corpus. This step takes as input a large collection of Chinese web pages and converts the sentences with pa-rentheses containing English text into pairs of candidates. Step 2 uses an unsupervised algorithm to align English and Chinese and identify the term being translated according to the left-most aligned Chinese word. If no word alignments can be es-tablished, the pair is not considered a translation. The next two sections present the details of each of the two steps. 3 Constructing a Partially Parallel Corpus 
3.1 Filtering out non-translations The first step of our algorithm is to extract paren-theticals and then filter out those that are not trans-lations. This filtering is required as parenthetical translations represent only a small fraction of the 
usages for parentheses (see Sec. 5.1).  Table 3 shows some example of parentheses that are not translations. The input to Step 1 is a collection of arbitrary web documents. We used the following criteria to identify candidate pairs: ? The pre-parenthesis text (Tp) is predominantly in Chinese and the in-parenthesis text (Ti) is pre-dominantly in English. ? The concatenation of the digits in Tp must be identical to the concatenation of the digits in Ti. For example, rows a, b and c in Table 3 can be ruled out this way. ? If Tp contains some text in English, the same text must also appear in Ti. This filters out row d. ? Remove the pairs where Ti is part of anchor text. This rule is often applied to instances like row e where the file type tends to be inside a clickable link to a media file. ? The punctuation characters in Tp must also ap-pear in Ti, unless they are quotation marks. The example in row f  is ruled out because ?/? is not found in the pre-parenthesis text.  Examples with translations in italic Function of the in-parenthesis text a ??????1.4~3.0?? (MacArthur, 1967) The range of its values is within 1.4~3.0 (MacArthur, 1967)  
to provide citation 
b ????/??? (VN901 15:20-22:30) Vietnam Airlines Beijing/Ho Chi Minh (VN901 15:20-22:30)  
flight information 
c ??????255-8FT? sale of pool table (255-8FT)  product Id.  d // ??? // void main  ( void  ) // main program // void main  (void )  
function declaration 
e ????: ??? (DVD) movie title: Thousand Year Lake (DVD) 
DVD is the file type 
f ?? ? ?? ? ?? ( g/L) mass consumed by water sample (g/L) 
measurement unit 
g ?????? (Sensitive) gentle protective facial cream (Sensitive)  
to indicate the type of the cream  
h ????????????? (Ask Jeeves) Evaluation of Nine Main Search Engines in the US: Chapter 4 (Ask Jeeves) 
Chapter 4 is about Ask Jeeves 
Table 3: Other uses of parentheses 
996
The instances in rows g and h cannot be eliminated by these simple rules, and are filtered only later, as we fail to discover a convincing word alignment. 3.2  Constraining term boundaries Similar to (Cao et al 2007), we segmented the pre-parenthesis Chinese text and restrict the term boundary to be one of the segmentation bounda-ries. Since parenthetical translations are mostly translation of terms, it makes sense to further con-strain the left boundary of the Chinese side to be a term boundary. Determining what should be counted as a term is a difficult task and there are not yet well-accepted solutions (Sag et al 2003).  We compiled an approximate term vocabulary by taking the top 5 million most frequent Chinese queries as according to a fully anonymized collec-tion of search engine query logs. Given a Chinese sentence, we first identify all (possibly overlapping) sequences of words in the sentence that match one of the top-5M queries. A matching sequence is called a maximal match if it is not properly contained in another matching se-quence. We then define the potential boundary positions to be the boundaries of maximal matches or words that are not covered by any of the top-5M queries.  3.3 Length-based trimming If there are numerous Chinese words preceding a pair of parentheses containing two English words, it is very unlikely for all but the right-most few Chinese words to be part of the translation of the English words. Including extremely long se-quences as potential candidates introduces signifi-cantly more noise and makes word alignment harder than necessary. We therefore trimmed the pre-parenthesis text with a length-based constraint. The cut-off point is the first (counting from right to left) potential boundary position (see Sec. 3.2) such that C ? 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been 
proposed (Brown et al 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in de-scending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of align-ment algorithms and found Competitive Linking to have one of the highest precision scores. A disad-vantage of Competitive Linking, however, is that the alignments are restricted word-to-word align-ments, which implies that multi-word expressions can only be partially linked at best.  4.1 Dealing with multi-word alignment We made a small change to Competitive Linking to allow consecutive sequence of words on one side to be linked to the same word on the other side. Specifically, instead of requiring both ei and fj to have no previous linkages, we only require that at least one of them be unlinked and that (suppose ei is unlinked and fj is linked to ek) none of the words between ei and ek be linked to any word other than fj.  4.2 Link scoring We used ?2 (Gale and Church, 1991) as the link score in the modified competitive linking algo-rithm, although there are many other possible choices for the link scores, such as ?2 (Zhang, S. Vogel. 2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al 2005). The ?2 statistics for a pair of words ei and fj is computed as 
( )
( )( )( )( )dcdbcaba
bcad
++++
?
=
2
2?  where a is the number of sentence pairs containing both ei and fj; a+b is the number of sentence pairs containing ei; a+c is the number of sentence pairs containing  fj; d is the number of sentence pairs containing nei-ther ei nor fj. 
997
The ?2 score ranges from 0 to 1. We set a threshold at 0.001, below which the ?2 scores are treated as 0. 4.3 Bias in the partially parallel corpus Since only the last few Chinese words in a candi-date pair are expected to be translated, there should be a preference for linking the words towards the end of the Chinese text. One advantage of Com-petitive Linking is that it is quite easy to introduce such preferences into the algorithm, by using the word positions to break ties of the ?2 scores when sorting the word pairs. 4.4 Capturing syllable-level regularities Many of the parenthetical translations involve proper names, which are often transliterated ac-cording to the sound. Word alignment algorithms have generally ignored syllable-level regularities in transliterated terms.  Consider again the Shapiro example in the introduction section. There are nu-merous correct transliterations for the same Eng-lish word, some of which are not very frequent. For example, the word ???happens to have a similar ?2 score with Shapiro as the word ?? (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.  Previous approaches to parenthetical translations relied on specialized algorithms to deal with trans-literations (Cao et al 2007; Jiang et al 2007; Wu and Chang, 2007). They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model. We adopted a simpler approach that does not re-quire any additional resources such as pronuncia-tion dictionaries and bilingual dictionaries. In addition to computing the ?2 scores between words, we also compute the ?2 scores of prefixes and suffixes of Chinese and English words. For both languages, the prefix of a word is defined as the first three bytes of the word and the suffix is defined as the last three bytes. Since we used UTF-8 encoding, the first and last three bytes of a Chi-nese word, except in very rare cases, correspond to the first and last Chinese character of the word. Table 4 lists the English prefixes and suffixes that have the highest ?2 scores with the Chinese prefix?and suffix?. 
 Type Chinese English prefix ? sha, amo, cha, sum, haw, lav, lun, xia, xal, hnl, shy, eve, she, cfh, ? suffix ? rlo, llo, ouh, low, ilo, owe, lol, lor, zlo, klo, gue, ude, vir, row, oro, olo, aro, ulo, ero, iro, rro, loh, lok, ? Table 4: Example prefixes and suffixes with top ?2 In our modified version of the competitive link-ing algorithm, the link score of a pair of words is the sum of the ?2 scores of the words themselves, their prefixes and their suffixes.  In addition to syllable-level correspondences in transliterations, the ?2 scores of prefixes and suf-fixes can also capture correlations in morphologi-cally composed words. For example, the Chinese prefix ? (three) has a relatively high ?2 score with the English prefix tri. Such scores enable word alignments to be made that may otherwise be missed. Consider the following text snippet: ...... ?  ? ??? (triaziflam)  The correct translation for triaziflam is?????.  However, the Chinese term is segmented as ? + ? + ???. The association between? (three) and triaziflam is very weak because ?is a very frequent word, whereas triaziflam is an extremely rare word. With the addition of the ?2 score be-tween ?and tri, we were able to correctly estab-lish the connection between triaziflam and ?. It turns out to be quite effective to assume pre-fixes and suffixes of words consist of three bytes, despite its apparent simplicity. The benefit of ?2 scores for prefixes and suffixes is not limited to morphemes that happen to be three bytes long.  For example, the English morpheme ?du-? corresponds to the Chinese character ? (two). Although the ?2 between du and? won?t be computed, we do find high ?2 scores between? and due and between? and dua. The three letter prefixes account for many of the words with the du- prefix. 5 Experimental Results We extracted from Chinese web pages about 1.58 billion unique sentences with parentheses that con-tain ASCII text. We removed duplicate sentences so that duplications of web documents will not skew the statistics. By applying the filtering algo-rithm in Sec. 3.1, we constructed a partially paral-
998
lel corpus with 126,612,447 candidate pairs (46,791,841 unique), which is about 8% of the number of sentences. Using the word alignment algorithm in Sec. 4, we extracted 26,753,972 trans-lation pairs between 13,471,221 unique English terms and 11,577,206 unique Chinese terms. Parenthetical translations mined from the Web have mostly been evaluated by manual examina-tion of a small sample of results (usually a few hundred entries) or in a Cross Lingual Information Retrieval setup. There does not yet exist a common evaluation data set.  5.1 Evaluation with Wikipedia Our first evaluation is based on translations in Wikipedia, which contains far more terminology and proper names than bilingual dictionaries. We extracted the titles of Chinese and English Wikipe-dia articles that are linked to each other and treated them as gold standard translations. There are 79,714 such pairs. We removed the following types of pairs because they are not translations or are not terms: ? Pairs with identical strings. For example, both English and Chinese versions have an entry ti-tled ?.ch?; ? Pairs where the English term begins with a digit, e.g., ?245?, ?300 BC?, ?1991 in film?; ? Pairs where the English term matches the regu-lar expression ?List of .*?, e.g., ?List of birds?, ?List of cinemas in Hong Kong?; ? Pairs where the Chinese title does not have any non-ASCII code. For example, the English en-try ?Syncfusion? is linked to ?.NET Frame-work? in the Chinese Wikipedia. The resulting data set contains 68,131 transla-tion pairs between 62,581 Chinese terms and 67,613 English terms. Only a small percentage of terms have more than one translation.  Whenever there is more than one translation, we randomly pick one as the answer key. For each Chinese and English word in the Wikipedia data, we first find whether there is a translation for the word in the extracted translation pairs. The Coverage of the Wikipedia data is measured by the percentage of words for which one or more translations are found. We then see whether our most frequent translation is an Exact Match of the answer key in the Wikipedia data.    
  Coverage Exact Match Full 70.8% 36.4% -term 67.1% 34.8% -pre-suffix 67.6% 34.4% IBM 67.6% 31.2% LDC 10.8% 4.8% Table 5: Chinese to English Results    Coverage Exact Match Full 59.6% 27.9% -term 59.6% 27.5% -pre-suffix 58.9% 27.4% IBM 52.4% 13.4% LDC 3.0% 1.4% Table 6: English to Chinese Results  Table 5 and 6 show the Chinese-to-English and English-to-Chinese results for the following sys-tems: Full refers to our system described in Sec. 3 and 4; -term is the system without the use of query logs to restrict potential term boundary posi-tions (Sec. 3.2); -pre-suffix is the system without using the ?2 score of the prefixes and suffixes; IBM refers to a system where we substitute our word alignment algorithm with IBM Model 1 and Model 2 followed by the HMM alignment (Och and Ney 2003), which is a common configuration for the word align-ment components in machine translations systems; LDC refers to the LDC2.0 English to Chinese bilingual dictionary with 161,117 translation pairs. It can be seen that the use of queries to constrain boundary positions and the addition of ?2 scores of prefixes/suffixes improve the percentage of Exact Match. The IBM Model tends to make many more alignments than Completive Linking. While this is often beneficial for machine translation systems, it is not very suitable for creating bilingual dictionar-ies, where precision is of paramount importance. The LDC dictionary was manually compiled from diverse resources within LDC and (mostly) from the Internet. Its coverage of Wikipedia data is ex-tremely low, compared to our method.  
999
English Wikipedia Translation Parenthetical Translation Pumping lemma ??? ??1 Topic-prominent language ?????? ?????1 Yoido Full Gos-pel Church ???????? ??????1 First Bulgarian Empire ???????? ?????????2 Vespid ?? ??????2 Ibrahim Rugova ???????? ???3 Jerry West ?????? ???3 Nicky Butt ????? ??3 Benito Mussolini ???????? ????3 Ecology of Hong Kong ???? ?????* Paracetamol ?????? ????* Thermidor ?? ??* Udo ?? ?? Public opinion ?? ???? Michael Bay ???? ????? Dagestan ??????? ???? Battle of Leyte Gulf ????? ??????? Glock ????? ??? Ergonomics ????? ??? Frank Sinatra ??????? ?????? Zaragoza ????? ???? Komodo ???? ???? Eli Vance ????? ??????? Manitoba ???? ????? Giant Bottlenose Whale ????? ???? Exclusionary rule ?????? ?????? Computer worm ???? ????? Social network ????? ???? Glasgow School of Art ???????? ???????? Dee Hock ????? ????? Bondage ?? ?? The China Post ?????? ???? Rachel ?? ?? John Nash ????? ????? Hattusa ??? ??? Bangladesh ???? ??? Table 7: A random sample of non-exact-matches                                                            1the extracted translation is too short 2the extracted translation is too long 3the extracted translation contains only the last name *the extracted term is completely wrong.   
 Note that Exact Match is a rather stringent crite-rion. Table 7 shows a random sample of extracted parenthetical translations that failed the Exact Match test. Only a small percentage of them are genuine errors. We nonetheless adopted this meas-ure because it has the advantage of automated evaluation and our goal is mainly to compare the relative performances. To determine the upper bound of the coverage of our web data, for each Wikipedia English term we searched within the total set of available paren-thesized text fragments (our English candidate set before filtering as by Step 1).  We discovered 81% of the Wikipedia titles, which is approximately 10% above the coverage of our final output. This indicates a minor loss of recall because of mistakes made in filtering (Sec. 3.1) and/or word alignment.  5.2 Evaluation with term translation requests To evaluate the coverage of output produced by their method, Cao et al(2007) extracted English queries from the query log of a Chinese search en-gine. They assume that the reason why users typed the English queries in a Chinese search box is mostly to find out their Chinese translations. Ex-amining our own Chinese query logs, however, the most-frequent English queries appear to be naviga-tional queries instead of translation requests. We therefore used the following regular expression to identify queries that are unambiguously translation requests:  /^[a-zA-Z ]* ???$/ where???means ??s Chinese?. This regular ex-pression matched 1579 unique queries in the logs. We manually judged the translation for 200 of them. A small random sample of the 200 is shown in Table 8. The empty cells indicate that the Eng-lish term is missing from our translation pairs. We use * to mark incorrect translations. When com-pared with the sample queries in (Cao et al, 2007), the queries in our sample seem to contain more phrasal words and technical terminology. It is in-teresting to see that even though parenthetical translations tend to be out-of-vocabulary words, as we have remarked in the introduction, the sheer size of the web means that occasionally transla-tions of common words such as ?use? are some-times included as well. 
1000
We compared our results with translations ob-tained from Google and Yahoo?s translation serv-ices. The numbers of correct translations for the random sample of 200 queries are as follows: Systems Google Yahoo! Mined Mined+G Correct 115 84 116 135 Our system?s outputs (Mined) have the same accuracy as the Google Translate. Our outputs have results for 154 out of the 200 queries. The 46 missing results are considered incorrect. If we combine our results with Google Translate by looking up Google results for missing entries, the accuracy increases from 56% to 68% (Mined+G). If we treat the LDC Chinese-English Dictionary 2.0 as a translator, it only covers 20.5% of the 200 queries.  5.3 Evaluation with SMT The extracted translations may serve as training data for statistical machine translation systems. To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al 2003; Brants et al 2007) with the FBIS Chinese-English parallel text (NIST, 2003). We then added the extracted translation pairs as 
additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. 6 Related Work Nagata et al (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration. It is difficult to build a truly large-scale translation lexicon this way because the English terms them-selves may be hard to come by.  Cao et al (2007), like us, used a 300GB collec-tion of web documents as input. They used super-vised learning to build models that deal with phonetic transliterations and semantic translations separately. Our work relies on unsupervised learn-ing and does not make a distinction between trans-lations and transliterations. Furthermore, we are able to extract two orders of magnitude more trans-lations from than (Cao et al, 2007). 7 Conclusion We presented a method to apply a word alignment algorithm on a partially parallel corpus to extract translation pairs from the web. Treating the transla-tion extraction problem as a word alignment prob-lem allowed us to generalize across instances involving different in-parenthesis terms. Our algo-rithm extends Competitive Linking to deal with multi-word alignments and takes advantage of word-internal correspondences between transliter-ated words or morphologically composed words. Finally, through our discussion of parallel Wikipe-dia topic titles as a gold standard, we presented the first evaluation of such an extraction system that went beyond manual judgments on small sized samples. Acknowledgments We would like to thank the anonymous reviewers for their valuable comments.  
buckingham palace ???? chinadaily ???? coo ????? diammonium sulfate  emilio pucci ??????? finishing school ???? gloria ???? horny ?????* jam ?? lean six sigma ?????? meiosis ???? near miss ???? pachycephalosaurus ??? pops ???????? recreation vehicle ????? shanghai ethylene cracker complex  stenonychosaurus ??? theanine ??? use ?? with you all the time ??????????? Table 8: A small sample of manually judged query translations 
1001
References  T. Brants, A. Popat, P. Xu, F. Och and J. Dean, Large Language Models for Machine Translation, EMNLP-CoNLL-2007. P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Compu-tational Linguistics, 19(2):263?311. G. Cao, J. Gao and J.Y. Nie. 2007. A system to mine large-scale bilingual dictionaries from monolingual Web pages, MT Summit, pp. 57-64. T. Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguis-tics 19, 1. W. Gale and K. Church. 1991. Identifying word corre-spondence in parallel text. In Proceedings of the DARPA NLP Workshop. L. Jiang, M. Zhou, L.F. Chien, C. Niu. 2007. Named Entity Translation with Web Mining and Translitera-tion. In Proc. of IJCAI-2007. pp. 1629-1634. P. Koehn, F. Och and D. Marcu, Statistical Phrase-based Translation, In Proc. of HLT-NAACL 2003. K.L. Kwok, P. Deng, N. Dinstl, H.L. Sun, W. Xu, P. Peng, and J. Doyon. 2005. CHINET: a Chinese name finder system for document triage. In Proceedings of 2005 International Conference on Intelligence Analysis. I.D. Melamed. 2000. Models of translational equiva-lence among words. Computational Linguistics, 26(2):221?249. M. Nagata, T. Saito, and K. Suzuki. 2001. Using the Web as a bilingual dictionary. In Proc. of ACL 2001 DD-MT Workshop, pp.95-102. NIST. 2003. The NIST machine translation evaluations. http://www.nist.gov/speech/tests/mt/. F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19?51. I.A. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Proc. of CICLing-2002, pp 1?15, Mexico City, Mexico. B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-criminative matching approach to word alignment. In Proc. of HLT/EMNLP-05. Vancouver, BC. J. Tiedemann. 2004. Word to word alignment strategies. In Proceedings of the 20th international Conference on Computational Linguistics. Geneva, Switzerland.  
J.C. Wu and J.S. Chang. 2007. Learning to Find English to Chinese Transliterations on the Web. In Proc. of EMNLP-CoNLL-2007.  pp.996-1004. Prague, Czech Republic. Y. Zhang, S. Vogel. 2005 Competitive Grouping in In-tegrated Phrase Segmentation and Alignment Model. in Proceedings of ACL-05 Workshop on Building and Parallel Text. Ann Arbor, MI.  
1002
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1030?1038,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Phrase Clustering for Discriminative Learning 
Dekang Lin and Xiaoyun Wu 
Google, Inc. 
1600 Amphitheater Parkway, Mountain View, CA 
{lindek,xiaoyunwu}@google.com 
 
Abstract 
We present a simple and scalable algorithm for 
clustering tens of millions of phrases and use 
the resulting clusters as features in 
discriminative classifiers. To demonstrate the 
power and generality of this approach, we 
apply the method in two very different 
applications: named entity recognition and 
query classification. Our results show that 
phrase clusters offer significant improvements 
over word clusters. Our NER system achieves 
the best current result on the widely used 
CoNLL benchmark. Our query classifier is on 
par with the best system in KDDCUP 2005 
without resorting to labor intensive knowledge 
engineering efforts. 
1 Introduction 
Over the past decade, supervised learning 
algorithms have gained widespread acceptance in 
natural language processing (NLP). They have 
become the workhorse in almost all sub-areas 
and components of NLP, including part-of-
speech tagging, chunking, named entity 
recognition and parsing. To apply supervised 
learning to an NLP problem, one first represents 
the problem as a vector of features. The learning 
algorithm then optimizes a regularized, convex 
objective function that is expressed in terms of 
these features.  The performance of such 
learning-based solutions thus crucially depends 
on the informativeness of the features. The 
majority of the features in these supervised 
classifiers are predicated on lexical information, 
such as word identities. The long-tailed 
distribution of natural language words implies 
that most of the word types will be either unseen 
or seen very few times in the labeled training 
data, even if the data set is a relatively large one 
(e.g., the Penn Treebank). 
While the labeled data is generally very costly 
to obtain, there is a vast amount of unlabeled 
textual data freely available on the web. One way 
to alleviate the sparsity problem is to adopt a 
two-stage strategy: first create word clusters with 
unlabeled data and then use the clusters as 
features in supervised training. Under this 
approach, even if a word is not found in the 
training data, it may still fire cluster-based 
features as long as it shares cluster assignments 
with some words in the labeled data.  
Since the clusters are obtained without any 
labeled data, they may not correspond directly to 
concepts that are useful for decision making in 
the problem domain. However, the supervised 
learning algorithms can typically identify useful 
clusters and assign proper weights to them, 
effectively adapting the clusters to the domain. 
This method has been shown to be quite 
successful in named entity recognition (Miller et 
al. 2004) and dependency parsing (Koo et al, 
2008).  
In this paper, we present a semi-supervised 
learning algorithm that goes a step further. In 
addition to word-clusters, we also use phrase-
clusters as features. Out of context, natural 
language words are often ambiguous. Phrases are 
much less so because the words in a phrase 
provide contexts for one another.  
Consider the phrase ?Land of Odds?. One 
would never have guessed that it is a company 
name based on the clusters containing Odds and 
Land. With phrase-based clustering, ?Land of 
Odds? is grouped with many names that are 
labeled as company names, which is a strong 
indication that it is a company name as well. The 
disambiguation power of phrases is also 
evidenced by the improvements of phrase-based 
machine translation systems (Koehn et. al., 
2003) over word-based ones. 
Previous approaches, e.g., (Miller et al 2004) 
and (Koo et al 2008), have all used the Brown 
algorithm for clustering (Brown et al 1992). The 
main idea of the algorithm is to minimize the 
bigram language-model perplexity of a text 
corpus. The algorithm is quadratic in the number 
of elements to be clustered. It is able to cluster 
tens of thousands of words, but is not scalable 
enough to deal with tens of millions of phrases. 
Uszkoreit and Brants (2008) proposed a 
1030
distributed clustering algorithm with a similar 
objective function as the Brown algorithm. It 
substantially increases the number of elements 
that can be clustered. However, since it still 
needs to load the current clustering of all 
elements into each of the workers in the 
distributed system, the memory requirement 
becomes a bottleneck. 
We present a distributed version of a much 
simpler K-Means clustering that allows us to 
cluster tens of millions of elements. We 
demonstrate the advantages of phrase-based 
clusters over word-based ones with experimental 
results from two distinct application domains: 
named entity recognition and query 
classification. Our named entity recognition 
system achieves an F1-score of 90.90 on the 
CoNLL 2003 English data set, which is about 1 
point higher than the previous best result. Our 
query classifier reaches the same level of 
performance as the KDDCUP 2005 winning 
systems, which were built with a great deal of 
knowledge engineering. 
2 Distributed K-Means clustering 
K-Means clustering (MacQueen 1967) is one of 
the simplest and most well-known clustering 
algorithms. Given a set of elements represented 
as feature vectors and a number, k, of desired 
clusters, the K-Means algorithm consists of the 
following steps: 
Step Operation 
i. Select k elements as the initial centroids 
for k clusters. 
ii. Assign each element to the cluster with 
the closest centroid according to a 
distance (or similarity) function. 
iii. Recompute each cluster?s centroid by 
averaging the vectors of its elements 
iv. Repeat Steps ii and iii until 
convergence 
Before describing our parallel implementation of 
the K-Means algorithm, we first describe the 
phrases to be clusters and how their feature 
vectors are constructed. 
2.1 Phrases 
To obtain a list of phrases to be clustered, we 
followed the approach in (Lin et al, 2008) by 
collecting 20 million unique queries from an 
anonymized query log that are found in a 700 
billion token web corpus with a minimum 
frequency count of 100. Note that many of these 
queries are not phrases in the linguistic sense. 
However, this does not seem to cause any real 
problem because non-linguistic phrases may 
form their own clusters. For example, one cluster 
contains {?Cory does?, ?Ben saw?, ?I can?t 
lose?, ?..}.  
To reduce the memory requirement for storing 
a large number of phrases, we used Bloom Filter 
(Bloom 1970) to decide whether a sequence of 
tokens is a phrase. The Bloom filter allows a 
small percentage of false positives to pass 
through. We did not remove them with post 
processing since our notion of phrases is quite 
loose to begin with. 
2.2 Context representation 
Distributional word clustering is based on the 
assumption that words that appear in similar 
contexts tend to have similar meanings. The 
same assumption holds for phrases as well. 
Following previous approaches to distributional 
clustering of words, we represent the contexts of 
a phrase as a feature vector. There are many 
possible definitions for what constitutes the 
contexts. In the literature, contexts have been 
defined as subject and object relations involving 
the word (Hindle, 1990), as the documents 
containing the word (Deerwester et al 1990), or 
as search engine snippets for the word as a query 
(Sahami and Heilman, 2006). We define the 
contexts of a phrase to be small, fixed-sized 
windows centered on occurrences of the phrase 
in a large corpus. The features are the words 
(tokens) in the window. The context feature 
vector of a phrase is constructed by first 
aggregating the frequency counts of the words in 
the context windows of different instances of the 
Table 1 Cluster of ?English lessons? 
Window Cluster members (partial list) 
size=1 environmental courses, summer school 
courses, professional development 
classes, professional training programs, 
further education courses, leadership 
courses, accelerated courses, vocational 
classes, technical courses, technical 
classes, special education courses, ?.. 
size=3 learn english spanish, grammar learn, 
language learning spanish, translation 
spanish language, learning spanish 
language, english spanish language, 
learn foreign language, free english 
learning, language study english, 
spanish immersion course, how to 
speak french, spanish learning games, 
?.. 
1031
phrase. The frequency counts are then converted 
into point-wise mutual information (PMI) values: 
2/+:LDN? B; L ???F 2:LDN? B;2:LDN;2:B;G 
where phr is a phrase and  f  is a feature of 
phr. PMI effectively discounts the prior 
probability of the features and measures how 
much beyond random a feature tends to occur in 
a phrase?s context window.  Given two feature 
vectors, we compute the similarity between two 
vectors as the cosine function of the angle 
between the vectors. Note that even though a 
phrase phr can have multiple tokens, its feature f 
is always a single-word token.  
We impose an upper limit on the number of 
instances of each phrase when constructing its 
feature vector. The idea is that if we have already 
seen 300K instances of a phrase, we should have 
already collected enough data for the phrase. 
More data for the same phrase will not 
necessarily tell us anything more about it. There 
are two benefits for such an upper limit. First, it 
drastically reduces the computational cost. 
Second, it reduces the variance in the sizes of the 
feature vectors of the phrases. 
2.3 K-Means by MapReduce  
K-Means is an embarrassingly parallelizable 
algorithm. Since the centroids of clusters are 
assumed to be constant within each iteration, the 
assignment of elements to clusters (Step ii) can 
be done totally independently. 
The algorithm fits nicely into the MapReduce 
paradigm for parallel programming (Dean and 
Ghemawat, 2004). The most straightforward 
MapReduce implementation of K-Means would 
be to have mappers perform Step ii and reducers 
perform Step iii. The keys of intermediate pairs 
are cluster ids and the values are feature vectors 
of elements assigned to the corresponding 
cluster. When the number of elements to be 
clustered is very large, sorting the intermediate 
pairs in the shuffling stage can be costly. 
Furthermore, when summing up a large number 
of features vectors, numerical underflow 
becomes a potential problem.  
A more efficient and numerically more stable 
method is to compute, for each input partition, 
the partial vector sums of the elements belonging 
to each cluster. When the whole partition is done, 
the mapper emits the cluster ids as keys and the 
partial vector sums as values. The reducers then 
aggregate the partial sums to compute the 
centroids. 
2.4 Indexing centroid vectors 
In a na?ve implementation of Step ii of K-Means, 
one would compute the similarities between a 
feature vector and all the centroids in order to 
find the closest one. The kd-tree algorithm 
(Bentley 1980) aims at speeding up nearest 
neighbor search. However, it only works when 
the vectors are low-dimensional, which is not the 
case here. Fortunately, the high-dimensional and 
sparse nature of our feature vectors can also be 
exploited.  
Since the cosine measure of two unit length 
vectors is simply their dot product, when 
searching for the closest centroid to an element, 
we only care about features in the centroids that 
are in common with the element. We therefore 
create an inverted index that maps a feature to 
the list of centroids having that feature. Given an 
input feature vector, we can iterate through all of 
its components and compute its dot product with 
all the centroids at the same time. 
2.5 Sizes of context window 
In our experiments, we use either 1 or 3 as the 
size of the context windows. Window size has an 
interesting effect on the types of clusters. With 
larger windows, the clusters tend to be more 
topical, whereas smaller windows result in 
categorical clusters.  
For example, Table 1 contains the cluster that 
the phrase ?English lessons? belongs to. With 3-
word context windows, the cluster is about 
language learning and translation. With 1-word 
context windows, the cluster contains different 
types of lessons. 
The ability to produce both kinds of clusters 
turns out to be very useful. In different 
applications we need different types of clusters. 
For example, in the named entity recognition 
task, categorical clusters are more successful, 
whereas in query categorization, the topical 
clusters are much more beneficial.  
The Brown algorithm uses essentially the 
same information as our 1-word window 
clusters. We therefore expect it to produce 
mostly categorical clusters.  
2.6 Soft clustering 
Although K-Means is generally described as a 
hard clustering algorithm (each element belongs 
to at most one cluster), it can produce soft 
clustering simply by assigning an element to all 
clusters whose similarity to the element is greater 
than a threshold. For natural language words and 
1032
phrases, the soft cluster assignments often reveal 
different senses of a word. For example, the 
word Whistler may refer to a town in British 
Columbia, Canada, which is also a ski resort, or 
to a painter. These meanings are reflected in the 
top clusters assignments for Whistler in Table 2 
(window size = 3). 
2.7 Clustering data sets 
We experimented with two corpora (Table 3). 
One contains web documents with 700 billion 
tokens. The second consists of various news texts 
from LDC: English Gigaword, the Tipster corpus 
and Reuters RCV1. The last column lists the 
numbers of phrases we used when running the 
clustering with that corpus.  
Even though our cloud computing 
infrastructure made phrase clustering possible, 
there is no question that it is still very time 
consuming. To create 3000 clusters among 20 
million phrases using 3-word windows, each K-
Means iteration takes about 20 minutes on 1000 
CPUs. Without using the indexing technique in 
Section 2.4, each iteration takes about 4 times as 
long. In all our experiments, we set the 
maximum number of iterations to be 50. 
3 Named Entity Recognition 
Named entity recognition (NER) is one of the 
first steps in many applications of information 
extraction, information retrieval, question 
answering and other applications of NLP. 
Conditional Random Fields (CRF) (Lafferty et. 
al. 2001) is one of the most competitive NER 
algorithms. We employed a linear chain CRF 
with L2 regularization as the baseline algorithm 
to which we added phrase cluster features. 
The CoNLL 2003 Shared Task (Tjong Kim 
Sang and Meulder 2003) offered a standard 
experimental platform for NER. The CoNLL 
data set consists of news articles from Reuters1. 
The training set has 203,621 tokens and the 
development and test set have 51,362 and 46,435 
tokens, respectively. We adopted the same 
evaluation criteria as the CoNLL 2003 Shared 
Task. 
To make the clusters more relevant to this 
domain, we adopted the following strategy: 
1. Construct the feature vectors for 20 
million phrases using the web data. 
2. Run K-Means clustering on the phrases 
that appeared in the CoNLL training data 
to obtain K centroids. 
3. Assign each of the 20 million phrases to 
the nearest centroid in the previous step. 
3.1 Baseline features 
The features in our baseline CRF classifier are a 
subset of the conventional features. They are 
defined with the following templates: >U??,>U??5???,<>U?? S??=?@??5?>5 ? <>U??5??? S??=?@??5?>5 ,  <>U?? OBTu??=?@??5?>5 ,   <>U??5??? OBTu??=?@??5?>5 ,  <<>U?? SPL?? ?=?@??5?>5 =?@68 ,<<>U??5??? SPL?? ?=?@??5?>5 =?@68 ?  <>U?? S??5???=?@??>5 ,<>U??5??? S??5???=?@??>5 ,       <<>U?? SPL??5??? ?=?@??>5=?@57 ,<<>U??5??? SPL??5??? ?=?@??>5=?@57  
Here, s denotes a position in the input sequence; 
ys is a label that indicates whether the token at 
position s is a named entity as well as its type; wu 
is the word at position u; sfx3 is a word?s three-
letter suffix; <SPL?=?@58  are indicators of 
                                                          
1
 http://www.reuters.com/researchandstandards/ 
Table 2 Soft clusters for Whistler 
cluster1: sim=0.17, members=104048 
bc vancouver, british columbia accommodations, 
coquitlam vancouver, squamish vancouver, 
langley vancouver, vancouver surrey,  ? 
cluster2: sim=0. 16, members= 182692 
vail skiing, skiing colorado, tahoe ski vacation, 
snowbird skiing, lake tahoe skiing, breckenridge 
skiing, snow ski packages, ski resort whistler, ? 
cluster3: sim=0.12, members= 91895 
ski chalets france, ski chalet holidays, france ski, 
catered chalets, luxury ski chalets, france skiing, 
france skiing, ski chalet holidays, ?? 
cluster4: sim=0.11, members=237262 
ocean kayaking, mountain hiking, horse trekking, 
river kayaking, mountain bike riding, white water 
canoeing, mountain trekking, sea kayaking, ?? 
cluster5: sim=0.10, members=540775 
rent cabin, pet friendly cabin, cabins rental, cabin 
vacation, cabins colorado, cabin lake tahoe, maine 
cabin, tennessee mountain cabin,  ? 
cluster6: sim=0.09, members=117365 
mary cassatt, oil painting reproductions, henri 
matisse, pierre bonnard, edouard manet, auguste 
renoir, paintings famous, picasso paintings, ?? 
?? 
 
Table 3 Corpora used in experiments 
Corpus Description tokens phrases 
Web web documents 700B 20M 
LDC News text from LDC 3.4B 700K 
1033
different word types: wtp1 is true when a word is 
punctuation; wtp2 indicates whether a word is in 
lower case, upper case, or all-caps; wtp3 is true 
when a token is a number; wtp4 is true when a 
token is a hyphenated word with different 
capitalization before and after the hyphen. 
NER systems often have global features to 
capture discourse-level regularities (Chieu and 
Ng 2003). For example, documents often have a 
full mention of an entity at the beginning and 
then refer to the entity in partial or abbreviated 
forms. To help in recognizing the shorter 
versions of the entities, we maintain a history of 
unigram word features. If a token is encountered 
again, the word unigram features of the previous 
instances are added as features for the current 
instance as well. We have a total of 48 feature 
templates. In comparison, there are 79 templates 
in (Suzuki and Isozaki, 2008).  
Part-of-speech tags were used in the top-
ranked systems in CoNLL 2003, as well as in 
many follow up studies that used the data set 
(Ando and Zhang 2005; Suzuki and Isozaki 
2008).  Our system does not need this 
information to achieve its peak performance. An 
important advantage of not needing a POS tagger 
as a preprocessor is that the system is much 
easier to adapt to other languages, since training 
a tagger often requires a larger amount of more 
extensively annotated data than the training data 
for NER. 
3.2 Phrase cluster features 
We used hard clustering with 1-word context 
windows for NER. For each input token 
sequence, we identify all sequences of tokens 
that are found in the phrase clusters. The phrases 
are allowed to overlap with or be nested in one 
another. If a phrase belonging to cluster c is 
found at positions b to e (inclusive), we add the 
following features to the CRF classifier: >U??5? $??? >U?>5? #??? >U??6???5? $??? >U???>5? #?? >U?? 5??? <>U?? /??=?@?>5??5 ? >U? ? '?? >U??5??? 5??? <>U??5??? /??=?@?>5??5 ? >U??5?? ? '?? 
where B (before), A (after), S (start), M (middle), 
and E (end) denote a position in the input 
sequence relative to the phrase belonging to 
cluster c. We treat the cluster membership as 
binary. The similarity between an element and its 
cluster centroid is ignored. For example, suppose 
the input sentence is ?? guitar legend Jimi 
Hendrix was ?? and ?Jimi Hendrix? belongs to 
cluster 183. Figure 1 shows the attributes at 
different input positions. The cluster features are 
the cross product of the unigram/bigram labels 
and the attributes. 
 
Figure 1 Phrase cluster features 
 
The phrasal cluster features not only help in 
resolving the ambiguities of words within a 
phrase, the B and A features also allow words 
adjacent to a phrase to consider longer contexts 
than a single word. Although one may argue 
longer n-grams can also capture this information, 
the sparseness of n-grams means that long n-
gram features are rarely useful in practice.  
We can easily use multiple clusterings in 
feature extraction. This allows us to side-step the 
matter of choosing the optimal value k in the K-
Means clustering algorithm.  
Even though the phrases include single token 
words, we create word clusters with the same 
clustering algorithm as well. The reason is that 
the phrase list, which comes from query logs, 
does not necessarily contain all the single token 
words in the documents. Furthermore, due to 
tokenization differences between the query logs 
and the documents, we systematically missed 
some words, such as hyphenated words. When 
creating the word clusters, we do not rely on a 
predefined list. Instead, any word above a 
minimum frequency threshold is included.  
In their dependency parser with cluster-based 
features, Koo et al (2008) found it helpful to 
restrict lexicalized features to only relatively 
frequent words. We did not observe a similar 
phenomenon with our CRF. We include all 
words as features and rely on the regularized 
CRF to select from them.  
3.3 Evaluation results 
Table 4 summarizes the evaluation results for 
our NER system and compares it with the two 
best results on the data set in the literature, as 
well the top-3 systems in CoNLL 2003. In this 
table, W and P refer to word and phrase clusters 
created with the web corpus. The superscripts are 
the numbers of clusters. LDC refers to the 
clusters created with the smaller LDC corpus and 
+pos indicates the use of part-of-speech tags as 
features.  
The performance of our baseline system is 
rather mediocre because it has far fewer feature 
functions than the more competitive systems. 
1034
The Top CoNLL 2003 systems all employed 
gazetteers or other types of specialized resources 
(e.g., lists of words that tend to co-occur with 
certain named entity types) in addition to part-of-
speech tags. 
Introducing the word clusters immediately 
brings the performance up to a very competitive 
level. Phrasal clusters obtained from the LDC 
corpus give the same level of improvement as 
word clusters from the web corpus that is 20 
times larger. The best F-score of 90.90, which is 
about 1 point higher than the previous best result, 
is obtained with a combination of clusters. 
Adding POS tags to this configuration caused a 
small drop in F1. 
4 Query Classification 
We now look at the use of phrasal clusters in a 
very different application: query classification. 
The goal of query classification is to determine 
to which ones of a predefined set of classes a 
query belongs. Compared with documents, 
queries are much shorter and their categories are 
much more ambiguous.  
4.1 KDDCUP 2005 data set 
The task in the KDDCUP 2005 competition2 is to 
classify 800,000 internet user search queries into 
67 predefined topical categories. The training set 
consists of 111 example queries, each of which 
belongs to up to 5 of the 67 categories. Table 5 
shows three example queries and their classes.  
Three independent human labelers classified 
800 queries that were randomly selected from the 
                                                          
2
 http://www.acm.org/sigs/sigkdd/kdd2005/kddcup.html 
complete set of 800,000. The participating 
systems were evaluated by their average F-scores 
(F1) and average precision (P) over these three 
sets of answer keys for the 800 selected queries.  
 L ? S?????????????????????????gg ? S????????????????gg  
 L ? S?????????????????????????gg? S?????????????????????gg  
	s L t H  H E   
Here, ?tagged as? refer to systems outputs and 
?labeled as? refer to human judgments. The 
subscript i ranges over all the query classes. 
Table 6 shows the scores of each of the three 
human labelers when each of them is evaluated 
against the other two. It can be seen that the 
consistency among the labelers is quite low, 
indicating that the query classification task is 
very difficult even for humans.  
To maximize the little information we have 
about the query classes, we treat the words in 
query class names as additional example queries. 
For example, we added three queries: living, 
tools, and hardware to the class Living\Tools & 
Hardware. 
4.2 Baseline classifier 
Since the query classes are not mutually 
exclusive, we treat the query classification task 
as 67 binary classification problems. For each 
query class, we train a logistic regression 
classifier (Vapnik 1999) with L2 regularization. 
Table 4 CoNLL NER test set results 
System Test F1  Improv. 
Baseline CRF (Sec. 3.1) 83.78  
W500 88.34 +4.56 
P64 89.73 +5.94 
P125 89.80 +6.02 
W500 + P125 90.62 +6.84 
W500 + P64 90.63 +6.85 
W500 + P125 + P64 90.90 +7.12 
W500 + P125 + P64+pos 90.62 +6.84 
LDC64 87.24 +3.46 
LDC125 88.33 +4.55 
LDC64 +LDC125 88.44 +4.66 
(Suzuki and Isozaki, 2008) 89.92  
(Ando and Zhang, 2005) 89.31  
(Florian et al, 2003) 88.76  
(Chieu and Ng, 2003) 88.31  
(Klein et al, 2003) 86.31  
Table 5 Example queries and their classes 
ford field 
   Sports/American Football 
   Information/Local & Regional 
   Sports/Schedules & Tickets 
john deere gator 
   Living/Landscaping & Gardening 
   Living/Tools & Hardware 
   Information/Companies & Industries 
   Shopping/Stores & Products 
   Shopping/Buying Guides & Researching 
justin timberlake lyrics 
   Entertainment/Music 
   Information/Arts & Humanities 
   Entertainment/Celebrities 
Table 6 Labeler Consistency 
 
L1  L2 L3 Average
F1 0.538 0.477 0.512 0.509
P 0.501 0.613 0.463 0.526
1035
Given an input x, represented as a vector of m 
features: (x1, x2, ....., xm), a logistic regression 
classifier with parameter vector ? L(w1, w2, ....., 
wm) computes the posterior probability of the 
output y, which is either 1 or -1, as 
L:U?; L s
sE A????	? 
We tag a query as belonging to a class if the 
probability of the class is among the highest 5 
and is greater than 0.5. 
The baseline system uses only the words in the 
queries as features (the bag-of-words 
representation), treating the query classification 
problem as a typical text categorization problem. 
We found the prior distribution of the query 
classes to be extremely important. In fact, a 
system that always returns the top-5 most 
frequent classes has an F1 score of 26.55, which 
would have outperformed 2/3 of the 37 systems 
in the KDDCUP and ranked 13th. 
We made a small modification to the objective 
function for logistic regression to take into 
account the prior distribution and to use 50% as a 
uniform decision boundary for all the classes. 
Normally, training a logistic regression classifier 
amounts to solving: 
???IEJ? ]???? E sJ? ???@sE A?????	??A
?
?@5
a 
where n is the number of training examples and ?  
is the regularization constant. In this formula, 1/n 
can be viewed as the weight of an example in the 
training corpus. When training the classifier for a 
class with p positive examples out of a total of n 
examples, we change the objective function to: 
???IEJ? P????E ? ???@sE A?????	??A??@5J E U?:tL F J; Q 
With this modification, the total weight of the 
positive and negative examples become equal. 
4.3 Phrasal clusters in query classification 
Since topical information is much more relevant 
to query classification than categorical 
information, we use clusters created with 3-word 
context windows. Moreover, we use soft 
clustering instead of hard clustering. A phrase 
belongs to a cluster if the cluster?s centroid is 
among the top-50 most similar centroids to the 
phrase (by cosine similarity), and the similarity is 
greater than 0.04.  
Given a query, we first retrieve all its phrases 
(allowing overlap) and the clusters they belong 
to. For each of these clusters, we sum the 
cluster?s similarity to all the phrases in the query 
and select the top-N as features for the logistic 
regression classifier (N=150 in our experiments). 
When we extract features from multiple 
clusterings, the selection of the top-N clusters is 
done separately for each clustering. Once a 
cluster is selected, its similarity values are 
ignored. Using the numerical feature values in 
our experiments always led to worse results. We 
suspect that such features make the optimization 
of the objective function much more difficult. 
 
Figure 2 Comparison with KDDCUP systems 
4.4 Evaluation results 
Table 7 contains the evaluation results of various 
configurations of our system. Here, bow 
indicates the use of bag-of-words features; WN 
refers to word clusters of size N; and PN refers to 
phrase clusters of size N. All the clusters are soft 
clusters created with the web corpus using 3-
word context windows. 
The bag-of-words features alone have dismal 
performance. This is obviously due to the 
extreme paucity of training examples. In fact, 
only 12% of the words in the 800 test queries are 
found in the training examples. Using word 
clusters as features resulted in a big increase in 
F-score. The phrasal cluster features offer 
another big improvement. The best result is 
achieved with multiple phrasal clusterings.  
Figure 2 compares the performance of our 
system (the dark bar at 2) with the top tercile 
systems in KDDCUP 2005. The best two 
systems in the competition (Shen et al, 2005) 
and (Vogel et al, 2005) resorted to knowledge 
engineering techniques to bridge the gap between 
0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8 9 10 11 12 13
Table 7 Query Classification results 
System F1  
bow 11.58 
bow+W3K 34.71 
bow+P500 39.84 
bow+P3K 40.80 
bow+P500+P1K +P2K +P3K+P5K 43.80 
1036
the small set of examples and the new queries. 
They manually constructed a mapping from the 
query classes to hierarchical directories such as 
Google Directory3 or Open Directory Project4. 
They then sent training and testing queries to 
internet search engines to retrieve the top pages 
in these directories. The positions of the result 
pages in the directory hierarchies as well as the 
words in the pages are used to classify the 
queries. With phrasal clusters, we can achieve 
top-level performance without manually 
constructed resources, or having to rely on 
internet search results.  
5 Discussion and Related Work  
In earlier work on semi-supervised learning, e.g., 
(Blum and Mitchell 1998), the classifiers learned 
from unlabeled data were used directly. Recent 
research shows that it is better to use whatever is 
learned from the unlabeled data as features in a 
discriminative classifier. This approach is taken 
by (Miller et. al. 2004), (Wong and Ng 2007), 
(Suzuki and Isozaki 2008), and (Koo et. al., 
2008), as well as this paper.  
Wong and Ng (2007) and Suzuki and Isozaki 
(2008) are similar in that they run a baseline 
discriminative classifier on unlabeled data to 
generate pseudo examples, which are then used  
to train a different type of classifier for the same 
problem. Wong and Ng (2007) made the 
assumption that each proper named belongs to 
one class (they observed that this is true about 
85% of the time for English). Suzuki and Isozaki 
(2008), on the other hand, used the automatically 
labeled corpus to train HMMs. 
Ando and Zhang (2005) defined an objective 
function that combines the original problem on 
the labeled data with a set of auxiliary problems 
on unlabeled data. The definition of an auxiliary 
problem can be quite flexible as long as it can be 
automatically labeled and shares some structural 
properties with the original problem. The 
combined objective function is then alternatingly 
optimized with the labeled and unlabeled data. 
This training regime puts pressure on the 
discriminative learner to exploit the structures 
uncovered from the unlabeled data. 
In the two-stage cluster-based approaches such 
as ours, clustering is mostly decoupled from the 
supervised learning problem. However, one can 
rely on a discriminative classifier to establish the 
connection by assigning proper weights to the 
                                                          
3
 http://directory.google.com 
4
 http://www.dmoz.org 
cluster features. One advantage of the two-stage 
approach is that the same clusterings may be 
used for different problems or different 
components of the same system. Another 
advantage is that it can be applied to a wider 
range of domains and problems. Although the 
method in (Suzuki and Isozaki 2008) is quite 
general, it is hard to see how it can be applied to 
the query classification problem. 
Compared with Brown clustering, our 
algorithm for distributional clustering with 
distributed K-Means offers several benefits: (1) it 
is more scalable and parallelizable; (2) it has the 
ability to generate topical as well as categorical 
clusters for use in different applications; (3) it 
can create soft clustering as well as hard ones. 
There are two main scenarios that motivate 
semi-supervised learning. One is to leverage a 
large amount of unsupervised data to train an 
adequate classifier with a small amount of 
labeled data. Another is to further boost the 
performance of a supervised classifier that is 
already trained with a large amount of supervised 
data. The named entity problem in Section 3 and 
the query classification problem in Section 4 
exemplify the two scenarios. 
One nagging issue with K-Means clustering is 
how to set k. We show that this question may not 
need to be answered because we can use 
clusterings with different k?s at the same time 
and let the discriminative classifier cherry-pick 
the clusters at different granularities according to 
the supervised data. This technique has also been 
used with Brown clustering (Miller et. al. 2004, 
Koo, et. al. 2008). However, they require clusters 
to be strictly hierarchical, whereas we do not. 
6 Conclusions 
We presented a simple and scalable algorithm to 
cluster tens of millions of phrases and we used 
the resulting clusters as features in discriminative 
classifiers. We demonstrated the power and 
generality of this approach on two very different 
applications: named entity recognition and query 
classification. Our system achieved the best 
current result on the CoNLL NER data set. Our 
query categorization system is on par with the 
best system in KDDCUP 2005, which, unlike 
ours, involved a great deal of knowledge 
engineering effort. 
 
Acknowledgments 
The authors wish to thank the anonymous 
reviewers for their comments. 
1037
References  
R. Ando and T. Zhang A Framework for Learning 
Predictive Structures from Multiple Tasks and 
Unlabeled Data. Journal of Machine Learning 
Research, Vol 6:1817-1853, 2005. 
B.H. Bloom. 1970, Space/time trade-offs in hash 
coding with allowable errors, Communications of 
the ACM 13 (7): 422?426 
A. Blum and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. Proceedings of 
the Eleventh Annual Conference on Computational 
Learning Theory pp. 92?100. 
P.F. Brown, V.J. Della Pietra, P.V. de Souza, J.C. Lai, 
and R.L. Mercer. 1992. Class-based n-gram models 
of natural language. Computational Linguistics, 
18(4):467?479.  
H. L. Chieu and H. T. Ng. Named entity recognition 
with a maximum entropy approach. In Proceedings 
CoNLL-2003, pages 160?163, 2003. 
J. Dean and S. Ghemawat. 2004. MapReduce: 
Simplified data processing on large clusters. In 
Proceedings of the Sixth Symposium on Operating 
System Design and Implementation (OSDI-04), 
San Francisco, CA, USA 
S Deerwester, S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. A. Harshman. 1990. Indexing by 
latent semantic analysis, Journal of the American 
Society for Information Science, 1990, 41(6), 391-
407 
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 
Named entity recognition through classifier 
combination. In Proceedings CoNLL-2003, pages 
168?171, 2003. 
D. Klein, J. Smarr, H. Nguyen, and C. D. Manning. 
Named entity recognition with character-level 
models. In Proceedings CoNLL-2003, pages 188?
191, 2003. 
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical 
phrase-based translation. In Proceedings of HLT-
NAACL 2003, pp. 127?133. 
T. Koo, X. Carreras, and M. Collins. Simple Semi-
supervised Dependency Parsing. Proceedings of 
ACL, 2008. 
J. Lafferty, A. McCallum, F. Pereira. Conditional 
random fields: Probabilistic models for segmenting 
and labeling sequence data. In: Proc. 18th 
International Conf. on Machine Learning, Morgan 
Kaufmann, San Francisco, CA (2001) 282?289 
Y. Li, Z. Zheng, and H.K. Dai, KDD Cup-2005 
Report: Facing a Great Challenge. SIGKDD 
Explorations, 7 (2), 2005, 91-99. 
D. Lin, S. Zhao, and B. Van Durme, and M. Pasca. 
2008. Mining Parenthetical Translations from the 
Web by Word Alignment. Proc. of ACL-08. 
Columbus, OH. 
J.  Lin. Scalable Language Processing Algorithms for 
the Masses: A Case Study in Computing Word Co-
occurrence Matrices with MapReduce. Proceedings 
of  EMNLP 2008, pp. 419-428, Honolulu, Hawaii. 
J. B. MacQueen (1967): Some Methods for 
classification and Analysis of Multivariate 
Observations, Proc. of 5-th Berkeley Symposium 
on Mathematical Statistics and Probability", 
Berkeley, University of California Press, 1:281-
297 
S. Miller, J. Guinness, and A. Zamanian. 2004. Name 
Tagging with Word Clusters and Discriminative 
Training. In Proceedings of HLT-NAACL, pages 
337?342. 
M. Sahami and T.D. Heilman. 2006. A web-based 
kernel function for measuring the similarity of 
short text snippets. Proceedings of the 15th 
international conference on World Wide Web, pp. 
377?386. 
D. Shen, R. Pan, J.T. Sun, J.J. Pan, K. Wu, J. Yin, Q. 
Yang. Q2C@UST: our winning solution to query 
classification in KDDCUP 2005. SIGKDD 
Explorations, 2005: 100~110. 
J. Suzuki, and H. Isozaki. 2008. Semi-Supervised 
Sequential Labeling and Segmentation using Giga-
word Scale Unlabeled Data. In Proc. of ACL/HLT-
08. Columbus, Ohio. pp. 665-673. 
E. T. Tjong Kim Sang and F. De Meulder. 2003. 
Introduction to the CoNLL-2003 Shared Task: 
Language-Independent Named Entity Recognition. 
In Proc. of CoNLL-2003, pages 142?147. 
Y. Wong and H. T. Ng, 2007. One Class per Named 
Entity: Exploiting Unlabeled Text for Named 
Entity Recognition. In Proc. of IJCAI-07,  
Hyderabad, India. 
J. Uszkoreit and T. Brants. 2008. Distributed Word 
Clustering for Large Scale Class-Based Language 
Modeling in Machine Translation. Proceedings of 
ACL-08: HLT, pp. 755-762. 
V. Vapnik, 1999. The Nature of Statistical Learning 
Theory, 2nd edition. Springer Verlag. 
D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. 
Siemen, S. Bridges, T. Scheffer. Classifying 
Search Engine Queries Using the Web as 
Background Knowledge. SIGKDD Explorations 
7(2): 117-122. 2005. 
 
1038
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 152?159,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Strictly Lexical Dependency Parsing 
 
 
Qin Iris Wang and Dale Schuurmans Dekang Lin 
Department of Computing Science Google, Inc. 
University of Alberta 1600 Amphitheatre Parkway 
Edmonton, Alberta, Canada, T6G 2E8 Mountain View, California, USA, 94043 
{wqin,dale}@cs.ualberta.ca lindek@google.com 
 
 
 
 
Abstract 
We present a strictly lexical parsing 
model where all the parameters are based 
on the words. This model does not rely 
on part-of-speech tags or grammatical 
categories. It maximizes the conditional 
probability of the parse tree given the 
sentence. This is in contrast with most 
previous models that compute the joint 
probability of the parse tree and the sen-
tence. Although the maximization of 
joint and conditional probabilities are 
theoretically equivalent, the conditional 
model allows us to use distributional 
word similarity to generalize the ob-
served frequency counts in the training 
corpus. Our experiments with the Chi-
nese Treebank show that the accuracy of 
the conditional model is 13.6% higher 
than the joint model and that the strictly 
lexicalized conditional model outper-
forms the corresponding unlexicalized 
model based on part-of-speech tags. 
1 Introduction 
There has been a great deal of progress in statisti-
cal parsing in the past decade (Collins, 1996; 
Collins, 1997; Chaniak, 2000). A common charac-
teristic of these parsers is their use of lexicalized 
statistics. However, it was discovered recently that 
bi-lexical statistics (parameters that involve two 
words) actually played much smaller role than 
previously believed.  It was found in (Gildea, 
2001) that the removal of bi-lexical statistics from 
a state-of-the-art PCFG parser resulted very small 
change in the output. Bikel (2004) observed that 
the bi-lexical statistics accounted for only 1.49% 
of the bigram statistics used by the parser. When 
considering only bigram statistics involved in the 
highest probability parse, this percentage becomes 
28.8%. However, even when the bi-lexical statis-
tics do get used, they are remarkably similar to 
their back-off values using part-of-speech tags. 
Therefore, the utility of bi-lexical statistics be-
comes rather questionable. Klein and Manning 
(2003) presented an unlexicalized parser that 
eliminated all lexicalized parameters. Its perform-
ance was close to the state-of-the-art lexicalized 
parsers. 
We present a statistical dependency parser that 
represents the other end of spectrum where all 
statistical parameters are lexical and the parser 
does not require part-of-speech tags or grammati-
cal categories. We call this strictly lexicalized 
parsing. 
A part-of-speech lexicon has always been con-
sidered to be a necessary component in any natu-
ral language parser. This is true in early rule-based 
as well as modern statistical parsers and in de-
pendency parsers as well as constituency parsers. 
The need for part-of-speech tags arises from the 
sparseness of natural language data. They provide 
generalizations of words that are critical for pars-
ers to deal with the sparseness. Words belonging 
to the same part-of-speech are expected to have 
the same syntactic behavior. 
Instead of part-of-speech tags, we rely on dis-
tributional word similarities computed automati-
cally from a large unannotated text corpus. One of 
the benefits of strictly lexicalized parsing is that 
152
 fundsinvestors continue  to  pour cash into moneyMany? 
0 1 2 3 4 5 6 7 8 9
 
 
 
 
the parser can be trained with a treebank that only 
contains the dependency relationships between 
words. The annotators do not need to annotate 
parts-of-speech or non-terminal symbols (they 
don?t even have to know about them), making the 
construction of the treebank easier.  
Strictly lexicalized parsing is especially benefi-
cial for languages such as Chinese, where parts-
of-speech are not as clearly defined as English. In 
Chinese, clear indicators of a word's part-of-
speech such as suffixes -ment, -ous or function 
words such as the, are largely absent. In fact, 
monolingual Chinese dictionaries that are mainly 
intended for native speakers almost never contain 
part-of-speech information. 
In the next section, we present a method for 
modeling the probabilities of dependency trees. 
Section 3 applies similarity-based smoothing to 
the probability model to deal with data sparseness. 
We then present experimental results with the 
Chinese Treebank in Section 4 and discuss related 
work in Section 5.  
2 A Probabilistic Dependency Model 
Let S be a sentence. The dependency structure T 
of S is a directed tree connecting the words in S. 
Each link in the tree represents a dependency rela-
tionship between two words, known as the head 
and the modifier. The direction of the link is from 
the head to the modifier. We add an artificial root 
node (?) at the beginning of each sentence and a 
dependency link from ? to the head of the sen-
tence so that the head of the sentence can be 
treated in the same way as other words. Figure 1 
shows an example dependency tree. 
We denote a dependency link l by a triple (u, v, 
d), where u and v are the indices (u < v) of the 
words connected by l, and d specifies the direction 
of the link l. The value of d is either L or R. If d = 
L, v is the index of the head word; otherwise, u is 
the index of the head word.  
Dependency trees are typically assumed to be 
projective (without crossing arcs), which means 
that if there is an arc from h to m, h is an ancestor 
of all the words between h and m. Let F(S) be the 
set of possible directed, projective trees spanning 
on S. The parsing problem is to find  
 
( ) ( )STPSFT |maxarg ?  
 
 Generative parsing models are usually defined 
recursively from top down, even though the de-
coders (parsers) for such models almost always 
take a bottom-up approach. The model proposed 
here is a bottom-up one. Like previous ap-
proaches, we decompose the generation of a parse 
tree into a sequence of steps and define the prob-
ability of each step.  The probability of the tree is 
simply the product of the probabilities of the steps 
involved in the generation process. This scheme 
requires that different sequences of steps must not 
lead to the same tree. We achieve this by defining 
a canonical ordering of the links in a dependency 
tree. Each generation step corresponds to the con-
struction of a dependency link in the canonical 
order. 
Given two dependency links l and l' with the 
heads being h and h' and the modifiers being m 
and m', respectively, the order between l and l' are 
determined as follows: 
? If h ? h' and there is a directed path from one 
(say h) to the other (say h?), then l? precedes l. 
? If h ? h' and there does not exist a directed path 
between h and h?, the order between l and l? is 
determined by the order of h and h? in the sen-
tence (h precedes h? ? l precedes l?). 
? If h = h' and the modifiers m and m? are on dif-
ferent sides of h, the link with modifier on the 
right precedes the other. 
? If h = h' and the modifiers m and m? are on the 
same side of the head h, the link with its modi-
fier closer to h precedes the other one. 
 
Figure 1. An Example Dependency Tree. 
153
For example, the canonical order of the links in 
the dependency tree in Figure 1 is: (1, 2, L), (5, 6, 
R), (8, 9, L), (7, 9, R), (5, 7, R), (4, 5, R), (3, 4, 
R), (2, 3, L), (0, 3, L). 
The generation process according to the ca-
nonical order is similar to the head outward gen-
eration process in (Collins, 1999), except that it is 
bottom-up whereas Collins? models are top-down. 
Suppose the dependency tree T is constructed in 
steps G1, ?, GN in the canonical order of the de-
pendency links, where N is the number of words 
in the sentence. We can compute the probability 
of T as follows: 
( )
( )
( )? = ?=
=
N
i ii
N
GGSGP
SGGGP
STP
1 11
21
,...,,|
|,...,,
|
 
Following (Klein and Manning, 2004), we re-
quire that the creation of a dependency link from 
head h to modifier m be preceded by placing a left 
STOP and a right STOP around the modifier m 
and ?STOP between h and m. 
Let LwE  (and 
R
wE ) denote the event that there 
are no more modifiers on the left (and right) of a 
word w. Suppose the dependency link created in 
the step i is (u, v, d).  If d = L, Gi is the conjunc-
tion of the four events: RuE , 
L
uE , 
L
vE? and 
linkL(u, v). If d = R, Gi consists of four events: 
L
vE ,
R
vE , 
R
uE? and linkR(u, v).  
The event Gi is conditioned on 11,...,, ?iGGS , 
which are the words in the sentence and a forest of 
trees constructed up to step i-1. Let LwC  (and 
R
wC ) 
be the number of modifiers of w on its left (and 
right). We make the following independence as-
sumptions: 
? Whether there is any more modifier of w on 
the d side depends only on the number of 
modifiers already found on the d side of w. 
That is, dwE  depends only on w and 
d
wC .  
? Whether there is a dependency link from a 
word h to another word m depends only on the 
words h and m and the number of modifiers of 
h between m and h. That is,  
o linkR(u,v) depends only on u, v, and RuC . 
o linkL(u,v) depends only on u, v, and LvC . 
Suppose Gi corresponds to a dependency link (u, 
v, L). The probability ( )11,...,,| ?ii GGSGP  can be 
computed as: 
 
 
( )
( )( )
( ) ( )
( )( ) ( )( )LvLLvLv
R
u
R
u
L
u
L
u
iL
L
v
R
u
L
u
ii
CvuvulinkPCvEP
CuEPCuEP
GGSvulinkEEEP
GGSGP
,,|,,|1
,|,|
,...,,|,,,,
,...,,|
11
11
??
??=
?= ?
?
 
 
The events RwE  and 
L
wE  correspond to the 
STOP events in (Collins, 1999) and (Klein and 
Manning, 2004). They are crucial for modeling 
the number of dependents. Without them, the 
parse trees often contain some ?obvious? errors, 
such as determiners taking arguments, or preposi-
tions having arguments on their left (instead of 
right). 
Our model requires three types of parameters: 
? ( )dwdw CwEP ,| , where w is a word, d is a di-
rection (left or right). This is the probability of 
a STOP after taking dwC  modifiers on the d 
side. 
? ( )( )RuR CvuvulinkP ,,|,  is the probability of v 
being the ( 1+RuC )?th modifier of u on the 
right. 
? ( )( )LvL CvuvulinkP ,,|,  is the probability of u 
being the ( 1+LvC )?th modifier of v on the 
left. 
 
The Maximum Likelihood estimations of these 
parameters can be obtained from the frequency 
counts in the training corpus: 
? C(w, c, d): the frequency count of  w with c 
modifiers on the d side. 
? C(u, v, c, d): If d = L, this is the frequency 
count words u and v co-occurring in a sen-
tence and v has c modifiers between itself and 
u. If d = R, this is the frequency count words u 
and v co-occurring in a sentence and u has c 
modifiers between itself and v. 
? K(u, v, c, d): similar to C(u, v, c, d) with an 
additional constraint that linkd(u, v) is true. 
154
( ) ( )( )?
?
=
cc
d
w
d
w dcwC
dcwCCwEP
'
,',
,,,|  , where c = dwC ; 
( )( ) ( )( )RcvuC RcvuKCvuvulinkP RuR ,,, ,,,,,|, = ,   
where  c = RuC ; 
( )( ) ( )( )LcvuC LcvuKCvuvulinkP LvL ,,, ,,,,,|, = ,   
where  c = LvC . 
We compute the probability of the tree condi-
tioned on the words. All parameters in our model 
are conditional probabilities where the left sides of 
the conditioning bar are binary variables. In con-
trast, most previous approaches compute joint 
probability of the tree and the words in the tree. 
Many of their model parameters consist of the 
probability of a word in a given context. 
We use a dynamic programming algorithm 
similar to chart parsing as the decoder for this 
model. The algorithm builds a packed parse forest 
from bottom up in the canonical order of the 
parser trees. It attaches all the right children be-
fore attaching the left ones to maintain the canoni-
cal order as required by our model.  
3 Similarity-based Smoothing 
3.1 Distributional Word Similarity 
Words that tend to appear in the same contexts 
tend to have similar meanings. This is known as 
the Distributional Hypothesis in linguistics (Harris, 
1968). For example, the words test and exam are 
similar because both of them follow verbs such as 
administer, cancel, cheat on, conduct, ... and both of 
them can be preceded by adjectives such as aca-
demic, comprehensive, diagnostic, difficult, ... 
Many methods have been proposed to compute 
distributional similarity between words (Hindle, 
1990; Pereira et al, 1993; Grefenstette, 1994; Lin, 
1998). Almost all of the methods represent a word 
by a feature vector where each feature corre-
sponds to a type of context in which the word ap-
peared. They differ in how the feature vectors are 
constructed and how the similarity between two 
feature vectors is computed.   
We define the features of a word w to be the set 
of words that occurred within a small context win-
dow of w in a large corpus. The context window 
of an instance of w consists of the closest non-
stop-word on each side of w and the stop-words in 
between. In our experiments, the set of stop-words 
are defined as the top 100 most frequent words in 
the corpus. The value of a feature w' is defined as 
the point-wise mutual information between the w' 
and w: 
( ) ( )( ) ( )???
????
??=
'
',log',
wPwP
wwPwwPMI  
where P(w, w?) is the probability of w and w? co-
occur in a context window. 
The similarity between two vectors is computed 
as the cosine of the angle between the vectors. 
The following are the top similar words for the 
word keystone obtained from the English Giga-
word Corpus: 
centrepiece 0.28, figment 0.27, fulcrum 0.21, culmi-
nation 0.20, albatross 0.19, bane 0.19, pariahs 0.18, 
lifeblood 0.18, crux 0.18, redoubling 0.17, apotheo-
sis 0.17, cornerstones 0.17, perpetuation 0.16, fore-
runners 0.16, shirking 0.16, cornerstone 0.16, 
birthright 0.15, hallmark 0.15, centerpiece 0.15, evi-
denced 0.15, germane 0.15, gist 0.14, reassessing 
0.14, engrossed 0.14, Thorn 0.14, biding 0.14, nar-
rowness 0.14, linchpin 0.14, enamored 0.14, formal-
ised 0.14, tenths 0.13, testament 0.13, certainties 
0.13, forerunner 0.13, re-evaluating 0.13, antithetical 
0.12, extinct 0.12, rarest 0.12, imperiled 0.12, remiss 
0.12, hindrance 0.12, detriment 0.12, prouder 0.12, 
upshot 0.12, cosponsor 0.12, hiccups 0.12, premised 
0.12, perversion 0.12, destabilisation 0.12, prefaced 
0.11, ?? 
3.2 Similarity-based Smoothing 
The parameters in our model consist of condi-
tional probabilities P(E|C) where E is the binary 
variable linkd(u, v) or dwE  and the context C is 
either [ ]dwCw,  or [ ]dwCvu ,, , which involves one 
or two words in the input sentence. Due to the 
sparseness of natural language data, the contexts 
observed in the training data only covers a tiny 
fraction of the contexts whose probability distri-
bution are needed during parsing. The standard 
approach is to back off the probability to word 
classes (such as part-of-speech tags). We have 
taken a different approach. We search in the train-
155
ing data to find a set of similar contexts to C and 
estimate the probability of E based on its prob-
abilities in the similar contexts that are observed 
in the training corpus. 
Similarity-based smoothing was used in (Dagan 
et al, 1999) to estimate word co-occurrence prob-
abilities. Their method performed almost 40% 
better than the more commonly used back-off 
method. Unfortunately, similarity-based smooth-
ing has not been successfully applied to statistical 
parsing up to now.  
In (Dagan et al, 1999), the bigram probability 
P(w2|w1) is computed as the weighted average of 
the conditional probability of w2 given similar 
words of w1. 
( ) ( )( ) ( )( )??= 11' 121 1112 '|
',|
wSw
MLESIM wwPwnorm
wwsimwwP  
where ( )11 ', wwsim  denotes the similarity (or an 
increasing function of the similarity) between w1 
and w?1, S(w1) denote the set of words that are 
most similar to w1 and norm(w1) is the normaliza-
tion factor ( ) ( )
( )??= 11' 111 ',wSw wwsimwnorm .   
The underlying assumption of this smoothing 
scheme is that a word is more likely to occur after 
w1 if it tends to occur after similar words of w1.  
We make a similar assumption: the probability 
P(E|C) of event E given the context C is computed 
as the weight average of P(E|C?) where C? is a 
similar context of C and is attested in the training 
corpus:  
( ) ( )( ) ( )( )???= OCSC MLESIM CEPCnorm
CCsimCEP
'
'|',|  
where S(C) is the set of top-K most similar con-
texts of C (in the experiments reported in this pa-
per, K = 50); O is the set of contexts observed in 
the training corpus, sim(C,C?) is the similarity 
between two contexts  and  norm(C) is the nor-
malization factor.  
In our model, a context is either  [ ]dwCw,  or [ ]dwCvu ,, . Their similar contexts are defined as:  
[ ]( ) [ ] ( ){ }
[ ]( ) [ ]{ })('),(',',',,
',', '
vSvuSuCvuCvuS
wSwCwCwS
d
w
d
w
d
w
d
w
??=
?=
 
where S(w) is the set of top-K similar words of w 
(K = 50). 
Since all contexts used in our model contain at 
least one word, we compute the similarity be-
tween two contexts, sim(C, C?), as the geometric 
average of the similarities between corresponding 
words: 
[ ] [ ]( ) ( )
[ ] [ ]( ) ( ) ( )',',,',',,,
',,',,
'
'
vvsimuusimCvuCvusim
wwsimCwCwsim
d
w
d
w
d
w
d
w
?=
=
 
Similarity-smoothed probability is only neces-
sary when the frequency count of the context C in 
the training corpus is low. We therefore compute  
P(E | C) = ? PMLE(E | C) + (1 ? ?) PSIM(E | C) 
where the smoothing factor 
5||
1||
+
+=
C
C?  and |C| is 
the frequency count of the context C in the train-
ing data. 
A difference between similarity-based smooth-
ing in (Dagan et al, 1999) and our approach is 
that our model only computes probability distribu-
tions of binary variables. Words only appear as 
parts of contexts on the right side of the condition-
ing bar. This has two important implications. 
Firstly, when a context contains two words, we 
are able to use the cross product of the similar 
words, whereas (Dagan et al, 1999) can only use 
the similar words of one of the words. This turns 
out to have significant impact on the performance 
(see Section 4).  
Secondly, in (Dagan et al, 1999), the distribu-
tion P(?|w?1) may itself be sparsely observed. 
When ( )12 '| wwPMLE  is 0, it is often due to data 
sparseness. Their smoothing scheme therefore 
tends to under-estimate the probability values. 
This problem is avoided in our approach. If a con-
text did not occur in the training data, we do not 
include it in the average. If it did occur, the 
Maximum Likelihood estimation is reasonably 
accurate even if the context only occurred a few 
times, since the entropy of the probability distri-
bution is upper-bounded by log 2. 
4 Experimental Results 
We experimented with our parser on the Chinese 
Treebank (CTB) 3.0.  We used the same data split 
as (Bikel, 2004): Sections 1-270 and 400-931 as 
156
the training set, Sections 271-300 as testing and 
Sections 301-325 as the development set. The 
CTB contains constituency trees. We converted 
them to dependency trees using the same method 
and the head table as (Bikel, 2004).  Parsing Chi-
nese generally involve segmentation as a pre-
processing step. We used the gold standard seg-
mentation in the CTB.  
The distributional similarities between the Chi-
nese words are computed using the Chinese Gi-
gaword corpus. We did not segment the Chinese 
corpus when computing the word similarity.  
We measure the quality of the parser by the un-
directed accuracy, which is defined as the number 
of correct undirected dependency links divided by 
the total number of dependency links in the corpus 
(the treebank parse and the parser output always 
have the same number of links). The results are 
summarized in Table 1. It can be seen that the per-
formance of the parser is highly correlated with 
the length of the sentences. 
 
Max Sentence Length 10 15 20 40 
Undirected Accuracy 90.8 85.6 84.0 79.9 
Table 1. Evaluation Results on CTB 3.0 
 
We also experimented with several alternative 
models for dependency parsing. Table 2 summer-
izes the results of these models on the test corpus 
with sentences up to 40 words long. 
One of the characteristics of our parser is that it 
uses the similar words of both the head and the 
modifier for smoothing. The similarity-based 
smoothing method in (Dagan et al, 1999) uses the 
similar words of one of the words in a bigram. We 
can change the definition of similar context as 
follows so that only one word in a similar context 
of C may be different from a word in C (see 
Model (b) in Table 2): 
[ ]( )
[ ]{ } [ ]{ })(',',)(',,' ,, vSvCvuuSuCvu CvuS dwdw
d
w
???=  
where w is either v or u depending on whether d is 
L or R. This change led to a 2.2% drop in accuracy 
(compared with Model (a) in Table 2), which we 
attribute to the fact that many contexts do not have 
similar contexts in the training corpus.  
Since most previous parsing models maximize 
the joint probability of the parse tree and the sen-
tence P(T, S) instead of P(T | S),  we also imple-
mented a joint model (see Model (c) in Table 2): 
( ) ( ) ( )( )( ) ( )?= ??
??=
N
i
d
hii
d
hi
d
h
R
mi
R
m
L
mi
L
m
i
iii
iiii
ChmPChEP
CmEPCmEP
STP
1 ,|,|1
,|,|
,  
where hi and mi are the head and the modifier of 
the i'th dependency link. The probability ( )i
i
d
hii ChmP ,|  is smoothed by averaging the 
probabilities ( )i
i
d
hii ChmP ,'| , where h?i is a similar 
word of hi, as in (Dagan et al, 1999). The result 
was a dramatic decrease in accuracy from the con-
ditional model?s 79.9%. to 66.3%.  
 Our use of distributional word similarity can 
be viewed as assigning soft clusters to words. In 
contrast, parts-of-speech can be viewed as hard 
clusters of words. We can modify both the condi-
tional and joint models to use part-of-speech tags, 
instead of words. Since there are only a small 
number of tags, the modified models used MLE  
without any smoothing except using a small con-
stant as the probability of unseen events. Without 
smoothing, maximizing the conditional model is 
equivalent to maximizing the joint model. The 
accuracy of the unlexicalized models (see Model 
(d) and Model (e) in Table 2) is 71.1% which is 
considerably lower than the strictly lexicalized 
conditional model, but higher than the strictly 
lexicalized joint model. This demonstrated that 
soft clusters obtained through distributional word 
similarity perform better than the part-of-speech 
tags when used appropriately. 
 
Models Accuracy 
(a) Strictly lexicalized conditional model 79.9 
(b)   At most one word is different in a similar context 77.7 
(c)  Strictly lexicalized  joint model 66.3 
(d)  Unlexicalized conditional mod-els 71.1 
(e)  Unlexicalized joint models 71.1 
Table 2. Performance of Alternative Models 
 
157
5 Related Work  
Previous parsing models (e.g., Collins, 1997; 
Charniak, 2000) maximize the joint probability 
P(S, T) of a sentence S and its parse tree T. We 
maximize the conditional probability P(T | S). Al-
though they are theoretically equivalent, the use of 
conditional model allows us to take advantage of 
similarity-based smoothing. 
Clark et al (2002) also computes a conditional 
probability of dependency structures. While the 
probability space in our model consists of all pos-
sible non-projective dependency trees, their prob-
ability space is constrained to all the dependency 
structures that are allowed by a Combinatorial 
Category Grammar (CCG) and a category diction-
ary (lexicon). They therefore do not need the 
STOP markers in their model. Another major dif-
ference between our model and (Clark et al, 
2002) is that the parameters in our model consist 
exclusively of conditional probabilities of binary 
variables. 
Ratnaparkhi?s maximum entropy model (Rat-
naparkhi, 1999) is also a conditional model. How-
ever, his model maximizes the probability of the 
action during each step of the parsing process, 
instead of overall quality of the parse tree.  
Yamada and Matsumoto (2002) presented a de-
pendency parsing model using support vector ma-
chines. Their model is a discriminative model that 
maximizes the differences between scores of the 
correct parse and the scores of the top competing 
incorrect parses.  
In many dependency parsing models such as 
(Eisner, 1996) and (MacDonald et al, 2005), the 
score of a dependency tree is the sum of the scores 
of the dependency links, which are computed in-
dependently of other links. An undesirable conse-
quence of this is that the parser often creates 
multiple dependency links that are separately 
likely but jointly improbable (or even impossible). 
For example, there is nothing in such models to 
prevent the parser from assigning two subjects to 
a verb. In the DMV model (Klein and Manning, 
2004), the probability of a dependency link is 
partly conditioned on whether or not there is a 
head word of the link already has a modifier. Our 
model is quite similar to the DMV model, except 
that we compute the conditional probability of the 
parse tree given the sentence, instead of the joint 
probability of the parse tree and the sentence. 
There have been several previous approaches to 
parsing Chinese with the Penn Chinese Treebank 
(e.g., Bikel and Chiang, 2000; Levy and Manning, 
2003). Both of these approaches employed phrase-
structure joint models and used part-of-speech 
tags in back-off smoothing. Their results were 
evaluated with the precision and recall of the 
bracketings implied in the phrase structure parse 
trees. In contrast, the accuracy of our model is 
measured in terms of the dependency relation-
ships. A dependency tree may correspond to more 
than one constituency trees.  Our results are there-
fore not directly comparable with the precision 
and recall values in previous research. Moreover, 
it was argued in (Lin 1995) that dependency based 
evaluation is much more meaningful for the appli-
cations that use parse trees, since the semantic 
relationships are generally embedded in the de-
pendency relationships. 
6 Conclusion 
To the best of our knowledge, all previous natural 
language parsers have to rely on part-of-speech 
tags. We presented a strictly lexicalized model for 
dependency parsing that only relies on word sta-
tistics. We compared our parser with an unlexical-
ized parser that employs the same probabilistic 
model except that the parameters are estimated 
using gold standard tags in the Chinese Treebank. 
Our experiments show that the strictly lexicalized 
parser significantly outperformed its unlexicalized 
counter-part. 
An important distinction between our statistical 
model from previous parsing models is that all the 
parameters in our model are conditional probabil-
ity of binary variables. This allows us to take ad-
vantage of similarity-based smoothing, which has 
not been successfully applied to parsing before. 
Acknowledgements 
The authors would like to thank Mark Steedman 
for suggesting the comparison with unlexicalized 
parsing in Section 4 and the anonymous reviewers 
for their comments. This work was supported in 
part by NSERC, the Alberta Ingenuity Centre for 
Machine Learning and the Canada Research 
158
Chairs program. Qin Iris Wang was also sup-
ported by iCORE Scholarship. 
References  
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing 
Model. Computational Linguistics, 30(4), pp. 479-
511.  
Daniel M. Bikel and David Chiang. 2000. Two Statisti-
cal Parsing Models applied to the Chinese Treebank. 
In Proceedings of the second Chinese Language 
Processing Workshop, pp. 1-6.  
Eugene Charniak. 2000. A Maximum-Entropy-Inspired 
Parser. In Proceedings of the Second Meeting of 
North American Chapter of Association for Compu-
tational Linguistics (NAACL-2000), pp. 132-139.  
Stephen Clark, Julia Hockenmaier and Mark Steedman. 
2002. Building Deep Dependency Structures with a 
Wide-Coverage CCG Parser. In Proceedings of the 
40th Annual Meeting of the ACL, pp. 327-334.  
Michael Collins. 1996. A New Statistical Parser Based 
on Bigram Lexical Dependencies. In Proceedings of 
the 34th Annual Meeting of the ACL, pp. 184-191. 
Santa Cruz.  
Michael Collins. 1997. Three Generative, Lexicalised 
Models for Statistical Parsing. In Proceedings of the 
35th Annual Meeting of the ACL (jointly with the 8th 
Conference of the EACL), pp. 16-23. Madrid.  
Michael Collins. 1999. Head-Driven Statistical Models 
for Natural Language Parsing. PhD Dissertation, 
University of Pennsylvania.  
Ido Dagan, Lillian Lee and Fernando Pereira. 1999. 
Similarity-based models of cooccurrence probabili-
ties. Machine Learning, Vol. 34(1-3) special issue 
on Natural Language Learning, pp. 43-69.  
Jason M. Eisner. 1996. Three new probabilistic models 
for dependency parsing: An exploration. In Proceed-
ings of COLING-96, pp. 340-345, Copenhagen.  
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Proceedings of EMNLP-2001, pp. 167-
202. Pittsburgh, PA.  
Gregory Grefenstette. 1994. Explorations in Automatic 
Thesaurus Discovery. Kluwer Academic Press, Bos-
ton, MA.  
Zelig S. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.  
Donald Hindle. 1990. Noun Classification from Predi-
cate-Argument Structures. In Proceedings of ACL-
90, pp. 268-275. Pittsburg, Pennsylvania.  
Dan Klein and Chris Manning. 2002. Fast exact infer-
ence with a factored model for natural language 
parsing. In Proceedings of Neural Information 
Processing Systems.  
Dan Klein and Chris Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st Annual 
Meeting of the ACL, pp. 423-430.  
Dan Klein and Chris Manning. 2004. Corpus-Based 
Induction of Syntactic Structure: Models of De-
pendency and Constituency. In Proceedings of the 
42nd Annual Meeting of the ACL, pp. 479-486.  
Roger Levy and Chris Manning. 2003. Is it harder to 
parse Chinese, or the Chinese Treebank?  In Pro-
ceedings of the 41st Annual Meeting of the ACL, pp. 
439-446. 
Dekang Lin. 1995. A dependency-based method for 
evaluating broad-coverage parsers. In Proceedings 
of IJCAI-95, pp.1420-1425.  
Dekang Lin. 1998. Automatic Retrieval and Clustering 
of Similar Words. In Proceeding of COLING-
ACL98, pp. 768-774. Montreal, Canada.  
Ryan McDonald, Koby Crammer, and Fernando 
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL-2005, pp. 
91-98.  
Fernando Pereira, Naftali Z. Tishby, and Lillian Lee. 
1993. Distributional clustering of English words. In 
Proceedings of ACL-1993, pp. 183-190, Columbus, 
Ohio.  
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal Dependency Analysis with Support Vector Ma-
chines. In Proceedings of the 8th International 
Workshop on Parsing Technologies, pp.195-206.  
159
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 17?24,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Inversion Transduction Grammar for Joint Phrasal Translation Modeling
Colin Cherry
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
colinc@cs.ualberta.ca
Dekang Lin
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA, USA, 9403
lindek@google.com
Abstract
We present a phrasal inversion trans-
duction grammar as an alternative to
joint phrasal translation models. This
syntactic model is similar to its flat-
string phrasal predecessors, but admits
polynomial-time algorithms for Viterbi
alignment and EM training. We demon-
strate that the consistency constraints that
allow flat phrasal models to scale also help
ITG algorithms, producing an 80-times
faster inside-outside algorithm. We also
show that the phrasal translation tables
produced by the ITG are superior to those
of the flat joint phrasal model, producing
up to a 2.5 point improvement in BLEU
score. Finally, we explore, for the first
time, the utility of a joint phrasal transla-
tion model as a word alignment method.
1 Introduction
Statistical machine translation benefits greatly from
considering more than one word at a time. One
can put forward any number of non-compositional
translations to support this point, such as the col-
loquial Canadian French-English pair, (Wo les mo-
teurs, Hold your horses), where no clear word-to-
word connection can be drawn. Nearly all cur-
rent decoding methods have shifted to phrasal rep-
resentations, gaining the ability to handle non-
compositional translations, but also allowing the de-
coder to memorize phenomena such as monolingual
agreement and short-range movement, taking pres-
sure off of language and distortion models.
Despite the success of phrasal decoders, knowl-
edge acquisition for translation generally begins
with a word-level analysis of the training text, tak-
ing the form of a word alignment. Attempts to apply
the same statistical analysis used at the word level
in a phrasal setting have met with limited success,
held back by the sheer size of phrasal alignment
space. Hybrid methods that combine well-founded
statistical analysis with high-confidence word-level
alignments have made some headway (Birch et al,
2006), but suffer from the daunting task of heuris-
tically exploring a still very large alignment space.
In the meantime, synchronous parsing methods effi-
ciently process the same bitext phrases while build-
ing their bilingual constituents, but continue to be
employed primarily for word-to-word analysis (Wu,
1997). In this paper we unify the probability models
for phrasal translation with the algorithms for syn-
chronous parsing, harnessing the benefits of both
to create a statistically and algorithmically well-
founded method for phrasal analysis of bitext.
Section 2 begins by outlining the phrase extrac-
tion system we intend to replace and the two meth-
ods we combine to do so: the joint phrasal transla-
tion model (JPTM) and inversion transduction gram-
mar (ITG). Section 3 describes our proposed solu-
tion, a phrasal ITG. Section 4 describes how to ap-
ply our phrasal ITG, both as a translation model and
as a phrasal word-aligner. Section 5 tests our system
in both these capacities, while Section 6 concludes.
2 Background
2.1 Phrase Table Extraction
Phrasal decoders require a phrase table (Koehn et
al., 2003), which contains bilingual phrase pairs and
17
scores indicating their utility. The surface heuris-
tic is the most popular method for phrase-table con-
struction. It extracts all consistent phrase pairs from
word-aligned bitext (Koehn et al, 2003). The word
alignment provides bilingual links, indicating trans-
lation relationships between words. Consistency is
defined so that alignment links are never broken by
phrase boundaries. For each token w in a consistent
phrase pair p?, all tokens linked tow by the alignment
must also be included in p?. Each consistent phrase
pair is counted as occurring once per sentence pair.
The scores for the extracted phrase pairs are pro-
vided by normalizing these flat counts according to
common English or Foreign components, producing
the conditional distributions p(f? |e?) and p(e?|f?).
The surface heuristic can define consistency ac-
cording to any word alignment; but most often, the
alignment is provided by GIZA++ (Och and Ney,
2003). This alignment system is powered by the
IBM translation models (Brown et al, 1993), in
which one sentence generates the other. These mod-
els produce only one-to-many alignments: each gen-
erated token can participate in at most one link.
Many-to-many alignments can be created by com-
bining two GIZA++ alignments, one where English
generates Foreign and another with those roles re-
versed (Och and Ney, 2003). Combination ap-
proaches begin with the intersection of the two
alignments, and add links from the union heuris-
tically. The grow-diag-final (GDF) combination
heuristic (Koehn et al, 2003) adds links so that each
new link connects a previously unlinked token.
2.2 Joint phrasal translation model
The IBM models that power GIZA++ are trained
with Expectation Maximization (Dempster et al,
1977), or EM, on sentence-aligned bitext. A transla-
tion model assigns probabilities to alignments; these
alignment distributions are used to count translation
events, which are then used to estimate new parame-
ters for the translation model. Sampling is employed
when the alignment distributions cannot be calcu-
lated efficiently. This statistically-motivated process
is much more appealing than the flat counting de-
scribed in Section 2.1, but it does not directly in-
clude phrases.
The joint phrasal translation model (Marcu and
Wong, 2002), or JPTM, applies the same statistical
techniques from the IBMmodels in a phrasal setting.
The JPTM is designed according to a generative pro-
cess where both languages are generated simultane-
ously. First, a bag of concepts, or cepts, C is gener-
ated. Each ci ? C corresponds to a bilingual phrase
pair, ci = (e?i, f?i). These contiguous phrases are
permuted in each language to create two sequences
of phrases. Initially, Marcu and Wong assume that
the number of cepts, as well as the phrase orderings,
are drawn from uniform distributions. That leaves
a joint translation distribution p(e?i, f?i) to determine
which phrase pairs are selected. Given a lexicon of
possible cepts and a predicate L(E,F,C) that de-
termines if a bag of cepts C can be bilingually per-
muted to create the sentence pair (E, F ), the proba-
bility of a sentence pair is:
p(E,F ) ?
?
{C|L(E,F,C)}
?
?
?
ci?C
p(e?i, f?i)
?
? (1)
If left unconstrained, (1) will consider every phrasal
segmentation of E and F , and every alignment be-
tween those phrases. Later, a distortion model based
on absolute token positions is added to (1).
The JPTM faces several problems when scaling
up to large training sets:
1. The alignment space enumerated by the sum
in (1) is huge, far larger than the one-to-many
space explored by GIZA++.
2. The translation distribution p(e?, f?) will cover
all co-occurring phrases observed in the bitext.
This is far too large to fit in main memory, and
can be unwieldly for storage on disk.
3. Given a non-uniform p(e?, f?), there is no effi-
cient algorithm to compute the expectation of
phrase pair counts required for EM, or to find
the most likely phrasal alignment.
Marcu and Wong (2002) address point 2 with a lexi-
con constraint; monolingual phrases that are above
a length threshold or below a frequency threshold
are excluded from the lexicon. Point 3 is handled
by hill-climbing to a likely phrasal alignment and
sampling around it. However, point 1 remains unad-
dressed, which prevents the model from scaling to
large data sets.
Birch et al (2006) handle point 1 directly by re-
ducing the size of the alignment space. This is
18
accomplished by constraining the JPTM to only
use phrase pairs that are consistent with a high-
confidence word alignment, which is provided by
GIZA++ intersection. We refer to this constrained
JPTM as a C-JPTM. This strikes an interesting
middle ground between the surface heuristic de-
scribed in Section 2.1 and the JPTM. Like the sur-
face heuristic, a word alignment is used to limit the
phrase pairs considered, but the C-JPTM reasons
about distributions over phrasal alignments, instead
of taking flat counts. The consistency constraint al-
lows them to scale their C-JPTM up to 700,000 sen-
tence pairs. With this constraint in place, the use of
hill-climbing and sampling during EM training be-
comes one of the largest remaining weaknesses of
the C-JPTM.
2.3 Inversion Transduction Grammar
Like the JPTM, stochastic synchronous grammars
provide a generative process to produce a sentence
and its translation simultaneously. Inversion trans-
duction grammar (Wu, 1997), or ITG, is a well-
studied synchronous grammar formalism. Terminal
productions of the form A ? e/f produce a to-
ken in each stream, or a token in one stream with
the null symbol ? in the other. To allow for move-
ment during translation, non-terminal productions
can be either straight or inverted. Straight produc-
tions, with their non-terminals inside square brack-
ets [. . .], produce their symbols in the given order in
both streams. Inverted productions, indicated by an-
gled brackets ?. . .?, are output in reverse order in the
Foreign stream only.
The work described here uses the binary bracket-
ing ITG, which has a single non-terminal:
A ? [AA] | ?AA? | e/f (2)
This grammar admits an efficient bitext parsing al-
gorithm, and holds no language-specific biases.
(2) cannot represent all possible permutations of
concepts that may occur during translation, because
some permutations will require discontinuous con-
stituents (Melamed, 2003). This ITG constraint is
characterized by the two forbidden structures shown
in Figure 1 (Wu, 1997). Empirical studies suggest
that only a small percentage of human translations
violate these constraints (Cherry and Lin, 2006).
e1
e2
e3
e4
f1 f2 f3 f4 f1 f2 f3 f4
e1
e2
e3
e4
Figure 1: The two ITG forbidden structures.
calmez vous
c
a
l
m
d
o
w
n
calmez vous
c
a
l
m
d
o
w
n
calmez vous
c
a
l
m
d
o
w
n
a) A?[AA] b) A?<AA>
c) A?e/f
Figure 2: Three ways in which a phrasal ITG can
analyze a multi-word span or phrase.
Stochastic ITGs are parameterized like their
PCFG counterparts (Wu, 1997); productions
A ? X are assigned probability Pr(X|A). These
parameters can be learned from sentence-aligned bi-
text using the EM algorithm. The expectation task
of counting productions weighted by their probabil-
ity is handled with dynamic programming, using the
inside-outside algorithm extended to bitext (Zhang
and Gildea, 2004).
3 ITG as a Phrasal Translation Model
This paper introduces a phrasal ITG; in doing so,
we combine ITG with the JPTM. ITG parsing al-
gorithms consider every possible two-dimensional
span of bitext, each corresponding to a bilingual
phrase pair. Each multi-token span is analyzed in
terms of how it could be built from smaller spans us-
ing a straight or inverted production, as is illustrated
in Figures 2 (a) and (b). To extend ITG to a phrasal
setting, we add a third option for span analysis: that
the span under consideration might have been drawn
directly from the lexicon. This option can be added
to our grammar by altering the definition of a termi-
nal production to include phrases: A ? e?/f? . This
third option is shown in Figure 2 (c). The model
implied by this extended grammar is trained using
inside-outside and EM.
Our approach differs from previous attempts to
use ITGs for phrasal bitext analysis. Wu (1997)
used a binary bracketing ITG to segment a sen-
19
tence while simultaneously word-aligning it to its
translation, but the model was trained heuristically
with a fixed segmentation. Vilar and Vidal (2005)
used ITG-like dynamic programming to drive both
training and alignment for their recursive translation
model, but they employed a conditional model that
did not maintain a phrasal lexicon. Instead, they
scored phrase pairs using IBM Model 1.
Our phrasal ITG is quite similar to the JPTM.
Both models are trained with EM, and both em-
ploy generative stories that create a sentence and its
translation simultaneously. The similarities become
more apparent when we consider the canonical-form
binary-bracketing ITG (Wu, 1997) shown here:
S ? A | B | C
A ? [AB] | [BB] | [CB] |
[AC] | [BC] | [CC]
B ? ?AA? | ?BA? | ?CA? |
?AC? | ?BC? | ?CC?
C ? e?/f?
(3)
(3) is employed in place of (2) to reduce redundant
alignments and clean up EM expectations.1 More
importantly for our purposes, it introduces a preter-
minal C, which generates all phrase pairs or cepts.
When (3) is parameterized as a stochastic ITG, the
conditional distribution p(e?/f? |C) is equivalent to
the JPTM?s p(e?, f?); both are joint distributions over
all possible phrase pairs. The distributions condi-
tioned on the remaining three non-terminals assign
probability to concept movement by tracking inver-
sions. Like the JPTM?s distortion model, these pa-
rameters grade each movement decision indepen-
dently. With terminal productions producing cepts,
and inversions measuring distortion, our phrasal ITG
is essentially a variation on the JPTM with an alter-
nate distortion model.
Our phrasal ITG has two main advantages over
the JPTM. Most significantly, we gain polynomial-
time algorithms for both Viterbi alignment and EM
expectation, through the use of ITG parsing and
inside-outside algorithms. These phrasal ITG algo-
rithms are no more expensive asymptotically than
their word-to-word counterparts, since each poten-
tial phrase needs to be analyzed anyway during
1If the null symbol ? is included among the terminals, then
redundant parses will still occur, but far less frequently.
constituent construction. We hypothesize that us-
ing these methods in place of heuristic search and
sampling will improve the phrasal translation model
learned by EM. Also, we can easily incorporate links
to ? by including the symbol among our terminals.
To minimize redundancy, we allow only single to-
kens, not phrases, to align to ?. The JPTM does not
allow links to ?.
The phrasal ITG also introduces two new compli-
cations. ITG Viterbi and inside-outside algorithms
have polynomial complexity, but that polynomial is
O(n6), where n is the length of the longer sentence
in the pair. This is too slow to train on large data
sets without massive parallelization. Also, ITG al-
gorithms explore their alignment space perfectly, but
that space has been reduced by the ITG constraint
described in Section 2.3. We will address each of
these issues in the following two subsections.
3.1 Pruning Spans
First, we address the problem of scaling ITG to large
data. ITG dynamic programming algorithms work
by analyzing each bitext span only once, storing its
value in a table for future use. There are O(n4) of
these spans, and each analysis takes O(n2) time. An
effective approach to speeding up ITG algorithms
is to eliminate unlikely spans as a preprocessing
step, assigning them 0 probability and saving the
time spent processing them. Past approaches have
pruned spans using IBM Model 1 probability esti-
mates (Zhang and Gildea, 2005) or using agreement
with an existing parse tree (Cherry and Lin, 2006).
The former is referred to as tic-tac-toe pruning be-
cause it uses both inside and outside estimates.
We propose a new ITG pruning method that lever-
ages high-confidence links by pruning all spans that
are inconsistent with a provided alignment. This
is similar to the constraint used in the C-JPTM,
but we do not just eliminate those spans as poten-
tial phrase-to-phrase links: we never consider any
ITG parse that builds a non-terminal over a pruned
span.2 This fixed-link pruning will speed up both
Viterbi alignment and EM training by reducing the
number of analyzed spans, and so long as we trust
2Birch et al (2006) re-introduce inconsistent phrase-pairs in
cases where the sentence pair could not be aligned otherwise.
We allow links to ? to handle these situations, completely elim-
inating the pruned spans from our alignment space.
20
our high-confidence links, it will do so harmlessly.
We demonstrate the effectiveness of this pruning
method experimentally in Section 5.1.
3.2 Handling the ITG Constraint
Our remaining concern is the ITG constraint. There
are some alignments that we just cannot build, and
sentence pairs requiring those alignments will occur.
These could potentially pollute our training data; if
the system is unable to build the right alignment, the
counts it will collect from that pair must be wrong.
Furthermore, if our high-confidence links are not
ITG-compatible, our fixed-link pruning will prevent
the aligner from forming any alignments at all.
However, these two potential problems cancel
each other out. Sentence pairs containing non-ITG
translations will tend to have high-confidence links
that are also not ITG-compatible. Our EM learner
will simply skip these sentence pairs during train-
ing, avoiding pollution of our training data. We can
use a linear-time algorithm (Zhang et al, 2006) to
detect non-ITG movement in our high-confidence
links, and remove the offending sentence pairs from
our training corpus. This results in only a minor re-
duction in training data; in our French-English train-
ing set, we lose less than 1%. In the experiments de-
scribed in Section 5, all systems that do not use ITG
will take advantage of the complete training set.
4 Applying the model
Any phrasal translation model can be used for two
tasks: translation modeling and phrasal word align-
ment. Previous work on JPTM has focused on only
the first task. We are interested in phrasal alignment
because it may be better suited to heuristic phrase-
extraction than word-based models. This section de-
scribes how to use our phrasal ITG first as a transla-
tion model, and then as a phrasal aligner.
4.1 Translation Modeling
We can test our model?s utility for translation by
transforming its parameters into a phrase table for
the phrasal decoder Pharaoh (Koehn et al, 2003).
Any joint model can produce the necessary condi-
tional probabilities by conditionalizing the joint ta-
ble in both directions. We use our p(e?/f? |C) dis-
tribution from our stochastic grammar to produce
p(e?|f?) and p(f? |e?) values for its phrasal lexicon.
Pharaoh also includes lexical weighting param-
eters that are derived from the alignments used to
induce its phrase pairs (Koehn et al, 2003). Us-
ing the phrasal ITG as a direct translation model,
we do not produce alignments for individual sen-
tence pairs. Instead, we provide a lexical preference
with an IBM Model 1 feature pM1 that penalizes un-
matched words (Vogel et al, 2003). We include both
pM1(e?|f?) and pM1(f? |e?).
4.2 Phrasal Word Alignment
We can produce a translation model using inside-
outside, without ever creating a Viterbi parse. How-
ever, we can also examine the maximum likelihood
phrasal alignments predicted by the trained model.
Despite its strengths derived from using phrases
throughout training, the alignments predicted by our
phrasal ITG are usually unsatisfying. For exam-
ple, the fragment pair (order of business, ordre des
travaux) is aligned as a phrase pair by our system,
linking every English word to every French word.
This is frustrating, since there is a clear compo-
sitional relationship between the fragment?s com-
ponent words. This happens because the system
seeks only to maximize the likelihood of its train-
ing corpus, and phrases are far more efficient than
word-to-word connections. When aligning text, an-
notators are told to resort to many-to-many links
only when no clear compositional relationship ex-
ists (Melamed, 1998). If we could tell our phrasal
aligner the same thing, we could greatly improve the
intuitive appeal of our alignments. Again, we can
leverage high-confidence links for help.
In the high-confidence alignments provided by
GIZA++ intersection, each token participates in at
most one link. Links only appear when two word-
based IBM translation models can agree. Therefore,
they occur at points of high compositionality: the
two words clearly account for one another. We adopt
an alignment-driven definition of compositional-
ity: any phrase pair containing two or more high-
confidence links is compositional, and can be sep-
arated into at least two non-compositional phrases.
By removing any phrase pairs that are compositional
by this definition from our terminal productions,
we can ensure that our aligner never creates such
phrases during training or alignment. Doing so pro-
duces far more intuitive alignments. Aligned with
21
a model trained using this non-compositional con-
straint (NCC), our example now forms three word-
to-word connections, rather than a single phrasal
one. The phrases produced with this constraint are
very small, and include only non-compositional con-
text. Therefore, we use the constraint only to train
models intended for Viterbi alignment, and not when
generating phrase tables directly as in Section 4.1.
5 Experiments and Results
In this section, we first verify the effectiveness of
fixed-link pruning, and then test our phrasal ITG,
both as an aligner and as a translation model. We
train all translation models with a French-English
Europarl corpus obtained by applying a 25 to-
ken sentence-length limit to the training set pro-
vided for the HLT-NAACL SMT Workshop Shared
Task (Koehn and Monz, 2006). The resulting cor-
pus has 393,132 sentence pairs. 3,376 of these
are omitted for ITG methods because their high-
confidence alignments have ITG-incompatible con-
structions. Like our predecessors (Marcu and Wong,
2002; Birch et al, 2006), we apply a lexicon con-
straint: no monolingual phrase can be used by any
phrasal model unless it occurs at least five times.
High-confidence alignments are provided by inter-
secting GIZA++ alignments trained in each direc-
tion with 5 iterations each of Model 1, HMM, and
Model 4. All GIZA++ alignments are trained with
no sentence-length limit, using the full 688K corpus.
5.1 Pruning Speed Experiments
To measure the speed-up provided by fixed-link
pruning, we timed our phrasal inside-outside algo-
rithm on the first 100 sentence pairs in our training
set, with and without pruning. The results are shown
in Table 1. Tic-tac-toe pruning is included for com-
parison. With fixed-link pruning, on average 95%
of the possible spans are pruned, reducing running
time by two orders of magnitude. This improvement
makes ITG training feasible, even with large bitexts.
5.2 Alignment Experiments
The goal of this experiment is to compare the Viterbi
alignments from the phrasal ITG to gold standard
human alignments. We do this to validate our non-
compositional constraint and to select good align-
ments for use with the surface heuristic.
Table 1: Inside-outside run-time comparison.
Method Seconds Avg. Spans Pruned
No Prune 415 -
Tic-tac-toe 37 68%
Fixed link 5 95%
Table 2: Alignment Comparison.
Method Prec Rec F-measure
GIZA++ Intersect 96.7 53.0 68.5
GIZA++ Union 82.5 69.0 75.1
GIZA++ GDF 84.0 68.2 75.2
Phrasal ITG 50.7 80.3 62.2
Phrasal ITG + NCC 75.4 78.0 76.7
Following the lead of (Fraser and Marcu, 2006),
we hand-aligned the first 100 sentence pairs of
our training set according to the Blinker annota-
tion guidelines (Melamed, 1998). We did not dif-
ferentiate between sure and possible links. We re-
port precision, recall and balanced F-measure (Och
and Ney, 2003). For comparison purposes, we in-
clude the results of three types of GIZA++ combina-
tion, including the grow-diag-final heuristic (GDF).
We tested our phrasal ITG with fixed link prun-
ing, and then added the non-compositional con-
straint (NCC). During development we determined
that performance levels off for both of the ITG mod-
els after 3 EM iterations. The results are shown in
Table 2.
The first thing to note is that GIZA++ Intersection
is indeed very high precision. Our confidence in it
as a constraint is not misplaced. We also see that
both phrasal models have significantly higher recall
than any of the GIZA++ alignments, even higher
than the permissive GIZA++ union. One factor con-
tributing to this is the phrasal model?s use of cepts:
it completely interconnects any phrase pair, while
GIZA++ union and GDF may not. Its global view
of phrases also helps in this regard: evidence for a
phrase can be built up over multiple sentences. Fi-
nally, we note that in terms of alignment quality,
the non-compositional constraint is an unqualified
success for the phrasal ITG. It produces a 25 point
improvement in precision, at the cost of 2 points
22
of recall. This produces the highest balanced F-
measure observed on our test set, but the utility of
its alignments will depend largely on one?s desired
precision-recall trade-off.
5.3 Translation Experiments
In this section, we compare a number of different
methods for phrase table generation in a French to
English translation task. We are interested in an-
swering three questions:
1. Does the phrasal ITG improve on the C-JPTM?
2. Can phrasal translation models outperform the
surface heuristic?
3. Do Viterbi phrasal alignments provide better
input for the surface heuristic?
With this in mind, we test five phrase tables. Two
are conditionalized phrasal models, each EM trained
until performance degrades:
? C-JPTM3 as described in (Birch et al, 2006)
? Phrasal ITG as described in Section 4.1
Three provide alignments for the surface heuristic:
? GIZA++ with grow-diag-final (GDF)
? Viterbi Phrasal ITG with and without the non-
compositional constraint
We use the Pharaoh decoder (Koehn et al, 2003)
with the SMT Shared Task baseline system (Koehn
and Monz, 2006). Weights for the log-linear model
are set using the 500-sentence tuning set provided
for the shared task with minimum error rate train-
ing (Och, 2003) as implemented by Venugopal
and Vogel (2005). Results on the provided 2000-
sentence development set are reported using the
BLEU metric (Papineni et al, 2002). For all meth-
ods, we report performance with and without IBM
Model 1 features (M1), along with the size of the re-
sulting tables in millions of phrase pairs. The results
of all experiments are shown in Table 3.
We see that the Phrasal ITG surpasses the C-
JPTM by more than 2.5 BLEU points. A large com-
ponent of this improvement is due to the ITG?s use
of inside-outside for expectation calculation, though
3Supplied by personal communication. Run with default pa-
rameters, but with maximum phrase length increased to 5.
Table 3: Translation Comparison.
Method BLEU +M1 Size
Conditionalized Phrasal Model
C-JPTM 26.27 28.98 1.3M
Phrasal ITG 28.85 30.24 2.2M
Alignment with Surface Heuristic
GIZA++ GDF 30.46 30.61 9.8M
Phrasal ITG 30.31 30.39 5.8M
Phrasal ITG + NCC 30.66 30.80 9.0M
there are other differences between the two sys-
tems.4 This improvement over search and sampling
is demonstrated by the ITG?s larger table size; by ex-
ploring more thoroughly, it is extracting more phrase
pairs from the same amount of data. Both systems
improve drastically with the addition of IBM Model
1 features for lexical preference. These features also
narrow the gap between the two systems. To help
calibrate the contribution of these features, we pa-
rameterized the ITG?s phrase table using only Model
1 features, which scores 27.17.
Although ITG+M1 comes close, neither phrasal
model matches the performance of the surface
heuristic. Whatever the surface heuristic lacks in
sophistication, it makes up for in sheer coverage,
as demonstrated by its huge table sizes. Even the
Phrasal ITG Viterbi alignments, which over-commit
wildly and have horrible precision, score slightly
higher than the best phrasal model. The surface
heuristic benefits from capturing as much context
as possible, while still covering smaller translation
events with its flat counts. It is not held back by
any lexicon constraints. When GIZA++ GDF+M1
is forced to conform to a lexicon constraint by drop-
ping any phrase with a frequency lower than 5 from
its table, it scores only 29.26, for a reduction of 1.35
BLEU points.
Phrases extracted from our non-compositional
Viterbi alignments receive the highest BLEU score,
but they are not significantly better than GIZA++
GDF. The two methods also produce similarly-sized
tables, despite the ITG?s higher recall.
4Unlike our system, the Birch implementation does table
smoothing and internal lexical weighting, both of which should
help improve their results. The systems also differ in distortion
modeling and ? handling, as described in Section 3.
23
6 Conclusion
We have presented a phrasal ITG as an alternative
to the joint phrasal translation model. This syntactic
solution to phrase modeling admits polynomial-time
training and alignment algorithms. We demonstrate
that the same consistency constraints that allow joint
phrasal models to scale also dramatically speed up
ITGs, producing an 80-times faster inside-outside
algorithm. We show that when used to learn phrase
tables for the Pharaoh decoder, the phrasal ITG is
superior to the constrained joint phrasal model, pro-
ducing tables that result in a 2.5 point improve-
ment in BLEU when used alone, and a 1 point im-
provement when used with IBM Model 1 features.
This suggests that ITG?s perfect expectation count-
ing does matter; other phrasal models could benefit
from either adopting the ITG formalism, or improv-
ing their sampling heuristics.
We have explored, for the first time, the utility of a
joint phrasal model as a word alignment method. We
present a non-compositional constraint that turns the
phrasal ITG into a high-recall phrasal aligner with
an F-measure that is comparable to GIZA++.
With search and sampling no longer a concern,
the remaining weaknesses of the system seem to lie
with the model itself. Phrases are just too efficient
probabilistically: were we to remove all lexicon con-
straints, EM would always align entire sentences to
entire sentences. This pressure to always build the
longest phrase possible may be overwhelming oth-
erwise strong correlations in our training data. A
promising next step would be to develop a prior over
lexicon size or phrase size, allowing EM to intro-
duce large phrases at a penalty, and removing the
need for artificial constraints on the lexicon.
Acknowledgments Special thanks to Alexandra
Birch for the use of her code, and to our reviewers
for their comments. The first author is funded by
Alberta Ingenuity and iCORE studentships.
References
A. Birch, C. Callison-Burch, M. Osborne, and P. Koehn. 2006.
Constraining the phrase-based, joint probability statistical
translation model. In HLT-NAACL Workshop on Statistical
Machine Translation, pages 154?157.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?312.
C. Cherry and D. Lin. 2006. A comparison of syntactically
motivated word alignment spaces. In EACL, pages 145?152.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, 39(1):1?38.
A. Fraser and D. Marcu. 2006. Semi-supervised training for
statistical word alignment. In ACL, pages 769?776.
P. Koehn and C. Monz. 2006. Manual and automatic evalu-
ation of machine translation. In HLT-NACCL Workshop on
Statistical Machine Translation, pages 102?121.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In HLT-NAACL, pages 127?133.
D. Marcu and W. Wong. 2002. A phrase-based, joint probabil-
ity model for statistic machine translation. In EMNLP, pages
133?139.
I. D. Melamed. 1998. Manual annotation of translational
equivalence: The blinker project. Technical Report 98-07,
Institute for Research in Cognitive Science.
I. D. Melamed. 2003. Multitext grammars and synchronous
parsers. In HLT-NAACL, pages 158?165.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?52.
F. J. Och. 2003. Minimum error rate training for statistical
machine translation. In ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU:
a method for automatic evaluation of machine translation. In
ACL, pages 311?318.
A. Venugopal and S. Vogel. 2005. Considerations in maximum
mutual information and minimum classification error train-
ing for statistical machine translation. In EAMT.
J. M. Vilar and E. Vidal. 2005. A recursive statistical transla-
tion model. In Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts, pages 199?207.
S. Vogel, Y. Zhang, F. Huang, A. Tribble, A. Venugopal,
B. Zhang, and A. Waibel. 2003. The CMU statistical ma-
chine translation system. In MT Summmit.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?403.
H. Zhang and D. Gildea. 2004. Syntax-based alignment: Su-
pervised or unsupervised? In COLING, pages 418?424.
H. Zhang and D. Gildea. 2005. Stochastic lexicalized inversion
transduction grammar for alignment. In ACL, pages 475?
482.
H. Zhang, L. Huang, D. Gildea, and K. Knight. 2006. Syn-
chronous binarization for machine translation. In HLT-
NAACL, pages 256?263.
24
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 120?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Glen, Glenda or Glendale:
Unsupervised and Semi-supervised Learning of English Noun Gender
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View
California, 94301
lindek@google.com
Randy Goebel
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
goebel@cs.ualberta.ca
Abstract
English pronouns like he and they reliably re-
flect the gender and number of the entities to
which they refer. Pronoun resolution systems
can use this fact to filter noun candidates that
do not agree with the pronoun gender. In-
deed, broad-coverage models of noun gender
have proved to be the most important source
of world knowledge in automatic pronoun res-
olution systems.
Previous approaches predict gender by count-
ing the co-occurrence of nouns with pronouns
of each gender class. While this provides use-
ful statistics for frequent nouns, many infre-
quent nouns cannot be classified using this
method. Rather than using co-occurrence in-
formation directly, we use it to automatically
annotate training examples for a large-scale
discriminative gender model. Our model col-
lectively classifies all occurrences of a noun
in a document using a wide variety of con-
textual, morphological, and categorical gender
features. By leveraging large volumes of un-
labeled data, our full semi-supervised system
reduces error by 50% over the existing state-
of-the-art in gender classification.
1 Introduction
Pronoun resolution is the process of determining
which preceding nouns are referred to by a partic-
ular pronoun in text. Consider the sentence:
(1) Glen told Glenda that she was wrong about
Glendale.
A pronoun resolution system should determine that
the pronoun she refers to the noun Glenda. Pro-
noun resolution is challenging because it requires a
lot of world knowledge (general knowledge of word
types). If she is replaced with the pronoun he in (1),
Glen becomes the antecedent. Pronoun resolution
systems need the knowledge of noun gender that ad-
vises that Glen is usually masculine (and thus re-
ferred to by he) while Glenda is feminine.
English third-person pronouns are grouped in four
gender/number categories: masculine (he, his, him,
himself ), feminine (she, her, herself ), neutral (it, its,
itself ), and plural (they, their, them, themselves). We
broadly refer to these gender and number classes
simply as gender. The objective of our work is to
correctly assign gender to English noun tokens, in
context; to determine which class of pronoun will
refer to a given noun.
One successful approach to this problem is to
build a statistical gender model from a noun?s asso-
ciation with pronouns in text. For example, Ge et al
(1998) learn Ford has a 94% chance of being neu-
tral, based on its frequent co-occurrence with neu-
tral pronouns in text. Such estimates are noisy but
useful. Both Ge et al (1998) and Bergsma and Lin
(2006) show that learned gender is the most impor-
tant feature in their pronoun resolution systems.
English differs from other languages like French
and German in that gender is not an inherent gram-
matical property of an English noun, but rather a
property of a real-world entity that is being referred
to. A common noun like lawyer can be (semanti-
cally) masculine in one document and feminine in
another. While previous statistical gender models
learn gender for noun types only, we use document
context to correctly determine the current gender
class of noun tokens, making dynamic decisions on
common nouns like lawyer and ambiguous names
like Ford. Furthermore, if a noun type has not yet
120
been observed (an unknown word), previous ap-
proaches cannot estimate the gender. Our system,
on the other hand, is able to correctly determine that
unknown words corroborators and propeller-heads
are plural, while Pope Formosus is masculine, using
learned contextual and morphological cues.
Our approach is based on the key observation that
while gender information from noun-pronoun co-
occurrence provides imperfect noun coverage, it can
nevertheless provide rich and accurate training data
for a large-scale discriminative classifier. The clas-
sifier leverages a wide variety of noun properties to
generalize from the automatically-labeled examples.
The steps in our approach are:
1. Training:
(a) Automatically extract a set of seed
(noun,gender) pairs from high-quality in-
stances in a statistical gender database.
(b) In a large corpus of text, find documents con-
taining these nouns.
(c) For all instances of each noun in each document,
create a single, composite feature vector repre-
senting all the contexts of the noun in the docu-
ment, as well as encoding other selected proper-
ties of the noun type.
(d) Label each feature vector with the seed noun?s
corresponding gender.
(e) Train a 4-way gender classifier (masculine, fem-
inine, neutral, plural) from the automatically-
labeled vectors.
2. Testing:
(a) Given a new document, create a composite fea-
ture vector for all occurrences of each noun.
(b) Use the learned classifier to assign gender to
each feature vector, and thus all occurrences of
all nouns in the document.
This algorithm achieves significantly better per-
formance than the existing state-of-the-art statisti-
cal gender classifier, while requiring no manually-
labeled examples to train. Furthermore, by training
on a small number of manually-labeled examples,
we can combine the predictions of this system with
the counts from the original gender database. This
semi-supervised extension achieves 95.5% accuracy
on final unseen test data, an impressive 50% reduc-
tion in error over previous work.
2 Path-based Statistical Noun Gender
Seed (noun,gender) examples can be extracted re-
liably and automatically from raw text, providing
the training data for our discriminative classifier.
We call these examples pseudo-seeds because they
are created fully automatically, unlike the small set
of manually-created seeds used to initialize other
bootstrapping approaches (cf. the bootstrapping ap-
proaches discussed in Section 6).
We adopt a statistical approach to acquire the
pseudo-seed (noun,gender) pairs. All previous sta-
tistical approaches rely on a similar observation: if
a noun like Glen is often referred to by masculine
pronouns, like he or his, then Glen is likely a mas-
culine noun. But for most nouns we have no an-
notated data recording their coreference with pro-
nouns, and thus no data from which we can ex-
tract the co-occurrence statistics. Thus previous ap-
proaches rely on either hand-crafted coreference-
indicating patterns (Bergsma, 2005), or iteratively
guess and improve gender models through expec-
tation maximization of pronoun resolution (Cherry
and Bergsma, 2005; Charniak and Elsner, 2009). In
statistical approaches, the more frequent the noun,
the more accurate the assignment of gender.
We use the approach of Bergsma and Lin (2006),
both because it achieves state-of-the-art gender
classification performance, and because a database
of the obtained noun genders is available online.1
Bergsma and Lin (2006) use an unsupervised
algorithm to identify syntactic paths along which a
noun and pronoun are highly likely to corefer. To
extract gender information, they processed a large
corpus of news text, and obtained co-occurrence
counts for nouns and pronouns connected with these
paths in the corpus. In their database, each noun is
listed with its corresponding masculine, feminine,
neutral, and plural pronoun co-occurrence counts,
e.g.:
glen 555 42 32 34
glenda 8 102 0 11
glendale 24 2 167 18
glendalians 0 0 0 1
glenn 3182 207 95 54
glenna 0 6 0 0
1Available at http://www.cs.ualberta.ca/?bergsma/Gender/
121
This sample of the gender data shows that the
noun glenda, for example, occurs 8 times with mas-
culine pronouns, 102 times with feminine pronouns,
0 times with neutral pronouns, and 11 times with
plural pronouns; 84% of the time glenda co-occurs
with a feminine pronoun. Note that all nouns in the
data have been converted to lower-case.2
There are gender counts for 3.1 million English
nouns in the online database. These counts form the
basis for the state-of-the-art gender classifier. We
can either take the most-frequent pronoun-gender
(MFPG) as the class (e.g. feminine for glenda), or
we can supply the logarithm of the counts as features
in a 4-way multi-class classifier. We implement the
latter approach as a comparison system and refer to
it as PATHGENDER in our experiments.
In our approach, rather than using these counts
directly, we process the database to automatically
extract a high-coverage but also high-quality set of
pseudo-seed (noun,gender) pairs. First, we filter
nouns that occur less than fifty times and whose
MFPG accounts for less than 85% of counts. Next,
we note that the most reliable nouns should occur
relatively often in a coreferent path. For exam-
ple, note that importance occurs twice as often on
the web as Clinton, but has twenty-four times less
counts in the gender database. This is because im-
portance is unlikely to be a pronoun?s antecedent.
We plan to investigate this idea further in future
work as a possible filter on antecedent candidates
for pronoun resolution. For the present work, sim-
ply note that a high ratio of database-count to web-
count provides a good indication of the reliability of
a noun?s gender counts, and thus we filter nouns that
have such ratios below a threshold.3 After this fil-
tering, we have about 45 thousand nouns to which
we automatically assign gender according to their
MFPG. These (noun,gender) pairs provide the seed
examples for the training process described in the
2Statistical approaches can adapt to the idiosyncrasies of the
particular text domain. In the news text from which this data
was generated, for example, both the word ships and specific
instances of ships (the USS Cole, the Titanic, etc.) are neutral.
In Wikipedia, on the other hand, feminine pronouns are often
used for ships. Such differences can be learned automatically.
3We roughly tuned all the thresholds to obtain the highest
number of seeds such that almost all of them looked correct
(e.g. Figure 1). Further work is needed to determine whether a
different precision/recall tradeoff can improve performance.
. . .
stefanie
steffi graf
steinem
stella mccartney
stellar jayne
stepdaughter
stephanie
stephanie herseth
stephanie white
stepmother
stewardess
. . .
Figure 1: Sample feminine seed nouns
following section. Figure 1 provides a portion of the
ordered feminine seed nouns that we extracted.
3 Discriminative Learning of Gender
Once we have extracted a number of pseudo-seed
(noun,gender) pairs, we use them to automatically-
label nouns (in context) in raw text. The auto-
labeled examples provide training data for discrimi-
native learning of noun gender.
Since the training pairs are acquired from a
sparse and imperfect model of gender, what can
we gain by training over them? We can regard the
Bergsma and Lin (2006) approach and our discrim-
inative system as two orthogonal views of gender,
in a co-training sense (Blum and Mitchell, 1998).
Some nouns can be accurately labeled by noun-
pronoun co-occurrence (a view based on pronoun
co-occurrence), and these examples can be used to
deduce other gender-indicating regularities (a view
based on other features, described below).
We presently explain how examples are extracted
using our pseudo-seed pairs, turned into auto-
labeled feature vectors, and then used to train a su-
pervised classifier.
3.1 Automatic example extraction
Our example-extraction module processes a large
collection of documents (roughly a million docu-
ments in our experiments). For each document, we
extract all the nouns, including context words within
?5 tokens of each noun. We then group the nouns by
122
Class=masculine String=?Lee?
Contexts =
?led some to suggest that ? , who was born in?
?? also downloaded secret files to?
?? says he was just making?
?by mishandling the investigation of ? .?
. . .
Figure 2: Sample noun training instance
their (lower-case) string. If a group?s noun-string is
in our set of seed (noun,gender) pairs, we assign the
corresponding gender to be the class of the group.
Otherwise, we discard the group. To prevent fre-
quent nouns from dominating our training data, we
only keep the first 200 groups corresponding to each
noun string. Figure 2 gives an example training noun
group with some (selected) context sentences. At
test time, all nouns in the test documents are con-
verted to this format for further processing.
We group nouns because there is a strong ten-
dency for nouns to have only one sense (and hence
gender) per discourse. We extract contexts because
nearby words provide good clues about which gen-
der is being used. The notion that nouns have only
one sense per discourse/collocation was also ex-
ploited by Yarowsky (1995) in his seminal work on
bootstrapping for word sense disambiguation.
3.2 Feature vectors
Once the training instances are extracted, they are
converted to labeled feature vectors for supervised
learning. The automatically-determined gender pro-
vides the class label (e.g., masculine for the group
in Figure 2). The features identify properties of the
noun and its context that potentially correlate with a
particular gender category. We divide the features
into two sets: those that depend on the contexts
within the document (Context features: features of
the tokens in the document), and those that depend
on the noun string only (Type features). In both
cases we induce the feature space from the train-
ing examples, keeping only those features that occur
more than 5 times.
3.2.1 Context features
The first set of features represent the contexts of
the word, using all the contexts in the noun group.
To illustrate the potential utility of the context infor-
mation, consider the context sentences for the mas-
culine noun in Figure 2. Even if these snippets were
all the information we were given, it would be easy
to guess the gender of the noun.
We use binary attribute-value features to flag, for
any of the contexts, the presence of all words at con-
text positions ?1,?2, etc. (sometimes called col-
location features (Golding and Roth, 1999)). For
example, feature 255920 flags that the word two-to-
the-right of the noun is he. We also provide fea-
tures for the presence of all words anywhere within
?5 tokens of the noun (sometimes called context
words). We also parse the sentence and provide a
feature for the noun?s parent (and relationship with
the parent) in the parse tree. For example, the in-
stance in Figure 2 has features downloaded(subject),
says(subject), etc. Since plural nouns should be gov-
erned by plural verbs, this feature is likely to be es-
pecially helpful for number classification.
3.2.2 Type features
The next group of features represent morpholog-
ical properties of the noun. Binary features flag the
presence of all prefixes and suffixes of one-to-four
characters. For multi-token nouns, we have features
for the first and last token in the noun. Thus we hope
to learn that Bob begins masculine nouns while inc.
ends neutral ones.
Finally, we have features that indicate if the noun
or parts of the noun occur on various lists. Indica-
tor features specify if any token occurs on in-house
lists of given names, family names, cities, provinces,
countries, corporations, languages, etc. A feature
also indicates if a token is a corporate designation
(like inc. or ltd.) or a human one (like Mr. or Sheik).
We also made use of the person-name/instance
pairs automatically extracted by Fleischman et al
(2003).4 This data provides counts for pairs such
as (Zhang Qiyue, spokeswoman) and (Thorvald
Stoltenberg, mediator). We have features for all con-
cepts (like spokeswoman and mediator) and there-
fore learn their association with each gender.
3.3 Supervised learning and classification
Once all the feature vectors have been extracted,
they are passed to a supervised machine learn-
4Available at http://www.mit.edu/?mbf/instances.txt.gz
123
ing algorithm. We train and classify using a
multi-class linear-kernel Support Vector Machine
(SVM) (Crammer and Singer, 2001). SVMs are
maximum-margin classifiers that achieve good per-
formance on a range of tasks. At test time, nouns in
test documents are processed exactly as the training
instances described above, converting them to fea-
ture vectors. The test vectors are classified by the
SVM, providing gender classes for all the nouns in
the test document. Since all training examples are
labeled automatically (auto-trained), we denote sys-
tems using this approach as -AUTO.
3.4 Semi-supervised extension
Although a good gender classifier can be learned
from the automatically-labeled examples alone, we
can also use a small quantity of gold-standard la-
beled examples to achieve better performance.
Combining information from our two sets of la-
beled data is akin to a domain adaptation prob-
lem. The gold-standard data can be regarded as
high-quality in-domain data, and the automatically-
labeled examples can be regarded as the weaker, but
larger, out-of-domain evidence.
There is a simple but effective method for com-
bining information from two domains using predic-
tions as features. We train a classifier on the full set
of automatically-labeled data (as described in Sec-
tion 3.3), and then use this classifier?s predictions as
features in a separate classifier, which is trained on
the gold-standard data. This is like the competitive
Feats domain-adaptation system in Daume? III and
Marcu (2006).
For our particular SVM classifier (Section 4.1),
predictions take the form of four numerical scores
corresponding to the four different genders. Our
gold-standard classifier has features for these four
predictions plus features for the original path-based
gender counts (Section 2).5 Since this approach uses
both automatically-labeled and gold-standard data in
a semi-supervised learning framework, we denote
systems using this approach as -SEMI.
5We actually use 12 features for the path-based counts: the
4 original, and then 4 each for counts for the first and last token
in the noun string. See PATHGENDER+ in Section 4.2.
4 Experiments
4.1 Set-up
We parsed the 3 GB AQUAINT corpus (Vorhees,
2002) using Minipar (Lin, 1998) to create our un-
labeled data. We process this data as described in
Section 3, making feature vectors from the first 4
million noun groups. We train from these exam-
ples using a linear-kernel SVM via the the efficient
SVMmulticlass instance of the SVMstruct software
package (Tsochantaridis et al, 2004).
To create our gold-standard gender data, we fol-
low Bergsma (2005) in extracting gender informa-
tion from the anaphora-annotated portion6 of the
American National Corpus (ANC) (Ide and Sud-
erman, 2004). In each document, we first group
all nouns with a common lower-case string (exactly
as done for our example extraction (Section 3.1)).
Next, for each group we determine if a third-person
pronoun refers to any noun in that group. If so, we
label all nouns in the group with the gender of the
referring pronoun. For example, if the pronoun he
refers to a noun Brown, then all instances of Brown
in the document are labeled as masculine. We ex-
tract the genders for 2794 nouns in the ANC train-
ing set (in 798 noun groups) and 2596 nouns in the
ANC test set (in 642 groups). We apply this method
to other annotated corpora (including MUC corpora)
to create a development set.
The gold standard ANC training set is used to
set the weights on the counts in the PATHGENDER
classifiers, and to train the semi-supervised ap-
proaches. We also use an SVM to learn these
weights. We use the development set to tune the
SVM?s regularization parameter, both for systems
trained on automatically-generated data, and for sys-
tems trained on gold-standard data. We also opti-
mize each automatically-trained system on the de-
velopment set when we include this system?s pre-
dictions as features in the semi-supervised exten-
sion. We evaluate and state performance for all ap-
proaches on the final unseen ANC test set.
4.2 Evaluation
The primary purpose of our experiments is to de-
termine if we can improve on the existing state-of-
the-art in gender classification (path-based gender
6Available at http://www.cs.ualberta.ca/?bergsma/CorefTags/
124
counts). We test systems both trained purely on
automatically-labeled data (Section 3.3), and those
that leverage some gold-standard annotations in a
semi-supervised setting (Section 3.4). Another pur-
pose of our experiments is to investigate the relative
value of our context-based features and type-based
features. We accomplish these objectives by imple-
menting and evaluating the following systems:
1. PATHGENDER:
A classifier with the four path-based gender
counts as features (Section 2).
2. PATHGENDER+:
A method of back-off to help classify unseen
nouns: For multi-token nouns (like Bob John-
son), we also include the four gender counts
aggregated over all nouns sharing the first to-
ken (Bob .*), and the four gender counts over
all nouns sharing the last token (.* Johnson).
3. CONTEXT-AUTO:
Auto-trained system using only context fea-
tures (Section 3.2.1).
4. TYPE-AUTO:
Auto-trained system using only type features
(Section 3.2.2).
5. FULL-AUTO:
Auto-trained system using all features.
6. CONTEXT-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the CONTEXT-AUTO predictions.
7. TYPE-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the TYPE-AUTO predictions.
8. FULL-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the FULL-AUTO predictions.
We evaluate using accuracy: the percentage of
labeled nouns that are correctly assigned a gender
class. As a baseline, note that always choosing
neutral achieves 38.1% accuracy on our test data.
5 Results and Discussion
5.1 Main results
Table 1 provides our experimental results. The orig-
inal gender counts already do an excellent job clas-
sifying the nouns; PATHGENDER achieves 91.0%
accuracy by looking for exact noun matches. Our
1. PATHGENDER 91.0
2. PATHGENDER+ 92.1
3. CONTEXT-AUTO 79.1
4. TYPE-AUTO 89.1
5. FULL-AUTO 92.6
6. CONTEXT-SEMI 92.4
7. TYPE-SEMI 91.3
8. FULL-SEMI 95.5
Table 1: Noun gender classification accuracy (%)
simple method of using back-off counts for the first
and last token, PATHGENDER+, achieves 92.1%.
While PATHGENDER+ uses gold standard data to
determine optimum weights on the twelve counts,
FULL-AUTO achieves 92.6% accuracy using no
gold standard training data. This confirms that our
algorithm, using no manually-labeled training data,
can produce a competitive gender classifier.
Both PATHGENDER and PATHGENDER+ do
poorly on the noun types that have low counts in
the gender database, achieving only 63% and 66%
on nouns with less than ten counts. On these
same nouns, FULL-AUTO achieves 88% perfor-
mance, demonstrating the robustness of the learned
classifier on the most difficult examples for previ-
ous approaches (FULL-SEMI achieves 94% on these
nouns).
If we break down the contribution of the two fea-
ture types in FULL-AUTO, we find that we achieve
89.1% accuracy by only using type features, while
we achieve 79.1% with only context features. While
not as high as the type-based accuracy, it is impres-
sive that almost four out of five nouns can be classi-
fied correctly based purely on the document context,
using no information about the noun itself. This is
information that has not previously been systemati-
cally exploited in gender classification models.
We examine the relationship between training
data size and accuracy by plotting a (logarithmic-
scale) learning curve for FULL-AUTO (Figure 3).
Although using four million noun groups originally
seemed sufficient, performance appears to still be in-
creasing. Since more training data can be generated
automatically, it appears we have not yet reached the
full power of the FULL-AUTO system. Of course,
even with orders of magnitude more data, the system
125
 70
 75
 80
 85
 90
 95
 100
 1000  10000  100000  1e+06  1e+07
Ac
cu
ra
cy
 (%
)
Number of training examples
Figure 3: Noun gender classification learning curve for
FULL-AUTO
does not appear destined to reach the performance
obtained through other means described below.
We achieve even higher accuracy when the output
of the -AUTO systems are combined with the orig-
inal gender counts (the semi-supervised extension).
The relative value of the context and type-based fea-
tures is now reversed: using only context-based fea-
tures (CONTEXT-SEMI) achieves 92.4%, while us-
ing only type-based features (TYPE-SEMI) achieves
91.3%. This is because much of the type informa-
tion is already implicit in the PATHGENDER counts.
The TYPE-AUTO predictions contribute little infor-
mation, only fragmenting the data and leading to
over-training and lower accuracy. On the other hand,
the CONTEXT-AUTO predictions improve accuracy,
as these scores provide orthogonal and hence helpful
information for the semi-supervised classifier.
Combining FULL-AUTO with our enhanced path
gender counts, PATHGENDER+, results in the over-
all best performance, 95.5% for FULL-SEMI, signif-
icantly better than PATHGENDER+ alone.7 This is
a 50% error reduction over the PATHGENDER sys-
tem, strongly confirming the benefit of our semi-
supervised approach.
To illustrate the importance of the unlabeled data,
we created a system that uses all features, including
the PATHGENDER+ counts, and trained this system
using only the gold standard training data. This sys-
tem was unable to leverage the extra features to im-
prove performance; its accuracy was 92.0%, roughly
equal to PATHGENDER+ alone. While SVMs work
7We evaluate significance using McNemar?s test, p<0.01.
Since McNemar?s test assumes independent classifications, we
apply the test to the classification of noun groups, not instances.
well with high-dimensional data, they simply cannot
exploit features that do not occur in the training set.
5.2 Further improvements
We can improve performance further by doing some
simple coreference before assigning gender. Cur-
rently, we only group nouns with the same string,
and then decide gender collectively for the group.
There are a few cases, however, where an ambiguous
surname, such as Willey, can only be classified cor-
rectly if we link the surname to an earlier instance of
the full name, e.g. Katherine Willey. We thus added
the following simple post-processing rule: If a noun
is classified as masculine or feminine (like the am-
biguous Willey), and it was observed earlier as the
last part of a larger noun, then re-assign the gender
to masculine or feminine if one of these is the most
common path-gender count for the larger noun. We
back off to counts for the first name (e.g. Kathleen
.*) if the full name is unobserved.
This enhancement improved the PATHGENDER
and PATHGENDER+ systems to 93.3% and 94.3%,
respectively, while raising the accuracy of our
FULL-SEMI system to 96.7%. This demonstrates
that the surname-matching post-processor is a sim-
ple but worthwhile extension to a gender predictor.8
The remaining errors represent a number of chal-
lenging cases: United States, group, and public la-
beled as plural but classified as neutral ; spectator
classified as neutral , etc. Some of these may yield
to more sophisticated joint classification of corefer-
ence and gender, perhaps along the lines of work in
named-entity classification (Bunescu and Mooney,
2004) or anaphoricity (Denis and Baldridge, 2007).
While gender has been shown to be the key fea-
ture for statistical pronoun resolution (Ge et al,
1998; Bergsma and Lin, 2006), it remains to be
seen whether the exceptional accuracy obtained here
will translate into improvements in resolution per-
formance. However, given the clear utility of gender
in coreference, substantial error reductions in gender
8One might wonder, why not provide special features so that
the system can learn how to handle ambiguous nouns that oc-
curred as sub-phrases in earlier names? The nature of our train-
ing data precludes this approach. We only include unambiguous
examples as pseudo-seeds in the learning process. Without
providing ambiguous (but labeled) surnames in some way, the
learner will not take advantage of features to help classify them.
126
assignment will likely be a helpful contribution.
6 Related Work
Most coreference and pronoun resolution papers
mention that they use gender information, but few
explain how it is acquired. Kennedy and Boguraev
(1996) use gender information produced by their en-
hanced part-of-speech tagger. Gender mistakes ac-
count for 35% of their system?s errors. Gender is
less crucial in some genres, like computer manuals;
most nouns are either neutral or plural and gender
can be determined accurately based solely on mor-
phological information (Lappin and Leass, 1994).
A number of researchers (Evans and Ora?san,
2000; Soon et al, 2001; Harabagiu et al, 2001) use
WordNet classes to infer gender knowledge. Unfor-
tunately, manually-constructed databases like Word-
Net suffer from both low coverage and rare senses.
Pantel and Ravichandran (2004) note that the nouns
computer and company both have a WordNet sense
that is a hyponym of person, falsely indicating these
nouns would be compatible with pronouns like he
or she. In addition to using WordNet classes, Soon
et al (2001) assign gender if the noun has a gen-
dered designator (like Mr. or Mrs.) or if the first
token is present on a list of common human first
names. Note that we incorporate such contextual
and categorical information (among many other in-
formation sources) automatically in our discrimina-
tive classifier, while they manually specify a few
high-precision rules for particular gender cues.
Ge et al (1998) pioneered the statistical approach
to gender determination. Like others, they consider
gender and number separately, only learning statis-
tical gender for the masculine, feminine, and neu-
tral classes. While gender and number can be han-
dled together for pronoun resolution, it might be use-
ful to learn them separately for other applications.
Other statistical approaches to English noun gender
are discussed in Section 2.
In languages with ?grammatical? gender and plen-
tiful gold standard data, gender can be tagged along
with other word properties using standard super-
vised tagging techniques (Hajic? and Hladka?, 1997).
While our approach is the first to exploit a dual
or orthogonal representation of English noun gen-
der, a bootstrapping approach has been applied to
determining grammatical gender in other languages
by Cucerzan and Yarowsky (2003). In their work,
the two orthogonal views are: 1) the context of the
noun, and 2) the noun?s morphological properties.
Bootstrapping with these views is possible in other
languages where context is highly predictive of gen-
der class, since contextual words like adjectives and
determiners inflect to agree with the grammatical
noun gender. We initially attempted a similar system
for English noun gender but found context alone to
be insufficiently predictive.
Bootstrapping is also used in general information
extraction. Brin (1998) shows how to alternate be-
tween extracting instances of a class and inducing
new instance-extracting patterns. Collins and Singer
(1999) and Cucerzan and Yarowsky (1999) apply
bootstrapping to the related task of named-entity
recognition. Our approach was directly influenced
by the hypernym-extractor of Snow et al (2005) and
we provided an analogous summary in Section 1.
While their approach uses WordNet to label hyper-
nyms in raw text, our initial labels are generated au-
tomatically. Etzioni et al (2005) also require no la-
beled data or hand-labeled seeds for their named-
entity extractor, but by comparison their classifier
only uses a very small number of both features and
automatically-generated training examples.
7 Conclusion
We have shown how noun-pronoun co-occurrence
counts can be used to automatically annotate the
gender of millions of nouns in unlabeled text. Train-
ing from these examples produced a classifier that
clearly exceeds the state-of-the-art in gender classi-
fication. We incorporated thousands of useful but
previously unexplored indicators of noun gender as
features in our classifier. By combining the pre-
dictions of this classifier with the original gender
counts, we were able to produce a gender predic-
tor that achieves 95.5% classification accuracy on
2596 test nouns, a 50% reduction in error over the
current state-of-the-art. A further name-matching
post-processor reduced error even further, resulting
in 96.7% accuracy on the test data. Our final system
is the broadest and most accurate gender model yet
created, and should be of value to many pronoun and
coreference resolution systems.
127
References
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In COLING-
ACL, pages 33?40.
Shane Bergsma. 2005. Automatic acquisition of gen-
der information for anaphora resolution. In Canadian
Conference on Artificial Intelligence, pages 342?353.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT,
pages 92?100.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In WebDB Workshop at
6th International Conference on Extending Database
Technology, pages 172?183.
Razvan Bunescu and Raymond J. Mooney. 2004. Col-
lective information extraction with relational Markov
networks. In ACL, pages 438?445.
Eugene Charniak and Micha Elsner. 2009. EM works for
pronoun anaphora resolution. In EACL.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution. In
CoNLL, pages 88?95.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In EMNLP-
VLC, pages 100?110.
Koby Crammer and Yoram Singer. 2001. On the al-
gorithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Re-
search, 2:265?292.
Silviu Cucerzan and David Yarowsky. 1999. Language
independent named entity recognition combining mor-
phological and contextual evidence. In EMNLP-VLC,
pages 90?99.
Silviu Cucerzan and David Yarowsky. 2003. Mini-
mally supervised induction of grammatical gender. In
NAACL, pages 40?47.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference using integer
programming. In NAACL-HLT, pages 236?243.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91?134.
Richard Evans and Constantin Ora?san. 2000. Improving
anaphora resolution by identifying animate entities in
texts. In DAARC, pages 154?162.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In ACL, pages 1?7.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161?171.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Jan Hajic? and Barbora Hladka?. 1997. Probabilistic and
rule-based tagger of an inflective language: a compar-
ison. In ANLP, pages 111?118.
Sanda Harabagiu, Razvan Bunescu, and Steven Maio-
rano. 2001. Text and knowledge mining for coref-
erence resolution. In NAACL, pages 55?62.
Nancy Ide and Keith Suderman. 2004. The American
National Corpus first release. In LREC, pages 1681?
84.
Christopher Kennedy and Branimir Boguraev. 1996.
Anaphora for everyone: Pronominal anaphora resolu-
tion without a parser. In COLING, pages 113?118.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):535?561.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In LREC Workshop on the Evaluation of
Parsing Systems.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In HLT-NAACL,
pages 321?328.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS, pages 1297?1304.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In ICML.
Ellen Vorhees. 2002. Overview of the TREC 2002 ques-
tion answering track. In Proceedings of the Eleventh
Text REtrieval Conference (TREC).
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL, pages
189?196.
128
  
Concept Discovery from Text 
 
Dekang Lin and Patrick Pantel 
Department of Computing Science 
University of Alberta 
Edmonton, Alberta, Canada, T6G 2E8 
{lindek,ppantel}@cs.ualberta.ca 
 
 
Abstract 
Broad-coverage lexical resources such as 
WordNet are extremely useful. However, 
they often include many rare senses while 
missing domain-specific senses. We present 
a clustering algorithm called CBC (Cluster-
ing By Committee) that automatically 
discovers concepts from text. It initially 
discovers a set of tight clusters called 
committees that are well scattered in the 
similarity space. The centroid of the 
members of a committee is used as the 
feature vector of the cluster. We proceed by 
assigning elements to their most similar 
cluster. Evaluating cluster quality has 
always been a difficult task. We present a 
new evaluation methodology that is based 
on the editing distance between output 
clusters and classes extracted from WordNet 
(the answer key). Our experiments show that 
CBC outperforms several well-known 
clustering algorithms in cluster quality. 
1 Introduction 
Broad-coverage lexical resources such as 
WordNet are extremely useful in applications 
such as Word Sense Disambiguation (Leacock, 
Chodorow, Miller 1998) and Question-
Answering (Pasca and Harabagiu 2001). 
However, they often include many rare senses 
while missing domain-specific senses. For 
example, in WordNet, the words dog, computer 
and company all have a sense that is a hyponym 
of person. Such rare senses make it difficult for 
a coreference resolution system to use WordNet 
to enforce the constraint that personal pronouns 
(e.g. he or she) must refer to a person. On the 
other hand, WordNet misses the user-interface-
object sense of the word dialog (as often used in 
software manuals). One way to deal with these 
problems is to use a clustering algorithm to 
automatically induce semantic classes (Lin and 
Pantel 2001). 
Many clustering algorithms represent a cluster 
by the centroid of all of its members (e.g., K-
means) (McQueen 1967) or by a representative 
element (e.g., K-medoids) (Kaufmann and 
Rousseeuw 1987). When averaging over all 
elements in a cluster, the centroid of a cluster 
may be unduly influenced by elements that only 
marginally belong to the cluster or by elements 
that also belong to other clusters. For example, 
when clustering words, we can use the contexts 
of the words as features and group together the 
words that tend to appear in similar contexts. For 
instance, U.S. state names can be clustered this 
way because they tend to appear in the following 
contexts: 
(List A) ___ appellate court campaign in ___ 
 ___ capital governor of ___ 
 ___ driver's license illegal in ___ 
 ___ outlaws sth. primary in ___ 
 ___'s sales tax senator for ___ 
If we create a centroid of all the state names, the 
centroid will also contain features such as: 
(List B) ___'s airport archbishop of ___ 
 ___'s business district fly to ___ 
 ___'s mayor mayor of ___ 
 ___'s subway outskirts of ___ 
because some of the state names (like New York 
and Washington) are also names of cities. 
Using a single representative from a cluster 
may be problematic too because each individual 
element has its own idiosyncrasies that may not 
be shared by other members of the cluster. 
In this paper, we propose a clustering algo-
rithm, CBC (Clustering By Committee), in 
which the centroid of a cluster is constructed by 
averaging the feature vectors of a subset of the 
cluster members. The subset is viewed as a 
committee that determines which other elements 
belong to the cluster. By carefully choosing 
committee members, the features of the centroid 
tend to be the more typical features of the target 
  
class. For example, our system chose the 
following committee members to compute the 
centroid of the state cluster: Illinois, Michigan, 
Minnesota, Iowa, Wisconsin, Indiana, Nebraska 
and Vermont. As a result, the centroid contains 
only features like those in List A. 
Evaluating clustering results is a very difficult 
task. We introduce a new evaluation methodol-
ogy that is based on the editing distance between 
output clusters and classes extracted from 
WordNet (the answer key). 
2 Previous Work 
Clustering algorithms are generally categorized 
as hierarchical and partitional. In hierarchical 
agglomerative algorithms, clusters are 
constructed by iteratively merging the most 
similar clusters. These algorithms differ in how 
they compute cluster similarity. In single-link 
clustering, the similarity between two clusters is 
the similarity between their most similar 
members while complete-link clustering uses the 
similarity between their least similar members. 
Average-link clustering computes this similarity 
as the average similarity between all pairs of 
elements across clusters. The complexity of 
these algorithms is O(n2logn), where n is the 
number of elements to be clustered (Jain, Murty, 
Flynn 1999).  
Chameleon is a hierarchical algorithm that 
employs dynamic modeling to improve 
clustering quality (Karypis, Han, Kumar 1999). 
When merging two clusters, one might consider 
the sum of the similarities between pairs of 
elements across the clusters (e.g. average-link 
clustering). A drawback of this approach is that 
the existence of a single pair of very similar 
elements might unduly cause the merger of two 
clusters. An alternative considers the number of 
pairs of elements whose similarity exceeds a 
certain threshold (Guha, Rastogi, Kyuseok 
1998). However, this may cause undesirable 
mergers when there are a large number of pairs 
whose similarities barely exceed the threshold. 
Chameleon clustering combines the two 
approaches. 
K-means clustering is often used on large data 
sets since its complexity is linear in n, the 
number of elements to be clustered. K-means is 
a family of partitional clustering algorithms that 
iteratively assigns each element to one of K 
clusters according to the centroid closest to it 
and recomputes the centroid of each cluster as 
the average of the cluster?s elements. K-means 
has complexity O(K?T?n) and is efficient for 
many clustering tasks. Because the initial 
centroids are randomly selected, the resulting 
clusters vary in quality. Some sets of initial 
centroids lead to poor convergence rates or poor 
cluster quality. 
Bisecting K-means (Steinbach, Karypis, 
Kumar 2000), a variation of K-means, begins 
with a set containing one large cluster consisting 
of every element and iteratively picks the largest 
cluster in the set, splits it into two clusters and 
replaces it by the split clusters. Splitting a cluster 
consists of applying the basic K-means 
algorithm ? times with K=2 and keeping the 
split that has the highest average element-
centroid similarity. 
Hybrid clustering algorithms combine 
hierarchical and partitional algorithms in an 
attempt to have the high quality of hierarchical 
algorithms with the efficiency of partitional 
algorithms. Buckshot (Cutting, Karger, 
Pedersen, Tukey 1992) addresses the problem of 
randomly selecting initial centroids in K-means 
by combining it with average-link clustering. 
Buckshot first applies average-link to a random 
sample of n  elements to generate K clusters. It 
then uses the centroids of the clusters as the 
initial K centroids of K-means clustering. The 
sample size counterbalances the quadratic 
running time of average-link to make Buckshot 
efficient: O(K?T?n + nlogn). The parameters K 
and T are usually considered to be small 
numbers.  
3 Word Similarity 
Following (Lin 1998), we represent each word 
by a feature vector. Each feature corresponds to 
a context in which the word occurs. For 
example, ?threaten with __? is a context. If the 
word handgun occurred in this context, the 
context is a feature of handgun. The value of the 
feature is the pointwise mutual information 
(Manning and Sch?tze 1999 p.178) between the 
feature and the word. Let c be a context and 
Fc(w) be the frequency count of a word w 
occurring in context c. The pointwise mutual 
information between c and w is defined as: 
( )
( ) ( )
N
jF
N
wF
N
wF
cw
j
c
i
i
c
mi ???
=,  
  
where N = ( )??
i j
i jF  is the total frequency 
counts of all words and their contexts. A well-
known problem with mutual information is that 
it is biased towards infrequent words/features. 
We therefore multiplied miw,c with a discounting 
factor: 
( )
( )
( ) ( )
( ) ( ) 11 +???
?
???
?
???
?
???
?
?+ ? ?
? ?
i j
ci
i j
ci
c
c
jF,wFmin
jF,wFmin
wF
wF  
We compute the similarity between two words 
wi and wj using the cosine coefficient (Salton and 
McGill 1983) of their mutual information 
vectors: 
( ) ??
?
?
?
=
c
cw
c
cw
c
cwcw
ji
ji
ji
mimi
mimi
w,wsim
22
 
4 CBC Algorithm 
CBC consists of three phases. In Phase I, we 
compute each element?s top-k similar elements. 
In our experiments, we used k = 20. In Phase II, 
we construct a collection of tight clusters, where 
the elements of each cluster form a committee. 
The algorithm tries to form as many committees 
as possible on the condition that each newly 
formed committee is not very similar to any 
existing committee. If the condition is violated, 
the committee is simply discarded. In the final 
phase of the algorithm, each element is assigned 
to its most similar cluster. 
4.1. Phase I: Find top-similar elements 
Computing the complete similarity matrix 
between pairs of elements is obviously 
quadratic. However, one can dramatically reduce 
the running time by taking advantage of the fact 
that the feature vector is sparse. By indexing the 
features, one can retrieve the set of elements that 
have a given feature. To compute the top similar 
words of a word w, we first sort w?s features 
according to their mutual information with w. 
We only compute pairwise similarities between 
w and the words that share a high mutual 
information feature with w. 
4.2. Phase II: Find committees 
The second phase of the clustering algorithm 
recursively finds tight clusters scattered in the 
similarity space. In each recursive step, the 
algorithm finds a set of tight clusters, called 
committees, and identifies residue elements that 
are not covered by any committee. We say a 
committee covers an element if the element?s 
similarity to the centroid of the committee 
exceeds some high similarity threshold. The 
algorithm then recursively attempts to find more 
committees among the residue elements. The 
output of the algorithm is the union of all 
committees found in each recursive step. The 
details of Phase II are presented in Figure 1. 
In Step 1, the score reflects a preference for 
bigger and tighter clusters. Step 2 gives 
preference to higher quality clusters in Step 3, 
where a cluster is only kept if its similarity to all 
previously kept clusters is below a fixed 
threshold. In our experiments, we set ?1 = 0.35. 
Input: A list of elements E to be clustered, a 
similarity database S from Phase I, thresh-
olds ?1 and ?2. 
Step 1: For each element e ? E 
  Cluster the top similar elements of e from S 
using average-link clustering. 
  For each cluster discovered c compute the 
following score: |c| ? avgsim(c), where 
|c| is the number of elements in c and 
avgsim(c) is the average pairwise simi-
larity between elements in c. 
  Store the highest-scoring cluster in a list L. 
Step 2: Sort the clusters in L in descending order of 
their scores. 
Step 3: Let C be a list of committees, initially 
empty. 
  For each cluster c ? L in sorted order 
  Compute the centroid of c by averaging the 
frequency vectors of its elements and 
computing the mutual information vector 
of the centroid in the same way as we did 
for individual elements.  
  If c?s similarity to the centroid of each 
committee previously added to C is be-
low a threshold ?1, add c to C. 
Step 4: If C is empty, we are done and return C. 
Step 5: For each element e ? E 
  If e?s similarity to every committee in C is 
below threshold ?2, add e to a list of resi-
dues R.   
Step 6: If R is empty, we are done and return C. 
  Otherwise, return the union of C and the 
output of a recursive call to Phase II us-
ing the same input except replacing E 
with R. 
Output: A list of committees. 
Figure 1. Phase II of CBC. 
  
Step 4 terminates the recursion if no committee 
is found in the previous step. The residue 
elements are identified in Step 5 and if no 
residues are found, the algorithm terminates; 
otherwise, we recursively apply the algorithm to 
the residue elements. 
Each committee that is discovered in this 
phase defines one of the final output clusters of 
the algorithm. 
4.3. Phase III: Assign elements to clusters 
In Phase III, every element is assigned to the 
cluster containing the committee to which it is 
most similar. This phase resembles K-means in 
that every element is assigned to its closest 
centroid. Unlike K-means, the number of 
clusters is not fixed and the centroids do not 
change (i.e. when an element is added to a 
cluster, it is not added to the committee of the 
cluster). 
5 Evaluation Methodology 
Many cluster evaluation schemes have been 
proposed. They generally fall under two 
categories: 
? comparing cluster outputs with manually 
generated answer keys (hereon referred to 
as classes); or 
? embedding the clusters in an application 
and using its evaluation measure. 
An example of the first approach considers the 
average entropy of the clusters, which measures 
the purity of the clusters (Steinbach, Karypis, 
and Kumar 2000). However, maximum purity is 
trivially achieved when each element forms its 
own cluster. An example of the second approach 
evaluates the clusters by using them to smooth 
probability distributions (Lee and Pereira 1999). 
Like the entropy scheme, we assume that there 
is an answer key that defines how the elements 
are supposed to be clustered. Let C be a set of 
clusters and A be the answer key. We define the 
editing distance, dist(C, A), as the number of 
operations required to make C consistent with A. 
We say that C is consistent with A if there is a 
one to one mapping between clusters in C and 
the classes in A such that for each cluster c in C, 
all elements of c belong to the same class in A. 
We allow two editing operations: 
? merge two clusters; and 
? move an element from one cluster to 
another. 
Let B be the baseline clustering where each 
element is its own cluster. We define the quality 
of a set of clusters C as follows: 
( )
( )ABdist
ACdist
,
,1?  
Suppose the goal is to construct a clustering 
consistent with the answer key. This measure 
can be interpreted as the percentage of 
operations saved by starting from C versus 
starting from the baseline.  
We aim to construct a clustering consistent 
with A as opposed to a clustering identical to A 
because some senses in A may not exist in the 
corpus used to generate C. In our experiments, 
we extract answer classes from WordNet. The 
word dog belongs to both the Person and Animal 
classes. However, in the newspaper corpus, the 
Person sense of dog is at best extremely rare. 
There is no reason to expect a clustering 
algorithm to discover this sense of dog. The 
baseline distance dist(B, A) is exactly the 
number of elements to be clustered. 
We made the assumption that each element 
belongs to exactly one cluster. The transforma-
tion procedure is as follows: 
1. Suppose there are m classes in the answer 
key. We start with a list of m empty sets, 
each of which is labeled with a class in the 
answer key. 
2. For each cluster, merge it with the set 
whose class has the largest number of 
elements in the cluster (a tie is broken 
arbitrarily). 
3. If an element is in a set whose class is not 
the same as one of the element?s classes, 
move the element to a set where it be-
longs. 
dist(C, A) is the number of operations performed 
using the above transformation rules on C. 
a
b
e
c
d
e
a
c
d
b e
b
a
c
d
e
a
b
c
d
e
A) B)
C) D) E)
Figure 2. An example of applying the transformation rules 
to three clusters. A) The classes in the answer key; B) the 
clusters to be transformed; C) the sets used to reconstruct 
the classes (Rule 1); D) the sets after three merge 
operations (Step 2); E) the sets after one move operation 
(Step 3). 
  
Figure 2 shows an example. In D) the cluster 
containing e could have been merged with either 
set (we arbitrarily chose the second). The total 
number of operations is 4. 
6 Experimental Results 
We generated clusters from a news corpus using 
CBC and compared them with classes extracted 
from WordNet (Miller 1990). 
6.1. Test Data 
To extract classes from WordNet, we first 
estimate the probability of a random word 
belonging to a subhierarchy (a synset and its 
hyponyms). We use the frequency counts of 
synsets in the SemCor corpus (Landes, Leacock, 
Tengi 1998) to estimate the probability of a 
subhierarchy. Since SemCor is a fairly small 
corpus, the frequency counts of the synsets in 
the lower part of the WordNet hierarchy are very 
sparse. We smooth the probabilities by assuming 
that all siblings are equally likely given the 
parent. A class is then defined as the maximal 
subhierarchy with probability less than a 
threshold (we used e-2). 
We used Minipar 1  (Lin 1994), a broad-
coverage English parser, to parse about 1GB 
(144M words) of newspaper text from the TREC 
collection (1988 AP Newswire, 1989-90 LA 
Times, and 1991 San Jose Mercury) at a speed 
of about 500 words/second on a PIII-750 with 
512MB memory. We collected the frequency 
counts of the grammatical relationships 
(contexts) output by Minipar and used them to 
compute the pointwise mutual information 
values from Section 3. The test set is constructed 
by intersecting the words in WordNet with the 
nouns in the corpus whose total mutual 
information with all of its contexts exceeds a 
threshold m. Since WordNet has a low coverage 
of proper names, we removed all capitalized 
nouns. We constructed two test sets: S13403 
consisting of 13403 words (m = 250) and S3566 
consisting of 3566 words (m = 3500). We then 
removed from the answer classes the words that 
did not occur in the test sets. Table 1 summa-
rizes the test sets. The sizes of the WordNet 
classes vary a lot. For S13403 there are 99 classes 
that contain three words or less and the largest 
class contains 3246 words. For S3566, 78 classes 
have three or less words and the largest class 
contains 1181 words. 
                                                 
1Available at www.cs.ualberta.ca/~lindek/minipar.htm. 
6.2. Cluster Evaluation 
We clustered the test sets using CBC and the 
clustering algorithms of Section 2 and applied 
the evaluation methodology from the previous 
section. Table 2 shows the results. The columns 
are our editing distance based evaluation 
measure. Test set S3566 has a higher score for all 
algorithms because it has a higher number of 
average features per word than S13403. 
For the K-means and Buckshot algorithms, we 
set the number of clusters to 250 and the 
maximum number of iterations to 8. We used a 
sample size of 2000 for Buckshot. For the 
Bisecting K-means algorithm, we applied the 
basic K-means algorithm twice (? = 2 in Section 
2) with a maximum of 8 iterations per split. Our 
implementation of Chameleon was unable to 
complete clustering S13403 in reasonable time due 
to its time complexity.  
Table 2 shows that K-means, Buckshot and 
Average-link have very similar performance. 
CBC outperforms all other algorithms on both 
data sets.  
6.3. Manual Inspection 
Let c be a cluster and wn(c) be the WordNet 
class that has the largest intersection with c. The 
precision of c is defined as: 
Table 1. A description of the test sets in our experiments.
DATA 
SET 
TOTAL 
WORDS 
m Average # 
of Features 
TOTAL 
CLASSES 
S13403 13403 250 740.8 202 
S3566 3566 3500 2218.3 150 
 
DATA 
SET 
TOTAL 
WORDS 
M Avg. Features 
per Word 
13403 250 740.8 
3566 3500 2218.3 
Table 2. Cluster quality (%) of several clustering 
algorithms on the test sets. 
ALGORITHM S13403 S3566 
CBC 60.95 65.82 
K-means (K=250) 56.70 62.48 
Buckshot 56.26 63.15 
Bisecting K-means  43.44 61.10 
Chameleon n/a 60.82 
Average-link 56.26 62.62 
Complete-link 49.80 60.29 
Single-link 20.00 31.74 
 
  
( ) ( )
c
cwnc
cprecision
?=  
CBC discovered 943 clusters. We sorted them 
according to their precision. Table 3 shows five 
of the clusters evenly distributed according to 
their precision ranking along with their Top-15 
features with highest mutual-information. The 
words in the clusters are listed in descending 
order of their similarity to the cluster centroid. 
For each cluster c, we also include wn(c). The 
underlined words are in wn(c). The first cluster 
is clearly a cluster of firearms and the second 
one is of pests. In WordNet, the word pest is 
curiously only under the person hierarchy. The 
words stopwatch and houseplant do not belong 
to the clusters but they have low similarity to 
their cluster centroid. The third cluster 
represents some kind of control. In WordNet, the 
legal power sense of jurisdiction is not a 
hyponym of social control as are supervision, 
oversight and governance. The fourth cluster is 
about mixtures. The words blend and mix as the 
event of mixing are present in WordNet but not 
as the result of mixing. The last cluster is about 
consumers. Here is the consumer class in 
WordNet 1.5: 
addict, alcoholic, big spender, buyer, client, 
concert-goer, consumer, customer, cutter, diner, 
drinker, drug addict, drug user, drunk, eater, 
feeder, fungi, head, heroin addict, home buyer, 
junkie, junky, lush, nonsmoker, patron, policy-
holder, purchaser, reader, regular, shopper, 
smoker, spender, subscriber, sucker, taker, user, 
vegetarian, wearer 
In our cluster, only the word client belongs to 
WordNet?s consumer class. The cluster is ranked 
very low because WordNet failed to consider 
words like patient, tenant and renter as 
consumers.  
Table 3 shows that even the lowest ranking 
CBC clusters are fairly coherent. The features 
associated with each cluster can be used to 
classify previously unseen words into one or 
more existing clusters. 
Table 4 shows the clusters containing the word 
cell that are discovered by various clustering 
algorithms from S13403. The underlined words 
represent the words that belong to the cell class 
in WordNet. The CBC cluster corresponds 
almost exactly to WordNet?s cell class. K-means 
and Buckshot produced fairly coherent clusters. 
The cluster constructed by Bisecting K-means is 
obviously of inferior quality. This is consistent 
with the fact that Bisecting K-means has a much 
lower score on S13403 compared to CBC, K-
means and Buckshot. 
Table 3. Five of the 943 clusters discovered by CBC from S13403 along with their features with top-15 highest mutual 
information and the WordNet classes that have the largest intersection with each cluster. 
RANK MEMBERS TOP-15 FEATURES wn(c) 
1 handgun, revolver, shotgun, pistol, rifle, 
machine gun, sawed-off shotgun, 
submachine gun, gun, automatic pistol, 
automatic rifle, firearm, carbine, 
ammunition, magnum, cartridge,  
automatic, stopwatch 
__ blast, barrel of __ , brandish __, fire __, point __, 
pull out __, __ discharge, __ fire, __ go off, arm with 
__, fire with __, kill with __, open fire with __, shoot 
with __, threaten with __ 
artifact / artifact 
236 whitefly, pest, aphid, fruit fly, termite, 
mosquito, cockroach, flea, beetle, killer 
bee, maggot, predator, mite, houseplant, 
cricket 
__ control, __ infestation, __ larvae, __ population, 
infestation of __, specie of __, swarm of __ , attract 
__, breed __, eat __, eradicate __, feed on __, get rid 
of __, repel __, ward off __ 
animal / animate being / 
beast / brute / creature / 
fauna 
471 supervision, discipline, oversight, 
control, governance, decision making, 
jurisdiction 
breakdown in __, lack of __ , loss of __, assume __, 
exercise __, exert __, maintain __, retain __, seize __, 
tighten __, bring under __, operate under __, place 
under __, put under __, remain under __ 
act / human action / 
human activity 
706 blend, mix, mixture, combination, 
juxtaposition, combine, amalgam, 
sprinkle, synthesis, hybrid, melange 
dip in __, marinate in __, pour in __, stir in __, use in 
__, add to __, pour __, stir __, curious __, eclectic __, 
ethnic __, odd __, potent __, unique __, unusual __ 
group / grouping 
941 employee, client, patient, applicant,  
tenant, individual, participant, renter, 
volunteer, recipient, caller, internee, 
enrollee, giver 
benefit for __, care for __, housing for __, benefit to 
__, service to __, filed by __, paid by __, use by __, 
provide for __, require for --, give to __, offer to __, 
provide to __, disgruntled __, indigent __ 
worker 
 
  
7 Conclusion 
We presented a clustering algorithm, CBC, for 
automatically discovering concepts from text. It 
can handle a large number of elements, a large 
number of output clusters, and a large sparse 
feature space. It discovers clusters using well-
scattered tight clusters called committees. In our 
experiments, we showed that CBC outperforms 
several well known hierarchical, partitional, and 
hybrid clustering algorithms in cluster quality. 
For example, in one experiment, CBC 
outperforms K-means by 4.25%. 
By comparing the CBC clusters with WordNet 
classes, we not only find errors in CBC, but also 
oversights in WordNet. 
Evaluating cluster quality has always been a 
difficult task. We presented a new evaluation 
methodology that is based on the editing 
distance between output clusters and classes 
extracted from WordNet (the answer key). 
Acknowledgements 
The authors wish to thank the reviewers for their 
helpful comments. This research was partly 
supported by Natural Sciences and Engineering 
Research Council of Canada grant OGP121338 
and scholarship PGSB207797. 
References 
Cutting, D. R.; Karger, D.; Pedersen, J.; and Tukey, J. W. 1992. 
Scatter/Gather: A cluster-based approach to browsing large 
document collections. In Proceedings of SIGIR-92. pp. 318?329. 
Copenhagen, Denmark. 
Guha, S.; Rastogi, R.; and Kyuseok, S. 1999. ROCK: A robust 
clustering algorithm for categorical attributes. In Proceedings of 
ICDE?99. pp. 512?521. Sydney, Australia. 
Jain, A. K.; Murty, M. N.; and Flynn, P. J. 1999. Data Clustering: A 
Review. ACM Computing Surveys 31(3):264?323. 
Kaufmann, L. and Rousseeuw, P. J. 1987. Clustering by means of 
medoids. In Dodge, Y. (Ed.) Statistical Data Analysis based on the 
L1 Norm. pp. 405?416. Elsevier/North Holland, Amsterdam. 
Karypis, G.; Han, E.-H.; and Kumar, V. 1999. Chameleon: A 
hierarchical  clustering algorithm using dynamic modeling. IEEE 
Computer: Special Issue on Data Analysis and Mining 32(8):68?75. 
Landes, S.; Leacock, C.; and Tengi, R. I. 1998. Building Semantic 
Concordances. In WordNet: An Electronic Lexical Database, edited 
by C. Fellbaum. pp. 199-216. MIT Press. 
Leacock, C.; Chodorow, M.; and Miller; G. A. 1998. Using corpus 
statistics and WordNet relations for sense identification. 
Computational Linguistics, 24(1):147-165. 
Lee, L. and Pereira, F. 1999. Distributional similarity models: 
Clustering vs. nearest neighbors. In Proceedings of ACL-99. pp. 33-
40. College Park, MD. 
Lin, D. 1994. Principar - an Efficient, Broad-Coverage, Principle-Based 
Parser. In Proceedings of COLING-94. pp. 42-48. Kyoto, Japan. 
Lin, D. 1998. Automatic retrieval and  clustering of similar words. In 
Proceedings of COLING/ACL-98. pp. 768-774. Montreal, Canada. 
Lin, D. and Pantel, P. 2001. Induction of semantic classes from natural 
language text. In Proceedings of SIGKDD-01. pp. 317-322. San 
Francisco, CA. 
Manning, C. D. and Sch?tze, H. 1999. Foundations of Statistical 
Natural Language Processing. MIT Press. 
McQueen, J. 1967. Some methods for classification and analysis of 
multivariate observations. In Proceedings of 5th Berkeley Symposium 
on Mathematics, Statistics and Probability, 1:281-298. 
Miller, G. 1990. WordNet: An Online Lexical Database. International 
Journal of Lexicography, 1990. 
Pasca, M. and Harabagiu, S. 2001. The informative role of WordNet in 
Open-Domain Question Answering. In Proceedings of NAACL-01 
Workshop on WordNet and Other Lexical Resources. pp. 138-143. 
Pittsburgh, PA. 
Salton, G. and McGill, M. J. 1983. Introduction to Modern Information 
Retrieval. McGraw Hill. 
Steinbach, M.; Karypis, G.; and Kumar, V. 2000. A comparison of 
document clustering techniques. Technical Report #00-034. 
Department of Computer Science and Engineering, University of 
Minnesota.s 
Table 4. The clusters representing the cell concept for several clustering algorithms using S13403. 
ALGORITHMS CLUSTERS THAT HAVE THE LARGEST INTERSECTION WITH THE WORDNET CELL CLASS. 
CBC white blood cell, red blood cell, brain cell, cell, blood cell, cancer cell, nerve cell, embryo, neuron 
K-means cadaver, meteorite, secretion, receptor, serum, handwriting, cancer cell, thyroid, body part, hemoglobin, red blood 
cell, nerve cell, urine, gene, chromosome, embryo, plasma, heart valve, saliva, ovary, white blood cell, intestine, 
lymph node, sperm, heart, colon, cell, blood, bowel, brain cell, central nervous system, spinal cord, blood cell, 
cornea, bladder, prostate, semen, brain, spleen, organ, nervous system, pancreas, tissue, marrow, liver, lung, 
marrow, kidney 
Buckshot cadaver, vagina, meteorite, human body, secretion, lining, handwriting, cancer cell, womb, vein, bloodstream, 
body part, eyesight, polyp, coronary artery, thyroid, membrane, red blood cell, plasma, gene, gland, embryo, 
saliva, nerve cell, chromosome, skin, white blood cell, ovary, sperm, uterus, blood, intestine, heart, spinal cord, 
cell, bowel, colon, blood vessel, lymph node, brain cell, central nervous system, blood cell, semen, cornea, 
prostate, organ, brain, bladder, spleen, nervous system, tissue, pancreas, marrow, liver, lung, bone marrow, kidney 
Bisecting K-means picket line, police academy, sphere of influence, bloodstream, trance, sandbox, downtown, mountain, camera, 
boutique, kitchen sink, kiln, embassy, cellblock, voting booth, drawer, cell, skylight, bookcase, cupboard, 
ballpark, roof, stadium, clubhouse, tub, bathtub, classroom, toilet, kitchen, bathroom, 
WordNet Class blood cell, brain cell, cancer cell, cell, cone, egg, nerve cell, neuron, red blood cell, rod, sperm, white blood cell 
 
A Path-based Transfer Model for Machine Translation 
Dekang Lin 
Department of Computing Science  
University of Alberta 
Edmonton, Alberta, Canada 
lindek@cs.ualberta.ca 
 
 
Abstract 
We propose a path-based transfer model for 
machine translation. The model is trained with 
a word-aligned parallel corpus where the 
source language sentences are parsed. The 
training algorithm extracts a set of transfer 
rules and their probabilities from the training 
corpus. A rule translates a path in the source 
language dependency tree into a fragment in 
the target dependency tree. The problem of 
finding the most probable translation becomes 
a graph-theoretic problem of finding the 
minimum path covering of the source 
language dependency tree. 
1 Introduction 
Given a source language sentence S, a statistical 
machine translation (SMT) model translates it by 
finding the target language sentence T such that the 
probability P(T|S) is maximized. In word-based 
models, such as IBM Model 1-5 (Brown et al
1993), the probability P(T|S) is decomposed into 
statistical parameters involving words. There have 
been many recent proposals to improve translation 
quality by decomposing P(T|S) into probabilities 
involving phrases.  
Phrase-based SMT approaches can be classified 
into two categories. One type of approach works 
with parse trees. In (Yamada&Knight 2001), for 
example, the translation model applies three 
operations (re-order, insert, and translate) to an 
English parse tree to produce its Chinese 
translation. A parallel corpus of English parse trees 
and Chinese sentences are used to obtain the 
probabilities of the operations.  
In the second type of phrase-based SMT models, 
phrases are defined as a block in a word aligned 
corpus such that words within the block are aligned 
with words inside the block (Och et al1999, 
Marcu&Wong 2002). This definition will treat as 
phrases many word sequences that are not 
constituents in parse trees. This may look 
linguistically counter-intuitive. However, (Koehn 
et al2003) found that it is actually harmful to 
restrict phrases to constituents in parse trees, 
because the restriction would cause the system to 
miss many reliable translations, such as the 
correspondence between ?there is? in English and 
?es gibt? (?it gives?) in German. 
In this paper, we present a path-based transfer 
model for machine translation. The model is 
trained with a word-aligned parallel corpus where 
the source language side consists of dependency 
trees. The training algorithm extracts a set of paths 
from the dependency trees and determines the 
translations of the paths using the word alignments. 
The result of the training process is a set of rules 
for translating paths in the source language into 
tree fragments in the target language with certain 
probabilities. To translate a sentence, we first parse 
it and extract a set of paths from its dependency 
tree S. We then find a set of transfer rules that 
cover S and produce a set of tree fragments 
obtained to form a tree T* such that T*=argmaxT 
P(T|S). The output sentence can then simply be 
read off T*. 
In the remainder of the paper, we first define 
paths in dependency trees. We then describe an 
algorithm for learning transfer rules and their 
probabilities. The translation algorithm is 
presented in Section 4. Experimental result is 
presented in Section 5. We then discuss related 
work in Section 6. 
2 Paths in Dependency Trees 
The dependency tree of a sentence consists of a set 
of nodes, each of which corresponds to a word in 
the sentence. A link in the tree represents a 
dependency relationship between a pair of words. 
The links are directed from the head towards the 
modifier. Except the root of tree, every node has 
exactly one incoming link. An example 
dependency tree is shown in Fig. 1. 
John found a solution to the problem.
det detsubj
obj to
 
Figure 1. An example dependency tree 
A sequence of nodes n1, ?, nk, ? nm and the 
dependency links between them form a path if the 
following conditions hold:  
a. ? i (1? i < k), there is a link from ni+1 to ni. 
b. ? i (k? i < m), there is a link from ni to ni+1. 
A set of paths is said to cover a dependency tree 
if the union of the nodes and links in the set of 
paths include all of the nodes and links in the 
dependency tree. 
3 Acquisition of Transfer Rules 
A transfer rule specifies how a path in the source 
language dependency tree is translated. We extract 
transfer rules automatically from a word-aligned 
corpus. For example, Fig. 2(b-g) are some of the 
rules extracted from the word aligned sentence in 
Fig. 2(a). The left hand side of a rule is a path in 
the source dependency tree. The right hand side of 
a rule is a fragment of a dependency tree in the 
target language. It encodes not only the 
dependency relations, but also the relative linear 
order among the nodes in the fragment. For 
example, the rule in Fig. 2(e) specifies that when 
the path Connect? to?controller is translated 
into French Branchez precedes (but not necessarily 
adjacent to) sur, and sur precedes (but not 
necessarily adjacent to) contr?leur.  
Note that the transfer rules also contain word-to-
word mapping between the nodes in the source and 
the target (obtained from word alignments). These 
mappings are not shown in order not to clutter the 
diagrams.  
Connect cables to controller Branchez les c?bles sur contr?leur
Connect to controller Branchez sur contr?leur
Connect cables Branchez les c?bles
power cables c?bles d' alimentation
both cables deux c?bles
Connect both power cables to the controller
Branchez les deux c?bles d' alimentation sur le contr?leur
(a)
(b)
(c)
(d)
(e)
(f)
1 2 3 4 5 6 7 8 9
Connect to the controller Branchez sur le contr?leur
(g)
 
Figure 2. Examples of transfer rules extracted 
from a word-aligned corpus 
3.1 Spans 
The rule extraction algorithm makes use of the 
notion of spans (Fox 2002, Lin&Cherry 2003). 
Given a word alignment and a node n in the source 
dependency tree, the spans of n induced by the 
word alignment are consecutive sequences of 
words in the target sentence. We define two types 
of spans: 
Head span: the word sequence aligned with the 
node n. 
Phrase span: the word sequence from the lower 
bound of the head spans of all nodes in the 
subtree rooted at n to the upper bound of the 
same set of spans. 
For example, the spans of the nodes in Fig. 2(a) are 
listed in Table 1. We used the word-alignment 
algorithm in (Lin&Cherry 2003a), which enforces 
a cohesion constraint that guarantees that if two 
spans overlap one must be fully contained in the 
other. 
Table 1. Spans of nodes in Figure 2(a) 
Node Head Span Phrase Span 
Connect [1,1] [1,9] 
both [3,3] [3,3] 
power [6,6] [6,6] 
cables [4,4] [3,6] 
to  [8,9] 
the [8,8] [8,8] 
controller [9,9] [8,9] 
 
3.2 Rule-Extraction Algorithm 
For each word-aligned dependency tree in the 
training corpus, we extract all the paths where all 
the nodes are aligned with words in the target 
language sentence, except that a preposition in the 
middle of a path is allowed to be unaligned. In the 
dependency tree in Fig. 2(a), we can extract 21 
such paths, 6 of which are single nodes 
(degenerated paths). 
We first consider the translation of simple paths 
which are either a single link or a chain of two 
links with the middle node being an unaligned 
preposition. An example of the latter case is the 
path Connect?to?controller in Fig. 2(a). In such 
cases, we treat the two dependency link as if it is a 
single link (e.g., we call ?Connect? the parent of 
?controller?).  
Suppose Si is a simple path from node h to node 
m. Let h' and m' be target language words aligned 
with h and m respectively. Let s be the phrase span 
of a sibling of m that is located in between h? and 
m? and is the closest to m? among all such phrase 
spans. If m does not have such a sibling, let s be 
the head span of h. 
The translation Ti of Si consists of the following 
nodes and links: 
? Two nodes labeled h' and m', and a link from h' 
to m'. 
? A node corresponding to each word between s 
and the phrase span of m and a link from each 
of these nodes to m?. 
Fig. 2(b-e) are example translations constructed 
this way. The following table lists the words h' and 
m' and the span s in these instances: 
Table 2. Example spans 
Example h' m' s 
Figure 2(b) c?bles deux [4,4]
Figure 2(c) c?bles alimention [4,4]
Figure 2(d) Branchez c?bles [1,1]
Figure 2(e) Branchez contr?leur [4,6]
In general, a path is either a single node, or a 
simple path, or a chain of simple paths. The 
translations of single nodes are determined by the 
word alignments. The translation of a chain of 
simple paths can be obtained by chaining the 
translations of the simple paths. Fig. 2(f) provides 
an example. 
Note that even though the target of a rule is 
typically a path, it is not necessarily the case (e.g., 
Fig. 2(g)). Our rule extraction algorithm guarantees 
the following property of target tree fragments: if a 
node in a target tree fragment is not aligned with a 
node in the source path, it must be a leaf node in 
the tree fragment.  
3.3 Generalization of Rules 
In addition to the rules discussed the in the 
previous subsection, we also generalize the rules 
by replacing one of the end nodes in the path with 
a wild card and the part of speech of the word. For 
example the rule in Fig. 2(b) can be generalized in 
two ways. The generalized versions of the rule 
apply to any determiner modifying cable and both 
modifying any noun, respectively.  
*/Det cables */Det c?bles
both */N deux */N
both cables deux c?bles
generalize
 
Figure 3. Generalization of Transfer rule 
3.4 Translation Probability 
Let Si be a path in the source language dependency 
tree and Ti be a tree fragment in the target 
language. The translation probability P(Ti|Si) can 
be computed as  
( ) ( )( ) MSc
STcSTP
i
ii
ii +=
,|   
where c(Si) is the count of Si in the training corpus, 
c(Ti,Si) is the number of times Ti is the translation 
of Si, and M is a smoothing constant. 
4 Path-based Translation 
Given a source language sentence, it is translated 
into the target language in the following steps: 
Step 1: Parse the sentence to obtain its dependency 
structure. 
Step 2: Extract all the paths in the dependency tree 
and retrieve the translations of all the paths. 
Step 3: Find a set of transfer rules such that 
a) They cover the whole dependency tree. 
b) The tree fragments in the rules can be 
consistently merged into a target language 
dependency tree. 
c) The merged tree has the highest probability 
among all the trees satisfying the above 
conditions. 
Step 4: Output the linear sequence of words in the 
dependency tree.  
4.1 Merging Tree Fragments 
In Step 3 of our algorithm, we need to merge the 
tree fragments obtained from a set of transfer rules 
into a single dependency tree. For example, the 
mergers of target tree fragments in Fig. 4(b-d) 
result in the tree in Fig. 4(e). Since the paths in 
these rules cover the dependency tree in Fig. 4(a), 
Fig. 4(e) is a translation of Fig. 4(a). The merger of 
target tree fragments is constrained by the fact that 
if two target nodes in different fragments are 
mapped to the same source node, they must be 
merged into a single node.  
Proposition 1: The merger of two target tree 
fragments does not contain a loop. 
Proof: The unaligned nodes in each tree fragment 
will not be merged with another node. They have 
degree 1 in the original tree fragment and will still 
have degree 1 after the merger. If there is a loop in 
the merged graph, the degree of a node on the loop 
is at least 2. Therefore, all of the nodes on the loop 
are aligned nodes. This implies that there is a loop 
in the source dependency tree, which is clearly 
false.  
Proposition 2: If the condition parts of a set of 
transfer rules cover the input dependency tree, the 
merger of the right hand side of the rules is a tree. 
Proof: To prove it is a tree, we only need to prove 
that it is connected since Proposition 1 guarantees 
that there is no loop. Consider the condition part of 
a rule, which is a path A in the source dependency 
tree. Let r be the node in the path that is closest to 
the root node of the tree. If r is not the root node of 
the tree, there must exist another path B that covers 
the link between r and its parent. The paths A and 
B map r to the same target language node. 
Therefore, the target language tree fragments for A 
and B are connected. Using mathematical 
induction, we can establish that all the tree 
fragments are connected. 
The above two propositions establish the fact 
that the merge the tree fragments form a tree 
structure. 
(a)
(b)
(c)
(d)
existing cables c?bles existants
both cables deux c?bles
coaxial cables c?bles coaxiaux
both existing coaxial cables
deux c?bles coaxiaux existants
(e)
 
Figure 4. Examples of word ordering 
4.2 Node Ordering 
For each node in the merged structure, we must 
also determine the ordering of among it and its 
children. If a node is present in only one of the 
original tree fragments, the ordering between it and 
its children will be the same as the tree fragment. 
Suppose a node h is found in two tree fragments. 
For the children of h that come from the same 
fragment, their order is already specified. If two 
children m1 and m2 come from different fragments, 
we determine their order as follows: 
? If m1 and m2 are on different sides of h in their 
original fragments, their order can be inferred 
from their positions relative to h. For example, 
the combination of the rules in Fig. 4(b) and 
Fig. 4(c) translate both existing cables into 
deux c?bles existants. 
? If m1 and m2 are on the same side of h and their 
source language counterparts are also on the 
same side of h, we maintain their relative 
closeness to the parent nodes: whichever word 
was closer to the parent in the source remains 
to be closer to the parent in the target. For 
example, the combination of the rules in Fig. 
4(c) and Fig. 4(d) translates existing coaxial 
cables into c?bles coaxiaux existants.  
? If m1 and m2 are on the same side of h but their 
source language counterpart are on different 
sides of h, we will use the word order of their 
original in the source language.  
4.3 Conflicts in Merger 
Conflicts may arise when we merge tree fragments. 
Consider the two rules in Fig. 5. The rule in Fig. 
5(a) states that when the word same is used to 
modify a noun, it is translated as m?me and appears 
after the noun. The rule in Fig. 5(b) states that 
same physical geometry is translated into 
g?om?trie physique identique. When translating the 
sentence in Fig. 5(c), both of these rules can be 
applied to parts of the tree. However, they cannot 
be used at the same time as they translate same to 
different words and place them on different 
location. 
same */N */N m?me
same physical geometry g?om?trie physique identique
(a)
(b)
the disks have the same physical geometry
(c)
 Figure 5. Example Conflicts 
4.4 Probabilistic Model 
Our translation model is a direct translation model 
as opposed to a noisy channel model which is 
commonly employed in statistical machine 
translation. Given the dependency tree S of a 
source language sentence, the probability of the 
target dependency tree T, P(T|S),  is computed by 
decomposing it into a set of path translations: ( ) ( )?
?
=
CS
ii
i
STPSTP |max|
C
 
where C is a set of paths covering S; Si?s are paths 
in C; Ti?s are possible translations for the 
corresponding Si?s and T is the merger of all Ti?s. 
Note that the paths in C are allowed to overlap. 
However, no path should be totally contained in 
another, as we can always remove the shorter path 
to increase the probability without compromising 
the total coverage of C. 
4.5 Graph-theoretic Formulation 
If we ignore the conflict in merging tree fragments 
and assign the weight -log P(Ti|Si) to the path Si,  
the problem of finding the most probable 
translation can be formulated as the following 
graph theory problem:  
Given a tree and a collection of paths in the tree 
where each path is assigned a weight. Find a 
subset of the paths such that they cover all the 
nodes and edges in the tree and have the 
minimum total weight. 
We call this problem the Minimum Path 
Covering of Trees. A closely related problem is 
the Minimum Set Covering Problem: 
Given a collection F of subset set of a given set 
X, find a minimum-cardinality subcollection C 
of F such that the union of the subsets in C is X. 
Somewhat surprisingly, while the Minimum Set 
Covering Problem is a very well-known NP-
Complete problem, the problem of Minimum Path 
Covering of Trees has not previously been studied. 
It is still an open problem whether this problem is 
NP-Complete or has a polynomial solution. 
If we assume that the number of paths covering 
any particular node is bounded by a constant, there 
exists a dynamic programming algorithm with 
O(n) complexity where n is the size of the tree 
(Lin&Lin, 2004). In the machine translation, this 
seems to be a reasonable assumption. 
5 Experimental Results 
We implemented a path-based English-to-French 
MT system. The training corpus consists of the 
English-French portion of the 1999 European 
Parliament Proceedings1 (Koehn 2002). It consists 
of 116,889 pairs of sentences (3.4 million words). 
As in (Koehn, et. al. 2003), 1755 sentences of 
length 5-15 were used for testing. We parsed the 
English side of the corpus with Minipar2 (Lin 
2002). We then performed word-align on the 
parsed corpus with the ProAlign system 
(Cherry&Lin 2003, Lin&Cherry 2003b). 
From the training corpus, we extracted 
2,040,565 distinct paths with one or more 
translations. The BLEU score of our system on the 
test data is 0.2612. Compared with the English to 
French results in (Koehn et. al. 2003), this is 
higher than the IBM Model 4 (0.2555), but lower 
than the phrasal model (0.3149). 
6 Related Work and Discussions 
6.1 Transfer-based MT 
Both our system and transfer-based MT systems 
take a parse tree in the source language and 
translate it into a parse tree in the target language 
with transfer rules. There have been many recent 
proposals to acquire transfer rules automatically 
from word-aligned corpus (Carbonell et al2002, 
Lavoie et al2002, Richardson et al2001). There 
are two main differences between our system and 
previous transfer-based approach: the unit of 
transfer and the generation module. 
The units of transfer in previous transfer based 
approach are usually subtrees in the source 
                                                     
1 http://www.isi.edu/~koehn/europarl/ 
2 http://www.cs.ualberta.ca/~lindek/minipar.htm 
language parse tree. While the number of subtrees 
of a tree is exponential in the size of the tree, the 
number of paths in a tree is quadratic. The reduced 
number of possible transfer units makes the data 
less sparse. 
The target parse tree in a transfer-based system 
typically does not include word order information. 
A separate generation module, which often 
involves some target language grammar rules, is 
used to linearize the words in the target parse tree. 
In contrast, our transfer rules specify linear order 
among nodes in the rule. The ordering among 
nodes in different rules is determined with a couple 
of simply heuristics. There is no separate 
generation module and we do not need a target 
language grammar. 
6.2 Translational Divergence 
The Direct Correspondence Assumption (DCA) 
states that the dependency tree in source and target 
language have isomorphic structures (Hwa et. al. 
2002).  DCA is often violated in the presence of 
translational divergence. It has been shown in 
(Habash&Dorr 2002) that translational divergences 
are quite common (as much as 35% between 
English and Spanish). For example, Fig. 6(a) is a 
Head Swapping Divergence. 
Even though we map the dependency tree in the 
source language into a dependency tree in the 
target language, we are using a weaker assumption 
than DCA. We induce a target language structure 
using a source language structure and the word 
alignment. There is no guarantee that this target 
language dependency tree is what a target language 
linguist would construct. For example, derived 
dependency tree for ?X cruzar Y nadando? is 
shown in Fig. 6(b). Even though it is not a correct 
dependency tree for Spanish, it does generate the 
correct word order. 
X swim across Y
X cruzar Y nadando X cruzar Y nadando
(a) (b)
X cross   Y swimming
 
Figure 6. Translational Divergence 
7 Conclusion and Future Work 
We proposed a path-based transfer model for 
machine translation, where the transfer rules are 
automatically acquired from a word-aligned 
parallel corpus. The problem of finding the most 
probable translation is formulated as a graph-
theoretic problem of finding the minimum path 
covering of the source language dependency tree. 
8 Acknowledgements 
This research is supported by NSERC and Sun 
Microsystems, Inc. 
References  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: 
Parameter estimation. Computational Linguistics, 
19(2):263?311. 
Colin Cherry and Dekang Lin, 2003. A Probability 
Model to Improve Word Alignment. In Proceedings 
of ACL-03. pp.88-95. Sapporo, Japan. 
Heidi J. Fox. 2002. Phrasal cohesion and statistical 
machine translation. In Proceedings of EMNLP-02, 
pages 304-311. Philadelphia, PA. 
Habash, Nizar and Bonnie J. Dorr, 2002. Handling 
Translation Divergences: Combining Statistical and 
Symbolic Techniques in Generation-Heavy Machine 
Translation, In Proceedings of the Fifth Conference 
of the Association for Machine Translation in the 
Americas, AMTA-2002,Tiburon, CA. 
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002. 
Evaluating Translational Correspondence using   
Annotation Projection. In the Proceedings of the 40th 
Annual Meeting of the ACL, Philadelphia, PA. 
Philipp Koehn. 2002. Europarl: A Multilingual Corpus 
for Evaluation of Machine Translation. Unpublished 
Draft. 
http://www.isi.edu/~koehn/publications/europarl.ps 
Philipp Koehn, Franz Josef Och, and Daniel Marcu, 
2003. Statistical Phrase-Based Translation, In 
Proceedings of HLT/NAACL 2003 pp. 127-133, 
Edmonton, Canada. 
Lavoie, Benoit; White, Michael; and Korelsky, Tanya 
2002. Learning Domain-Specific Transfer Rules: An 
Experiment with Korean to English Translation. In 
Proceedings of the COLING 2002 Workshop on 
Machine Translation in Asia, Taipei, Taiwan, pp. 60-
66. 
Dekang Lin and Colin Cherry, 2003a. Word Alignment 
with Cohesion Constraint. In Proceedings of 
HLT/NAACL 2003. Companion Volume, pp. 49-51, 
Edmonton, Canada. 
Dekang Lin and Colin Cherry, 2003b. ProAlign: Shared 
Task System Description. In Proceedings of the 
Workshop on Building and Using Parallel Texts, pp. 
11-14. Edmonton, Canada. 
Guohui Lin and Dekang Lin. 2004. Minimum Path 
Covering of Trees. Submitted to Information 
Processing Letters. 
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical 
Machine Translation. Proceedings of the Conference 
on EMNLP-2002, pp.133-139. Philadelphia, PA. 
Franz Josef Och, Christoph Tillmann, Hermann Ney, 
1999. Improved Alignment Models for Statistical 
Machine Translation. pp. 20-28; In Proceedings of 
EMNLP-99. University of Maryland, College Park, 
MD. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL), pages 311?318. 
Steve Richardson, W. Dolan, A. Menezes, and J. 
Pinkham. 2001. Achieving commercial-quality 
translation with example-based methods. In 
Proceedings of MT Summit VIII, Santiago De 
Compostela, Spain, pp. 293-298. 
Kenji Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation model. In Proceedings of the 
39th Annual Meeting of the Association for 
Computational Linguistics (ACL?01), Toulouse, 
France. 
 
LaTaT: Language and Text Analysis Tools 
Dekang Lin  
University of Alberta 
Department of Computing Science 
Edmonton, Alberta T6H 2E1 Canada 
lindek@cs.ualberta.ca 
 
 
ABSTRACT 
LaTaT is a Language and Text Analysis Toolset. This paper gives 
a brief description of the components comprising LaTaT, 
including a Minimalist parser and language and concept learning 
programs.  
1. INTRODUCTION 
In natural language processing, syntactic and semantic knowledge 
are deeply intertwined with each other, both in their acquisition 
and usage. The goal of our research is to build a syntactic and 
semantic knowledge base through an iterative process that 
involves both language processing and language acquisition. We 
start the process by parsing a large corpus with a manually 
constructed parser that has only syntactic knowledge. We then 
extract lexical semantic and statistical knowledge from the parsed 
corpus, such as similar words and phrases, collocations and 
idiomatic expressions, and selectional preferences. In the second 
cycle, the text corpus is parsed again with the assistance of the 
newly acquired semantic and statistical knowledge, which allows 
the parser to better resolve systematic syntactic ambiguities, 
removing unlikely parts of speech. Our hypothesis is that this will 
result in higher quality parse trees, which in turn allows extraction 
of higher quality semantic and statistical knowledge in the second 
and later cycles. 
LaTaT is a Language and Text Analysis Toolset that demonstrates 
this iterative learning process. The main components in the toolset 
consist of the following: 
? A broad coverage English parser, called Minipar. The 
grammar is constructed manually, based on the Minimalist 
Program (Chomsky 1995). Instead of using a large number 
of CFG rules, Minipar achieves its broad coverage by using a 
small set of principles to constrain the overgerating X-bar 
schema; 
? A collocation extractor that extracts frequency counts of 
grammatical dependency relationships from a corpus parsed 
with Minipar. The frequency counts are then injected into 
Minipar to help it rank candidate parse trees; 
? A thesaurus constructor (Lin, 1998) that automatically 
computes the word similarities based on the distributional 
characteristics of words in the parsed corpus. The resulting 
word similarity database can then be used to smooth the 
probability distribution in statistical language models (Dagan 
et al 1997); 
? A clustering algorithm that constructs Roget-like semantic 
categories in an unsupervised fashion (Lin and Pantel, 
2001a); and 
? An unsupervised learner to identify similar expressions from 
a parsed corpus (Lin and Pantel, 2001b). 
2. Minipar 
Minipar is a principle-based English parser (Berwick et al 1991). 
Like Principar (Lin, 1993), Minipar represents its grammar as a 
network where nodes represent grammatical categories and links 
represent types of syntactic (dependency) relationships. The 
grammar network consists of 35 nodes and 59 links. Additional 
nodes and links are created dynamically to represent 
subcategories of verbs.  
Minipar employs a message passing algorithm that essentially 
implements distributed chart parsing. Instead of maintaining a 
single chart, each node in the grammar network maintains a chart 
containing partially built structures belonging to the grammatical 
category represented by the node. The grammatical principles are 
implemented as constraints associated with the nodes and links.   
The lexicon in Minipar is derived from WordNet (Miller, 1990). 
With additional proper names, the lexicon contains about 130,000 
entries (in base form). The lexicon entry of a word lists all 
possible parts of speech of the word and its subcategorization 
frames (if any). The lexical ambiguities are handled by the parser 
instead of a tagger.  
Minipar works with a constituency grammar internally. However, 
the output of Minipar is a dependency tree. A dependency 
relationship is an asymmetric binary relationship between a word 
called head, and another word called modifier (Mel'?uk, 1987). 
The structure of a sentence can be represented by a set of 
dependency relationships that form a tree. A word in the sentence 
may have several modifiers, but each word may modify at most 
one word. The root of the dependency tree does not modify any 
word. It is also called the head of the sentence.  
Figure 1 shows an example dependency tree for the sentence 
?John found a solution to the problem.? The links in the diagram 
represent dependency relationships. The direction of a link is 
from the head to the modifier in the relationship. Labels 
 
associated with the links represent types of dependency relations. 
Table 1 lists a subset of the dependency relations in Minipar 
outputs.  
Minipar constructs all possible parses of an input sentence. 
However, only the highest ranking parse tree is outputted. 
Although the grammar is manually constructed, the selection of 
the best parse tree is guided by the statistical information obtained 
by parsing a 1GB corpus with Minipar. The statistical ranking of 
parse trees is based on the following probabilistic model. The 
probability of a dependency tree is defined as the product of the 
probabilities of the dependency relationships in the tree. 
Formally, given a tree T with root root consisting of D 
dependency relationships (headi, relationshipi, modifieri), the 
probability of T is given by: 
( ) ( ) ( )?
=
=
D
i
iii head|ifiermod,iprelationshProotPTP
1
 
where P(relationshipi, modifieri | headi) is obtained using 
Maximum Likelihood Estimation. 
Minipar parses newspaper text at about 500 words per second on 
a Pentium-III 700Mhz with 500MB memory. Evaluation with the 
manually parsed SUSANNE corpus (Sampson, 1995) shows that 
about 89% of the dependency relationships in Minipar outputs are 
correct. 
3. Collocation and Word Similarity 
We define a collocation to be a dependency relationship that 
occurs more frequently than predicted by assuming the two words 
in the relationship are independent of each other. Lin (1998) 
presented a method to create a collocation database by parsing a 
large corpus. Given a word w, the database can be used to retrieve 
all the dependency relationships involving w and the frequency 
counts of the dependency relationships. Table 2 shows excerpts of 
the entries in the collocation database for the words duty and 
responsibility. For example, in the corpus from which the 
collocation database is constructed, fiduciary duty occurs 319 
times and assume [the] responsibility occurs 390 times. 
The collocation database entry of a given word can be viewed as a 
feature vector for that word. Similarity between words can be 
computed using the feature vectors. Intuitively, the more features 
that are shared between two words, the higher the similarity 
between the two words will be. This intuition is captured by the 
Distributional Hypothesis (Harris, 1985). 
Features of words are of varying degree of importance. For 
example, while almost any noun can be used as object of include, 
very few nouns can be modified by fiduciary. Two words sharing 
the feature object-of-include is less indicative of their similarity 
Table 2. Excerpts of entries in the collocation database for duty and responsibility. 
DUTY RESPONSIBILITY 
modified-
by 
adjectives 
fiduciary 319, active 251, other 82, official 76, additional 47, 
administrative 44, military 44, constitutional 41, reserve 24, 
high 23, moral 21, double 16, day-to-day 15, normal 15, 
specific 15, assigned 14, extra 13, operating 13, temporary 
13, corporate 12, peacekeeping 12, possible 12, regular 12, 
retaliatory 12, heavy 11, routine 11, sacred 11, stiff 11, 
congressional 10, fundamental 10, hazardous 10, main 10, 
patriotic 10, punitive 10, special 10, ? 
modified-
by 
adjectives 
more 107, full 92, fiduciary 89, primary 88, personal 79, 
great 69, financial 64, fiscal 59, social 59, moral 48, 
additional 46, ultimate 39, day-to-day 37, special 37, 
individual 36, legal 35, other 35, corporate 30, direct 30, 
constitutional 29, given 29, overall 29, added 28, sole 25, 
operating 23, broad 22, political 22, heavy 20, main 18, 
shared 18, professional 17, current 15, federal 14, joint 14, 
enormous 13, executive 13, operational 13, similar 13, 
administrative 10, fundamental 10, specific 10, ? 
object-of 
verbs 
have 253, assume 190, perform 153, do 131, impose 118, 
breach 112, carry out 79, violate 54, return to 50, fulfill 44, 
handle 42, resume 41, take over 35, pay 26, see 26, avoid 19, 
neglect 18, shirk 18, include 17, share 17, discharge 16, 
double 16, relinquish 16, slap 16, divide 14, split 13, take up 
13, continue 11, levy 11, owe 10, ? 
object-of 
verbs 
have 747, claim 741, take 643, assume 390, accept 220, bear 
187, share 103, deny 86, fulfill 53, meet 48, feel 47, retain 
47, shift 47, carry out 45, take over 41, shoulder 29, escape 
28, transfer 28, delegate 26, give 25, admit 23, do 21, 
acknowledge 20, exercise 20, shirk 20, divide 19, get 19, 
include 19, assign 18, avoid 17, put 17, recognize 17, hold 
16, understand 16, evade 15, disclaim 12, handle 12, turn 
over 12, become 11, expand 11, relinquish 11, show 11, 
violate 11, discharge 10, duck 10, increase 10, ? 
Table 1. A subset of dependency relations in Minipar outputs. 
RELATION DESCRIPTION EXAMPLE 
appo appositive of a noun the CEO, John 
det determiner of a noun the dog 
gen genitive modifier of a noun John?s dog 
mod adjunct modifier of any head tiny hole 
nn prenominal modifier of a noun station manager 
pcomp complement of a preposition in the garden 
subj subject of a verb John loves Mary. 
 
John found a solution to the problem.
det detsubj
obj
mod
pcomp
 
Figure 1. Example dependency tree. 
than if they shared the feature modified-by-fiduciary. The 
similarity measure proposed in (Lin, 1998) takes this into account 
by computing the mutual information between two words 
involved in a dependency relationship. 
Using the collocation database, (Lin, 1998) presented an 
unsupervised method to construct a similarity matrix. Given a 
word w, the matrix returns a set of similar words of w along with 
their similarity to w. For example, the 35 most similar words of 
duty, Beethoven, and eat are shown in Table 3. The similarity 
matrix consists of about 20,000 nouns, 4,000 verbs and 6,000 
adjectives and adverbs. 
4. Unsupervised Induction of Semantic Classes 
Consider the similar words of Beethoven. The quality of similar 
words obviously decreases as the similarity value decreases. 
Some of the words have non-zero similarity simply because they 
share common features with Beethoven by accident. For example, 
tough guy is similar to Beethoven because both Beethoven and 
tough guy can be used as the object of the verb play.  
The similar words of duty exemplify another problem: The top 
similar words of a given word may be similar to different senses 
of the word. However, this is not made explicit by the similarity 
matrix.  
LaTaT includes an algorithm called UNICON (UNsupervised 
Induction of CONcepts) that clusters similar words to create 
semantic classes (Lin and Pantel, 2001a). UNICON uses a 
heuristic maximal-clique algorithm, called CLIMAX, to find 
clusters in the similar words of a given word. The purpose of 
CLIMAX is to find small, tight clusters. For example, two of the 
clusters returned by CLIMAX are: 
(Nq34 
"Harvard University" 0.610996 
Harvard 0.482834 
"Stanford University" 0.469302 
"University of Chicago" 0.454686 
"Columbia University" 0.44262 
"New York University" 0.436737 
"University of Michigan" 0.43055 
"Yale university" 0.416731 
MIT 0.414907 
"University of Pennsylvania" 0.384016 
"Cornell University" 0.333958 
) 
(Nq184 
"University of Rochester" 0.525389 
"University of Miami" 0.466607 
"University of Colorado" 0.46347 
"Ohio State University" 0.430326 
"University of Florida"  0.398765 
"Harvard Medical School" 0.39485 
"University of North Carolina" 0.394256 
"University of Houston" 0.371618 
) 
Nq34 and Nq184 are automatically generated names for the 
clusters. The number after each word in the clusters is the 
similarity between the word and the centroid of that cluster.  
The UNICON algorithm computes the centroids of a cluster by 
averaging the collocational features of the words in the cluster. 
The CLIMAX algorithm is then recursively used to construct 
clusters of centroids and the clusters whose centroids are clustered 
together are merged. This process continues until no more clusters 
are merged. The details of the UNICON and CLIMAX algorithms 
are presented in (Lin and Pantel, 2001a). Table 4 shows 10 
sample semantic classes identified by the UNICON algorithm, 
using a 1GB newspaper text corpus.  
5. Automatic Discovery of Inference Rules 
In many natural language processing and information retrieval 
applications, it is very useful to know the paraphrase relationships 
between natural language expressions. LaTaT includes an 
unsupervised method for discovering paraphrase inference rules 
from text, such as ?X is author of Y ? X wrote Y?, ?X solved Y ?  
X found a solution to Y?, and ?X caused Y ? Y is triggered by X? 
(Lin and Pantel, 2001b). Our algorithm is based on an extended 
version of Harris? Distributional Hypothesis. Instead of using this 
hypothesis on words, we apply it to paths in the dependency trees 
of a parsed corpus.  
Table 3. The top 35 most similar words of duty, Beethoven and 
eat as given by (Lin, 1998). 
WORD SIMILAR WORDS (WITH SIMILARITY SCORE) 
DUTY responsibility 0.182, obligation 0.138, job 0.127, 
function 0.121, post 0.121, task 0.119, role 0.116, 
assignment 0.114, mission 0.109, requirement 
0.109, tariff 0.109, position 0.108, restriction 
0.103, procedure 0.101, tax 0.101, salary 0.1, fee 
0.099, training 0.097, commitment 0.096, penalty 
0.095, burden 0.094, quota 0.094, work 0.093, 
staff 0.093, regulation 0.093, sanction 0.093, 
liability 0.092, personnel 0.092, service 0.091, 
action 0.09, activity 0.09, rule 0.089, practice 
0.089, authority 0.088 
BEETHOVEN Mozart 0.193, Brahms 0.178, Schubert 0.148, 
Mahler 0.143, Bach 0.142, Tchaikovsky 0.128, 
Prokofiev 0.118, Wagner 0.089, chamber music 
0.087, Handel 0.073, cello 0.069, classical music 
0.067, Strauss 0.066, Shakespeare 0.063, 
concerto 0.062, Cole Porter 0.062, Verdi 0.06, 
Sonata 0.057, violin 0.056, Elvis 0.053, Berg 
0.053, composer 0.053, Lenin 0.052, flute 0.049, 
Bernstein 0.047, jazz 0.047, Beatles 0.046, Frank 
Sinatra 0.045, Warhol  0.043, Bob Dylan 0.043, 
Napoleon 0.043, symphony 0.042, solo 0.042, 
tough guy 0.042, Bruce Springsteen 0.041, 
grandparent 0.041 
EAT drink 0.204, cook 0.193, smoke 0.164, sleep 
0.162, consume 0.156, love 0.153, enjoy 0.152, 
pick up 0.142, look at 0.141, feed 0.141, wear 
0.14, talk about 0.139, watch 0.138, forget 0.136, 
like 0.136, taste 0.134, go out 0.133, sit 0.133, 
pack 0.133, wash 0.132, stay 0.131, burn 0.13, 
serve 0.129, ride 0.128, pick 0.128, grab 0.128, 
freeze 0.126, go through 0.126, throw 0.126, 
remember 0.124, get in 0.123, feel 0.123, learn 
0.123, live 0.123 
 
 
 Table 4. Ten concepts discovered by UNICON. 
CONCEPT SIZE MEMBERS 
Nq1 210 "Max von Sydow", "Paul Newman", "Jeremy Irons", "Lynn Redgrave", "Lloyd Bridges", "Jack Lemmon", 
"Jaclyn Smith", "Judd Nelson", "Beau Bridges", "Raymond Burr", "Gerald McRaney", "Robert de Niro", 
"Tim Matheson", "Kevin Costner", "Kurt Russell", "Arnold Schwarzenegger", "Michael J. Fox", "Dustin 
Hoffman", "Tom Hanks", "Robert Duvall", "Michael Keaton", "Edward James Olmos", "John Turturro", 
"Robin Williams", "Sylvester Stallone", "John Candy", "Whoopi Goldberg", "Eddie Murphy", "Rene 
Auberjonois", "Vanessa Redgrave", "Jeff Bridges", "Robert Mitchum", "Clint Eastwood", "James 
Woods", "Al Pacino", "William Hurt", "Richard Dreyfuss", "Tom Selleck", "Barry Bostwick", "Harrison 
Ford", "Tom Cruise", "Jon Cryer", "Pierce Brosnan", "Donald Sutherland", "Anthony Quinn", "Farrah 
Fawcett", "Louis Gossett Jr.", "Mark Harmon", "Steven Bauer", "William Shatner", "Diane Keaton", 
"Billy Crystal", "Omar Sharif", "Paul Hogan", "Woody Allen", "Fred Savage", "Jodie Foster", "Chuck 
Norris", "Kirk Douglas", "Glenn Close", "Ed Asner", "Dan Aykroyd", "Steve Guttenberg", "Sissy 
Spacek", "Jonathan Pryce", "Sean Penn", "Bill Cosby", "Robert Urich", "Steve Martin", "Karl Malden", 
"John Lithgow", "Charles Bronson", "Danny DeVito", "Michael Douglas", "John Ritter", "Gerard 
Depardieu", "Val Kilmer", "Jamie Lee Curtis", "Randy Quaid", "John Cleese", "James Garner", "Albert 
Finney", "Richard Gere", "Jim Belushi", "Christopher Reeve", "Telly Savalas", "Chevy Chase".... 
Nq178 39 Toyota, Honda, Volkswagen, Mazda, Oldsmobile, BMW, Audi, Mercedes-Benz, Cadillac, Volvo, Subaru, 
Chevrolet, Mercedes, Buick, Porsche, Nissan, VW, Mitsubishi, Renault, Hyundai, Isuzu, Jaguar, Suzuki, 
Dodge, Rolls-Royce, Pontiac, Fiat, Chevy, Saturn, Yugo, Ferrari, "Mercedes Benz", Plymouth, mustang, 
Beretta, Panasonic, Corvette, Nintendo, Camaro 
Nq214 41 mathematics, physic, math, "political science", chemistry, "computer science", biology, sociology, 
"physical education", "electrical engineering", anthropology, astronomy, "social science", geology, 
psychology, "mechanical engineering", physiology, geography, economics, psychiatry, calculus, 
biochemistry, algebra, science, civics, journalism, literature, theology, "molecular biology", humanity, 
genetics, archaeology, nursing, anatomy, pathology, arithmetic, pharmacology, literacy, architecture, 
undergraduate, microbiology 
Nq223 59 shirt, jacket, dress, pant, skirt, coat, sweater, T-shirt, hat, blouse, jean, trouser, sock, gown, scarf, slack, 
vest, boot, uniform, shoe, robe, cloth, sunglasses, clothing, outfit, glove, underwear, sneaker, blazer, 
jersey, costume, wig, mask, helmet, button, hair, collar, ribbon, short, belt, necktie, bra, stocking, sleeve, 
silk, red, pin, banner, badge, sheet, sticker, makeup, stripe, bow, logo, linen, curtain, shade, quilt 
Nq292 31 barley, oat, sorghum, "feed grain", alfalfa, "soybean meal", "soybean oil", "sugar beet", maize, sunflower, 
"pork belly", soybean, millet, Rye, oilseed, wheat, "grain sorghum", rapeseed, canola, hay, "palm oil", 
durum, safflower, psyllium, "sunflower seed", flaxseed, bran, broiler, buckwheat, cantaloupe, cottonseed 
Nq293 22 "Joseph Cicippio", "Terry Anderson", "Terry Waite", Cicippio, Waite, "Terry A. Anderson", "William 
Higgins", "John McCarthy", "Joseph James Cicippio", "Thomas Sutherland", "Brian Keenan", "Alann 
Steen", "Jesse Turner", "Alec Collett", "Edward Austin Tracy", "Edward Tracy", "Frank Reed", 
"American Terry Anderson", "Jack Mann", Buckley, westerner, "Giandomenico Picco", "Robert Polhill", 
"Benjamin Weir" 
Nq352 8 heroin, cocaine, marijuana, narcotic, alcohol, steroid, crack, opium 
Nq356 15 Saskatchewan, Alberta, Manitoba, "British Columbia", Ontario, "New Brunswick", Newfoundland, 
Quebec, Guangdong, "Prince Edward Island", "Nova Scotia", "Papua New Guinea", "Northwest 
Territories", Luzon, Mindanao 
Nq396 29 sorrow, sadness, grief, anguish, remorse, indignation, insecurity, loneliness, discomfort, agony, despair, 
regret, heartache, dismay, shame, revulsion, angst, jubilation, humiliation, bitterness, pity, outrage, 
anxiety, empathy, happiness, mourning, letdown, distaste, indignity 
Nq776 30 baldness, hemophilia, acne, infertility, sepsis, "cold sore", "sleeping sickness", "morning sickness", 
"kidney stone", "common cold", heartburn, "eye disease", "heroin addiction", osteoporosis, "pneumocystis 
carinii pneumonia", dwarfism, incontinence, "manic depression", atherosclerosis, "Dutch elm disease", 
hyperthyroidism, discoloration, "cancer death", spoilage, gonorrhea, hemorrhoid, wart, mildew, sterility, 
"athlete's foot" 
In the dependency trees generated by Minipar, each link between 
two words in a dependency tree represents a direct semantic 
relationship. A path allows us to represent indirect semantic 
relationships between two content words. We name a path by 
concatenating dependency relationships and words along the path, 
excluding the words at the two ends. For the sentence in Figure 1, 
the path between John and problem is named: 
N:subj:V!find"V:obj:N"solution"N:to:N (meaning ?X finds 
solution to Y?). The root of the path is find.  
A path begins and ends with two dependency relations. We call 
them the two slots of the path: SlotX on the left-hand side and 
SlotY on the right-hand side. The words connected by the path are 
the fillers of the slots. For example, John fills the SlotX and 
problem fills the SlotY in the above example. 
We extract the fillers and frequency counts of all the slots of all 
the paths in a parsed corpus. Table 5 shows an excerpt of the 
fillers of two paths. The underlying assumption of algorithm is 
that when the meanings of paths are similar, their corresponding 
sets of fillers share a large number of common words.  
Richardson (1997) extracted semantic relationships (e.g., 
hypernym, location, material and purpose) from dictionary 
definitions using a parser and constructed a semantic network. He 
then described an algorithm that uses paths in the semantic 
network to compute the similarity between words. In a sense, our 
algorithm is a dual of Richardson?s approach. While Richardson 
used paths as features to compute the similarity between words, 
we use words as features to compute the similarity of paths. 
We use the notation |p, SlotX, w| to denote the frequency count of 
word w filling in the SlotX of a path p, and |p, SlotX, *| to denote 
?
w
wSlotXp ,, , and |*, *, *| to denote ?
wsp
wsp
,,
,, .  
Following (Lin, 1998), the mutual information between a path slot 
and its filler can be computed by the formula: 
 ( ) ???
?
???
?
?
?=
wSlotSlotp
SlotwSlotp
wSlotpmi
,*,,*,
,**,,,
log,,  (1) 
The similarity between a pair of slots: slot1 = (p1, s) and slot2 = 
(p2, s), is defined as: 
 ( ) ( ) ( )( ) ( )( ) ( )( )( )? ?
?
? ?
??
+
+=
spTw spTw
spTspTw
wspmiwspmi
wspmiwspmi
slotslotsim
, , 21
,, 21
21
1 2
21
,,,,
,,,,
,
 (2) 
where p1 and p2 are paths, s is a slot, T(pi, s) is the set of words 
that fill in the s slot of path pi. 
The similarity between a pair of paths p1 and p2 is defined as the 
geometric average of the similarities of their SlotX and SlotY 
slots: 
 ( ) ( ) ( )212121 ,,, SlotYSlotYsimSlotXSlotXsimppS ?=  (3) 
Table 6 and 7 list the top-50 most similar paths to ?X solves Y?. 
and "X causes Y" generated by our algorithm. The ones tagged 
with an asterisk (*) are incorrect. Most of the paths can be 
considered as paraphrases of the original expression. 
 
 
6. References 
Berwick R., Abney S., and Tenny, C, editors. Principle-Based 
Parsing: Computation and Psycholinguistics. Kluwer 
Academic Publishers, 1991. 
 
 
Table 5. Sample slot fillers for two paths extracted from a 
newspaper corpus. 
?X finds a solution to Y? ?X solves Y? 
SLOTX SLOTY SLOTX SLOTY 
commission strike committee problem 
committee civil war clout crisis 
committee crisis government problem 
government crisis he mystery 
government problem she problem 
he problem petition woe 
legislator budget deficit researcher mystery 
sheriff dispute sheriff murder 
 
Table 6. The top-50 most similar paths to ?X solves Y?. 
Y is solved by X X clears up Y 
X resolves Y *X creates Y 
X finds a solution to Y *Y leads to X 
X tries to solve Y *Y is eased between X 
X deals with Y X gets down to Y 
Y is resolved by X X worsens Y 
X addresses Y X ends Y 
X seeks a solution to Y *X blames something for Y 
X do something about Y X bridges Y 
X solution to Y X averts Y 
Y is resolved in X *X talks about Y 
Y is solved through X X grapples with Y 
X rectifies Y *X leads to Y 
X copes with Y X avoids Y 
X overcomes Y X solves Y problem 
X eases Y X combats Y 
X tackles Y X handles Y 
X alleviates Y X faces Y 
X corrects Y X eliminates Y 
X is a solution to Y Y is settled by X 
X makes Y worse *X thinks about Y 
X irons out Y X comes up with a solution to Y 
*Y is blamed for X X offers a solution to Y 
X wrestles with Y X helps somebody solve Y 
X comes to grip with Y *Y is put behind X 
 
Chomsky N. 1995. Minimalist Program. MIT Press. 
Dagan I, Lee L, and Pereira F., Similarity-based Methods for 
Word Sense Disambiguation. In Proceedings of ACL/EACL-97, 
pp.56-63. Madrid, Spain. 
Harris, Z. 1985. Distributional Structure. In: Katz, J. J. (ed.) The 
Philosophy of Linguistics. New York: Oxford University Press. 
pp. 26-47. 
Lin, D. and Pantel, P. 2001a. Induction of Semantic Classes from 
Natural Language Text. To appear in Proceedings of KDD-
2001. San Francisco, CA. 
Lin, D. and Pantel, P. 2001b. DIRT: Discovery of Inference Rules 
from Text. To appear in Proceedings of KDD-2001. San 
Francisco, CA. 
Lin, D. 1998. Extracting Collocations from Text Corpora. 
Workshop on Computational Terminology. pp. 57-63. 
Montreal, Canada. 
Lin, D. 1993. Parsing Without OverGeneration. In Proceedings 
ACL-93. pp. 112-120. Columbus, OH. 
Mel'?uk, I. A. 1987. Dependency Syntax: theory and practice. 
State University of New York Press. Albany, NY. 
Miller, G. 1990. WordNet: An Online Lexical Database. 
International Journal of Lexicography, 1990. 
Richardson, S. D. 1997. Determining Similarity and the Inferring 
Relations in a Lexical Knowledge-Base. Ph.D. Thesis. The 
City University of New York. 
Sampson, G. 1995. English for the Computer - The SUSANNE 
Corpus and Analytic Scheme. Clarendon Press. Oxford, 
England.  
 
Table 7. The top-50 most similar paths to ?X causes Y?. 
Y is caused by X *Y contributes to X 
X cause something Y *X results from Y 
X leads to Y *X adds to Y 
X triggers Y X means Y 
*X is caused by Y *X reflects Y 
*Y causes X X creates Y 
Y is blamed on X *Y prompts X 
X contributes to Y X provoke Y 
X is blamed for Y Y reflects X 
X results in Y X touches off Y 
X is the cause of Y X poses Y 
*Y leads to X Y is sparked by X 
Y results from X *X is attributed to Y 
Y is result of X *Y is cause of X 
X prompts Y *X stems from Y 
X sparks Y *Y is blamed for X 
*Y triggers X *X is triggered by Y 
X prevents Y Y is linked to X 
*X is blamed on Y X sets off Y 
Y is triggered by X X is a factor in Y 
Y is attributed to X X exacerbates Y 
X stems from Y X eases Y 
*Y results in X Y is related to X 
*X is result of Y X is linked to Y 
X fuels Y X is responsible for Y 
 
Word Alignment with Cohesion Constraint
Dekang Lin and Colin Cherry
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{lindek,colinc}@cs.ualberta.ca
Abstract
We present a syntax-based constraint for word
alignment, known as the cohesion constraint. It
requires disjoint English phrases to be mapped
to non-overlapping intervals in the French sen-
tence. We evaluate the utility of this constraint
in two different algorithms. The results show
that it can provide a significant improvement in
alignment quality.
1 Introduction
The IBM statistical machine translation (SMT) models
have been extremely influential in computational linguis-
tics in the past decade. The (arguably) most striking char-
acteristic of the IBM-style SMT models is their total lack
of linguistic knowledge. The IBM models demonstrated
how much one can do with pure statistical techniques,
which have inspired a whole new generation of NLP re-
search and systems.
More recently, there have been many proposals to
introduce syntactic knowledge into SMT models (Wu,
1997; Alshawi et al, 2000; Yamada and Knight, 2001;
Lopez et al, 2002). A common theme among these
approaches is the assumption that the syntactic struc-
tures of a pair of source-target sentences are isomor-
phic (or nearly isomorphic). This assumption seems too
strong. Human translators often use non-literal transla-
tions, which result in differences in syntactic structures.
According to a study in (Dorr et al, 2002), such transla-
tional divergences are quite common, involving 11-31%
of the sentences.
We introduce a constraint that uses the dependency tree
of the English sentence to maintain phrasal cohesion in
the French sentence. In other words, if two phrases are
disjoint in the English sentence, the alignment must not
map them to overlapping intervals in the French sentence.
For example, in Figure 1, the cohesion constraint will rule
out the possibility of aligning to with a`. The phrases the
reboot and the host to discover all the devices are dis-
joint, but the partial alignment in Figure 1 maps them to
overlapping intervals. This constraint is weaker than iso-
morphism. However, we will show that it can produce a
significant increase in alignment quality.
The reboot causes the host to discover all the devices
det subj det subj aux
pre
det
objmod
  ?    laSuite r?initialisation  ,  l'  h?te rep?re tous les p?riph?riques
after to the reboot the host locate all the peripherals
1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10 11
Figure 1: A cohesion constraint violation
2 Cohesion Constraint
Given an English sentence E = e1e2 . . . el and a French
sentence F = f1f2 . . . fm, an alignment is a set of links
between the words in E and F . An alignment can be
represented as a binary relation A in [1, l] ? [1,m]. A
pair (i, j) is in A if ei and fj are a translation (or part
of a translation) of each other. We call such pairs links.
In Figure 2, the links in the alignment are represented by
dashed lines.
The reboot causes the host to discover all the devices
det subj det subjaux
pre
det
objcomp
  ?    laSuite r?initialisation  ,  l'  h?te rep?re tous les p?riph?riques
1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10
11
after to the reboot the host locate all the peripherals
Figure 2: An example pair of aligned sentence
The cohesion constraint (Fox, 2002) uses the depen-
dency tree TE (Mel?c?uk, 1987) of the English sentence
to restrict possible link combinations. Let TE(ei) be
the subtree of TE rooted at ei. The phrase span of ei,
spanP (ei, TE , A), is the image of the English phrase
headed by ei in F given a (partial) alignment A. More
precisely, spanP (ei, TE , A) = [k1, k2], where
k1 = min{j|(u, j) ? A, eu ? TE(ei)}
k2 = max{j|(u, j) ? A, eu ? TE(ei)}
The head span is the image of ei itself. We define
spanH(ei, TE , A) = [k1, k2], where
k1 = min{j|(i, j) ? A}
k2 = max{j|(i, j) ? A}
In Figure 2, the phrase span of the node discover is
[6, 11] and the head span is [8, 8]; the phrase span of the
node reboot is [3, 4] and the head span is [4, 4]. The word
cause has a phrase span of [3,11] and its head span is the
empty set ?.
With these definitions of phrase and head spans, we de-
fine two notions of overlap, originally introduced in (Fox,
2002) as crossings. Given a head node eh and its modi-
fier em, a head-modifier overlap occurs when:
spanH(eh, TE , A) ? spanP (em, TE , A) 6= ?
Given two nodes em1 and em2 which both modify the
same head node, a modifier-modifier overlap occurs
when:
spanP (em1 , TE , A) ? spanP (em2 , TE , A) 6= ?
Following (Fox, 2002), we say an alignment is cohe-
sive with respect to TE if it does not introduce any head-
modifier or modifier-modifier overlaps. For example, the
alignment A in Figure 1 is not cohesive because there
is an overlap between spanP (reboot, TE , A)=[4, 4] and
spanP (discover, TE , A)=[2, 11].
If an alignmentA? violates the cohesion constraint, any
alignment A that is a superset of A? will also violate the
cohesion constraint. This is because any pair of nodes
that have overlapping spans in A? will still have overlap-
ping spans in A.
Cohesion Checking Algorithm:
We now present an algorithm that checks whether an
individual link (ei, fj) causes a cohesion constraint vi-
olation when it is added to a partial alignment. Let
ep0 , ep1 , ep2 , . . . be a sequence of nodes in TE such that
ep0=ei and epk=parentOf (epk?1) (k = 1, 2, . . .)
1. For all k ? 0, update the spanP and the spanH of
epk to include j.
2. For each epk (k > 0), check for a modifier-modifier
overlap between the updated the phrase span of
epk?1 and the the phrase span of each of the other
children of epk .
3. For each epk (k > 0), check for a head-modifier
overlap between the updated phrase span of epk?1
and the head span of epk .
4. If an overlap is found, return true (the constraint is
violated). Otherwise, return false.
3 Evaluation
To determine the utility of the cohesion constraint, we
incorporated it into two alignment algorithms. The algo-
rithms take as input an English-French sentence pair and
the dependency tree of the English sentence. Both algo-
rithms build an alignment by adding one link at a time.
We implement two versions of each algorithm: one with
the cohesion constraint and one without. We will describe
the versions without cohesion constraint below. For the
versions with cohesion constraint, it is understood that
each new link must also pass the test described in Sec-
tion 2.
The first algorithm is similar to Competitive Linking
(Melamed, 1997). We use a sentence-aligned corpus
to compute the ?2 correlation metric (Gale and Church,
1991) between all English-French word pairs. For a given
sentence pair, we begin with an empty alignment. We
then add links in the order of their ?2 scores so that each
word participates in at most one link. We will refer to this
as the ?2 method.
The second algorithm uses a best-first search (with
fixed beam width and agenda size) to find an alignment
that maximizes P (A|E,F ). A state in this search space
is a partial alignment. A transition is defined as the ad-
dition of a single link to the current state. The algorithm
computes P (A|E,F ) based on statistics obtained from a
word-aligned corpus. We construct the initial corpus with
a system that is similar to the ?2 method. The algorithm
then re-aligns the corpus and trains again for three iter-
ations. We will refer to this as the P (A|E,F ) method.
The details of this algorithm are described in (Cherry and
Lin, 2003).
We trained our alignment programs with the same 50K
pairs of sentences as (Och and Ney, 2000) and tested it on
the same 500 manually aligned sentences. Both the train-
ing and testing sentences are from the Hansard corpus.
We parsed the training and testing corpora with Minipar.1
We adopted the evaluation methodology in (Och and Ney,
2000), which defines three metrics: precision, recall and
alignment error rate (AER).
Table 1 shows the results of our experiments. The first
four rows correspond to the methods described above. As
a reference point, we also provide the results reported in
(Och and Ney, 2000). They implemented IBM Model 4
by bootstrapping from an HMM model. The rows F?E
1available at http://www.cs.ualberta.ca/? lindek/minipar.htm
Table 1: Evaluation Results
Method Prec Rec AER
?2 w/o cohesion 82.7 84.6 16.5
w/ cohesion 89.2 82.7 13.8
P (A|E,F ) w/o cohesion 87.3 85.3 13.6
w/ cohesion 95.7 86.4 8.7
F?E 80.5 91.2 15.6
Och&Ney E?F 80.0 90.8 16.0
Refined 85.9 92.3 11.7
and E?F are the results obtained by this model when
treating French as the source and English as the target
or vice versa. The row Refined shows results obtained
by taking the intersection of E?F and F?E and then
refining this intersection to increase recall.
From Table 1, we can see that the addition of the cohe-
sion constraint leads to significant improvements in per-
formance with both algorithms. The relative reduction in
error rate is 16% with the ?2 method and 36% with the
P (A|E,F ) method. The improvement comes primarily
from increased precision. With the P (A|E,F ) method,
this increase in precision does not come at the expense of
recall.
4 Related Work
There has been a growing trend in the SMT community
to attempt to leverage syntactic data in word alignment.
Methods such as (Wu, 1997), (Alshawi et al, 2000) and
(Lopez et al, 2002) employ a synchronous parsing proce-
dure to constrain a statistical alignment. The work done
in (Yamada and Knight, 2001) measures statistics on op-
erations that transform a parse tree from one language
into another.
The syntactic knowledge that is leveraged in these
methods is tightly coupled with the alignment method it-
self. We have presented a modular constraint that can be
plugged into different alignment algorithms. This has al-
lowed us to test the contribution of the constraint directly.
(Fox, 2002) studied the extent to which the cohesion
constraint holds in a parallel corpus and the reasons for
the violations, but did not apply the constraint to an align-
ment algorithm.
5 Conclusion
We have presented a syntax-based constraint for word
alignment, known as the cohesion constraint. It requires
disjoint English phrases to be mapped to non-overlapping
intervals in the French sentence. Our experiments have
shown that the use of this constraint can provide a rela-
tive reduction in alignment error rate of 36%.
Acknowledgments
We wish to thank Franz Och for providing us with manu-
ally aligned evaluation data. This project is funded by and
jointly undertaken with Sun Microsystems, Inc. We wish
to thank Finola Brady, Bob Kuhns and Michael McHugh
for their help.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. Submitted.
Bonnie J. Dorr, Lisa Pearl, Rebecca Hwa, and Nizar
Habash. 2002. Duster: A method for unraveling
cross-language divergences for statistical word-level
alignment. In Stephen D. Richardson, editor, Proceed-
ings of AMTA-02, pages 31?43, Tiburon, CA, October.
Springer.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP-02, pages
304?311.
W.A. Gale and K.W. Church. 1991. Identifying word
correspondences in parallel texts. In Proceedings of
the 4th Speech and Natural Language Workshop, pages
152?157. DARPA, Morgan Kaufmann.
Adam Lopez, Michael Nossal, Rebecca Hwa, and Philip
Resnik. 2002. Word-level alignment for multilingual
resource acquisition. In Proceedings of the Workshop
on Linguistic Knowledge Acquisition and Representa-
tion: Bootstrapping Annotated Language Data.
I. Dan Melamed. 1997. A word-to-word model of trans-
lational equivalence. In Proceedings of the ACL-97,
pages 490?497. Association for Computational Lin-
guistics.
Igor A. Mel?c?uk. 1987. Dependency syntax: theory and
practice. State University of New York Press, Albany.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447, Hong Kong, China, Octo-
ber.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):374?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Meeting of the Associ-
ation for Computational Linguistics, pages 523?530.
Automatically Discovering Word Senses 
Patrick Pantel and Dekang Lin 
Department of Computing Science  
University of Alberta 
Edmonton, Alberta T6G 2E8 Canada 
{ppantel, lindek}@cs.ualberta.ca 
 
 
 
 
Abstract  
We will demonstrate the output of a distribu-
tional clustering algorithm called Clustering 
by Committee that automatically discovers 
word senses from text1. 
1 Introduction 
Using word senses versus word forms is useful in many 
applications such as information retrieval (Voorhees 
1998), machine translation (Hutchins and Sommers 
1992), and question-answering (Pasca and Harabagiu 
2001). 
The Distributional Hypothesis (Harris 1985) states 
that words that occur in the same contexts tend to be 
similar. There have been many approaches to compute 
the similarity between words based on their distribution 
in a corpus (Hindle 1990; Landauer and Dumais 1997; 
Lin 1998). The output of these programs is a ranked list 
of similar words to each word. For example, Lin?s 
approach outputs the following similar words for wine 
and suit: 
wine: beer, white wine, red wine, 
Chardonnay, champagne, fruit, food, 
coffee, juice, Cabernet, cognac, 
vinegar, Pinot noir, milk, vodka,? 
suit: lawsuit, jacket, shirt, pant, dress, 
case, sweater, coat, trouser, claim, 
business suit, blouse, skirt, litiga-
tion, ? 
The similar words of wine represent the meaning of 
wine. However, the similar words of suit represent a 
mixture of its clothing and litigation senses. Such lists 
of similar words do not distinguish between the 
multiple senses of polysemous words. 
                                                     
1 The demonstration is currently available online at 
www.cs.ualberta.ca/~lindek/demos/wordcluster.htm. 
We will demonstrate the output of a distributional 
clustering algorithm called Clustering by Committee 
(CBC) that discovers word senses automatically from 
text. Each cluster that a word belongs to corresponds to 
a sense of the word. The following is a sample output 
from our algorithm: 
(suit 
 0.39 (blouse, slack, legging, sweater) 
 0.20 (lawsuit, allegation, case, charge) 
) 
(plant 
 0.41 (plant, factory, facility, 
refinery) 
 0.20 (shrub, ground cover, perennial, 
bulb) 
) 
(heart 
 0.27 (kidney, bone marrow, marrow, 
liver) 
 0.17 (psyche, consciousness, soul, mind) 
) 
Each entry shows the clusters to which the head-
word belongs along with its similarity to the cluster. 
The lists of words are the top-4 most similar members 
to the cluster centroid. Each cluster corresponds to a 
sense of the headword. 
2 Feature Representation 
Following (Lin 1998), we represent each word by a 
feature vector. Each feature corresponds to a context in 
which the word occurs. For example, ?sip __? is a verb-
object context. If the word wine occurred in this 
context, the context is a feature of wine. These features 
are obtained by parsing a large corpus using Minipar 
(Lin 1994), a broad-coverage English parser. The value 
of the feature is the pointwise mutual information 
(Manning and Sch?tze 1999) between the feature and 
the word. Let c be a context and Fc(w) be the frequency 
count of a word w occurring in context c. The pointwise 
mutual information, miw,c, between c and w is defined 
as: 
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 21-22
                                                         Proceedings of HLT-NAACL 2003
( )
( ) ( )
N
jF
N
wF
N
wF
cw
j
c
i
i
c
mi ???
=,  
where N is the total frequency counts of all words and 
their contexts. We compute the similarity between two 
words wi and wj using the cosine coefficient (Salton and 
McGill 1983) of their mutual information vectors: 
( ) ??
?
?
?
=
c
cw
c
cw
c
cwcw
ji
ji
ji
mimi
mimi
w,wsim
22
 
3 Clustering by Committee 
CBC finds clusters by first discovering the underlying 
structure of the data. It does this by searching for sets 
of representative elements for each cluster, which we 
refer to as committees. The goal is to find committees 
that unambiguously describe the (unknown) target 
classes. By carefully choosing committee members, the 
features of the centroid tend to be the more typical 
features of the target class. For example, our system 
chose the following committee members to compute 
the centroid of the state cluster: Illinois, Michigan, 
Minnesota, Iowa, Wisconsin, Indiana, Nebraska and 
Vermont. States like Washington and New York are not 
part of the committee because they are polysemous. 
The centroid of a cluster is constructed by averaging 
the feature vectors of the committee members. 
CBC consists of three phases. Phase I computes 
each element?s top-k similar elements. In Phase II, we 
do a first pass through the data and discover the 
committees. The goal is that we form tight committees 
(high intra-cluster similarity) that are dissimilar from 
one another (low inter-cluster similarity) and that cover 
the whole similarity space. The method is based on 
finding sub-clusters in the top-similar elements of every 
given element. 
In the final phase of the algorithm, each word is 
assigned to its most similar clusters (represented by a 
committee). Suppose a word w is assigned to a cluster 
c. We then remove from w its features that intersect 
with the features in c. Intuitively, this removes the c 
sense from w, allowing CBC to discover the less 
frequent senses of a word and to avoid discovering 
duplicate senses. The word w is then assigned to its 
next most similar cluster and the process is repeated. 
4 Conclusion 
We will demonstrate the senses discovered by CBC for 
54,685 words on the 3GB ACQUAINT corpus. CBC 
discovered 24,497 polysemous words. 
References 
Harris, Z. 1985. Distributional structure. In: Katz, J. J. 
(ed.) The Philosophy of Linguistics. New York: 
Oxford University Press. pp. 26?47. 
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90. pp. 
268?275. Pittsburgh, PA. 
Hutchins, J. and Sommers, H. 1992. Introduction to 
Machine Translation. Academic Press. 
Landauer, T. K., and Dumais, S. T. 1997. A solution to 
Plato's problem: The Latent Semantic Analysis 
theory of the acquisition, induction, and representa-
tion of knowledge. Psychological Review, 104:211?
240. 
Lin, D. 1994. Principar - an efficient, broad-coverage, 
principle-based parser. In Proceedings of COLING-
94. pp. 42?48. Kyoto, Japan. 
Lin, D. 1998. Automatic retrieval and  clustering of 
similar words. In Proceedings of COLING/ACL-98. 
pp. 768?774. Montreal, Canada. 
Manning, C. D. and Sch?tze, H. 1999. Foundations of 
Statistical Natural Language Processing. MIT Press. 
Pasca, M. and Harabagiu, S. 2001. The informative role 
of WordNet in Open-Domain Question Answering. 
In Proceedings of NAACL-01 Workshop on WordNet 
and Other Lexical Resources. pp. 138?143. 
Pittsburgh, PA. 
Salton, G. and McGill, M. J. 1983. Introduction to 
Modern Information Retrieval. McGraw Hill. 
Voorhees, E. M. 1998. Using WordNet for text 
retrieval. In WordNet: An Electronic Lexical 
Database, edited by C. Fellbaum. pp. 285?303. MIT 
Press. 
An Unsupervised Approach to Prepositional Phrase Attachment
using Contextually Similar Words
Patrick Pantel and Dekang Lin
Department of Computing Science
University of Alberta1
Edmonton, Alberta T6G 2H1 Canada
{ppantel, lindek}@cs.ualberta.ca
                                            
1This research was conducted at the University of Manitoba.
Abstract
Prepositional phrase attachment is a
common source of ambiguity in natural
language processing. We present an
unsupervised corpus-based approach to
prepositional phrase attachment that
achieves similar performance to supervised
methods. Unlike previous unsupervised
approaches in which training data is
obtained by heuristic extraction of
unambiguous examples from a corpus, we
use an iterative process to extract training
data from an automatically parsed corpus.
Attachment decisions are made using a
linear combination of features and low
frequency events are approximated using
contextually similar words.
Introduction
Prepositional phrase attachment is a common
source of ambiguity in natural language
processing. The goal is to determine the
attachment site of a prepositional phrase in a
sentence. Consider the following examples:
1. Mary ate the salad with a fork.
2. Mary ate the salad with croutons.
In both cases, the task is to decide whether the
prepositional phrase headed by the preposition
with attaches to the noun phrase (NP) headed by
salad or the verb phrase (VP) headed by ate. In
the first sentence, withattaches to the VP since
Mary is using a fork to eat her salad. In sentence
2, with attaches to the NP since it is the salad
that contains croutons.
Formally, prepositional phrase attachment is
simplified to the following classification task.
Given a 4-tuple of the form (V, N1, P, N2), where
V is the head verb, N1 is the head noun of the
object of V, P is a preposition, and N2 is the head
noun of the prepositional complement, the goal
is to classify as either adverbial attachment
(attaching to V) or adjectival attachment
(attaching to N1). For example, the 4-tuple (eat,
salad, with, fork) has target classification V.
In this paper, we present an unsupervised
corpus-based approach to prepositional phrase
attachment that outperforms previous
unsupervised techniques and approaches the
performance of supervised methods. Unlike
previous unsupervised approaches in which
training data is obtained by heuristic extraction
of unambiguous examples from a corpus, we use
an iterative process to extract training data from
an automatically parsed corpus. The attachment
decision for a 4-tuple (V, N1, P, N2) is made as
follows. First, we replace V and N2 by their
contextually similar words and compute the
average adverbial attachment score. Similarly,
the verage adjectival attachment score is
computed by replacing N1 and N2 by their
contextually similar words. Attachment scores
are obtained using a linear combination of
features of the 4-tuple. Finally, we combine the
average attachment scores with the attachment
score of N2 attaching to the original V and the
attachment score of N2 attaching to the original
N1. The proposed classification represents the
attachment site that scored highest.
1 Previous Work
Altmann and Steedman (1988) showed that
current discourse context is often required for
disambiguating attachments. Recent work shows
that it is generally sufficient to utilize lexical
information (Brill and Resnik, 1994; Collins and
Brooks, 1995; Hindle and Rooth, 1993;
Ratnaparkhi et al, 1994).
One of the earliest corpus-based approaches to
prepositional phrase attachment used lexical
preference by computing co-occurrence
frequencies (lexical associations) of verbs and
nouns with prepositions (Hindle and Rooth,
1993). Training data was obtained by extracting
all phrases of the form (V, N1, P, N2) from a
large parsed corpus.
Supervised methods later improved
attachment accuracy. Ratnaparkhi et al (1994)
used a maximum entropy model considering
only lexical information from within the verb
phrase (ignoring N2). They experimented with
both word features and word class features, their
combination yielding 81.6% attachment
accuracy.
Later, Collins and Brooks (1995) achieved
84.5% accuracy by employing a backed-off
model to smooth for unseen events. They
discovered that P is the most informative lexical
item for attachment disambiguation and keeping
low frequency events increases performance.
A non-statistical supervised approach by Brill
and Resnik (1994) yielded 81.8% accuracy using
a transformation-based approach (Brill, 1995)
and incorporating word-class information. They
report that the top 20 transformations learned
involved specific prepositions supporting
Collins and Brooks? claim that the preposition is
the most important lexical item for resolving the
attachment ambiguity.
The state of the art is a supervised algorithm
that employs a semantically tagged corpus
(Stetina and Nagao, 1997). Each word in a
labelled corpus is sense-tagged using an
unsupervised word-sense disambiguation
algorithm with WordNet (Miller, 1990). Testing
examples are classified using a decision tree
induced from the training examples. They report
88.1% attachment accuracy approaching the
human accuracy of 88.2% (Ratnaparkhi et al,
1994).
The current unsupervised state of the art
achieves 81.9% attachment accuracy
(Ratnaparkhi, 1998). Using an extraction
heuristic, unambiguous prepositional phrase
attachments of the form (V, P, N2) and (N1, P,
N2) are extracted from a large corpus. Co-
occurrence frequencies are then used to
disambiguate examples with ambiguous
attachments.
2 Resources
The input to our algorithm includes a collocation
database and a corpus-based thesaurus, both
available on the Internet2. Below, we briefly
describe these resources.
2.1 Collocation database
Given a word w in a dependency relationship
(such as subject or object), the collocation
database is used to retrieve the words that
occurred in that relationship with w, in a large
corpus, along with their frequencies (Lin,
1998a). Figure 1 shows excerpts of the entries in
                                            
2Available at www.cs.ualberta.ca/~lindek/demos.htm.
eat:
object: almond 1, apple 25, bean 5, beam 1, binge 1,
bread 13, cake 17, cheese 8, dish 14,
disorder 20, egg 31, grape 12, grub 2, hay 3,
junk 1, meat 70, poultry 3, rabbit 4, soup 5,
sandwich 18, pasta 7, vegetable 35, ...
subject: adult 3, animal 8, beetle 1, cat 3, child 41,
decrease 1, dog 24, family 29, guest 7, kid
22, patient 7, refugee 2, rider 1, Russian 1,
shark 2, something 19, We 239, wolf 5, ...
salad:
adj-modifier:assorted 1, crisp 4, fresh 13, good 3, grilled
5, leftover 3, mixed 4, olive 3, prepared 3,
side 4, small 6, special 5, vegetable 3, ...
object-of: add 3, consume 1, dress 1, grow 1, harvest 2,
have 20, like 5, love 1, mix 1, pick 1, place
3, prepare 4, return 3, rinse 1, season 1, serve
8, sprinkle 1, taste 1, test 1, Toss 8, try 3, ...
Figure 1. Excepts of entries in the collocation database for
eat and salad.
Table 1. The top 20 most similar words of eatand salad as
given by (Lin, 1998b).
WORD SIMILAR WORDS (WITH SIMILARITY SCORE)
EAT cook 0.127, drink 0.108, consume 0.101, feed 0.094,
taste 0.093, like 0.092, serve 0.089, bake 0.087, sleep
0.086, pick 0.085, fry 0.084, freeze 0.081, enjoy
0.079, smoke 0.078, harvest 0.076, love 0.076, chop
0.074, sprinkle 0.072, Toss 0.072, chew 0.072
SALAD soup 0.172, sandwich 0.169, sauce 0.152, pasta
0.149, dish 0.135, vegetable 0.135, cheese 0.132,
dessert 0.13, entree 0.121, bread 0.116, meat 0.116,
chicken 0.115, pizza 0.114, rice 0.112, seafood 0.11,
dressing 0.109, cake 0.107, steak 0.105, noodle
0.105, bean 0.102
the collocation database for the words eat and
salad. The database contains a total of 11
million unique dependency relationships.
2.2 Corpus-based thesaurus
Using the collocation database, Lin (1998b) used
an unsupervised method to construct a corpus-
based thesaurus consisting of 11839 nouns, 3639
verbs and 5658 adjectives/adverbs. Given a
word w, the thesaurus returns a set of similar
words of w along with their similarity to w. F r
example, the 20 most similar words of eat and
salad are shown in Table 1.
3 Training Data Extraction
We parsed a 125-million word newspaper
corpus with Minipar3, a descendent of Principar
(Lin, 1994). Minipar outputs dependency trees
(Lin, 1999) from the input sentences. For
example, the following sentence is decomposed
into a dependency tree:
Occasionally, the parser generates incorrect
dependency trees. For example, in the above
sentence, the prepositional phrase headed by
with should attach to saw(as opposed to d g).
Two separate sets of training data were then
extracted from this corpus. Below, we briefly
describe how we obtained these data sets.
3.1 Ambiguous Data Set
For each input sentence, Minipar outputs a
single dependency tree. For a sentence
containing one or more prepositions, we use a
program to detect any alternative prepositional
attachment sites. For example, in the above
sentence, the program would detect that wi h
could attach to saw. Using an iterative
algorithm, we initially create a table of co-
occurrence frequencies for 3-tuples of th  f rm
(V, P, N2) and (N1, P, N2). For each k possible
attachment site of a preposition P, we increment
the frequency of the corresponding 3-tuple by
1/k. For example, Table 2 shows the initial co-
occurrence frequency table for the
corresponding 3-tuples of the above sentence.
                                            
3Available at www.cs.ualberta.ca/~lindek/minipar.htm.
In the following iterations of the algorithm, we
update the frequency table as follows. For each k
possible attachment site of a preposition P, we
refine its attachment score using the formulas
described in Section 4: VScore(Vk, Pk, N2k) and
NScore(N1k, Pk, N2k). For any tuple (Wk, Pk, N2k),
where Wk is either Vk or N2k, we update its
frequ ncy as:
( ) ( )( )? =
= k
i iii
kkk
kNkPkW NPWScore
NPWScore
fr
1 2
2
2,,
,,
,,
where Score(Wk, Pk, N2k) = VScore(Wk, Pk, N2k)
if Wk = Vk; otherwise Score(Wk, Pk, N2k) =
NScore(Wk, Pk, N2k).
Suppose that after the initial frequency table is
set NScore(man, in, park) = 1.23, VScore(saw,
with, telescope) = 3.65, and NScore(dog, with,
telescope) = 0.35. Then, the updated co-
occurrence frequencies for (man, in, park) and
(saw, with, telescope) are:
fr(man, in, park) =    23.1
23.1 = 1.0
fr(saw, with, telescope) = 35.065.3
65.3
+ = 0.913
Table 3 shows the updated frequency table
after the first iteration of the algorithm. The
resulting database contained 8,900,000 triples.
3.2 Unambiguous Data Set
As in (Ratnaparkhi, 1998), we constructed a
training data set consisting of only unambiguous
Table 2. Initial co-occurrence frequency table entries for A
man in the park saw a dog with a telescope.
V OR N1 P N2 FREQUENCY
man in park 1.0
saw with telescope 0.5
dog with telescope 0.5
Table 3. Co-occurrence frequency table entries for A man
in the park saw a dog with a telescope after one iteration.
V OR N1 P N2 FREQUENCY
man in park 1.0
saw with telescope 0.913
dog with telescope 0.087
A  man  in  the  park  saw  a  dog  with  a  telescope.
det det det det
pcomppcomp
mod
subj
obj
mod
attachments of the form (V, P, N2) and (N1, P,
N2). We only extract a 3-tuple from a sentence
when our program finds no alternative
attachment site for its preposition. Each
extracted 3-tuple is assigned a frequency count
of 1. For example, in the previous sentence,
(man, in, park) is extracted since it contains only
one attachment site; (dog, with, telescope) is not
extracted since with has an alternative
attachment site. The resulting database
contained 4,400,000 triples.
4 Classification Model
Roth (1998) presented a unified framework for
natural language disambiguation tasks.
Essentially, several language learning algorithms
(e.g. na?ve Bayes estimation, back-off
estimation, transformation-based learning) were
successfully cast as learning linear separators in
their feature space. Roth modelled prepositional
phrase attachment as linear combinations of
features. The features consisted of all 15
possible sub-sequences of the 4-tuple (V, N1, P,
N2) shown in Table 4. The asterix (*) in features
represent wildcards.
Roth used supervised learning to adjust the
weights of the features. In our experiments, we
only considered features that contained P since
the preposition is the most important lexical item
(Collins and Brooks, 1995). Furthermore, we
omitted features that included both V and N1
since their co-occurrence is independent of the
attachment decision. The resulting subset of
features considered in our system is shown in
bold in Table 4 (equivalent to assigning a weight
of 0 or 1 to each feature).
Let |head, rel, mod| represent the frequency,
obtained from the training data, of the head
occurring in the given relationship rel with the
modifier. We then assign a score to each feature
as follows:
1. (*, *, P, *) = log(|*, P, *| / |*, *, *|)
2. (V, *, P, N2) = log(|V, P, N2| / |*, *, *|)
3. (*, N1, P, N2) = log(|N1, P, N2| / |*, *, *|)
4. (V, *, P, *) = log(|V, P, *| / |V, *, *|)
5. (*, N1, P, *) = log(|N1, P, *| / |N1, *, *|)
6. (*, *, P, N2) = log(|*, P, N2| / |*, *, N2|)
1, 2, and 3 are the prior probabilities of P, V P
N2, and N1 P N2 respectively. 4, 5, and 6
represent conditional probabilities P(V, P | V),
P(N1, P | N1), and P(P N2 | N2) respectively.
We estimate the adverbial and adjectival
attachment scores, VScore(V, P, N2) and
NScore(N1, P, N2), as a linear combination of
these features:
VScore(V, P, N2) =(*, *, P, *) + (V, *, P, N2) +
(V, *, P, *) + (*, *, P, N2)
NScore(N1, P, N2) =(*, *, P, *) + (*, N1, P, N2) +
(*, N1, P, *) + (*, *, P, N2)
For example, the attachment scores for (eat,
salad, with, fork) are VScore(eat, with, fork) =
-3.47 and NScore(salad, with, fork) = -4.77. The
model correctly assigns a higher score to the
dv rbial attachment.
5 Contextually Similar Words
The contextually similar words of a word w are
words similar to the intended meaning of w i  its
context. Below, we describe an algorithm for
constructing contextually similar words and we
present a method for approximating the
attachment scores using these words.
5.1 Algorithm
For our purposes, a context of w is simply a
dependency relationship involving w. For
example, a dependency relationship for aw in
the example sentence of Section 3 is
saw:obj:dog.  Figure 2 gives the data flow
diagram for our algorithm for constructing the
contextually similar words of w. We retrieve
from the collocation database the words that
occurred in the same dependency relationship as
w. We refer to this set of words as the cohort of
w for the dependency relationship. Consider the
words eat and salad in the context eat salad.
The cohort of eat consists of verbs that appeared
Table 4. The 15 features for prepositional phrase
attachment.
FEATURES
(V, *, *, *) (V, *, P, *) (*, N1, *, N2)
(V, N1, *, *) (V, *, *, N2) (*, N1, P, N2)
(V, N1, P, *) (V, *, P, N2) (*, *, P, *)
(V, N1, *, N2) (*, N1, *, *) (*, *, *, N2)
(V, N1, P, N2) (*, N1, P, *) (*, *, P, N2)
with object salad in Figure 1 (e.g. add, consume,
cover, ? ) and the cohort of salad consists of
nouns that appeared as object of eat in Figure 1
(e.g. almond, apple, bean,  ?).
Intersecting the set of similar words and the
cohort then forms the set of contextually similar
words of w. For example, Table 5 shows the
contextually similar words of eat and salad in
the context eat salad and the contextually
similar words of fork in the contexts eat with
fork and salad with fork. The words in the first
row are retrieved by intersecting the similar
words of eat in Table 1 with the cohort of eat
while the second row represents the intersection
of the similar words of alad in Table 1 and the
cohort of salad. The third and fourth rows are
determined in a similar manner. In the
nonsensical context salad with fork (in row 4),
no contextually similar words are found.
While previous word sense disambiguation
algorithms rely on a lexicon to provide sense
inventories of words, the contextually similar
words provide a way of distinguishing between
different senses of words without committing to
any particular sense inventory.
5.2 Attachment Approximation
Often, sparse data reduces our confidence in the
attachment scores of Section 4. Using
contextually similar words, we can approximate
these scores. Given the tuple (V, N1, P, N2),
adverbial attachments are approximated as
follows. We first construct a list CSV containing
the contextually similar words of V in context
V:obj:N1 and a list CSN2V containing the
contextually similar words of N2 in context
V:P:N2 (i.e. assuming adverbial attachment). For
each verb v in CSV, we compute VScore(v, P, N2)
and set SV as the average of the largest k of these
scores. Similarly, for each noun  in CSN2V, we
compute VScore(V, P, n) and set SN2V as the
average of the largest k of these scores. Then,
the approximated adverbial attachment score,
Vscore', is:
VScore'(V, P, N2) = max(SV, SN2V)
We approximate the adjectival attachment
score in a similar way. First, we construct a list
CSN1 containing the contextually similar words
of N1 in context V:obj:N1 and a list CSN2N1
containing the contextually similar words of N2
in context N1:P:N2 (i.e. assuming adjectival
attachment). Now, we compute SN1 as the
average of the largest k of NScore(n, P, N2) for
each noun  in CSN1 and SN2N1 as the average of
the largest k of NScore(N1, P, n) for each noun n
in CSN2N1. Then, the approximated adjectival
attachment score, NScore', is:
NScore'(N1, P, N2) = max(SN1, SN2N1)
For example, suppose we wish to approximate
the attachment score for the 4-tuple (eat, salad,
with, fork). First, we retrieve the contextually
similar words of eat and salad in context eat
salad, and the contextually similar words of fork
in contexts eat with fork and salad with fork as
shown in Table 5. Let k = 2. Table 6 shows the
calculation of SV and SN2V while the calculation
of SN1 and SN2N1 is shown in Table 7. Only the
Figure 2. Data flow diagram for identifying the
contextually similar words of a word in a dependency
relationship.
word in dependency
relationship
Similar Words Cohorts
Corpus-Based
Thesaurus
Retrieve
Intersect
Get Similar
Words
Collocation
DB
Contextually
Similar Words
Table 5. Contextually similar words of eat and salad.
WORD CONTEXT CONTEXTUALLY SIMILAR WORDS
EAT eat salad consume, taste, like, serve, pick,
harvest, love, sprinkle, Toss,?
SALAD eat salad soup, sandwich, pasta, dish, cheese,
vegetable, bread, meat, cake, bean, ?
FORK eat with fork spoon, knife, finger
FORK salad with fork ---
top k = 2 scores are shown in these tables. We
have:
VScore' (eat, with, fork) = max(SV, SN2V)
= -2.92
NScore' (salad, with, fork) = max(SN1, SN2N1)
= -4.87
Hence, the approximation correctly prefers the
adverbial attachment to the adjectival
attachment.
6 Attachment Algorithm
Figure 3 describes the prepositional phrase
attachment algorithm. As in previous
approaches, examples with P = of are always
classified as adjectival attachments.
Suppose we wish to approximate the
attachment score for the 4-tuple (eat, salad,
with, fork). From the previous section, Step 1
returns averageV = -2.92 and averageN1 = -4.87.
From Section 4, Step 2 gives aV = -3.47 and
aN1 = -4.77. In our training data, fV = 2.97 and
fN1 = 0, thus Step 3 gives f = 0.914. In Step 4, we
compute:
S(V) = -3.42 and
S(N1) = -4.78
Since S(V) > S(N1), the algorithm correctly
classifies this example as an adverbial
attachment.
Given the 4-tuple (eat, salad, with, croutons),
the algorithm returns S(V) = -4.31 and S(N1) =
-3.88. Hence, the algorithm correctly attaches
the prepositional phrase to the noun salad.
7 Experimental Results
In this section, we describe our test data and the
baseline for our experiments. Finally, we present
our results.
7.1 Test Data
The test data consists of 3097 examples derived
from the manually annotated attachments in the
Penn Treebank Wall Street Journal data
(Ratnaparkhi et al, 1994)4. Each line in the test
data consists of a 4-tuple and a target
classification: V N1 P N2 target.
                                            
4Available at ftp.cis.upenn.edu/pub/adwait/PPattachData.
The data set contains several erroneous tuples
and attachments. For instance, 133 examples
contain the word the as N1 or N2. There are also
improbable attachments such as (sing, birthday,
to, you) with the target attachment birthday.
Table 6. Calculation of SV and SN2V for (eat, salad, with,
fork).
4-TUPLE VSCORE
(mix, salad, with, fork) -2.60
(sprinkle, salad, with, fork) -3.24
SV -2.92
(eat, salad, with, spoon) -3.06
(eat, salad, with, finger) -3.50
SN2V -3.28
Table 7. Calculation of SN1 and SN2N1 for (eat, salad, with,
fork).
4-TUPLE NSCORE
(eat, pasta, with, fork) -4.71
(eat, cake, with, fork) -5.02
SN1 -4.87
--- n/a
--- n/a
SN2N1 n/a
Input: A 4-tuple (V, N1, P, N2)
Step 1: Using the contextually similar words algorithm
and the formulas from Section 5.2 compute:
averageV = VScore'(V, P, N2)
averageN1 = NScore'(N1, P, N2)
Step 2: Compute the adverbial attachment score, av,
and the adjectival attachment score, an1:
aV = VScore(V, P, N2)
aN1 = NScore(N1, P, N2)
Step 3: Retrieve from the training data set the
frequency of the 3-tuples (V, P, N2) and
(N1, P, N2) ? fV and fN1, respectively.
Let f = (fV + fN1 + 0.2) / (fV + fN1 +0.5)
Step 4: Combine the scores of Steps 1-3 to obtain the
final attachment scores:
S(V) = fav + (1 - f)averagev
S(N1) = fan1 + (1 - f)averagen1
Output:The attachment decision: N1 if S(N1) > S(V) or
P = of; V otherwise.
Figure 3. The prepositional phrase attachment algorithm.
7.2 Baseline
Choosing the most common attachment site, N1,
yields an accuracy of 58.96%. However, we
achieve 70.39% accuracy by classifying each
occurrence of P = of as N1, and V otherwise.
Human accuracy, given the full context of a
sentence, is 93.2% and drops to 88.2% when
given only tuples of the form (V, N1, P, N2)
(Ratnaparkhi et al, 1994). Assuming that human
accuracy is the upper bound for automatic
methods, we expect our accuracy to be bounded
above by 88.2% and below by 70.39%.
7.3 Results
We used the 3097-example testing corpus
described in Section 7.1. Table 8 presents the
precision and recall of our algorithm and Table 9
presents a performance comparison between our
system and previous supervised and
unsupervised approaches using the same test
data. We describe the different classifiers below:
clbase: the baseline described in Section 7.2
clR1: uses a maximum entropy model
(Ratnaparkhi et al, 1994)
clBR5: uses transformation-based learning (Brill
and Resnik, 1994)
clCB: uses a backed-off model (Collins and
Brooks, 1995)
clSN: induces a decision tree with a sense-tagged
corpus, using a semantic dictionary
(Stetina and Nagao, 1997)
clHR6: uses lexical preference (Hindle and Rooth,
1993)
clR2: uses a heuristic extraction of unambiguous
attachments (Ratnaparkhi, 1998)
clPL: uses the algorithm described in this paper
Our classifier outperforms all previous
unsupervised techniques and approaches the
performance of supervised algorithm.
We reconstructed the two earlier unsupervised
classifiers clHR and clR2. Table 10 presents the
accuracy of our reconstructed classifiers. The
originally reported accuracy for lR2 is within the
95% confidence interval of our reconstruction.
Our reconstruction of clHR achieved slightly
higher accuracy than the original report.
                                            
5The accuracy is reported in (Collins and Brooks, 1995).
6The accuracy was obtained on a smaller test set but, from
the same source as our test data.
Our classifier used a mixture of the two
training data sets described in Section 3. In
Table 11, we compare the performance of our
system on the following training data sets:
UNAMB: the data set of unambiguous examples
described in Section 3.2
EM0: the data set of Section 3.1 after
frequency table initialization
EM1: EM0 + one iteration of algorithm 3.1
EM2: EM0 + two iterations of algorithm 3.1
EM3: EM0 + three iterations of algorithm 3.1
1/8-EM1:one eighth of the data in EM1
MIX: The concatenation of UNAMB and EM1
Table 11 illustrates a slight but consistent
increase in performance when using contextually
similar words. However, since the confidence
intervals overlap, we cannot claim with certainty
Table 8. Precision and recall for attachment sites V and N1.
CLASS ACTUAL CORRECT INCORRECT PRECISION RECALL
V 1203 994 209 82.63% 78.21%
N1 1894 1617 277 84.31% 88.55%
Table 9. Performance comparison with other approaches.
METHOD LEARNING ACCURACY
CLBASE --- 70.39%
CLR1 supervised 81.6%
CLBR supervised 81.9%
CLCB supervised 84.5%
CLSN supervised 88.1%
CLHR unsupervised 75.8%
CLR2 unsupervised 81.91%
CLPL unsupervised 84.31%
Table 10. Accuracy of our reconstruction of (Hindle &
Rooth, 1993) and (Ratnaparkhi, 1998).
METHOD ORIGINAL
REPORTED
ACCURACY
RECONSTRUCTED
SYSTEM ACCURACY
(95% CONF)
CLHR 75.8% 78.40% ? 1.45%
CLR2 81.91% 82.40% ? 1.34%
that the contextually similar words improve
performance.
In Section 7.1, we mentioned some testing
examples contained N1 = the or N2 = the. For
supervised algorithms, the is represented in the
training set as any other noun. Consequently,
these algorithms collect training data for the nd
performance is not affected. However,
unsupervised methods break down on such
examples. In Table 12, we illustrate the
performance increase of our system when
removing these erroneous examples.
Conclusion and Future Work
The algorithms presented in this paper advance
the state of the art for unsupervised approaches
to prepositional phrase attachment and draws
near the performance of supervised methods.
Currently, we are exploring different functions
for combining contextually similar word
approximations with the attachment scores. A
promising approach considers the mutual
information between the prepositional
relationship of candidate attachments and N2. As
the mutual information decreases, our
confidence in the attachment score decreases
and the contextually similar word approximation
is weighted higher. Also, improving the
construction algorithm for contextually similar
words would possibly improve the accuracy of
the system. One approach first clusters the
similar words. Then, dependency relationships
are used to select the most representative
clusters as the contextually similar words. The
assumption is that more representative similar
words produce better approximations.
Acknowledgements
The authors wish to thank the reviewers for their
helpful comments. This research was partly
supported by Natural Sciences and Engineering
Research Council of Canada grant OGP121338
and scholarship PGSB207797.
References
Altmann, G. and Steedman, M. 1988. Interaction with Context
During Human Sentence Processing. Cognition, 30:191-238.
Brill, E. 1995. Transformation-based Error-driven Learning and
Natural Language Processing: A case study in part of speech
tagging. Computational Linguistics, December.
Brill, E. and Resnik. P. 1994. A Rule-Based Approach to
Prepositional Phrase Attachment Disambiguation. In
Proceedings of COLING-94. Kyoto, Japan.
Collins, M. and Brooks, J. 1995. Prepositional Phrase Attachment
through a Backed-off Model. In Proceedings of the Third
Workshop on Very Large Corpora, pp. 27-38. Cambridge,
Massachusetts.
Hindle, D. and Rooth, M. 1993. Structural Ambiguity and Lexical
Relations. Computational Linguistics, 19(1):103-120.
Lin, D. 1999. Automatic Identification of Non-Compositional
Phrases. In Proceedings of ACL-99, pp. 317-324. College Park,
Maryland.
Lin, D. 1998a. Extracting Collocations from Text Corpora.
Workshop on Computational Terminology. Montreal, Canada.
Lin, D. 1998b. Automatic Retrieval and Clustering of Similar
Words. In Proceedings of COLING-ACL98. Montreal, Canada.
Lin, D. (1994). Principar - an Efficient, Broad-Coverage,
Principle-Based Parser. In Proceedings of COLING-94. Kyoto,
Japan.
Miller, G. 1990. Wordnet: an On-Line Lexical Database.
International Journal of Lexicography, 1990.
Ratnaparkhi, A. 1998. Unsupervised Statistical Models for
Prepositional Phrase Attachment. In Proceedings of COLING-
ACL98. Montreal, Canada.
Ratnaparkhi, A., Reynar, J., and Roukos, S. 1994. A Maximum
Entropy Model for Prepositional Phrase Attachment. In
Proceedings of the ARPA Human Language Technology
Workshop, pp. 250-255. Plainsboro, N.J.
Roth, D. 1998. Learning to Resolve Natural Language
Ambiguities: A Unified Approach. In Proceedings of AAAI-98,
pp. 806-813. Madison, Wisconsin.
Stetina, J. and Nagao, M. 1997. Corpus Based PP Attachment
Ambiguity Resolution with a Semantic Dictionary. In
Proceedings of the Fifth Workshop on Very Large Corpora, pp.
66-80. Beijing and Hong Kong.
Table 11. Performance comparison of different data sets.
DATABASE ACCURACY
WITHOUT
SIMWORDS
(95% CONF)
ACCURACY
WITH
SIMWORDS
(95% CONF)
UNAMBIGUOUS 83.15% ? 1.32% 83.60% ? 1.30%
EM0 82.24% ? 1.35% 82.69% ? 1.33%
EM1 83.76% ? 1.30% 83.92% ? 1.29%
EM2 83.66% ? 1.30% 83.70% ? 1.31%
EM3 83.20% ? 1.32% 83.20% ? 1.32%
1/8-EM1 82.98% ? 1.32% 83.15% ? 1.32%
MIX 84.11% ? 1.29% 84.31% ? 1.28%
Table 12. Performance with removal of the asN1 or N2.
DATA SET ACCURACY
WITHOUT
SIMWORDS
(95% CONF)
ACCURACY
WITH
SIMWORDS
(95% CONF)
WITH THE 84.11% ? 1.29% 84.31% ? 1.32%
WITHOUT THE 84.44% ? 1.31% 84.65% ? 1.30%
A Probability Model to Improve Word Alignment
Colin Cherry and Dekang Lin
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{colinc,lindek}@cs.ualberta.ca
Abstract
Word alignment plays a crucial role in sta-
tistical machine translation. Word-aligned
corpora have been found to be an excellent
source of translation-related knowledge.
We present a statistical model for comput-
ing the probability of an alignment given a
sentence pair. This model allows easy in-
tegration of context-specific features. Our
experiments show that this model can be
an effective tool for improving an existing
word alignment.
1 Introduction
Word alignments were first introduced as an in-
termediate result of statistical machine translation
systems (Brown et al, 1993). Since their intro-
duction, many researchers have become interested
in word alignments as a knowledge source. For
example, alignments can be used to learn transla-
tion lexicons (Melamed, 1996), transfer rules (Car-
bonell et al, 2002; Menezes and Richardson, 2001),
and classifiers to find safe sentence segmentation
points (Berger et al, 1996).
In addition to the IBM models, researchers have
proposed a number of alternative alignment meth-
ods. These methods often involve using a statistic
such as ?2 (Gale and Church, 1991) or the log likeli-
hood ratio (Dunning, 1993) to create a score to mea-
sure the strength of correlation between source and
target words. Such measures can then be used to
guide a constrained search to produce word align-
ments (Melamed, 2000).
It has been shown that once a baseline alignment
has been created, one can improve results by using
a refined scoring metric that is based on the align-
ment. For example Melamed uses competitive link-
ing along with an explicit noise model in (Melamed,
2000) to produce a new scoring metric, which in turn
creates better alignments.
In this paper, we present a simple, flexible, sta-
tistical model that is designed to capture the infor-
mation present in a baseline alignment. This model
allows us to compute the probability of an align-
ment for a given sentence pair. It also allows for
the easy incorporation of context-specific knowl-
edge into alignment probabilities.
A critical reader may pose the question, ?Why in-
vent a new statistical model for this purpose, when
existing, proven models are available to train on a
given word alignment?? We will demonstrate exper-
imentally that, for the purposes of refinement, our
model achieves better results than a comparable ex-
isting alternative.
We will first present this model in its most general
form. Next, we describe an alignment algorithm that
integrates this model with linguistic constraints in
order to produce high quality word alignments. We
will follow with our experimental results and dis-
cussion. We will close with a look at how our work
relates to other similar systems and a discussion of
possible future directions.
2 Probability Model
In this section we describe our probability model.
To do so, we will first introduce some necessary no-
tation. Let E be an English sentence e1, e2, . . . , em
and let F be a French sentence f1, f2, . . . , fn. We
define a link l(ei, fj) to exist if ei and fj are a trans-
lation (or part of a translation) of one another. We
define the null link l(ei, f0) to exist if ei does not
correspond to a translation for any French word in
F . The null link l(e0, fj) is defined similarly. An
alignment A for two sentences E and F is a set of
links such that every word in E and F participates in
at least one link, and a word linked to e0 or f0 partic-
ipates in no other links. If e occurs in E x times and
f occurs in F y times, we say that e and f co-occur
xy times in this sentence pair.
We define the alignment problem as finding the
alignment A that maximizes P (A|E,F ). This cor-
responds to finding the Viterbi alignment in the
IBM translation systems. Those systems model
P (F,A|E), which when maximized is equivalent to
maximizing P (A|E,F ). We propose here a system
which models P (A|E,F ) directly, using a different
decomposition of terms.
In the IBM models of translation, alignments exist
as artifacts of which English words generated which
French words. Our model does not state that one
sentence generates the other. Instead it takes both
sentences as given, and uses the sentences to deter-
mine an alignment. An alignment A consists of t
links {l1, l2, . . . , lt}, where each lk = l(eik , fjk) for
some ik and jk. We will refer to consecutive subsets
of A as lji = {li, li+1, . . . , lj}. Given this notation,
P (A|E,F ) can be decomposed as follows:
P (A|E,F ) = P (lt1|E,F ) =
t?
k=1
P (lk|E,F, l
k?1
1 )
At this point, we must factor P (lk|E,F, lk?11 ) to
make computation feasible. Let Ck = {E,F, lk?11 }
represent the context of lk. Note that both the con-
text Ck and the link lk imply the occurrence of eik
and fjk . We can rewrite P (lk|Ck) as:
P (lk|Ck) =
P (lk, Ck)
P (Ck)
=
P (Ck|lk)P (lk)
P (Ck, eik , fjk)
=
P (Ck|lk)
P (Ck|eik , fjk)
?
P (lk, eik , fjk)
P (eik , fjk)
= P (lk|eik , fjk)?
P (Ck|lk)
P (Ck|eik , fjk)
Here P (lk|eik , fjk) is link probability given a co-
occurrence of the two words, which is similar in
spirit to Melamed?s explicit noise model (Melamed,
2000). This term depends only on the words in-
volved directly in the link. The ratio P (Ck|lk)P (Ck|eik ,fjk )
modifies the link probability, providing context-
sensitive information.
Up until this point, we have made no simplify-
ing assumptions in our derivation. Unfortunately,
Ck = {E,F, l
k?1
1 } is too complex to estimate con-
text probabilities directly. Suppose FTk is a set
of context-related features such that P (lk|Ck) can
be approximated by P (lk|eik , fjk , FTk). Let C ?k =
{eik , fjk}?FTk. P (lk|C
?
k) can then be decomposed
using the same derivation as above.
P (lk|C
?
k) = P (lk|eik , fjk)?
P (C ?k|lk)
P (C ?k|eik , fjk)
= P (lk|eik , fjk)?
P (FTk|lk)
P (FTk|eik , fjk)
In the second line of this derivation, we can drop
eik and fjk from C ?k, leaving only FTk, because they
are implied by the events which the probabilities are
conditionalized on. Now, we are left with the task
of approximating P (FTk|lk) and P (FTk|eik , fjk).
To do so, we will assume that for all ft ? FTk,
ft is conditionally independent given either lk or
(eik , fjk). This allows us to approximate alignment
probability P (A|E,F ) as follows:
t?
k=1
?
?P (lk|eik , fjk)?
?
ft?FTk
P (ft|lk)
P (ft|eik , fjk)
?
?
In any context, only a few features will be ac-
tive. The inner product is understood to be only over
those features ft that are present in the current con-
text. This approximation will cause P (A|E,F ) to
no longer be a well-behaved probability distribution,
though as in Naive Bayes, it can be an excellent es-
timator for the purpose of ranking alignments.
If we have an aligned training corpus, the prob-
abilities needed for the above equation are quite
easy to obtain. Link probabilities can be deter-
mined directly from |lk| (link counts) and |eik , fj,k|
(co-occurrence counts). For any co-occurring pair
of words (eik , fjk), we check whether it has the
feature ft. If it does, we increment the count of
|ft, eik , fjk |. If this pair is also linked, then we in-
crement the count of |ft, lk|. Note that our definition
of FTk allows for features that depend on previous
links. For this reason, when determining whether or
not a feature is present in a given context, one must
impose an ordering on the links. This ordering can
be arbitrary as long as the same ordering is used in
training1 and probability evaluation. A simple solu-
tion would be to order links according their French
words. We choose to order links according to the
link probability P (lk|eik , fjk) as it has an intuitive
appeal of allowing more certain links to provide con-
text for others.
We store probabilities in two tables. The first ta-
ble stores link probabilities P (lk|eik , fjk). It has an
entry for every word pair that was linked at least
once in the training corpus. Its size is the same as
the translation table in the IBM models. The sec-
ond table stores feature probabilities, P (ft|lk) and
P (ft|eik , fjk). For every linked word pair, this table
has two entries for each active feature. In the worst
case this table will be of size 2?|FT |?|E|?|F |. In
practice, it is much smaller as most contexts activate
only a small number of features.
In the next subsection we will walk through a sim-
ple example of this probability model in action. We
will describe the features used in our implementa-
tion of this model in Section 3.2.
2.1 An Illustrative Example
Figure 1 shows an aligned corpus consisting of
one sentence pair. Suppose that we are concerned
with only one feature ft that is active2 for eik
and fjk if an adjacent pair is an alignment, i.e.,
l(eik?1, fjk?1) ? l
k?1
1 or l(eik+1, fjk+1) ? l
k?1
1 .
This example would produce the probability tables
shown in Table 1.
Note how ft is active for the (a, v) link, and is
not active for the (b, u) link. This is due to our se-
lected ordering. Table 1 allows us to calculate the
probability of this alignment as:
1In our experiments, the ordering is not necessary during
training to achieve good performance.
2Throughout this paper we will assume that null alignments
are special cases, and do not activate or participate in features
unless otherwise stated in the feature description.
a b a
u v v
e
f0
0
Figure 1: An Example Aligned Corpus
Table 1: Example Probability Tables
(a) Link Counts and Probabilities
eik fjk |lk| |eik , fjk | P (lk|eik , fjk)
b u 1 1 1
a f0 1 2 12
e0 v 1 2 12
a v 1 4 14
(b) Feature Counts
eik fjk |ft, lk| |ft, eik , fjk |
a v 1 1
(c) Feature Probabilities
eik fjk P (ft|lk) P (ft|eik , fjk)
a v 1 14
P (A|E,F ) = P (l(b, u)|b, u)?
P (l(a, f0)|a, f0)?
P (l(e0, v)|e0, v)?
P (l(a, v)|a, v)P (ft|l(a,v))P (ft|a,v)
= 1? 12 ?
1
2 ?
1
4 ?
1
1
4
= 14
3 Word-Alignment Algorithm
In this section, we describe a world-alignment al-
gorithm guided by the alignment probability model
derived above. In designing this algorithm we have
selected constraints, features and a search method
in order to achieve high performance. The model,
however, is general, and could be used with any in-
stantiation of the above three factors. This section
will describe and motivate the selection of our con-
straints, features and search method.
The input to our word-alignment algorithm con-
sists of a pair of sentences E and F , and the depen-
dency tree TE for E. TE allows us to make use of
features and constraints that are based on linguistic
intuitions.
3.1 Constraints
The reader will note that our alignment model as de-
scribed above has very few factors to prevent unde-
sirable alignments, such as having all French words
align to the same English word. To guide the model
to correct alignments, we employ two constraints to
limit our search for the most probable alignment.
The first constraint is the one-to-one constraint
(Melamed, 2000): every word (except the null words
e0 and f0) participates in exactly one link.
The second constraint, known as the cohesion
constraint (Fox, 2002), uses the dependency tree
(Mel?c?uk, 1987) of the English sentence to restrict
possible link combinations. Given the dependency
tree TE , the alignment can induce a dependency tree
for F (Hwa et al, 2002). The cohesion constraint
requires that this induced dependency tree does not
have any crossing dependencies. The details about
how the cohesion constraint is implemented are out-
side the scope of this paper.3 Here we will use a sim-
ple example to illustrate the effect of the constraint.
Consider the partial alignment in Figure 2. When
the system attempts to link of and de, the new link
will induce the dotted dependency, which crosses a
previously induced dependency between service and
donne?es. Therefore, of and de will not be linked.
 the status of the data service
l' ?tat du service de donn?es
nn
det
pcomp
moddet
Figure 2: An Example of Cohesion Constraint
3.2 Features
In this section we introduce two types of features
that we use in our implementation of the probabil-
ity model described in Section 2. The first feature
3The algorithm for checking the cohesion constraint is pre-
sented in a separate paper which is currently under review.
the host discovers all the devices
det
subj pre
det
obj
 l'  h?te rep?re tous les p?riph?riques
1 2 3 4 5
1 2 3 4 5 6
6
the host locate all the peripherals
Figure 3: Feature Extraction Example
type fta concerns surrounding links. It has been ob-
served that words close to each other in the source
language tend to remain close to each other in the
translation (Vogel et al, 1996; Ker and Change,
1997). To capture this notion, for any word pair
(ei, fj), if a link l(ei? , fj?) exists where i? 2 ? i? ?
i + 2 and j ? 2 ? j? ? j + 2, then we say that the
feature fta(i?i?, j?j?, ei?) is active for this context.
We refer to these as adjacency features.
The second feature type ftd uses the English
parse tree to capture regularities among grammati-
cal relations between languages. For example, when
dealing with French and English, the location of
the determiner with respect to its governor4 is never
swapped during translation, while the location of ad-
jectives is swapped frequently. For any word pair
(ei, fj), let ei? be the governor of ei, and let rel be
the relationship between them. If a link l(ei? , fj?)
exists, then we say that the feature ftd(j?j?, rel) is
active for this context. We refer to these as depen-
dency features.
Take for example Figure 3 which shows a par-
tial alignment with all links completed except for
those involving ?the?. Given this sentence pair and
English parse tree, we can extract features of both
types to assist in the alignment of the1. The word
pair (the1, l?) will have an active adjacency feature
fta(+1,+1, host) as well as a dependency feature
ftd(?1, det). These two features will work together
to increase the probability of this correct link. In
contrast, the incorrect link (the1, les) will have only
ftd(+3, det), which will work to lower the link
probability, since most determiners are located be-
4The parent node in the dependency tree.
fore their governors.
3.3 Search
Due to our use of constraints, when seeking the
highest probability alignment, we cannot rely on a
method such as dynamic programming to (implic-
itly) search the entire alignment space. Instead, we
use a best-first search algorithm (with constant beam
and agenda size) to search our constrained space of
possible alignments. A state in this space is a par-
tial alignment. A transition is defined as the addi-
tion of a single link to the current state. Any link
which would create a state that does not violate any
constraint is considered to be a valid transition. Our
start state is the empty alignment, where all words in
E and F are linked to null. A terminal state is a state
in which no more links can be added without violat-
ing a constraint. Our goal is to find the terminal state
with highest probability.
For the purposes of our best-first search, non-
terminal states are evaluated according to a greedy
completion of the partial alignment. We build this
completion by adding valid links in the order of
their unmodified link probabilities P (l|e, f) until no
more links can be added. The score the state receives
is the probability of its greedy completion. These
completions are saved for later use (see Section 4.2).
4 Training
As was stated in Section 2, our probability model
needs an initial alignment in order to create its prob-
ability tables. Furthermore, to avoid having our
model learn mistakes and noise, it helps to train on a
set of possible alignments for each sentence, rather
than one Viterbi alignment. In the following sub-
sections we describe the creation of the initial align-
ments used for our experiments, as well as our sam-
pling method used in training.
4.1 Initial Alignment
We produce an initial alignment using the same al-
gorithm described in Section 3, except we maximize
summed ?2 link scores (Gale and Church, 1991),
rather than alignment probability. This produces a
reasonable one-to-one word alignment that we can
refine using our probability model.
4.2 Alignment Sampling
Our use of the one-to-one constraint and the cohe-
sion constraint precludes sampling directly from all
possible alignments. These constraints tie words in
such a way that the space of alignments cannot be
enumerated as in IBM models 1 and 2 (Brown et
al., 1993). Taking our lead from IBM models 3, 4
and 5, we will sample from the space of those high-
probability alignments that do not violate our con-
straints, and then redistribute our probability mass
among our sample.
At each search state in our alignment algorithm,
we consider a number of potential links, and select
between them using a heuristic completion of the re-
sulting state. Our sample S of possible alignments
will be the most probable alignment, plus the greedy
completions of the states visited during search. It
is important to note that any sampling method that
concentrates on complete, valid and high probabil-
ity alignments will accomplish the same task.
When collecting the statistics needed to calcu-
late P (A|E,F ) from our initial ?2 alignment, we
give each s ? S a uniform weight. This is rea-
sonable, as we have no probability estimates at this
point. When training from the alignments pro-
duced by our model, we normalize P (s|E,F ) so
that
?
s?S P (s|E,F ) = 1. We then count links and
features in S according to these normalized proba-
bilities.
5 Experimental Results
We adopted the same evaluation methodology as in
(Och and Ney, 2000), which compared alignment
outputs with manually aligned sentences. Och and
Ney classify manual alignments into two categories:
Sure (S) and Possible (P ) (S?P ). They defined the
following metrics to evaluate an alignment A:
recall = |A?S||S| precision =
|A?P |
|P |
alignment error rate (AER) = |A?S|+|A?P ||S|+|P |
We trained our alignment program with the same
50K pairs of sentences as (Och and Ney, 2000) and
tested it on the same 500 manually aligned sen-
tences. Both the training and testing sentences are
from the Hansard corpus. We parsed the training
Table 2: Comparison with (Och and Ney, 2000)
Method Prec Rec AER
Ours 95.7 86.4 8.7
IBM-4 F?E 80.5 91.2 15.6
IBM-4 E?F 80.0 90.8 16.0
IBM-4 Intersect 95.7 85.6 9.0
IBM-4 Refined 85.9 92.3 11.7
and testing corpora with Minipar.5 We then ran the
training procedure in Section 4 for three iterations.
We conducted three experiments using this
methodology. The goal of the first experiment is to
compare the algorithm in Section 3 to a state-of-the-
art alignment system. The second will determine
the contributions of the features. The third experi-
ment aims to keep all factors constant except for the
model, in an attempt to determine its performance
when compared to an obvious alternative.
5.1 Comparison to state-of-the-art
Table 2 compares the results of our algorithm with
the results in (Och and Ney, 2000), where an HMM
model is used to bootstrap IBM Model 4. The rows
IBM-4 F?E and IBM-4 E?F are the results ob-
tained by IBM Model 4 when treating French as the
source and English as the target or vice versa. The
row IBM-4 Intersect shows the results obtained by
taking the intersection of the alignments produced
by IBM-4 E?F and IBM-4 F?E. The row IBM-4
Refined shows results obtained by refining the inter-
section of alignments in order to increase recall.
Our algorithm achieved over 44% relative error
reduction when compared with IBM-4 used in ei-
ther direction and a 25% relative error rate reduc-
tion when compared with IBM-4 Refined. It also
achieved a slight relative error reduction when com-
pared with IBM-4 Intersect. This demonstrates that
we are competitive with the methods described in
(Och and Ney, 2000). In Table 2, one can see that
our algorithm is high precision, low recall. This was
expected as our algorithm uses the one-to-one con-
straint, which rules out many of the possible align-
ments present in the evaluation data.
5available at http://www.cs.ualberta.ca/? lindek/minipar.htm
Table 3: Evaluation of Features
Algorithm Prec Rec AER
initial (?2) 88.9 84.6 13.1
without features 93.7 84.8 10.5
with ftd only 95.6 85.4 9.3
with fta only 95.9 85.8 9.0
with fta and ftd 95.7 86.4 8.7
5.2 Contributions of Features
Table 3 shows the contributions of features to our al-
gorithm?s performance. The initial (?2) row is the
score for the algorithm (described in Section 4.1)
that generates our initial alignment. The without fea-
tures row shows the score after 3 iterations of refine-
ment with an empty feature set. Here we can see that
our model in its simplest form is capable of produc-
ing a significant improvement in alignment quality.
The rows with ftd only and with fta only describe
the scores after 3 iterations of training using only de-
pendency and adjacency features respectively. The
two features provide significant contributions, with
the adjacency feature being slightly more important.
The final row shows that both features can work to-
gether to create a greater improvement, despite the
independence assumptions made in Section 2.
5.3 Model Evaluation
Even though we have compared our algorithm to
alignments created using IBM statistical models, it
is not clear if our model is essential to our perfor-
mance. This experiment aims to determine if we
could have achieved similar results using the same
initial alignment and search algorithm with an alter-
native model.
Without using any features, our model is similar
to IBM?s Model 1, in that they both take into account
only the word types that participate in a given link.
IBM Model 1 uses P (f |e), the probability of f be-
ing generated by e, while our model uses P (l|e, f),
the probability of a link existing between e and f .
In this experiment, we set Model 1 translation prob-
abilities according to our initial ?2 alignment, sam-
pling as we described in Section 4.2. We then use the
?n
j=1 P (fj |eaj ) to evaluate candidate alignments in
a search that is otherwise identical to our algorithm.
We ran Model 1 refinement for three iterations and
Table 4: P (l|e, f) vs. P (f |e)
Algorithm Prec Rec AER
initial (?2) 88.9 84.6 13.1
P (l|e, f) model 93.7 84.8 10.5
P (f |e) model 89.2 83.0 13.7
recorded the best results that it achieved.
It is clear from Table 4 that refining our initial ?2
alignment using IBM?s Model 1 is less effective than
using our model in the same manner. In fact, the
Model 1 refinement receives a lower score than our
initial alignment.
6 Related Work
6.1 Probability models
When viewed with no features, our proba-
bility model is most similar to the explicit
noise model defined in (Melamed, 2000). In
fact, Melamed defines a probability distribution
P (links(u, v)|cooc(u, v), ?+, ??) which appears to
make our work redundant. However, this distribu-
tion refers to the probability that two word types u
and v are linked links(u, v) times in the entire cor-
pus. Our distribution P (l|e, f) refers to the proba-
bility of linking a specific co-occurrence of the word
tokens e and f . In Melamed?s work, these probabil-
ities are used to compute a score based on a prob-
ability ratio. In our work, we use the probabilities
directly.
By far the most prominent probability models in
machine translation are the IBM models and their
extensions. When trying to determine whether two
words are aligned, the IBM models ask, ?What is
the probability that this English word generated this
French word?? Our model asks instead, ?If we are
given this English word and this French word, what
is the probability that they are linked?? The dis-
tinction is subtle, yet important, introducing many
differences. For example, in our model, E and F
are symmetrical. Furthermore, we model P (l|e, f ?)
and P (l|e, f ??) as unrelated values, whereas the IBM
model would associate them in the translation prob-
abilities t(f ?|e) and t(f ??|e) through the constraint
?
f t(f |e) = 1. Unfortunately, by conditionalizing
on both words, we eliminate a large inductive bias.
This prevents us from starting with uniform proba-
bilities and estimating parameters with EM. This is
why we must supply the model with a noisy initial
alignment, while IBM can start from an unaligned
corpus.
In the IBM framework, when one needs the model
to take new information into account, one must cre-
ate an extended model which can base its parame-
ters on the previous model. In our model, new in-
formation can be incorporated modularly by adding
features. This makes our work similar to maximum
entropy-based machine translation methods, which
also employ modular features. Maximum entropy
can be used to improve IBM-style translation prob-
abilities by using features, such as improvements to
P (f |e) in (Berger et al, 1996). By the same token
we can use maximum entropy to improve our esti-
mates of P (lk|eik , fjk , Ck). We are currently inves-
tigating maximum entropy as an alternative to our
current feature model which assumes conditional in-
dependence among features.
6.2 Grammatical Constraints
There have been many recent proposals to leverage
syntactic data in word alignment. Methods such as
(Wu, 1997), (Alshawi et al, 2000) and (Lopez et al,
2002) employ a synchronous parsing procedure to
constrain a statistical alignment. The work done in
(Yamada and Knight, 2001) measures statistics on
operations that transform a parse tree from one lan-
guage into another.
7 Future Work
The alignment algorithm described here is incapable
of creating alignments that are not one-to-one. The
model we describe, however is not limited in the
same manner. The model is currently capable of
creating many-to-one alignments so long as the null
probabilities of the words added on the ?many? side
are less than the probabilities of the links that would
be created. Under the current implementation, the
training corpus is one-to-one, which gives our model
no opportunity to learn many-to-one alignments.
We are pursuing methods to create an extended
algorithm that can handle many-to-one alignments.
This would involve training from an initial align-
ment that allows for many-to-one links, such as one
of the IBM models. Features that are related to
multiple links should be added to our set of feature
types, to guide intelligent placement of such links.
8 Conclusion
We have presented a simple, flexible, statistical
model for computing the probability of an alignment
given a sentence pair. This model allows easy in-
tegration of context-specific features. Our experi-
ments show that this model can be an effective tool
for improving an existing word alignment.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
P. F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?312.
Jaime Carbonell, Katharina Probst, Erik Peterson, Chris-
tian Monson, Alon Lavie, Ralf Brown, and Lori Levin.
2002. Automatic rule learning for resource-limited mt.
In Proceedings of AMTA-02, pages 1?10.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74, March.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proceedings of EMNLP-02,
pages 304?311.
W.A. Gale and K.W. Church. 1991. Identifying word
correspondences in parallel texts. In Proceedings
of the 4th Speech and Natural Language Workshop,
pages 152?157. DARPA, Morgan Kaufmann.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating translational correspondence
using annotation projection. In Proceeding of ACL-02,
pages 392?399.
Sue J. Ker and Jason S. Change. 1997. Aligning more
words with high precision for small bilingual cor-
pora. Computational Linguistics and Chinese Lan-
guage Processing, 2(2):63?96, August.
Adam Lopez, Michael Nossal, Rebecca Hwa, and Philip
Resnik. 2002. Word-level alignment for multilingual
resource acquisition. In Proceedings of the Workshop
on Linguistic Knowledge Acquisition and Representa-
tion: Bootstrapping Annotated Language Data.
I. Dan Melamed. 1996. Automatic construction of clean
broad-coverage translation lexicons. In Proceedings
of the 2nd Conference of the Association for Machine
Translation in the Americas, pages 125?134, Mon-
treal.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249, June.
Igor A. Mel?c?uk. 1987. Dependency syntax: theory and
practice. State University of New York Press, Albany.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the Workshop on Data-Driven Machine Trans-
lation.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447, Hong Kong, China, Octo-
ber.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-based
word alignment in statistical translation. In Proceed-
ings of COLING-96, pages 836?841, Copenhagen,
Denmark, August.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):374?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Meeting of the Associ-
ation for Computational Linguistics, pages 523?530.
ProAlign: Shared Task System Description
Dekang Lin and Colin Cherry
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{lindek,colinc}@cs.ualberta.ca
Abstract
ProAlign combines several different ap-
proaches in order to produce high quality word
word alignments. Like competitive linking,
ProAlign uses a constrained search to find high
scoring alignments. Like EM-based methods,
a probability model is used to rank possible
alignments. The goal of this paper is to give
a bird?s eye view of the ProAlign system to
encourage discussion and comparison.
1 Alignment Algorithm at a Glance
We have submitted the ProAlign alignment system to
the WPT?03 shared task. It received a 5.71% AER on
the English-French task and 29.36% on the Romanian-
English task. These results are with the no-null data; our
output was not formatted to work with explicit nulls.
ProAlign works by iteratively improving an align-
ment. The algorithm creates an initial alignment us-
ing search, constraints, and summed ?2 correlation-based
scores (Gale and Church, 1991). This is similar to the
competitive linking process (Melamed, 2000). It then
learns a probability model from the current alignment,
and conducts a constrained search again, this time scor-
ing alignments according to the probability model. The
process continues until results on a validation set begin to
indicate over-fitting.
For the purposes of our algorithm, we view an align-
ment as a set of links between the words in a sen-
tence pair. Before describing the algorithm, we will de-
fine the following notation. Let E be an English sen-
tence e1, e2, . . . , em and let F be a French sentence
f1, f2, . . . , fn. We define a link l(ei, fj) to exist if ei and
fj are a translation (or part of a translation) of one an-
other. We define the null link l(ei, f0) to exist if ei does
not correspond to a translation for any French word in F .
The null link l(e0, fj) is defined similarly. An alignment
A for two sentences E and F is a set of links such that ev-
ery word in E and F participates in at least one link, and
a word linked to e0 or f0 participates in no other links. If
e occurs in E x times and f occurs in F y times, we say
that e and f co-occur xy times in this sentence pair.
ProAlign conducts a best-first search (with constant
beam and agenda size) to search a constrained space of
possible alignments. A state in this space is a partial
alignment, and a transition is defined as the addition of
a single link to the current state. Any link which would
create a state that does not violate any constraint is con-
sidered to be a valid transition. Our start state is the empty
alignment, where all words in E and F are implicitly
linked to null. A terminal state is a state in which no more
links can be added without violating a constraint. Our
goal is to find the terminal state with the highest proba-
bility.
To complete this algorithm, one requires a set of con-
straints and a method for determining which alignment is
most likely. These are presented in the next two sections.
The algorithm takes as input a set of English-French sen-
tence pairs, along with dependency trees for the English
sentences. The presence of the English dependency tree
allows us to incorporate linguistic features into our model
and linguistic intuitions into our constraints.
2 Constraints
The model used for scoring alignments has no mecha-
nism to prevent certain types of undesirable alignments,
such as having all French words align to the same En-
glish word. To guide the search to correct alignments, we
employ two constraints to limit our search for the most
probable alignment. The first constraint is the one-to-one
constraint (Melamed, 2000): every word (except the null
words e0 and f0) participates in exactly one link.
The second constraint, known as the cohesion con-
straint (Fox, 2002), uses the dependency tree (Mel?c?uk,
1987) of the English sentence to restrict possible link
combinations. Given the dependency tree TE and a (par-
tial) alignment A, the cohesion constraint requires that
phrasal cohesion is maintained in the French sentence. If
two phrases are disjoint in the English sentence, the align-
ment must not map them to overlapping intervals in the
French sentence. This notion of phrasal constraints on
alignments need not be restricted to phrases determined
from a dependency structure. However, the experiments
conducted in (Fox, 2002) indicate that dependency trees
demonstrate a higher degree of phrasal cohesion during
translation than other structures.
Consider the partial alignment in Figure 1. The most
probable lexical match for the English word to is the
French word a`. When the system attempts to link to and
a`, the distinct English phrases [the reboot] and [the host
to discover all the devices] will be mapped to intervals
in the French sentence, creating the induced phrasal in-
tervals [a` . . . [re?initialisation] . . . pe?riphe?riques]. Re-
gardless of what these French phrases will be after the
alignment is completed, we know now that their intervals
will overlap. Therefore, this link will not be added to the
partial alignment.
The?reboot?causes?the?host?to?discover?all?the?devices?
det? subj? det? subj?aux?
pre?
det?
obj?mod?
  ?    la?Suite? r?initialisation  ,? l'  h?te?rep?re?tous?les?p?riph?riques?
after?to? the? reboot? the?host? locate? all? the? peripherals?
1? 2? 3? 4? 5?6? 7? 8? 9? 10?
1? 2? 3? 4? 5?6?7? 8? 9?10? 11?
Figure 1: An Example of Cohesion Constraint
To define this notion more formally, let TE(ei) be
the subtree of TE rooted at ei. The phrase span of
ei, spanP(ei, TE , A), is the image of the English phrase
headed by ei in F given a (partial) alignment A. More
precisely, spanP(ei, TE , A) = [k1, k2], where
k1 = min{j|l(u, j) ? A, eu ? TE(ei)}
k2 = max{j|l(u, j) ? A, eu ? TE(ei)}
The head span is the image of ei itself. We define
spanH(ei, TE , A) = [k1, k2], where
k1 = min{j|l(i, j) ? A}
k2 = max{j|l(i, j) ? A}
In Figure 1, for the node reboot, the phrase span is
[4,4] and the head span is also [4,4]; for the node discover
(with the link between to and a` in place), the phrase span
is [2,11] and the head span is the empty set ?.
With these definitions of phrase and head spans, we de-
fine two notions of overlap, originally introduced in (Fox,
2002) as crossings. Given a head node eh and its modi-
fier em, a head-modifier overlap occurs when:
spanH(eh, TE , A) ? spanP(em, TE , A) 6= ?
Given two nodes em1 and em2 which both modify the
same head node, a modifier-modifier overlap occurs
when:
spanP(em1 , TE , A) ? spanP(em2 , TE , A) 6= ?
Following (Fox, 2002), we say an alignment is co-
hesive with respect to TE if it does not introduce
any head-modifier or modifier-modifier overlaps. For
example, the alignment A in Figure 1 is not cohe-
sive because spanP (reboot, TE , A) = [4, 4] intersects
spanP (discover, TE , A) = [2, 11]. Since both reboot
and discover modify causes, this creates a modifier-
modifier overlap. One can check for constraint viola-
tions inexpensively by incrementally updating the vari-
ous spans as new links are added to the partial alignment,
and checking for overlap after each modification. More
details on the cohesion constraint can be found in (Lin
and Cherry, 2003).
3 Probability Model
We define the word alignment problem as finding the
alignment A that maximizes P (A|E,F ). ProAlign mod-
els P (A|E,F ) directly, using a different decomposition
of terms than the model used by IBM (Brown et al,
1993). In the IBM models of translation, alignments ex-
ist as artifacts of a stochastic process, where the words
in the English sentence generate the words in the French
sentence. Our model does not assume that one sentence
generates the other. Instead it takes both sentences as
given, and uses the sentences to determine an alignment.
An alignment A consists of t links {l1, l2, . . . , lt}, where
each lk = l(eik , fjk) for some ik and jk. We will refer to
consecutive subsets of A as lji = {li, li+1, . . . , lj}. Given
this notation, P (A|E,F ) can be decomposed as follows:
P (A|E,F ) = P (lt1|E,F ) =
t?
k=1
P (lk|E,F, l
k?1
1 )
At this point, we factor P (lk|E,F, lk?11 ) to make com-
putation feasible. Let Ck = {E,F, lk?11 } represent the
context of lk. Note that both the context Ck and the link
lk imply the occurrence of eik and fjk . We can rewrite
P (lk|Ck) as:
P (lk|Ck) =
P (lk, Ck)
P (Ck)
=
P (Ck|lk)P (lk)
P (Ck, eik , fjk)
= P (lk|eik , fjk)?
P (Ck|lk)
P (Ck|eik , fjk)
Here P (lk|eik , fjk) is link probability given a co-
occurrence of the two words, which is similar in spirit to
Melamed?s explicit noise model (Melamed, 2000). This
term depends only on the words involved directly in the
link. The ratio P (Ck|lk)P (Ck|eik ,fjk ) modifies the link probability,
providing context-sensitive information.
Ck remains too broad to deal with in practical sys-
tems. We will consider only a subset FT k of relevant
features of Ck. We will make the Na??ve Bayes-style as-
sumption that these features ft ? FT k are conditionally
independent given either lk or (eik , fjk). This produces a
tractable formulation for P (A|E,F ):
t?
k=1
?
?P (lk|eik , fjk)?
?
ft?FTk
P (ft |lk)
P (ft |eik , fjk)
?
?
More details on the probability model used by ProAlign
are available in (Cherry and Lin, 2003).
3.1 Features used in the shared task
For the purposes of the shared task, we use two feature
types. Each type could have any number of instantiations
for any number of contexts. Note that each feature type
is described in terms of the context surrounding a word
pair.
The first feature type fta concerns surrounding links.
It has been observed that words close to each other in
the source language tend to remain close to each other in
the translation (S. Vogel and Tillmann, 1996). To capture
this notion, for any word pair (ei, fj), if a link l(ei? , fj?)
exists within a window of two words (where i?2 ? i? ?
i+2 and j?2 ? j? ? j+2), then we say that the feature
fta(i? i
?, j ? j?, ei?) is active for this context. We refer
to these as adjacency features.
The second feature type ftd uses the English parse tree
to capture regularities among grammatical relations be-
tween languages. For example, when dealing with French
and English, the location of the determiner with respect
to its governor is never swapped during translation, while
the location of adjectives is swapped frequently. For any
word pair (ei, fj), let ei? be the governor of ei, and let
rel be the relationship between them. If a link l(ei? , fj?)
exists, then we say that the feature ftd(j ? j?, rel) is ac-
tive for this context. We refer to these as dependency
features.
Take for example Figure 2 which shows a partial align-
ment with all links completed except for those involving
the. Given this sentence pair and English parse tree, we
can extract features of both types to assist in the align-
ment of the1. The word pair (the1, l?) will have an active
adjacency feature fta(+1,+1, host) as well as a depen-
dency feature ftd(?1, det). These two features will work
together to increase the probability of this correct link.
the host discovers all the devices
det
subj pre
det
obj
 l'  h?te rep?re tous les p?riph?riques
1 2 3 4 5
1 2 3 4 5 6
6
the host locate all the peripherals
Figure 2: Feature Extraction Example
In contrast, the incorrect link (the1, les) will have only
ftd(+3, det), which will work to lower the link probabil-
ity, since most determiners are located before their gov-
ernors.
3.2 Training the model
Since we always work from a current alignment, training
the model is a simple matter of counting events in the
current alignment. Link probability is the number of time
two words are linked, divided by the number of times
they co-occur. The various feature probabilities can be
calculated by also counting the number of times a feature
occurs in the context of a linked pair of words, and the
number of times the feature is active for co-occurrences
of the same word pair.
Considering only a single, potentially noisy alignment
for a given sentence pair can result in reinforcing errors
present in the current alignment during training. To avoid
this problem, we sample from a space of probable align-
ments, as is done in IBM models 3 and above (Brown
et al, 1993), and weight counts based on the likelihood
of each alignment sampled under the current probability
model. To further reduce the impact of rare, and poten-
tially incorrect events, we also smooth our probabilities
using m-estimate smoothing (Mitchell, 1997).
4 Multiple Alignments
The result of the constrained alignment search is a high-
precision, word-to-word alignment. We then relax the
word-to-word constraint, and use statistics regarding col-
locations with unaligned words in order to make many-to-
one alignments. We also employ a further relaxed link-
ing process to catch some cases where the cohesion con-
straint ruled out otherwise good alignments. These auxil-
iary methods are currently not integrated into our search
or our probability model, although that is certainly a di-
rection for future work.
5 Conclusions
We have presented a brief overview of the major ideas
behind our entry to the WPT?03 Shared Task. Primary
among these ideas are the use of a cohesion constraint in
search, and our novel probability model.
Acknowledgments
This project is funded by and jointly undertaken with Sun
Microsystems, Inc. We wish to thank Finola Brady, Bob
Kuhns and Michael McHugh for their help. We also wish
to thank the WPT?03 reviewers for their helpful com-
ments.
References
P. F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?312.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. Submitted.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In 2002 Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP
2002), pages 304?311.
W.A. Gale and K.W. Church. 1991. Identifying word
correspondences in parallel texts. In 4th Speech
and Natural Language Workshop, pages 152?157.
DARPA, Morgan Kaufmann.
Dekang Lin and Colin Cherry. 2003. Word alignment
with cohesion constraint. Submitted.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249, June.
Igor A. Mel?c?uk. 1987. Dependency syntax: theory and
practice. State University of New York Press, Albany.
Tom Mitchell. 1997. Machine Learning. McGraw Hill.
H. Ney S. Vogel and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In 16th In-
ternational Conference on Computational Linguistics,
pages 836?841, Copenhagen, Denmark, August.
Word-for-Word Glossing with Contextually Similar Words 
Patrick Pantel and Dekang Lin 
Department of Computer Science 
University of Manitoba 
Winnipeg, Manitoba R3T 2N2 Canada 
{ppantel, indek} @cs.umanitoba.ca 
Abstract 
Many corpus-based machine translation 
systems require parallel corpora. In this 
paper, we present a word-for-word glossing 
algorithm that requires only a source 
language corpus. To gloss a word, we first 
identify its similar words that occurred in 
the same context in a large corpus. We then 
determine the gloss by maximizing the 
similarity between the set of contextually 
similar words and the different ranslations 
of the word in a bilingual thesaurus. 
1. Introduction 
Word-for-word glossing is the process of 
directly translating each word or term in a 
document without considering the word order. 
Automating this process would benefit many 
NLP applications. For example, in cross- 
language information retrieval, glossing a 
document often provides a sufficient ranslation 
for humans to comprehend the key concepts. 
Furthermore, a glossing algorithm can be used 
for lexical selection in a full-fledged machine 
translation (MT) system. 
Many corpus-based MT systems require 
parallel corpora (Brown et al, 1990; Brown et 
al., 1991; Gale and Church, 1991; Resnik, 
1999). Kikui (1999) used a word sense 
disambiguation algorithm and a non-paralM 
bilingual corpus to resolve translation 
ambiguity. 
In this paper, we present a word-for-word 
glossing algorithm that requires only a source 
language corpus. The intuitive idea behind our 
algorithm is the following. Suppose w is a word 
to be translated. We first identify a set of words 
similar to w that occurred in the same context as 
w in a large corpus. We then use this set (called 
the contextually similar words of w) to select a 
translation for w. For example, the contextually 
similar words of duty in fiduciary duty include 
responsibility, obligation, role, ... This list is 
then used to select a translation for duty. 
In the next section, we describe the resources 
required by our algorithm. In Section 3, we 
present an algorithm for constructing the 
contextually similar words of a word in a 
context. Section 4 presents the word-for-word 
glossing algorithm and Section 5 describes the 
group similarity metric used in our algorithm. In 
Section 6, we present some experimental results 
and finally, in Section 7, we conclude with a 
discussion of future work. 
2. Resources 
The input to our algorithm includes a collocation 
database (Lin, 1998b) and a corpus-based 
thesaurus (Lin, 1998a), which are both available 
on the Interne0. In addition, we require a 
bilingual thesaurus. Below, we briefly describe 
these resources. 
2.1. Collocation database 
Given a word w in a dependency relationship 
(such as subject or object), the collocation 
database can be used to retrieve the words that 
occurred in that relationship with w, in a large 
corpus, along with their frequencies 2. Figure 1 
shows excerpts of the entries in the collocation 
database for the words corporate, duty, and 
fiduciary. The database contains a total of 11 
million unique dependency relationships. 
I Available at www.cs.umanitoba.ca/-lindek/depdb.htm 
and www.cs.umanitoba.ca/-lindek/simdb.htm 
2 We use the term collocation to refer to a pair of 
words that occur in a dependency relationship (rather 
than the linear proximity of a pair of words). 
78 
Table 1. Clustered similar words of  duty as given by (Lin, 
1998a). 
CLUSTER CLUSTERED SIMILAR WORDS OF DUTY 
(WITH SIMILARITY SCORE) 
responsibility 0.16, obligation 0.109, task 0.101, 
function 0.098, role 0.091, post 0.087, position 
0.086, job 0.084, chore 0.08, mission 0.08, 
assignment 0.079, liability 0.077 ....  
tariff0.091, restriction 0.089, tax 0.086, 
regulation 0.085, requirement 0.081, procedure 
0.079, penalty 0.079, quota 0.074, rule 0.07, levy 
0.061 .... 
fee 0.085, salary 0.081, pay 0.064, fine 0.058 
personnel 0.073, staff0.073 
training 0.072, work 0.064, exercise 0.061 
privilege 0.069, right 0.057, license 0.056 
2.2. Corpus-based thesaurus 
Using the collocation database, Lin used an 
unsupervised method to construct a corpus- 
based thesaurus (Lin, 1998a) consisting of  
11839 nouns, 3639 verbs and 5658 
adjectives/adverbs. Given a word w, the 
thesaurus returns a clustered list of similar words 
of w along with their similarity to w. For 
example, the clustered similar words of duty are 
shown in Table 1. 
2.3. Bi l ingual thesaurus 
Using the corpus-based thesaurus and a bilingual 
dictionary, we manually constructed a bilingual 
thesaurus. The entry for a source language word 
w is constructed by manually associating one or 
more clusters of similar words of w to each 
candidate translation of w. We refer to the 
assigned clusters as Words Associated with a 
Translation (WAT). For example, Figure 2 
shows an excerpt of our Engl ish~French 
bilingual thesaurus for the words account  and 
duty. 
Although the WAT assignment is a manual 
process, it is a considerably easier task than 
providing lexicographic definitions. Also, we 
only require entries for source language words 
that have multiple translations. In Section 7, we 
corporate: 
modifier-of: 
duty: 
objeet-of: 
subject-of: 
adj-modifier: 
fiduciary: 
modifier-of: 
client 196, debt 236, development 179, fee 6, 
function 16, headquarter 316, IOU 128, levy 
3, liability 14, manager 203, market 195, 
obligation 1, personnel 7, profit 595, 
responsibility 27, rule 7, staff 113, tax 201, 
training 2, vice president 231 .... 
assume 177, breach 111, carry out 71, do 
114, have 257, impose 114, perform 151 .... 
affect 4, apply 6, include 42, involve 8, keep 
5, officer 22, protect 8, require 13, ... 
active 202, additional 46, administrative 44, 
fiduciary 317, official 66, other 83, ... 
act 2, behavior I, breach 2, claim I, 
company 2, duty 317, irresponsibility 2, 
obligation 32, requirement 1, responsibility 
89, role 2, ... 
Figure 1. Excepts o f  entries in the collocation database for 
the words corporate, duty, and fiduciary. 
account: 
1. compte: 
2. rapport: 
duty: 
1. devoir: 
2. taxe: 
investment, transaction, payment, saving, i 
money, contract, Budget, reserve, security,! 
contribution, debt, property holding 
report, statement, testimony, card, story, 
record, document, data, information, view, 
cheek, figure, article, description, estimate, 
assessment, number, statistic, comment, 
letter, picture, note, ... 
responsibility, obligation, task, function, 
role, post, position, job, chore, mission, 
assignment, liability . . . .  
tariff, restriction, tax, regulation, 
requirement, procedure, penalty, quota, rule, 
levy, ... 
WAT for 
ii~::::= ........................ 
Figure 2. Bilingual thesaurus entries for account and duty. 
discuss a method for automatically assigning the 
WATs. 
3. Contextual ly Similar Words 
The contextually similar words of a word w are 
words similar to the intended meaning of w in its 
context. Figure 3 gives the data flow diagram for 
our algorithm for identifying the contextually 
similar words of w. Data are represented by 
ovals, external resources by double ovals and 
processes by rectangles. 
By parsing a sentence with Minipar 3, we 
extract the dependency relationships involving 
w. For each dependency relationship, we retrieve 
3 Available at www.cs.umanitoba.ca/-lindek/minipar.htm 
79 
Input 
1 I 
Retrieve 
I 
sContextually ~'~ imilar Words,,\] 
Figure 3. Data flow diagram for identifying the 
contextually similar words of a word in context. 
from the collocation database the words that 
occurred in the same dependency relationship as 
w. We refer to this set of words as the cohort of 
w for that dependency relationship. Consider the 
word duty in the contexts corporate duty and 
fiduciary duty. The cohort of duty in corporate 
duty consists of nouns modified by corporate in 
Figure 1 (e.g. client, debt, development . . . .  ) and 
the cohort of duty in fiduciary duty consists of 
nouns modified by fiduciary in Figure 1 (e.g. 
act, behaviour, breach . . . .  ). 
Intersecting the set of similar words and the 
cohort then forms the set of contextually similar 
words of w. For example, Table 2 shows the 
contextually similar words of duty in the 
contexts corporate duty and fiduciary duty. The 
words in the first row are retrieved by 
intersecting the words in Table 1 with the nouns 
modified by corporate in Figure 1. Similarly, 
the second row represents he intersection of the 
words in Table I and the nouns modified by 
fiduciary in Figure 1. 
The first set of contextually similar words in 
Table 2 contains words that are similar to both 
Table 2. The words similar to duty that occurred in the 
contexts corporate duty and fiduciary duty. 
CONTEXT CONTEXTUALLY SIMILAR WORDS OF DUTY 
corporate duty fee, function, levy, liability, obligation, 
personnel, responsibility, rule, staff, tax, 
training 
obligation, requirement, responsibility, role fiducia~ duty 
the responsibility and tax senses of duty, 
reflecting the fact that the meaning of duty is 
indeed ambiguous if corporate duty is its sole 
context. In contrast, the second row in Table 2 
clearly indicates the responsibility sense of duty. 
While previous word sense disambiguation 
algorithms rely on a lexicon to provide sense 
inventories of words, the contextually similar 
words provide a way of distinguishing between 
different senses of words without committing to 
any particular sense inventory. 
4. Overview of the Word-for-Word 
Glossing Algorithm 
Figure 4 illustrates the data flow of the word- 
for-word glossing algorithm and Figure 5 
describes it. 
For example, suppose we wish to translate 
into French the word duty in the context 
corporate fiduciary duty. Step 1 retrieves the 
candidate translations for duty and its WATs 
from Figure 2. In Step 2, we construct two lists 
of contextually similar words, one for the 
dependency context corporate duty and one for 
the dependency context fiduciary duty, shown in 
Table 2. The proposed translation for the context 
is obtained by maximizing the group similarities 
between the lists of contextually similar words 
and the WATs. 
Using the group similarity measure from 
Section 5, Table 3 lists the group similarity 
scores between each list of contextually similar 
words and each WAT as well as the final 
combined score for each candidate translation. 
The combined score for a candidate is the sum 
of the logs of all group similarity scores 
involving its WAT. The correct proposed 
translation for duty in this context is devoir since 
its WAT received the highest score. 
80 
Input 
I Step 1 Step 2 getWATs getCSWLists 
(  w?,?sl 
Step 3 groupSim I 
l 
Matrix of ~%~ 
I Step4 I ?ornbineScores 
Translation )
Figure 4. Data flow diagram for the word-for-word 
glossing algorithm. 
Table 3. Group similarity scores between the contextually 
similar words of duty in corporate duty and fiduciary duty 
with the WATs for candidate translations devoir and taxe. 
CANDIDATE CANDIDATE 
DEVOIR TAXE 
corporate duty 60.3704 16.569 
fiduciary duty 51.2960 4.8325 
Combined Score 8.0381 4.3829 
Figure 6. An example illustrating the difference between 
the interconnectivity and closeness measures. The 
interconnectivity in (a) and (b) remains constant while the 
closeness in (a) is higher than in (b) since there are more 
zero similarity pairs in (b). 
Input: 
Step 1: 
Step 2: 
Step 3: 
Step 4: 
Output: 
A word w to be translated and a set of 
dependency contexts involving w. 
Retrieve the candidate translations ofw and 
the corresponding WATs from the bilingual 
thesaurus. 
Find the contextually similar words of w in 
each dependency context using the algorithm 
from Section 3. 
Compute the group similarity (see details in 
Section 5) between each set of contextually 
similar words and each WAT; the results are 
stored in a matrix t, where t\[i,j\] is the group 
similarity between the ?h list of contextually 
similar words and thef  h WAT. 
Add the logs of the group similarity scores in 
column of t  to obtain a score for each WAT. 
The candidate translation corresponding to
the WAT with the highest score. 
Figure 5. The word-for-word glossing algorithm. 
5. Group Similarity 
The corpus-based thesaurus contains only the 
similarities between individual pairs of words. In 
our algorithm, we require the similarity between 
groups of  words. The group similarity measure 
we use is proposed by Karypis et al (1999). It 
takes as input two groups of  elements, Gl and 
G2, and a similarity matrix, sim, which specifies 
the similarity between individual elements. GI 
and G2 are describable by graphs where the 
vertices are the words and each weighted edge 
between vertices wl and w2 represents the 
similarity, sim(wl, w2), between the words wl 
and Wz. 
Karypis et al consider both the 
interconnectivity and the closeness of the 
groups. The absolute interconnectivity between 
G t and G 2, AI(G t, G2), is defined as the aggregate 
similarity between the two groups: 
x~Gi YEG2 
The absolute closeness between G~ and G2, 
AC(G~, G2), is defined as the average similarity 
between a pair of elements, one from each 
group: 
Ic, lc l 
81 
Table 4. Candidate translations for each testing word along with their frequency of occurrence 
in the test corpus. 
WORD CANDIDATE ENGLISH SENSE FREQUENCY OF 
TRANSLATION OCCURRENCE 
account compte bank account, business 245 
rapport report, statement 55 
duty devoir responsibility, obligation 80 
taxe tax 30 
race course contest 87 
race racial group 23 
suit proems lawsuit 281 
costume garment 17 
check ch6que draft, bank order 105 
contr61e evaluation, verification 25 
record record unsurpassed statistic/performance 98 
enregistremen t recorded ata or documentation 12 
The difference between the absolute 
interconnectivity and the absolute closeness is 
that the latter takes zero similarity pairs into 
account. In Figure 6, the interconnectivity n (a) 
and (b) remains constant. However, the 
closeness in (a) is higher than in (b) since there 
are more zero similarity pairs in (b). 
Karypis et al normalized the absolute 
interconnectivity and closeness by the internal 
interconnectivity and closeness of the individual 
groups. The normalized measures are referred to 
as relative interconnectivity, RI(GI, G2), and 
relative closeness, RC(GI, G2). The internal 
interconnectivity and closeness are obtained by 
first computing a minimal edge bisection of 
each group. An even-sized partition {G', G"} of 
a group G is called a minimal edge bisection of 
G if AI(G', G") is minimal among all such 
partitions. The internal interconnectivity of G, 
II(G), is defined as II(G) = AI(G', G") and the 
internal closeness of G, IC(G), as IC(G) = 
AC(G', G"). 
Minimal edge bisection is performed for all 
WATs and all sets of contextually similar words. 
However, the minimal edge bisection problem is 
NP-complete (Garey and Johnson, 1979). 
Fortunately, state of the art graph partitioning 
algorithms can approximate these bisections in 
polynomial time (Goehring and Saad, 1994; 
Karypis and Kumar, 1999; Kernighan and Lin, 
1970). We used the same approximation 
methods as in (Karypis et al, 1999). 
The similarity between G1 and G2 is then 
defined as follows: 
groupSim(G,, G2)= R/(G,, G2)? RC(G,, G 2 ) 
where 
2AI(G,,G2) 
xI(G,)+ II(G ) 
is the relative interconnectivity and 
RC(G,,G2)= AC(G,,G ) 
IG'I IC(G,)4 IG2I IC(G2) 
IG, I+IG=I IG, I?IG21 
is the relative closeness. 
6. Experimental Results 
The design of our glossing algorithm is 
applicable to any source/destination language 
pair as long as a source language parser is 
available. We considered English-to-French 
translations in our experiments. 
We experimented with six English nouns that 
have multiple French translations: account, duty, 
race, suit, check, and record. Using the 1987 
Wall Street Journal files on the LDC/DCI CD- 
8R 
ROM, we extracted a testing corpus 4consisting 
of the first 100 to 300 sentences containing the 
non-idiomatic usage of the six nouns . Then, we 
manually tagged each sentence with one of the 
candidate translations shown in Table 4. 
Each noun in Table 4 translates more 
frequently to one candidate translation than the 
other. In fact, always choosing the candidate 
procbs as the translation for suit yields 94% 
accuracy. A better measure for evaluating the 
system's classifications considers both the 
algorithm's precision and recall on each 
candidate translation. Table 5 illustrates the 
precision and recall of our glossing algorithm for 
each candidate translation. Albeit precision and 
recall are used to evaluate the quality of the 
classifications, overall accuracy is sufficient for 
comparing different approaches with our system. 
In Section 3, we presented an algorithm for 
identifying the contextually similar words of a 
word in a context using a corpus-based thesaurus 
and a collocation database. Each of the six nouns 
has similar words in the corpus-based thesaurus. 
However, in order to find contextually similar 
words, at least one similar word for each noun 
must occur in the collocation database in a given 
context. Thus, the algorithm for constructing 
contextually similar words is dependent on the 
coverage of the collocation database. We 
estimated this coverage by counting the number 
of times each of the six nouns, in several 
different contexts, has at least one contextually 
similar word. The result is shown in Table 6. 
In Section 5, we described a group similarity 
metric, groupSim, which we use for comparing a
WAT with a set of contextually similar words. 
In Figure 7, we compare the translation accuracy 
of our algorithm using other group similarity 
metrics. Suppose G~ and (/2 are two groups of 
words and w is the word that we wish to 
translate. The metrics used are: 
I. closest& 
sum of similarity of the three closest 
pairs of words from each group. 
4 Available at fip.cs.umanitoba.ca/pub/ppantei/ 
download/wfwgtest.zip 
5 Omitted idiomatic phrases include take into 
account, keep in check, check out, ... 
Table 5. Precision vs. Recall for each candidate translation. 
WORD CANDIDATE PRECISION RECALL 
account compte 0.982 0.902 
rapport 0.680 0.927 
duty devoir 0.951 0.963 
taxe 0.897 0.867 
race course 0.945 0.989 
race 0.947 0.783 
suit proc6s 0.996 0.993 
costume 0.889 0.941 
check ch6que 0.951 0.924 
contr61e 0.714 0.800 
record record 0.968 0.918 
enregistrement 0.529 0.750 
Table 6. The coverage of the collocation database, shown 
by the frequency with which a word in a given context has 
at least one contextually similar word. 
WORD NUMBER OF COVERAGE 
CONTEXTS 
account 1074 95.7% 
duty 343 93.3% 
race 294 92.5% 
suit 332 91.9% 
check 2519 87.5% 
record 1655 92.8% 
2. gs: 
Z sim(x, w )x max sire(x, y)+ Z sire(y, w)x max sire(y, x) 
Z.,'im{x, w)+ Z ~im@, ~) 
3. dC: 
as defined in Section 5. 
4. AI: 
as defined in Section 5. 
5. RC: 
as defined in Section 5. 
6. RI: 
as defined in Section 5. 
83 
Group Similarity Comparison 
01.01 .... ..... 'i ~ ~R o 7 ? i, 06 .5 
0.4 ; ~ I ; 
02 ;~ 
0o 
i}il;;i  iii ijii !ii!! i}ii iii iiiii 
Figure 7. Performance comparison of different group similarity metrics. 
In mostFrequent, we include the results 
obtained if we always choose the translation that 
occurs most frequently in the testing corpus. 
We also compared the accuracy of our 
glossing algorithm with Systran's translation 
system by feeding the testing sentences into 
Systran's web interface 6 and manually 
examining the results. Figure 8 summarizes the 
overall accuracy obtained by each system and 
the baseline on the testing corpus. Systran 
tended to prefer one candidate translation over 
the other and committed the majority of its 
errors on the non-preferred senses. 
Consequently, Systran is very accurate if its 
preferred sense is the frequent sense (as in 
account and duty) but is very inaccurate if its 
preferred sense is the infrequent one (as in race, 
suit, and check). 
7. Conclusion and Future Work 
This paper presents a word-for-word glossing 
algorithm. The gloss of a word is determined by 
maximizing the similarity between the set of 
contextually similar words and the different 
translations of the word in a bilingual thesaurus. 
6 Available at babelfish.altavista.com/cgi-bin/translate 
The algorithm presented in this paper can be 
improved and extended in many ways. At 
present, our glossing algorithm does not take the 
prior probabilities of translations into account. 
For example, in WSJ, the bank account sense of 
account is much more common than the report 
sense. We should thus tend to prefer this sense 
of account. This is achievable by weighting the 
translation scores by the prior probabilities of 
the translations. We are investigating an 
Expectation-Maximization (EM) (Dempster et 
al., 1977) algorithm to learn these prior 
probabilities. Initially, we assume that the 
candidate translations for a word are uniformly 
distributed. After glossing each word in a large 
corpus, we refine the prior probabilities using 
the frequency counts obtained. This process is 
repeated several times until the empirical prior 
probabilities closely approximate the true prior 
probabilities. 
Finally, as discussed in Section 2.3, 
automatically constructing the bilingual 
thesaurus is necessary to gloss whole 
documents. This is attainable by adding a 
corpus-based destination language thesaurus to 
our system. The process of assigning a cluster of 
similar words as a WAT to a candidate 
translation c is as follows. First, we 
84 
1.0 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
0.0 
\[\] mostFrequent 
? Systran 
\[\]  G oss ng 
Word-for-Word Glossing vs. Systran 
Z 
\]ccour 
0,816; 
0.856; 
0.906; 
[ t~ mostFrequent ? Systran 
\[\] Glossing 
Figure 8. Performance omparison fthe word-for-word glossing algorithm and Systran. 
automatically obtain the candidate translations 
for a word using a bilingual dictionary. With the 
destination language thesaurus, we obtain a list S 
of all words similar to c. With the bilingual 
dictionary, replace each word in S by its source 
language translations. Using the group similarity 
metric from Section 5, assign as the WAT the 
cluster of similar words (obtained from the 
source language thesaurus) most similar to S. 
Acknowledgements 
The authors wish to thank the reviewers for their 
helpful comments. This research was partly 
supported by Natural Sciences and Engineering 
Research Council of Canada grants OGP 121338 
and PGSA207797. 
References 
Peter F. Brown; John Cocke; Stephen A. Della Pietra; 
Vincent J. Della Pietra; Fredrick Jelinek; John D. 
Lafferty; Robert L. Mercer and Paul S. Roossin. 
1990. A Statistical Approach to Machine 
Translation. Computation Linguistics, 16(2). 
Peter F. Brown; Jennifer C. Lai and Robert L. 
Mercer. 1991. Aligning Sentences in Parallel 
Corpora. In Proceedings ofACL91. Berkeley. 
A. P. Dempster; N. M. Laird; & D. B. Rubin. 1977. 
Maximum likelihood from incomplete data via the 
EM algorithm. Journal of the Royal Statistical 
Society, Series B, 39(I). 
W. A. Gale and K. W. Church. 1991. A Program for 
Aligning Sentences in Bilingual Corpora. In 
Proceedings of ACL91. Berkeley. 
M. R. Garey and D. S. Johnson. 1979. Computers 
and Intractability: A Guide to the Theory of NP- 
Completeness. W H. Freeman. 
T. Goehring and Y. Saad. 1994. Heuristic Algorithms 
for Automatic Graph Partitioning. Technical 
Report. Department of Computer Science, 
University of Minnesota. 
George Karypis and Vipin Kumar. 1999. A Fast and 
High Quality Multilevel Scheme for Partitioning 
Irregular Graphs. SIAM Journal on Scientific 
Computing, 20(1 ). 
George Karypis; Eui-Hong Han and Vipin Kumar. 
1999. Chameleon: A Hierarchical Clustering 
Algorithm Using Dynamic Modeling. IEEE 
Computer: Special Issue on Data Analysis and 
Mining, 32(8).  http:l/www-users.cs.umn.edu/ 
-karypis/publications/Papers/PDF/chameleon.pdf 
B. W. Kernighan and S. Lin. 1970. An Efficient 
Heuristic Procedure for Partitioning Graphs. The 
Bell System Technical Journal. 
Genichiro Kikui. 1999. Resolving Translation 
ambiguity using Non-parallel Bilingual Corpora. 
In Proceedings of ACL99 Workshop on 
Unsupervised Learning in Natural Language 
Processing. 
Dekang Lin. 1998a. Automatic Retrieval and 
Clustering of Similar Wordv. In Proceedings of 
COLING-ACL98. Montreal, Canada. 
Dekang Lin. 1998b. Extracting Collocations from 
Text Corpora. Workshop on Computational 
Terminology. Montreal, Canada. 
Philip Resnik. 1999. Mining the Web for Bilingual 
Text. In Proceedings of ACL99. College Park, 
Maryland. 
85 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 886?894,
Beijing, August 2010
Using Web-scale N-grams to Improve Base NP Parsing Performance
Emily Pitler
Computer and Information Science
University of Pennsylvania
epitler@seas.upenn.edu
Shane Bergsma
Department of Computing Science
University of Alberta
sbergsma@ualberta.ca
Dekang Lin
Google, Inc.
lindek@google.com
Kenneth Church
Human Language Technology Center of Excellence
Johns Hopkins University
kenneth.church@jhu.edu
Abstract
We use web-scale N-grams in a base NP
parser that correctly analyzes 95.4% of the
base NPs in natural text. Web-scale data
improves performance. That is, there is no
data like more data. Performance scales
log-linearly with the number of parame-
ters in the model (the number of unique
N-grams). The web-scale N-grams are
particularly helpful in harder cases, such
as NPs that contain conjunctions.
1 Introduction
Noun phrases (NPs) provide an index to the
world?s information. About 70% of web queries
are NPs (Barr et al, 2008). A robust NP parser
could help search engines improve retrieval per-
formance on multi-word NP queries (Zhai, 1997).
For example, by knowing the correct parse of
?washed (baby carrots),? a search engine could
ensure that returned pages (and advertisements)
concern clean carrots rather than clean babies. NP
structure is also helpful for query expansion and
substitution (Jones et al, 2006).
This paper is concerned with base NP pars-
ing. We are given a base NP string as input,
and the task is to produce a parse tree as output.
Base NPs are NPs that do not contain embedded
noun phrases. These are sometimes called NP
chunks, or core/non-recursive NPs (Church, 1988;
Ramshaw and Marcus, 1995). Correctly parsing
(or, equivalently, bracketing) base NPs is chal-
lenging because the same part-of-speech (POS)
sequence can be parsed differently depending on
the specific words involved. For example, ?retired
(science teacher)? and ?(social science) teacher?
have different structures even though they have
identical POS sequences.
Lexical statistics are therefore needed in order
to parse the above examples, and they must be
computed over a lot of text to avoid sparsity. All
of our lexical statistics are derived from a new
and improved web-scale N-gram corpus (Lin et
al., 2010), which we call Google V2.
Despite the importance of base NPs, most
sentence parsers do not parse base NPs, since
the main training corpus for parsers, the Penn
Treebank (PTB) (Marcus et al, 1994), leaves a
flat structure for base NPs. Recent annotations
by Vadas and Curran (2007a) added NP structure
to the PTB. We use these annotations (described
in Section 3) for our experiments.
NP parsers usually focus on bracketing three-
word noun compounds. Parsing three-word noun
compounds is a fairly artificial task; we show that
sequences of three nouns make up less than 1%
of the three-word-or-longer base NPs in natural
text. As the NP length increases, the number of
possible binary trees (parses) increases with the
Catalan numbers (Church and Patil, 1982). NPs of
length three have just two possible parses (chance
is 50%), while NPs of length six already have
forty-two possible parses (chance is 2%). Long
NPs therefore provide much more opportunity to
improve performance over the baseline. In Table
1 (Section 7), we show the distribution of base NP
length in the PTB. While most NPs are of length
three, NP length has a long tail.
886
The three-word noun compound assumption
also restricts research to the case in which all
words are nouns, while base NPs also contain de-
terminers, possessives, adjectives, and conjunc-
tions. Conjunctions and their scopes are particu-
larly challenging. For example, in the NP, ?French
television and movie producers,? a parser should
conjoin ?(television) and (movie),? as opposed to
?(French television) and (movie),? ?(French tele-
vision) and (movie producers)? or ?(television)
and (movie producers).?
To resolve these issues, we train a classifier
which uses contextual information from the entire
NP and lexical statistics derived from the web-
scale N-gram corpus to predict if a given span
is a constituent. Our parser then uses this clas-
sifier to produce a score for every possible NP-
internal bracketing and creates a chart of bracket-
ing scores. This chart can be used as features in a
full sentence parser or parsed directly with a chart
parser. Our parses are highly accurate, creating a
strong new standard for this task.
Finally, we present experiments that investigate
the effects of N-gram frequency cutoffs and vari-
ous sources of N-gram data. We show an interest-
ing relationship between accuracy and the number
of unique N-gram types in the data.
2 Related Work
2.1 Three-Word Noun Compounds
The most commonly used data for NP parsing is
from Lauer (1995), who extracted 244 three-word
noun compounds from the Grolier encyclopedia.
When there are only three words, this task reduces
to a binary decision:
? Left Branching: * [retired science] teacher
? Right Branching: retired [science teacher]
In Lauer (1995)?s set of noun compounds, two-
thirds are left branching.
The main approach to these three-word noun
compounds has been to compute association
statistics between pairs of words and then choose
the bracketing that corresponds to the more highly
associated pair. The two main models are the
adjacency model (Marcus, 1980; Liberman and
Sproat, 1992; Pustejovsky et al, 1993; Resnik,
1993) and the dependency model (Lauer, 1995).
Under the adjacency model, the bracketing deci-
sion is made by comparing the associations be-
tween words one and two versus words two and
three (i.e. comparing retired science versus sci-
ence teacher). In contrast, the dependency model
compares the associations between one and two
versus one and three (retired science versus retired
teacher). Lauer (1995) compares the two models
and finds the dependency model to be more accu-
rate.
Nakov and Hearst (2005) compute the associ-
ation scores using frequencies, conditional proba-
bilities, ?2, and mutual information, for both pairs
of words and for linguistically-motivated para-
phrases. Lapata and Keller (2005) found that us-
ing web-scale data for associations is better than
using the (smaller) 100M-word British National
Corpus.
2.2 Longer NPs
Focusing on only the three word case misses a
large opportunity for base NP parsing. NPs longer
than three words commonly occur, making up
29% of our test set. In addition, a chance baseline
does exponentially worse as the length of the NP
increases. These longer NPs are therefore a major
opportunity to improve overall base NP parsing.
Since in the general case, NP parsing can no
longer be thought of as a single binary classifica-
tion problem, different strategies are required.
Barker (1998) reduces the task of parsing
longer NPs to making sequential three-word de-
cisions, moving a sliding window along the NP.
The window is first moved from right-to-left, in-
serting right bracketings, and then again from left-
to-right, finalizing left bracketings. While Barker
(1998) assumes that these three-word decisions
can be made in isolation, this is not always valid.1
Vadas and Curran (2007b) employ Barker?s algo-
rithm, but use a supervised classifier to make the
sequential bracketing decisions. Because these
approaches rely on a sequence of binary decisions,
1E.g., although the right-most three words are identical
in 1) ?soap opera stars and television producers,? and 2)
?movie and television producers,? the initial right-bracketing
decision for ?and television producers? should be different
in each.
887
early mistakes can cascade and lead to a chain of
incorrect bracketings.
Our approach differs from previous work in NP
parsing; rather than greedily inserting brackets as
in Barker?s algorithm, we use dynamic program-
ming to find the global maximum-scoring parse.
In addition, unlike previous approaches that have
used local features to make local decisions, we use
the full NP to score each potential bracketing.
A related line of research aims to segment
longer phrases that are queried on Internet search
engines (Bergsma and Wang, 2007; Guo et al,
2008; Tan and Peng, 2008). Bergsma and Wang
(2007) focus on NP queries of length four or
greater. They use supervised learning to make
segmentation decisions, with features derived
from the noun compound bracketing literature.
Evaluating the benefits of parsing NP queries,
rather than simply segmenting them, is a natural
application of our system.
3 Annotated Data
Our training and testing data are derived from re-
cent annotations by Vadas and Curran (2007a).
The original PTB left a flat structure for base noun
phrases. For example, ?retired science teacher,?
would be represented as:
(NP (JJ retired) (NN science) (NN teacher))
Vadas and Curran (2007a) annotated NP-internal
structure by adding annotations whenever there is
a left-bracketing. If no annotations were added,
right-branching is assumed. The inter-annotator
agreement for exactly matching the brackets on an
NP was 98.5%.
This data provides a valuable new resource for
parsing research, but little work has so far made
use of it. Vadas and Curran (2007b) perform
some preliminary experiments on NP bracketing,
but use gold standard part-of-speech and named-
entity annotations as features in their classifier.
Our work establishes a strong and realistic stan-
dard on this data; our results will serve as a basis
for further research on this topic.
4 Unlabeled N-gram Data
All of our N-gram features described in Sec-
tion 6.1 rely on probabilities derived from unla-
beled data. To use the largest amount of data
possible, we exploit web-scale N-gram corpora.
N-gram counts are an efficient way to compress
large amounts of data (such as all the text on the
web) into a manageable size. An N-gram corpus
records how often each unique sequence of words
occurs. Co-occurrence probabilities can be calcu-
lated directly from the N-gram counts. To keep
the size manageable, N-grams that occur with a
frequency below a particular threshold can be fil-
tered.
The corpus we use is Google V2 (Lin et al,
2010): a new N-gram corpus with N-grams of
length 1-5 that we created from the same 1 tril-
lion word snapshot of the web as Google N-grams
Version 1 (Brants and Franz, 2006), but with sev-
eral enhancements. Duplicate sentences are re-
moved, as well as ?sentences? which are probably
noise (indicated by having a large proportion of
non-alphanumeric characters, being very long, or
being very short). Removing duplicate sentences
is especially important because automatically-
generated websites, boilerplate text, and legal dis-
claimers skew the source web data, with sentences
that may have only been authored once occurring
millions of times. We use the suffix array tools
described in Lin et al (2010) to quickly extract
N-gram counts.
5 Base NP Parsing Approach
Our goal is to take a base NP string as input and
produce a parse tree as output. In practice, it
would be most useful if the NP parse could be
integrated into a sentence parser. Previous NP
parsers are difficult to apply in practice.2 Work
in prepositional phrase attachment that assumes
gold-standard knowledge of the competing attach-
ment sites has been criticized as unrealistic (At-
terer and Schu?tze, 2007).
Our system can easily be integrated into full
parsers. Its input can be identified quickly and
reliably and its output is compatible with down-
stream parsers.
2For example, Vadas and Curran (2007b) report results on
NP parsing, but these results include NPs containing preposi-
tional or adverbial phrases (confirmed by personal communi-
cation). Practical application of their system would therefore
require resolving prepositional phrase attachment as a pre-
processing step.
888
Our parser?s input is base NPs, which can be
identified with very high accuracy. Kudo and Mat-
sumoto (2001) report 95.8% NP chunking accu-
racy on PTB data.
Once provided with an NP, our system uses a
supervised classifier to predict the probability of
a particular contiguous subsequence (span) of the
NP being a constituent, given the entire NP as con-
text. This probability can be inserted into the chart
that a standard chart parser would use.
For example, the base NP ?French television
and movie producers? would be decomposed into
nine different classification problems, scoring the
following potential bracketings:
(French television) and movie producers
French (television and) movie producers
(French television and) movie producers ...
French television and (movie producers)
In Section 6, we detail the set of statistical and
structural features used by the classifier.
The output of our classifier can be easily used
as a feature in a full-sentence structured prediction
parser, as in Taskar et al (2004). Alternatively,
our work could be integrated into a full-sentence
parser by using our feature representations di-
rectly in a discriminative CFG parser (Finkel et
al., 2008), or in a parse re-ranker (Ratnaparkhi et
al., 1994; Collins and Koo, 2005; Charniak and
Johnson, 2005).
While our main objective is to use web-scale
lexical statistics to create an accurate classifier for
base NP-internal constituents, we do produce a
parse tree for evaluation purposes. The probabil-
ity of a parse tree is defined as the product of the
probabilities of all the spans (constituents) in the
tree. The most probable tree is computed with the
CYK algorithm.
6 Features
Over the course of development experiments, we
discovered that the more position-specific our fea-
tures were, the more effectively we could parse
NPs. We define a word?s position as its distance
from the right of the full NP, as the semantic head
of NPs is most often the right-most word. Ulti-
mately, we decided to conjoin each feature with
the position of the proposed bracketing. Since
the features for differing proposed bracketings are
now disjoint, this is equivalent to scoring bracket-
ings with different classifiers, with each classifier
chosen according to the bracketing position. We
now outline the feature types that are common,
but weighted differently, in each proposed brack-
eting?s feature set.
6.1 N-gram Features
All of the features described in this section require
estimates of the probability of specific words or
sequences of words. All probabilities are com-
puted using Google V2 (Section 4).
6.1.1 PMI
Recall that the adjacency model for the three-
word task uses the associations of the two pairs of
adjacent words, while the dependency model uses
the associations of the two pairs of attachment
sites for the initial noun. We generalize the ad-
jacency and dependency models by including the
pointwise mutual information (Church and Hanks,
1990) between all pairs of words in the NP:
PMI(x, y) = log p(?x y?)p(?x?)p(?y?) (1)
For NPs of length n, for each proposed bracket-
ing, we include separate features for the PMI be-
tween all
(n
2
)
pairs of words in the NP. For NPs in-
cluding conjunctions, we include additional PMI
features (Section 6.1.2).
Since these features are also tied to the pro-
posed bracketing positions (as explained above),
this allows us to learn relationships between var-
ious associations within the NP and each poten-
tial bracketing. For example, consider a proposed
bracketing from word 4 to word 5. We learn that
a high association of words inside a bracketing
(here, a high association between word 4 and word
5) indicates a bracketing is likely, while a high
association between words that cross a proposed
bracketing (e.g., a high association between word
3 and word 4) indicates the bracketing is unlikely.
The value of these features is the PMI, if it is
defined. If the PMI is undefined, we include one
of two binary features:
p(?x y?) = 0 or p(?x?) ? p(?y?) = 0.
889
We illustrate the PMI features with an example.
In deciding whether (movie producers) is a rea-
sonable bracketing within ?French television and
movie producers,? the classifier weighs features
for all of:
PMI(French, television)
PMI(French, and)
. . .
PMI(television, producers)
PMI(and, producers)
PMI(movie, producers)
6.1.2 Conjunctions
Properly handling NPs containing conjunc-
tions (NP+conj) requires special statistical fea-
tures. For example, television and movie are
commonly conjoined, but the relevant statistics
that suggest placing brackets around the phrase
?television and movie? are not provided by the
above PMI features (i.e., this is not clear from
PMI(television, and), PMI(television, movie), nor
PMI(and, movie)). Rather, we want to know if the
full phrase ?television and movie? is common.
We thus have additional NP+conj features that
consider the PMI association across the word and:
PMIand(x, y) = log
p(?x and y?)
p(?x and?)p(?and y?) (2)
When PMIand between a pair of words is high,
they are likely to be the constituents of a conjunc-
tion.
Let NP=(w1 . . . wi?1, ?and?, wi+1 . . . wn) be
an NP+conj. We include the PMIand features be-
tween wi?1 and all w ? wi+1 . . . wn. In the exam-
ple ?French television and movie producers,? we
would include features PMIand(television, movie)
and PMIand(television, producers).
In essence, we are assuming wi?1 is the head
of one of the items being conjoined, and we score
the likelihood of each of the words to the right
of the and being the head for the other item. In
our running example, the conjunction has narrow
scope, and PMIand(television, movie) is greater
than PMIand(television, producers), indicating to
our classifier that (television and movie) is a good
bracketing. In other examples the conjunction will
join heads that are further apart, as in ((French TV)
and (British radio)) stars, where both of the fol-
lowing hold:
PMIand(TV, radio) > PMIand(TV, British)
PMIand(TV, radio) > PMIand(TV, stars)
6.2 Lexical
We include a binary feature to indicate the pres-
ence of a particular word at each position in the
NP. We learn that, for instance, the word Inc. in
names tends to occur outside of brackets.
6.3 Shape
Previous work on NP bracketing has used gold-
standard named entity tags (Vadas and Curran,
2007b) as features. We did not want to use any
gold-standard features in our experiments, how-
ever NER information is helpful in separating pre-
modifiers from names, i.e. (news reporter) (Wal-
ter Cronkite).
As an expedient way to get both NER informa-
tion and useful information from hyphenated ad-
jectives, abbreviations, and other punctuation, we
normalize each string using the following regular
expressions:
[A-Z]+ ? A [a-z]+ ? a
We use this normalized string as an indicator
feature. E.g. the word ?Saudi-born? will fire the
binary feature ?Aa-a.?
6.4 Position
We also include the position of the proposed
bracketing as a feature. This represents the prior
of a particular bracketing, regardless of the actual
words.
7 Experiments
7.1 Experimental Details
We use Vadas and Curran (2007a)?s annotations
(Section 3) to create training, development and
testing data for base NPs, using standard splits of
the Penn Treebank (Table 1). We consider all non-
trivial base NPs, i.e., those longer than two words.
For training, we expand each NP in our train-
ing set into independent examples corresponding
to all the possible internal NP-bracketings, and
represent these examples as feature vectors (Sec-
tion 5). Each example is positively labeled if it is
890
Data Set Train Dev Test Chance
PTB Section 2-22 24 23
Length=3 41353 1428 2498 50%
Length=4 12067 445 673 20%
Length=5 3930 148 236 7%
Length=6 1272 34 81 2%
Length>6 616 29 34 < 1%
Total NPs 59238 2084 3522
Table 1: Breakdown of the PTB base NPs used in
our experiments. Chance = 1/Catalan(length).
Features All NPs NP+conj NP-conj
All features 95.4 89.7 95.7
-N-grams 94.0 84.0 94.5
-lexical 92.2 87.4 92.5
-shape 94.9 89.7 95.2
-position 95.3 89.7 95.6
Right 72.6 58.3 73.5bracketing
Table 2: Accuracy (%) of base NPs parsing; abla-
tion of different feature classes.
consistent with the gold-standard bracketing, oth-
erwise it is a negative example.
We train using LIBLINEAR, an efficient linear
Support Vector Machine (SVM).3 We use an L2-
loss function, and optimize the regularization pa-
rameter on the development set (reaching an opti-
mum at C=1). We converted the SVM output to
probabilities.4 Perhaps surprisingly, since SVMs
are not probabilistic, performance on the devel-
opment set with these SVM-derived probabilities
was higher than using probabilities from the LIB-
LINEAR logistic regression solver.
At test time, we again expand the NPs and cal-
culate the probability of each constituent, insert-
ing the score into a chart. We run the CYK algo-
rithm to find the most probable parse of the entire
NP according to the chart. Our evaluation metric
is Accuracy: the proportion of times our proposed
parse of the NP exactly matches the gold standard.
8 Results
8.1 Base NPs
Our method improves substantially over the base-
line of assuming a completely right-branching
structure, 95.4% versus 72.6% (Table 2). The ac-
curacy of the constituency classifier itself (before
the CYK parser is used) is 96.1%.
The lexical features are most important, but all
feature classes are somewhat helpful. In particu-
lar, including N-gram PMI features significantly
improves the accuracy, from 94.0% to 95.4%.5
Correctly parsing more than 19 base NPs out of 20
is an exceptional level of accuracy, and provides a
strong new standard on this task. The most com-
parable result is by Vadas and Curran (2007b),
who achieved 93.0% accuracy on a different set of
PTB noun phrases (see footnote 2), but their clas-
sifier used features based on gold-standard part-
of-speech and named-entity information.
Exact match is a tough metric for parsing, and
the difficulty increases as the length of the NP
increases (because there are more decisions to
make correctly). At three word NPs, our accu-
racy is 98.5%; by six word NPs, our accuracy
drops to 79.0% (Figure 1). Our method?s accu-
racy decreases as the length of the NP increases,
but much less rapidly than a right-bracketing or
chance baseline.
8.2 Base NPs with Conjunctions
N-gram PMI features help more on NP+conj than
on those that do not contain conjunctions (NP-
conj) (Table 2). N-gram PMI features are the most
important features for NP+conj, increasing accu-
racy from 84.0% to 89.7%, a 36% relative reduc-
tion in error.
8.3 Effect of Thresholding N-gram data
We now address two important related questions:
1) how does our parser perform as the amount
of unlabeled auxiliary data varies, and 2) what
is the effect of thresholding an N-gram corpus?
The second question is of widespread relevance as
3www.csie.ntu.edu.tw/
?
cjlin/liblinear/
4Following instructions in http://www.csie.ntu.
edu.tw/
?
cjlin/liblinear/FAQ.html
5McNemar?s test, p < 0.05
891
 1
 10
 100
6543
Ac
cu
ra
cy
 (%
)
Length of Noun Compound (words)
Proposed
Right-bracketing
Chance
Figure 1: Accuracy (log scale) over different NP
lengths, of our method, the right-bracketing base-
line, and chance (1/Catalan(length)).
thresholded N-gram corpora are now widely used
in NLP. Without thresholds, web-scale N-gram
data can be unmanageable.
While we cannot lower the threshold after cre-
ating the N-gram corpus, we can raise it, filtering
more N-grams, and then measure the relationship
between threshold and performance.
Threshold Unique N-grams Accuracy
10 4,145,972,000 95.4%
100 391,344,991 95.3%
1,000 39,368,488 95.2%
10,000 3,924,478 94.8%
100,000 386,639 94.8%
1,000,000 37,567 94.4%
10,000,000 3,317 94.0%
Table 3: There is no data like more data. Accuracy
improves with the number of parameters (unique
N-grams).
We repeat the parsing experiments while in-
cluding in our PMI features only N-grams with
a count ?10 (the whole data set), ?100, ?1000,
. . ., ?107. All other features (lexical, shape, posi-
tion) remain unchanged. The N-gram data almost
perfectly exhibits Zipf?s power law: raising the
threshold by a factor of ten decreases the number
of unique N-grams by a factor of ten (Table 3).
The improvement in accuracy scales log-linearly
with the number of unique N-grams. From a prac-
tical standpoint, we see a trade-off between stor-
Corpus # of tokens ? # of types
NEWS 3.2 B 1 3.7 B
Google V1 1,024.9 B 40 3.4 B
Google V2 207.4 B 10 4.1 B
Table 4: N-gram data, with total number of words
(tokens) in the original corpus (in billions, B), fre-
quency threshold used to filter the data, ? , and to-
tal number of unique N-grams (types) remaining
in the data after thresholding.
age and accuracy. There are consistent improve-
ments in accuracy from lowering the threshold
and increasing the amount of auxiliary data. If for
some application it is necessary to reduce storage
by several orders of magnitude, then one can eas-
ily estimate the resulting impact on performance.
We repeat the thresholding experiments using
two other N-gram sources:
NEWS: N-gram data created from a large set
of news articles including the Reuters and Giga-
word (Graff, 2003) corpora, not thresholded.
Google V1: The original web-scale N-gram
corpus (Section 4).
Details of these sources are given in Table 4.
For a given number of unique N-grams, using
any of the three sources does about the same (Fig-
ure 2). It does not matter that the source corpus
for Google V1 is about five times larger than the
source corpus for Google V2, which in turn is
sixty-five times larger than NEWS (Table 4). Ac-
curacies increase linearly with the log of the num-
ber of types in the auxiliary data set.
Google V1 is the one data source for which
the relationship between accuracy and number of
N-grams is not monotonic. After about 100 mil-
lion unique N-grams, performance starts decreas-
ing. This drop shows the need for Google V2.
Since Google V1 contains duplicated web pages
and sentences, mistakes that should be rare can
appear to be quite frequent. Google V2, which
comes from the same snapshot of the web as
Google V1, but has only unique sentences, does
not show this drop.
We regard the results in Figure 2 as a compan-
ion to Banko and Brill (2001)?s work on expo-
nentially increasing the amount of labeled train-
ing data. Here we see that varying the amount of
892
 94
 94.5
 95
 95.5
 96
1e91e81e71e61e51e4
Ac
cu
ra
cy
 (%
)
Number of Unique N-grams
Google V1
Google V2
NEWS
Figure 2: There is no data like more data. Ac-
curacy improves with the number of parameters
(unique N-grams). This trend holds across three
different sources of N-grams.
unlabeled data can cause an equally predictable
improvement in classification performance, with-
out the cost of labeling data.
Suzuki and Isozaki (2008) also found a log-
linear relationship between unlabeled data (up to
a billion words) and performance on three NLP
tasks. We have shown that this trend continues
well beyond Gigaword-sized corpora. Brants et
al. (2007) also found that more unlabeled data (in
the form of input to a language model) leads to
improvements in BLEU scores for machine trans-
lation.
Adding noun phrase parsing to the list of prob-
lems for which there is a ?bigger is better? rela-
tionship between performance and unlabeled data
shows the wide applicability of this principle. As
both the amount of text on the web and the power
of computer architecture continue to grow expo-
nentially, collecting and exploiting web-scale aux-
iliary data in the form of N-gram corpora should
allow us to achieve gains in performance linear in
time, without any human annotation, research, or
engineering effort.
9 Conclusion
We used web-scale N-grams to produce a new
standard in performance of base NP parsing:
95.4%. The web-scale N-grams substantially im-
prove performance, particularly in long NPs that
include conjunctions. There is no data like more
data. Performance improves log-linearly with the
number of parameters (unique N-grams). One can
increase performance with larger models, e.g., in-
creasing the size of the unlabeled corpora, or by
decreasing the frequency threshold. Alternatively,
one can decrease storage costs with smaller mod-
els, e.g., decreasing the size of the unlabeled cor-
pora, or by increasing the frequency threshold. Ei-
ther way, the log-linear relationship between accu-
racy and model size makes it easy to estimate the
trade-off between performance and storage costs.
Acknowledgments
We gratefully acknowledge the Center for Lan-
guage and Speech Processing at Johns Hopkins
University for hosting the workshop at which this
research was conducted.
References
Atterer, M. and H. Schu?tze. 2007. Prepositional
phrase attachment without oracles. Computational
Linguistics, 33(4):469?476.
Banko, M. and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In ACL.
Barker, K. 1998. A trainable bracketer for noun mod-
ifiers. In Twelfth Canadian Conference on Artificial
Intelligence (LNAI 1418).
Barr, C., R. Jones, and M. Regelson. 2008. The lin-
guistic structure of English web-search queries. In
EMNLP.
Bergsma, S. and Q.I. Wang. 2007. Learning noun
phrase query segmentation. In EMNLP-CoNLL.
Brants, T. and A. Franz. 2006. The Google Web 1T
5-gram Corpus Version 1.1. LDC2006T13.
Brants, T., A.C. Popat, P. Xu, F.J. Och, and J. Dean.
2007. Large language models in machine transla-
tion. In EMNLP.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In ACL.
Church, K.W. and P. Hanks. 1990. Word associa-
tion norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Church, K. and R. Patil. 1982. Coping with syntactic
ambiguity or how to put the block in the box on the
table. Computational Linguistics, 8(3-4):139?149.
893
Church, K.W. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In ANLP.
Collins, M. and T. Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational
Linguistics, 31(1):25?70.
Finkel, J.R., A. Kleeman, and C.D. Manning. 2008.
Efficient, feature-based, conditional random field
parsing. In ACL.
Graff, D. 2003. English Gigaword. LDC2003T05.
Guo, J., G. Xu, H. Li, and X. Cheng. 2008. A unified
and discriminative model for query refinement. In
SIGIR.
Jones, R., B. Rey, O. Madani, and W. Greiner. 2006.
Generating query substitutions. In WWW.
Kudo, T. and Y. Matsumoto. 2001. Chunking with
support vector machines. In NAACL.
Lapata, M. and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1):1?31.
Lauer, M. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In ACL.
Liberman, M. and R. Sproat. 1992. The stress and
structure of modified noun phrases in English. Lex-
ical matters, pages 131?181.
Lin, D., K. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
K. Dalwani, and S. Narsale. 2010. New tools for
web-scale n-grams. In LREC.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Marcus, M.P. 1980. Theory of Syntactic Recogni-
tion for Natural Languages. MIT Press, Cambridge,
MA, USA.
Nakov, P. and M. Hearst. 2005. Search engine statis-
tics beyond the n-gram: Application to noun com-
pound bracketing. In CoNLL.
Pustejovsky, J., P. Anick, and S. Bergler. 1993. Lex-
ical semantic techniques for corpus analysis. Com-
putational Linguistics, 19(2):331?358.
Ramshaw, L.A. and M.P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In 3rd
ACL Workshop on Very Large Corpora.
Ratnaparkhi, A., S. Roukos, and R.T. Ward. 1994.
A maximum entropy model for parsing. In Third
International Conference on Spoken Language Pro-
cessing.
Resnik, P. 1993. Selection and information: a class-
based approach to lexical relationships. Ph.D. the-
sis, University of Pennsylvania.
Suzuki, J. and H. Isozaki. 2008. Semi-supervised se-
quential labeling and segmentation using giga-word
scale unlabeled data. In ACL.
Tan, B. and F. Peng. 2008. Unsupervised query
segmentation using generative language models and
Wikipedia. In WWW.
Taskar, B., D. Klein, M. Collins, D. Koller, and
C. Manning. 2004. Max-margin parsing. In
EMNLP.
Vadas, D. and J.R. Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In ACL.
Vadas, D. and J.R. Curran. 2007b. Large-scale su-
pervised models for noun phrase bracketing. In PA-
CLING.
Zhai, C. 1997. Fast statistical parsing of noun phrases
for document indexing. In ANLP.
894
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 773?782,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Translation Sense Clustering
Mohit Bansal?
UC Berkeley
mbansal@cs.berkeley.edu
John DeNero
Google
denero@google.com
Dekang Lin
Google
lindek@google.com
Abstract
We propose an unsupervised method for clus-
tering the translations of a word, such that
the translations in each cluster share a com-
mon semantic sense. Words are assigned to
clusters based on their usage distribution in
large monolingual and parallel corpora using
the softK-Means algorithm. In addition to de-
scribing our approach, we formalize the task
of translation sense clustering and describe a
procedure that leverages WordNet for evalu-
ation. By comparing our induced clusters to
reference clusters generated from WordNet,
we demonstrate that our method effectively
identifies sense-based translation clusters and
benefits from both monolingual and parallel
corpora. Finally, we describe a method for an-
notating clusters with usage examples.
1 Introduction
The ability to learn a bilingual lexicon from a
parallel corpus was an early and influential area
of success for statistical modeling techniques in
natural language processing. Probabilistic word
alignment models can induce bilexical distributions
over target-language translations of source-language
words (Brown et al, 1993). However, word-to-word
correspondences do not capture the full structure of
a bilingual lexicon. Consider the example bilingual
dictionary entry in Figure 1; in addition to enumerat-
ing the translations of a word, the dictionary author
has grouped those translations into three sense clus-
ters. Inducing such a clustering would prove use-
ful in generating bilingual dictionaries automatically
or building tools to assist bilingual lexicographers.
?Author was a summer intern with Google Research while
conducting this research project.
Colocar [co?lo?car?], va. 1. To arrange, to put in
due place or order. 2. To place, to put in any place,
rank condition or office, to provide a place or em-
ployment. 3. To collocate, to locate, to lay.
Figure 1: This excerpt from a bilingual dictionary groups
English translations of the polysemous Spanish word colocar
into three clusters that correspond to different word senses
(Vela?zquez de la Cadena et al, 1965).
This paper formalizes the task of clustering a set
of translations by sense, as might appear in a pub-
lished bilingual dictionary, and proposes an unsu-
pervised method for inducing such clusters. We also
show how to add usage examples for the translation
sense clusters, hence providing complete structure
to a bilingual dictionary.
The input to this task is a set of source words and
a set of target translations for each source word. Our
proposed method clusters these translations in two
steps. First, we induce a global clustering of the en-
tire target vocabulary using the soft K-Means algo-
rithm, which identifies groups of words that appear
in similar contexts (in a monolingual corpus) and are
translated in similar ways (in a parallel corpus). Sec-
ond, we derive clusters over the translations of each
source word by projecting the global clusters.
We evaluate these clusters by comparing them to
reference clusters with the overlapping BCubed met-
ric (Amigo et al, 2009). We propose a clustering cri-
terion that allows us to derive reference clusters from
the synonym groups of WordNet R? (Miller, 1995).1
Our experiments using Spanish-English and
Japanese-English datasets demonstrate that the au-
tomatically generated clusters produced by our
method are substantially more similar to the
1WordNet is used only for evaluation; our sense clustering
method is fully unsupervised and language-independent.
773
Sense cluster WordNet sense description Usage example
collocate group or chunk together in a certain
order or place side by side
colocar juntas todas los libros
collocate all the books
invest, place, put make an investment capitales para colocar
capital to invest
locate, place assign a location to colocar el nu?mero de serie
locate the serial number
place, position, put put into a certain place or abstract
location
colocar en un lugar
put in a place
Figure 2: Correct sense clusters for the translations of Spanish verb s = colocar, assuming that it has translation set Ts =
{collocate, invest, locate, place, position, put}. Only the sense clusters are outputs of the translation sense clustering task; the
additional columns are presented for clarity.
WordNet-based reference clusters than naive base-
lines. Moreover, we show that bilingual features
collected from parallel corpora improve clustering
accuracy over monolingual distributional similarity
features alone.
Finally, we present a method for annotating clus-
ters with usage examples, which enrich our automat-
ically generated bilingual dictionary entries.
2 Task Description
We consider a three-step pipeline for generating
structured bilingual dictionary entries automatically.
(1) The first step is to identify a set of high-quality
target-side translations for source lexical items. In
our experiments, we ask bilingual human annota-
tors to create these translation sets.2 We restrict our
present study to word-level translations, disallowing
multi-word phrases, in order to leverage existing lex-
ical resources for evaluation.
(2) The second step is to cluster translations of each
word according to common word senses. This clus-
tering task is the primary focus of the paper, and we
formalize it in this section.
(3) The final step annotates clusters with usage ex-
amples to enrich the structure of the output. Sec-
tion 7 describes a method of identifying cluster-
specific usage examples.
In the task of translation sense clustering, the
second step, we assume a fixed set of source lexi-
cal items of interest S, each with a single part of
2We do not use automatically extracted translation sets in
our experiments, in order to isolate the clustering task on clean
input.
speech3, and for each s ? S a set Ts of target trans-
lations. Moreover, we assume that each target word
t ? Ts has a set of senses in common with s. These
senses may also be shared among different target
words. That is, each target word may have multiple
senses and each sense may be expressed by multiple
words.
Given a translation set Ts, we define a clusterG ?
Ts to be a correct sense cluster if it is both coherent
and complete.
? A sense cluster G is coherent if and only if
there exists some sense B shared by all of the
target words in G.
? A sense clusterG is complete if and only if, for
every sense B shared by all words in G, there
is no other word in Ts but not in G that also
shares that sense.
The full set of correct clusters for a set of translations
consists of all sense clusters that are both coherent
and complete.
The example translation set for the Spanish word
colocar in Figure 2 is shown with four correct sense
clusters. For descriptive purposes, these clusters are
annotated by WordNet senses and bilingual usage
examples. However, the task we have defined does
not require the WordNet sense or usage example
to be identified: we must only produce the correct
sense clusters within a set of translations. In fact, a
cluster may correspond to more than one sense.
Our definition of correct sense clusters has sev-
eral appealing properties. First, we do not attempt
to enumerate all senses of the source word. Sense
3A noun and verb that share the same word form would con-
stitute two different source lexical items.
774
Notation
Ts : The set of target-language translations (given)
Dt : The set of synsets in which t appears (given)
C : A synset; a set of target-language words
B : A source-specific synset; a subset of Ts
B : A set of source-specific synsets
G : A set of correct sense clusters for Ts
The Cluster Projection Algorithm:
B ?
{
C ? Ts : C ?
?
t?Ts
Dt
}
G ? ?
for B ? B do
if @B? ? B such that B ? B? then
add B to G
return G
Figure 3: The Cluster Projection (CP) algorithm projects
language-level synsets (C) to source-specific synsets (B) and
then filters the set of synsets for redundant subsets to produce
the complete set of source-specific synsets that are both coher-
ent and complete (G).
distinctions are only made when they affect cross-
lingual lexical choice. If a source word has many
fine-grained senses but translates in the same way
regardless of the sense intended, then there is only
one correct sense cluster for that translation.
Second, no correct sense cluster can be a super-
set of another, because the subset would violate the
completeness condition. This criterion encourages
larger clusters that are easier to interpret, as their
unifying senses can be identified as the intersection
of senses of the translations in the cluster.
Third, the correct clusters need not form a parti-
tion of the input translations. It is common in pub-
lished bilingual dictionaries for a translation to ap-
pear in multiple sense clusters. In our example, the
polysemous English verbs place and put appear in
multiple clusters.
3 Generating Reference Clusters
To construct a reference set for the translation
sense clustering task, we first collected English
translations of Spanish and Japanese nouns, verbs,
and adverbs. Translation sets were curated by hu-
man annotators to keep only high-quality single-
word translations.
Rather than gathering reference clusters via an ad-
ditional annotation effort, we leverage WordNet, a
large database of English lexical semantics (Miller,
1995). WordNet groups words into sets of cogni-
Synsets 
 collocate collocate, lump, chunk 
 invest, put, commit, place invest, clothe, adorn invest, vest, enthrone ? 
 locate, turn up situate, locate locate, place, site ? 
 put, set, place, pose, position, lay rate, rank, range, order, grade, place locate, place, site invest, put, commit, place 
? 
 position put, set, place, pose, position, lay 
 put, set, place, pose, position, lay put frame, redact, cast, put, couch invest, put, commit, place ? 
Words 
 collocate 
  
invest 
  
 locate 
 
  
 place 
  
 
 position   
put 
Sense Clusters 
 collocate 
 invest, place, put 
 
locate, place  
place, position, put 
Figure 4: An example of cluster projection on WordNet, for the
Spanish source word colocar. We show the target translation
words to be clustered, their WordNet synsets (with words not in
the translation set grayed out), and the final set of correct sense
clusters.
tive synonyms called synsets, each expressing a dis-
tinct concept. We use WordNet version 2.1, which
has wide coverage of nouns, verbs, and adverbs, but
sparser coverage of adjectives and prepositions.4
Reference clusters for the set of translations Ts
of some source word s are generated algorithmi-
cally from WordNet synsets via the Cluster Projec-
tion (CP) algorithm defined in Figure 3. An input
to the CP algorithm is the translation set Ts of some
source word s. Also, each translation t ? Ts be-
longs to some set of synsets Dt, where each synset
C ? Dt contains target-language words that may
or may not be translations of s. First, the CP algo-
rithm constructs a source-specific synset B for each
C, which contains only translations of s. Second,
it identifies all correct sense clusters G that are both
coherent and complete with respect to the source-
specific senses B. A sense cluster must correspond
to some synset B ? B to be coherent, and it must
4WordNet version 2.1 is almost identical to ver-
sion 3.0, for Unix-like systems, as described in
http://wordnetcode.princeton.edu/3.0/CHANGES. The lat-
est version 3.1 is not yet available for download.
775
not have a proper superset in B to be complete.5
Figure 4 illustrates the CP algorithm for the trans-
lations of the Spanish source word colocar that ap-
pear in our input dataset.
4 Clustering with K-Means
In this section, we describe an unsupervised method
for inducing translation sense clusters from the us-
age statistics of words in large monolingual and par-
allel corpora. Our method is language independent.
4.1 Distributed SoftK-Means Clustering
As a first step, we cluster all words in the target-
language vocabulary in a way that relates words that
have similar distributional features. Several methods
exist for this task, such as the K-Means algorithm
(MacQueen, 1967), the Brown algorithm (Brown
et al, 1992) and the exchange algorithm (Kneser
and Ney, 1993; Martin et al, 1998; Uszkoreit and
Brants, 2008). We use a distributed implementa-
tion of the ?soft? K-Means clustering algorithm de-
scribed in Lin and Wu (2009). Given a feature vec-
tor for each element (a word type) and the number
of desired clusters K, the K-Means algorithm pro-
ceeds as follows:
1. Select K elements as the initial centroids for
K clusters.
repeat
2. Assign each element to the top M clusters
with the nearest centroid, according to a simi-
larity function in feature space.
3. Recompute each cluster?s centroid by aver-
aging the feature vectors of the elements in that
cluster.
until convergence
4.2 Monolingual Features
Following Lin and Wu (2009), each word to be clus-
tered is represented as a feature vector describing the
distributional context of that word. In our setup, the
5One possible shortcoming of our approach to constructing
reference sets for translation sense clustering is that a cluster
may correspond to a sense that is not shared by the original
source word used to generate the translation set. All translations
must share some sense with the source word, but they may not
share all senses with the source word. It is possible that two
translations are synonymous in a sense that is not shared by the
source. However, we did not observe this problem in practice.
context of a word w consists of the words immedi-
ately to the left and right of w. The context feature
vector of w is constructed by first aggregating the
frequency counts of each word f in the context of
each w. We then compute point-wise mutual infor-
mation (PMI) features from the frequency counts:
PMI(w, f) = log
c(w, f)
c(w)c(f)
where w is a word, f is a neighboring word, and
c(?) is the count of a word or word pair in the cor-
pus.6 A feature vector for w contains a PMI feature
for each word type f (with relative position left or
right) for all words that appears a sufficient number
of times as a neighbor of w. The similarity of two
feature vectors is the cosine of the angle between the
vectors. We follow Lin and Wu (2009) in applying
various thresholds during K-Means, such as a fre-
quency threshold for the initial vocabulary, a total-
count threshold for the feature vectors, and a thresh-
old for PMI scores.
4.3 Bilingual Features
In addition to the features described in Lin and Wu
(2009), we introduce features from a bilingual par-
allel corpus that encode reverse-translation informa-
tion from the source-language (Spanish or Japanese
in our experiments). We have two types of bilin-
gual features: unigram features capture source-side
reverse-translations ofw, while bigram features cap-
ture both the reverse-translations and source-side
neighboring context words to the left and right. Fea-
tures are expressed again as PMI computed from
frequency counts of aligned phrase pairs in a par-
allel corpus. For example, one unigram feature for
place would be the PMI computed from the number
of times that place was in the target side of a phrase
pair whose source side was the unigram lugar. Sim-
ilarly, a bigram feature for place would be the PMI
computed from the number of times that place was
in the target side of a phrase pair whose source side
was the bigram lugar de. These features characterize
the way in which a word is translated, an indication
of its meaning.
6PMI is typically defined in terms of probabilities, but has
proven effective previously when defined in terms of counts.
776
4.4 Predicting Translation Clusters
As a result of softK-Means clustering, each word in
the target-language vocabulary is assigned to a list of
up to M clusters. To predict the sense clusters for a
set of translations of a source word, we apply the CP
algorithm (Figure 3), treating the K-Means clusters
as synsets (Dt).
5 Related Work
To our knowledge, the translation sense clustering
task has not been explored previously. However,
much prior work has explored the related task of
monolingual word and phrase clustering. Uszkor-
eit and Brants (2008) uses an exchange algorithm
to cluster words in a language model, Lin and Wu
(2009) uses distributed K-Means to cluster phrases
for various discriminative classification tasks, Vla-
chos et al (2009) uses Dirichlet Process Mixture
Models for verb clustering, and Sun and Korhonen
(2011) uses a hierarchical Levin-style clustering to
cluster verbs.
Previous word sense induction work (Diab and
Resnik, 2002; Kaji, 2003; Ng et al, 2003; Tufis
et al, 2004; Apidianaki, 2009) relates to our work
in that these approaches discover word senses au-
tomatically through clustering, even using multilin-
gual parallel corpora. However, our task of clus-
tering multiple words produces a different type of
output from the standard word sense induction task
of clustering in-context uses of a single word. The
underlying notion of ?sense? is shared across these
tasks, but the way in which we use and evaluate in-
duced senses is novel.
6 Experiments
The purpose of our experiments is to assess whether
our unsupervised soft K-Means clustering method
can effectively recover the reference sense clusters
derived from WordNet.
6.1 Datasets
We conduct experiments using two bilingual
datasets: Spanish-to-English (S?E) and Japanese-
to-English (J?E). Table 1 shows, for each dataset,
the number of source words and the total number
of target words in their translation sets. The datasets
Dataset No. of src-words Total no. of tgt-words
S?E 52 230
J?E 369 1639
Table 1: Sizes of the Spanish-to-English (S?E) and Japanese-
to-English (J?E) datasets.
are limited in size because we solicited human anno-
tators to filter the set of translations for each source
word. The S?E dataset has 52 source-words with a
part-of-speech-tag distribution of 38 nouns, 10 verbs
and 4 adverbs. The J?E dataset has 369 source-
words with 319 nouns, 38 verbs and 12 adverbs. We
included only these parts of speech because Word-
Net version 2.1 has adequate coverage for them.
Most source words have 3 to 5 translations each.
Monolingual features for K-Means clustering
were computed from an English corpus of Web
documents with 700 billion tokens of text. Bilin-
gual features were computed from 0.78 (S?E) and
1.04 (J?E) billion tokens of parallel text, primar-
ily extracted from the Web using automated paral-
lel document identification (Uszkoreit et al, 2010).
Word alignments were induced from the HMM-
based alignment model (Vogel et al, 1996), initial-
ized with the bilexical parameters of IBM Model 1
(Brown et al, 1993). Both models were trained us-
ing 2 iterations of the expectation maximization al-
gorithm. Phrase pairs were extracted from aligned
sentence pairs in the same manner used in phrase-
based machine translation (Koehn et al, 2003).
6.2 Clustering Evaluation Metrics
The quality of text clustering algorithms can be eval-
uated using a wide set of metrics. For evaluation
by set matching, the popular measures are Purity
(Zhao and Karypis, 2001) and Inverse Purity and
their harmonic mean (F measure, see Van Rijsber-
gen (1974)). For evaluation by counting pairs, the
popular metrics are the Rand Statistic and Jaccard
Coefficient (Halkidi et al, 2001; Meila, 2003).
Metrics based on entropy include Cluster Entropy
(Steinbach et al, 2000), Class Entropy (Bakus et al,
2002), VI-measure (Meila, 2003), Q0 (Dom, 2001),
V-measure (Rosenberg and Hirschberg, 2007) and
Mutual Information (Xu et al, 2003). Lastly, there
exist the BCubed metrics (Bagga and Baldwin,
1998), a family of metrics that decompose the clus-
777
tering evaluation by estimating precision and recall
for each item in the distribution.
Amigo et al (2009) compares the various clus-
tering metrics mentioned above and their properties.
They define four formal but intuitive constraints on
such metrics that explain which aspects of clustering
quality are captured by the different metric families.
Their analysis shows that of the wide range of met-
rics, only BCubed satisfies those constraints. After
defining each constraint below, we briefly describe
its relevance to the translation sense clustering task.
Homogeneity: In a cluster, we should not mix items
belonging to different categories.
Relevance: All words in a proposed cluster should
share some common WordNet sense.
Completeness: Items belonging to the same cate-
gory should be grouped in the same cluster.
Relevance: All words that share some common
WordNet sense should appear in the same cluster.
Rag Bag: Introducing disorder into a disordered
cluster is less harmful than introducing disorder into
a clean cluster.
Relevance: We prefer to maximize the number of
error-free clusters, because these are most easily in-
terpreted and therefore most useful.
Cluster Size vs. Quantity: A small error in a big
cluster is preferable to a large number of small er-
rors in small clusters.
Relevance: We prefer to minimize the total number
of erroneous clusters in a dictionary.
Amigo et al (2009) also show that BCubed ex-
tends cleanly to settings with overlapping clusters,
where an element can simultaneously belong to
more than one cluster. For these reasons, we focus
on BCubed for cluster similarity evaluation.7
The BCubed metric for scoring overlapping clus-
ters is computed from the pair-wise precision and
recall between pairs of items:
P(e, e?) =
min(|C(e) ? C(e?)|, |L(e) ? L(e?)|)
|C(e) ? C(e?)|
R(e, e?) =
min(|C(e) ? C(e?)|, |L(e) ? L(e?)|)
|L(e) ? L(e?)|
where e and e? are two items, L(e) is the set of ref-
erence clusters for e and C(e) is the set of predicted
7An evaluation using purity and inverse purity (extended to
overlapping clusters) has been omitted for space, but leads to
the same conclusions as the evaluation using BCubed.
clusters for e (i.e., clusters to which e belongs). Note
that P(e, e?) is defined only when e and e? share
some predicted cluster, and R(e, e?) when e and e?
share some reference cluster.
The BCubed precision associated to one item is its
averaged pair-wise precision over other items shar-
ing some of its predicted clusters, and likewise for
recall8; and the overall BCubed precision (or recall)
is the averaged precision (or recall) of all items:
PB3 = Avge[Avge?s.t.C(e)?C(e?)6=?[P(e, e
?)]]
RB3 = Avge[Avge?s.t.L(e)?L(e?)6=?[R(e, e
?)]]
6.3 Results
Figure 5 shows the F?-score for various ? values:
F? =
(1 + ?2) ? PB3 ? RB3
?2 ? PB3 + RB3
This graph gives us a trade-off between precision
and recall (? = 0 is exact precision and ? ? ?
tends to exact recall).9
Each curve in Figure 5 represents a particular
clustering method. We include three naive baselines:
ewnc: Each word in its own cluster
aw1c: All words in one cluster
Random: Each target word is assigned M random
cluster id?s in the range 1 to K, then translation
sets are clustered with the CP algorithm.
The curves for K-Means clustering include one
condition with monolingual features alone and two
curves that include bilingual features as well.10 The
bilingual curves correspond to two different feature
sets: the first includes only unigram features (t1),
while the second includes both unigram and bigram
features (t1t2).
Each point on an F? curve in Figure 5 (including
the baseline curves) represents a maximum over two
8The metric does include in this computation the relation of
each item with itself.
9Note that we use the micro-averaged version of F-score
where we first compute PB3 and RB3 for each source-word,
then compute the average PB3 and RB3 over all source-words,
and finally compute the F-score using these averaged PB3 and
RB3.
10All bilingual K-Means experiments include monolingual
features also. K-Means with only bilingual features does not
produce accurate clusters.
778
0.65 
0.7 
0.75 
0.8 
0.85 
0.9 
0.95 
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 
F ! s
core
 
!  
Spanish-English BCubed Results 
ewnc aw1c Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2 
0.7 
0.75 
0.8 
0.85 
0.9 
0.95 
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 
F ! s
core
 
!  
Japanese-English BCubed Results 
ewnc aw1c Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2 
Figure 5: BCubed F? plot for the Spanish-English dataset (top) and Japanese-English dataset (bottom).
Source word: ayudar
Monolingual [[aid], [assist, help]] P=1.0, R=0.56
Bilingual [[aid, assist, help]] P=1.0, R=1.0
Source word: concurso
Monolingual [[competition, contest, match], [concourse], [contest, meeting]] P=0.58, R=1.0
Bilingual [[competition, contest], [concourse], [match], [meeting]] P=1.0, R=1.0
Table 2: Examples showing improvements in clustering when we move from K-Means clustering with only monolingual features
to clustering with additional bilingual features.
779
0.79 
0.84 
0.89 
0.94 
0.99 
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Reca
ll 
Precision 
Japanese-English BCubed Results 
Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2 ewnc aw1c 
Figure 6: BCubed Precision-Recall scatter plot for the Japanese-English dataset. Each point represents a particular choice of cluster
count K and clusters per word M .
parameters: K, the number of clusters created in the
whole corpus andM , the number of clusters allowed
per word (in M -best soft K-Means). As both the
random baseline and proposed clustering methods
can be tuned to favor precision or recall, we show
the best result from each technique across this spec-
trum of F? metrics. We vary ? to highlight different
potential objectives of translation sense clustering.
An application that focuses on synonym discovery
would favor recall, while an application portraying
highly granular sense distinctions would favor pre-
cision.
Clustering accuracy improves over the baselines
with monolingual features alone, and it improves
further with the addition of bilingual features, for a
wide range of ? values. Our unsupervised approach
with bilingual features achieves up to 6-8% absolute
improvement over the random baseline, and is par-
ticularly effective for recall-weighted metrics.11 As
an example, in a S?E experiment with a K-Means
setting ofK = 4096 : M = 3, the overall F1.5 score
11It is not surprising that a naive baseline like random clus-
tering can achieve a high precision: BCubed counts each word
itself as correctly clustered, and so even trivial techniques that
create many singleton clusters will have high precision. High
recall (without very low precision) is harder to achieve, because
it requires positing larger clusters, and it is for recall-focused
objectives that our technique substantially outperforms the ran-
dom baseline.
increases from 80.58% to 86.12% upon adding bilin-
gual features. Table 2 shows two examples from that
experiment for which bilingual features improve the
output clusters.
The parameter values we use in our experiments
are K ? {23, 24, . . . , 212} and M ? {1, 2, 3, 4, 5}.
To provide additional detail, Figure 6 shows the
BCubed precision and recall for each induced clus-
tering, as the values of K and M vary, for Japanese-
English.12 Each point in this scatter plot represents a
clustering methodology and a particular value for K
and M . Soft K-Means with bilingual features pro-
vides the strongest performance across a broad range
of cluster parameters.
6.4 Evaluation Details
Certain special cases needed to be addressed in order
to complete this evaluation.
Target words not in WordNet: Words that did not
have any synset in WordNet were each assigned to a
singleton reference cluster.13 The S?E dataset has
only 2 out of 225 target types missing in WordNet
and the J?E dataset has only 55 out of 1351 target
12Spanish-English precision-recall results are omitted due to
space constraints, but depict similar trends.
13Note that certain words with WordNet synsets also end up
in their own singleton cluster because all other words in their
cluster are not in the translation set.
780
types missing.
Target words not clustered by K-Means: The K-
Means algorithm applies various thresholds during
different parts of the process. As a result, there
are some target word types that are not assigned
any cluster at the end of the algorithm. For ex-
ample, in the J?E experiment with K = 4096
and with bilingual (t1 only) features, only 49 out
of 1351 target-types are not assigned any cluster by
K-Means. These unclustered words were each as-
signed to a singleton cluster in post-processing.
7 Identifying Usage Examples
We now briefly consider the task of automatically
extracting usage examples for each predicted clus-
ter. We identify these examples among the extracted
phrase pairs of a parallel corpus.
Let Ps be the set of source phrases containing
source word s, and letAt be the set of source phrases
that align to target phrases containing target word
t. For a source word s and target sense cluster G,
we identify source phrases that contain s and trans-
late to all words in G. That is, we collect the set
of phrases Ps ?
?
t?GAt. We use the same parallel
corpus as we used to compute bilingual features.
For example, if we consider the cluster [place, po-
sition, put] for the Spanish word colocar, then we
find Spanish phrases that contain colocar and also
align to English phrases containing place, position,
and put somewhere in the parallel corpus. Sample
usage examples extracted by this approach appear in
Figure 7. We have not performed a quantitative eval-
uation of these extracted examples, although quali-
tatively we have found that the technique surfaces
useful phrases. We look forward to future research
that further explores this important sub-task of auto-
matically generating bilingual dictionaries.
8 Conclusion
We presented the task of translation sense clustering,
a critical second step to follow translation extraction
in a pipeline for generating well-structured bilingual
dictionaries automatically. We introduced a method
of projecting language-level clusters into clusters for
specific translation sets using the CP algorithm. We
used this technique both for constructing reference
clusters, via WordNet synsets, and constructing pre-
debajo
["below","beneath"]    ? debajo de la superficie (below the surface)
["below","under"]     ? debajo de la l?nea (below the line)
["underneath"]     ? debajo de la piel (under the skin)
??
["break"]     ? ???? ?? ? ?? ?? ?? ? ? ?? ?? . 
(I worked hard and I deserve a good break.)
["recreation"]     ? ?? ? ?? ? ?? ?? 
(Traditional healing and recreation activities)
["rest"]     ? ??? ? ?? ?? ?? ? ?? ?? . 
(Bed rest is the only treatment required.)
??
["application"]     ? ??????? ?? ?? 
(Computer-aided technique)
["use","utilization"]     ? ?? ? ?? ?? ? ?? ?? 
(Promote effective use of land)
??
["draw","pull"]     ? ???? ? ?? 
(Draw the curtain)
["subtract"]     ? A ?? B ? ?? 
(Subtract B from A)
["tug"]     ? ? ? ??? ?? 
(Tug at someone's sleeve)
Figure 7: Usage examples for Spanish and Japanese words and
their English sense clusters. Our approach extracts multiple
examples per cluster, but we show only one. We also show
the translation of the examples back into English produced by
Google Translate.
dicted clusters from the output of a vocabulary-level
clustering algorithm.
Our experiments demonstrated that the soft K-
Means clustering algorithm, trained using distribu-
tional features from very large monolingual and
bilingual corpora, recovered a substantial portion of
the structure of reference clusters, as measured by
the BCubed clustering metric. The addition of bilin-
gual features improved clustering results over mono-
lingual features alone; these features could prove
useful for other clustering tasks as well. Finally, we
annotated our clusters with usage examples.
In future work, we hope to combine our cluster-
ing method with a system for automatically gen-
erating translation sets. In doing so, we will de-
velop a system that can automatically induce high-
quality, human-readable bilingual dictionaries from
large corpora using unsupervised learning methods.
Acknowledgments
We would like to thank Jakob Uszkoreit, Adam
Pauls, and the anonymous reviewers for their helpful
suggestions.
781
References
Enrique Amigo, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4):461486.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of EACL.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of COLING-ACL.
J. Bakus, M. F. Hussin, and M. Kamel. 2002. A SOM-
based document clustering using phrases. In Proceed-
ings of ICONIP.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467479.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL.
B.E. Dom. 2001. An information-theoretic external
cluster-validity measure. In IBM Technical Report RJ-
10219.
M. Halkidi, Y. Batistakis, and M. Vazirgiannis. 2001. On
clustering validation techniques. Journal of Intelligent
Information Systems, 17(2-3):107?145.
Hiroyuki Kaji. 2003. Word sense acquisition from bilin-
gual comparable corpora. In Proceedings of NAACL.
Reinherd Kneser and Hermann Ney. 1993. Improved
clustering techniques for class-based statistical lan-
guage modelling. In Proceedings of the 3rd European
Conference on Speech Communication and Technol-
ogy.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL.
J. B. MacQueen. 1967. Some methods for classifica-
tion and analysis of multivariate observations. In Pro-
ceedings of 5th Berkeley Symposium on Mathematical
Statistics and Probability.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
Speech Communication, 24:19?37.
M. Meila. 2003. Comparing clusterings by the variation
of information. In Proceedings of COLT.
George A. Miller. 1995. Wordnet: A lexical database for
English. In Communications of the ACM.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of ACL.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of EMNLP.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. In Proceedings of KDD Workshop on Text
Mining.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In Proceedings
of EMNLP.
Dan Tufis, Radu Ion, and Nancy Ide. 2004. Fine-grained
word sense disambiguation based on parallel corpora,
word alignment, word clustering and aligned word-
nets. In Proceedings of COLING.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-
biner. 2010. Large scale parallel document mining for
machine translation. In Proceedings of COLING.
C. Van Rijsbergen. 1974. Foundation of evaluation.
Journal of Documentation, 30(4):365?373.
Mariano Vela?zquez de la Cadena, Edward Gray, and
Juan L. Iribas. 1965. New Revised Vela?zques Spanish
and English Dictionary. Follet Publishing Company.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and constrained Dirichlet
process mixture models for verb clustering. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
W. Xu, X. Liu, and Y. Gong. 2003. Document-clustering
based on non-negative matrix factorization. In Pro-
ceedings of SIGIR.
Y. Zhao and G. Karypis. 2001. Criterion functions for
document clustering: Experiments and analysis. In
Technical Report TR 01-40, Department of Computer
Science, University of Minnesota, Minneapolis, MN.
782
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865?874,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Creating Robust Supervised Classifiers via Web-Scale N-gram Data
Shane Bergsma
University of Alberta
sbergsma@ualberta.ca
Emily Pitler
University of Pennsylvania
epitler@seas.upenn.edu
Dekang Lin
Google, Inc.
lindek@google.com
Abstract
In this paper, we systematically assess the
value of using web-scale N-gram data in
state-of-the-art supervised NLP classifiers.
We compare classifiers that include or ex-
clude features for the counts of various
N-grams, where the counts are obtained
from a web-scale auxiliary corpus. We
show that including N-gram count features
can advance the state-of-the-art accuracy
on standard data sets for adjective order-
ing, spelling correction, noun compound
bracketing, and verb part-of-speech dis-
ambiguation. More importantly, when op-
erating on new domains, or when labeled
training data is not plentiful, we show that
using web-scale N-gram features is essen-
tial for achieving robust performance.
1 Introduction
Many NLP systems use web-scale N-gram counts
(Keller and Lapata, 2003; Nakov and Hearst,
2005; Brants et al, 2007). Lapata and Keller
(2005) demonstrate good performance on eight
tasks using unsupervised web-based models. They
show web counts are superior to counts from a
large corpus. Bergsma et al (2009) propose un-
supervised and supervised systems that use counts
from Google?s N-gram corpus (Brants and Franz,
2006). Web-based models perform particularly
well on generation tasks, where systems choose
between competing sequences of output text (such
as different spellings), as opposed to analysis
tasks, where systems choose between abstract la-
bels (such as part-of-speech tags or parse trees).
In this work, we address two natural and related
questions which these previous studies leave open:
1. Is there a benefit in combining web-scale
counts with the features used in state-of-the-
art supervised approaches?
2. How well do web-based models perform on
new domains or when labeled data is scarce?
We address these questions on two generation
and two analysis tasks, using both existing N-gram
data and a novel web-scale N-gram corpus that
includes part-of-speech information (Section 2).
While previous work has combined web-scale fea-
tures with other features in specific classification
problems (Modjeska et al, 2003; Yang et al,
2005; Vadas and Curran, 2007b), we provide a
multi-task, multi-domain comparison.
Some may question why supervised approaches
are needed at all for generation problems. Why
not solely rely on direct evidence from a giant cor-
pus? For example, for the task of prenominal ad-
jective ordering (Section 3), a system that needs
to describe a ball that is both big and red can sim-
ply check that big red is more common on the web
than red big, and order the adjectives accordingly.
It is, however, suboptimal to only use N-gram
data. For example, ordering adjectives by direct
web evidence performs 7% worse than our best
supervised system (Section 3.2). No matter how
large the web becomes, there will always be plau-
sible constructions that never occur. For example,
there are currently no pages indexed by Google
with the preferred adjective ordering for bedrag-
gled 56-year-old [professor]. Also, in a particu-
lar domain, words may have a non-standard usage.
Systems trained on labeled data can learn the do-
main usage and leverage other regularities, such as
suffixes and transitivity for adjective ordering.
With these benefits, systems trained on labeled
data have become the dominant technology in aca-
demic NLP. There is a growing recognition, how-
ever, that these systems are highly domain de-
pendent. For example, parsers trained on anno-
tated newspaper text perform poorly on other gen-
res (Gildea, 2001). While many approaches have
adapted NLP systems to specific domains (Tsu-
ruoka et al, 2005; McClosky et al, 2006; Blitzer
865
et al, 2007; Daume? III, 2007; Rimell and Clark,
2008), these techniques assume the system knows
on which domain it is being used, and that it has
access to representative data in that domain. These
assumptions are unrealistic in many real-world sit-
uations; for example, when automatically process-
ing a heterogeneous collection of web pages. How
well do supervised and unsupervised NLP systems
perform when used uncustomized, out-of-the-box
on new domains, and how can we best design our
systems for robust open-domain performance?
Our results show that using web-scale N-gram
data in supervised systems advances the state-of-
the-art performance on standard analysis and gen-
eration tasks. More importantly, when operating
out-of-domain, or when labeled data is not plen-
tiful, using web-scale N-gram data not only helps
achieve good performance ? it is essential.
2 Experiments and Data
2.1 Experimental Design
We evaluate the benefit of N-gram data on multi-
class classification problems. For each task, we
have some labeled data indicating the correct out-
put for each example. We evaluate with accuracy:
the percentage of examples correctly classified in
test data. We use one in-domain and two out-of-
domain test sets for each task. Statistical signifi-
cance is assessed with McNemar?s test, p<0.01.
We provide results for unsupervised approaches
and the majority-class baseline for each task.
For our supervised approaches, we represent the
examples as feature vectors, and learn a classi-
fier on the training vectors. There are two fea-
ture classes: features that use N-grams (N-GM)
and those that do not (LEX). N-GM features are
real-valued features giving the log-count of a par-
ticular N-gram in the auxiliary web corpus. LEX
features are binary features that indicate the pres-
ence or absence of a particular string at a given po-
sition in the input. The name LEX emphasizes that
they identify specific lexical items. The instantia-
tions of both types of features depend on the task
and are described in the corresponding sections.
Each classifier is a linear Support Vector Ma-
chine (SVM), trained using LIBLINEAR (Fan et al,
2008) on the standard domain. We use the one-vs-
all strategy when there are more than two classes
(in Section 4). We plot learning curves to mea-
sure the accuracy of the classifier when the num-
ber of labeled training examples varies. The size
of the N-gram data and its counts remain constant.
We always optimize the SVM?s (L2) regulariza-
tion parameter on the in-domain development set.
We present results with L2-SVM, but achieve sim-
ilar results with L1-SVM and logistic regression.
2.2 Tasks and Labeled Data
We study two generation tasks: prenominal ad-
jective ordering (Section 3) and context-sensitive
spelling correction (Section 4), followed by two
analysis tasks: noun compound bracketing (Sec-
tion 5) and verb part-of-speech disambiguation
(Section 6). In each section, we provide refer-
ences to the origin of the labeled data. For the
out-of-domain Gutenberg and Medline data used
in Sections 3 and 4, we generate examples our-
selves.1 We chose Gutenberg and Medline in order
to provide challenging, distinct domains from our
training corpora. Our Gutenberg corpus consists
of out-of-copyright books, automatically down-
loaded from the Project Gutenberg website.2 The
Medline data consists of a large collection of on-
line biomedical abstracts. We describe how la-
beled adjective and spelling examples are created
from these corpora in the corresponding sections.
2.3 Web-Scale Auxiliary Data
The most widely-used N-gram corpus is the
Google 5-gram Corpus (Brants and Franz, 2006).
For our tasks, we also use Google V2: a new
N-gram corpus (also with N-grams of length one-
to-five) that we created from the same one-trillion-
word snapshot of the web as the Google 5-gram
Corpus, but with several enhancements. These in-
clude: 1) Reducing noise by removing duplicate
sentences and sentences with a high proportion
of non-alphanumeric characters (together filtering
about 80% of the source data), 2) pre-converting
all digits to the 0 character to reduce sparsity for
numeric expressions, and 3) including the part-of-
speech (POS) tag distribution for each N-gram.
The source data was automatically tagged with
TnT (Brants, 2000), using the Penn Treebank tag
set. Lin et al (2010) provide more details on the
1http://webdocs.cs.ualberta.ca/?bergsma/Robust/
provides our Gutenberg corpus, a link to Medline, and also
the generated examples for both Gutenberg and Medline.
2
www.gutenberg.org. All books just released in 2009 and
thus unlikely to occur in the source data for our N-gram cor-
pus (from 2006). Of course, with removal of sentence dupli-
cates and also N-gram thresholding, the possible presence of
a test sentence in the massive source data is unlikely to affect
results. Carlson et al (2008) reach a similar conclusion.
866
N-gram data and N-gram search tools.
The third enhancement is especially relevant
here, as we can use the POS distribution to collect
counts for N-grams of mixed words and tags. For
example, we have developed an N-gram search en-
gine that can count how often the adjective un-
precedented precedes another adjective in our web
corpus (113K times) and how often it follows one
(11K times). Thus, even if we haven?t seen a par-
ticular adjective pair directly, we can use the posi-
tional preferences of each adjective to order them.
Early web-based models used search engines to
collect N-gram counts, and thus could not use cap-
italization, punctuation, and annotations such as
part-of-speech (Kilgarriff and Grefenstette, 2003).
Using a POS-tagged web corpus goes a long way
to addressing earlier criticisms of web-based NLP.
3 Prenominal Adjective Ordering
Prenominal adjective ordering strongly affects text
readability. For example, while the unprecedented
statistical revolution is fluent, the statistical un-
precedented revolution is not. Many NLP systems
need to handle adjective ordering robustly. In ma-
chine translation, if a noun has two adjective mod-
ifiers, they must be ordered correctly in the tar-
get language. Adjective ordering is also needed
in Natural Language Generation systems that pro-
duce information from databases; for example, to
convey information (in sentences) about medical
patients (Shaw and Hatzivassiloglou, 1999).
We focus on the task of ordering a pair of adjec-
tives independently of the noun they modify and
achieve good performance in this setting. Follow-
ing the set-up of Malouf (2000), we experiment
on the 263K adjective pairs Malouf extracted from
the British National Corpus (BNC). We use 90%
of pairs for training, 5% for testing, and 5% for
development. This forms our in-domain data.3
We create out-of-domain examples by tokeniz-
ing Medline and Gutenberg (Section 2.2), then
POS-tagging them with CRFTagger (Phan, 2006).
We create examples from all sequences of two ad-
jectives followed by a noun. Like Malouf (2000),
we assume that edited text has adjectives ordered
fluently. We extract 13K and 9.1K out-of-domain
pairs from Gutenberg and Medline, respectively.4
3BNC is not a domain per se (rather a balanced corpus),
but has a style and vocabulary distinct from our OOD data.
4Like Malouf (2000), we convert our pairs to lower-case.
Since the N-gram data includes case, we merge counts from
the upper and lower case combinations.
The input to the system is a pair of adjectives,
(a1, a2), ordered alphabetically. The task is to
classify this order as correct (the positive class) or
incorrect (the negative class). Since both classes
are equally likely, the majority-class baseline is
around 50% on each of the three test sets.
3.1 Supervised Adjective Ordering
3.1.1 LEX features
Our adjective ordering model with LEX features is
a novel contribution of this paper.
We begin with two features for each pair: an in-
dicator feature for a1, which gets a feature value of
+1, and an indicator feature for a2, which gets a
feature value of ?1. The parameters of the model
are therefore weights on specific adjectives. The
higher the weight on an adjective, the more it is
preferred in the first position of a pair. If the alpha-
betic ordering is correct, the weight on a1 should
be higher than the weight on a2, so that the clas-
sifier returns a positive score. If the reverse order-
ing is preferred, a2 should receive a higher weight.
Training the model in this setting is a matter of as-
signing weights to all the observed adjectives such
that the training pairs are maximally ordered cor-
rectly. The feature weights thus implicitly produce
a linear ordering of all observed adjectives. The
examples can also be regarded as rank constraints
in a discriminative ranker (Joachims, 2002). Tran-
sitivity is achieved naturally in that if we correctly
order pairs a ? b and b ? c in the training set,
then a ? c by virtue of the weights on a and c.
While exploiting transitivity has been shown
to improve adjective ordering, there are many
conflicting pairs that make a strict linear order-
ing of adjectives impossible (Malouf, 2000). We
therefore provide an indicator feature for the pair
a1a2, so the classifier can memorize exceptions
to the linear ordering, breaking strict order tran-
sitivity. Our classifier thus operates along the lines
of rankers in the preference-based setting as de-
scribed in Ailon and Mohri (2008).
Finally, we also have features for all suffixes of
length 1-to-4 letters, as these encode useful infor-
mation about adjective class (Malouf, 2000). Like
the adjective features, the suffix features receive a
value of +1 for adjectives in the first position and
?1 for those in the second.
3.1.2 N-GM features
Lapata and Keller (2005) propose a web-based
approach to adjective ordering: take the most-
867
System IN O1 O2
Malouf (2000) 91.5 65.6 71.6
web c(a1, a2) vs. c(a2, a1) 87.1 83.7 86.0
SVM with N-GM features 90.0 85.8 88.5
SVM with LEX features 93.0 70.0 73.9
SVM with N-GM + LEX 93.7 83.6 85.4
Table 1: Adjective ordering accuracy (%). SVM
and Malouf (2000) trained on BNC, tested on
BNC (IN), Gutenberg (O1), and Medline (O2).
frequent order of the words on the web, c(a1, a2)
vs. c(a2, a1). We adopt this as our unsupervised
approach. We merge the counts for the adjectives
occurring contiguously and separated by a comma.
These are indubitably the most important N-GM
features; we include them but also other, tag-based
counts from Google V2. Raw counts include cases
where one of the adjectives is not used as a mod-
ifier: ?the special present was? vs. ?the present
special issue.? We include log-counts for the
following, more-targeted patterns:5 c(a1 a2 N.*),
c(a2 a1 N.*), c(DT a1 a2 N.*), c(DT a2 a1 N.*).
We also include features for the log-counts of
each adjective preceded or followed by a word
matching an adjective-tag: c(a1 J.*), c(J.* a1),
c(a2 J.*), c(J.* a2). These assess the positional
preferences of each adjective. Finally, we include
the log-frequency of each adjective. The more fre-
quent adjective occurs first 57% of the time.
As in all tasks, the counts are features in a clas-
sifier, so the importance of the different patterns is
weighted discriminatively during training.
3.2 Adjective Ordering Results
In-domain, with both feature classes, we set a
strong new standard on this data: 93.7% accuracy
for the N-GM+LEX system (Table 1). We trained
and tested Malouf (2000)?s program on our data;
our LEX classifier, which also uses no auxiliary
corpus, makes 18% fewer errors than Malouf?s
system. Our web-based N-GM model is also su-
perior to the direct evidence web-based approach
of Lapata and Keller (2005), scoring 90.0% vs.
87.1% accuracy. These results show the benefit
of our new lexicalized and web-based features.
Figure 1 gives the in-domain learning curve.
With fewer training examples, the systems with
N-GM features strongly outperform the LEX-only
system. Note that with tens of thousands of test
5In this notation, capital letters (and regular expressions)
are matched against tags while a1 and a2 match words.
 60
 65
 70
 75
 80
 85
 90
 95
 100
1e51e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM+LEX
N-GM
LEX
Figure 1: In-domain learning curve of adjective
ordering classifiers on BNC.
 60
 65
 70
 75
 80
 85
 90
 95
 100
1e51e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM+LEX
N-GM
LEX
Figure 2: Out-of-domain learning curve of adjec-
tive ordering classifiers on Gutenberg.
examples, all differences are highly significant.
Out-of-domain, LEX?s accuracy drops a shock-
ing 23% on Gutenberg and 19% on Medline (Ta-
ble 1). Malouf (2000)?s system fares even worse.
The overlap between training and test pairs helps
explain. While 59% of the BNC test pairs were
seen in the training corpus, only 25% of Gutenberg
and 18% of Medline pairs were seen in training.
While other ordering models have also achieved
?very poor results? out-of-domain (Mitchell,
2009), we expected our expanded set of LEX fea-
tures to provide good generalization on new data.
Instead, LEX is very unreliable on new domains.
N-GM features do not rely on specific pairs in
training data, and thus remain fairly robust cross-
domain. Across the three test sets, 84-89% of
examples had the correct ordering appear at least
once on the web. On new domains, the learned
N-GM system maintains an advantage over the un-
supervised c(a1, a2) vs. c(a2, a1), but the differ-
ence is reduced. Note that training with 10-fold
868
cross validation, the N-GM system can achieve up
to 87.5% on Gutenberg (90.0% for N-GM + LEX).
The learning curve showing performance on
Gutenberg (but still training on BNC) is particu-
larly instructive (Figure 2, performance on Med-
line is very similar). The LEX system performs
much worse than the web-based models across
all training sizes. For our top in-domain sys-
tem, N-GM + LEX, as you add more labeled ex-
amples, performance begins decreasing out-of-
domain. The system disregards the robust N-gram
counts as it is more and more confident in the LEX
features, and it suffers the consequences.
4 Context-Sensitive Spelling Correction
We now turn to the generation problem of context-
sensitive spelling correction. For every occurrence
of a word in a pre-defined set of confusable words
(like peace and piece), the system must select the
most likely word from the set, flagging possible
usage errors when the predicted word disagrees
with the original. Contextual spell checkers are
one of the most widely used NLP technologies,
reaching millions of users via compressed N-gram
models in Microsoft Office (Church et al, 2007).
Our in-domain examples are from the New York
Times (NYT) portion of Gigaword, from Bergsma
et al (2009). They include the 5 confusion sets
where accuracy was below 90% in Golding and
Roth (1999). There are 100K training, 10K devel-
opment, and 10K test examples for each confusion
set. Our results are averages across confusion sets.
Out-of-domain examples are again drawn from
Gutenberg and Medline. We extract all instances
of words that are in one of our confusion sets,
along with surrounding context. By assuming the
extracted instances represent correct usage, we la-
bel 7.8K and 56K out-of-domain test examples for
Gutenberg and Medline, respectively.
We test three unsupervised systems: 1) Lapata
and Keller (2005) use one token of context on the
left and one on the right, and output the candidate
from the confusion set that occurs most frequently
in this pattern. 2) Bergsma et al (2009) measure
the frequency of the candidates in all the 3-to-5-
gram patterns that span the confusable word. For
each candidate, they sum the log-counts of all pat-
terns filled with the candidate, and output the can-
didate with the highest total. 3) The baseline pre-
dicts the most frequent member of each confusion
set, based on frequencies in the NYT training data.
System IN O1 O2
Baseline 66.9 44.6 60.6
Lapata and Keller (2005) 88.4 78.0 87.4
Bergsma et al (2009) 94.8 87.7 94.2
SVM with N-GM features 95.7 92.1 93.9
SVM with LEX features 95.2 85.8 91.0
SVM with N-GM + LEX 96.5 91.9 94.8
Table 2: Spelling correction accuracy (%). SVM
trained on NYT, tested on NYT (IN) and out-of-
domain Gutenberg (O1) and Medline (O2).
 70
 75
 80
 85
 90
 95
 100
1e51e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM+LEX
N-GM
LEX
Figure 3: In-domain learning curve of spelling
correction classifiers on NYT.
4.1 Supervised Spelling Correction
Our LEX features are typical disambiguation fea-
tures that flag specific aspects of the context. We
have features for the words at all positions in
a 9-word window (called collocation features by
Golding and Roth (1999)), plus indicators for a
particular word preceding or following the con-
fusable word. We also include indicators for all
N-grams, and their position, in a 9-word window.
For N-GM count features, we follow Bergsma
et al (2009). We include the log-counts of all
N-grams that span the confusable word, with each
word in the confusion set filling the N-gram pat-
tern. These features do not use part-of-speech.
Following Bergsma et al (2009), we get N-gram
counts using the original Google N-gram Corpus.
While neither our LEX nor N-GM features are
novel on their own, they have, perhaps surpris-
ingly, not yet been evaluated in a single model.
4.2 Spelling Correction Results
The N-GM features outperform the LEX features,
95.7% vs. 95.2% (Table 2). Together, they
achieve a very strong 96.5% in-domain accuracy.
869
This is 2% higher than the best unsupervised ap-
proach (Bergsma et al, 2009). Web-based models
again perform well across a range of training data
sizes (Figure 3).
The error rate of LEX nearly triples on Guten-
berg and almost doubles on Medline (Table 2). Re-
moving N-GM features from the N-GM + LEX sys-
tem, errors increase around 75% on both Guten-
berg and Medline. The LEX features provide no
help to the combined system on Gutenberg, while
they do help significantly on Medline. Note the
learning curves for N-GM+LEX on Gutenberg and
Medline (not shown) do not display the decrease
that we observed in adjective ordering (Figure 2).
Both the baseline and LEX perform poorly on
Gutenberg. The baseline predicts the majority
class from NYT, but it?s not always the majority
class in Gutenberg. For example, while in NYT
site occurs 87% of the time for the (cite, sight,
site) confusion set, sight occurs 90% of the time in
Gutenberg. The LEX classifier exploits this bias as
it is regularized toward a more economical model,
but the bias does not transfer to the new domain.
5 Noun Compound Bracketing
About 70% of web queries are noun phrases (Barr
et al, 2008) and methods that can reliably parse
these phrases are of great interest in NLP. For
example, a web query for zebra hair straightener
should be bracketed as (zebra (hair straightener)),
a stylish hair straightener with zebra print, rather
than ((zebra hair) straightener), a useless product
since the fur of zebras is already quite straight.
The noun compound (NC) bracketing task is
usually cast as a decision whether a 3-word NC
has a left or right bracketing. Most approaches are
unsupervised, using a large corpus to compare the
statistical association between word pairs in the
NC. The adjacency model (Marcus, 1980) pro-
poses a left bracketing if the association between
words one and two is higher than between two
and three. The dependency model (Lauer, 1995a)
compares one-two vs. one-three. We include de-
pendency model results using PMI as the associ-
ation measure; results were lower with the adja-
cency model.
As in-domain data, we use Vadas and Curran
(2007a)?s Wall-Street Journal (WSJ) data, an ex-
tension of the Treebank (which originally left NPs
flat). We extract all sequences of three consec-
utive common nouns, generating 1983 examples
System IN O1 O2
Baseline 70.5 66.8 84.1
Dependency model 74.7 82.8 84.4
SVM with N-GM features 89.5 81.6 86.2
SVM with LEX features 81.1 70.9 79.0
SVM with N-GM + LEX 91.6 81.6 87.4
Table 3: NC-bracketing accuracy (%). SVM
trained on WSJ, tested on WSJ (IN) and out-of-
domain Grolier (O1) and Medline (O2).
 60
 65
 70
 75
 80
 85
 90
 95
 100
1e310010
Ac
cu
ra
cy
 (%
)
Number of labeled examples
N-GM+LEX
N-GM
LEX
Figure 4: In-domain NC-bracketer learning curve
from sections 0-22 of the Treebank as training, 72
from section 24 for development and 95 from sec-
tion 23 as a test set. As out-of-domain data, we
use 244 NCs from Grolier Encyclopedia (Lauer,
1995a) and 429 NCs from Medline (Nakov, 2007).
The majority class baseline is left-bracketing.
5.1 Supervised Noun Bracketing
Our LEX features indicate the specific noun at
each position in the compound, plus the three pairs
of nouns and the full noun triple. We also add fea-
tures for the capitalization pattern of the sequence.
N-GM features give the log-count of all subsets
of the compound. Counts are from Google V2.
Following Nakov and Hearst (2005), we also in-
clude counts of noun pairs collapsed into a single
token; if a pair occurs often on the web as a single
unit, it strongly indicates the pair is a constituent.
Vadas and Curran (2007a) use simpler features,
e.g. they do not use collapsed pair counts. They
achieve 89.9% in-domain on WSJ and 80.7% on
Grolier. Vadas and Curran (2007b) use compara-
ble features to ours, but do not test out-of-domain.
5.2 Noun Compound Bracketing Results
N-GM systems perform much better on this task
(Table 3). N-GM+LEX is statistically significantly
870
better than LEX on all sets. In-domain, errors
more than double without N-GM features. LEX
performs poorly here because there are far fewer
training examples. The learning curve (Figure 4)
looks much like earlier in-domain curves (Fig-
ures 1 and 3), but truncated before LEX becomes
competitive. The absence of a sufficient amount of
labeled data explains why NC-bracketing is gen-
erally regarded as a task where corpus counts are
crucial.
All web-based models (including the depen-
dency model) exceed 81.5% on Grolier, which
is the level of human agreement (Lauer, 1995b).
N-GM + LEX is highest on Medline, and close
to the 88% human agreement (Nakov and Hearst,
2005). Out-of-domain, the LEX approach per-
forms very poorly, close to or below the base-
line accuracy. With little training data and cross-
domain usage, N-gram features are essential.
6 Verb Part-of-Speech Disambiguation
Our final task is POS-tagging. We focus on one
frequent and difficult tagging decision: the distinc-
tion between a past-tense verb (VBD) and a past
participle (VBN). For example, in the troops sta-
tioned in Iraq, the verb stationed is a VBN; troops
is the head of the phrase. On the other hand, for
the troops vacationed in Iraq, the verb vacationed
is a VBD and also the head. Some verbs make the
distinction explicit (eat has VBD ate, VBN eaten),
but most require context for resolution.
Conflating VBN/VBD is damaging because it af-
fects downstream parsers and semantic role la-
belers. The task is difficult because nearby POS
tags can be identical in both cases. When the
verb follows a noun, tag assignment can hinge on
world-knowledge, i.e., the global lexical relation
between the noun and verb (E.g., troops tends to
be the object of stationed but the subject of vaca-
tioned).6 Web-scale N-gram data might help im-
prove the VBN/VBD distinction by providing rela-
tional evidence, even if the verb, noun, or verb-
noun pair were not observed in training data.
We extract nouns followed by a VBN/VBD in the
WSJ portion of the Treebank (Marcus et al, 1993),
getting 23K training, 1091 development and 1130
test examples from sections 2-22, 24, and 23, re-
spectively. For out-of-domain data, we get 21K
6HMM-style taggers, like the fast TnT tagger used on our
web corpus, do not use bilexical features, and so perform es-
pecially poorly on these cases. One motivation for our work
was to develop a fast post-processor to fix VBN/VBD errors.
examples from the Brown portion of the Treebank
and 6296 examples from tagged Medline abstracts
in the PennBioIE corpus (Kulick et al, 2004).
The majority class baseline is to choose VBD.
6.1 Supervised Verb Disambiguation
There are two orthogonal sources of information
for predicting VBN/VBD: 1) the noun-verb pair,
and 2) the context around the pair. Both N-GM
and LEX features encode both these sources.
6.1.1 LEX features
For 1), we use indicators for the noun and verb,
the noun-verb pair, whether the verb is on an in-
house list of said-verb (like warned, announced,
etc.), whether the noun is capitalized and whether
it?s upper-case. Note that in training data, 97.3%
of capitalized nouns are followed by a VBD and
98.5% of said-verbs are VBDs. For 2), we provide
indicator features for the words before the noun
and after the verb.
6.1.2 N-GM features
For 1), we characterize a noun-verb relation via
features for the pair?s distribution in Google V2.
Characterizing a word by its distribution has a
long history in NLP; we apply similar techniques
to relations, like Turney (2006), but with a larger
corpus and richer annotations. We extract the 20
most-frequent N-grams that contain both the noun
and the verb in the pair. For each of these, we con-
vert the tokens to POS-tags, except for tokens that
are among the most frequent 100 unigrams in our
corpus, which we include in word form. We mask
the noun of interest as N and the verb of interest
as V . This converted N-gram is the feature label.
The value is the pattern?s log-count. A high count
for patterns like (N that V), (N have V) suggests
the relation is a VBD, while patterns (N that were
V), (N V by), (V some N) indicate a VBN. As al-
ways, the classifier learns the association between
patterns and classes.
For 2), we use counts for the verb?s context co-
occurring with a VBD or VBN tag. E.g., we see
whether VBD cases like troops ate or VBN cases
like troops eaten are more frequent. Although our
corpus contains many VBN/VBD errors, we hope
the errors are random enough for aggregate counts
to be useful. The context is an N-gram spanning
the VBN/VBD. We have log-count features for all
five such N-grams in the (previous-word, noun,
verb, next-word) quadruple. The log-count is in-
871
System IN O1 O2
Baseline 89.2 85.2 79.6
ContextSum 92.5 91.1 90.4
SVM with N-GM features 96.1 93.4 93.8
SVM with LEX features 95.8 93.4 93.0
SVM with N-GM + LEX 96.4 93.5 94.0
Table 4: Verb-POS-disambiguation accuracy (%)
trained on WSJ, tested on WSJ (IN) and out-of-
domain Brown (O1) and Medline (O2).
 80
 85
 90
 95
 100
1e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM (N,V+context)
LEX (N,V+context)
N-GM (N,V)
LEX (N,V)
Figure 5: Out-of-domain learning curve of verb
disambiguation classifiers on Medline.
dexed by the position and length of the N-gram.
We include separate count features for contexts
matching the specific noun and for when the noun
token can match any word tagged as a noun.
ContextSum: We use these context counts in an
unsupervised system, ContextSum. Analogously
to Bergsma et al (2009), we separately sum the
log-counts for all contexts filled with VBD and
then VBN, outputting the tag with the higher total.
6.2 Verb POS Disambiguation Results
As in all tasks, N-GM+LEX has the best in-domain
accuracy (96.4%, Table 4). Out-of-domain, when
N-grams are excluded, errors only increase around
14% on Medline and 2% on Brown (the differ-
ences are not statistically significant). Why? Fig-
ure 5, the learning curve for performance on Med-
line, suggests some reasons. We omit N-GM+LEX
from Figure 5 as it closely follows N-GM.
Recall that we grouped the features into two
views: 1) noun-verb (N,V) and 2) context. If we
use just (N,V) features, we do see a large drop out-
of-domain: LEX (N,V) lags N-GM (N,V) even us-
ing all the training examples. The same is true us-
ing only context features (not shown). Using both
views, the results are closer: 93.8% for N-GM and
93.0% for LEX. With two views of an example,
LEX is more likely to have domain-neutral fea-
tures to draw on. Data sparsity is reduced.
Also, the Treebank provides an atypical num-
ber of labeled examples for analysis tasks. In a
more typical situation with less labeled examples,
N-GM strongly dominates LEX, even when two
views are used. E.g., with 2285 training exam-
ples, N-GM+LEX is statistically significantly bet-
ter than LEX on both out-of-domain sets.
All systems, however, perform log-linearly with
training size. In other tasks we only had a handful
of N-GM features; here there are 21K features for
the distributional patterns of N,V pairs. Reducing
this feature space by pruning or performing trans-
formations may improve accuracy in and out-of-
domain.
7 Discussion and Future Work
Of all classifiers, LEX performs worst on all cross-
domain tasks. Clearly, many of the regularities
that a typical classifier exploits in one domain do
not transfer to new genres. N-GM features, how-
ever, do not depend directly on training examples,
and thus work better cross-domain. Of course, us-
ing web-scale N-grams is not the only way to cre-
ate robust classifiers. Counts from any large auxil-
iary corpus may also help, but web counts should
help more (Lapata and Keller, 2005). Section 6.2
suggests that another way to mitigate domain-
dependence is having multiple feature views.
Banko and Brill (2001) argue ?a logical next
step for the research community would be to di-
rect efforts towards increasing the size of anno-
tated training collections.? Assuming we really do
want systems that operate beyond the specific do-
mains on which they are trained, the community
also needs to identify which systems behave as in
Figure 2, where the accuracy of the best in-domain
system actually decreases with more training ex-
amples. Our results suggest better features, such
as web pattern counts, may help more than ex-
panding training data. Also, systems using web-
scale unlabeled data will improve automatically as
the web expands, without annotation effort.
In some sense, using web counts as features
is a form of domain adaptation: adapting a web
model to the training domain. How do we ensure
these features are adapted well and not used in
domain-specific ways (especially with many fea-
tures to adapt, as in Section 6)? One option may
872
be to regularize the classifier specifically for out-
of-domain accuracy. We found that adjusting the
SVM misclassification penalty (for more regular-
ization) can help or hurt out-of-domain. Other
regularizations are possible. In each task, there
are domain-neutral unsupervised approaches. We
could encode these systems as linear classifiers
with corresponding weights. Rather than a typical
SVM that minimizes the weight-norm ||w|| (plus
the slacks), we could regularize toward domain-
neutral weights. This regularization could be opti-
mized on creative splits of the training data.
8 Conclusion
We presented results on tasks spanning a range of
NLP research: generation, disambiguation, pars-
ing and tagging. Using web-scale N-gram data
improves accuracy on each task. When less train-
ing data is used, or when the system is used on a
different domain, N-gram features greatly improve
performance. Since most supervised NLP systems
do not use web-scale counts, further cross-domain
evaluation may reveal some very brittle systems.
Continued effort in new domains should be a pri-
ority for the community going forward.
Acknowledgments
We gratefully acknowledge the Center for Lan-
guage and Speech Processing at Johns Hopkins
University for hosting the workshop at which part
of this research was conducted.
References
Nir Ailon and Mehryar Mohri. 2008. An efficient re-
duction of ranking to classification. In COLT.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In ACL.
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English web-search
queries. In EMNLP.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2009. Web-scale N-gram models for lexical disam-
biguation. In IJCAI.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram Corpus Version 1.1. LDC2006T13.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In EMNLP.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In ANLP.
Andrew Carlson, Tom M. Mitchell, and Ian Fette.
2008. Data analysis project: Leveraging massive
textual corpora using n-gram statistics. Technial Re-
port CMU-ML-08-107.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with Golomb
coding. In EMNLP-CoNLL.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9.
Dan Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459?484.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the Web as corpus.
Computational Linguistics, 29(3):333?347.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction. In BioLINK 2004: Linking Biological Lit-
erature, Ontologies and Databases.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Transactions on Speech and Language Processing,
2(1):1?31.
Mark Lauer. 1995a. Corpus statistics meet the noun
compound: Some empirical results. In ACL.
Mark Lauer. 1995b. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale N-grams. In LREC.
873
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Mitchell P. Marcus. 1980. Theory of Syntactic Recog-
nition for Natural Languages. MIT Press, Cam-
bridge, MA, USA.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In COLING-ACL.
Margaret Mitchell. 2009. Class-based ordering of
prenominal modifiers. In 12th European Workshop
on Natural Language Generation.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the Web in machine learning for
other-anaphora resolution. In EMNLP.
Preslav Nakov and Marti Hearst. 2005. Search engine
statistics beyond the n-gram: Application to noun
compound bracketing. In CoNLL.
Preslav Ivanov Nakov. 2007. Using the Web as an Im-
plicit Training Set: Application to Noun Compound
Syntax and Semantics. Ph.D. thesis, University of
California, Berkeley.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English
POS Tagger. crftagger.sourceforge.net.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In EMNLP.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In ACL.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
David Vadas and James R. Curran. 2007a. Adding
noun phrase structure to the Penn Treebank. In ACL.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In
PACLING.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005.
Improving pronoun resolution using statistics-based
semantic compatibility information. In ACL.
874
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 172?181,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Improved Natural Language Learning via
Variance-Regularization Support Vector Machines
Shane Bergsma
University of Alberta
sbergsma@ualberta.ca
Dekang Lin
Google, Inc.
lindek@google.com
Dale Schuurmans
University of Alberta
dale@cs.ualberta.ca
Abstract
We present a simple technique for learn-
ing better SVMs using fewer training ex-
amples. Rather than using the standard
SVM regularization, we regularize toward
low weight-variance. Our new SVM ob-
jective remains a convex quadratic func-
tion of the weights, and is therefore com-
putationally no harder to optimize than a
standard SVM. Variance regularization is
shown to enable dramatic improvements
in the learning rates of SVMs on three lex-
ical disambiguation tasks.
1 Introduction
Discriminative training is commonly used in NLP
and speech to scale the contribution of different
models or systems in a combined predictor. For
example, discriminative training can be used to
scale the contribution of the language model and
translation model in machine translation (Och and
Ney, 2002). Without training data, it is often rea-
sonable to weight the different models equally. We
propose a simple technique that exploits this intu-
ition for better learning with fewer training exam-
ples. We regularize the feature weights in a Sup-
port Vector Machine (Cortes and Vapnik, 1995) to-
ward a low-variance solution. Since the new SVM
quadratic program is convex, it is no harder to op-
timize than the standard SVM objective.
When training data is generated through hu-
man effort, faster learning saves time and money.
When examples are labeled automatically, through
user feedback (Joachims, 2002) or from tex-
tual pseudo-examples (Smith and Eisner, 2005;
Okanohara and Tsujii, 2007), faster learning can
reduce the lag before a new system is useful.
We demonstrate faster learning on lexical dis-
ambiguation tasks. For these tasks, a system pre-
dicts a label for a word in text, based on the
word?s context. Possible labels include part-of-
speech tags, named-entity types, and word senses.
A number of disambiguation systems make pre-
dictions with the help of N-gram counts from a
web-scale auxiliary corpus, typically via a search-
engine (Lapata and Keller, 2005) or N-gram cor-
pus (Bergsma et al, 2009). When discriminative
training is used to weigh the counts for classifi-
cation, many of the learned feature weights have
similar values. Good weights have low variance.
For example, consider the task of preposition
selection. A system selects the most likely prepo-
sition given the context, and flags a possible error
if it disagrees with the user?s choice:
? I worked in Russia from 1997 to 2001.
? I worked in Russia *during 1997 to 2001.
Bergsma et al (2009) use a variety of web counts
to predict the correct preposition. They have fea-
tures for COUNT(in Russia from), COUNT(Russia
from 1997), COUNT(from 1997 to), etc. If these are
high, from is predicted. Similarly, they have fea-
tures for COUNT(in Russia during), COUNT(Russia
during 1997), COUNT(during 1997 to). These fea-
tures predict during. All counts are in the log
domain. The task has thirty-four different prepo-
sitions to choose from. A 34-way classifier is
trained on examples of correct preposition usage;
it learns which context positions and sizes are most
reliable and assigns feature weights accordingly.
A very strong unsupervised baseline, however,
is to simply weight all the count features equally.
In fact, in Bergsma et al (2009), the supervised
approach requires over 30,000 training examples
before it outperforms this baseline. In contrast,
we show that by regularizing a classifier toward
equal weights, a supervised predictor outperforms
the unsupervised approach after only ten exam-
ples, and does as well with 1000 examples as the
standard classifier does with 100,000.
172
Section 2 first describes a general multi-class
SVM. We call the base vector of information
used by the SVM the attributes. A standard
multi-class SVM creates features for the cross-
product of attributes and classes. E.g., the attribute
COUNT(Russia during 1997) is not only a feature
for predicting the preposition during, but also for
predicting the 33 other prepositions. The SVM
must therefore learn to disregard many irrelevant
features. We observe that this is not necessary,
and develop an SVM that only uses the relevant
attributes in the score for each class. Building on
this efficient framework, we incorporate variance
regularization into the SVM?s quadratic program.
We apply our algorithms to three tasks: prepo-
sition selection, context-sensitive spelling correc-
tion, and non-referential pronoun detection (Sec-
tion 4). We reproduce Bergsma et al (2009)?s
results using a multi-class SVM. Our new mod-
els achieve much better accuracy with fewer train-
ing examples. We also exceed the accuracy of a
reasonable alternative technique for increasing the
learning rate: including the output of the unsuper-
vised system as a feature in the SVM.
Variance regularization is an elegant addition to
the suite of methods in NLP that improve perfor-
mance when access to labeled data is limited. Sec-
tion 5 discusses some related approaches. While
we motivate our algorithm as a way to learn better
weights when the features are counts from an aux-
iliary corpus, there are other potential uses of our
method. We outline some of these in Section 6,
and note other directions for future research.
2 Three Multi-Class SVM Models
We describe three max-margin multi-class classi-
fiers and their corresponding quadratic programs.
Although we describe linear SVMs, they can be
extended to nonlinear cases in the standard way
by writing the optimal function as a linear combi-
nation of kernel functions over the input examples.
In each case, after providing the general tech-
nique, we relate the approach to our motivating
application: learning weights for count features in
a discriminative web-scale N-gram model.
2.1 Standard Multi-Class SVM
We define a K-class SVM following Crammer and
Singer (2001). This is a generalization of binary
SVMs (Cortes and Vapnik, 1995). We have a set
{(x?1, y1), ..., (x?M , yM )} of M training examples.
Each x? is an N -dimensional attribute vector, and
y ? {1, ...,K} are classes. A classifier, H , maps
an attribute vector, x?, to a class, y. H is parame-
terized by a K-by-N matrix of weights, W:
HW(x?) =
Kargmax
r=1
{W?r ? x?} (1)
where W?r is the rth row of W. That is, the pre-
dicted label is the index of the row of W that has
the highest inner-product with the attributes, x?.
We seek weights such that the classifier makes
few errors on training data and generalizes well
to unseen data. There are KN weights to learn,
for the cross-product of attributes and classes.
The most common approach is to train K sep-
arate one-versus-all binary SVMs, one for each
class. The weights learned for the rth SVM pro-
vide the weights W?r in (1). We call this approach
OvA-SVM. Note in some settings various one-
versus-one strategies may be more effective than
one-versus-all (Hsu and Lin, 2002).
The weights can also be found using a single
constrained optimization (Vapnik, 1998; Weston
and Watkins, 1998). Following the soft-margin
version in Crammer and Singer (2001):
min
W,?1,...,?M
1
2
K
?
i=1
||W?i||2 + C
m
?
i=1
?i
subject to : ?i ?0
?r 6= yi, W?yi ? x?i ? W?r ? x?i ?1 ? ?i (2)
The constraints require the correct class to be
scored higher than other classes by a certain mar-
gin, with slack for non-separable cases. Minimiz-
ing the weights is a form of regularization. Tuning
the C-parameter controls the emphasis on regular-
ization versus separation of training examples.
We call this the K-SVM. The K-SVM out-
performed the OvA-SVM in Crammer and Singer
(2001), but see Rifkin and Klautau (2004). The
popularity of K-SVM is partly due to convenience;
it is included in popular SVM software like SVM-
multiclass1 and LIBLINEAR (Fan et al, 2008).
Note that with two classes, K-SVM is less effi-
cient than a standard binary SVM. A binary classi-
fier outputs class 1 if (w? ? x? > 0) and class 2 other-
wise. The K-SVM encodes a binary classifier using
W?1 = w? and W?2 = ?w?, therefore requiring twice
the memory of a binary SVM. However, both bi-
nary and 2-class formulations have the same solu-
tion (Weston and Watkins, 1998).
1http://svmlight.joachims.org/svm multiclass.html
173
2.1.1 Web-Scale N-gram K-SVM
K-SVM was used with N-gram models in Bergsma
et al (2009). For preposition selection, attributes
were web counts of patterns filled with 34 preposi-
tions, corresponding to the 34 classes. Each prepo-
sition serves as the filler of each context pattern.
Fourteen patterns were used for each filler: all five
5-grams, four 4-grams, three 3-grams, and two 2-
grams spanning the position to be predicted. There
are N = 14?34 = 476 total attributes, and therefore
KN = 476 ? 34 = 16184 weights in W.
This K-SVM classifier can potentially exploit
very subtle information. Let W?in and W?before
be weights for the classes in and before. Notice
some of the attributes weighted in the inner prod-
ucts W?before ? x? and W?in ? x? will be for counts of
the preposition after. Relatively high counts for a
context with after should deter us from choosing
in more than from choosing before. These cor-
relations can be encoded in the classifier via the
corresponding weights on after-counts in W?in and
W?before. How useful are these correlations and
how much training data is needed before they can
be learned and exploited effectively?
We next develop a model that, for each class,
only scores those attributes deemed to be directly
relevant to the class. Our experiments thus empir-
ically address these questions for different tasks.
2.2 SVM with Class-Specific Attributes
Suppose we can partition our attribute vectors into
sub-vectors that only include attributes that we de-
clare as relevant to the corresponding class: x? =
(x?1, ..., x?K). We develop a classifier that only
uses the class-specific attributes in the score for
each class. The classifier uses an N -dimensional
weight vector, w?, which follows the attribute par-
tition, w? = (w?1, ..., w?K). The classifier is:
Hw?(x?) =
Kargmax
r=1
{w?r ? x?r} (3)
We call this classifier the CS-SVM (an SVM with
Class-Specific attributes).
The weights can be determined using the follow
(soft-margin) optimization:
min
w?,?1,...,?m
1
2 w?
T w? + C
m
?
i=1
?i
subject to : ?i ?0
?r 6= yi, w?yi ? x?iyi ? w?r ? x?ir ?1 ? ?i (4)
There are several advantages to this formula-
tion. Foremost, rather than having KN weights,
it can have only N . For linear classifiers, the
number of examples needed to reach optimum
performance is at most linear in the number of
weights (Vapnik, 1998; Ng and Jordan, 2002). In
fact, both the total number and number of active
features per example decrease by K. Thus this re-
duction saves far more memory than what could
be obtained by an equal reduction in dimensional-
ity via pruning infrequent attributes.
Also, note that unlike the K-SVM (Section 2.1),
in the binary case the CS-SVM is completely equiv-
alent (thus equally efficient) to a standard SVM.
We will not always a priori know the class as-
sociated with each attribute. Also, some attributes
may be predictive of multiple classes. In such
cases, we can include ambiguous attributes in ev-
ery sub-vector (needing N+D(K-1) total weights
if D attributes are duplicated). In the degenerate
case where every attribute is duplicated, CS-SVM
is equivalent to K-SVM; both have KN weights.
2.2.1 Optimization as a Binary SVM
We could solve the optimization problem in (4)
directly using a quadratic programming solver.
However, through an equivalent transformation
into a binary SVM, we can take advantage of effi-
cient, custom SVM optimization algorithms.
We follow Har-Peled et al (2003) in transform-
ing a multi-class example into a set of binary
examples, each specifying a constraint from (4).
We extend the attribute sub-vector corresponding
to each class to be N -dimensional. We do this
by substituting zero-vectors for all the other sub-
vectors in the partition. The attribute vector for the
rth class is then z?r = (0?, ..., 0?, x?r, 0?, ..., 0?). This is
known as Kesler?s Construction and has a long his-
tory in classification (Duda and Hart, 1973; Cram-
mer and Singer, 2003). We then create binary rank
constraints for a ranking SVM (Joachims, 2002)
(ranking SVMs reduce to standard binary SVMs).
We create K instances for each multi-class exam-
ple (x?i, yi), with the transformed vector of the true
class, z?yi , assigned a higher-rank than all the other,
equally-ranked classes, z?{r 6=yi}. Training a rank-
ing SVM using these constraints gives the same
weights as solving (4), but allows us to use effi-
cient, custom SVM software.2 Note the K-SVM
2One subtlety is whether to use a single slack, ?i, for all
K-1 constraints per example i (Crammer and Singer, 2001),
or a different slack for each constraint (Joachims, 2002). Us-
174
can also be trained this way, by including every
attribute in every sub-vector, as described earlier.
2.2.2 Web-Scale N-gram CS-SVM
Returning to our preposition selection example, an
obvious attribute partition for the CS-SVM is to
include as attributes for predicting preposition r
only those counts for patterns filled with preposi-
tion r. Thus x?in will only include counts for con-
text patterns filled with in and x?before will only
include counts for context patterns filled with be-
fore. With 34 sub-vectors and 14 attributes in each,
there are only 14 ? 34 = 476 total weights. In con-
trast, K-SVM had 16184 weights to learn.
It is instructive to compare the CS-SVM in (3) to
the unsupervised SUMLM approach in Bergsma et
al. (2009). That approach can be written as:
H(x?) = Kargmax
r=1
{1? ? x?r} (5)
where 1? is an N -dimensional vector of ones. This
is CS-SVM with all weights set to unity. The
counts for each preposition are simply summed,
and whichever one scores the highest is taken as
the output (actually only a subset of the counts are
used, see Section 4.1). As mentioned earlier, this
system performs remarkably well on several tasks.
2.3 Variance Regularization SVMs
Suppose we choose our attribute partition well and
train the CS-SVM on a sufficient number of exam-
ples to achieve good performance. It is a reason-
able hypothesis that the learned weights will be
predominantly positive. This is because each sub-
vector x?r was chosen to only include attributes
that are predictive of class r. Unlike the classifier
in (1) which weighs positive and negative evidence
together for each class, in CS-SVM, negative evi-
dence only plays a roll as it contributes to the score
of competing classes.
If all the attributes are equally important, the
weights should be equal, as in the unsupervised
approach in (5). If some are more important than
others, the training examples should reflect this
and the learner can adjust the weights accord-
ingly.3 In the absence of this training evidence, it
is reasonable to bias the classifier toward an equal-
weight solution.
ing the former may be better as it results in a tighter bound
on empirical risk (Tsochantaridis et al, 2005).
3E.g., the true preposition might be better predicted by the
counts of patterns that tend to include the preposition?s gram-
matical object, i.e., patterns that include more right-context.
Rather than the standard SVM regularization
that minimizes the norm of the weights as in (4),
we therefore regularize toward weights that have
low variance. More formally, we can regard the
set of weights, w1, ..., wN , as the distribution of a
discrete random variable, W . We can calculate the
mean and variance of this variable from its distri-
bution. We seek a variable that has low variance.
We begin with a more general objective and
then explain how a specific choice of covariance
matrix, C, minimizes the variance of the weights.
We propose the regularizer:
min
w?,?1,...,?m
1
2 w?
TCw? +C
m
?
i=1
?i
subject to : ?i ?0
?r 6= yi, w?yi ? x?iyi ? w?r ? x?ir ?1 ? ?i (6)
where C is a normalized covariance matrix such
that
?
i,j Ci,j = 0. This ensures uniform weight
vectors receive zero regularization penalty. Since
all covariance matrices are positive semi-definite,
the quadratic program (QP) remains convex in w?,
and thus amenable to general purpose QP-solvers.
Since the unsupervised system in (5) has zero
weight variance, the SVM learned in (6) should do
as least as well as (5) as we tune the C-parameter
on development data. That is, as C approaches
zero, variance minimization becomes the sole ob-
jective of (6), and uniform weights are produced.
We use covariance matrices of the form:
C = diag(p?) ? p?p?T (7)
where diag(p?) is the matrix constructed by putting
p? on the main diagonal. Here, p? is an arbitrary
N -dimensional weighting vector, such that p ?
0 and ?i pi = 1. p? dictates the contribution of
each wi to the mean and variance of the weights
in w?. It is easy to see that
?
i,j Ci,j =
?
i pi ?
?
i
?
j pipj = 0.
We now show that w?T (diag(p?) ? p?p?T )w? ex-
presses the variance of the weights in w? with re-
spect to the probability weighting p?. The variance
of a random variable with mean E[W ] = ? is:
Var[W ] = E[(W ? ?)2] = E[W 2] ? E[W ]2
The mean of the weights using probability weight-
ing p? is E[W ] = w?T p? = p?w?. Also, E[W 2] =
w?T diag(p?)w?. Thus:
Var[W ] = w?T diag(p?)w? ? (w?T p?)(p?w?)
= w?T (diag(p?) ? p?p?)w?
175
In our experiments, we deem each weight to be
equally important to the variance calculation, and
set pi = 1N ,?i = 1, . . . , N .
The goal of the regularization in (6) using C
from (7) can be regarded as directing the SVM to-
ward a good unsupervised system, regardless of
the constraints (training examples). In some un-
supervised systems, however, only a subset of the
attributes are used. In other cases, distinct subsets
of weights should have low variance, rather than
minimizing the variance across all weights. There
are examples of these situations in Section 4.
We can account for these cases in our QP. We
provide separate terms in our quadratic function
for the subsets of w? that should have low vari-
ance. Suppose we create L subsets of w?: ??1, ...??L,
where ??j is w? with elements set to zero that are not
in subset j. We then minimize 12(??T1 C1??1 + ... +
??TLCL??L). If the terms in subset j have low vari-
ance, Cj = C from (7) is used. If the subset corre-
sponds to attributes that are not a priori known to
be useful, an identity matrix can instead be used,
Cj = I, and these weights will be regularized to-
ward zero as in a standard SVM.4
Variance regularization therefore exploits extra
knowledge by the system designer. The designer
decides which weights should have similar values,
and the SVM is biased to prefer this solution.
One consequence of being able to regularize
different subsets of weights is that we can also ap-
ply variance regularization to the standard multi-
class SVM (Section 2.1). We can use an identity
Ci matrix for all irrelevant weights, i.e., weights
that correspond to class-attribute pairs where the
attribute is not directly relevant to the class. In our
experiments, however, we apply variance regular-
ization to the more efficient CS-SVM.
We refer to a CS-SVM trained using the variance
minimization quadratic program as the VAR-SVM.
2.3.1 Web-Scale N-gram VAR-SVM
If variance regularization is applied to all weights,
attributes COUNT(in Russia during), COUNT(Russia
during 1997), and COUNT(during 1997 to) will be
encouraged to have similar weights in the score for
class during. Furthermore, these will be weighted
similarly to other patterns, filled with other prepo-
sitions, used in the scores for other classes.
4Weights must appear in ?1 subsets (possibly only in the
Cj = I subset). Each occurs in at most one in our experi-
ments. Note it is straightforward to express this as a single
covariance matrix regularizer over w?; we omit the details.
Alternatively, we could minimize the variance
separately over all 5-gram patterns, then over all
4-gram patterns, etc., or over all patterns with a
filler in the same position. In our experiments, we
took a very simple approach: we minimized the
variance of all attributes that are weighted equally
in the unsupervised baselines. If a feature is not in-
cluded in a baseline, it is regularized toward zero.
3 Experimental Details
We use the data sets from Bergsma et al (2009).
These are the three tasks where web-scale N-gram
counts were previously used as features in a stan-
dard K-SVM. In each case a classifier makes a de-
cision for a particular word based on the word?s
surrounding context. The attributes of the classi-
fier are the log counts of different fillers occurring
in the context patterns. We retrieve counts from
the web-scale Google Web 5-gram Corpus (Brants
and Franz, 2006), which includes N-grams of
length one to five. We apply add-one smoothing
to all counts. Every classifier also has bias fea-
tures (for every class). We simply include, where
appropriate, attributes that are always unity.
We use LIBLINEAR (Fan et al, 2008) to train
K-SVM and OvA-SVM, and SVMrank (Joachims,
2006) to train CS-SVM. For VAR-SVM, we solve
the primal form of the quadratic program directly
in CPLEX (2005), a general optimization package.
We vary the number of training examples for
each classifier. The C-parameters of all SVMs are
tuned on development data. We evaluate using ac-
curacy: the percentage of test examples that are
classified correctly. We also provide the accuracy
of the majority-class baseline and best unsuper-
vised system, as defined in Bergsma et al (2009).
As an alternative way to increase the learning
rate, we augment a classifier?s features using the
output of the unsupervised system: For each class,
we include one feature for the sum of all counts (in
the unsupervised system) that predict that class.
We denote these augmented systems with a + as
in K-SVM+ and CS-SVM+.
4 Applications
4.1 Preposition Selection
Preposition errors are common among new En-
glish speakers (Chodorow et al, 2007). Systems
that can reliably identify these errors are needed
in word processing and educational software.
176
Training Examples
System 10 100 1K 10K 100K
OvA-SVM 16.0 50.6 66.1 71.1 73.5
K-SVM 13.7 50.0 65.8 72.0 74.7
K-SVM+ 22.2 56.8 70.5 73.7 75.2
CS-SVM 27.1 58.8 69.0 73.5 74.2
CS-SVM+ 39.6 64.8 71.5 74.0 74.4
VAR-SVM 73.8 74.2 74.7 74.9 74.9
Table 1: Accuracy (%) of preposition-selection
SVMs. Unsupervised accuracy is 73.7%.
In our experiments, a classifier must choose the
correct preposition among 34 candidates, using
counts for filled 2-to-5-gram patterns. We use
100K training, 10K development, and 10K test
examples. The unsupervised approach sums the
counts of all 3-to-5-gram patterns for each prepo-
sition. We therefore regularize the variance of the
3-to-5-gram weights in VAR-SVM, and simultane-
ously minimize the norm of the 2-gram weights.
4.1.1 Results
The majority-class is the preposition of; it occurs
in 20.3% of test examples. The unsupervised sys-
tem scores 73.7%. For further perspective on these
results, note Chodorow et al (2007) achieved 69%
with 7M training examples, while Tetreault and
Chodorow (2008) found the human performance
was around 75%. However, these results are not
directly comparable as they are on different data.
Table 1 gives the accuracy for different amounts
of training data. Here, as in the other tasks, K-SVM
mirrors the learning rate in Bergsma et al (2009).
There are several distinct phases among the rela-
tive ranking of the systems. For smaller amounts
of training data (?1000 examples) K-SVM per-
forms worst, while VAR-SVM is statistically sig-
nificantly better than all other systems, and al-
ways exceeds the performance of the unsupervised
approach.5 Augmenting the attributes with sum
counts (the + systems) strongly helps with fewer
examples, especially in conjunction with the more
efficient CS-SVM. However, VAR-SVM clearly
helps more. We noted earlier that VAR-SVM is
guaranteed to do as well as the unsupervised sys-
tem on the development data, but here we confirm
that it can also exploit even small amounts of train-
ing data to further improve accuracy.
CS-SVM outperforms K-SVM except with 100K
5Significance is calculated using a ?2 test over the test set
correct/incorrect contingency table.
Training Examples
System 10 100 1K 10K 100K
CS-SVM 86.0 93.5 95.1 95.7 95.7
CS-SVM+ 91.0 94.9 95.3 95.7 95.7
VAR-SVM 94.9 95.3 95.6 95.7 95.8
Table 2: Accuracy (%) of spell-correction SVMs.
Unsupervised accuracy is 94.8%.
examples, while OvA-SVM is better than K-SVM
for small amounts of data.6 K-SVM performs best
with all the data; it uses the most expressive repre-
sentation, but needs 100K examples to make use
of it. On the other hand, feature augmentation
and variance regularization provide diminishing
returns as the amount of training data increases.
4.2 Context-Sensitive Spelling Correction
Context-sensitive spelling correction, or real-word
error/malapropism detection (Golding and Roth,
1999; Hirst and Budanitsky, 2005), is the task of
identifying errors when a misspelling results in a
real word in the lexicon, e.g., using site when sight
or cite was intended. Contextual spell checkers are
among the most widely-used NLP technology, as
they are included in commercial word processing
software (Church et al, 2007).
For every occurrence of a word in a pre-defined
confusion set (e.g. {cite, sight, cite}), the clas-
sifier selects the most likely word from the set.
We use the five confusion sets from Bergsma et al
(2009); four are binary and one is a 3-way classi-
fication. We use 100K training, 10K development,
and 10K test examples for each, and average ac-
curacy across the sets. All 2-to-5 gram counts are
used in the unsupervised system, so the variance
of all weights is regularized in VAR-SVM.
4.2.1 Results
On this task, the majority-class baseline is much
higher, 66.9%, and so is the accuracy of the top un-
supervised system: 94.8%. Since four of the five
sets are binary classifications, where K-SVM and
CS-SVM are equivalent, we only give the accuracy
of the CS-SVM (it does perform better on the one
3-way set). VAR-SVM again exceeds the unsuper-
vised accuracy for all training sizes, and generally
6Rifkin and Klautau (2004) argue OvA-SVM is as good
as K-SVM, but this is ?predicated on the assumption that the
classes are ?independent?,? i.e., that examples from class 0
are no closer to class 1 than to class 2. This is not true of this
task (e.g. x?before is closer to x?after than x?in, etc.).
177
Training Examples
System 10 100 1K
CS-SVM 59.0 71.0 84.3
CS-SVM+ 59.4 74.9 84.5
VAR-SVM 70.2 76.2 84.5
VAR-SVM+FreeB 64.2 80.3 84.5
Table 3: Accuracy (%) of non-referential detection
SVMs. Unsupervised accuracy is 80.1%.
performs as well as the augmented CS-SVM+ us-
ing an order of magnitude less training data (Ta-
ble 2). Differences from ?1K are significant.
4.3 Non-Referential Pronoun Detection
Non-referential detection predicts whether the En-
glish pronoun it refers to a preceding noun (?it
lost money?) or is used as a grammatical place-
holder (?it is important to...?). This binary clas-
sification is a necessary but often neglected step
for noun phrase coreference resolution (Paice and
Husk, 1987; Bergsma et al, 2008; Ng, 2009).
Bergsma et al (2008) use features for the counts
of various fillers in the pronoun?s context patterns.
If it is the most common filler, the pronoun is
likely non-referential. If other fillers are common
(like they or he), it is likely a referential instance.
For example, ?he lost money? is common on the
web, but ?he is important to? is not. We use the
same fillers as in previous work, and preprocess
the N-gram corpus in the same way.
The unsupervised system picks non-referential
if the difference between the summed count of
it fillers and the summed count of they fillers is
above a threshold (note this no longer fits (5),
with consequences discussed below). We thus
separately minimize the variance of the it pattern
weights and the they pattern weights. We use 1K
training, 533 development, and 534 test examples.
4.3.1 Results
The most common class is referential, occurring
in 59.4% of test examples. The unsupervised sys-
tem again does much better, at 80.1%.
Annotated training examples are much harder
to obtain for this task and we experiment with a
smaller range of training sizes (Table 3). The per-
formance of VAR-SVM exceeds the performance
of K-SVM across all training sizes (bold accura-
cies are significantly better than either CS-SVM for
?100 examples). However, the gains were not
as large as we had hoped, and accuracy remains
worse than the unsupervised system when not us-
ing all the training data. When using all the data,
a fairly large C-parameter performs best on devel-
opment data, so regularization plays less of a role.
After development experiments, we speculated
that the poor performance relative to the unsuper-
vised approach was related to class bias. In the
other tasks, the unsupervised system chooses the
highest summed score. Here, the difference in it
and they counts is compared to a threshold. Since
the bias feature is regularized toward zero, then,
unlike the other tasks, using a low C-parameter
does not produce the unsupervised system, so per-
formance can begin below the unsupervised level.
Since we wanted the system to learn this thresh-
old, even when highly regularized, we removed
the regularization penalty from the bias weight,
letting the optimization freely set the weight to
minimize training error. With more freedom, the
new classifier (VAR-SVM+FreeB) performs worse
with 10 examples, but exceeds the unsupervised
approach with 100 training points. Although
this was somewhat successful, developing better
strategies for bias remains useful future work.
5 Related Work
There is a large body of work on regularization in
machine learning, including work that uses posi-
tive semi-definite matrices in the SVM quadratic
program. The graph Laplacian has been used to
encourage geometrically-similar feature vectors to
be classified similarly (Belkin et al, 2006). An ap-
pealing property of these approaches is that they
incorporate information from unlabeled examples.
Wang et al (2006) use Laplacian regularization
for the task of dependency parsing. They regular-
ize such that features for distributionally-similar
words have similar weights. Rather than penal-
ize pairwise differences proportional to a similar-
ity function, we simply penalize weight variance.
In the field of computer vision, Tefas et al
(2001) (binary) and Kotsia et al (2009) (multi-
class) also regularize weights with respect to a co-
variance matrix. They use labeled data to find the
sum of the sample covariance matrices from each
class, similar to linear discriminant analysis. We
propose the idea in general, and instantiate with
a different C matrix: a variance regularizer over
w?. Most importantly, our instantiated covariance
matrix does not require labeled data to generate.
In a Bayesian setting, Raina et al (2006) model
178
feature correlations in a logistic regression clas-
sifier. They propose a method to construct a co-
variance matrix for a multivariate Gaussian prior
on the classifier?s weights. Labeled data for other,
related tasks is used to infer potentially correlated
features on the target task. Like in our results, they
found that the gains from modeling dependencies
diminish as more training data is available.
We also mention two related online learning ap-
proaches. Similar to our goal of regularizing to-
ward a good unsupervised system, Crammer et al
(2006) regularize w? toward a (different) target vec-
tor at each update, rather than strictly minimizing
||w?||2. The target vector is the vector learned from
the cumulative effect of previous updates. Dredze
et al (2008) maintain the variance of each weight
and use this to guide the online updates. However,
covariance between weights is not considered.
We believe new SVM regularizations in gen-
eral, and variance regularization in particular, will
increasingly be used in combination with related
NLP strategies that learn better when labeled data
is scarce. These may include: using more-general
features, e.g. ones generated from raw text (Miller
et al, 2004; Koo et al, 2008), leveraging out-of-
domain examples to improve in-domain classifi-
cation (Blitzer et al, 2007; Daume? III, 2007), ac-
tive learning (Cohn et al, 1994; Tong and Koller,
2002), and approaches that treat unlabeled data as
labeled, such as bootstrapping (Yarowsky, 1995),
co-training (Blum and Mitchell, 1998), and self-
training (McClosky et al, 2006).
6 Future Work
The primary direction of future research will be
to apply the VAR-SVM to new problems and tasks.
There are many situations where a system designer
has an intuition about the role a feature will play in
prediction; the feature was perhaps added with this
role in mind. By biasing the SVM to use features
as intended, VAR-SVM may learn better with fewer
training examples. The relationship between at-
tributes and classes may be explicit when, e.g.,
a rule-based system is optimized via discrimina-
tive learning, or annotators justify their decisions
by indicating the relevant attributes (Zaidan et al,
2007). Also, if features are a priori thought to
have different predictive worth, the attribute val-
ues could be scaled such that variance regulariza-
tion, as we formulated it, has the desired effect.
Other avenues of future work will be to extend
the VAR-SVM in three directions: efficiency, rep-
resentational power, and problem domain.
While we optimized the VAR-SVM objective in
CPLEX, general purpose QP-solvers ?do not ex-
ploit the special structure of [the SVM optimiza-
tion] problem,? and consequently often train in
time super-linear with the number of training ex-
amples (Joachims et al, 2009). It would be useful
to fit our optimization problem to efficient SVM
training methods, especially for linear classifiers.
VAR-SVM?s representational power could be ex-
tended by using non-linear SVMs. Kernels can
be used with a covariance regularizer (Kotsia et
al., 2009). Since C is positive semi-definite, the
square root of its inverse is defined. We can there-
fore map the input examples using (C? 12 x?), and
write an equivalent objective function in terms of
kernel functions over the transformed examples.
Also, since structured-prediction SVMs build
on the multi-class framework (Tsochantaridis et
al., 2005), variance regularization can be incor-
porated naturally into more complex prediction
tasks, such as parsers, taggers, and aligners.
VAR-SVM may also help in new domains where
annotated data is lacking. VAR-SVM should be
stronger cross-domain than K-SVM; regulariza-
tion with domain-neutral prior-knowledge can off-
set domain-specific biases. Learned weight vec-
tors from other domains may also provide cross-
domain regularization guidance.
7 Conclusion
We presented variance-regularization SVMs, an
approach to learning that creates better classi-
fiers using fewer training examples. Variance reg-
ularization incorporates a bias for known good
weights into the SVM?s quadratic program. The
VAR-SVM can therefore exploit extra knowledge
by the system designer. Since the objective re-
mains a convex quadratic function of the weights,
the program is computationally no harder to opti-
mize than a standard SVM. We also demonstrated
how to design multi-class SVMs using only class-
specific attributes, and compared the performance
of this approach to standard multi-class SVMs on
the task of preposition selection.
While variance regularization is most helpful on
tasks with many classes and features, like prepo-
sition selection, it achieved gains on all our tasks
when training with smaller sample sizes. It should
be useful on a variety of other NLP problems.
179
References
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled exam-
ples. JMLR, 7:2399?2434.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Distributional identification of non-referential
pronouns. In ACL-08: HLT.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2009. Web-scale N-gram models for lexical disam-
biguation. In IJCAI.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram Corpus Version 1.1. LDC2006T13.
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In ACL-SIGSEM Workshop on Prepo-
sitions.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with Golomb
coding. In EMNLP-CoNLL.
David Cohn, Les Atlas, and Richard Ladner. 1994. Im-
proving generalization with active learning. Mach.
Learn., 15(2):201?221.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Mach. Learn., 20(3):273?297.
CPLEX. 2005. IBM ILOG CPLEX 9.1. www.ilog.
com/products/cplex/.
Koby Crammer and Yoram Singer. 2001. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. JMLR, 2:265?292.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
JMLR, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JMLR, 7:551?585.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML.
Richard O. Duda and Peter E. Hart. 1973. Pattern
Classification and Scene Analysis. John Wiley &
Sons.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. JMLR,
9:1871?1874.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
based approach to context-sensitive spelling correc-
tion. Mach. Learn., 34(1-3):107?130.
Sariel Har-Peled, Dan Roth, and Dav Zimak. 2003.
Constraint classification for multiclass classification
and ranking. In NIPS.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. Nat. Lang. Eng., 11(1):87?111.
Chih-Wei Hsu and Chih-Jen Lin. 2002. A comparison
of methods for multiclass support vector machines.
IEEE Trans. Neur. Networks, 13(2):415?425.
Thorsten Joachims, Thomas Finley, and Chun-
Nam John Yu. 2009. Cutting-plane training of
structural SVMs. Mach. Learn., 77(1):27?59.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In KDD.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL-08: HLT.
Irene Kotsia, Stefanos Zafeiriou, and Ioannis Pitas.
2009. Novel multiclass classifiers based on the min-
imization of the within-class variance. IEEE Trans.
Neur. Networks, 20(1):14?34.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Trans. Speech and Language Processing, 2(1):1?31.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL.
Andrew Y. Ng and Michael I. Jordan. 2002. Discrim-
inative vs. generative classifiers: A comparison of
logistic regression and naive bayes. In NIPS.
Vincent Ng. 2009. Graph-cut-based anaphoricity de-
termination for coreference resolution. In NAACL-
HLT.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In ACL.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A dis-
criminative language model with pseudo-negative
samples. In ACL.
180
Chris D. Paice and Gareth D. Husk. 1987. Towards the
automatic recognition of anaphoric features in En-
glish text: the impersonal pronoun ?it?. Computer
Speech and Language, 2:109?132.
Rajat Raina, Andrew Y. Ng, and Daphne Koller. 2006.
Constructing informative priors using transfer learn-
ing. In ICML.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense
of one-vs-all classification. JMLR, 5:101?141.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In ACL.
Anastasios Tefas, Constantine Kotropoulos, and Ioan-
nis Pitas. 2001. Using support vector machines to
enhance the performance of elastic graph matching
for frontal face authentication. IEEE Trans. Pattern
Anal. Machine Intell., 23:735?746.
Joel R. Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in ESL
writing. In COLING.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. JMLR, 2:45?66.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. JMLR, 6:1453?1484.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. John Wiley & Sons.
Qin Iris Wang, Colin Cherry, Dan Lizotte, and Dale
Schuurmans. 2006. Improved large margin depen-
dency parsing via local constraints and Laplacian
regularization. In CoNLL.
Jason Weston and Chris Watkins. 1998. Multi-class
support vector machines. Technical Report CSD-
TR-98-04, Department of Computer Science, Royal
Holloway, University of London.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL.
Omar Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ?annotator rationales? to improve ma-
chine learning for text categorization. In NAACL-
HLT.
181
