Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2280?2290, Dublin, Ireland, August 23-29 2014.
A Probabilistic Co-Bootstrapping Method for Entity Set Expansion 
 
 
Bei Shi,    Zhengzhong Zhang Le Sun,    Xianpei Han 
Institute of Software, 
 Chinese Academy of Sciences, 
 Beijing, China 
State Key Laboratory of Computer Science, 
Institute of Software,  
Chinese Academy of Sciences,  
Beijing, China 
{shibei, zhenzhong, sunle, xianpei}@nfs.iscas.ac.cn 
 
 
Abstract 
Entity Set Expansion (ESE) aims at automatically acquiring instances of a specific target category. 
Unfortunately, traditional ESE methods usually have the expansion boundary problem and the semantic 
drift problem. To resolve the above two problems, this paper proposes a probabilistic Co-Bootstrapping 
method, which can accurately determine the expansion boundary using both the positive and the 
discriminant negative instances, and resolve the semantic drift problem by effectively maintaining and 
refining the expansion boundary during bootstrapping iterations. Experimental results show that our 
method can achieve a competitive performance. 
1 Introduction 
Entity Set Expansion (ESE) aims at automatically acquiring instances of a specific target category 
from text corpus or Web. For example, given the capital seeds {Rome, Beijing, Paris}, an ESE system 
should extract all other capitals from Web, such as Ottawa, Moscow and London. ESE system has 
been used in many applications, e.g., dictionary construction (Cohen and Sarawagi, 2004), word sense 
disambiguation (Pantel and Lin, 2002), query refinement (Hu et al., 2009), and query suggestion (Cao 
et al., 2008). 
Due to the limited supervision provided by ESE (in most cases only 3-5 seeds are given), traditional 
ESE systems usually employ bootstrapping methods (Cucchiarelli and Velardi, 2001; Etzioni et al., 
2005; Pasca, 2007; Riloff and Jones, 1999; Wang and Cohen, 2008). That is, the entity set is 
iteratively expanded through a pattern generation step and an instance extraction step. Figure 1(a) 
demonstrates a simple bootstrapping process.? 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                                                        
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
Rome 
Beijing 
Paris 
Milan 
Tokyo 
Shanghai 
London 
* is the city of 
 
at the embassy in * 
* is the capital of 
at the hotel in * 
Chicago 
Berlin 
Pattern Generation Instance Extraction 
Rome 
Beijing 
Paris 
Milan 
Tokyo 
Shanghai 
London 
Sydney 
Boston 
* is the city of 
 
at the embassy in * 
 
* is the capital of 
 
to cities such as * 
 
at the hotel in * 
Chicago 
Nokia * the official web site 
 
New York 
Pattern Generation Instance Co-Extraction 
Negative Positive 
 
(a) (b) 
Figure 1: A demo of Bootstrapping (a) and Co-Bootstrapping (b) 
2280
However, the traditional bootstrapping methods have two main drawbacks:  
1) The expansion boundary problem. That is, using only positive seeds (i.e., some example 
entities from the category we want to expand), it is difficult to represent which entities we want to 
expand and which we don?t want. For example, starting from positive seeds {Rome, Beijing, Paris}, 
we can expand entities at many different levels, e.g., all capitals, all cities, or even all locations. And 
all these explanations are reasonable.  
2) The semantic drift problem. That is, the expansion category may change gradually when noisy 
instances/patterns are introduced during the bootstrapping iterations. For example, in Figure 1 (a), the 
instance Rome will introduce a pattern ?* is the city of?, which will introduce many noisy city 
instances such as Milan and Chicago for the expansion of Capital. And these noisy cities in turn will 
introduce more city patterns and instances, and finally will lead to a semantic drift from Capital to 
City. 
In recent years, some methods (Curran et al, 2007; Pennacchiotti and Pantel, 2011) have exploited 
mutual exclusion constraint to resolve the semantic drift problem. These methods expand multiple 
categories simultaneously, and will determine the expansion boundary based on the mutually 
exclusive property of the pre-given categories. For instance, the exclusive categories Fruit and 
Company will be jointly expanded and the expansion boundary of {Apple, Banana, Cherry} will be 
limited by the expansion boundary of {Google, Microsoft, Apple Inc.}. These methods, however, still 
have the following two drawbacks: 
1) These methods require that the expanded categories should be mutually exclusive. However, in 
many cases the mutually exclusive assumption does not hold. For example, many categories hold a 
hyponymy relation (e.g., the categories City and Capital, because the patterns for Capital are also the 
patterns for City) or a high semantic overlap (e.g., the categories Movies and Novels, because some 
movies are directly based on the novels of the same title.). 
2) These methods require the manually determination of the mutually exclusive categories. 
Unfortunately, it is often very hard for even the experts to determine the categories which can define 
the expansion boundaries for each other. For example, in order to expand the category Chemical 
Element, it is difficult to predict its semantic drift towards Color caused by the ambiguous instances 
{Silver, Gold}. 
In this paper, to resolve the above problems, we propose a probabilistic Co-Bootstrapping method. 
The first advantage of our method is that we propose a method to better define the expansion boundary 
using both the positive and the discriminant negative seeds, which can both be automatically populated 
during the bootstrapping process. For instance, in Figure 1(b), in order to expand Capital, the 
Co-Bootstrapping algorithm will populate both positive instances from the positive seeds {Rome, 
Beijing, Paris}, and negative instances from the negative seeds {Boston, Sydney, New York}. In this 
way the expansion boundary of Capital can be accurately determined. 
The second advantage of our method is that we can maintain and refine the expansion boundary 
during bootstrapping iterations, so that the semantic drift problem can be effectively resolved. 
Specifically, we propose an effective scoring algorithm to estimate the probability that an extracted 
instance belongs to the target category. Based on this scoring algorithm, this paper can effectively 
select positive instances and discriminant negative instances. Therefore the expansion boundary can be 
maintained and refined through the above jointly expansion process. 
We have evaluated our method on the expansion of thirteen categories of entities. The experimental 
results show that our method can achieve 6%~15% P@200 performance improvement over the 
baseline methods. 
This paper is organized as follows. Section 2 briefly reviews related work. Section 3 defines the 
problem and proposes a probabilistic Co-Bootstrapping approach. Experiments are presented in 
Section 4. Finally, we conclude this paper and discuss some future work in Section 5. 
2 Related Work 
In recent years, ESE has received considerable attentions from both research (An et al., 2003; 
Cafarella et al., 2005; Pantel and Ravichandran, 2004; Pantel et al., 2009; Pasca, 2007; Wang and 
Cohen, 2008) and industry communities (e.g., Google Sets). Till now, most ESE systems employ 
bootstrapping methods, such as DIPRE (Brin, 1998), Snowball (Agichtein and Gravano, 2000), etc. 
2281
The main drawbacks of the traditional bootstrapping methods are the expansion boundary problem 
and the semantic drift problem. Currently, two strategies have been exploited to resolve the semantic 
drift problem. The first is the ranking based approaches (Pantel and Pennacchiotti, 2006; Talukdar et 
al., 2008), which select highly confident patterns and instances through a ranking algorithm, with the 
assumption that high-ranked instances will be more likely to be the instances of the target category. 
The second is the mutual exclusion constraint based methods (Curran et al., 2007; McIntosh and 
Curran, 2008; Pennacchiotti and Pantel, 2011; Thelen and Riloff, 2002; Yangarber et al., 2002), which 
expand multiple categories simultaneously and determine the expansion boundary based on the 
mutually exclusive property of the pre-given categories. 
3 The Co-Bootstrapping Method 
3.1 The Framework of Probabilistic Co-Bootstrapping 
Given the initial positive seeds and negative seeds, the goal of our method is to extract instances of a 
specific target semantic category. For demonstration, we will describe our method through the running 
example shown in Figure 1(b). 
Specifically, Figure 2 shows the framework of our method. The central tasks of our 
Co-Bootstrapping method are as follows: 
 
Figure 2: The framework of probabilistic Co-Bootstrapping 
1) Pattern Generation and Evaluation. This step generates and evaluates patterns using the 
statistics of the positive and the negative instances. Specifically, we propose three measures of pattern 
quality: the Generality (GE), the Precision of Extracted Instances (PE) and the Precision of Not 
Extracted Instances (PNE). 
2) Instance Co-Extraction. This step co-extracts the positive and the negative instances using 
highly confident patterns. Specifically, we propose an effective scoring algorithm to estimate the 
probability that an extracted instance belongs to the target category based on the statistics and the 
quality of the patterns which extract it. 
3) Seed Selection. This step selects the high ranked positive instances and discriminant negative 
instances to refine the expansion boundary by measuring how well a new instance can be used to 
define the expansion boundary. 
The above three steps will iterate until the number of extracted entities reaches a predefined 
threshold. We describe these steps as follows. 
3.2 Pattern Generation and Evaluation 
In this section, we describe the pattern generation and evaluation step. In this paper, each pattern is a 
4-grams lexical context of an entity. We use the Google Web 1T corpus?s (Brants and Franz, 2006) 
5-grams for both the pattern generation and the instance co-extraction in ESE. Our method generates 
patterns through two steps: 1) Generate candidate patterns by matching seeds with the 5-grams. 2) 
Evaluate the quality of the patterns. 
For the first step, we simply match each seed instance with all 5-grams, then we replace the 
matching instance with wildcard ?*? to generate the pattern. 
Extracted Positive (ep) London 
Extracted Negative (en) Shanghai, Milan 
Not Extracted Positive (nep) Tokyo 
Not Extracted Negative (nen) Chicago, Nokia 
 
Table 1: (a) shows the four classes of instances according to polarity and extraction. (b) shows the four 
classes of the instances given ?to cities such as *? 
Count Positive Negative 
Extracted Extracted Positive (ep) Extracted Negative (en) 
Not Extracted 
Not Extracted and Positive 
(nep) 
Not Extracted and Negative 
(nen) 
Pattern Generation and Evaluation 
Initial   
Seeds 
Pattern 
Positive Instance 
Discriminant Negative Instance    
Positive Instance  
Negative Instance 
Instance Co-Extraction 
Seeds Evaluation and Selection 
(a) (b) 
2282
For the second step, we propose three measures to evaluate the quality of a pattern, correspondingly 
the Generality (GE), the Precision of Extracted Instances (PE), and the Precision of Not Extracted 
Instances (PNE). Specifically, given a pattern, we observed that all instances can be categorized into 
four classes, according to whether they belong to the target category and whether they can be extracted 
by the pattern (shown in Table 1(a)). For example, given the pattern ?to cities such as *? in Figure 
1(b), the instances under its four classes are shown in Table 1 (b). 
The proposed three measures of the quality of a pattern can be computed as follows (In most cases, 
we cannot get the accurate number of ep, en, nep and nen. So this paper uses the corresponding known 
instances in the previous iteration to approximately compute ep, en, nep and nen): 
1) Generality (GE). The Generality of a pattern measures how many entities can be extracted by it. 
A more general pattern will cover more entities than a more specific pattern. Specifically, the GE of a 
pattern is computed as: 
 
That is, the proportion of the instances which can be extracted by the pattern in the previous iteration. 
2) Precision of Extracted Instances (PE). The PE measures how likely an instance extracted by a 
pattern will be positive. That is, a pattern with higher PE will be more likely to extract positive 
instances than a lower PE pattern. The PE is computed as: 
 
That is, the proportion of positive instances within all instances which can be extracted by the 
pattern in the previous iteration. 
3) Precision of Not Extracted Instances (PNE). The PNE measures how likely a not extracted 
instance is positive. Instances not extracted by a high PNE pattern will be more likely to be positive. 
PNE is computed as: 
 
Because the number of negative instances is usually much larger than the number of positive 
instances, we normalize the number of positive and negative instances in the formula. 
Table 2 shows these measures of some selected patterns evaluated using the Google Web 1T corpus. 
We can see that the above measures can effectively evaluate the quality of patterns. For instance, 
GE(?* is the city of?)=0.566 is larger than GE(?at the embassy in *?)=0.340, which is consistent with 
our intuition that the pattern ?* is the city of? is more general than ?at the embassy in *?. PE(?* is the 
capital of?)=0.928 is larger than PE(?* is the city of?)=0.269, which is consistent with our intuition 
that the instances extracted by ?* is the capital of? are more likely Capital than by?* is the city of?. 
 GE PE PNE 
at the embassy in * 0.340 0.833 0.312 
* is the capital of 0.321 0.928 0.224 
to cities such as * 0.426 0.875 0.566 
at the hotel in * 0.333 0.192 0.571 
* is the city of 0.566 0.269 0.592 
* the official web site 0.218 0.230 0.607 
Table 2: The GE, PE and PNE of some selected patterns 
3.3 Instance Co-Extraction 
In this section, we describe how to co-extract positive instances and discriminant negative instances. 
Given the generated patterns, the central task of this step is to measure the likelihood of an instance to 
be positive. The higher the likelihood, the more likely the instance belongs to the target category. To 
resolve the task, we propose a probabilistic method which predicts the probability of an instance to be 
positive, i.e., the Instance Positive Probability and we denote it as P+. Generally, the P+ is determined 
by both the statistics and the quality of patterns. We start with the observation that: 
2283
1) If an instance is extracted by a pattern with a high PE, the instance will have a high P+. 
2) If an instance is not extracted by a high PNE pattern, the instance will have a high P+. 
3) If an instance is extracted by many patterns with high PE and not extracted by many patterns 
with high PNE, the instance will have a high P+, and vice versa. 
Based on the above observations, the computation of P+ is as follows: 
The Situation of One Pattern 
For the situation that only one pattern exists, the P+ of an instance can be simply computed as: 
 
where e denotes an extracted instance and p denotes a pattern which extracts e. This formula means 
that if the instance is extracted by a pattern, the P+ is determined by the PE of the pattern. For 
example, in Figure 3 (a), the instance Tokyo is only extracted by the pattern ?at the embassy in *? and 
the P+ is determined by the PE of ?at the embassy in *?, i.e., P+(Tokyo)=PE(?at the embassy in *?). 
The above formula also means when the instance cannot be extracted by the only pattern, the P+ 
will be determined by the PNE of the pattern. For example, in Figure 3 (b), the instance Tokyo is not 
extracted by the only pattern ?at the hotel in *? and the P+ is only determined by the PNE of ?at the 
hotel in *?, that is, P+(Tokyo)=PNE(?at the hotel in *?). 
 
 
 
 
Figure 3: (a) Tokyo is extracted by ?at the embassy in *?. (b) Tokyo is not extracted by ?at the hotel 
in *?. (c) London is extracted by ?at the embassy in *? and not extracted by ?to cities such as *?. 
The Situation of Multiple Patterns 
In this section, we describe how to compute P+ in the situation of multiple patterns. Specifically, we 
assume that an instance is extracted by different patterns independently. Therefore, given all the 
pattern-instance relations (i.e., whether a specific pattern extracts a specific instance), the likelihood 
for an instance e being positive is computed as: 
 
where R+ is all the patterns which extract e, and R- is all the patterns which do not extract e. I+ is the 
set of all positive instances.  is the probability of the event ?pattern p extracts 
instance e and e is positive?. Using Bayes rule, this probability can be computed as: 
 
where  is the probability of the event ?p extracts an instance e?, its value is GE(p); 
 is the conditional probability that e is positive under the condition ?p extracts e?, 
and its value is PE(p). Finally  is computed as: 
 
 is the probability of the event ?p does not extract e and e is positive?, which can 
be computed as: 
 
 is the probability of p not extracting an instance e, and its value is 1-GE(p). 
 is the conditional probability that e is positive under the condition ?p does not 
extract e?, and its value is PNE(p). Then  is finally computed as: 
 
Tokyo at the embassy in * Tokyo  at the hotel in * London 
at the embassy in * 
to cities such as * 
(a) (b) (c) 
2284
For example, in Figure 3 (c), the instance London is extracted by the pattern ?at the embassy in *? 
and not extracted by the pattern ?to cities such as *?. In this situation, PosLikelihood(London)= 
[GE(?at the embassy in *?) ? PE(?at the embassy in?)] ? [(1-GE(?to cities such as *?)) ? PNE(?to 
cities such as *?)]. 
Using the same intuition and the same method, the likelihood of an instance being negative is 
computed as: 
 
where  is the probability of the event ?p extracts e and e is negative?, which is 
computed as: 
 
 is the probability of the event ?p does not extract e and e is negative?, which is 
computed as: 
 
For instance, in Figure 3 (c), NegLikelihood(London) = [GE(?at the embassy in *?) ? (1-PE(?at the 
embassy in?))] ? [(1-GE(?to cities such as *?)) ? (1-PNE(?to cities such as *?))]. 
Finally, the Instance Positive Probability, P+, is computed as:  
 
3.4 Seed Selection 
In this section, we describe how to select positive and discriminant negative instances at each iteration. 
To determine whether an instance is positive, we use a threshold of P+ to determine the polarity of 
instances, which can be empirically estimated from data. The instances which have much higher P+ 
than the threshold will be added to the set of positive instances. For example, London and Tokyo in 
Figure 1 (b) are selected as positive instances. 
To select discriminant negative instances, we observed that not all negative instances are the same 
useful for the expansion boundary determination. Intuitively, the discriminant negative instances are 
those negative instances which are highly overlapped with the positive instances. For instance, due to 
the lower overlap between categories Fruit and Capital, Apple is not a discriminant negative instance 
since it provides little information for the expansion boundary determination. Therefore, the instances 
near the threshold are used as the discriminant negative instances in the next iteration. (Notice that, the 
computation of GE, PE and PNE still uses all positive and negative instances, rather than only 
discriminant negative instances). For example, in Figure 1(b), Shanghai, Milan and Chicago are 
selected as discriminate negative instances, and Nokia will be neglected. Finally the boundary between 
Capital and City can be determined by the positive instances and the discriminant negative instances. 
4 Experiments 
4.1 Experimental Settings 
Category Description Category Description 
CAP Place: capital name FAC Facilities: names of man-made structures 
ELE chemical element ORG Organization: e.g. companies, governmental 
FEM Person: female first name GPE Place: Geo-political entities 
MALE Person: male first name LOC Locations other than GPEs 
LAST Person: last name DAT Reference to a date or period 
TTL Honorific title LANG Any named language 
NORP Nationality, Religion, Political(adjectival)   
Table 3: Target categories 
Corpus: In our experiments, we used the Google Web 1T corpus (Brants and Franz, 2006) as our 
expansion corpus. Specifically, we use the open source package LIT-Indexer (Ceylan and Mihalcea, 
2011) to support efficient wildcard querying for pattern generation and instance extraction. 
2285
Target Expansion Categories: We conduct our experiments on thirteen categories, which are shown 
in Table 3. Eleven of them are from Curran et al. (2007). Besides the eleven categories, to evaluate 
how well ESE systems can resolve the semantic drift problem, we use two additional categories 
(Capital and Chemical Element) which are high likely to drift into other categories. 
Evaluation Criteria: Following Curran et al (2007), we use precision at top n (P@N) as the 
performance metrics, i.e., the percentage of correct entities in the top n ranked entities for a given 
category. In our experiments, we use P@10, P@20, P@50, P@100 and P@200. Since the output is a 
ranked list of extracted entities, we also choose the average precision (AP) as the evaluation metric. In 
our experiments, the correctness of all extracted entities is manually judged. In our experiments, we 
present results to 3 annotators, and an instance will be considered as positive if 2 annotators label it as 
positive. We also provide annotators some supporting resources for better evaluation, e.g., the entity 
list of target type collected from Wikipedia. 
4.2 Experimental Results 
In this section, we analyze the effect of negative instances, categories boundaries, and seed selection 
strategies. We compare our method with the following two baseline methods: i) Only_Pos (POS): 
This is an entity set expansion system which uses only positive seeds. ii) Mutual_Exclusion (ME): 
This is a mutual exclusion bootstrapping based ESE method, whose expansion boundary is determined 
by the exclusion of the categories. 
We implement our method using two different settings: i) Hum_Co-Bootstrapping (Hum_CB): 
This is the proposed Co-Bootstrapping method in which the initial negative seeds are manually given. 
Specifically, we randomly select five positive seeds from the list of the category?s instances while the 
initial negative seeds are manually provided. ii) Feedback_Co-Bootstrapping (FB_CB): This is our 
proposed probabilistic Co-Bootstrapping method with two steps of selecting initial negative seeds:   
1) Expand the entity set using only the positive seeds for only first iteration. Return the top ten 
instances. 2) Select the negative instances in the top ten results of the first iteration as negative seeds. 
4.2.1. Overall Performance 
Several papers have shown that the experimental performance may vary with different seed choices 
(Kozareva and Hovy, 2010; McIntosh and Curran, 2009; Vvas et al., 2009). Therefore, we input the 
ESE system with five different positive seed settings for each category. Finally we average the 
performance on the five settings so that the impact of seed selection can be reduced. 
 P@10 P@20 P@50 P@100 P@200 MAP 
POS 0.84 0.74 0.55 0.41 0.34 0.42 
ME 0.83(0.90) 0.79(0.87) 0.68(0.78) 0.58(0.67) 0.51(0.59) - 
Hum_CB 0.97 0.95 0.83 0.71 0.57 0.78 
FB_CB 0.97 0.96 0.90 0.79 0.66 0.85 
Table 4: The overall experimental results 
Table 4 shows the overall experimental results. The results in parentheses are the known results of 
eleven categories (without CAP and ELE) shown in (Curran et al., 2007). MAP of ME is missed 
because there are no available results in (Curran et al., 2007). From Table 4, we can see that: 
1) Our method can achieve a significant performance improvement: Compared with the 
baseline POS, our method Hum_CB and FB_CB can respectively achieve a 23% and 32% 
improvement on P@200; Compared with the baseline ME, our method Hum_CB and FB_CB can 
respectively improve P@200 by 6% and 15%. 
2) By explicitly representing the expansion boundary, the expansion performance can be 
increased: Compared with the baseline POS, ME can achieve a 17% improvement on P@200, and our 
method Hum_CB can achieve a 23% improvement on P@200. 
3) The negative seeds can better determine the expansion boundary than mutually exclusive 
categories. Compared with ME, Hum_CB and FB_CB can respectively achieve a 6% and 15% 
improvement on P@200. We believe this is because using negative instances is a more accurate and 
more robust way for defining and maintaining the expansion boundary than mutually exclusive 
categories. 
2286
4) The system?s feedback is useful for selecting negative instances: Compared with Hum_CB, 
FB_CB method can significantly improve the P@200 by 9.0%. We believe this is because that the 
system?s feedback is a good indicator of the semantic drift direction. In contrast, it is usually difficult 
for human to determine which directions the bootstrapping will drift towards. 
4.2.2. Detailed Analysis: Expansion Boundary 
In Table 5, we show the top 20 positive and negative Capital instances (FB_CB setting). From Table 5, 
we can make the following observations: 1) Our method can effectively generate negative instances. 
In Table 5, the negative instances contain cities, states, countries and general terms, all of which have 
a high semantic overlap with Capital category. 2) The positive instances and negative instances 
generated by our Co-Bootstrapping method can discriminately determine the expansion boundary. For 
instance, the negative instances Kyoto can distinguish Capital from City; Australia and China can 
distinguish Capital from Country; 
Positive Instances 
London,  Paris,  Moscow,  Beijing,  Madrid,  Amsterdam,  Washington,  Tokyo,  Berlin,  Rome,  
Vienna,  Baghdad,  Athens,  Bangkok,  Cairo,  Dublin,  Brussels,  Prague,  San,  Budapest 
Negative Instances 
(with categories)  
City Kyoto,  Kong,  Newcastle,  Zurich,  Lincoln,  Albany,  Lyon,  LA,  Shanghai 
Country China,  Australia 
General downtown,  April 
State Hawaii,  Oklahoma,  Manhattan 
Other Hollywood,  DC,  Tehran,  Charlotte 
Table 5: Top 20 positive instances and negative instances (True positive instances are in bold) 
4.2.3. Detailed Analysis: Semantic Drift Problem 
POS 
Stockholm,  Tampa,  East,  West,  Springfield,  Newport, Cincinnati,  Dublin,  Chattanooga,  Savannah,  
Omaha,  Cambridge,  Memphis,  Providence,  Panama,  Miami,  Cape,  Victoria,  Milan,  Berlin 
ME 
London,  Prague,  Newport,  Cape,  Dublin,  Savannah,  Chattanooga,  Beijing,  Memphis,  Athens,  
Berlin,  Miami,  Plymouth,  Victoria,  Omaha,  Tokyo,  Portland,  Troy,  Anchorage,  Bangkok 
Hum_CB 
London,  Rome,  Berlin,  Paris,  Athens,  Moscow,  Tokyo,  Beijing,  Prague,  Madrid,  Vienna,  
Dublin,  Budapest,  Amsterdam,  Bangkok,  Brussels,  Sydney,  Cairo,  Washington,  Barcelona 
FB_CB 
London,  Paris,  Moscow,  Beijing,  Washington,  Tokyo,  Berlin,  Rome,  Vienna,  Baghdad,  
Athens,  Bangkok,  Cairo,  Brussels,  Prague,  San,  Budapest,  Amsterdam,  Dublin,  Madrid 
Table 6: Top 20 instances of all methods (True positive instances are in bold) 
To analyze how our method can resolve the semantic drift problem, Table 6 shows the top 20 positive 
Capital instances of different methods. From Table 6, we can make the following observations: i) 
Different methods can resolve the semantic drift problem to different extent: ME is better than POS, 
with 50% instances being positive, and our method is better than ME, with 95% instances being 
positive. ii) The Co-Bootstrapping method can effectively resolve the semantic drift problem: 25% of 
POS?s top 20 instances and 50% of ME?s top 20 instances are positive. In contrast, 90% of Hum_CB?s 
top 20 instances and 95% of FB_CB?s top 20 instances are positive respectively. It proves that 
Co-Bootstrapping method can better resolve the semantic drift problem than POS and ME. 
4.3 Parameter Optimization 
 
Figure 4: The MAP vs. threshold of P+ 
Our method has only one parameter: threshold of P+, which determines the instance?s polarity. 
Intuitively, a larger threshold of P+ will improve the precision of the positive instances but will regard 
some positive instances as negative instances mistakenly. As shown in Figure 4, our method can 
achieve the best MAP performance when the value of the threshold is 0.6. 
0
0.2
0.4
0.6
0.8
1
0.0 0.2 0.4 0.6 0.8 1.0
MAP
Threshold of P+ 
 
MAP 
2287
4.4 Comparison with State-of-the-Art Systems 
We also compare our method with three state-of-the-art systems: Google Sets1-- an ESE application 
provided by Google, SEAL2 -- a state-of-the-art ESE method proposed by Wang and Cohen (2008), 
and WMEB -- a state-of-the-art mutual exclusion based system proposed in McIntosh and Curran 
(2008). To make a fair comparison, we directly use the results before the adjustment which miss 
P@10 and P@50 in their original paper (McIntosh and Curran, 2008) and compared the performance 
of these systems on nine categories in (McIntosh and Curran, 2008). For each system, we conduct the 
experiment five times to reduce the impact of seeds selection. The average P@10, P@50, P@100 and 
P@200 are shown in Figure 5. 
 
Figure 5: The results compared with three state-of-the-art systems 
From the results shown in Figure 5, we can see that our probabilistic Co-Bootstrapping method can 
achieve state-of-the-art performance on all metrics: Compared with the well-known baseline Google 
Sets, our method can get a 42.0% improvement on P@200; Compared with the SEAL baseline, our 
method can get a 35.0% improvement on P@200; Compared with the WMEB method, our method can 
achieve a 6.2% improvement on P@100 and a 3.1% improvement on P@200. 
5 Conclusion and Future Work 
In this paper, we proposed a probabilistic Co-Bootstrapping method for entity set expansion. By 
introducing negative instances to define and refine the expansion boundary, our method can 
effectively resolve the expansion boundary problem and the semantic drift problem. Experimental 
results show that our method achieves significant performance improvement over the baselines, and 
outperforms three state-of-the-art ESE systems. Currently, our method did not take into account the 
long tail entity expansion, i.e., the instances which appear only a few times in the corpus, such as 
Saipan, Roseau and Suva for the Capital category. For future work, we will resolve the long tail 
entities in our Co-Bootstrapping method by taking the sparsity of instances/patterns into consideration. 
6 Acknowledgements 
We would like to thank three anonymous reviewers for invaluable comments and suggestions to 
improve our paper. This work is supported by the National Natural Science Foundation of China under 
Grants no. 61100152 and 61272324, and the National High Technology Development 863 Program of 
China under Grants no. 2013AA01A603. 
References 
Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting Relations from Large Plain-Text Collections. 
In: Proceedings of the fifth ACM conference on Digital libraries (DL-00), Pages 85-94. 
Joohui An, Seungwoo Lee, and Gary Geunbae Lee. 2003. Automatic acquisition of named entity tagged corpus 
from world wide web. In: Proceedings of ACL-03, Pages 165-168, Volume 2. 
Thorsten Brants and Alex Franz. 2006. Web 1t-5gram version1. http://www.ldc.upenn.edu/Catalog/ 
catalogEntry.jsp?catalogId=LDC2006T13 
                                                        
1 https://docs.google.com/spreadsheet/ 
2 http://www.boowa.com/ 
0.978 0.909 
0.848 
0.773 
0
0.2
0.4
0.6
0.8
1
P@10 P@50 P@100 P@200
Google Sets SEAL WMEB Co-Bootstrapping
2288
Sergey Brin. 1998. Extracting patterns and relations from the World Wide Web. In: Proceedings of the 
Workshop at the 6th International Conference on Extending Database Technology, Pages 172-183. 
Michael J. Cafarella, Doug Downey, Stephen Soderland, and Oren Etzioni. 2005. KnowItNow: Fast, Scalable 
Information Extraction from the Web. In: Proceedings of EMNLP-05, Pages 563-570. 
Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao, Enhong Chen, and Hang Li. 2008. Context-aware 
query suggestion by mining click-through and session data. In Proceedings of KDD-08, pages 875?883. 
Hakan Ceylan and Rada Mihalcea. 2011. An Efficient Indexer for Large N-Gram Corpora. In: Proceedings of 
System Demonstrations of ACL-11, Pages 103-108. 
William W. Cohen and Sunita Sarawagi. 2004. Exploiting dictionaries in named entity extraction: combining 
semi-Markov extraction processes and data integration methods. In: Proceedings of KDD-04, Pages 89-98. 
Alessandro Cucchiarelli and Paola Velardi. 2001. Unsupervised Named Entity Recognition Using Syntactic and 
Semantic Contextual Evidence. In: Computational Linguistics, Pages 123-131, Volume 27. 
James R. Curran, Tara Murphy, and Bernhard Scholz. 2007. Minimising semantic drift with Mutual Exclusion 
Bootstrapping. In: Proceedings of the 10th Conference of the Pacific Association for Computational 
Linguistics, Pages 172?180. 
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. 
Weld, and Alexander Yates. 2005. Unsupervised Named-Entity Extraction from the Web: An Experimental 
Study. In: Artificial Intelligence, Pages 91-134, Volume 165. 
Jian Hu, Gang Wang, Fred Lochovsky, Jiantao Sun, and Zheng Chen. 2009. Understanding user?s query intent 
with Wikipedia. In Proceedings of WWW-09, Pages 471?480. 
Zornitsa Kozareva and Eduard Hovy. 2010. Learning arguments and supertypes of semantic relations using 
recursive patterns. In: Proceedings of ACL-10, Pages 1482?1491. 
Tara McIntosh and James R. Curran. 2008. Weighted mutual exclusion bootstrapping for domain independent 
lexicon and template acquisition. In: Proceedings of the Australasian Language Technology Association 
Workshop, Pages 97-105. 
Tara McIntosh and James R. Curran. 2009. Reducing semantic drift with bagging and distributional similarity. 
In: Proceedings of ACL-09, Pages 396-404. 
Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In: Proceedings of KDD-08, Pages 
613-619. 
Patrick Pantel and Deepak Ravichandran. 2004. Automatically Labeling Semantic Classes. In: Proceedings of 
HLT/NAACL, Pages 321-328, Volume 4. 
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging Generic Patterns for Automatically 
Harvesting Semantic Relations. In: Proceedings of ACL-06, Pages 113?120. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In: Proceedings of EMNLP-09, Pages 938-947. 
Marius Pasca. 2007. Weakly-supervised discovery of named entities using web search queries. In: Proceedings of 
CIKM-07, Pages 683-690. 
Marco Pennacchiotti, Patrick Pantel. 2011. Automatically building training examples for entity extraction. In: 
Proceedings of CoNLL-11, Pages 163-171. 
Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction using multi-level 
bootstrapping. In: Proceedings of AAAI-99, Pages 474-479. 
Partha P. Talukdar, Joseph Reisinger, Marius Pasca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 
2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In: Proceedings of 
EMNLP-08, Pages 582-590. 
Michael Thelen and Ellen Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction 
pattern contexts. In: Proceedings of ACL-02, Pages 214-221. 
Richard C. Wang and William W. Cohen. 2008. Iterative Set Expansion of Named Entities using the Web. In: 
Proceedings of ICDM-08, Pages 1091-1096. 
2289
Richard C. Wang and William W. Cohen. 2009. Automatic Set Instance Extraction using the Web. In: 
Proceedings of ACL-09, Pages 441-449. 
Vishnu Vvas, Patrick Pantel and Eric Crestan. 2009. Helping editors choose better seed sets for entity set 
expansion. In: Proceedings of CIKM-09, Pages 225-234 
Roman Yangarber, Winston Lin and Ralph Grishman. 2002. Unsupervised learning of generalized names. In: 
Proceedings of COLING-02, Pages 1-7. 
2290
Overview of the Chinese Word Sense Induction Task at CLP2010 
Le Sun 
Institute of Software 
Chinese Academy of 
Sciences 
sunle@iscas.ac.cn 
Zhenzhong Zhang 
Institute of Software, Graduate 
University Chinese Academy of 
Sciences 
zhenzhong@nfs.iscas.ac.cn
Qiang Dong 
Canada Keentime Inc. 
dongqiang@keenage.
com 
 
Abstract 
In this paper, we describe the Chinese 
word sense induction task at CLP2010. 
Seventeen teams participated in this task 
and nineteen system results were 
submitted. All participant systems are 
evaluated on a dataset containing 100 
target words and 5000 instances using 
the standard cluster evaluation. We will 
describe the participating systems and 
the evaluation results, and then find the 
most suitable method by comparing the 
different Chinese word sense induction 
systems. 
1 Introduction 
Word Sense Disambiguation (WSD) is an 
important task in natural language proceeding 
research and is critical to many applications 
which require language understanding. In 
traditional evaluations, the supervised methods 
usually can achieve a better WSD performance 
than the unsupervised methods. But the 
supervised WSD methods have some drawbacks: 
Firstly, they need large annotated dataset which 
is expensive to manually annotate (Agirre and 
Aitor, 2007). Secondly, the supervised WSD 
methods   are based on the ?fixed-list of 
senses? paradigm, i.e., the senses of a target 
word are represented as a closed list coming 
from a manually constructed dictionary (Agirre 
et al, 2006). Such a ?Fixed-list of senses? 
paradigm suffers from the lack of explicit and 
topic relations between word senses, are usually 
cannot reflect the exact context of the target 
word (Veronis, 2004). Furthermore, because the 
?fixed-list of senses? paradigm make the fix 
granularity assumption of the senses distinction, 
it may not be suitable in different situations 
(Samuel and Mirella, 2009). Thirdly, since most 
supervised WSD methods assign senses based 
on dictionaries or other lexical resources, it will 
be difficult to adapt them to new domains or 
languages when such resources are scare 
(Samuel and Mirella, 2009).  
To overcome the deficiencies of the 
supervised WSD methods, many unsupervised 
WSD methods have been developed in recent 
years, which can induce word senses directly 
from the unannotated dataset, i.e., Word Sense 
Induction (WSI). In this sense, WSI could be 
treat as a clustering task, which groups the 
instances of the target word according to their 
contextual similarity, with each resulting cluster 
corresponding to a specific ?word sense? or 
?word use? of the target word (in the task of 
WSI, the term ?word use? is more suitable than 
?word sense?(Agirre and Aitor, 2007)).  
Although traditional clustering techniques can 
be directly employed in WSI, in recent years 
some new methods have been proposed to 
enhance the WSI performance, such as the 
Bayesian approach (Samuel and Mirella, 2009) 
and the collocation graph approach (Ioannis and 
Suresh, 2008). Both the traditional and the new 
methods can achieve a good performance in the 
task of English word sense induction. However, 
the methods work well in English may not be 
suitable for Chinese due to the difference 
between Chinese and English.  So it is both 
important and critical to provide a standard 
testbed for the task of Chinese word sense 
induction (CWSI), in order to compare the 
performance of different Chinese WSI methods 
and find the methods which are suitable for the 
Chinese word sense induction task.  
In this paper, we describe the Chinese word 
sense induction task at CLP2010. The goal of 
this task is to provide a standard testbed for 
Chinese WSI task. By comparing the different 
Chinese WSI methods, we can find the suitable 
methods for the Chinese word sense induction 
task.  
This paper is organized as follow. Section 2 
describes the evaluation dataset in detail. Section 
3 demonstrates the evaluation criteria. Section 3 
describes the participated systems and their 
results. The conclusions are drawn in section 4. 
2 Dataset 
Two datasets are provided to the participants: 
the trial dataset and the test dataset. 
The trial dataset contains 50 Chinese words, 
and for each Chinese word, a set of 50 word 
instances are provided. All word instances are 
extracted from the Web and the newspapers like 
the Xinhua newspaper and the Renmin 
newspaper, and the HowNet senses of target 
words were manually annotated (Dong). Figure 
1 shows an example of the trial data without 
hand-annotated tag. Figure 2 shows an example 
of the trial data with hand-annotated tag. In 
Figure 1, the tag ?snum=2? indicates that the 
target word ???? has two different senses in 
this dataset. In each instance, the target word is 
marked between the tag ?<head>? and the tag 
?</head>?. In Figure 2, all instances between the 
tag ?<sense s=S0>? and the tag ?</sense>? are 
belong to the same sense class.  
 
 
Figure 1: Example of the trial data without 
hand-annotated tag. 
 
The case of the test dataset is similar to the 
trial dataset, but with little different in the 
number of target words. The test dataset contains 
100 target words (22 Chinese words containing 
one Chinese character and 78 Chinese words  
containing two or more Chinese ideographs). 
Figure 3 shows an example of a system?s output. 
In Figure 3, the first column represents the 
identifiers of target word, the second column 
represents the identifiers of instances, and the 
third column represents the identifiers of the 
resulting clusters and their weight (1.0 by default) 
generated by Chinese WSI systems. 
 
 
Figure 2: Example of the trial data with 
hand-annotated tag. 
 
 
Figure 3: Example of the output format. 
3 Evaluation Metric 
As described in Section 1, WSI could be 
conceptualized as a clustering problem. So we 
can measure the performance of WSI systems 
using the standard cluster evaluation metrics. As 
the same as Zhao and Karypis(2005), we use the 
FScore measure as the primary measure for 
assessing different WSI methods. The FScore is 
used in a similar way as at Information Retrieval 
field.  
In this case, the results of the WSI systems are 
treated as clusters of instances and the gold 
standard senses are classes. Then the precision 
of a class with respect to a cluster is defined as 
the number of their mutual instances divided by 
the total cluster size, and the recall of a class 
with respect to a cluster is defined as the number 
of their mutual instances divided by the total 
class size. The detailed definition is as bellows.  
Let the size of a particular class sr is nr, the 
size of a particular cluster hj is nj and the size of 
their common instances set is nr,j.,then the 
precision can be defined as: 
,( , ) r jr j
j
n
P s h
n
=  
The recall can be defined as: 
,( , ) r jr j
r
n
R s h
n
=  
Then FScore of this class and cluster is defined 
to be: 
2 ( , ) ( , )
( , )
( , ) ( , )
r j r j
r j
r j r j
P s h R s h
F s h
P s h R s h
? ?= +  
The FScore of a class sr, F(sr), is the maximum 
F(sr, hj) value attained by any cluster, and it is 
defined as: 
 ( ) max( ( , ))
j
r r jh
F s F s h=  
Finally, the FScore of the entire clustering 
solution is defined as the weighted average 
FScore of all class: 
1
( )q r r
r
n F s
FScore
n=
?=?  
where q is the number of classes and n is the size 
of the instance set for particular target word. 
Table 1 shows an example of a contingency 
table of classes and clusters, which can be used 
to calculate FScore. 
 
 Cluster 1 Cluster 2 
Class 1 100 500 
Class 2 400 200 
Table 1: A contingency table of classes and 
clusters 
 
Using this contingency table, we can calculate 
the FScore of this example is 0.7483. It is easy 
to know the FScore of a perfect clustering 
solution will be equal to one, where each cluster 
has exactly the same instances as one of the 
classes, and vice versa. This means that the 
higher the FScore, the better the clustering 
performance. 
Purity and entropy (Zhao and Karypis, 2005) 
are also used to measure the performance of the 
clustering solution. Compared to FScore, they 
have some disadvantages. FScore uses two 
complementary concepts, precision and recall, to 
assess the quality of a clustering solution. 
Precision indicates the degree of the instances 
that make up a cluster, which belong to a single 
class. On the other hand, recall indicates the 
degree of the instances that make up a class, 
which belong to a single cluster. But purity and 
entropy only consider one factor and discard 
another. So we use FScore measure to assess a 
clustering solution. 
For the sake of completeness, we also employ 
the V-Measure to assess different clustering 
solutions. V-Measure assesses a cluster solution 
by considering its homogeneity and its 
completeness (Rosenberg and Hirschberg, 2007). 
Homogeneity measures the degree that each 
cluster contains data points which belong to a 
single Gold Standard class. And completeness 
measures the degree that each Gold Standard 
class contains data points assigned to a single 
cluster (Rosenberg and Hirschberg, 2007). In 
general, the larger the V-Measure, the better the 
clustering performance. More details can be 
referred to (Rosenberg and Hirschberg, 2007). 
4 Results 
In this section we describe the participant 
systems and present their results. 
Since the size of test data may not be large 
enough to distinguish word senses, participants 
were provided the total number of the target 
word?s senses. And participants were also 
allowed to use extra resources without 
hand-annotated. 
4.1 Participant teams and systems 
There were 17 teams registered for the WSI task 
and 12 teams submitted their results. Totally 19 
participant system results were submitted (One 
was submitted after the deadline). 10 teams 
submitted their technical reports. Table 2 
demonstrates the statistics of the participant 
information.  
The methods used by the participated systems 
were described as follows: 
FDU: This system first extracted the triplets 
for target word in each instance and got the 
intersection of all related words of these triplets 
using Baidu web search engine. Then the triplets 
and their corresponding intersections were used 
to construct feature vectors of the target word?s 
instances. After that, sequential Information 
Bottleneck algorithm was used to group 
instances into clusters. 
BUPT: Three clustering algorithms- the 
k-means algorithm, the Expectation- 
maximization algorithm and the Locally 
Adaptive Clustering algorithm were employed to 
cluster instances, where all instances were 
represented using some combined features. In 
the end the Group-average agglomerative 
clustering was used to cluster the consensus 
matrix M, which was obtained from the  
 
 
Name of Participant Team Result Report
Natural Language Processing Laboratory at Northeastern University (NEU) ? ? 
Beijing University of Posts and Telecommunications (BUPT) ? ? 
Beijing Institute of Technology (BIT) ?  
Shanghai Jiao Tong University (SJTU)   
Laboratory of Intelligent Information Processing and Application 
Institutional at Leshan Teachers? College (LSTC) 
? ? 
 Natural Language Processing Laboratory at Soochow University (SCU) ? ? 
Fudan University (FDU) ? ? 
Institute of Computational Linguistics at Peking University 1 (PKU1) ? ? 
Beijing University of Information Science and Technology (BUIST) ?  
Tsinghua University Research Institute of Information Technology, 
Speech and Language Technologies R&D Center (THU) 
  
Information Retrieval Laboratory at Dalian University of Technology 
(DLUT) 
? ? 
Institute of Computational Linguistics at Peking University 2 (PKU2) ? ? 
City University of HK (CTU)   
Institute of Software Chinese Academy of Sciences (ISCAS) ? ? 
Cognitive Science Department at Xiamen University (XMU) ? ? 
Harbin Institute of Technology Shenzhen Graduate School (HITSZGS)   
National Taipei University of Technology (NTUT)   
Table 2: The registered teams. ??? means that the team submitted the result or the report. 
 
adjacency matrices of the individual clusters 
generated by the three single clustering 
algorithms mentioned above. 
LSTC: This team extracted the five neighbor 
words and their POSs around the target word as 
features. Then the k-means algorithm was used 
to cluster the instances of each target word. 
NEU: The ?Global collocation? and the 
?local collocation? were extracted as features. A 
constraint hierarchical clustering algorithm was 
used to cluster the instances of each target 
word. 
XMU: The neighbor words of the target 
word were extracted as features and TongYiCi 
CiLin1 was employed to measure the similarity 
between instances. The word instances are 
???????????????????????????????????????? ?????????????????????
1? http://www.ir?lab.org/?
clustered using the improved hierarchical 
clustering algorithm based on parts of speech. 
DLUT: This team used the information gain 
to determine the size of the feature window. 
TongYiCi CiLin was used to solve the data 
sparseness problem.  The word instances are 
clustered using an improvement k-means 
algorithm where k-initial centers were selected 
based on maximum distance. 
ISCAS: This team employed k-means 
clustering algorithm to cluster the second order 
co-occurrence vectors of contextual words. 
TongYiCi CiLin and singular value 
decomposition method were used to solve the 
problem of data sparseness. Please note that this 
system was submitted by the organizers. The 
organizers have taken great care in order to  
 
guaranty all participants are under the same 
conditions. 
PKU2: This team used local tokens, local 
bigram feature and topical feature to represent 
words as vectors. Spectral clustering method 
was used to cluster the instances of each target 
word. 
PKU1: This team extracted three types of 
features to represent instances as feature vectors. 
Then the clustering was done by using k-means 
algorithm. 
SCU: All words except stop words in 
instances were extracted to produce the feature 
vectors, based on which the similarity matrix 
were generated. After that, the spectral 
clustering algorithm was applied to group 
instances into clusters. 
4.2 Official Results 
In this section we present the official results of 
the participant systems (ISCAS* was submitted 
by organizers; BUIST** was submitted after the 
deadline). We also provide the result of a 
baseline -- 1c1w, which group all instances of a 
target word into a single cluster. 
Table 3 shows the FScore of the main 
systems submitted by participant teams on the 
test dataset. Table 4 shows the FScore and 
V-Measure of all participant systems. Systems 
were ranked according to their FScore. 
 
Systems Rank FScore 
BUPT_mainsys 1 0.7933 
PKU1_main_system 2 0.7812 
FDU 3 0.7788 
DLUT_main_system 4 0.7729 
PKU2 5 0.7598 
ISCAS* 6 0.7209 
SCU 7 0.7108 
NEU_WSI_1 8 0.6715 
XMU 9 0.6534 
BIT 10 0.6366 
1c1w 11 0.6147 
BUIST** 12 0.5972 
LSTC 13 0.5789 
Table 3: FScore of main systems on the test 
dataset including one baseline -1c1w. 
 
 
 
 
Systems Rank FScore V- 
Measure
BUPT_mainsys 1 0.7933 0.4628 
BUPT_LAC 2 0.7895 0.4538 
BUPT_EM 3 0.7855 0.4356 
BUPT_kmeans 4 0.7849 0.4472 
PKU1_main_system 5 0.7812 0.4300 
FDU 6 0.7788 0.4196 
DLUT_main_system 7 0.7729 0.5032 
PKU1_agglo 8 0.7651 0.4096 
PKU2 9 0.7598 0.4078 
ISCAS* 10 0.7209 0.3174 
SCU 11 0.7108 0.3131 
NEU_WSI_1 12 0.6715 0.2331 
XMU 13 0.6534 0.1954 
NEU_WSI_0 14 0.6520 0.1947 
BIT 15 0.6366 0.1713 
1c1w 16 0.6147 0.0 
DLUT_RUN2 17 0.6067 0.1192 
BUIST** 18 0.5972 0.1014 
DLUT_RUN3 19 0.5882 0.0906 
LSTC 20 0.5789 0.0535 
Table 4: FScore and V-Measure of all systems, 
including one baseline. 
 
From the results shown in Table 3 and 4, we 
can see that: 
1)  As described in section 4.1, most 
systems use traditional clustering 
methods. For example, the teams using 
the k-means algorithm contain BUPT, 
LSTC, PKU1, DLUT and ISCAS. The 
teams using the spectral clustering 
algorithm contain SCU and PKU2. The 
team XMU and NEU use hierarchical 
clustering algorithm. The results shows 
that if provided with the number of 
target word senses, traditional methods 
can achieve a good performance. But we 
also notice that even the same method 
can have a different performance. This 
seems to indicate that features which are 
predictive of word senses are important 
to the task of CWSI. 
2)  Most systems outperform the 1c1w 
baseline, which indicates these systems 
are able to induce correct senses of 
target words to some extent.   
3)  The rank of FScore is much the same as 
that of V-Measure but with little 
difference. This may be because that the 
two evaluation measures both assess 
quality of a clustering solution by 
considering two different aspects, where 
precision corresponds to homogeneity 
and recall corresponds to completeness. 
But when assessing the quality of a 
clustering solution, the FScore only 
considers the contributions from the 
classes which are most similar to the 
clusters while the V-Measure considers 
the contributions from all classes. 
 
Systems Characters Words 
BUPT_mainsys 0.6307 0.8392 
BUPT_LAC 0.6298 0.8346 
BUPT_EM 0.6191 0.8324 
BUPT_kmeans 0.6104 0.8341 
PKU1_main_system 0.6291 0.8240 
FDU 0.6964 0.8020 
DLUT_main_system 0.5178 0.8448 
PKU1_agglo 0.5946 0.8132 
PKU2 0.6157 0.8004 
ISCAS* 0.5639 0.7651 
SCU 0.5715 0.7501 
NEU_WSI_1 0.5786 0.6977 
XMU 0.5290 0.6885 
NEU_WSI_0 0.5439 0.6825 
BIT 0.5328 0.6659 
DLUT_RUN2 0.5196 0.6313 
BUIST** 0.5022 0.6240 
DLUT_RUN3 0.5066 0.6113 
LSTC 0.4648 0.6110 
1c1w 0.4611 0.6581 
Table 5: FScore of all systems on the dataset 
only containing either single characters or 
words respectively. 
 
A Chinese word can be constituted by single 
or multiple Chinese characters. Senses of 
Chinese characters are usually determined by 
the words containing the character. In order to 
compare the WSI performance on different 
granularity of words, we add 22 Chinese 
characters into the test corpus. Table 5 shows 
the results of the participant systems 
correspondingly on the corpus which only 
contains the 22 Chinese characters and the 
corpus which only contains the 78 Chinese 
words. 
From Table 5, we can see that: 
1) The FScore of systems on the corpus 
only containing single characters is 
significantly lower than that on the 
corpus only containing words. We 
believe this is because: 1) The Single 
Chinese characters usually contains 
more senses than Chinese words; 2) 
Their senses are not determined directly 
by their contexts but by the words 
containing them. Compared to the 
number of instances, the number of 
words containing the single character is 
large. So it is difficult to distinguish 
different senses of single characters 
because of the data sparseness.  
2) We noticed that all systems outperform 
the 1c1w baseline on the corpus only 
containing single characters but there 
are some systems? FScore are lower 
than the baseline on the corpus only 
containing words. It may be because the 
large number of characters? senses and 
the FScore favored the words which 
have small number of senses.  
5 Conclusions 
In this paper we describe the design and the 
results of CLP2010 back-off task 4-Chinese 
word sense induction task. 17 teams registered 
to this task and 12 teams submitted their results. 
In total there were 19 participant systems (One 
of them was submitted after the deadline). And 
10 teams submitted their technical reports. All 
systems are evaluated on a corpus containing 
100 target words and 5000 instances using 
FScore measure and V-Measure. Participants 
are also provided with the number of senses and 
allowed to use resources without 
hand-annotated. 
The evaluation results have shown that most 
of the participant systems achieve a better 
performance than the 1c1w baseline. We also 
notice that it is more difficult to distinguish 
senses of Chinese characters than words. For 
future work, in order to test the performances of 
Chinese word sense induction systems under 
different conditions, corpus from different 
fields will be constructed and the number of 
target word senses will not be provided and will 
leave as an open task to the participant systems. 
Acknowledgments 
This work has been partially funded by National 
Natural Science Foundation of China under 
grant #60773027, #60736044 and #90920010 
and by ?863? Key Projects #2006AA010108, 
?863? Projects #2008AA01Z145. We would 
like to thank Dr. Han Xianpei and Zhang Weiru 
for their detailed comments. We also want to 
thank the annotators for their hard work on 
preparing the trial and test dataset. 
References 
Andrew Rosenberg and Julia Hirschberg. 2007. 
V-Measure: A conditional entropy-based external 
cluster evaluation measure. In Proceedings of the 
2007 Joint Conference on Empirical Methods in 
Natural Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL), 
pages 410?420. 
Eneko Agirre, David Mart??nez, Oier L?opez de 
Lacalle,and Aitor Soroa. 2006. Two graph-based 
algorithms for state-of-the-art WSD. In 
Proceedings of the 2006 Conference on Empirical 
Methods in Natural Language Processing, pages 
585?593, Sydney, Australia. 
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 
task2: Evaluating word sense induction and 
discrimination systems. In Proceedings of 
SemEval-2007. Association for Computational 
Llinguistics, pages 7-12, Prague. 
Ioannis P. Klapaftis and Suresh Manandhar, 2008. 
Word Sense Induction Using Graphs of 
Collocations. In Proceeding of the 2008 
conference on 18th European Conference on 
Artificial Intelligence, Pages: 298-302. 
Jean. V?eronis. 2004. Hyperlex: lexical cartography 
for information retrieval. Computer Speech & 
Language,18(3):223.252. 
Samuel Brody and Mirella Lapata, 2009. Bayesian 
word sense induction. In Proceedings of the 12th 
Conference of the European Chapter of the 
Association for Computational Linguistics, pages 
103-111, Athens, Greece. 
Ying Zhao and George Karypis. 2005. Hierarchical 
clustering algorithms for document datasets. Data 
Mining and Knowledge Discovery,10(2):141.168. 
Zhendong  Dong, 
http://www.keenage.com/zhiwang/e_zhiwang.html 
ISCAS?A System for Chinese Word Sense Induction Based on 
K-means Algorithm 
  Zhenzhong Zhang*           Le Sun? Wenbo Li? 
*Institute of Software, Graduate University 
Chinese Academy of Sciences 
zhenzhong@nfs.iscas.ac.cn 
?Institute of Software 
Chinese Academy of Sciences 
{sunle,wenbo02}@iscas.ac.cn
 
Abstract 
This paper presents an unsupervised 
method for automatic Chinese word 
sense induction. The algorithm is based 
on clustering the similar words according 
to the contexts in which they occur. First, 
the target word which needs to be 
disambiguated is represented as the 
vector of its contexts. Then, reconstruct 
the matrix constituted by the vectors of 
target words through singular value 
decomposition (SVD) method, and use 
the vectors to cluster the similar words. 
Our system participants in CLP2010 
back off task4-Chinese word sense 
induction. 
1 Introduction 
It has been shown that using word senses instead 
of surface word forms could improve 
performance on many nature language 
processing tasks such as information extraction 
(Joyce and Alan, 1999), information retrieval 
(Ozlem et al, 1999) and machine translation 
(David et al, 2005). Historically, word senses 
are represented as a fixed-list of definitions 
coming from a manually complied dictionary. 
However, there seem to be some disadvantages 
associated with such fixed-list of senses 
paradigm. Since dictionaries usually contain 
general definitions and lack explicit semantic, 
they can?t reflect the exact content of the context 
where the target word appears. Another 
disadvantage is that the granularity of sense 
distinctions is fixed, so it may not be entirely 
suitable for different applications. 
In order to overcome these limitations, some 
techniques like word sense induction (WSI) have 
been proposed for discovering words? senses 
automatically from the unannotated corpus. The 
word sense induction algorithms are usually base 
on the Distributional Hypothesis, proposed by 
(Zellig, 1954), which showed that words with 
similar meanings appear in similar contexts 
(Michael, 2009). And the hypothesis is also 
popularized with the phrase ?a word characte-
rized by the company it keeps? (John, 1957). 
This concept shows us a method to automatical-
ly discover senses of words by clustering the 
target words with similar contexts (Lin, 1998). 
The word sense induction can be regarded as an 
unsupervised clustering problem. First, select 
some features to be used when comparing simi-
larity between words. Second, represent disam-
biguated words as vectors of selected features 
according to target words? contexts. Third, clus-
ter the similar words using the vectors. But 
compared with European languages such as Eng-
lish, Chinese language has its own characteris-
tics. For example, Chinese ideographs have 
senses while the English alphabets don?t have. 
So the methods which work well in English may 
not be entirely suitable for Chinese. 
  This paper proposes a method for Chinese 
word sense induction, which contains two stage 
processes: features selecting and context cluster-
ing. Chinese ideographs and Chinese words 
which have two or more Chinese ideographs are 
used different strategies when selecting features. 
The vectors of target word?s instances are put 
together to constitute a matrix, whose row is in-
stances and column is features. Reconstruct the 
matrix through singular value decomposition to 
get a new vector for each instance. Then, K-
means clustering algorithm is employed to clus-
ter the vectors of disambiguated words? contexts. 
Each cluster to which some instances belong to 
identifies a sense of corresponding target word. 
Our system participants in CLP2010 back off 
task4 - Chinese word sense induction. 
The remainder of this paper is organized as 
follows. Section 2 presents the Chinese word 
senses induction algorithm. Section 3 presents 
the evaluation sheme and the results of our 
system. Section 4 gives some discussions and 
conclusions. 
2 Chinese Word Senses Induction 
This section will present the strategies of select-
ing features for disambiguated Chinese words 
and k-means algorithm for clustering vectors of 
the contexts.  
2.1 Features Selection 
Since the input instances of target words are un-
structured, it's necessary to select features and 
transform them into structured format to fit the 
automatic clustering algorithm. Following the 
example in (Ted, 2007), words are chosen as 
features to represent the contexts where target 
words appear. A word w in the context of the 
target word can be represented as a vector whose 
ith component is the average of the calculated 
conditional probabilities of w and wj.  
The target words are usually removed from 
the corpus in the task of English word sense in-
duction. But Chinese language is very different 
from European languages such as English. Chi-
nese ideographs usually have meanings of their 
own while English   alphabets don?t have. In 
Chinese word senses induction tasks, the target 
word may be a Chinese word which could have 
one or more Chinese ideographs or a Chinese 
ideograph. And the meaning of Chinese ideo-
graphs is determined by the Chinese word where 
it appears. The following example shows us this 
case. 
z ??????????????? 162
???? 
z ?????????????????
??????????? 
In this example, the target word is Chinese 
ideograph ??? displayed in italic in the con-
texts. In the first context, its meaning is paddy 
which is determined by the Chinese word ??
? ?, and similarly in the second context its 
meaning is valley determined by ????. Since 
the meaning of the Chinese ideograph ??? is 
determined by the word where it appears, it may 
not be appropriate to remove it from the con-
texts simply while the others of the word are left. 
Different strategies are employed to remove tar-
get words.  If the target word contains two or 
more Chinese ideographs, it will be removed 
from the context. Otherwise it will be kept. 
  To solve the problem of data sparseness, we 
extracted extra 100 instances for each target 
word from Sogou Data and also used the 
thesauruses (TongYiCi CiLin of HIT) to reduce 
the dimensionality of the word space (feature 
space). Two filtering heuristics are applied when 
selecting features. The first one is the minimum 
frequency p1 of words, and the second one is the 
maximum frequency p2 of words. 
Each selected word (feature) should be as-
signed a weight, which indicates the relative fre-
quency of two co-occurring words. Using condi-
tional probabilities for weighting for object/verb 
and subject/verb pairs is better than point-wise 
mutual information (Philipp et al, 2005). So we 
used conditional probabilities for weighting 
words pairs. Let numi,j denote the number of the 
instances where the word i and word j co-occur , 
and numi denote the number of the instances in 
which the word i appears. Then the jth compo-
nent of the vector of the word i can be calculated 
using the following equation. 
,
( | ) ( | )
2i j
p j i p i j
w
+=  
Where 
  
,( | ) i j
j
n u m
p i j
n u m
=  
The contexts of each target word are represented 
as the centroid of the vectors of the words occur-
ring in the target contexts. Figure 1 shows an 
example of context vector, where the Chinese 
word ???? co-occurs with Chinese words ??
??and ????. 
 
Figure 1: An example of  a context vector for 
????, calculated as the centroid of vectors of 
???? and ????. 
2.2 Clustering Algorithm 
K-means algorithm is applied to cluster the vec-
tors of the target word. It assigns each element to 
one of K clusters according to which centroid 
the element is close to by the similarity function. 
The cosine function is used to measure the simi-
larity between two vectors V and W: 
1
2 2
1 1
( , )
| | | |
n
i i
i
n n
i i
i i
VW
V W
sim V W
V W
V W
=
= =
?= =?
?
? ?
 
where n is the number of features in each vector. 
Before clustering the vectors of instances, we 
put together the vectors of instances in the cor-
pus and obtain a co-occurrence matrix of in-
stances and words. Singular value decomposi-
tion is applied to reduce the dimensionality of 
the resulting multidimensional space and finds 
the major axes of variation in the word space 
(Golub and Van Loan, 1989). After the reduc-
tion, the similarity between two instances can be 
measured using the cosine function mentioned as 
above between the corresponding vectors. The 
clustering algorithm stops when the centroid of 
each cluster does not change or the iteration of 
the algorithm exceed a user-defined threshold p3. 
And the number of the clusters is determined by 
the corpus where the target word appears. Each 
cluster to which some instances belong 
represents one senses of the target word 
represented by the vector. 
We also employed a graph-based clustering 
algorithm -Chinese Whispers (CW) (Chris, 2006) 
to deal with the task of Chinese WSI. CW does 
not require any input parameters and has a good 
performance in WSI (Chris, 2006). For more 
details about CW algorithm please refer to 
(Chris, 2006). We first constructed a graph, 
whose vertexes were instances of target word 
and edges? weight was the similarity of the cor-
responding two vertexes. Then we removed the 
edges with minimum weight until the percentage 
of the kept edges? sum respect the total was be-
low a threshold p4. CW algorithm was employed 
to cluster the graph and each clusters represented 
a sense of target word. 
3 Evaluation 
This section presents the evaluation scheme, set 
of parameters and the result of our system. 
3.1 Evaluation Scheme 
We use standard cluster evaluation methods to 
measure the performance of our WSI system. 
Following the former practice (Zhao and Kary-
pis, 2005), we consider the FScore measure for 
assessing WSI methods. The FScore is used in a 
similar fashion to Information Retrieval exercis-
es. 
Let we assume that the size of a particular 
class sr is nr, the size of a particular cluster hj is 
nj and the size of their common instances set is 
nr,j. The precision can be calculated as follow: 
,( , ) r jr j
j
n
P s h
n
=  
The recall value can be defined as: 
,( , ) r jr j
r
n
R s h
n
=  
Then FScore of this class and cluster is defined 
to be: 
2 ( , ) ( , )
( , )
( , ) ( , )
r j r j
r j
r j r j
P s h R s h
F s h
P s h R s h
? ?= +  
The FScore of class sr, F(sr), is the maximum 
F(sr, hj) value attained by any cluster, and it is 
defined as: 
 ( ) max( ( , ))
j
r r jh
F s F s h=  
Finally, the FScore of the entire clustering solu-
tion is defined as the weighted average FScore 
of each class: 
1( )q r r
r
n F s
FScore
n=
?=?  
  Where q is the number of classes and n is the 
total number of the instances where target word 
appears. 
3.2 Tuning the Parameters 
We tune the parameters of our system on the 
training data. But because of time restrictions, 
we do not optimize these parameters. The max-
imum frequency of a word (p2) and the maxi-
mum number of the K-means? iteration (p3) are 
tuned on the training data. The minimum fre-
quency of a word (p1) was set to two following 
our intuition. The last parameter K -the number 
of the clusters is determined by the test data in 
which the target word appears. When tuning pa-
rameters, we first fixed the parameter p3 and 
found the best value of parameter p2, which 
could lead to the best performance. The results 
have been shown in Table 1 and Table 2. 
 
Parameters FScore 
P3=300,p2=35 0.7502 
P3=400,p2=40 0.7523 
P3=500,p2=40 0.7582 
Table 1: The results of K-means with SVD 
 
Parameters FScore 
P3=300,p2=40 0.7454 
P3=400,p2=40 0.7493 
P3=500,p2=45 0.7404 
Table 2: The results of K-means 
 
The performance of CW algorithm is shown 
in Table 3. The parameter p4 is a threshold for 
pruning graph as describing in section 2.2.  
Parameter FScore 
P4=0.55 0.6325 
P4=0.6 0.6321 
P4=0.65 0.6278 
P4=0.7 0.6393 
P4=0.75 0.6289 
P4=0.8 0.6345 
P4=0.85 0.6326 
P4=0.9 0.6342 
P4=0.95 0.6355 
Table 3: The results of CW. 
The result shows that the K-means algorithm 
has a better performance than CW. That may 
because CW can?t use the information of the 
number of clusters, but K-means could. Another 
problem for CW is that the size of corpus is 
small and the constructed graph can?t reflect the 
inherent relation between the instances.  
Based on the result of experiments, we em-
ployed K-means algorithm for our system and 
the parameters is shown in Table 4. 
 
Parameters Value
P1: Minimum frequency of a word 2 
P2: Maximum frequency of a word 40 
P3: Maximum number of K-means ite-
ration 
500 
K: the number of the cluster - 
Table 4: Parameters for the system. The last pa-
rameter K is provided by the test data. 
3.3 Result 
Our system participants in the CLP2010 back-
off task4 and disambiguate 100 target words, 
total 5000 instances. The F-score of our system 
on the test data is 0.7209 against the F-score 
0.7933 of the best system. 
4 Conclusion 
We have presented a model for Chinese word 
sense induction. Different strategies are applied 
to deal with Chinese ideographs and Chinese 
words that contain two or more Chinese ideo-
graphs. After selecting the features ?words, sin-
gular value decomposition is used to find the 
major axes of variation in the feature space and 
reconstruct the vector of each context. Then we 
employ k-means cluster algorithm to cluster the 
vectors of contexts. Result shows that our sys-
tem is able to induce correct senses. One draw-
back of our system is that it overlooks the infre-
quent senses because of lacking enough data. 
And our system only uses the information of 
word co-occurrences. So in the future we would 
like to integrate different kinds of information 
such as topical information, syntactic informa-
tion and semantic information, and see if we 
could get a better result. 
Acknowledgement 
This work has been partially funded by National 
Natural Science Foundation of China under 
grant #60773027, #60736044 and #90920010 
and by ?863? Key Projects #2006AA010108, 
?863? Projects #2008AA01Z145. We would like 
to thank anonymous reviewers for their detailed 
comments. 
References 
Chris Biemann, 2006.  Chinese whispers - an efficient 
graph clustering algorithm and its application to 
natural language processing problems, In Pro-
ceedings of TextGraphs, pp. 73?80, New York, 
USA. 
David Vickrey, Luke Biewald, Marc Teyssley, and 
Daphne Koller. 2005. Word-sense disambiguation 
for machine translation. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, 
pages 771-778, Vancouver, British Columbia, 
Canada 
Dekang Lin. 1998. Automatic retrieval and clustering 
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics, 
volume 2, pages 768-774, Montreal, Quebec, Can-
ada 
Golub, G. H. and Van Loan, C. F. 1989. Matrix 
Computations. The John Hopkins University Press, 
Baltimore, MD 
John, R., Firth. 1957. A Synopsis of Linguistic Theory 
1930-1955, pages 1-32. 
Joyce Yue Chai and Alan W. Biermann. 1999. The 
use of word sense disambiguation in an informa-
tion extraction system. In Proceedings of the six-
teenth national conference on Artificial intelli-
gence and the eleventh Innovative applications of 
artificial intelligence conference innovative appli-
cations of artificial intelligence, pages 850-855, 
Orlando, Florida, United States. 
Michael Denkowski. 2009. A Survey of Techniques 
for Unsupervised Word Sense Induction. 
Ozlem Uzuner, Boris Katz, and Deniz Yuret. 1999. 
Word sense disambiguation for information re-
trieval. In Proceedings of the sixteenth national 
conference on Artificial intelligence and the ele-
venth Innovative applications of artificial intelli-
gence conference innovative applications of artifi-
cial intelligence, page 985, Orlando, Florida, Unit-
ed States. 
Philipp Cimiano, Andreas Hotho, and Steffen Staab, 
2005.  Learning concept hierarchies from text cor-
pora using formal concept analysis, Journal of Ar-
tificial Intelligence Research (JAIR), 24, 305?339. 
Ted Pedersen, 2007. Umnd2: Senseclusters applied to 
the sense induction task of senseval-4. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations, pages 394?397, Prague, Czech 
Republic. 
Zellig Harris. 1954. Distributional Structure, pages 
146-162.  
Ying Zhao and George Karypis. 2005. Hierarchical 
clustering algorithms for document datasets. Data 
Mining and Knowledge Discovery, 10(2):141.168. 
