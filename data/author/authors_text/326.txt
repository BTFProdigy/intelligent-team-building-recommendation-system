Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 168?175,
New York, June 2006. c?2006 Association for Computational Linguistics
Multilevel Coarse-to-fine PCFG Parsing
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil,
David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths,
Jeremy Moore, Michael Pozar, and Theresa Vu
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
ec@cs.brown.edu
Abstract
We present a PCFG parsing algorithm
that uses a multilevel coarse-to-fine
(mlctf) scheme to improve the effi-
ciency of search for the best parse.
Our approach requires the user to spec-
ify a sequence of nested partitions or
equivalence classes of the PCFG non-
terminals. We define a sequence of
PCFGs corresponding to each parti-
tion, where the nonterminals of each
PCFG are clusters of nonterminals of
the original source PCFG. We use the
results of parsing at a coarser level
(i.e., grammar defined in terms of a
coarser partition) to prune the next
finer level. We present experiments
showing that with our algorithm the
work load (as measured by the total
number of constituents processed) is
decreased by a factor of ten with no de-
crease in parsing accuracy compared to
standard CKY parsing with the origi-
nal PCFG. We suggest that the search
space over mlctf algorithms is almost
totally unexplored so that future work
should be able to improve significantly
on these results.
1 Introduction
Reasonably accurate constituent-based parsing
is fairly quick these days, if fairly quick means
about a second per sentence. Unfortunately, this
is still too slow for many applications. In some
cases researchers need large quantities of parsed
data and do not have the hundreds of machines
necessary to parse gigaword corpora in a week
or two. More pressingly, in real-time applica-
tions such as speech recognition, a parser would
be only a part of a much larger system, and
the system builders are not keen on giving the
parser one of the ten seconds available to pro-
cess, say, a thirty-word sentence. Even worse,
some applications require the parsing of multi-
ple candidate strings per sentence (Johnson and
Charniak, 2004) or parsing from a lattice (Hall
and Johnson, 2004), and in these applications
parsing efficiency is even more important.
We present here a multilevel coarse-to-fine
(mlctf) PCFG parsing algorithm that reduces
the complexity of the search involved in find-
ing the best parse. It defines a sequence of in-
creasingly more complex PCFGs, and uses the
parse forest produced by one PCFG to prune
the search of the next more complex PCFG.
We currently use four levels of grammars in our
mlctf algorithm. The simplest PCFG, which we
call the level-0 grammar, contains only one non-
trivial nonterminal and is so simple that min-
imal time is needed to parse a sentence using
it. Nonetheless, we demonstrate that it identi-
fies the locations of correct constituents of the
parse tree (the ?gold constituents?) with high
recall. Our level-1 grammar distinguishes only
argument from modifier phrases (i.e., it has two
nontrivial nonterminals), while our level-2 gram-
mar distinguishes the four major phrasal cate-
gories (verbal, nominal, adjectival and preposi-
tional phrases), and level 3 distinguishes all of
the standard categories of the Penn treebank.
168
The nonterminal categories in these grammars
can be regarded as clusters or equivalence classes
of the original Penn treebank nonterminal cat-
egories. (In fact, we obtain these grammars by
relabeling the node labels in the treebank and
extracting a PCFG from this relabelled treebank
in the standard fashion, but we discuss other ap-
proaches below.) We require that the partition
of the nonterminals defined by the equivalence
classes at level l + 1 be a refinement of the par-
tition defined at level l. This means that each
nonterminal category at level l+1 is mapped to a
unique nonterminal category at level l (although
in general the mapping is many to one, i.e., each
nonterminal category at level l corresponds to
several nonterminal categories at level l + 1).
We use the correspondence between categories
at different levels to prune possible constituents.
A constituent is considered at level l + 1 only
if the corresponding constituent at level l has
a probability exceeding some threshold.. Thus
parsing a sentence proceeds as follows. We first
parse the sentence with the level-0 grammar to
produce a parse forest using the CKY parsing
algorithm. Then for each level l + 1 we reparse
the sentence with the level l + 1 grammar us-
ing the level l parse forest to prune as described
above. As we demonstrate, this leads to consid-
erable efficiency improvements.
The paper proceeds as follows. We next dis-
cuss previous work (Section 2). Section 3 out-
lines the algorithm in more detail. Section
4 presents some experiments showing that the
work load (as measured by the total number of
constituents processed) is decreased by a fac-
tor of ten over standard CKY parsing at the
final level. We also discuss some fine points of
the results therein. Finally in section 5 we sug-
gest that because the search space of mlctf al-
gorithms is, at this point, almost totally unex-
plored, future work should be able to improve
significantly on these results.
2 Previous Research
Coarse-to-fine search is an idea that has ap-
peared several times in the literature of com-
putational linguistics and related areas. The
first appearance of this idea we are aware of is
in Maxwell and Kaplan (1993), where a cover-
ing CFG is automatically extracted from a more
detailed unification grammar and used to iden-
tify the possible locations of constituents in the
more detailed parses of the sentence. Maxwell
and Kaplan use their covering CFG to prune the
search of their unification grammar parser in es-
sentially the same manner as we do here, and
demonstrate significant performance improve-
ments by using their coarse-to-fine approach.
The basic theory of coarse-to-fine approxima-
tions and dynamic programming in a stochastic
framework is laid out in Geman and Kochanek
(2001). This paper describes the multilevel
dynamic programming algorithm needed for
coarse-to-fine analysis (which they apply to de-
coding rather than parsing), and show how
to perform exact coarse-to-fine computation,
rather than the heuristic search we perform here.
A paper closely related to ours is Goodman
(1997). In our terminology, Goodman?s parser
is a two-stage ctf parser. The second stage is a
standard tree-bank parser while the first stage is
a regular-expression approximation of the gram-
mar. Again, the second stage is constrained by
the parses found in the first stage. Neither stage
is smoothed. The parser of Charniak (2000)
is also a two-stage ctf model, where the first
stage is a smoothed Markov grammar (it uses
up to three previous constituents as context),
and the second stage is a lexicalized Markov
grammar with extra annotations about parents
and grandparents. The second stage explores
all of the constituents not pruned out after the
first stage. Related approaches are used in Hall
(2004) and Charniak and Johnson (2005).
A quite different approach to parsing effi-
ciency is taken in Caraballo and Charniak (1998)
(and refined in Charniak et al (1998)). Here
efficiency is gained by using a standard chart-
parsing algorithm and pulling constituents off
the agenda according to (an estimate of) their
probability given the sentence. This probability
is computed by estimating Equation 1:
p(nki,j | s) =
?(nki,j)?(nki,j)
p(s) . (1)
169
It must be estimated because during the
bottom-up chart-parsing algorithm, the true
outside probability cannot be computed. The
results cited in Caraballo and Charniak (1998)
cannot be compared directly to ours, but are
roughly in the same equivalence class. Those
presented in Charniak et al (1998) are superior,
but in Section 5 below we suggest that a com-
bination of the techniques could yield better re-
sults still.
Klein and Manning (2003a) describe efficient
A? for the most likely parse, where pruning is
accomplished by using Equation 1 and a true
upper bound on the outside probability. While
their maximum is a looser estimate of the out-
side probability, it is an admissible heuristic and
together with an A? search is guaranteed to find
the best parse first. One question is if the guar-
antee is worth the extra search required by the
looser estimate of the true outside probability.
Tsuruoka and Tsujii (2004) explore the frame-
work developed in Klein and Manning (2003a),
and seek ways to minimize the time required
by the heap manipulations necessary in this
scheme. They describe an iterative deepening
algorithm that does not require a heap. They
also speed computation by precomputing more
accurate upper bounds on the outside proba-
bilities of various kinds of constituents. They
are able to reduce by half the number of con-
stituents required to find the best parse (com-
pared to CKY).
Most recently, McDonald et al (2005) have
implemented a dependency parser with good
accuracy (it is almost as good at dependency
parsing as Charniak (2000)) and very impres-
sive speed (it is about ten times faster than
Collins (1997) and four times faster than Char-
niak (2000)). It achieves its speed in part be-
cause it uses the Eisner and Satta (1999) algo-
rithm for n3 bilexical parsing, but also because
dependency parsing has a much lower grammar
constant than does standard PCFG parsing ?
after all, there are no phrasal constituents to
consider. The current paper can be thought of
as a way to take the sting out of the grammar
constant for PCFGs by parsing first with very
few phrasal constituents and adding them only
Level: 0 1 2 3
S1
{
S1
{
S1
{
S1
P
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
HP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
S
?
?
?
?
?
?
?
?
?
?
?
?
?
S
VP
UCP
SQ
SBAR
SBARQ
SINV
N
?
?
?
?
?
?
?
?
?
?
?
?
?
NP
NAC
NX
LST
X
UCP
FRAG
MP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
A
?
?
?
?
?
?
?
?
?
?
?
?
?
ADJP
QP
CONJP
ADVP
INTJ
PRN
PRT
P
?
?
?
?
?
?
?
?
?
?
?
?
?
PP
PRT
RRC
WHADJP
WHADVP
WHNP
WHPP
Figure 1: The levels of nonterminal labels
after most constituents have been pruned away.
3 Multilevel Course-to-fine Parsing
We use as the underlying parsing algorithm a
reasonably standard CKY parser, modified to
allow unary branching rules.
The complete nonterminal clustering is given
in Figure 1. We do not cluster preterminals.
These remain fixed at all levels to the standard
Penn-tree-bank set Marcus et al (1993).
Level-0 makes two distinctions, the root node
and everybody else. At level 1 we make one
further distinction, between phrases that tend
to be heads of constituents (NPs, VPs, and Ss)
and those that tend to be modifiers (ADJPs,
PPs, etc.). Level-2 has a total of five categories:
root, things that are typically headed by nouns,
those headed by verbs, things headed by prepo-
sitions, and things headed by classical modifiers
(adjectives, adverbs, etc.). Finally, level 3 is the
170
S1
P
P
PRP
He
P
VBD
ate
P
IN
at
P
DT
the
NN
mall
.
.
S1
HP
HP
PRP
He
HP
VBD
ate
MP
IN
at
HP
DT
the
NN
mall
.
.
S1
S_
N_
PRP
He
S_
VBD
ate
P_
IN
at
N_
DT
the
NN
mall
.
.
S1
S
NP
PRP
He
VP
VBD
ate
PP
IN
at
NP
DT
the
NN
mall
.
.
Figure 2: A tree represented at levels 0 to 3
classical tree-bank set. As an example, Figure 2
shows the parse for the sentence ?He ate at the
mall.? at levels 0 to 3.
During training we create four grammars, one
for each level of granularity. So, for example, at
level 1 the tree-bank rule
S ?NP VP .
would be translated into the rule
HP ?HP HP .
That is, each constituent type found in ?S ?NP
VP .? is mapped into its generalization at level 1.
The probabilities of all rules are computed us-
ing maximum likelihood for constituents at that
level.
The grammar used by the parser can best be
described as being influenced by four compo-
nents:
1. the nonterminals defined at that level of
parsing,
2. the binarization scheme,
3. the generalizations defined over the bina-
rization, and
4. extra annotation to improve parsing accu-
racy.
The first of these has already been covered. We
discuss the other three in turn.
In anticipation of eventually lexicalizing the
grammar we binarize from the head out. For
example, consider the rule
A ?a b c d e
where c is the head constituent. We binarize
this as follows:
A ?A1 e
A1 ?A2 d
A2 ?a A3
A3 ?b c
Grammars induced in this way tend to be
too specific, as the binarization introduce a very
large number of very specialized phrasal cat-
egories (the Ai). Following common practice
Johnson (1998; Klein and Manning (2003b) we
Markovize by replacing these nonterminals with
ones that remember less of the immediate rule
context. In our version we keep track of only the
parent, the head constituent and the constituent
immediately to the right or left, depending on
which side of the constituent we are processing.
With this scheme the above rules now look like
this:
A ?Ad,c e
Ad,c ?Aa,c d
Aa,c ?a Ab,c
Ab,c ?b c
So, for example, the rule ?A ?Ad,c e? would
have a high probability if constituents of type
A, with c as their head, often have d followed
by e at their end.
Lastly, we add parent annotation to phrasal
categories to improve parsing accuracy. If we
assume that in this case we are expanding a rule
for an A used as a child of Q, and a, b, c, d, and
e are all phrasal categories, then the above rules
become:
AQ ?Ad,c eA
Ad,c ?Aa,c dA
Aa,c ?aA Ab,c
Ab,c ?bA cA
171
10?8 10?7 10?6 10?5 10?4 10?3
0.0001
0.001
0.01
0.1
 
 
Level 0
Level 1
Level 2
Level 3
Figure 3: Probability of a gold constituent being
pruned as a function of pruning thresholds for
the first 100 sentences of the development corpus
Once we have parsed at a level, we evaluate
the probability of a constituent p(nki,j | s) ac-
cording to the standard inside-outside formula
of Equation 1. In this equation nki,j is a con-
stituent of type k spanning the words i to j, and
?(?) and ?(?) are the outside and inside proba-
bilities of the constituent, respectively. Because
we prune at the end each granularity level, we
can evaluate the equation exactly; no approxi-
mations are needed (as in, e.g., Charniak et al
(1998)).
During parsing, instead of building each con-
stituent allowed by the grammar, we first test
if the probability of the corresponding coarser
constituent (which we have from Equation 1 in
the previous round of parsing) is greater than
a threshold. (The threshold is set empirically
based upon the development data.) If it is below
the threshold, we do not put the constituent in
the chart. For example, before we can use a NP
and a VP to create a S (using the rule above),
we would first need to check that the probability
in the coarser grammar of using the same span
HP and HP to create a HP is above the thresh-
old. We use the standard inside-outside for-
mula to calculate this probability (Equation 1).
The empirical results below justify our conjec-
ture that there are thresholds that allow signifi-
cant pruning while leaving the gold constituents
untouched.
10?8 10?7 10?6 10?5 10?4 10?3
0.001
0.01
0.1
1
 
 
Level 0
Level 1
Level 2
Level 3
Figure 4: Fraction of incorrect constituents kept
as a function of pruning thresholds for the first
100 sentences of the development corpus
4 Results
In all experiments the system is trained on the
Penn tree-bank sections 2-21. Section 23 is used
for testing and section 24 for development. The
input to the parser are the gold-standard parts
of speech, not the words.
The point of parsing at multiple levels of gran-
ularity is to prune the results of rough levels be-
fore going on to finer levels. In particular, it is
necessary for any pruning scheme to retain the
true (gold-standard WSJ) constituents in the
face of the pruning. To gain an idea of what
is possible, consider Figure 3. According to the
graph, at the zeroth level of parsing and a the
pruning level 10?4 the probability that a gold
constituent is deleted due to pruning is slightly
more than 0.001 (or 0.1%). At level three it is
slightly more that 0.01 (or 1.0%).
The companion figure, Figure 4 shows the
retention rate of the non-gold (incorrect) con-
stituents. Again, at pruning level 10?4 and pars-
ing level 0 we retain about .3 (30%) of the bad
constituents (so we pruned 70%), whereas at
level 3 we retain about .004 (0.4%). Note that
in the current paper we do not actually prune
at level 3, instead return the Viterbi parse. We
include pruning results here in anticipation of
future work in which level 3 would be a precur-
sor to still more fine-grained parsing.
As noted in Section 2, there is some (implicit)
172
Level Constits Constits % Pruned
Produced Pruned
?106 ?106
0 8.82 7.55 86.5
1 9.18 6.51 70.8
2 11.2 9.48 84.4
3 11,8 0 0.0
total 40.4 ? ?
3-only 392.0 0 0
Figure 5: Total constituents pruned at all levels
for WSJ section 23, sentences of length ? 100
debate in the literature on using estimates of
the outside probability in Equation 1, or instead
computing the exact upper bound. The idea is
that an exact upper bound gives one an admis-
sible search heuristic but at a cost, since it is a
less accurate estimator of the true outside prob-
ability. (Note that even the upper bound does
not, in general, keep all of the gold constituents,
since a non-perfect model will assign some of
them low probability.) As is clear from Figure
3, the estimate works very well indeed.
On the basis of this graph, we set the lowest
allowable constituent probability at ? 5 ? 10?4,
? 10?5, and ? 10?4 for levels 0,1, and 2, re-
spectively. No pruning is done at level 3, since
there is no level 4. After setting the pruning
parameters on the development set we proceed
to parse the test set (WSJ section 23). Figure 5
shows the resulting pruning statistics. The to-
tal number of constituents created at level 0, for
all sentences combined, is 8.82 ? 106. Of those
7.55 ? 106 (or 86.5%) are pruned before going on
to level 1. At level 1, the 1.3 million left over
from level 0 expanded to a total of 9.18 ? 106.
70.8% of these in turn are pruned, and so forth.
The percent pruned at, e.g., level 1 in Figure 3
is much higher than that shown here because it
considers all of the possible level-1 constituents,
not just those left unpruned after level 0.
There is no pruning at level 3. There we sim-
ply return the Viterbi parse. We also show that
with pruning we generate a total of 40.4 ? 106
constituents. For comparison exhaustively pars-
ing using the tree-bank grammar yields a total
of 392 ? 106 constituents. This is the factor-of-10
Level Time for Level Running Total
0 1598 1598
1 2570 4168
2 4303 8471
3 1527 9998
3-only 114654 ?
Figure 6: Running times in seconds on WSJ sec-
tion 23, with and without pruning
workload reduction mentioned in Section 1.
There are two points of interest. The first is
that each level of pruning is worthwhile. We do
not get most of the effect from one or the other
level. The second point is that we get signif-
icant pruning at level 0. The reader may re-
member that level 0 distinguishes only between
the root node and the rest. We initially ex-
pected that it would be too coarse to distinguish
good from bad constituents at this level, but it
proved as useful as the other levels. The expla-
nation is that this level does use the full tree-
bank preterminal tags, and in many cases these
alone are sufficient to make certain constituents
very unlikely. For example, what is the proba-
bility of any constituent of length two or greater
ending in a preposition? The answer is: very
low. Similarly for constituents of length two or
greater ending in modal verbs, and determiners.
Not quite so improbable, but nevertheless less
likely than most, would be constituents ending
in verbs, or ending just short of the end of the
sentence.
Figure 6 shows how much time is spent at each
level of the algorithm, along with a running to-
tal of the time spent to that point. (This is for
all sentences in the test set, length ? 100.) The
number for the unpruned parser is again about
ten times that for the pruned version, but the
number for the standard CKY version is prob-
ably too high. Because our CKY implementa-
tion is quite slow, we ran the unpruned version
on many machines and summed the results. In
all likelihood at least some of these machines
were overloaded, a fact that our local job dis-
tributer would not notice. We suspect that the
real number is significantly lower, though still
173
No pruning 77.9
With pruning 77.9
Klein and Manning (2003b) 77.4
Figure 7: Labeled precision/recall f-measure,
WSJ section 23, all sentences of length ? 100
much higher than the pruned version.
Finally Figure 7 shows that our pruning is ac-
complished without loss of accuracy. The results
with pruning include four sentences that did not
receive any parses at all. These sentences re-
ceived zeros for both precision and recall and
presumably lowered the results somewhat. We
allowed ourselves to look at the first of these,
which turned out to contain the phrase:
(NP ... (INTJ (UH oh) (UH yes)) ...)
The training data does not include interjections
consisting of two ?UH?s, and thus a gold parse
cannot be constructed. Note that a different
binarization scheme (e.g. the one used in Klein
and Manning (2003b) would have smoothed over
this problem. In our case the unpruned version
is able to patch together a lot of very unlikely
constituents to produce a parse, but not a very
good one. Thus we attribute the problem not to
pruning, but to binarization.
We also show the results for the most similar
Klein and Manning (2003b) experiment. Our
results are slightly better. We attribute the dif-
ference to the fact that we have the gold tags
and they do not, but their binarization scheme
does not run into the problems that we encoun-
tered.
5 Conclusion and Future Research
We have presented a novel parsing algorithm
based upon the coarse-to-fine processing model.
Several aspects of the method recommend it.
First, unlike methods that depend on best-first
search, the method is ?holistic? in its evalua-
tion of constituents. For example, consider the
impact of parent labeling. It has been repeat-
edly shown to improve parsing accuracy (John-
son, 1998; Charniak, 2000; Klein and Manning,
2003b), but it is difficult if not impossible to
integrate with best-first search in bottom-up
chart-parsing (as in Charniak et al (1998)). The
reason is that when working bottom up it is diffi-
cult to determine if, say, ssbar is any more or less
likely than ss, as the evidence, working bottom
up, is negligible. Since our method computes
the exact outside probability of constituents (al-
beit at a coarser level) all of the top down in-
formation is available to the system. Or again,
another very useful feature in English parsing
is the knowledge that a constituent ends at the
right boundary (minus punctuation) of a string.
This can be included only in an ad-hoc way when
working bottom up, but could be easily added
here.
Many aspects of the current implementation
that are far from optimal. It seems clear to
us that extracting the maximum benefit from
our pruning would involve taking the unpruned
constituents and specifying them in all possible
ways allowed by the next level of granularity.
What we actually did is to propose all possi-
ble constituents at the next level, and immedi-
ately rule out those lacking a corresponding con-
stituent remaining at the previous level. This
was dictated by ease of implementation. Before
using mlctf parsing in a production parser, the
other method should be evaluated to see if our
intuitions of greater efficiency are correct.
It is also possible to combine mlctf parsing
with queue reordering methods. The best-first
search method of Charniak et al (1998) esti-
mates Equation 1. Working bottom up, estimat-
ing the inside probability is easy (we just sum
the probability of all the trees found to build
this constituent). All the cleverness goes into
estimating the outside probability. Quite clearly
the current method could be used to provide a
more accurate estimate of the outside probabil-
ity, namely the outside probability at the coarser
level of granularity.
There is one more future-research topic to add
before we stop, possibly the most interesting of
all. The particular tree of coarser to finer con-
stituents that governs our mlctf algorithm (Fig-
ure 1) was created by hand after about 15 min-
utes of reflection and survives, except for typos,
with only two modifications. There is no rea-
174
son to think it is anywhere close to optimal. It
should be possible to define ?optimal? formally
and search for the best mlctf constituent tree.
This would be a clustering problem, and, for-
tunately, one thing statistical NLP researchers
know how to do is cluster.
Acknowledgments
This paper is the class project for Computer
Science 241 at Brown University in fall 2005.
The faculty involved were supported in part
by DARPA GALE contract HR0011-06-2-0001.
The graduate students were mostly supported
by Brown University fellowships. The under-
graduates were mostly supported by their par-
ents. Our thanks to all.
References
Sharon Caraballo and Eugene Charniak. 1998. Fig-
ures of merit for best-first probabalistic parsing.
Computational Linguistics, 24(2):275?298.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 2005 Meeting of
the Association for Computational Linguistics.
Eugene Charniak, Sharon Goldwater, and Mark
Johnson. 1998. Edge-based best-first chart pars-
ing. In Proceedings of the Sixth Workshop on
Very Large Corpora, pages 127?133. Morgan Kauf-
mann.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 132?139.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, San Francisco. Mor-
gan Kaufmann.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 457?464.
Stuart Geman and Kevin Kochanek. 2001. Dy-
namic programming and the representation of
soft-decodable codes. IEEE Transactions on In-
formation Theory, 47:549?568.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 1997).
Keith Hall and Mark Johnson. 2004. Attention shift-
ing for parsing speech. In The Proceedings of the
42th Annual Meeting of the Association for Com-
putational Linguistics, pages 40?46.
Keith Hall. 2004. Best-first Word-lattice Pars-
ing: Techniques for Integrated Syntactic Language
Modeling. Ph.D. thesis, Brown University.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 33?
39.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003a. A* parsing:
Fast exact viterbi parse selection. In Proceedings
of HLT-NAACL?03.
Dan Klein and Christopher Manning. 2003b. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics.
Michell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?
330.
John T. Maxwell and Ronald M. Kaplan. 1993.
The interface between phrasal and functional con-
straints. Computational Linguistics, 19(4):571?
590.
Ryan McDonald, Toby Crammer, and Fernando
Pereira. 2005. Online large margin training of
dependency parsers. In Proceedings of the 43rd
Meeting of the Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2004. It-
erative cky parsing for probabilistic context-free
grammars. In International Joint Conference on
Natural-Language Processing.
175
Proceedings of NAACL HLT 2007, pages 436?443,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Unified Local and Global Model for Discourse Coherence
Micha Elsner, Joseph Austerweil, and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{melsner,ec}@cs.brown.edu, joseph.austerweil@gmail.com
Abstract
We present a model for discourse co-
herence which combines the local entity-
based approach of (Barzilay and Lapata,
2005) and the HMM-based content model
of (Barzilay and Lee, 2004). Unlike the
mixture model of (Soricut and Marcu,
2006), we learn local and global features
jointly, providing a better theoretical ex-
planation of how they are useful. As the
local component of our model we adapt
(Barzilay and Lapata, 2005) by relaxing
independence assumptions so that it is ef-
fective when estimated generatively. Our
model performs the ordering task compet-
itively with (Soricut and Marcu, 2006),
and significantly better than either of the
models it is based on.
1 Introduction
Models of coherent discourse are central to several
tasks in natural language processing: such mod-
els have been used in text generation (Kibble and
Power, 2004) and evaluation of human-produced
text in educational applications (Miltsakaki and Ku-
kich, 2004; Higgins et al, 2004). Moreover, an ac-
curate model can reveal information about document
structure, aiding in such tasks as supervised summa-
rization (Barzilay and Lapata, 2005).
Models of coherence tend to fall into two classes.
Local models (Lapata, 2003; Barzilay and Lapata,
2005; Foltz et al, 1998) attempt to capture the gen-
eralization that adjacent sentences often have similar
content, and therefore tend to contain related words.
Models of this type are good at finding sentences
that belong near one another in the document. How-
ever, they have trouble finding the beginning or end
of the document, or recovering from sudden shifts in
topic (such as occur at paragraph boundaries). Some
local models also have trouble deciding which of a
pair of related sentences ought to come first.
In contrast, the global HMM model of Barzilay
and Lee (2004) tries to track the predictable changes
in topic between sentences. This gives it a pro-
nounced advantage in ordering sentences, since it
can learn to represent beginnings, ends and bound-
aries as separate states. However, it has no local
features; the particular words in each sentence are
generated based only on the current state of the doc-
ument. Since information can pass from sentence
to sentence only in this restricted manner, the model
sometimes fails to place sentences next to the correct
neighbors.
We attempt here to unify the two approaches by
constructing a model with both sentence-to-sentence
dependencies providing local cues, and a hidden
topic variable for global structure. Our local fea-
tures are based on the entity grid model of (Barzilay
and Lapata, 2005; Lapata and Barzilay, 2005). This
model has previously been most successful in a con-
ditional setting; to integrate it into our model, we
first relax its independence assumptions to improve
its performance when used generatively. Our global
model is an HMM like that of Barzilay and Lee
(2004), but with emission probabilities drawn from
the entity grid. We present results for two tasks,
the ordering task, on which global models usually
do well, and the discrimination task, on which lo-
cal models tend to outperform them. Our model im-
proves on purely global or local approaches on both
436
tasks.
Previous work by Soricut and Marcu (2006) has
also attempted to integrate local and global fea-
tures using a mixture model, with promising results.
However, mixture models lack explanatory power;
since each of the individual component models is
known to be flawed, it is difficult to say that the com-
bination is theoretically more sound than the parts,
even if it usually works better. Moreover, since the
model we describe uses a strict subset of the fea-
tures used in the component models of (Soricut and
Marcu, 2006), we suspect that adding it to the mix-
ture would lead to still further improved results.
2 Naive Entity Grids
Entity grids, first described in (Lapata and Barzilay,
2005), are designed to capture some ideas of Cen-
tering Theory (Grosz et al, 1995), namely that ad-
jacent utterances in a locally coherent discourses are
likely to contain the same nouns, and that important
nouns often appear in syntactically important roles
such as subject or object. An entity grid represents
a document as a matrix with a column for each en-
tity, and a row for each sentence. The entry ri,j de-
scribes the syntactic role of entity j in sentence i:
these roles are subject (S), object (O), or some other
role (X)1. In addition there is a special marker (-)
for nouns which do not appear at all in a given sen-
tence. Each noun appears only once in a given row
of the grid; if a noun appears multiple times, its grid
symbol describes the most important of its syntac-
tic roles: subject if possible, then object, or finally
other. An example text is figure 1, whose grid is fig-
ure 2.
Nouns are also treated as salient or non-salient,
another important concern of Centering Theory. We
condition events involving a noun on the frequency
of that noun. Unfortunately, this way of representing
salience makes our model slightly deficient, since
the model conditions on a particular noun occurring
e.g. 2 times, but assigns nonzero probabilities to
documents where it occurs 3 times. This is theo-
1Roles are determined heuristically using trees produced by
the parser of (Charniak and Johnson, 2005). Following previous
work, we slightly conflate thematic and syntactic roles, marking
the subject of a passive verb as O.
2The numeric token ?1300? is removed in preprocessing,
and ?Nuevo Laredo? is marked as ?PROPER?.
0 [The commercial pilot]O , [sole occupant of [the airplane]X]X
, was not injured .
1 [The airplane]O was owned and operated by [a private
owner]X .
2 [Visual meteorological conditions]S prevailed for [the per-
sonal cross country flight for which [a VFR flight plan]O was
filed]X .
3 [The flight]S originated at [Nuevo Laredo , Mexico]X , at
[approximately 1300]X .
Figure 1: A section of a document, with syntactic
roles of noun phrases marked.
0 1 2 3
PLAN - - O -
AIRPLANE X O - -
CONDITION - - S -
FLIGHT - - X S
PILOT O - - -
PROPER - - - X
OWNER - X - -
OCCUPANT X - - -
Figure 2: The entity grid for figure 12.
retically quite unpleasant but in comparing different
orderings of the same document, it seems not to do
too much damage.
Properly speaking entities may be referents of
many different nouns and pronouns throughout the
discourse, and both (Lapata and Barzilay, 2005) and
(Barzilay and Lapata, 2005) present models which
use coreference resolution systems to group nouns.
We follow (Soricut and Marcu, 2006) in dropping
this component of the system, and treat each head
noun as having an individual single referent.
To model transitions in this entity grid model,
Lapata and Barzilay (2005) takes a generative ap-
proach. First, the probability of a document is de-
fined as P (D) = P (Si..Sn), the joint probability of
all the sentences. Sentences are generated in order
conditioned on all previous sentences:
P (D) =
?
i
P (Si|S0..(i?1)). (1)
We make a Markov assumption of order h (in our
experiments h = 2) to shorten the history. We repre-
sent the truncated history as ~Shi?1 = S(i?h)..S(i?1).
Each sentence Si can be split up into a set of
nouns representing entities, Ei, and their corre-
sponding syntactic roles Ri, plus a set of words
which are not entities, Wi. The model treats Wi as
independent of the previous sentences. For any fixed
437
set of sentences Si,
?
i P (Wi) is always constant,
and so cannot help in finding a coherent ordering.
The probability of a sentence is therefore dependent
only on the entities:
P (Si|~Sh(i?1)) = P (Ei, Ri|~Sh(i?1)). (2)
Next, the model assumes that each entity ej ap-
pears in sentences and takes on syntactic roles in-
dependent of all the other entities. As we show
in section 3, this assumption can be problem-
atic. Once we assume this, however, we can sim-
plify P (Ei, Ri|~Sh(i?1)) by calculating for each en-
tity whether it occurs in sentence i and if so, which
role it takes. This is equivalent to predicting ri,j .
We represent the history of the specific entity ej as
~r h(i?1),j = r(i?h),j ..r(i?1),j , and write:
P (Ei, Ri|~Sh(i?1)) ?
?
j
P (ri,j|~r h(i?1),j). (3)
For instance, in figure 2, the probability of S3 with
horizon 1 is the product of P (S|X) (for FLIGHT),
P (X|?) (for PROPER), and likewise for each other
entity, P (?|O), P (?|S), P (?|?)3.
Although this generative approach outperforms
several models in correlation with coherence ratings
assigned by human judges, it suffers in comparison
with later systems. Barzilay and Lapata (2005) uses
the same grid representation, but treats the transi-
tion probabilities P (ri,j |~ri,j) for each document as
features for input to an SVM classifier. Soricut and
Marcu (2006)?s implementation of the entity-based
model also uses discriminative training.
The generative model?s main weakness in com-
parison to these conditional models is its assump-
tion of independence between entities. In real doc-
uments, each sentence tends to contain only a few
nouns, and even fewer of them can fill roles like
subject and object. In other words, nouns compete
with each other for the available syntactic positions
in sentences; once one noun is chosen as the sub-
ject, the probability that any other will also become
a subject (of a different subclause of the same sen-
tence) is drastically lowered. Since the generative
entity grid does not take this into account, it learns
that in general, the probability of any given entity
appearing in a specific sentence is low. Thus it gen-
erates blank sentences (those without any nouns at
all) with overwhelmingly high probability.
It may not be obvious that this misallocation of
probability mass also reduces the effectiveness of
the generative entity grid in ordering fixed sets of
sentences. However, consider the case where an en-
tity has a history ~r h, and then does not appear in
the next sentence. The model treats this as evidence
that entities generally do not occur immediately af-
ter ~r h? but it may also happen that the entity was
outcompeted by some other word with even more
significance.
3 Relaxed Entity Grid
In this section, we relax the troublesome assump-
tion of independence between entities, thus mov-
ing the probability distribution over documents away
from blank sentences. We begin at the same point as
above: sequential generation of sentences: P (D) =
?
i P (Si|S0..(i?1)). We similarly separate the words
into entities and non-entities, treat the non-entities as
independent of the history ~S and omit them. We also
distinguish two types of entities. Let the known set
Ki = ej : ej ? ~S(i?1), the set of all entities which
have appeared before sentence i. Of the entities ap-
pearing in Si, those in Ki are known entities, and
those which are not are new entities. Since each en-
tity in the document is new precisely once, we treat
these as independent and omit them from our calcu-
lations as we did the non-entities. We return to both
groups of omitted words in section 4 below when
discussing our topic-based models.
To model a sentence, then, we generate the set of
known entities it contains along with their syntac-
tic roles, given the history and the known set Ki.
We truncate the history, as above, with horizon h;
note that this does not make the model Markovian,
since the known set has no horizon. Finally, we con-
sider only the portion of the history which relates to
known nouns (since all non-known nouns have the
same history - -). In all the equations below, we re-
strict Ei to known entities which actually appear in
sentence i, and Ri to roles filled by known entities.
The probability of a sentence is now:
P (Si|~Sh(i?1)) = P (Ei, Ri|~Rh(i?1)). (4)
We make one further simplification before begin-
ning to approximate: we first generate the set of syn-
tactic slots Ri which we intend to fill with known en-
tities, and then decide which entities from the known
438
set to select. Again, we assume independence from
the history, so that the contribution of P (Ri) for any
ordering of a fixed set of sentences is constant and
we omit it:
P (Ei, Ri|~Rh(i?1),j) = P (Ei|Ri, ~Rh(i?1),j). (5)
Estimating P (Ei|Ri, ~Rh(i?1),j) proves to be dif-
ficult, since the contexts are very sparse. To con-
tinue, we make a series of approximations. First let
each role be filled individually (where r ? e is the
boolean indicator function ?noun e fills role r?):
P (Ei|Ri, ~Rh(i?1),j) ?
?
r?Ri
P (r ? ej |r, ~Rh(i?1),j).
(6)
Notice that this process can select the same noun ej
to fill multiple roles r, while the entity grid cannot
represent such an occurrence. The resulting distri-
bution is therefore slightly deficient.
Unfortunately, we are still faced with the sparse
context ~Rh(i?1),j , the set of histories of all currently
known nouns. It is much easier to estimate P (r ?
ej |r,~r h(i?1),j), where we condition only on the his-
tory of the particular noun which is chosen to fill
slot r. However, in this case we do not have a proper
probability distribution: i.e. the probabilities do not
sum to 1. To overcome this difficulty we simply nor-
malize by force3:
P (r ? ej|r, ~Rh(i?1),j) ? (7)
P (r ? ej |r,~r h(i?1),j)
?
j?Ki P (r ? ej|r,~r h(i?1),j)
The individual probabilities P (r ? ej |r,~r h(i?1),j)
are calculated by counting situations in the train-
ing documents in which a known noun has his-
tory ~r h(i?1),j and fills slot r in the next sentence,
versus situations where the slot r exists but is
filled by some other noun. Some rare contexts are
still sparse, and so we smooth by adding a pseu-
docount of 1 for all events. Our model is ex-
pressed by equations (1),(4),(5),(6) and (7). In
3Unfortunately this estimator is not consistent (that is, given
infinite training data produced by the model, the estimated pa-
rameters do not converge to the true parameters). We are in-
vestigating maximum entropy estimation as a solution to this
problem.
figure 2, the probability of S3 with horizon 1 is
now calculated as follows: the known set con-
tains PLAN, AIRPLANE, CONDITION, FLIGHT,
PILOT, OWNER and OCCUPANT. There is one syn-
tactic role filled by a known noun, S. The proba-
bility is then calculated as P (+|S,X) (the proba-
bility of selecting a noun with history X to fill the
role of S) normalized by P (+|S,O)+P (+|S,S)+
P (+|S,X) + 4? P (+|S,?).
Like Lapata and Barzilay (2005), our relaxed
model assigns low probability to sentences where
nouns with important-seeming histories do not ap-
pear. However, in our model, the penalty is less
severe if there are many competitor nouns. On the
other hand, if the sentence contains many slots, giv-
ing the noun more opportunity to fill one of them,
the penalty is proportionally greater if it does not
appear.
4 Topic-Based Model
The model we describe above is a purely local one,
and moreover it relies on a particular set of local fea-
tures which capture the way adjacent sentences tend
to share lexical choices. Its lack of any global struc-
ture makes it impossible for the model to recover at
a paragraph boundary, or to accurately guess which
sentence should begin a document. Its lack of lexi-
calization, meanwhile, renders it incapable of learn-
ing dependences between pairs of words: for in-
stance, that a sentence discussing a crash is often
followed by a casualty report.
We remedy both these problems by extending our
model of document generation. Like Barzilay and
Lee (2004), we learn an HMM in which each sen-
tence has a hidden topic qi, which is chosen con-
ditioned on the previous state qi?1. The emission
model of each state is an instance of the relaxed en-
tity grid model as described above, but in addition
to conditioning on the role and history, we condi-
tion also on the state and on the particular set of
lexical items lex(Ki) which may be selected to fill
the role: P (r ? ej |r, ~Rh(i?1),j , qi, lex(Ki)). This
distribution is approximated as above by the nor-
malized value of P (r ? ej |r,~r h(i?1),j , qi, lex(ej)).
However, due to our use of lexical information,
even this may be too sparse for accurate estima-
tion, so we back off by interpolating with the pre-
439
Figure 3: A single time-slice of our HMM.
Wi ? PY (?|qi; ?LM , discountLM )
Ni ? PY (?|qi; ?NN , discountNN )
Ei ? EGrid(?|R, ~R2i?1, qi, lex(Ki); ?EG)
qi ? DP (?|qi?1)
In the equations above, only the manually set inter-
polation hyperparameters are indicated.
vious model. In each context, we introduce ?EG
pseudo-observations, split fractionally according to
the backoff distribution: if we abbreviate the context
in the relaxed entity grid as C and the event as e, this
smoothing corresponds to:
P (e|C, qi, ej) =
#(e,C, qi, ej) + ?EGP (e|C)
#(e,C, qi, ej) + ?EG
.
This is equivalent to defining the topic-based entity
grid as a Dirichlet process with parameter ?EG sam-
pling from the relaxed entity grid.
In addition, we are now in a position to gener-
ate the non-entity words Wi and new entities Ni in
an informative way, by conditioning on the sentence
topic qi. Since they are interrupted by the known
entities, they do not form contiguous sequences of
words, so we make a bag-of-words assumption. To
model these sets of words, we use unigram ver-
sions of the hierarchical Pitman-Yor processes of
(Teh, 2006), which implement a Bayesian version
of Kneser-Ney smoothing.
To represent the HMM itself, we adapt the non-
parametric HMM of (Beal et al, 2001). This is
a Bayesian alternative to the conventional HMM
model learned using EM, chosen mostly for conve-
nience. Our variant of it, unlike (Beal et al, 2001),
has no parameter ? to control self-transitions; our
emission model is complex enough to make it un-
necessary.
The actual number of states found by the model
depends mostly on the backoff constants, the ?s
(and, for Pitman-Yor processes, discounts) chosen
for the emission models (the entity grid, non-entity
word model and new noun model), and is relatively
insensitive to particular choices of prior for the other
hyperparameters. As the backoff constants decrease,
the emission models become more dependent on the
state variable q, which leads to more states (and
eventually to memorization of the training data). If
instead the backoff rate increases, the emission mod-
els all become close to the general distribution and
the model prefers relatively few states. We train with
interpolations which generally result in around 40
states.
Once the interpolation constants are set, the
model can be trained by Gibbs sampling. We also
do inference over the remaining hyperparameters of
the model by Metropolis sampling from uninforma-
tive priors. Convergence is generally very rapid; we
obtain good results after about 10 iterations. Unlike
Barzilay and Lee (2004), we do not initialize with
an informative starting distribution.
When finding the probability of a test document,
we do not do inference over the full Bayesian model,
because the number of states, and the probability of
different transitions, can change with every new ob-
servation, making dynamic programming impossi-
ble. Beal et al (2001) proposes an inference algo-
rithm based on particle filters, but we feel that in
this case, the effects are relatively minor, so we ap-
proximate by treating the model as a standard HMM,
using a fixed transition function based only on the
training data. This allows us to use the conventional
Viterbi algorithm. The backoff rates we choose at
training time are typically too small for optimal in-
ference in the ordering task. Before doing tests, we
set them to higher values (determined to optimize
ordering performance on held-out data) so that our
emission distributions are properly smoothed.
5 Experiments
Our experiments use the popular AIRPLANE cor-
pus, a collection of documents describing airplane
crashes taken from the database of the National
440
Transportation Safety Board, used in (Barzilay and
Lee, 2004; Barzilay and Lapata, 2005; Soricut and
Marcu, 2006). We use the standard division of
the corpus into 100 training and 100 test docu-
ments; for development purposes we did 10-fold
cross-validation on the training data. The AIRPLANE
documents have some advantages for coherence re-
search: they are short (11.5 sentences on average)
and quite formulaic, which makes it easy to find lex-
ical and structural patterns. On the other hand, they
do have some oddities. 46 of the training documents
begin with a standard preamble: ?This is prelimi-
nary information, subject to change, and may con-
tain errors. Any errors in this report will be corrected
when the final report has been completed,? which
essentially gives coherence models the first two sen-
tences for free. Others, however, begin abruptly with
no introductory material whatsoever, and sometimes
without even providing references for their definite
noun phrases; one document begins: ?At V1, the
DC-10-30?s number 1 engine, a General Electric
CF6-50C2, experienced a casing breach when the
2nd-stage low pressure turbine (LPT) anti-rotation
nozzle locks failed.? Even humans might have trou-
ble identifying this sentence as the beginning of a
document.
5.1 Sentence Ordering
In the sentence ordering task, (Lapata, 2003; Barzi-
lay and Lee, 2004; Barzilay and Lapata, 2005; Sori-
cut and Marcu, 2006), we view a document as an
unordered bag of sentences and try to find the or-
dering of the sentences which maximizes coherence
according to our model. This type of ordering pro-
cess has applications in natural language generation
and multi-document summarization. Unfortunately,
finding the optimal ordering according to a prob-
abilistic model with local features is NP-complete
and non-approximable (Althaus et al, 2004). More-
over, since our model is not Markovian, the relax-
ation used as a heuristic for A? search by Soricut
and Marcu (2006) is ineffective. We therefore use
simulated annealing to find a high-probability order-
ing, starting from a random permutation of the sen-
tences. Our search system has few Estimated Search
Errors as defined by Soricut and Marcu (2006); it
rarely proposes an ordering which has lower proba-
? Discr. (%)
(Barzilay and Lapata, 2005) - 90
(Barzilay and Lee, 2004) .44 745
(Soricut and Marcu, 2006) .50 -6
Topic-based (relaxed) .50 94
Table 1: Results for AIRPLANE test data.
bility than the original ordering4 .
To evaluate the quality of the orderings we predict
as optimal, we use Kendall?s ? , a measurement of
the number of pairwise swaps needed to transform
our proposed ordering into the original document,
normalized to lie between ?1 (reverse order) and 1
(original order). Lapata (2006) shows that it corre-
sponds well with human judgements of coherence
and reading times. A slight problem with ? is that
it does not always distinguish between proposed or-
derings of a document which disrupt local relation-
ships at random, and orderings in which paragraph-
like units move as a whole. In longer documents, it
may be worth taking this problem into account when
selecting a metric; however, the documents in the
AIRPLANE corpus are mostly short and have little
paragraph structure, so ? is an effective metric.
5.2 Discrimination
Our second task is the discriminative test used by
(Barzilay and Lapata, 2005). In this task we gen-
erate random permutations of a test document, and
measure how often the probability of a permutation
is higher than that of the original document. This
task bears some resemblance to the task of discrim-
inating coherent from incoherent essays in (Milt-
sakaki and Kukich, 2004), and is also equivalent
in the limit to the ranking metric of (Barzilay and
Lee, 2004), which we cannot calculate because our
model does not produce k-best output. As opposed
to the ordering task, which tries to measure how
close the model?s preferred orderings are to the orig-
inal, this measurement assesses how many orderings
the model prefers. We use 20 random permutations
per document, for 2000 total tests.
441
? Discr. (%)
Naive Entity Grid .17 81
Relaxed Entity Grid .02 87
Topic-based (naive) .39 85
Topic-based (relaxed) .54 96
Table 2: Results for 10-fold cross-validation on AIR-
PLANE training data.
6 Results
Since the ordering task requires a model to propose
the complete structure for a set of sentences, it is
very dependent on global features. To perform ad-
equately, a model must be able to locate the begin-
ning and end of the document, and place intermedi-
ate sentences relative to these two points. Without
any way of doing this, our relaxed entity grid model
has ? of approximately 0, meaning its optimal or-
derings are essentially uncorrelated with the correct
orderings7 . The HMM content model of (Barzilay
and Lee, 2004), which does have global structure,
performs much better on ordering, at ? of .44. How-
ever, local features can help substantially for this
task, since models which use them are better at plac-
ing related sentences next to one another. Using both
sets of features, our topic-based model achieves state
of the art performance (? = .5) on the ordering task,
comparable with the mixture model of (Soricut and
Marcu, 2006).
The need for good local coherence features is es-
pecially clear from the results on the discrimination
task (table 1). Permuting a document may leave ob-
vious ?signposts? like the introduction and conclu-
sion in place, but it almost always splits up many
pairs of neighboring sentences, reducing local co-
herence. (Barzilay and Lee, 2004), which lacks lo-
cal features, does quite poorly on this task (74%),
while our model performs extremely well (94%).
It is also clear from the results that our relaxed en-
tity grid model (87%) improves substantially on the
generative naive entity grid (81%). When used on
40 times on test data, 3 times in cross-validation.
5Calculated on our test permutations using the code at
http://people.csail.mit.edu/regina/code.html.
6Soricut and Marcu (2006) do not report results on this task,
except to say that their implementation of the entity grid per-
forms comparably to (Barzilay and Lapata, 2005).
7Barzilay and Lapata (2005) do not report ? scores.
its own, it performs much better on the discrimina-
tion task, which is the one for which it was designed.
(The naive entity grid has a higher ? score, .17, es-
sentially by accident. It slightly prefers to generate
infrequent nouns from the start context rather than
the context - -, which happens to produce the correct
placement for the ?preliminary information? pream-
ble.) When used as the emission model for known
entities in our topic-based system, the relaxed en-
tity grid shows its improved performance even more
strongly (table 2); its results are about 10% higher
than the naive version under both metrics.
Our combined model uses only entity-grid fea-
tures and unigram language models,a strict subset of
the feature set of (Soricut and Marcu, 2006). Their
mixture includes an entity grid model and a version
of the HMM of (Barzilay and Lee, 2004), which
uses n-gram language modeling. It also uses a model
of lexical generation based on the IBM-1 model for
machine translation, which produces all words in the
document conditioned on words from previous sen-
tences. In contrast, we generate only entities con-
ditioned on words from previous sentences; other
words are conditionally independent given the topic
variable. It seems likely therefore that using our
model as a component of a mixture might improve
on the state of the art result.
7 Future Work
Ordering in the AIRPLANE corpus and similar con-
strained sets of short documents is by no means a
solved problem, but the results so far show a good
deal of promise. Unfortunately, in longer and less
formulaic corpora, the models, inference algorithms
and even evaluation metrics used thus far may prove
extremely difficult to scale up. Domains with more
natural writing styles will make lexical prediction a
much more difficult problem. On the other hand,
the wider variety of grammatical constructions used
may motivate more complex syntactic features, for
instance as proposed by (Siddharthan et al, 2004) in
sentence clustering.
Finding optimal orderings is a difficult task even
for short documents, and will become exponen-
tially more challenging in longer ones. For multi-
paragraph documents, it is probably impractical to
use full-scale coherence models to find optimal or-
442
derings directly. A better approach may be a coarse-
to-fine or hierarchical system which cuts up longer
documents into more manageable chunks that can be
ordered as a unit.
Multi-paragraph documents also pose a problem
for the ? metric itself. In documents with clear the-
matic divisions between their different sections, a
good ordering metric should treat transposed para-
graphs differently than transposed sentences.
8 Acknowledgements
We are extremely grateful to Regina Barzilay, for her
code, data and extensive support, Mirella Lapata for
code and advice, and the BLLIP group, especially
Tom Griffiths, Sharon Goldwater and Mark Johnson,
for comments and criticism. We were supported by
DARPA GALE contract HR0011-06-2-0001 and the
Karen T. Romer Foundation. Finally we thank three
anonymous reviewers for their comments.
References
Ernst Althaus, Nikiforos Karamanis, and Alexander
Koller. 2004. Computing locally coherent discourses.
In Proceedings of the 42nd ACL, Barcelona.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The infinite Hidden Markov
Model. In NIPS, pages 577?584.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173?180.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&3):285?307.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In HLT-NAACL, pages 185?
192.
Roger Kibble and Richard Power. 2004. Optimising ref-
erential coherence in text generation. Computational
Linguistics, 30(4):401?416.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085?1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting of ACL, 2003.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall?s tau. Computational Linguis-
tics, 32(4):1?14.
E. Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25?55.
Advaith Siddharthan, Ani Nenkova, and Kathleen McK-
eown. 2004. Syntactic simplification for improving
content selection in multi-document summarization.
In COLING04, pages 896?902.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
Y.W. Teh. 2006. A Bayesian interpretation of interpo-
lated Kneser-Ney. Technical Report TRA2/06, Na-
tional University of Singapore.
443
