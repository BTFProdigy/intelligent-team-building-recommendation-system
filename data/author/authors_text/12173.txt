Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 87?96,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Interactive Gesture in Dialogue: a PTT Model
Hannes Rieser
Bielefeld University
Hannes.Rieser@uni-Bielefeld.de
Massimo Poesio
Universit? di Trento/University of Essex
poesio@essex.ac.uk
Abstract
Gestures are usually looked at in isola-
tion or from an intra-propositional per-
spective essentially tied to one speaker.
The Bielefeld multi-modal Speech-And-
Gesture-Alignment (SAGA) corpus has
many interactive gestures relevant for the
structure of dialogue (Rieser 2008, 2009).
To describe them, a dialogue theory is
needed which can serve as a speech-
gesture interface. PTT (Poesio and Traum
1997, Poesio and Rieser submitted a) can
do this job in principle, how this can be
achieved is the main topic of this paper.
As a precondition, the empirical research
procedure from systematic corpus annota-
tion via gesture typology to a partial on-
tology for gestures is described. It is then
explained how PTT is extended to provide
an incremental modelling of speech plus
gesture in an assertion-acknowledgement
adjacency pair where grounding between
dialogue participants is obtained through
gesture.
1 Introduction and Overview
We present work combining experimental meth-
ods, body-movement tracking techniques, corpus
linguistics and theoretical modelling in order to in-
vestigate the role of iconic gesture in dialogue. We
propose to map speech meaning and gesture mean-
ing into a single compositional meaning which is
then used in grounding and up-dating of infor-
mation states in discourse, using PTT (Poesio &
Traum 1997, Poesio & Rieser submitted 2009a)
to account for the speech-gesture interface. We
argue that several design features of PTT are es-
sential for this purpose, such as accepting sub-
propositional inputs, extracting information from
linguistic surface, using dynamic semantics, bas-
ing the dialogue engine on a theory of grounded-
ness and grounding, and allowing for the resolu-
tion of anaphora across turns.
The structure of the paper is as follows. Sec-
tion 2 looks at the Bielefeld Speech-and-Gesture-
Alignment corpus SAGA from which the data
comes. Section 3 then deals with multi-modal acts
using one example from SAGA (Dial 1 p.??). In
section 4 a short introduction into PTT is provided.
Sections 5 and 6 explain how a gesture typology
and a partial ontology can be extracted from the
annotated data. Both (see Appendix) serve as the
basis for the integration of gesture meaning and
verbal meaning. In section 7 PTT is developed
as an interface for verbal and gestural meaning.
First a PTT description of Dial 1 is provided using
(Poesio and Rieser submitted b, Poesio to appear)
dealing inter alia with anaphora resolution (7.1).
Secondly, PTTs interface properties are detailed
(7.2), the semantic defaults for combining speech
and gesture meaning are set up (7.3), and a gestu-
ral dialogue act is described (7.4). Section 8 con-
tains some preliminary insights into the grounding
of multi-modal content.
2 The Multi-modal SAGA Corpus
The SAGA corpus contains 25 route-description
dialogues taken from three camera perspectives
using body tracking technologies.1 The setting
comes with a Router ?riding on a car? through
a virtual landscape passing five landmarks. The
landmarks are connected by streets. Fig. 1a in Ap-
pendix B shows the Router, Fig. 1b the site, Fig.
1cf. Bergmann, K. et al (2007, 2008)
87
1c the town hall. After the ride the Router reports
his trip in detail to a Follower. We collected audio
and body movement data as well as eye-tracking
data from the Router. The dialogues have all been
annotated, use of functional predicates like IN-
DEXING, MODELLING, SHAPING2 etc. was
rated.
3 An Example from the SAGA corpus
In the dialogue passage (Dial 1) the Router uses
gestures to explain the looks of the town-hall.
We?ll focus on the numbered utterances in this pa-
per; utterances omitted in the reconstruction are
reported in italics, omitted phrases in brackets.
DIAL 1 [ROUTER:] [. . . ]
[. . . ]
und
and
[du]
[you]
folgst
follow
dann
then
dem
the
Stra?enverlauf
street
einfach
simply
nur bis
until
du
you
ah
ah
vor
before
nem
a
gr??eren
larger
Geb?ude
building
stehst.
stand.
(2.1) Das
That
ist
is
dann
then
das
the
Rathaus.
townhall.
(2.2) [Ahm]
[Ahm]
das
that
ist
is
ein
a
U-f?rmiges
U-shaped
Geb?ude.
building.
(2.3) Du
You
blickst
look
[praktisch]
[practically]
da
into
rein.
it.
(2.4) [Das
[That
hei?t]
is]
es
it
hat
has
vorne
to the front
zwei
two
Buchtungen
bulges
und
and
geht
closes
hinten
in the rear
zusammen dann.
then.
[FOLLOWER:] OK.
In (Dial 1) Router?s gestures first come with
two BEATS.3 Shortly after, the BEATs extend
into an ICONIC gesture overlapping town hall
in (2.1)(stills in Apendix B), cf. the still Two-
Handed-Prism-Segment-1. Then the Router?s
DRAWN U-shaped gesture (still One-Handed-
U-Shape) intersects the word U-shaped. Next
his SHAPING the sides of a prism (still Two-
Handed-U-Shaped-Prism-Segment) aligns with
[look pactically] into it. The gesture following
is two-handed: one hand SHAPES the U?s left
branch and the other both the U?s right branch
and its rear bend linking up to the left branch
(stills Two-Handed-Prism-Segment-2A and 2B).
The STROKE overlaps with the words and closes
in the rear. The Follower copies the two-handed
2Annotation PREDICATES are written in capital letters.
Cf. also fn. 5.
3BEATS largely rest on supra-segmentals and would de-
mand a paper of their own.
town hall gesture of the Router in his acknowl-
edgement (still Two-Handed-Prism-Segment-3).
In other words: the Follower?s gesture is aligned
to the Router?s. Being copies of each other, the se-
mantics of the Router?s and the Follower?s gesture
can enter the common ground (cf. 7.4 and 8). In
the reconstruction we will use the translation with
the English word order standardised.4
4 A Short Introduction to PTT
Explanation of dialogue rests on three things:
making clear how the succession of speakers? con-
tributions emerges, stating what the impact of con-
tributions on speakers? minds is and specifying
how information is extracted incrementally from
the contributions. Turning to emerging struc-
ture, PTT assumes that participants perform (often
fragmentary) contributions, discourse units (DUs),
which are dynamic propositions (DRSs in the
sense of (Muskens, 1996)). They contain locu-
tionary acts, conversational events/dialogue acts
plus their propositional contents/DRSs. DUs may
be sub-propositional micro-conversational events.
Dialogue acts are either core speech acts or
grounding acts. Core speech acts can be related to
the present like assert, towards the past like accept
or towards the future like commit. Grounding acts
are acknowledge or repair (Traum 2009). Putting
the distinctions above to work, we obviously can
already model adjacency pairs. For the problems
at issue we do not need more, cf. (Dial 1).
Which attitudes are assumed in current PTT
and which changes of participants? minds are ac-
counted for? Agents can have individual and pri-
vate or common and public intentions. All sorts
of actions, verbal or domain ones, are as a rule
intended, at the outset of changes we have indi-
vidual intentions. Common intentions are for ex-
ample needed in order to explain completions and
repairs (Poesio and Rieser submitted a). Most of
the cooperation facts investigated in Clark (1996)
need common intentions, most prominently, the
intention to carry out a communicative task felic-
itously. Frequently, the vehicle for these types of
intentions are (partial) plans. Plans can also be in-
dividual or shared. In (Dial 1) for example, the
Router has an individual plan how to best map out
his ride and the intention to communicate it to the
4We will end up with a mixture of German gesture
and English wording here. However, for didactic purposes
(sketch the main ideas) this seems acceptable. Sometimes we
will simulate German constructions in English.
88
Follower. The Follower in turn intends to let the
Router control her beliefs. Both have the collec-
tive intention to enable the Follower to follow the
Router?s route. Information presupposed or gener-
ated is contained in the discourse situation which,
in PTT, is just a normal situation with objects and
events, i.e., a DRS.
Conversational participants have command over
information states. An information state is up-
dated whenever a new event is perceived, includ-
ing events such as sub-sentential utterances, and
non-verbal events such as gestures or nods. Hence
the possibility is already implemented in PTT to
model accumulation of information due to ges-
ture. Information common to the dialogue par-
ticipants can be considered as grounded by de-
fault. This assumption connects PTT with other
dialogue theories, for example Clark?s (cf. Clark
and Marshall, 1981, Clark and Schaefer, 1989)
and Traum?s (Traum 2009). Acknowledged in-
formation is at the heart of the grounding pro-
cess. What is grounded is mutually believed ce-
teris paribus. Therefore, grounded information is
part of the pragmatic machinery driving a dialogue
forward (Rieser 2009). Grounding acts are taken
as meta-discoursive devices and not included in
discourse units proper. Besides beliefs and inten-
tions we have obligations as mental attitudes. In
PTT every conversational action induces an obli-
gation on the participant indicated to address that
action.
Information states raise the question of how
changes of information are brought about on the
basic grammatical level, viz. the interpretation
of incrementally produced locutionary acts. The
grammar in which syntactic and semantic interpre-
tation is implemented is LTAG (Abeille? & Ram-
bow (eds), 2000). LTAG is a tree-grammar en-
coding syntactic projections which do the duty
of, say, HPSGs rules, principles and constraints.
Nodes and projecting leaves are decorated with se-
mantic information based on Compositional DRT
as developed in (Muskens, 1996, 2001). A spe-
cific trait of PTT is working with semantic non-
monotonicity at all compositional levels: PTT hy-
pothesizes that semantic computation is the result
of defeasible inferences over DRSs obtained con-
catenating updates of single contributions. These
default inference rules have the effect of seman-
tic composition rules. Due to the impact of inter-
preted LTAG one can say that PTT is well founded
in a bottom up fashion. Especially the default
mechanism of PTT is used to make it a workable
interface for speech and gesture (cf. 7.2 - 7.4).
5 Setting up the Speech-gesture
Interface: Typology and Partial
Ontology
As mentioned, this paper is based on the sys-
tematic annotation of SAGA carried out over the
years 2007-2009 (Rieser 2009). Like many ges-
ture researchers we assume that the semantic and
pragmatic centre of a gesture is its stroke. The
stroke overlaps as a rule with part of a com-
plex constituent, for example the head or the log-
ical subject. The range of speech-gesture over-
lap usually marks the functional position where
the gestures meaning has to be merged into the
speech content. Technically, the annotation is
an ELAN-grid. From the annotation, a set of
gesture types has been factored out in the fol-
lowing way (Rieser 2009). AGENCY5 is in-
stalled as a root feature dominating the role fea-
tures ROUTER and FOLLOWER. Next come the
Router?s and the Follower?s LEFT and RIGHT
HAND and BOTH their HANDS. HANDEDNESS
in turn is mapped onto single annotation fea-
tures like HANDSHAPE, WRISTMOVEMENT,
PATHOFWRISTMOVEMENT etc. Bundles of
features make feature CLUSTERs which yield
classes of objects like curved, straight etc. en-
tities. These build up SHAPES of different di-
mensions:6 ABSTRACT OBJECTs of 0 DIMEN-
SION and LINEs, one-dimensional entities of dif-
ferent curvature. Among the two dimensional
entities are LOCATIONs, RECTANGLEs, CIR-
CLEs7 etc. Then three dimensional sorts come
up: CUBOIDSs CYLINDERs, PRISMs and so on.
In the end we get COMPOSITEs of SHAPES,
for example a BENT LINE in a SPHERE, and
SEQUENCES OF COMPOSITEs.8 The central is-
sue of ?How does a gesture acquire meaning?? is
answered in the following way: A gesture type is
mapped onto a partial ontology description, a stip-
ulation encoding the content attributed to a gesture
by raters. As a rule, gesture content is underspec-
5Gesture types, organised in an inheritance hierarchy
working with defaults (cf. Rieser 2009), are written in CAP-
ITAL ITALICS.
6In the following geometry terms are used mnemonically.
7SHAPEs can in general be fully developed or come in
SEGMENTs. We do not deal with SEGMENTs here.
8SEQUENCES encode evolution of SHAPEs in time.
89
ified and will be completed to some extent when
interfacing with verbal meaning. As an example
of a gesture type and its partial ontology, see e.g.
TwoHandedPrismSegment1 and ?Partial Ontolo-
gyTwoHandedPrismSegment1? in Appendix A.
6 Setting up the Speech-gesture
Interface: Levels of Interaction
Our starting point is the hypothesis detailed in
(Rieser 2008) that a genuine understanding of di-
alogues like (Dial 1) requires integration of multi-
modal meaning at different levels of discourse,
from fine grained lexical definitions up to rhetor-
ical relations. In the rest of the paper, we will
specify how information from spoken utterances
merges with information from gestures, using
(Dial 1) as an example. Omitting the two BEATS
on that is [then], we have the following gestures
on the Router?s side (see stills in Appendix B):
6.1 the PRISM SEGMENT covering the town hall; cf. still
Two-Handed-Prism-Segment
6.2 the DRAWN U-shape overlapping the adjective U-
shaped; still One-Handed-U-Shape
6.3 the PRISM SEGMENT affiliated to [practically] look
into it; still Two-Handed-U-Shaped-Prism-Segment
6.4 the two-handed U-shaped PRISM SEGMENT going
with and closes in the rear; stills Two-Handed-Prism-
Segment-2A and 2B.
The Follower uses a variant of
6.5 the Router?s PRISM SEGMENT in (6.4) followed by
OK; still Two-Handed-Prism-Segment-3.
The key observation from Rieser (2009) is that
gestures interact with verbal contributions at dif-
ferent levels. (6.1) to (6.4) must be integrated at
the level of the semantic interpretation of LTAG.
(6.3) is involved since the stroke covers three con-
stituents in the German wording, the modal ad-
verb [practically], the pronoun it, and the separa-
ble prefix da rein/into of the verb blickst/you look.
We will develop a simplified solution here using
the ?verb? look-into. Similarly, in (6.4) the ges-
ture contains information relevant for closes in the
rear, i.e. for the whole VP. The gesture informa-
tion has to be integrated into the Router?s dialogue
acts at the interface points mentioned. Therefrom
several side issues arise, for example the treatment
of anaphora across Router?s or Follower?s contri-
butions. In (Dial 1) the Follower uses gestural
information only to acknowledge. It is a multi-
modal example of acknowledging by imitating the
Router?s multi-modal acts. Her gesture and the
OK form a kind of ?complex acknowledgement?.
This way the Router?s contributions (6.2) to (6.4)
and the Follower?s contribution (6.5) show the in-
teractive role of gesture, more specifically, gesture
content in its use for grounding. We will briefly
comment upon that in section 8.
7 Using PTT as an Interface for Verbal
Meaning and Gestural Meaning
7.1 The verbal part of (Dial 1)
According to PTT, the discourse situation after the
verbal updates brought about by (Dial 1) would be
as follows.9 (We only represent one aspect of the
content of the initial utterances of (Dial 1).):
[DU0, DU1, DU2, DU3, DU4, DU5 |
DU0 is [. . . K1, . . . |
K1 is [b1 | building(b1), large(b1)],
. . . ]
DU1 is [u2.1, K2, ce2.1 |
u2.1: utter(Router,?Das ist das Rathaus?),
sem(u2.1) is K2,
K2 is [th1, tnhl |
th1 is ?y1. K1; [ | y1 is b1],
tnhl is ?u. [ | town hall (u)],
th1 is tnhl,
ce2.1: assert(Router, Follower, K2),
generate(u2.1, ce2.1)],
DU2 is [u2.2, K4, ce2.2 |
u2.2: utter(Router, ?das ist ein
U-f?rmiges Geb?ude.?),
sem(u2.2) is K4,
K4 is [th2 | th2 is ?y2. K5; [ |s: y2 is b1],
building(th2), U-shaped(th2),
K5 is K1],
ce2.2: assert(Router, Follower, K4),
generate(u2.2, ce2.2)],
DU3 is [u2.3, K7, ce2.3|
u2.3: utter(Router, ?Du blickst da rein?),
sem(u2.3) is K7,
K7 is [th3, s1 | th3 is ?y3. K8; [|s: y3 is b1],
s1: look-into(Follower, th3),10
K8 is K4],
ce2.3: assert(Router, Follower, K7),
generate(u2.3, ce2.3)],
DU4 is [u2.4, K9, ce2.4|
u4: utter(Router, ?es hat vorne
zwei Buchtungen und geht hinten zus. dann?),
sem(u2.4) is K9,
K9 is [th4,bu1,bu2,s2,s3,s4,s5,s6,
re1,re2 |
th4 is ?y4. K10;
[ | y4 is th3], K10 is
K7,
bulge(bu1), bulge(bu2),
s2: has(th4, bu1),
9Abbreviations used in the PTT-fragment: The prefixes
are usually followed by a number n ? 0. DU = discourse
unit, ce = conversational event, K = DRS, u = utterance, sem
= semantic function, x, y, z . . . = DRs, e: event, s: = situation.
In the DRSs ?,? stands for conjunction an ?;? between DRSs
for composition of DRSs.
90
s3: has(th4, bu2),
to-front-of(bu1, th4),
to-front-of(bu2, th4),
rear(re1), s4: has(bu1,
re1), rear(re2)11,
s5:has(bu2, re2),
s6: meet(re1, re2)],
ce2.44: assert(Router, Follower, K9),
generate(u2.4, ce2.4)].
The model of anaphora resolution accounting for
the anaphoric cases is developed in (Poesio and
Rieser, submitted 2009 b). The anaphoric Das/this
in DU1 depends on the discourse entity a larger
building introduced at the beginning of the con-
versation in DRS K1: K1 is the resource situation
for the anaphoric definite. The second das/this
still depends on the same resource situation. The
pronouns, however, behave differently: Pronoun
da/there in DU3 takes up the antecedent a U-
shaped building, whereas the es/it in DU4 in turn
refers to the it in DU3. Observe that the verbal part
of (Dial 1) alone would already specify the inter-
pretation completely: nothing essential is missing.
As it will become clear below, what gestures do in
this example is to add details to the verbally deter-
mined models and restrict the model set.
7.2 Tying in Gestures with Utterances
What we have got so far is a PTT-representation
of the verbal part of (Dial 1). We now move on
to how the information coming from the Router?s
gestures gets integrated with the verbal informa-
tion ? in particular, how this integration can take
place below the sentential level. Our account
builds on two key ideas from PTT. First of all,
gestures are part of the discourse situation ? i.e.,
the occurrence of gestures is recorded in the infor-
mation state?s representation of the discourse sit-
uation. Second, every occurrence of a sentence
constituent counts as a conversational event ? a
MICRO CONVERSATIONAL EVENT (MCE).
With these assumptions in place, the interaction
of speech meaning and gesture meaning ? how
the two types of meanings combine to specify the
overall meaning of a contribution ? can be spec-
ified using the same mechanisms that specify the
meaning of MCEs: i.e., with (prioritized) defaults
in the sense of (Reiter, 1980, Brewka 1989). One
10Observe that the town-hall and the U-shaped building are
the same.
11Observe that the gesture dynamically shapes two rears
which meet.
example of a default specifying semantic com-
position is the BINARY SEMANTIC COMPO-
SITION (BSC) developed in (Poesio to appear,
Poesio and Rieser submitted a) to specify the de-
fault way in which MCEs meanings can be derived
from the meanings of their constituents. (We use
the notation > to indicate defeasible inference, ?
to indicate ?dominated by?.)
BSC: u1?u, u2?u, sem(u1) is ????? sem(u2) is
?? , complete(u,u1,u2)
> sem(u) is ?(? )
BSC can however be overridden in a number
of circumstances: most notably, when anaphora
interpretation processes identify a referent for a
definite description like uNP1: utter(?the build-
ing?), in which case sem(uNP1) will be the refer-
ent as opposed to a set of properties; or in cases
of metonymy such as those studied by Nunberg
(2004), in which the meaning of a MCE may be
derived even more indirectly. We hypothesize that
the integration of utterance meaning and gesture
meaning is specified by interface defaults that
may override the general meaning in a similar
way by enriching the normal meaning of MCEs.
We provide several examples of interface defaults
below. For reasons of space, we only specify
the results of default inference, without provid-
ing full derivations of the multi-modal meanings.
For the gestures only the semantics12 is speci-
fied, abstracted from the description of the par-
tial ontology (cf. Appendix A for details). Utter-
ance meaning then operates on the partial ontol-
ogy information. MM abbreviates ?multi-modal?;
?lex-entry? means the word-form at stake, ?lex-
definition? means an explict dictionary definition
for the word, for example in the style of the OED,
cast into PL1.
7.3 The Interface Defaults
The general heuristic strategy for setting up inter-
face defaults designed to combine verbal mean-
ing and gesture meaning is to probe into the PTT
structure as deep as you need in order to fit in the
gestural content properly. Gestures may be rele-
vant at any level of discourse, as shown in (Rieser,
2008) and demonstrated below; this means that
sometimes gestural content has to be stored ?deep
12This is due to the fact that we do not integrate gestures
into the discourse situation here. If these are integrated one
will use their type description as syntax in AVM format. Ges-
tures do not have the normal category syntax.
91
in? the lexical definition of a word, at other times
one has to remain on the top level of semantic
composition or even follow up the contributions
produced so far. The interface defaults mostly fol-
low the general schema:
? -prefix mentioning the open parameters + lex-
icon definition + open parameters applied to iconic
meaning = ? -abstracted partial ontology descrip-
tion where the ? -bound parameters secure bind-
ing.
An exception to this is (7.3.5.1) which uses the
notion of satisfaction (see stills in Appendix B).
7.3.1 The PRISM SEGMENT aligned with
[the] town hall (6.1). To begin with, gestural
meaning can enrich the meaning of a nominal
utterance. The interface default allowing this is
called Noun meaning extended (NMExt)13
NMExt: Noun(u), sem(u) is ?x lex-
definition(x), u?u?, N?(u?), u overlaps g,
gesture(g), iconic-meaning(g) is ?p partial
ontology(p)
>sem(u?) is ?x (lex-definition(x)) iconic-
meaning(g)(x)
For instance in the dialogue under consideration
lex-definition is the predicate ?large building used
for the administration of local government? abbre-
viated as ??P?x [[ |s: large building(x), used for
the administration of local government(x)]; P(x)]?
and the Partial Ontology TwoHandedPrismSeg-
ment1 from the Appendix A, resulting in the fol-
lowing meaning for the utterance of ?town hall?
accompanied by the gesture:
(7.3.1.1) ?x [ ls, rs, loc|s: large building(x), used
for the administration of local government(x),
side(ls, x), left(ls, Router), side(rs, x), right(rs,
Router), location(loc, x)]
Observe that the fine-grained local information is
provided by the gesture.
7.3.2 The DRAWN U-shape overlapping the ad-
jective U-shaped is an example of gesture enrich-
ing an adjectival meaning through the interface de-
fault Adjective meaning extended (AdjMExt)
AdjMExt: Adjective(u), sem(u) is ?P?x [|lex-
entry(x), P(x)], u?u?, N?(u?), u overlaps g, ges-
ture(g), iconic-meaning(g) is ?p partial ontol-
ogy(p)
> sem(u?) is ?P?Q?x([|lex-entry(x), P(x)];
Q(x)) iconic- meaning(g)(x).
13??p partial ontology (p)? in NMExt and the following
defaults is used in the following way: The expression ?partial
ontology? refers to information from the partial ontology list
in the Appendix A. What has to be chosen can be seen from
the application of the default below.
Using AdjMExt and the meaning of the gesture
OneHanded-U-shape in the Partial Ontology we
obtain (7.3.2.1) as an enriched meaning for ?U-
shaped?, ??? denoting mereological composition:
(7.3.2.1) ?Q?x([|U-shaped(x), ?us(strai- ght-
line(lr, us), arc(lb, us), straight-line(ll, us), us =
lr ? lb ? ll )(x)]; Q(x))
After fitting in the noun modified by the multi-
modal content into position ?Q?, the DRs will have
to be correctly bound.
Observe that we could apply (NMext) and (Ad-
jMext) iteratively to arrive at a complex MM
Nom-meaning.
7.3.3 The PRISM SEGMENT affiliated to
[practically] look into it is computed using
the interface default Verb meaning extended
(VMExt).
VMExt: VP(u), V(u1), NP(u2), u1? u, u2? u,
sem(u1) is ?P?x([|s: lex-definition(x), P(x)], u
overlaps g, gesture(g), iconic-meaning(g) is ?p
partial ontology(p)
> sem(u) is ?P?x([|s: lex-definition(x), P(x)])
iconic-meaning(g)(x)
VMExt gives us, again using the informa-
tion from the Partial Ontology TwoHanded-U-
shapedPrism from the Appendix:
(7.3.3.1) ?x([|s: focus(agent, x), space(x),
bounded(x), empty(x), ?p[hl, ls, lel, fs, hr, rs,
ler, d| prism(p), height(hl, ls), left-side(ls, p),
front-side(fs, p), left(ls, Router), height(hr, rs),
right-side(rs, p), length(ler, rs), right(rs, Router),
length(lel, ls), distance(d, ls, lr), lel = ler](x)])
Again we see that fine-grained information is
provided by the gesture, especially the prag-
matic anchoring of the space looked into from the
Router?s position.
7.3.4 Finally, the two-handed U-shaped PRISM
SEGMENT going with and closes in the rear
needs a default VP meaning extended (VPMex-
tended). The gesture information is distributed
among the verb ?closes? and the PP ?in the rear?,
the assumption being that the object closing does
so at a particular location which is part of the ob-
ject itself. So we have:
VPMExt: V(u) ? VP(uph1), P(u) ? PP(uph2),
Det(u) ? NP(uph3), Nom(uph4) ? NP(uph3),
PP(uph2) ? VP(uph1), sem(u) is ?P?x([|lex-
definition(x)]; P(x)), u overlaps g, gesture(g),
iconic-meaning(g) is ?p partial ontology(p)
> sem(uph1) = ?P?x([|lex-definition(x), P(x)];
iconic-meaning(g)(x)
The default using Appendix A, Partial On-
tology TwoHanded-U-shapedPrism, generates the
following MM meaning:
92
(7.3.4.1) ?x([ |s: close(x), at(s, loc), prism(leftp),
prism(rightp), part(leftp, x), part(rightp, x), sec-
tion(sectl, leftp), leftside(lefts, leftp), length(ll,
lefts), left(lefts, Router), section(sectr, rightp),
rightside(rights, rightp), frontside(fronts, rightp),
bent(rightp), meet(lefts, rights, loc), right(rights,
Router), parallel(lefts, rights), distance(d, lfts,
rhts)]).
7.3.5 The Follower?s U-shaped gesture: So far,
gesture meaning constrained word meanings or
constituent meanings. In contrast, the Follower?s
U-shaped gesture invades dialogue structure. The
Follower?s reply has two steps. Her iconic ges-
ture yields a predicate U-shaped in much the same
way as the Router?s contribution in DU2 and DU4
does. This is combined with a DR anaphorically
linked to the Router?s preceding its and thats. The
gesture in turn takes up the Router?s U-shapes
from DU2 and DU4. So we get an anaphora
related to antecedent multi-modal information.14
Her ?OK? then simply acknowledges her own
DU5 filled up. Acknowledgement of the Router?s
contributions is achieved indirectly. In order to
model all that, we have to Hook up the Gesture?s
Content with a DR. This is simply
(7.3.5.1) ?p(iconic-meaning(p))DR for some
preceding discourse referent DR satisfying
iconic-meaning.
The relevant iconic meaning is taken from Par-
tial Ontology TwoHandedPrismSegment3: sec-
tion(sect, p), leftpart(lftp, p), lengthl(lftp),
left(leftp, Follower), rightpart(rtp, p), right(rightp,
Follower), lengthr(rtp), lftp = rtp, p = lftp ? rtp.
7.4 A Gestural Dialogue Act of Assertion
Concerning dialogue structure, we have concen-
trated on the verbal part of (Dial 1) in 7.1. In the
SAGA corpus there are many data showing how
dialogue structure interfaces with gesture mean-
ing. In 7.3.5 a default for the follower?s U-shaped
gesture was given. Its embedding into the PTT-
description of (Dial 1) is shown in DU5 below:
(7.4) DU5 is [g1, K10|
g1: gesticulate(Follower, Router, U-shape),
sem(g1) is K10,
K10 is [ |s: th5 is th4, ?p(section(sect, p),
leftpart(lftp, p), lengthl(lftp),
left(leftp, Follower), rightpart(rtp, p),
right(rightp, Follower), lengthr(rtp),
lftp is rtp, p is lftp ? rtp)(th5))]
ce5: assert(Follower, Router, K10),
generate(g1, ce5)],],
14These anaphorical relations are not reconstructed here
but delegated to a follow-up paper.
[ce6, u6|
u6: utter(Follower,?OK?),
ce6: ack(Follower, DU5),
textbfgenerate(u6, ce6)]]
In the multi-modal dialogue passage we have
?gesticulate? instead of ?utter?. The semantics, us-
ing the default (7.3.5.1) ?Hook up the Gesture?s
Content with a DR? and material from Appendix
A is provided in the standard way by K10. It is as-
sumed that gestural content can be generated and
asserted. The Follower?s acknowledgement is a
sort of self-acknowledgement that percolates up
through anaphora.
8 Grounding by Gesture: a Genuine
Case of Gestural Alignment
The different defaults, Noun-meaning ex-
tended (NMextended), Adjective meaning
extended (AdjMextended), Verb meaning ex-
tended (VMextended), VP meaning extended
(VPMextended) and Hook up the Gesture?s
Content with a DR, clearly indicate that integra-
tion of gesture meaning has to operate on levels of
different grain. Gesture can operate on a sub-word
level if one has to attach its meaning to parts of a
lexical definition, on the word level, on the level
of constituents, and, as a consequence of all that,
on specific dialogue acts. Furthermore, we have
seen gesture at two inter-propositional levels at
work, at the interface among the contributions of
one agent (see Router?s contributions which are
all ?united? by communicating the appearance of
the town hall) and at the interface among contri-
butions of different agents (Router-Follower).The
Follower acknowledges by imitating gestures of
the Router; this is a genuine case of gestural align-
ment. Alternatively, she could also acknowledge
verbally, uttering ?U-shaped? but she chooses a
gestural content. Obviously, speakers think that
this works. Her ?OK? furthermore shows that
verbal and gestural means can work in tandem.
So, in the end, the U-shape of the town hall is
rooted in the common ground by default and the
Router can continue with describing the route
leading to the next landmark.
Acknowledgements
Support by the SFB 674, Bielefeld University,
is gratefully acknowledged. We also want to
than three anonymous reviewers for carful reading
93
and suggestions for improvement. Hannes Rieser
wants to thank Florian Hahn for common work on
gesture typology starting in 2007.
References
Abeill?, A & Rambow, O. (eds) 2004. Tree Adjoining
Grammars. CSLI Publ. Stanford, CA.
Bergmann, K., Fr?hlich, C., Hahn, F., Kopp, St., L?ck-
ing, A. and Rieser, H. June 2007. Wegbeschreibung-
sexperiment: Grobannotationsschema. Univ. Biele-
feld, MS.
Bergmann, K., Damm, O., Fr?hlich, C., Hahn, F.,
Kopp, St., L?cking, A., Rieser, H. and Thomas,
N. June 2008. Annotationsmanual zur Gestenmor-
phologie. Univ. Bielefeld, MS.
Brewka, G. 1989. Nonmonotonic reasoning: from the-
oretical foundation towards efficient computation.
Hamburg, Univ., Diss.
Clark, H. H. 1996. Using Language. Cambridge Uni-
versity Press.
Clark, H. H. and Marshall, C. R. 1981. Definite Ref-
erence and Mutual Knowledge. In A. K. Joshi, B.
Webber, and I. A. Sag (eds.),Elements of Discourse
Understanding. CUP
Clark, H. H. and Schaefer, E. F. 1989. Contributing to
Discourse. Cognitive Science, 13, 259-294.
Muskens, R. 1996. Combining Montague Semantics
and Discourse Representation. Linguistics and Phi-
losophy 19, pp. 143-186.
Muskens, R. 2001. Talking about Trees and Truth-
conditions. Journal of Logic, Language and Infor-
mation, 10(4), pp. 417-455.
Nunberg, G. 2004. The Pragmatics of Deferred Inter-
pretation. In: Horn, L.R. and Ward, G.: The Hand-
book of Pragmatics. Blackwell Publishing Ltd.
Poesio, M. to appear. Incrementality and underspec-
ification in semantic interpretation. CSLI Publica-
tions.
Poesio, M. February 2009. Grounding in PTT. Talk
given at Bielefeld Univ.
Poesio, M. and Rieser, H. submitted a. Completions,
Coordination, and Alignment in Dialogue.
Poesio, M. and Rieser, H. submitted b. Anaphora and
Direct Reference: Empirical Evidence from Point-
ing.
Poesio, M. and Traum, D. 1997. ?Conversational Ac-
tions and Discourse Situations?, Computational In-
telligence, v. 13, n.3, 1997, pp.1- 45.
Rieser, H. 2008. Aligned Iconic Gesture in Different
Strata of MM Route-description Dialogue. In Pro-
ceedings of LONdial 2008, pp. 167-174
Rieser, H. 2009. On Factoring out a Gesture Typology
from the Bielefeld Speech-And- Gesture-Alignment
Corpus. Talk given at the GW 2009, ZiF Bielefeld,
to appear in the Proceedings of GW 2009.
Traum, D. 2009. Computational Models of Ground-
ing for Human-Computer Dialogue. Talk given at
Bielefeld Univ., February 2009
94
Appendices
Appendix A: Gesture Types and Description of Partial Ontology
Due to limited space gesture types and ontology descriptions are only partially characterised.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TwoHandedPrismSegment 1
R.G.Left.HandShapeShape loose B spread
R.G.Left.HandPalmDirection PDN/PTR
R.G.Left.BackOfHandDirection BAB
R.G.Left.Practice grasping-indexing
R.G.Left.Perspective speaker
R.G.Right.HandShapeShape loose B spread
R.G.Right.HandPalmDirection PDN/PTL
R.G.Right.BackOfHandDirection BAB
R.G.Right.Practice grasping-indexing
R.G.Right.Perspective speaker
R.Two-handed-configuration TT
R.Movement-relative-to-other-hand 0
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Partial Ontology TwoHandedPrismSegment 1
R.G.Left.HandShapeShape-loose B spread side(ls, p)
R.G.Left.HandPalmDirection-PDN/PTR left(ls, Router)
R.G.Right.HandShapeShape-loose B spread side(rs, p)
R.G.Right.HandPalmDirection-PDN/PTL right(rs, Router)
R.Two-handed-configuration-TT location(loc, p)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
OneHanded-U-shape
R.G.Right.HandShapeShape G
R.G.Right.PalmDirection PDN/PTL>PDN/PTB>PDN
R.G.Right.BackOfHandDirection BAB/BTL>BAB/BDN>BAB/BDN/BTL
R.G.Right.PathOfWristLocation ARC
R.G.Right.WristLocationMovementDirectio MR>MF>ML
R.G.Right.Extent MEDIUM
R.G.Right.Practice drawing
R.G.Right.Pespective speaker
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Partial Ontology OneHanded-U-shape
R.G.Right.PathOfWristLocation-ARC U-shape(us)
R.G.Right.WristLocation straight-line(lr, us) ?
MovementDirection-MR>MF>ML bent-line(lb, us) ?
straight-line(ll, us)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TwoHandedPrismSegment 2
R.G.Left.HandShapeShape B spread
R.G.Left.HandPalmDirection PTR
R.G.Left.BackOfHandDirection BAB/BUP > BAB
R.G.Left.PathOfWristLocation LINE
R.G.Left.WristLocation MF
MovementDirection
R.G.Left.Practice shaping-modelling
R.G.Left.Perspective speaker
R.G.Right.HandShapeShape B spread
R.G.Right.HandPalmDirection PTL
R.G.Right.BackOfHandDirection BAB/BUP > BAB
R.G.Right.PathOfWristLocation LINE
R.G.Right.WristLocation MF
MovementDirection
R.G.Right.Practice shaping-modelling
R.G.Right.Perspective speaker
R.Two-handed-configuration PF
R.Movement-relative-to-other-hand SYNC
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Partial Ontology TwoHandedPrismSegment 2
R.G.Left.HandShapeShape-B spread hight(hl, ls)
R.G.Left.HandPalmDirection-PTR leftside(ls, p)
? prism(p)
R.G.Left.PathOfWristLocation-LINE length(lel, ls)
R.G.Left.WristLocation frontside(fs, p)
MovementDirection-MF
R.G.Left.Perspective-speaker left(ls, speaker)
R.G.Right.HandShapeShape-B spread hight(hr, rs)
R.G.Right.HandPalmDirection-PTL rightside(rs, p)
? prism(p)
R.G.Right.PathOfWristLocation-LINE length(ler, rs)
R.G.Right.WristLocation frontside(fs, p)
MovementDirection-MF
R.G.Right.Perspective-speaker right(rs, speaker)
R.Two-handed-configuration-PF distance(d, ls, lr)
R.Movement-relative-to-other-hand-SYNC lel = ler
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TwoHanded-U-shapedPrism
R.G.Left.HandShapeShape small C
R.G.Left.HandPalmDirection PAB
R.G.Left.BackOfHandDirection BAB/BTR
R.G.Left.PathOfWristLocation LINE
R.G.Left.WristLocation MF
MovementDirection
R.G.Left.Practice shaping
R.G.Left.Perspective speaker
R.G.Right.HandShapeShape small C
R.G.Right.HandPalmDirection PAB/PTL>
PTL>PTB/PTL
R.G.Right.BackOfHandDirection BAB/BTR>
BAB>BAB/BTL
R.G.Right.PathOfWristLocation LINE>LINE
R.G.Right.WristLocation MF>ML
MovementDirection
R.G.Right.Practice shaping
R.G.Right.Perspective speaker
R.Two-handed-configuration BHA
R.Movement-relative-to-other-hand SYNC
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Partial Ontology TwoHanded-U-shapedPrism
R.G.Left.HandShapeShape-small C section(sectl, leftp)
R.G.Left.PathOfWristLocation-LINE leftside(lefts, leftp)
R.G.Left.WristLocation length(ll, lefts)
MovementDirection -MF
R.G.Left.Perspective-speaker left(lefts, speaker)
R.G.Right.HandShapeShape-small section(sectr, rightp)
R.G.Right.PathOfWristLocation-LINE>LINE rightside(rights, rightp) ?
frontside(fronts, rightp)
R.G.Right.WristLocation>ML bent(rightp) ?
MovementDirection-MF meet(lefts, rights)
R.G.Right.Perspective-speaker right(rights, speaker)
R.Movement-relative-to-other-hand-SYNC parallel(lefts, rights) ?
distance(d, lefts, rights)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
95
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TwoHandedPrismSegment 3
R.G.Left.HandShapeShape C
R.G.Right.HandPalmDirection PDN/PTR>
PAB/PUP
R.G.Left.BackOfHandDirection BAB>
BTL/BUP
R.G.Left.PathOfWristLocation ARC
R.G.Left.WristLocationMovementDirection ML>MB
R.G.Left.Practice shaping
R.G.Left.Perspective speaker
R.G.Right.HandShapeShape C
R.G.Right.HandPalmDirection PDN/PTL>
PAB/PUP
R.G.Right.BackOfHandDirection BAB>
BTR/BUP
R.G.Right.PathOfWristLocation ARC
R.G.Right.WristLocationMovementDirection MR>MB
R.G.Right.Practice shaping
R.G.Right.Perspective speaker
R.Two-handed-configuration BHA
R.Movement-relative-to-other-hand Mirror-Sagittal
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Partial Ontology TwoHandedPrismSegment 3
R.G.Left.HandShapeShape section(sect, p)
R.G.Left.PathOfWristLocation leftpart(lftp, p)
R.G.Left.WristLocationMovementDirection lengthl(lftp)
R.G.Left.Perspective left(leftp, speaker)
R.G.Right.HandShapeShape section(sect, p)
R.G.Right.PathOfWristLocation rightpart(rtp, p)
R.G.Right.WristLocationMovementDirection lengthr(rtp)
R.G.Right.Perspective speaker
R.Two-handed-configuration lftp = rtp
R.Movement-relative-to-other-hand p = lftp ? rtp
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Appendix B: Figure 1
(a) The Router on his trip. (b) The site traversed by the
Router. The U-shaped building
is the town hall
(c) Fig. 1c shows the town hall
as described and gestured by
the Router.
(d) Two-Handed-Prism-
Segment-1
(e) One-Handed-U-Shape (f) Two-Handed-U-Shaped-
Prism-Segment
(g) Two-Handed-Prism-
Segment-2A
(h) Two-Handed-Prism-
Segment-2B
(i) Two-Handed-Prism-
Segment-3
Figure 1: The SAGA Setting
96
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 88?97,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
 
Regulating Dialogue with Gestures?Towards an Empirically Grounded 
Simulation with Conversational Agents 
 
Kirsten Bergmann1,2 Hannes Rieser1 Stefan Kopp1,2
 
1 Collaborative Research Center 673 ?Alignment in Communication?, Bielefeld University 
2 Center of Excellence ?Cognitive Interaction Technology?(CITEC), Bielefeld University 
   
{kbergman,skopp}@TechFak.Uni-Bielefeld.DE 
hannes.rieser@Uni-Bielefeld.DE 
 
 
 
Abstract 
Although not very well investigated, a crucial as-
pect of gesture use in dialogues is to regulate the 
organisation of the interaction. People use gestures 
decisively, for example to indicate that they want 
someone to take the turn, to 'brush away' what 
someone else said, or to acknowledge others' con-
tributions. We present first insights from a corpus-
based investigation of how gestures are used to 
regulate dialogue, and we provide first results from 
an account to capture these phenomena in agent-
based communication simulations. By advancing a 
model for autonomous gesture generation to also 
cover gesture interpretation, this account enables a 
full gesture turn exchange cycle of generation, un-
derstanding and acceptance/generation in virtual 
conversational agents. 
1 Motivation 
Research on gestures must combine empirical, 
theoretical and simulation methods to investigate 
form, content and function of gestures in relation 
to speech. Our work is based on a corpus of multi-
modal data, the Bielefeld Speech and Gesture 
Alignment corpus of route-description dialogues 
(SAGA corpus, L?cking et al 2010). The point of 
departure of our research has been work on iconic 
and deictic gestures over many years. In this paper 
we focus on a not very well investigated function 
of gestures which we have repeatedly observed in 
this corpus, namely, the regulation of dialogue.  
Most of current gesture research is oriented to-
wards the semiotics of a Peircean tradition as can  
for instance be seen from McNeill?s ?Kendon?s 
continuum? (McNeill 1992, p. 37). As a conse-
quence of this Peircian orientation, gestures have 
been viewed as single signs interfacing with 
speech. Going beyond the integration of in-
put/output modalities in single speech-gesture 
compositions (Johnston and Bangalore, 2005), lit-
tle effort has been spent on the investigation of 
sequences of gestures and speech-gesture composi-
tion both within and across speakers (Hahn and 
Rieser 2010, Rieser 2010). Furthermore, research 
of gesture meaning was restricted to the contribu-
tion of gesture content to propositional content. An 
exception to this research line has been the work of 
Bavelas et al (1992, 1995). It is characterised by 
two features, a functional perspective on gesture in 
opposition to purely classificatory and typological 
ones and an interest to systematically investigate 
the role of gesture in interaction. In particular, 
Bavelas et al (1992) proposed a distinction be-
tween ?topic gestures? and ?interactive gestures?: 
Topic gestures depict semantic information di-
rectly related to the topic of discourse, while inter-
active gestures refer to some aspect of the process 
of conversing with another person. Interactive ges-
tures include delivery gestures (e.g. marking in-
formation status as new, shared, digression), citing 
gestures (acknowledging others? prior contribu-
tions), seeking gestures (seeking agreement, or 
help in finding a word), and turn coordination ges-
 
88
 tures (e.g. taking or giving the turn). Gill et al 
(1999) noted similar functions of gesture use, add-
ing body movements to the repertoire of pragmatic 
acts used in dialogue act theory (e.g. turn-taking, 
grounding, acknowledgements).  
We aim to find out how gestures are related to and 
help regulate the structure of dialogue. We will call 
these gestures `discourse gestures?. Relevant re-
search questions in this respect are the following: 
How can gesture support next speaker selection if 
this follows regular turn distribution mechanisms 
such as current speaker selects next? From the dia-
logues in SAGA we know that averting next 
speaker?s self-selection is of similar importance as 
handing over the floor to the next speaker. So, how 
can averting self-selection of other be accom-
plished gesturally? A still different problem is how 
gesture is utilised to establish an epistemically 
transparent, reliable common ground, say a tight 
world of mutual belief. A precondition for that is 
how gesture can help to indicate a gesturer?s stance 
to the information he provides. Natural language 
has words to indicate degrees of confidence in in-
formation such as probably, seemingly, approxi-
mately, perhaps, believe, know, guess etc. Can ges-
tures acquire this function as well?  
All these issues can be synopsised as follows: How 
can gestures?apart from their manifest contribu-
tion to propositional content?be used to push the 
dialogue machinery forward? In our research, ges-
ture simulation and theory of speech-gesture inte-
gration are developed in tandem. Up to now, both  
have been tied to occurrences of single gestures 
and their embedding in dialogue acts. In this paper, 
we present first steps along both methodological 
strands to explore the use and function of gesture 
in dialogue. We start with an empirical perspective 
on discourse gestures in section 2. In section 3 we 
briefly describe our gesture simulation model 
which so far simulates gesture use employing the 
virtual agent MAX independent of discourse struc-
tures. Section 4 analyses a corpus example of a 
minimal discourse which is regulated mainly by 
gestures of the two interactants. This provides the 
basis for our proposed extension of the gesture 
generation approach to capture the discourse func-
tion of gestures as described in section 5. This ex-
tension will encompass a novel approach to em-
ploy the very generation model used for gesture 
production, and hence all the heuristic gesture 
knowledge it captures, also for gesture interpreta-
tion in dialogue. Section 6 discusses the difference 
between pure interactive gestures and discourse 
gestures and proposes further steps that need to be 
taken to elucidate how gestures are used as a vehi-
cle for regulating dialogue.  
2 Empirical Work on Discourse Gestures 
In looking for discourse gestures we started from 
the rated annotation of 6000 gestures in the SAGA 
corpus. We managed to annotate and rate about 
5000 of them according to traditional criteria using 
practices and fine-grained gesture morphology like 
hand-shape and wrist-movement. About 1000 ges-
tures could not be easily subsumed under the tradi-
tional gesture types (iconics, deictics, metaphorics, 
beats). Furthermore, they were observed to corre-
late with discourse properties such as current 
speaker?s producing his contribution or non-
regular interruption by other speaker.  
For purposes of the classification of the remaining 
1000 gestures we established the following func-
tional working definition: `Discourse gestures? are 
gestures tied up with properties or functions of 
agents? contributions in dialogue such as success-
fully producing current turn, establishing coher-
ence across different speakers? turns by gestural 
reference or indicating who will be next speaker.  
What did we use for dialogue structure? Being fa-
miliar with dialogue models such as SDRT (Asher 
and Lascarides, 2003), PTT (Poesio and Traum, 
1997), and KoS (Ginzburg, 2011) we soon found 
that these were too restricted to serve descriptive 
purposes. So we oriented our ?classification of dia-
logue gesture enterprise? on the well known turn 
taking organisation model of Sacks et al (1974) 
and Levinson?s (1983) discussion of it. However, it 
soon turned out that even these approaches were 
too normative for the SAGA data: This is due to 
the fact that dialogue participants develop enor-
mous creativity in establishing new rules of con-
tent production and of addressing violations of 
prima facie rules.  
Rules of turn-taking, for example, are not hard and 
fast rules, they can be skirted if the need arises, 
albeit there is a convention that this has to be ac-
knowledged and negotiated. A very clear example 
of an allowed interruption of an on-going produc-
tion is a quickly inserted clarification request serv-
ing the communicative goals of current speaker 
and the aims of the dialogue in general. Another 
 
89
 problem with the Sacks et al model consists in the 
following fact: Since its origination many dialogue 
regularities have been discovered which cannot be 
easily founded on a phenomenological or observa-
tional stratum which is essentially semantics-free. 
This can for example be seen from the develop-
ment of the notion of grounding and common 
ground as originally discussed by Stalnaker (1978), 
Clark (1996) and others. Nevertheless, grounding 
(roughly, coming to agree on the meaning of what 
has been said (see e.g. Traum, 1999; Roque and 
Traum, 2008;  Ginzburg 2011, ch. 4.2 for the op-
tions available) generates verbal structure and ver-
bal structure interfaces with gesture. Other exam-
ples in this class are acknowledgements or accepts 
discussed in more detail below. 
How did we decide on which distinctions of ges-
ture annotation have to be used for characterising 
discourse gestures? In other words, how did we 
conceive of the map between gestures of a certain 
sort and discourse structures? First of all we ob-
served that two types of discourse gestures emerge 
from the SAGA data. Some of them come with 
their own global shape and are close to emblems, 
(i.e. conveyors of stable meaning like the victory 
sign). This is true for example of the ?brush aside 
or brush away? gesture shown in Figure 1 (left), 
indicating a gesturer?s assessment of the down-
rated relevance of information, actions or situa-
tions. Discourse gestures of the second class ex-
ploit the means of, for instance, referring gestures 
or iconic gestures. An example of an iconic gesture 
in this role will be discussed to some extent in sec-
tion 4. Its simulation will be described in sections 3 
and 5.  
Here we explain the phenomenon with respect to 
referring pointing gestures which are easier to fig-
ure out (see Figure 1 (right)). Their usage as under 
focus here is not tied to the information under dis-
cussion but to objects in the immediate discourse 
situation, preferably to the participants of the dia-
logue. These uses have a Gricean flavour in the 
following way: Only considerations of relevance 
and co-occurrence with a turn transition relevance 
place together indicate that prima facie not general 
reference is at stake but indication of next speaker 
role. It wouldn?t make sense to point to the other 
person singling her or him out by indexing, be-
cause her or his identity is clear and well estab-
lished through the on-going interaction. Thus we 
see that a gestural device associated with estab-
lished morphological features, pointing, acquires a 
new function, namely indicating the role of next-
speaker. 
Figure 1: Examples of discourse gestures: the brush-away 
gesture (left) and situated pointing to the upper part of the 
interlocutor?s torso (right) used for next speaker selection in 
a ?Gricean? sense (see text for explanation). 
Now both classes of gestures, ?brush away? used 
to indicate informational or other non-relevance 
and pointing, indicating the role of being next 
speaker exploit the motor equipment of the hands. 
For this reason, annotation of discourse gestures 
can safely be based on the classification schemas 
we have developed for practices like indexing, 
shaping or modelling and for the fine-grained mo-
tor behaviour of the hands as exhibited by palm 
orientation, back-of-hand trajectory etc. In work by 
Hahn & Rieser (2009-2011) the following broad 
classes of discourse gestures were established. We 
briefly comment upon these classes of gestures 
found in the SAGA corpus relevant for dialogue 
structure and interaction:  
? Managing of own turn: A speaker may in-
dicate how successful he is in editing out his 
current production.  
? Mechanisms of next-speaker selection as 
proposed in classical CA research, for in-
stance, pointing to the other?s torso is often 
used as a means to indicate next speaker.  
? In grounding acts and feed-back especially 
iconic gestures are used to convey proposi-
tional content. 
? Clarification requests to work on contribu-
tions: An addressee may indicate the need 
for a quick interruption using a pointing to 
demand a clarification. In contrast, a current 
speaker can ward off the addressee?s incipi-
ent interruption using a palm-up gesture di-
 
90
 rected against the intruder thus setting up a 
?fence?.  
? Evidentials for establishing a confidence 
leve: There are fairly characteristic gestures 
indicating the confidence a speaker has in 
the information he is able to convey. 
? Handling of non-canonical moves by dis-
course participants: Interaction sequences 
consisting of attempts by other speaker to in-
terrupt and to thwart this intention by current 
speaker or to give way to it show how dis-
course participants handle non-canonical 
moves.  
? Assessment of relevance by discourse par-
ticipants: Speakers provide an assessment 
of which information is central and which 
one they want to consider as subsidiary. 
? An indication of topical information with 
respect to time, place or objects is fre-
quently given by pointing or by ?placing ob-
jects? into the gesture space.  
 
We know that this list is open and could, more-
over, depend on the corpus. In this paper the focus 
will be on grounding acts and feedback (see sec-
tions 3-5). The reason is that this way we can pro-
vide an extension of existing work on the simula-
tion of gesture production in a fairly direct manner. 
 
3 Simulating Gesture Use: The Genera-
tion Perspective 
Our starting point to simulate gestural behavior in 
dialogue is a gesture generation system which is 
able to simulate speaker-specific use of iconic ges-
tures given (1) a communicative intention, (2) dis-
course contextual information, and (3) an imagistic 
representation of the object to be described. Our 
approach is based on empirical evidence that 
iconic gesture production in humans is influenced 
by several factors. Apparently, iconic gestures 
communicate through iconicity, that is their physi-
cal form depicts object features such as shape or 
spatial properties. Recent findings indicate that a 
gesture?s form is also influenced by a number of 
contextual constraints such as information struc-
ture (see for instance Cassell and Prevost, 1996), or 
the use of more general gestural representation 
techniques such as shaping or drawing is decisive. 
In addition, inter-subjective differences in gestur-
ing are pertinent. There is, for example, wide vari-
ability in how much individuals gesture when they 
speak. Similarly, inter-subjective differences are 
found in preferences for particular representation 
techniques or low-level morphological features 
such as handshape or handedness (Bergmann & 
Figure 2: Schema of a gesture generation network in which 
gesture production choices are considered either 
probabilistically (chance nodes drawn as ovals) or rule-based 
(decision nodes drawn as rectangles). Each choice is 
depending on a number of contextual variables. The links are 
either learned from speaker-specific corpus data (dotted lines) 
or defined in a set of if-then rules (solid lines). 
 
 
Kopp, 2009).  
To meet the challenge of considering general and 
individual patterns in gesture use, we have pro-
posed GNetIc, a gesture net specialised for iconic 
gestures (Bergmann & Kopp, 2009a), in which we 
model the process of gesture formulation with 
Bayesian decision networks (BDNs) that supple-
ment standard Bayesian networks by decision 
nodes. This formalism provides a representation of 
a finite sequential decision problem, combining 
probabilistic and rule-based decision-making. Each 
decision to be made in the formation of an iconic 
gesture (e.g., whether or not to gesture at all or 
which representation technique to use) is repre-
sented in the network either as a decision node 
(rule-based) or as a chance node with a specific 
probability distribution. Factors which contribute 
to these choices (e.g., visuo-spatial referent fea-
tures) are taken as input to the model (see Figure 2) 
The structure of the network as well as local condi-
tional probability tables are learned from the 
SAGA corpus by means of automated machine 
 
 
91
 learning techniques and supplemented with rule-
based decision making. Individual as well as gen-
eral networks are learned from the SAGA corpus 
by means of automated machine learning tech-
niques and supplemented with rule-based decision 
making. So far, three different factors have been 
incorporated into this model: discourse context, the 
previously performed gesture, and features of the 
referent. The latter are extracted from a hierarchi-
cal representation called Imagistic Description 
Trees (IDT), which is designed to cover all deci-
sive visuo-spatial features of objects one finds in 
iconic gestures (Sowa & Wachsmuth, 2009). Each 
node in an IDT contains an imagistic description  
which holds a schema representing the shape of an 
object or object part. Features extracted from this 
representation in order to capture the main charac-
teristics of a gesture?s referent are whether an ob-
ject can be decomposed into detailed subparts 
(whole-part relations), whether it has any symmet-
rical axes, its main axis, its position in the VR 
stimulus, and its shape properties extracted on the 
 are not only present in the 
ly in terms of likeability, competence and 
communicative intent 
 describe the landmark townhall with respect to 
cular speaker) 
sulting in a posterior distribution of probabilities 
nique is decided to be ?drawing?, to be 
basis of so called multimodal concepts (see Berg-
mann & Kopp, 2008). 
Analyzing the GNetIc modelling results enabled us 
to gain novel insights into the production process 
of iconic gestures: the resulting networks for indi-
vidual speakers differ in their structure and in their 
conditional probability distributions, revealing that  
individual differences
overt gestures, but also in the production process 
they originate from.  
The GNetIc model has been extensively evaluated. 
First, in a prediction-based evaluation, the auto-
matically generated gestures were compared 
against their empirically observed counterparts, 
which yielded very promising results (Bergmann & 
Kopp, 2010). Second, we evaluated the GNetIc 
models in a perception-based evaluation study with 
human addressees. Results showed that GNetIc-
generated gestures actually helped to increase the 
perceived quality of object descriptions given by 
MAX. Moreover, gesturing behaviour generated 
with individual speaker networks was rated more 
positive
human-likeness (Bergmann, Kopp & Eyssel, 
2010). 
GNetIc gesture formulation has been embedded in 
a larger production architecture for speech and ges-
ture production. This architecture comprises mod-
ules that carry out content planning, formulation, 
and realisation for speech and gesture separately, 
but in close and systematic coordination (Berg-
mann & Kopp, 2009). To illustrate gesture genera-
tion on the basis of GNetIc models, consider the 
following example starting upon the arrival of a 
message which specifies the 
to
its characteristic properties:  
 
   lmDescrProperty (townhall-1). 
 
Based on this communicative intention, the imag-
istic description of the involved object gets acti-
vated and the agent adopts a spatial perspective 
towards it from which the object is to be described 
(see Figure 3). The representation is analyzed for 
referent features required by the GNetIc model: 
position, main axis, symmetry, number of subparts, 
and shape properties. Regarding the latter, a unifi-
cation of the imagistic townhall-1 representation 
and a set of underspecified shape property repre-
sentations (e.g. for ?longish?, ?round? etc.) reveals 
?U-shaped? as the most salient property to be de-
picted. All evidence available  (referent features, 
discourse context, previous gesture and linguistic 
context) is propagated through the network 
(learned from the data of  one parti
re
for the values in each chance node.  
 
 
Figure 3: The townhall in the virtual world (left) and sche-
matic of the corresponding IDT content (right); activated parts 
are marked. 
 
This way, it is first decided to generate a gesture in 
the current discourse situation at all, the represen-
ation techt
realized with both hands and the pointing hand-
shape ASL-G. Next, the model?s decision nodes 
are employed to decide on the palm and back of 
hand (BoH) orientation as well as movement type 
and direction: as typical in drawing gestures, the 
palm is oriented downwards and the BoH away 
from the speaker?s body. These gesture features are 
combined with a linear movement  consisting of 
two segments per hand (to the right and backwards 
with the right hand; accordingly mirror-
symmetrical with the left hand) to depict the shape 
of the townhall.  
Accompanying speech is generated from selected 
propositional facts using an NLG engine. Syn-
 
92
 chrony between speech and gesture follows co-
expressivity and is set to hold between the gesture 
 
would approach the town-hall and 
 initial 
sequent 
transition 
ore than a repetition of the word 
 
Router: Das ist dann das Rathaus [placing]. 
This is then the townhall [placing]. 
 Das ist ein u-f?rmiges Geb?ude [drawing].  
That is a U-shaped building [drawing]. 
 Du blickst praktisch da rein [shaping].Y
stroke (depicting the U-shape property) and corre-
sponding linguistic element. These values are used 
to fill the slots of a gesture feature matrix which is 
transformed into an XML representation to be real-
ized with the virtual agent MAX (see Figure 4).  
 
 
Figure 4: Specification (left) and realization (right) of an 
autonomously generated drawing gesture which depicts the U-
haped townhall. s
4 Example of a Minimal Discourse 
To start with the analysis of how gestures are not
only employed to carry referential content but also 
to regulate dialogue and discourse, we first present 
a datum from the SAGA corpus showing how the 
Follower?s gesture aligns with the Router?s gesture 
to indicate acknowledgement or accept. The situa-
tion is as follows: the Router describes to the Fol-
lower that he 
how it looks to him. A transcription of the
dialogue passage by the Router and the sub
crucial speech-gesture annotation, including the 
Follower, in ELAN looks as displayed in Figure 5 
(placing, drawing, and shaping are names of anno-
tated gestural representation techniques). 
A short comment on the data might be in order: 
When introducing the townhall as a U-shaped 
building, the Router draws the boundary of it, 
namely a ?U?. He then goes on to describe how the 
on-looker apprehends the building. This is accom-
panied by a forward-oriented direction gesture with 
both hands, mimicking into it. In principle, all the 
information necessary to identify the townhall 
from a front perspective is given by then. There is 
a short pause and we also have a turn 
relevance place here. However, there is no feed-
back by the Follower at this point. Therefore the 
Router selects a typical pattern for self-repairs or 
continuations in German, a that is construction in 
the guise of a propositional apposition. Overlap-
ping the production of kind, he produces a three-
dimensional partial U-shaped object maintaining 
the same perspective as in his first drawing of the 
U-shaped border.  
Observe that the Follower already gives feedback 
after front. The most decisive contribution is the 
Follower?s acknowledgement, however. She imi-
tates the Router?s gesture but from her perspective 
as a potential observer. Also, at the level of single 
form features, she performs the gesture differently. 
(different movement direction, different symmetry) 
The imitating gesture overlaps with her nod and 
her contribution OK. It is important to see that her 
gesture provides m
townhall could possibly give. It refers at the same 
time to the town-hall (standing for a discourse ref-
erent) and provides the information of a U-shape 
indicating property, in other words, it expresses the 
propositional information ?This building being U-
shaped? with this building acting as a definite 
anaphora to the occurrence of a building in the first 
part of the Router?s contribution. Hence, assessed 
from a dialogue perspective the following happens: 
The grounding process triggered by the Follower?s 
acknowledgement amounts to mutual belief among 
Router and Follower that the town hall is U-shaped 
and the approaching on-looker on the route per-
ceives it from the open side of the U. 
o
look practically there into it  [shaping]. 
 Das heisst, es hat vorne so zwei Buchtungen
That is, it has to the front kind of two bulges. 
 und geht hinten zusammen dann.and closes i
the rear then. 
Figure 5: Example showing the Router?s and the Fol-
lower?s gestures and their crucial exchange in terms of 
the Router?s assertion and the Follower?s acknowl-
edgement.  
 
93
 Figure 6: Overview of the production and understanding cycle in the simulation model.
 
5 Extending the Simulation: The Under-
standing-Acceptance/Generation Cycle 
How can we go beyond the simulation of isolated 
speaker-specific gestures towards the generation of 
gestures in dialogues? We build on our findings in 
 the corpus study, briefly taken up here again (see
list in section 2 and the respective comments): 
Gesture helps in structuring the dialogue support-
n
ment of the current speaker?s (Router?s or Fol-
 
re 5 (R1) and the sub-
e fact that the BDN 
-
ing next speaker selection or indicating non-regular 
co tributions of other speaker. It enables assess-
lower?s) communicative intentions by the ad-
dressee, for example of whether the Router wants 
to keep the turn but indicates current memory and 
recapitulation problems thus appealing to the ad-
dressee?s cooperation. In addition, appraisal of the 
reliability of the information given by the Router 
can be read off from some of the Router?s gestures. 
Finally, as shown in section 4, gestures comple-
menting or even replacing verbal information is 
used in acknowledgements. 
Building on these observations, our goal is to 
simulate such dialogic interaction with two virtual 
agents (Router and Follower), each of whom pro-
vided with a speaker-specific GNetIc model. In the 
minimal discourse example Router and Follower  
use similar gestures which, notably, differ with 
respect to some details (e.g. speaker?s perspective). 
In the simulation we essentially capture the 
Router?s contribution in Figu
sequent acknowledgement by the Follower (F1). In 
order to vary the Router?s gesturing behavior we 
use the representation technique of drawing instead 
of shaping in the simulation. 
What we need to extend the model with is an 
analysis of the Follower?s understanding of the 
Router?s gesture. Psychologically plausible but 
beyond commonly specialised technical ap-
proaches, we want to employ the same model of an 
agent?s ?gesture knowledge? for both generating 
and understanding gestures. For an overview of the 
production and understanding cycle see Figure 6.  
Here we can make use of th
formalism allows for two different types of infer-
ence, causal inferences that follow the causal inter 
actions from cause to effect, and diagnostic infer-
ences that allow for introducing evidence for ef-
fects and infer the most likely causes of these ef-
fects. This bi-directional use of BDNs could be 
complementary to approaches of plan/intention 
recognition such as in Geib and Goldman (2003). 
To model a use of gestures for regulation as ob-
served with the Follower F1, the Router agent?s 
gestural activity is set as evidence for the output 
nodes of the Follower?s BDN. A diagnostic infer-
ence then yields the most likely causes, that is, the 
most likely referent properties and values of dis-
course contextual variables. In other words, we 
employ the same speaker-specific GNetIc model 
for generation and for understanding. That is, in
formation about the physical appearance of the 
 
94
 Router?s gesture (as specified in Figure 4) is pro-
vided as evidence for the Follower?s GNetIc model 
revealing?correctly?that the gesture?s representa-
tion technique is ?drawing? and the shape property 
is ?U-shaped?.  
Notably, just as the gesture generation process has 
to make choices between similarly probable alter-
natives, not all diagnostic inferences which are 
drawn by employing the Follower agent?s GNetIc 
model are necessarily in line with the evidence 
from which the Router agent?s gesture was origi-
nally generated. For instance, the communicative 
goal as inferred by the Follower agent is 
?lmDescrPosition? (with a likelihood of .65) in-
simulate such iconic ges-
nts 
gue structure such as 
ext speaker selection or acknowledgement and 
outer?s 
e? 
posed in classical CA research 
back 
 participants 
stead of ?lmDescrProperty?. Nevertheless, the in-
ferred knowledge reveals an underspecified repre-
sentation of the referent (see Figure 7) as well as 
the most likely specification of the discourse con-
text. That way, the Follower agent develops his 
own hypothesis of the Router agent?s communica-
tive goal and the content being depicted gesturally.  
This hypothesis is forwarded to the follower 
agent?s dialogue manager, which responds to such 
declaratives by the Router with an acknowledge-
ment grounding act. Now the very same generation 
process as described in section 3 sets in. The Fol-
lower agent?s feedback is generated by employing 
his GNetIc model for causal inference. The result-
ing gesture is, notably, different from the Router 
agent?s gesture: it is a two-handed shaping gesture 
with handshape ASL-C. Movement type and 
movement features are the same as in the Router 
agent?s drawing gesture. Palm and BoH orientation 
are different due to representation technique spe-
cific patterns which are implemented in the deci-
sion nodes (see Figure 7). This case of using iconic 
gesture for regulating dialogue has been success-
fully implemented using GNetIc and the overall 
production architecture. 
6 Discussion and further research agenda 
In this paper we addressed the dialogue-regulating 
function of gestures. Based on empirical observa-
tions of interactional patterns from the SAGA cor-
pus, the starting points for the simulation of these 
gestures were non-interactional propositional ones 
such as iconics used to describe routes or land-
marks. We achieved to 
tures used in their function as acknowledgeme
shown in section 3 which clearly transcends their 
mere representational task. 
 
 
Figure 7: Imagistic representation of what the Follower un-
derstood from the Router?s gestural depiction of the townhall 
(left) and the simulation of the Follower?s autonomously gen-
erated shaping gesture used as an acknowledgement. 
 
We first note that we draw a distinction between 
gestures relevant for dialo
n
those which focus on influencing the social climate 
among the dialogue participants. We did not have 
many of the latter in SAGA but observed some 
which we classified as ?calming down? and ?don?t 
bother?. In certain communication cultures also 
touching the other?s body is accepted. 
As for a research agenda to elucidate further the 
functions of gestures in dialogue, we do not go too 
deeply into matters of dialogue theory here. We 
already have shown that gestures accompanying 
base-line information, being part of the R
report or the Follower?s uptake can be modelled in 
PTT (Poesio and Rieser 2009, Rieser and Poesio 
2009), if one assumes a unified representation for 
verbal and gestural meaning. Here we concentrate 
on how the simulation work can be pushed forward 
based on theoretical analyses of empirical data.  
Note that on the list of discourse gestures given in 
section 2 the following items are tied to Router?s 
behaviour and can be generated in an autonomous 
fashion: 
? managing of own turn 
? evidentials for establishing a confidence 
level 
? assessment of relevance by discourse par-
ticipants 
? indication of topicality with respect to time, 
place or objects.  
Observe, however, that these will also have an im-
pact on the mental state of the Follower as is e.g., 
obvious for evidentials or the ?brush away gestur
(Figure 1). Relevant for the sequencing of multi-
modal contributions are clearly the following: 
? mechanisms of next-speaker selection as 
pro
? grounding acts and feed
? handling of non-canonical moves by dis-
course
 
95
 ? clarification requests to work on contribu-
tions. 
Th
of adj ing a current and a next 
cep-
tan
this ki ation. 
Ac
Thi r
the
es-
ogue. Personality and 
lletin, 21(4):394?405 
 
 Kopp, S. (2009). Increasing expres-
siveness for virtual agents?Autonomous generation 
d gesture in spatial description tasks. In 
 
n-
n-
  
Ca
Cla
Ge
Gil
gy 
Gin
ress).  
Ha 9-2011): Dialogue Struc-
Ha ch-
ing Gesture 
Lev 983). Pragmatics. Cambridge Uni-
L?c
 M. Kipp et al (Eds.), 
Mc
Poe
nd et al (Eds.), Proceedings of the 13th Work-
Rie
and Wachsmuth (Eds.), 
Rie io, M. (2009). Interactive Gesture in 
Poe ordi-
, 1?89 
Ro  
d-
Sac
king 
Sta  
Sow 9). A computational 
guage and Dialogue, pages 132?
146. Oxford University Press. 
ese are intrinsically involved in the production 
acency pairs, hav
contribution and it is on these that simulation will 
focus on in future work. In combination with an 
information state-based multimodal discourse re-
cord (Traum & Larsson, 2003), the implementated 
cycle of generation, understanding and ac
ce/generation provides the basis for modeling 
nd of gesture-based discourse regul
knowledgments 
esearch is partially ss upported by the DFG in 
 CRC 673 ?Alignment in Communication? and 
the Center of Excellence ?Cognitive Interaction 
Technology?. 
References  
Asher, N. and Lascarides, A. (2003). The Logic of Con-
versation. Cambridge University Press 
Bavelas, J., Chovil, N., Lawrie, D., and Wade, A. 
(1992). Interactive gestures. Discourse Processes, 
15(4):469?491. 
Bavelas, J., Chovil N., Coated, L., Roe, L. (1995). G
tures Specialised for Dial
Social Psychology Bu
Bergmann, K., & Kopp, S. (2010). Modelling the Pro-
duction of Co-Verbal Iconic Gestures by Learning
Bayesian Decision Networks. Applied Artificial In-
telligence, 24(6):530?551. 
Bergmann, K. &
of speech an
Proceedings of AAMAS 2009, pages 361?368.  
Bergmann, K. & Kopp, S. (2009a). GNetIc?Using
Bayesian Decision Networks for iconic gesture ge
eration. In Proceedings of the 9th International Co
ference on Intelligent Virtual Agents, pages 76?89.
rgmann, K., KoppBe , S., and Eyssel, F. (2010). Indi-
vidualized gesturing outperforms average gesturing?
Evaluating gesture production in virtual humans. In 
Proceedings of IVA 2010, pages 104?117, Ber-
lin/Heidelberg. Springer.  
ssell, J. and S. Prevost (1996). Distribution of Seman-
tic Features Across Speech and Gesture by Humans 
and Computers. Proceedings of the Workshop on the 
Integration of Gesture in Language and Speech. 
rk, H.H. (1996). Using Language. CUP 
ib, C., Goldman, R.,(2003). Recognizing Plan/Goal 
Abandonment. In Proceedings of the International 
Joint Conference on Artificial Intelligence (IJCAI), 
pp. 1515?1517. 
l, S. P., Kawamori, M., Katagiri, Y., and Shimojima, 
A. (1999). Pragmatics of body moves. In Proceed-
ings of the 3rd International Cognitive Technolo
Conference, pages 345?358.  
zburg, J. (2011). The Interactive Stance. Meaning 
for Conversation. Oxford University Press (in p
hn, F. and Rieser, H. (200
ture Gestures and Interactive Gestures. Manual, 1st 
version. CRC 673 Working Paper. Bielefeld Univer-
sity 
hn, F. and Rieser, H. (2010): Explaining Spee
Gesture Alignment in MM Dialogue Us
Typology. In P. Lupowski and M. Purver (Eds.), As-
pects of Semantics and Pragmatics of Dialogue. 
SemDial 2010, pp. 99?111. 
inson, St. C. (1
versity Press. 
king, A., Bergmann, K., Hahn, F., Kopp, S., & Rie-
ser, H. (2010): The Bielefeld Speech and Gesture 
Alignment Corpus (SaGA). In
LREC 2010 Workshop: Multimodal Corpora. 
Neill, D. (1992). Hand and Mind. Chicago Univer-
sity Press. 
sio, M. & Rieser, H. (2009). Anaphora and Direct 
Reference: Empirical Evidence from Pointing. In J. 
Edlu
shop on the Semantics and Pragmatics of Dialogue 
(DiaHolmia) (pp. 35?43). Stockholm, Sweden. 
ser, H. (2010). On Factoring out a Gesture Typology 
from the Bielefeld Speech-And-Gesture-Alignment 
Corpus (SAGA). In Kopp 
Proceedings of GW 2009. Springer, pp. 47?61. 
ser, H. & Poes
Dialogue: a PTT Model. In P. Healey et al (Eds.), 
Proceedings of the SIGDIAL 2009 Conference (pp. 
87?96). London, UK: ACL. 
sio, M. and Rieser, H. (2010). Completions, co
nation and alignment in dialogue. Dialogue and Dis-
course 1(1)
Poesio, M. and Traum, D. (1997). Conversational ac-
tions and discourse situations. Computational Intel-
ligence, 13(3): 309?347 
que, A. and Traum, D. (2008). Degrees of Grounding
Based on Evidence of Understanding. In Procee
ings of the 9th SIGdial Workshop on Discourse and 
Dialogue, pp. 54?63 
ks, H., Schegloff, E., Jefferson, G. (1974). A sim-
plest systematics for the organization of turn-ta
for conversation. Language, 50: 696?735 
lnaker, R. (1978): Assertion. In Cole, P. (Ed.) Syntax
and Semantics 9: Pragmatics, pp. 315?322. 
a, T. and Wachsmuth, I. (200
model for the representation an processing of shape 
in coverbal iconic gestures. In K. Coventry et al 
(Eds.), Spatial Lan
 
96
 Traum, D. (1999). Computational models of groundin
in collaborative systems. In Working Notes of AAAI 
Fall Symposium on Psychol
g 
ogical Models of Com-
Tra e 
elt (Eds.), Current and New 
munication, pp. 124?131. 
um, D., & Larsson, S. (2003). The information stat
approach to dialogue management. In R.W. Smith 
and J.C.J. van Kuppev
Directions in Discourse & Dialogue (pp. 325?353). 
Kluwer Academic Publishers. 
 
97
Proceedings of the SIGDIAL 2013 Conference, pages 270?279,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Gesture Semantics Reconstruction Based on Motion Capturing and
Complex Event Processing: a Circular Shape Example
Thies Pfeiffer, Florian Hofmann
Artificial Intelligence Group
Faculty of Technology
Bielefeld University, Germany
(tpfeiffe|fhofmann)
@techfak.uni-bielefeld.de
Florian Hahn, Hannes Rieser, Insa Ro?pke
Collaborative Research Center
?Alignment in Communication? (CRC 673)
Bielefeld University, Germany
(fhahn2|hannes.rieser|iroepke)
@uni-bielefeld.de
Abstract
A fundamental problem in manual based
gesture semantics reconstruction is the
specification of preferred semantic con-
cepts for gesture trajectories. This issue
is complicated by problems human raters
have annotating fast-paced three dimen-
sional trajectories. Based on a detailed
example of a gesticulated circular trajec-
tory, we present a data-driven approach
that covers parts of the semantic recon-
struction by making use of motion captur-
ing (mocap) technology. In our FA3ME
framework we use a complex event pro-
cessing approach to analyse and annotate
multi-modal events. This framework pro-
vides grounds for a detailed description of
how to get at the semantic concept of cir-
cularity observed in the data.
1 Introduction
Focussing on iconic gestures, we discuss the ben-
efit of motion capturing (mocap) technology for
the reconstruction of gesture meaning and speech
meaning: A fundamental problem is the specifica-
tion of semantic concepts for gesture trajectories,
e.g., for describing circular movements or shapes.
We start with demonstrating the limitations of our
manual based annotation. Then we discuss two
strategies of how to deal with these, pragmatic in-
ference vs. low level annotation based on mocap
technology yielding a more precise semantics. We
then argue that the second strategy is to be pre-
ferred to the inferential one.
The annotation of mocap data can be re-
alised semi-automatically by our FA3ME frame-
work for the analysis and annotation of multi-
modal events, which we use to record multi-modal
corpora. For mocap we use the tracking sys-
tem ART DTrack2 (advanced realtime tracking
GmbH, 2013), but the framework is not restricted
to this technical set-up. In cooperation with others
(e.g., (Kousidis et al, 2012)), we also have used
products from Vicon Motion Systems (2013) and
the Microsoft Kinect (Microsoft, 2013). Pfeiffer
(2013) presents an overview on mocap technology
for documenting multi-modal studies.
We thus provide details about the way gestures
are analysed with FA3ME and about the procedure
to reconstruct the gesture meaning for the circular
movement in our chosen example. We conclude
with a discussion of how these low-level recon-
structions can be integrated into the reconstruction
of speech and gesture meaning.
2 From Linguistic Annotation to MoCap
In this section we describe our methodology for
the reconstruction of gesture meaning, speech
meaning and its interfacing, illustrated by an ex-
ample. We then show a shortcoming of our
corpus-based annotation and discuss two possible
solutions to amend it, pragmatic inference vs. se-
mantics based on mocap technology. The technol-
ogy described in Section 3 will in the end enable
us to get the preferred reconstruction of gesture se-
mantics.
The reconstruction of the gesture meaning and
its fusion with speech meaning to get a multi-
modal proposition works as follows: On the
speech side we start with a manual transcription,
upon which we craft a context free syntax analy-
sis followed by a formal semantics. On the ges-
ture side we build an AVM-based representation
of the gesture resting on manual annotation.1 Tak-
ing the gesture as a sign with independent mean-
ing (Rieser, 2010), this representation provides the
basis for the formal gesture semantics. In the next
1We do not use an explicit gesture model, which would
go against our descriptive intentions. The range of admissible
gestures is fixed by annotation manuals and investigations in
gesture typology.
270
Figure 1: Our example: a circular gesture (left:
video still) to describe the path around the pond
(right).
step, the gesture meaning and the speech meaning
are fused into an interface (Ro?pke et al, 2013).
Every step in these procedures is infested by un-
derspecification which, however, we do not deal
with here. These are, for instance, the selection
of annotation predicates, the attribution of logical
form to gestures and the speech analysis.
In our example, we focus on two gesture pa-
rameters, the movement feature of the gesture
and the representation technique used. It orig-
inates from the systematically annotated corpus,
called SaGA, the Bielefeld Speech-and-Gesture
Alignment-corpus (Lu?cking et al, 2012). It con-
sists of 25 dialogues of dyads conversing about a
?bus ride? through a Virtual Reality town. One
participant of each dyad, the so-called Route-
Giver (RG), has done this ride and describes the
route and the sights passed to the other participant,
the so-called Follower (FO). The taped conversa-
tions are annotated in a fine-grained way.
In the example, the RG describes a route section
around a pond to the FO. While uttering ?Du gehst
drei Viertel rum/You walk three quarters around?,
she produces the gesture depicted in Figure 1. In-
tuitively, the gesture conveys a circularity infor-
mation not expressed in the verbal meaning. In or-
der to explicate the relation of speech and gesture
meaning, we use our methodology as described
above. To anticipate, we get a clear contribution
of the speech meaning which is restricted by the
gesture meaning conveying the roundness infor-
mation. The first step is to provide a syntactical
analysis which you can see in Figure 2.2
2The gesture stroke extends over the whole utterance.
Verb phrases can feature so-called ?sentence brackets?. Here,
due to a sentence bracket, the finite verb stem ?gehst? is sepa-
rated from its prefix (?rum?). Together they embrace the Ger-
man Mittelfeld, here ?drei Viertel?. Observe the N-ellipsis
??? in the NP. The prefix and the finite verb stem cannot be
fully interpreted on their own and are therefore marked with
S
VP
VPref*
rum
around
VFin**
NP
N
?
Quant
N
Viertel
quarters
NUM
drei
three
VFin*
gehst
walk
NP
PN
Du
You
gesture stroke
Figure 2: Syntax analysis
The speech representation is inspired by a
Montague-Parsons-Reichenbach event ontology,
and uses type-logic notions. Ignoring the embed-
ding in an indirect speech act3, the speech se-
mantics represents an AGENT (the FO) who is
engaged in a WALK-AROUND event e related to
some path F, and a THEME relating the WALK-
AROUND event e with the path F.
?eyF 3/4x(WALK-AROUND(e) ?
AGENT(e, FO) ? THEME(e, x)
? F(x, y)) (1)
The gesture semantics is obtained using the an-
notated gesture features. The relevant features are
the movement of the wrist (Path of Wrist) and the
Representation Technique used.
?
??
Path of Wrist ARC<ARC<
ARC<ARC
Representation Technique Drawing
?
??
4
Interpreting the values ARC<ARC<
ARC<ARC and Drawing, respectively, the
calculated gesture semantics represents a bent
trajectory consisting of four segments:
an asterisk.
3We have treated the function of speech-gesture ensem-
bles in dialogue acts and dialogues elsewhere (Rieser and
Poesio (2009), Rieser (2011), Hahn and Rieser (2011),
Lu?ecking et al (2012)).
4This is a shortened version of the full gesture-AVM. Fea-
tures like hand shape etc. are ignored. See Rieser (2010) for
other annotation predicates.
271
?xy1y2y3y4(TRAJECTORY0(x) ? BENT(y1) ?
BENT(y2)? BENT(y3)? BENT(y4)?y1 < y2 < y3
< y4 ? SEGMENT(y1, x) ? SEGMENT(y2, x) ?
SEGMENT(y3, x) ? SEGMENT(y4, x)). (2)
The paraphrase is: There exists a TRAJECTORY0
x which consists of four BENT SEGMENTS y1, y2,
y3, y4. We abbreviate this formula to:
?x1(TRAJECTORY1(x1)) (3)
In more mundane verbiage: There is a particu-
lar TRAJECTORY1 x1. In a speech-gesture inter-
face5 (Rieser, 2010) both formulae are extended
by adding a parameter in order to compositionally
combine them:
?Y.?eyF 3/4x (WALK-AROUND(e) ?
AGENT(e, FO) ? THEME(e, x)
? F(x, y) ? Y(y)) (4)
We read this as: There is a WALK-AROUND event
e the AGENT of which is FO related to a three
quarters (path) F. This maps x onto y which is in
turn equipped with property Y.
?z.?x1(TRAJECTORY1(x1) ? x1 = z) (5)
This means ?There is a TRAJECTORY1 x1 identical
with an arbitrary z?. The extensions (4) and (5) are
based on the intuition that the preferred reading is
a modification of the (path) F by the gesture.
Taking the gesture representation as an argu-
ment for the speech representation, we finally get
a simplified multi-modal interface formula. The
resulting proposition represents an AGENT (FO)
who is engaged in a WALK-AROUND event e and a
THEME that now is specified as being related to
a bent trajectory of four arcs due to formula (2):
?ey 3/4x ?F(WALK-AROUND(e)?
AGENT(e, FO) ? THEME(e, x) ? F(x, y)
? TRAJECTORY1(y)) (6)
We take this to mean ?There is an AGENT FO?s
WALK-AROUND event e related to a three quarters
(path) F having a TRAJECTORY1 y?.
As a result, the set of models in which the
orginal speech proposition is true is restricted to
5How our model deals with interfacing speech meaning
and gesture meaning has been elaborated in a series of papers
(see footnote 3). We are well aware of the work on gesture-
speech integration by Lascarides and colleagues which we
deal with in a paper on interfaces (Rieser (2013)).
the set of models that contain a bent trajectory
standing in relation to the (path) F. But this restric-
tion is too weak. Intuitively, the gesture conveys
the meaning of a horizontal circular trajectory and
not just four bent arcs. To see the shortcoming,
note that the set of models also includes models
which include a path having four bends that do not
form a circular trajectory.
We envisage two methods to get the appropri-
ate circularity intuition: pragmatic enrichment and
an improvement of our gesture datum to capture
the additional information conveyed in the ges-
ture: By pragmatic enrichment, on the one hand,
horizontal orientation and circularity of the ges-
ture trajectory are inferred using abduction or de-
faults. However, the drawback of working with
defaults or abduction rules is that we would have
to set up too many of them depending on the vari-
ous shapes and functions of bent trajectories.
On the other hand, the datum can be improved
to yield a circularly shaped trajectory instead of
the weaker one consisting of four bent arcs. Our
motion capture data supports the second method:
The motion capture data allows us to compute the
complete trajectory drawn in the gesture space.
This will be the basis for producing a mapping
from gesture parameters to qualitative relations
which we need in the truth conditions. In the end,
we achieve a circular trajectory that is defined as
one approximating a circle, see Section 4.3.
In this mapping procedure resides an under-
specification, which is treated by fixing a thresh-
old for the application of qualitative predicates
through raters? decisions. This threshold value
will be used in giving truth conditions for, e.g.,
(11), especially for determining APPROXIMATE.
We prefer the second method since it captures
our hypothesis that the gesture as a sign conveys
the meaning circular trajectory. The gain of the
automated annotation via mocap which we will
see subsequently is an improvement of our orig-
inal gesture datum to a more empirically founded
one. As a consequence, the set of models that sat-
isfy our multi-modal proposition can be specified.
This is also the reason for explicitly focussing on
gesture semantics in this paper.
3 FA3ME - Automatic Annotation as
Complex Event Processing
The creation of FA3ME, our Framework for the
Automatic Annotation and Augmentation of Multi-
272
modal Events, is inter alia motivated by our key
insight from previous studies that human raters
have extreme difficulties when annotating 3D ges-
ture poses and trajectories. This is especially
true when they only have a restricted view on the
recorded gestures. A typical example is the restric-
tion to a fixed number of different camera angles
from which the gestures have been recorded. In
previous work (Pfeiffer, 2011), we proposed a so-
lution for the restricted camera perspectives based
on mocap data: Our Interactive Augmented Data
Explorer (IADE) allowed human raters to immerse
into the recorded data via virtual reality technol-
ogy. Using a 3D projection in a CAVE (Cruz-
Neira et al, 1992), the raters were enabled to
move freely around and through the recorded mo-
cap data, including a 3D reconstruction of the ex-
perimental setting. This interactive 3D visuali-
sation supported an advanced annotation process
and improved the quality of the annotations but
at high costs. Since then, we only know of Kipp
(2010) who makes mocap data visible for anno-
tators by presenting feature graphs in his annota-
tion tool Anvil in a desktop-based setting. In later
work, Nguyen and Kipp (2010) also support a 3D
model of the speaker, but this needed to be hand-
crafted by human annotators. A more holistic ap-
proach for gesture visualizations are the Gesture
Space Volumes Pfeiffer (2011), which summarize
gesture trajectories over longer timespans or mul-
tiple speakers.
The IADE system also allowed us to add visual
augmentations during the playback of the recorded
data. These augmentations were based on the
events from the mocap data, but aggregated sev-
eral events to higher-level representations. In a
study on pointing gestures (Lu?cking et al, 2013),
we could test different hypotheses about the con-
struction of the direction of pointing by adding
visual pointing rays shooting in a 3D reconstruc-
tion of the original real world setting. This al-
lowed us to asses the accuracy of pointing at a
very high level in a data-driven manner and derive
a new model for the direction of pointing (Pfeiffer,
2011).
3.1 Principles in FA3ME
In the FA3ME project, we iteratively refine our
methods for analysing multi-modal events. As a
central concept, FA3ME considers any recorded
datum as a first-level multi-modal event (see Fig-
ure 3, left). This can be a time-stamped frame
from a video camera, an audio sample, 6-degree-
of-freedom matrices from a mocap system or gaze
information from an eye-tracking system (e.g., see
Kousidis et al (2012)).
A distinctive factor of FA3ME is that we con-
sider annotations as second-level multi-modal
events. That is, recorded and annotated data
share the same representation. Annotations can be
added by both, human raters and classification al-
gorithms (the event rules in Figure 3, middle). An-
notations can themselves be target of annotations.
This allows us, for example, to create automatic
classifiers that rely on recorded data and manual
annotations (e.g., the first yellow event in Figure 3
depends on first-level events above and the blue
second-level event to the right). This is helpful
when classifiers for complex events are not (yet)
available. If, for instance, no automatic classifiers
for the stroke of a gesture exists, these annotations
can be made by human raters. Once this is done,
the automatic classifiers can describe the move-
ments during the meaningful phases by analysing
the trajectories of the mocap data.
Third-level multi-modal events are augmenta-
tions or extrapolations of the data. They might
represent hypotheses, such as in the example of
different pointing rays given above.
3.2 Complex Event Processing
In FA3ME, we consider the analysis of multi-
modal events as a complex event processing (CEP)
problem. CEP is an area of computer science ded-
icated to the timely detection, analysis, aggrega-
tion and processing of events (Luckham, 2002). In
the past years, CEP has gained an increased atten-
tion especially in the analysis of business relevant
processes where large amount of data, e.g., share
prices, with high update rates are analysed. This
has fostered many interesting tools and frame-
works for the analysis of structured events (Arasu
et al, 2004a; EsperTech, 2013; Gedik et al, 2008;
StreamBase, 2013). Hirte et al (2012) apply
CEP to a motion tracking stream from a Microsoft
Kinect for real-time interaction, but we know of
no uses of CEP for the processing of multi-modal
event streams for linguistic analysis.
Dedicated query languages have been devel-
oped by several CEP frameworks which allow us
to specify our event aggregations descriptively at
a high level of abstraction (Arasu et al, 2004b;
273
Figure 3: In FA3ME, incoming multi-modal events are handled by a complex event processing frame-
work that matches and aggregates events based on time windows to compose 2nd level multi-modal
events. All multi-modal events can then be mapped to tiers in an annotation tool.
Gedik et al, 2008). The framework we use for
FA3ME is Esper (EsperTech, 2013), which pro-
vides a SQL-like query language. As a central ex-
tension of SQL, CEP query languages introduce
the concept of event streams and time windows as
a basis for aggregation (see Figure 3).
The CEP approach of FA3ME allows us to cre-
ate second- and third-level multi-modal events on-
the-fly. We can thus provide near real-time anno-
tations of sensor events. However, we have to con-
sider the latencies introduced by sensors or com-
putations and back-date events accordingly.
As a practical result, once we have specified our
annotation descriptions formally in the language
of CEP, these descriptions can be used to create
classifiers that operate both on pre-recorded multi-
modal corpora and on real-time data. This makes
CEP interesting for projects where research in Lin-
guistics and Human-Computer Interaction meet.
4 From MoCap to Linguistic Models
In this section, we will now address the problem
of annotating the circular trajectory. In order to
get the preferred semantics we yet cannot rely ex-
clusively on the automatic annotation. We need
the qualitative predicate ?phase? to identify the
meaningful part of the gesture (the stroke). Addi-
tionally, the qualitative predicate ?representation
technique? is required to select the relevant mo-
cap trackers. For instance, the representation tech-
nique ?drawing? selects the marker of the tip of
the index finger. We thus require a hybrid model
of manual and automatic annotations. In the fol-
lowing, we will focus on the automatic annotation.
First of all, when using mocap to record data,
a frame of reference has to be specified as a ba-
Figure 4: The coordinate system of the speaker
(left). The orientations of the palms are classified
into eight main directions (right).
sis for all coordinate systems. We chose a person-
centered frame of reference anchored in the solar
plexus (see Figure 4). The coronal plane is de-
fined by the solar plexus and the two shoulders.
The transverse plane is also defined by the solar
plexus, perpendicular to the coronal plane with a
normal-vector from solar plexus to the point ST
(see Figure 4) between the two shoulders.
4.1 Basic Automatic Gesture Annotations
The analysis of mocap data allows us to create ba-
sic annotations that we use in our corpora on-the-
fly. This speeds up the annotation process and lets
human raters focus on more complex aspects. One
basic annotation that can be achieved automati-
cally is the classification of the position of gestur-
ing hands according to the gesture space model of
McNeill (1992). As his annotation schema (see
Figure 5, right) is tailored for the annotation of
274
Figure 5: Our extended gesture space categorisa-
tion (upper left) is based on the work of McNeill
(lower right).
video frames, we extended this model to support
mocap as well (see Figure 5, left). The important
point is that the areas of our schema are derived
from certain markers attached to the observed par-
ticipant. The upper right corner of the area C-UR
(Center-Upper Right), for example, is linked to the
marker for the right shoulder. Our schema thus
scales directly with the size of the participant. Be-
sides this, the sub-millimeter resolution of the mo-
cap system also allows us to have a more detailed
structure of the center area. The schema is also
oriented according to the current coronal plane of
the participant and not, e.g., according to the per-
spective of the camera.
A second example is the classification of the ori-
entation of the hand, which is classified according
to the scheme depicted in Figure 4, right. This
classification is made relative to the transversal
plane of the speaker?s body.
4.2 Example: The Circular Trajectory
For the detection and classification of gestures
drawing shapes two types of multi-modal events
are required. First, multi-modal events generated
by the mocap system for the hand. These events
contain matrices describing the position and ori-
entation of the back of the hand. Second, multi-
modal events that mark the gesture stroke (one
event for the start and one for the end) have to be
generated, either by hand or automatically. At the
moment, we rely on our manual annotations for
the existing SaGA corpus.
We realise the annotation of circular trajecto-
ries in two steps. First, we reduce the trajectory
provided by the mocap system to two dimensions.
Second, we determine how closely the 2D trajec-
tory approximates a circle.
Projection of the 3D Trajectory
The classifier for circles collects all events for the
hand that happened between the two events for the
stroke. As noted above, these events represent the
position and orientation of the hand in 3D-space.
There are several alternatives to reduce these three
dimensions to two for classifying a circle (a 3D
Object matching a 2D circle would be a sphere, a
circular trajectory through all 3 dimensions a spi-
ral). The principal approach is to reduce the di-
mensions by projecting the events on a 2D plane.
?xy (TRAJECTORY(x)? PROJECTION- OF(x, y)
? TRAJECTORY2D(y)) (7)
Which plane to chose depends on the choice
made for the annotation (e.g., global for the cor-
pus) and thus on the context. For the description
of gestures in dialogue there are several plausible
alternatives. First, the movements could be pro-
jected onto one of the three body planes (sagit-
tal plane, coronal plane, transversal plane). In our
context, the transversal plane is suitable, as we are
dealing with descriptions of routes, which in our
corpus are made either with respect to the body
of the speaker or with respect to the plane of an
imaginary map, both extend parallel to the floor.
Figure 6 (upper left) shows the circular movement
in the transversal plane. A different perspective
is presented in Figure 6 (right). There the perspec-
tive of a bystander is chosen. This kind of perspec-
tive can be useful for describing what the recipient
of a dialogue act perceives, e.g., to explain misun-
derstandings. For this purpose, the gesture could
also be annotated twice, once from the speaker?s
and once from the recipient?s perspective.
At this point we want to emphasise that posi-
tion and orientation of the planes do not have to be
static. They can be linked to the reference points
provided by the mocap system. Thus when the
speaker turns her body, the sagittal, coronal and
275
Figure 6: The circle-like gesture from our exam-
ple can be visualised based on the mocap data. The
right side shows the visualisation from the per-
spective of an interlocutor, the visualisation in the
upper left corner is a projection of the movement
on the transversal plane of the speaker.
transversal planes will move accordingly and the
gestures are always interpreted according to the
current orientation.
The plane used for projection can also be de-
rived from the gesture itself. Using principal com-
ponent analysis, the two main axes used by the
gesture can be identified. These axes can then have
arbitrary orientations. This could be a useful ap-
proach whenever 3D objects are described and the
correct position and orientation of the ideal circle
has to be derived from the gesture.
Circle Detection
Once the gesture trajectory has been projected
onto a 2D plane, the resulting coordinates are clas-
sified. For this, several sketch-recognition algo-
rithms have been proposed (e.g., (Alvarado and
Davis, 2004; Rubine, 1991)). These algorithms
have been designed for sketch-based interfaces
(such as tablets or digitisers), either for recognis-
ing commands or for prettifying hand-drawn dia-
grams. However, once the 3D trajectory has been
mapped to 2D, they can also be applied to natural
gestures. The individual sketch-recognition algo-
rithms differ in the way they are approaching the
classification problem. Many algorithms follow a
feature-based approach in which the primitives to
be recognised are described by a set of features
(such as aspect ratio or ratio of covered area) (Ru-
bine, 1991). This approach is especially suited,
when new primitives are to be learned by example.
An alternative approach is the model-based ap-
proach in which the primitives to be recognised are
described based on geometric models (Alvarado
and Davis, 2004; Hammond and Davis, 2006).
Some hybrid approaches also exist (Paulson et al,
2008). The model-based approaches are in line
with our declarative approach to modelling, and
are thus our preferred way for classifying shapes.
In our case, the projected 2D trajectory
of the gesture is thus classified by a model-
based sketch-recognition algorithm, which clas-
sifies the input into one of several shape classes
(circle, rectangle, ...) with a correspond-
ing member function ISSHAPE(y, CIRCLE) ?
[0 . . . 1]. By this, we can satisfy a subformula
APPROXIMATES(y, z) ? CIRCLE(z) by pre-setting
a certain threshold. The threshold has to be cho-
sen by the annotators, e.g., by rating positive and
negative examples, as it may vary between partic-
ipants and express the sloppiness of their gestures.
4.3 From MoCap to a Revision of Semantics
The result of the FA3ME reconstruction of our
gesture example can be expressed as follows:
?xyz (TRAJECTORY(x)
? PROJECTION- OF(x, y) ? TRAJECTORY2D(y)
? APPROXIMATES(y, z) ? CIRCLE(z)) (8)
So we have: There is a projection of TRAJEC-
TORY x, TRAJECTORY2D y, which is approximat-
ing a circle. We can now provide a description of
the domain which can satisfy formula (8). Conse-
quently, formula (8) is enhanced by definition (9).
CIRCULAR TRAJECTORY(x) =DEF
?yz(TRAJECTORY2(x)? PROJECTION- OF(x, y)?
APPROXIMATES(y, z) ? circle(z)) (9)
This definition reads as ?a CIRCULAR TRAJEC-
TORY x is a TRAJECTORY2 which has a PROJEC-
TION y that approximates some circle z?.
The formula (9) substitutes the TRAJECTORY1
notion. The improved multi-modal meaning is
(10):
?ey 3/4x ?F(WALK-AROUND(e)?
AGENT(e, FO) ? THEME(e, x) ? F(x, y)
? CIRCULAR TRAJECTORY(y)) (10)
Interfacing the new gesture representation with
the speech representation captures our intuition
that the gesture reduces the original set of mod-
els to a set including a circular-shaped trajectory.
276
Speech
Semantics
Gesture
Semantics
Linguistic Model
Classification
of Real World
Events
FA
3
ME
Preferred Models
Figure 7: Specification of gesture semantics due
to results of classification in FA3ME. Simulation
data feed into the gesture semantics which inter-
faces with the speech semantics.
The division of labour between linguistic seman-
tics and FA3ME technology regarding the seman-
tic reconstruction is represented in Figure 7.
By way of explanation: We have the multi-
modal semantics integrating speech semantics and
gesture semantics accomplished via ?-calculus
techniques as shown in Section 2. As also ex-
plained there, it alone would be too weak to de-
limit the models preferred with respect to the
gesture indicating roundness. Therefore FA3ME
technology leading to a definition of CIRCU-
LAR TRAJECTORY is used which reduces the set
of models to the preferred ones assuming a thresh-
old n for the gestures closeness of fit to a circle.
Thus, the relation between some gesture parame-
ters and qualitative relations like circular can be
considered as a mapping, producing values in the
range [0 . . . 1]. Still, it could happen that formula
(8) cannot be satisfied in the preferred models. As
a consequence, the multi-modal meaning would
then fall short of satisfaction.
5 Conclusion
During our work on the interface between speech
and gesture meaning our previous annotations
turned out to be insufficient to support the seman-
tics of concepts such as CIRCULAR TRAJECTORY.
This concept is a representative of many others
that for human annotators are difficult to rate with
the rigidity required for the symbolic level of se-
mantics. Scientific visualisations, such as depicted
in Figure 6, can be created to support the human
raters. However, there is still the problem of per-
spective distortions three dimensional gestures are
subject to when viewed from different angles and
in particular when viewed on a 2D screen. It is
also difficult to follow the complete trajectory of
such gestures over time. Thus, one and the same
gesture can be rated differently depending on the
rater, while an algorithm with a defined threshold
is not subject to these problems.
The presented hybrid approach based on quali-
tative human annotations, mocap and our FA3ME
framework is able to classify the particular 2D tra-
jectories we are interested in following a three-
step process: After the human annotator identi-
fied the phase and selected relevant trackers, the
dimensions are reduced to two and a rigid model-
based sketch-recognition algorithm is used to clas-
sify the trajectories. This classification is re-
peatable, consistent and independent of perspec-
tive. A first comparison of the manually anno-
tated data and the automatic annotations revealed
a high match. All differences between the annota-
tions can be explained by restrictions of the video
data which yielded a lower precision in the hu-
man annotations specifying the slant of the hand.
Thus, the main issues we had with the results
of human raters have been addressed, however a
more formal evaluation on a large corpus remains
to be done. What also remains is a specification
of membership functions for each kind of ges-
ture trajectories of interest (e.g., circular, rectan-
gular, etc.). For this, a formal specification of what
we commonly mean by, for instance, CIRCULAR,
RECTANGULAR etc. is required.
The automated annotation via mocap im-
proves our original gesture datum to capture the
circularity-information conveyed in the gesture.
We have a better understanding of the gesture
meaning adopted vis-a`-vis the datum considered.
As it turns out, resorting to pragmatic inference
cannot be avoided entirely, but we will exclude
a lot of unwarranted readings which the manual-
based logical formulae would still allow by us-
ing the approximation provided by body tracking
methods. Not presented here is the way third-level
multi-modal events are generated by re-simulating
the data in a 3D world model to generate context
events, e.g., to support pragmatics.
Acknowledgments
This work has been funded by the Deutsche
Forschungsgemeinschaft (DFG) in the Collabora-
tive Research Center 673, Alignment in Communi-
cation. We are grateful to three reviewers whose
arguments we took up in this version.
277
References
[advanced realtime tracking GmbH2013] A.R.T.
advanced realtime tracking GmbH. 2013.
Homepage. Retrieved May 2013 from
http://www.ar-tracking.de.
[Alvarado and Davis2004] Christine Alvarado and
Randall Davis. 2004. SketchREAD: a multi-
domain sketch recognition engine. In Proceedings
of the 17th annual ACM symposium on User
interface software and technology, UIST ?04, pages
23?32, New York, NY, USA. ACM.
[Arasu et al2004a] Arvind Arasu, Brian Babcock,
Shivnath Babu, John Cieslewicz, Mayur Datar,
Keith Ito, Rajeev Motwani, Utkarsh Srivastava, and
Jennifer Widom. 2004a. Stream: The stanford data
stream management system. Technical report, Stan-
ford InfoLab.
[Arasu et al2004b] Arvind Arasu, Shivnath Babu, and
Jennifer Widom. 2004b. CQL: A language for
continuous queries over streams and relations. In
Database Programming Languages, pages 1?19.
Springer.
[Cruz-Neira et al1992] Carolina Cruz-Neira, Daniel J.
Sandin, Thomas A. DeFanti, Robert V. Kenyon, and
John C. Hart. 1992. The cave: audio visual expe-
rience automatic virtual environment. Communica-
tions fo the ACM 35 (2), 35(6):64?72.
[EsperTech2013] EsperTech. 2013. Homepage of Es-
per. Retrieved May 2013 from http://esper.
codehaus.org/.
[Gedik et al2008] Bugra Gedik, Henrique Andrade,
Kun-Lung Wu, Philip S Yu, and Myungcheol Doo.
2008. SPADE: The System S Declarative Stream
Processing Engine. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1123?1134. ACM.
[Hahn and Rieser2011] Florian Hahn and Hannes
Rieser. 2011. Gestures supporting dialogue struc-
ture and interaction in the Bielefeld speech and
gesture alignment corpus (SaGA). In Proceedings
of SEMdial 2011, Los Angelogue, 15th Workshop on
the Semantics and Pragmatics of Dialogue, pages
182?183, Los Angeles, California.
[Hammond and Davis2006] Tracy Hammond and Ran-
dall Davis. 2006. LADDER: A language to describe
drawing, display, and editing in sketch recognition.
In ACM SIGGRAPH 2006 Courses, page 27. ACM.
[Hirte et al2012] Steffen Hirte, Andreas Seifert,
Stephan Baumann, Daniel Klan, and Kai-Uwe
Sattler. 2012. Data3 ? a kinect interface for OLAP
using complex event processing. Data Engineering,
International Conference on, 0:1297?1300.
[Kipp2010] Michael Kipp. 2010. Multimedia annota-
tion, querying and analysis in anvil. Multimedia in-
formation extraction, 19.
[Kousidis et al2012] Spyridon Kousidis, Thies Pfeiffer,
Zofia Malisz, Petra Wagner, and David Schlangen.
2012. Evaluating a minimally invasive labora-
tory architecture for recording multimodal conversa-
tional data. In Proceedings of the Interdisciplinary
Workshop on Feedback Behaviors in Dialog, IN-
TERSPEECH2012 Satellite Workshop, pages 39?42.
[Luckham2002] David Luckham. 2002. The Power
of Events: An Introduction to Complex Event Pro-
cessing in Distributed Enterprise Systems. Addison-
Wesley Professional.
[Lu?cking et al2012] Andy Lu?cking, Kirsten Bergman,
Florian Hahn, Stefan Kopp, and Hannes Rieser.
2012. Data-based analysis of speech and gesture:
the Bielefeld Speech and Gesture Alignment corpus
(SaGA) and its applications. Journal on Multimodal
User Interfaces, -:1?14.
[Lu?cking et al2013] Andy Lu?cking, Thies Pfeiffer, and
Hannes Rieser. 2013. Pointing and reference recon-
sidered. International Journal of Corpus Linguis-
tics. to appear.
[McNeill1992] David McNeill. 1992. Hand and Mind:
What Gestures Reveal about Thought. University of
Chicago Press, Chicago.
[Microsoft2013] Microsoft. 2013. Homepage of
KINECT for Windows. Retrieved May 2013
from http://www.microsoft.com/en-us/
kinectforwindows/develop/.
[Nguyen and Kipp2010] Quan Nguyen and Michael
Kipp. 2010. Annotation of human gesture using
3d skeleton controls. In Proceedings of the 7th In-
ternational Conference on Language Resources and
Evaluation. ELDA.
[Paulson et al2008] Brandon Paulson, Pankaj Rajan,
Pedro Davalos, Ricardo Gutierrez-Osuna, and Tracy
Hammond. 2008. What!?! no Rubine features?: us-
ing geometric-based features to produce normalized
confidence values for sketch recognition. In HCC
Workshop: Sketch Tools for Diagramming, pages
57?63.
[Pfeiffer2011] Thies Pfeiffer. 2011. Understanding
Multimodal Deixis with Gaze and Gesture in Con-
versational Interfaces. Berichte aus der Informatik.
Shaker Verlag, Aachen, Germany, December.
[Pfeiffer2013] Thies Pfeiffer. 2013. Documentation
with motion capture. In Cornelia Mu?ller, Alan
Cienki, Ellen Fricke, Silva H. Ladewig, David
McNeill, and Sedinha Teendorf, editors, Body-
Language-Communication: An International Hand-
book on Multimodality in Human Interaction, Hand-
books of Linguistics and Communication Science.
Mouton de Gruyter, Berlin, New York. to appear.
[Rieser and Poesio2009] Hannes Rieser and M. Poe-
sio. 2009. Interactive Gesture in Dialogue: a
PTT Model. In P. Healey, R. Pieraccini, D. Byron,
S. Yound, and M. Purver, editors, Proceedings of the
SIGDIAL 2009 Conference, pages 87?96.
278
[Rieser2010] Hannes Rieser. 2010. On factoring
out a gesture typology from the Bielefeld Speech-
And-Gesture-Alignment corpus (SAGA). In Ste-
fan Kopp and Ipke Wachsmuth, editors, Proceed-
ings of GW 2009: Gesture in Embodied Communi-
cation and Human-Computer Interaction, pages 47?
60, Berlin/Heidelberg. Springer.
[Rieser2011] Hannes Rieser. 2011. Gestures indicat-
ing dialogue structure. In Proceedings of SEMdial
2011, Los Angelogue, 15th Workshop on the Seman-
tics and Pragmatics of Dialogue, pages 9?18, Los
Angeles, California.
[Rieser2013] Hannes Rieser. 2013. Speech-gesture
Interfaces. An Overview. In Heike Wiese and
Malte Zimmermann, editors, Proceedings of 35th
Annual Conference of the German Linguistic Society
(DGfS), March 12-15 2013 in Potsdam, pages 282?
283.
[Ro?pke et al2013] Insa Ro?pke, Florian Hahn, and
Hannes Rieser. 2013. Interface Constructions for
Gestures Accompanying Verb Phrases. In Heike
Wiese and Malte Zimmermann, editors, Proceed-
ings of 35th Annual Conference of the German Lin-
guistic Society (DGfS), March 12-15 2013 in Pots-
dam, pages 295?296.
[Rubine1991] Dean Rubine. 1991. Specifying gestures
by example. In Proceedings of the 18th annual con-
ference on Computer graphics and interactive tech-
niques, SIGGRAPH ?91, pages 329?337, New York,
NY, USA. ACM.
[StreamBase2013] StreamBase. 2013. Homepage of
StreamBase. Retrieved May 2013 from http://
www.streambase.com/.
[Vicon Motion Systems2013] Vicon Motion Systems.
2013. Homepage. Retrieved May 2013 from
http://www.vicon.com.
279
