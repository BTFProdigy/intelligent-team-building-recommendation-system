Proceedings of NAACL-HLT 2013, pages 649?654,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Phrase Training Based Adaptation for Statistical Machine Translation
Saab Mansour and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department
RWTH Aachen University, Aachen, Germany
{mansour,ney}@cs.rwth-aachen.de
Abstract
We present a novel approach for translation
model (TM) adaptation using phrase train-
ing. The proposed adaptation procedure is ini-
tialized with a standard general-domain TM,
which is then used to perform phrase training
on a smaller in-domain set. This way, we bias
the probabilities of the general TM towards
the in-domain distribution. Experimental re-
sults on two different lectures translation tasks
show significant improvements of the adapted
systems over the general ones. Additionally,
we compare our results to mixture modeling,
where we report gains when using the sug-
gested phrase training adaptation method.
1 Introduction
The task of domain-adaptation attempts to exploit
data mainly drawn from one domain (e.g. news,
parliamentary discussion) to maximize the perfor-
mance on the test domain (e.g. lectures, web fo-
rums). In this work, we focus on translation model
(TM) adaptation. A prominent approach in recent
work is weighting at different levels of granularity.
Foster and Kuhn (2007) perform weighting at the
corpus level, where different corpora receive differ-
ent weights and are then combined using mixture
modeling. A finer grained weighting is that of Mat-
soukas et al (2009), who weight each sentence in the
bitexts using features of meta-information and opti-
mize a mapping from the feature vectors to weights
using a translation quality measure.
In this work, we propose to perform TM adapta-
tion using phrase training. We start from a general-
domain phrase table and adapt the probabilities by
training on an in-domain data. Thus, we achieve
direct phrase probabilities adaptation as opposed to
weighting. Foster et al (2010) perform weighting
at the phrase level, assigning each phrase pair a
weight according to its relevance to the test domain.
They compare phrase weighting to a ?flat? model,
where the weight directly approximates the phrase
probability. In their experiments, the weighting
method performs better than the flat model, there-
fore, they conclude that retaining the original rela-
tive frequency probabilities of the TM is important
for good performance. The ?flat? model of Foster
et al (2010) is similar to our work. We differ in
the following points: (i) we use the same procedure
to perform the phrase training based adaptation and
the search thus avoiding inconsistencies between the
two; (ii) we do not directly interpolate the original
statistics with the new ones, but use a training pro-
cedure to manipulate the original statistics. We per-
form experiments on the publicly available IWSLT
TED task, on both Arabic-to-English and German-
to-English lectures translation tracks. We compare
our suggested phrase training adaptation method to
a variety of baselines and show its effectiveness. Fi-
nally, we experiment with mixture modeling based
adaptation. We compare mixture modeling to our
adaptation method, and apply our method within a
mixture modeling framework.
In Section 2, we present the phrase training
method and explain how it is utilized for adaptation.
Experimental setup including corpora statistics and
the SMT system are described in Section 3. Sec-
tion 4 summarizes the phrase training adaptation re-
sults ending with a comparison to mixture modeling.
649
2 Phrase Training
The standard phrase extraction procedure in SMT
consists of two phases: (i) word-alignment training
(e.g., IBM alignment models), (ii) heuristic phrase
extraction and relative frequency based phrase trans-
lation probability estimation. In this work, we utilize
phrase training for the task of adaptation. We use
the forced alignment (FA) method (Wuebker et al,
2010) to perform the phrase alignment training and
probability estimation. We perform phrase training
by running a normal SMT decoder on the training
data and constrain the translation to the given target
instance. Using n-best possible phrase segmentation
for each training instance, the phrase probabilities
are re-estimated over the output. Leaving-one-out is
used during the forced alignment procedure phase to
avoid over-fitting (Wuebker et al, 2010).
In the standard phrase training procedure, we
are given a training set y, from which an initial
heuristics-based phrase table p0y is generated. FA
training is then done over the training set y using the
phrases and probabilities in p0y (possibly updated by
the leaving-one-out method). Finally, re-estimation
of the phrase probabilities is done over the decoder
output, generating the FA phrase table p1. We ex-
plain next how to utilize FA training for adaptation.
2.1 Adaptation
In this work, we utilize phrase training for the task
of adaptation. The main idea is to generate the initial
phrase table required for FA using a general-domain
training data y?, thus resulting in p0y? , and perform
the FA training over yIN , the in-domain training
data (instead of y? in the standard procedure). This
way, we bias the probabilities of p0y? towards the in-
domain distribution. We denote this new procedure
by Y?-FA-IN. This differs from the standard IN-FA-
IN by that we have more phrase pairs to use for FA.
Thus, we obtain phrase pairs relevant to IN in ad-
dition to ?general? phrase pairs which were not ex-
tracted from IN, perhaps due to faulty word align-
ments. The probabilities of the general phrase table
will be tailored towards IN. In practice, we usually
have in-domain IN and other-domain OD data. We
denote by ALL the concatenation of IN and OD. To
adapt the ALL phrase table, we perform the FA pro-
cedure ALL-FA-IN. We also utilize leaving-one-out
to avoid over-fitting.
Another procedure we experimented with is
adapting the OD phrase table using FA over IN,
without leaving-one-out. We denote it by OD-FA0-
IN. In this FA scenario, we do not use leaving-one-
out as IN is not contained in OD, therefore, over-
fitting will not occur. By this procedure, we train
phrases from OD that are relevant for both OD and
IN, while the probabilities will be tailored to IN. In
this case, we do not expect improvements over the
IN based phrase table, but, improvements over OD
and reduction in the phrase table size.
We compare our suggested FA based adaptation
to the standard FA procedure.
3 Experimental Setup
3.1 Training Corpora
To evaluate the introduced methods experimentally,
we use the IWSLT 2011 TED Arabic-to-English and
German-to-English translation tasks. The IWSLT
2011 evaluation campaign focuses on the transla-
tion of TED talks, a collection of lectures on a
variety of topics ranging from science to culture.
For Arabic-to-English, the bilingual data consists
of roughly 100K sentences of in-domain TED talks
data and 8M sentences of ?other?-domain United
Nations (UN) data. For the German-to-English task,
the data consists of 130K TED sentences and 2.1M
sentences of ?other?-domain data assembled from
the news-commentary and the europarl corpora. For
language model training purposes, we use an addi-
tional 1.4 billion words (supplied as part of the cam-
paign monolingual training data).
The bilingual training and test data for the Arabic-
to-English and German-to-English tasks are sum-
marized in Table 11. The English data was tok-
enized and lowercased while the Arabic data was
tokenized and segmented using MADA v3.1 (Roth
et al, 2008) with the ATB scheme. The German
source is decompounded (Koehn and Knight, 2003)
and part-of-speech-based long-range verb reorder-
ing rules (Popovic? and Ney, 2006) are applied.
From Table 1, we note that using the general
data considerably reduces the number of out-of-
1For a list of the IWSLT TED 2011 training cor-
pora, see http://www.iwslt2011.org/doku.php?
id=06_evaluation
650
Set Sen Tok OOV/IN OOV/ALL
German-to-English
IN 130K 2.5M
OD 2.1M 55M
dev 883 20K 398 (2.0%) 215 (1.1%)
test 1565 32K 483 (1.5%) 227 (0.7%)
eval 1436 27K 490 (1.8%) 271 (1.0%)
Arabic-to-English
IN 90K 1.6M
OD 7.9M 228M
dev 934 19K 408 (2.2%) 184 (1.0%)
test 1664 31K 495 (1.6%) 228 (0.8%)
eval 1450 27K 513 (1.9%) 163 (0.6%)
Table 1: IWSLT 2011 TED bilingual corpora statistics:
the number of tokens is given for the source side. OOV/X
denotes the number of OOV words in relation to corpus
X (the percentage is given in parentheses). IN is the TED
in-domain data, OD denotes other-domain data, ALL de-
notes the concatenation of IN and OD.
vocabulary (OOV) words. This comes with the price
of increasing the size of the training data by a factor
of more than 20. A simple concatenation of the cor-
pora might mask the phrase probabilities obtained
from the in-domain corpus, causing a deterioration
in performance. One way to avoid this contamina-
tion is by filtering the general corpus, but this dis-
cards phrase translations completely from the phrase
model. A more principled way is by adapting the
phrase probabilities of the full system to the domain
being tackled. We perform this by phrase training
the full phrase table over the in-domain training set.
3.2 Translation System
The baseline system is built using the open-source
SMT toolkit Jane 2.0, which provides a state-of-
the-art phrase-based SMT system (Wuebker et al,
2012a). In addition to the phrase based decoder,
Jane 2.0 implements the forced alignment procedure
used in this work for the purpose of adaptation. We
use the standard set of models with phrase transla-
tion probabilities for source-to-target and target-to-
source directions, smoothing with lexical weights,
a word and phrase penalty, distance-based reorder-
ing and an n-gram target language model. The SMT
systems are tuned on the dev (dev2010) development
set with minimum error rate training (Och, 2003) us-
ing BLEU (Papineni et al, 2002) accuracy measure
as the optimization criterion. We test the perfor-
mance of our system on the test (tst2010) and eval
(tst2011) sets using the BLEU and translation edit
rate (TER) (Snover et al, 2006) measures. We use
TER as an additional measure to verify the consis-
tency of our improvements and avoid over-tuning.
The Arabic-English results are case sensitive while
the German-English results are case insensitive.
4 Results
For TM training, we define three different sets: in-
domain (IN) which is the TED corpus, other-domain
(OD) which consists of the UN corpus for Arabic-
English and a concatenation of news-commentary
and europarl for German-English, and ALL which
consists of the concatenation of IN and OD. We ex-
periment with the following extraction methods:
? Heuristics: standard phrase extraction using
word-alignment training and heuristic phrase
extraction over the word alignment. The ex-
traction is performed for the three different
training data, IN, OD and ALL.
? FA standard: standard FA phrase training
where the same training set is used for initial
phrase table generation as well as the FA pro-
cedure. We perform the training on the three
different training sets and denote the resulting
systems by IN-FA, OD-FA and ALL-FA.
? FA adaptation: FA based adaptation phrase
training, where the initial table is generated
from some general data and the FA training is
performed on the IN data to achieve adapta-
tion. We perform two experiments, OD-FA0-
IN without leaving-one-out and ALL-FA-IN
with leaving-one-out.
The results of the various experiments over both
Arabic-English and German-English tasks are sum-
marized in Table 2. The usefulness of the OD
data differs between the Arabic-to-English and the
German-to-English translation tasks. For Arabic-to-
English, the OD system is 2.5%-4.3% BLEU worse
than the IN system, whereas for the German-to-
English task the differences between IN and OD are
smaller and range from 0.9% to 1.6% BLEU. The
651
Phrase training System Rules dev test eval
method number BLEU TER BLEU TER BLEU TER
Arabic-to-English
Heuristics
IN 1.1M 27.2 54.1 25.3 57.1 24.3 59.9
OD 36.3M 24.7 57.7 21.2 62.6 21.0 64.7
ALL 36.9M 27.1 54.8 24.4 58.6 23.8 61.1
FA standard
IN-FA 1.0M 27.0 54.4 25.0 57.5 23.8 60.3
OD-FA 1.8M 24.5 57.7 21.0 62.4 21.2 64.3
ALL-FA 2.0M 27.2 54.2 24.5 58.1 23.8 60.6
FA adaptation
OD-FA0-IN 0.3M 25.8 55.8 23.6 59.4 22.7 61.7
ALL-FA-IN 0.5M 27.7 53.7 25.3 56.9 24.7 59.3
German-to-English
Heuristics
IN 1.3M 31.0 48.9 29.3 51.0 32.7 46.8
OD 7.3M 29.8 49.2 27.7 51.5 31.8 47.5
ALL 7.8M 31.2 48.3 29.5 50.5 33.6 46.1
FA standard
IN-FA 0.5M 31.6 48.2 29.7 50.5 33.3 46.4
OD-FA 3.0M 29.1 51.0 27.6 53.0 30.7 49.6
ALL-FA 3.2M 31.4 48.3 29.4 50.8 33.6 46.2
FA adaptation
OD-FA0-IN 0.9M 31.2 48.7 29.1 50.9 32.7 46.9
ALL-FA-IN 0.9M 31.8 47.4 29.7 49.7 33.6 45.5
Table 2: TED 2011 translation results. BLEU and TER are given in percentages. IN denotes the TED lectures in-
domain corpus, OD denotes the other-domain corpus, ALL is the concatenation of IN and OD. FA0 denotes forced
alignment training without leaving-one-out (otherwise, leaving-one-out is used).
inferior performance of the OD system can be re-
lated to noisy data or bigger discrepancy between
the OD data domain distribution and the IN distri-
bution. The ALL system performs according to the
usefulness of the OD training set, where for Arabic-
to-English we observe deterioration in performance
for all test sets and up-to -0.9% BLEU on the test
set. On the other hand, for German-to-English, the
ALL system is improving over IN where the biggest
improvement is observed on the eval set with +0.9%
BLEU improvement.
The standard FA procedure achieves mixed re-
sults, where IN-FA deteriorates the results over the
IN counterpart for Arabic-English, while improving
for German-English. ALL-FA performs comparably
to the ALL system on both tasks, while reducing the
phrase table size considerably. The OD-FA system
deteriorates the results in comparison to the OD sys-
tem in most cases, which is expected as training over
the OD set fits the phrase model on the OD domain,
making it perform worse on IN. (Wuebker et al,
2012b) also report mixed results with FA training.
The FA adaptation results are summarized in the
last block of the experiments. The OD-FA0-IN im-
proves over the OD system, which means that the
training procedure was able to modify the OD prob-
abilities to perform well on the IN data. On the
German-to-English task, the OD-FA0-IN performs
comparably to the IN system, whereas for Arabic-
to-English OD-FA0-IN was able to close around half
of the gap between OD and IN.
The FA adapted ALL system (ALL-FA-IN) per-
forms best in our experiments, improving on both
BLEU and TER measures. In comparison to the
best heuristics system (IN for Arabic-English and
ALL for German-English), +0.4% BLEU and -0.6%
TER improvements are observed on the eval set for
Arabic-English. For German-English, the biggest
improvements are observed on TER with -0.8% on
test and -0.5% on eval. The results suggest that ALL-
FA-IN is able to learn more useful phrases than the
IN system and adjust the ALL phrase probabilities
towards the in-domain distribution.
652
System dev test
BLEU TER BLEU TER
Arabic-to-English
Heuristicsbest 27.2 54.1 25.3 57.1
IN,OD 28.2 53.1 25.5 56.8
IN,OD-FA0-IN 28.4 52.9 25.7 56.5
German-to-English
Heuristicsbest 31.2 48.3 29.5 50.5
IN,OD 31.6 48.2 29.9 50.5
IN,OD-FA0-IN 31.8 47.8 30.0 50.0
Table 3: TED 2011 mixture modeling results.
Heuristicsbest is the best heuristics based system, IN for
Arabic-English and ALL for German-English. X,Y de-
notes linear interpolation between X and Y phrase tables.
4.1 Mixture Modeling
In this section, we compare our method to mixture
modeling based adaptation, in addition to applying
mixture modeling on top of our method. We focus
on linear interpolation (Foster and Kuhn, 2007) of
the in-domain (IN) and other-domain phrase tables,
where we vary the latter between the heuristically
extracted phrase table (OD) and the FA adapted one
(OD-FA0-IN). The interpolation weight is uniform
for the interpolated phrase tables (0.5). The results
of mixture modeling are summarized in Table 3. In
this table, we include the best heuristics based sys-
tem (Heuristicsbest) from Table 2 as a reference sys-
tem. The results on the eval set are omitted as they
show similar tendencies to the test set results.
Linear interpolation of IN and OD (IN,OD) is per-
forming well in our experiments, with big improve-
ments over the dev set, +1.0% BLEU for Arabic-to-
English and +0.4% BLEU for German-to-English.
On the test set, we observe smaller improvements.
Interpolating IN with the phrase training adapted
system OD-FA0-IN (IN,OD-FA0-IN) achieves ad-
ditional gains over the IN,OD system, the biggest
are observed on TER for German-to-English, with
-0.4% and -0.5% improvements on the dev and test
sets correspondingly.
Comparing heuristics based interpolation
(IN,OD) to our best phrase training adapted system
(ALL-FA-IN) shows mixed results. For Arabic-to-
English, the systems are comparable, while for the
German-to-English test set, IN,OD is +0.2% BLEU
better and +0.8% TER worse than ALL-FA-IN. We
hypothesize that for Arabic-to-English interpolation
is important due to the larger size of the OD data,
where it could reduce the masking of the IN training
data by the much larger OD data. Nevertheless,
as mentioned previously, using phrase training
adapted phrase table in a mixture setup consistently
improves over using heuristically extracted tables.
5 Conclusions
In this work, we propose a phrase training procedure
for adaptation. The phrase training is implemented
using the FA method. First, we extract a standard
phrase table using the whole available training data.
Using this table, we initialize the FA procedure and
perform training on the in-domain set.
Experiments are done on the Arabic-to-English
and German-to-English TED lectures translation
tasks. We show that the suggested procedure is im-
proving over unadapted baselines. On the Arabic-
to-English task, the FA adapted system is +0.9%
BLEU better than the full unadapted counterpart on
both test sets. Unlike the Arabic-to-English setup,
the German-to-English OD data is helpful and pro-
duces a strong unadapted baseline in concatenation
with IN. In this case, the FA adapted system achieves
BLEU improvements mainly on the development set
with +0.6% BLEU, on the test and eval sets, im-
provements of -0.8% and -0.6% TER are observed
correspondingly. As a side effect of the FA training
process, the size of the adapted phrase table is less
than 10% of the size of the full table.
Finally, we experimented with mixture model-
ing where improvements are observed over the un-
adapted baselines. The results show that using our
phrase training adapted OD table yields better per-
formance than using the heuristically extracted OD
in a mixture framework.
Acknowledgments
This material is based upon work supported by the
DARPA BOLT project under Contract No. HR0011-
12-C-0015. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of DARPA.
653
References
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128?135, Prague, Czech Republic, June. Association
for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In Proc. 10th Conf. of the
Europ. Chapter of the Assoc. for Computational Lin-
guistics (EACL), pages 347?354, Budapest, Hungary,
April.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 708?717, Singapore, Au-
gust. Association for Computational Linguistics.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of the
41th Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, Penn-
sylvania, USA, July.
M. Popovic? and H. Ney. 2006. POS-based Word Re-
orderings for Statistical Machine Translation. In In-
ternational Conference on Language Resources and
Evaluation, pages 1278?1283.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological tag-
ging, diacritization, and lemmatization using lexeme
models and feature ranking. In Proceedings of ACL-
08: HLT, Short Papers, pages 117?120, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA, Au-
gust.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of the
Assoc. for Computational Linguistics, pages 475?484,
Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Man-
sour, and Hermann Ney. 2012a. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, Mumbai, India, Decem-
ber.
Joern Wuebker, Mei-Yuh Hwang, and Chris Quirk.
2012b. Leave-one-out phrase model training for large-
scale deployment. In NAACL 2012 Seventh Work-
shop on Statistical Machine Translation, pages 460?
467, Montreal, Canada, June. Association for Compu-
tational Linguistics.
654
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 93?97,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2010
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor Leusch,
Saab Mansour, Daniel Stein and Hermann Ney
RWTH Aachen University
Aachen, Germany
surname@cs.rwth-aachen.de
Abstract
In this paper we describe the statisti-
cal machine translation system of the
RWTH Aachen University developed for
the translation task of the Fifth Workshop
on Statistical Machine Translation. State-
of-the-art phrase-based and hierarchical
statistical MT systems are augmented
with appropriate morpho-syntactic en-
hancements, as well as alternative phrase
training methods and extended lexicon
models. For some tasks, a system combi-
nation of the best systems was used to gen-
erate a final hypothesis. We participated
in the constrained condition of German-
English and French-English in each trans-
lation direction.
1 Introduction
This paper describes the statistical MT system
used for our participation in the WMT 2010 shared
translation task. We used it as an opportunity to in-
corporate novel methods which have been investi-
gated at RWTH over the last year and which have
proven to be successful in other evaluations.
For all tasks we used standard alignment and
training tools as well as our in-house phrase-
based and hierarchical statistical MT decoders.
When German was involved, morpho-syntactic
preprocessing was applied. An alternative phrase-
training method and additional models were tested
and investigated with respect to their effect for the
different language pairs. For two of the language
pairs we could improve performance by system
combination.
An overview of the systems and models will fol-
low in Section 2 and 3, which describe the base-
line architecture, followed by descriptions of the
additional system components. Morpho-syntactic
analysis and other preprocessing issues are cov-
ered by Section 4. Finally, translation results for
the different languages and system variants are
presented in Section 5.
2 Translation Systems
For the WMT 2010 Evaluation we used stan-
dard phrase-based and hierarchical translation sys-
tems. Alignments were trained with a variant of
GIZA++. Target language models are 4-gram lan-
guage models trained with the SRI toolkit, using
Kneser-Ney discounting with interpolation.
2.1 Phrase-Based System
Our phrase-based translation system is similar to
the one described in (Zens and Ney, 2008). Phrase
pairs are extracted from a word-aligned bilingual
corpus and their translation probability in both di-
rections is estimated by relative frequencies. Ad-
ditional models include a standard n-gram lan-
guage model, phrase-level IBM1, word-, phrase-
and distortion-penalties and a discriminative re-
ordering model as described in (Zens and Ney,
2006).
2.2 Hierarchical System
Our hierarchical phrase-based system is similar to
the one described in (Chiang, 2007). It allows for
gaps in the phrases by employing a context-free
grammar and a CYK-like parsing during the de-
coding step. It has similar features as the phrase-
based system mentioned above. For some sys-
tems, we only allowed the non-terminals in hierar-
chical phrases to be substituted with initial phrases
as in (Iglesias et al, 2009), which gave better re-
sults on some language pairs. We will refer to this
as ?shallow rules?.
2.3 System Combination
The RWTH approach to MT system combination
of the French?English systems as well as the
German?English systems is a refined version of
the ROVER approach in ASR (Fiscus, 1997) with
93
German?English French?English English?French
BLEU # Phrases BLEU # Phrases BLEU # Phrases
Standard 19.7 128M 25.5 225M 23.7 261M
FA 20.0 12M 25.9 35M 24.0 33M
Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For
German?English and English?French phrase table interpolation was applied.
additional steps to cope with reordering between
different hypotheses, and to use true casing infor-
mation from the input hypotheses. The basic con-
cept of the approach has been described by Ma-
tusov et al (2006). Several improvements have
been added later (Matusov et al, 2008). This ap-
proach includes an enhanced alignment and re-
ordering framework. Alignments between the sys-
tems are learned by GIZA++, a one-to-one align-
ment is generated from the learned state occupa-
tion probabilities.
From these alignments, a confusion network
(CN) is then built using one of the hypotheses as
?skeleton? or ?primary? hypothesis. We do not
make a hard decision on which of the hypothe-
ses to use for that, but instead combine all pos-
sible CNs into a single lattice. Majority voting on
the generated lattice is performed using the prior
probabilities for each system as well as other sta-
tistical models such as a special trigram language
model. This language model is also learned on
the input hypotheses. The intention is to favor
longer phrases contained in individual hypotheses.
The translation with the best total score within this
lattice is selected as consensus translation. Scal-
ing factors of these models are optimized similar
to MERT using the Downhill Simplex algorithm.
As the objective function for this optimization, we
selected a linear combination of BLEU and TER
with a weight of 2 on the former; a combination
that has proven to deliver stable results on sev-
eral MT evaluation measures in preceding experi-
ments.
In contrast to previous years, we now include a
separate consensus true casing step to exploit the
true casing capabilities of some of the input sys-
tems: After generating a (lower cased) consensus
translation from the CN, we sum up the counts of
different casing variants of each word in a sen-
tence over the input hypotheses, and use the ma-
jority casing over those. In previous experiments,
this showed to work significantly better than us-
ing a fixed non-consensus true caser, and main-
tains flexibility on the input systems.
3 New Additional Models
3.1 Forced Alignment
For the German?English, French?English and
English?French language tasks we applied a
forced alignment procedure to train the phrase
translation model with the EM algorithm, sim-
ilar to the one described in (DeNero et al,
2006). Here, the phrase translation probabil-
ities are estimated from their relative frequen-
cies in the phrase-aligned training data. The
phrase alignment is produced by a modified
version of the translation decoder. In addi-
tion to providing a statistically well-founded
phrase model, this has the benefit of produc-
ing smaller phrase tables and thus allowing
more rapid experiments. For the language pairs
German?English and English?French the best
results were achieved by log-linear interpolation
of the standard phrase table with the generative
model. For French?English we directly used the
model trained by forced alignment. A detailed
description of the training procedure is given in
(Wuebker et al, 2010). Table 1 shows the system
performances and phrase table sizes with the stan-
dard phrase table and the one trained with forced
alignment after the first EM iteration. We can see
that the generative model reduces the phrase table
size by 85-90% while increasing performance by
0.3% to 0.4% BLEU.
3.2 Extended Lexicon Models
In previous work, RWTH was able to show the
positive impact of extended lexicon models that
cope with lexical context beyond the limited hori-
zon of phrase pairs and n-gram language models.
Mauser et al (2009) report improvements of
up to +1% in BLEU on large-scale systems for
Chinese?English and Arabic?English by incor-
porating discriminative and trigger-based lexicon
models into a state-of-the-art phrase-based de-
coder. They discuss how the two types of lexicon
94
models help to select content words by capturing
long-distance effects.
The triplet model is a straightforward extension
of the IBM model 1 with a second trigger, and like
the former is trained iteratively using the EM al-
gorithm. In search, the triggers are usually on the
source side, i.e., p(e|f, f ?) is modeled. The path-
constrained triplet model restricts the first source
trigger to the aligned target word, whereas the sec-
ond trigger can move along the whole source sen-
tence. See (Hasan et al, 2008) for a detailed de-
scription and variants of the model and its training.
For the WMT 2010 evaluation, triplets mod-
eling p(e|f, f ?) were trained and applied di-
rectly in search for all relevant language pairs.
Path-constrained models were trained on the in-
domain news-commentary data only and on the
news-commentary plus the Europarl data. Al-
though experience from similar setups indicates
that triplet lexicon models can be beneficial for
machine translation between the languages En-
glish, French, and German, on this year?s WMT
translation tasks slight improvements on the devel-
opment sets did not or only partially carry over to
the held-out test sets. Nevertheless, systems with
triplets were used for system combination, as ex-
tended lexicon models often help to predict con-
tent words and to capture long-range dependen-
cies. Thus they can help to find a strong consensus
hypothesis.
3.3 Unsupervised Training
Due to the small size of the English?German re-
sources available for language modeling as well as
for lexicon extraction, we decided to apply the un-
supervised adaptation suggested in (Schwenk and
Senellart, 2009). We use a baseline SMT system to
translate in-domain monolingual source data, fil-
ter the translations according to a decoder score
normalized by sentence length, add this synthetic
bilingual data to the original one and rebuild the
SMT system from scratch.
The motivation behind the method is that the
phrase table will adapt to the genre, and thus
let phrases which are domain related have higher
probabilities. Two phenomena are observed from
phrase tables and the corresponding translations:
? Phrase translation probabilities are changed,
making the system choose better phrase
translation candidates.
Running Words
English German
Bilingual 44.3M 43.4M
Dict. 1.4M 1.2M
AFP 610.7M
AFP unsup. 152.0M 157.3M
Table 2: Overview on data for unsupervised train-
ing.
BLEU
Dev Test
baseline 15.0 14.7
+dict. 15.1 14.6
+unsup.+dict 15.4 14.9
Table 3: Results for unsupervised training method.
? Phrases which appear repeatedly in the do-
main get higher probabilities, so that the de-
coder can better segment the sentence.
To implement this idea, we translate the AFP part
of the English LDC Gigaword v4.0 and obtain the
synthetic data.
To decrease the number of OOV words, we use
dictionaries from the stardict directory as addi-
tional bilingual data to translate the AFP corpus.
We filter sentences with OOV words and sentences
longer than 100 tokens. A summary of the addi-
tional data used is shown in Table 2.
We tried to use the best 10%, 20% and 40% of
the synthetic data, where the 40% option worked
best. A summary of the results is given in Table 3.
Although this is our best result for the
English?German task, it was not submitted, be-
cause the use of the dictionary is not allowed in
the constrained track.
4 Preprocessing
4.1 Large Parallel Data
In addition to the provided parallel Europarl and
news-commentary corpora, also the large French-
English news corpus (about 22.5 Mio. sentence
pairs) and the French-English UN corpus (about
7.2 Mio. sentence pairs) were available. Since
model training and tuning with such large cor-
pora takes a very long time, we extracted about
2 Mio. sentence pairs of both of these corpora. We
filter sentences with the following properties:
95
? Only sentences of minimum length of 4 to-
kens were considered.
? At least 92% of the vocabulary of each sen-
tence occur in the development set.
? The ratio of the vocabulary size of a sen-
tence and the number of its tokens is mini-
mum 80%.
4.2 Morpho-Syntactic Analysis
German, as a flexible and morphologically rich
language, raises a couple of problems in machine
translation. We picked two major problems and
tackled them with morpho-syntactic pre- and post-
processing: compound splitting and long-range
verb reordering.
For the translation from German into English,
German compound words were split using the
frequency-based method described in (Koehn and
Knight, 2003). Thereby, we forbid certain words
and syllables to be split. For the other trans-
lation direction, the English text was first trans-
lated into the modified German language with
split compounds. The generated output was then
postprocessed by re-merging the previously gen-
erated components using the method described in
(Popovic? et al, 2006).
Additionally, for the German?English phrase-
based system, the long-range POS-based reorder-
ing rules described in (Popovic? and Ney, 2006)
were applied on the training and test corpora as a
preprocessing step. Thereby, German verbs which
occur at the end of a clause, like infinitives and
past participles, are moved towards the beginning
of that clause. With this, we improved our baseline
phrase-based system by 0.6% BLEU.
5 Experimental Results
For all translation directions, we used the provided
parallel corpora (Europarl, news) to train the trans-
lation models and the monolingual corpora to train
BLEU
Dev Test
phrase-based baseline 19.9 19.2
phrase-based (+POS+mero+giga) 21.0 20.3
hierarchical baseline 20.2 19.6
hierarchical (+giga) 20.5 20.1
system combination 21.4 20.4
Table 4: Results for the German?English task.
the language models. We improved the French-
English systems by enriching the data with parts of
the large addional data, extracted with the method
described in Section 4.1. Depending on the sys-
tem this gave an improvement of 0.2-0.7% BLEU.
We also made use of the large giga-news as well
as the LDC Gigaword corpora for the French and
English language models. All systems were opti-
mized for BLEU score on the development data,
newstest2008. The newstest2009 data is
used as a blind test set.
In the following, we will give the BLEU scores
for all language tasks of the baseline system and
the best setup for both, the phrase-based and the
hierarchical system. We will use the following
notations to indicate the several methods we used:
(+POS) POS-based verb reordering
(+mero) maximum entropy reordering
(+giga) including giga-news and
LDC Gigaword in LM
(fa) trained by forced alignment
(shallow) allow only shallow rules
We applied system combination of up to 6 sys-
tems with several setups. The submitted systems
are marked in tables 4-7.
6 Conclusion
For the participation in the WMT 2010 shared
translation task, RWTH used state-of-the-art
phrase-based and hierarchical translation systems.
To deal with the rich morphology and word or-
der differences in German, compound splitting
and long range verb reordering were applied in a
preprocessing step. For the French-English lan-
guage pairs, RWTH extracted parts of the large
news corpus and the UN corpus as additional
training data. Further, training the phrase trans-
lation model with forced alignment yielded im-
provements in BLEU. To obtain the final hypothe-
sis for the French?English and German?English
BLEU
Dev Test
phrase-based baseline 14.8 14.5
phrase-based (+mero) 15.0 14.7
hierarchical baseline 14.2 13.9
hierarchical (shallow) 14.5 14.3
Table 5: Results for the English?German task.
96
BLEU
Dev Test
phrase-based baseline 21.8 25.1
phrase-based (fa+giga) 23.0 26.1
hierarchical baseline 21.9 25.0
hierarchical (shallow+giga) 22.7 25.6
system combination 23.1 26.1
Table 6: Results for the French?English task.
BLEU
Dev Test
phrase-based baseline 20.9 23.2
phrase-based (fa+mero+giga) 23.0 24.6
hierarchical baseline 20.6 22.5
hierarchical (shallow,+giga) 22.4 24.3
Table 7: Results for the English?French task.
language pairs, RWTH applied system combina-
tion. Altogether, by application of these meth-
ods RWTH was able to increase performance in
BLEU by 0.8% for German?English, 0.2% for
English?German, 1.0% for French?English and
1.4% for English?French on the test set over the
respective baseline systems.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Sur-
face Heuristics. In Proceedings of the Workshop on
Statistical Machine Translation, pages 31?38.
J.G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Vot-
ing Error Reduction (ROVER). In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-
Ferrer. 2008. Triplet Lexicon Models for Statisti-
cal Machine Translation. In Proceedings of Emperi-
cal Methods of Natural Language Processing, pages
372?381.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380?388.
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extend-
ing Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 210?217.
M. Popovic? and H. Ney. 2006. POS-based Word Re-
orderings for Statistical Machine Translation. In In-
ternational Conference on Language Resources and
Evaluation, pages 1278?1283.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
H. Schwenk and J. Senellart. 2009. Translation Model
Adaptation for an Arabic/French News Translation
System by Lightly-Supervised Training. In MT
Summit XII.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics. To ap-
pear.
R. Zens and H. Ney. 2006. Discriminative Reorder-
ing Models for Statistical Machine Translation. In
Workshop on Statistical Machine Translation, pages
55?63.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statis-
tical Machine Translation. In International Work-
shop on Spoken Language Translation.
97
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 405?412,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2011
Matthias Huck, Joern Wuebker, Christoph Schmidt, Markus Freitag, Stephan Peitz,
Daniel Stein, Arnaud Dagnelies, Saab Mansour, Gregor Leusch and Hermann Ney
RWTH Aachen University
Aachen, Germany
surname@cs.rwth-aachen.de
Abstract
This paper describes the statistical machine
translation (SMT) systems developed by
RWTH Aachen University for the translation
task of the EMNLP 2011 Sixth Workshop on
Statistical Machine Translation. Both phrase-
based and hierarchical SMT systems were
trained for the constrained German-English
and French-English tasks in all directions. Ex-
periments were conducted to compare differ-
ent training data sets, training methods and op-
timization criteria, as well as additional mod-
els on dependency structure and phrase re-
ordering. Further, we applied a system com-
bination technique to create a consensus hy-
pothesis from several different systems.
1 Overview
We sketch the baseline architecture of RWTH?s se-
tups for the WMT 2011 shared translation task by
providing an overview of our translation systems in
Section 2. In addition to the baseline features, we
adopted several novel methods, which will be pre-
sented in Section 3. Details on the respective se-
tups and translation results for the French-English
and German-English language pairs (in both trans-
lation directions) are given in Sections 4 and 5. We
finally conclude the paper in Section 6.
2 Translation Systems
For the WMT 2011 evaluation we utilized RWTH?s
state-of-the-art phrase-based and hierarchical trans-
lation systems as well as our in-house system com-
bination framework. GIZA++ (Och and Ney, 2003)
was employed to train word alignments, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
2.1 Phrase-Based System
We applied a phrase-based translation (PBT) system
similar to the one described in (Zens and Ney, 2008).
Phrase pairs are extracted from a word-aligned bilin-
gual corpus and their translation probability in both
directions is estimated by relative frequencies. The
standard feature set moreover includes an n-gram
language model, phrase-level single-word lexicons
and word-, phrase- and distortion-penalties. To lexi-
calize reordering, a discriminative reordering model
(Zens and Ney, 2006a) is used. Parameters are opti-
mized with the Downhill-Simplex algorithm (Nelder
and Mead, 1965) on the word graph.
2.2 Hierarchical System
For the hierarchical setups described in this paper,
the open source Jane toolkit (Vilar et al, 2010) was
employed. Jane has been developed at RWTH and
implements the hierarchical approach as introduced
by Chiang (2007) with some state-of-the-art exten-
sions. In hierarchical phrase-based translation, a
weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The standard models integrated into
our Jane systems are: phrase translation probabil-
ities and lexical translation probabilities on phrase
level, each for both translation directions, length
405
penalties on word and phrase level, three binary fea-
tures marking hierarchical phrases, glue rule, and
rules with non-terminals at the boundaries, source-
to-target and target-to-source phrase length ratios,
four binary count features and an n-gram language
model. The model weights are optimized with stan-
dard MERT (Och, 2003) on 100-best lists.
2.3 System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (Matusov et al, 2006;
Matusov et al, 2008). This approach includes an
enhanced alignment and reordering framework. A
lattice is built from the input hypotheses. The trans-
lation with the best score within the lattice according
to a couple of statistical models is selected as con-
sensus translation.
3 Translation Modeling
We incorporated several novel methods into our sys-
tems for the WMT 2011 evaluation. This section
provides a short survey of three of the methods
which we suppose to be of particular interest.
3.1 Language Model Data Selection
For the English and German language models,
we applied the data selection method proposed in
(Moore and Lewis, 2010). Each sentence is scored
by the difference in cross-entropy between a lan-
guage model trained from in-domain data and a lan-
guage model trained from a similar-sized sample of
the out-of-domain data. As in-domain data we used
the news-commentary corpus. The out-of-domain
data from which the data was selected are the news
crawl corpus for both languages and for English the
109 corpus and the LDC Gigaword data. We used a
3-gram trained with the SRI toolkit to compute the
cross-entropy. For the news crawl corpus, only 1/8
of the sentences were discarded. Of the 109 corpus
we retained 1/2 and of the LDC Gigaword data we
retained 1/4 of the sentences to train the language
models.
3.2 Phrase Model Training
For the German?English and French?English
translation tasks we applied a forced alignment pro-
cedure to train the phrase translation model with the
EM algorithm, similar to the one described in (DeN-
ero et al, 2006). Here, the phrase translation prob-
abilities are estimated from their relative frequen-
cies in the phrase-aligned training data. The phrase
alignment is produced by a modified version of the
translation decoder. In addition to providing a statis-
tically well-founded phrase model, this has the ben-
efit of producing smaller phrase tables and thus al-
lowing more rapid experiments. A detailed descrip-
tion of the training procedure is given in (Wuebker
et al, 2010).
3.3 Soft String-to-Dependency
Given a dependency tree of the target language,
we are able to introduce language models that span
over longer distances than the usual n-grams, as in
(Shen et al, 2008). To obtain dependency structures,
we apply the Stanford parser (Klein and Manning,
2003) on the target side of the training material.
RWTH?s open source hierarchical translation toolkit
Jane has been extended to include dependency infor-
mation in the phrase table and to build dependency
trees on the output hypotheses at decoding time from
this information.
Shen et al (2008) use only phrases that meet cer-
tain restrictions. The first possibility is what the au-
thors call a fixed dependency structure. With the
exception of one word within this phrase, called
the head, no outside word may have a dependency
within this phrase. Also, all inner words may only
depend on each other or on the head. For a second
structure, called a floating dependency structure, the
head dependency word may also exist outside the
phrase. If the dependency structure of a phrase con-
forms to these restrictions, it is denoted as valid.
In our phrase table, we mark those phrases that
possess a valid dependency structure with a binary
feature, but all phrases are retained as translation op-
tions. In addition to storing the dependency informa-
tion, we also memorize for all hierarchical phrases
if the content of gaps has been dependent on the left
or on the right side. We utilize the dependency in-
formation during the search process by adding three
406
French English
Sentences 3 710 985
Running Words 98 352 916 87 689 253
Vocabulary 179 548 216 765
Table 1: Corpus statistics of the preprocessed high-
quality training data (Europarl, news-commentary, and
selected parts of the 109 and UN corpora) for the
RWTH systems for the WMT 2011 French?English and
English?French translation tasks. Numerical quantities
are replaced by a single category symbol.
features to the log-linear model: merging errors to
the left, merging errors to the right, and the ratio of
valid vs. non-valid dependency structures. The de-
coder computes the corresponding costs when it tries
to construct a dependency tree of a (partial) hypothe-
sis on-the-fly by merging the dependency structures
of the used phrase pairs.
In an n-best reranking step, we compute depen-
dency language model scores on the dependencies
which were assembled on the hypotheses by the
search procedure. We apply one language model
for left-side dependencies and one for right-side de-
pendencies. For head structures, we also compute
their scores by exploiting a simple unigram language
model. We furthermore include a language count
feature that is incremented each time we compute
a dependency language model score. As trees with
few dependencies have less individual costs to be
computed, they tend to obtain lower overall costs
than trees with more complex structures in other
sentences. The intention behind this feature is thus
comparable to the word penalty in combination with
a normal n-gram language model.
4 French-English Setups
We set up both hierarchical and standard phrase-
based systems for the constrained condition of the
WMT 2011 French?English and English?French
translation tasks. The English?French RWTH pri-
mary submission was produced with a single hierar-
chical system, while a system combination of three
systems was used to generate a final hypothesis for
the French?English primary submission.
Besides the Europarl and news-commentary cor-
pora, the provided parallel data also comprehends
French English
Sentences 29 996 228
Running Words 916 347 538 778 544 843
Vocabulary 1 568 089 1 585 093
Table 2: Corpus statistics of the preprocessed full training
data for the RWTH primary system for the WMT 2011
English?French translation task. Numerical quantities
are replaced by a single category symbol.
the large French-English 109 corpus and the French-
English UN corpus. Since model training with
such a huge amount of data requires a consider-
able computational effort, RWTH decided to select
a high-quality part of altogether about 2 Mio. sen-
tence pairs from the latter two corpora. The selec-
tion of parallel sentences was carried out according
to three criteria: (1) Only sentences of minimum
length of 4 tokens are considered, (2) at least 92%
of the vocabulary of each sentence occurs in new-
stest2008, and (3) the ratio of the vocabulary size
of a sentence and the number of its tokens is mini-
mum 80%. Word alignments in both directions were
trained with GIZA++ and symmetrized according to
the refined method that was proposed in (Och and
Ney, 2003). The phrase tables of the translation
systems are extracted from the Europarl and news-
commentary parallel training data as well as the se-
lected high-quality parts the 109 and UN corpora
only. The only exception is the hierarchical system
used for the English?French RWTH primary sub-
mission which comprehends a second phrase table
with lexical (i.e. non-hierarchical) phrases extracted
from the full parallel data (approximately 30 Mio.
sentence pairs).
Detailed statistics of the high-quality parallel
training data (Europarl, news-commentary, and the
selected parts of the 109 and UN corpora) are given
in Table 1, the corpus statistics of the full parallel
data from which the second phrase table with lexi-
cal phrases for the English?French RWTH primary
system was created are presented in Table 2.
The translation systems use large 4-gram lan-
guage models with modified Kneser-Ney smooth-
ing. The French language model was trained on
most of the provided French data including the
monolingual LDC Gigaword corpora, the English
407
newstest2009 newstest2010
French?English BLEU TER BLEU TER
System combination of ? systems (primary) 26.7 56.0 27.4 54.9
PBT with triplet lexicon, no forced alignment (contrastive) ? 26.2 56.7 27.2 55.3
Jane as below + improved LM (contrastive) 26.3 57.4 26.7 56.2
Jane with parse match + syntactic labels + dependency ? 26.2 57.5 26.5 56.4
PBT with forced alignment phrase training ? 26.0 57.1 26.3 56.0
Table 3: RWTH systems for the WMT 2011 French?English translation task (truecase). BLEU and TER results are
in percentage.
newstest2009 newstest2010
English?French BLEU TER BLEU TER
Jane shallow + in-domain TM + lexical phrases from full data 25.3 60.1 27.1 57.2
Jane shallow + in-domain TM + triplets + DWL + parse match 24.8 60.5 26.6 57.5
PBT with triplets, DWL, sentence-level word lexicon, discrim. reord. 24.8 60.1 26.5 57.3
Table 4: RWTH systems for the WMT 2011 English?French translation task (truecase). BLEU and TER results are
in percentage.
language model was trained on automatically se-
lected English data (cf. Section 3.1) from the pro-
vided resources including the 109 corpus and LDC
Gigaword.
The scaling factors of the log-linear model com-
bination are optimized towards BLEU on new-
stest2009, newstest2010 is used as an unseen test set.
4.1 Experimental Results French?English
The results for the French?English task are given in
Table 3. RWTH?s three submissions ? one primary
and two contrastive ? are labeled accordingly in the
table. The first contrastive submission is a phrase-
based system with a standard feature set plus an ad-
ditional triplet lexicon model (Mauser et al, 2009).
The triplet lexicon model was trained on in-domain
news commentary data only. The second contrastive
submission is a hierarchical Jane system with three
syntax-based extensions: A parse match model (Vi-
lar et al, 2008), soft syntactic labels (Stein et al,
2010), and the soft string-to-dependency extension
as described in Section 3.3. The primary submis-
sion combines the phrase-based contrastive system,
a hierarchical system that is very similar to the Jane
contrastive submission but with a slightly worse lan-
guage model, and an additional PBT system that has
been trained with forced alignment (Wuebker et al,
2010) on WMT 2010 data only.
4.2 Experimental Results English?French
The results for the English?French task are given
in Table 4. We likewise submitted two contrastive
systems for this translation direction. The first con-
trastive submission is a phrase-based system, en-
hanced with a triplet lexicon model and a discrim-
inative word lexicon model (Mauser et al, 2009) ?
both trained on in-domain news commentary data
only ? as well as a sentence-level single-word lex-
icon model and a discriminative reordering model
(Zens and Ney, 2006a). The second contrastive sub-
mission is a hierarchical Jane system with shallow
rules (Iglesias et al, 2009), a triplet lexicon model, a
discriminative word lexicon, the parse match model,
and a second phrase table extracted from in-domain
data only. Our primary submission is very similar
to the latter Jane setup. It does not comprise the ex-
tended lexicon models and the parse match exten-
sion, but instead includes lexical phrases from the
full 30 Mio. sentence corpus as described above.
5 German-English Setups
We trained phrase-based and hierarchical transla-
tion systems for both translation directions of the
German-English language pair. The corpus statis-
408
German English
Sentences 1 857 745
Running Words 48 449 977 50 559 217
Vocabulary 387 593 123 470
Table 5: Corpus statistics of the preprocessed train-
ing data for the WMT 2011 German?English and
English?German translation tasks. Numerical quantities
are replaced by a single category symbol.
tics can be found in Table 5. Word alignments were
generated with GIZA++ and symmetrized as for the
French-English setups.
The language models are 4-grams trained on the
bilingual data as well as the provided News crawl
corpus. For the English language model the 109
French-English and LDC Gigaword corpora were
used additionally. For the 109 French-English and
LDC Gigaword corpora RWTH applied the data se-
lection technique described in Section 3.1. We ex-
amined two different language models, one with
LDC data and one without.
Systems were optimized on the newstest2009 data
set, newstest2008 was used as test set. The scores
for newstest2010 are included for completeness.
5.1 Morpho-Syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the source side
was preprocessed by splitting German compound
words with the frequency-based method described
in (Koehn and Knight, 2003). To further reduce
translation complexity, we performed the long-range
part-of-speech based reordering rules proposed by
(Popovic? et al, 2006). For additional experiments
we used the TreeTagger (Schmid, 1995) to produce
a lemmatized version of the German source.
5.2 Optimization Criterion
We studied the impact of different optimization cri-
teria on tranlsation performance. The usual prac-
tice is to optimize the scaling factors to maximize
BLEU. We also experimented with two different
combinations of BLEU and Translation Edit Rate
(TER): TER?BLEU and TER?4BLEU. The first
denotes the equally weighted combination, while for
the latter BLEU is weighted 4 times as strong as
TER.
5.3 Experimental Results German?English
For the German?English task we conducted ex-
periments comparing the standard phrase extraction
with the phrase training technique described in Sec-
tion 3.2. For the latter we applied log-linear phrase-
table interpolation as proposed in (Wuebker et al,
2010). Further experiments included the use of addi-
tional language model training data, reranking of n-
best lists generated by the phrase-based system, and
different optimization criteria. We also carried out
a system combination of several systems, including
phrase-based systems on lemmatized German and
on source data without compound splitting and two
hierarchical systems optimized for different criteria.
The results are given in Table 6.
A considerable increase in translation quality can
be achieved by application of German compound
splitting. The system that operates on German
surface forms without compound splitting (SUR)
clearly underperforms the baseline system with mor-
phological preprocessing. The system on lemma-
tized German (LEM) is at about the same level as
the system on surface forms.
In comparison to the standard heuristic phrase ex-
traction technique, performing phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006b),
sentence length model, a 6-gram LM and single-
word lexicon models in both normal and inverse di-
rection. These models are combined in a log-linear
fashion and the scaling factors are tuned in the same
manner as the baseline system (using TER?4BLEU
on newstest2009).
The table includes three identical Jane systems
which are optimized for different criteria. The one
optimized for TER?4BLEU offers the best balance
between BLEU and TER, but was not finished in
time for submission. As primary submission we
chose the reranked PBT system, as secondary the
system combination.
409
newstest2008 newstest2009 newstest2010
German?English opt criterion BLEU TER BLEU TER BLEU TER
Syscombi of ? (secondary) TER?BLEU 21.1 62.1 20.8 61.2 23.7 59.2
Jane +GW ? BLEU 21.5 63.9 21.0 63.3 22.9 61.7
Jane +GW TER?4BLEU 21.4 62.6 21.1 62.0 23.5 60.3
PBT (FA) rerank +GW (primary) ? TER?4BLEU 21.4 62.8 21.1 61.9 23.4 60.1
PBT (FA) +GW ? TER?4BLEU 21.1 63.0 21.1 62.2 23.3 60.3
Jane +GW ? TER?BLEU 20.9 61.1 20.4 60.5 23.4 58.3
PBT (FA) TER?4BLEU 21.1 63.2 20.6 62.4 23.2 60.4
PBT TER?4BLEU 20.6 62.7 20.3 61.9 23.3 59.7
PBT (SUR) ? TER?4BLEU 19.5 66.5 18.9 65.8 21.0 64.9
PBT (LEM) ? TER?4BLEU 19.2 66.1 18.9 65.4 21.0 63.5
Table 6: RWTH systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results
are in percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model.
SUR and LEM denote the systems without compound splitting and on the lemmatized source, respectively. The three
hierarchical Jane systems are identical, but used different parameter optimization criterea.
newstest2008 newstest2009 newstest2010
English?German opt criterion BLEU TER BLEU TER BLEU TER
PBT + discrim. reord. (primary) TER?4BLEU 15.3 70.2 15.1 69.8 16.2 65.6
PBT + discrim. reord. BLEU 15.2 70.6 15.2 70.1 16.2 66.0
PBT TER?4BLEU 15.2 70.7 15.2 70.2 16.2 66.1
Jane BLEU 15.1 72.1 15.4 71.2 16.4 67.4
Jane TER?4BLEU 15.1 68.4 14.6 69.5 14.6 65.9
Table 7: RWTH systems for the WMT 2011 English?German translation task (truecase). BLEU and TER results are
in percentage.
5.4 Experimental Results English?German
We likewise studied the effect of using BLEU only
versus using TER?4BLEU as optimization crite-
rion in the English?German translation direction.
Moreover, we tested the impact of the discriminative
reordering model (Zens and Ney, 2006a). The re-
sults can be found in Table 7. For the phrase-based
system, optimizing towards TER?4BLEU leads to
slightly better results both in BLEU and TER than
optimizing towards BLEU. Using the discriminative
reordering model yields some improvements both on
newstest2008 and newstest2010. In the case of the
hierarchical system, the effect of the optimization
criterion is more pronounced than for the phrase-
based system. However, in this case it clearly leads
to a tradeoff between BLEU and TER, as the choice
of TER?4BLEU harms the translation results of
test2010 with respect to BLEU.
6 Conclusion
For the participation in the WMT 2011 shared trans-
lation task, RWTH experimented with both phrase-
based and hierarchical translation systems. We used
all bilingual and monolingual data provided for the
constrained track. To limit the size of the lan-
guage model, a data selection technique was applied.
Several techniques yielded improvements over the
baseline, including three syntactic models, extended
lexicon models, a discriminative reordering model,
forced alignment training, reranking methods and
different optimization criteria.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
410
References
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Surface
Heuristics. In Proceedings of the Workshop on Statis-
tical Machine Translation, pages 31?38.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380?388.
D. Klein and C.D. Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics
- Volume 1, ACL ?03, pages 423?430.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extending Sta-
tistical Machine Translation with Discriminative and
Trigger-Based Lexicon Models. In Conference on
Empirical Methods in Natural Language Processing,
pages 210?217.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
H. Schmid. 1995. Improvements in Part-of-Speech Tag-
ging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50, Dublin,
Ireland, March.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In Proceedings
of ACL-08: HLT. Association for Computational Lin-
guistics, pages 577?585, June.
D. Stein, S. Peitz, D. Vilar, and H. Ney. 2010. A Cocktail
of Deep Syntactic Features for Hierarchical Machine
Translation. In Conference of the Association for Ma-
chine Translation in the Americas 2010, page 9, Den-
ver, USA, October.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901 ? 904, Denver, Col-
orado, USA, September.
D. Vilar, D. Stein, and H. Ney. 2008. Analysing Soft
Syntax Features and Heuristics for Hierarchical Phrase
Based Machine Translation. In Proc. of the Int. Work-
shop on Spoken Language Translation (IWSLT), pages
190?197, Waikiki, Hawaii, October.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006a. Discriminative Reordering
Models for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
R. Zens and H. Ney. 2006b. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
411
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
412
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185?192,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joint WMT 2013 Submission of the QUAERO Project
?Stephan Peitz, ?Saab Mansour, ?Matthias Huck, ?Markus Freitag, ?Hermann Ney,
?Eunah Cho, ?Teresa Herrmann, ?Mohammed Mediani, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Quoc Khanh Do,
?Bianka Buschbeck, ?Tonio Wandmacher
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint submis-
sion of the QUAERO project for the
German?English translation task of the
ACL 2013 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013).
The submission was a system combina-
tion of the output of four different transla-
tion systems provided by RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy (KIT), LIMSI-CNRS and SYSTRAN
Software, Inc. The translations were
joined using the RWTH?s system com-
bination approach. Experimental results
show improvements of up to 1.2 points in
BLEU and 1.2 points in TER compared to
the best single translation.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in ma-
chine translation is mainly assigned to the four
groups participating in this joint submission. The
aim of this submission was to show the quality of
a joint translation by combining the knowledge of
the four project partners. Each group develop and
maintain their own different machine translation
system. These single systems differ not only in
their general approach, but also in the preprocess-
ing of training and test data. To take advantage
of these differences of each translation system, we
combined all hypotheses of the different systems,
using the RWTH system combination approach.
This paper is structured as follows. First, the
different engines of all four groups are introduced.
In Section 3, the RWTH Aachen system combina-
tion approach is presented. Experiments with dif-
ferent system selections for system combination
are described in Section 4. This paper is concluded
in Section 5.
2 Translation Systems
For WMT 2013, each QUAERO partner trained
their systems on the parallel Europarl (EPPS),
News Commentary (NC) corpora and the web-
crawled corpus. All single systems were tuned on
the newstest2009 and newstest2010 development
set. The newstest2011 development set was used
to tune the system combination parameters. Fi-
nally, on newstest2012 the results of the different
system combination settings are compared. In this
Section, all four different translation engines are
presented.
2.1 RWTH Aachen Single System
For the WMT 2013 evaluation, RWTH utilized a
phrase-based decoder based on (Wuebker et al,
2012) which is part of RWTH?s open-source SMT
toolkit Jane 2.1 1. GIZA++ (Och and Ney, 2003)
was employed to train a word alignment, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
After phrase pair extraction from the word-
aligned parallel corpus, the translation probabil-
ities are estimated by relative frequencies. The
standard feature set alo includes an n-gram lan-
guage model, phrase-level IBM-1 and word-,
phrase- and distortion-penalties, which are com-
bined in log-linear fashion. Furthermore, we used
an additional reordering model as described in
(Galley and Manning, 2008). By this model six
1http://www-i6.informatik.rwth-aachen.
de/jane/
185
additional feature are added to the log-linear com-
bination. The model weights are optimized with
standard Mert (Och, 2003a) on 200-best lists. The
optimization criterion is BLEU.
2.1.1 Preprocessing
In order to reduce the source vocabulary size trans-
lation, the German text was preprocessed by split-
ting German compound words with the frequency-
based method described in (Koehn and Knight,
2003). To further reduce translation complexity
for the phrase-based approach, we performed the
long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.2 Translation Model
We applied filtering and weighting for domain-
adaptation similarly to (Mansour et al, 2011) and
(Mansour and Ney, 2012). For filtering the bilin-
gual data, a combination of LM and IBM Model
1 scores was used. In addition, we performed
weighted phrase extraction by using a combined
LM and IBM Model 1 weight.
2.1.3 Language Model
During decoding a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl,
the 109 French-English, UN and LDC Gigaword
Fourth Edition corpora.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
The training data was preprocessed prior to the
training. Symbols such as quotes, dashes and
apostrophes are normalized. Then the first words
of each sentence are smart-cased. For the Ger-
man part of the training corpus, the hunspell2 lex-
icon was used, in order to learn a mapping from
old German spelling to new German writing rules.
Compound-splitting was also performed as de-
scribed in Koehn and Knight (2003). We also re-
moved very long sentences, empty lines, and sen-
tences which show big mismatch on the length.
2.2.2 Filtering
The web-crawled corpus was filtered using an
SVM classifier as described in (Mediani et al,
2011). The lexica used in this filtering task were
obtained from Giza alignments trained on the
2http://hunspell.sourceforge.net/
cleaner corpora, EPPS and NC. Assuming that this
corpus is very noisy, we biased our classifier more
towards precision than recall. This was realized
by giving higher number of false examples (80%
of the training data).
This filtering technique ruled out more than
38% of the corpus (the unfiltered corpus contains
around 2.4M pairs, 0.9M of which were rejected
in the filtering task).
2.2.3 System Overview
The in-house phrase-based decoder (Vogel, 2003)
is used to perform decoding. Optimization with
regard to the BLEU score is done using Minimum
Error Rate Training (MERT) as described in Venu-
gopal et al (2005).
2.2.4 Reordering Model
We applied part-of-speech (POS) based reordering
using probabilistic continuous (Rottmann and Vo-
gel, 2007) and discontinuous (Niehues and Kolss,
2009) rules. This was learned using POS tags gen-
erated by the TreeTagger (Schmid, 1994) for short
and long range reorderings respectively.
In addition to this POS-based reordering, we
also used tree-based reordering rules. Syntactic
parse trees of the whole training corpus and the
word alignment between source and target lan-
guage are used to learn rules on how to reorder the
constituents in a German source sentence to make
it match the English target sentence word order
better (Herrmann et al, 2013). The training corpus
was parsed by the Stanford parser (Rafferty and
Manning, 2008). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are en-
coded in a word lattice which is used as input to
the decoder.
Moreover, our reordering model was extended
so that it could include the features of lexicalized
reordering model. The reordering probabilities for
each phrase pair are stored as well as the origi-
nal position of each word in the lattice. During
the decoding, the reordering origin of the words
is checked along with its probability added as an
additional score.
2.2.5 Translation Models
The translation model uses the parallel data of
EPPS, NC, and the filtered web-crawled data. As
word alignment, we used the Discriminative Word
Alignment (DWA) as shown in (Niehues and Vo-
186
gel, 2008). The phrase pairs were extracted using
different source word order suggested by the POS-
based reordering models presented previously as
described in (Niehues et al, 2009).
In order to extend the context of source lan-
guage words, we applied a bilingual language
model (Niehues et al, 2011). A Discriminative
Word Lexicon (DWL) introduced in (Mauser et
al., 2009) was extended so that it could take the
source context also into the account. For this,
we used a bag-of-ngrams instead of representing
the source sentence as a bag-of-words. Filtering
based on counts was then applied to the features
for higher order n-grams. In addition to this, the
training examples were created differently so that
we only used the words that occur in the n-best list
but not in the reference as negative example.
2.2.6 Language Models
We build separate language models and combined
them prior to decoding. As word-token based
language models, one language model is built on
EPPS, NC, and giga corpus, while another one is
built using crawled data. We combined the LMs
linearly by minimizing the perplexity on the de-
velopment data. As a bilingual language model we
used the EPPS, NC, and the web-crawled data and
combined them. Furthermore, we use a 5-gram
cluster-based language model with 1,000 word
clusters, which was trained on the EPPS and NC
corpus. The word clusters were created using the
MKCLS algorithm.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine transla-
tion system based on bilingual n-gram3. In this
approach, the translation model relies on a spe-
cific decomposition of the joint probability of a
sentence pair using the n-gram assumption: a sen-
tence pair is decomposed into a sequence of bilin-
gual units called tuples, defining a joint segmen-
tation of the source and target. In the approach of
(Marin?o et al, 2006), this segmentation is a by-
product of source reordering which ultimately de-
rives from initial word and phrase alignments.
2.3.2 An overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using
3http://ncode.limsi.fr/
a n-gram model of (source,target) pairs (Casacu-
berta and Vidal, 2004). Training this model re-
quires to reorder source sentences so as to match
the target word order. This is performed by
a stochastic finite-state reordering model, which
uses part-of-speech information4 to generalize re-
ordering patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized re-
ordering models (Tillmann, 2004) aiming at pre-
dicting the orientation of the next translation unit;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which compensate for the system prefer-
ence for short translations. The four lexicon mod-
els are similar to the ones use in a standard phrase
based system: two scores correspond to the rel-
ative frequencies of the tuples and two lexical
weights estimated from the automatically gener-
ated word alignments. The weights associated to
feature functions are optimally combined using a
discriminative training framework (Och, 2003b).
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the
tuple extraction process. The resulting reordering
hypotheses are passed to the decoder in the form
of word lattices (Crego and Mario, 2006).
2.3.3 Continuous space translation models
One critical issue with standard n-gram translation
models is that the elementary units are bilingual
pairs, which means that the underlying vocabu-
lary can be quite large, even for small translation
tasks. Unfortunately, the parallel data available to
train these models are typically order of magni-
tudes smaller than the corresponding monolingual
corpora used to train target language models. It is
very likely then, that such models should face se-
vere estimation problems. In such setting, using
neural network language model techniques seem
all the more appropriate. For this study, we fol-
low the recommendations of Le et al (2012), who
propose to factor the joint probability of a sen-
tence pair by decomposing tuples in two (source
and target) parts, and further each part in words.
This yields a word factored translation model that
4Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
187
can be estimated in a continuous space using the
SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the com-
putational cost of computing n-gram probabilities.
The solution used here was to resort to a two pass
approach: the first pass uses a conventional back-
off n-gram model to produce a k-best list; in the
second pass, the k-best list is reordered using the
probabilities of m-gram SOUL translation models.
In the following experiments, we used a fixed con-
text size for SOUL of m= 10, and used k = 300.
2.3.4 Corpora and data pre-processing
All the parallel data allowed in the constrained
task are pooled together to create a single par-
allel corpus. This corpus is word-aligned using
MGIZA++5 with default settings. For the English
monolingual training data, we used the same setup
as last year6 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we also took advantage of our in-
house text processing tools for the tokenization
and detokenization steps (Dchelotte et al, 2008)
and our system is built in ?true-case?. As Ger-
man is morphologically more complex than En-
glish, the default policy which consists in treat-
ing each word form independently is plagued with
data sparsity, which is detrimental both at training
and decoding time. Thus, the German side was
normalized using a specific pre-processing scheme
(described in (Allauzen et al, 2010; Durgar El-
Kahlout and Yvon, 2010)), which notably aims at
reducing the lexical redundancy by (i) normalizing
the orthography, (ii) neutralizing most inflections
and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
In the past few years, SYSTRAN has been focus-
ing on the introduction of statistical approaches
to its rule-based backbone, leading to Hybrid Ma-
chine Translation.
The technique of Statistical Post-Editing
(Dugast et al, 2007) is used to automatically edit
the output of the rule-based system. A Statistical
Post-Editing (SPE) module is generated from a
bilingual corpus. It is basically a translation mod-
ule by itself, however it is trained on rule-based
5http://geek.kyloo.net/software
6The fifth edition of the English Gigaword
(LDC2011T07) was not used.
translations and reference data. It applies correc-
tions and adaptations learned from a phrase-based
5-gram language model. Using this two-step
process will implicitly keep long distance re-
lations and other constraints determined by the
rule-based system while significantly improving
phrasal fluency. It has the advantage that quality
improvements can be achieved with very little
but targeted bilingual data, thus significantly
reducing training time and increasing translation
performance.
The basic setup of the SPE component is identi-
cal to the one described in (Dugast et al, 2007).
A statistical translation model is trained on the
rule-based translation of the source and the target
side of the parallel corpus. Language models are
trained on each target half of the parallel corpora
and also on additional in-domain corpora. More-
over, the following measures - limiting unwanted
statistical effects - were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is sig-
nificantly reduced. In addition, entity trans-
lation is handled more reliably by the rule-
based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the
reference translation) is used to produce an
additional parallel corpus (whose target is
identical to the source). This was added to the
parallel text in order to improve word align-
ment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side
are also discarded.
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained on 2M
phrases from the news/europarl and Common-
Crawl corpora, provided as training data for WMT
2013. Weights for these separate models were
tuned by the Mert algorithm provided in the Moses
toolkit (Koehn et al, 2007), using the provided
news development set.
188
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
3 RWTH Aachen System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses gener-
ated with different translation engines. First, a
word to word alignment for the given single sys-
tem hypotheses is produced. In a second step a
confusion network is constructed. Then, the hy-
pothesis with the highest probability is extracted
from this confusion network. For the alignment
procedure, each of the given single systems gen-
erates one confusion network with its own as pri-
mary system. To this primary system all other hy-
potheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1. The final network for one source sen-
tence is the union of all confusion networks gen-
erated from the different primary systems. That
allows the system combination to select the word
order from different system outputs.
Before performing system combination, each
translation output was normalized by tokenization
and lowercasing. The output of the combination
was then truecased based on the original truecased
output.
The model weights of the system combination
are optimized with standard Mert (Och, 2003a)
on 100-best lists. We add one voting feature for
each single system to the log-linear framework of
the system combination. The voting feature fires
for each word the single system agrees on. More-
over, a word penalty, a language model trained on
the input hypotheses, a binary feature which pe-
nalizes word deletions in the confusion network
and a primary feature which marks the system
which provides the word order are combined in
this log-linear model. The optimization criterion
is 4BLEU-TER.
4 Experimental Results
In this year?s experiments, we tried to improve the
result of the system combination further by com-
bining single systems tuned on different develop-
Table 1: Comparison of single systems tuned on
newstest2009 and newstest2010. The results are
reported on newstest2012.
single systems tuned on newstest2012
newstest BLEU TER
KIT 2009 24.6 58.4
2010 24.6 58.6
LIMSI 2009 22.5 61.5
2010 22.6 59.8
SYSTRAN 2009 20.9 63.3
2010 21.2 62.2
RWTH 2009 23.7 60.8
2010 24.4 58.8
ment sets. The idea is to achieve a more stable
performance in terms of translation quality, if the
single systems are not optimized on the same data
set. In Table 1, the results of each provided single
system tuned on newstest2009 and newstest2010
are shown. For RWTH, LIMSI and SYSTRAN,
it seems that the performance of the single system
depends on the chosen tuning set. However, the
translation quality of the single systems provided
by KIT is stable.
As initial approach and for the final submis-
sion, we grouped single systems with dissimilar
approaches. Thus, KIT (phrase-based SMT) and
SYSTRAN (rule-based MT) tuned their system on
newstest2010, while RWTH (phrase-based SMT)
and LIMSI (n-gram) optimized on newstest2009.
To compare the impact of this approach, all pos-
sible combinations were checked (Table 2). How-
ever, it seems that the translation quality can not be
improved by this approach. For the test set (new-
stest2012), BLEU is steady around 25.6 points.
Even if the single system with lowest BLEU are
combined (KIT 2010, LIMSI 2009, SYSTRAN
2010, RWTH 2009), the translation quality in
terms of BLEU is comparable with the combina-
tion of the best single systems (KIT 2009, LIMSI
2010, SYSTRAN 2010, RWTH 2010). However,
we could gain 1.0 point in TER.
Due to the fact, that for the final submission the
initial grouping was available only, we kept this
189
Table 2: Comparison of different system combination settings. For each possible combination of systems
tuned on different tuning sets, a system combination was set up, re-tuned on newstest2011 and evaluated
on newstest2012. The setting used for further experiments is set in boldface.
single systems system combinations
KIT LIMSI SYSTRAN RWTH newstest2011 newstest2012
tuned on newstest BLEU TER BLEU TER
2009 2009 2009 2009 24.6 58.0 25.6 56.8
2010 2010 2010 2010 24.2 58.1 25.6 57.7
2010 2009 2009 2009 24.5 57.9 25.7 57.4
2009 2010 2009 2009 24.4 58.3 25.7 57.0
2009 2009 2010 2009 24.5 57.9 25.6 57.0
2009 2009 2009 2010 24.5 58.0 25.6 56.8
2009 2010 2010 2010 24.1 57.5 25.4 56.4
2010 2009 2010 2010 24.3 57.6 25.6 56.9
2010 2010 2009 2010 24.2 58.0 25.6 57.3
2010 2010 2010 2009 24.3 57.9 25.5 57.6
2010 2010 2009 2009 24.4 58.1 25.6 57.5
2009 2009 2010 2010 24.4 57.8 25.5 56.6
2009 2010 2010 2009 24.4 58.2 25.5 57.0
2009 2010 2009 2010 24.2 57.8 25.5 56.8
2010 2009 2009 2010 24.4 57.9 25.6 57.4
2010 2009 2010 2009 24.4 57.7 25.6 57.4
Table 3: Results of the final submission (bold-
face) compared with best single system on new-
stest2012.
newstest2011 newstest2012
BLEU TER BLEU TER
best single 23.2 60.9 24.6 58.4
system comb. 24.4 57.7 25.6 57.4
+ IBM-1 24.6 58.1 25.6 57.6
+ bigLM 24.6 57.9 25.8 57.2
combination. To improve this baseline further, two
additional models were added. We applied lexi-
cal smoothing (IBM-1) and an additional language
model (bigLM) trained on the English side of the
parallel data and the News shuffle corpus. The re-
sults are presented in Table 3.
The baseline was slightly improved by 0.2
points in BLEU and TER. Note, this system com-
bination was the final submission.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, the partners of the QUAERO
project (Karlsruhe Institute of Technology, RWTH
Aachen University, LIMSI-CNRS and SYSTRAN
Software, Inc.) provided a joint submission. By
joining the output of four different translation sys-
tems with RWTH?s system combination, we re-
ported an improvement of up to 1.2 points in
BLEU and TER.
Combining systems optimized on different tun-
ing sets does not seem to improve the translation
quality. However, by adding additional model, the
baseline was slightly improved.
All in all, we conclude that the variability in
terms of BLEU does not influence the final result.
It seems that using different approaches of MT in
a system combination is more important (Freitag
et al, 2012).
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
190
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Josep M. Crego and Jose? B. Mario. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on systran?s rule-based trans-
lation system. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 220?223, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Daniel Dchelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Markus Freitag, Stephan Peitz, Matthias Huck, Her-
mann Ney, Teresa Herrmann, Jan Niehues, Alex
Waibel, Alexandre Allauzen, Gilles Adda, Bianka
Buschbeck, Josep Maria Crego, and Jean Senellart.
2012. Joint wmt 2012 submission of the quaero
project. In NAACL 2012 Seventh Workshop on Sta-
tistical Machine Translation, pages 322?329, Mon-
treal, Canada, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 847?855, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantine, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
pages 177?180, Prague, Czech Republic, June.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An Automatic Metric for MT Evaluation
with High Levels of Correlation with Human Judg-
ments. pages 228?231, Prague, Czech Republic,
June.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Saab Mansour and Hermann Ney. 2012. A sim-
ple and effective weighted phrase extraction for ma-
chine translation adaptation. In International Work-
shop on Spoken Language Translation, pages 193?
200, Hong Kong, December.
Sab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA,
December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
191
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation Systems for IWSLT
2011. In Proceedings of the Eighth Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003a. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sap-
poro, Japan, July.
Franz Josef Och. 2003b. Minimum error rate training
in statistical machine translation. In ACL ?03: Proc.
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Evelyne Tzoukermann and SusanEditors Arm-
strong, editors, Proceedings of the ACL SIGDAT-
Workshop, pages 47?50. Kluwer Academic Publish-
ers.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Int. Conf. on Spo-
ken Language Processing, volume 2, pages 901?
904, Denver, Colorado, USA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
192
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 193?199,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2013
Stephan Peitz, Saab Mansour, Jan-Thorsten Peter, Christoph Schmidt,
Joern Wuebker, Matthias Huck, Markus Freitag and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
This paper describes the statistical ma-
chine translation (SMT) systems devel-
oped at RWTH Aachen University for
the translation task of the ACL 2013
Eighth Workshop on Statistical Machine
Translation (WMT 2013). We partici-
pated in the evaluation campaign for the
French-English and German-English lan-
guage pairs in both translation directions.
Both hierarchical and phrase-based SMT
systems are applied. A number of dif-
ferent techniques are evaluated, including
hierarchical phrase reordering, translation
model interpolation, domain adaptation
techniques, weighted phrase extraction,
word class language model, continuous
space language model and system combi-
nation. By application of these methods
we achieve considerable improvements
over the respective baseline systems.
1 Introduction
For the WMT 2013 shared translation task1
RWTH utilized state-of-the-art phrase-based and
hierarchical translation systems as well as an in-
house system combination framework. We give
a survey of these systems and the basic meth-
ods they implement in Section 2. For both
the French-English (Section 3) and the German-
English (Section 4) language pair, we investigate
several different advanced techniques. We con-
centrate on specific research directions for each
of the translation tasks and present the respec-
tive techniques along with the empirical results
they yield: For the French?English task (Sec-
tion 3.2), we apply a standard phrase-based sys-
tem with up to five language models including a
1http://www.statmt.org/wmt13/
translation-task.html
word class language model. In addition, we em-
ploy translation model interpolation and hierarchi-
cal phrase reordering. For the English?French
task (Section 3.1), we train translation mod-
els on different training data sets and augment
the phrase-based system with a hierarchical re-
ordering model, a word class language model,
a discriminative word lexicon and a insertion
and deletion model. For the German?English
(Section 4.3) and English?German (Section 4.4)
tasks, we utilize morpho-syntactic analysis to pre-
process the data (Section 4.1), domain-adaptation
(Section 4.2) and a hierarchical reordering model.
For the German?English task, an augmented hi-
erarchical phrase-based system is set up and we
rescore the phrase-based baseline with a continu-
ous space language model. Finally, we perform a
system combination.
2 Translation Systems
In this evaluation, we employ phrase-based trans-
lation and hierarchical phrase-based translation.
Both approaches are implemented in Jane (Vilar et
al., 2012; Wuebker et al, 2012), a statistical ma-
chine translation toolkit which has been developed
at RWTH Aachen University and is freely avail-
able for non-commercial use.2
2.1 Phrase-based System
In the phrase-based decoder (source cardinality
synchronous search, SCSS), we use the standard
set of models with phrase translation probabilities
and lexical smoothing in both directions, word and
phrase penalty, distance-based distortion model,
an n-gram target language model and three bi-
nary count features. Optional additional models
used in this evaluation are the hierarchical reorder-
ing model (HRM) (Galley and Manning, 2008), a
word class language model (WCLM) (Wuebker et
2http://www.hltpr.rwth-aachen.de/jane/
193
al., 2012), a discriminative word lexicon (DWL)
(Mauser et al, 2009), and insertion and deletion
models (IDM) (Huck and Ney, 2012). The param-
eter weights are optimized with minimum error
rate training (MERT) (Och, 2003). The optimiza-
tion criterion is BLEU.
2.2 Hierarchical Phrase-based System
In hierarchical phrase-based translation (Chiang,
2007), a weighted synchronous context-free gram-
mar is induced from parallel text. In addition to
continuous lexical phrases, hierarchical phrases
with up to two gaps are extracted. The search is
carried out with a parsing-based procedure. The
standard models integrated into our Jane hierar-
chical systems (Vilar et al, 2010; Huck et al,
2012c) are: phrase translation probabilities and
lexical smoothing probabilities in both translation
directions, word and phrase penalty, binary fea-
tures marking hierarchical phrases, glue rule, and
rules with non-terminals at the boundaries, four
binary count features, and an n-gram language
model. Optional additional models comprise IBM
model 1 (Brown et al, 1993), discriminative word
lexicon and triplet lexicon models (Mauser et al,
2009; Huck et al, 2011), discriminative reordering
extensions (Huck et al, 2012a), insertion and dele-
tion models (Huck and Ney, 2012), and several
syntactic enhancements like preference grammars
(Stein et al, 2010) and soft string-to-dependency
features (Peter et al, 2011). We utilize the cube
pruning algorithm for decoding (Huck et al, 2013)
and optimize the model weights with MERT. The
optimization criterion is BLEU.
2.3 System Combination
System combination is used to produce consensus
translations from multiple hypotheses generated
with different translation engines. First, a word
to word alignment for the given single system hy-
potheses is produced. In a second step a confusion
network is constructed. Then, the hypothesis with
the highest probability is extracted from this con-
fusion network. For the alignment procedure, one
of the given single system hypotheses is chosen as
primary system. To this primary system all other
hypotheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1.
The model weights of the system combination
are optimized with standard MERT on 100-best
lists. For each single system, a factor is added to
the log-linear framework of the system combina-
tion. Moreover, this log-linear model includes a
word penalty, a language model trained on the in-
put hypotheses, a binary feature which penalizes
word deletions in the confusion network and a pri-
mary feature which marks the system which pro-
vides the word order. The optimization criterion is
4BLEU-TER.
2.4 Other Tools and Techniques
We employ GIZA++ (Och and Ney, 2003) to train
word alignments. The two trained alignments are
heuristically merged to obtain a symmetrized word
alignment for phrase extraction. All language
models (LMs) are created with the SRILM toolkit
(Stolcke, 2002) and are standard 4-gram LMs
with interpolated modified Kneser-Ney smooth-
ing (Kneser and Ney, 1995; Chen and Goodman,
1998). The Stanford Parser (Klein and Manning,
2003) is used to obtain parses of the training data
for the syntactic extensions of the hierarchical sys-
tem. We evaluate in truecase with BLEU (Papineni
et al, 2002) and TER (Snover et al, 2006).
2.5 Filtering of the Common Crawl Corpus
The new Common Crawl corpora contain a large
number of sentences that are not in the labelled
language. To clean these corpora, we first ex-
tracted a vocabulary from the other provided cor-
pora. Then, only sentences containing at least
70% word from the known vocabulary were kept.
In addition, we discarded sentences that contain
more words from target vocabulary than source
vocabulary on the source side. These heuristics
reduced the French-English Common Crawl cor-
pus by 5,1%. This filtering technique was also ap-
plied on the German-English version of the Com-
mon Crawl corpus.
3 French?English Setups
We trained phrase-based translation systems for
French?English and for English?French. Cor-
pus statistics for the French-English parallel data
are given in Table 1. The LMs are 4-grams trained
on the provided resources for the respective lan-
guage (Europarl, News Commentary, UN, 109,
Common Crawl, and monolingual News Crawl
194
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
Table 1: Corpus statistics of the preprocessed
French-English parallel training data. EPPS de-
notes Europarl, NC denotes News Commentary,
CC denotes Common Crawl. In the data, numeri-
cal quantities have been replaced by a single cate-
gory symbol.
French English
EPPS Sentences 2.2M
+ NC Running Words 64.7M 59.7M
Vocabulary 153.4K 132.2K
CC Sentences 3.2M
Running Words 88.1M 80.9.0M
Vocabulary 954.8K 908.0K
UN Sentences 12.9M
Running Words 413.3M 362.3M
Vocabulary 487.1K 508.3K
109 Sentences 22.5M
Running Words 771.7M 661.1M
Vocabulary 1 974.0K 1 947.2K
All Sentences 40.8M
Running Words 1 337.7M 1 163.9M
Vocabulary 2 749.8K 2 730.1K
language model training data).3
3.1 Experimental Results English?French
For the English?French task, separate translation
models (TMs) were trained for each of the five
data sets and fed to the decoder. Four additional
indicator features are introduced to distinguish the
different TMs. Further, we applied the hierar-
chical reordering model, the word class language
model, the discriminative word lexicon, and the
insertion and deletion model. Table 2 shows the
results of our experiments.
As a development set for MERT, we use new-
stest2010 in all setups.
3.2 Experimental Results French?English
For the French?English task, a translation model
(TM) was trained on all available parallel data.
For the baseline, we interpolated this TM with
3The parallel 109 corpus is often also referred to as WMT
Giga French-English release 2.
an in-domain TM trained on EPPS+NC and em-
ployed the hierarchical reordering model. More-
over, three language models were used: The first
language model was trained on the English side
of all available parallel data, the second one on
EPPS and NC and the third LM on the News Shuf-
fled data. The baseline was improved by adding a
fourth LM trained on the Gigaword corpus (Ver-
sion 5) and a 5-gram word class language model
trained on News Shuffled data. For the WCLM,
we used 50 word classes clustered with the tool
mkcls (Och, 2000). All results are presented in Ta-
ble 3.
4 German?English Setups
For both translation directions of the German-
English language pair, we trained phrase-based
translation systems. Corpus statistics for German-
English can be found in Table 4. The language
models are 4-grams trained on the respective tar-
get side of the bilingual data as well as on the
provided News Crawl corpus. For the English
language model the 109 French-English, UN and
LDC Gigaword Fifth Edition corpora are used ad-
ditionally.
4.1 Morpho-syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the German text
is preprocessed by splitting German compound
words with the frequency-based method described
in (Koehn and Knight, 2003). To further reduce
translation complexity, we employ the long-range
part-of-speech based reordering rules proposed by
Popovic? and Ney (2006).
4.2 Domain Adaptation
This year, we experimented with filtering and
weighting for domain-adaptation for the German-
English task. To perform adaptation, we define a
general-domain (GD) corpus composed from the
news-commentary, europarl and Common Crawl
corpora, and an in-domain (ID) corpus using
a concatenation of the test sets (newstest{2008,
2009, 2010, 2011, 2012}) with the correspond-
ing references. We use the test sets as in-domain
195
Table 2: Results for the English?French task (truecase). newstest2010 is used as development set.
BLEU and TER are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011 newstest2012
English?French BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
TM:EPPS + HRM 22.9 63.0 25.0 60.0 27.8 56.7 28.9 54.4 27.2 57.1
TM:UN + HRM 22.7 63.4 25.0 60.0 28.3 56.4 29.5 54.2 27.3 57.1
TM:109 + HRM 23.5 62.3 26.0 59.2 29.6 55.2 30.3 53.3 28.0 56.4
TM:CC + HRM 23.5 62.3 26.2 58.8 29.2 55.3 30.3 53.3 28.2 56.0
TM:NC 21.0 64.8 22.3 61.6 25.6 58.7 26.9 56.6 25.7 58.5
+ HRM 21.5 64.3 22.6 61.2 26.1 58.4 27.3 56.1 26.0 58.2
+ TM:EPPS,CC,UN 23.9 61.8 26.4 58.6 29.9 54.7 31.0 52.7 28.6 55.6
+ TM:109 24.0 61.5 26.5 58.4 30.2 54.2 31.1 52.3 28.7 55.3
+ WCLM, DWL, IDM 24.0 61.6 26.5 58.3 30.4 54.0 31.4 52.1 28.8 55.2
Table 3: Results for the French?English task (truecase). newstest2010 is used as development set.
BLEU and TER are given in percentage.
newstest2010 newstest2011 newstest2012
French?English BLEU TER BLEU TER BLEU TER
SCSS baseline 28.1 54.6 29.1 53.3 - -
+ GigaWord.v5 LM 28.6 54.2 29.6 52.9 29.6 53.3
+ WCLM 29.1 53.8 30.1 52.5 29.8 53.1
(newswire) as the other corpora are coming from
differing domains (news commentary, parliamen-
tary discussions and various web sources), and on
initial experiments, the other corpora did not per-
form well when used as an in-domain representa-
tive for adaptation. To check whether over-fitting
occurs, we measure the results of the adapted
systems on the evaluation set of this year (new-
stest2013) which was not used as part of the in-
domain set.
The filtering experiments are done similarly to
(Mansour et al, 2011), where we compare filtering
using LM and a combined LM and IBM Model 1
(LM+M1) based scores. The scores for each sen-
tence pair in the general-domain corpus are based
on the bilingual cross-entropy difference of the
in-domain and general-domain models. Denoting
HLM (x) as the cross entropy of sentence x ac-
cording to LM , then the cross entropy difference
DHLM (x) can be written as:
DHLM (x) = HLMID(x)?HLMGD(x)
The bilingual cross entropy difference for a sen-
tence pair (s, t) in the GD corpus is then defined
by:
DHLM (s) + DHLM (t)
For IBM Model 1 (M1), the cross-entropy
HM1(s|t) is defined similarly to the LM cross-
entropy, and the resulting bilingual cross-entropy
difference will be of the form:
DHM1(s|t) + DHM1(t|s)
The combined LM+M1 score is obtained by
summing the LM and M1 bilingual cross-entropy
difference scores. To perform filtering, the GD
corpus sentence pairs are scored by the appropri-
ate method, sorted by the score, and the n-best sen-
tences are then used to build an adapted system.
In addition to adaptation using filtering, we ex-
periment with weighted phrase extraction similar
to (Mansour and Ney, 2012). We differ from their
work by using a combined LM+M1 weight to per-
form the phrase extraction instead of an LM based
weight. We use a combined LM+M1 weight as
this worked best in the filtering experiments, mak-
ing scoring with LM+M1 more reliable than LM
scores only.
4.3 Experimental Results German?English
For the German?English task, the baseline is
trained on all available parallel data and includes
the hierarchical reordering model. The results of
the various filtering and weighting experiments are
summarized in Table 5.
196
Table 5: German-English results (truecase). BLEU and TER are given in percentage. Corresponding
development set is marked with *. ? labels the single systems selected for the system combination.
newstest2009 newstest2010 newstest2011 newstest2012 newstest2013
German?English BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
SCSS baseline 21.7 61.1 24.8* 58.9* 22.0 61.1 23.4 60.0 26.1 56.4
LM 800K-best 21.6 60.5 24.7* 58.3* 22.0 60.5 23.6 59.7 - -
LM+M1 800K-best 21.4 60.5 24.7* 58.1* 22.0 60.4 23.7 59.2 - -
(LM+M1)*TM 22.1 60.2 25.4* 57.8* 22.5 60.1 24.0 59.1 - -
(LM+M1)*TM+GW 22.8 59.5 25.7* 57.2* 23.1 59.5 24.4 58.6 26.6 55.5
(LM+M1)*TM+GW? 22.9* 61.1* 25.2 59.3 22.8 61.5 23.7 60.8 26.4 57.1
SCSS baseline 22.6* 61.6* 24.1 60.1 22.1 62.0 23.1 61.2 - -
CSLM rescoring? 22.0 60.4 25.1* 58.3* 22.4 60.2 23.9 59.3 26.0 56.0
HPBT? 21.9 60.4 24.9* 58.2* 22.3 60.3 23.6 59.6 25.9 56.3
system combination - - - - 23.4* 59.3* 24.7 58.5 27.1 55.3
Table 6: English-German results (truecase). newstest2009 was used as development set. BLEU and TER
are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011 newstest2012
English?German BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
SCSS baseline 14.9 70.9 14.9 70.4 16.0 66.3 15.4 69.5 15.7 67.5
LM 800K-best 15.1 70.9 15.1 70.3 16.2 66.3 15.6 69.4 15.9 67.4
(LM+M1) 800K-best 15.8 70.8 15.4 70.0 16.2 66.2 16.0 69.3 16.1 67.4
(LM+M1) ifelse 16.1 70.6 15.7 69.9 16.5 66.0 16.2 69.2 16.3 67.2
Table 4: Corpus statistics of the preprocessed
German-English parallel training data (Europarl,
News Commentary and Common Crawl). In the
data, numerical quantities have been replaced by a
single category symbol.
German English
Sentences 4.1M
Running Words 104M 104M
Vocabulary 717K 750K
For filtering, we use the 800K best sentences
from the whole training corpora, as this se-
lection performed best on the dev set among
100K,200K,400K,800K,1600K setups. Filtering
seems to mainly improve on the TER scores, BLEU
scores are virtually unchanged in comparison to
the baseline. LM+M1 filtering improves further
on TER in comparison to LM-based filtering.
The weighted phrase extraction performs best
in our experiments, where the weights from the
LM+M1 scoring method are used. Improvements
in both BLEU and TER are achieved, with BLEU
improvements ranging from +0.4% up-to +0.6%
and TER improvements from -0.9% and up-to -
1.1%.
As a final step, we added the English Gigaword
corpus to the LM (+GW). This resulted in further
improvements of the systems.
In addition, the system as described above was
tuned on newstest2009. Using this development
set results in worse translation quality.
Furthermore, we rescored the SCSS baseline
tuned on newstest2009 with a continuous space
language model (CSLM) as described in (Schwenk
et al, 2012). The CSLM was trained on the eu-
roparl and news-commentary corpora. For rescor-
ing, we used the newstest2011 set as tuning set and
re-optimized the parameters with MERT on 1000-
best lists. This results in an improvement of up to
0.8 points in BLEU compared to the baseline.
We compared the phrase-based setups with a
hierarchical translation system, which was aug-
mented with preference grammars, soft string-
to-dependency features, discriminative reordering
extensions, DWL, IDM, and discriminative re-
197
ordering extensions. The phrase table of the hier-
archical setup has been extracted from News Com-
mentary and Europarl parallel data only (not from
Common Crawl).
Finally, three setups were joined in a system
combination and we gained an improvement of up
to 0.5 points in BLEU compared to the best single
system.
4.4 Experimental Results English?German
The results for the English?German task are
shown in Table 6. While the LM-based filter-
ing led to almost no improvement over the base-
line, the LM+M1 filtering brought some improve-
ments in BLEU. In addition to the sentence fil-
tering, we tried to combine the translation model
trained on NC+EPPS with a TM trained on Com-
mon Crawl using the ifelse combination (Mansour
and Ney, 2012). This combination scheme con-
catenates both TMs and assigns the probabilities
of the in-domain TM if it contains the phrase,
else it uses the probabilities of the out-of-domain
TM. Appling this method, we achieved further im-
provements.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, RWTH experimented with both
phrase-based and hierarchical translation systems.
Several different techniques were evaluated and
yielded considerable improvements over the re-
spective baseline systems as well as over our last
year?s setups (Huck et al, 2012b). Among these
techniques are a hierarchical phrase reordering
model, translation model interpolation, domain
adaptation techniques, weighted phrase extraction,
a word class language model, a continuous space
language model and system combination.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311, June.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, Massachusetts, USA, August.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201?
228.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, Hawaii, USA,
October.
Matthias Huck and Hermann Ney. 2012. Insertion and
Deletion Models for Statistical Machine Translation.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics - Human
Language Technologies conference, pages 347?351,
Montre?al, Canada, June.
Matthias Huck, Saab Mansour, Simon Wiesler, and
Hermann Ney. 2011. Lexicon Models for Hierar-
chical Phrase-Based Machine Translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 191?198, San
Francisco, California, USA, December.
Matthias Huck, Stephan Peitz, Markus Freitag, and
Hermann Ney. 2012a. Discriminative Reordering
Extensions for Hierarchical Phrase-Based Machine
Translation. In 16th Annual Conference of the Eu-
ropean Association for Machine Translation, pages
313?320, Trento, Italy, May.
Matthias Huck, Stephan Peitz, Markus Freitag, Malte
Nuhn, and Hermann Ney. 2012b. The RWTH
Aachen Machine Translation System for WMT
2012. In NAACL 2012 Seventh Workshop on
Statistical Machine Translation, pages 304?311,
Montre?al, Canada, June.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012c. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Matthias Huck, David Vilar, Markus Freitag, and
Hermann Ney. 2013. A Performance Study of
Cube Pruning for Large-Scale Hierarchical Machine
Translation. In Proceedings of the NAACL 7thWork-
shop on Syntax, Semantics and Structure in Statis-
tical Translation, pages 29?38, Atlanta, Georgia,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proc. of the 41th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 423?430, Sapporo, Japan,
July.
198
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-gram Language Model-
ing. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 181?184, May.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings of
European Chapter of the ACL (EACL 2009), pages
187?194.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
ACL 2007 Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic, June.
Saab Mansour and Hermann Ney. 2012. A Simple and
Effective Weighted Phrase Extraction for Machine
Translation Adaptation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 193?200, Hong Kong, December.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, California, USA, December.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proc. of the Conf. on Empirical Methods for Natu-
ral Language Processing (EMNLP), pages 210?218,
Singapore, August.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2000. mkcls: Training
of word classes for language modeling.
http://www.hltpr.rwth-aachen.de/
web/Software/mkcls.html.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
Jan-Thorsten Peter, Matthias Huck, Hermann Ney, and
Daniel Stein. 2011. Soft String-to-Dependency
Hierarchical Machine Translation. In International
Workshop on Spoken Language Translation, pages
246?253, San Francisco, California, USA, Decem-
ber.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Transla-
tion. In International Conference on Language Re-
sources and Evaluation, pages 1278?1283, Genoa,
Italy, May.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT 2012 Workshop: Will
We Ever Really Replace the N-gram Model? On the
Future of Language Modeling for HLT, pages 11?
19, Montre?al, Canada, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Fea-
tures for Hierarchical Machine Translation. In Conf.
of the Association for Machine Translation in the
Americas (AMTA), Denver, Colorado, USA, Octo-
ber/November.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, Colorado, USA,
September.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statis-
tical Machine Translation. In International Confer-
ence on Computational Linguistics, pages 483?491,
Mumbai, India, December.
199
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 457?465,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Unsupervised Adaptation for Statistical Machine Translation
Saab Mansour and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department
RWTH Aachen University
Aachen, Germany
{mansour,ney}@cs.rwth-aachen.de
Abstract
In this work, we tackle the problem of
language and translation models domain-
adaptation without explicit bilingual in-
domain training data. In such a scenario,
the only information about the domain
can be induced from the source-language
test corpus. We explore unsupervised
adaptation, where the source-language test
corpus is combined with the correspond-
ing hypotheses generated by the transla-
tion system to perform adaptation. We
compare unsupervised adaptation to su-
pervised and pseudo supervised adapta-
tion. Our results show that the choice of
the adaptation (target) set is crucial for
successful application of adaptation meth-
ods. Evaluation is conducted over the
German-to-English WMT newswire trans-
lation task. The experiments show that the
unsupervised adaptation method generates
the best translation quality as well as gen-
eralizes well to unseen test sets.
1 Introduction
Over the last few years, large amounts of statistical
machine translation (SMT) monolingual and bilin-
gual corpora were collected. Early years focused
on structured data translation such as newswire.
Nowadays, due to the relative success of SMT,
new domains of translation are being explored,
such as lecture and patent translation (Cettolo et
al., 2012; Goto et al., 2013).
The task of domain adaptation tackles the prob-
lem of utilizing existing resources mainly drawn
from one domain (e.g. parliamentary discussion)
to maximize the performance on the target (test)
domain (e.g. newswire).
To be able to perform adaptation, a target set
representing the test domain is used to manipu-
late the general-domain models. Previous work
on SMT adaptation focused on the scenario where
(small) bilingual in-domain or pseudo in-domain
training data are available. Furthermore, small at-
tention was given to the choice of the target set for
adaptation. In this work, we explore the problem
of adaptation where no explicit bilingual data from
the test domain is available for training, and the
only resource encapsulating information about the
domain is the source-language test corpus itself.
We explore how to utilize the source-language
test corpus for adapting the language model (LM)
and the translation model (TM). A combination
of source and automatically translated target of
the test set is compared to using the source side
only for TM adaptation. Furthermore, we com-
pare using the test set to using in-domain data and
a pseudo in-domain data (e.g. news-commentary
as opposed to newswire).
Experiments are done on the WMT 2013
German-to-English newswire translation task.
Our best adaptation method shows competitive re-
sults to the best submissions of the evaluation.
This paper is structured as follows. We review
related work in Section 2 and introduce the basic
adaptation methods in Section 3. The experimen-
tal setup is described in Section 4, results are dis-
cussed in Section 5 and we conclude in Section 6.
2 Related Work
A broad range of methods and techniques have
been suggested in the past for domain adaptation
for both SMT and automatic speech recognition
(ASR).
For ASR, (Bellegarda, 2004) gives an overview
of LM adaptation methods. He differentiates be-
tween two cases regarding the availability of in-
domain adaptation data: (i) the data is available
and can be directly used to manipulate a back-
ground (general domain) corpus, and (ii) the data
is not available or too small, and then it can be
gathered or automatically generated during the
457
recognition process. (Bacchiani and Roark, 2003)
compare supervised against unsupervised (using
automatic transcriptions) in-domain data for LM
training for the task of ASR. They show that aug-
menting the supervised in-domain to the train-
ing of the LM performs better than the unsuper-
vised in-domain. In addition, they perform ?self-
training?, where the test set is automatically tran-
scribed and added to the LM. When using a strong
baseline, no improvements in recognition quality
are achieved. We differ from their work by us-
ing the unsupervised test data to adapt a general-
domain bilingual corpus. We also performed ini-
tial experiments of ?self-training? for language
modeling, where (artificial) perplexity improve-
ment was achieved but without an impact on the
machine translation (MT) quality.
(Zhao et al., 2004) tackle LM adaptation for
SMT. Similarly to our work, they use automati-
cally generated hypotheses to perform adaptation.
We extend their work by using the hypotheses
also for TM adaptation. (Hildebrand et al., 2005)
perform LM and TM adaptation based on infor-
mation retrieval methods. They use the source-
language test corpus to filter the bilingual data,
and then use the target side of the filtered bilingual
data to perform LM adaptation. We differ from
their work by using both the in-domain source-
language corpus and its corresponding automatic
translation for adaptation, which is shown in our
experiments to achieve superior results than when
using the source-side information only. (Foster
and Kuhn, 2007) perform LM and TM adaptation
using mixture modeling. In their setting, the mix-
ture weights are modified to express adaptation.
They compare cross-domain (in-domain available)
against dynamic adaptation. In the dynamic adap-
tation scenario, they utilize the source side of the
development set to adapt the mixture weights (LM
adaptation is possible as they only use parallel
training data, which enables filtering based on the
source side and then keeping the corresponding
target side of the data). For an in-domain test set,
the cross-domain setup performs better than the
dynamic adaptation method. (Ueffing et al., 2007)
use the test set translations as additional data to
train the TM. One important aspect in their work
is confidence measurement to remove noisy trans-
lation. In our approach, we use the automatic test
set translations to adapt the SMT models rather
than augmenting it as additional TM data. We also
compare different adaptation sets. Furthermore,
we do not use confidence measures to filter the au-
tomatic translations as they are only used to adapt
the general-domain system and are not augmented
to the TM.
In this work, we apply cross-entropy scoring for
adaptation as done by (Moore and Lewis, 2010).
Moore and Lewis (2010) apply adaptation by us-
ing an LM-based cross-entropy filtering for LM
training. Axelrod et al. (2011) generalized the
method for TM adaptation by interpolating the
source and target LMs. These two works focused
on a scenario where in-domain training data are
available for adaptation. In this work, we focus on
a scenario where in-domain training data is not la-
beled, and the main resource for adaptation is the
source-language test data.
In recent WMT evaluations, the method of
(Moore and Lewis, 2010) was utilized by several
translation systems (Koehn and Haddow, 2012;
Rubino et al., 2013). These systems use pseudo
in-domain corpus, i.e., news-commentary, as the
target domain (while the test domain is newswire).
The contribution of this work is two fold: we
show that the choice of the target set is crucial for
adaptation, in addition, we show that an unsuper-
vised target set performs best in terms of transla-
tion quality as well as generalization performance
to unseen test sets (in comparison to using pseudo
in-domain data or the references as target sets).
3 Cross-Entropy Adaptation
In this work, we use sample scoring for the pur-
pose of adaptation. We start by introducing the
scoring framework and then show how we utilize it
to perform filtering based adaptation and weighted
phrase extraction based adaptation.
LM cross-entropy scores can be used for both
monolingual data weighting for LM training as
done by (Moore and Lewis, 2010), or bilingual
weighting for TM training as done by (Axelrod et
al., 2011).
We differentiate between two types of data sets:
the adaptation set (target) representative of the
test-domain which we refer to also as in-domain
(IN), and the general-domain (GD) set which we
want to adapt.
The scores for each sentence in the general-
domain corpus are based on the cross-entropy dif-
ference of the IN and GD models. Denoting
H
M
(x) as the cross entropy of sentence x accord-
458
ing to model M , then the cross entropy difference
DH
M
(x) can be written as:
DH
M
(x) = H
M
IN
(x)?H
M
GD
(x) (1)
The intuition behind eq. (1) is that we are inter-
ested in sentences as close as possible to the in-
domain, but also as far as possible from the gen-
eral corpus. Moore and Lewis (2010) show that
using eq. (1) for LM filtering performs better in
terms of perplexity than using in-domain cross-
entropy only (H
M
IN
(x)). For more details about
the reasoning behind eq. (1) we refer the reader to
(Moore and Lewis, 2010).
Axelrod et al. (2011) adapted eq. (1) for bilin-
gual data filtering for the purpose of TM training.
The bilingual LM cross entropy difference for a
sentence pair (f
r
, e
r
) in the GD corpus is then de-
fined by:
DH
LM
(f
r
, e
r
) = DH
LM
src
(f
r
) +DH
LM
trg
(e
r
)
(2)
For IBM Model 1 (M1), the cross-entropy
H
M1
(f
r
|e
r
) is defined similarly to the LM cross-
entropy, and the resulting bilingual cross-entropy
difference will be of the form:
DH
M1
(f
r
, e
r
) = DH
M1
(f
r
|e
r
) +DH
M1
(e
r
|f
r
)
The combined LM+M1 score is obtained by
summing the LM and M1 bilingual cross-entropy
difference scores:
d
r
= DH
LM
(f
r
, e
r
) +DH
M1
(f
r
, e
r
) (3)
3.1 Filtering
A common framework to perform sample filtering
is to score each sample according to a model, and
then assigning a threshold on the score which fil-
ters out unwanted samples. If the score we gener-
ate is related to the probability that the sample was
drawn from the same distribution as the in-domain
data, we are selecting the samples most relevant to
our domain. In this way we can achieve adaptation
of the general-domain data.
We use the LM cross-entropy difference from
eq. (1) for LM filtering and a combined LM+M1
score (eq. (3) for TM filtering. We sort the sen-
tences in the general-domain according to the
score and select the best 50%,25%,...,6.25% train-
ing instances. Our models are then trained on
the selected portions of the training data, and the
best performing portion (according to perplexity
for LM training and BLEU for TM training) on the
development set is chosen as the adapted corpus.
3.2 Weighted Phrase Extraction
The classical phrase model is trained using a ?sim-
ple? maximum likelihood estimation, resulting in
phrase translation probabilities being defined by
relative frequency:
p(
?
f |e?) =
?
r
c
r
(
?
f, e?)
?
?
f
?
?
r
c
r
(
?
f
?
, e?)
(4)
Here,
?
f, e? are contiguous phrases, c
r
(
?
f, e?) de-
notes the count of (
?
f, e?) being a translation of each
other (usually according to word alignment and
heuristics) in sentence pair (f
r
, e
r
). One method
to introduce weights to eq. (4) is by weighting
each sentence pair by a weight w
r
. Eq. (4) will
now have the extended form:
p(
?
f |e?) =
?
r
w
r
? c
r
(
?
f, e?)
?
?
f
?
?
r
w
r
? c
r
(
?
f
?
, e?)
(5)
It is easy to see that setting {w
r
= 1} will result
in eq. (4) (or any non-zero equal weights). Increas-
ing the weight w
r
of the corresponding sentence
pair will result in an increase of the probabilities
of the phrase pairs extracted. Thus, by increasing
the weight of in-domain sentence pairs, the prob-
ability of in-domain phrase translations could also
increase.
We utilize d
r
from eq. (3) using a combined
LM+M1 scores for our suggested weighted phrase
extraction. d
r
can be assigned negative values, and
lower d
r
indicates sentence pairs which are more
relevant to the in-domain. Therefore, we negate
the term d
r
to get the notion of higher is closer
to the in-domain, and use an exponent to ensure
positive values. The final weight is of the form:
w
r
= e
?d
r
(6)
This term is proportional to perplexities, as the
exponent of entropy is perplexity by definition.
One could also use filtering for TM adaptation,
but, as shown in (Mansour and Ney, 2012), filter-
ing for TM could only reduce the size and weight-
ing performs better than filtering.
4 Experimental Setup
4.1 Training Data
The experiments are done on the recent German-
to-English WMT 2013 translation task
1
. For
1
The translation task resources of WMT 2013 are avail-
able under: http://www.statmt.org/wmt13/
459
Corpus Sent De En
Training data
news-commentary 177K 4.8M 4.5M
europarl 1 888K 51.5M 51.9M
common-crawl 2 030K 47.8M 47.7M
total 4 095K 104.1M 104M
Test data
newstest08 2051 52446 49749
newstest09 2525 68512 65648
newstest10 2489 68232 62024
newstest11 3003 80181 74856
newstest12 3003 79912 73089
newstest13 3000 69066 64900
Table 1: German-English bilingual training and
test data statistics: the number of sentence pairs
(Sent), German (De) and English (En) words are
given.
German-English WMT 2013, the common-crawl
bilingual corpus was introduced, enabling more
impact for TM adaptation on the SMT system
quality. Monolingual English data exists with
more than 1 billion words, making LM adapta-
tion and size reduction a wanted feature. We use
newstest08 throughout newstest13 to evaluate the
SMT systems. The baseline systems are built
using all (unfiltered) available monolingual and
bilingual training data. The bilingual corpora and
the test data statistics are summarized in Table 1.
In Table 2, we summarize the size and LM per-
plexity of the different monolingual corpora for
the German-English task over the LM develop-
ment set newstest09 and test set newstest13. The
corpora are split into three parts, the English side
of the bilingual side (bi.en), the giga-fren joined
with undoc (giun) and the news-shuffle (ns) cor-
pus. To keep the perplexity results comparable,
we use the intersection vocabulary of the different
corpora as a reference vocabulary. From the table,
we notice that as expected, the in-domain corpus
news-shuffle generate the best perplexity values.
4.2 SMT System
The baseline system is built using the open-source
SMT toolkit Jane
2
, which provides state-of-the-art
phrase-based SMT system (Wuebker et al., 2012).
We use the standard set of models with phrase
translation probabilities for source-to-target and
2
www.hltpr.rwth-aachen.de/jane
Corpus Tokens ppl
[M] dev test
bi.en 88 216.5 192.7
giun 775 229.0 198.9
ns 1 479 144.1 122.7
Table 2: German-English monolingual corpora
statistics: the number of tokens is given in millions
[M], ppl is the perplexity of the corresponding cor-
pus.
target-to-source directions, smoothing with lexi-
cal weights, a word and phrase penalty, distance-
based reordering, hierarchical reordering model
(Galley and Manning, 2008) and a 4-gram target
language model. The baseline system is compet-
itive and using adaptation we will show compa-
rable results to the best systems of WMT 2013.
The SMT system was tuned on the development
set newstest10 with minimum error rate training
(MERT) (Och, 2003) using the BLEU (Papineni
et al., 2002) error rate measure as the optimiza-
tion criterion. We test the performance of our sys-
tem on the newstest08...newstest13 sets using the
BLEU and translation edit rate (TER) (Snover et
al., 2006) measures. We use TER as an additional
measure to verify the consistency of our improve-
ments and avoid over-tuning. All results are based
on true-case evaluation. We perform bootstrap re-
sampling with bounds estimation as described by
(Koehn, 2004). We use the 90% and 95% (denoted
by ? and ? correspondingly in the tables) confi-
dence thresholds to draw significance conclusions.
5 Results
To perform adaptation, an adaptation set repre-
senting the in-domain needs to be specified to be
plugged in eq. (1) as IN. The choice of the adap-
tation corpus is crucial for the successful appli-
cation of the cross-entropy based scoring, as the
closer the corpus is to our test domain, the bet-
ter adaptation we get. For the WMT task, the
choice of the adaptation corpus is not an easy
task. The genre of the test sets is newswire, while
the bilingual training data is composed of news-
commentary, parliamentary records (europarl) and
common-crawl noisy data. On the other hand, the
monolingual data includes large amounts of in-
domain newswire data (news-shuffle).
For LM training, the task of adaptation might
be unprofitable in terms of performance, as the
460
 110
 120
 130
 140
 150
 160
 170
6.25% 12.5% 25% 50% 100%
per
ple
xity
size
REF-devREF-testHYP-devHYP-test
Figure 1: Size (fraction of news-shuffle data)
against the resulting LM perplexity on dev and
test, using different filtering sets.
majority of the training is in-domain. Still, one
might hope that by using adaptation, a more com-
pact and comparable LM can be generated. An-
other point is that LM training is less demanding
than TM training, and a comparison of the results
of LM and TM adaptation might prove fruitful and
convey additional information.
Next, we start with LM adaptation experiments
where we mainly compare different adaptation
sets for filtering over the final translation quality.
A comparison to the full (unfiltered LM) is also
produced. For TM adaptation, we repeat the adap-
tation sets choice experiment and analyze the dif-
ference between the sets.
5.1 LM Adaptation
To evaluate our methods experimentally, we use
the German-English translation task to compare
different adaptation sets for filtering and then an-
alyze the full versus the filtered LM SMT system
results. We recall that newstest09 is used as a de-
velopment set and newstest13 as a test set in the
LM experiments.
The different adaptation sets for filtering that we
explore are: (i) unsupervised: an automatic trans-
lation of the test sets (newstest08...newstest13),
where the baseline system (without adaptation)
is used to generate the hypotheses which then
define the adaptation corpus for filtering (HYP),
(ii) supervised: the references of the test sets new-
stest08...newstest12 concatenated, newstest13 is
kept as a blind set, which will also help us deter-
mine if overfitting occurs (REF), and (iii) pseudo
supervised: a pseudo in-domain corpus, news-
Corpus Adapt Optimal ppl
set size dev test
ns
none 100% 144 123
NC 100% 144 123
REF 6.25% 111 161
HYP 50% 139 118
giun
none 100% 229 199
NC 50% 215 185
REF 6.25% 161 171
HYP 12.5% 187 159
Table 3: Optimal size portion and resulting per-
plexities, across adaptation sets (NC, REF and
HYP) and monolingual LM training corpora.
commentary, where the domain is similar to the
test set domain, but the style might differ (NC).
Next, we filter the news-shuffle (ns) and giga-
fren+undoc (giun) according to the three sug-
gested adaptations sets, where we plug each adap-
tation set in eq. (1) as IN and compare their per-
formance.
5.1.1 Perplexity Results
In Figure 1, we draw the size portion versus the
dev and test perplexities for the REF and HYP
adaptation sets over the news-shuffle corpus. REF
performs best for filtering the dev set, where an
optimum is achieved when using only 6.25% of
the news-shuffle data, with a perplexity of 111 in
comparison to 144 perplexity of the full LM. Mea-
suring perplexities over newstest08-12, REF based
filtering achieves 109 while the full LM achieves
140. The good performance on the seen sets
comes with the cost of severe overfitting, where
the test set perplexity using 6.25% of the data is
161, much higher than 123 generated by the full
LM. On the other hand, HYP achieves an optimum
for both sets when using 50% of the data. A sum-
mary of the best results across monolingual cor-
pora and adaptation sets is given in Table 3. Fil-
tering the giun monolingual corpus shows similar
results to ns filtering, where overfitting occurs on
the blind test set when using REF as the target do-
main. HYP-based adaptation achieves the best LM
perplexity on the blind test set. NC-based adapta-
tion retains the biggest amount of data, 50% for
the giun corpus and 100% (no filtering) for the ns
corpus. REF-based adaptation shows overfitting
on the seen dev set, and the worst results on the
blind test set when filtering the ns corpus.
461
LM data Adapt. ppl newstest10 newstest11 newstest12 newstest13
set BLEU TER BLEU TER BLEU TER BLEU TER
bi.en+giun
none 162 23.2 59.6 21.2 61.0 21.8 60.9 24.6 57.2
NC 160 23.2 59.3 21.5 61.0 21.9 60.7 24.6 57.0
REF 158 23.7 59.2 21.9 60.5 22.2 60.5 24.5 57.3
HYP 151 23.6 59.2 21.5 60.9 22.2 60.4 25.1 56.7
+ns
none 111 24.5 59.1 22.1 61.3 23.3 60.1 25.9 56.7
NC 111 24.4 58.7 22.1 60.5 23.4 59.7 25.5 56.6
REF 143 25.7 57.8 23.0 59.9 24.2 59.4 24.1 57.8
HYP 109 25.0 58.2 22.1 60.6 23.5 59.6 25.9 56.3
Table 4: German-English LM filtering results using different adaptation sets. The LM perplexity over
the blind test set nestest13, as well as BLEU and TER percentages are presented.
5.1.2 Translation Results
Next, we measure whether the improvements of
the single adapted corpora carry over to the mix-
ture LM both in perplexity and translation quality.
The mixture LM is created by linear interpolation
(of bi.en, giun and ns) with perplexity minimiza-
tion on the dev set using the SRILM toolkit
3
. We
carry out two experiments, in the first we interpo-
late the English side of the bilingual data with a
giun LM, then we add the ns LM. This way we
measure whether the effects of adaptation carry
over to a stronger baseline.
The SMT systems built using the full and fil-
tered LMs are compared in Table 4. The table
includes the data used for LM training, the adap-
tation set used to filter the data, the perplexity
of the resulting LM on the test set (newstest13)
and the resulting SMT system quality over new-
stest10...newstest13.
Starting with the first block of experiments us-
ing LM data composed from the English side
of the bilingual corpora and the giun corpus
(bi.en+giun), the unfiltered LM performs worse,
both in terms of perplexity and translation qual-
ity. The NC based adaptation improves the results
slightly, with gains upto +0.3% BLEU on new-
stest11 and -0.3% TER on newstest10. The over-
fitting behavior of REF adapted LMs carries over
to the mixture LM, mainly on the translation qual-
ity. The REF adapted LM system translation re-
sults are better on the test sets used to perform the
adaptation, but worse on the blind test set (new-
stest13). The HYP system performs best in terms
of perplexity. REF is better than HYP over the
non-blind test sets, but HYP outperforms REF on
3
http://www.speech.sri.com/projects/srilm/
newstest13 with an improvement of +0.6% BLEU
and -0.6% TER.
The second block of experiments where news-
shuffle (ns) is added to the mixture shows even
stronger overfitting for REF. The REF based adap-
tation is performing worse in terms of perplexity,
143 in comparison to 111 for the full LM. On the
blind set newstest13, REF is hindering the results
with a loss of -1.8% BLEU in comparison to the
full system, and a loss of -0.4% BLEU in compar-
ison to the corresponding system without ns. On
the non-blind sets, REF is performing best, show-
ing typical overfitting. Comparing the full LM
system to the HYP adapted LM, big improvements
are mainly observed on TER, with significance at
the 95% level for newstest10.
We conclude that using the references as adap-
tation set causes overfitting, using a pseudo in-
domain set as the news-commentary does not im-
prove the results, and the best choice is using the
automatic translations (HYP).
As already mentioned in Section 2, we experi-
mented with adding the automatic translations of
the test sets (HYP) to the LM. Doing so resulted
in 8 points perplexity reduction, but no impact on
the MT quality was observed. Therefore, we deem
these perplexity improvements by adding HYP as
artificial.
5.2 TM Adaptation
In the LM adaptation experiments, we found that
using the test sets automatic translation as the
adaptation set (HYP system) for filtering per-
formed best, in terms of LM quality (perplex-
ity) and translation quality, when compared to the
other suggested adaptation sets, especially on the
blind test set.
462
LM TM newstest10 newstest11 newstest12 newstest13
BLEU TER BLEU TER BLEU TER BLEU TER
full full 24.5 59.1 22.1 61.3 23.3 60.1 25.9 56.7
HYP
full 25.0 58.2? 22.1 60.6 23.5 59.6 25.9 56.3
TM Filtering
REF-25% 25.1 57.9? 22.4 60.2? 24.0? 59.1? 25.5 56.7
HYP-50% 25.2 58.0? 22.2 60.5? 23.8? 59.4? 26.0 56.4
TM Weighting
ppl.NC 25.0 58.1? 22.5 60.2? 23.6 59.5? 26.1 56.2
ppl.TST 24.8 58.8 22.3 60.7 23.6 59.7 26.0 56.3
ppl.REF 24.8 58.2? 22.2 60.3? 23.7 59.5? 25.5 56.4
ppl.HYP 25.4? 57.8? 22.5 60.1? 23.9? 59.3? 26.4? 55.9?
Table 5: German-English TM filtering and weighting results using different adaptation sets. The results
are given in BLEU and TER percentages. Significance is measured over the full system (first row).
For TM adaptation, we experiment with filter-
ing and weighting based adaptation. By using
weighting, we expect further improvements over
the baseline and better differentiation between the
competing adaptation sets.
To perform filtering, we concatenate all the
bilingual corpora in Table 1 and sort them accord-
ing to the combined LM+M1 cross-entropy score.
We then extract the top 50%,25%,... bilingual sen-
tence from the sorted corpus, generate the phrase
table for each setup and reoptimize the system us-
ing MERT on the development set.
Weighted phrase extraction is based on the same
LM+M1 combined cross entropy score as filter-
ing, but instead of discarding whole sentences we
weight them according to their relevance to the
adaptation set being used.
In this section, we compare the three adapta-
tion sets suggested for LM filtering for the TM
component. In addition, one might argue that for
the bilingual case, the source side of the test set
might be sufficient to perform adaptation, or even
it might perform better for TM adaptation as the
automatically generated translation might not be
as reliable. We perform an experiment using the
source side of the test sets as an adaptation set to
score the source side of the bilingual corpora (de-
noted TST in the experiments). To summarize, we
collect 4 corpora as adaptation sets to be used for
adapting the TM: (i) NC, HYP, and REF as defined
for LM but using both source and target (automat-
ically generated for HYP) sides, and (ii) TST using
only the source side of the test sets.
The results comparing the 4 suggested adapta-
tion sets for filtering and weighting are given in
Table 5. In this table, we use newstest10 as be-
fore for MERT optimization and display results for
newstest10...newstest13. Note that for TM filter-
ing and weighting we use the HYP adapted LM as
it achieves the best results in the previous section.
For filtering, the NC and TST adaptation sets
could not improve the dev results over the full sys-
tem therefore they are omitted. REF based adapta-
tion achieves the best dev results when using 25%
of the bilingual data while HYP based adaptation
uses 50% of the data. For TM filtering, only slight
overfitting is observed, where the REF system is
slightly better than HYP on the non blind sets and
is worse on the blind test set. We hypothesize that
no severe overfitting is observed for TM filtering
as we use a strong LM adapted with the HYP set,
therefore degradation is lessened.
Next, we focus on weighted phrase extraction
for adaptation using the various adaptation sets.
Comparing filtering to weighting, weighting im-
proves for the ppl.HYP based adaptation but a
slight loss is observed for the ppl.REF system ex-
cept on the blind test set. We conclude that due to
the usage of more data in the weighting scenario,
overfitting is lessened. Using the source side of the
test sets for weighting (ppl.TST) achieves good re-
sults, with improvements over the ppl.REF system
on newstest13.
The ppl.HYP system achieves the best results
among the weighted systems. Comparing the
full unadapted system with the LM+TM adapted
ppl.HYP system, we achieve significant BLEU im-
provements on most sets, TER improvements are
significant in all cases with 95% significance level.
The highest gains are on the development set with
463
+0.9% BLEU and -1.3% TER improvements, on
the test sets, newstest12 improves with +0.6%
BLEU and -0.8% TER and newstest13 improves
with +0.5% BLEU and -0.8% TER. The ppl.HYP
system is comparable to the best single system
of WMT 2013
4
(26.4% BLEU vs 26.8% BLEU
for Edinburgh submission, RWTH submission is a
system combination). Note that we are not using
the LDC GigaWord corpus.
We conclude that using in-domain automatic
translations (HYP) for TM weighting performs
best, better than using source side only in-domain
(TST) and better than using the references (REF)
especially on the blind test set. TM adaptation
shows further improvements on top of LM adap-
tation and achieves significant gains.
6 Conclusion
In this work, we tackle the problem of adaptation
without labeled bilingual in-domain training data.
The only information about the test domain is en-
capsulated in the test sets themselves. We experi-
ment with unsupervised adaptation for SMT, using
automatic translations of the test sets, focusing on
adaptation for the LM and the TM components.
We use cross-entropy based scoring for the task
of adaptation, as this method proved successful in
previous work. We utilize filtering for LM adapta-
tion, while we compare filtering and weighting for
TM adaptation.
For LM adaptation, the setup we devise al-
ready contains a majority of in-domain data, still
we could report improvements over the unadapted
baseline. We compose three different adaptation
sets for filtering using automatic translation of the
test data (HYP), a pseudo in-domain set (NC) and
the references (REF) of the test sets (keeping one
blind test set). The NC based filtering is not able to
perform good selection, for news-shuffle the whole
corpus is retained and for giun 50% of the data is
retained. The perplexity results and the translation
quality are virtually unchanged in comparison to
the full system. Using REF as the target set causes
overfitting, where the results are better on the seen
test sets but worse on the blind test set. The best
performing target set in our experiments is the un-
supervised HYP adaptation set, achieving the best
perplexity as well as the best translation quality on
the blind test set. Therefore, we conclude that for
4
http://matrix.statmt.org/matrix/
systems_list/1712
developing a successful SMT system that can gen-
eralize to new data the HYP based adaptation is
preferred.
Next, we perform TM adaptation, where we re-
peat the comparison between the different adapta-
tion sets for filtering as well as weighting. We also
compare to adaptation based only on the source
side of the test sets (TST). The LM adaptation
results hold for TM adaptation, where using the
automatic translations method shows the best re-
sults for the blind test set. Our experiments show
that using the source side only of the test set for
adaptation performs worse than the unsupervised
method, reminiscent to results reported in previous
work comparing supervised source side against
bilingual filtering (Axelrod et al., 2011). For filter-
ing, the REF system suffers from overfitting, while
when using weighting for adaptation, overfitting
is lessened. Comparing the unadapted baseline to
the adapted LM and TM system using the HYP
set, improvements of +1.0% BLEU and -1.3% TER
are reported on the development set while +0.5%
BLEU and -0.8% TER improvements are reported
on the blind test set.
Acknowledgments
This material is based upon work supported by
the DARPA BOLT project under Contract No.
HR0011-12-C-0015. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
M. Bacchiani and B. Roark. 2003. Unsupervised
language model adaptation. In Acoustics, Speech,
and Signal Processing, 2003. Proceedings. (ICASSP
?03). 2003 IEEE International Conference on, vol-
ume 1, pages I?224 ? I?227 vol.1, april.
Jerome R Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42(1):93 ? 108. Adaptation Meth-
ods for Speech Recognition.
M Federico M Cettolo, L Bentivogli, M Paul, and
S St?uker. 2012. Overview of the iwslt 2012 eval-
uation campaign. In International Workshop on
464
Spoken Language Translation, pages 12?33, Hong
Kong, December.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128?135, Prague, Czech Republic, June.
Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, Hawaii, USA,
October.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NTCIR Conference, vol-
ume 10, pages 260?286, Tokyo, Japan, June.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
EAMT conference on ?Practical applications of ma-
chine translation?, pages 133?1142, May.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 317?321,
Montr?eal, Canada, June. Association for Computa-
tional Linguistics.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing (EMNLP), pages 388?395, Barcelona,
Spain, July.
Saab Mansour and Hermann Ney. 2012. A sim-
ple and effective weighted phrase extraction for ma-
chine translation adaptation. In International Work-
shop on Spoken Language Translation, pages 193?
200, Hong Kong, December.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Franz J. Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41th Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
Raphael Rubino, Antonio Toral, Santiago
Cort?es Va??llo, Jun Xie, Xiaofeng Wu, Stephen
Doherty, and Qun Liu. 2013. The CNGL-DCU-
Prompsit translation systems for WMT13. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 213?218, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 25?32, Prague, Czech Republic,
June. Association for Computational Linguistics.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, Mumbai, India, Decem-
ber.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th international conference on
Computational Linguistics, COLING ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
465
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 110?119,
Dublin, Ireland, August 23 2014.
Improved Sentence-Level Arabic Dialect Classification
Christoph Tillmann and Yaser Al-Onaizan
IBM T.J. Watson Research Center
Yorktown Heights, NY, USA
{ctill,onaizan}@us.ibm.com
Saab Mansour
?
Aachen University
Aachen, Germany
mansour@cs.rwth-aachen.de
Abstract
The paper presents work on improved sentence-level dialect classification of Egyptian Arabic
(ARZ) vs. Modern Standard Arabic (MSA). Our approach is based on binary feature functions
that can be implemented with a minimal amount of task-specific knowledge. We train a feature-
rich linear classifier based on a linear support-vector machine (linear SVM) approach. Our best
system achieves an accuracy of 89.1 % on the Arabic Online Commentary (AOC) dataset (Zaidan
and Callison-Burch, 2011) using 10-fold stratified cross validation: a 1.3 % absolute accuracy
improvement over the results published by (Zaidan and Callison-Burch, 2014). We also evaluate
the classifier on dialect data from an additional data source. Here, we find that features which
measure the informalness of a sentence actually decrease classification accuracy significantly.
1 Introduction
The standard form of written Arabic is Modern Standard Arabic (MSA) . It differs significantly from
various spoken varieties of Arabic (Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014;
Elfardy and Diab, 2013). Even though these dialects do not originally exist in written form, they are
present in social media texts. Recently a dataset of dialectal Arabic has been made available in the form
of the Arabic Online Commentary (AOC) set (Zaidan and Callison-Burch, 2011; Zaidan and Callison-
Burch, 2014). The data consists of reader commentary from the online versions of Arabic newspapers,
which have a high degree of dialect content. Data for the following dialects has been collected: Levan-
tine, Gulf, and Egyptian. The data had been obtained by a crowd-sourcing effort. In the current paper, we
present results for a binary classification task only, where we predict the dialect of Egyptian Arabic ARZ
vs. MSA sentences from the Al-Youm Al-Sabe? newspaper online commentaries
1
. Our ultimate goal
is to use the dialect classifier for building a dialect-aware Arabic-English statistical machine translation
(SMT) system. Our Arabic-English training data contains a significant amount of Egyptian dialect data
only, and we would like to adapt the components of our hierarchical phrase-based SMT system (Zhao
and Al-Onaizan, 2008) to that data.
Similar to (Elfardy and Diab, 2013), we present a sentence-level classifier that is trained in a supervised
manner. Our approach is based on an Arabic tokenizer, but we do not use a range of specialized tokenizers
or orthography normalizers. In contrast to the language-model (LM) based classifier used by (Zaidan and
Callison-Burch, 2014), we present a linear classifier approach that works best without the use of LM-
based features. Some improvements in terms of classification accuracy and 10-fold cross validation under
the same data conditions as (Zaidan and Callison-Burch, 2011; Elfardy and Diab, 2013) are presented.
In general, we aim at a smaller amount of domain specific feature engineering than previous related
approaches.
The paper is structured as follows. In Section 2, we present related work on language and dialect
identification. In Section 3, we discuss the linear classification model used in this paper. In Section 4, we
evaluate the classifier performance in terms of classification accuracy on two data sets and present some
?
Part of the work was done while the author was a student intern at the IBM T.J. Watson Research Center.
1
We use the ISO 639-3 code ARZ for denoting Egyptian Arabic.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
110
error analysis. Finally, in Section 5, we discuss future work on improved dialect-level classification and
its application to system adaptation for machine translation.
2 Related Work
From a computational perpective, we can view dialect identification as a more fine-grained form of lan-
guage identification (ID). Previous work on language ID examined the use of character histograms (Cav-
nar and Trenkle, 1994; Dunning, 1994), and high accuracy prediction results have been reported even
for languages with a common character set. (Baldwin and Lui, 2010) present a range of document-level
language identification techniques on three different data sets. They use n-gram counting techniques and
different tokenization schemes that are adopted to those data sets. Their classification task deals with
several languages, and it becomes more difficult as the number of languages increases. They present an
SVM-based multiclass classification approach similar to the one presented in this paper which performs
well on one of their data sets. (Trieschnigg et al., 2012) generates n-gram features based on character or
word sequences to classify dialectal documents in a dutch-language fairy-tale collection. Their baseline
model uses N -gram based text classification techniques as popularised in the TextCat tool (Cavnar and
Trenkle, 1994). Following (Baldwin and Lui, 2010), the authors extend the usage of n-gram features with
nearest neighbour and nearest-prototype models together with appropriately chosen similarity metrics.
(Zampieri and Gebre, 2012) classify two varieties of the same language: European and Brazilian Por-
tuguese. They use word and character-based language model classification techniques similar to (Zaidan
and Callison-Burch, 2014). (Huang and Lee, 2008) present simple bag-of-word techniques to classify
varieties of Chinese from the Chinese Gigaword corpus. (Kruengkrai et al., 2005) extend the use of n-
gram features to using string kernels: they may take into account all possible sub-strings for comparison
purposes. The resulting kernel-based classifier is compared against the method in (Cavnar and Trenkle,
1994). (Lui and Cook, 2013) present a dialect classification approach to identify Australian, British, and
Canadian English. They present results where they draw training and test data from different sources.
The successful transfer of models from one text source to another is evidence that their classifier indeed
captures dialectal rather than stylistic or formal differences. Language identification of related languages
is also addressed in the DSL (Discriminating Similar Languages) task of the present Vardial workshop
at COLING 14 (Tan et al., 2014).
While most of the above work focuses on document-level language classification, recent work on
handling Arabic dialect data addresses the problem of sentence-level classification (Zaidan and Callison-
Burch, 2011; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013; Zaidan and Callison-Burch,
2014). The work is based on the data collection effort by (Zaidan and Callison-Burch, 2014) which
crowdsources the annotation task to workers on Amazons Mechanical Turk. The classification results
by (Zaidan and Callison-Burch, 2014) are based on n-gram language-models, where the n-grams are
defined both on words and characters. The authors find that unigram word-based models perform best.
The word-based models are obtained after a minimal amount of preprocessing such as proper handling
of HTML entities and Arabic numbers. Classification accuracy is significantly reduced for shorter sen-
tences. (Elfardy and Diab, 2013) presents classifcation result based on various tokinization and ortho-
graphic normalization techniques as well as so-called meta features that estimate the informalness of the
data. Like our work, the authors focus on a binary dialect classification based on the ARZ-MSA portion
of the dataset in (Zaidan and Callison-Burch, 2011).
3 Classification Model
We use a linear model and compute a score s(t
n
1
) for a tokenized input sentence consisting of n tokens
t
i
:
s(t
n
1
) =
d
?
s=1
w
s
?
n
?
i=1
?
s
(c
i
, t
i
) (1)
where ?
s
(c
i
, t
i
) is a binary feature function which takes into account the context c
i
of token t
i
. w ? R
d
is a high-dimensional weight vector obtained during training. In our experiments, we classify a tokenized
111
Description MSA ARZ
# sentences # words # sentences # words
ARZ-MSA portion of AOC 13, 512 334K 12, 527 327K
DEV12 tune set 585 8.4K 634 9.3K
Table 1: We used the following dialect data: 1) the ARZ-MSA portion of the AOC data from commen-
taries of the Egyptian newspaper Al-Youm Al-Sabe?, and 2) the DEV12 tune set (1219 sentences) which
is the LDC2012E30 corpus BOLT Phase 1 dev-tune set. The DEV12 tune set was annotated by a native
speaker of Arabic.
sentence as being Egyptian dialect (ARZ) if s(t
n
1
) > 0. To train the weights w in Eq. 1, we use a linear
SVM approach (Hsieh et al., 2008; Fan et al., 2008). The trainer can easily handle a huge number of
instances and features. The training data is given as instance-label pairs (x
i
, y
i
) where i ? {1, ? ? ? , l} and
l is the number of training sentences. The x
i
are d-dimensional vectors of integer-valued features that
count how often a binary feature fired for a tokenized sentence t
n
1
. y
i
? {+1,?1} are the class labels
where a label of ?+1? represents Egyptian dialect. During training, we solve the following optimization
problem:
min
w
||w||
1
+ C
l
?
i=1
max(0, 1? y
i
w
T
x
i
) , (2)
i.e. we use L1 regularized L2-loss support vector classification. We set the penalty term C = 0.5. For
our experiments, we use the data set provided in (Zaidan and Callison-Burch, 2011) which also has been
used in the experiments in (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2014). We focus on the
binary classification between MSA and ARZ. Details on the data sources can be found in Table 1. We
present accuracy results in terms of 10-fold stratified cross-validation which are comparable to previously
published work.
3.1 Tokenization and Dictionaries
The Arabic tokenizer used in the current paper is based on (Lee et al., 2003). It is a general purpose
tokenizer which has been optimized towards improving machine translation quality of SMT systems
rather than dialect classification. Together with the tokenized text, a maximum-entropy based tagger
provides the part-of-speech (PoS) tags for each token. In addition, we have explored a range of features
that are based on the output of the AIDA software package (Elfardy and Diab, 2012; Mona Diab et
al., 2009 2011). The AIDA software has been made available to the participants of the DARPA-funded
Broad Operational Language Translation (BOLT) project. AIDA is a system for dialect identification,
classification and glossing on the token and sentence level for written Arabic. AIDA aggregates several
components including dictionaries and language models in order to perform named entity recognition,
dialect identification classification, and MSA English linearized glossing of the input text. We created
a dictionary from AIDA resources that includes about 41 000 ARZ tokens. In addition, we obtained a
second small dictionary of about 70 ARZ dialect tokens with the help of a native speaker of Arabic. The
list was created by training two IBM Model 1 lexicons, one on Egyptian Arabic data and another on
MSA data. We then inspected the ARZ lexicon entries with the highest cosine distance to their MSA
counterparts and kept the ones that are strong ARZ words. The tokens in both dictionaries are not ARZ
exclusive, but could occur in MSA as well.
3.2 Feature Set
In our work, we employ a simple set of binary feature functions based on the tokenized Arabic sentence.
For example, we define a token bigram feature as follows:
?
Bi
(t
k
, t
k?1
) =
{
1 t
k
= ??


?

?? and t
k?1
= ???g?
0 otherwise
. (3)
112
Token unigram and trigram features are defined accordingly. We also define unigram, bigram, and tri-
gram features based on PoS tags. Currently, just PoS unigrams are used in the experiments. We define
dictionary-based features as follows:
?
Dict
l
(t
k
) =
{
1 t
k
= ?

I

???X? and t
k
? Dict
l
0 otherwise
, (4)
where we use the two dictionaries Dict
1
and Dict
2
as described in Section 3.1. The dictionaries are
handled as token sets and we generate separate features for each of them. We generate some features
based on the AIDA tool output. AIDA provides a dialect label for each input token t
k
as well as a single
dialect label at the sentence level. A sentence-level binary feature based on the AIDA sentence level
classification is defined as follows:
?
AIDA
(t
n
1
) =
{
1 AIDA(t
n
1
) is ARZ
0 otherwise
(5)
where AIDA(t
n
1
) is the sentence-level classification of the AIDA tool. A word-level feature ?
AIDA
(t
k
) is
defined accordingly. These features improve the classification accuracy of our best system significantly.
We have also experimented with some real-valued feature. For example, we derived a feature from
dialect-specific language model probabilities:
?
LM
(t
n
1
) = 1/n ? [ log(p
MSA
(t
n
1
)) ? log(p
ARZ
(t
n
1
))] ,
where log(p
ARZ
(t
n
1
)) is the language-model log probability for the dialect class ARZ . We used a trigram
language model. p
MSA
(?) is defined accordingly. In addition, we have implemented a range of so-called
?meta? features similar to the ones defined in (Elfardy and Diab, 2013). For example, we define a feature
?
Excl
(t
n
1
) which is equal to the length of the longest consecutive sequence of exclamation marks in
the tokenized sentence t
n
1
. Similarly, we define features that count the longest sequence of punctuation
marks, the number of tokens, the averaged character-length of a token in the sentence, and the percentage
of words with word-lengthening effects. These features do not directly model dialectalness of the data
but rather try to capture the degree of in-formalness. Contrary to (Elfardy and Diab, 2013) we find that
those features do not improve accuracy of our best model in the cross-validation experiments. On the
DEV12 set, the use of the meta features results in a significant drop in accuracy.
4 Experiments
In this section, we present experimental results. Firstly, Section 4.1 demonstrates that our data is anno-
tated consistently. In Section 4.2, we present dialect prediction results in terms of accuracy and F-score
on our two data sets. In Section 4.3, we perform some qualitative error analysis for our classifier. In
Section 4.4, we present some preliminary effects on training a SMT system.
4.1 Annotator Agreement
To confirm the consistent annotation of our data, we have measured some inter-annotator and intra-
annotator agreement on it. A native speaker of Arabic was asked to classify the ARZ-MSA portion
of the dialect data using the following three labels: ARZ, MSA, Other. We randomly sampled 250
sentences from the ARZ-MSA portion of the Zaidan data maintaining the original dialect distribution.
The confusion matrix is shown in Table 2. It corresponds to a kappa value of 0.84 (using the definition of
(Fleiss, 1971)), which indicates a very high agreement. In addition, we did re-annotate a sub-set of 200
sentences from the DEV12 set over a time period of three months using our own annotator. The kappa
value of the corresponding confusion matrix is 0.93, indicating very high agreement as well.
4.2 Classification Experiments
Following previous work, we present dialect prediction results in terms of accuracy:
ACC =
# sent correctly tagged
# sent
, (6)
113
Predicted Class (IBM)
ARZ MSA Other
Actual ARZ 125 4 1
Class MSA 14 105 1
(AOC) Other 0 0 0
Table 2: Inter annotator agreement on 250 randomly selected AOC sentences from the data in Table 1.
An in-lab annotator?s dialect prediction is compared against the AOC data gold-standard dialect labels.
where ?# sent? is the number of sentences. In addition, we present dialect prediction results in terms of
precision, recall, and F-score. They are defined as follows:
Prec =
# sent correctly tagged as ARZ
# sent tagged as ARZ
(7)
Recall =
# sent correctly tagged as ARZ
# ref sent tagged as ARZ
F =
2 ? Prec ?Recall
(Prec+Recall)
.
MSA prediction F-score is defined analogously. Experimental results are presented in Table 3, where we
present results for different sets of feature types and the two test sets in Table 1. In the top half of the
table, results are presented in terms of 10-fold cross validation on the ARZ-MSA portion of the AOC
data. In the bottom half, we present results on DEV12 tune set, where we use the entire dialect data in
Table 1 for training (about 26K sentences).
As our baseline we have re-implemented the language-model-perplexity based approach reported in
(Zaidan and Callison-Burch, 2011). We train language models on the dialect-labeled commentary train-
ing data for each of the dialect classes c ? {MSA,ARZ}. During testing, we compute the language
model probability of a sentence s for each of the classes c. We assign a sentence to the class c with the
highest probability (or the lowest perplexity) . For the 10-fold cross validation experiments, 10 language
models are built and perplexities are computed on 10 different test sets. The resulting (averaged) ac-
curacy is 83.3 % for cross-validation and 82.2 % on the DEV12 tune set. In comparison, (Elfardy and
Diab, 2013) reports an accuracy of 80.4 % as perplexity-based baseline. We have carried out additional
experiments with a simple feature set that consists of only unigram token and bigram token features as
defined in Eq. 3. Such a system performs surprisingly well under both testing conditions: we achieved an
accuracy of 87.7 % on the AOC data and an accuracy of 83.4 % on the DEV12 test set. On the AOC set
using 10-fold cross validation, we achieve only a small improvement from using the dictionary features
defined in Eq. 4. The accuracy is improved from 87.7 % to 88.0 %. On the DEV12 set, we obtain a
much larger improvement from using these features. Furthermore, we have investigated the usefulness
of the AIDA-based features. The stand-alone sentence-level classification of the AIDA tool performs
quite poorly. On the DEV12 set, it achieves an accuracy of just 77.9 %. But using the AIDA assigned
sentence-level and token-level dialect labels based on the binary features defined in Eq. 5 improves ac-
curacy significantly, e.g. from 85.3 % to 87.8 % on the DEV12 set. In the current experiments, the
so-called meta features which are computed at the sentence level do not improve classification accuracy.
The meta features are only useful in classifying dialect data based on the in-formalness of the data, i.e.
the ARZ news commentaries tend to exhibit more in-formalness than the MSA commentaries. Finally,
the sentence-level perplexity feature defined in Eq. 6 did not improve accuracy as well (no results for this
feature are presented in Table 3).
4.3 Classifier Analysis
In this section, we perform a simple error analysis of the classifier performance on some dialect data for
which the degree of dialectalness is known. The data comes from news sources that differ from the data
used to train the classifier. The classifier is evaluated on data from the DARPA-funded BOLT project.
114
Feature Types MSA ARZ
ACC [%] PREC REC F PREC REC F
10-fold language-model 83.3 86.7 90.2 88.4 89.0 85.0 86.9
AOC aida-sentence label 81.0 84.2 78.0 81.0 78.0 84.3 81.0
uni,bi 87.7 86.6 90.2 88.4 89.0 85.0 86.9
uni,bi,dict,pos 88.0 86.9 90.4 88.6 89.2 85.3 87.2
uni,bi,dict,pos,aida 89.1 87.5 92.2 89.8 91.1 85.7 88.3
uni,bi,dict,pos,aida,meta 88.8 87.4 91.7 89.5 90.6 85.7 88.1
DEV12 language-model 82.2 85.1 76.2 80.4 80.0 87.7 83.7
aida-sentence label 77.9 80.9 70.8 75.5 75.8 84.5 79.9
uni,bi 83.4 81.1 85.1 83.1 85.6 81.7 83.6
uni,bi,dict,pos 85.3 83.5 87.5 85.5 88.0 84.1 86.0
uni,bi,dict,pos,aida 87.8 83.4 93.0 88.0 92.8 83.0 87.6
uni,bi,dict,pos,aida,meta 68.3 61.8 90.8 73.5 85.0 48.3 61.6
Table 3: Arabic Dialect Classification Results: predicting MSA vs. (ARZ) dialect in terms of 10-fold
cross-validation on the AOC data and on the DEV12 set using all the AOC data for training.
Corpus #Sent #Sent [ARZ] %[ARZ]
ARZ web forum 299K 183K 61%
Broadcast 169K 18K 11%
Newswire 885K 29K 3%
Table 4: Sub-corpora together with total number as well as percentage of sentences that are classified as
ARZ.
The BOLT data consists of several corpora collected from various resources. These resources include
newswire, web-logs, ARZ web forum data and others. Classification statistics are presented in Table 4,
where we report the number of sentences along with the percentage of those sentences classified as ARZ.
The distribution of the dialect labels in the classifier output appears to correspond to the expected origin
of the data. For example, the ARZ web forum data contains a majority of ARZ sentences, but quite a
few sentences are MSA such as greetings and quotations from Islamic resources (Quran, Hadith ...). The
broadcast conversation data is mainly MSA, but sometimes the speaker switches to dialectal usage for
a short phrase and then switches back to MSA. Lastly, the newswire data has a vast majority of MSA
sentences. Examining a small portion of newswire sentences classified as ARZ, the sentences labeled as
ARZ are mostly classification errors.
Example sentence classifications from the BOLT data are shown in Table 5. The first two text frag-
ments are taken from the Egyptian Arabic (ARZ) web forum data. In the first document fragment, the
user starts with MSA sentences, then switches to Egyptian (ARZ) dialect marked by the ARZ indicator
?


?
?@ and using the prefix # H
.
before a verb which is not allowed in MSA. The user then switches back
to MSA. The classifier is able to classify the Egyptian Arabic (ARZ) sentence correctly. In the second
document fragment, the user uses several Egyptian Arabic (ARZ) words. In the forth sentence no ARZ
words exist, and the classifier correctly classifies the sentence as MSA. The third text fragment shows
115
Predicted Arabic English
Dialect
MSA . X?XQ?@ ? ??
	
???
?
@

H@Q

? A
	
K @ i read the topic and the replies .
MSA .

???g

?Q?
	
? ??
	
???
?
@ the topic is great !
ARZ ??

?K


# H
.
?


?
? @ pB@ ?? A
	
K @ # ? i agree with the brother who said
MSA

?k
.
Ag ?? ?


	
? ???
	
?K


Y?@ Islam is significant in all
ARZ ZCJ
.
? @
?


?
?

H
Q

.
? ?


X ?A
	
J? @
	
?A

??? because they accept affliction with patience
ARZ PA?

J
	
K @
Q

.
? @

?X ?A?
g
?+

I??
?
?


?
?@ ? what Hamas did was a victory
ARZ ?C

Jk@

?? ?


	
? @?
	
?

?? ?A?
g
?


	
P who encountered the occupation
MSA PA?k
?


?
? @?
Q

.
? ? and they were patient despite the siege
ARZ ??+

?
	
?A? A
	
K+ H
.
P ?Y?
	
?A

??? that ?s why Allah rewarded them
ARZ* ?? ?


X ?



G

HXA

? Y

? # ? tdk ... led
ARZ*

??

?
C?@# H
.
?

?
	
J? @ Z @
Q

.
	
g ?j
	
JK


# ? transport experts blame
ARZ* . ?


+ # ? ?+ ?A

? A? Q?
	
Y

K ?J


?

?@ B i cannot remember what he told me
Table 5: Automatic classification examples for the dialect classes ARZ and MSA. Arabic source and
English target sentences are given. Dialectal words are in bold. Incorrect predictions are marked by an
asterisk (*).
some sentences from the newswire corpus that are mis-classified. The first sentence contains the word
?


X which corresponds to the letter ?d? in the abbreviation ?tdk?. The word is contained in one of our ARZ
dictionaries such that the binary AIDA-based feature in Eq. 5 fires and triggers a mis-classification. In
this context, the word is part of an abbreviation which is split in the Arabic text. In the other examples,
only a few of the binary features defined in Section 3.2 apply and features that correspond to Arabic
prefixes tend to support a classification as ARZ dialect.
4.4 Preliminary Application for SMT
The dialect classification of Arabic data for SMT can be used in various ways. Examples include domain-
specific tuning, mixture modeling, and the use of so-called provenance features (Chiang et al., 2011)
among others . As a motivation for the future use of the dialect classifier in SMT, we classify the BOLT
bilingual training data into ARZ and MSA parts and examine the effect on the phrase table scores. Phrase
translation pairs demonstrating the use of the classified training data are shown in Table 6. The ARZ web
forum data is split into an ARZ part and an MSA part and two separate phrase probability tables are
trained on these two splits. The ARZ web forum data is highly ambiguous with respect to dialect and it
is difficult to obtain good dialect-dependent splits of the data. In the first example in the table, the word

?J


K
.
Q??@ could mean ?Arab? in MSA, but in ARZ it could also mean ?car?. The phrase table scores obtained
from the classifier-split training data correctly reflect this ambiguity. The phrase pair with ?car? has the
lowest translation score for the BOLT.ARZ phrase table, while it has a higher cost in the BOLT.MSA
phrase table. In the full phrase table (BOLT), ?car? is the fifth translation candidate with a score of 2.09.
116
BOLT.ARZ BOLT.MSA
f e cost e cost

?J


K
.
Q??@
the car 1.20 arab 0.80
arab 1.25 the arab 1.32
the arab 1.70 Arabic 1.52
?


??Q?
merci 1.53 marsa 1.99
marsa 1.63 thanks 2.01
mursi 1.91 morcy 2.13
Table 6: Phrase tables based on classified training data. BOLT.ARZ is trained on the ARZ portion of
the ARZ web forums data, while BOLT.MSA is trained on the MSA part. The table includes Arabic
words and the top three phrase translation candidates, sorted (first is best) by the phrase model cost
(cost= ?log(p(f |e)) ).
In the second example, the word ?


??Q ? could function as a proper noun with its English translation
?mursi? or ?marsa?, but only in ARZ it could also be translated as ?thanks? (?merci?). In this case, the
classifier is unable to distinguish between the ARZ dialect and the MSA usage. We found out that the
word token ?merci? appears only 4 times in the training data, rendering its binary features unreliable
reliable. In general we note that the phrase tables build on the classified data become more domain-
specific, and it is left to future work to check whether improvements could carry over to the translation
quality.
5 Discussion and Future Work
The ultimate goal is to use the ARZ vs. MSA dialect classifier for training an adapted SMT system.
We split the training data at the sentence level using our classifier and train dialect-specific systems
on each of these splits along with a general dialect-independent system. We will be using techniques
similar to (Koehn and Schroeder, 2007; Chiang et al., 2011; Sennrich, 2012; Chen et al., 2013) to adapt
the general SMT system to a target domain with a predominant dialect. Or, we will be adopting an
SMT system to a development or test set where we use the classifier to predict the dialect for each
sentence and use a dialect-specific SMT system on each of them individually. Our approach of using
just binary feature functions in connection with a sentence-level global linear model can be related to
work on PoS-tagging (Collins, 2002). (Collins, 2002) trains a linear model based on Viterbi decoding
and the perceptron algorithm. The gold-standard PoS tags are given at the word-level, but the training
uses a global representation at the sentence level. Similarly, we use linear SVMs (Hsieh et al., 2008)
to train a classification model at the sentence level without access to sentence length statistics, i.e. our
best performing classifier does not compute features like the percentage of punctuation, numbers, or
averaged word length as has been proposed previously (Elfardy and Diab, 2013). All of our features are
actually computed at the token level (with the exception of a single sentence-level AIDA-based feature).
An interesting direction for future work could be to train the dialect classifier at the sentence level, but
use it to compute token-level predictions for a more fine-grained analysis. Even though the token-level
prediction task corresponds to a word-level tag set of just size 2, Viterbi decoding techniques could be
used to introduce novel context-dependent features, e.g. dialect tag n-gram features. Such a token-level
predictions might be used for weighting each phrase pair in an SMT system using methods like the
instance-based adaptation approach in (Foster et al., 2010).
Acknowledgement
The current work has been funded through the Broad Operational Language Translation (BOLT) program
under the project number DARPA HR0011-12-C-0015.
117
References
Timothy Baldwin and Marco Lui. 2010. Language Identification: The Long and the Short of the Matter. In Proc.
of HLT?10, pages 229?237, Los Angeles, California, June.
William Cavnar and John M. Trenkle. 1994. N-gram-based Text Categorization. In In Proceedings of SDAIR-94,
3rd Annual Symposium on Document Analysis and Information Retrieval, pages 161?175.
Boxing Chen, George Foster, and Roland Kuhn. 2013. Adaptation of reordering models for statistical machine
translation. In Proc. of HLT?13, pages 938?946, Atlanta, Georgia, June.
David Chiang, Steve DeNeefe, and Michael Pust. 2011. Two Easy Improvements to Lexical Weighting. In Proc.
of HLT?11, pages 455?460, Portland, Oregon, USA, June.
Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proceedings of EMNLP?02, pages 1?8, Philadelphia,PA, July.
Ted Dunning. 1994. Statistical Identification of Language. technical report mccs 94-273. Technical report, New
Mexico State University.
Heba Elfardy and Mona Diab. 2012. Aida: Automatic Identification and Glossing of Dialectal Arabic. In Pro-
ceedings of the 16th EAMT Conference (Project Papers), pages 83?83, Trento, Italy, May.
Heba Elfardy and Mona Diab. 2013. Sentence level dialect Identification in arabic. In Proc. of the ACL 2013
(Volume 2: Short Papers), pages 456?461, Sofia, Bulgaria, August.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: a Library
for Large Linear Classification. Machine Learning Journal, 9:1871?1874.
Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin,
76(5):378.
George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative Instance Weighting for Domain Adaptation
in Statistical Machine Translation. In Proc. of EMNLP?10, pages 451?459.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S.S. Keerthi, and S.Sundararajan. 2008. A Dual Coordinate Descent Method
for Large-scale linear SVM. In ICML, pages 919?926, Helsinki,Finland.
Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive Approach towards Text Source Classication based on
top-bag-of-word Similarity. In PACLIC 2008, pages 404?410, Cebu City, Philippines.
Philipp Koehn and Josh Schroeder. 2007. Experiments in Domain Adaptation for Statistical Machine Translation.
In Proceedings of the Second Workshop on Statistical Machine Translation (WMT07), pages 224?227.
Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlertlamvanich, and Hitoshi Isahara. 2005. Language
Identification based on string kernels. In In Proceedings of the 5th International Symposium on Communications
and Information Technologies (ISCIT-2005, pages 896?899.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Ossama Emam, and Hany Hassan. 2003. Language Model
Based Arabic Word Segmentation. In Proc. of the 41st Annual Conf. of the Association for Computational
Linguistics (ACL 03), pages 399?406, Sapporo, Japan, July.
Marco Lui and Paul Cook. 2013. Classifying English Documents by National Dialect. In Proc. Australasian
Language Technology Workshop, pages 5?15.
Mona Mona Diab, Heba Elfardy, and Yassine Benajiba. 2009?2011. AIDA Automatic Identification of Arabic Di-
alectal Text. a Tool for Dialect Identification & Classification, Named Entity Recognition, English and Modern
Standard Arabic Glossing and Normalization.
Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine
translation. In Proc. of EACL?12, pages 539?549.
Liling Tan, Marcos Zampieri, Nicola Ljube?si?c, and J?org Tiedemann. 2014. Merging Comparable Data Sources
for the Discrimination of Similar Languages: The DSL Corpus Collection. In 7th Workshop on Building and
Using Comparable Corpora at LREC?14, Reykjavik, Iceland, September.
D. Trieschnigg, D. Hiemstra, M. Theune F. Jong, and T. Meder. 2012. An Exploration of Language Identication
Techniques for the Dutch Folktale Database. In Adaptation of Language Resources and Tools for Processing
Cultural Heritage Workshop (LREC 2012), Istanbul, Turkey, May.
118
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowdsourcing translation: Professional quality from non-
professionals. In Proc. of ACL / HLT 11, pages 1220?1229, Portland, Oregon, USA, June.
Omar F. Zaidan and Chris Callison-Burch. 2014. Arabic Dialect Classification. CL, 40(1):171?202.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic Identication of Language Varieties: The case
of Portuguese. In Konvens 12, pages 233?237, Vienna, Austria.
Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing Local and Non-Local word-reordering patterns for syntax-
based machine translation. In Proc. of EMNLP?08, pages 572?581, Honolulu, Hawaii, October.
119
