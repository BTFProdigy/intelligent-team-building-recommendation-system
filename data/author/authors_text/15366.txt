Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 140?150,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Domain-Assisted Product Aspect Hierarchy Generation: Towards
Hierarchical Organization of Unstructured Consumer Reviews
Jianxing Yu1, Zheng-Jun Zha1, Meng Wang1, Kai Wang2, Tat-Seng Chua1
1School of Computing, National University of Singapore
2Institute for Infocomm Research, Singapore
{jianxing, zhazj, wangm, chuats}@comp.nus.edu.sg kwang@i2r.a-star.edu.sg
Abstract
This paper presents a domain-assisted ap-
proach to organize various aspects of a prod-
uct into a hierarchy by integrating domain
knowledge (e.g., the product specifications),
as well as consumer reviews. Based on the
derived hierarchy, we generate a hierarchical
organization of consumer reviews on various
product aspects and aggregate consumer opin-
ions on these aspects. With such organiza-
tion, user can easily grasp the overview of
consumer reviews. Furthermore, we apply the
hierarchy to the task of implicit aspect identi-
fication which aims to infer implicit aspects of
the reviews that do not explicitly express those
aspects but actually comment on them. The
experimental results on 11 popular products in
four domains demonstrate the effectiveness of
our approach.
1 Introduction
With the rapidly expanding e-commerce, most retail
Web sites encourage consumers to write reviews to
express their opinions on various aspects of prod-
ucts. Huge collections of consumer reviews are
now available on the Web. These reviews have be-
come an important resource for both consumers and
firms. Consumers commonly seek quality informa-
tion from online consumer reviews prior to purchas-
ing a product, while many firms use online reviews
as an important resource in their product develop-
ment, marketing, and consumer relationship man-
agement. However, the reviews are disorganized,
leading to the difficulty in information navigation
and knowledge acquisition. It is impractical for user
to grasp the overview of consumer reviews and opin-
ions on various aspects of a product from such enor-
mous reviews. Among hundreds of product aspects,
it is also inefficient for user to browse consumer re-
views and opinions on a specific aspect. Thus, there
is a compelling need to organize consumer reviews,
so as to transform the reviews into a useful knowl-
edge structure. Since the hierarchy can improve in-
formation representation and accessibility (Cimiano,
2006), we propose to organize the aspects of a prod-
uct into a hierarchy and generate a hierarchical or-
ganization of consumer reviews accordingly.
Towards automatically deriving an aspect hierar-
chy from the reviews, we could refer to traditional
hierarchy generation methods in ontology learning,
which first identify concepts from the text, then
determine the parent-child relations between these
concepts using either pattern-based or clustering-
based methods (Murthy et al, 2010). However,
pattern-based methods usually suffer from inconsis-
tency of parent-child relationships among the con-
cepts, while clustering-based methods often result
in low accuracy. Thus, by directly utilizing these
methods to generate an aspect hierarchy from con-
sumer reviews, the resulting hierarchy is usually in-
accurate, leading to unsatisfactory review organiza-
tion. On the other hand, domain knowledge of prod-
ucts is now available on the Web. For example,
there are more than 248,474 product specifications
in the product sellingWeb site CNet.com (Beckham,
2005). These product specifications cover some
product aspects and provide coarse-grained parent-
child relations among these aspects. Such domain
knowledge is useful to help organize the product as-
140
Figure 1: Sample hierarchical organization for iPhone 3G
pects into a hierarchy. However, the initial hierarchy
obtained from domain knowledge usually cannot fit
the review data well. For example, the initial hierar-
chy is usually too coarse and may not cover the spe-
cific aspects commented in the reviews, while some
aspects in the hierarchy may not be of interests to
users in the reviews.
Motivated by the above observations, we propose
in this paper to organize the product aspects into a
hierarchy by simultaneously exploiting the domain
knowledge (e.g., the product specification) and con-
sumer reviews. With derived aspect hierarchy, we
generate a hierarchical organization of consumer re-
views on various aspects and aggregate consumer
opinions on these aspects. Figure 1 illustrates a sam-
ple of hierarchical review organization for the prod-
uct ?iPhone 3G?. With such organization, users can
easily grasp the overview of product aspects as well
as conveniently navigate the consumer reviews and
opinions on any aspect. For example, users can find
that 623 reviews, out of 9,245 reviews, are about the
aspect ?price?, with 241 positive and 382 negative
reviews.
Given a collection of consumer reviews on a spe-
cific product, we first automatically acquire an ini-
tial aspect hierarchy from domain knowledge and
identify the aspects from the reviews. Based on the
initial hierarchy, we develop a multi-criteria opti-
mization approach to construct an aspect hierarchy
to contain all the identified aspects. Our approach
incrementally inserts the aspects into the initial hi-
erarchy based on inter-aspect semantic distance, a
metric used to measure the semantic relation among
aspects. In order to derive reliable semantic dis-
tance, we propose to leverage external hierarchies,
sampled from WordNet and Open Directory Project,
to assist semantic distance learning. With resultant
aspect hierarchy, the consumer reviews are then or-
ganized to their corresponding aspect nodes in the
hierarchy. We then perform sentiment classification
to determine consumer opinions on these aspects.
Furthermore, we apply the hierarchy to the task of
implicit aspect identification. This task aims to infer
implicit aspects of the reviews that do not explic-
itly express those aspects but actually comment on
them. For example, the implicit aspect of the review
?It is so expensive? is ?price.? Most existing aspect
identification approaches rely on the appearance of
aspect terms, and thus are not able to handle implicit
aspect problem. Based on our aspect hierarchy, we
can infer the implicit aspects by clustering the re-
views into their corresponding aspect nodes in the
hierarchy. We conduct experiments on 11 popular
products in four domains. More details of the corpus
are discussed in Section 4. The experimental results
demonstrate the effectiveness of our approach.
The main contributions of this work can be sum-
marized as follows:
1) We propose to hierarchically organize con-
sumer reviews according to an aspect hierarchy, so
as to transfer the reviews into a useful knowledge
structure.
2) We develop a domain-assisted approach to
generate an aspect hierarchy by integrating domain
knowledge and consumer reviews. In order to de-
rive reliable semantic distance between aspects, we
propose to leverage external hierarchies to assist se-
mantic distance learning.
3) We apply the aspect hierarchy to the task of im-
plicit aspect identification, and achieve satisfactory
performance.
The rest of this paper is organized as follows. Our
approach is elaborated in Section 2 and applied to
implicit aspect identification in Section 3. Section
4 presents the evaluations, while Section 5 reviews
141
related work. Finally, Section 6 concludes this paper
with future works.
2 Approach
Our approach consists of four components, includ-
ing initial hierarchy acquisition, aspect identifica-
tion, semantic distance learning, and aspect hierar-
chy generation. Next, we first define some prelimi-
nary and notations and then elaborate these compo-
nents.
2.1 Preliminary and Notations
Preliminary 1. An aspect hierarchy is defined as a
tree that consists of a set of unique aspects A and
a set of parent-child relations R between these as-
pects.
Given the consumer reviews of a product, let
A = {a1, ? ? ? , ak} denotes the product aspects com-
mented in the reviews. H0(A0,R0) denotes the ini-
tial hierarchy derived from domain knowledge. It
contains a set of aspects A0 and relations R0. Our
task is to construct an aspect hierarchy H(A,R), to
cover all the aspects in A and their parent-child re-
lations R, so that the consumer reviews are hierar-
chically organized. Note that H0 can be empty.
2.2 Initial Hierarchy Acquisition
As aforementioned, product specifications on prod-
uct selling websites cover some product aspects and
coarse-grained parent-child relations among these
aspects. Such domain knowledge is useful to help
organize aspects into a hierarchy. We here employ
the approach proposed by Ye and Chua (2006) to au-
tomatically acquire an initial aspect hierarchy from
the product specifications. The method first identi-
fies the Web page region covering product descrip-
tions and removes the irrelevant contents from the
Web page. It then parses the region containing the
product information to identify the aspects as well as
their structure. Based on the aspects and their struc-
ture, it generates an aspect hierarchy.
2.3 Aspect Identification
To identify aspects in consumer reviews, we first
parse each review using the Stanford parser 1. Since
the aspects in consumer reviews are usually noun
1http://nlp.stanford.edu/software/lex-parser.shtml
Figure 2: Sample Pros and Cons reviews
or noun phrases (Liu, 2009), we extract the noun
phrases (NP) from the parse tree as aspect candi-
dates. While these candidates may contain much
noise, we leverage Pros and Cons reviews (see Fig-
ure 2), which are prevalent in forum Web sites,
to assist identify aspects from the candidates. It
has been shown that simply extracting the frequent
noun terms from the Pros and Cons reviews can get
high accurate aspect terms (Liu el al., 2005). Thus,
we extract the frequent noun terms from Pros and
Cons reviews as features, then train a one-class SVM
(Manevitz et al, 2002) to identify aspects from the
candidates. While the obtained aspects may con-
tain some synonym terms, such as ?earphone? and
?headphone?, we further perform synonym cluster-
ing to get unique aspects. Specifically, we first ex-
pand each aspect term with its synonym terms ob-
tained from the synonym termsWeb site 2, then clus-
ter them to obtain unique aspects based on unigram
feature.
2.4 Semantic Distance Learning
Our aspect hierarchy generation approach is essen-
tially based on the semantic relations among as-
pects. We here define a metric, Semantic Distance,
d(ax, ay), to quantitatively measure the semantic re-
lation between aspects ax and ay. We formulate
d(ax, ay) as the weighted sum of some underlying
features,
d(ax, ay) =
?
j
wjfj(ax, ay), (1)
where wj is the weight for j-th feature function
fj(?).
Next, we first introduce the linguistic features
used in our work and then present the semantic dis-
tance learning algorithm that aims to find the opti-
mal weights in Eq.(1).
2http://thesaurus.com
142
2.4.1 Linguistic Features
Given two aspects ax and ay, a feature is defined
as a function generating a numeric score f(ax, ay)
or a vector of scores. The features include Contex-
tual, Co-occurrence, Syntactic, Pattern and Lexical
features (Yang and Callan, 2009). These features are
generated based on auxiliary documents collected
from Web.
Specifically, we issue each aspect term and aspect
term pair as queries to Google and Wikipedia, re-
spectively, and collect the top 100 returned docu-
ments of each query. We then split the documents
into sentences. Based on these documents and sen-
tences, the features are generated as follows.
Contextual features. For each aspect, we collect
the documents containing the aspect as context to
build a unigram language model without smoothing.
Given two aspects, the KL-divergence between their
language models is computed as the Global-Context
feature between them. Similarly, we collect the left
two and right two words surrounding each aspect as
context and build a unigram language model. The
KL-divergence between the language models of two
aspects is defined as the Local-Context feature.
Co-occurrence features. We measure the co-
occurrence of two aspects by Pointwise Mutual
Information (PMI): PMI(ax,ay)=log(Count(ax,ay)/
Count(ax) Count(ay)), where Count(?) stands for the
number of documents or sentences containing the
aspect(s), or the number of Google document hits
for the aspect(s). Based on different definitions of
Count(?), we define the features of Document PMI,
Sentence PMI, and Google PMI, respectively.
Syntactic features. We parse the sentences that
contain each aspect pair into syntactic trees via the
Stanford Parser. The Syntactic-path feature is de-
fined as the average length of the shortest syntactic
path between the aspect pair in the tree. In addi-
tion, for each aspect, we collect a set of sentences
containing it, and label the semantic role of the sen-
tences via ASSERT parser 3. Given two aspects,
the number of the Subject terms overlaps between
their sentence sets is computed as the Subject Over-
lap feature. Similarly, for other semantic roles, such
as objects, modifiers, and verbs, we define the fea-
tures of Object Overlap, Modifier Overlap, and Verb
3http://cemantix.org/assert.html
Overlap, respectively.
Pattern features. 46 patterns are used in our
work, including 6 patterns indicating the hypernym
relations of two aspects (Hearst, 1992), and 40 pat-
terns measuring the part-of relations of two aspects
(Girju et al, 2006). These pattern features are
asymmetric, and they take the parent-child relations
among the aspects into consideration. All the pat-
terns are listed in Appendix A (submitted as supple-
mentary material). Based on these patterns, a 46-
dimensional score vector is obtained for each aspect
pair. A score is 1 if two aspects match a pattern, and
0 otherwise.
Lexical features. We take the word length differ-
ence between two aspects, as Length Difference fea-
ture. In addition, we issue the query ?define:aspect?
to Google, and collect the definition of each aspect.
We then count the word overlaps between the defini-
tions of two aspects, as Definition Overlap feature.
2.4.2 Semantic Distance Learning
This section elaborates the learning algorithm
that optimizes the semantic distance metric, i.e.,
the weighting parameters in Eq.(1). Typically, we
can utilize the initial hierarchy as training data.
The ground-truth distance between two aspects
dG(ax, ay) is generated by summing up all the edge
distances along the shortest path between ax and ay,
where every edge weight is assumed as 1. The dis-
tance metric is then obtained by solving the follow-
ing optimization problem,
argmin
wj |mj=1
?
ax,ay?A0
x<y
(dG(ax, ay) ?
m?
j=1
wjfj(ax, ay))2+??
m?
j=1
w2j ,
(2)
where m is the dimension of linguistic feature, ? is
a tradeoff parameter. Eq.(2) can be rewrote to its
matrix form as,
argmin
w
??d? fTw??2 + ? ? ?w?2 , (3)
where vector d contains the ground-truth distance of
all the aspect pairs. Each element corresponds to
the distance of certain aspect pair, and f is the corre-
sponding feature vector. The optimal solution of w
is given as
w? = (fT f + ? ? I)?1(fTd) (4)
143
where I is the identity metric.
The above learning algorithm can perform well
when sufficient training data (i.e., aspect (term)
pairs) is available. However, the initial hierarchy is
usually too coarse and thus cannot provide sufficient
information. On the other hand, abundant hand-
crafted hierarchies are available on the Web, such
as WordNet and Open Directory Project (ODP). We
here propose to leverage these external hierarchies
to assist semantic distance learning. A distance met-
ric w0 is learned from the external hierarchies us-
ing the above algorithm. Since w0 might be biased
to the characteristics of the external hierarchies, di-
rectly using w0 in our task may not perform well.
Alternatively, we use w0 as prior knowledge to as-
sist learning the optimal distance metric w from the
initial hierarchy. The learning problem is formulated
as follows,
argmin
w
??d? fTw??2 + ? ? ?w?2 + ? ? ?w? w0?2 ,
(5)
where ? and ? are tradeoff parameters.
The optimal solution of w can be obtained as
w? = (fT f + (? + ?) ? I)?1(fTd + ? ? w0). (6)
As a result, we can compute the semantic distance
between each two aspects according to Eq.(1).
2.5 Aspect Hierarchy Generation
Given the aspectsA = {a1, ? ? ? , ak} identified from
reviews and the initial hierarchy H0(A0,R0) ob-
tained from domain knowledge, our task is to con-
struct an aspect hierarchy to contain all the aspects
in A. Inspired by Yang and Callan (2009), we adopt
a multi-criteria optimization approach to incremen-
tally insert the aspects into appropriate positions in
the hierarchy based on multiple criteria.
Before going to the details, we first introduce an
information function to measure the amount of in-
formation carried in a hierarchy. An information
function Info(H) is defined as the sum of the se-
mantic distances of all the aspect pairs in the hierar-
chy (Yang and Callan, 2009).
Info(H(A,R)) =
?
x<y;ax,ay?A
d(ax, ay). (7)
Based on this information function, we then intro-
duce the following three criteria for aspect insertion:
minimum Hierarchy Evolution, minimum Hierarchy
Discrepancy and minimum Semantic Inconsistency.
Hierarchy Evolution is designed to monitor the
structure evolution of a hierarchy. The hierarchy is
incrementally hosting more aspects until all the as-
pects are allocated. The insertion of a new aspect a
into different positions in the current hierarchy H(i)
leads to different new hierarchies. Among these new
hierarchies, we here assume that the optimal one
H(i+1) should introduce the least changes of infor-
mation to H(i).
H?(i+1) = argmin
H(i+1)
?Info(H(i+1) ?H(i)). (8)
By plugging in Eq.(7) and using least square to
measure the information changes, we have,
obj1 = argmin
H(i+1)
(?x<y;ax,ay?Ai?{a} d(ax, ay)
??x<y;ax,ay?Ai d(ax, ay))2, (9)
Hierarchy Discrepancy is used to measure the
global changes of the structure. We assume a good
hierarchy should bring the least changes to the initial
hierarchy,
H?(i+1) = argmin
H(i+1)
?Info(H(i+1) ?H(0))
i + 1 . (10)
We then get,
obj2 = argmin
H(i+1)
1
i+1(
?
x<y;ax,ay?Ai?{a} d(ax, ay)
??x<y;ax,ay?A0 d(ax, ay))2. (11)
Semantic Inconsistency is introduced to quantify
the inconsistency between the semantic distance es-
timated via the hierarchy and that computed from
the feature functions. We assume that a good hier-
archy should precisely reflect the semantic distance
between aspects. For two aspects, their semantic
distance reflected by the hierarchy is computed as
the sum of adjacent distances along the shortest path
between them,
dH(ax, ay) =
?
p<q;(ap,aq)?SP (ax,ay)
d(ap, aq),
(12)
where SP (ax, ay) is the shortest path between the
aspects (ax, ay), (ap, aq) are the adjacent nodes
along the path.
144
We then define the following criteria to find the
hierarchy with minimum semantic inconsistency,
obj3 = argmin
H(i+1)
?
x<y;ax,ay?Ai?{a};
(dH(ax, ay)?d(ax, ay))2,
(13)
where d(ax, ay) is the distance computed based on
the feature functions in Section 2.4.
Through integrating the above criteria, the multi-
criteria optimization framework is formulated as,
obj = argmin
H(i+1)
(?1 ? obj1 + ?2 ? obj2 + ?3 ? obj3)
?1 + ?2 + ?3 = 1; 0 ? ?1, ?2, ?3 ? 1.
(14)
where ?1, ?2, ?3 are the tradeoff parameters.
To summarize, our aspect hierarchy generation
process starts from an initial hierarchy and inserts
the aspects into it one-by-one until all the aspects
are allocated. Each aspect is inserted to the op-
timal position found by Eq.(14). It is worth not-
ing that the insertion order may influence the result.
To avoid such influence, we select the aspect with
the least objective function value in Eq.(14) to in-
sert. Based on resultant hierarchy, the consumer re-
views are then organized to their corresponding as-
pect nodes in the hierarchy. We further prune out the
nodes without reviews from the hierarchy.
Moreover, we perform sentiment classification to
determine consumer opinions on various aspects. In
particular, we train a SVM sentiment classifier based
on the Pros and Cons reviews described in Section
2.3. We collect sentiment terms in the reviews as
features and represent reviews as feature vectors us-
ing Boolean weighting. Note that we define senti-
ment terms as those appear in the sentiment lexicon
provided by MPQA project (Wilson et al, 2005).
3 Implicit Aspect Identification
In this section, we apply the aspect hierarchy to the
task of implicit aspect identification. This task aims
to infer the aspects of reviews that do not explic-
itly express those aspects but actually comment on
them (Liu et al 2005). Take the review ?The phone
is too large? as an example, the task is to infer its
implicit aspect ?size.? It has been observed that the
reviews commenting on a same aspect usually use
some same sentiment terms (Su et al, 2008). There-
fore, sentiment term is an effective feature for identi-
fying implicit aspects. We here collect the sentiment
terms as features to represent each review into a fea-
ture vector. For each aspect node in the hierarchy,
we define its centroid as the average of its feature
vectors, i.e., the feature vectors of all the reviews
that are allocated at this node. We then calculate
the cosine similarity of each implicit-aspect review
to the centriods of all the aspect nodes, and allo-
cate the review into the node with maximum sim-
ilarity. As a result, the implicit aspect reviews are
grouped to their related aspect nodes. In other word,
their aspects are identified as the corresponding as-
pect nodes.
4 Evaluations
In this section, we evaluate the effectiveness of our
approach on aspect identification, aspect hierarchy
generation, and implicit aspect identification.
4.1 Data and Experimental Setting
The details of our product review corpus are given
in Table 1. This corpus contains consumer reviews
on 11 popular products in four domains. These
reviews were crawled from several prevalent fo-
rum Web sites, including cnet.com, viewpoints.com,
reevoo.com and gsmarena.com. All of the reviews
were posted between June, 2009 and Sep 2010. The
aspects of the reviews, as well as the opinions on
the aspects were manually annotated. We also in-
vited five annotators to construct the gold-standard
hierarchies for the products by providing them the
initial hierarchies and the aspects in reviews. The
conflicts between annotators were resolved through
their discussions. For semantic distance learning, we
collected 50 hierarchies from WordNet and ODP, re-
spectively. The details are shown in Table 2. We
listed the topics of the hierarchies in Appendix B
(submitted as supplementary material).
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
Table 1: Statistics of the reviews corpus, # denotes the
size of the reviews/sentences
145
Statistic WordNet ODP
Total # hierarchies 50 50
Total # terms 1,964 2,210
Average # depth 5.5 5.9
Total # related topics 12 16
Table 2: Statistics of the External Hierarchies
Figure 3: Evaluations on Aspect Identification. t-test, p-
values<0.05
We employed F1-measure, which is the combina-
tion of precision and recall, as the evaluation metric
for all the evaluations. For the evaluation on aspect
hierarchy, we defined precision as the percentage of
correctly returned parent-child pairs out of the to-
tal returned pairs, and recall as the percentage of
correctly returned parent-child pairs out of the to-
tal pairs in the gold standard. Throughout the ex-
periments, we empirically set ?1 = 0.4, ?2 = 0.3,
?3 = 0.3, ? = 0.4 and ? = 0.6.
4.2 Evaluations on Aspect Identification
We compared our approach against two state-of-the-
art methods: a) the method proposed by Hu and Liu
(2004), which is based on the association rule min-
ing, and b) the method proposed byWu et al (2009),
which is based on the dependency parser. The re-
sults are presented in Figure 3. On average, our
approach significantly outperforms Hu?s and Wu?s
method in terms of F1-measure by over 5.87% and
3.27%, respectively.
4.3 Evaluations on Aspect Hierarchy
4.3.1 Comparisons with the State-of-the-Arts
We compared our approach against four tra-
ditional hierarchy generation methods in the re-
searches on ontology learning, including a) pattern-
based method (Hearst, 1992) and b) clustering-based
method by Shi et al (2008), c) the method proposed
Figure 4: Evaluations on Aspect Hierarchy Generation. t-
test, p-values<0.05. w/ H denotes the methods with ini-
tial hierarchy, accordingly, w/o H refers to the methods
without initial hierarchy.
by Snow et al (2006) which was based on a proba-
bilistic model, and d) the method proposed by Yang
and Callan (2009). Since our approach and Yang?s
method can utilize initial hierarchy to assist hier-
archy generation, we evaluated their performance
with or without initial hierarchy, respectively. For
the sake of fair comparison, Snow?s, Yang?s and our
methods used the same linguistic features in Section
2.4.1.
Figure 4 shows the performance comparisons
of these five methods. We can see that our ap-
proach without using initial hierarchy outperforms
the pattern-based, clustering-based, Snow?s, and
Yang?s methods by over 17.9%, 19.8%, 2.9% and
6.1% respectively in terms of average F1-measure.
By exploiting initial hierarchy, our approach im-
proves the performance significantly. As compared
to the pattern-based, clustering-based and Snow?s
methods, it improves the average performance by
over 49.4%, 51.2% and 34.3% respectively. Com-
pared to Yang?s method with initial hierarchy, it
achieves 4.7% improvements on the average perfor-
mance.
The results show that pattern-based and
clustering-based methods perform poor. Pattern-
based method may suffer from the problem of low
coverage of patterns, especially when the patterns
are manually pre-defined, while the clustering-
based method (Shi et al, 2008) may sustain to the
bisection clustering mechanism which can only
generate a binary-tree. The results also illustrate
that our approach outperforms Snow?s and Yang?s
methods. By exploiting external hierarchies, our
146
Figure 5: Evaluations on the Impact of Initial Hierarchy.
t-test, p-values<0.05.
approach is able to derive reliable semantic distance
between aspects and thus improve the performance.
4.3.2 Evaluations on Effectiveness of Initial
Hierarchy
In this section, we show that even based on a small
part of the initial hierarchy, our approach can still
generate a satisfactory hierarchy. We explored dif-
ferent proportion of initial hierarchy, including 0%,
20%, 40%, 60% and 80% of the aspect pairs which
are collected top-down from the initial hierarchy. As
shown in Figure 5, the performance increases when
larger proportion of the initial hierarchy is used.
Thus, we can speculate that domain knowledge is
valuable in aspect hierarchy generation.
4.3.3 Evaluations on Effectiveness of
Optimization Criteria
We conducted a leave-one-out study to evaluate
the effectiveness of each optimization criterion. In
particular, we set one of the tradeoff parameters (?1,
?2, ?3) in Eq.(14) to zero, and distributed its weight
to the rest parameters averagely. From Figure 6, we
find that removing any optimization criterion would
degrade the performance on most products. It is in-
teresting to note that removing the third optimiza-
tion criterion, i.e., minimum semantic inconsistency,
slightly increases the performance on two products
(ipad touch and sony MP3). The reason might be
that the values of the three tradeoff parameters (em-
pirically set in Section 4.1) are not suitable for these
two products.
Figure 6: Evaluations of the Optimization Criteria. % of
change in F1-measure when a single criterion is removed.
t-test, p-values<0.05.
Figure 7: Evaluations on the Impact of Linguistic Fea-
tures. t-test, p-values<0.05.
4.3.4 Evaluations on Semantic Distance
Learning
In this section, we evaluated the impact of the fea-
tures and external hierarchies in semantic distance
learning. We investigated five sets of features as de-
scribed in Section 2.4.1, including contextual, co-
occurrence, syntactic, pattern and lexical features.
From Figure 7, we observe that the co-occurrence
and pattern features perform much better than con-
textual and syntactic features. A possible reason
is that co-occurrence and pattern features are more
likely to indicate parent-child aspect relationships,
while contextual and syntactic features are proba-
ble to measure sibling aspect relationships. Among
these features, the lexical features perform the worst.
The combination of all the features achieves the best
performance.
Next, we evaluated the effectiveness of external
hierarchies in semantic distance learning. We com-
pared the performance of our approach with or with-
out the external hierarchies. From Figure 8, we find
that by exploiting the external hierarchies, our ap-
147
Figure 8: Evaluations on the Impact of External Hierar-
chy. t-test, p-values<0.05.
proach improves the performance significantly. The
improvement is over 2.81% in terms of average F1-
measure. This implies that by using external hier-
archies, our approach can obtain effective semantic
distance, and thus improve the performance of as-
pect hierarchy generation.
Additionally, for sentiment classification, our
SVM classifier achieves an average F1-measure of
0.787 in the 11 products.
4.4 Evaluations on Implicit Aspect
Identification
To evaluate the performance of our approach on im-
plicit aspect identification, we collected 29,657 im-
plicit aspect review sentences on the 11 products
from the four forum Web sites introduced in Section
4.1. While most existing approaches for implicit as-
pect identification rely on hand-crafted rules (Liu,
2009), the method proposed in Su et al (2008) can
identify implicit aspects without hand-crafted rules
based on mutual clustering. Therefore, we adopt
Su?s method as the baseline here. Figure 9 illustrates
the performance comparison between Su?s and our
approach. We can see that our approach outperforms
Su?s method by over 9.18% in terms of average F1-
measure. This shows that our approach can iden-
tify the implicit aspects accurately by exploiting the
underlying associations among the sentiment terms
and each aspect in the hierarchy.
5 Related Work
Some researches treated review organization as a
multi-document summarization problem, and gen-
erated a summary by selecting and ordering sen-
tences taken from multiple reviews (Nishikawa et
Figure 9: Evaluations on Implicit Aspects Identification.
t-test, p-values<0.05
al., 2010). These works did not drill down to the
fine-grained level to explore the opinions on the
product aspects. Other researchers proposed to pro-
duce a summary covering consumer opinions on
each aspect. For example, Hu and Liu (2004) fo-
cused on extracting the aspects and determining
opinions on the aspects. However, their gener-
ated summary was unstructured, where the possible
relationships between aspects were not recognized
(Cadilhac et al, 2010). Subsequently, Carenini et
al. (2006) proposed to map the aspect to a user-
defined taxonomy, but the taxonomy was hand-
crafted which was not scalable.
Different from the previous works, we focus on
automatically generating an aspect hierarchy to hi-
erarchically organize consumer reviews. There are
some related works on ontology learning, which
first identify concepts from text, and then determine
parent-child relations between these concepts us-
ing either pattern-based or clustering-based methods
(Murthy et al, 2010). Pattern-based methods usu-
ally defined some lexical syntactic patterns to extract
the relations, while clustering-based methods mostly
utilized the hierarchical clustering methods to build
a hierarchy (Roy et al, 2006). Some works proposed
to integrate the pattern-based and clustering-based
methods in a general model, such as the probabilistic
model (Snow et al, 2006) and metric-based model
(Yang and Callan, 2009).
The researches on aspect identification are also
related to our work. Various aspect identification
methods have been proposed (Popescu et al, 2005),
including supervised methods (Liu el al., 2005), and
unsupervised methods (Mei et al, 2007). Different
148
features have been investigated for this task. For
example, Wu et al (2009) identified aspects based
on the features explored by dependency parser.
For implicit aspect identification, some works pro-
posed to define rules for identification (Liu el al.,
2005), while others suggested to automatically gen-
erate rules via mutual clustering (Su et al, 2008).
On the other hand, there are some related works
on sentiment classification (Pang and Lee, 2008).
These works can be categorized into four granu-
larities: document-level, sentence-level, aspect-level
and word-level sentiment classification (Liu, 2009).
Existing researches have been studied unsupervised
(Kim et al, 2004), supervised (Pang et al, 2002;
Pang et al, 2005) and semi-supervised classification
approaches (Goldberg et al, 2006; Li et al, 2009)
on these four levels.
6 Conclusions and Future Works
In this paper, we have developed a domain-assisted
approach to generate product aspect hierarchy by in-
tegrating domain knowledge and consumer reviews.
Based on the derived hierarchy, we can generate
a hierarchical organization of consumer reviews as
well as consumer opinions on the aspects. With such
organization, user can easily grasp the overview of
consumer reviews, as well as seek consumer reviews
and opinions on any specific aspect by navigating
through the hierarchy. We have further applied the
hierarchy to the task of implicit aspect identification.
We have conducted evaluations on 11 different prod-
ucts in four domains. The experimental results have
demonstrated the effectiveness of our approach. In
the future, we will explore other linguistic features
to learn the semantic distance between aspects, as
well as apply our approach to other applications.
Acknowledgments
This work is supported by NUS-Tsinghua Extreme
Search (NExT) project under the grant number: R-
252-300-001-490. We give warm thanks to the
project and anonymous reviewers for their valuable
comments.
References
P. Beineke, T. Hastie, C. Manning, and S. Vaithyanathan.
An Exploration of Sentiment Summarization. AAAI,
2003.
J. Beckham. The Cnet E-commerce Data set. Technical
Reports, 2005.
G. Carenini, R. Ng, and E. Zwart. Multi-document Sum-
marization of Evaluative Text. ACL, 2006.
A. Cadilhac, F. Benamara, and N. Aussenac-Gilles. On-
tolexical Resources for Feature based OpinionMining:
a Case-study. Ontolex, 2010.
P. Cimiano, A. Madche, S. Staab, and J. Volker. Ontology
Learning. Handbook on Ontologies, Springer, 2004.
P. Cimiano, A. Hotho, and S. Staab. Learning Concept
Hierarchies from Text Corpora using Formal Concept
Analysis. Artificial Intelligence, 2005.
P. Cimiano. Ontology Learning and Population from
Text: Algorithms, Evaluation and Applications.
Springer-Verlag New York, Inc. Secaucus, NJ, USA,
2006.
S. Dasgupta and V. Ng. Mine the Easy, Classify the Hard:
A Semi-supervised Approach to Automatic Sentiment
Classification. ACL, 2009.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. Un-
supervised Named-entity Extraction from theWeb: An
Experimental Study. Artificial Intelligence, 2005.
A. Esuli and F. Sebastiani. A Publicly Available Lexical
Resource for Opinion Mining. LREC, 2006.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
Pulse: Mining Customer Opinions from Free Text.
IDA, 2005.
R. Girju and A. Badulescu. Automatic Discovery of Part-
whole Relations Computational Linguistics, 2006.
A. Goldberg and X. Zhu. Seeing Stars When There
Aren?t Many Stars: Graph-based Semi-supervised
Learning for Sentiment Categorization. ACL, 2006.
M.A. Hearst. Automatic Acquisition of Hyponyms from
Large Text Corpora. Coling, 1992.
M. Hu and B. Liu. Mining and Summarizing Customer
Reviews. SIGKDD, 2004.
X. Hu, N. Sun, C. Zhang, and T.-S. Chua Exploiting
Internal and External Semantics for the Clustering of
Short Texts Using World Knowledge. CIKM, 2009.
S. Kim and E. Hovy. Determining the Sentiment of Opin-
ions. COLING, 2004.
A. C. Konig and E. Brill. Reducing the Human Overhead
in Text Categorization. KDD, 2006.
Z. Kozareva, E. Riloff, and E. Hovy. Semantic Class
Learning from the Web with Hyponym Pattern Link-
age Graphs. ACL, 2008.
T. Li, Y. Zhang, and V. Sindhwani. A Non-negative Ma-
trix Tri-factorization Approach to Sentiment Classifi-
cation with Lexical Prior Knowledge. ACL, 2009.
B. Liu, M. Hu, and J. Cheng. Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web. WWW,
2005.
149
B. Liu. Handbook Chapter: Sentiment Analysis and Sub-
jectivity. Handbook of Natural Language Processing.
Marcel Dekker, Inc. New York, NY, USA, 2009.
L.M.Manevitz andM. Yousef. One-class SVMs for Doc-
ument Classification. Machine Learning, 2002.
Q.Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. Topic
Sentiment Mixture: Modeling Facets and Opinions in
Weblogs. WWW, 2007.
X. Meng and H. Wang. Mining User Reviews: from
Specification to Summarization. ACL-IJCNLP, 2009.
K. Murthy, T.A. Faruquie, L.V. Subramaniam,
K.H. Prasad, and M. Mohania. Automatically
Generating Term-frequency-induced Taxonomies.
ACL, 2010.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Optimizing Informativeness and Readability for Senti-
ment Summarization. ACL, 2010.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Tech-
niques. EMNLP, 2002.
B. Pang and L. Lee. Seeing Stars: Exploiting Class Rela-
tionships for Sentiment Categorization with respect to
Rating Scales. ACL, 2005.
B. Pang and L. Lee. Opinion mining and sentiment anal-
ysis. Foundations and Trends in Information Retrieval,
2008.
HH. Pang, J. Shen, and R. Krishnan Privacy-Preserving,
Similarity-Based Text Retrieval. ACM Transactions
on Internet Technology, 2010.
A.M. Popescu and O. Etzioni. Extracting Product Fea-
tures and Opinions from Reviews. HLT/EMNLP,
2005.
H. Poon and P. Domingos. Unsupervised Ontology In-
duction from Text. ACL, 2010.
G. Qiu, B. Liu, J. Bu, and C. Chen. Expanding Domain
Sentiment Lexicon through Double Propagation. IJ-
CAI, 2009.
S. Roy and L.V. Subramaniam. Automatic Generation
of Domain Models for Call Centers from Noisy Tran-
scriptions. ACL, 2009.
B. Shi and K. Chang. Generating a Concept Hierarchy
for Sentiment Analysis. SMC, 2008.
R. Snow and D. Jurafsky. Semantic Taxonomy Induction
from Heterogenous Evidence. ACL, 2006.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
I. Titov and R. McDonald. A Joint Model of Text and
Aspect Ratings for Sentiment Summarization. ACL,
2008.
P. Turney. Thumbs up or thumbs down? Semantic Orien-
tation Applied to Unsupervised Classification of Re-
views. ACL, 2002.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. Phrase Depen-
dency Parsing for Opinion Mining. ACL, 2009.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
H. Yang and J. Callan A Metric-based Framework for
Automatic Taxonomy Induction. ACL, 2009.
S. Ye and T.-S. Chua. Learning Object Models from
Semi-structured Web Documents. IEEE Transactions
on Knowledge and Data Engineering, 2006.
J. Yi, T. Nasukawa, W. Niblack, and R. Bunescu. Senti-
ment Analyzer: Extracting Sentiments about a Given
Topic using Natural Language Processing Techniques.
ICDM, 2003.
L. Zhuang,F. Jing, and X.Y. Zhu Movie Review Mining
and Summarization CIKM, 2006.
150
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 391?401, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Answering Opinion Questions on Products by Exploiting Hierarchical
Organization of Consumer Reviews
Jianxing Yu, Zheng-Jun Zha, Tat-Seng Chua
School of Computing
National University of Singapore
{jianxing, zhazj, chuats}@comp.nus.edu.sg
Abstract
This paper proposes to generate appropriate
answers for opinion questions about prod-
ucts by exploiting the hierarchical organiza-
tion of consumer reviews. The hierarchy orga-
nizes product aspects as nodes following their
parent-child relations. For each aspect, the re-
views and corresponding opinions on this as-
pect are stored. We develop a new framework
for opinion Questions Answering, which en-
ables accurate question analysis and effective
answer generation by making use the hierar-
chy. In particular, we first identify the (ex-
plicit/implicit) product aspects asked in the
questions and their sub-aspects by referring
to the hierarchy. We then retrieve the corre-
sponding review fragments relevant to the as-
pects from the hierarchy. In order to gener-
ate appropriate answers from the review frag-
ments, we develop a multi-criteria optimiza-
tion approach for answer generation by simul-
taneously taking into account review salience,
coherence, diversity, and parent-child rela-
tions among the aspects. We conduct eval-
uations on 11 popular products in four do-
mains. The evaluated corpus contains 70,359
consumer reviews and 220 questions on these
products. Experimental results demonstrate
the effectiveness of our approach.
1 Introduction
With the rapid development of E-commerce, most
retail websites encourage consumers to post reviews
to express their opinions on the products. For exam-
ple, the review ?The battery of Nokia N95 is amaz-
ing.? reveals positive opinion on the aspect ?bat-
Figure 1: Overview of product opinion-QA framework
tery? of product Nokia N95. An aspect here refers
to a component or an attribute of a certain prod-
uct. Numerous consumer reviews are now available
online, and these reviews contain rich opinionated
information on various aspects of products. They
are naturally a valuable resource for answering opin-
ion questions about products, such as ?How do peo-
ple think about the battery of Nokia N95?? Opin-
ion Question Answering (opinion-QA) on products
seeks to uncover consumers? thinking and feeling
about the products or aspects of products. It is dif-
ferent from traditional factual QA, where the ques-
tions ask for the fact, such as ?Where is the capital
of United States?? and the answer is ?Washington,
D.C.?
For a product opinionated question, the answer
should not be just a best answer. It should reflect the
opinions of various segments of users, and incorpo-
391
rate both positive and negative viewpoints. Hence
the answer should be a summarization of public
opinions and comments on the product or specific
aspect asked in the question (Jiang et al2010).
In addition, it should also include public opinions
and comments on the sub-aspects. Such answers
would help users to understand the inherent reasons
of the opinions on the asked aspect. For exam-
ple, the question ?What do people think the cam-
era of Nokia 5800?? asks for public positive and
negative opinions on the aspect ?camera? of prod-
uct ?Nokia 5800.? The summarization of opinions
on the sub-aspects such as ?lens? and ?resolution?
would help users better understand that the public
complaints on the aspect ?camera? are due to the
poor ?lens? and/or low ?resolution.? Moreover, the
answer should be presented following the general-
to-specific logic, i.e., from general aspects to spe-
cific sub-aspects. This makes the answer easier to
understand by the users (Ouyang et al2009).
Current Opinion-QA methods mainly include
three components, including question analysis that
identifies aspects and opinions asked in the ques-
tions, answer fragment retrieval, and answer gen-
eration which summarizes the retrieved fragments
(Lloret et al2011). Although existing methods
show encouraging performance, they are usually not
able to generate satisfactory answers due to the fol-
lowing drawbacks. First, current methods often
identify aspects as the noun phrases in the questions.
However, noun phrases contain noises that are not
aspects. This gives rise to imprecise aspect identifi-
cation. For example, in the question ?What reasons
can I persuade my wife that people prefer the battery
of Nokia N95?? noun phrases ?wife? and ?people?
are not aspects. Moreover, current methods relied
on noun phrases are not able to reveal the implicit
aspects, which are not explicitly asked in the ques-
tions. For example, the question ?Is iPhone 4 expen-
sive?? asks about the aspect ?price?, but the term
?price? does not appear in the question. Second,
current methods cannot discover sub-aspects of the
asked aspect due to its ignorance of parent-child re-
lations among aspects. Third, the answers generated
by the existing methods do not follow the general-to-
specific logic, leading to difficulty in understanding
the answers.
To overcome these problems, we can resort to
the hierarchical organization of consumer reviews
on products. As illustrated in Figure 2, the hier-
archy organizes product aspects as nodes, follow-
ing their parent-child relations. For each aspect, the
reviews and corresponding opinions on this aspect
are stored. Such hierarchy can naturally facilitate to
identify aspects asked in questions. While explicit
aspects can be recognized by referring to the hier-
archy, implicit aspects can be inferred based on the
associations between sentiment terms and aspects in
the hierarchy (Yu et al2011). The sentiment terms
are discovered from the reviews on corresponding
aspects. Moreover, by following the parent-child re-
lations in the hierarchy, sub-aspects of the asked as-
pect can be directly acquired, and the answers can
present aspects from general to specific.
Motivated by the above observations, we propose
to exploit the hierarchical organization of consumer
reviews for product opinion-QA. As illustrated in
Figure 1, our framework first organizes consumer
reviews of a certain product into a hierarchical or-
ganization. The resulting hierarchy is in turn used
to help question analysis and relevant review frag-
ments retrieval. In order to generate appropriate
answers from the retrieved fragments, we develop
a multi-criteria optimization approach by simulta-
neously taking into account review salience, co-
herence, and diversity. The parent-child relations
among aspects are also incorporated into the ap-
proach to ensure the answers be general-to-specific.
We conduct evaluations on 11 popular products in
four domains. The evaluated corpus contains 70,359
consumer reviews and 220 questions on these prod-
ucts. More details of the dataset are discussed in
Section 4. Experimental results to demonstrate the
effectiveness of our approach.
The main contributions of this paper include,
? We propose to exploit the hierarchical organi-
zation of consumer reviews for answering opin-
ion questions on products.
? With the help of the hierarchy, our pro-
posed framework can accurately identify (ex-
plicit/implicit) aspects asked in the questions,
and the corresponding sub-aspects.
? We develop a multi-criteria optimization ap-
proach to generate informative, coherent, di-
verse and general-to-specific answers.
392
Figure 2: Hierarchical organization for Nokia N95
The rest of this paper is organized as follows. Sec-
tion 2 introduces the components of hierarchical or-
ganization of reviews, question analysis, and answer
fragment retrieval. Section 3 elaborates the multi-
criteria optimization approach for answer generation
. Section 4 presents experimental details, while Sec-
tion 5 reviews related works. Finally, Section 6 con-
cludes this paper with future works.
2 Hierarchical Organization, Question
Analysis, and Answer Fragment
Retrieval
Let R = {r1, ? ? ? , r|R|} denote a collection of con-
sumer reviews of a certain product. Each review re-
flects consumer opinions on the product and/or prod-
uct aspects. Let q denote an opinion question, which
asks for public opinions on a product or some as-
pects of the product. The task is to retrieve the opin-
ionated review fragments relevant to the asked prod-
uct/product aspects, and summarize these fragments
to form an appropriate answer to question q.
Next, we introduce the components of hierarchi-
cal organization that organizes consumer reviews
into a hierarchy, question analysis which identifies
the products/aspects and opinions asked in the ques-
tions, and answer fragment retrieval that retrieves re-
view fragments relevant to the questions.
2.1 Hierarchical Organization of Reviews
We employ the method proposed by Yu et al2011)
to organize consumer reviews of a product into a hi-
erarchical organization. As shown in Figure 2, the
hierarchy organizes product aspects as nodes, fol-
lowing their parent-child relations. In particular, this
method first automatically acquires an initial aspect
hierarchy from the domain knowledge and identifies
aspects commented in the reviews. It then incremen-
tally inserts the identified aspects into appropriate
positions in the initial hierarchy, and finally obtains
an aspect hierarchy that allocates all the newly iden-
tified aspects. The consumer reviews are then orga-
nized to their corresponding aspect nodes in the hi-
erarchy. Sentiment classification is then performed
to determine consumer opinions on the reviews.
The reported performance of Yu et al2011)
on aspect identification, aspect hierarchy generation
and sentiment classification are 0.731, 0.705, 0.787
in terms of average F1-measure, respectively.
2.2 Question Analysis and Answer Fragment
Retrieval
Question analysis consists of five sub-tasks: recog-
nizing product asked in the question; identifying as-
pects in the question; classifying opinions that the
question asks for (the asked opinion could be posi-
tive, negative or both); identifying the question type
(e.g. asking for public opinions, or the reason of
the opinions, etc.); and identifying the question form
(i.e. comparative question or single form question).
Recognizing the product: A name entity recog-
nizer 1 is trained to recognize the product name. In
particular, we collect 420 auxiliary questions from
Yahoo!Answer 2, and manually annotate the prod-
uct names (submitted as supplementary material in
Appendix A). A name entity recognizer for product
is learned on these data, with unigrams and POS tags
as features. Given a testing question, the recognizer
predicts each word as B, I, E or O, where B, I, E de-
note the begin, internal, and end of a product name
respectively, and O corresponds to other words.
Identifying aspects: As aforementioned, simply
extracting the noun phrases as aspects would import
noises. Also, some ?implicit? aspects do not ex-
1http://nlp.stanford.edu/software/CRF-NER.shtml
2http://answers.yahoo.com
393
plicitly appear in the reviews. One simple solution
for these problems can resort to the review hierar-
chy. The hierarchy has organized product aspects,
which can be used to filter the noise noun phrases
for accurately identifying the explicit aspects. For
the implicit aspects, we observe they are usually
modified by some peculiar sentiment terms (Su et
al., 2008). For example, the aspect ?size? is often
modified by the sentiment terms such as ?large?,
but seldom by the terms such as ?expensive.? Thus,
there are some associations between the aspects and
sentiment terms. Such associations can be learned
from the hierarchy and leveraged to infer the im-
plicit aspects (Yu et al2011). In order to simul-
taneously identify the (explicit/implicit) aspects, we
adopt a hierarchical classification technique. The
technique simultaneously learns to identify explicit
aspects, and discovers the associations between as-
pects and sentiment terms by multiple classifiers. In
particular, given a testing question, we identify its
aspect by hierarchically classify (Silla et al2011) it
into the appropriate aspect node of a particular prod-
uct hierarchy. The classification greedily searches a
path in the hierarchy from top to down. The search
begins at the root node, and stops at the leaf node
or a specific node where the relevance score is lower
than a pre-defined threshold. The relevance score on
each node is determined by a SVM classifier. Mul-
tiple SVM classifiers are learned on the hierarchy,
one distinct classifier for a node. The reviews that
are stored in the node and its child-nodes are used
as training samples. We employ the features of noun
terms, and sentiment terms in the sentiment lexicon
provided by MPQA project (Wilson et al2005).
Classifying the opinions: Given a set of testing
questions, we first distinguish the opinion questions
from the factual ones (Yu et al2003). Since the
opinion questions often contain one or more senti-
ment terms, we classify them by employing the sen-
timent terms in the sentiment lexicon provided from
MPQA project (Wilson et al2005). Subsequently,
we learn a SVM sentiment classifier to determine the
opinion polarity of the opinion questions. In partic-
ular, the reviews and corresponding opinions stored
in the hierarchy are used as training samples, which
are represented by the unigram features.
Identifying the question type: Opinion questions
are often categorized into four types (Ku et al
2007),
? Attitude question, asking for public opinion on
a product or product aspect, such as ?What do
people think iPhone 3gs??
? Reason question, asking for the reason of pub-
lic opinion on a product or product aspect, such
as ?Why do people like iPhone 3gs??
? Target question, asking for the object in the
public opinion, such as ?Which phone is better
than Nokia N95??
? Yes/No question, asking for whether a state-
ment is correct, such as ?Is Nokia N95 bad??
We formulate the question type identification as a
multi-class classification problem. A multi-class
SVM classifier 3 is learned for the classification. We
collect 420 auxiliary questions from Yahoo!Answer
and manually annotate their types (submitted as sup-
plementary material in Appendix B). These ques-
tions are used for training, with POS tags and ques-
tion words (i.e. why, what, how, do, is) as features.
Identifying the question form: Question form in-
cludes single and comparative. A question is viewed
as comparative if it contains comparative adjectives
and adverbs (e.g. cheaper, etc.), otherwise as the sin-
gle form (Moghaddam et al2011). The POS tags
are exploited to detect comparative adjectives (i.e.
tag ?JJR?) and adverbs (i.e. tag ?RBR?).
After analyzing the question, we retrieve all re-
view sentences on the asked aspect and all its sub-
aspects from a certain product hierarchy, and choose
the ones relevant to the opinion asked in the ques-
tion. For the single form question, we view the
retrieved sentences as the answer fragments. For
the comparative questions, we select comparative
sentences on the compared products from the re-
trieved sentences, and treat them as the answer frag-
ments. Subsequently, question type is used to define
the template for the answers. In particular, for the
questions asking for reason and attitude, we gener-
ate the answers by summarizing corresponding an-
swer fragments. For questions seeking for a target
as the answer, we output the product names based
on the majority voting of the opinions in the re-
trieved answer fragments. For the yes/no questions,
we first generate the ?yes/no? answer based on the
3http://svmlight.joachims.org/svm multiclass.html
394
consistency between the asked opinions and the ma-
jor opinions in the answer fragments, and then sum-
marize these fragments to form the answers.
3 Answer Generation
Answer generation aims to generate an appropriate
answer for a given opinion question based on the
retrieved answer fragments, i.e., review sentences.
An answer is essentially a sequence of sentences.
Hence, the task of answer generation is to select sen-
tences from the retrieved answer fragments and or-
der them appropriately. We formulate this task into
a multi-criteria optimization problem. We incorpo-
rate multiple criteria in the answer generation pro-
cess, including answer salience, coherence, and di-
versity. The parent-child relations between aspects
is also incorporated to ensure the answer follow the
general-to-specific logic. In the next subsections, we
will introduce details of the proposed multi-criteria
optimization approach.
3.1 Formulation
We first introduce the multiple criteria and then
present the optimization problem.
Salience is used to measure the representative-
ness of the answer. A good answer should consist
of salient review sentences. Let S denote the set
of retrieved sentences. We define a binary variable
si ? {0, 1} to indicate the selection of sentence i
for the answer, i.e. si = 1 (or 0) indicates that si is
selected (or not). Let ?i denote the salience of sen-
tence i. The estimation of ?i will be described in
Section 3.2. The salience score of the answer (i.e.,
a set of sentences) is computed by summing up the
scores of all its constituent sentences, as
?
i?S ?isi.
Coherence is used to quantify the readability of
an answer. To make the answer readable, the con-
stituent sentences in the answer should be ordered
properly. That is, the adjacent sentences should
be coherent. We define ei,j ? {0, 1} to indicate
whether the sentences i and j are adjacent in the an-
swer; where ei,j = 1 (or 0) means they are (or not)
adjacent. The coherence between two adjacent sen-
tences is measured by cij . The estimation of cij will
be described in Section 3.3. As aforementioned, the
answer is expected to be presented in a general-to-
specific manner, i.e. from general aspects to specific
sub-aspects. We define hi,j in Eq.1 to measure the
general-to-specific coherence of sentences i and j.
hi,j =
{
e?
1
leveli?levelj ; if leveli ?= levelj ;
1; otherwise,
(1)
where leveli denotes level position of the aspect
commented in sentence i by referring to the hi-
erarchy, with the root level being 0. The coher-
ence score of the answer is computed by sum-
ming up the scores of all its adjacent sentences as,
?
j?S
?
i?S hi,jci,jei,j .
Diversity. A good answer should diversely cover
all the important information. We introduce a ma-
trix M in Eq.2 to measure the pairwise diversities
among sentences. Mij corresponds to the diversity
between sentences i and j. When sentences i and
j comment on the same aspects, Mij will favor to
select the pair of sentences that discusses on diverse
content (i.e. low similarity). Otherwise, the pair of
sentences commented on different aspects is viewed
to be diverse, and Mij is set as a constant bigger
than one.
Mij =
{
1? similar(i, j) if i, j commented on same aspect
? otherwise,
(2)
where ? is a constant 4.
Multi-Criteria Optimization We integrate the
above criteria into the multi-criteria optimization
formulation,
max{?1 ?
?
i?S ?isi + ?2 ?
?
j?S
?
i?S hi,jci,jei,j
+ ?3 ?
?
j?S
?
i?S siMij ;
{
si, ei,j ? {0, 1},?i, j;
?1 + ?2 + ?3 = 1, 0 ? ?1, ?2, ?3 ? 1,
(3)
where ?1, ?2, ?3 are the trade-off parameters.
We further incorporate the following constrains
into the optimization framework, so as to derive ap-
propriate answers.
? The length of the answer is up to K,
?
i?S
lisi ? K, (4)
where li is the length of sentence i.
? When sentence i is not selected (i.e. si = 0),
the adjacency between any sentence to i is set
4Empirically set to 10 in the experiment.
395
to zero (i.e.
?
i?S ei,j =
?
i?S ej,i = 0).
When sentence i is selected, there are two sen-
tences adjacent to sentence i in the answer, one
before i and another after i. (i.e.
?
i?S ei,j =
?
i?S ej,i = 1).
?
i?S
ei,j =
?
i?S
ej,i = sj , ?j. (5)
? In order to avoid falling into a cycle in sentence
selection, we employ the following constraints
(Deshpande et al2009).
?
i?S f0,i = n + 1;
?
i?S fi,n+1 ? 1;
?
i?S fi,j ?
?
i?S fj,i = sj , ?j;
0 ? fi,j ? (n + 1) ? ei,j , ?i, j,
(6)
where the variable fi,j is an integer to number
the selected adjacent sentences from 1 to n+1,
and the first selected sentence is numbered f0,i
= n + 1. If the last selected sentence obtains a
number fi,n+1 which is bigger then 1, then the
selection has no cycle.
Solution
Given the salience weights ?i|Si=1, and coherence
weights ci,j |Si,j=1, the above multi-criteria optimiza-
tion problem can be solved by Integer Linear Pro-
gramming (Schrijver et al1998). The optimal so-
lutions si|Si=1 and ei,j |Si,j=1 indicate the selected sen-
tences and the order of them. In the next subsec-
tions, we will introduce the estimations of ?i|Si=1
and ci,j |Si,j=1.
3.2 Salience Weight Estimation
The salience weight of sentence i is formulated as
?i =
?G
g=1 ?g(i)/G, where ?(i) denotes the mea-
surement for the importance of sentence i. We de-
fine seven measurements (i.e. G = 7) below.
Helpfulness: Many forum websites provide a
helpfulness score, which is used to rate the quality
of a review. The sentences that come from helpful
reviews are often representative (Mizil et al2009).
We compute ?(i) of sentence i by using helpfulness
score from its host review.
Timeliness: The new coming sentence often con-
tains more updated and useful information (Liu et
al., 2008). ?(i) is the post time of sentence i. We
normalize it to [0, 1].
Grammaticality: The grammatical sentence is
often more readable. We employ the method in
Agichtein et al2008) to calculate the grammar
score. In particular, ?(i) is calculated by the KL-
divergence between language models of sentence i
to Wikipedia articles.
Position: The first sentence in a review is usu-
ally informative (He et al2011). ?(i) is computed
based on the position of the sentence in the review,
i.e. ?(i) = 1/positioni.
Aspect Frequency: The sentence that contains the
frequent aspects is often salient (Nishikawa et al
2010). Hence, ?(i) is computed as the sum of the
frequency for aspects in sentence i.
Centroid Distance: As aforementioned, review
sentences are stored in the corresponding aspect
nodes in the hierarchy. The sentence that is close to
the centroid of the reviews stored in an aspect node
is more likely to be salient (Erkan et al2004). ?(i)
is computed as the Cosine similarity between sen-
tence i to the corresponding review cluster centroid
based on the unigram features.
Local Density: The sentence would be informa-
tive when it is in the dense part of the aspect node
in the feature space (Scott et al1992). We em-
ploy Multivariate Kernel Density Estimation to es-
timate the density. We first represent all the sen-
tences stored in each node into feature vectors, with
unigram as features. The density of a sentence is
then calculated as ?(x) =
?n
i=1 KH(x? xi)/n,
where x denotes the feature vector of sentence i,
n is the size of sentences stored in the node, and
KH(x) = (2?)?1/2 exp(?1/2(xTx)) represents
the Gaussian kernel.
3.3 Coherence Weight Estimation
The coherence ci,j between sentences i and j is
formulated as ci,j = ? ? ?(i, j), where ? is a
weight vector, and ?(i, j) denotes the feature func-
tion. ?(i, j) takes two sentences i and j as input,
and outputs a vector with each dimension indicating
the present/absent of a feature. In order to capture
the sequential relations among sentences, we utilize
features as the Cartesian product over the terms of
N-gram (N=1,2) and POS tags generated from sen-
tences i and j (Lapata et al2003).
To learn the weight vector ?, we employ
the Passive-Aggressive algorithm (Crammer et al
396
2006). It is an online learning algorithm, so that we
can update the weight when more consumer reviews
are available. The algorithm takes up one training
sample and outputs the solution that has the highest
score under the current weight. If the output differs
from training samples, the weight vector is updated
according to Eq.7. Since the consumer reviews often
include multiple sentences, we can directly use the
adjacency of these sentences as training samples. In
particular, we treat the adjacent sentence pairs in the
reviews as training samples (i.e. ci,j = 1).
min
?
??i+1 ? ?i
?
?
{
?i+1 ??(p,q?)? ?i+1 ??(p, q?) ? ?(q?,q?);
?(q?,q?) = 2?T (q?,q
?)
m(m?1)/2 ,
(7)
where ?i is the current weight vector and ?i+1 is
the updated vector, q? and q? are the gold standard
and predicted sequence of sentences, respectively, p
denotes a set of sentences, ?(?) is the feature func-
tion on the whole feature space (i.e.
?
?(?)), ?(?, ?)
is a Kendall?s tau lost function (Lapata et al2006),
T (?, ?) represents the number of inversion operations
that needs to bring q? to q?, and m denotes the num-
ber of sentences.
4 Evaluations
In this section, we evaluate the effectiveness of the
proposed approach, in terms of question analysis
and answer generation.
4.1 Data Set and Experimental Settings
We employed the product review dataset used in Yu
et al2011) as corpus. As illustrated in Table 1, the
dataset contained 70,359 reviews about 11 popular
products in four domains. In addition, we created
220 questions for these products by referring to real
questions in Yahoo!Anwser service. We corrected
the typos and grammar errors for these real ques-
tions. Each product contains 15 opinion questions
and 5 factual questions, respectively. All questions
were shown in Appendix C in supplementary mate-
rial. Three annotators were invited to generate the
gold standard. Each question was labeled by two
annotators. The labels include product name, prod-
uct aspect, opinion, question type and question form.
The average inter-rater agreement in terms of Kappa
statistics is 89%. These annotators were then invited
to read the reviews, and create the ground truth an-
swers by selecting and ordering some review sen-
tences. Such process is time consuming and labor-
intensive. We speed up the annotation process as fol-
lows. We first collected all the review sentences in
the answers generated by three evaluated methods to
be discussed in Section 4.3.1. In addition, we sam-
pled the top-N (N=20) sentences on each asked as-
pect and its sub-aspects respectively, where the sen-
tences were ranked based on their salient weights in
Section 3.2. We then provided such subset of review
sentences to the three annotators, and let them indi-
vidually create an answer of up to 100 words (i.e.
K=100) for each question.
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
Table 1: Statistics of the product review dataset, # denotes
the number of the reviews/sentences.
We employed precision (P), recall (R) and F1-
measure (F1) as the evaluation metric for question
analysis, and utilized ROUGE (Lin et al2003) as
the metric to evaluate the quality of answer gener-
ation. ROUGE is a widely accepted standard for
summarization, which measures the quality of the
summarized answers by counting the overlapping N-
grams between the answers generated by machine
and human, respectively. In the experiment, we
reported the F1-measure of ROUGE-1, ROUGE-2
and ROUGE-SU4, which count the overlapping un-
igrams, bigrams and skip-4 bigrams respectively.
ROUGE-1 can measure informativeness of the an-
swers, while higher order ROUGE-N (N=2,4) cap-
tures the matching of subsequences, which can mea-
sure the fluency and readability of the answers. For
the trade-off parameters, we empirically set ?1 =
0.4, ?2 = 0.3 and ?3 = 0.3.
4.2 Evaluations on Question Analysis
We first evaluated the performance of product recog-
nition, opinion/factual question classification, opin-
ion classification, question type and question form
identification. The experimental results are shown
397
in Table 2. The results show that traditional methods
achieve encouraging performance on the aforemen-
tioned tasks.
Evaluated Topics P R F1
Product recognition 0.755 0.618 0.680
Opinion/factual 0.897 0.895 0.893
Opinion classification 0.755 0.745 0.748
Question type 0.800 0.775 0.783
Question form 0.910 0.903 0.905
Table 2: Performance of question analysis.
Methods P R F1
Our method 0.851* 0.763* 0.805*
Balahur?s method 0.825 0.400 0.538
Table 3: Performance of aspect identification for question
analysis. * denotes the results (i.e. P , R, F1) are tested
for statistical significance using T-Test, p-values<0.05.
Methods P R F1
Our method 0.726* 0.643* 0.682*
Su?s method 0.689 0.571 0.625
Table 4: Performance of implicit aspect identification for
question analysis. T-Test, p-values<0.05
We next examined the performance of our ap-
proach on aspect identification. The method pro-
posed by Balahur et al2008) was reimplemented
as the baseline, which identifies aspects based on
noun phrase extraction. This method achieved good
performance on the opinion QA task in TAC 2008
and was employed in subsequent works. As demon-
strated in Table 3, our approach significantly outper-
forms Balahur?s method by over 49.4% in terms of
average F1-measure. A probable reason is that Bal-
ahur?s method relies on noun phrases, which may
mis-identify some noise noun phrases as aspects,
while our approach performs hierarchical classifica-
tion based on the hierarchy, which can leverage the
prior knowledge encoded in the hierarchy to filter
out the noise and obtain accurate aspects.
Moreover, we evaluated the effectiveness of our
approach on implicit aspect identification. The 70
implicit aspect questions in our question corpus
were used here. The method proposed by Su et al
(2008) was reimplemented as the baseline. It identi-
fies implicit aspects by mutual clustering, and it was
Figure 3: Evaluations on multiple optimization criteria
in terms of ROUGE-1, ROUGE-2, and ROUGE-SU4, re-
spectively.
evaluated in Yu et al2011). As shown in Table 4,
our approach significantly outperforms Su?s method
by over 9.1% in terms of average F1-measure. The
results show that the hierarchy can help to identify
implicit aspects by exploiting the underlying associ-
ations among sentiment terms and aspects.
Methods ROUGE1 ROUGE2 ROUGE-SU4
Our method 0.364* 0.137* 0.138*
Li?s method 0.127 0.043 0.049
Lloret?s method 0.149 0.058 0.065
Table 5: Performance of answer generation. T-Test, p-
values<0.05.
4.3 Evaluations on Answer Generation
4.3.1 Comparisons to the State-of-the-Arts
We compared our multi-criteria optimization ap-
proach against two state-of-the-arts methods: a) the
398
method presented in Li et al2009), which selects
some retrieved sentences to generate the answers
based on a graph-based algorithm; b) the method
proposed by Lloret et al2011) that forms the an-
swers by re-ranking the retrieved sentences.
As shown in Table 5, our approach outperforms
Li?s method and Lloret?s method by the significant
absolute gains of over 23.7%, and 21.5% respec-
tively, in terms of average ROUGE-1. It improves
the performance over these two methods in terms
of average ROUGE-2 by the absolute gains of over
9.41% and 7.87%, respectively; and in terms of
ROUGE-SU4 by the absolute gains of over 8.86%
and 7.31%, respectively. By analyzing the results,
we find that the improvements come from the use
of the hierarchical organization and the answer gen-
eration algorithm which exploits multiple criteria,
especially the parent-child relation among aspects.
In addition, our approach can generate the answers
by following the general-to-specific logic, while Li?s
and Lloret?s methods fail to do so due to their igno-
rance of parent-child relations among aspects.
4.3.2 Evaluations on the Effectiveness of
Multiple Criteria
We further evaluated the effectiveness of each op-
timization criterion by tuning the trade-off parame-
ters (i.e. ?1, ?2, and ?3). We fixed ?1 as a con-
stant in [0, 1] with 0.1 as an interval, and updated ?2
from 0 to 1 ? ?1, ?3 = 1 ? ?1 ? ?2, correspond-
ingly. The performance change is shown in Figure
3 in terms of ROUGE-1, ROUGE-2, and ROUGE-
SU4, respectively. The best performance is achieved
at ?1 = 0.4, ?2 = 0.3, ?3 = 0.3. We observe the
performance drops dramatically when any parame-
ter (i.e. ?1, ?2, ?3) is close to 0 (i.e. remove any of
the corresponding criterion). Thus, we can conclude
that all the criteria are useful in answer generation.
We also find that the performance change is sharp
when ?1 changes. This indicates that the salience
criterion is crucial for answer generation.
Table 6 shows the exemplar answers generated by
our approach. Each answer first gives the statis-
tic of positive and negative reviews. This helps
user to quickly get an overview of public opin-
ions. The summary of relevant review sentences
is then presented in the answer. The answer di-
versely comments the asked aspect and all its avail-
able sub-aspects following the general-to-specific
logic. Moreover, we feel that the answers are in-
formative and readable.
5 Related Works
In this section, we review existing works related
to the four components of our approach, including
organization of reviews, question analysis, answer
fragment retrieval, and answer generation.
For organization of reviews, Carenini et al2006)
proposed to organize the reviews by a hand-crafted
taxonomy, which was not scalable. Yu et al2011)
exploited the domain knowledge and consumer re-
views to automatically generate a hierarchy for or-
ganizing consumer reviews.
Question analysis often has to distinguish the
opinion question from the factual one, and find the
key points asked in the questions, such as the prod-
uct aspect and product name. For example, Yu et
al. (2003) proposed to separate opinions from facts
at both document and sentence level, and determine
the polarity on the opinionated sentences in the an-
swer documents. Similarly, Somasundaran et al
(2007) utilized a SVM classifier to recognize opin-
ionated sentences. The paper argued that the sub-
jective types (i.e. sentiment and arguing) can im-
prove the performance of opinion-QA. Later, Ku et
al. (2007) proposed a two-layered classifier for ques-
tion analysis, and retrieved the answer-fragments by
keyword matching. In particular, they first identified
the opinion questions, and classified them into six
predefined question types, including holder, target,
attitude, reason, majority, and yes/no. These ques-
tion types and corresponding polarity on the ques-
tions were used to filter non-relevant sentences in
the answer fragments. F1-measure was employed as
the evaluation metric.
For the topic of answer generation in opinion-QA,
Li et al2009) formulated it as a sentence ranking
task. They argued that the answers should be simul-
taneously relevant to topics and opinions asked in
the questions. They thus designed the graph-based
methods (i.e. PageRank and HITS) to select some
high-ranked sentences to form answers. They first
built a graph on the retrieved sentences, with each
sentence as the node, and the similarity (i.e. Co-
sine similarity) between each sentences pair as the
399
Question 1: What reasons do people give for preferring iPhone 3gs?
There are 9,928 opinionated reviews about product ?iphone 3gs?, with 5,717 positive and 4,221 negative reviews.
This phone is amazing and I would recommend it to anyone. It looks funky and cool. It is worth the money. It?s great
organiser, simple easy to use software. It is super fast, excellent connection via wifi or 3G. It is able to instantly access email.
It?s amazing and has so many free apps. The design is so simple and global. The hardware is good and reliable. The camera is
a good and colors are vibrant. The touch screen is user friendly and the aesthetics are top notch. Battery is charged quickly,
and power save right after stop using.
Question 2: Does anyone think it is expensive to get a iPhone 3GS?
Yes.
There are 2,645 opinionated reviews on aspect ?price? about product ?iphone 3gs?, with 889 positive and 1,756 negative
reviews.
Throw the costly phone, apple only knows to sell stupid stuff expensively. Don?t fool yourself with iPhone 3gs, believing that it
costs much by Apple luxurious advertising. Apple is so greedy and it just wants to earn easy & fast money by selling its
techless product expensively. The phone will charge once you insert any sim card. iPhone 3gs is high-priced due to the
capacitive and Apple license. You need to pay every application at the end it costs too much. The network provider will make
up some of the cost of the phone on your call charges.
Table 6: Sample answers of our approach.
weight of the corresponding edge. Given a question,
its similarity to each sentence in the graph was com-
puted. Such similarity was viewed as the relevant
score to the corresponding sentence. The sentences
then were ranked based on three metric, i.e. relevant
score to the query, similarity score obtained from the
graph algorithm over sentences, and degree of opin-
ion matching to the query. Respectively, Lloret et
al. (2011) proposed to form answers by re-ranking
the retrieved sentences based on the metric of word
frequency, non-redundancy and the number of noun
phrases. Their method includes three components,
including information retrieval, opinion mining and
text summarization. Evaluations were conducted on
the TAC 2008 Opinion Summarization track. After-
wards, Moghaddam et al2011) developed a system
called AQA to generate answers for questions about
products (i.e. opinion QA on products). It classi-
fies the questions into five types, including target,
attitude, reason, majority and yes/no. As compared
to Ku et al2007), the question types of holder
and majority are not included. They argued that
product questions were seldom asked for the hold-
ers, since the holders (i.e. reviewers) were com-
monly shown in the reviews. Also, product ques-
tions mainly asked for majority opinions, and ma-
jority type was thus not considered. The AQA sys-
tem includes five components, including question
analysis, question expansion, high quality review re-
trieval, subjective sentence extraction, and answer
grouping. The answers are generated by aggregat-
ing opinions in the retrieved fragments.
6 Conclusions and Future Works
In this paper, we have developed a new product
opinion-QA framework, which exploits the hierar-
chical organization of consumer reviews on prod-
ucts. With the help of the hierarchical organization,
our framework can accurately identify the aspects
asked in the questions and also discover their sub-
aspects. We have further formulated the answer gen-
eration from retrieved review sentences as a multi-
criteria optimization problem. The multiple criteria
used include answer salience, diversity, and coher-
ence. The parent-child relations between the aspects
are incorporated into the approach to ensure that the
answers follow the general-to-specific logic. The
proposed framework has been evaluated on 11 pop-
ular products in four domains using 220 questions
on the products. Significant performance improve-
ments were obtained. In the future, we will explore
the more sophisticated NLP features to improve the
proposed framework. This will be done by incorpo-
rating more NLP features in salience and coherence
weights estimation.
Acknowledgments
This work is supported in part by NUS-Tsinghua Ex-
treme Search (NExT) project under the grant num-
ber: R-252-300-001-490. We give warm thanks to
the project and anonymous reviewers for their com-
ments.
400
References
E. Agichtein, C. Castillo, and D. Donato. Finding High-
Quality Content in Social Media. WSDM, 2008.
A. Balahur, E. Boldrini, O. Ferrandez, A. Montoyo,
M. Palomar, and R. Munoz. The DLSIUAES Team?s
Participation in the TAC 2008 Tracks. TAC, 2008.
C. Cardie, J. Wiebe, T. Wilson, and D. Litman. Com-
bining Low-level and Summary Representations of
Opinions for Multi-Perspective Question Answering.
AAAI, 2003.
G. Carenini, R. Ng, and E. Zwart. Multi-document Sum-
marization of Evaluative Text. ACL, 2006.
P. Cimiano. Ontology Learning and Population from
Text: Algorithms, Evaluation and Applications.
Springer-Verlag New York, Inc. Secaucus, NJ, USA,
2006.
K. Crammer, O. Dekel, J. Keshet, S.S. Shwartz, and
Y. Singer. Online Passive Aggressive Algorithms.
Journal of Machine Learning Research, 2006.
P. Deshpande, R. Barzilay, and D.R. Karger. Random-
ized Decoding for Selection-and-Ordering Problems.
NAACL, 2007.
G. Erkan and D.R. Radev. LexRank: Graph-based lexi-
cal centrality as salience in text summarization. AAAI,
2004.
T. Givon. Syntax: A functional-typological Introduction.
Benjamins Pub, 1990.
J. He and D. Dai. Summarization of Yes/No Questions
Using a Feature Function Model. JMLR, 2011.
P. Jiang, H. Fu, C. Zhang, and Z. Niu. A Framework for
Opinion Question Answering. IMS, 2010.
H.D. Kim, D.H. Park, V.G.V. Vydiswaran, and
C.X. Zhai. Opinion Summarization using Entity Fea-
tures and Probabilistic Sentence Coherence Optimiza-
tion: UIUC at TAC 2008 Opinion Summarization Pi-
lot. TAC, 2008.
D. Koller and M. Sahami. Hierarchically Classifying
Documents Using Very Few Words. ICML, 1997.
L.W. Ku, Y.T. Liang, and H.H. Chen. Question Analysis
and Answer Passage Retrieval for Opinion Question
Answering Systems. International Journal of Compu-
tational Linguistics & Chinese Language Processing,
2007.
M. Lapata. Probabilistic Text Structuring: Experiments
with Sentence Ordering. ACL, 2003.
M. Lapata. Automatic Evaluation of Information Order-
ing: Kendalls? Tau. Computational Linguistics, 2006.
F. Li, Y. Tang, M. Huang, and X. Zhu. Answering
Opinion Questions with Random Walks on Graphs.
ACL/AFNLP, 2009.
C.Y. Lin and E.Hovy. Automatic Evaluation of Sum-
maries Using N-gram Co-Occurrence Statistics. HLT-
NAACL, 2003.
Y. Liu, X. Huang, A. An, and X. Yu. Modeling and Pre-
dicting the Helpfulness of Online Reviews. ICDM,
2008.
E. Lloret, A. Balahur, M. Palomar, and A. Montoyo. To-
wards a Unified Approach for Opinion Question An-
swering and Summarization. ACL-HLT, 2011.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Opinion Summarization with Integer Linear Program-
ming Formulation for Sentence Extraction and Order-
ing. COLING, 2010.
C.D. Mizil and G. Kossinets and J. Kleinberg and L. Lee.
How Opinions are Received by Online Communities:
A Case Study on Amazon.com Helpfulness Votes.
WWW, 2009.
S. Moghaddam and M. Ester. AQA: Aspect-based Opin-
ion Question Answering. IEEE-ICDMW, 2011.
Y. Ouyang, W. Li, and Q. Lu. An Integrated Multi-
document Summarization Approach based on Word
Hierarchical Representation. ACL-IJCNLP, 2009.
A. Schrijver. Theory of Linear and Integer Programming.
John Wiley & Sons, 1998.
D.W. Scott. Multivariate Density Estimation: Theory,
Practice, and Visualization. John Wiley & Sons, Inc.,
1992.
C. Silla and A. Freitas. A Survey of Hierarchical Classi-
fication Across Different Application Domains. Data
Mining and Knowledge Discovery, 2011.
S. Somasundaran, T. Wilson, J. Wiebe and V. Stoyanov.
QA with Attitude: Exploiting Opinion Type Analysis
for Improving Question Answering in Online Discus-
sions and the News. ICWSM, 2007.
V. Stoyanov, C. Cardie and J. Wiebe. Multi-
Perspective Question Answering Using the OpQA
Corpus. EMNLP, 2005.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
J. Yu, Z.J. Zha, M. Wang, K. Wang and T.S. Chua.
Domain-Assisted Product Aspect Hierarchy Genera-
tion: Towards Hierarchical Organization of Unstruc-
tured Consumer Reviews. EMNLP, 2011.
J. Yu, Z.J. Zha, M. Wang, and T.S. Chua. Hierarchi-
cal Organization of Unstructured Consumer Reviews.
WWW, 2011.
J. Yu, Z.J. Zha, M. Wang and T.S. Chua. Aspect Rank-
ing: Identifying Important Product Aspects from On-
line Consumer Reviews. ACL, 2011.
H. Yu and V. Hatzivassiloglou. Towards Answering
Opinion Questions: Separating Facts from Opinions
and Identifying the Polarity of Opinion Sentences.
EMNLP, 2003.
401
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1496?1505,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Aspect Ranking: Identifying Important Product Aspects from Online
Consumer Reviews
Jianxing Yu, Zheng-Jun Zha, Meng Wang, Tat-Seng Chua
School of Computing
National University of Singapore
{jianxing, zhazj, wangm, chuats}@comp.nus.edu.sg
Abstract
In this paper, we dedicate to the topic of aspect
ranking, which aims to automatically identify
important product aspects from online con-
sumer reviews. The important aspects are
identified according to two observations: (a)
the important aspects of a product are usually
commented by a large number of consumers;
and (b) consumers? opinions on the important
aspects greatly influence their overall opin-
ions on the product. In particular, given con-
sumer reviews of a product, we first identify
the product aspects by a shallow dependency
parser and determine consumers? opinions on
these aspects via a sentiment classifier. We
then develop an aspect ranking algorithm to
identify the important aspects by simultane-
ously considering the aspect frequency and
the influence of consumers? opinions given to
each aspect on their overall opinions. The ex-
perimental results on 11 popular products in
four domains demonstrate the effectiveness of
our approach. We further apply the aspect
ranking results to the application of document-
level sentiment classification, and improve the
performance significantly.
1 Introduction
The rapidly expanding e-commerce has facilitated
consumers to purchase products online. More than
$156 million online product retail sales have been
done in the US market during 2009 (Forrester Re-
search, 2009). Most retail Web sites encourage con-
sumers to write reviews to express their opinions on
various aspects of the products. This gives rise to
Figure 1: Sample reviews on iPhone 3GS product
huge collections of consumer reviews on the Web.
These reviews have become an important resource
for both consumers and firms. Consumers com-
monly seek quality information from online con-
sumer reviews prior to purchasing a product, while
many firms use online consumer reviews as an im-
portant resource in their product development, mar-
keting, and consumer relationship management. As
illustrated in Figure 1, most online reviews express
consumers? overall opinion ratings on the product,
and their opinions on multiple aspects of the prod-
uct. While a product may have hundreds of aspects,
we argue that some aspects are more important than
the others and have greater influence on consumers?
purchase decisions as well as firms? product devel-
opment strategies. Take iPhone 3GS as an exam-
ple, some aspects like ?battery? and ?speed,? are
more important than the others like ?moisture sen-
sor.? Generally, identifying the important product
aspects will benefit both consumers and firms. Con-
sumers can conveniently make wise purchase deci-
sion by paying attentions on the important aspects,
while firms can focus on improving the quality of
1496
these aspects and thus enhance the product reputa-
tion effectively. However, it is impractical for people
to identify the important aspects from the numerous
reviews manually. Thus, it becomes a compelling
need to automatically identify the important aspects
from consumer reviews.
A straightforward solution for important aspect
identification is to select the aspects that are fre-
quently commented in consumer reviews as the im-
portant ones. However, consumers? opinions on
the frequent aspects may not influence their over-
all opinions on the product, and thus not influence
consumers? purchase decisions. For example, most
consumers frequently criticize the bad ?signal con-
nection? of iPhone 4, but they may still give high
overall ratings to iPhone 4. On the other hand,
some aspects, such as ?design? and ?speed,? may not
be frequently commented, but usually more impor-
tant than ?signal connection.? Hence, the frequency-
based solution is not able to identify the truly impor-
tant aspects.
Motivated by the above observations, in this pa-
per, we propose an effective approach to automat-
ically identify the important product aspects from
consumer reviews. Our assumption is that the
important aspects of a product should be the as-
pects that are frequently commented by consumers,
and consumers? opinions on the important aspects
greatly influence their overall opinions on the prod-
uct. Given the online consumer reviews of a spe-
cific product, we first identify the aspects in the re-
views using a shallow dependency parser (Wu et al,
2009), and determine consumers? opinions on these
aspects via a sentiment classifier. We then design an
aspect ranking algorithm to identify the important
aspects by simultaneously taking into account the
aspect frequency and the influence of consumers?
opinions given to each aspect on their overall opin-
ions. Specifically, we assume that consumer?s over-
all opinion rating on a product is generated based
on a weighted sum of his/her specific opinions on
multiple aspects of the product, where the weights
essentially measure the degree of importance of the
aspects. A probabilistic regression algorithm is then
developed to derive these importance weights by
leveraging the aspect frequency and the consistency
between the overall opinions and the weighted sum
of opinions on various aspects. We conduct ex-
periments on 11 popular products in four domains.
The consumer reviews on these products are crawled
from the prevalent forum Web sites (e.g., cnet.com
and viewpoint.com etc.) More details of our review
corpus are discussed in Section 3. The experimen-
tal results demonstrate the effectiveness of our ap-
proach on important aspects identification. Further-
more, we apply the aspect ranking results to the ap-
plication of document-level sentiment classification
by carrying out the term-weighting based on the as-
pect importance. The results show that our approach
can improve the performance significantly.
The main contributions of this paper include,
1) We dedicate to the topic of aspect ranking,
which aims to automatically identify important as-
pects of a product from consumer reviews.
2) We develop an aspect ranking algorithm to
identify the important aspects by simultaneously
considering the aspect frequency and the influence
of consumers? opinions given to each aspect on their
overall opinions.
3) We apply aspect ranking results to the applica-
tion of document-level sentiment classification, and
improve the performance significantly.
There is another work named aspect ranking
(Snyder et al, 2007). The task in this work is differ-
ent from ours. This work mainly focuses on predict-
ing opinionated ratings on aspects rather than iden-
tifying important aspects.
The rest of this paper is organized as follows. Sec-
tion 2 elaborates our aspect ranking approach. Sec-
tion 3 presents the experimental results, while Sec-
tion 4 introduces the application of document-level
sentiment classification. Section 5 reviews related
work and Section 6 concludes this paper with future
works.
2 Aspect Ranking Framework
In this section, we first present some notations and
then elaborate the key components of our approach,
including the aspect identification, sentiment classi-
fication, and aspect ranking algorithm.
2.1 Notations and Problem Formulation
Let R = {r1, ? ? ? , r|R|} denotes a set of online con-
sumer reviews of a specific product. Each review
r ? R is associated with an overall opinion rating
1497
Or, and covers several aspects with consumer com-
ments on these aspects. Suppose there arem aspects
A = {a1, ? ? ? , am} involved in the review corpus
R, where ak is the k-th aspect. We define ork as the
opinion on aspect ak in review r. We assume that
the overall opinion rating Or is generated based on
a weighted sum of the opinions on specific aspects
ork (Wang et al, 2010). The weights are denoted as
{?rk}mk=1, each of which essentially measures the
degree of importance of the aspect ak in review r.
Our task is to derive the important weights of as-
pects, and identify the important aspects.
Next, we will introduce the key components of
our approach, including aspect identification that
identifies the aspects ak in each review r, aspect sen-
timent classification which determines consumers?
opinions ork on various aspects, and aspect ranking
algorithm that identifies the important aspects.
2.2 Aspect Identification
As illustrated in Figure 1, there are usually two types
of reviews, Pros and Cons review and free text re-
views on the Web. For Pros and Cons reviews, the
aspects are identified as the frequent noun terms in
the reviews, since the aspects are usually noun or
noun phrases (Liu, 2009), and it has been shown
that simply extracting the frequent noun terms from
the Pros and Cons reviews can get high accurate
aspect terms (Liu el al., 2005). To identify the as-
pects in free text reviews, we first parse each review
using the Stanford parser 1, and extract the noun
phrases (NP) from the parsing tree as aspect can-
didates. While these candidates may contain much
noise, we leverage the Pros and Cons reviews to
assist identify aspects from the candidates. In par-
ticular, we explore the frequent noun terms in Pros
and Cons reviews as features, and train a one-class
SVM (Manevitz et al, 2002) to identify aspects in
the candidates. While the obtained aspects may con-
tain some synonym terms, such as ?earphone? and
?headphone,? we further perform synonym cluster-
ing to get unique aspects. Specifically, we first ex-
pand each aspect term with its synonym terms ob-
tained from the synonym terms Web site 2, and then
cluster the terms to obtain unique aspects based on
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://thesaurus.com
unigram feature.
2.3 Aspect Sentiment Classification
Since the Pros and Cons reviews explicitly express
positive and negative opinions on the aspects, re-
spectively, our task is to determine the opinions in
free text reviews. To this end, we here utilize Pros
andCons reviews to train a SVM sentiment classifier.
Specifically, we collect sentiment terms in the Pros
and Cons reviews as features and represent each re-
view into feature vector using Boolean weighting.
Note that we select sentiment terms as those appear
in the sentiment lexicon provided by MPQA project
(Wilson et al, 2005). With these features, we then
train a SVM classifier based on Pros and Cons re-
views. Given a free text review, since it may cover
various opinions on multiple aspects, we first locate
the opinionated expression modifying each aspect,
and determine the opinion on the aspect using the
learned SVM classifier. In particular, since the opin-
ionated expression on each aspect tends to contain
sentiment terms and appear closely to the aspect (Hu
and Liu, 2004), we select the expressions which con-
tain sentiment terms and are at the distance of less
than 5 from the aspect NP in the parsing tree.
2.4 Aspect Ranking
Generally, consumer?s opinion on each specific as-
pect in the review influences his/her overall opin-
ion on the product. Thus, we assume that the con-
sumer gives the overall opinion rating Or based on
the weighted sum of his/her opinion ork on each as-
pect ak:
?m
k=1 ?rkork, which can be rewritten as
?rTor, where?r and or are the weight and opinion
vectors. Inspired by the work of Wang et al (2010),
we viewOr as a sample drawn from aGaussian Dis-
tribution, with mean ?rTor and variance ?2,
p(Or) =
1?
2??2
exp[?(Or ? ?r
Tor)2
2?2
]. (1)
To model the uncertainty of the importance
weights ?r in each review, we assume ?r as a sam-
ple drawn from a Multivariate Gaussian Distribu-
tion, with ? as the mean vector and? as the covari-
ance matrix,
p(?r) =
1
(2pi)n/2|?|1/2 exp[?
1
2(?r ? ?)
T??1(?r ? ?)].
(2)
1498
We further incorporate aspect frequency as a prior
knowledge to define the distribution of ? and ?.
Specifically, the distribution of ? and ? is defined
based on its Kullback-Leibler (KL) divergence to a
prior distribution with a mean vector?0 and an iden-
tity covariance matrix I in Eq.3. Each element in?0
is defined as the frequency of the corresponding as-
pect: frequency(ak)/
?m
i=1 frequency(ai).
p(?,?) = exp[?? ?KL(Q(?,?)||Q(?0, I))],
(3)
where KL(?, ?) is the KL divergence, Q(?,?) de-
notes a Multivariate Gaussian Distribution, and ? is
a tradeoff parameter.
Base on the above definition, the probability of
generating the overall opinion rating Or on review r
is given as,
p(Or|?, r) =
?
p(Or|?rTor, ?2)
? p(?r|?,?) ? p(?,?)d?r,
(4)
where? = {?,?,?, ?2} are the model parameters.
Next, we utilize Maximum Log-likelihood (ML)
to estimate the model parameters given the con-
sumer reviews corpus. In particular, we aim to find
an optimal ?? to maximize the probability of observ-
ing the overall opinion ratings in the reviews corpus.
?? = argmax
?
?
r?R
log(p(Or|?, r))
= argmin
?
(|R| ? 1) log det(?) +
?
r?R
[log ?2+
(Or??rTor)2
?2 + (?r ? ?)
T??1(?r ? ?)]+
(tr(?) + (?0 ? ?)TI(?0 ? ?)).
(5)
For the sake of simplicity, we denote the objective
function
?
r?R log(p(Or|?, r)) as ?(?).
The derivative of the objective function with re-
spect to each model parameter vanishes at the mini-
mizer:
??(?)
??r = ?
(?rTor?Or)or
?2 ??
?1(?r ? ?)
= 0;
(6)
??(?)
?? =
?
r?R
[???1(?r ? ?)]? ? ? I(?0 ? ?)
= 0;
(7)
??(?)
?? =
?
r?R
{?(??1)T ? [?(??1)T (?r ? ?)
(?r ? ?)T (??1)T ]}+ ? ?
[
(??1)T ? I
]
= 0;
(8)
??(?)
??2 =
?
r?R
(? 1?2 +
(Or??rTor)2
?4 ) = 0,
(9)
which lead to the following solutions:
??r = (oror
T
?2 +?
?1)?1(Oror?2 +?
?1?);
(10)
?? = (|R|??1 + ? ? I)?1(??1
?
r?R
?r + ? ? I?0);
(11)
?? = {[ 1?
?
r?R
[
(?r ? ?)(?r ? ?)T
]
+
( |R|??2? )
2I]1/2 ? (|R|??)2? I}
T ;
(12)
??2 = 1|R|
?
r?R
(Or ? ?rTor)2.
(13)
We can see that the above parameters are involved
in each other?s solution. We here utilize Alternating
Optimization technique to derive the optimal param-
eters in an iterative manner. We first hold the param-
eters ?, ? and ?2 fixed and update the parameters
?r for each review r ? R. Then, we update the
parameters ?, ? and ?2 with fixed ?r (r ? R).
These two steps are alternatively iterated until the
Eq.5 converges. As a result, we obtain the optimal
importance weights ?r which measure the impor-
tance of aspects in review r ? R. We then compute
the final importance score ?k for each aspect ak by
integrating its importance score in all the reviews as,
?k =
1
|R|
?
r?R
?rk, k = 1, ? ? ? ,m (14)
It is worth noting that the aspect frequency is con-
sidered again in this integration process. According
to the importance score ?k, we can identify impor-
tant aspects.
3 Evaluations
In this section, we evaluate the effectiveness of our
approach on aspect identification, sentiment classi-
fication, and aspect ranking.
3.1 Data and Experimental Setting
The details of our product review data set is given
in Table 1. This data set contains consumer reviews
on 11 popular products in 4 domains. These reviews
were crawled from the prevalent forum Web sites,
including cnet.com, viewpoints.com, reevoo.com
and gsmarena.com. All of the reviews were posted
1499
between June, 2009 and Sep 2010. The aspects of
the reviews, as well as the opinions on the aspects
were manually annotated as the gold standard for
evaluations.
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
Table 1: Statistics of the Data Sets, # denotes the size of
the reviews/sentences.
To examine the performance on aspect identifi-
cation and sentiment classification, we employed
F1-measure, which was the combination of preci-
sion and recall, as the evaluation metric. To evalu-
ate the performance on aspect ranking, we adopted
Normalized Discounted Cumulative Gain at top k
(NDCG@k) (Jarvelin and Kekalainen, 2002) as the
performance metric. Given an aspect ranking list
a1, ? ? ? , ak, NDCG@k is calculated by
NDCG@k = 1
Z
k
?
i=1
2t(i) ? 1
log(1 + i)
, (15)
where t(i) is the function that represents the reward
given to the aspect at position i, Z is a normaliza-
tion term derived from the top k aspects of a perfect
ranking, so as to normalize NDCG@k to be within
[0, 1]. This evaluation metric will favor the ranking
which ranks the most important aspects at the top.
For the reward t(i), we labeled each aspect as one of
the three scores: Un-important (score 1), Ordinary
(score 2) and Important (score 3). Three volunteers
were invited in the annotation process as follows.
We first collected the top k aspects in all the rank-
ings produced by various evaluated methods (maxi-
mum k is 15 in our experiment). We then sampled
some reviews covering these aspects, and provided
the reviews to each annotator to read. Each review
contains the overall opinion rating, the highlighted
aspects, and opinion terms. Afterward, the annota-
tors were required to assign an importance score to
each aspect. Finally, we took the average of their
scorings as the corresponding importance scores of
the aspects. In addition, there is only one parameter
? that needs to be tuned in our approach. Through-
out the experiments, we empirically set ? as 0.001.
3.2 Evaluations on Aspect Identification
We compared our aspect identification approach
against two baselines: a) the method proposed by
Hu and Liu (2004), which was based on the asso-
ciation rule mining, and b) the method proposed by
Wu et al (2009), which was based on a dependency
parser.
The results are presented in Table 2. On average,
our approach significantly outperforms Hu?s method
and Wu? method in terms of F1-measure by over
5.87% and 3.27%, respectively. In particular, our
approach obtains high precision. Such results imply
that our approach can accurately identify the aspects
from consumer reviews by leveraging the Pros and
Cons reviews.
Data set Hu?s Method Wu?s Method Our Method
Canon EOS 0.681 0.686 0.728
Fujifilm 0.685 0.666 0.710
Panasonic 0.636 0.661 0.706
MacBook 0.680 0.733 0.747
Samsung 0.594 0.631 0.712
iPod Touch 0.650 0.660 0.718
Sony NWZ 0.631 0.692 0.760
BlackBerry 0.721 0.730 0.734
iPhone 3GS 0.697 0.736 0.740
Nokia 5800 0.715 0.745 0.747
Nokia N95 0.700 0.737 0.741
Table 2: Evaluations on Aspect Identification. * signifi-
cant t-test, p-values<0.05.
3.3 Evaluations on Sentiment Classification
In this experiment, we implemented the follow-
ing sentiment classification methods (Pang and Lee,
2008):
1) Unsupervised method. We employed one un-
supervised method which was based on opinion-
ated term counting via SentiWordNet (Ohana et al,
2009).
2) Supervised method. We employed three su-
pervised methods proposed in Pang et al (2002),
including Na??ve Bayes (NB), Maximum Entropy
(ME), SVM. These classifiers were trained based on
the Pros and Cons reviews as described in Section
2.3.
1500
The comparison results are showed in Table 3. We
can see that supervised methods significantly outper-
form unsupervised method. For example, the SVM
classifier outperforms the unsupervised method in
terms of average F1-measure by over 10.37%. Thus,
we can deduce from such results that the Pros and
Cons reviews are useful for sentiment classification.
In addition, among the supervised classifiers, SVM
classifier performs the best in most products, which
is consistent with the previous research (Pang et al,
2002).
Data set Senti NB SVM ME
Canon EOS 0.628 0.720 0.739 0.726
Fujifilm 0.690 0.781 0.791 0.778
Panasonic 0.625 0.694 0.719 0.697
MacBook 0.708 0.820 0.828 0.797
Samsung 0.675 0.723 0.717 0.714
iPod Touch 0.711 0.792 0.805 0.791
Sony NWZ 0.621 0.722 0.737 0.725
BlackBerry 0.699 0.819 0.794 0.788
iPhone 3GS 0.717 0.811 0.829 0.822
Nokia 5800 0.736 0.840 0.851 0.817
Nokia N95 0.706 0.829 0.849 0.826
Table 3: Evaluations on Sentiment Classification. Senti
denotes the method based on SentiWordNet. * significant
t-test, p-values<0.05.
3.4 Evaluations on Aspect Ranking
In this section, we compared our aspect ranking al-
gorithm against the following three methods.
1) Frequency-based method. The method ranks
the aspects based on aspect frequency.
2) Correlation-based method. This method mea-
sures the correlation between the opinions on spe-
cific aspects and the overall opinion. It counts the
number of the cases when such two kinds of opin-
ions are consistent, and ranks the aspects based on
the number of the consistent cases.
3) Hybrid method. This method captures both the
aspect frequency and correlation by a linear combi-
nation, as ?? Frequency-based Ranking + (1 ? ?)?
Correlation-based Ranking, where ? is set to 0.5.
The comparison results are showed in Table 4. On
average, our approach outperforms the frequency-
based method, correlation-based method, and hy-
brid method in terms of NDCG@5 by over 6.24%,
5.79% and 5.56%, respectively. It improves the
performance over such three methods in terms of
NDCG@10 by over 3.47%, 2.94% and 2.58%, re-
spectively, while in terms of NDCG@15 by over
4.08%, 3.04% and 3.49%, respectively. We can de-
duce from the results that our aspect ranking algo-
rithm can effectively identify the important aspects
from consumer reviews by leveraging the aspect fre-
quency and the influence of consumers? opinions
given to each aspect on their overall opinions. Ta-
ble 5 shows the aspect ranking results of these four
methods. Due to the space limitation, we here only
show top 10 aspects of the product iphone 3GS. We
can see that our approach performs better than the
others. For example, the aspect ?phone? is ranked at
the top by the other methods. However, ?phone? is
a general but not important aspect.
# Frequency Correlated Hybrid Our Method
1 Phone Phone Phone Usability
2 Usability Usability Usability Apps
3 3G Apps Apps 3G
4 Apps 3G 3G Battery
5 Camera Camera Camera Looking
6 Feature Looking Looking Storage
7 Looking Feature Feature Price
8 Battery Screen Battery Software
9 Screen Battery Screen Camera
10 Flash Bluetooth Flash Call quality
Table 5: iPhone 3GS Aspect Ranking Results.
To further investigate the reasonability of our
ranking results, we refer to one of the public user
feedback reports, the ?china unicom 100 customers
iPhone user feedback report? (Chinaunicom Report,
2009). The report demonstrates that the top four as-
pects of iPhone product, which users most concern
with, are ?3G Network? (30%), ?usability? (30%),
?out-looking design? (26%), ?application? (15%).
All of these aspects are in the top 10 of our rank-
ing results.
Therefore, we can conclude that our approach is
able to automatically identify the important aspects
from numerous consumer reviews.
4 Applications
The identification of important aspects can support
a wide range of applications. For example, we can
1501
Frequency Correlation Hybrid Our Method
Data set @5 @10 @15 @5 @10 @15 @5 @10 @15 @5 @10 @15
Canon EOS 0.735 0.771 0.740 0.735 0.762 0.779 0.735 0.798 0.742 0.862 0.824 0.794
Fujifilm 0.816 0.705 0.693 0.760 0.756 0.680 0.816 0.759 0.682 0.863 0.801 0.760
Panasonic 0.744 0.807 0.783 0.763 0.815 0.792 0.744 0.804 0.786 0.796 0.834 0.815
MacBook 0.744 0.771 0.762 0.763 0.746 0.769 0.763 0.785 0.772 0.874 0.776 0.760
Samsung 0.964 0.765 0.794 0.964 0.820 0.840 0.964 0.820 0.838 0.968 0.826 0.854
iPod Touch 0.836 0.830 0.727 0.959 0.851 0.744 0.948 0.785 0.733 0.959 0.817 0.801
Sony NWZ 0.937 0.743 0.742 0.937 0.781 0.797 0.937 0.740 0.794 0.944 0.775 0.815
BlackBerry 0.837 0.824 0.766 0.847 0.825 0.771 0.847 0.829 0.768 0.874 0.797 0.779
iPhone 3GS 0.897 0.836 0.832 0.886 0.814 0.825 0.886 0.829 0.826 0.948 0.902 0.860
Nokia 5800 0.834 0.779 0.796 0.834 0.781 0.779 0.834 0.781 0.779 0.903 0.811 0.814
Nokia N95 0.675 0.680 0.717 0.619 0.619 0.691 0.619 0.678 0.696 0.716 0.731 0.748
Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,
and NDCG@15, respectively. * significant t-test, p-values<0.05.
provide product comparison on the important as-
pects to users, so that users can make wise purchase
decisions conveniently.
In the following, we apply the aspect ranking re-
sults to assist document-level review sentiment clas-
sification. Generally, a review document contains
consumer?s positive/negative opinions on various as-
pects of the product. It is difficult to get the ac-
curate overall opinion of the whole review without
knowing the importance of these aspects. In ad-
dition, when we learn a document-level sentiment
classifier, the features generated from unimportant
aspects lack of discriminability and thus may dete-
riorate the performance of the classifier (Fang et al,
2010). While the important aspects and the senti-
ment terms on these aspects can greatly influence the
overall opinions of the review, they are highly likely
to be discriminative features for sentiment classifica-
tion. These observations motivate us to utilize aspect
ranking results to assist classifying the sentiment of
review documents.
Specifically, we randomly sampled 100 reviews of
each product as the testing data and used the remain-
ing reviews as the training data. We first utilized our
approach to identify the importance aspects from the
training data. We then explored the aspect terms and
sentiment terms as features, based on which each re-
view is represented as a feature vector. Here, we
give more emphasis on the important aspects and
the sentiment terms that modify these aspects. In
particular, we set the term-weighting as 1 + ? ??k,
where ?k is the importance score of the aspect ak,
? is set to 100. Based on the weighted features, we
then trained a SVM classifier using the training re-
views to determine the overall opinions on the test-
ing reviews. For the performance comparison, we
compared our approach against two baselines, in-
cluding Boolean weighting method and frequency
weighting (tf ) method (Paltoglou et al, 2010) that
do not utilize the importance of aspects. The com-
parison results are shown in Table 6. We can see
that our approach (IA) significantly outperforms the
other methods in terms of average F1-measure by
over 2.79% and 4.07%, respectively. The results
also show that the Boolean weighting method out-
performs the frequency weighting method in terms
of average F1-measure by over 1.25%, which are
consistent with the previous research by Pang et al
(2002). On the other hand, from the IA weight-
ing formula, we observe that without using the im-
portant aspects, our term-weighting function will be
equal to Boolean weighting. Thus, we can speculate
that the identification of important aspects is ben-
eficial to improving the performance of document-
level sentiment classification.
5 Related Work
Existing researches mainly focused on determining
opinions on the reviews, or identifying aspects from
these reviews. They viewed each aspect equally
without distinguishing the important ones. In this
section, we review existing researches related to our
work.
Analysis of the opinion on whole review text had
1502
SVM + Boolean SVM + tf SVM + IA
Data set P R F1 P R F1 P R F1
Canon EOS 0.689 0.663 0.676 0.679 0.654 0.666 0.704 0.721 0.713
Fujifilm 0.700 0.687 0.693 0.690 0.670 0.680 0.731 0.724 0.727
Panasonic 0.659 0.717 0.687 0.650 0.693 0.671 0.696 0.713 0.705
MacBook 0.744 0.700 0.721 0.768 0.675 0.718 0.790 0.717 0.752
Samsung 0.755 0.690 0.721 0.716 0.725 0.720 0.732 0.765 0.748
iPod Touch 0.686 0.746 0.714 0.718 0.667 0.691 0.749 0.726 0.737
Sony NWZ 0.719 0.652 0.684 0.665 0.646 0.655 0.732 0.684 0.707
BlackBerry 0.763 0.719 0.740 0.752 0.709 0.730 0.782 0.758 0.770
iPhone 3GS 0.777 0.775 0.776 0.772 0.762 0.767 0.820 0.788 0.804
Nokia 5800 0.755 0.836 0.793 0.744 0.815 0.778 0.805 0.821 0.813
Nokia N95 0.722 0.699 0.710 0.695 0.708 0.701 0.768 0.732 0.750
Table 6: Evaluations on Term Weighting methods for Document-level Review Sentiment Classification. IA denotes
the term weighing based on the important aspects. * significant t-test, p-values<0.05.
been extensively studied (Pang and Lee, 2008). Ear-
lier research had been studied unsupervised (Kim et
al., 2004), supervised (Pang et al, 2002; Pang et al,
2005) and semi-supervised approaches (Goldberg et
al., 2006) for the classification. For example, Mullen
et al (2004) proposed an unsupervised classifica-
tion method which exploited pointwise mutual in-
formation (PMI) with syntactic relations and other
attributes. Pang et al (2002) explored several ma-
chine learning classifiers, including Na??ve Bayes,
Maximum Entropy, SVM, for sentiment classifica-
tion. Goldberg et al (2006) classified the sentiment
of the review using the graph-based semi-supervised
learning techniques, while Li el al. (2009) tackled
the problem using matrix factorization techniques
with lexical prior knowledge.
Since the consumer reviews usually expressed
opinions on multiple aspects, some works had
drilled down to the aspect-level sentiment analysis,
which aimed to identify the aspects from the reviews
and to determine the opinions on the specific aspects
instead of the overall opinion. For the topic of aspect
identification, Hu and Liu (2004) presented the asso-
ciation mining method to extract the frequent terms
as the aspects. Subsequently, Popescu et al (2005)
proposed their system OPINE, which extracted the
aspects based on the KnowItAll Web information
extraction system (Etzioni et al, 2005). Liu el al.
(2005) proposed a supervised method based on lan-
guage pattern mining to identify the aspects in the
reviews. Later, Mei et al (2007) proposed a prob-
abilistic topic model to capture the mixture of as-
pects and sentiments simultaneously. Afterwards,
Wu et al (2009) utilized the dependency parser to
extract the noun phrases and verb phrases from the
reviews as the aspect candidates. They then trained
a language model to refine the candidate set, and
to obtain the aspects. On the other hand, for the
topic of sentiment classification on the specific as-
pect, Snyder et al (2007) considered the situation
when the consumers? opinions on one aspect could
influence their opinions on others. They thus built
a graph to analyze the meta-relations between opin-
ions, such as agreement and contrast. And they pro-
posed a Good Grief algorithm to leveraging such
meta-relations to improve the prediction accuracy
of aspect opinion ratings. In addition, Wang et al
(2010) proposed the topic of latent aspect rating
which aimed to infer the opinion rating on the as-
pect. They first employed a bootstrapping-based al-
gorithm to identify the major aspects via a few seed
word aspects. They then proposed a generative La-
tent Rating Regression model (LRR) to infer aspect
opinion ratings based on the review content and the
associated overall rating.
While there were usually huge collection of re-
views, some works had concerned the topic of
aspect-based sentiment summarization to combat
the information overload. They aimed to summa-
rize all the reviews and integrate major opinions on
various aspects for a given product. For example,
Titov et al (2008) explored a topic modeling method
to generate a summary based on multiple aspects.
They utilized topics to describe aspects and incor-
1503
porated a regression model fed by the ground-truth
opinion ratings. Additionally, Lu el al. (2009) pro-
posed a structured PLSA method, which modeled
the dependency structure of terms, to extract the as-
pects in the reviews. They then aggregated opinions
on each specific aspects and selected representative
text segment to generate a summary.
In addition, some works proposed the topic of
product ranking which aimed to identify the best
products for each specific aspect (Zhang et al,
2010). They used a PageRank style algorithm to
mine the aspect-opinion graph, and to rank the prod-
ucts for each aspect.
Different from previous researches, we dedicate
our work to identifying the important aspects from
the consumer reviews of a specific product.
6 Conclusions and Future Works
In this paper, we have proposed to identify the im-
portant aspects of a product from online consumer
reviews. Our assumption is that the important as-
pects of a product should be the aspects that are fre-
quently commented by consumers and consumers?
opinions on the important aspects greatly influence
their overall opinions on the product. Based on this
assumption, we have developed an aspect ranking al-
gorithm to identify the important aspects by simulta-
neously considering the aspect frequency and the in-
fluence of consumers? opinions given to each aspect
on their overall opinions. We have conducted exper-
iments on 11 popular products in four domains. Ex-
perimental results have demonstrated the effective-
ness of our approach on important aspects identifi-
cation. We have further applied the aspect ranking
results to the application of document-level senti-
ment classification, and have significantly improved
the classification performance. In the future, we will
apply our approach to support other applications.
Acknowledgments
This work is supported in part by NUS-Tsinghua Ex-
treme Search (NExT) project under the grant num-
ber: R-252-300-001-490. We give warm thanks to
the project and anonymous reviewers for their com-
ments.
References
P. Beineke, T. Hastie, C. Manning, and S. Vaithyanathan.
An Exploration of Sentiment Summarization. AAAI,
2003.
G. Carenini, R.T. Ng, and E. Zwart. Extracting Knowl-
edge from Evaluative Text. K-CAP, 2005.
G. Carenini, R.T. Ng, and E. Zwart. Multi-document
Summarization of Evaluative Text. ACL, 2006.
China Unicom 100 Customers iPhone User Feedback
Report, 2009.
Y. Choi and C. Cardie. Hierarchical Sequential Learning
for Extracting Opinions and Their Attributes. ACL,
2010.
H. Cui, V. Mittal, and M. Datar. Comparative Experi-
ments on Sentiment Classification for Online Product
Reviews. AAAI, 2006.
S. Dasgupta and V. Ng. Mine the Easy, Classify the Hard:
A Semi-supervised Approach to Automatic Sentiment
Classification. ACL, 2009.
K. Dave, S. Lawrence, and D.M. Pennock. Opinion Ex-
traction and Semantic Classification of Product Re-
views. WWW, 2003.
A. Esuli and F. Sebastiani. A Publicly Available Lexical
Resource for Opinion Mining. LREC, 2006.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. Un-
supervised Named-entity Extraction from theWeb: An
Experimental Study. Artificial Intelligence, 2005.
J. Fang, B. Price, and L. Price. Pruning Non-Informative
Text Through Non-Expert Annotations to Improve
Aspect-Level Sentiment Classification. COLING,
2010.
O. Feiguina and G. Lapalme. Query-based Summariza-
tion of Customer Reviews. AI, 2007.
Forrester Research. State of Retailing Online 2009: Mar-
keting Report. http://www.shop.org/soro, 2009.
A. Goldberg and X. Zhu. Seeing Stars when There aren?t
Many Stars: Graph-based Semi-supervised Learning
for Sentiment Categorization. ACL, 2006.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
Pulse: Mining Customer Opinions from Free Text.
IDA, 2005.
M. Hu and B. Liu. Mining and Summarizing Customer
Reviews. SIGKDD, 2004.
K. Jarvelin and J. Kekalainen. Cumulated Gain-based
Evaluation of IR Techniques. TOIS, 2002.
S. Kim and E. Hovy. Determining the Sentiment of Opin-
ions. COLING, 2004.
J. Kim, J.J. Li, and J.H. Lee. Discovering the Discrimi-
native Views: Measuring Term Weights for Sentiment
Analysis. ACL, 2009.
1504
Kelsey Research and comscore. Online Consumer-
Generated Reviews Have Significant Impact on Offline
Purchase Behavior.
K. Lerman, S. Blair-Goldensohn, and R. McDonald.
Sentiment Summarization: Evaluating and Learning
User Preferences. EACL, 2009.
B. Li, L. Zhou, S. Feng, and K.F. Wong. A Unified Graph
Model for Sentence-Based Opinion Retrieval. ACL,
2010.
T. Li and Y. Zhang, and V. Sindhwani. A Non-negative
Matrix Tri-factorization Approach to Sentiment Clas-
sification with Lexical Prior Knowledge. ACL, 2009.
B. Liu, M. Hu, and J. Cheng. Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web. WWW,
2005.
B. Liu. Handbook Chapter: Sentiment Analysis and Sub-
jectivity. Handbook of Natural Language Processing.
Marcel Dekker, Inc. New York, NY, USA, 2009.
Y. Lu, C. Zhai, and N. Sundaresan. Rated Aspect Sum-
marization of Short Comments. WWW, 2009.
L.M. Manevitz and M. Yousef. One-class svms for Doc-
ument Classification. The Journal of Machine Learn-
ing, 2002.
R. McDonal, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. Structured Models for Fine-to-coarse Sen-
timent Analysis. ACL, 2007.
Q.Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. Topic
Sentiment Mixture: Modeling Facets and Opinions in
Weblogs. WWW, 2007.
H.J. Min and J.C. Park. Toward Finer-grained Sentiment
Identification in Product Reviews Through Linguistic
and Ontological Analyses. ACL, 2009.
T. Mullen and N. Collier. Sentiment Analysis using
Support Vector Machines with Diverse Information
Sources. EMNLP, 2004.
N. Nanas, V. Uren, and A.D. Roeck. Building and Ap-
plying a Concept Hierarchy Representation of a User
Profile. SIGIR, 2003.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Optimizing Informativeness and Readability for Senti-
ment Summarization. ACL, 2010.
B. Ohana and B. Tierney. Sentiment Classification of Re-
views Using SentiWordNet. IT&T Conference, 2009.
G. Paltoglou and M. Thelwall. A study of Information
Retrieval Weighting Schemes for Sentiment Analysis.
ACL, 2010.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Tech-
niques. EMNLP, 2002.
B. Pang, L. Lee, and S. Vaithyanathan. A Sentimen-
tal Education: Sentiment Analysis using Subjectivity
Summarization based on Minimum cuts Techniques.
ACL, 2004.
B. Pang and L. Lee. Seeing stars: Exploiting Class Re-
lationships for Sentiment Categorization with Respect
to Rating Scales. ACL, 2005.
B. Pang and L. Lee. Opinion mining and sentiment
analysis. Foundations and Trends in Information Re-
trieval, 2008.
A.-M. Popescu and O. Etzioni. Extracting Product Fea-
tures and Opinions from Reviews. HLT/EMNLP,
2005.
R. Prabowo and M. Thelwall. Sentiment analysis: A
Combined Approach. Journal of Informetrics, 2009.
G. Qiu, B. Liu, J. Bu, and C. Chen.. Expanding Domain
Sentiment Lexicon through Double Propagation. IJ-
CAI, 2009.
M. Sanderson and B. Croft. Document-word Co-
regularization for Semi-supervised Sentiment Analy-
sis. ICDM, 2008.
B. Snyder and R. Barzilay. Multiple Aspect Ranking us-
ing the Good Grief Algorithm. NAACL HLT, 2007.
S. Somasundaran, G. Namata, L. Getoor, and J. Wiebe.
Opinion Graphs for Polarity and Discourse Classifica-
tion. ACL, 2009.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
C. Toprak, N. Jakob, and I. Gurevych. Sentence and
Expression Level Annotation of Opinions in User-
Generated Discourse. ACL, 2010.
P. Turney. Thumbs up or Thumbs down? Semantic Ori-
entation Applied to Unsupervised Classification of Re-
views. ACL, 2002.
I. Titov and R. McDonald. A Joint Model of Text and
Aspect Ratings for Sentiment Summarization. ACL,
2008.
H. Wang, Y. Lu, and C.X. Zhai. Latent Aspect Rating
Analysis on Review Text Data: A Rating Regression
Approach. KDD, 2010.
B. Wei and C. Pal. Cross Lingual Adaptation: An Exper-
iment on Sentiment Classifications. ACL, 2010.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
T. Wilson and J. Wiebe. Annotating Attributions and Pri-
vate States. ACL, 2005.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. Phrase Depen-
dency Parsing for Opinion Mining. ACL, 2009.
K. Zhang, R. Narayanan, and A. Choudhary. Voice of
the Customers: Mining Online Customer Reviews for
Product Feature-based Ranking. WOSN, 2010.
J. Zhu, H. Wang, and B.K. Tsou. Aspect-based Sentence
Segmentation for Sentiment Summarization. TSA,
2009.
L. Zhuang, F. Jing, and X.Y. Zhu. Movie Review Mining
and Summarization. CIKM, 2006.
1505
