Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 940?946,
Prague, June 2007. c?2007 Association for Computational Linguistics
Probabilistic Parsing Action Models for Multi-lingual Dependency 
Parsing 
Xiangyu Duan 
Institute of Automation, Chi-
nese Academy of Sciences 
xyduan@nlpr.ia.ac.cn 
Jun Zhao 
Institute of Automation, Chi-
nese Academy of Sciences 
jzhao@nlpr.ia.ac.cn 
Bo Xu 
Institute of Automation, Chi-
nese Academy of Sciences 
xubo@hitic.ia.ac.cn 
 
 
Abstract 
Deterministic dependency parsers use pars-
ing actions to construct dependencies. 
These parsers do not compute the probabil-
ity of the whole dependency tree. They 
only determine parsing actions stepwisely 
by a trained classifier. To globally model 
parsing actions of all steps that are taken on 
the input sentence, we propose two kinds 
of probabilistic parsing action models that 
can compute the probability of the whole 
dependency tree. The tree with the maxi-
mal probability is outputted. The experi-
ments are carried on 10 languages, and the 
results show that our probabilistic parsing 
action models outperform the original de-
terministic dependency parser. 
1 Introduction 
The target of CoNLL 2007 shared task (Nivre et al, 
2007) is to parse texts in multiple languages by 
using a single dependency parser that has the ca-
pacity to learn from treebank data. Among parsers 
participating in CoNLL 2006 shared task 
(Buchholz et al, 2006), deterministic dependency 
parser shows great efficiency in time and compa-
rable performances for multi-lingual dependency 
parsing (Nivre et al, 2006). Deterministic parser 
regards parsing as a sequence of parsing actions 
that are taken step by step on the input sentence. 
Parsing actions construct dependency relations be-
tween words. 
Deterministic dependency parser does not score 
the entire dependency tree as most of state-of-the-
art parsers. They only stepwisely choose the most 
probable parsing action. In this paper, to globally 
model parsing actions of all steps that are taken on 
the input sentence, we propose two kinds of prob-
abilistic parsing action models that can compute 
the entire dependency tree?s probability. Experi-
ments are evaluated on diverse data set of 10 lan-
guages provided by CoNLL 2007 shared-task 
(Nivre et al, 2007). Results show that our prob-
abilistic parsing action models outperform the 
original deterministic dependency parser. We also 
present a general error analysis across a wide set of 
languages plus a detailed error analysis of Chinese. 
Next we briefly introduce the original determi-
nistic dependency parsing algorithm that is a basic 
component of our models. 
2 Introduction of Deterministic Depend-
ency Parsing 
There are mainly two representative deterministic 
dependency parsing algorithms proposed respec-
tively by Nivre (2003), Yamada and Matsumoto 
(2003). Here we briefly introduce Yamada and 
Matsumoto?s algorithm, which is adopted by our 
models, to illustrate deterministic dependency 
parsing. The other representative method of Nivre 
also parses sentences in a similar deterministic 
manner except different data structure and parsing 
actions. 
Yamada?s method originally focuses on unla-
beled dependency parsing. Three kinds of parsing 
actions are applied to construct the dependency 
between two focus words. The two focus words are 
the current sub tree?s root and the succeeding (right) 
sub tree?s root given the current parsing state. 
Every parsing step results in a new parsing state, 
which includes all elements of the current partially 
built tree. Features are extracted about these two 
focus words. In the training phase, features and the 
corresponding parsing action compose the training
940
 
 
 
 
 
 
 
 
 
 
 
 
 
He provides confirming evidence RIGHT
He
provides confirming evidence
SHIFT
LEFT
RIGHTconfirming 
He 
provides evidence provides evidence 
He confirming 
provides 
He evidence 
confirming 
Figure 1. The example of the parsing process of Yamada and Matsumoto?s method. The input sentence 
is ?He provides confirming evidence.? 
 
data. In the testing phase, the classifier determines 
which parsing action should be taken based on the 
features. The parsing algorithm ends when there is 
no further dependency relation can be made on the 
whole sentence. The details of the three parsing 
actions are as follows: 
LEFT: it constructs the dependency that the 
right focus word depends on the left focus word. 
RIGHT: it constructs the dependency that the 
left focus word depends on the right focus word. 
SHIFT: it does not construct dependency, just 
moves the parsing focus. That is, the new left focus 
word is the previous right focus word, whose suc-
ceeding sub tree?s root is the new right focus word. 
The illustration of these three actions and the 
parsing process is presented in figure 1. Note that 
the focus words are shown as bold black box. 
We extend the set of parsing actions to do la-
beled dependency parsing. LEFT and RIGHT are 
concatenated by dependency labels, while SHIFT 
remains the same. For example in figure 1, the 
original action sequence ?RIGHT -> SHIFT -> 
RIGHT -> LEFT? becomes ?RIGHT-SBJ -> 
SHIFT -> RIGHT-NMOD -> LEFT-OBJ?. 
3 Probabilistic Parsing Action Models 
Deterministic dependency parsing algorithms are 
greedy. They choose the most probable parsing 
action at every parsing step given the current pars-
ing state, and do not score the entire dependency 
tree. To compute the probability of whole depend-
ency tree, we propose two kinds of probabilistic 
models that are defined on parsing actins: parsing 
action chain model (PACM) and parsing action 
phrase model (PAPM). 
3.1 Parsing Action Chain Model (PACM) 
The parsing process can be viewed as a Markov 
Chain. At every parsing step, there are several can-
didate parsing actions. The objective of this model 
is to find the most probable sequence of parsing 
actions by taking the Markov assumption. As 
shown in figure 1, the action sequence ?RIGHT-
SBJ -> SHIFT -> RIGHT-NMOD -> LEFT-
OBJ? constructs the right dependency tree of the 
example sentence. Choosing this action sequence 
among all candidate sequences is the objective of 
this model.  
Firstly, we should define the probability of the 
dependency tree conditioned on the input sentence. 
)1(),...|()|(
...1
10?
=
?=
ni
ii SdddPSTP  
Where T denotes the dependency tree, S denotes 
the original input sentence,  denotes the parsing 
action at time step i. We add an artificial parsing 
action  as initial action. 
id
0d
We introduce a variable  to denote the 
resulting parsing state when the action  is taken 
on .  is the original input sen-
tence. 
id
context
id
1?idcontext 0dcontext
Suppose  are taken sequentially on the 
input sentence S, and result in a sequence of pars-
ing states , then P(T|S) de-
fined in equation (1) becomes as below: 
ndd ...0
ndd
contextcontext ...
0
941
)4()|(
)3()|(
)2(),...,|(
...1
...1
...1
1
1
10
?
?
?
=
=
=
?
?
?
=
?
ni
di
ni
dd
ni
ddd
i
ii
ii
contextdP
contextcontextP
contextcontextcontextP
 
Formula (3) comes from formula (2) by obeying 
the Markov assumption. Note that formula (4) is 
about the classifier of parsing actions. It denotes 
the probability of the parsing action given the 
parsing state . If we train a classifier 
that can predict with probability output, then we 
can compute P(T|S) by computing the product of 
the probabilities of parsing actions. The classifier 
we use throughout this paper is SVM (Vapnik, 
1995). We adopt Libsvm (Chang and Lin, 2005), 
which can train multi-class classifier and support 
training and predicting with probability output 
(Chang and Lin, 2005). 
id
1?idcontext
For this model, the objective is to choose the 
parsing action sequence that constructs the de-
pendency tree with the maximal probability. 
)5()|(max)|(max
...1
... 11
?
= ?
=
ni
didd in
contextdPSTP  
Because this model chooses the most probable 
sequence, not the most probable parsing action at 
only one step, it avoids the greedy property of the 
original deterministic parsers. 
We use beam search for the decoding of this 
model. We use m to denote the beam size. Then 
beam search is carried out as follows. At every 
parsing step, all parsing states are ordered (or par-
tially m ordered) according to their probabilities. 
Probability of a parsing state is determined by 
multiplying the probabilities of actions that gener-
ate that state. Then we choose m best parsing 
states for this step, and next parsing step only con-
sider these m best parsing states. Parsing termi-
nates when the first entire dependency tree is con-
structed. To obtain a list of n-best parses, we sim-
ply continue parsing until either n trees are found, 
or no further parsing can be fulfilled. 
3.2 Parsing Action Phrase Model (PAPM) 
In the Parsing Action Chain Model (PACM), ac-
tions are competing at every parsing step. Only m 
best parsing states resulted by the corresponding 
actions are kept for every step. But for the parsing 
problem, it is reasonable that actions are competing 
for which phrase should be built. For dependency 
syntax, one phrase consists of the head word and 
all its children. Based on this motivation, we pro-
pose Parsing Action Phrase Model (PAPM), which 
divides parsing actions into two classes: construct-
ing action and shifting action. 
If a phrase is built after an action is performed, 
the action is called constructing action. In original 
Yamada?s algorithm, constructing actions are 
LEFT and RIGHT. For example, if LEFT is taken, 
it indicates that the right focus word has found all 
its children and becomes the head of this new 
phrase. Note that one word with no children can 
also be viewed as a phrase if its dependency on 
other word is constructed. In the extended set of 
parsing actions for labeled parsing, compound ac-
tions, which consist of LEFT and RIGHT con-
catenated by dependency labels, are constructing 
actions. 
If no phrase is built after an action is performed, 
the action is called shifting action. Such action is 
SHIFT. 
We denote  as constructing action and  as 
shifting action. j indexes the time step. Then we 
introduce a new concept: parsing action phrase. 
We use  to denote the ith parsing action phrase. 
It can be expanded as . That is, 
parsing action phrase  is a sequence of parsing 
actions that constructs the next syntactic phrase. 
ja jb
iA
jjkji abbA 1... ???
iA
For example, consider the parsing process in 
figure 1,  is ?RIGHT-SBJ?,  is ?SHIFT, 
RIGHT-NMOD?,  is ?LEFT-OBJ?. Note that 
 consists of a constructing action,  consists 
of a shifting action and a constructing action,  
consists of a constructing action. 
1A 2A
3A
1A 2A
3A
The indexes are different for both sides of the 
expansion ,  is the ith parsing 
action phrase corresponding to both constructing 
action  at time step j and all its preceding shift-
ing actions. Note that on the right side of the ex-
pansion, only one constructing action is allowed 
and is always at the last position, while shifting 
action can occur several times or does not occur at 
all. It is parsing action phrases, i.e. sequences of 
parsing actions, that are competing for which next 
phrase should be built. 
jjkji abbA 1... ??? iA
ja
942
The probability of the dependency tree given the 
input sentence is redefined as: 
)|())|((
)|(
)|...(
)|(
)|(
)...|(
)6(),...|()|(
1
1
1
1
1
11
...2
1
...1
...1
1
...1
...1
...1
...1
11
??
?
?
?
?
?
?
?=
=
=
?
=
=
?
?
?
?
?
?
?
=
+?
=
?
=
??
=
=
=
=
?
jtj
i
i
i
ii
ii
bj
kt
btj
ni
Akj
ni
Ajjkj
ni
Ai
ni
AA
ni
AAA
ni
ii
contextaPcontextbP
contextbP
contextabbP
contextAP
contextcontextP
contextcontextcontextP
SAAAPSTP
 
Where k represents the number of steps that shift-
ing action can be taken.  is the parsing 
state resulting from a sequence of actions 
 taken on . 
iA
context
jjkj abb 1... ?? 1?iAcontext
The objective in this model is to find the most 
probable sequence of parsing action phrases. 
)7()|(max)|(max
...1
... 11
?
= ?
=
ni
AiAA in
contextAPSTP  
Similar with parsing action chain model 
(PACM), we use beam search for the decoding of 
parsing action phrase model (PAPM). The differ-
ence is that PAPM do not keep m best parsing 
states at every parsing step. Instead, PAPM keep m 
best states which are corresponding to m best cur-
rent parsing action phrases (several steps of 
SHIFT and the last step of a constructing action). 
4 Experiments and Results 
Experiments are carried on 10 languages provided 
by CoNLL 2007 shared-task organizers (Nivre et 
al., 2007). Among these languages, Chinese (Chen 
et al, 2003), Catalan (Mart? et al, 2007) and Eng-
lish (Johansson and Nugues, 2007) have low per-
centage of non-projective relations, which are 
0.0%, 0.1% and 0.3% respectively. Except these 
three languages, we use software of projectiviza-
tion/deprojectivization provided by Nivre and 
Nilsson (2005) for other languages. Because our 
algorithm only deals with projective parsing, we 
should projectivize training data at first to prepare 
for the following training of our algorithm. During 
testing, deprojectivization is applied to the output 
of the parser. 
Considering the classifier of Libsvm (Chang and 
Lin, 2005), the features are extracted from the fol-
lowing fields of the data representation: FORM, 
LEMMA, CPOSTAG, POSTAG, FEATS and DE-
PREL. We split values of FEATS field into its 
atomic components. We only use available features 
of DEPREL field during deterministic parsing. We 
use similar feature context window as used in Ya-
mada?s algorithm (Yamada and Matsumoto, 2003). 
In detail, the size of feature context window is six, 
which consists of left two sub trees, two focus 
words related sub trees and right two sub trees. 
This feature template is used for all 10 languages. 
4.1 Results of PACM and Yamada?s Method 
After submitting the testing results of Parsing Ac-
tion Chain Model (PACM), we also perform origi-
nal deterministic parsing proposed by Yamada and 
Matsumoto (2003). The total results are shown in 
table 1. The experimental results are mainly evalu-
ated by labeled attachment score (LAS), unlabeled 
attachment score (UAS) and labeled accuracy (LA). 
Table 1 shows that Parsing Action Chain Model 
(PACM) outperform original Yamada?s parsing 
method for all languages. The LAS improvements 
range from 0.60 percentage points to 1.71 percent-
age points. Note that the original Yamada?s 
method still gives testing results above the official 
reported average performance of all languages. 
 Ara Bas Cat Chi Cze Eng Gre Hun Ita Tur 
YamLAS  69.31 69.67 83.26 81.88 74.63 84.81 72.75 76.24 80.08 73.94
YamUAS  78.93 75.86 88.53 86.17 80.11 85.83 79.45 79.97 83.69 79.79
YamLA  81.13 75.71 88.36 84.56 82.10 89.71 82.58 88.37 86.93 80.81
PACMLAS  69.91 71.26 84.95 82.58 75.34 85.83 74.29 77.06 80.75 75.03
PACMUAS  79.04 77.57 89.71 86.88 80.82 86.97 80.77 80.66 84.20 81.03
PACMLA  81.40 77.35 89.55 85.35 83.17 90.57 83.87 88.92 87.32 81.17
Table 1. The performances of Yamada?s method (Yam) and Parsing Action Chain Model (PACM). 
 
943
4.2 Results of PAPM 
Not all languages have only one root node of a 
sentence. Since Parsing Action Phrase Model 
(PAPM) only builds dependencies, and shifting 
action is not the ending action of a parsing action 
phrase, PAPM always ends with one root word. 
This property makes PAPM only suitable for 
Catalan, Chinese, English and Hungarian, which 
are unary root languages. PAPM result of Catalan 
was not submitted before deadline due to the    
shortage of time and computing resources. We 
report Catalan?s PAPM result together with that of 
other three languages in table 2.  
 
 Cat Chi Eng Hun 
PAPMLAS  87.26 82.64 86.69 76.89 
PAPMUAS  92.07 86.94 87.87 80.53 
PAPMLA  91.89 85.41 92.04 89.73 
Table 2. The performance of Parsing Action 
Phrase Model (PAPM) for Catalan, Chinese, Eng-
lish and Hungarian. 
 
Compared with the results of PACM shown in 
table 1, the performance of PAPM differs among 
different languages. Catalan and English show 
that PAPM improves 2.31% and 0.86% respec-
tively over PACM, while the improvement of Chi-
nese is marginal, and there is a little decrease of 
Hungarian. Hungarian has relatively high percent-
age of non-projective relations. If phrase consists 
of head word and its non-projective children, the 
constructing actions that are main actions in 
PAPM will be very difficult to be learned because 
some non-projective children together with their 
heads have no chance to be simultaneously as fo-
cus words. Although projectivization is also per-
formed for Hungarian, the built-in non-projective 
property still has negative influence on the per-
formance. 
5 Error Analysis 
In the following we provide a general error analy-
sis across a wide set of languages plus a detailed 
analysis of Chinese. 
5.1 General Error Analysis 
One of the main difficulties in dependency parsing 
is the determination of long distance dependencies. 
Although all kinds of evaluation scores differ 
dramatically among different languages, 69.91% 
to 85.83% regarding LAS, there are some general 
observations reflecting the difficulty of long dis-
tance dependency parsing. We study this difficulty 
from two aspects about our full submission of 
PACM: precision of dependencies of different arc 
lengths and precision of root nodes. 
For arcs of length 1, all languages give high 
performances with lowest 91.62% of Czech 
(B?hmova et al, 2003) to highest 96.8% of Cata-
lan (Mart? et al, 2007). As arcs lengths grow 
longer, various degradations are caused. For Cata-
lan, score of arc length 2 is similar with that of arc 
length 1, but there are dramatic degradations for 
longer arc lengths, from 94.94% of arc length 2 to 
85.22% of length 3-6. For English (Johansson and 
Nugues, 2007) and Italian (Montemagni et al, 
2003), there are graceful degradation for arcs of 
length 1,2 and 3-6, with 96-91-85 of English and 
95-85-75 of Italian. For other languages, long arcs 
also give remarkable degradations that pull down 
the performance. 
Precision of root nodes also reflects the per-
formance of long arc dependencies because the 
arc between the root and its children are often 
long arcs. In fact, it is the precision of roots and 
arcs longer than 7 that mainly pull down the over-
all performance. Yamada?s method is a bottom-up 
parsing algorithm that builds short distance de-
pendencies at first. The difficulty of building long 
arc dependencies may partially be resulted from 
the errors of short distance dependencies. The de-
terministic manner causes error propagation, and 
it indirectly indicates that the errors of roots are 
the final results of error propagation of short dis-
tance dependencies. But there is an exception oc-
curred in Chinese. The root precision is 90.48%, 
only below the precision of arcs of length 1. This 
phenomenon exists because the sentences in Chi-
nese data set (Chen et al, 2003) are in fact clauses 
with average length of 5.9 rather than entire sen-
tences. The root words are heads of clauses. 
Both Parsing Action Chain Model (PACM) and 
Parsing Action Phrase Model (PAPM) avoid 
greedy property of original Yamada?s method. It 
can be expected that there will be a precision im-
provement of long distance dependencies over 
original Yamada?s method. For PACM, the results 
of Basque (Aduriz et al, 2003), Catalan (Mart? et 
al., 2007), Chinese (Chen et al, 2003), English 
(Johansson and Nugues, 2007) and Greek (Pro-
944
kopidis et al, 2005) show that the root precision 
improvement over Yamada?s method is more con-
spicuous than that of other long distance depend-
encies. The largest improvement of roots precision 
is 10.7% of Greek. While for Arabic (Hajic et al, 
2004), Czech (B?hmova et al, 2003), Hungarian 
(Csendes et al, 2005), Italian (Montemagni et al, 
2003) and Turkish (Oflazer et al, 2003), the im-
provement of root precision is small, but depend-
encies of arcs longer than 1 give better scores. For 
PAPM, good performances of Catalan and English 
also give significant improvements of root preci-
sion over PACM. For Catalan, the root precision 
improvement is from 63.86% to 95.21%; for Eng-
lish, the root precision improvement is from 
62.03% to 89.25%. 
5.2 Error Analysis of Chinese 
There are mainly two sources of errors regarding 
LAS in Chinese dependency parsing. 
One is from conjunction words (C) that have a 
relatively high percentage of wrong heads (about 
20%), and therefore 19% wrong dependency la-
bels. In Chinese, conjunction words often con-
catenate clauses. Long distance dependencies be-
tween clauses are bridged by conjunction words. 
It is difficult for conjunction words to find their 
heads. 
The other source of errors comes from auxiliary 
words (DE) and preposition words (P). Unlike 
conjunction words, auxiliary words and preposi-
tion words have high performance of finding right 
head, but label accuracy (LA) decrease signifi-
cantly. The reason may lie in the large depend-
ency label set consisting of 57 kinds of depend-
ency labels in Chinese. Moreover, auxiliary words 
(DE) and preposition words (P) have more possi-
ble dependency labels than other coarse POS have. 
This introduces ambiguity for parsers. 
Most common POS including noun and verb 
contribute much to the overall performance of 
83% Labeled Attachment Scores (LAS). Adverbs 
obtain top score while adjectives give the worst. 
6 Conclusion 
We propose two kinds of probabilistic models 
defined on parsing actions to compute the prob-
ability of entire sentence. Compared with original 
Yamada and Matsumoto?s deterministic depend-
ency method which stepwisely chooses most 
probable parsing action, the two probabilistic 
models improve the performance regarding all 10 
languages in CoNLL 2007 shared task. Through 
the study of parsing results, we find that long dis-
tance dependencies are hard to be determined for 
all 10 languages. Further analysis about this diffi-
culty is needed to guide the research direction. 
Feature exploration is also necessary to provide 
more informative features for hard problems. 
Ackowledgements 
This work was supported by Hi-tech Research and 
Development Program of China under grant No. 
2006AA01Z144, the Natural Sciences Foundation 
of China under grant No. 60673042, and the Natu-
ral Science Foundation of Beijing under grant No. 
4052027, 4073043. 
References 
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 
2006. CoNLL-X shared task on multilingual de-
pendency parsing. SIGNLL. 
Chih-Chung Chang and Chih-Jen Lin. 2005. LIBSVM: 
A library for support vector machines. 
J. Nivre. 2003. An efficient algorithm for projective 
dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies 
(IWPT). 
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proc. of ACL-2005, pages 99?
106. 
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, S. Marinov. 
2006. Labeled Pseudo-Projective Dependency 
Parsing with Support Vector Machines. In Proc. of 
the Tenth Conference on Computational Natural 
Language Learning (CoNLL). 
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 
shared task on dependency parsing. In Proc. of the 
Joint Conf. on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL). 
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In 
Proceedings of the 8th International Workshop on 
Parsing Technologies (IWPT). 
V. Vapnik. 1995. The Nature of StatisticalLearning 
Theory. Springer. 
945
A. Abeill?, editor. 2003. Treebanks: Building and 
Using Parsed Corpora. Kluwer.  
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque Dependency Tree-
bank. In Proc. of the 2nd Workshop on Treebanks 
and Linguistic Theories (TLT), pages 201?204. 
A. B?hmov?, J. Hajic, E. Hajicov? and B. Hladk?. 
2003. The PDT: a 3-level annotation scenario. In 
Abeill? (2003), chapter 7, 103?127. 
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. 
Huang and Z. Gao. 2003. Sinica Treebank: Design 
Criteria, Representational Issues and Implementa-
tion. In Abeill? (2003), chapter 13, pages 231?248. 
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 
2005. The Szeged Treebank. Springer.  
J. Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. 
Beska. 2004. Prague Arabic Dependency Treebank: 
Development in Data and Tools. In Proc. of the 
NEMLAR Intern. Conf. on Arabic Language Re-
sources and Tools, pages 110?117. 
R. Johansson and P. Nugues. 2007. Extended 
constituent-to-dependency conversion for English. 
In Proc. of the 16th Nordic Conference on 
Computational Linguistics (NODALIDA).  
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313?330. 
M. A. Mart?, M. Taul?, L. M?rquez and M. Bertran. 
2007. CESS-ECE: A Multilingual and Multilevel 
Annotated Corpus. Available for download from: 
http://www.lsi.upc.edu/~mbertran/cess-ece/. 
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, 
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. 
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. 
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R. 
Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (2003), chapter 11, 
pages 189?210.  
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 
shared task on dependency parsing. In Proc. of the 
CoNLL 2007 Shared Task. Joint Conf. on Empirical 
Methods in Natural Language Processing and 
Computational Natural Language Learning 
(EMNLP-CoNLL). 
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. 
T?r. 2003. Building a Turkish treebank. In Abeill? 
(2003), chapter 15, pages 261?277.  
P. Prokopidis, E. Desypri, M. Koutsombogera, H. 
Papageorgiou, and S. Piperidis. 2005. Theoretical 
and practical issues in the construction of a Greek 
depen- dency treebank. In Proc. of the 4th 
Workshop on Treebanks and Linguistic Theories 
(TLT), pages 149?160. 
946
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 117?126,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Adding Redundant Features for CRFs-based Sentence Sentiment 
Classification 
 
 
Jun Zhao, Kang Liu, Gen Wang 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China 
{jzhao, kliu, gwang}@nlpr.ia.ac.cn 
 
 
 
 
 
 
Abstract 
In this paper, we present a novel method 
based on CRFs in response to the two special 
characteristics of ?contextual dependency? 
and ?label redundancy? in sentence sentiment 
classification. We try to capture the contextual 
constraints on sentence sentiment using CRFs. 
Through introducing redundant labels into the 
original sentimental label set and organizing 
all labels into a hierarchy, our method can add 
redundant features into training for capturing 
the label redundancy.  The experimental 
results prove that our method outperforms the 
traditional methods like NB, SVM, MaxEnt 
and standard chain CRFs. In comparison with 
the cascaded model, our method can 
effectively alleviate the error propagation 
among different layers and obtain better 
performance in each layer. 
1 Introduction* 
There are a lot of subjective texts in the web, such 
as product reviews, movie reviews, news, 
editorials and blogs, etc. Extracting these 
subjective texts and analyzing their orientations 
play significant roles in many applications such as 
electronic commercial, etc. One of the most 
important tasks in this field is sentiment 
                                                           
* Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cn 
classification, which can be performed in several 
levels: word level, sentence level, passage level, 
etc. This paper focuses on sentence level sentiment 
classification. 
Commonly, sentiment classification contains 
three layers of sub-tasks. From upper to lower, (1) 
Subjective/Objective classification: the subjective 
texts are extracted from the corpus teeming with 
both subjective and objective texts. (2) Polarity 
classification: a subjective text is classified into 
?positive? or ?negative? according to the 
sentimental expressions in the text. (3) Sentimental 
strength rating: a subjective text is classified into 
several grades which reflect the polarity degree of 
?positive? or ?negative?. It is a special multi-class 
classification problem, where the classes are 
ordered. In machine learning, this kind of problem 
is also regarded as an ordinal regression problem 
(Wei Wu et al 2005). In this paper, we mainly 
focus on this problem in sentiment classification. 
Sentiment classification in sentence level has its 
special characteristics compared with traditional 
text classification tasks. Firstly, the sentiment of 
each sentence in a discourse is not independent to 
each other. In other words, the sentiment of each 
sentence is related to those of other adjacent 
sentences in the same discourse. The sentiment of 
a sentence may vary in different contexts. If we 
detach a sentence from the context, its sentiment 
may not be inferred correctly. Secondly, there is 
redundancy among the sentiment classes, 
117
especially in sentimental strength classes. For 
example: 
?I love the scenario of ?No country for old man? 
very much!!? 
?This movie sounds good.? 
The first sentence is labeled as ?highly praised? 
class and the second one is labeled as ?something 
good? class. Both the sentences express positive 
sentiment for the movie, but the former expresses 
stronger emotion than the latter. We can see that 
both ?highly praised? and ?something good? 
belong to an implicit class ?positive?, which can be 
regarded as the relation between them.  If we add 
these implicit classes in the label set, the sentiment 
classes will form a hierarchical structure. For 
example, ?positive? can be regarded as the parent 
class of ?highly praised? and ?something good?, 
?subjective? can be regarded as the parent class of 
?positive? and ?negative?. This implicit 
hierarchical structure among labels should not be 
neglected because it may be beneficial for 
improving the accuracy of sentiment classification. 
In the paper, we call this characteristic of 
sentiment classification as ?label redundancy?. 
Unfortunately, in our knowledge most of the 
current research treats sentiment classification as a 
traditional multi-classification task or an ordinal 
regression task, which regard the sentimental 
classes being independent to each other and each 
sentence is also independent to the adjacent 
sentences in the context. In other words, they 
neglect the contextual information and the 
redundancy among sentiment classes. 
In order to consider the contextual information in 
the process of the sentence sentiment classification, 
some research defines contextual features and 
some uses special graph-based formulation, like 
(Bo Pang, et al 2005). In order to consider the 
label redundancy, one potential solution is to use a 
cascaded framework which can combine 
subjective/objective classification, polarity 
classification and sentimental strength 
classification together, where the classification 
results of the preceding step will be the input of the 
subsequent one. However, the subsequent 
classification cannot provide constraint and 
correction to the results of the preceding step, 
which will lead to the accumulation and 
propagation of the classification errors. As a result, 
the performance of sentiment analysis of sentences 
is often not satisfactory.  
This paper focuses on the above two special 
characteristics of the sentiment classification 
problem in the sentence level. To the first 
characteristic, we regard the sentiment 
classification as a sequence labeling problem and 
use conditional random field (CRFs) model to 
capture the relation between two adjacent 
sentences in the context. To the second 
characteristic, we propose a novel method based on 
a CRF model, in which the original task is mapped 
to a classification on a hierarchical structure, which 
is formed by the original label set and some 
additional implicit labels. In the hierarchical 
classification framework, the relations between the 
labels can be represented as the additional features 
in classification. Because these features are related 
to the original labels but unobserved, we name 
them as ?redundant features? in this paper. They 
can be used to capture the redundant and 
hierarchical relation between different sentiment 
classes. In this way, not only the performance of 
sentimental strength rating is improved, the 
accuracies of subjective/objective classification 
and polarity classification are also improved 
compared with the traditional sentiment 
classification method. And in comparison with the 
cascaded method, the proposed approach can 
effectively alleviate error propagation. The 
experimental results on movie reviews prove the 
validity of our method. 
2 Capturing Contextual Influence for 
Sentiment Classification 
For capturing the influence of the contexts to the 
sentiment of a sentence, we treat original sentiment 
classification as a sequence labeling problem. We 
regard the sentiments of all the sentences 
throughout a paragraph as a sequential flow of 
sentiments, and we model it using a conditional 
model. In this paper, we choose Conditional 
Random Fields (CRFs) (Lafferty et al 2001) 
because it has better performance than other 
sequence labeling tools in most NLP applications.  
CRFs are undirected graphical models used to 
calculate the conditional probability of a set of 
labels given a set of input variables. We cite the 
definitions of CRFs in (Lafferty et al 2001). It 
defines the conditional probability proportional to 
the product of potential functions on cliques of the 
graph, 
118
exp ( , )( | )
( )
F Y XP Y X
Z X?
? ?
=      (1) 
where X is a set of input random variables and Y is 
a set of random labels. ( , )F Y X is an arbitrary 
feature function over its arguments, ? is a learned 
weight for each feature function and  
( ) exp( ( , ))
y
XZ F Y X?= ?? . 
The training of CRFs is based on Maximum 
Likelihood Principle (Fei Sha et al 2003). The log 
likelihood function is 
[ ]( ) ( , ) log ( )k k kkL F Y X Z X?? ?= ? ??  
Therefore, Limited-memory BFGS (L-BFGS) 
algorithm is used to find this nonlinear 
optimization parameters.  
3 Label Redundancy in Sentiment 
Classification 
In this section, we explain the ?label redundancy? 
in sentiment classification mentioned in the first 
section. We will analyze the effect of the label 
redundancy on the performance of sentiment 
classification from the experimental view.  
We conduct the experiments of polarity 
classification and sentimental strength rating on the 
corpus which will be introduced in section 5 later. 
The class set is also illustrated in that section. 
Polarity classification is a three-class classification 
process, and sentimental strength rating is a five-
class classification process. We use first 200 
reviews as the training set which contains 6,079 
sentences, and other 49 reviews, totally 1,531 
sentences, are used as the testing set. Both the 
three-class classification and the five-class 
classification use standard CRFs model with the 
same feature set. The results are shown in Table 1, 
2 and 3, where ?Answer? denotes the results given 
by human, ?Results? denotes the results given by 
?CRFs model ?Correct? denotes the number of 
correct samples which is labeled by CRFs model. 
We use precision, recall and F1 value as the 
evaluation metrics.  
Table 1 gives the result of sentimental strength 
rating. Table 2 shows the polarity classification 
results extracted from the results of sentimental 
strength rating in Table 1. The extraction process is 
as follows. In the sentimental strength rating 
results, we combine the sentences with ?PP? class 
and the sentences with ?P? class into ?Pos? class, 
and the sentences with ?NN? class and the 
sentences with ?N? class into ?Neg? class. So the 
results of five-class classification are transformed 
into the results of three-class classification. Table 3 
is the results of performing polarity classification 
in the data set by CRFs directly. 
 
Label Answer Results Correct Precision Recall F1 
PP 51 67 5 0.0746 0.0980 0.0847 
P 166 177 32 0.1808 0.1928 0.1866 
Neu 1190 1118 968 0.8658 0.81.34 0.8388 
N 105 140 25 0.1786 0.2381 0.2041 
NN 19 29 1 0.0345 0.0526 0.0417 
Total 1531 1531 1031 0.67.34 0.6734 0.6734 
Table 1. Result of Sentimental Strength Rating 
Label Answer Results Correct Precision Recall F1 
Pos 217 244 79 0.3238 0.3641 0.3427 
Neu 1190 1118 968 0.8658 0.8134 0.8388 
Neg 124 169 41 0.2426 0.3306 0.2799 
Total 1531 1531 1088 0.7106 0.7106 0.7106 
Table 2.  Result of Polarity Classification Extracted from Table 1. 
Label Answer Results Correct Precision Recall F1 
Pos 217 300 108 0.3600 0.4977 0.4178 
Neu 1190 1101 971 0.8819 0.8160 0.8477 
Neg 124 130 40 0.3077 0.3226 0.3150 
Total 1531 1531 1119 0.7309 0.7309 0.7309 
Table 3. Result of Polarity Classification 
119
From the results we can find the following 
phenomena.  
(1) The corpus is severely unbalanced, the 
objective sentences take the absolute majority in 
the corpus, which leads to the poor accuracy for 
classifying subjective sentences. The experiment in 
Table 1 puts polarity classification and sentimental 
strength rating under a unique CRFs model, 
without considering the redundancy and 
hierarchical structure between different classes. As 
a result, the features for polarity classification will 
usually cover the features for sentimental strength 
rating. These reasons can explain why there is only 
one sample labeled as ?NN? correctly and only 5 
samples labeled as ?PP? correctly. 
(2) Comparing Table 2 with 3, we can find that, 
the F1 value of the polarity classification results 
extracted from sentimental strength rating results is 
lower than that of directly conducting polarity 
classification. That is because the redundancy 
between sentimental strength labels makes the 
classifier confused to determine the polarity of the 
sentence. Therefore, we should deal with the 
sentiment analysis in a hierarchical frame which 
can consider the redundancy between the different 
classes and make full use of the subjective and 
polarity information implicitly contained in 
sentimental strength classes. 
4 Capturing Label Redundancy for CRFs 
via Adding Redundant Features 
As mentioned above, it?s important for a classifier 
to consider the redundancy between different 
labels. However, from the standard CRFs 
described in formula (1), we can see that the 
training of CRFs only maximizes the probabilities 
of the observed labels Y  in the training corpus. 
Actually, the redundant relation between sentiment 
labels is unobserved. The standard CRFs still treats 
each class as an isolated item so that its 
performance is not satisfied.  
In this section, we propose a novel method for 
sentiment classification, which can capture the 
redundant relation between sentiment labels 
through adding redundant features. In the 
following, we firstly show how to add these 
redundant features, then illustrate the 
characteristics of this method. After that, for the 
sentiment analysis task, the process of feature 
generation will be presented. 
4.1 Adding Redundant Features for CRFs 
Adding redundant features has two steps. Firstly, 
an implicit redundant label set is designed, which 
can form a multi-layer hierarchical structure 
together with the original labels. Secondly, in the 
hierarchical classification framework, the implicit 
labels, which reflect the relations between the 
original labels, can be used as redundant features 
in the training process. We will use the following 
example to illustrate the first step for sentimental 
strength rating task.  
For the task of sentimental strength rating, the 
original label set is {?PP (highly praised)?, ?P 
(something good)?, ?Neu (objective description)?, 
?N (something that needs improvement)? and ?NN 
(strong aversion)?}. In order to introduce 
redundant labels, the 5-class classification task is 
decomposed into the following three layers shown 
in Figure 1. The label set in the first layer is 
{?subjective?, ?objective?}, The label set in the 
second layer is for polarity classification 
{?positive?, ?objective?, ?negative?}, and the label 
set in the third layer is the original set.  Actually, 
the labels in the first and second layers are 
unobserved redundant labels, which will not be 
reflected in the final classification result obviously.  
 
Figure 1. The hierarchical structure of 
 sentimental labels 
In the second step, with these redundant labels, 
some implicit features can be generated for CRFs. 
So the standard CRFs can be rewritten as follows. 
The first layer 
The third layer 
The second layer 
Sentiment Analysis 
Subjective Objective 
Positive Negative 
P PP N NN 
Objective 
Objective 
120
11
exp( ( , ) )( | )
( )
exp( ( , ) )
exp( ( , ) )
T
m
j j j
j
m
j j j
T j
F X TP T X
Z X
F X Y
F X Y
?
?
?
=
=
?
=
?
=
?
?
? ?
        (2) 
where 1 2( ), ,... ...,j mT Y Y Y Y= , and jY denotes the 
label sequence in the jth layer. ( , )j jF X Y denotes 
the arbitrary feature function in the jth layer. 
From the formula (2), we can see that the 
original label set is rewritten as 
1 2( ), ,... ...,j mT Y Y Y Y= , which contains implicit 
labels in the hierarchical structure shown in Figure 
1. The difference between our method and the 
standard chain CRFs is that we make some implicit 
redundant features to be active when training. The 
original feature function ( , )F Y X is replaced by 
1
( , )
m
j j
j
F X Y
=
? . We use an example to illustrate the 
process of feature generation. When a sentence 
including the word ?good? is labeled as ?PP?, our 
model not only generate the state feature (good, 
?PP?), but also two implicit redundant state feature 
(good, ?positive?) and (good, ?subjective?). 
Through adding larger-granularity labels ?positive? 
and ?negative? into the model, our method can 
increase the probability of ?positive? and decrease 
the probability of ?negative?. Furthermore, ?P? and 
?PP? will share the probability gain of ?positive?, 
therefore the probability of ?P? will be larger than 
that of ?N?. For the transition feature, the same 
strategy is used. Therefore the complexity of its 
training procedure is ( )
m
j
j
O M N F l? ? ??  where M 
is the number of the training samples, N is the 
average sentence length, jF  is the average number 
of activated features in the jth layer, l  is the 
number of the original labels and m is the number 
of the layers. For the complexity of the decoding 
procedure, our method has ( )
m
j
j
O N F l? ?? . 
It?s worth noting that, (1) transition features are 
extracted in each layer separately rather than 
across different layers. For example, feature (good, 
?subjective?, ?positive?) will never be extracted 
because ?subjective? and ?positive? are from 
different layers; (2) if one sentence is labeled as 
?Neu?, no implicit redundant features will be 
generated.  
4.2 The Characteristics of Our Method 
Our method allows that the label sets are 
dependent and redundant. As a result, it can 
improve the performance of not only the classifier 
for the original sentimental strength rating task, but 
also the classifiers for other tasks in the 
hierarchical frame, i.e. polarity classification and 
subjective/objective classification. This kind of 
dependency and redundancy can lead to two 
characteristics of the proposed method for 
sentiment classification compared with traditional 
methods, such as the cascaded method. 
(1) Error-correction: Two dependent tasks in the 
neighboring layers can correct the errors of each 
other relying on the inconsistent redundant 
information. For example, if in the first layer, the 
features activated by ?objective? get larger scores 
than the features activated by ?subjective?, and in 
the second layer the features activated by 
?positive? get larger scores than the features 
activated by ?objective?, then inconsistency 
emerges. At this time, our method can globally 
select the label with maximum probability. This 
characteristic can make up the deficiency of the 
cascaded method which may induce error 
propagation. 
(2) Differentiating the ordinal relation among 
sentiment labels: Our method organizes the ordinal 
sentiment labels into a hierarchy through 
introducing redundant labels into standard chain 
CRFs, in this way the degree of classification 
errors can be controlled. In the different layers of 
sentiment analysis task, the granularities of 
classification are different. Therefore, when an 
observation cannot be correctly labeled on a 
smaller-granularity label set, our method will use 
the larger-granularity labels in the upper layer to 
control the final classification labels.  
4.3 Feature Selection in Different Layers 
For feature selection, our method selects different 
features for each layer in the hierarchical frame. 
In the top layer of the frame shown in Figure 1, 
for subjective/objective classification task, we use 
121
not only adjectives and the verbs which contain 
subjective information (e.g., ?believe?, ?think?) as 
the features, but also the topic words. The topic 
words are defined as the nouns or noun phases 
which frequently appear in the corpus. We believe 
that some topic words contain subjective 
information. 
In the middle and bottom layers, we not only use 
the features in the first layer, but also some special 
features as follows.  
(1) The prior orientation scores of the sentiment 
words: Firstly, a sentiment lexicon is generated by 
extending the synonymies and antonyms in 
WordNet2 from a positive and negative seed list. 
Then, the positive score and the negative score of a 
sentiment word are individually accumulated and 
weighted according to the polarity of its 
synonymies and antonyms. At last we scale the 
normalized distance of the two scores into 5 levels, 
which will be the prior orientation of the word. 
When there is a negative word, like {not, no, can?t, 
merely, never, ?}, occurring nearby the feature 
word in the range of 3 words size window, the 
orientation of this word will be reversed and ?NO? 
will be added in front of the original feature word 
for creating a new feature word.  
(2) Sentence transition features: We consider two 
types of sentence transition features. The first type 
is the conjunctions and the adverbs occurring in the 
beginning of this sentence. These conjunctions and 
adverbs are included in a word list which is 
manually selected, like {and, or, but, though, 
however, generally, contrarily, ?}. The second 
type of the sentence transition feature is the 
position of the sentence in one review. The reason 
lies in that: the reviewers often follow some 
writing patterns, for example some reviewers 
prefer to concede an opposite factor before 
expressing his/her real sentiment. Therefore, we 
divide a review into five parts, and assign each 
sentence with the serial number of the part which 
the sentence belongs to. 
5 Experiments 
5.1 Data and Baselines 
In order to evaluate the performance of our method, 
we conducted experiments on a sentence level 
                                                           
2 http://wordnet.princeton.edu/ 
annotation corpus obtained from Purdue University, 
which is also used in (Mao and Lebanon 07). This 
corpus contains 249 movie reviews and 7,610 
sentences totally, which is randomly selected from 
the Cornell sentence polarity dataset v1.0. Each 
sentence was hand-labeled with one of five classes: 
PP (highly praised), P (something good), Neu 
(objective description), N (something that needs 
improvement) and NN (strong aversion), which 
contained the orientation polarity of each sentence. 
Based on the 5-class manually labeled results 
mentioned above, we also assigned each sentence 
with one of three classes: Pos (positive polarity), 
Neu (objective description), Neg (negative 
polarity). Data statistics for the corpus are given in 
Table 4. 
Pos Neu Neg 
Label 
PP P Neu N NN 
Total 
5 classes 383 860 5508 694 165 7610 
3 classes 1243 5508 859 7610 
Table 4. Data Statistics for Movies Reviews 
Corpus 
There is a problem in the dataset that more than 
70% of the sentences are labeled as ?Neu? and 
labels are seriously unbalanced. As a result, the 
?Neu? label is over-emphasized. For this problem, 
Mao and Lebanon (2007) made a balanced data set 
(equal number sentences for different labels) which 
is sampled in the original corpus. Since randomly 
sampling sentences from the original corpus will 
break the intrinsic relationship between two 
adjacent sentences in the context, we don?t create 
balanced label data set. 
For the evaluation of our method, we choose 
accuracy as the evaluation metrics and some 
classical methods as the baselines. They are Na?ve 
Bayes (NB), Support Vector Machine (SVM), 
Maximum Entropy (MaxEnt) (Kamal Nigam et al 
1999) and standard chain CRFs (Fei et al 2003). 
We also regard cascaded-CRFs as our baseline for 
comparing our method with the cascaded-based 
method. For NB, we use Laplace smoothing 
method. For SVM, we use the LibSVM3  with a 
linear kernel function4. For MaxEnt, we use the 
implementation in the toolkit Mallet5. For CRFs, 
                                                           
3 http://www.csie.ntu.tw/~cjlin/libsvm 
4 http://svmlight.joachims.org/ 
5 http://mallet.cs.umass.edu/index.php/Main_Page 
122
Label NB SVM MaxEnt Standard CRF Cascaded CRF Our Method 
PP 0.1745 0.2219 0.2055 0.2027 0.2575 0.2167 
P 0.2049 0.2877 0.2353 0.2536 0.2881 0.3784 
Neu 0.8083 0.8685 0.8161 0.8273 0.8554 0.8269 
N 0.2636 0.3014 0.2558 0.2981 0.3092 0.4204 
NN 0.0976 0.1162 0.1148 0.1379 0.1510 0.2967 
Total 0.6442 0.6786 0.6652 0.6856 0.7153 0.7521 
Table 5. The accuracy of Sentimental Strength Rating 
Label NB SVM MaxEnt Standard CRF Cascaded-CRF Our Method 
Pos 0.4218 0.4743 0.4599 0.4405 0.5122 0.6008 
Neu 0.8147 0.8375 0.8424 0.8260 0.8545 0.8269 
Neg 0.3217 0.3632 0.2739 0.3991 0.4067 0.5481 
Total 0.7054 0.7322 0.7318 0.7327 0.7694 0.7855 
Table 6?The Results of Polarity Classification 
Label NB SVM MaxEnt Standard CRF Our Method 
Subjective 0.4743 0.5847 0.4872 0.5594 0.6764 
Objective 0.8170 0.8248 0.8212 0.8312 0.8269 
Total 0.7238 0.7536 0.7518 0.7561 0.8018 
Table 7. The accuracy of Subjective/Objective Classification 
 
we use the implementation in Flex-CRFs6. We set 
the iteration number to 120 in the training process 
of the method based on CRFs. In the cascaded 
model we set 3 layers for sentimental strength 
rating, where the first layer is subjective/objective 
classification, the second layer is polarity 
classification and the last layer is sentimental 
strength classification. The upper layer passes the 
results as the input to the next layer. 
5.2 Sentimental Strength Rating 
In the first experiment, we evaluate the 
performance of our method for sentimental 
strength rating. Experimental results for each 
method are given in Table 5. We not only give the 
overall accuracy of each method, but also the 
performance for each sentimental strength label. 
All baselines use the same feature space mentioned 
in section 4.3, which combine all the features in 
the three layers together, except cascaded CRFs 
and our method. In cascaded-CRFs and our method, 
we use different features in different layers 
mentioned in section 4.3. These results were 
gathered using 5-fold cross validation with one 
fold for testing and the other 4 folds for training.  
From the results, we can obtain the following 
conclusions. (1) The three versions of CRFs 
perform consistently better than Na?ve Bayes, 
                                                           
6 http://flexcrfs.sourceforge.net 
SVM and MaxEnt methods. We think that is 
because CRFs model considers the contextual 
influence of each sentence. (2) Comparing the 
performance of cascaded CRFs with that of 
standard sequence CRFs, we can see that not only 
the overall accuracy but also the accuracy for each 
sentimental strength label are improved, where the 
overall accuracy is increased by 3%. It proves that 
taking the hierarchical relationship between labels 
into account is very essential for sentiment 
classification. The reason is that: the cascaded 
model performs sentimental strength rating in three 
hierarchical layers, while standard chain CRFs 
model treats each label as an independent 
individual. So the performance of the cascaded 
model is superior to the standard chain CRFs. (3) 
The experimental results also show that our 
method performs better than the Cascaded CRFs. 
The classification accuracy is improved from 
71.53% to 75.21%. We think that is because our 
method adds the label redundancy among the 
sentimental strength labels into consideration 
through adding redundant features into the feature 
sets, and the three subtasks in the cascaded model 
are merged into a unified model. So the output 
result is a global optimal result. In this way, the 
problem of error propagation in the cascaded frame 
can be alleviated. 
123
5.3 Sentiment Polarity Classification 
In the second experiment, we evaluate the 
performance of our method for sentiment polarity 
classification. Our method is based on a 
hierarchical frame, which can perform different 
tasks in different layers at the same time.  For 
example, it can determine the polarity of sentences 
when sentimental strength rating is performed. 
Here, the polarity classification results of our 
method are extracted from the results of the 
sentimental strength rating mentioned above. In the 
sentimental strength rating results, we combine the 
sentences with PP label and the sentences with P 
label into one set, and the sentences with NN label 
and the sentences with N label into one set. So the 
results of 5-class classification are transformed into 
the results of 3-class classification. Other methods 
like NB, SVM, MaxEnt, standard chain CRFs 
perform 3-class classification directly, and their 
label sets in the training corpus is {Pos, Neu, Neg}. 
The parameter setting is the same as sentimental 
strength rating. For the cascaded-CRFs method, we 
firstly perform subjective/objective classification, 
and then determine the polarity of the sentences 
based on the subjective sentences. The 
experimental results are given in Table 6. 
From the experimental results, we can obtain the 
following conclusion for sentiment polarity 
classification, which is similar to the conclusion 
for sentimental strength rating mentioned in 
section 5.2. That is both our model and the 
cascaded model can get better performance than 
other traditional methods, such as NB, SVM, 
MaxEnt, etc. But the performance of the cascaded 
CRFs (76.94%) is lower than that of our method 
(78.55%). This indicates that because our method 
exploits the label redundancy in the different layers, 
it can increase the accuracies of both polarity 
classification and sentimental strength rating at the 
same time compared with other methods. 
5.4 Subjective/Objective Classification 
In the last experiment, we test our method for 
subjective/objective classification. The 
subjective/objective label of the data is extracted 
from its original label like section 5.3. As the same 
as the experiment for polarity classification, all 
baselines perform subjective/objective 
classification directly. It?s no need to perform the 
cascaded-based method because it?s a 2-class task. 
The results of our method are extracted from the 
results of the sentimental strength rating too. The 
results are shown in Table 7. From it, we can 
obtain the similar conclusion, i.e. our method 
outperforms other methods and has the 80.18% 
classification accuracy. Our method, which 
introduces redundant features into training, can 
increase the accuracies of all tasks in the different 
layers at the same time compared with other 
baselines. It proves that considering label 
redundancy are effective for promoting the 
performance of a sentimental classifier. 
6 Related Works 
Recently, many researchers have devoted into the 
problem of the sentiment classification. Most of 
researchers focus on how to extract useful textual 
features (lexical, syntactic, punctuation, etc.) for 
determining the semantic orientation of the 
sentences using machine learning algorithm (Bo et 
al. 2002; Kim and Hovy, 2004; Bo et al 2005, Hu 
et al 2004; Alina et al2008; Alistair et al2006). 
But fewer researchers deal with this problem using 
CRFs model.  
For identifying the subjective sentences, there 
are several research, like (Wiebe et al 2005). For 
polarity classification on sentence level, (Kim and 
Hovy, 2004) judged the sentiment by classifying a 
pseudo document composed of synonyms of 
indicators in one sentence. (Pang and Lee, 04) 
proposed a semi-supervised machine learning 
method based on subjectivity detection and 
minimum-cut in graph.  
Cascaded models for sentiment classification 
were studied by (Pang and Lee, 2005). Their work 
mainly used the cascaded frame for determining 
the orientation of a document and the sentences. In 
that work, an initial model is used to determine the 
orientation of each sentence firstly, then the top 
subjective sentences are input into a document -
level model to determine the document?s 
orientation.  
The CRFs has previously been used for 
sentiment classification. Those methods based on 
CRFs are related to our work. (Mao et al 2007) 
used a sequential CRFs regression model to 
measure the polarity of a sentence in order to 
determine the sentiment flow of the authors in 
reviews. However, this method must manually 
124
select a word set for constraints, where each 
selected word achieved the highest correlation with 
the sentiment. The performance of isotonic CRFs 
is strongly related to the selected word set. 
(McDonald et al2007; Ivan et al2008) proposed a 
structured model based on CRFs for jointly 
classifying the sentiment of text at varying levels 
of granularity. They put the sentence level and 
document level sentiment analysis in an integrated 
model and employ the orientation of the document 
to influence the decision of sentence?s orientation. 
Both the above two methods didn?t consider the 
redundant and hierarchical relation between 
sentimental strength labels. So their methods 
cannot get better results for the problem mentioned 
in this paper. 
Another solution to this problem is to use a joint 
multi-layer model, such as dynamic CRFs, multi-
layer CRFs, etc. Such kind of models can treat the 
three sub-tasks in sentiment classification as a 
multi-task problem and can use a multi-layer or 
hierarchical undirected graphic to model the 
sentiment of sentences. The main difference 
between our method and theirs is that we consider 
the problem from the feature representation view. 
Our method expands the feature set according to 
the number of layers in the hierarchical frame. So 
the complexity of its decoding procedure is lower 
than theirs, for example the complexity of the 
multi-layer CRFs is ( )j
j
lO N F? ?? when 
decoding and our method only has ( )j
j
FO N l? ?? , 
where N is the average sentence length, jF  is the 
average number of activated features in the jth layer, 
l  is the number of the original labels. 
7 Conclusion and Future Work 
In the paper, we propose a novel method for 
sentiment classification based on CRFs in response 
to the two special characteristics of ?contextual 
dependency? and ?label redundancy? in sentence 
sentiment classification.  We try to capture the 
contextual constraints on the sentence sentiment 
using CRFs. For capturing the label redundancy 
among sentiment classes, we generate a 
hierarchical framework through introducing 
redundant labels, under which redundant features 
can be introduced. The experimental results prove 
that our method outperforms the traditional 
methods (like NB, SVM, ME and standard chain 
CRFs). In comparison with cascaded CRFs, our 
method can effectively alleviate error propagation 
among different layers and obtain better 
performance in each layer.  
For our future work, we will explore other 
hierarchical models for sentimental strength rating 
because the experiments presented in this paper 
prove this hierarchical frame is effective for 
ordinal regression. We would expand the idea in 
this paper  into other models, such as Semi-CRFs 
and Hierarchical-CRFs. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under Grants no. 
60673042, the Natural Science Foundation of 
Beijing under Grants no. 4073043 and the National 
High Technology Development 863 Program of 
China under Grants no. 2006AA01Z144. 
References 
Alina A. and Sabine B. 2008. When Specialists and 
Generalists Work Together: Overcoming Domain 
Dependence in Sentiment Tagging. In Proc. of ACL-
08 
Alistair Kennedy and Diana Inkpen. 2006. Sentiment 
Classification of Movie Reviews Using Contextual 
Valence Shifters. Computational Intelligence, 22(2), 
pages 110-125 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
machine learning techniques. In Proceedings of 
EMNLP 2002, pp.79-86. 
Bo Pang and Lillian Lee. 2004. A sentimental education: 
Sentiment analysis using subjectivity summarization 
based on minimum cuts. In Proceedings of ACL 2004, 
pp.271-278. 
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting 
class relationships for sentiment categorization with 
respect to rating scales. In Proceedings of ACL 2005, 
pp.115-124.  
Ivan Titov and Ryan McDonald. 2008. A Joint Model o 
f Text and Aspect Ratings of Sentiment 
Summarization. In Proceedings of ACL-08, pages 
308-316 
125
Janyce Webie, Theresa Wilson and Claire Cardie. 2005. 
Annotating expressions of opinions and emotions in 
lauguage. Language Resources and Evaluation 2005 
Fei Sha and Fernando Pereira, 2003 Shallow Parsing 
with Conditional Random Fields, In Proceedings 
ofHLT-NAACL 2003, Edmonton, Canada, pp. 213-
220. 
Kim, S and Edward H. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of COLING-
04. 
Kamal Nigam, John Lafferty and Andrew McCallum. 
1999. Using Maximum Entropy for Text 
Classification. In Proceedings of IJCAI Workshop on 
Machine Learning for Information Filtering, pages 
61-67. 
J Lafferty, A McCallum, F Pereira. 2001. Conditional 
random fields: Probabilistic models for segmenting 
and labeling sequence data. In Proceedings of  
ICML-01,  pages 282.289.  
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie review 
mining and summarization. In Proceedings of the 
15th ACM international conference on Information 
and knowledge management (CIKM), pages 43-50. 
M. Hu and B. Liu. 2004a. Mining and summarizing 
customer reviews. In Proceedings of the 2004 ACM 
SIGKDD international conference on Knowledge 
discovery and data mining, pages 168-177. 
Ryan McDonald, Kerry Hannan and Tyler Neylon et al 
Structured Models for Fine-to-Coarse Sentiment 
Analysis. In Proceedings of ACL 2007, pp. 432-439. 
Wei Wu, Zoubin Ghahraman, 2005. Gaussian 
Processes for Oridinal Regression. The Journal of 
Machine learning Research, 2005 
Y. Mao and G. Lebanon, 2007. Isotonic Conditional 
Random Fields and Local Sentiment Flow. Advances 
in Neural Information Processing Systems 19, 2007 
126
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 427?434, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Chinese Named Entity Recognition Based on Multiple Features 
 
 
Youzheng Wu, Jun Zhao, Bo Xu Hao Yu 
National Laboratory of Pattern Recognition Fujitsu R&D Center Co., Ltd 
Institute of Automation, CAS Beijing 100016, China 
Beijing, 100080, China yu@frdc.fujitsu.com 
(yzwu,jzhao,bxu)@nlpr.ia.ac.cn  
 
 
 
 
Abstract 
This paper proposes a hybrid Chinese 
named entity recognition model based on 
multiple features. It differentiates from 
most of the previous approaches mainly 
as follows. Firstly, the proposed Hybrid 
Model integrates coarse particle feature 
(POS Model) with fine particle feature 
(Word Model), so that it can overcome 
the disadvantages of each other. Secondly, 
in order to reduce the searching space and 
improve the efficiency, we introduce heu-
ristic human knowledge into statistical 
model, which could increase the perform-
ance of NER significantly. Thirdly, we 
use three sub-models to respectively de-
scribe three kinds of transliterated person 
name, that is, Japanese, Russian and 
Euramerican person name, which can im-
prove the performance of PN recognition. 
From the experimental results on People's 
Daily testing data, we can conclude that 
our Hybrid Model is better than the mod-
els which only use one kind of features. 
And the experiments on MET-2 testing 
data also confirm the above conclusion, 
which show that our algorithm has consis-
tence on different testing data. 
1 Introduction 
Named Entity Recognition (NER) is one of the key 
techniques in the fields of Information Extraction, 
Question Answering, Parsing, Metadata Tagging in 
Semantic Web, etc. In MET-2 held in conjunction 
with the Seventh Message Understanding Confer-
ence (MUC-7), the task of NER is defined as rec-
ognizing seven sub-categories entities: person (PN), 
location (LN), organization (ON), time, date, cur-
rency and percentage. As for Chinese NEs, we fur-
ther divide PN into five sub-classes, that is, 
Chinese PN (CPN), Japanese PN (JPN), Russian 
PN (RPN), Euramerican PN (EPN) and abbrevi-
ated PN (APN) like "???/Mr. Wu". Similarly, 
LN is split into common LN (LN) like "???
/Zhongguancun" and abbreviated LN (ALN) such 
as "?/Beijing", "?/Shanghai". The recognition of 
time (TM) and numbers (NM) is comparatively 
simpler and can be implemented via finite state 
automata. Therefore, our research focuses on the 
recognition of CPN, JPN, RPN, EPN, APN, LN, 
ALN and ON. 
Compared to English NER, Chinese NER is 
more difficult. We think that the main differences 
between Chinese NER and English NER lie in: (1) 
Unlike English, Chinese lacks the capitalization 
information which can play very important roles in 
identifying named entities. (2) There is no space 
between words in Chinese, so we have to segment 
the text before NER. Consequently, the errors in 
word segmentation will affect the result of NER. 
In this paper, we proposes a hybrid Chinese 
NER model based on multiple features which em-
phasizes on (1) combining fine particle features 
(Word Model) with coarse particle features (POS 
Model); (2) integrating human knowledge into sta-
tistical model; (3) and using diverse sub-models 
for different kinds of entities. Especially, we divide 
transliterated person name into three sub-classes 
according to their characters set, that is, JPN, RPN 
and EPN. In order to deduce the complexity of the 
model and the searching space, we divide the rec-
427
ognition process into two steps: (1) word segmen-
tation and POS tagging; (2) named entity recogni-
tion based on the first step. 
Trained on the NEs labeled corpus of five-
month People's Daily corpus and tested on one-
month People's Daily corpus, the Hybrid Model 
achieves the following performance. The precision 
and the recall of PN (including CPN, JPN, RPN, 
EPN, AP N), LN (including ALN) and ON are re-
spectively (94.06%, 95.21%), (93.98%, 93.48%), 
and (84.69%, 86.86%). From the experimental re-
sults on People's Daily testing data, we can con-
clude that our Hybrid Model is better than other 
models which only use one kind of features. And 
the experiments on MET-2 testing data also con-
firm the above conclusion, which show that our 
algorithm has consistence on different testing data. 
2 Related Work 
On the impelling of international evaluations like 
MUC, CoNLL, IEER and ACE, the researches on 
English NER have achieved impressive results. For 
example, the best English NER system[Chinchor. 
1998] in MUC7 achieved 95% precision and 92% 
recall. However, Chinese NER is far from mature. 
For example, the performance (precision, recall) of 
the best Chinese NER system in MET-2 is (66%, 
92%), (89%, 91%), (89%, 88%) for PN, LN and 
ON respectively.  
Recently, approaches for NER are a shift away 
from handcrafted rules[Grishman, et al 1995] 
[Krupka, et al 1998][Black et al 1998] towards 
machine learning algorithms, i.e. unsupervised 
model like DL-CoTrain, CoBoost[Collins, 1999, 
2002], supervised learning like Error-driven [Ab-
erdeen, et al 1995], Decision Tree [Sekine, et al 
1998], HMM[Bikel, et al 1997] and Maximum 
Entropy[Borthwick, et al 1999][Mikheev, et 
al.1998].  
Similarly, the models for Chinese NER can also 
be divided into two categories: Individual Model 
and Integrated Model.  
Individual Model[Chen, et al 1998][Sun, et al 
1994][Zheng, et al 2000] consists of several sub-
models, each of them deals with a kind of entities. 
For example, the recognition of PN may be statis-
tical-based model, while LN and ON may be rule-
based model like [Chen, et al 1998]. Integrated 
Model[Sun, et al 2002] [Zhang, et al 2003][Yu, et 
al. 1998][Chua, et al 2002] deals with all kinds of 
entities in a unified statistical framework. Most of 
these integrated models can be viewed as a HMM 
model. The differences among them are the defini-
tion of state and the features used in entity model 
and context model.  
In fact, a NER model recognizes named entities 
through mining the intrinsic features in the entities 
and the contextual features around the entities. 
Most of existing approaches employ either coarse 
particle features, like POS and ROLE[Zhang, et al 
2003], or fine particle features like word. The data 
sparseness problem is serious if only using fine 
particle features, and coarse particle features will 
lose much important information though without 
serious data sparseness problem. Our idea is that 
coarse particle features should be integrated into 
fine particle features to overcome the disadvan-
tages of them. However, most systems do not com-
bine them and especially ignore the impact of POS. 
Inspired by the algorithms of identifying 
BaseNP and Chunk[Xun, et al 2000], we propose 
a hybrid NER model which emphasizes on com-
bining coarse particle features (POS Model) with 
fine particle features (Word Model). Though the 
Hybrid Model can overcome the disadvantages of 
the Word Model and the POS Model, there are still 
some problems in such a framework. Data sparse-
ness still exists and very large searching space in 
decoding will influence efficiency. Our idea is that 
heuristic human knowledge can not only improve 
the time efficiency, but also solve the data sparse-
ness problem to some extent by restricting the gen-
eration of entity candidates. So we intend to 
incorporate human knowledge into the statistical 
model to improve efficiency and effectivity of the 
Hybrid Model.  
Similarly, for capturing intrinsic features in dif-
ferent types of entities, we design several sub-
models for each kind of entities. For example, we 
divide transliterated person name into three sub-
classes according to their characters sets, that is, 
JPN, RPN and EPN. 
3 Chinese NER with Multiple Features 
Chinese NEs have very distinct word features in 
their composition and contextual information. For 
example, about 365 highest frequently used sur-
names cover 99% Chinese surnames[Sun, et al 
1994]. Similarly the characters used for transliter-
ated names are also limited. LNs and ONs often 
428
end with the specific words like "?/province" and 
"??/company". However, data sparseness is very 
serious when using word features. So we try to 
introduce coarse particle feature to overcome the 
data sparseness problem. POS features are simplest 
and easy to obtain. Therefore, our hybrid model 
combines word feature with POS feature to recog-
nize Chinese NEs. 
Given a word/pos sequence as equation (1): 
nnii twtwtwTW //// 11 LL=                    (1) 
where n is the number of words and ti is the POS 
of word wi. The task of Chinese NE identification 
is to find the optimal sequence WC*/ TC* by split-
ting, combining and classifying the sequence of (1). 
mmii21 tc/wctc/wctc/wc*TC/*WC LL=     (2) 
where [ ]ljji wwwc += L , [ ]ljji tttc += L , nm ? . 
Note that the definition of words in {wi} set is 
that each kind of NEs (including PN, APN, LN, 
ALN, ON, TM, NM) is defined as a word and all 
the other words in the vocabulary are also defined 
as individual words. Consequently, {wi} set has 
|V|+7 words, where |V| is the size of vocabulary. 
The size of {ti} set is 48 which include PKU POS 
tagging set1 and each kind of NEs. 
Obviously, we could obtain the optimal se-
quence WC*/TC* through the following three 
models: the Word Model, the POS Model and the 
Hybrid Model.  
The Word Model employs word features for 
NER, which is introduced by [Sun, et al 2002]. 
The POS Model employs POS features for NER. 
This paper proposes a Hybrid Model which com-
bines word features with POS features.  
We will describe these models in detail in fol-
lowing section. 
3.1 The Hybrid Model 
For the convenience of description, we take apart 
equation (1) into two components: word sequence 
as equation (3) and POS sequence as (4).  
ni21 wwwwW LL=                                     (3) 
ni21 ttttT LL=                                          (4) 
The Word Model estimates the probability of 
generating a NE from the viewpoint of word se-
quence, which can be expressed in equation (5).  
                                                          
1 http://icl.pku.edu.cn/nlp-tools/catetkset.html 
( ) ( )WC|WPWCPargmax*WC wc=                  (5) 
The POS Model estimates the probability of 
generating a NE from the viewpoint of POS se-
quence, which can be expressed in equation (6). 
( ) ( )TC|TPTCPargmax*TC TC=                      (6) 
Our proposed Hybrid Model combines the Word 
Model with the POS Model, which can be ex-
pressed in the equation (7). 
( )
( ) ( )
( ) ( ) ( )
( ) ( )
( ) ( ) ( ) ( ) ( ) ?]TCPTC|T[PWCPWC|WPargmax
W,TWC,TC,Pargmax
T,WPW,TWC,TC,Pargmax
W,T|WC,TCPargmax
*TC*,WC
TCWC,
TCWC,
TCWC,
TCWC,
?
=
=
=
  (7) 
where factor ? > 0 is to balance the Word Model 
and the POS Model. 
Therefore, the Hybrid Model consists of four 
sub-models: word context model P(WC), POS con-
text model P(TC), word entity model P(W|WC) 
and POS entity model P(T|TC). 
3.2 Context Model 
The word context model and the POS context 
model estimate the probability of generating a 
word or a POS given previous context. P(WC) and 
P(TC) can be estimated according to (8) and (9) 
respectively.  
( ) ( )?
=
=
m
1i
1i2ii wcwc|wcPWCP                       (8) 
( ) ( )?
=
=
m
1i
1i2ii tctc|tcPTCP                             (9) 
3.3 Word Entity Model  
Different types of NEs have different structures 
and intrinsic characteristics. Therefore, a single 
model can't capture all types of entities. Typical, 
character-based model is more appropriate for PNs, 
whereas, word-based model is more competent for 
LNs and ONs. Especially, we divided transliterated 
PN into three categories such as JPN, RPN and 
EPN.  
For the sake of estimating the probability of 
generating a NE, we define 19 sub-classes shown 
as Table 1 according to their position in NEs. 
 
429
Tag Description 
Sur Surname of CPN 
Dgb First character of Given Name of CPN 
Dge Last character of Give Name of CPN 
Bfn First character of EPN 
Mfn Middle character of EPN 
Efn Last character of EPN 
RBfn First character of RPN 
RMfn Middle character of RPN 
REfn Last character of RPN 
JBfn surname of JPN 
JMfn Middle character of JPN 
JEfn Last character of JPN 
Bol First word of LN 
Mol Middle word of LN 
Eol Last word of LN 
Aloc Single character LN 
Boo First word of ON 
Moo Middle word of ON 
Eoo Last word of ON 
Table 1 Sub-classes in Entity Model 
3.3.1 Word Entity Model for PN 
For the class of PN (including CPN, APN, JPN, 
RPN and EPN), the word entity model is a charac-
ter-based trigram model which can be expressed in 
equation (10). ( )
( ) ( )( )
( )( )1kiik
1liil1i
ik1i
ik1i
wcwc
1k
2l
wcwcwc
2k
wcwc
iwcwc
w,ENe|wP
w,MNe|wPBNe|wP
ENeMNeMNeBNe|wwP
wc|wwP
?
?
?
??
???
?
???
?=
??
=
? 448476
LL
L
       (10) 
where, BNe, MNe and ENe denotes the first, mid-
dle and last characters respectively. 
The word entity models for PN are estimated 
with Chinese, Japanese, Russian and Euramerican 
names lists which contain 15.6 million, 0.15 mil-
lion, 0.44 million, 0.4 million entities respectively. 
3.3.2 Word Entity Model for LN and ON 
For the class of LN and ON, the word entity model 
is a word-based trigram model. The model can be 
expressed by (11). 
( )
( ) ( )
( )( ) ( )
( )( ) ( )ikwcwcwcwc
wcwcwc
1k
2l
1liil
wcendwcstartwcwc
2k
wcwcwc
iendwcstartwc
wc|wwPwc,ENe|wcP
wc|wwPwc,MNe|wcP
wc|w..wPBNe|wcP
ENeMNeMNeBNe|wcwcwcP
wc|wwP
denikstartik1kiik
ilendilstartil
1i1i1i1i
ikil1i
ii
?
L
L
448476
LLL
L
?
?
=
???
?
???
?=
=
 (11) 
The word entity models and the POS entity 
model for LN and ON are estimated with LN and 
ON names lists which respectively contain 0.44 
mil-lion and 3.2 million entities. 
3.3.3 Word Entity Model for ALN 
For the class of ALN, we use word-based bi-gram 
model. The entity model for ALN can be expressed 
by equation (12). 
( ) ( )
)LocA(C
ocAL,wC
ocAL|wP ii =                           (12) 
where wi is the ALN which includes single and 
multiple characters ALN. 
3.4 POS Entity Model 
But for the class of PN, it's very difficult to obtain 
the corpus to train POS Entity Model. For the sake 
of simplification, we use word entity model shown 
in equation (10) to replace the POS entity model. 
For the class of LN and ON, POS entity model 
can be expressed by equation (13). ( )
( ) ( )
( )( ) ( )
( )( ) ( )iktctctctc
tctctc
1k
2l
1liil
tcendwcstarttctc
2k
tctctc
iendtcstarttc
tc|ttPtc,ENe|tcP
tc|ttPtc,MNe|tcP
tc|t..tPBNe|tcP
ENeMNeMNeBNe|tctctcP
tc|ttP
denikstartik1kiik
ilendilstartil
1i1i1i1i
ikil1i
ii
?
L
L
448476
LLL
L
?
?
=
???
?
???
?=
=
    (13) 
While for the class of ALN, POS entity model is 
shown as equation (14). 
( ) ( )
)ocAL(C
ocAL,tiCocAL|tP i =                               (14) 
4 Heuristic Human Knowledge 
In this section, we will introduce heuristic human 
knowledge that is used for Chinese NER and the 
430
method of how to incorporate them into statistical 
model which are shown as follows. 
1. CPN surname list (including 476 items) and 
JPN surnames list (including 9189 items): Only 
those characters in the surname list can trigger per-
son name recognition. 
2. RPN and EPN characters lists: Only those 
consecutive characters in the transliterated charac-
ter list form a candidate transliterated name. 
3. Entity Length Restriction: Person name can-
not span any punctuation and the length of CN 
cannot exceed 8 characters while the length of TN 
is unrestrained. 
4. Location keyword list (including 607 items):  
If the word belongs to the list, 2~6 words before 
the salient word are accepted as candidate LNs. 
5. General word list (such as verbs and preposi-
tions): Words in the list usually is followed by a 
location name, such as "?/at", "?/go". If the cur-
rent word is in the list, 2~6 words following it are 
accepted as candidate LNs. 
6. ALN name list (including 407 items): If the 
current word belongs to the list, we accept it as a 
candidate ALN. 
7. Organization keyword list (including 3129 
items): If the current word is in organization key-
word list, 2~6 words before keywords are accepted 
as the candidate ONs. 
8. An organization name template list: We 
mainly use organization name templates to recog-
nize the missed nested ONs in the statistical model. 
Some of these templates are as follows: 
ON-->LN D* OrgKeyWord 
ON-->PN D* OrgKeyWord 
ON-->ON OrgKeyWord 
D and OrgKeyWord denote words in the middle 
of ONs and ONs keywords. D* means repeating 
zero or more times. 
5 Back-off Model to Smooth 
Data sparseness problem still exists. As some pa-
rameters were never observed in training corpus, 
the model will back off to a less powerful model. 
The escape probability[Black, et al 1998] was ad-
opted to smooth the statistical model shown as (15). 
00N11N2N1N
1N1NN1N1N
^
p)W(p)WWW(p
)WWW(p)WWW(p
???
?
+++
+=
LL
LL  (15) 
where NN e1? = , Ni0,e)e1(?
N
1ik
kii <<=
+=
? , and ei 
is the escape probability which can be estimated by 
equation (16). 
)WWW(f
)WWW(q
e
1N21
1N21
N L
L=                           (16) 
q(w1w2?wN-1) in (16) denotes the number of dif-
ferent symbol wN that have directly followed the 
word sequence w1w2?wN-1. 
6 Experiments 
In this chapter, we will conduct experiments to 
answer the following questions.  
Will the Hybrid Model be more effective than 
the Word Model and the POS Model? To answer 
this question, we will compare the performances of 
models with different parameter ? and find the best 
value of ? in equation (7). 
Will the conclusion from different testing sets be 
consistent? To answer this question, we evaluate 
models on the MET-2 test data and compare the 
performances of the Word Model, the POS Model 
and the Hybrid Model. 
Will the performance be improved significantly 
after combining human knowledge? To answer this 
question, we compare two models with and with-
out human knowledge.  
In our evaluation, only NEs with correct 
boundaries and correct categories are considered as 
the correct recognition. We conduct evaluations in 
terms of precision, recall and F-Measure. Note that 
PNs in experiments includes all kinds of PNs and 
LNs include ALNs. 
6.1 Will the Hybrid Model be More Effective 
Than the Word Model and POS Model? 
The parameter ? in equation (7) denotes the balanc-
ing factor of the Word Model and the POS Model. 
The larger ?, the larger contribution of the POS 
Model. The smaller ?, the larger contribution of the 
Word Model. So the task of this experiment is to 
find the best value of ?. In this experiment, the 
training corpus is from five-month's People's Daily 
tagged with NER tags and the testing set is from 
one-month's People's Daily. 
With the change of ?, the performances of rec-
ognizing PNs are shown in Fig.1.  
Note that the left, middle and right point in ab-
scissa respectively denote the performance of the 
431
Word Model, the Hybrid Model and the POS 
Model. 
0 1.6 3.2 4.8 6.4 8 9.6
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.96
Lamda
%
Precision
Recall
F?Measure
 
Fig.1 Performance of Recognizing LNs Impacted 
by ? 
From Fig.1, we can find that the performances 
of recognizing PNs are improved with the increas-
ing of ? in the beginning stage but decline in the 
ending. This experiment shows that the Word 
Model and the POS Model can overcome their dis-
advantages, and it is a feasible approach to inte-
grate the Word Model and the POS Model in order 
to improve the performance PNs recognition.  
With the change of ?, the performances of rec-
ognizing LNs are shown in Fig.2. 
0 1.6 3.2 4.8 6.4 8 9.6
0.9
0.91
0.92
0.93
0.94
Lamda
%
Precision
Recall
F?Measure
 
Fig.2 Performance of Recognizing LNs Impacted 
by ? 
As the Fig.2 shows, the precision and recall of 
LNs are improved with the increasing of ? and de-
creased in the later stage. This phenomenon also 
proves that the Hybrid Model is better for recog-
nizing LN than either the Word Model or the POS 
Model. 
Similarly, with the change of ?, the perform-
ances of recognizing ONs are shown in Fig.3. 
 
0 1.6 3.2 4.8 6.4 8 9.6
0.7
0.75
0.8
0.85
Lamda
%
Precision
Recall
F?Measure
 
Fig.3 Performance of Recognizing LNs Impacted 
by ? 
Comparing Fig.3 with Fig.1 and Fig.2, we find 
that the POS Model has different impact on recog-
nizing ONs from that on recognizing PNs and LNs. 
Especially, the POS Model has obvious side-effect 
on the recall. We speculate that the reasons may be 
that the probability of generating POS sequence by 
POS entity model is lower than that by POS con-
text model. 
According to Fig.1~Fig.3, we choose the best 
value ? = 2.8. And the performances of different 
models are shown in Table 2 in detail. 
 P(%) R(%) F(%) 
PN 94.06 95.21 94.63 
LN 93.98 93.48 93.73 
Hybrid 
Model 
(?= 2.8) 
ON 84.69 86.86 85.76 
 
PN 88.24 90.11 89.16 
LN 91.50 93.17 92.32 
Word 
Model 
ON 78.85 88.77 83.52 
    
PN 93.44 95.11 94.27 
LN 89.97 92.20 91.07 
POS 
Model 
ON 80.90 69.29  74.65 
Table 2 Performance of the Hybrid Model, the 
Word Model and the POS Model 
From Table 2, we find that the F-Measures of 
the Hybrid Model for PN, LN, ON are improved 
by 5.4%, 1.4%, 2.2% respectively in comparison 
with the Word Model, and these F-Measures are 
improved by 0.4%, 2.7%, 11.1% respectively in 
comparison with the POS Model. 
432
Conclusion 1: The experimental results validate 
our idea that the Hybrid Model can improve the 
performance of both the Word Model and the POS 
Model. However, the improvements for PN, LN 
and ON are different. That is, the POS Model has 
obvious side-effect on the recall of ON recognition 
at all times, while the recalls for PN and ON rec-
ognition are improved in the beginning but de-
creased in the ending with the increasing of ?. 
6.2 Will the Conclusion from Different Test-
ing Sets be Consistent? 
We also conduct experiments on the MET-2 test-
ing corpus to validate our conclusion from Exp.1, 
that is, the Hybrid Model could achieve better per-
formance than either the Word Model or the POS 
Model alone. The experimental results (F-Measure) 
on MET-2 are shown in Table 3. 
Model Word Model 
Hybrid 
Model 
POS 
Model 
PN 75.21% 80.77% 76.61% 
LN 89.78% 90.95% 89.81% 
ON 76.30% 80.21% 76.83% 
Table 3 F-Measure on MET-2 test corpus  
Comparing Table 3 with Table 2, we find that 
the performances of models on MET-2 are not as 
good as that on People Daily's testing data. The 
main reason lies in that the NE definitions in Peo-
ple Daily's corpus are different from that in MET-2. 
However, Table 3 can still validate our conclude 1, 
that is, the Hybrid Model is better than both the 
Word Model and the POS Model. For example, the 
F-Measures of the Hybrid Model for PN, LN and 
ON are improved by 5.6%, 1.2% and 3.9% respec-
tively in comparison with the Word Model, and 
these F-Measures are improved by 4.2%, 3.1% and 
3.4% respectively in comparison with the POS 
Model. 
Conclusion 2: Though the performances of the 
Hybrid Model on MET-2 are not as good as that 
on People's Daily corpus, the experimental results 
also support conclusion 1, i.e. the Hybrid Model 
which combining the Word Model with the POS 
Model can achieve better performance than either 
the Word Model or the POS Model. 
6.3 Will the Performance be Improved Sig-
nificantly after Incorporating Human 
Knowledge?  
One of our ideas in this paper is that human 
knowledge can not only reduce the search space, 
but also improve the performance through avoiding 
generating the noise NEs. This experiment will be 
conducted to validate this idea. Table 4 shows the 
performances of models with and without human 
knowledge.  
 P(%) R(%) F(%) 
PN 91.81 70.65 79.85 
LN 79.47 88.83 83.89 Model I 
ON 64.95 80.63 71.95 
 
PN 94.06 95.21 94.63 
LN 93.98 93.48 93.73 Model II 
ON 84.69 86.86 85.76 
Table 4 Performances Impacted by Human Know-
ledge 
From Table 4, we find that F-Measure of model 
with human knowledge (Model II) is improved by 
14.8%?9.8%?13.8% for PN, LN and ON respec-
tively compared with that of the model without 
human knowledge (Model I). 
Conclusion 3: From this experiment, we learn 
that human knowledge can not only reduce the 
search space, but also significantly improve the 
performance of pure statistical model. 
7 Conclusion 
In this paper, we propose a hybrid Chinese NER 
model which combines multiple features. The main 
contributions are as follows: ? The proposed Hy-
brid Model emphasizes on integrating coarse parti-
cle feature (POS Model) with fine particle feature 
(Word Model), so that it can overcome the disad-
vantages of each other; ? In order to reduce the 
search space and improve the efficiency of model, 
we incorporate heuristic human knowledge into 
statistical model, which could increase the per-
formance of NER significantly; ? For capturing 
intrinsic features in different types of entities, we 
design several sub-models for different entities. 
Especially, we divide transliterated person name 
into three sub-classes according to their characters 
set, that is, CPN JPN, RPN and EPN. 
There is a lack of effective recognition strategy 
for abbreviated ONs such as ????(Kunming 
Machine Tool Co.,Ltd), ? ? ? ? (Phoenix 
Photonics Ltd) in this paper. And most of mis-
433
recognized ONs in current system belong to them. 
So in the future work, we will be focusing more on 
recognizing abbreviated ONs. 
8 Acknowledgements 
This research is carried out as part of the coopera-
tive project with Fujitsu R&D Center Co., Ltd. We 
would like to thank Yingju Xia, Fumihito Nisino 
for helpful feedback in the process of developing 
and implementing. This work was supported by the 
Natural Sciences Foundation of China under grant 
No. 60372016 and 60272041, the Natural Science 
Foundation of Beijing under grant No. 4052027. 
References  
N.A. Chinchor: Overview of MUC-7/MET-2. In: Pro-
ceedings of the Seventh Message Understanding 
Conference (MUC-7), April. (1998). 
Youzheng Wu, Jun Zhao, Bo Xu: Chinese Named En-
tity Recognition Combining Statistical Model with 
Human Knowledge. In: The Workshop attached with 
41st ACL for Multilingual and Mix-language Named 
Entity Recognition, Sappora, Japan. (2003) 65-72. 
Endong Xun, Changning Huang, Ming Zhou: A Unified 
Statistical Model for the Identification of English 
BaseNP. In: Proceedings of ACL-2000, Hong Kong. 
(2000). 
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou, 
Changning Huang: Chinese Named Entity Identifica-
tion Using Class-based Language Model. In: 
COLING 2002. Taipei, August 24-25. (2002). 
Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng, 
Shuo Bai: Chinese Named Entity Recognition Using 
Role Model. In: the International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, vol.8, No.2. (2003) 29-60. 
D.M. Bikel, Scott Miller, Richard Schwartz, Ralph 
Weischedel: Nymble: a High-Performance Learning 
Name-finder. In: Fifth Conference on Applied Natu-
ral Language Processing, (published by ACL). (1997) 
194-201. 
Borthwick .A: A Maximum Entropy Approach to 
Named Entity Recognition. PhD Dissertation. (1999). 
Mikheev A., Grover C. and Moens M: Description of 
the LTG System Used for MUC-7. In: Proceedings of 
7th Message Understanding Conference (MUC-7), 
1998. 
Sekine S., Grishman R. and Shinou H: A decision tree 
method for finding and classifying names in Japanese 
texts. In: Proceedings of the Sixth Workshop on Very 
Large Corpora, Canada, 1998. 
Aberdeen, John, et al MITRE: Description of the 
ALEMBIC System Used for MUC-6. In: Proceedings 
of the Sixth Message Understanding Conference 
(MUC-6), November. (1995) 141-155. 
Ralph Grishman and Beth Sundheim: Design of the 
MUC-6 evaluation. In: 6th Message Understanding 
Conference, Columbia, MD. (1995) 
Krupka, G. R. and Hausman, K. IsoQuest: Inc.: Descrip-
tion of the NetOwl TM Extractor System as Used for 
MUC-7. In Proceedings of the MUC-7, 1998. 
Black, W.J.; Rinaldi, F, Mowart, D: FACILE: Descrip-
tion of the NE System Used for MUC-7. In Proceed-
ings of the MUC-7, 1998. 
Michael Collins, Yoram Singer: Unsupervised models 
for named entity classification. In Proceedings of 
EMNLP. (1999) 
Michael Collins: Ranking Algorithms for Named Entity 
Extraction: Boosting and the Voted Perceptron. In: 
Proceeding of ACL-2002. (2002) 489-496. 
S.Y.Yu, et al Description of the Kent Ridge Digital 
Labs System Used for MUC-7. In: Proceedings of the 
Seventh Message Understanding Conference, 1998. 
H.H. Chen, et al Description of the NTU System Used 
for MET2. In: Proceedings of the Seventh Message 
Understanding Conference. 
Tat-Seng Chua, et al Learning Pattern Rules for Chi-
nese Named Entity Extraction. In: Proceedings of 
AAAI'02. (2002) 
Maosong Sun, et al Identifying Chinese Names in Un-
restricted Texts. Journal of Chinese Information 
Processing. (1994). 
Jiahen Zheng, Xin Li, Hongye Tan: The Research of 
Chinese Names Recognition Methods Based on Cor-
pus. In: Journal of Chinese Information Processing. 
Vol.14 No.1. (2000). 
CoNLL. http://cnts.uia.ac.be/conll2004/ 
IEER. http://www.nist.gov/speech/tests/ie-er/er99/er99. 
htm 
ACE. http://www.itl.nist.gov/iad/894.01/tests/ace/ 
 
434
Product Named Entity Recognition Based on Hierarchical Hidden
Markov Model?
Feifan Liu, Jun Zhao, Bibo Lv, Bo Xu
National Laboratory of Pattern Recognition
Institute of Automation Chinese Academy of Sciences
Beijing P.O. Box 2728, 100080
{ffliu,jzhao,bblv,xubo}@nlpr.ia.ac.cn
Hao Yu
FUJITSU R&D
Xiao Yun Road No.26
Chao Yang District, Beijing, 100016
yu@frdc.fujitsu.com
Abstract
A hierarchical hidden Markov model
(HHMM) based approach of product
named entity recognition (NER) from
Chinese free text is presented in this pa-
per. Characteristics and challenges in
product NER is also investigated and
analyzed deliberately compared with
general NER. Within a unified statis-
tical framework, the approach we pro-
posed is able to make probabilistically
reasonable decisions to a global opti-
mization by leveraging diverse range
of linguistic features and knowledge
sources. Experimental results show that
our approach performs quite well in two
different domains.
1 Introduction
Named entity recognition(NER) plays a sig-
nificantly important role in information extrac-
tion(IE) and many other applications. Previous
study on NER is mainly focused either on the
proper name identification of person(PER), lo-
cation(LOC), organization(ORG), time(TIM) and
numeral(NUM) expressions almost in news do-
main, which can be viewed as general NER, or
other named entity (NE) recognition in specific
domain such as biology.
As far as we know, however, there is little prior
research work conducted by far on product named
0This work was supported by the Natural Sciences Foun-
dation of China(60372016,60272041) and the Natural Sci-
ence Foundation of Beijing(4052027).
entity recognition which can be crucial and valu-
able in many business IE applications, especially
with the increasing research interest in Market
Intelligence Management(MIM), Enterprise Con-
tent Management (ECM) [Pierre 2002] and etc.
This paper describes a prototype system for
product named entity recognition, ProNER, in
which a HHMM-based approach is employed.
Within a unified statistical framework, the ap-
proach based on a mixture model is able to make
probabilistically reasonable decisions to a global
optimization by exploiting diverse range of lin-
guistic features and knowledge sources. Experi-
mental results show that ProNER performs quite
well in two different domains.
2 Related Work
Up to now not much work has been done on
product named entity recognition, nor systematic
analysis of characteristics for this task. [Pierre
2002] developed an English NER system capable
of identifying product names in product views. It
employed a simple Boolean classifier for identi-
fying product name, which was constructed from
the list of product names. The method is sim-
ilar to token matching and has a limitation for
product NER applications. [Bick et al 2004] rec-
ognized named entities including product names
based on constraint grammar based parser for
Danish. This rule-based approach is highly de-
pendent on the performance of Danish parser and
suffers from its weakness in system portability.
[C. Niu et al 2003] presented a bootstrapping ap-
proach for English named entity recognition us-
ing successive learners of parsing-based decision
40
System Statistical Model Linguistic Feature Combinative Points
[Zhang et al 2003] HMM semantic role, tokens pattern rules
[Sun et al 2002] class-based LM word form, NE category cue words list
[Tsai et al 2004] ME model tokens knowledge representation
Table 1: Comparison between several Chinese NER systems1
list and HMM, and promising experiment results
(F-measure: 69.8%) on product NE (correspond-
ing to our PRO) were obtained. Its main advan-
tage lies in that manual annotation of a sizable
training corpus can be avoided, but it suffers from
two problems, one is that it is difficult to find suf-
ficient concept-based seeds needed in bootstrap-
ping for the coverage of the variations of PRO
subcategories, another it is highly dependent on
parser performance as well.
Research on product NER is still at its early
stage, especially in Chinese free text collec-
tions. However, considerable amount of work
has been done in the last decade on the gen-
eral NER task and biological NER task. The
typical machine learning approaches for English
NE are transformation-based learning[Aberdeen
et al 1995], hidden Markov model[Bikel et
al. 1997], maximum entropy model[Borthwick,
1999], support vector machine learning[Eunji Yi
et al 2004], unsupervised model[Collins et al
1999]and etc.
For Chinese NER, the prevailing methodology
applied recently also lie in machine learning com-
bining other knowledge base or heuristic rules,
which can be compared on the whole in three as-
pects showed in Table 1.
In short, the trend in NER is to adopt a statis-
tical framework which try to exploit some knowl-
edge base as well as different level of text features
within and outside NEs. Further those ideas, we
present a hybrid approach based on HHMM [S.
Fine et al 1998] which will be described in de-
tail.
3 Problem Statements and Analysis
3.1 Task Definition
3.1.1 Definition of Product Named Entity
In our study, only three kinds of prod-
uct named entities are considered, namely
1Note: LM(language model); ME(maximum entropy).
Brand Name(BRA), Product Type(TYP), Product
Name(PRO), and BRA and TYP are often embed-
ded in PRO. In the following two examples, there
are two BRA NEs, one TYP NE and one PRO
NE all of which belong to the family of product
named entities.
Exam 1: ??(Benq)/BRA ??(brand)?
? ? ? ?(market shares)? ?(steadily)?
?(ascend)b
Exam 2: ? ?(corporation)? ?(will)?
?(deliver) [Canon/BRA 334?(ten thou-
sand)? ?(pixels)? ?(digital)? ?(camera)
Pro90IS/TYP]/PROb
Brand Name refer to proper name of product
trademark such as ???(Benq)? in Exam 1.
Product Type is a kind of product named en-
tities indicating version or series information of
product, which can consist of numbers, English
characters, or other symbols such as ?+? and ?-
? etc.In our study, two principles should be fol-
lowed.
(1) Chinese characters are not considered to
be TYP, nor subpart of TYP although some of
them can contain version or series information.
For instance, in ?2005????(happy new
year)?(version)??(cell phone)?, here ???
??(happy new year)?(version)?should not be
considered as a TYP.
(2) Numbers are essential elements in prod-
uct type entity. For instance, in ?PowerShot
??(series)??(digital)??(camera)?, ?Pow-
erShot? is not considered as a TYP, however,
in ?PowerShot S10 ??(digital)??(camera)?,
?PowerShot S10? can make up of a TYP.
Product Name, as showed above in Exam 2, is
a kind of product named entities expressing self-
contained proper name for some specified product
in real world compared to BRA and TYP which
only express one attribute of product. i.e. a PRO
NE must be assigned with distinctly discrimina-
tive information which can not shared with other
general product-related expressions.
41
(1) Product-related expressions which are em-
bedded with either BRA or TYP can be qual-
ified to be a PRO entity. e.g. ?BenQ?
?(flash)?(disk)? is a PRO entity, but the gen-
eral product-related expression ???(flash)?
?(market)??(investigation)? cannot make up
of a PRO entity.
(2) Product-related expressions indicating
some specific version or series information which
is unique for a BRA can also be considered as a
PRO entity. e.g. ?DIGITAL IXUS??(series)?
?(digital)? ?(camera)? is a PRO because
?DIGITAL IXUS? series is unique for Canon
product, but ?? ?(intelligent)?(version)?
?(cell phone)? is not a PRO because the at-
tribute of ?intelligent version? can be assigned to
any cell phone product.
3.1.2 Product Named Entity Recognition
Product named entity recognition involves the
identification of product-related proper names
in free text and their classification into differ-
ent kinds of product named entities, referring to
PRO, TYP and BRA in this paper.In comparison
with general NER, nested product NEs should be
tagged separately rather than being tagged just as
a single item, shown as Figure 1.
3.2 Challenges for Product Named Entity
Recognition
?For general named entities, there are some
cues which are very useful for entity recogni-
tion, such as ???(city), ????(Inc.), and etc. In
comparison, product named entities have no such
named conventions and cues, resulting in higher
boundary ambiguities and more complex NE can-
didate triggering difficulties.
?In comparison with general NER, more chal-
lenges in product NER result from miscellaneous
classification ambiguities. Many entities with
identical form can be a kind of general named en-
tity, a kind of product named entity, or just com-
mon words.
?In comparison with general named entities,
product named entities show more flexible vari-
ant forms. The same entity can be expressed in
several different forms due to spelling variation,
word permutation and etc. This also compounds
the difficulties in product named entity recogni-
tion.
?In comparison with general named entities,
it is more frequent that product named entities are
nested as Figure 1 illustrates. More efforts have
to be made to identify such named entities sepa-
rately.
3.3 Our Solutions
We adopt the following strategies in triggering
and disambiguating process respectively.
(1) As to product NER, it?s pivotal to control
the triggering candidates efficiently for the bal-
ance between precision and recall. Here we use
the knowledge base such as brand word list, and
other heuristic information which can be easily
acquired.
(2)After triggering candidates, we try to em-
ploy a statistical model to make the most of
multi-level context information mentioned above
in disambiguation. We choose hierarchical hid-
den Markov model (HHMM) [S. Fine et al 1998]
for its more powerful ability to model the multi-
plicity of length scales and recursive nature of se-
quences.
42
4 Hybrid Approach for Product NE
Recognition
4.1 Overall Workflow of ProNER
?Preprocessing: Segment, POS tagging and
general NER is primarily conducted using our off-
shelf SegNer2.0 toolkit on input text.
?Generating Product NE Candidates: First,
BRA or ORG and TYP are triggered by brand
word list and some word features respectively.
Here we categorize the triggering word features
into six classes: alphabet string, alphanumeric
string, digits, alphabet string with fullwidth, dig-
its with fullwidth and other symbols except Chi-
nese characters. Then PRO are triggered by BRA
and TYP candidates as well as some clue words
indicating type information to some extent such
as ???(version), ????(series). In this step the
model structure(topology) of HHMM[S. Fine et
al. 1998] is dynamically constructed, and some
conjunction words or punctuations and specified
maximum length of product NE are used to con-
trol it.
?Disambiguating Candidates: In this mod-
ule, boundary and classification ambiguities be-
tween candidates are resolved simultaneously.
And Viterbi algorithm is applied for most-likely
state sequences based on the HHMM topology.
4.2 Integration with Heuristic Information
To get more efficient control in triggering process
above, we try to integrate some heuristic informa-
tion. The heuristic rules we used are as domain-
independent as possible in order that they can
be integrated with statistical model systematically
rather than just some tricks on it.
(1) Stop Word List:
Common English words, English brand word,
and some punctuations are extracted automati-
cally from training set to make up of stop word
list for TYP; by co-occurrence statistics between
ORG and its contexts, some words are extracted
from the contexts to make up of stop word list
for PRO in order to overcome the case that brand
word is prone to bind its surroundings to be a
PRO.
(2) Constrain Rules:
Rule 1: For the highly frequent pattern ??
?+?????(number + English quantifier
ES PS5IS2IS1
IS0
ES PS1 PS2 PS4PS3ES
0.2 0.5
0.3
0.7  0.3 0.5 0.30.7
0.2
0.3
Figure 2 Structure of Hierarchical Hidden
Markov Model (HHMM)
word), all the corresponding TYP candidates trig-
gered by categorized word features(CWF) should
be removed.
Rule 2: Product NE candidates in which some
binate symbols don?t match each other should be
removed.
Rule 3: Unreasonable symbols such as ?-? or
?:? should not occur in the beginning or end of
product NE candidates.
4.3 HHMM for product NER application
By HHMM [S. Fine et al 1998] the product
NER can be formulated as a tagging problem us-
ing Viterbi algorithm. Unlike traditional HMM
in POS tagging, here the topology of HHMM is
not fixed and internal states can be also a similar
stochastic model on themselves, called internal
states compared to production states which will
emit only observations.
Our HHMM structure actually consists of three
level approximately illustrated as figure 2 in
which IS denotes internal state, PS denotes pro-
duction state and ES denote end state at ev-
ery level. For our application, an input se-
quence from our SegNer2.0 toolkit can be formal-
ized as w1/t1w2/t2 . . . wi/ti . . . wn/tn, among
which wi and ti is the ith word and its part-of-
speech, n is the number of words. The POS
tag set here is the combination of tag set from
Peking University(PKU-POS) and our general
NE categories(GNEC) including PER(person),
LOC(location), ORG(organization), TIM(time ex-
pression), NUM(numeric expression). Therefore
we can construct our HHMM model by the state
set {S} consisting of {GNEC}, {BRA, PRO,
TYP}, and {V} as well as the observation set {O}
consisting of {V} which is the word set from
training data. That is to say, the word forms
43
in {V} which are not included in NEs are also
viewed as production states.
In our model, only PRO are internal state which
may activate other production states such as BRA
and TYP resulting in recursive HMM. In consis-
tence with S. Fine?s work, qdi (1? d ? D) is used
to indicate the ith state in the dth level of hierar-
chy. So, the product NER problem is to find the
most-likely state activation sequence Q*, a multi-
scale list of states, based on the dynamic topol-
ogy of HHMM given a observation sequence W
= w1w2 . . . wi . . . wn, formulated as follows based
on Bayes rule (P (W )=1).
Q?= argmax
Q
P (Q|W )= argmax
Q
P (Q)P (W |Q)
(1)
From the root node of HHMM, activity flows
to all other nodes at different levels according to
their transition probability. For description conve-
nience, we take the kth level as example(activated
by the mth state at the k-1th level).
P (Q) ?= p(qk1 |qk?1m )
? ?? ?
vertical transition
horizontal transition
? ?? ?
p(qk2 |qk1 )
|qk|
?
j=3
p(qkj |qkj?1, qkj?2)
(2)
P (W |Q)=
?
???????
???????
?=
|qkPS |?
j=1
p([wqkj ?begin...wqkj ?end]|q
k
j )
if qkj /? {IS}
activate other states recursively
if qkj ? {IS}
(3)
Where |qk| is the number of all states and |qkPS |
is the number of production states in the kth level;
wqkj ?begin...wqkj ?end indicates the word sequence
corresponding to the state qkj .
(1) In equation (3), if qkj ? {{GNEC},{V}},
p([wqkj ?begin...wqkj ?end]|q
k
j )=1, because we as-
sume that the general NER results from the pre-
ceding toolkit are correct;
(2) If qkj = PRO, production states in the
(k+1)th level will be activated by this internal
state through equation (2),(3) and go back when
arriving at an end state, thus hierarchical compu-
tation is implemented;
(3) If qkj =BRA, we assign equation (3) a con-
stant value in that BRA candidates consist of only
a single brand word in our method. In addition
brand word can also generate ORG candidates,
thus we can assign equation (3) as follows.
p([wqkj ?begin...wqkj ?end]|q
k
j = BRA) = 0.5 (4)
(4) If qkj = TY P , categorized word fea-
tures(CWFs) defined in section 4.1 are applied,
i.e. the words associated with the current state are
replaced with their CWFs (WC) acting as obser-
vations. Then we can compute the emission prob-
ability of this TYP production state as the follow-
ing equation, among which |qkj | is the length of
observation sequence associated with the current
state.
p([wqkj ?begin...wqkj ?end]|q
k
j = TY P )
?=p(wc1|begin)p(end|wc|qkj |)
|qkj |?
m=2
p(wcm|wcm?1)
All the parameters in every level of HHMM can
be acquired using maximum likelihood method
with smoothing from training data.
4.4 Mixture of Two Hierarchical Hidden
Markov Models
Now we have implemented a simple HHMM
for product NER. Note that in the above
model(HHMM-1), we exploit both internal and
external features of product NEs only at lev-
els of simply semantic classification and just
word form. To achieve our motivation in sec-
tion 3.3, we construct another HHMM(HHMM-
2) for exploiting multi-level contexts by mixing
with HHMM-1.
In HHMM-2, the difference from HHMM-1
lies in the state set SII and observation set OII .
Because the input text will be processed by seg-
ment, POS tagging and general NER, as a alterna-
tive, we can also take T=t1t2 . . . ti . . . tn as obser-
vation sequence, i.e. OII={PKU-POS}. Accord-
ingly, SII= {{PKU-POS}, {GNEC}, BRA, TYP,
44
Data Sets PRO BRA TYP PER LOC ORG
DataSetPRO1.2 12,432 5,047 10,606 424 1,733 4,798
OpenTestSet 1800 803 1364 39 207 614
CloseTestSet 1553 513 1296 55 248 619
Table 2: Overview of Data Sets
PRO}, among which PRO is internal state. Sim-
ilarly, the problem is formulated as follows with
HHMM-2.
Q?II = argmax
QII
P (QII |T )
= argmax
QII
P (QII)P (T |QII) (5)
The description and computation of HHMM-2
is similar to HHMM-1 and is omitted here.
We can see that besides making use of semantic
classification of NEs in common, HHMM-1 and
HHMM-2 exploit word form and part-of-speech
(POS) features respectively. Word form features
make the model more discriminative, while POS
features result in robustness. Intuitively, the mix-
ture of these two models is desirable for higher
performance in product NER by balancing the ro-
bustness and discrimination which can be formu-
lated in logarithmic form as follows.
(Q?, Q?II)
= argmax
Q,QII
{log(P (Q)) + log(P (W |Q))
+ ?[log(P (QII)) + log(P (T |QII))]} (6)
Where ? is a tuning parameter for adjusting the
weight of two models.
5 Experiments and analysis
5.1 Data Set Preparation
A large number of web pages in mobile phone
and digital domain are compiled into text collec-
tions, DataSetPRO, on which multi-level process-
ing were performed. Our final version, DataSet-
PRO1.2, consists of 1500 web pages, roughly
1,000,000 Chinese characters. Randomly se-
lected 140 texts (digital 70, mobile phone 70) are
separated from DataSetPRO1.2 as our OpenTest-
Set, the rest as TrainingSet, from which 160 texts
are extracted as CloseTestSet. Table 2 illustrates
the overview of them.
5.2 Experiments
Due to various and flexible forms of product NEs,
though some boundaries of recognized NEs are
inconsistent with manual annotation, they are also
reasonable. So soft evaluation is also applied
in our experiments to make the evaluation more
reasonable. The main idea is that a discount
score will be given to recognized NEs with wrong
boundary but correct detection and classification.
However, strict evaluation only score completely
correct ones.
All the results is conducted on OpenTestSet un-
less it is particularly specified. Also, the evalu-
ation scores used below are obtained mainly by
45
Digital Domain (??8)
Product NEs Close Test Open TestPrecision Recall F-measure Precision Recall F-measure
PRO 0.864 0.799 0.830 0.762 0.744 0.753
TYP 0.903 0.906 0.905 0.828 0.944 0.882
BRA 0.824 0.702 0.758 0.723 0.705 0.714
Mobile Phone Domain (??8)
Product NEs Close Test Open TestPrecision Recall F-measure Precision Recall F-measure
PRO 0.917 0.935 0.926 0.799 0.856 0.827
TYP 0.959 0.976 0.967 0.842 0.886 0.864
BRA 0.911 0.741 0.818 0.893 0.701 0.785
Table 3: Experimental Results in Digital and Mobile Phone Domain
soft metrics, and strict scores are also given for
comparison in experiment 3.
1. Evaluation on the Influence of ? in the Mix-
ture Model.
In the mixture model denoted as equation (6),
the ? value reflects the different contribution of
two individual models to the overall system per-
formance. The larger ?, the more contribution
made by HHMM-2. Figure 3, 4, 5 illustrate the
varying curves of recognition performance with
the ? value on PRO, TYP, BRA respectively.
Note that, if ? equal to 1 then two models
are mixed with equivalent weight. We can see
that, as ? goes up, the F-measures of PRO and
TYP increase obviously firstly, and begin to go
down slightly after a period of growing flat. It
can be explained that HHMM-2 mainly exploits
part-of-speech and general NER features which
can relieve the sparseness problem to some ex-
tent, which is more serious in HHMM-1 due to
using lower level of contextual information such
as word form. However, as ? becomes larger,
the problem of imprecise modeling in HHMM-
2 will be more salient and begin to illustrate a
side-effect in the mixture model. Whereas, the
influence of ? on BRA is negligible because its
candidates are triggered by the relatively reliable
knowledge base and its sub-model in HHMM is
assigned a constant as shown in equation(4).
Summings-up:
(1) Mixture with HHMM-2 can make up the
weakness of HHMM-1.
(2) HHMM-2 can make more contributions
to the mixture model under the conditions that
limited annotated data is available at present. In
our system, ? is assigned to 8 based on above ex-
perimental results.
2. Evaluation on the portability of ProNER in
two domains.
First, we can see from Table 3 that ProNER
have achieved fairly high performance in both
digital and mobile phone domain. This can val-
idate to some extent the portability of our sys-
tem?which is consistent with our initial motiva-
tion.
Second, the results also show that our system
performs slightly better in mobile phone domain
for both close test and open test. This can be ex-
plained that there are more challenging ambigui-
ties in digital domain due to more complex prod-
uct taxonomy and more flexible variants of prod-
uct NEs.
Summings-up: The results provide promising
evidence on the portability of our system to dif-
ferent domains though there are some differences
between them.
3. Evaluation on the efficiency of the mixture
model and the improvement of the triggering
control with heuristics.
In table 4, ?1? denotes HHMM-1; ?2? denotes
HHMM-2; ?+? means the mixture model; ?*?
means integrating with heuristics mentioned in
section 4.2.
The results reveal that the mixture model out-
performs each individual model with both soft
and strict metrics. Also, the results show that
heuristic information can increase the F-measure
of PRO and TYP by 10 points or so for both indi-
46
HHMM
BRA TYP PRO
strict
score
soft
score
strict
score
soft
score
strict
score
soft
score
1 0.68 0.72 0.57 0.66 0.52 0.61
1* 0.70 0.74 0.70 0.80 0.63 0.72
2 0.67 0.73 0.66 0.74 0.61 0.68
2* 0.70 0.74 0.76 0.85 0.70 0.76
1+2 0.70 0.75 0.67 0.77 0.67 0.72
1+2* 0.72 0.76 0.76 0.87 0.75 0.80
Table 4: Improvement results (F-measure) with
heuristics and model mixture
vidual model and the mixture model. Addition-
ally we can see that HHMM-2 performs better
on the whole than HHMM-1, which is consistent
with experiment 1 that heavier weights should be
assigned to HHMM-2 in the mixture model.
Summings-up:
(1) Either HHMM-1 or HHMM-2 can not
perform quite well independently, but systemat-
ical integration of them can achieve obvious per-
formance improvement due to the leverage of di-
verse levels of linguistic features by their efficient
interaction.
(2) Heuristic information can highly enhance
the performance for both individual model and the
mixture model.
6 Conclusions and Future Work
This paper presented a hierarchical HMM (hidden
Markov model) based approach of product named
entity recognition from Chinese free text. By uni-
fying some heuristic rules into a statistical frame-
work based on a mixture model of HHMM, the
approach we proposed can leverage diverse range
of linguistic features and knowledge sources to
make probabilistically reasonable decisions for a
global optimization. The prototype system we
built achieved the overall F-measure of 79.7%,
86.9%, 75.8% corresponding to PRO, TYP, BRA
respectively, which also provide experimental ev-
idence to some extent on its portability to differ-
ent domains.
Our future work will focus on the following:
(1) Using long dependency information;
(2) Integrating segment, POS tagging, general
NER and product NER to avoid error spread.
References
John M. Pierre. (2002) Mining Knowledge from Text
Collections Using Automatically Generated Meta-
data. In: Procs of Fourth International Conference
on Practical Aspects of Knowledge Management.
Michael Collins and Yoram Singer. (1999) Unsuper-
vised Models for Named Entity Classification. In:
Proc. of EMNLP/VLC-99.
Eunji Yi, Gary Geunbae Lee, and Soo-Jun Park.
(2004) SVM-based Biological Named Entity
Recognition using Minimum Edit-Distance Feature
Boosted by Virtual Examples. In: Proceedings of
the First International Joint Conference on Natural
Language Processing (IJCNLP-04).
Bick, Eckhard (2004) A Named Entity Recognizer for
Danish. In: Proc. of 4th International Conf. on Lan-
guage Resources and Evaluation,pp:305-308.
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou,
Changning Huang. (2002) Chinese Named Entity
Identification Using Class-based Language Model.
In: COLING 2002. Taipei, Taiwan.
Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng,
Shuo Bai. Chinese Named Entity Recognition Us-
ing Role Model. Special Iissue ?Word Formation
and Chinese Language processing? of the Inter-
national Journal of Computational Linguistics and
Chinese Language Processing, 8(2),2003, pp:29-60
Aberdeen, John et al (1995)MITRE: Description of
the ALEMBIC System Used for MUC-6. Proc. of
MUC-6, pp. 141-155
D.M. Bikel, S. Miller, R. Schwartz, R. Weischedel.
(1997) Nymble: a High-Performance Learning
Name-finder. In: Fifth Conference on Applied Nat-
ural Language Processing, pp 194-201.
Borthwick. A. (1999) A Maximum Entropy Approach
to Named Entity Recognition. PhD Dissertation.
Tzong-Han Tsai, S.H. Wu, C.W. Lee, Cheng-Wei
Shih, and Wen-Lian Hsu. (2004) Mencius: A Chi-
nese Named Entity Recognizer Using the Maxi-
mum Entropy-based Hybrid Model. International
Journal of Computational Linguistics and Chinese
Language Processing, Vol. 9, No 1.
Cheng Niu, W. Li, J.h. Ding and R.K. Srihari. (2003) A
Bootstrapping Approach to Named Entity Classifi-
cation Using Successive Learners. In: Proceedings
of the 41st ACL, Sapporo, Japan, pp:335-342.
S. Fine, Y. Singer, N. Tishby. (1998) The Hierarchical
Hidden Markov Model: Analysis and Applications.
Machine Learning. 32(1), pp:41-62
47
CRFs-Based Named Entity Recognition Incorporated with Heuristic 
Entity List Searching 
Fan Yang 
National Laboratory of 
Pattern Recognition 
Institute of Automation, 
Chinese Academy of 
Sciences  
fyang@nlpr.ia.ac.cn 
Jun Zhao 
National Laboratory of 
Pattern Recognition 
Institute of Automation, 
Chinese Academy of 
Sciences  
jzhao@nlpr.ia.ac.cn
Bo Zou 
National Laboratory of 
Pattern Recognition 
Institute of Automation, 
Chinese Academy of 
Sciences  
bzou@nlpr.ia.ac.cn
 
 
Abstract 
Chinese Named entity recognition is one of 
the most important tasks in NLP. Two 
kinds of Challenges we confront are how to 
improve the performance in one corpus and 
keep its performance in another different 
corpus. We use a combination of statistical 
models, i.e. a language model to recognize 
person names and two CRFs models to 
recognize Location names and 
Organization names respectively. We also 
incorporate an efficient heuristic named 
entity list searching process into the 
framework of statistical model in order to 
improve both the performance and the 
adaptability of the statistical NER system. 
We participate in the NER tests on open 
tracks of MSRA. The testing results show 
that our system can performs well. 
1 Introduction 
Named Entity Recognition (NER) is one of the 
most important tasks in NLP, and acts as a critical 
role in some language processing applications, 
such as Information Extraction and Integration, 
Text Classification etc. Many efforts have been 
paid to improve the performance of NER. 
NER task in Chinese has some differences from 
in English as follows. 1) There is no space between 
Chinese characters, which make boundary 
recognition more difficult. 2) In English, a 
capitalized letter at the beginning position of a 
word implies that the word is a part of a named 
entity. However, this kind of characteristic does 
not exist in Chinese. 
In the paper, we will focus on two kinds of 
problems. 1) How to improve the performance of 
Chinese NER in one corpus, which contains 
boosting precision rate, recall rate and F-measure 
rate. 2) How to enhance the  adaptability of a 
Chinese NER system, which means that a system 
can get a good performance on a testing set which 
has many differences from the training set. To 
solve the first problem, we should select a good 
model and adjust parameters carefully. But there is 
no framework that can solve the second problem 
completely.  
Our goal is to find a way to solve these two 
problems. We select a language model to recognize 
Person names, and two CRFs models are used to 
recognize Location and Organization separately.  
We also try to incorporate a large-scale named 
entity list into the statistical model, where a 
heuristic searching method is developed to match 
the entities in the list quickly and efficiently. 
2 Framework of NER System 
The Input of the system is a raw text. We will 
apply some pre-processing such as code 
transformation. Then the heuristic searching will 
be executed to find the appearance of the entities in 
the named entity list. After that, two CRFs that 
have been trained before will be used to recognize 
Location and Organization based on the result of 
word segmentation, and a language model will be 
used to find Person names. All the results will be 
integrated at last. 
171
Sixth SIGHAN Workshop on Chinese Language Processing
 
Figure 1.  System Frameworks 
3 System Details 
3.1 Using heuristic method to search entity 
list 
NER task meets many difficulties which come 
from the complexity of the construction of named 
entities. Named entities have flexible internal 
styles and external environments. Building a good 
model to describe the condition precisely will have 
many troubles. The statistical models we common-
ly used have some shortcomings, especially when 
they are adapted to a corpus of new domains or 
styles. We try to use an improved searching 
method to make up the relatively poor adaptability 
of the statistical models. The heuristic searching 
method is more flexible especially in the following 
two aspects. 1) Abbreviation can be matched. 2) 
Suffix in location and organization is universal, but 
it should not be taken in count when we search 
named entities. 
The framework of the algorithm can be briefly 
described as follows: 
 
1) Building an inverse index using Chinese characters 
as key term; 
2) Using the text as a query to search for entities; 
3) When comes terminal condition, a heuristic 
function is invoked to determine whether the 
character sequence is an entity; 
4) When comes creation condition, a heuristic 
function is invoked to judge whether a new entity is 
created; 
5) The labeled sequence is output. 
Table 1. Heuristic Searching Method 
One advantage of heuristic searching method is 
that the heuristic function can be set to fit a special 
corpus. The heuristic searching method we used in 
Bakeoff-4 is as follows:  
Un-segmenting test 
Person recognition 
Heuristic searching 
Location 
recognition 
Organization 
recognition 
Word segmentation 
Output results 
z Ignoring the suffix key word in Location and 
Organization names. For example 
??? ??/Tongfang /Corporation? and 
???/Tongfang? will get same score under 
this heuristic rule. 
z Ignoring the Location name as a prefix in an 
Organization name. For example, 
???/Ameri ??can /General Motors ? and 
???/General Motors? will get same score 
under this heuristic rule 
z Taking Abbreviation rules in consideration. 
For example??? ??/Peking /University? 
can be abbreviated as ?? ?/Bei /Da? rather 
than ???/Peking? or ???/University? 
Heuristic searching method also has such advan-
tages as follows: 
It is easy to be expanded to a corpus of new 
domain or style. We only need to add the entities 
in the new domain into list 
Searching method will improve the recall 
performance remarkably 
But the precision will be reduced for the ambi-
guities, i.e. whether a sequence that matches an 
entity in the list really constructs an entity in the 
text. We will disambiguate it using statistical 
models. 
3.2 Conditional Random Fields Model 
Conditional Random Fields (CRFs) is an 
undirected graphical model that encodes a 
conditional probability distribution using a given 
set of features. Currently it is widely used as a 
discriminate model for sequence labeling: 
1 2
1
1
( | ) exp( ( , , ))
k
i i c c
c C i
P Y X f y y X
Z
?
? =
= ??     (1) 
CRFs is considered to be a very effective model 
to resolve the issue of sequence labeling for the 
following characteristics: 
Because it uses a non-greedy whole sentence 
joint labeling method, high accuracy rate can be 
guaranteed and bias labeling can be avoided. 
Any types of features can be integrated in the 
model flexibly. 
Over-fitting can be avoided to some extent by 
integrating a priori with training data. 
172
Sixth SIGHAN Workshop on Chinese Language Processing
As a discriminate model, CRFs inherits the 
advantages of both Hidden Markov Model (HMM) 
and Maximum Entropy Markov Model (MEMM) 
as well. 
3.3 Person names recognize 
We use language model to recognize Personal 
names. We use character-based rather than word-
based model to avoid the word segmentation errors. 
We construct a context model and an entity model 
to respectively describe external and internal 
features of Personal names. The details of the 
model are as follows: 
We use a tri-gram model as the context model: 
( ) ( )? --?
m
1i
1i2ii wcwc|wcPWCP
=
               (2) 
Entity model: 
( )
( ) ( )( ) ( )( )
1 1
1 1 1
2
1 2 1
1
1
2
| |
| | , | ,
i ik i ik
i il iki l i k
k
wc wc i wc wc k k
k
wc wc l wc wc k wc
l
P w w wc P w w B M M E
P w B P w M w P w E w
? ?
?
?
?
=
? ?? ?= ??
? ? ??

" " " ?? (3) 
Where B means the beginning of the entity, M 
means middle, E means end. 
Some expert knowledge is employed to assist 
the recognition process of language model. 
z A Chinese family name list (476) and a 
Japanese family name list (9189) are used to 
restrict and select the generated candidates. 
z A list of commonly used character in Russian 
and European name. 
z Constrain of name length: A Chinese name 
cannot contain more than 8 characters. 
3.4 Location names recognition 
Location names have some composition characters. 
1) There may be some key words as suffix, such as: 
??/Shi, ?/Zheng, ?/Hu, ?/Shan? etc. 2) Other 
parts of Location names are always OOV words, 
such as ????/Dagang Village, ???/Fuzi 
Temple?.  So the right boundary of Location can 
be determined easily. The mainly problem in 
Location recognition is on abbreviation, such as 
?????/JinJiLuYu? is the combination of four 
location abbreviations. In our system, CRFs model 
can be supported by the heuristic searching method 
because it can match the abbreviation of entity in 
list. Using the searching method can boost the 
recall rate of location recognition significantly. We 
construct the recognition model based on the word-
segmented texts. 
To construct a CRFs model, we select the 
following features: 
z A list of key word suffix is used to trigger the 
recognition processing. 
z Using a list of indication words to restrict the 
boundary. 
z Heuristic searching method is used to assist 
Location recognition. 
The features we used in CRFs model is followed: 
 
W0 Current Word 
W-1?W-2 , W1?W2 Two words before and 
behind 
W-1W0?W0W1 Bi-gram Features 
POS0 POS tag 
PRE-1?PRE0?PRE1 Pre-Position reference 
words 
SRE-1?SRE0?SRE1 Suf-Position reference 
words 
Key Has Key suffix 
DIC In Dictionary 
Table 2. Features used in Location recognition 
Statement: 
The indication words used in Location 
recognition include ?for-indicate? and ?back-
indicate? words, where ?for-indicate? denotes the 
indicating words that occur as the left neighbor of 
the candidate Location named entity, while ?back-
indicate? denotes the indicating words that occur 
as the right neighbor. ?for-indicate? and ?back-
indicate? words are got from the training corpus. 
We calculate the mutual information between 
neighbor words and location entity, and get the top 
N words as indication words. 
( , )
( , ) ( , ) log
( ) ( )
p x y
MI x y p x y
p x p y
=     (4) 
We select 1216 for-indicate words and 1227 
back-indicate words. We also get 607 key words as 
location name suffix. 
3.5 Organization names recognition 
Organization name recognition is the most difficult 
part in NER task. The difficulties are as follows. 1) 
The composition of Organization name is very 
complex. For example: ??? ??/Dalian /Shide 
??/Group?, the first words in the entity is a 
location name. The second is a phonetic name 
173
Sixth SIGHAN Workshop on Chinese Language Processing
which is also an OOV word and the last one is a 
key word as suffix.2) The boundary of organizat-
ion name is hard to be classified, and the length of 
organization names is dynamitic. 3) Organization 
names are easily confused as Location names. We 
must use contextual information to determine its 
type. 4) To recognize the abbreviation of an 
organization is also a difficult task. So we choose 
the following features to solve the above problems. 
z A list of key word suffix is used to trigger the 
recognition processing. 
z Using indication words to define the boundary 
of organization. 
z Heuristic searching method is used to assist 
Location recognition. 
The features we used in the CRFs model are the 
same as used in Location model. We use the 
mutual information to select 513 for-indicate 
words and 1195 back-indicate words from training 
corpus. The number of key suffix words is 3129. 
4 Experiments 
We participate in the SigHAN Microsoft Research 
Asia (MSRA) corpus in open track. The table 3 is 
the official result of NER by our system. 
 
 R P F 
Person 0.9657 0.9574 0.9615 
Location 0.9593 0.9769 0.968 
Organization 0.8778 0.9338 0.9049 
overall 0.9377 0.9603 0.9489 
Table 3.  SigHAN MSRA corpus test results 
The training corpora we used comes from 1) 
1998 People?s Daily corpus; 2) the training corpus 
supplied by MSRA for SigHAN bakeoff 4. These 
two corpora have many difference and we focus on 
how to get a good performance both on training 
corpus and testing corpus. We select some general 
features and get assistance from the heuristic 
searching method. A good list is very important, 
which has been proved by the experimental data. 
We collect nearly 1 million personal names, 40 
thousand location names and more than 300,000 
organization names. 
5 Conclusion 
In the paper, we give a presentation to our Chinese 
Named Entity Recognition System. It uses a 
language mode to recognize personal names, and 
two CRFs models to find Location and 
organization separately. We also have a flexible 
heuristic searching method to match entity in 
named entity list with text characters sequence. 
Our system achieves a good result in the open 
NER track of MSRA corpus.  
Acknowledgement 
The work is supported by the National High 
Technology Development 863 Program of China 
under Grants no. 2006AA01Z144, the National 
Natural Science Foundation of China under Grants 
No. 60673042, the Natural Science Foundation of 
Beijing under Grants no. 4052027, 4073043. 
References 
J. Lafferty, A. McCallum, and F. Pereira. 2001. 
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data, In Proc. 
ICML 2001.  
F?Sha, F. Pereira. Shallow parsing with conditional 
random  fields. In Proc. NAACL 2003 
HP Zhang, HK Yu, DY Xiong, Q Liu and Liu Qun. 
HHMM-based Chinese Lexical Analyzer ICTCLAS. 
In Proc. Second of SIGHAN Workshop on Chinese 
Language Processing 2003. 
Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named Entity 
Recognition Model Based on Multiple Features. In 
Proceedings of HLT/EMNLP 2005 
Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named Entity 
Recognition Combining a Statistical Model with 
Human Knowledge. In Proceedings of ACL2003 
Workshop on Multilingual and Mixed-language 
Named Entity Recognition 
 
174
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of ACL-08: HLT, pages 541?549,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
Chinese-English Backward Transliteration Assisted with Mining Mono-lingual Web Pages    Fan Yang, Jun Zhao, Bo Zou, Kang Liu, Feifan Liu  National Laboratory of Pattern Recognition   Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China {fyang,jzhao,bzou,kliu,ffliu}@nlpr.ia.ac.cn     
Abstract 
In this paper, we present a novel backward transliteration approach which can further as-sist the existing statistical model by mining monolingual web resources. Firstly, we em-ploy the syllable-based search to revise the transliteration candidates from the statistical model. By mapping all of them into existing words, we can filter or correct some pseudo candidates and improve the overall recall. Secondly, an AdaBoost model is used to re-rank the revised candidates based on the in-formation extracted from monolingual web pages. To get a better precision during the re-ranking process, a variety of web-based in-formation is exploited to adjust the ranking score, so that some candidates which are less possible to be transliteration names will be as-signed with lower ranks. The experimental re-sults show that the proposed framework can significantly outperform the baseline translit-eration system in both precision and recall. 1 Introduction* The task of Name Entity (NE) translation is to translate a name entity from source language to target language, which plays an important role in machine translation and cross-language informa-tion retrieval (CLIR). Transliteration is a subtask in NE translation, which translates NEs based on the phonetic similarity. In NE translation, most person names are transliterated, and some parts of location names or organization names also need to be trans-literated. Transliteration has two directions: for-ward transliteration which transforms an original name into target language, and backward translit-eration which recovers a name back to its original expression. For instance, the original English per-                                                           *Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cn.  
son name ?Clinton? can be forward transliterated to its Chinese expression ??/ke ?/lin?/dun? and the backward transliteration is the inverse process-ing. In this paper, we focus on backward translit-eration from Chinese to English. Many previous researches have tried to build a transliteration model using statistical approach [Knight and Graehl, 1998; Lin and Chen, 2002; Virga and Khudanpur, 2003; Gao, 2004]. There are two main challenges in statistical backward trans-literation: First, statistical transliteration approach selects the most probable translations based on the knowledge learned from the training data. This approach, however, does not work well when there are multiple standards [Gao, 2004]. Second, back-ward transliteration is more challenging than for-ward transliteration as it is required to disambiguate the noises introduced in the forward transliteration and estimate the original name as close as possible [Lin and Chen, 2002]. One of the most important causes in introducing noises is that: some silent syllables in original names have been missing when they are transliterated to target lan-guage. For example, when ?Campbell? is translit-erated into ??/kan?/bei?/er?, the ?p? is missing.  In order to make up the disadvantages of statisti-cal approach, some researchers have been seeking for the assistance of web resource. [Wang et al, 2004; Cheng et al, 2004; Nagata et al, 2001; Zhang et al 2005] used bilingual web pages to ex-tract translation pairs. Other efforts have been made to combine a statistical transliteration model with web mining [Al-Onaizan and Knight, 2002; Long Jiang et al 2007]. Most of these methods need bilingual resources. However, those kinds of resources are not readily available in many cases. Moreover, to search for bilingual pages, we have to depend on the performance of search engines. We can?t get Chinese-English bilingual pages when the input is a Chinese query. Therefore, the existing 
541
 
assistance approaches using web-mining to assist transliteration are not suitable for Chinese to Eng-lish backward transliteration. Thus in this paper, we mainly focus on the fol-lowing two problems to be solved in transliteration. Problem I: Some silent syllables are missing in English-Chinese forward transliteration. How to recover them effectively and efficiently in back-ward transliteration is still an open problem. Problem II: Statistical transliteration always chooses the translations based on probabilities. However, in some cases, the correct translation may have lower probability. Therefore, more stud-ies are needed on combination with other tech-niques as supplements. Aiming at these two problems, we propose a method which mines monolingual web resources to assist backward transliteration. The main ideas are as follows. We assume that for every Chinese en-tity name which needs to be backward transliter-ated to an English original name, the correct transliteration exists somewhere in the web. What we need to do is to find out the answers based on the clues given by statistical transliteration results. Different from the traditional methods which ex-tract transliteration pairs from bilingual pages, we only use monolingual web resources. Our method has two advantages. Firstly, there are much more monolingual web resources available to be used. Secondly, our method can revise the transliteration candidates to the existing words before the subse-quent re-ranking process, so that we can better mine the correct transliteration from the Web. Concretely, there are two phases involved in our approach. In the first phase, we split the result of transliteration into syllables, and then a syllable-based searching processing can be employed to revise the result in a word list generated from web pages, with an expectation of higher recall of trans-literation. In the second phase, we use a revised word as a search query to get its contexts and hit information, which are integrated into the AdaBoost classifier to determine whether the word is a transliteration name or not with a confidence score. This phase can readjust the candidate?s score to a more reasonable point so that precision of transliteration can be improved. Table 1 illustrates how to transliterate the Chinese name ??/a?/jia?/xi? back to ?Agassi?.  Chinese name Transliteration results Revised Candidate Re-rank Results 
??? a  jia xi Agassi 
aggasi agahi agacy agasie ? 
agasi agathi agathe agassi ? 
agassi agasi agache agga ? Table 1. An example of transliteration flow The experimental results show that our approach improves the recall from 41.73% to 59.28% in open test when returning the top-100 results, and the top-5 precision is improved from 19.69% to 52.19%. The remainder of the paper is structured as fol-lows. Section 2 presents the framework of our sys-tem. We discuss the details of our statistical transliteration model in Section 3. In Section 4, we introduce the approach of revising and re-ranking the results of transliteration. The experiments are reported in Section 5. The last section gives the conclusion and the prediction of future work. 2 System Framework  Our system has three main modules. 
  Figure 1. System framework 1) Statistical transliteration: This module re-ceives a Chinese Pinyin sequence as its input, and output the N-best results as the transliteration can-didates.  2) Candidate transliteration revision through syllable-based searching: In the module, a transliteration candidate is transformed into a syllable query. We use a syllable-based searching strategy to select the revised candidate from a huge word list. Each word in the list is indexed by sylla-bles, and the similarity between the word and the query is calculated. The most similar words are returned as the revision results. This module guar-
Monolingual web pages 
Words list 
Chinese name 
Statistical model 
Transliteration candidates Syllable-based search 
Revised candidates Re-ranking phase 
Final results Search engine 
542
 
antees the transliteration candidates are all existing words. 3) Revised candidate re-ranking in web pages: In the module, we search the revised candi-dates to get their contexts and hit information which we can use to score the probability of being a transliteration name. This phase doesn?t generate new candidates, but re-rank the revised candidate set to improve the performance in top-5. Under this framework, we can solve the two problems of statistical model mentioned above.  (1) The silent syllables will be given lower weights in syllable-based search, so the missing syllables will be recovered through selecting the most similar existing words which can contain some silent syllables.  (2) The query expansion technology can recall more potential transliteration candidates by ex-panding syllables to their ?synonymies?. So the mistakes introduced when selecting syllables in statistical transliteration will be corrected through giving suitable weights to synonymies.  Through the revision phase, the results of statis-tical model which may have illegal spelling will be mapped to its most similar existing words. That can improve the recall. In re-ranking phase, the revised candidate set will be re-ranked to put the right answer on the top using hybrid information got from web resources. So the precision of trans-literation will be improved. 3 Statistical Transliteration Model We use syllables as translation units to build a sta-tistical Chinese-English backward transliteration model in our system. 3.1 Traditional Statistical Translation Model [P. Brown et al, 1993] proposed an IBM source-channel model for statistical machine translation (SMT). When the channel output f= f1,f2 ?. fn ob-served, we use formula (1) to seek for the original sentence e=e1,e2 ?. en with the most likely poste-riori. 
' argmax ( | ) argmax ( | ) ( )
e e
e P e f P f e P e= =
      (1) The translation model ( | )P f e  is estimated from a paired corpus of foreign-language sentences and their English translations. The language model ( )P e  is trained from English texts. 
3.2 Our Transliteration Model The alignment method is the base of statistical transliteration model. There are mainly two kinds of alignment methods: phoneme-based alignment [Knight and Graehl, 1998; Virga and Khudanpur, 2003] and grapheme-based alignment [Long Jiang, 2007]. In our system, we adopt the syllable-based alignment from Chinese pinyin to English syllables, where the syllabication rules mentioned in [Long Jiang et al, 2007] are used. For example, Chinese name ??/xi ?/er ?/dun? and its backward transliteration ?Hilton? can be aligned as follows. ?Hilton? is split into syllable sequence as ?hi/l/ton?, and the alignment pairs are ?xi-hi?, ?er-l?, ?dun-ton?.  Based on the above alignment method, we can get our statistical Chinese-English backward trans-literation model as, 
argmax ( | ) ( )
E
E p PY ES p ES=
            (2) Where, PY is a Chinese Pinyin sequence, ES is a English syllables sequence, ( | )p PY ES  is the probability of translating ES into PY, ( )p E S  is the generative probability of a English syllable lan-guage model. 3.3 The Difference between Backward Trans-literation and Traditional Translation Chinese-English backward transliteration has some differences from traditional translation. 1) We don?t need to adjust the order of sylla-bles when transliteration.  2) The language model in backward translitera-tion describes the relationship of syllables in words. It can?t work as well as the language model de-scribing the word relationship in sentences. We think that the crucial problem in backward transliteration is selecting the right syllables at every step. It?s very hard to obtain the exact an-swer only based on the statistical transliteration model. We will try to improve the statistical model performance with the assistance of mining web resources. 4 Mining Monolingual Web Pages to As-sist Backward Transliteration  In order to get assistance from monolingual Web resource to improve statistical transliteration, our 
543
 
method contains two main phases: ?revision? and ?re-ranking?. In the revision phase, transliteration candidates are revised using syllable-based search in the word list, which are generated by collecting the existing words in web pages. Because the proc-ess of named entity recognition may lose some NEs, we will reserve all the words in web corpus without any filtering. The revision process can im-prove the recall through correcting some mistakes in the transliteration results of statistical model. In the re-ranking phase, we search every revised candidate on English pages, score them according to their contexts and hit information so that the right answer will be given a higher rank.  4.1 Using Syllable-based Retrieval to Revise Transliteration Candidates In this section, we will propose two methods re-spectively for the two problems of statistical model mentioned in section 1.  4.1.1  Syllable-based retrieval model When we search a transliteration candidate tci in the word list, we firstly split it into syllables {es1,es2,?..esn}. Then this syllable sequence is used as a query for syllable-based searching.  We define some notions here. ? Term set T={t1,t2?.tk} is an orderly set of all syllables which can be viewed as terms.  ? Pinyin set P={py1,py2?.pyk} is an orderly set of all Pinyin.  ? An input word can be represented by a vec-tor of syllables {es1,es2,?..esn}.  We calculate the similarity between a translitera-tion result and each word in the list to select the most similar words as the revised candidates. The {es1,es2,?..,esn} will be transformed into a vector Vquery={t1,t2?.tk} where ti represents the ith term in T. The value of ti is equal to 0 if the ith term doesn?t appear in query. In the same way, the word in list can also be transformed into vector represen-tation. So the similarity can be calculated as the inner product between these two vectors.  We don?t use tf and idf conceptions as traditional information retrieval (IR) to calculate the terms? weight. We use the weight of ti to express the ex-pectation probability of ith term having pronuncia-tion. If the term has a lower probability of having pronunciation, its weight is low. So when we searching, the missing silent syllables in the results 
of statistical transliteration model can be recovered because such syllables have little impact on simi-larity measurement. The formula we used is as fol-lows. 
( , )
/
query word
word py
V V
Sim query word
L L
?
=
            (3) 
The numerator is the inner product of two vec-tors. The denominator is the length of word Lword divided by the length of Chinese pinyin sequence Lpy. In this formula, the more syllables in one word, the higher score of inner production it may get, but the word will get a loss for its longer length. The word which has the shortest length and the highest syllable hitting ratio will be the best. Another difference from traditional IR is how to deal with the order of the words in a query. Ac-cording to transliteration, the similarity must be calculated under the limitation of keeping order, which can?t be satisfied by current methods. We use the algorithm like calculating the edit distance between two words. The syllables are viewed as the units which construct a word. The edit distance calculation finds the best matching with the least operation cost to change one word to another word by using deletion/addition/insertion operations on syllables. But the complexity will be too high to afford if we calculate the edit distance between a query and each word in the list. So, we just calcu-late the edit distance for the words which get high score without the order limitation. This trade off method can save much time but still keep perform-ance. 4.1.2  Mining the Equivalent through Syllable Expansion In most collections, the same concept may be re-ferred to using different words. This issue, known as synonymy, has an impact on the recall of most information retrieval systems. In this section, we try to use the expansion technology to solve prob-lem II. There are three kinds of expansions to be explained below.  Syllable expansion based on phonetic similar-ity: The syllables which correspond to the same Chinese pinyin can be viewed as synonymies. For example, the English syllables ?din? and ?tin? can be aligned to the same Chinese pinyin ?ding?. Given a Chinese pinyin sequence {py1,py2,?..pyn} as the input of transliteration model, for every pyi, there are a set of syllables 
544
 
{es1, es2 ?.. esk} which can be selected as its translation. The statistical model will select the most probable one, while others containing the right answer are discarded. To solve this problem, we expand the query to take the synonymies of terms into consideration. We create an expansion set for each Chinese pinyin. A syllable esi will be selected into the expansion set of pyj based on the alignment probability P(esi|pyj) which can be ex-tracted from the training corpus. The phonetic similarity expansion is based on the input Chinese Pinyin sequence, so it?s same for all candidates. Syllable expansion based on syllable similar-ity: If two syllables have similar alignment prob-ability with every pinyin, we can view these two syllables as synonymy. Therefore, if a syllable is in the query, its synonymies should be contained too. For example, ?fea? and ?fe? can replace each other. To calculate the similarity, we first obtain the alignment probability P(pyj|esk) of every syllable. Then the distance between any two syllables will be calculated using formula (4). 
1
1
( , ) ( | ) ( | )
N
j k i j i k
i
Sim es es P py es P py es
N
=
=
?
(4) This formula is used to evaluate the similarity of two syllables in alignment. The expansion set of the ith syllable can be generated by selecting the most similar N syllables. This kind of expansion is conducted upon the output of statistical translitera-tion model. Syllable expansion based on syllable edit dis-tance: The disadvantage of last two expansions is that they are entirely dependent on the training set. In other word, if some syllables haven?t appeared in the training corpus, they will not be expanded. To solve the problem, we use the method of expan-sion based on edit distance. We use edit distance to measure the similarity between two syllables, one is in training set and the other is absent. Because the edit distance expansion is not very relevant to pronunciation, we will give this expansion method a low weight in combination. It works when new syllables arise.  Combine the above three strategies: We will combine the three kinds of expansion method to-gether. We use the linear interpolation to integrate them. The formulas are follows.   
(1 )
pre sy ed
S S S S? ? ?= ? + +                (5) 
(1 )
pre py ed
S S S S? ? ?= ? + +                (6) 
where Spre is the score of exact matching, Ssy is the score of expansion based on syllables similarity and Spy based on phonetic similarity. We will ad-just these parameters to get the best performance. The experimental results and analysis will be re-ported in section 5.3. 4.2 Re-Ranking the Revised Candidates Set using the Monolingual Web Resource In the first phase, we have generated the revised candidate set {rc1,rc2,?,rcn} from the word list us-ing the transliteration results as clues. The objec-tive is to improve the overall recall. In the second phase, we try to improve the precision, i.e. we wish to re-rank the candidate set so that the correct an-swer will be put in a higher rank. [Al-Onaizan et al, 2002] has proposed some methods to re-score the transliteration candidates. The limitation of their approach is that some can-didates are propbale not existing words, with which we will not get any information from web. So it can only re-rank the transliteration results to improve the precision of top-5. In our work, we can improve the recall of transliteration through the revising process before re-ranking. In this section, we employ the AdaBoost frame-work which integrates several kinds of features to re-rank the revised candidate set. The function of the AdaBoost classifier is to calculate the probabil-ity of the candidate being a NE. Then we can re-rank the revised candidate set based on the score. The features used in our system are as follows. NE or not: Using rci as query to search for monolingual English Web Pages, we can get the context set {Ti1, Ti2??Tin} of rci. Then for every Tik, we use the named entity recognition (NER) software to determine whether rci is a NE or not. If rci is recognized as a NE in some Tik, rci will get a score. If rci can?t be recognized as NE in any con-texts, it will be pruned. The hit of the revised candidate: We can get the hit information of rci from search engine. It is used to evaluate the importance of rci. Unlike [Al-Onaizan et al, 2002], in which the hit can be used to eliminate the translation results which contain illegal spelling, we just use hit number as a feature. The limitation of compound NEs: When trans-literating a compound NE, we always split them into several parts, and then combine their translit-eration results together. But in this circumstance, 
545
 
every part can add a limitation in the selection of the whole NE. For example: ??/xi?/la?/li ?  ?/ke?/lin?/dun? is a compound name. ??/xi?/la?/li? can be transliterate to ?Hilary? or ?Hilaly? and ??/ke?/lin?/dun? can be transliterate to ?Clinton? or ?Klinton?. But the combination of ?Hilary?Clinton? will be selected for it is the most common combination. So the hit of combination query will be extracted as a feature in classifier. Hint words around the NE: We can take some hint words around the NE into the query, in order to add some limitations to filter out noisy words. For example: ??? (president)? can be used as hint word for ???? (Clinton)?. To find the hint words, we first search the Chinese name in Chi-nese web pages. The frequent words can be ex-tracted as hint words and they will be translated to English using a bilingual dictionary. These hint words are combined with the revised candidates to search English web pages. So, the hit of the query will be extracted as feature. The formula of AdaBoost is as follow. 
1
( ) ( ( ))
T
t t
t
H x sign h x?
=
=
?
                 (7) 
Where 
t
?  is the weight for the ith weak classifier 
( )
t
h x . 
t
?  can be calculated based on the precision of its corresponding classifier. 5 Experiments We carry out experiments to investigate how much the revision process and the re-ranking process can improve the performance compared with the base-line of statistical transliteration model. We will also evaluate to which extents we can solve the two problems mentioned in section 1 with the as-sistance of Web resources. 5.1 Experimental data The training corpus for statistical transliteration model comes from the corpus of Chinese <-> Eng-lish Name Entity Lists v 1.0 (LDC2005T34). It contains 565,935 transliteration pairs. Ruling out those pairs which are not suitable for the research on Chinese-English backward transliteration, such as Chinese-Japanese, we select a training set which contains 14,443 pairs of Chinese-European & American person names. In the training set, 1,344 
pairs are selected randomly as the close test data. 1,294 pairs out of training set are selected as the open test data. To set up the word list, a 2GB-sized collection of web pages is used. Since 7.42% of the names in the test data don?t appear in the list, we use Google to get the web page containing the ab-sent names and add these pages into the collection. The word list contains 672,533 words. 5.2 Revision phase vs. statistical approach Using the results generated from statistical model as baseline, we evaluate the revision module in recall first. The statistical transliteration model works in the following 4 steps: 1) Chinese name are transformed into pinyin representation and the English names are split into syllables. 2) The GIZA++1 tool is invoked to align pinyin to sylla-bles, and the alignment probabilities ( | )P py es are obtained. 3) Those frequent sequences of syllables are combined as phrases. For example, ?be/r/g???berg?, ?s/ky???sky?. 4) Camel 2  de-coder is executed to generate 100-best candidates for every name. We compare the statistical transliteration results with the revised results in Table 2. From Table 2 we can find that the recall of top-100 after revision is improved by 13.26% in close test set and 17.55% in open test set. It proves that the revision module is effective for correcting the mistakes made in statistical transliteration model. Transliteration results Revised results  close open close open Top1 33.64% 9.41% 27.15% 11.04% Top5 40.37% 13.38% 42.83% 19.69% Top10 47.79% 17.56% 56.98% 26.52% Top20 61.88% 25.44% 71.05% 37.81% Top50 66.49% 36.19% 82.16% 46.22% Top100 72.52% 41.73% 85.78% 59.28% Table 2. Statistical model vs. Revision module To show the effects of the revision on the two above-mentioned problems in which the statistical model does not solve well: the losing of silent syl-lables and the selection bias problem, we make a statistics of the improvements with a measurement of ?correction time?. For a Chinese word whose correct transliteration appears in top-100 candidates only if it has been 
                                                           1 http://www.fjoch.com/GIZA++.html 2 http://www.nlp.org.cn 
546
 
revised, we count the ?correction time?. For exam-ple, when ?Argahi? is revised to ?Agassi? the cor-rection time is ?1? for Problem II and ?1? for Problem I, because in ?hi?? ?si? the syllable is expanded, and in ?si? ??ssi? an ?s? is added.   Close test Open test Problem I 0.6931 0.7853 Problem II 0.9264 1.1672 Table 3. Average time of correction This measurement reflects the efficiency of the revision of search strategy, in contrast to those spelling correction techniques in which several operations of ?add? and ?expand? are inevitable. It has proved that the more an average correction time is, the more efficient our strategy is.  
?
???
???
???
???
???
???
???
???
???
?
? ? ? ? ? ? ? ? ?
??????? ?????????  Figure 2. Length influence in recall comparison The recall of the statistical model relies on the length of English name in some degree. It is more difficult to obtain an absolutely correct answer for longer names, because they may contain more si-lent and confused syllables. However, through the revision phase, this tendency can be effectively alleviated. In Figure 2, we make a comparison be-tween the results of the statistical model and the revision module with the changing of syllable?s length in open test. The curves demonstrate that the revision indeed prevents the decrease of recall for longer names. 5.3 Parameter setting in the revision phase We will show the experimental results when set-ting different parameters for query expansion. In the expansion based on phonetic similarity, for every Chinese pinyin, we select at most 20 sylla-bles to create an expansion set. We set 0.1? =  in formula (5). The results are shown in the columns labeled ?exp1? in Table 4. From the results we can conclude that, we get the best performance when 
0.4? = . That means the performance is best when the weight of exact 
matching is a little larger than the weight of fuzzy matching. We can also see that, higher weight of exact matching will lead to low recall, while higher weight of fuzzy matching will bring noise in. The expansion method based on syllable similar-ity is also evaluated. For every syllable, we select at most 15 syllables to create the expansion set. We set 0.1? = . The results are shown in the columns labeled ?exp2? in Table 4. From the results we can conclude that, we get the best performance when 0.5? = . It means that we can?t put emphasis on any matching methods. Comparison with the expansion based on phonetic similarity, the performance is poorer. It means that the expansion based on phonetic similarity is more suitable for revising transliteration candidates. 5.4 Revision phase vs. re-ranking phase After the phase of revising transliteration candi-dates, we re-rank the revised candidate set with the assistance of monolingual web resources. In this section, we will show the improvement in preci-sion after re-ranking. We have selected four kinds of features to inte-grate in the AdaBoost framework. To determine whether the candidate is NE or not in its context, we use the software tool Lingpipe3. The queries are sent to google, so that we can get the hit of queries and the top-10 snippets will be extracted as context. The comparison of revision results and re-ranking results is shown as follows. Revised results Re-ranked results  close open close open Top1 27.15% 11.04% 58.08% 38.63% Top5 42.83% 19.69% 76.35% 52.19% Top10 56.98% 26.52% 83..92% 54.33% Top20 71.05% 37.81% 83.92% 57.61% Top50 82.16% 46.22% 83.92% 57.61% Top100 85.78% 59.28% 85.78% 59.28% Table 5. Revision results vs. Re-ranking results From these results we can conclude that, after re-ranking phase, the noisy words will get a lower 
                                                           3 http://www.alias-i.com/lingpipe/ 
547
 
0.2? =  0.3? =   0.4? =  0.5? =  0.6? =  0.7? =  0.8? =   exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 Top1 13.46 13.32 13.79 13.61 11.04 12.70 11.65 10.93 10.83 11.25 9.62 10.63 8.73 10.18 Top5 21.58 19.59 23.27 20.17 19.69 18.28 21.07 17.25 22.05 16.84 17.90 16.26 17.38 15.34 Top10 27.39 22.71 28.41 24.73 26.52 22.93 26.83 21.81 27.26 20.39 24.38 21.20 25.42 18.20 Top20 35.23 34.88 35.94 29.49 37.81 31.57 38.59 33.04 36.52 31.72 35.25 29.75 34.65 27.62 Top50 43.91 40.63 43.75 40.85 46.22 41.46 48.72 42.79 45.48 40.49 41.57 39.94 42.81 38.07 Top100 53.76 48.47 54.38 52.04 59.28 53.15 57.36 53.46 55.19 51.83 55.63 49.52 53.41 47.15 Table 4.  Parameters Experiment rank. Through the revision module, we get both higher recall and higher precision than statistical transliteration model when at most 5 results are returned. We also use the average rank and average recip-rocal rank (ARR) [Voorhees and Tice, 2000] to evaluate the improvement. ARR is calculated as       
1
1 1
( )
M
i
ARR
M R i
=
=
?
                             (8) where ( )R i  is the rank of the answer of ith test word. M is the size of test set. The higher of ARR, the better the performance is. The results are shown as Table 6. Statistical  model Revision  module Re-rank  Module  close open close open close open Average rank 37.63 70.94 24.52 58.09 16.71 43.87 ARR 0.3815 0.1206 0.3783 0.1648 0.6519 0.4492 Table 6. ARR and AR evaluation The ARR after revision phase is lower than the statistical model. Because the goal of revision module is to improve the recall as possible as we can, some noisy words will be introduced in. The noisy words will be pruned in re-ranking module. That is why we get the highest ARR value at last. So we can conclude that the revision module im-proves recall and re-ranking module improves pre-cision, which help us get a better performance than pure statistical transliteration model 6 Conclusion In this paper, we present a new approach which can revise the results generated from statistical transliteration model with the assistance of mono-lingual web resource. Through the revision process, the recall of transliteration results has been im-proved from 72.52% to 85.78% in the close test set and from 41.73% to 59.28% in open test set, re-spectively. We improve the precision in re-ranking phase, the top-5 precision can be improved to 76.35% in close test and 52.19% in open test. The 
promising results show that our approach works pretty well in the task of backward transliteration. In the future, we will try to improve the similar-ity measurement in the revision phase. And we also wish to develop a new approach using the transliteration candidates to search for their right answer more directly and effectively. Acknowledgments The work is supported by the National High Tech-nology Development 863 Program of China under Grants no. 2006AA01Z144, the National Natural Science Foundation of China under Grants No. 60673042, the Natural Science Foundation of Bei-jing under Grants no. 4073043. References  Yaser Al-Onaizan and Kevin Knight. 2002. Translating named entities using monolingual and bilingual re-sources. In Proc.of ACL-02.  Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration. Computational Linguistics 24(4). Wei-Hao Lin and Hsin-His Chen. 2002 Backward Ma-chine Transliteration by Learning Phonetic Similarity. In Proc. Of the 6th CoNLL Donghui Feng, Yajuan Lv, and Ming Zhou. 2004. A New Approach for English-Chinese Named Entity Alignment. In Proc. of EMNLP-2004. Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng Niu, 2007. Named Entity Translation with Web Min-ing and Transliteration. In Proc. of IJCAI-2007. Wei Gao. 2004. Phoneme-based Statistical Translitera-tion of Foreign Name for OOV Problem. A thesis of Master. The Chinese University of Hong Kong. Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining translations of OOV terms from the web through cross-lingual query expansion. SIGIR 2005. Pu-Jen Cheng, Wen-Hsiang Lu, Jer-Wen Teng, and Lee-Feng Chien. 2004 Creating Multilingual Transla-tion Lexicons with Regional Variations Using Web Corpora. In Proc. of ACL-04 Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001. Using the Web as a Bilingual Dictionary. In Proc. of ACL 2001 Workshop on Data-driven Methods in Machine Translation. 
548
 
Paola Virga and Sanjeev Khudanpur. 2003. Translitera-tion of proper names in cross-lingual information re-trieval. In Proc. of the ACL workshop on Multi-lingual Named Entity Recognition. Jenq-Haur Wang, Jei-Wen Teng, Pu-Jen Cheng, Wen-Hsiang Lu, Lee-Feng Chien. 2004. Translating un-known cross-lingual queries in digital libraries using a web-based approach. In Proc. of JCDL 2004. E.M.Voorhees and D.M.Tice. 2000. The trec-8 question answering track report. In Eighth Text Retrieval Con-ference (TREC-8) 
549
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 387?395,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Chinese-English Organization Name Translation System Using 
Heuristic Web Mining and Asymmetric Alignment 
 
 
Fan Yang, Jun Zhao, Kang Liu 
National Laboratory of Pattern Recognition  
 Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China 
{fyang,jzhao,kliu}@nlpr.ia.ac.cn 
  
  
 
Abstract 
In this paper, we propose a novel system for 
translating organization names from Chinese 
to English with the assistance of web 
resources. Firstly, we adopt a chunking-
based segmentation method to improve the 
segmentation of Chinese organization names 
which is plagued by the OOV problem. 
Then a heuristic query construction method 
is employed to construct an efficient query 
which can be used to search the bilingual 
Web pages containing translation 
equivalents. Finally, we align the Chinese 
organization name with English sentences 
using the asymmetric alignment method to 
find the best English fragment as the 
translation equivalent. The experimental 
results show that the proposed method 
outperforms the baseline statistical machine 
translation system by 30.42%. 
1 Introduction 
The task of Named Entity (NE) translation is to 
translate a named entity from the source language 
to the target language, which plays an important 
role in machine translation and cross-language 
information retrieval (CLIR). The organization 
name (ON) translation is the most difficult 
subtask in NE translation. The structure of ON is 
complex and usually nested, including person 
name, location name and sub-ON etc. For 
example, the organization name ???????
????? (Beijing Nokia Communication 
Ltd.)? contains a company name (???/Nokia) 
and a location name (??/Beijing). Therefore, 
the translation of organization names should 
combine transliteration and translation together.  
Many previous researchers have tried to solve 
ON translation problem by building a statistical 
model or with the assistance of web resources. 
The performance of ON translation using web 
knowledge is determined by the solution of the 
following two problems:  
 The efficiency of web page searching: how 
can we find the web pages which contain the 
translation equivalent when the amount of the 
returned web pages is limited? 
 The reliability of the extraction method: how 
reliably can we extract the translation equivalent 
from the web pages that we obtained in the 
searching phase?  
For solving these two problems, we propose a 
Chinese-English organization name translation 
system using heuristic web mining and 
asymmetric alignment, which has three 
innovations.  
1) Chunking-based segmentation: A Chinese 
ON is a character sequences, we need to segment 
it before translation. But the OOV words always 
make the ON segmentation much more difficult. 
We adopt a new two-phase method here. First, 
the Chinese ON is chunked and each chunk is 
classified into four types. Then, different types of 
chunks are segmented separately using different 
strategies. Through chunking the Chinese ON 
first, the OOVs can be partitioned into one chunk 
which will not be segmented in the next phase. In 
this way, the performance of segmentation is 
improved.  
2) Heuristic Query construction: We need to 
obtain the bilingual web pages that contain both 
the input Chinese ON and its translation 
equivalent. But in most cases, if we just send the 
Chinese ON to the search engine, we will always 
get the Chinese monolingual web pages which 
don?t contain any English word sequences, let 
alone the English translation equivalent. So we 
propose a heuristic query construction method to 
generate an efficient bilingual query. Some 
words in the Chinese ON are selected and their 
translations are added into the query. These 
English words will act as clues for searching 
387
bilingual web pages. The selection of the Chinese 
words to be translated will take into 
consideration both the translation confidence of 
the words and the information contents that they 
contain for the whole ON.  
3) Asymmetric alignment: When we extract the 
translation equivalent from the web pages, the 
traditional method should recognize the named 
entities in the target language sentence first, and 
then the extracted NEs will be aligned with the 
source ON. However, the named entity 
recognition (NER) will always introduce some 
mistakes. In order to avoid NER mistakes, we 
propose an asymmetric alignment method which 
align the Chinese ON with an English sentence 
directly and then extract the English fragment 
with the largest alignment score as the equivalent. 
The asymmetric alignment method can avoid the 
influence of improper results of NER and 
generate an explicit matching between the source 
and the target phrases which can guarantee the 
precision of alignment.  
In order to illustrate the above ideas clearly,  
we give an example of translating the Chinese 
ON ??????????? (China Huarong 
Asset Management Corporation)?.  
Step1: We first chunk the ON, where ?LC?, 
?NC?, ?MC? and ?KC? are the four types of 
chunks defined in Section 4.2. 
??(China)/LC  ??(Huarong)/NC  ????
(asset management)/MC  ??(corporation)/KC 
Step2: We segment the ON based on the 
chunking results.  
??(china)  ??(Huarong)  ??(asset)     
??(management)  ??(corporation) 
If we do not chunk the ON first, the OOV 
word ???(Huarong)? may be segmented as ??   
??. This result will certainly lead to translation 
errors. 
Step 3: Query construction:  
We select the words ???? and ???? to 
translate and a bilingual query is constructed as: 
? ? ? ? ? ? ? ? ? ? ? ? + asset + 
management 
If we don?t add some English words into the 
query, we may not obtain the web pages which 
contain the English phrase ?China Huarong Asset 
Management Corporation?. In that case, we can 
not extract the translation equivalent. 
Step 4: Asymmetric Alignment: We extract a 
sentence ??President of China Huarong Asset 
Management Corporation?? from the returned 
snippets. Then the best fragment of the sentence 
?China Huarong Asset Management 
Corporation? will be extracted as the translation 
equivalent. We don?t need to implement English 
NER process which may make mistakes. 
The remainder of the paper is structured as 
follows. Section 2 reviews the related works. In 
Section 3, we present the framework of our 
system. We discuss the details of the ON 
chunking in Section 4. In Section 5, we introduce 
the approach of heuristic query construction. In 
section 6, we will analyze the asymmetric 
alignment method. The experiments are reported 
in Section 7. The last section gives the 
conclusion and future work. 
2 Related Work 
In the past few years, researchers have proposed 
many approaches for organization translation. 
There are three main types of methods. The first 
type of methods translates ONs by building a 
statistical translation model. The model can be 
built on the granularity of word [Stalls et al, 
1998], phrase [Min Zhang et al, 2005] or 
structure [Yufeng Chen et al, 2007]. The second 
type of methods finds the translation equivalent 
based on the results of alignment from the source 
ON to the target ON [Huang et al, 2003; Feng et 
al., 2004; Lee et al, 2006]. The ONs are 
extracted from two corpora. The corpora can be 
parallel corpora [Moore et al, 2003] or content-
aligned corpora [Kumano et al, 2004]. The third 
type of methods introduces the web resources 
into ON translation. [Al-Onaizan et al, 2002] 
uses the web knowledge to assist NE translation 
and [Huang et al, 2004; Zhang et al, 2005; Chen 
et al, 2006] extracts the translation equivalents 
from web pages directly.  
The above three types of methods have their 
advantages and shortcomings. The statistical 
translation model can give an output for any 
input. But the performance is not good enough on 
complex ONs. The method of extracting 
translation equivalents from bilingual corpora 
can obtain high-quality translation equivalents. 
But the quantity of the results depends heavily on 
the amount and coverage of the corpora. So this 
kind of method is fit for building a reliable ON 
dictionary. In the third type of method, with the 
assistance of web pages, the task of ON 
translation can be viewed as a two-stage process. 
Firstly, the web pages that may contain the target 
translation are found through a search engine. 
Then the translation equivalent will be extracted 
from the web pages based on the alignment score 
with the original ON. This method will not 
388
depend on the quantity and quality of the corpora 
and can be used for translating complex ONs. 
3 The Framework of Our System 
The Framework of our ON translation system 
shown in Figure 1 has four modules.  
 
Figure 1. System framework 
1) Chunking-based ON Segmentation Module: 
The input of this module is a Chinese ON. The 
Chunking model will partition the ON into 
chunks, and label each chunk using one of four 
classes. Then, different segmentation strategies 
will be executed for different types of chunks. 
2) Statistical Organization Translation Module: 
The input of the module is a word set in which 
the words are selected from the Chinese ON. The 
module will output the translation of these words.  
3) Web Retrieval Module: When input a 
Chinese ON, this module generates a query 
which contains both the ON and some words? 
translation output from the translation module. 
Then we can obtain the snippets that may contain 
the translation of the ON from the search engine. 
The English sentences will be extracted from 
these snippets.  
4) NE Alignment Module: In this module, the 
asymmetric alignment method is employed to 
align the Chinese ON with these English 
sentences obtained in Web retrieval module. The 
best part of the English sentences will be 
extracted as the translation equivalent. 
4 The Chunking-based Segmentation 
for Chinese ONs  
In this section, we will illustrate a chunking-
based Chinese ON segmentation method, which 
can efficiently deal with the ONs containing 
OOVs. 
4.1 The Problems in ON Segmentation 
The performance of the statistical ON translation 
model is dependent on the precision of the 
Chinese ON segmentation to some extent. When 
Chinese words are aligned with English words, 
the mistakes made in Chinese segmentation may 
result in wrong alignment results. We also need 
correct segmentation results when decoding. But 
Chinese ONs usually contain some OOVs that 
are hard to segment, especially the ONs 
containing names of people or brand names. To 
solve this problem, we try to chunk Chinese ONs 
firstly and the OOVs will be partitioned into one 
chunk. Then the segmentation will be executed 
for every chunk except the chunks containing 
OOVs. 
4.2 Four Types of Chunks  
We define the following four types of chunks for 
Chinese ONs: 
 Location Chunk (LC): LC contains the 
location information of an ON. 
 Name Chunk (NC): NC contains the name   
or brand information of an ON. In most 
cases, Name chunks should be 
transliterated. 
 Modification Chunk (MC): MC contains 
the modification information of an ON. 
 Key word Chunk (KC): KC contains the 
type information of an ON. 
The following is an example of an ON 
containing these four types of chunks. 
??(Beijing)/LC ? ? ? (Peregrine)/NC
????(investment consulting)/MC  ????
(co.)/KC  
In the above example, the OOV ????
(Peregrine)? is partitioned into name chunk. Then 
the name chunk will not be segmented.  
4.3 The CRFs Model for Chunking 
Considered as a discriminative probabilistic 
model for sequence joint labeling and with the 
advantage of flexible feature fusion ability, 
Conditional Random Fields (CRFs) [J.Lafferty et 
al., 2001] is believed to be one of the best 
probabilistic models for sequence labeling tasks. 
So the CRFs model is employed for chunking. 
We select 6 types of features which are proved 
to be efficient for chunking through experiments. 
The templates of features are shown in Table 1,  
389
Description Features 
current/previous/success 
character C0?C-1?C1 
whether the characters is 
a word 
W(C
-2C-1C0)?W(C0C1C2)?
W(C
-1C0C1) 
whether the characters is 
a location name 
L(C
-2C-1C0)?L(C0C1C2)?    
L(C
-1C0C1) 
whether the characters is 
an ON suffix 
SK(C
-2C-1C0)?SK(C0C1C2)? 
SK(C
-1C0C1) 
whether the characters is 
a location suffix 
SL(C
-2C-1C0)?SL(C0C1C2)?
SL(C
-1C0C1) 
relative position in the 
sentence 
POS(C0) 
Table 1. Features used in CRFs model 
where Ci denotes a Chinese character, i denotes 
the position relative to the current character. We 
also use bigram and unigram features but only 
show trigram templates in Table 1. 
5 Heuristic Query Construction 
In order to use the web information to assist 
Chinese-English ON translation, we must firstly 
retrieve the bilingual web pages effectively. So 
we should develop a method to construct 
efficient queries which are used to obtain web 
pages through the search engine. 
5.1 The Limitation of Monolingual Query 
We expect to find the web pages where the 
Chinese ON and its translation equivalent co-
occur. If we just use a Chinese ON as the query, 
we will always obtain the monolingual web 
pages only containing the Chinese ON. In order 
to solve the problem, some words in the Chinese 
ON can be translated into English, and the 
English words will be added into the query as the 
clues to search the bilingual web pages. 
5.2 The Strategy of Query Construction  
We use the metric of precision here to evaluate 
the possibility in which the translation equivalent 
is contained in the snippets returned by the search 
engine. That means, on the condition that we 
obtain a fixed number of snippets, the more the 
snippets which contain the translation equivalent 
are obtained, the higher the precision is. There 
are two factors to be considered. The first is how 
efficient the added English words can improve 
the precision. The second is how to avoid adding 
wrong translations which may bring down the 
precision. The first factor means that we should 
select the most informative words in the Chinese 
ON. The second factor means that we should 
consider the confidence of the SMT model at the 
same time. For example: 
??/LC  ??/NC 
?
?? /MC ????/KC 
(Tianjin   Honda     motor           co. ltd.) 
There are three strategies of constructing 
queries as follows: 
Q1.?????????????  Honda 
Q2.?????????????  Ltd. 
Q3.???????????? ? Motor 
Tianjin 
In the first strategy, we translate the word ??
?(Honda)? which is the most informative word 
in the ON. But its translation confidence is very 
low, which means that the statistical model gives 
wrong results usually. The mistakes in translation 
will mislead the search engine. In the second 
strategy, we translate the word which has the 
largest translation confidence. Unfortunately the 
word is so common that it can?t give any help in 
filtering out useless web pages. In the third 
strategy, the words which have sufficient 
translation confidence and information content 
are selected.  
5.3 Heuristically Selecting the Words to be 
Translated 
The mutual information is used to evaluate the 
importance of the words in a Chinese ON. We 
calculate the mutual information on the 
granularity of words in formula 1 and chunks in 
formula 2. The integration of the two kinds of 
mutual information is in formula 3. 
y Y
p ( x ,y )( , ) = lo g
p ( x ) p ( y )M I W x Y ??
     (1) 
Y
p ( y , c )( , ) = lo g
p ( y ) p ( c )y
M I C c Y
?
?       (2) 
( , )= ( , )+(1- ) ( , )xIC x Y MIW x Y MIC c Y? ?     (3) 
Here, MIW(x,Y) denotes the mutual 
information of word x with ON Y. That is the 
summation of the mutual information of x with 
every word in Y. MIC(c,Y) is similar. cx denotes 
the label of the chunk containing x. 
We should also consider the risk of obtaining 
wrong translation results. We can see that the 
name chunk usually has the largest mutual 
information. However, the name chunk always 
needs to be transliterated, and transliteration is 
often more difficult than translation by lexicon. 
So we set a threshold Tc for translation 
confidence. We only select the words whose 
translation confidences are higher than Tc, with 
their mutual information from high to low. 
390
6 Asymmetric Alignment Method for 
Equivalent Extraction 
After we have obtained the web pages with the 
assistant of search engine, we extract the 
equivalent candidates from the bilingual web 
pages. So we first extract the pure English 
sentences and then an asymmetric alignment 
method is executed to find the best fragment of 
the English sentences as the equivalent candidate. 
6.1 Traditional Alignment Method 
To find the translation candidates, the traditional 
method has three main steps.  
1) The NEs in the source and the target 
language sentences are extracted separately. The 
NE collections are Sne and Tne. 
2) For each NE in Sne, calculate the alignment 
probability with every NE in Tne. 
3) For each NE in Sne, the NE in Tne which has 
the highest alignment probability will be selected 
as its translation equivalent. 
This method has two main shortcomings: 
1) Traditional alignment method needs the 
NER process in both sides, but the NER process 
may often bring in some mistakes. 
2) Traditional alignment method evaluates the 
alignment probability coarsely. In other words, 
we don?t know exactly which target word(s) 
should be aligned to for the source word. A 
coarse alignment method may have negative 
effect on translation equivalent extraction.                                                                                                                                                    
6.2 The Asymmetric Alignment Method 
To solve the above two problems, we propose an 
asymmetric alignment method. The alignment 
method is so called ?asymmetric? for that it 
aligns a phrase with a sentence, in other words, 
the alignment is conducted between two objects 
with different granularities. The NER process is 
not necessary for that we align the Chinese ON 
with English sentences directly.  
[Wai Lam et al, 2007] proposed a method 
which uses the KM algorithm to find the optimal 
explicit matching between a Chinese ON and a 
given English ON. KM algorithm [Kuhn, 1955] 
is a traditional graphic algorithm for finding the 
maximum matching in bipartite weighted graph. 
In this paper, the KM algorithm is extended to be 
an asymmetric alignment method. So we can 
obtain an explicit matching between a Chinese 
ON and a fragment of English sentence. 
A Chinese NE CO={CW1, CW2, ?, CWn} is a 
sequence of Chinese words CWi and the English 
sentence ES={EW1, EW2, ?, EWm} is a sequence 
of English words EWi. Our goal is to find a 
fragment EWi,i+n={EWi, ?, EWi+n} in ES, which 
has the highest alignment score with CO. 
Through executing the extended KM algorithm, 
we can obtain an explicit matching L. For any 
CWi, we can get its corresponding English word 
EWj, written as L(CWi)=EWj and vice versa. We 
find the optimal matching L between two phrases, 
and calculate the alignment score based on L. An 
example of the asymmetric alignment will be 
given in Fig2. 
 
Fig2. An example of asymmetric alignment 
In Fig2, the Chinese ON ???????? is 
aligned to an English sentence ?? the 
Agriculture Bank of China is the four??. The 
stop words in parentheses are deleted for they 
have no meaning in Chinese. In step 1, the 
English fragment contained in the square 
brackets is aligned with the Chinese ON. We can 
obtain an explicit matching L1, shown by arrows, 
and an alignment score. In step 2, the square 
brackets move right by one word, we can obtain a 
new matching L2 and its corresponding alignment 
score, and so on. When we have calculated every 
consequent fragment in English sentence, we can 
find the best fragment ?the Agriculture Bank of 
China? according to the alignment score as the 
translation equivalent.  
The algorithm is shown in Fig3. Where, m is 
the number of words in an English sentence and 
n is the number of words in a Chinese ON. KM 
algorithm will generate an equivalent sub-graph 
by setting a value to each vertex. The edge whose 
weight is equal to the summation of the values of 
its two vertexes will be added into the sub-graph. 
Then the Hungary algorithm will be executed in 
the equivalent sub-graph to find the optimal 
matching. We find the optimal matching between 
CW1,n and EW1,n first. Then we move the window 
right and find the optimal matching between 
CW1,n and EW2,n+1. The process will continue 
until the window arrives at the right most of the 
? [(The) Agriculture Bank (of) China] (is) (the) four 
??    ??      ?? 
 (The) Agriculture [Bank (of) China] (is) (the) four]? 
??    ??      ?? 
Step 1: 
Step 2: 
391
English sentence. When the window moves right, 
we only need to find a new matching for the new 
added English vertex EWend and the Chinese 
vertex Cdrop which has been matched with EWstart 
in the last step. In the Hungary algorithm, the 
matching is added through finding an augmenting 
path. So we only need to find one augmenting 
path each time. The time complexity of finding 
an augmenting path is O(n3). So the whole 
complexity of asymmetric alignment is O(m*n3). 
Algorithm: Asymmetric Alignment Algorithm 
Input: A segmented Chinese ON CO and an 
English sentence ES. 
Output: an English fragment EWk,k+n 
1. Let start=1, end=n, L0=null 
2. Using KM algorithm to find the optimal 
matching between two phrases CW1,n and 
EWstart,end based on the previous matching Lstart-
1. We obtain a matching Lstart and calculate the 
alignment score Sstart based on Lstart. 
3. CWdrop = L(EWstart)  L(CWdrop)=null. 
4. If (end==m) go to 7, else start=start+1, 
end=end+1. 
5. Calculate the feasible vertex labeling for the 
vertexes CWdrop and EWend 
6. Go to 2. 
7. The fragment EWk,k+n-1 which has the highest 
alignment score will be returned. 
Fig3. The asymmetric alignment algorithm 
6.3 Obtain the Translation Equivalent 
For each English sentence, we can obtain a 
fragment ESi,i+n which has the highest alignment 
score. We will also take into consideration the 
frequency information of the fragment and its 
distance away from the Chinese ON. We use 
formula (4) to obtain a final score for each 
translation candidate ETi and select the largest 
one as translation result.  
( )= + log( +1)+ log(1 / +1)i i i iS ET SA C D? ? ?  (4) 
Where Ci denotes the frequency of ETi, and Di 
denotes the nearest distance between ETi and the 
Chinese ON. 
7 Experiments 
We carried out experiments to investigate the 
performance improvement of ON translation 
under the assistance of web knowledge.  
7.1 Experimental Data 
Our experiment data are extracted from 
LDC2005T34. There are two corpora, 
ldc_propernames_org_ce_v1.beta (Indus_corpus 
for short) and ldc_propernames_indu 
stry_ce_v1.beta (Org_corpus for short). Some 
pre-process will be executed to filter out some 
noisy translation pairs. For example, the 
translation pairs involving other languages such 
as Japanese and Korean will be filtered out. 
There are 65,835 translation pairs that we used as 
the training corpus and the chunk labels are 
added manually. 
We randomly select 250 translation pairs from  
the Org_corpus and 253 translation pairs from 
the Indus_corpus. Altogether, there are 503 
translation pairs as the testing set. 
7.2 The Effect of Chunking-based 
Segmentation upon ON Translation  
In order to evaluate the influence of segmentation 
results upon the statistical ON translation system, 
we compare the results of two translation models. 
One model uses chunking-based segmentation 
results as input, while the other uses traditional 
segmentation results. 
To train the CRFs-chunking model, we 
randomly selected 59,200 pairs of equivalent 
translations from Indus_corpus and org_corpus. 
We tested the performance on the set which 
contains 6,635 Chinese ONs and the results are 
shown as Table-2. 
For constructing a statistical ON translation 
model, we use GIZA++1 to align the Chinese NEs 
and the English NEs in the training set. Then the 
phrase-based machine translation system 
MOSES2 is adopted to translate the 503 Chinese 
NEs in testing set into English. 
 Precision Recall F-measure 
LC 0.8083 0.7973 0.8028 
NC 0.8962 0.8747 0.8853 
MC 0.9104 0.9073 0.9088 
KC 0.9844 0.9821 0.9833 
All 0.9437 0.9372 0.9404 
Table 2. The test results of CRFs-chunking model 
We have two metrics to evaluate the 
translation results. The first metric L1 is used to 
evaluate whether the translation result is exactly 
the same as the answer. The second metric L2 is 
used to evaluate whether the translation result 
contains almost the same words as the answer, 
                                                          
1
 http://www.fjoch.com/GIZA++.html 
2
 http://www.statmt.org/moses/ 
392
without considering the order of words. The 
results are shown in Table-3: 
 chunking-based 
segmentation  
traditional 
segmentation 
L1 21.47% 18.29% 
L2 40.76% 36.78% 
Table 3. Comparison of segmentation influence 
From the above experimental data, we can see 
that the chunking-based segmentation improves 
L1 precision from 18.29% to 21.47% and L2 
precision from 36.78% to 40.76% in comparison 
with the traditional segmentation method. 
Because the segmentation results will be used in 
alignment, the errors will affect the computation 
of alignment probability. The chunking based 
segmentation can generate better segmentation 
results; therefore better alignment probabilities 
can be obtained.  
7.3 The Efficiency of Query Construction 
The heuristic query construction method aims to 
improve the efficiency of Web searching. The 
performance of searching for translation 
equivalents mostly depends on how to construct 
the query. To test its validity, we design four 
kinds of queries and evaluate their ability using 
the metric of average precision in formula 5 and 
macro average precision (MAP) in formula 6, 
1
1P r
N
i
i i
HA vera g e ec is io n
N S
=
= ?             (5) 
where Hi is the count of snippets that contain at 
least one equivalent for the ith query. And Si is 
the total number of snippets we got for the ith 
query, 
1= 1
1
( )
1 j
i
HN
j j
i
M A P
R iN H
=
= ??               (6) 
where R(i) is the order of snippet where the ith 
equivalent occurs. We construct four kinds of 
queries for the 503 Chinese ONs in testing set as 
follows: 
Q1: only the Chinese ON.  
Q2: the Chinese ON and the results of the 
statistical translation model.  
Q3: the Chinese ON and some parts? 
translation selected by the heuristic query 
construction method.  
Q4: the Chinese ON and its correct English 
translation equivalent.  
We obtain at most 100 snippets from Google 
for every query. Sometimes there are not enough 
snippets as we expect. We set ? in formula 4 at 
0.7?and the threshold of translation confidence 
at 0.05. The results are shown as Table 4.  
 Average 
precision 
MAP 
Q1 0.031 0.0527 
Q2 0.187 0.2061 
Q3 0.265 0.3129 
Q4 1.000 1.0000 
Table 4. Comparison of four types query 
Here we can see that, the result of Q4 is the 
upper bound of the performance, and the Q1 is 
the lower bound of the performance. We 
concentrate on the comparison between Q2 and 
Q3. Q2 contains the translations of every word in 
a Chinese ON, while Q3 just contains the 
translations of the words we select using the 
heuristic method. Q2 may give more information 
to search engine about which web pages we 
expect to obtain, but it also brings in translation 
mistakes that may mislead the search engine. The 
results show that Q3 is better than Q2, which 
proves that a careful clue selection is needed. 
7.4 The Effect of Asymmetric Alignment 
Algorithm 
The asymmetric alignment method can avoid the 
mistakes made in the NER process and give an 
explicit alignment matching. We will compare 
the asymmetric alignment algorithm with the 
traditional alignment method on performance. 
We adopt two methods to align the Chinese NE 
with the English sentences. The first method has 
two phases, the English ONs are extracted from 
English sentences firstly, and then the English 
ONs are aligned with the Chinese ON. Lastly, the 
English ON with the highest alignment score will 
be selected as the translation equivalent. We use 
the software Lingpipe3 to recognize NEs in the 
English sentences. The alignment probability can 
be calculated as formula 7: 
( , ) ( | )i j
i j
Score C E p e c= ??       (7) 
The second method is our asymmetric 
alignment algorithm. Our method is different 
from the one in [Wai Lam et al, 2007] which 
segmented a Chinese ON using an English ON as 
suggestion. We segment the Chinese ON using 
the chunking-based segmentation method. The 
English sentences extracted from snippets will be 
preprocessed. Some stop words will be deleted, 
such as ?the?, ?of?, ?on? etc. To execute the 
extended KM algorithm for finding the best 
alignment matching, we must assure that the 
vertex number in each side of the bipartite is the 
                                                          
3
 http://www.alias-i.com/lingpipe/ 
393
same. So we will execute a phrase combination 
process before alignment, which combines some 
frequently occurring consequent English words 
into single vertex, such as ?limited company? etc. 
The combination is based on the phrase pair table 
which is generated from phrase-based SMT 
system. The results are shown in Table 5: 
 Asymmetric 
Alignment 
Traditional 
method 
Statistical 
model 
Top1 48.71% 36.18% 18.29% 
Top5 53.68% 46.12% -- 
Table 5. Comparison the precision of alignment 
method 
From the results (column 1 and column 2) we 
can see that, the Asymmetric alignment method 
outperforms the traditional alignment method. 
Our method can overcome the mistakes 
introduced in the NER process. On the other 
hand, in our asymmetric alignment method, there 
are two main reasons which may result in 
mistakes, one is that the correct equivalent 
doesn?t occur in the snippet; the other is that 
some English ONs can?t be aligned to the 
Chinese ON word by word.  
7.5 Comparison between Statistical ON 
Translation Model and Our Method 
Compared with the statistical ON translation 
model, we can see that the performance is 
improved from 18.29% to 48.71% (the bold data 
shown in column 1 and column 3 of Table 5) by 
using our Chinese-English ON translation system. 
Transforming the translation problem into the 
problem of searching for the correct translation 
equivalent in web pages has three advantages. 
First, word order determination is difficult in 
statistical machine translation (SMT), while 
search engines are insensitive to this problem. 
Second, SMT often loses some function word 
such as ?the?, ?a?, ?of?, etc, while our method 
can avoid this problem because such words are 
stop words in search engines. Third, SMT often 
makes mistakes in the selection of synonyms. 
This problem can be solved by the fuzzy 
matching of search engines. In summary, web 
assistant method makes Chinese ON translation 
easier than traditional SMT method.  
8 Conclusion 
In this paper, we present a new approach which 
translates the Chinese ON into English with the 
assistance of web resources. We first adopt the 
chunking-based segmentation method to improve 
the ON segmentation. Then a heuristic query 
construction method is employed to construct a 
query which can search translation equivalent 
more efficiently. At last, the asymmetric 
alignment method aligns the Chinese ON with 
English sentences directly. The performance of 
ON translation is improved from 18.29% to 
48.71%. It proves that our system can work well 
on the Chinese-English ON translation task. In 
the future, we will try to apply this method in 
mining the NE translation equivalents from 
monolingual web pages. In addition, the 
asymmetric alignment algorithm also has some 
space to be improved. 
Acknowledgement 
The work is supported by the National High 
Technology Development 863 Program of China 
under Grants no. 2006AA01Z144, and the 
National Natural Science Foundation of China 
under Grants no. 60673042 and 60875041. 
 
 
References  
Yaser Al-Onaizan and Kevin Knight. 2002. 
Translating named entities using monolingual and 
bilingual resources. In Proc of ACL-2002.  
Yufeng Chen, Chenqing Zong. 2007. A Structure-
Based Model for Chinese Organization Name 
Translation. In Proc. of ACM Transactions on 
Asian Language Information Processing (TALIP) 
Donghui Feng, Yajuan Lv, Ming Zhou. 2004. A new 
approach for English-Chinese named entity 
alignment. In Proc. of  EMNLP 2004. 
Fei Huang, Stephan Vogal. 2002. Improved named 
entity translation and bilingual named entity 
extraction. In Proc. of the 4th IEEE International 
Conference on Multimodal Interface. 
Fei Huang, Stephan Vogal, Alex Waibel. 2003. 
Automatic extraction of named entity translingual 
equivalence based on multi-feature cost 
minimization. In Proc. of the 2003 Annual 
Conference of the ACL, Workshop on Multilingual 
and Mixed-language Named Entity Recognition 
Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 
2001. Using the Web as a Bilingual Dictionary. In 
Proc. of ACL 2001 Workshop on Data-driven 
Methods in Machine Translation. 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Proc. of 
ACL 2005. 
Conrad Chen, Hsin-His Chen. 2006. A High-Accurate 
Chinese-English NE Backward Translation System 
Combining Both Lexical Information and Web 
Statistics. In Proc. of ACL 2006. 
394
Wai Lam,  Shing-Kit Chan. 2007. Named Entity 
Translation Matching and Learning: With 
Application for Mining Unseen Translations. In 
Proc. of ACM Transactions on Information 
Systems.  
Chun-Jen Lee, Jason S. Chang, Jyh-Shing R. Jang. 
2006. Alignment of bilingual named entities in 
parallel corpora using statistical models and 
multiple knowledge sources. In Proc. of ACM 
Transactions on Asian Language Information 
Processing (TALIP). 
Kuhn, H. 1955. The Hungarian method for the 
assignment problem. Naval Rese. Logist. Quart 
2,83-97. 
Min Zhang., Haizhou Li, Su Jian, Hendra Setiawan. 
2005. A phrase-based context-dependent joint 
probability model for named entity translation. In 
Proc. of the 2nd International Joint Conference on 
Natural Language Processing(IJCNLP)  
Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining 
translations of OOV terms from the web through 
cross-lingual query expansion. In Proc. of the 28th 
ACM SIGIR. 
Bonnie Glover Stalls and Kevin Knight. 1998. 
Translating names and technical terms in Arabic 
text. In Proc. of the COLING/ACL Workshop on 
Computational Approaches to Semitic Language. 
J. Lafferty, A. McCallum, and F. Pereira. 2001.  
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data. In Proc. 
ICML-2001. 
Tadashi Kumano, Hideki Kashioka, Hideki Tanaka 
and Takahiro Fukusima. 2004. Acquiring bilingual 
named entity translations from content-aligned 
corpora. In Proc. IJCNLP-04. 
Robert C. Moore. 2003. Learning translation of 
named-entity phrases from parallel corpora. In Proc. 
of 10th conference of the European chapter of ACL.  
 
 
 
 
395
Chinese Named Entity Recognition Combining a Statistical Model with 
Human Knowledge 
Youzheng WU Jun ZHAO Bo XU 
National Laboratory of Pattern Recognition 
Institute of Automation Chinese Academy of Sciences 
No.95 Zhongguancun East Road, 100080, Beijing, China 
(yzwu, jzhao,boxu)@nlpr.ia.ac.cn 
 
 
Abstract 
Named Entity Recognition is one of the 
key techniques in the fields of natural 
language processing, information retrieval, 
question answering and so on. 
Unfortunately, Chinese Named Entity 
Recognition (NER) is more difficult for 
the lack of capitalization information and 
the uncertainty in word segmentation. In 
this paper, we present a hybrid algorithm 
which can combine a class-based 
statistical model with various types of 
human knowledge very well. In order to 
avoid data sparseness problem, we 
employ a back-off model and?????
?/TONG YI CI CI LIN? , a Chinese 
thesaurus, to smooth the parameters in the 
model. The F-measure of person names, 
location names, and organization names 
on the newswire test data for the 1999 
IEER evaluation in Mandarin is 86.84%, 
84.40% and 76.22% respectively. 
1 Introduction 
The NER task was first introduced as Message 
Understanding Conference (MUC) subtask in 1995 
(MUC-6). Named Entities were defined as entity 
names (organizations, persons and locations), 
temporal expressions (dates and times) and number 
expressions (monetary values and percentages). 
Compared with the entity name recognition, the 
recognition of temporal and number expressions is 
simpler. So, our research focuses on the 
recognition of person, location and organization 
names. 
The Multilingual NE task first started in 
1995(MET-1), including Chinese, Japanese, and 
Spanish in that year, and continued for Chinese, 
Japanese in 1998(MET-2). Compared with English 
NER, Chinese NER is more difficult. We think the 
main differences between Chinese NER and 
English NER lie in:  
First, unlike English, Chinese lacks the 
capitalization information that plays an important 
role in signaling named entities.  
Second, there is no space between words in 
Chinese, and we have to segment the text before 
NER. However, the errors in word segmentation 
will affect the result of NER. 
Third, Different types of named entities have 
different structures, especially for abbreviative 
entities. Therefore, a single unified model can?t 
capture all the types of entities. Typical structures 
of Chinese person name (CN), location name (LN) 
and organization name (ON) are as follows: 
CN--><surname> <given name> 
LN--><name part>* <a salient word> 
ON-->{[person name] [organization name] [place 
name] [kernel name] }*  [organization type] <a 
salient word> 
Here <>* means repeating one or several times. 
{}* means selecting at least one of items. 
Fourth, there are few openly available resources 
for Chinese NER. Thus we have to resort to the 
algorithm that doesn?t rely on large NER-tagged 
text corpus. 
Based on the above analysis, we present a 
hybrid algorithm that incorporating various types 
of human knowledge into a statistical model. The 
innovative points of our paper are as follows. 
First, the hybrid algorithm can make the best 
use of existing limited resources to develop an 
effective NER system. These resources include 
one-month?s Chinese People?s Daily tagged with 
NER tags by Peking University (which contains 
about two-million Chinese characters) and various 
types of human knowledge. 
Second, in order to compensate for the lack of 
labeled corpus, we use several types of human 
knowledge, such as??????/TONG YI CI 
CI LIN? [Mei.J.J, et al 1983], a general location 
names list, the list of the salient words in location 
name, the list of the salient words in organization 
names, a Chinese surnames list, the list of Chinese 
characters that could be included in transliterated 
person names, and so on. 
Third, we emphasize that human knowledge 
and statistical information should be combined 
very well. For example, a general LN list and a 
general famous ON list are used in our system. 
However, we only accept words in the lists as 
entity candidates with a probability. Whether it is 
a LN or ON depends on the context. This is 
different from other systems which accept them as 
a LN or ON once the system meets them. More 
details refer to section 4. 
This paper will be organized as follows. Section 
2 is the background of NER. Section 3 describes 
the class-based statistical baseline Chinese NER 
model. Section 4 describes different types of 
human knowledge for different named entities 
recognitions and how to combine them with a 
statistical model organically in details. Section 5 is 
the evaluation and section 6 is the conclusion. 
2 Backgroud 
The researches on English NER have made 
impressive achievement. The best NER system 
[Mikheev, et al 1999] in MUC7 achieved 95% 
precision and 92% recall. Recent methods for 
English NER focus on machine-learning 
algorithms such as DL-CoTrain, CoBoost [Collins 
and Singer 1999], HMM [Daniel M. Bikel 1997], 
maximum entropy model [Borthwick, et al 1999] 
and so on. 
However, Chinese NER is still at its immature 
phase. Typical Chinese NER systems are as 
follows. 
NTU system [Hsin-His Chen, et al 1997] relied 
on a statistical model when recognizing person 
names, but rules when recognizing location and 
organization names. In the formal run of MET-2, 
the total F-measure is 79.61%. As a result, they 
may miss the person names whose probability is 
lower than the threshold, the location and 
organization names may also be missed for those 
which don?t accord with the rules. 
[Yu et al 1998] uses both a contextual model 
and a morphological model. However, their system 
requires information of POS tags, semantic tags 
and NE lists. The system obtains 86.38% F-
measure. 
[CHUA et al 2000] employs a combination of 
template-based rules supplemented by the default-
exception trees and decision tree that obtains over 
91% F-measure on MET-2 test data. It also uses 
HowNet [Dong & Dong 2000] to cluster 
semantically related words. 
[Jian Sun, 2002] presents a class-based 
language model for Chinese NER which achieves 
81.79% F-measure on MET-2 test set and 78.75% 
F-measure on IEER test data. However, the model 
heavily depends on statistical information, and 
must be trained on large labeled corpus. 
For Chinese NER, we can?t achieve satisfactory 
performance if we use only a statistical model or 
handcrafted heuristic rules. Therefore, we have to 
resort to the algorithm that can incorporate human 
knowledge into a statistical model. 
In the following sections, we will introduce a 
statistical Chinese NER model first, and then 
incorporate various types of human knowledge into 
the statistical model in order to show the power of 
human knowledge for Chinese NER. 
3 The Baseline Class-based Statistical 
Model 
We regard NER as a tagging problem. Given a 
sequence of Chinese string nwwwW L21= , the task 
of NER is to find the most likely sequence of class 
sequence ( )nmcccC m <== L21*  that maximizes 
the probability ( )WCP | . We use Bayes? Rule to 
rewrite ( )WCP |  as equation (3.1): 
( ) ( )( )
( )
( )WP
CPCWP
WP
WCPWCP ?== )|(,|             (3.1) 
So, the class-based baseline model can be 
expressed as equation (3.2). 
( ) ( )( )
( ) ( )( )
( ) ( )???
????
? ??
?=
?=
?
=
?
m
i
iiiiji
C
mmn
C
C
ccPcwwP
cccPcccwwwP
CPCWPC
1
11
212121
||maxarg
|maxarg
|maxarg*
L
LLL (3.2) 
We call ( )CP  as the contextual model and 
( )CWP |  as the morphological model. Formally, we 
can regard such a class-based statistical model as 
HMM. The classes used in our model are shown in 
Table 1, where |V| means the size of vocabulary 
used for word segmentation. 
Class Description 
PN Person Name 
LN Location Name 
ON Organization Name 
TM Time Name 
NM Number Name 
Other One word is on Class 
Total |V| + 5 
Table 1 Classes used in our model 
3.1 Contextual Model 
Due to our small-sized labeled corpus, we use a 
statistical bi-gram language model as the 
contextual model. This model can be described as 
equation (3.3). 
( ) ( )?=
=
??
mi
i
ii ccPCP
1
1|                                       (3.3) 
Theoretically, trigram is more powerful for 
NER than bi-gram, however when training corpus 
is small, trigram can?t work effectively. Using bi-
gram model, we still need ( )25+V  transmission 
probabilities, some of which can?t be observed in 
our small-sized labeled corpus and some of which 
are unauthentic. That is, data sparseness is still 
serious. We will explain how to resolve data 
sparseness problem in details in section 3 and 4. 
3.2 Morphological Model 
Recognition of Person Names 
The model of person names recognition 
(including Chinese person names abbreviated to 
CN and Transliterated person names abbreviated to 
TN) is a character-based tri-states unigram model. 
In principle, Chinese person name is composed 
of a surname (including single-character surname 
like "?/wu" and double-character surname like"?
? /Ouyang") and a given name (one or two 
characters like "?/peng" or "??/youzheng"). So 
we divide Chinese name words into three parts as 
the surname (surCN), the middle name (midCN) 
and the end name (endCN), which means the 
probability of a specific character used in different 
position in person names isn?t equal. For example, ( ) ( )
( )endCNc|?/wu
CNsc|?/wusurCNc|?/wu
j
jj
=?
=?=
P
ecPP         (3.4) 
The model for three-character-CN recognition 
is described as equation (3.5). ( )
( ) ( )
( )endCNcwP
midCNcwPsurCNcwP
CNcwwwP
jj
jjjj
jjjj
=?
=?=?
=
|
||
|
3
21
321
        (3.5) 
The model for two-character-CN recognition is 
described as equation (3.6). ( )
( ) ( )endCNwPsurCNwP
CNcwwP
jj
jjj
||
|
21
21
??
=        (3.6) 
where ( )CNcwwwP jjjj =|321  means the probability 
of emitting the candidate person name 321 jjj www  
under the state of CN. 
For TN, we don?t divide transliterated name 
words into several different parts. That is, the 
probability of a word used in different position in 
TN is same. The model is as follows. 
( ) ( )?=
=
=?=
ki
i
jjijjkjj TNcwPTNcwwwP
1
21 ||L (3.7) 
Must be mentioned is that all these probabilities 
are estimated from labeled corpus using maximum 
likelihood estimation. 
Recognition of Location Names 
For location names recognition, we use a word-
based bi-state unigram model, and divide words 
used in the location name into two parts: location-
end-words (LE) and non-location-end words 
(NLE). That means the probability of the word 
used in the end position of location name is 
different from that of in other position. 
The model for location name recognition is 
shown in equation (3.8). ( )
( ) ( )LEcwPNLEcwP
LNcwwwP
jjk
ki
i
jji
jjkjj
=?=
?=
??=
=
||
|
1
1
21 L
           (3.8) 
The parameters in equation (3.8) are also 
estimated from labeled training corpus. 
Recognition of Organization Names 
For the model of organization names 
recognition, we use bi-state unigram that is similar 
to the location morphological model shown as 
equation (3.9): ( )
( ) ( )NOEcwPOEcwP
ONcwwwP
jjk
ki
i
jji
jjkjj
=?=
==
??=
=
||
|
1
1
21 L
           (3.9) 
where OE means the word used in the end position 
of organization name, while NOE is not. 
The parameters in equation (3.9) are also 
estimated from the labeled training corpus. 
Back-off Models to Smooth 
Data sparseness problem still exists. As some 
parameters were never observed in trained corpus, 
the model will back off to a less-powerful model. 
We employ escape probability to smooth the 
statistical model [Teahan, et al 1999]. 
An escape probability is the probability that a 
previously unseen character will occur. There is no 
theoretical basis for choosing the escape 
probability optimally. Here we estimate the escape 
probability in a particular context as: 
n
d5.0=?                                                      (3.10) 
The probability of a word ci that has occurred c 
times in that context ci-1 is: 
( )
n
cccP ii
5.0| 1
?=?                                        (3.11) 
While the probability of a word that has never 
occurred in that context is: 
( ) ( )iii cPccP ?=? ?1|                                     (3.12) 
where n is the number of times that context has 
appeared and d is the number of different symbols 
that have directly followed it. 
As a example, if we observe the bi-gram "A B" 
once in training corpus and ?A C" three times, and 
nowhere else did we see the word "A", then 
( )
31
5.03| +
?=ACP , while the escape probability 
31
25.0
+
?=?  and unseen transition probability of 
( ) ( )DPADP ?= ?| . 
The Evaluation for the Baseline 
The baseline model was evaluated in terms of 
precision (P), recall (R) and F-measure (F) metrics.  
responsesofnumber
responsescorrectofnumberP =  
NEallofnumber
responsescorrectofnumberR =  
( )
( ) RP
RPF +?
??+= ?
? 12                                       (3.13) 
where ?  is a weighted constant often set to 1. 
We test the baseline system on the newswire 
test data for the 1999 IEER evaluation in Mandarin 
(http://www.nist.gov/speech/tests/ie-r/er_99/er_ 99. 
htm). Table 2 in section 4 summarizes the result of 
baseline model. 
 Precision Recall F-measure
PN 80.23% 89.55% 84.63% 
LN 45.05% 66.96% 53.86% 
ON 42.98% 61.45% 50.58% 
Total 52.61% 71.53% 60.63% 
Table 2 The Performance of The Baseline 
4 The Hybrid Model Incorporating 
Human Knowledge into the Baseline 
From table 1, we find that the performance of 
the above statistical baseline model isn?t 
satisfactory. The problems mainly lie in: 
? Data sparseness is still serious though we 
only use bi-gram contextual model, unigram 
morphological model and smooth the parameters 
with a back-off model.  
? In order to recognize the named entities, 
we have to estimate the probability of every word 
in text as named entities. Thus redundant 
candidates not only enlarge search space but also 
result in many unpredictable errors.  
? Abbreviative named entities especially 
organization abbreviation can?t be resolved by the 
baseline model. Because abbreviations have weak 
statistical regularities, so can?t be captured by such 
a baseline model. 
We try to resolve these problems by 
incorporating human knowledge. In fact, human 
being usually uses prior knowledge when 
recognizing named entities. In this section, we 
introduce the human knowledge that is used for 
NER and the method of how to incorporate them 
into the baseline model.  
Given a sequence of Chinese characters, the 
recognition process after combined with human 
knowledge consists of the five steps shown in Figure1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1 Recognition Process of the Hybrid Model 
 
4.1 Incorporate Knowledge for Person Name 
Recognition 
Chinese person names are composed of a 
surname and a given name. Usually the characters 
used for Chinese person names are limited. 
[Maosong Sun, Changning Huang, 1994] presents 
365 most high frequently used surnames cover 
99% Chinese surnames. 1141 most high frequently 
used characters cover 99% Chinese given names. 
Similarly the characters used for transliterated 
names are also limited. We extract about 476 
transliterated characters from the training corpus. 
The following is the human knowledge used for 
person name recognition and the method of how to 
incorporate them into the baseline. 
? A Chinese single and plural surname list: 
Only those characters in the surname list can 
trigger person name recognition. 
? A list of person title list: Only when the 
current character belongs to the surname list and 
the next word is in the title list, candidates are 
accepted. 
? A transliterated character list: Only 
those consecutive characters in the transliterated 
character list form a candidate transliterated name. 
? Person name can?t span any punctuation 
and the length of CN can?t exceed 8 characters 
while the length of TN is unrestrained. 
All these knowledge are used for restricting 
search space. 
4.2 Incorporate Knowledge for Location 
Name Recognition 
A complete location name is composed of the 
name part and a salient word. For the location 
name "???/Beijing City", the name part is "?
? /Beijing" and the salient word is "? /city". 
Unfortunately, the salient word is omitted in many 
occasions. So it is unfeasible to trigger LN 
recognition only depending on the salient words in 
location name. In order to improve the precision 
and recall of LN recognition, we use the following 
human knowledge. The method of incorporating 
them is also explained. 
? A general location name list: The list 
includes the names of Chinese provinces and 
counties, foreign country and its capitals, some 
famous geographical names and foreign cities. If 
the current word is in the list, we accept it as a 
candidate LN. 
? A location salient word list: If the word 
wi belongs to the list, 2~6 words before the salient 
word are accepted as candidate LNs. 
? A general word list (such as verbs and 
prepositions) which usually is followed by a 
location name, such as "? /at", "? /go". If the 
word wi is in the list, 2~6 words following it are 
accepted as candidate LNs. 
? An abbreviative location name list: If the 
current word is in the list, we accept it as a 
candidate LN such as "?/China", "?/America". 
PN and LN 
Generate 
NE Candidates
Recognition Nested 
Organization Names
Named Entities
Human Knowledge 
TONG YI CI CI LIN 
Extract 
Organization Kernel 
Word Segmentation
Nested Organization 
Name Templates
NE Pools
Text
Search the Max. 
P(C|W)
? Coordinate LN recognition: If wi-2 is a 
candidate LN and wi-1 is "? "(a punctuation 
denoting coordinate relation), LN recognition is 
triggered at the position of word wi. 
? Location name can?t span punctuations and 
its length couldn?t exceed 6 words. 
Knowledge ?, ?, ?, ?, ? can restrict 
search space while knowledge ? deals with 
abbreviative location name. 
4.3 Incorporate Knowledge for Organization 
Name Recognition 
The organization names recognition is the most 
difficult task. The reasons lie in nested ONs and 
abbreviative ONs especially. 
Nested ON means there are one or more 
location names, person names and/or organization 
names embedded in organization name. Typical 
structure of ON has been given in section 1. We 
can capture most of the nested organization names 
by several ON templates mentioned in the 
following section. 
Abbreviative ONs include continuous and 
discrete abbreviation which omits some words in 
the full name. Take "????????????
" as example, abbreviative ON of it may omit LN "
?? /Shanghai", organization types like"??
/supermarket", "??/stock", "??/limited", and 
salient word like "??/company" from full names 
but usually remains organization kernel "??
/Hualian". Table 3 lists some examples of 
abbreviative ONs. 
?????? 
?????? 
Shanghai Hualian 
Co.,Ltd 
????
Shanghai
Hualian Continuous Abbreviation 
???? 
Tsinghua niversity 
?? 
Tsinghua
??????? 
Shanghai Stock 
Exchange 
?? 
Shanghai 
Stock Discrete Abbreviation ???? 
Peking University 
?? 
Bei Da 
Table 3 Nest Organization Full Names and Its 
Abbreviative Names 
So it is important to extract organization kernel 
from the full name in order to recognize 
abbreviative ON like "????". Moreover, an 
organization's abbreviative names usually occur 
after its' full name, unless it is a well-known 
organization. So this strategy for abbreviation 
organization name recognition is effective. 
The following is the human knowledge used for 
ON recognition and the method of how to 
incorporate them. 
? An organization salient word (OrgSws) 
list: If the current word wi is in OrgSws list, 2~6 
words before OrgSw are accepted as the candidate 
ONs. 
? A general famous organization name list: 
If the current word is in the list, we accept it as a 
candidate ON such as "???/ State Department", 
"???/ U.N. ". 
? An organization names template list: We 
mainly use organization name templates to 
recognize the nested ONs. Some of these templates 
are as follows: 
ON-->LN D* OrgSw 
ON-->PN D* OrgSw 
ON-->ON OrgSw 
D means words used in the middle of organization 
names. D* means repeating zero or more times. 
This component runs in the end stage of 
recognition process shown in Figure 1. 
? An organization type list: The list is used 
to extract organization kernels from recognized 
ONs. We have a pool which memorizes ONs 
recognized in current paragraph and its kernel. If 
the current word belongs to organization kernel in 
pool, we accept it as a candidate ON. The idea is 
effective especially in financial domain which 
contains many stocks such as"????/Shanghai 
Hualian", "????/Changjiang Technology". 
Knowledge ?, ?, ? restrict search space 
while knowledge ? deals with abbreviative 
organization name. 
4.4 Semantic Similarity Computation for 
Data Sparseness 
??????/TONG YI CI CI LIN?classifies 
the words in terms of semantic similarity. Here we 
use it to resolve data sparseness problem. If current 
transmission probability doesn?t exist, we resort to 
its synonym transmission. In statistical sense, 
synonym transmissions are approximate. Take an 
example, the probability of P(A|B) doesn?t exist, 
but there has P(C|B), meanwhile, the word A and 
C are thesaurus according to ??????/TONG 
YI CI CI LIN?, then we use P(C|B) to replace 
P(A|B). 
5 Results of Evaluation 
We also test our hybrid model on IEER-99 neswire 
test data. The performance is shown in Table 4. 
 Precision Recall F-measure
PN 83.30% 92.28% 87.56% 
LN 88.31% 84.69% 86.47% 
ON 84.49% 71.08% 77.21% 
Total 86.09% 83.18% 84.61% 
Table 4 The Performance of the Hybrid Model 
Comparing Table 1 with 4, we find that the 
performance of the hybrid model increases 
remarkably. More specifically, the precision and 
the recall of PNs increase from 80.23% to 83.30% 
and from 89.55% to 92.28% respectively. The 
precision and recall of LNs increase from 45.05% 
to 82.18% and from 66.96% to 86.74% 
respectively. The precision and recall of ONs 
increase from 42.98% to 80.86% and from 61.45% 
to 72.09% respectively. The reason that the 
improvement of PNs is slighter than that of ONs 
and LNs is that the statistical information 
estimated from labeled corpus for PNs is good 
enough but not for LNs and ONs. 
Must be mentioned is that, in our evaluation, 
only NEs with both correct boundary and correct 
type label are considered as the correct 
recognitions, which is a little different from other 
evaluation systems. 
We also test our system on data set of sport, 
finance, news and entertainment domains. These 
test data are downloaded from Internet shown in 
Table 4.  
Number of NE 
Domain 
PN LN ON
File 
size 
Sport(S) 954 510 609 91K
Finance(F) 212 406 461 80K
News(N) 526 961 437 76K
Entertainment(E) 1016 511 133 100K
Total 2708 2388 1640 247K
Table 4 Statistic of Multi-field Test Data 
The results are shown in Table 5. 
 Precision Recall F-measure
S 80.17% 91.10% 85.28% 
F 61.35% 94.34% 74.35% 
N 88.66% 83.27% 85.88% 
PN 
E 82.20% 82.28% 82.24% 
S 82.90% 81.76% 82.33% 
F 83.72% 81.03% 82.35% 
N 91.95% 91.56% 91.75% 
LN
E 81.64% 87.87% 84.64% 
S 73.43% 67.16% 70.15% 
F 65.88% 60.30% 62.97% 
N 92.52% 84.70% 88.44% 
ON
E 78.30% 62.41% 69.46% 
Total 81.01% 81.24% 81.12% 
Table 5 Results on different domain 
Table 5 shows that the performance on financial 
domain is much lower. The reason is that, in 
financial domain, there are many stock names 
which are the abbreviation of organization names. 
Moreover, organization full name never appear in 
the text. So the system can?t recognize them as an 
organization name. However, on many occasions, 
they are recognized as person names. As a result, 
the precision of PNs declines, meanwhile, the 
precision and recall of ONs can?t be high. 
Based on the above analysis, we find that the 
main sources of errors in our system are as follows. 
First, we still have not found a good strategy for 
the abbreviation location names and organization 
names. Because abbreviative LNs and ONs 
sometimes appear before full LN, sometimes not, 
so the pool strategy can?t work well. 
Second, some famous organization names that 
always appear in the shape of abbreviation can?t be 
recognized as ON because the full name never 
appear such as ?? /GaoTong, ?? /Xinlang. 
However, these ONs are often recognized as PNs. 
Such errors are especially serious in finance 
domain shown Table 5. 
Third, many words can?t be found in ????
??/TONG YI CI CI LIN?. 
6 Conclusions 
Chinese NER is a more difficult task than English 
NER. Though many approaches have been tried, 
the result is still not satisfactory. In this paper, we 
present a hybrid algorithm of incorporating human 
knowledge into statistical model. Thus we only 
need a relative small-sized labeled corpus (one-
month?s Chinese People?s Daily tagged with NER 
tags at Peking University) and human knowledge, 
but can achieve better performance. The main 
contribution of this paper is putting forward an 
approach which can make up for the limitation of 
using the statistical model or human knowledge 
purely by combining them organically. 
Our lab was mainly devoted to cross-language 
information processing and its application. So in 
the future we will shift our algorithm to other 
languages. And fine-tune to a specific domain such 
as sports. 
ACKNOWLEDGEMENT 
This paper is supported by the National ?973? 
project G1998030501A-06 and the Natural Science 
Foundation of China 60272041. 
References 
Jian Sun, et al 2002. Chinese Named Entity 
Identification Using Class-based Language Model. 
Proceedings of the 19th International Conference on 
Computational Linguistics 
Hsin-His Chen, et al 1997. Description of the NTU 
System Used for MET2. Proceedings of the Seventh 
Message Understanding Conference 
Tat-Seng Chua, et al 2002. Learning Pattern Rules for 
Chinese Named Entity Extraction. Proceedings of 
AAAI?02 
W.J.Teahan, et al 1999. A Compression-based 
Algorithm for Chinese Word Segmentation. 
Computational Linguistic 26(2000) 375-393 
Maosong Sun, et al 1994. Identifying Chinese Names in 
Unrestricted Texts. Journal of Chinese Information 
Processing. 1994,8(2) 
Collins, Singer. 1999. Unsupervised Models for Named 
Entity Classification. Proceedings of 1999 Joint 
SIGDAT Conference on Empirical Methods in NLP 
and Very Large Corpora 
Daniel M. Bikel, et al 1997. Nymble: a High-
Performance Learning Name-finder. Proceedings of 
ANLP-97, page 194-201, 1997 
Yu et al 1998. Description of the Kent Ridge Digital 
Labs System Used for MUC-7. Proceedings of the 
Seventh Message Understanding Conference 
Silviu Cucerzan, David Yarowsky. 1999. Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. 
Proceedings 1999 Joint SIGDAT Conference on 
EMNLP and VLC 
Peter F.Brown, et al 1992. Class-Based n-gram Model 
of Natural Language. 1992 Association for 
Computational Linguistics 
A.Mikheev, M.Moens, and C.Grover. 1999. Named 
entity recognition without gazetteers. Proceedings of 
the Ninth Conference of the European Chapter of the 
Association for Computational Linguistics. Bergen, 
Norway 
Borthwich. A. 1999. A Maximum Entropy Approach to 
Named Entity Recognition. PhD Dissertation 
Dong & Dong. 2000. Hownet. At: http://www.keenage. 
com 
Yu.S.W. 1999. The Specification and Manual of 
Chinese Word Segmentation and Part of Speech 
Tagging. At: http://www.icl.pku.edu.cn/Introduction/ 
corpustagging. htm 
Mei.J.J, et al 1983. ??????/TONG YI CI CI 
LIN?. Shanghai CISHU Press 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 56?63,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Cluster-based Language Model for Sentence Retrieval in Chinese 
Question Answering 
 
 
Youzheng Wu                               Jun Zhao                               Bo Xu 
National Laboratory of Pattern Recognition 
Institute of Automation Chinese Academy of Sciences 
No.95 Zhongguancun East Road, 100080, Beijing, China 
(yzwu, jzhao,boxu)@nlpr.ia.ac.cn 
 
  
 
Abstract 
Sentence retrieval plays a very important 
role in question answering system. In this 
paper, we present a novel cluster-based 
language model for sentence retrieval in 
Chinese question answering which is mo-
tivated in part by sentence clustering and 
language model. Sentence clustering is 
used to group sentences into clusters. 
Language model is used to properly rep-
resent sentences, which is combined with 
sentences model, cluster/topic model and 
collection model. For sentence clustering, 
we propose two approaches that are One-
Sentence-Multi-Topics and One-
Sentence-One-Topic respectively. From 
the experimental results on 807 Chinese 
testing questions, we can conclude that 
the proposed cluster-based language 
model outperforms over the standard lan-
guage model for sentence retrieval in 
Chinese question answering. 
1 Introduction 
To facilitate the answer extraction of question 
answering, the task of retrieval module is to find 
the most relevant passages or sentences to the 
question. So, the retrieval module plays a very 
important role in question answering system, 
which influences both the performance and the 
speed of question answering. In this paper, we 
mainly focus on the research of improving the 
performance of sentence retrieval in Chinese 
question answering. 
Many retrieval approaches have been pro-
posed for sentence retrieval in English question 
answering. For example, Ittycheriach [Ittycheriah, 
et al 2002] and H. Yang [Hui Yang, et al 2002] 
proposed vector space model. Andres [Andres, et 
al. 2004] and Vanessa [Vanessa, et al 2004] pro-
posed language model and translation model re-
spectively. Compared to vector space model, 
language model is theoretically attractive and a 
potentially very effective probabilistic frame-
work for researching information retrieval prob-
lems [Jian-Yun Nie. 2005]. 
However, language model for sentence re-
trieval is not mature yet, which has a lot of diffi-
cult problems that cannot be solved at present. 
For example, how to incorporate the structural 
information, how to resolve data sparseness 
problem. In this paper, we mainly focus on the 
research of the smoothing approach of language 
model because sparseness problem is more seri-
ous for sentence retrieval than for document re-
trieval. 
At present, the most popular smoothing ap-
proaches for language model are Jelinek-Mercer 
method, Bayesian smoothing using Dirichlet pri-
ors, absolute discounting and so on [C. Zhai, et al 
2001]. The main disadvantages of all these 
smoothing approaches are that each document 
model (which is estimated from each document) 
is interpolated with the same collection model 
(which is estimated from the whole collection) 
through a unified parameter. Therefore, it does 
not make any one particular document more 
probable than any other, on the condition that 
neither the documents originally contains the 
query term. In other word, if a document is rele-
vant, but does not contain the query term, it is 
still no more probable, even though it may be 
topically related. 
As we know, most smoothing approaches of 
sentence retrieval in question answering are 
learned from document retrieval without many 
adaptations. In fact, question answering has some 
56
characteristics that are different from traditional 
document retrieval, which could be used to im-
prove the performance of sentence retrieval. 
These characteristics lie in: 
1. The input of question answering is natural 
language question which is more unambiguous 
than query in traditional document retrieval. 
For traditional document retrieval, it?s difficult 
to identify which kind of information the users 
want to know. For example, if the user submit 
the query {??/invent, ??/telephone}, search 
engine does not know what information is 
needed, who invented telephone, when telephone 
was invented, or other information. On the other 
hand, for question answering system, if the user 
submit the question {???????/who in-
vented the telephone?}, it?s easy to know that the 
user want to know the person who invented the 
telephone, but not other information. 
2. Candidate answers extracted according to 
the semantic category of the question?s answer 
could be used for sentence clustering of question 
answering. 
Although the first retrieved sentences are re-
lated to the question, they usually deal with one 
or more topics. That is, relevant sentences for a 
question may be distributed over several topics. 
Therefore, treating the question?s words in re-
trieved sentences with different topics equally is 
unreasonable. One of the solutions is to organize 
the related sentences into several clusters, where 
a sentence can belong to about one or more clus-
ters, each cluster is regarded as a topic. This is 
sentence clustering. Obviously, cluster and topic 
have the same meaning and can be replaced each 
other. In the other word, a particular entity type 
was expected for each question, and every spe-
cial entity of that type found in a retrieved sen-
tence was regarded as a cluster/topic.  
In this paper, we propose two novel ap-
proaches for sentence clustering. The main idea 
of the approaches is to conduct sentence cluster-
ing according to the candidate answers which are 
also considered as the names of the clusters.  
For example, given the question {?????
??/who invented telephone?}, the top ten re-
trieved sentences and the corresponding candi-
date answers are shown as Table 1. Thus, we can 
conduct sentence clustering according to the 
candidate answers, that are, {??/Bell, ???
/Siemens, ???/Edison,??/Cooper, ???
/Stephen}.
 
ID Top 10 Sentences Candidate Answer 
S1 1876? 3? 10???????/Bell invented telephone on Oct. 3th, 1876. ??/Bell 
S2 
????????????????????????
/ Bell, Siemens and Edison invented telephone, electromo-
tor and electric light respectively. 
???/ Siemens 
??/Bell 
???/ Edison 
S3 
??? ??????? ????????????
/Recently, the public paid a great deal of attention to Cooper 
who is Father of Mobile Phone. 
??/Cooper 
S4 1876 ????????????? /In 1876, Bell in-vented telephone. ??/Bell 
S5 
???1876 ???????????????1879 ?
??????????????/Subsequently, American 
scientist Bell invented the phone in 1876; Edison invented 
the electric light in 1879. 
??/Bell 
???/Edison 
S6 1876 ? 3 ? 7 ???????????????/On March 7th, 1876, Bell became the patentee of telephone. ??/Bell 
S7 
????????????????????????
???/Bell not only invented telephone, but also estab-
lished his own company for spreading his invention. 
??/Bell 
S8 
??????????? 30 ???????????
????????????????/Thirty years after 
the invention of first mobile phone, Cooper still anticipated 
the date of the realization of future phone?s technology. 
??/Cooper 
57
S9 
????????????????????????
????????????????????????
???/Cooper said, he was surprised at the speed that the 
consumers switched to mobile phones; but the populariza-
tion of mobile phone isn?t omnipresent, which made him a 
little bit disappointed. 
??/Cooper 
S10 
????????????????????????
???????????/England inventor Stephen de-
signed the paper-clicked CMOS chip which included all 
electronic components. 
???/Stephen 
Table 1 The Top 10 Retrieved Sentences and its Candidate Answers 
Based on the above analysis, this paper pre-
sents cluster-based language model for sentence 
retrieval of Chinese question answering. It dif-
fers from most of the previous approaches 
mainly as follows. 1. Sentence Clustering is con-
ducted according to the candidate answers ex-
tracted from the top 1000 sentences. 2. The in-
formation of the cluster of the sentence, which is 
also called as topic, is incorporated into language 
model through aspect model. For sentence clus-
tering, we propose two novel approaches that are 
One-Sentence-Multi-Topics and One-Sentence-
One-Topic respectively. The experimental results 
show that the performances of cluster-based lan-
guage model for sentence retrieval are improved 
significantly. 
The framework of cluster-based language 
model for sentence retrieval is shown as Figure 1. 
 
Figure 1 The Framework of Cluster-based Language Model for Sentence Retrieval 
2 Language Model for Information Re-
trieval 
Language model for information retrieval is pre-
sented by Ponte & Croft in 1998[J. Ponte, et al 
1998] which has more advantages than vector 
space model. After that, many improved models 
are proposed like J.F. Gao [J.F Gao, et al 2004], 
C. Zhai [C. Zhai, et al 2001], and so on. In 1999, 
Berger & Lafferty [A. Berger, et al 1999] pre-
sented statistical translation model for informa-
tion retrieval. 
The basic approach of language model for in-
formation retrieval is to model the process of 
generating query Q. The approach has two steps. 
1. Constructing document model for each docu-
ment in the collection; 2. Ranking the documents 
according to the probabilities p(Q|D). A classical 
unigram language model for IR could be ex-
pressed in equation (1). 
( ) ( )?
Qw
i
i
D|wpD|Qp
?
=                                   (1) 
where, wi is a query term, p(wi|D) is document 
model which represents terms distribution over 
document. Obviously, estimating the probability 
p(wi|D) is the key of document model. To solve 
the sparseness problem, Jelinek-Mercer is com-
monly used which could be expressed by equa-
tion (2). 
( ) ( ) ( ) ( )C|wp?1D|wp?D|wp MLML ?+?= -   (2) 
where, pML(w|D) and pML(w|C) are document 
model and collection model respectively esti-
mated via maximum likelihood. 
Question 
Document 
Retrieval 
Sentence 
Splitter 
Candidate An-
swer Extraction
Language Model 
for Sentence Re-
trieval
Sentence Clus-
tering 
Results 
Cluster-based Lan-
guage Model for 
Sentence Retrieval
Question 
Analyzer 
58
As described above, the disadvantages of 
standard language model is that it does not make 
any one particular document any more probable 
than any other, on the condition that neither the 
documents originally contain the query term. In 
the other word, if a document is relevant, but 
does not contain the query term, it is still no 
more probable, even though it may be topically 
related. Thus, the smoothing approaches based 
on standard language model are improper. In this 
paper, we propose a novel cluster-based lan-
guage model to overcome it. 
3 Cluster-based Language Model for 
Sentence Retrieval 
Note that document model p(w|D) in document 
retrieval is replace by p(w|S) called sentence 
model in sentence retrieval. 
The assumption of cluster-based language 
model for retrieval is that topic-related sentences 
tend to be relevant to the same query. So, incor-
porating the topic of sentences into language 
model can improve the performance of sentence 
retrieval based on standard language model. 
The proposed cluster-based language model is 
a mixture model of three components, that are 
sentence model pML(w|S), cluster/topic model 
p_topicML(w|T) and collection model pML(w|C). 
We can formulate our model as equation (3). 
( ) ( ) ( )
( ) ( ) ( )( )Cwp?1Twp_topic?
?1Swp?S|wp
MLML
ML
|?+|?
?+|?=
-
-          (3) 
In fact, the cluster-based language model can 
also be viewed as a two-stage smoothing ap-
proach. The cluster model is first smoothed using 
the collection model, and the sentence model is 
then smoothed with the smoothed cluster model. 
In this paper, the cluster model is in the form 
of term distribution over cluster/topic, associated 
with the distribution of clusters/topics over sen-
tence, which can be expressed by equation (4).  
( ) ( ) ( )?
?Tt
StptwpTwp_topic ||=|                     (4) 
where, T is the set of clusters/topics. p_topic(w|T) 
is cluster model. p(t|S) is topic sentence distribu-
tion which means the distribution of topic over 
sentence. And p(w|t) is term topic distribution 
which means the term distribution over topics. 
Before estimating the sentence model p(w|S), 
topic-related sentences should be organized into 
clusters/topics to estimate p(t|S) and p(w|t) prob-
abilities. For sentence clustering, this paper pre-
sents two novel approaches that are One-
Sentence-Multi-Topics and One-Sentence-One-
Topic respectively. 
3.1 One-Sentence-Multi-Topics 
The main idea of One-Sentence-Multi-Topics 
can be summarized as follows. 
1. If a sentence includes M different candidate 
answers, then the sentence consists of M different 
topics. 
For example, the sentence S5 in Table 1 includes 
two topics which are ???????/Bell in-
vented telephone? and ????????/Edison 
invented electric light? respectively. 
2. Different sentences have the same topic if two 
candidate answers are same. 
For example, the sentence S4 and S5 in Table 1 
have the same topic ??????? /Bell in-
vented telephone? because both of sentences 
have the same candidate answer ???/Bell?. 
Based on the above ideas, the result of sen-
tence clustering based on One-Sentence-Multi-
Topics is shown in Table 2. 
Name of Clusters Sentences 
??/Bell S1 S2 S4 S5 S6 S7 S8 
???/Siemens S2 
???/Edison S2 S5 
??/Cooper S3 S8 S9 
???/Stephen S10 
Table 2 The Result of One-Sentence-Multi-
Topics Sentence Clustering 
So, we could estimate term topic distribution 
using equation (5). 
( ) ( )( )?
w'
t,w'n
twn
twp
,=|                                         (5) 
Topic sentence distribution can be estimated 
using equation (6) and (7). 
( )
? /
/=|
t
st
st
kl1
kl1
Stp                                            (6) 
( ) ( ) ( )( )?w ML
ML
MLst t|wp
swp
logs|wptsKLkl
|?=||=    (7) 
where, klst means the Kullback-Leibler diver-
gence between the sentence with the cluster/topic. 
k denotes the number of cluster/topic. The main 
idea of equation (6) is that the closer the Kull-
back-Leibler divergence, the larger the topic sen-
tence probability p(t|S). 
3.2 One-Sentence-One-Topic 
The main idea of One-Sentence-One-Topic also 
could be summarized as follows. 
59
1. A sentence only has one kernel candidate an-
swer which represents the kernel topic no matter 
how many candidate answers is included. 
For example, the kernel topic of sentence S5 in 
Table 1 is ???????/Bell invented tele-
phone? though it includes three different candi-
date answers. 
2. Different sentences have the same topic if two 
kernel candidate answers are same. 
For example, the sentence S4 and S5 in Table 1 
have the same topic ??????? /Bell in-
vented telephone?. 
3. The kernel candidate answer has shortest av-
erage distance to all query terms. 
Based on the above ideas, the result of sen-
tence clustering based on One-Sentence-One-
Topic is shown in Table 3. 
Name of Clusters Sentences 
??/Bell S1 S2 S4 S5 S6 S7
??/Cooper S3 S8 S9 
???/Stephen S10 
Table 3 The Result of One-Sentence-One-Topic 
Sentence Clustering 
Equation (8) and (9) can be used to estimate 
the kernel candidate answer and the distances of 
candidate answers respectively. Term topic dis-
tribution in One-Sentence-One-Topic can be es-
timated via equation (5). And topic sentence dis-
tribution is equal to 1 because a sentence only 
belongs to one cluster/topic. { }
i
i
a
a
*
i SemDis  a argmin=                               (8) 
( )
N
q,aSemDis
SemDis j
ji
ai
?
=
                         (9) 
( )
ji qaji PositionPositionqaSemDis -=,           (10) 
where, ai* is the kernel candidate answer. ai is 
the i-th candidate answer, 
iaSemDis is the average 
distance of i-th candidate answer. qj is the j-th 
query term, N is the number of all query terms. 
jqPosition and iaPosition  mean the position of 
query term qj and candidate answer ai. 
4 Experiments and Analysis 
Research on Chinese question answering, is still 
at its early stage. And there is no public evalua-
tion platform for Chinese question answering. So 
in this paper, we use the evaluation environment 
presented by [Youzheng Wu, et al 2004] which 
is similar to TREC question answering track 
[Ellen. M. Voorhees. 2004]. The documents col-
lection is downloaded from Internet which size is 
1.8GB. The testing questions are collected via 
four different approaches which has 7050 Chi-
nese questions currently. 
In this section, we randomly select 807 testing 
questions which are fact-based short-answer 
questions. Moreover, the answers of all testing 
questions are named entities identified by 
[Youzheng Wu, et al 2005]. Figure 2 gives the 
details. Note that, LOC, ORG, PER, NUM and 
TIM denote the questions which answer types 
are location, organization, person, number and 
time respectively, SUM means all question types. 
165
311
28
168
135
0
100
200
300
400
PER LOC ORG TIM NUM
 
Figure 2 The Distribution of Various Question 
Types over Testing Questions 
Chinese question answering system is to re-
turn a ranked list of five answer sentences per 
question and will be strictly evaluated (unsup-
ported answers counted as wrong) using mean 
reciprocal rank (MRR). 
4.1 Baseline: Standard Language Model for 
Sentence Retrieval 
Based on the standard language model for infor-
mation retrieval, we can get the baseline per-
formance, as is shown in Table 4, where ? is the 
weight of document model. 
? 0.6 0.7 0.8 0.9 
LOC 49.95 51.50 52.63 54.54
ORG 53.69 51.01 50.12 51.01
PER 63.10 64.42 65.94 65.69
NUM 48.43 49.86 51.78 53.26
TIM 56.97 58.38 58.77 61.49
SUM 53.98 55.28 56.40 57.93
Table 4 The Baseline MRR5 Performance 
60
In the following chapter, we conduct experi-
ments to answer two questions.  
1. Whether cluster-based language model for 
sentence retrieval could improve the perform-
ance of standard language model for sentence 
retrieval? 
2. What are the performances of sentence clus-
tering for various question types? 
4.2 Cluster-based Language Model for Sen-
tence Retrieval 
In this part, we will conduct experiments to vali-
date the performances of cluster-based language 
models which are based on One-Sentence-Multi-
Topics and One-Sentence-One-Topic sentence 
clustering respectively. In the following experi-
ments, ? = 0.9. 
4.2.1 Cluster-based Language Model Based 
on One-Sentence-Multi-Topics 
The experimental results of cluster-based lan-
guage model based on One-Sentence-Multi-
Topics sentence clustering are shown in Table 5. 
The relative improvements are listed in the 
bracket. 
? 0.6 0.7 0.8 0.9 
LOC 55.57 (+11.2) 
55.61 
(+7.98) 
56.59 
(+7.52) 
57.70 
(+5.79)
ORG 59.05 (+9.98) 
59.46 
(+16.6) 
59.46 
(+18.6) 
59.76 
(+17.2)
PER 67.73 (+7.34) 
68.03 
(+5.60) 
67.71 
(+2.68) 
67.45 
(+2.68)
NUM 52.79 (+9.00) 
53.90 
(+8.10) 
54.45 
(+5.16) 
55.51 
(+4.22)
TIM 60.17 (+5.62) 
60.63 
(+3.85) 
62.33 
(+6.06) 
61.68 
(+0.31)
SUM 58.14 (+7.71) 
58.63 
(+6.06) 
59.30 
(+5.14) 
59.54 
(+2.78)
Table 5 MRR5 Performance of Cluster-based 
Language Model Based on One-Sentence-Multi-
Topics 
From the experimental results, we can find 
that by integrating the clusters/topics of the sen-
tence into language model, we can achieve much 
improvement at each stage of ?. For example, the 
largest and smallest improvements for all types 
of questions are about 7.7% and 2.8% respec-
tively. This experiment shows that the proposed 
cluster-based language model based on One-
Sentence-Multi-Topics is effective for sentence 
retrieval in Chinese question answering.  
4.2.2 Cluster-based Language Model Based 
on One-Sentence-One-Topic 
The performance of cluster-based language 
model based on One-Sentence-One-Topic sen-
tence clustering is shown in Table 6. The relative 
improvements are listed in the bracket. 
? 0.6 0.7 0.8 0.9 
LOC 53.02 (+6.15)
54.27 
(+5.38) 
56.14 
(+6.67) 
56.28 
(+3.19)
ORG 58.75 (+9.42)
58.75 
(+17.2) 
59.46 
(+18.6) 
59.46 
(+16.6)
PER 66.57 (+5.50)
67.07 
(+4.11) 
67.44 
(+2.27) 
67.29 
(+2.44)
NUM 49.95 (+3.14)
50.87 
(+2.02) 
52.15 
(+0.71) 
53.51 
(+0.47)
TIM 59.75 (+4.88)
60.65 
(+3.89) 
62.71 
(+6.70) 
62.20 
(+1.15)
SUM 56.48 (+4.63)
57.65 
(+4.29) 
58.82 
(+4.29) 
59.22 
(+2.23)
Table 6 MRR5 Performance of Cluster-based 
Language Model Based on One-Sentence-One-
Topic 
In Comparison with Table 5, we can find that 
the improvement of cluster-based language 
model based on One-Sentence-One-Topic is 
slightly lower than that of cluster-based language 
model based on One-Sentence-Multi-Topics. The 
reasons lie in that Clusters based on One-
Sentence-One-Topic approach are very coarse 
and much information is lost. But the improve-
ments over baseline system are obvious. 
Table 7 shows that MRR1 and MRR20 scores 
of cluster-based language models for all question 
types. The relative improvements over the base-
line are listed in the bracket. This experiment is 
to validate whether the conclusion based on dif-
ferent measurements is consistent or not. 
 One-Sentence-Multi-Topics 
One-Sentence-
One-Topic 
? MRR1 MRR20 MRR1 MRR20
0.6 50.00 (+14.97)
59.60 
(+7.66) 
48.33 
(+10.37) 
57.70 
(+4.23)
0.7 50.99 (+13.36)
60.03 
(+6.12) 
49.44 
(+9.92) 
58.62 
(+3.62)
0.8 51.05 (+8.99) 
60.68 
(+5.06) 
51.05 
(+8.99) 
60.01 
(+3.90)
0.9 51.92 (+5.81) 
61.05 
(+2.97) 
51.30 
(+4.54) 
60.25 
(+1.62)
Table 7 MRR1 and MRR20 Performances of 
Two Cluster-based Language Models 
61
Table 7 also shows that the performances of 
two cluster-based language models are higher 
than that of the baseline system under different 
measurements. For MRR1 scores, the largest 
improvements of cluster-based language models 
based on One-Sentence-Multi-Topics and One-
Sentence-One-Topic are about 15% and 10% 
respectively. For MRR20, the largest improve-
ments are about 7% and 4% respectively. 
Conclusion 1: The experiments show that the 
proposed cluster-based language model can im-
prove the performance of sentence retrieval in 
Chinese question answering under the various 
measurements. Moreover, the performance of 
clustering-based language model based on One-
Sentence-Multi-Topics is better than that based 
on One-Sentence-One-Topic. 
4.3 The Analysis of Sentence Clustering for 
Various Question Types 
The parameter ? in equation (3) denotes the bal-
ancing factor of the cluster model and the collec-
tion model. The larger ?, the larger contribution 
of the cluster model. The small ?, the larger con-
tribution of the collection model. If the perform-
ance of sentence retrieval decreased with the in-
creasing of ?, it means that there are many noises 
in sentence clustering. Otherwise, sentence clus-
tering is satisfactory for cluster-based language 
model. So the task of this experiment is to find 
the performances of sentence clustering for vari-
ous question types, which is helpful to select the 
most proper ? to obtain the best performance of 
sentence retrieval. 
With the change of ? and the fixed ? (? = 0.9), 
the performances of cluster-based language 
model based on One-Sentence-Multi-Topics are 
shown in Figure 3. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
SUM
LOC
ORG
PER
NUM
TIM
 
Figure 3 MRR5 Performances of Cluster-based 
Language Model Based on One-Sentence-Multi-
Topics with the Change of ? 
In Figure 3, the performances of TIM and 
NUM type questions decreased with the increas-
ing of the parameter ? (from 0.6 to 0.9), while 
the performances of LOC, PER and ORG type 
questions increased. This phenomenon showed 
that the performance of sentence clustering based 
on One-Sentence-Multi-Topics for TIM and 
NUM type questions is not as good as that for 
LOC, PER and ORG type questions. This is in 
fact reasonable. The number and time words fre-
quently appeared in the sentence, which does not 
represent a cluster/topic when they appear. While 
PER, LOC and ORG entities can represent a 
topic when they appeared in the sentence. 
Similarly, with the change of ? and the fixed ? 
(?=0.9), the performances of cluster-based lan-
guage model based on One-Sentence-One-Topic 
are shown in Figure 4. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
SUM
LOC
ORG
PER
NUM
TIM
 
Figure 4 MRR5 Performance of Cluster-based 
Language Model Based on One-Sentence-One-
Topic with the Change of ? 
In Figure 4, the performances of TIM, NUM, 
LOC and SUM type questions decreased with the 
increasing of ? (from 0.6 to 0.9). This phenome-
non shows that the performances of sentence 
clustering based on One-Sentence-One-Topic are 
not satisfactory for most of question types. But, 
compared to the baseline system, the cluster-
based language model based on this kind of sen-
tence clustering can still improve the perform-
ances of sentence retrieval in Chinese question 
answering. 
Conclusion 2: The performance of the pro-
posed sentence clustering based on One-
Sentence-Multi-Topics for PER, LOC and ORG 
type questions is higher than that for TIM and 
NUM type questions. Thus, for PER, LOC and 
ORG questions, we should choose the larger ? 
value (about 0.9) in cluster-based language 
model based on One-Sentence-Multi-Topics. 
While for TIM and NUM type questions, the 
62
value of ? should be smaller (about 0.5). But, the 
performance of sentence clustering based on 
One-Sentence-One-Topic for all questions is not 
ideal, so the value for cluster-based language 
model based on One-Sentence-One-Topic should 
be smaller (about 0.5) for all questions. 
5 Conclusion and Future Work 
The input of a question answering system is 
natural language question which contains richer 
information than the query in traditional docu-
ment retrieval. Such richer information can be 
used in each module of question answering sys-
tem. In this paper, we presented a novel cluster-
based language model for sentence retrieval in 
Chinese question answering which combines the 
sentence model, the cluster/topic model and the 
collection model. 
For sentence clustering, we presented two ap-
proaches that are One-Sentence-Multi-Topics 
and One-Sentence-One-Topic respectively. The 
experimental results showed that the proposed 
cluster-based language model could improve the 
performances of sentence retrieval in Chinese 
question answering significantly. 
However, we only conduct sentence clustering 
for questions, which have the property that their 
answers are named entities in this paper. In the 
future work, we will focus on all other type ques-
tions and improve the performance of the sen-
tence retrieval by introducing the structural, syn-
tactic and semantic information into language 
model. 
Reference 
J. Ponte, W. Bruce Croft. A Language Modeling Ap-
proach to Information Retrieval. In the Proceedings 
of ACM SIGIR 1998, pp 275-281, 1998. 
C. Zhai, J. Lafferty. A Study of Smoothing Tech-
niques for Language Modeling Applied to ad hoc 
Information Retrieval. In the Proceedings of the 
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, 2001. 
Ittycheriah, S. Roukos. IBM's Statistical Question 
Answering System-TREC 11. In the Eleventh Text 
Retrieval Conference (TREC 2002), Gaithersburg, 
Maryland, November 2002. 
Hui Yang, Tat-Seng Chua. The Integration of Lexical 
Knowledge and External Resources for Question 
Answering. In the Proceedings of the Eleventh 
Text REtrieval Conference (TREC?2002), Mary-
land, USA, 2002, page 155-161. 
Andres Corrada-Emmanuel, W.Bruce Croft, Vanessa 
Murdock. Answer Passage Retrieval for Question 
Answering. In the Proceedings of the 27th Annual 
International Conference on Research and Devel-
opment in Information Retrieval, pp. 516 ? 517, 
2004. 
Ellen M. Voorhees. Overview of the TREC 2004 
Question Answering Track. In Proceedings of the 
Twelfth Text REtrieval Conference (TREC 2004), 
2004. 
Vanessa Murdock, W. Bruce Croft. Simple Transla-
tion Models for Sentence Retrieval in Factoid 
Question Answering. In the Proceedings of the 
SIGIR 2004 Workshop on Information Retrieval 
for Question Answering, pp.31-35, 2004. 
Thomas Hofmann. Probabilistic Latent Semantic In-
dexing. In the Proceedings of the Twenty-Second 
Annual International SIGIR Conference on Re-
search and Development in Information Retrieval, 
1999. 
A. Berger and J. Lafferty. Information Retrieval as 
Statistical Translation. In the Proceedings of ACM 
SIGIR-1999, pp. 222?229, Berkeley, CA, August 
1999. 
A. Echihabi and D.Marcu. A noisy-channel approach 
to question answering. In the Proceeding of the 
41st Annual Meeting of the Association for Com-
putational Linguistics, Sappora, Japan, 2003. 
Leif Azzopardi, Mark Girolami and Keith van 
Rijsbergen. Topic Based Language Models for ad 
hoc Information Retrieval. In the Proceeding of 
IJCNN 2004 & FUZZ-IEEE 2004, July 25-29, 
2004, Budapest, Hungary. 
Jian-Yun Nie. Integrating Term Relationships into 
Language Models for Information Retrieval. Re-
port at ICT-CAS. 
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu and 
Guihong Cao. 2004b. Dependence language model 
for information retrieval. In SIGIR-2004. Sheffield, 
UK, July 25-29. 
Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named 
Entity Recognition Model Based on Multiple Fea-
tures. In the Proceeding of HLT/EMNLP 2005, 
Vancouver, B.C., Canada, pp.427-434, 2005. 
Youzheng Wu, Jun Zhao, Xiangyu Duan  and Bo Xu. 
Building an Evaluation Platform for Chinese Ques-
tion Answering Systems. In Proceeding of the First 
National Conference on Information Retrieval and 
Content Security. Shanghai, China, December, 
2004.(In Chinese) 
 
63
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 87?93,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Hybrid Approach to Chinese Base Noun Phrase Chunking 
 
Fang Xu Chengqing Zong Jun Zhao 
National Laboratory of Pattern Recognition 
Institute of Automation 
Chinese Academy of Sciences, Beijing 100080,China 
{fxu, cqzong, jzhao}@nlpr.ia.ac.cn 
 
 
 
 
Abstract 
In this paper, we propose a hybrid ap-
proach to chunking Chinese base noun 
phrases (base NPs), which combines 
SVM (Support Vector Machine) model 
and CRF (Conditional Random Field) 
model. In order to compare the result 
respectively from two chunkers, we use 
the discriminative post-processing 
method, whose measure criterion is the 
conditional probability generated from 
the CRF chunker. With respect to the 
special structures of Chinese base NP 
and complete analyses of the first two 
results, we also customize some appro-
priate grammar rules to avoid ambigui-
ties and prune errors. According to our 
overall experiments, the method 
achieves a higher accuracy in the final 
results. 
1 Introduction  
Chunking means extracting the non-overlapping 
segments from a stream of data. These segments 
are called chunks (Dirk and Satoshi, 2003). The 
definition of base noun phrase (base NP) is sim-
ple and non-recursive noun phrase which does 
not contain other noun phrase descendants. Base 
NP chunking could be used as a precursor for 
many elaborate natural language processing tasks, 
such as information retrieval, name entity extrac-
tion and text summarization and so on. Many 
other problems similar to text processing can also 
benefit from base NP chunking, for example, 
finding genes in DNA and phoneme information 
extraction. 
The initial work on base NP chunking is fo-
cused on the grammar-based method. Ramshaw 
and Marcus (1995) introduced a transformation-
based learning method which considered chunk-
ing as a kind of tagging problem. Their work in-
spired many others to study the applications of 
learning methods to noun phrase chunking. 
(Cardie and Pierce, 1998, 1999) applied a scoring 
method to select new rules and a naive heuristic 
for matching rules to evaluate the results' accu-
racy. 
CoNLL-2000 proposed a shared task (Tjong 
and Buchholz, 2000), which aimed at dividing a 
text in syntactically correlated parts of words. 
The eleven systems for the CoNLL-2000 shared 
task used a wide variety of machine learning 
methods. The best system in this workshop is on 
the basis of Support Vector Machines used by 
(Kudo and Matsumoto, 2000). 
Recently, some new statistical techniques, 
such as CRF (Lafferty et al 2001) and structural 
learning methods (Ando and Zhang, 2005) have 
been applied on the base NP chunking. (Fei and 
Fernando, 2003) considered chunking as a se-
quence labeling task and achieved good perform-
ance by an improved training methods of CRF. 
(Ando and Zhang, 2005) presented a novel semi-
supervised learning method on chunking and 
produced performances higher than the previous 
best results. 
The research on Chinese Base NP Chunking is, 
however, still at its developing stage. Research-
ers apply similar methods of English Base NP 
chunking to Chinese. Zhao and Huang (1998) 
made a strict definition of Chinese base NP and 
put forward a quasi-dependency model to analy-
sis the structure of Chinese base NPs. There are 
some other methods to deal with Chinese phrase 
(no only base NP) chunking, such as HMM 
(Heng Li et al, 2003), Maximum Entropy (Zhou 
Yaqian et al, 2003), Memory-Based Learning 
(Zhang and Zhou, 2002) etc. 
87
However, according to our experiments over 
30,000 Chinese words, the best results of Chi-
nese base NP chunking are about 5% less than 
that of English chunking (Although we should 
admit the chunking outcomes vary among differ-
ent sizes of corpus and rely on the details of ex-
periments). The differences between Chinese 
NPs and English NPs are summarized as follow-
ing points: First, the flexible structure of Chinese 
noun phrase often results in the ambiguities dur-
ing the recognition procedure. For example, 
many English base NPs begin with the determi-
native, while the margin of Chinese base NPs is 
more uncertain. Second, the base NPs begins 
with more than two noun-modifiers, such as ??
(high)/JJ  ?(new)/JJ ??(technology)/NN?, the 
noun-modifiers ??/JJ ? can not be completely 
recognized. Third, the usage of Chinese word is 
flexible, as a Chinese word may serve with multi 
POS (Part-of-Speech) tags. For example, a noun 
is used as a verbal or an adjective component in 
the sentence. In this way the chunker is puzzled 
by those multi-used words. Finally, there are no 
standard datasets and elevation systems for Chi-
nese base NP chunking as the CoNLL-2000 
shared task, which makes it difficult to compare 
and evaluate different Chinese base NP chunking 
systems. 
In this paper, we propose a hybrid approach to 
extract the Chinese base NPs with the help of the 
conditional probabilities derived from the CRF 
algorithm and some appropriate grammar rules. 
According to our preliminary experiments on 
SVM and CRF, our approach outperforms both 
of them.  
The remainder of the paper is organized as fol-
lows. Section 2 gives a brief introduction of the 
data representations and methods. We explain 
our motivations of the hybrid approach in section 
3. The experimental results and conclusions are 
introduced in section 4 and section 5 respectively. 
2 Task Description 
2.1 Data Representation 
Ramshaw and Marcus (1995) gave mainly two 
kinds of base NPs representation ?  the 
open/close bracketing and IOB tagging. For ex-
ample, a bracketed Chinese sentence, 
[ ??(foreign businessmen) ??(investment)] 
?? (become) [ ??  (Chinese) ?? (foreign 
trade)] [ ??(important) ???(growth)] ?  
The IOB tags are used to indicate the bounda-
ries for each base NP where letter ?B? means the 
current word starts a base NP, ?I? for a word in-
side a base NP and ?O? for a word outside a NP 
chunk. In this case the tokens for the former sen-
tence would be labeled as follows:    
??/B ??/I ??/V ??/B ??/I ??/B      
???/O  ?/O   
Currently, most of the work on base NP identi-
fication employs the trainable, corpus-based al-
gorithm, which makes full use of the tokens and 
corresponding POS tags to recognize the chunk 
segmentation of the test data. The SVM and CRF 
are two representative effective models widely 
used. 
2.2 Chunking with SVMs  
SVM is a machine learning algorithm for a linear 
binary classifier in order to maximize the margin 
of confidence of the classification on the training 
data set. According to the different requirements, 
distinctive kernel functions are employed to 
transfer non-linear problems into linear problems 
by mapping it to a higher dimension space.  
By transforming the training data into the form 
with IOB tags, we can view the base NP chunk-
ing problem as a multi-class classification prob-
lem. As SVMs are binary classifiers, we use the 
pairwise method to convert the multi-class prob-
lem into a set of binary class problem, thus the 
I/O/B classifier is reduced into 3 kinds of binary 
classifier ? I/O classifier, O/B classifier, B/I 
classifier. 
In our experiments, we choose TinySVM 1  to-
gether with YamCha 2  (Kudo and Matsumoto, 
2001) as the one of the baseline systems for our 
chunker. In order to construct the feature sets for 
training SVMs, all information available in the 
surrounding contexts, including tokens, POS tags 
and IOB tags. The tool YamCha makes it possi-
ble to add new features on your own. Therefore, 
in the training stage, we also add two new fea-
tures according to the words. First, we give spe-
cial tags to the noun words, especially the proper 
noun, as we find in the experiment the proper 
nouns sometimes bring on errors, such as base 
                                                          
1 http://chasen.org/~taku/software/TinySVM/ 
2 http://chasen.org/~taku/software/yamcha 
88
NP ???(Sichuan)/NR ??(basin)/NN?, con-
taining the proper noun ??? /NR?, could be 
mistaken for a single base NP ???/NN?; Sec-
ond, some punctuations such as separating marks, 
contribute to the wrong chunking, because many 
Chinese compound noun phrases are connected 
by separating mark, and the ingredients in the 
sentence are a mixture of simple nouns and noun 
phrases, for example, 
??? (National)/NN ??? ( Statistics Of-
fice)/NN???(Chinese)/NR ??(Social Sci-
ences)/NN ??? (Academy)/NN ? (and)/CC 
??? (Chinese Academy of Sciences)/NN-
SHORT? 
The part of base NP ? ???/B ??/I ???
/I? can be recognized as three independent base 
NPs --???/B ??/B ???/B?. The kind of 
errors comes from the conjunction ??(and)? and 
the successive sequences of nouns, which con-
tribute little to the chunker. More information 
d analyses will be provided in Section 4. an  
2.3 Conditional Random Fields 
Lafferty et al( 2001) present the Conditional 
Random Fields for building probabilistic models 
to segment and label sequence data, which was 
used effectively for base NP chunking (Sha & 
Pereira, 2003). Lafferty et al (2001) point out 
that each of the random variable label sequences 
Y conditioned on the random observation se-
quence X. The joint distribution over the label 
sequence Y given X has the form 
1
1
1
( | , ) exp( ( , ))
( )
( , ) ( , , , )
j j
j
n
i i
i
p y x F y x
Z x
F y x f y y x i
? ?
?
=
=
=
?
?
 
where 1( , , ,j i i )f y y x i?  is either a transition fea-
ture function ( 1, , ,i is y y x i? ) or a state feature 
function 1( , , , )i it y y x i?  ; 1,iy y? i are labels, x is 
an input sequence, i  is an input position, ( )Z x is 
a normalization factor; k? is the parameter to be 
estimated from training data.
Then we use the maximum likelihood training, 
such as the log-likelihood to train CRF given 
training data ( ){ },k kT x y= , 
1
( ) log ( , )
( ) k kk k
L F y x
Z x
? ??= + ?? ?? ??
( )L ? is minimized by finding unique zero of 
the gradient 
( | , )( ) [ ( , ) ( , )]kk k p Y x k
k
L F y x E F Y x??? = ??  
( | , ) ( , )kp Y x kE F Y x?  can be computed using a vari-
ant of the forward-backward algorithm. We de-
fine a transition matrix as following: 
' '( , | ) exp( ( , , , ))i j j
j
M y y x f y y x i?= ?  
Then, 
1
1
1
1
( | , ) ( , | )
( )
n
i i i
i
p y x M y y x
Z x
? + ?
=
= ?  
and let * denote component-wise matrix product,   
( | , )
1
( , ) ( | , ) ( , )
( )
( )
( ) 1
kp Y x k k k
y
T
i i i i
i
T
n
E F Y x p Y y x F y x
f M
Z x
Z x a
? ?
? ??
= =
?                           =
 = ?
?
?
 
Where i i? ?,  as the forward and backward 
state-cost vectors defined by 
1 1 1 1,
1 0 1
T
i i T i i
i i
M i n M i n
i i n
? ?? ?? + +    0 < ? ?  ? <?=  =? ?              =              =? ?
    Sha & Pereira (2003) provided a thorough dis-
cussion of CRF training methods including pre-
conditioned Conjugate Gradient, limited-
Memory Quasi-Newton and voted perceptron. 
They also present a novel approach to model 
construction and feature selection in shallow 
parsing.  
We use the software CRF++3 as our Chinese 
base NP chunker baseline software. The results 
of CRF are better than that of SVM, which is the 
same as the outcome of the English base NP 
chunking in (Sha & Pereira, 2003). However, we 
find CRF products some errors on identifying 
long-range base NP, while SVM performs well 
in this aspect and the errors of SVM and CRF are 
of different types. In this case, we develop a 
combination approach to improve the results. 
 
3 Our Approach 
?
 
(Tjong et al, 2000) pointed out that the perform-
ance of machine learning can be improved by 
combining the output of different systems, so 
they combined the results of different classifiers 
                                                          
3 http://www.chasen.org/~taku/software/CRF++/ 
89
and obtained good performance. Their combina-
tion system generated different classifiers by us-
ing different data labels and applied respective 
voting weights accordingly. (Kudo and Matsu-
moto 2001) designed a voting arrangement by 
applying cross validation and VC-bound and 
Leave-One-Out bound for the voting weights.  
The voting systems improve the accuracy, the 
choices of weights and the balance between dif-
ferent weights is based on experiences, which 
does not concern the inside features of the classi-
fication, without the guarantee of persuasive 
theoretical supports. Therefore, we developed a 
hybrid approach to combine the results of the 
SVM and CRF and utilize their advantages. 
(Simon, 2003) pointed out that the SVM guaran-
tees a high generalization using very rich features 
from the sentences, even with a large and high-
dimension training data. CRF can build efficient 
and robust structure model of the labels, when 
one doesn?t have prior knowledge about data. 
Figure 1 shows the preliminary chunking and 
pos-processing procedure in our experiments 
 First of all, we use YamCha and CRF++ re-
spectively to treat with the testing data. We got 
two original results from those chunkers, which 
use the exactly same data format; in this case we 
can compare the performance between CRF and 
SVM. After comparisons, we can figure out the 
same words with different IOB tags from the two 
former chunkers. Afterward, there exist two 
problems: how to pick out the IOB tags identi-
fied improperly and how to modify those wrong 
IOB tags.  
To solve the first question, we use the condi-
tional probability from the CRF to help deter-
mine the wrong IOB tags. For each word of the 
testing data, the CRF chunker works out a condi-
tional probability for each IOB tag and chooses 
the most probable tag for the output. We bring 
out the differences between the SVM and CRF, 
such as ??? (Sichuan)? in a base noun phrase 
is recognized as ?I? and ?O? respectively, and 
the distance between P(I| ????) and P(O| ??
??) is tiny. According to our experiment, about 
80% of the differences between SVM and CRF 
share the same statistical characters, which indi-
cate the correct answers are inundated by the 
noisy features in the classifier.  
  
CRF SVM 
Testing data 
Comparison 
Error pruning with 
rules and P (Y|X) 
Final result  
Figure 1 the Experiments? Procedure 
 
Using the comparison between SVM and CRF 
we can check most of those errors. Then we 
could build some simple grammar rules to figure 
out the correct tags for the ambiguous words cor-
responding to the surrounding contexts. Then At 
the error pruning step, judging from the sur-
rounding texts and the grammar rules, the base 
NP is corrected to the right form. We give 5 
mainly representative grammar rules to explain 
how they work in the experiments.  
The first simple sample of grammar rules is 
just like ?BNP ? NR NN?, used to solve the 
proper noun problems. Take the ???  (Si-
chuan)/NR/B ?? (basin)/NN/I? for example, 
the comparison finds out the base NP recognized 
as ??? (Sichuan)/NR/I ?? (basin)/NN/B?. 
Second, with respect to the base NP connecting 
with separating mark and conjunction words, two 
rules ?BNP ? BNP CC (BNP | Noun), BNP 
?BNP PU (BNP | Noun)? is used to figure out 
those errors; Third, with analyzing our experi-
ment results, the CRF and SVM chunker recog-
nize differently on the determinative, therefore 
the rule ?BNP ? JJ BNP?, our combination 
methods figure out new BNP tags from the pre-
liminary results according to this rule. Finally, 
the most complex situation is the determination 
of the Base NPs composed of series of nouns, 
especially the proper nouns. With figuring out 
the maximum length of this kind of noun phrase, 
we highlight the proper nouns and then separate 
the complex noun phrase to base noun phrases, 
and according to the our experiments, this 
90
method could solve close to 75% of the ambigu-
ity in the errors from complex noun phrases. To-
tally, the rules could solve about 63% of the 
found errors.  
 
4 Experiments 
The CoNLL 2000 provided the software4 to con-
vert Penn English Treebank II into the IOB tags 
form. We use the Penn Chinese Treebank 5.05, 
which is improved and involved with more POS 
tags, segmentation and syntactic bracketing. As 
the sentences in the Treebank are longer and re-
lated to more complicated structures, we modify 
the software with robust heuristics to cope with 
those new features of the Chinese Treebank and 
generate the training and testing data sets from 
the Treebank. Afterward we also make some 
manual adjustments to the final data.  
 In our experiments, the SVM chunker uses a 
polynomial kernel with degree 2; the cost per 
unit violation of the margin, C=1; and tolerance 
of the termination criterion, 0.01? = . 
In the base NPs chunking task, the evaluation 
metrics for base NP chunking include precision P, 
recall R and the F? . Usually we refer to the F?  
as the creditable metric.  
2
2
#
100%
#
#
100%
#
( 1)
( 1)
of correct proposed baseNP
P
of proposed baseNP
of correct proposed baseNP
R
of corect baseNP
RF
F
R F?
? ??
     =    
     =    
+=      =+
?
?
                                                          
 
All the experiments were performed on a 
Linux system with 3.2 GHz Pentium 4 and 2G 
memory. The total size of the Penn Chinese 
Treebank words is 13 MB, including about 
500,000 Chinese words. The quantity of training 
corpus amounts to 300,000 Chinese words. Each 
word contains two Chinese characters in average. 
We mainly use five kinds of corpus, whose sizes 
include 30000, 40000, 50000, 60000 and 70,000 
words. The corpus with an even larger size is 
improper according to the training corpus 
amount.  
4 http://ilk.kub.nl/~sabine/chunklink/ 
5 http://www.cis.upenn.edu/~chinese/  
 
From Figure 2, we can see that the results 
from CRF are better than that from SVM and the 
error-pruning performs the best. Our hybrid er-
ror-pruning method achieves an obvious im-
provement F-scores by combining the outcome 
from SVM and CRF classifiers. The test F-scores 
are decreasing when the sizes of corpus increase. 
The best performance with F-score of 89.27% is 
achieved by using a test corpus of 30k words. 
We get about 1.0% increase of F-score after us-
ing the hybrid approach. The F-score is higher 
than F-score 87.75% of Chinese base NP chunk-
ing systems using the Maximum Entropy method 
in (Zhou et al, 2003),. Which used the smaller 3 
MB Penn Chinese Treebank II as the corpus.  
The Chinese Base NP chunkers are not supe-
rior to those for English. Zhang and Ando (2005) 
produce the best English base NP accuracy with 
F-score of 94.39+ (0.79), which is superior to our 
best results. The previous work mostly consid-
ered base NP chunking as the classification prob-
lem without special attention to the lexical 
information and syntactic dependence of words. 
On the other hand, we add some grammar rules 
to strength the syntactic dependence between the 
words. However, the syntactic structure derived 
from Chinese is much more flexible and complex 
than that from English. First, some Chinese 
words contain abundant meanings or play differ-
ent syntactic roles. For example, "?? (among 
which)/NN ? ? (Chongqing)/NR ? ?
(district)/NN" is recognized as a base NP. Actu-
91
ally the Chinese word ???/NN (among)? refers 
to the content in the previous sentence and ??? 
(thereinto)? sometimes used as an adverb. Sec-
ond, how to deal with the conjunctions is a major 
problem, especially the words ?? (and)? can 
appear in the preposition structure ?? ?? ?
? (relate to)?, which makes it difficult to judge 
those types of differences. Third, the chunkers 
can not handle with compact sequence data of 
chunks with name entities and new words (espe-
cially the transliterated words) satisfactorily, 
such as  
??? ( China ) /NR ????( Red Cross ) 
/NR?? ( Honorary ) /NN ?? (Chairman ) 
/NN ???( Jiang Ze-min ) /NR?  
As it points above, the English name entities 
sequences are connected with the conjunction 
such as ?of, and, in?. While in Chinese there are 
no such connection words for name entities se-
quences. Therefore when we use the statistical 
methods, those kinds of sequential chunks con-
tribute slightly to the feature selection and classi-
fier training, and are treated as the useless noise 
in the training data. In the testing section, it is 
close the separating margin and hardly deter-
mined to be in the right category. What?s more, 
some other factors such as Idiomatic and special-
ized expressions also account for the errors. By 
highlighting those kinds of words and using 
some rules which emphasize on those proper 
words, we use our error-pruning methods and 
useful grammar rules to correct about 60% errors.  
5 Conclusions  
This paper presented a new hybrid approach for 
identifying the Chinese base NPs. Our hybrid 
approach uses the SVM and CRF algorithm to 
design the preliminary classifiers for chunking. 
Furthermore with the direct comparison between 
the results from the former chunkers, we figure 
out that those two statistical methods are myopic 
about the compact chunking data of sequential 
noun. With the intention of capturing the syntac-
tic dependence within those sequential chunking 
data, we make use of the conditional probabili-
ties of the chunking tags given the corresponding 
tokens derived from CRF and some simple 
grammar rules to modify the preliminary results.  
The overall results achieve 89.27% precision 
on the base NP chunking. We attempt to explain 
some existing semantic problems and solve a 
certain part of problems, which have been dis-
covered and explained in the paper. Future work 
will concentrate on working out some adaptive 
machine learning methods to make grammar 
rules automatically, select better features and 
employ suitable classifiers for Chinese base NP 
chunking. Finally, the particular Chinese base 
phrase grammars need a complete study, and the 
approach provides a primary solution and 
framework to process an analyses and compari-
sons between Chinese and English parallel base 
NP chunkers. 
Acknowledgments 
This work was partially supported by the Natural 
Science Foundation of China under Grant No. 
60575043, and 60121302, the China-France PRA 
project under Grant No. PRA SI02-05, the Out-
standing Overseas Chinese Scholars Fund of the 
Chinese Academy of Sciences under Grant 
No.2003-1-1, and Nokia (China) Co. Ltd, as well. 
References  
Claire Cardie and David Pierce. 1998. Error-Driven 
Pruning of Treebank Grammars for Base Noun 
Phrase Identification. Proceedings of the 36th ACL 
and COLING-98, 218-224. 
Claire Cardie and David Pierce. 1999. The role of 
lexicalization and pruning for base noun phrase 
grammars. Proceedings of the 16th AAAI, 423-430. 
Dirk Ludtke and Satoshi Sato. 2003. Fast Base NP 
Chunking with Decision Trees ?? Experiments on 
Different POS tag Settings. CICLing 2003, 136-
147. LNC S2588, Springer-Verlag Berlin Heidel-
berg. 
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. 
Introduction to the CoNLL-2000 Shared Task: 
Chunking. Proceedings of CoNLL and LLL-2000, 
127-132. 
Erik F. Tjong Kim Sang, Walter Daelemans, Herv?   
D?ean, Rob Koeling, Yuval Krymolowski, Vasin 
Punyakanok, and Dan Roth. 2000. Applying system 
combination to base noun phrase identification.  
Proceedings of COLING 2000, 857-863. 
Fei Sha and Fernando Pereira. 2003. Shallow Parsing 
with Conditional Random Fields. Proceedings of 
HLT-NAACL 2003, 134-141.  
Heng Li, Jonathan J. Webster, Chunyu Kit, and 
Tianshun Yao. 2003. Transductive HMM based 
Chinese Text Chunking. IEEE NLP-KE 2003, Bei-
jing, 257-262. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. 
Text Chunking using Transformation-Based Learn-
ing.    Proceedings of the Third ACL Workshop on 
Very Large Corpora, 82?94. 
92
Lafferty A. McCallum and F. Pereira. 2001. Condi-
tional random Fields. Proceedings of ICML 2001, 
282-289. 
Rie Kubota Ando and Tong Zhang. 2004. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. RC23462. Technical 
report, IBM. 
Rie Kubota Ando and Tong Zhang. 2005. A High-
Performance Semi-Supervised Learning Method 
for Text Chunking. Proceedings of the 43rd Annual 
Meeting of ACL, 1-9. 
Simon Lacoste-Julien. 2003. Combining SVM with 
graphical models for supervised classification: an 
introduction to Max-Margin Markov Network. 
CS281A Project Report, UC Berkeley. 
Taku Kudo and Yuji Matsumoto. 2001. Chunking 
with support vector machine. Proceeding of the 
NAACL, 192-199. 
Zhang Yuqi and Zhou Qiang. 2002. Chinese Base-
Phrases Chunking. First SigHAN Workshop on 
Chinese Language Processing, COLING-02. 
Zhao Jun and Huang Changling. 1998. A Quasi-
Dependency Model for Structural Analysis of Chi-
nese BaseNPs. 36th Annual Meeting of the ACL 
and 17th International Conference on Computa-
tional Linguistics. 
Zhou Yaqian, Guo YiKun, Huang XuanLing and Wu 
Lide. 2003. Chinese and English Base NP Recogni-
tion on a Maximum Entropy Model. Vol140, No13. 
Journal of Computer Research and Development. 
(In Chinese) 
93
An Information-Theory-Based Feature Type Analysis for the
Modelling of Statistical Parsing
SUI Zhifang ??, ZHAO Jun ?, Dekai WU ?
? Hong Kong University of Science & Technology
Department of Computer Science
Human Language Technology Center
Clear Water Bay, Hong Kong
? Peking University
Department of Computer Science & Technology
Institute of Computational Linguistics
Beijing, China
suizf@icl.pku.edu.cn, zhaojun@cs.ust.hk, dekai@cs.ust.hk
Abstract
The paper proposes an information-theory-
based method for feature types analysis in
probabilistic evaluation modelling for
statistical parsing. The basic idea is that we
use entropy and conditional entropy to
measure whether a feature type grasps some
of the information for syntactic structure
prediction. Our experiment quantitatively
analyzes several feature types? power for
syntactic structure prediction and draws a
series of interesting conclusions.
1  Introduction
In the field of statistical parsing, various
probabilistic evaluation models have been
proposed where different models use different
feature types [Black, 1992] [Briscoe, 1993]
[Brown, 1991] [Charniak, 1997] [Collins, 1996]
[Collins, 1997] [Magerman, 1991] [Magerman,
1992] [Magerman, 1995] [Eisner, 1996]. How to
evaluate the different feature types? effects for
syntactic parsing? The paper proposes an
information-theory-based feature types analysis
model, which uses the measures of predictive
information quantity, predictive information
gain, predictive information redundancy and
predictive information summation to
quantitatively analyse the different contextual
feature types? or feature types combination?s
predictive power for syntactic structure.
  In the following, Section 2 describes the
probabilistic evaluation model for syntactic trees;
Section 3 proposes an information-theory-based
feature type analysis model; Section 4
introduces several experimental issues; Section 5
quantitatively analyses the different contextual
feature types or feature types combination in the
view of information theory and draws a series of
conclusion on their predictive powers for
syntactic structures.
2  The probabilistic evaluation model
for statistical syntactic parsing
Given a sentence, the task of statistical syntactic
parsing is to assign a probability to each
candidate parsing tree that conforms to the
grammar and select the one with highest
probability as the final analysis result. That is:
)|(maxarg STPT
T
best =  (1)
where S denotes the given sentence, T denotes
the set of all the candidate parsing trees that
conform to the grammar, P(T|S) denotes the
probability of parsing tree T for the given
sentence S.
  The task of probabilistic evaluation model in
syntactic parsing is the estimation of P(T|S). In
the syntactic parsing model which uses rule-
based grammar, the probability of a parsing tree
can be defined as the probability of the
derivation which generates the current parsing
tree for the given sentence. That is,
?
?
=
=
?
=
=
=
n
i
ii
n
i
ii
n
ShrP
SrrrrP
SrrrPSTP
1
1
121
21
),|(
),,,,|(
)|,,,()|(


(2)
Where, 121 ,,, ?irrr   denotes a derivation rule
sequence, hi denotes the partial parsing tree
derived from 121 ,,, ?irrr  .
  In order to accurately estimate the parameters,
we need to select some feature types
mFFF ,,, 21  , depending on which we can
divide the contextual condition Shi ,  for
predicting rule ri into some equivalence classes,
that is, ],[, ,,, 21 ShSh iFFFi m???? ??  , so that
??
==
?
n
i
ii
n
i
ii ShrPShrP
11
]),[|(),|(  (3)
According to the equation of (2) and (3), we
have the following equation:
?
=
?
n
i
ii ShrPSTP
1
]),[|()|(  (4)
  In this way, we can get a unite expression of
probabilistic evaluation model for statistical
syntactic parsing. The difference among the
different parsing models lies mainly in that they
use different feature types or feature type
combination to divide the contextual condition
into equivalent classes. Our ultimate aim is to
determine which combination of feature types is
optimal for the probabilistic evaluation model of
statistical syntactic parsing. Unfortunately, the
state of knowledge in this regard is very limited.
Many probabilistic evaluation models have been
published inspired by one or more of these
feature types [Black, 1992] [Briscoe, 1993]
[Charniak, 1997] [Collins, 1996] [Collins, 1997]
[Magerman, 1995] [Eisner, 1996], but
discrepancies between training sets, algorithms,
and hardware environments make it difficult, if
not impossible, to compare the models
objectively. In the paper, we propose an
information-theory-based feature type analysis
model by which we can quantitatively analyse
the predictive power of different feature types or
feature type combinations for syntactic structure
in a systematic way. The conclusion is expected
to provide reliable reference for feature type
selection in the probabilistic evaluation
modelling for statistical syntactic parsing.
3 The information-theory-based
feature type analysis model for statistical
syntactic parsing
In the prediction of stochastic events, entropy
and conditional entropy can be used to evaluate
the predictive power of different feature types.
To predict a stochastic event, if the entropy of
the event is much larger than its conditional
entropy on condition that a feature type is
known, it indicates that the feature type grasps
some of the important information for the
predicted event.
  According to the above idea, we build the
information-theory-based feature type analysis
model, which is composed of four concepts:
predictive information quantity, predictive
information gain, predictive information
redundancy and predictive information
summation.
z Predictive Information Quantity (PIQ)
);( RFPIQ , the predictive information quantity
of feature type F to predict derivation rule R, is
defined as the difference between the entropy of
R and the conditional entropy of R on condition
that the feature type F is known.
?
?? ?
=
?=
RrFf rPfP
rfP
rfP
FRHRHRFPIQ
,
)()(
),(log),(
)|()();(
   (5)
  Predictive information quantity can be used to
measure the predictive power of a feature type in
feature type analysis.
z Predictive Information Gain (PIG)
For the prediction of rule R,
PIG(Fx;R|F1,F2,...,Fi), the predictive information
gain of taking Fx as a variant model on top of a
baseline model employing F1,F2,...,Fi as feature
type combination, is defined as the difference
between the conditional entropy of predicting R
based on feature type combination F1,F2,...,Fi
and the conditional entropy of predicting R
based on feature type combination F1,F2,...,Fi,Fx.
)6(),,,(
),,(
),,,(
),,,,(log),,,,(
),,,|(),,|(),,|;(
1
1
1
1
1
111
11
rffP
ffP
fffP
rfffP
rfffP
FFFRHFFRHFFRFPIG
i
i
xi
xi
Rr
Ff
Ff
Ff
xi
xiiix
xx
ii







?=
?=
?
?
?
?
?
 If ),,,|;(),,,|;( 2121 iyix FFFRFPIGFFFRFPIG  > ,
then Fx is deemed to be more informative than
Fy for predicting R on top of F1,F2,...,Fi, no
matter whether PIQ(Fx;R) is larger than
PIQ(Fy;R) or not.
z Predictive Information Redundancy(PIR)
Based on the above two definitions, we can
further draw the definition of predictive
information redundancy as follows.
PIR(Fx,{F1,F2,...,Fi};R) denotes the redundant
information between feature type Fx and feature
type combination {F1,F2,...,Fi} in predicting R,
which is defined as the difference between
PIQ(Fx;R) and PIG(Fx;R|F1,F2,...,Fi). That is,
),,,|;();(
)};,,,{,(
21
21
ixx
ix
FFFRFPIGRFPIQ
RFFFFPIR


?=
 (7)
  Predictive information redundancy can be
used as a measure of the redundancy between
the predictive information of a feature type and
that of a feature type combination.
z Predictive Information Summation (PIS)
PIS(F1,F2,...,Fm;R), the predictive information
summation of feature type combination
F1,F2,...,Fm, is defined as the total information
that F1,F2,...,Fm can provide for the prediction of
a derivation rule. Exactly,
?
=
?
+=
m
i
ii
m
FFRFPIGRFPIQ
RFFFPIS
2
111
21
),,|;();(
);,,,(


 (8)
4 Experimental Issues
4.1 The classification of the feature
types
The predicted event of our experiment is the
derivation rule to extend the current non-
terminal node. The feature types for prediction
can be classified into two classes, history feature
types and objective feature types. In the
following, we will take the parsing tree shown in
Figure-1 as the example to explain the
classification of the feature types.
In Figure-1, the current predicted event is the
derivation rule to extend the framed non-
terminal node VP, the part connected by the
solid line belongs to history feature types, which
is the already derived partial parsing tree,
representing the structural environment of the
current non-terminal node. The part framed by
the larger rectangle belongs to the objective
feature types, which is the word sequence
containing the leaf nodes of the partial parsing
tree rooted by the current node, representing the
final objectives to be derived from the current
node.
4.2 The corpus used in the experiment
The experimental corpus is derived from Penn
TreeBank[Marcus,1993]. We semi-
automatically assign a headword and a POS tag
to each non-terminal node. 80% of the corpus
(979,767 words) is taken as the training set, used
for estimating the various co-occurrence
probabilities, 10% of the corpus (133,814 words)
is taken as the testing set, used to calculate
predictive information quantity, predictive
information gain, predictive information
redundancy and predictive information
summation. The other 10% of the corpus
(133,814 words) is taken as the held-out set. The
grammar rule set is composed of 8,126 CFG
rules extracted from Penn TreeBank.
S
V P
V P
N N P
Pierre
N N P
Vinken
M D
will
V B
join
D T
the
N N
board
IN
as
D T
a
JJ
nonexecut ive
N N
director
N N P
Nov.
C D
29
.
.
N P N P N P
P P
N P
Figure-1: The classification of feature types
4.3 The smoothing method used in the
experiment
In the information-theory-based feature type
analysis model, we need to estimate joint
probability ),,,,( 21 rfffP i . Let F1,F2,...,Fi be
the feature type series selected till now,
RrFfFfFf ii ???? ,,,, 2211  , we use a
blended probability ),,,,(~ 21 rfffP i  to
approximate probability ),,,,( 21 rfffP i  in
order to solve the sparse data problem[Bell,
1992].
  
?
=
??
++=
i
j
jj
i
rfffPwrPwrPw
rfffP
1
210011
21
),,,,()()(
),,,,(~


   (9)
In the above formula,
         
?
?
?
=
Rr
rc
rP
?
1 )?(
1)(
              (10)
  
?
?
=
Rr
rc
rc
rP
?
0 )?(
)()(                (11)
where )(rc is the total number of time that r has
been seen in the corpus.
  According to the escape mechanism in [Bell,
1992], we define the weights wk )1( ik ?<?  in
the formula (9) as follows.
   
ii
i
ks
skk
ew
ikeew
?=
????= ?
+=
1
1,)1(
1       (12)
where ek denotes the escape probability of
context ),,,( 21 kfff   , that is, the probability
in which (f1 , f2 , ... , fk , r) is unseen in the corpus.
In such case, the blending model has to escape
to the lower contexts to approximate
),,,,( 21 rfffP k . Exactly, escape probability is
defined as
  
???
???
?
?=
??
= ?
?
?
?
1,0
0,)?,,...,,(
)?,,...,,(
?
21
?
21
k
ik
rfffc
rfffd
e
Rr
k
Rr
k
k    (13)
where
   
??
?
=
>
=
0)?,,...,,(,0
0)?,,...,,(,1)?,,...,,(
21
21
21
rfffcif
rfffcif
rfffd
k
k
k  (14)
In the above blending model, a special
probability ?
?
?
=
Rr
rc
rP
?
1 )?(
1)(  is used, where all
derivation rules are given an equal probability.
As a result, 0),,,,(~ 21 >rfffP i  as long as
0)?(
?
>?
?Rr
rc .
5 The information-theory-based
feature type analysis
The experiments led to a number of interesting
conclusions on the predictive power of various
feature types and feature type combinations,
which is expected to provide reliable reference
for the modelling of probabilistic parsing.
5.1 The analysis to the predictive
information quantities of lexical feature
types, part-of-speech feature types and
constituent label feature types
z Goal
One of the most important variation in statistical
parsing over the last few years is that statistical
lexical information is incorporated into the
probabilistic evaluation model. Some statistical
parsing systems show that the performance is
improved after the lexical information is added.
Our research aims at a quantitative analysis of
the differences among the predictive information
quantities provided by the lexical feature types,
part-of-speech feature types and constituent
label feature types from the view of information
theory.
z Data
The experiment is conducted on the history
feature types of the nodes whose structural
distance to the current node is within 2.
  In Table-1, ?Y? in PIQ(X of Y; R) represents
the node, ?X? represents the constitute label, the
headword or POS of the headword of the node.
In the following, the units of PIQ are bits.
z Conclusion
Among the feature types in the same structural
position of the parsing tree, the predictive
information quantity of lexical feature type is
larger than that of part-of-speech feature type,
and the predictive information quantity of part-
of-speech feature type is larger than that of the
constituent label feature type.
Table-1: The predictive information quantity of the history feature type candidates
PIQ(X of Y; R) X= constituent label X= headword X= POS of
the headword
Y= the current node 2.3609 3.7333 2.7708
Y= the parent 1.1598 2.3253 1.1784
Y= the grandpa 0.6483 1.6808 0.6612
Y= the first right brother of the current node 0.4730 1.1525 0.7502
Y= the first left brother of the current node 0.5832 2.1511 1.2186
Y= the second right brother of the current node 0.1066 0.5044 0.2525
Y= the second left brother of the current node 0.0949 0.6171 0.2697
Y= the first right brother of the parent 0.1068 0.3717 0.2133
Y= the first left brother of the parent 0.2505 1.5603 0.6145
5.2 The analysis to the influence of the
structural relation and the structural
distance to the predictive information
quantities of the history feature types
z Goal:
In this experiment, we wish to find out the
influence of the structural relation and structural
distance between the current node and the node
that the given feature type related to has to the
predictive information quantities of these feature
types.
z Data:
In Table-2, SR represents the structural relation
between the current node and the node that the
given feature type related to. SD represents the
structural distance between the current node and
the node that the given feature type related to.
Table-2: The predictive information quantity of the selected history feature types
PIQ(constituent label
of Y; R)
SR= parent relation SR= brother relation SR= mixed parent and
brother relation
0.5832
(Y= the first left brother)
SD=1 1.1598
(Y= the parent)
0.4730
(Y= the first right brother)
0.2505
(Y= the first left brother
of the parent)
0.0949
(Y= the second left brother)
SD=2 0.6483
(Y= the grandpa)
0.1066
(Y= the second right brother)
0.1068
(Y= the first right
brother of the parent)
z Conclusion
Among the history feature types which have the
same structural relation with the current node
(the relations are both parent-child relation, or
both brother relation, etc), the one which has
closer structural distance to the current node will
provide larger predictive information quantity;
Among the history feature types which have the
same structural distance to the current node, the
one which has parent relation with the current
node will provide larger predictive information
quantity than the one that has brother relation or
mixed parent and brother relation to the current
node (such as the parent's brother node).
5.3 The analysis to the predictive
information quantities of the history
feature types and the objective feature
types
z Goal
Many of the existing probabilistic evaluation
models prefer to use history feature types other
than objective feature types. We select some of
history feature types and objective feature types,
and quantitatively compare their predictive
information quantities.
z Data
The history feature type we use here is the
headword of the parent, which has the largest
predictive information quantity among all the
history feature types. The objective feature types
are selected stochastically, which are the first
word and the second word in the objective word
sequence of the current node (Please see 4.1 and
Figure-1 for detailed descriptions on the selected
feature types).
Table-3: The predictive information quantity of the selected history and objective feature types
Class Feature type PIQ(Y;R)
History feature type Y= headword of the parent 2.3253
Y= the first word in the objective word sequence 3.2398Objective feature type
Y= the second word in the objective word sequence 3.0071
z Conclusion
Either of the predictive information quantity of
the first word and the second word in the
objective word sequence is larger than that of
the headword of the parent node which has the
largest predictive information quantity among all
of the history feature type candidates. That is to
say, objective feature types may have larger
predictive power than that of the history feature
type.
5.4 The analysis to the predictive
information quantities of the objective
features types selected respectively on the
physical position information, the
heuristic information of headword and
modifier, and the exact headword
information
z Goal
Not alike the structural history feature types, the
objective feature types are sequential. Generally,
the candidates of the objective feature types are
selected according to the physical position.
However, from the linguistic viewpoint, the
physical position information can hardly grasp
the relations between the linguistic structures.
Therefore, besides the physical position
information, our research try to select the
objective feature types respectively according to
the exact headword information and the heuristic
information of headword and modifier. Through
the experiment, we hope to find out what
influence the exact headword information, the
heuristic information of headword and modifier,
and the physical position information have
respectively to the predictive information
quantities of the feature types.
z Data:
  Table-4 gives the evidence for the claim.
Table-4: the predictive information quantity of the selected objective feature types
the information used to select the objective
feature types
PIQ(Y;R)
the physical position information 3.2398
(Y= the first word in the objective word sequence)
Heuristic information 1: determine whether a
word has the possibility to act as the headword of
the current constitute according to its POS
3.1401
(Y= the first word in the objective word sequence
which has the possibility to act as the headword of
the current constitute)
Heuristic information 2: determine whether a
word has the possibility to act as the modifier of
the current constitute according to its POS
3.1374
(Y= the first word in the objective word sequence
which has the possibility to act as the modifier of the
current constitute)
Heuristic information 3: given the current
headword, determine whether a word has the
possibility to modify the headword
2.8757
(Y= the first word in the objective word sequence
which has the possibility to modify the headword)
the exact headword information 3.7333
(Y= the headword of the current constitute)
z Conclusion
The predictive information quantity of the
headword of the current node is larger than that
of a feature type selected according to the
selected heuristic information of headword or
modifier, and larger than that of a feature type
selected according to the physical positions; The
predictive information quantity of a feature type
selected according to the physical positions is
larger than that of a feature types selected
according to the selected heuristic information
of headword or modifier.
5.5 The selection of the feature type
combination which has the optimal
predictive information summation
z Goal:
We aim at proposing a method to select the
feature types combination that has the optimal
predictive information summation for prediction.
z Approach
We use the following greedy algorithm to select
the optimal feature type combination.
  In building a model, the first feature type to
be selected is the feature type which has the
largest predictive information quantity for the
prediction of the derivation rule among all of the
feature type candidates, that is,
);(maxarg1 RFPIQF i
Fi ??
=    (15)
Where ?  is the set of candidate feature types.
  Given that the model has selected feature type
combination jFFF ,,, 21  , the next feature
type to be added into the model is the feature
type which has the largest predictive information
gain in all of the feature type candidate except
jFFF ,,, 21  , on condition that jFFF ,,, 21 
is known. That is,
)16(),,,|;( 21
},,2,1{
1 maxarg ji
jFFFiF
iF
j FFFRFPIGF 
?
??
+ =
z Data:
Among the feature types mentioned above, the
optimal feature type combination (i.e. the feature
type combination with the largest predictive
information summation) which is composed of 6
feature types is, the headword of the current
node (type1), the headword of the parent node
(type2), the headword of the grandpa node
(type3), the first word in the objective word
sequence(type4), the first word in the objective
word sequence which have the possibility to act
as the headword of the current constitute(type5),
the headword of the right brother node(type6).
The cumulative predictive information
summation is showed in Figure-2
0
1
2
3
4
5
6
7
type1 type2 type3 type4 type5 type6
feature type
cu
m
m
u
la
tiv
e 
pr
ed
ic
tin
g 
in
fo
rm
at
io
n
su
m
m
at
io
n
Figure-2: The cumulative predictive information summation of the feature type combinations
6 Conclusion
The paper proposes an information-theory-based
feature type analysis method, which not only
presents a series of heuristic conclusion on the
predictive power of the different feature types
and feature type combination for syntactic
parsing, but also provides a guide for the
modeling of syntactic parsing in the view of
methodology, that is, we can quantitatively
analyse the different contextual feature types or
feature types combination's effect for syntactic
structure prediction in advance. Based on these
analysis, we can select the feature type or feature
types combination that has the optimal
predictive information summation to build the
probabilistic parsing model.
  However, there are still some questions to be
answered in this paper. For example, what is the
beneficial improvement in the performance after
using this method in a real parser? Whether the
improvements in PIQ will lead to the
improvement of parsing accuracy or not? In the
following research, we will incorporate these
conclusions into a real parser to see whether the
parsing accuracy can be improved or not.
Another work we will do is to do some
experimental analysis to find the impact of data
sparseness on feature type analysis, which is
critical to the performance of real systems.
  The proposed feature type analysis method
can be used in not only the probabilistic
modelling for statistical syntactic parsing, but
also language modelling in more general fields
[WU, 1999a] [WU, 1999b].
References
Bell, T.C., Cleary, J.G., Witten,I.H. 1992. Text
Compression, PRENTICE HALL, Englewood
Cliffs, New Jersey 07632, 1992
Black, E., Jelinek, F.,Lafferty, J.,Magerman, D.M.,
Mercer, R. and Roukos, S. 1992.  Towards
history-based grammars: using richer models of
context in probabilistic parsing. In Proceedings of
the February 1992 DARPA Speech and Natural
Language Workshop, Arden House, NY.
Brown, P., Jelinek, F., & Mercer, R. 1991. Basic
method of probabilistic context-free grammars.
IBM internal Report, Yorktown Heights, NY.
T.Briscoe and J. Carroll. 1993. Generalized LR
parsing of natural language (corpora) with
unification-based grammars. Computational
Linguistics, 19(1): 25-60
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statics. In
Proceedings of the Fourteenth National Conference
on Artificial Intelligence, AAAI Press/MIT Press,
Menlo Park.
Stanley F. Chen and Joshua Goodman. 1999.  An
Empirical Study of Smoothing Techniques for
Language Modeling. Computer Speech and
Language, Vol.13, 1999
Michael John Collins. 1996.  A new statistical
parser based on bigram lexical dependencies. In
Proceedings of the 34th Annual Meeting of the
ACL.
Michael John Collins. 1997. Three generative
lexicalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of the
ACL.
J.Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In
Proceedings of COLING-96, pages 340-345
Joshua Goodman. 1998. Parsing Inside-Out. PhD.
Thesis, Harvard University, 1998
Magerman, D.M. and Marcus, M.P. 1991. Pearl: a
probabilistic chart parser. In Proceedings of the
European ACL Conference, Berlin, Germany.
Magerman, D.M. and Weir, C. 1992. Probabilistic
prediction and Picky chart parsing. In Proceedings
of the February 1992 DARPA Speech and Natural
Language Workshop, Arden House, NY.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33th
Annual Meeting of the ACL.
Mitchell P. Marcus, Beatrice Santorini & Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank.
Computational Linguistics 19, pages 313-330
C. E. Shannon. 1951. Prediction and Entropy of
Printed English. Bell System Technical Journal,
1951
Dekai,Wu, Sui Zhifang, Zhao Jun. 1999a. An
Information-Based Method for Selecting Feature
Types for Word Prediction. Proceedings of
Eurospeech'99, Budapest Hungary
Dekai, Wu, Zhao Jun, Sui Zhifang. 1999b. An
Information-Theoretic Empirical Analysis of
Dependency-Based Feature Types for Word
Prediction Models. Proceedings of EMNLP'99,
University of Maryland, USA
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 50?59,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Structural Semantic Relatedness: A Knowledge-Based Method to 
Named Entity Disambiguation 
 
Xianpei Han        Jun Zhao? 
National Laboratory of Pattern Recognition  
 Institute of Automation, Chinese Academy of Sciences 
Beijing 100190, China  
{xphan,jzhao}@nlpr.ia.ac.cn 
 
  
 
                                                 
? Corresponding author 
Abstract 
 
Name ambiguity problem has raised urgent 
demands for efficient, high-quality named ent-
ity disambiguation methods. In recent years, 
the increasing availability of large-scale, rich 
semantic knowledge sources (such as Wikipe-
dia and WordNet) creates new opportunities to 
enhance the named entity disambiguation by 
developing algorithms which can exploit these 
knowledge sources at best. The problem is that 
these knowledge sources are heterogeneous 
and most of the semantic knowledge within 
them is embedded in complex structures, such 
as graphs and networks. This paper proposes a 
knowledge-based method, called Structural 
Semantic Relatedness (SSR), which can en-
hance the named entity disambiguation by 
capturing and leveraging the structural seman-
tic knowledge in multiple knowledge sources. 
Empirical results show that, in comparison 
with the classical BOW based methods and 
social network based methods, our method can 
significantly improve the disambiguation per-
formance by respectively 8.7% and 14.7%. 
1 Introduction 
Name ambiguity problem is common on the Web. 
For example, the name ?Michael Jordan? 
represents more than ten persons in the Google 
search results. Some of them are shown below: 
Michael (Jeffrey) Jordan, Basketball Player 
Michael (I.) Jordan, Professor of Berkeley 
Michael (B.) Jordan, American Actor 
The name ambiguity has raised serious prob-
lems in many relevant areas, such as web person 
search, data integration, link analysis and know-
ledge base population. For example, in response 
to a person query, search engine returns a long, 
flat list of results containing web pages about 
several namesakes. The users are then forced 
either to refine their query by adding terms, or to 
browse through the search results to find the per-
son they are seeking. Besides, an ever-increasing 
number of question answering and information 
extraction systems are coming to rely on data 
from multi-sources, where name ambiguity will 
lead to wrong answers and poor results. For ex-
ample, in order to extract the birth date of the 
Berkeley professor Michael Jordan, a system 
may return the birth date of his popular name-
sakes, e.g., the basketball player Michael Jordan. 
So there is an urgent demand for efficient, 
high-quality named entity disambiguation me-
thods. Currently, the common methods for 
named entity disambiguation include name ob-
servation clustering (Bagga and Baldwin, 1998) 
and entity linking with knowledge base (McNa-
mee and Dang, 2009). In this paper, we focus on 
the method of name observation clustering. Giv-
en a set of observations O = {o1, o2, ?, on} of the 
target name to be disambiguated, a named entity 
disambiguation system should group them into a 
set of clusters C = {c1, c2, ?, cm}, with each re-
sulting cluster corresponding to one specific enti-
ty. For example, consider the following four ob-
servations of Michael Jordan: 
1) Michael Jordan is a researcher in Computer 
Science. 
2) Michael Jordan plays basketball in Chicago Bulls. 
3) Michael Jordan wins NBA MVP. 
4) Learning in Graphical Models: Michael Jordan. 
A named entity disambiguation system should 
group the 1st and 4th Michael Jordan observations 
into one cluster for they both refer to the Berke-
50
ley professor Michael Jordan, meanwhile group 
the other two Michael Jordan into another clus-
ter as they refer to another person, the Basketball 
Player Michael Jordan. 
To a human, named entity disambiguation is 
usually not a difficult task as he can make deci-
sions depending on not only contextual clues, but 
also the prior background knowledge. For exam-
ple, as shown in Figure 1, with the background 
knowledge that both Learning and Graphical 
models are the topics related to Machine learning, 
while Machine learning is the sub domain of 
Computer science, a human can easily determine 
that the two Michael Jordan in the 1st and 4th ob-
servations represent the same person. In the same 
way, a human can also easily identify that the 
two Michael Jordan in the 2nd and 3rd observa-
tions represent the same person. 
 
Figure 1. The exploitation of knowledge in human 
named entity disambiguation 
The development of systems which could rep-
licate the human disambiguation ability, however, 
is not a trivial task because it is difficult to cap-
ture and leverage the semantic knowledge as 
humankind. Conventionally, the named entity 
disambiguation methods measure the similarity 
between name observations using the bag of 
words (BOW) model (Bagga and Baldwin (1998); 
Mann and Yarowsky (2006); Fleischman and 
Hovy (2004); Pedersen et al (2005)), where a 
name observation is represented as a feature vec-
tor consisting of the contextual terms. This mod-
el measures similarity based on only the co-
occurrence statistics of terms, without consider-
ing all the semantic relations like social related-
ness between named entities, associative related-
ness between concepts, and lexical relatedness 
(e.g., acronyms, synonyms) between key terms. 
 
Figure 2. Part of the link structure of Wikipedia 
Fortunately, in recent years, due to the evolu-
tion of Web (e.g., the Web 2.0 and the Semantic 
Web) and many research efforts for the construc-
tion of knowledge bases, there is an increasing 
availability of large-scale knowledge sources, 
such as Wikipedia and WordNet. These large-
scale knowledge sources create new opportuni-
ties for knowledge-based named entity disam-
biguation methods as they contain rich semantic 
knowledge. For example, as shown in Figure 2, 
the link structure of Wikipedia contains rich se-
mantic relations between concepts. And we be-
lieve that the disambiguation performance can be 
greatly improved by designing algorithms which 
can exploit these knowledge sources at best. 
The problem of these knowledge sources is 
that they are heterogeneous (e.g., they contain 
different types of semantic relations and different 
types of concepts) and most of the semantic 
knowledge within them is embedded in complex 
structures, such as graphs and networks. For ex-
ample, as shown in Figure 2, the semantic rela-
tion between Graphical Model and Computer 
Science is embedded in the link structure of the 
Wikipedia. In recent years, some research has 
investigated to exploit some specific semantic 
knowledge, such as the social connection be-
tween named entities in the Web (Kalashnikov et 
al. (2008), Wan et al (2005) and Lu et al 
(2007)), the ontology connection in DBLP (Has-
sell et al, 2006) and the semantic relations in 
Wikipedia (Cucerzan (2007), Han and Zhao 
(2009)). These knowledge-based methods, how-
ever, usually are specialized to the knowledge 
sources they used, so they often have the know-
ledge coverage problem. Furthermore, these me-
thods can only exploit the semantic knowledge to 
a limited extent because they cannot take the 
structural semantic knowledge into consideration. 
To overcome the deficiencies of previous me-
thods, this paper proposes a knowledge-based 
method, called Structural Semantic Relatedness 
(SSR), which can enhance the named entity dis-
ambiguation by capturing and leveraging the 
structural semantic knowledge from multiple 
knowledge sources. The key point of our method 
is a reliable semantic relatedness measure be-
tween concepts (including WordNet concepts, 
NEs and Wikipedia concepts), called Structural 
Semantic Relatedness, which can capture both 
the explicit semantic relations between concepts 
and the implicit semantic knowledge embedded 
in graphs and networks. In particular, we first 
extract the semantic relations between two con-
cepts from a variety of knowledge sources and 
Computer Science
Machine learning
Statistics 
Graphical model Learning
Mathematic 
Probability Theory 
2) Michael Jordan plays basketball in Chicago Bulls. 
1) Michael Jordan is a researcher in Computer Science.
4) Learning in Graphical Models: Michael Jordan
3) Michael Jordan wins NBA MVP. 
Machine learning 
51
represent them using a graph-based model, se-
mantic-graph. Then based on the principle that 
?two concepts are semantic related if they are 
both semantic related to the neighbor concepts of 
each other?, we construct our Structural Seman-
tic Relatedness measure. In the end, we leverage 
the structural semantic relatedness measure for 
named entity disambiguation and evaluate the 
performance on the standard WePS data sets. 
The experimental results show that our SSR me-
thod can significantly outperform the traditional 
methods. 
This paper is organized as follows. Section 2 
describes how to construct the structural seman-
tic relatedness measure. Next in Section 3 we 
describe how to leverage the captured knowledge 
for named entity disambiguation. Experimental 
results are demonstrated in Sections 4. Section 5 
briefly reviews the related work. Section 6 con-
cludes this paper and discusses the future work. 
2 The Structural Semantic Relatedness 
Measure 
In this section, we demonstrate the structural se-
mantic relatedness measure, which can capture 
the structural semantic knowledge in multiple 
knowledge sources. Totally, there are two prob-
lems we need to address: 
1) How to extract and represent the seman-
tic relations between concepts, since there are 
many types of semantic relations and they may 
exist as different patterns (the semantic know-
ledge may exist as explicit semantic relations or 
be embedded in complex structures). 
2) How to capture all the extracted seman-
tic relations between concepts in our semantic 
relatedness measure. 
To address the above two problems, in follow-
ing we first introduce how to extract the semantic 
relations from multiple knowledge sources; then 
we represent the extracted semantic relations us-
ing the semantic-graph model; finally we build 
our structural semantic relatedness measure. 
2.1 Knowledge Sources 
We extract three types of semantic relations (se-
mantic relatedness between Wikipedia concepts, 
lexical relatedness between WordNet concepts 
and social relatedness between NEs) correspon-
dingly from three knowledge sources: Wikipedia, 
WordNet and NE Co-occurrence Corpus. 
1. Wikipedia1, a large-scale online encyc-
lopedia, its English version includes more than 
3,000,000 concepts and new articles are added 
quickly and up-to-date. Wikipedia contains rich 
semantic knowledge in the form of hyperlinks 
between Wikipedia articles, such as Polysemy 
(disambiguation pages), Synonym (redirect pages) 
and Associative relation (hyperlinks between 
Wikipedia articles). In this paper, we extract the 
semantic relatedness sr between Wikipedia con-
cepts using the method described in Milne and 
Witten(2008): 
log(max( )) log( )
( , ) 1
log( ) log(min( , ))
A B A B
sr a b
W A B
?= ? ?
??
 
where a and b are the two concepts of interest, A 
and B are the sets of all the concepts that are re-
spectively linked to a and b, and W is the entire 
Wikipedia. For demonstration, we show the se-
mantic relatedness between four selected con-
cepts in Table 1. 
 Statistics Basketball
Machine learning 0.58 0.00 
MVP 0.00 0.45 
Table 1. The semantic relatedness table of four se-
lected Wikipedia concepts 
2. WordNet 3.02 (Fellbaum et al, 1998), a 
lexical knowledge source includes over 110,000 
WordNet concepts (word senses about English 
words). Various lexical relations are recorded 
between WordNet concepts, such as hyponyms, 
holonym and synonym. The lexical relatedness lr 
between two WordNet concepts are measured 
using the Lin (1998)?s WordNet semantic simi-
larity measure. Table 2 shows some examples of 
the lexical relatedness. 
 school science 
university 0.67 0.10 
research 0.54 0.39 
Table 2. The lexical relatedness table of four selected 
WordNet concepts 
3. NE Co-occurrence Corpus, a corpus of 
documents for capturing the social relatedness 
between named entities. According to the fuzzy 
set theory (Baeza-Yates et al, 1999), the degree 
of named entities co-occurrence in a corpus is a 
measure of the relatedness between them. For 
example, in Google search results, the ?Chicago 
Bulls? co-occurs with ?NBA? in more than 
                                                 
1 http://www.wikipedia.org/ 
2 http:// wordnet.princeton.edu/ 
52
7,900,000 web pages, while only co-occurs with 
?EMNLP? in less than 1,000 web pages. So the 
co-occurrence statistics can be used to measure 
the social relatedness between named entities. In 
this paper, given a NE Co-occurrence Corpus D, 
the social relatedness scr between two named 
entities ne1 and ne2 is measured using the Google 
Similarity Distance (Cilibrasi and Vitanyi, 2007): 
1 2 1 2
1 2
1 2
log(max( , )) log( )
( , ) 1
log( ) log(min( , ))
D D D D
scr ne ne
D D D
?= ? ?
?  
where D1 and D2 are the document sets corres-
pondingly containing ne1 and ne2. An example of 
social relatedness is shown in Table 3, which is 
computed using the Web corpus through Google. 
 ACL NBA 
EMNLP 0.61 0.00 
Chicago Bulls 0.19 0.55 
Table 3. The social relatedness table of four selected 
named entities 
2.2 The Semantic-Graph Model 
In this section we present a graph-based repre-
sentation, called semantic-graph, to model the 
extracted semantic relations as a graph within 
which the semantic relations are interconnected 
and transitive. Concretely, the semantic-graph is 
defined as follows: 
A semantic-graph is a weighted graph G = (V, 
E), where each node represents a distinct con-
cept; and each edge between a pair of nodes 
represents the semantic relation between the 
two concepts corresponding to these nodes, 
with the edge weight indicating the strength of 
the semantic relation. 
For demonstration, Figure 3 shows a semantic-
graph which models the semantic knowledge 
extracted from Wikipedia for the Michael Jordan 
observations in Section 1. 
 
Figure 3. An example of semantic-graph 
Given a set of name observations, the con-
struction of semantic-graph takes two steps: con-
cept extraction and concept connection. In the 
following we respectively describe each step. 
1) Concept Extraction. In this step we ex-
tract all the concepts in the contexts of name ob-
servations and represent them as the nodes in the 
semantic-graph. We first gather all the N-grams 
(up to 8 words) and identify whether they corres-
pond to semantically meaningful concepts: if a 
N-gram is contained in the WordNet, we identify 
it as a WordNet concept, and use its primary 
word sense as its semantic meaning; to find 
whether a N-gram is a named entity, we match it 
to the named entity list extracted using the open-
Calais API3, which contains more than 30 types 
of named entities, such as Person, Organization 
and Award; to find whether a N-gram is a Wiki-
pedia concept, we match it to the Wikipedia anc-
hor dictionary, then find its corresponding Wiki-
pedia concept using the method described in 
(Medelyan et al 2008). After concept identifica-
tion, we filter out all the N-grams which do not 
correspond to the semantic meaningful concepts, 
such as the N-grams ?learning in? and ?wins 
NBA MVP?. The retained N-grams are identified 
as concepts, corresponding with their semantic 
meanings (a concept may have multiple semantic 
meaning explanation, e.g., the ?MVP? has three 
semantic meaning, as ?most valuable player, 
MVP? in WordNet, as the ?Most Valuable Play-
er? in Wikipedia and as a named entity of Award 
type). 
2) Concept Connection. In this step we 
represent the semantic relations as the edges be-
tween nodes. That is, for each pair of extracted 
concepts, we identify whether there are semantic 
relations between them: 1) If there is only one 
semantic relation between them, we connect 
these two concepts with an edge, where the edge 
weight is the strength of the semantic relation; 2) 
If there is more than one semantic relations be-
tween them, we choose the most reliable seman-
tic relation, i.e., we choose the semantic relation 
in the knowledge sources according to the order 
of WordNet, Wikipedia and NE Co-concurrence 
corpus (Suchanek et al, 2007). For example, if 
both Wikipedia and WordNet provide the seman-
tic relation between MVP and NBA, we choose 
the semantic relation provided by WordNet. 
                                                 
3 http://www.opencalais.com/ 
Researcher Graphical  
Model 
Learning 
NBA
MVP
Basketball 
Chicago Bulls 
Computer 
Science 
0.32 0.28 
0.48 
0.41 
0.58 
0.76 
0.45 
0.71 
0.71 0.57 
53
2.3 The Structural Semantic Relatedness 
Measure 
In this section, we describe how to capture the 
semantic relations between the concepts in se-
mantic-graph using a semantic relatedness meas-
ure. Totally, the semantic knowledge between 
concepts is modeled in two forms: 
1) The edges of semantic-graph. The 
edges model the direct semantic relations be-
tween concepts. We call this form of semantic 
knowledge as explicit semantic knowledge. 
2) The structure of semantic-graph. Ex-
cept for the edges, the structure of the semantic-
graph also models the semantic knowledge of 
concepts. For example, the neighbors of a con-
cept represent all the concepts which are explicit-
ly semantic-related to this concept; and the paths 
between two concepts represent all the explicit 
and implicit semantic relations between them. 
We call this form of semantic knowledge as 
structural semantic knowledge, or implicit se-
mantic knowledge. 
Therefore, in order to deduce a reliable seman-
tic relatedness measure, we must take both the 
edges and the structure of semantic-graph into 
consideration. Under the semantic-graph model, 
the measurement of semantic relatedness be-
tween concepts equals to quantifying the similar-
ity between nodes in a weighted graph. To simpl-
ify the description, we assign each node in se-
mantic-graph an integer index from 1 to |V| and 
use this index to represent the node, then we can 
write the adjacency matrix of the semantic-graph 
G as A, where A[i,j] or Aij is the edge weight be-
tween node i and node j. 
The problem of quantifying the relatedness be-
tween nodes in a graph is not a new problem, e.g., 
the structural equivalence and structural similar-
ity (the SimRank in Jeh and Widom (2002) and 
the similarity measure in Leicht et al (2006)). 
However, these similarity measures are not suit-
able for our task, because all of them assume that 
the edges are uniform so that they cannot take 
edge weight into consideration. 
In order to take both the graph structure and 
the edge weight into account, we design the 
structural semantic relatedness measure by ex-
tending the measure introduced in Leicht et al 
(2006). The fundamental principle behind our 
measure is ?a node u is semantically related to 
another node v if its immediate neighbors are 
semantically related to v?. This definition is natu-
ral, for example, as shown in Figure 3, the con-
cept Basketball and its neighbors NBA and Chi-
cago Bulls are all semantically related to MVP. 
This definition is recursive, and the starting point 
we choose is the semantic relatedness in the edge. 
Thus our structural semantic relatedness has two 
components: the neighbor term of the previous 
recursive phase which captures the graph struc-
ture and the semantic relatedness which captures 
the edge information. Thus, the recursive form of 
the structural semantic relatedness Sij between 
the node i and the node j can be written as: 
i
il
ij lj ij
l N i
A
S S A
d
? ?
?
= +?  
where ?  and ?  control the relative importance 
of the two components and 
Ni={j | Aij > 0} is the set of the immediate 
neighbors of node i; 
j Ni
d Aiji ?
?= is the degree of node i. 
In order to solve this formula, we introduce the 
following two notations: 
T: The relatedness transition matrix, where 
T[i,j]=Aij/di, indicating the transition rate of re-
latedness from node j to its neighbor i. 
S: The structural semantic relatedness matrix, 
where S[i,j]=Sij. 
Now we can turn our first form of structural se-
mantic relatedness into the matrix form: 
S TS A? ?= +  
By solving this equation, we can get: 
1( )S I T A? ? ?= ?  
where I is the identity matrix. Since ?  is a pa-
rameter which only contributes an overall scale 
factor to the relatedness value, we can ignore it 
and get the final form of the structural semantic 
relatedness as: 
1( )S I T A? ?= ?  
Because the S is asymmetric, the finally related-
ness between node i and node j is the average of 
Sij and Sji. 
The meaning of ? : The last question of our 
structural semantic relatedness measure is how to 
set the free parameter ? . To understand the 
meaning of ? , let us expand the similarity as a 
power series thus: 
2 2( ... ...)k kS I T T T A? ? ?= + + + + +  
Noting that the [Tk]ij element is the relatedness 
transition rate from node i to node j with path 
length k, we can view the ?  as a penalty factor 
for the transition path length: by setting the ?  
with a value within (0, 1), a longer graph path 
will contribute less to the final relatedness value. 
The optimal value of ?  is 0.6 through a learning 
54
process shown in Section 4. For demonstration, 
Table 4 shows some structural semantic related-
ness values of the Semantic-graph in Figure 3 
(CS represents computer science and GM 
represents Graphical model). From Table 4, we 
can see that the structural semantic relatedness 
can successfully capture the semantic knowledge 
embedded in the structure of semantic-graph, 
such as the implicit semantic relation between 
Researcher and Learning. 
 Researcher CS GM Learning
Researcher --- 0.50 0.27 0.31 
CS 0.50 --- 0.62 0.73 
GM 0.27 0.62 --- 0.80 
Learning 0.31 0.73 0.80 --- 
Table 4. The structural semantic relatedness of the 
semantic-graph shown in Figure 3 
3 Named Entity Disambiguation by Le-
veraging Semantic Knowledge 
In this section we describe how to leverage the 
semantic knowledge captured in the structural 
semantic relatedness measure for named entity 
disambiguation. Because the key problem of 
named entity disambiguation is to measure the 
similarity between name observations, we inte-
grate the structural semantic relatedness in the 
similarity measure, so that it can better reflect the 
actual similarity between name observations. 
Concretely, our named entity disambiguation 
system works as follows: 1) Measuring the simi-
larity between name observations; 2) Grouping 
name observations using the clustering algorithm. 
In the following we describe each step in detail. 
3.1 Measuring the Similarity between Name 
Observations 
Intuitively, if two observations of the target name 
represent the same entity, it is highly possible 
that the concepts in their contexts are closely re-
lated, i.e., the named entities in their contexts are 
socially related and the Wikipedia concepts in 
their contexts are semantically related. In con-
trast, if two name observations represent differ-
ent entities, the concepts within their contexts 
will not be closely related. Therefore we can 
measure the similarity between two name obser-
vations by summarizing all the semantic related-
ness between the concepts in their contexts. 
To measure the similarity between name ob-
servations, we represent each name observation 
as a weighted vector of concepts (including 
named entities, Wikipedia concepts and Word-
Net concepts), where the concepts are extracted 
using the same method described in Section 2.2, 
so they are just the same concepts within the se-
mantic-graph. Using the same concept index as 
the semantic-graph, a name observation oi is then 
represented as 1 2{ , ,..., }i i i ino w w w= , where wik is 
the kth concept?s weight in observation oi, com-
puted using the standard TFIDF weight model, 
where the DF is computed using the Google 
Web1T 5-gram corpus4. Given the concept vec-
tor representation of two name observations oi 
and oj, their similarity is computed as: 
( , )i j il jk lk il jk
l k l k
SIM o o w w S w w=?? ??  
which is the weighted average of all the structur-
al semantic relatedness between the concepts in 
the contexts of the two name observations. 
3.2 Grouping Name Observations through 
Hierarchical Agglomerative Clustering 
Given the computed similarities, name observa-
tions are disambiguated by grouping them ac-
cording to their represented entities. In this paper, 
we group name observations using the hierar-
chical agglomerative clustering(HAC) algorithm, 
which is widely used in prior disambiguation 
research and evaluation task (WePS1 and 
WePS2). The HAC produce clusters in a bottom-
up way as follows: Initially, each name observa-
tion is an individual cluster; then we iteratively 
merge the two clusters with the largest similarity 
value to form a new cluster until this similarity 
value is smaller than a preset merging threshold 
or all the observations reside in one common 
cluster. The merging threshold can be deter-
mined through cross-validation. We employ the 
single-link method to compute the similarity be-
tween two clusters, which has been applied wide-
ly in prior research (Bagga and Baldwin (1998); 
Mann and Yarowsky (2003)). 
4 Experiments 
To assess the performance of our method and 
compare it with traditional methods, we conduct 
a series of experiments. In the experiments, we 
evaluate the proposed SSR method on the task of 
personal name disambiguation, which is the most 
common type of named entity disambiguation. In 
the following, we first explain the general expe-
rimental settings in Section 4.1, 4.2 and 4.3; then 
evaluate and discuss the performance of our me-
thod in Section 4.4. 
                                                 
4 www.ldc.upenn.edu/Catalog/docs/LDC2006T13/ 
55
4.1 Disambiguation Data Sets 
We adopted the standard data sets used in the 
First Web People Search Clustering Task 
(WePS1) (Artiles et al, 2007) and the Second 
Web People Search Clustering Task (WePS2) 
(Artiles et al, 2009). The three data sets we used 
are WePS1_training data set, WePS1_test data 
set, and WePS2_test data set. Each of the three 
data sets consists of a set of ambiguous personal 
names (totally 109 personal names); and for each 
name, we need to disambiguate its observations 
in the web pages of the top N (100 for WePS1 
and 150 for WePS2) Yahoo! search results. 
The experiment made the standard ?one per-
son per document? assumption, which is widely 
used in the participated systems in WePS1 and 
WePS2, i.e., all the observations of the same 
name in a document are assumed to represent the 
same entity. Based on this assumption, the fea-
tures within the entire web page are used to dis-
ambiguate personal names. 
4.2 Knowledge Sources 
There were three knowledge sources we used for 
our experiments: the WordNet 3.0; the Sep. 9, 
2007 English version of Wikipedia; and the Web 
pages of each ambiguous name in WePS datasets 
as the NE Co-occurrence Corpus. 
4.3 Evaluation Criteria 
We adopted the measures used in WePS1 to eva-
luate the performance of name disambiguation. 
These measures are: 
Purity (Pur): measures the homogeneity of 
name observations in the same cluster; 
Inverse purity (Inv_Pur): measures the com-
pleteness of a cluster; 
F-Measure (F): the harmonic mean of purity 
and inverse purity. 
The detailed definitions of these measures can 
be found in Amigo, et al (2008). We use F-
measure as the primary measure just liking 
WePS1 and WePS2. 
4.4 Experimental Results 
We compared our method with four baselines: (1) 
BOW: The first one is the traditional Bag of 
Words model (BOW) based methods: hierarchic-
al agglomerative clustering (HAC) over term 
vector similarity, where the features including 
single words and NEs, and all the features are 
weighted using TFIDF. This baseline is also the 
state-of-art method in WePS1 and WePS2. (2) 
SocialNetwork: The second one is the social 
network based methods, which is the same as the 
method described in Malin et al (2005): HAC 
over the similarity obtained through random 
walk over the social network built from the web 
pages of the top N search results. (3)SSR-
NoKnowledge: The third one is used as a base-
line for evaluating the efficiency of semantic 
knowledge: HAC over the similarity computed 
on semantic-graph with no knowledge integrated, 
i.e., the similarity is computed as: 
( , )i j il jl il jk
l l k
SIM o o w w w w=? ??  
(4) SSR-NoStructure: The fourth one is used as 
a baseline for evaluating the efficiency of the 
semantic knowledge embedded in complex struc-
tures: HAC over the similarity computed by only 
integrating the explicit semantic relations, i.e., 
the similarity is computed as: 
( , )i j il jk lk il jk
l k l k
SIM o o w w A w w=?? ??  
4.4.1 Overall Performance 
We conducted several experiments on all the 
three WePS data sets: the four baselines, the pro-
posed SSR method and the proposed SSR me-
thod with only one special type knowledge added, 
respectively SSR-NE, SSR-WordNet and SSR-
Wikipedia. All the optimal merging thresholds 
used in HAC were selected by applying leave-
one-out cross validation. The overall perfor-
mance is shown in Table 5. 
Method 
WePS1_training
Pur Inv_Pur F
BOW 0.71 0.88 0.78
SocialNetwork 0.66 0.98 0.76
SSR-NoKnowledge 0.79 0.89 0.81
SSR-NoStructure 0.87 0.83 0.83
SSR-NE 0.80 0.86 0.82
SSR-WordNet 0.80 0.91 0.83
SSR-Wikipedia 0.82 0.90 0.84
SSR 0.82 0.92 0.85
WePS1_test 
Pur Inv_Pur F
BOW 0.74 0.87 0.74
SocialNetwork 0.83 0.63 0.65
SSR-NoKnowledge 0.80 0.74 0.75
SSR-NoStructure 0.80 0.78 0.78
SSR-NE 0.73 0.80 0.74
SSR-WordNet 0.81 0.77 0.77
SSR-Wikipedia 0.88 0.77 0.81
SSR 0.85 0.83 0.84
WePS2_test 
Pur Inv_Pur F
BOW 0.80 0.80 0.77
SocialNetwork 0.62 0.93 0.70
SSR-NoKnowledge 0.84 0.80 0.80
SSR-NoStructure 0.84 0.83 0.81
SSR-NE 0.78 0.88 0.80
SSR-WordNet 0.85 0.82 0.83
SSR-Wikipedia 0.84 0.81 0.82
SSR 0.89 0.84 0.86
Table 5. Performance results of baselines and SSR 
methods 
56
From the performance results in Table 5, we 
can see that: 
1) The semantic knowledge can greatly im-
prove the disambiguation performance: com-
pared with the BOW and the SocialNetwork 
baselines, SSR respectively gets 8.7% and 14.7% 
improvement on average on the three data sets. 
2) By leveraging the semantic knowledge 
from multiple knowledge sources, we can obtain 
a better named entity disambiguation perfor-
mance: compared with the SSR-NE?s 0% im-
provement, the SSR-WordNet?s 2.3% improve-
ment and the SSR-Wikipedia?s 3.7% improve-
ment, the SSR gets 6.3% improvement over the 
SSR-NoKnowledge baseline, which is larger than 
all the SSR methods with only one type of se-
mantic knowledge integrated. 
3) The exploitation of the structural seman-
tic knowledge can further improve the disambig-
uation performance: compared with SSR-
NoStructure, our SSR method achieves 4.3% im-
provement. 
 
Figure 4. The F-Measure vs. ?  on three data sets 
4.4.2 Optimizing Parameters 
There is only one parameter ?  needed to be con-
figured, which is the penalty factor for the rela-
tedness transition path length in the structural 
semantic relatedness measure. Usually a smaller 
?  will make the structural semantic knowledge 
contribute less in the resulting relatedness value. 
Figure 4 plots the performance of our method 
corresponding to the special ?  settings. As 
shown in Figure 4, the SSR method is not very 
sensitive to the ?  and can achieve its best aver-
age performance when the value of ?  is 0.6. 
4.4.3 Detailed Analysis 
To better understand the reasons why our SSR 
method works well and how the exploitation of 
structural semantic knowledge can improve per-
formance, we analyze the results in detail. 
The Exploitation of Semantic Knowledge. The 
primary advantage of our method is the exploita-
tion of semantic knowledge. Our method exploits 
the semantic knowledge in two directions: 
1) The Integration of Multiple Semantic 
Knowledge Sources. Using the semantic-graph 
model, our method can integrate the semantic 
knowledge extracted from multiple knowledge 
sources, while most traditional knowledge-based 
methods are usually specialized to one type of 
knowledge. By integrating multiple semantic 
knowledge sources, our method can improve the 
semantic knowledge coverage. 
2) The exploitation of Semantic Knowledge 
embedded in complex structures. Using the struc-
tural semantic relatedness measure, our method 
can exploit the implicit semantic knowledge em-
bedded in complex structures; while traditional 
knowledge-based methods usually lack this abili-
ty. 
The Rich Meaningful Features. One another 
advantage of our method is the rich meaningful 
features, which is brought by the multiple seman-
tic knowledge sources. With more meaningful 
features, our method can better describe the 
name observations with less information loss. 
Furthermore, unlike the traditional N-gram fea-
tures, the features enriched by semantic know-
ledge sources are all semantically meaningful 
units themselves, so little noisy features will be 
added. The effect of rich meaningful features can 
also be shown in Table 5: by adding these fea-
tures, the SSR-NoKnowledge respectively 
achieves 2.3% and 9.7% improvement over the 
BOW and the SocialNetwork baseline. 
5 Related Work 
In this section, we briefly review the related 
work. Totally, the traditional named entity dis-
ambiguation methods can be classified into two 
categories: the shallow methods and the know-
ledge-based methods. 
Most of previous named entity disambiguation 
researches adopt the shallow methods, which are 
mostly the natural extension of the bag of words 
(BOW) model. Bagga and Baldwin (1998) 
represented a name as a vector of its contextual 
words, then two names were predicted to be the 
same entity if their cosine similarity is above a 
threshold. Mann and Yarowsky (2003) and Niu 
et al (2004) extended the vector representation 
with extracted biographic facts. Pedersen et al 
(2005) employed significant bigrams to represent 
57
a name observation. Chen and Martin (2007) ex-
plored a range of syntactic and semantic features. 
In recent years some research has investigated 
employing knowledge sources to enhance the 
named entity disambiguation. Bunescu and Pasca 
(2006) disambiguated the names using the cate-
gory information in Wikipedia. Cucerzan (2007) 
disambiguated the names by combining the BOW 
model with the Wikipedia category information. 
Han and Zhao (2009) leveraged the Wikipedia 
semantic knowledge for computing the similarity 
between name observations. Bekkerman and 
McCallum (2005) disambiguated names based 
on the link structure of the Web pages between a 
set of socially related persons. Kalashnikov et al 
(2008) and Lu et al (2007) used the co-
occurrence statistics between named entities in 
the Web. The social network was also exploited 
for named entity disambiguation, where similari-
ty is computed through random walking, such as 
the work introduced in Malin (2005), Malin and 
Airoldi (2005), Yang et al(2006) and Minkov et 
al. (2006). Hassell et al (2006) used the relation-
ships from DBLP to disambiguate names in re-
search domain. 
6 Conclusions and Future Works 
In this paper we demonstrate how to enhance the 
named entity disambiguation by capturing and 
exploiting the semantic knowledge existed in 
multiple knowledge sources. In particular, we 
propose a semantic relatedness measure, Struc-
tural Semantic Relatedness, which can capture 
both the explicit semantic relations and the im-
plicit structural semantic knowledge. The expe-
rimental results on the WePS data sets demon-
strate the efficiency of the proposed method. For 
future work, we want to develop a framework 
which can uniformly model the semantic know-
ledge and the contextual clues for named entity 
disambiguation. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under Grants no. 
60875041 and 60673042, and the National High 
Technology Development 863 Program of China 
under Grants no. 2006AA01Z144. 
References  
Amigo, E., Gonzalo, J., Artiles, J. and Verdejo, F. 
2008. A comparison of extrinsic clustering evalua-
tion metrics based on formal constraints. Informa-
tion Retrieval. 
Artiles, J., Gonzalo, J. & Sekine, S. 2007. The Se-
mEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task. In 
SemEval. 
Artiles, J., Gonzalo, J. and Sekine, S. 2009. WePS2 
Evaluation Campaign: Overview of the Web 
People Search Clustering Task. In WePS2, WWW 
2009. 
Baeza-Yates, R., Ribeiro-Neto, B., et al 1999. Mod-
ern information retrieval. Addison-Wesley Reading, 
MA. 
Bagga, A. & Baldwin, B. 1998. Entity-based cross-
document coreferencing using the vector space 
model. Proceedings of the 17th international confe-
rence on Computational linguistics-Volume 1, pp. 
79-85. 
Bekkerman, R. & McCallum, A. 2005. Disambiguat-
ing web appearances of people in a social network. 
Proceedings of the 14th international conference on 
World Wide Web, pp. 463-470. 
Bunescu, R. & Pasca, M. 2006. Using encyclopedic 
knowledge for named entity disambiguation. Pro-
ceedings of EACL, vol. 6.  
Chen, Y. & Martin, J. 2007. Towards robust unsuper-
vised personal name disambiguation. Proceedings 
of EMNLP and CoNLL, pp. 190-198. 
Cilibrasi, R. L., Vitanyi, P. M. & CWI, A. 2007. The 
google similarity distance, IEEE Transactions on 
knowledge and data engineering, vol. 19, no. 3, pp. 
370-383. 
Cucerzan, S. 2007, Large-scale named entity disam-
biguation based on Wikipedia data. Proceedings of 
EMNLP-CoNLL, pp. 708-716. 
Fellbaum, C., et al 1998. WordNet: An electronic 
lexical database. MIT press Cambridge, MA. 
Fleischman, M. B. & Hovy, E. 2004. Multi-document 
person name resolution. Proceedings of ACL, Ref-
erence Resolution Workshop. 
Han, X. & Zhao, J. 2009. Named entity disambigua-
tion by leveraging Wikipedia semantic knowledge. 
Proceeding of the 18th ACM conference on Infor-
mation and knowledge management, pp. 215-224. 
Hassell, J., Aleman-Meza, B. & Arpinar, I. 2006. On-
tology-driven automatic entity disambiguation in 
unstructured text. Proceedings of The 2006 ISWC, 
pp. 44-57. 
Jeh, G. & Widom, J. 2002. SimRank: A measure of 
structural-context similarity, Proceedings of the 
eighth ACM SIGKDD international conference on 
Knowledge discovery and data mining, p. 543.  
58
Kalashnikov, D. V., Nuray-Turan, R. & Mehrotra, S. 
2008. Towards Breaking the Quality Curse. A 
Web-Querying Approach to Web People Search. In 
Proc. of SIGIR. 
Leicht, E. A.,  Petter Holme,  & M. E. J. Newman. 
2006. Vertex similarity in networks. Physical Re-
view E , vol. 73, no. 2, p. 26120. 
Lin., D. 1998. An information-theoretic definition of 
similarity. In Proc. of ICML. 
Lu, Y. & Nie , Z. et al 2007. Name Disambiguation 
Using Web Connection. In Proc. of AAAI. 
Malin, B. 2005. Unsupervised name disambiguation 
via social network similarity. SIAM SDM Work-
shop on Link Analysis, Counterterrorism and Secu-
rity. 
Malin, B., Airoldi, E. & Carley, K. M. 2005. A net-
work analysis model for disambiguation of names 
in lists. Computational & Mathematical Organiza-
tion Theory, vol. 11, no. 2, pp. 119-139. 
Mann, G. S. & Yarowsky, D. 2003. Unsupervised 
personal name disambiguation, Proceedings of the 
seventh conference on Natural language learning at 
HLT-NAACL 2003-Volume 4, p. 40. 
McNamee, P. & Dang, H. Overview of the TAC 2009 
Knowledge Base Population Track. In Proceedings 
of Text Analysis Conference (TAC-2009), 2009. 
Medelyan, O., Witten, I. H. & Milne, D. 2008. Topic 
indexing with Wikipedia. Proceedings of the AAAI 
WikiAI workshop. 
Milne, D., Medelyan, O. & Witten, I. H. 2006. Min-
ing domain-specific thesauri from wikipedia: A 
case study. IEEE/WIC/ACM International Confe-
rence on Web Intelligence, pp. 442-448. 
Minkov, E., Cohen, W. W. & Ng, A. Y. 2006. Con-
textual search and name disambiguation in email 
using graphs, Proceedings of the 29th annual inter-
national ACM SIGIR conference on Research and 
development in information retrieval, pp. 27-34. 
Niu C., Li W. and Srihari, R. K. 2004. Weakly Super-
vised Learning for Cross-document Person Name 
Disambiguation Supported by Information Extrac-
tion. Proceedings of ACL, pp. 598-605. 
Pedersen, T., Purandare, A. & Kulkarni, A. 2005. 
Name discrimination by clustering similar contexts. 
Computational Linguistics and Intelligent Text 
Processing, pp. 226-237. 
Strube, M. & Ponzetto, S. P. 2006. WikiRelate! Com-
puting semantic relatedness using Wikipedia, Pro-
ceedings of the National Conference on Artificial 
Intelligence, vol. 21, no. 2, p. 1419. 
Suchanek, F. M., Kasneci, G. & Weikum, G. 2007. 
Yago: a core of semantic knowledge, Proceedings 
of the 16th international conference on World 
Wide Web, p. 706. 
Wan, X., Gao, J., Li, M. & Ding, B. 2005. Person 
resolution in person search results: Webhawk. Pro-
ceedings of the 14th ACM international conference 
on Information and knowledge management, p. 
170. 
Witten, D. M. & Milne, D. 2008. An effective, low-
cost measure of semantic relatedness obtained from 
Wikipedia links. Proceeding of AAAI Workshop 
on Wikipedia and Artificial Intelligence: an Evolv-
ing Synergy, AAAI Press, Chicago, USA, pp. 25-
30. 
Yang, K. H., Chiou, K. Y., Lee, H. M. & Ho, J. M. 
2006. Web appearance disambiguation of personal 
names based on network motif. Proceedings of the 
2006 IEEE/WIC/ACM International Conference on 
Web Intelligence, pp. 386-389. 
 
59
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653?662,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Phrase-Based Translation Model for Question Retrieval in Community
Question Answer Archives
Guangyou Zhou, Li Cai, Jun Zhao?, and Kang Liu
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,lcai,jzhao,kliu}@nlpr.ia.ac.cn
Abstract
Community-based question answer (Q&A)
has become an important issue due to the pop-
ularity of Q&A archives on the web. This pa-
per is concerned with the problem of ques-
tion retrieval. Question retrieval in Q&A
archives aims to find historical questions that
are semantically equivalent or relevant to the
queried questions. In this paper, we propose
a novel phrase-based translation model for
question retrieval. Compared to the traditional
word-based translation models, the phrase-
based translation model is more effective be-
cause it captures contextual information in
modeling the translation of phrases as a whole,
rather than translating single words in isola-
tion. Experiments conducted on real Q&A
data demonstrate that our proposed phrase-
based translation model significantly outper-
forms the state-of-the-art word-based transla-
tion model.
1 Introduction
Over the past few years, large scale question and
answer (Q&A) archives have become an important
information resource on the Web. These include
the traditional Frequently Asked Questions (FAQ)
archives and the emerging community-based Q&A
services, such as Yahoo! Answers1, Live QnA2, and
Baidu Zhidao3.
?Correspondence author: jzhao@nlpr.ia.ac.cn
1http://answers.yahoo.com/
2http://qna.live.com/
3http://zhidao.baidu.com/
Community-based Q&A services can directly re-
turn answers to the queried questions instead of a
list of relevant documents, thus provide an effective
alternative to the traditional adhoc information re-
trieval. To make full use of the large scale archives
of question-answer pairs, it is critical to have func-
tionality helping users to retrieve historical answers
(Duan et al, 2008). Therefore, it is a meaningful
task to retrieve the questions that are semantically
equivalent or relevant to the queried questions. For
example in Table 1, given questionQ1,Q2 can be re-
turned and their answers will then be used to answer
Q1 because the answer ofQ2 is expected to partially
satisfy the queried question Q1. This is what we
called question retrieval in this paper.
The major challenge for Q&A retrieval, as for
Query:
Q1: How to get rid of stuffy nose?
Expected:
Q2: What is the best way to prevent a cold?
Not Expected:
Q3: How do I air out my stuffy room?
Q4: How do you make a nose bleed stop quicker?
Table 1: An example on question retrieval
most information retrieval models, such as vector
space model (VSM) (Salton et al, 1975), Okapi
model (Robertson et al, 1994), language model
(LM) (Ponte and Croft, 1998), is the lexical gap (or
lexical chasm) between the queried questions and
the historical questions in the archives (Jeon et al,
2005; Xue et al, 2008). For example in Table 1, Q1
and Q2 are two semantically similar questions, but
they have very few words in common. This prob-
653
lem is more serious for Q&A retrieval, since the
question-answer pairs are usually short and there is
little chance of finding the same content expressed
using different wording (Xue et al, 2008). To solve
the lexical gap problem, most researchers regarded
the question retrieval task as a statistical machine
translation problem by using IBM model 1 (Brown
et al, 1993) to learn the word-to-word translation
probabilities (Berger and Lafferty, 1999; Jeon et al,
2005; Xue et al, 2008; Lee et al, 2008; Bernhard
and Gurevych, 2009). Experiments consistently re-
ported that the word-based translation models could
yield better performance than the traditional meth-
ods (e.g., VSM. Okapi and LM). However, all these
existing approaches are considered to be context in-
dependent in that they do not take into account any
contextual information in modeling word translation
probabilities. For example in Table 1, although nei-
ther of the individual word pair (e.g., ?stuffy?/?cold?
and ?nose?/?cold?) might have a high translation
probability, the sequence of words ?stuffy nose? can
be easily translated from a single word ?cold? in Q2
with a relative high translation probability.
In this paper, we argue that it is beneficial to cap-
ture contextual information for question retrieval.
To this end, inspired by the phrase-based statistical
machine translation (SMT) systems (Koehn et al,
2003; Och and Ney, 2004), we propose a phrase-
based translation model (P-Trans) for question re-
trieval, and we assume that question retrieval should
be performed at the phrase level. This model learns
the probability of translating one sequence of words
(e.g., phrase) into another sequence of words, e.g.,
translating a phrase in a historical question into an-
other phrase in a queried question. Compared to the
traditional word-based translation models that ac-
count for translating single words in isolation, the
phrase-based translation model is potentially more
effective because it captures some contextual infor-
mation in modeling the translation of phrases as a
whole. More precise translation can be determined
for phrases than for words. It is thus reasonable to
expect that using such phrase translation probabili-
ties as ranking features is likely to improve the ques-
tion retrieval performance, as we will show in our
experiments.
Unlike the general natural language translation,
the parallel sentences between questions and an-
swers in community-based Q&A have very different
lengths, leaving many words in answers unaligned
to any word in queried questions. Following (Berger
and Lafferty, 1999), we restrict our attention to those
phrase translations consistent with a good word-
level alignment.
Specifically, we make the following contribu-
tions:
? we formulate the question retrieval task as a
phrase-based translation problem by modeling
the contextual information (in Section 3.1).
? we linearly combine the phrase-based transla-
tion model for the question part and answer part
(in Section 3.2).
? we propose a linear ranking model framework
for question retrieval in which different models
are incorporated as features because the phrase-
based translation model cannot be interpolated
with a unigram language model (in Section
3.3).
? finally, we conduct the experiments on
community-based Q&A data for question re-
trieval. The results show that our proposed ap-
proach significantly outperforms the baseline
methods (in Section 4).
The remainder of this paper is organized as fol-
lows. Section 2 introduces the existing state-of-the-
art methods. Section 3 describes our phrase-based
translation model for question retrieval. Section 4
presents the experimental results. In Section 5, we
conclude with ideas for future research.
2 Preliminaries
2.1 Language Model
The unigram language model has been widely used
for question retrieval on community-based Q&A
data (Jeon et al, 2005; Xue et al, 2008; Cao et al,
2010). To avoid zero probability, we use Jelinek-
Mercer smoothing (Zhai and Lafferty, 2001) due to
its good performance and cheap computational cost.
So the ranking function for the query likelihood lan-
guage model with Jelinek-Mercer smoothing can be
654
written as:
Score(q, D) =
?
w?q
(1 ? ?)Pml(w|D) + ?Pml(w|C)
(1)
Pml(w|D) =
#(w,D)
|D|
, Pml(w|C) =
#(w,C)
|C|
(2)
where q is the queried question, D is a document, C
is background collection, ? is smoothing parameter.
#(t,D) is the frequency of term t in D, |D| and |C|
denote the length of D and C respectively.
2.2 Word-Based Translation Model
Previous work (Berger et al, 2000; Jeon et al, 2005;
Xue et al, 2008) consistently reported that the word-
based translation models (Trans) yielded better per-
formance than the traditional methods (VSM, Okapi
and LM) for question retrieval. These models ex-
ploit the word translation probabilities in a language
modeling framework. Following Jeon et al (2005)
and Xue et al (2008), the ranking function can be
written as:
Score(q, D) =
?
w?q
(1??)Ptr(w|D)+?Pml(w|C) (3)
Ptr(w|D) =
?
t?D
P (w|t)Pml(t|D), Pml(t|D) =
#(t,D)
|D|
(4)
where P (w|t) denotes the translation probability
from word t to word w.
2.3 Word-Based Translation Language Model
Xue et al (2008) proposed to linearly mix two dif-
ferent estimations by combining language model
and word-based translation model into a unified
framework, called TransLM. The experiments show
that this model gains better performance than both
the language model and the word-based translation
model. Following Xue et al (2008), this model can
be written as:
Score(q, D) =
?
w?q
(1 ? ?)Pmx(w|D) + ?Pml(w|C)
(5)
Pmx(w|D) = ?
?
t?D
P (w|t)Pml(t|D)+(1??)Pml(w|D)
(6)
D:                      ?  for good cold home remedies ? document
E:                  [for,    good,    cold,    home remedies] segmentation
F:            [for1,    best2,    stuffy nose3,    home remedy4] translation
M:                     (1?3?2?1?3?4?4?2) permutation
q:                     best home remedy for stuffy nose queried question
Figure 1: Example describing the generative procedure
of the phrase-based translation model.
3 Our Approach: Phrase-Based
Translation Model for Question
Retrieval
3.1 Phrase-Based Translation Model
Phrase-based machine translation models (Koehn
et al, 2003; D. Chiang, 2005; Och and Ney,
2004) have shown superior performance compared
to word-based translation models. In this paper,
the goal of phrase-based translation model is to
translate a document4 D into a queried question
q. Rather than translating single words in isola-
tion, the phrase-based model translates one sequence
of words into another sequence of words, thus in-
corporating contextual information. For example,
we might learn that the phrase ?stuffy nose? can be
translated from ?cold? with relative high probabil-
ity, even though neither of the individual word pairs
(e.g., ?stuffy?/?cold? and ?nose?/?cold?) might have
a high word translation probability. Inspired by the
work of (Sun et al, 2010; Gao et al, 2010), we
assume the following generative process: first the
document D is broken into K non-empty word se-
quences t1, . . . , tK , then each t is translated into a
new non-empty word sequence w1, . . . ,wK , and fi-
nally these phrases are permutated and concatenated
to form the queried questions q, where t and w de-
note the phrases or consecutive sequence of words.
To formulate this generative process, let E
denote the segmentation of D into K phrases
t1, . . . , tK , and let F denote the K translation
phrases w1, . . . ,wK ?we refer to these (ti,wi)
pairs as bi-phrases. Finally, letM denote a permuta-
tion of K elements representing the final reordering
step. Figure 1 describes an example of the genera-
tive procedure.
Next let us place a probability distribution over
rewrite pairs. Let B(D,q) denote the set of E,
4In this paper, a document has the same meaning as a histor-
ical question-answer pair in the Q&A archives.
655
F , M triples that translate D into q. Here we as-
sume a uniform probability over segmentations, so
the phrase-based translation model can be formu-
lated as:
P (q|D) ?
?
(E,F,M)?
B(D,q)
P (F |D,E) ? P (M |D,E, F ) (7)
As is common practice in SMT, we use the maxi-
mum approximation to the sum:
P (q|D) ? max
(E,F,M)?
B(D,q)
P (F |D,E) ? P (M |D,E, F ) (8)
Although we have defined a generative model for
translatingD into q, our goal is to calculate the rank-
ing score function over existing q andD, rather than
generating new queried questions. Equation (8) can-
not be used directly for document ranking because
q and D are often of very different lengths, leav-
ing many words in D unaligned to any word in q.
This is the key difference between the community-
based question retrieval and the general natural lan-
guage translation. As pointed out by Berger and Laf-
ferty (1999) and Gao et al (2010), document-query
translation requires a distillation of the document,
while translation of natural language tolerates little
being thrown away.
Thus we attempt to extract the key document
words that form the distillation of the document, and
assume that a queried question is translated only
from the key document words. In this paper, the
key document words are identified via word align-
ment. We introduce the ?hidden alignments? A =
a1 . . . aj . . . aJ , which describe the mapping from a
word position j in queried question to a document
word position i = aj . The different alignment mod-
els we present provide different decompositions of
P (q, A|D). We assume that the position of the key
document words are determined by the Viterbi align-
ment, which can be obtained using IBM model 1 as
follows:
A? = argmax
A
P (q, A|D)
= argmax
A
{
P (J |I)
J
?
j=1
P (wj |taj )
}
=
[
argmax
aj
P (wj |taj )
]J
j=1
(9)
Given A?, when scoring a given Q&A pair, we re-
strict our attention to those E, F , M triples that are
consistent with A?, which we denote as B(D,q, A?).
Here, consistency requires that if two words are
aligned in A?, then they must appear in the same bi-
phrase (ti,wi). Once the word alignment is fixed,
the final permutation is uniquely determined, so we
can safely discard that factor. Thus equation (8) can
be written as:
P (q|D) ? max
(E,F,M)?B(D,q,A?)
P (F |D,E) (10)
For the sole remaining factor P (F |D,E), we
make the assumption that a segmented queried ques-
tion F = w1, . . . ,wK is generated from left to
right by translating each phrase t1, . . . , tK indepen-
dently:
P (F |D,E) =
K
?
k=1
P (wk|tk) (11)
where P (wk|tk) is a phrase translation probability,
the estimation will be described in Section 3.3.
To find the maximum probability assignment ef-
ficiently, we use a dynamic programming approach,
somewhat similar to the monotone decoding algo-
rithm described in (Och, 2002). We define ?j to
be the probability of the most likely sequence of
phrases covering the first j words in a queried ques-
tion, then the probability can be calculated using the
following recursion:
(1) Initialization:
?0 = 1 (12)
(2) Induction:
?j =
?
j?<j,w=wj?+1...wj
{
?j?P (w|tw)
}
(13)
(3) Total:
P (q|D) = ?J (14)
3.2 Phrase-Based Translation Model for
Question Part and Answer Part
In Q&A, a document D is decomposed into (q?, a?),
where q? denotes the question part of the historical
question in the archives and a? denotes the answer
part. Although it has been shown that doing Q&A
retrieval based solely on the answer part does not
perform well (Jeon et al, 2005; Xue et al, 2008),
the answer part should provide additional evidence
about relevance and, therefore, it should be com-
bined with the estimation based on the question part.
656
In this combined model, P (q|q?) and P (q|a?) are cal-
culated with equations (12) to (14). So P (q|D) will
be written as:
P (q|D) = ?1P (q|q?) + ?2P (q|a?) (15)
where ?1 + ?2 = 1.
In equation (15), the relative importance of ques-
tion part and answer part is adjusted through ?1 and
?2. When ?1 = 1, the retrieval model is based
on phrase-based translation model for the question
part. When ?2 = 1, the retrieval model is based on
phrase-based translation model for the answer part.
3.3 Parameter Estimation
3.3.1 Parallel Corpus Collection
In Q&A archives, question-answer pairs can be con-
sidered as a type of parallel corpus, which is used for
estimating the translation probabilities. Unlike the
bilingual machine translation, the questions and an-
swers in a Q&A archive are written in the same lan-
guage, the translation probability can be calculated
through setting either as the source and the other as
the target. In this paper, P (a?|q?) is used to denote
the translation probability with the question as the
source and the answer as the target. P (q?|a?) is used
to denote the opposite configuration.
For a given word or phrase, the related words
or phrases differ when it appears in the ques-
tion or in the answer. Following Xue et
al. (2008), a pooling strategy is adopted. First,
we pool the question-answer pairs used to learn
P (a?|q?) and the answer-question pairs used to
learn P (q?|a?), and then use IBM model 1 (Brown
et al, 1993) to learn the combined translation
probabilities. Suppose we use the collection
{(q?, a?)1, . . . , (q?, a?)m} to learn P (a?|q?) and use the
collection {(a?, q?)1, . . . , (a?, q?)m} to learn P (q?|a?),
then {(q?, a?)1, . . . , (q?, a?)m, (a?, q?)1, . . . , (a?, q?)m} is
used here to learn the combination translation prob-
ability Ppool(wi|tj).
3.3.2 Parallel Corpus Preprocessing
Unlike the bilingual parallel corpus used in SMT,
our parallel corpus is collected from Q&A archives,
which is more noisy. Directly using the IBM model
1 can be problematic, it is possible for translation
model to contain ?unnecessary? translations (Lee et
al., 2008). In this paper, we adopt a variant of Tex-
tRank algorithm (Mihalcea and Tarau, 2004) to iden-
tify and eliminate unimportant words from parallel
corpus, assuming that a word in a question or an-
swer is unimportant if it holds a relatively low sig-
nificance in the parallel corpus.
Following (Lee et al, 2008), the ranking algo-
rithm proceeds as follows. First, all the words in
a given document are added as vertices in a graph
G. Then edges are added between words if the
words co-occur in a fixed-sized window. The num-
ber of co-occurrences becomes the weight of an
edge. When the graph is constructed, the score of
each vertex is initialized as 1, and the PageRank-
based ranking algorithm is run on the graph itera-
tively until convergence. The TextRank score of a
word w in document D at kth iteration is defined as
follows:
Rkw,D = (1? d) + d ?
?
?j:(i,j)?G
ei,j
?
?l:(j,l)?G ej,l
Rk?1w,D
(16)
where d is a damping factor usually set to 0.85, and
ei,j is an edge weight between i and j.
We use average TextRank score as threshold:
words are removed if their scores are lower than the
average score of all words in a document.
3.3.3 Translation Probability Estimation
After preprocessing the parallel corpus, we will cal-
culate P (w|t), following the method commonly
used in SMT (Koehn et al, 2003; Och, 2002) to ex-
tract bi-phrases and estimate their translation proba-
bilities.
First, we learn the word-to-word translation prob-
ability using IBM model 1 (Brown et al, 1993).
Then, we perform Viterbi word alignment according
to equation (9). Finally, the bi-phrases that are con-
sistent with the word alignment are extracted using
the heuristics proposed in (Och, 2002). We set the
maximum phrase length to five in our experiments.
After gathering all such bi-phrases from the train-
ing data, we can estimate conditional relative fre-
quency estimates without smoothing:
P (w|t) = N(t,w)
N(t)
(17)
where N(t,w) is the number of times that t is
aligned to w in training data. These estimates are
657
source stuffy nose internet explorer
1 stuffy nose internet explorer
2 cold ie
3 stuffy internet browser
4 sore throat explorer
5 sneeze browser
Table 2: Phrase translation probability examples. Each
column shows the top 5 target phrases learned from the
word-aligned question-answer pairs.
useful for contextual lexical selection with sufficient
training data, but can be subject to data sparsity is-
sues (Sun et al, 2010; Gao et al, 2010). An alter-
nate translation probability estimate not subject to
data sparsity is the so-called lexical weight estimate
(Koehn et al, 2003). Let P (w|t) be the word-to-
word translation probability, and let A be the word
alignment between w and t. Here, the word align-
ment contains (i, j) pairs, where i ? 1 . . . |w| and
j ? 0 . . . |t|, with 0 indicating a null word. Then we
use the following estimate:
Pt(w|t, A) =
|w|
?
i=1
1
|{j|(j, i) ? A}|
?
?(i,j)?A
P (wi|tj)
(18)
We assume that for each position inw, there is ei-
ther a single alignment to 0, or multiple alignments
to non-zero positions in t. In fact, equation (18)
computes a product of per-word translation scores;
the per-word scores are the averages of all the trans-
lations for the alignment links of that word. The
word translation probabilities are calculated using
IBM 1, which has been widely used for question re-
trieval (Jeon et al, 2005; Xue et al, 2008; Lee et al,
2008; Bernhard and Gurevych, 2009). These word-
based scores of bi-phrases, though not as effective
in contextual selection, are more robust to noise and
sparsity.
A sample of the resulting phrase translation ex-
amples is shown in Table 2, where the top 5 target
phrases are translated from the source phrases ac-
cording to the phrase-based translation model. For
example, the term ?explorer? used alone, most likely
refers to a person who engages in scientific explo-
ration, while the phrase ?internet explorer? has a
very different meaning.
3.4 Ranking Candidate Historical Questions
Unlike the word-based translation models, the
phrase-based translation model cannot be interpo-
lated with a unigram language model. Following
(Sun et al, 2010; Gao et al, 2010), we resort to
a linear ranking framework for question retrieval in
which different models are incorporated as features.
We consider learning a relevance function of the
following general, linear form:
Score(q, D) = ?T ??(q, D) (19)
where the feature vector ?(q, D) is an arbitrary
function that maps (q, D) to a real value, i.e.,
?(q, D) ? R. ? is the corresponding weight vec-
tor, we optimize this parameter for our evaluation
metrics directly using the Powell Search algorithm
(Paul et al, 1992) via cross-validation.
The features used in this paper are as follows:
? Phrase translation features (PT):
?PT (q, D,A) = logP (q|D), where P (q|D)
is computed using equations (12) to (15), and
the phrase translation probability P (w|t) is
estimated using equation (17).
? Inverted Phrase translation features (IPT):
?IPT (D,q, A) = logP (D|q), where P (D|q)
is computed using equations (12) to (15) ex-
cept that we set ?2 = 0 in equation (15), and
the phrase translation probability P (w|t) is es-
timated using equation (17).
? Lexical weight feature (LW):
?LW (q, D,A) = logP (q|D), here P (q|D)
is computed by equations (12) to (15), and the
phrase translation probability is computed as
lexical weight according to equation (18).
? Inverted Lexical weight feature (ILW):
?ILW (D,q, A) = logP (D|q), here P (D|q)
is computed by equations (12) to (15) except
that we set ?2 = 0 in equation (15), and the
phrase translation probability is computed as
lexical weight according to equation (18).
? Phrase alignment features (PA):
?PA(q, D,B) =
?K
2 |ak ? bk?1 ? 1|,
where B is a set of K bi-phrases, ak is the start
position of the phrase in D that was translated
658
into the kth phrase in queried question, and
bk?1 is the end position of the phrase in D
that was translated into the (k ? 1)th phrase in
queried question. The feature, inspired by the
distortion model in SMT (Koehn et al, 2003),
models the degree to which the queried phrases
are reordered. For all possible B, we only
compute the feature value according to the
Viterbi alignment, B? = argmaxB P (q, B|D).
We find B? using the Viterbi algorithm, which is
almost identical to the dynamic programming
recursion of equations (12) to (14), except that
the sum operator in equation (13) is replaced
with the max operator.
? Unaligned word penalty features (UWP):
?UWP (q, D), which is defined as the ratio be-
tween the number of unaligned words and the
total number of words in queried questions.
? Language model features (LM):
?LM (q, D,A) = logPLM (q|D), where
PLM (q|D) is the unigram language model
with Jelinek-Mercer smoothing defined by
equations (1) and (2).
? Word translation features (WT):
?WT (q, D) = logP (q|D), where P (q|D) is
the word-based translation model defined by
equations (3) and (4).
4 Experiments
4.1 Data Set and Evaluation Metrics
We collect the questions from Yahoo! Answers and
use the getByCategory function provided in Yahoo!
Answers API5 to obtain Q&A threads from the Ya-
hoo! site. More specifically, we utilize the resolved
questions under the top-level category at Yahoo!
Answers, namely ?Computers & Internet?. The re-
sulting question repository that we use for question
retrieval contains 518,492 questions. To learn the
translation probabilities, we use about one million
question-answer pairs from another data set.6
In order to create the test set, we randomly se-
lect 300 questions for this category, denoted as
5http://developer.yahoo.com/answers
6The Yahoo! Webscope dataset Yahoo answers com-
prehensive questions and answers version 1.0.2, available at
http://reseach.yahoo.com/Academic Relations.
?CI TST?. To obtain the ground-truth of ques-
tion retrieval, we employ the Vector Space Model
(VSM) (Salton et al, 1975) to retrieve the top 20 re-
sults and obtain manual judgements. The top 20 re-
sults don?t include the queried question itself. Given
a returned result by VSM, an annotator is asked to
label it with ?relevant? or ?irrelevant?. If a returned
result is considered semantically equivalent to the
queried question, the annotator will label it as ?rel-
evant?; otherwise, the annotator will label it as ?ir-
relevant?. Two annotators are involved in the anno-
tation process. If a conflict happens, a third person
will make judgement for the final result. In the pro-
cess of manually judging questions, the annotators
are presented only the questions. Table 3 provides
the statistics on the final test set.
#queries #returned #relevant
CI TST 300 6,000 798
Table 3: Statistics on the Test Data
We evaluate the performance of our approach us-
ing Mean Average Precision (MAP). We perform
a significant test, i.e., a t-test with a default signif-
icant level of 0.05. Following the literature, we set
the parameters ? = 0.2 (Cao et al, 2010) in equa-
tions (1), (3) and (5), and ? = 0.8 (Xue et al, 2008)
in equation (6).
4.2 Question Retrieval Results
We randomly divide the test questions into five
subsets and conduct 5-fold cross-validation experi-
ments. In each trial, we tune the parameters ?1 and
?2 with four of the five subsets and then apply it to
one remaining subset. The experiments reported be-
low are those averaged over the five trials.
Table 4 presents the main retrieval performance.
Row 1 to row 3 are baseline systems, all these meth-
ods use word-based translation models and obtain
the state-of-the-art performance in previous work
(Jeon et al, 2005; Xue et al, 2008). Row 3 is simi-
lar to row 2, the only difference is that TransLM only
considers the question part, while Xue et al (2008)
incorporates the question part and answer part. Row
4 and row 5 are our proposed phrase-based trans-
lation model with maximum phrase length of five.
Row 4 is phrase-based translation model purely
based on question part, this model is equivalent to
659
# Methods Trans Prob MAP
1 Jeon et al (2005) Ppool 0.289
2 TransLM Ppool 0.324
3 Xue et al (2008) Ppool 0.352
4 P-Trans (?1 = 1, l = 5) Ppool 0.366
5 P-Trans (l = 5) Ppool 0.391
Table 4: Comparison with different methods for question
retrieval.
setting ?1 = 1 in equation (15). Row 5 is the phrase-
based combination model which linearly combines
the question part and answer part. As expected,
different parts can play different roles: a phrase to
be translated in queried questions may be translated
from the question part or answer part. All these
methods use pooling strategy to estimate the transla-
tion probabilities. There are some clear trends in the
result of Table 4:
(1) Word-based translation language model
(TransLM) significantly outperforms word-based
translation model of Jeon et al (2005) (row 1 vs. row
2). Similar observations have been made by Xue et
al. (2008).
(2) Incorporating the answer part into the models,
either word-based or phrase-based, can significantly
improve the performance of question retrieval (row
2 vs. row 3; row 4 vs. row 5).
(3) Our proposed phrase-based translation model
(P-Trans) significantly outperforms the state-of-the-
art word-based translation models (row 2 vs. row 4
and row 3 vs. row 5, all these comparisons are sta-
tistically significant at p < 0.05).
4.3 Impact of Phrase Length
Our proposed phrase-based translation model, due to
its capability of capturing contextual information, is
more effective than the state-of-the-art word-based
translation models. It is important to investigate the
impact of the phrase length on the final retrieval per-
formance. Table 5 shows the results, it is seen that
using the longer phrases up to the maximum length
of five can consistently improve the retrieval per-
formance. However, using much longer phrases in
the phrase-based translation model does not seem to
produce significantly better performance (row 8 and
row 9 vs. row 10 are not statistically significant).
# Systems MAP
6 P-Trans (l = 1) 0.352
7 P-Trans (l = 2) 0.373
8 P-Trans (l = 3) 0.386
9 P-Trans (l = 4) 0.390
10 P-Trans (l = 5) 0.391
Table 5: The impact of the phrase length on retrieval per-
formance.
Model # Methods Average MAP
P-Trans (l = 5) 11 Initial 69 0.38012 TextRank 24 0.391
Table 6: Effectiveness of parallel corpus preprocessing.
4.4 Effectiveness of Parallel Corpus
Preprocessing
Question-answer pairs collected from Yahoo! an-
swers are very noisy, it is possible for translation
models to contain ?unnecessary? translations. In this
paper, we attempt to identify and decrease the pro-
portion of unnecessary translations in a translation
model by using TextRank algorithm. This kind of
?unnecessary? translation between words will even-
tually affect the bi-phrase translation.
Table 6 shows the effectiveness of parallel corpus
preprocessing. Row 11 reports the average number
of translations per word and the question retrieval
performance when only stopwords 7 are removed.
When using the TextRank algorithm for parallel cor-
pus preprocessing, the average number of transla-
tions per word is reduced from 69 to 24, but the
performance of question retrieval is significantly im-
proved (row 11 vs. row 12). Similar results have
been made by Lee et al (2008).
4.5 Impact of Pooling Strategy
The correspondence of words or phrases in the
question-answer pair is not as strong as in the bilin-
gual sentence pair, thus noise will be inevitably in-
troduced for both P (a?|q?) and P (q?|a?).
To see how much the pooling strategy benefit the
question retrieval, we introduce two baseline meth-
ods for comparison. The first method (denoted as
P (a?|q?)) is used to denote the translation probabil-
ity with the question as the source and the answer as
7http://truereader.com/manuals/onix/stopwords1.html
660
Model # Trans Prob MAP
P-Trans (l = 5)
13 P (a?|q?) 0.387
14 P (q?|a?) 0.381
15 Ppool 0.391
Table 7: The impact of pooling strategy for question re-
trieval.
the target. The second (denoted as P (a?|q?)) is used
to denote the translation probability with the answer
as the source and the question as the target. Table 7
provides the comparison. From this Table, we see
that the pooling strategy significantly outperforms
the two baseline methods for question retrieval (row
13 and row 14 vs. row 15).
5 Conclusions and Future Work
In this paper, we propose a novel phrase-based trans-
lation model for question retrieval. Compared to
the traditional word-based translation models, the
proposed approach is more effective in that it can
capture contextual information instead of translating
single words in isolation. Experiments conducted
on real Q&A data demonstrate that the phrase-
based translation model significantly outperforms
the state-of-the-art word-based translation models.
There are some ways in which this research could
be continued. First, question structure should be
considered, so it is necessary to combine the pro-
posed approach with other question retrieval meth-
ods (e.g., (Duan et al, 2008; Wang et al, 2009;
Bunescu and Huang, 2010)) to further improve the
performance. Second, we will try to investigate the
use of the proposed approach for other kinds of data
set, such as categorized questions from forum sites
and FAQ sites.
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 60875041 and
No. 61070106). We thank the anonymous reviewers
for their insightful comments. We also thank Maoxi
Li and Jiajun Zhang for suggestion to use the align-
ment toolkits.
References
A. Berger and R. Caruana and D. Cohn and D. Freitag and
V. Mittal. 2000. Bridging the lexical chasm: statistical
approach to answer-finding. In Proceedings of SIGIR,
pages 192-199.
A. Berger and J. Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of SIGIR, pages
222-229.
D. Bernhard and I. Gurevych. 2009. Combining lexical
semantic resources with question & answer archives
for translation-based answer finding. In Proceedings
of ACL, pages 728-736.
P. F. Brown and V. J. D. Pietra and S. A. D. Pietra and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263-311.
R. Bunescu and Y. Huang. 2010. Learning the relative
usefulness of questions in community QA. In Pro-
ceedings of EMNLP, pages 97-107.
X. Cao and G. Cong and B. Cui and C. S. Jensen. 2010.
A generalized framework of exploring category infor-
mation for question retrieval in community question
answer archives. In Proceedings of WWW.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL.
H. Duan and Y. Cao and C. Y. Lin and Y. Yu. 2008.
Searching questions by identifying questions topics
and question focus. In Proceedings of ACL, pages
156-164.
J. Gao and X. He and J. Nie. 2010. Clickthrough-based
translation models for web search: from word models
to phrase models. In Proceedings of CIKM.
J. Jeon and W. Bruce Croft and J. H. Lee. 2005. Find-
ing similar questions in large question and answer
archives. In Proceedings of CIKM, pages 84-90.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into text. In Proceedings of EMNLP, pages 404-
411.
P. Koehn and F. Och and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL,
pages 48-54.
J. -T. Lee and S. -B. Kim and Y. -I. Song and H. -C. Rim.
2008. Bridging lexical gaps between queries and ques-
tions on large online Q&A collections with compact
translation models. In Proceedings of EMNLP, pages
410-418.
F. Och. 2002. Statistical mahcine translation: from sin-
gle word models to alignment templates. Ph.D thesis,
RWTH Aachen.
F. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computa-
tional Linguistics, 30(4):417-449.
661
J. M. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of
SIGIR.
W. H. Press and S. A. Teukolsky and W. T. Vetterling
and B. P. Flannery. 1992. Numerical Recipes In C.
Cambridge Univ. Press.
S. Robertson and S. Walker and S. Jones and M.
Hancock-Beaulieu and M. Gatford. 1994. Okapi at
trec-3. In Proceedings of TREC, pages 109-126.
G. Salton and A. Wong and C. S. Yang. 1975. A vector
space model for automatic indexing. Communications
of the ACM, 18(11):613-620.
X. Sun and J. Gao and D. Micol and C. Quirk. 2010.
Learning phrase-based spelling error models from
clickthrough data. In Proceedings of ACL.
K. Wang and Z. Ming and T-S. Chua. 2009. A syntactic
tree matching approach to finding similar questions in
community-based qa services. In Proceedings of SI-
GIR, pages 187-194.
X. Xue and J. Jeon and W. B. Croft. 2008. Retrieval
models for question and answer archives. In Proceed-
ings of SIGIR, pages 475-482.
C. Zhai and J. Lafferty. 2001. A study of smooth meth-
ods for language models applied to ad hoc information
retrieval. In Proceedings of SIGIR, pages 334-342.
662
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1556?1565,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Exploiting Web-Derived Selectional Preference to Improve Statistical
Dependency Parsing
Guangyou Zhou, Jun Zhao?, Kang Liu, and Li Cai
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,jzhao,kliu,lcai}@nlpr.ia.ac.cn
Abstract
In this paper, we present a novel approach
which incorporates the web-derived selec-
tional preferences to improve statistical de-
pendency parsing. Conventional selectional
preference learning methods have usually fo-
cused on word-to-class relations, e.g., a verb
selects as its subject a given nominal class.
This paper extends previous work to word-
to-word selectional preferences by using web-
scale data. Experiments show that web-scale
data improves statistical dependency pars-
ing, particularly for long dependency relation-
ships. There is no data like more data, perfor-
mance improves log-linearly with the number
of parameters (unique N-grams). More impor-
tantly, when operating on new domains, we
show that using web-derived selectional pref-
erences is essential for achieving robust per-
formance.
1 Introduction
Dependency parsing is the task of building depen-
dency links between words in a sentence, which has
recently gained a wide interest in the natural lan-
guage processing community. With the availabil-
ity of large-scale annotated corpora such as Penn
Treebank (Marcus et al, 1993), it is easy to train
a high-performance dependency parser using super-
vised learning methods.
However, current state-of-the-art statistical de-
pendency parsers (McDonald et al, 2005; McDon-
ald and Pereira, 2006; Hall et al, 2006) tend to have
?Correspondence author: jzhao@nlpr.ia.ac.cn
lower accuracies for longer dependencies (McDon-
ald and Nivre, 2007). The length of a dependency
from word wi to word wj is simply equal to |i ? j|.
Longer dependencies typically represent the mod-
ifier of the root or the main verb, internal depen-
dencies of longer NPs or PP-attachment in a sen-
tence. Figure 1 shows the F1 score1 relative to the
dependency length on the development set by using
the graph-based dependency parsers (McDonald et
al., 2005; McDonald and Pereira, 2006). We note
that the parsers provide very good results for adja-
cent dependencies (96.89% for dependency length
=1), while the dependency length increases, the ac-
curacies degrade sharply. These longer dependen-
cies are therefore a major opportunity to improve the
overall performance of dependency parsing. Usu-
ally, these longer dependencies can be parsed de-
pendent on the specific words involved due to the
limited range of features (e.g., a verb and its mod-
ifiers). Lexical statistics are therefore needed for
resolving ambiguous relationships, yet the lexical-
ized statistics are sparse and difficult to estimate di-
rectly. To solve this problem, some information with
different granularity has been investigated. Koo et
al. (2008) proposed a semi-supervised dependency
parsing by introducing lexical intermediaries at a
coarser level than words themselves via a cluster
method. This approach, however, ignores the se-
lectional preference for word-to-word interactions,
such as head-modifier relationship. Extra resources
1Precision represents the percentage of predicted arcs of
length d that are correct, and recall measures the percentage
of gold-standard arcs of length d that are correctly predicted.
F1 = 2? precision ? recall/(precision + recall)
1556
1 5 10 15 20 25 300.7
0.75
0.8
0.85
0.9
0.95
1
Dependency Length
F1 S
core
 (%)
MST1MST2
Figure 1: F score relative to dependency length.
beyond the annotated corpora are needed to capture
the bi-lexical relationship at the word-to-word level.
Our purpose in this paper is to exploit web-
derived selectional preferences to improve the su-
pervised statistical dependency parsing. All of our
lexical statistics are derived from two kinds of web-
scale corpus: one is the web, which is the largest
data set that is available for NLP (Keller and Lap-
ata, 2003). Another is a web-scale N-gram corpus,
which is a N-gram corpus with N-grams of length 1-
5 (Brants and Franz, 2006), we call it Google V1 in
this paper. The idea is very simple: web-scale data
have large coverage for word pair acquisition. By
leveraging some assistant data, the dependency pars-
ing model can directly utilize the additional informa-
tion to capture the word-to-word level relationships.
We address two natural and related questions which
some previous studies leave open:
Question I: Is there a benefit in incorporating
web-derived selectional preference features for sta-
tistical dependency parsing, especially for longer de-
pendencies?
Question II: How well do web-derived selec-
tional preferences perform on new domains?
For Question I, we systematically assess the value
of using web-scale data in state-of-the-art super-
vised dependency parsers. We compare dependency
parsers that include or exclude selectional prefer-
ence features obtained from web-scale corpus. To
the best of our knowledge, none of the existing stud-
ies directly address long dependencies of depen-
dency parsing by using web-scale data.
Most statistical parsers are highly domain depen-
dent. For example, the parsers trained on WSJ text
perform poorly on Brown corpus. Some studies have
investigated domain adaptation for parsers (Mc-
Closky et al, 2006; Daume? III, 2007; McClosky et
al., 2010). These approaches assume that the parsers
know which domain it is used, and that it has ac-
cess to representative data in that domain. How-
ever, in practice, these assumptions are unrealistic
in many real applications, such as when processing
the heterogeneous genre of web texts. In this paper
we incorporate the web-derived selectional prefer-
ence features to design our parsers for robust open-
domain testing.
We conduct the experiments on the English Penn
Treebank (PTB) (Marcus et al, 1993). The results
show that web-derived selectional preference can
improve the statistical dependency parsing, partic-
ularly for long dependency relationships. More im-
portantly, when operating on new domains, the web-
derived selectional preference features show great
potential for achieving robust performance (Section
4.3).
The remainder of this paper is divided as follows.
Section 2 gives a brief introduction of dependency
parsing. Section 3 describes the web-derived selec-
tional preference features. Experimental evaluation
and results are reported in Section 4. Finally, we dis-
cuss related work and draw conclusion in Section 5
and Section 6, respectively.
2 Dependency Parsing
In dependency parsing, we attempt to build head-
modifier (or head-dependent) relations between
words in a sentence. The discriminative parser we
used in this paper is based on the part-factored
model and features of the MSTParser (McDonald et
al., 2005; McDonald and Pereira, 2006; Carreras,
2007). The parsing model can be defined as a con-
ditional distribution p(y|x;w) over each projective
parse tree y for a particular sentence x, parameter-
ized by a vector w. The probability of a parse tree
is
p(y|x;w) = 1
Z(x;w)
exp
{
?
??y
w ??(x, ?)
}
(1)
where Z(x;w) is the partition function and ? are
part-factored feature functions that include head-
1557
modifier parts, sibling parts and grandchild parts.
Given the training set {(xi, yi)}Ni=1, parameter es-
timation for log-linear models generally resolve
around optimization of a regularized conditional
log-likelihood objective w? = argminwL(w)
where
L(w) = ?C
N
?
i=1
logp(yi|xi;w) +
1
2
||w||2 (2)
The parameter C > 0 is a constant dictating the
level of regularization in the model. Since objec-
tive function L(w) is smooth and convex, which is
convenient for standard gradient-based optimization
techniques. In this paper we use the dual exponenti-
ated gradient (EG)2 descent, which is a particularly
effective optimization algorithm for log-linear mod-
els (Collins et al, 2008).
3 Web-Derived Selectional Preference
Features
In this paper, we employ two different feature sets:
a baseline feature set3 which draw upon ?normal?
information source, such as word forms and part-of-
speech (POS) without including the web-derived se-
lectional preference4 features, a feature set conjoins
the baseline features and the web-derived selectional
preference features.
3.1 Web-scale resources
All of our selectional preference features described
in this paper rely on probabilities derived from unla-
beled data. To use the largest amount of data possi-
ble, we exploit web-scale resources. one is web, N-
gram counts are approximated by Google hits. An-
other we use isGoogle V1 (Brants and Franz, 2006).
This N-gram corpus records how often each unique
sequence of words occurs. N-grams appearing 40
2http://groups.csail.mit.edu/nlp/egstra/
3This kind of feature sets are similar to other feature sets in
the literature (McDonald et al, 2005; Carreras, 2007), so we
will not attempt to give a exhaustive description.
4Selectional preference tells us which arguments are plau-
sible for a particular predicate, one way to determine the se-
lectional preference is from co-occurrences of predicates and
arguments in text (Bergsma et al, 2008). In this paper, the
selectional preferences have the same meaning with N-grams,
which model the word-to-word relationships, rather than only
considering the predicates and arguments relationships.
obj
detdet
root
obj
mod
subj
Figure 2: An example of a labeled dependency tree. The
tree contains a special token ?$? which is always the root
of the tree. Each arc is directed from head to modifier and
has a label describing the function of the attachment.
times or more (1 in 25 billion) are kept, and appear
in the n-gram tables. All n-grams with lower counts
are discarded. Co-occurrence probabilities can be
calculated directly from the N-gram counts.
3.2 Web-derived N-gram features
3.2.1 PMI
Previous work on noun compounds bracketing
has used adjacency model (Resnik, 1993) and de-
pendency model (Lauer, 1995) to compute associa-
tion statistics between pairs of words. In this pa-
per we generalize the adjacency and dependency
models by including the pointwise mutual informa-
tion (Church and Hanks, 1900) between all pairs of
words in the dependency tree:
PMI(x, y) = log p(?x y?)
p(?x?)p(?y?)
(3)
where p(?x y?) is the co-occurrence probabilities.
When use the Google V1 corpus, this probabilities
can be calculated directly from the N-gram counts,
while using the Google hits, we send the queries to
the search engine Google5 and all the search queries
are performed as exact matches by using quotation
marks.6
The value of these features is the PMI, if it is de-
fined. If the PMI is undefined, following the work
of (Pitler et al, 2010), we include one of two binary
features:
p(?x y?) = 0 or p(?x?) ? p(?y?) = 0
Besides, we also consider the trigram features be-
5http://www.google.com/
6Google only allows automated querying through the
Google Web API, this involves obtaining a license key, which
then restricts the number of queries to a daily quota of 1000.
However, we obtained a quota of 20,000 queries per day by
sending a request to api-support@google.com for research pur-
poses.
1558
PMI(?hit with?)
xi-word=?hit?, xj-word=?with?, PMI(?hit with?)
xi-word=?hit?, xj-word=?with?, xj-pos=?IN?, PMI(?hit with?)
xi-word=?hit?, xi-pos=?VBD?, xj-word=?with?, PMI(?hit with?)
xi-word=?hit?, b-pos=?ball?, xj-word=?with?, PMI(?hit with?)
xi-word=?hit?, xj-word=?with?, PMI(?hit with?), dir=R, dist=3
. . .
Table 1: An example of the N-gram PMI features and the conjoin features with the baseline.
tween the three words in the dependency tree:
PMI(x, y, z) = log p(?x y z?)
p(?x y?)p(?y z?)
(4)
This kinds of trigram features, for example in MST-
Parser, which can directly capture the sibling and
grandchild features.
We illustrate the PMI features with an example
of dependency parsing tree in Figure 2. In deciding
the dependency between the main verb hit and its ar-
gument headed preposition with, an example of the
N-gram PMI features and the conjoin features with
the baseline are shown in Table 1.
3.2.2 PP-attachment
Propositional phrase (PP) attachment is one of
the hardest problems in English dependency pars-
ing. An English sentence consisting of a subject, a
verb, and a nominal object followed by a preposi-
tional phrase is often ambiguous. Ambiguity resolu-
tion reflects the selectional preference between the
verb and noun with their prepositional phrase. For
example, considering the following two examples:
(1) John hit the ball with the bat.
(2) John hit the ball with the red stripe.
In sentence (1), the preposition with depends on the
main verb hit; but in sentence (2), the prepositional
phrase is a noun attribute and the preposition with
needs to depends on the word ball. To resolve this
kind of ambiguity, there needs to measure the attach-
ment preference. We thus have PP-attachment fea-
tures that determine the PMI association across the
preposition word ?IN?7:
PMIIN (x, z) = log
p(?x IN z?)
p(x)
(5)
7Here, the preposition word ?IN? (e.g., ?with?, ?in?, . . .) is
any token whose part-of-speech is IN
N-gram feature templates
hw, mw, PMI(hw,mw)
hw, ht, mw, PMI(hw,mw)
hw, mw, mt, PMI(hw,mw)
hw, ht, mw, mt, PMI(hw,mw)
. . .
hw, mw, sw
hw, mw, sw, PMI(hw, mw, sw)
hw, mw, gw
hw, mw, gw, PMI(hw, mw, gw)
Table 2: Examples of N-gram feature templates. Each
entry represents a class of indicator for tuples of informa-
tion. For example, ?hw, mw? reprsents a class of indi-
cator features with one feature for each possible combi-
nation of head word and modifier word. Abbreviations:
hw=head word, ht= head POS. st, gt=likewise for sibling
and grandchild.
PMIIN (y, z) = log
p(?y IN z?)
p(y)
(6)
where the word x and y are usually verb and noun,
z is a noun which directly depends on the preposi-
tion word ?IN?. For example in sentence (1), we
would include the features PMIwith(hit, bat) and
PMIwith(ball, bat). If both PMI features exist and
PMIwith(hit, bat) > PMIwith(ball, bat), indicating
to our dependency parsing model that the preposi-
tion word with depends on the verb hit is a good
choice. While in sentence (2), the features include
PMIwith(hit, stripe) and PMIwith(ball, stripe).
3.3 N-gram feature templates
We generate N-gram features by mimicking the
template structure of the original baseline features.
For example, the baseline feature set includes indi-
cators for word-to-word and tag-to-tag interactions
between the head and modifier of a dependency. In
the N-gram feature set, we correspondingly intro-
duce N-gram PMI for word-to-word interactions.
1559
The N-gram feature set for MSTParser is shown
in Table 2. Following McDonald et al (2005),
all features are conjoined with the direction of
attachment as well as the distance between the two
words creating the dependency. In between N-gram
features, we include the form of word trigrams
and PMI of the trigrams. The surrounding word
N-gram features represent the local context of the
selectional preference. Besides, we also present
the second-order feature templates, including the
sibling and grandchild features. These features are
designed to disambiguate cases like coordinating
conjunctions and prepositional attachment. Con-
sider the examples we have shown in section 3.2.2,
for sentence (1), the dependency graph path feature
ball ? with ? bat should have a lower weight
since ball rarely is modified by bat, but is often
seen through them (e.g., a higher weight should be
associated with hit ? with ? bat). In contrast,
for sentence (2), our N-gram features will tell us
that the prepositional phrase is much more likely
to attach to the noun since the dependency graph
path feature ball ? with ? stripe should have a
high weight due to the high strength of selectional
preference between ball and stripe.
Web-derived selectional preference features
based on PMI values are trickier to incorporate
into the dependency parsing model because they
are continuous rather than discrete. Since all the
baseline features used in the literature (McDonald et
al., 2005; Carreras, 2007) take on binary values of 0
or 1, there is a ?mis-match? between the continuous
and binary features. Log-linear dependency parsing
model is sensitive to inappropriately scaled feature.
To solve this problem, we transform the PMI
values into a more amenable form by replacing the
PMI values with their z-score. The z-score of a
PMI value x is x??? , where ? and ? are the mean
and standard deviation of the PMI distribution,
respectively.
4 Experiments
In order to evaluate the effectiveness of our proposed
approach, we conducted dependency parsing exper-
iments in English. The experiments were performed
on the Penn Treebank (PTB) (Marcus et al, 1993),
using a standard set of head-selection rules (Yamada
and Matsumoto, 2003) to convert the phrase struc-
ture syntax of the Treebank into a dependency tree
representation, dependency labels were obtained via
the ?Malt? hard-coded setting.8 We split the Tree-
bank into a training set (Sections 2-21), a devel-
opment set (Section 22), and several test sets (Sec-
tions 0,9 1, 23, and 24). The part-of-speech tags for
the development and test set were automatically as-
signed by the MXPOST tagger10, where the tagger
was trained on the entire training corpus.
Web page hits for word pairs and trigrams are ob-
tained using a simple heuristic query to the search
engine Google.11 Inflected queries are performed
by expanding a bigram or trigram into all its mor-
phological forms. These forms are then submitted as
literal queries, and the resulting hits are summed up.
John Carroll?s suite of morphological tools12 is used
to generate inflected forms of verbs and nouns. All
the search terms are performed as exact matches by
using quotation marks and submitted to the search
engines in lower case.
We measured the performance of the parsers us-
ing the following metrics: unlabeled attachment
score (UAS), labeled attachment score (LAS) and
complete match (CM), which were defined by Hall
et al (2006). All the metrics are calculated as mean
scores per word, and punctuation tokens are consis-
tently excluded.
4.1 Main results
There are some clear trends in the results of Ta-
ble 3. First, performance increases with the order
of the parser: edge-factored model (dep1) has the
lowest performance, adding sibling and grandchild
relationships (dep2) significantly increases perfor-
mance. Similar observations regarding the effect of
model order have also been made by Carreras (2007)
and Koo et al (2008).
Second, note that the parsers incorporating the N-
gram feature sets consistently outperform the mod-
els using the baseline features in all test data sets,
regardless of model order or label usage. Another
8http://w3.msi.vxu.se/ nivre/research/MaltXML.html
9We removed a single 249-word sentence from Section 0 for
computational reasons.
10http://www.inf.ed.ac.uk/resources/nlp/local doc/MXPOST.html
11http://www.google.com/
12http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html.
1560
Sec dep1 +hits +V1 dep2 +hits +V1 dep1-L +hits-L +V1-L dep2-L +hits-L +V1-L
00 90.39 90.94 90.91 91.56 92.16 92.16 90.11 90.69 90.67 91.94 92.47 92.42
01 91.01 91.60 91.60 92.27 92.89 92.86 90.77 91.39 91.39 91.81 92.38 92.37
23 90.82 91.46 91.39 91.98 92.64 92.59 90.30 90.98 90.92 91.24 91.83 91.77
24 89.53 90.15 90.13 90.81 91.44 91.41 89.42 90.03 90.02 90.30 90.91 90.89
Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:
dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from
the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are
scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
finding is that the N-gram features derived from
Google hits are slightly better than Google V1 due
to the large N-gram coverage, we will discuss later.
As a final note, all the comparisons between the inte-
gration of N-gram features and the baseline features
in Table 3 are mildly significant using the Z-test of
Collins et al (2005) (p < 0.08).
Type Systems UAS CM
D
Yamada and Matsumoto (2003) 90.3 38.7
McDonald et al (2005) 90.9 37.5
McDonald and Pereira (2006) 91.5 42.1
Corston-Oliver et al (2006) 90.9 37.5
Hall et al (2006) 89.4 36.4
Wang et al (2007) 89.2 34.4
Carreras et al (2008) 93.5 -
GoldBerg and Elhadad (2010)? 91.32 40.41
Ours 92.64 46.61
C
Nivre and McDonald (2008)? 92.12 44.37
Martins et al (2008)? 92.87 45.51
Zhang and Clark (2008) 92.1 45.4
S
Koo et al (2008) 93.16 -
Suzuki et al (2009) 93.79 -
Chen et al (2009) 93.16 47.15
Table 4: Comparison of our final results with other best-
performing systems on the whole Section 23. Type
D, C and S denote discriminative, combined and semi-
supervised systems, respectively. ? These papers were
not directly reported the results on this data set, we im-
plemented the experiments in this paper.
To put our results in perspective, we also com-
pare them with other best-performing systems in Ta-
ble 4. To facilitate comparisons with previous work,
we only use Section 23 as the test data. The re-
sults show that our second order model incorpo-
rating the N-gram features (92.64) performs better
than most previously reported discriminative sys-
tems trained on the Treebank. Carreras et al (2008)
reported a very high accuracy using information of
constituent structure of TAG grammar formalism,
while in our system, we do not use such knowl-
edge. When compared to the combined systems, our
system is better than Nivre and McDonald (2008)
and Zhang and Clark (2008), but a slightly worse
than Martins et al (2008). We also compare our
method with the semi-supervised approaches, the
semi-supervised approaches achieved very high ac-
curacies by leveraging on large unlabeled data di-
rectly into the systems for joint learning and decod-
ing, while in our method, we only explore the N-
gram features to further improve supervised depen-
dency parsing performance.
Table 5 shows the details of some other N-gram
sources, where NEWS: created from a large set of
news articles including the Reuters and Gigword
(Graff, 2003) corpora. For a given number of unique
N-gram, using any of these sources does not have
significant difference in Figure 3. Google hits is
the largest N-gram data and shows the best perfor-
mance. The other two are smaller ones, accuracies
increase linearly with the log of the number of types
in the auxiliary data set. Similar observations have
been made by Pitler et al (2010). We see that the
relationship between accuracy and the number of N-
gram is not monotonic for Google V1. The reason
may be that Google V1 does not make detailed pre-
processing, containing many mistakes in the corpus.
Although Google hits is noisier, it has very much
larger coverage of bigrams or trigrams.
Some previous studies also found a log-linear
relationship between unlabeled data (Suzuki and
Isozaki, 2008; Suzuki et al, 2009; Bergsma et al,
2010; Pitler et al, 2010). We have shown that this
trend continues well for dependency parsing by us-
ing web-scale data (NEWS and Google V1).
13Google indexes about more than 8 billion pages and each
contains about 1,000 words on average.
1561
Corpus # of tokens ? # of types
NEWS 3.2B 1 3.7B
Google V1 1,024.9B 40 3.4B
Google hits13 8,000B 100 -
Table 5: N-gram data, with total number of words in the
original corpus (in billions, B). Following (Brants and
Franz, 2006; Pitler et al, 2010), we set the frequency
threshold to filter the data ?, and total number of unique
N-gram (types) remaining in the data.
1e4 1e5 1e6 1e7 1e8 1e991.9
92
92.1
92.2
92.3
92.4
92.5
92.6
92.7
Number of Unique N-grams
UAS
 Sco
re (%
)
NEWSGoogle V1Google hits
Figure 3: There is no data like more data. UAS accu-
racy improves with the number of unique N-grams but
still lower than the Google hits.
4.2 Improvement relative to dependency length
The experiments in (McDonald and Nivre, 2007)
showed a negative impact on the dependency pars-
ing performance from too long dependencies. For
our proposed approach, the improvement relative
to dependency length is shown in Figure 4. From
the Figure, it is seen that our method gives observ-
able better performance when dependency lengths
are larger than 3. The results here show that the
proposed approach improves the dependency pars-
ing performance, particularly for long dependency
relationships.
4.3 Cross-genre testing
In this section, we present the experiments to vali-
date the robustness the web-derived selectional pref-
erences. The intent is to understand how well the
web-derived selectional preferences transfer to other
sources.
The English experiment evaluates the perfor-
mance of our proposed approach when it is trained
1 10 20 300.75
0.8
0.85
0.9
0.95
1
Dependency Length
F1 S
core
 (%)
MST2MST2+N-gram
Figure 4: Dependency length vs. F1 score.
on annotated data from one genre of text (WSJ) and
is used to parse a test set from a different genre: the
biomedical domain related to cancer (PennBioIE.,
2005) with 2,600 parsed sentences. We divided the
data into 500 for training, 100 for development and
others for testing. We created five sets of train-
ing data with 100, 200, 300, 400, and 500 sen-
tences respectively. Figure 5 plots the UAS ac-
curacy as function of training instances. WSJ is
the performance of our second-order dependency
parser trained on section 2-21; WSJ+N-gram is the
performance of our proposed approach trained on
section 2-21; WSJ+BioMed is the performance of
the parser trained on WSJ and biomedical data.
WSJ+BioMed+N-gram is the performance of our
proposed approach trained on WSJ and biomedical
data. The results show that incorporating the web-
scale N-gram features can significantly improve the
dependency parsing performance, and the improve-
ment is much larger than the in-domain testing pre-
sented in Section 4.1, the reason may be that web-
derived N-gram features do not depend directly on
training data and thus work better on new domains.
4.4 Discussion
In this paper, we present a novel method to im-
prove dependency parsing by using web-scale data.
Despite the success, there are still some problems
which should be discussed.
(1) Google hits is less sparse than Google V1
in modeling the word-to-word relationships, but
Google hits are likely to be noisier than Google V1.
It is very appealing to carry out a correlation anal-
1562
100 150 200 250 300 350 400 450 50080
81
82
83
84
85
86
87
88
UAS
 Sco
re (%
)
WSJWSJ+N-gramWSJ+BioMedWSJ+BioMed+N-gram
Figure 5: Adapting a WSJ parser to biomedical text.
WSJ: performance of parser trained only on WSJ;
WSJ+N-gram: performance of our proposed approach
trained only on WSJ; WSJ+BioMed: parser trained on
WSJ and biomedical text; WSJ+BioMed+N-gram: our
approach trained on WSJ and biomedical text.
ysis to determine whether Google hits and Google
V1 are highly correlated. We will leave it for future
research.
(2) Veronis (2005) pointed out that there had been
a debate about reliability of Google hits due to the
inconsistencies of page hits estimates. However, this
estimate is scale-invariant. Assume that when the
number of pages indexed by Google grows, the num-
ber of pages containing a given search term goes to
a fixed fraction. This means that if pages indexed
by Google doubles, then so do the bigrams or tri-
grams frequencies. Therefore, the estimate becomes
stable when the number of indexed pages grows un-
boundedly. Some details are presented in Cilibrasi
and Vitanyi (2007).
5 Related Work
Our approach is to exploit web-derived selectional
preferences to improve the dependency parsing. The
idea of this paper is inspired by the work of Suzuki
et al (2009) and Pitler et al (2010). The former uses
the web-scale data explicitly to create more data for
training the model; while the latter explores the web-
scale N-grams data (Lin et al, 2010) for compound
bracketing disambiguation. Our research, however,
applies the web-scale data (Google hits and Google
V1) to model the word-to-word dependency rela-
tionships rather than compound bracketing disam-
biguation.
Several previous studies have exploited the web-
scale data for word pair acquisition. Keller and
Lapata (2003) evaluated the utility of using web
search engine statistics for unseen bigram. Nakov
and Hearst (2005) demonstrated the effectiveness of
using search engine statistics to improve the noun
compound bracketing. Volk (2001) exploited the
WWWas a corpus to resolve PP attachment ambigu-
ities. Turney (2007) measured the semantic orienta-
tion for sentiment classification using co-occurrence
statistics obtained from the search engines. Bergsma
et al (2010) created robust supervised classifiers
via web-scale N-gram data for adjective ordering,
spelling correction, noun compound bracketing and
verb part-of-speech disambiguation. Our approach,
however, extends these techniques to dependency
parsing, particularly for long dependency relation-
ships, which involves more challenging tasks than
the previous work.
Besides, there are some work exploring the word-
to-word co-occurrence derived from the web-scale
data or a fixed size of corpus (Calvo and Gel-
bukh, 2004; Calvo and Gelbukh, 2006; Yates et al,
2006; Drabek and Zhou, 2000; van Noord, 2007)
for PP attachment ambiguities or shallow parsing.
Johnson and Riezler (2000) incorporated the lex-
ical selectional preference features derived from
British National Corpus (Graff, 2003) into a stochas-
tic unification-based grammar. Abekawa and Oku-
mura (2006) improved Japanese dependency pars-
ing by using the co-occurrence information derived
from the results of automatic dependency parsing of
large-scale corpora. However, we explore the web-
scale data for dependency parsing, the performance
improves log-linearly with the number of parameters
(unique N-grams). To the best of our knowledge,
web-derived selectional preference has not been suc-
cessfully applied to dependency parsing.
6 Conclusion
In this paper, we present a novel method which in-
corporates the web-derived selectional preferences
to improve statistical dependency parsing. The re-
sults show that web-scale data improves the de-
pendency parsing, particularly for long dependency
relationships. There is no data like more data,
performance improves log-linearly with the num-
1563
ber of parameters (unique N-grams). More impor-
tantly, when operating on new domains, the web-
derived selectional preferences show great potential
for achieving robust performance.
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 60875041 and
No. 61070106), and CSIDM project (No. CSIDM-
200805) partially funded by a grant from the Na-
tional Research Foundation (NRF) administered by
the Media Development Authority (MDA) of Singa-
pore. We thank the anonymous reviewers for their
insightful comments.
References
T. Abekawa and M. Okumura. 2006. Japanese depen-
dency parsing using co-occurrence information and a
combination of case elements. In Proceedings of ACL-
COLING.
S. Bergsma, D. Lin, and R. Goebel. 2008. Discriminative
learning of selectional preference from unlabeled text.
In Proceedings of EMNLP, pages 59-68.
S. Bergsma, E. Pitler, and D. Lin. 2010. Creating robust
supervised classifier via web-scale N-gram data. In
Proceedings of ACL.
T. Brants and Alex Franz. 2006. The Google Web 1T
5-gram Corpus Version 1.1. LDC2006T13.
H. Calvo and A. Gelbukh. 2004. Acquiring selec-
tional preferences from untagged text for prepositional
phrase attachment disambiguation. In Proceedings of
VLDB.
H. Calvo and A. Gelbukh. 2006. DILUCT: An open-
source Spanish dependency parser based on rules,
heuristics, and selectional preferences. In Lecture
Notes in Computer Science 3999, pages 164-175.
X. Carreras. 2007. Experiments with a higher-order pro-
jective dependency parser. In Proceedings of EMNLP-
CoNLL, pages 957-961.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In Proceedings of CoNLL.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.
2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC
No. LDC2000T43.Linguistic Data Consortium.
W. Chen, D. Kawahara, K. Uchimoto, and Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP,
pages 570-579.
K. W. Church and P. Hanks. 1900. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22-29.
R. L. Cilibrasi and P. M. B. Vitanyi. 2007. The Google
similarity distance. IEEE Transaction on Knowledge
and Data Engineering, 19(3):2007. pages 370-383.
M. Collins, A. Globerson, T. Koo, X. Carreras, and P.
L. Bartlett. 2008. Exponentiated gradient algorithm
for conditional random fields and max-margin markov
networks. Journal of Machine Learning Research,
pages 1775?1822.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of ACL, pages 531-540.
S. Corston-Oliver, A. Aue, Kevin. Duh, and E. Ringger.
2006. Multilingual dependency parsing using bayes
point machines. In Proceedings of NAACL.
H. Daume? III. 2007. Frustrating easy domain adaptation.
In Proceedings of ACL.
E. F. Drabek and Q. Zhou. 2000. Using co-occurrence
statistics as an information source for partial parsing of
Chinese. In Proceedings of Second Chinese Language
Processing Workshop, ACL, pages 22-28.
Y. GoldBerg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proceedings of NAACL, pages 742-750.
D. Graff. 2003. English Gigaword, LDC2003T05.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discrimina-
tive classifier for deterministic dependency parsing. In
Proceedings of ACL, pages 316-323.
M. Johnson and S. Riezler. 2000. Exploiting auxiliary
distribution in stochastic unification-based garmmars.
In Proceedings of NAACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
of ACL, pages 595-603.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459-484.
M. Lapata and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1), pages 1-30.
M. Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
ACL.
D. K. Lin, H. Church, S. Ji, S. Sekine, D. Yarowsky, S.
Bergsma, K. Patil, E. Pitler, E. Lathbury, V Rao, K.
Dalwani, and S. Narsale. 2010. New tools for web-
scale n-grams. In Proceedings of LREC.
M.P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. Computational Linguistics.
1564
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proceedings
of EMNLP, pages 157-166.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of ACL.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic Domain Adapatation for Parsing. In Proceed-
ings of NAACL-HLT.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81-88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, pages 91-98.
P. Nakov and M. Hearst. 2005. Search engine statis-
tics beyond the n-gram: application to noun compound
bracketing. In Proceedings of CoNLL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL, pages 950-958.
G. van Noord. 2007. Using self-trained bilexical pref-
erences to improve disambiguation accuracy. In Pro-
ceedings of IWPT, pages 1-10.
PennBioIE. 2005. Mining the bibliome project, 2005.
http:bioie.ldc.upenn.edu/.
E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Us-
ing web-scale N-grams to improve base NP parsing
performance. In Proceedings of COLING, pages 886-
894.
P. Resnik. 1993. Selection and information: a class-
based approach to lexical relationships. Ph.D. thesis,
University of Pennsylvania.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of EMNLP, pages 551-560.
J. Suzuki and H. Isozaki. 2008. Semi-supervised sequen-
tial labeling and segmentation using giga-word scale
unlabeled data. In Proceedings of ACL, pages 665-
673.
P. D. Turney. 2003. Measuring praise and criticism:
Inference of semantic orientation from association.
ACM Transactions on Information Systems, 21(4).
J. Veronis. 2005. Web: Google adjusts its counts. Jean
Veronis? blog: http://aixtal.blogsplot.com/2005/03/
web-google-adjusts-its-count.html.
M. Volk. 2001. Exploiting the WWW as corpus to re-
solve PP attachment ambiguities. In Proceedings of
the Corpus Linguistics.
Q. I. Wang, D. Lin, and D. Schuurmans. 2007. Simple
training of dependency parsers via structured boosting.
In Proceedings of IJCAI, pages 1756-1762.
Yamada and Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proceedings
of IWPT, pages 195-206.
A. Yates, S. Schoenmackers, and O. Etzioni. 2006. De-
tecting parser errors using web-based semantic filters.
In Proceedings of EMNLP, pages 27-34.
Y. Zhang and S. Clark. 2008. A tale of two parsers: in-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of EMNLP, pages 562-571.
1565
