Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 105?112
Manchester, August 2008
Regenerating Hypotheses for Statistical Machine Translation 
Boxing Chen, Min Zhang, Aiti Aw and Haizhou Li 
Department of Human Language Technology 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, 119613, Singapore 
{bxchen, mzhang, aaiti, hli}@i2r.a-star.edu.sg 
 Abstract 
This paper studies three techniques that 
improve the quality of N-best hypotheses 
through additional regeneration process. 
Unlike the multi-system consensus ap-
proach where multiple translation sys-
tems are used, our improvement is 
achieved through the expansion of the N-
best hypotheses from a single system. We 
explore three different methods to im-
plement the regeneration process: re-
decoding, n-gram expansion, and confu-
sion network-based regeneration. Ex-
periments on Chinese-to-English NIST 
and IWSLT tasks show that all three 
methods obtain consistent improvements. 
Moreover, the combination of the three 
strategies achieves further improvements 
and outperforms the baseline by 0.81 
BLEU-score on IWSLT?06, 0.57 on 
NIST?03, 0.61 on NIST?05 test set re-
spectively. 
1 Introduction 
State-of-the-art Statistical Machine Translation 
(SMT) systems usually adopt a two-pass search 
strategy (Och, 2003; Koehn, et al, 2003) as 
shown in Figure 1. In the first pass, a decoding 
algorithm is applied to generate an N-best list of 
translation hypotheses, while in the second pass, 
the final translation is selected by rescoring and 
re-ranking the N-best translations through addi-
tional feature functions. The fundamental as-
sumption behind using a second pass is that the 
generated N-best list may contain better transla-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
tions than the best choice found by the decoder. 
Therefore, the performance of a two-pass SMT 
system can be improved from two aspects, i.e. 
scoring models and the quality of the N-best hy-
potheses. 
Rescoring pass improves the performance of 
machine translation by enhancing the scoring 
models with more global sophisticated and dis-
criminative feature functions. The idea for apply-
ing two passes instead of one is that some global 
feature functions cannot be easily decomposed 
into local scores and computed during decoding. 
Furthermore, rescoring allows some feature func-
tions, such as word and n-gram posterior prob-
abilities, to be estimated on the N-best list (Uef-
fing, 2003; Chen et al, 2005; Zens and Ney, 
2006). 
In this two-pass method, translation perform-
ance hinges on the N-best hypotheses that are 
generated in the first pass (since rescoring occurs 
on these), so adding the translation candidates 
generated by other MT systems to these hypothe-
ses could potentially improve the performance. 
This technique is called system combination 
(Bangalore et al, 2001; Matusov et al, 2006; 
Sim et al, 2007; Rosti et al, 2007a; Rosti et al, 
2007b). 
We have instead chosen to regenerate new hy-
potheses from the original N-best list, a tech-
nique which we call regeneration. Regeneration 
is an intermediate pass between decoding and 
rescoring as depicted in Figure 2. Given the 
original N-best list (N-best1) generated by the 
decoder, this regeneration pass creates new trans-
lation hypotheses from this list to form another 
N-best list (N-best2). These two N-best lists are 
then combined and given to the rescoring pass to 
derive the best translation. 
We implement three methods to regenerate 
new hypotheses: re-decoding, n-gram expansion 
and confusion network. Re-decoding (Rosti et al, 
2007a) based regeneration re-decodes the source 
sentence using original LM as well as new trans-
105
lation and reordering models that are trained on 
the source-to-target N-best translations generated 
in the first pass. N-gram expansion (Chen et al, 
2007) regenerates more hypotheses by continu-
ously expanding the partial hypotheses through 
an n-gram language model trained on the original 
N-best translations. And confusion network gen-
erates new hypotheses based on confusion net-
work decoding (Matusov et al, 2006), where the 
confusion network is built on the original N-best 
translations. 
Confusion network and re-decoding have been 
well studied in the combination of different MT 
systems (Bangalore et al, 2001; Matusov et al, 
2006; Sim et al, 2007; Rosti et al, 2007a; Rosti 
et al, 2007b). Researchers have used confusion 
network to compute consensus translations from 
the outputs of different MT systems and improve 
the performance over each single systems. (Rosti 
et al, 2007a) also used re-decoding to do system 
combination by extracting sentence-specific 
phrase translation tables from the outputs of dif-
ferent MT systems and running a phrase-based 
decoding with this new translation table. Finally, 
N-gram expansion method (Chen et al, 2007) 
collects sub-strings occurring in the N-best list to 
produce alternative translations. 
This work demonstrates that a state-of-the-art 
MT system can be further improved by means of 
regeneration which expands its own N-best 
translations other than taking the translation can-
didates from the other MT systems. 
 
Figure 1: Structure of a typical two-pass ma-
chine translation system. N-best translations are 
generated by the decoder and the 1-best transla-
tion is returned after rescored with additional 
feature functions. 
 
Figure 2: Structure of a three-pass machine 
translation system with the new regeneration 
pass. The original N-best translations list (N-
best1) is expanded to generate a new N-best 
translations list (N-best2) before the rescoring 
pass. 
2 SMT Process 
Phrase-based statistical machine translation sys-
tems are usually modeled through a log-linear 
framework (Och and Ney, 2002). By introducing 
the hidden word alignment variable a  (Brown et 
al., 1993), the optimal translation can be 
searched for based on the following criterion: 
*
1,
arg max( ( , , ))
M
m mme a
e h?== e f a?             (1) 
where  is a string of phrases in the target lan-
guage, 
e
f
f a
 is the source language string of 
phrases,  h e  are feature functions, 
weights 
( , , )m
m? are typically optimized to maximize 
the scoring function (Och, 2003). 
Our MT baseline system is based on Moses 
decoder (Koehn et al, 2007) with word align-
ment obtained from GIZA++ (Och et al, 2003). 
The translation model (TM), lexicalized word 
reordering model (RM) are trained using the 
tools provided in the open source Moses package. 
Language model (LM) is trained with SRILM 
toolkit (Stolcke, 2002) with modified Kneser-
Ney smoothing method (Chen and Goodman, 
1998). 
3 Regeneration Methods 
Given the original N-best translations, regenera-
tion pass is to generate M new target translations 
which are not seen in the original N-best choices. 
3.1 Regeneration with Re-decoding 
One way of regeneration is by running the de-
coding again to obtain new hypotheses through a 
re-decoding process (Rosti et al, 2007a). In this 
work, the same decoder (Moses) is used to pro-
duce the new M-best translations using a new 
translation model and reordering model trained 
over the word-aligned source input and original 
N-best target hypotheses. Although the target-to-
source phrase alignments are available in the 
original N-best hypotheses, to enlarge the differ-
ence between the new M-best translations and 
the original N-best translations, we re-align the 
words using GIZA++. 
Weights of the decoder are re-optimized by 
the tool in the Moses package over the develop-
ment set. The process of such a re-decoding is 
summarized as follows: 
106
1. Run GIZA++ to align the words between the 
source input and target N-best translations; 
2. Train translation and reordering model; 
3. Optimize the weights of the decoder with 
the new models; 
4. Decode the source input by using new mod-
els and new weights to generate N+M dis-
tinct translations (?distinct? here refers to 
the target language string only, not consider-
ing the phrase segmentation, etc.); 
5. Output M-best translations which are not 
seen in the original N-best translations. 
Re-decoding on test set follows the same steps, 
but without the tuning step, step 3. 
3.2 Regeneration with N-gram Expansion 
N-gram expansion (Chen et al, 2007) combines 
the sub-strings occurred in the original N-best 
translations to generate new hypotheses. Firstly, 
all n-grams from the original N-best translations 
are collected. Then the partial hypotheses are 
continuously expanded by appending a word 
through the n-grams collected in the first step. 
We explain this method in more detail using the 
following example. 
Suppose we have four original hypotheses 
shown in Figure 3. Firstly, we collect all the 3-
grams from the original hypotheses. The first n-
grams of all original entries in the N-best list are 
set as the initial partial hypotheses. They are: it's 
5 minutes, it is 5, it?s about 5 and i walk 5. Then 
the expansion of a partial hypothesis starts by 
computing the set of n-grams matching its last n-
1 words. As shown in Figure 4, the n-gram 5 
minutes on matches the last two words of the 
partial hypothesis it?s about 5 minutes. So the 
hypothesis is expanded to it?s about 5 minutes on. 
The expansion continues until the partial hy-
pothesis ends with a special end-of-sentence 
symbol that occurs at the end of all N-best strings. 
Figure 5 shows some new hypotheses that are 
generated from the example in Figure 3. This is 
an example excerpted from our development data. 
One reference is also given in Figure 5; the first 
new generated hypothesis is equal to this refer-
ence.  But unfortunately, there is no such hy-
pothesis in the original N-best translations. 
During the new hypotheses generation, the 
translation outputs of a given source sentence are 
computed through a beam-search algorithm with 
a log-linear combination of the feature functions. 
In addition to n-gram frequency and n-gram pos-
terior probability which have been used in (Chen 
et al, 2007), we also used language model, di-
rect/inverse IBM model 1, and word penalty in 
this work. The size of the beam is set to N+M, to 
ensure more than M new hypotheses are gener-
ated. 
 
 
Original 
hypotheses 
1. it's 5 minutes on foot . 
2. it is 5 minutes on foot . 
3. it?s about 5 minutes? to walk . 
4. i walk 5 minutes . 
 
n-grams 
it's 5 minutes, 5 minutes on, ??
on foot ., about 5 minutes ?? 
5 minutes . 
 
Figure 3: Example of original hypotheses and 3-
grams collected from them. 
 
partial hyp. it?s about 5 minutes  
n-gram +                    5    minutes    on
new partial hyp. it?s about 5 minutes on
 
Figure 4: Expanding a partial hypothesis via a 
matching n-gram. 
 
 
New 
hypotheses
it?s about 5 minutes on foot . 
it's 5 minutes . 
i walk 5 minutes on foot . 
?? 
Reference it's about five minutes on foot . 
 
Figure 5: New generated hypotheses through n-
gram expansion and one reference. 
3.3 Regeneration with Confusion Network 
Confusion network based regeneration builds a 
confusion network over the original N-best hy-
potheses, and then extracts M-best hypotheses 
from it. The word order in the N-best translations 
could be very different, so we need to choose a 
hypothesis with the ?most correct? word order as 
the confusion network skeleton (alignment refer-
ence), then align and reorder other hypotheses in 
this word order. 
Some previous work compute the consensus 
translation under MT system combination, which 
differ from ours in the way of choosing the skele-
ton and aligning the words. Matusov et al (2006) 
let every hypothesis play the role of the skeleton 
once and used GIZA++ to get word alignment. 
Bangalore et al (2001), Sim et al (2007), Rosti 
et al (2007a), and Rosti et al (2007b) chose the 
hypothesis that best agrees with other hypotheses 
on average as the skeleton. Bangalore et al 
(2001) used a WER based alignment and Sim et 
al. (2007), Rosti et al (2007a), and Rosti et al 
(2007b) used minimum Translation Error Rate 
107
(TER) based alignment to build the confusion 
network. 
1. it?s 5 minutes on foot .  
Original 
hypotheses
2. it is 5 minutes on foot . 
Choosing alignment reference: Since the N-
best translations are ranked, choosing the first 
best hypothesis as the skeleton is straightforward 
in our work. 
3. it?s about 5 minutes? to walk . 
4. i walk 5 minutes . ?  it?s 5 minutes on foot . 
Alignments it 5 minutes on foot . is 
Aligning words: As a confusion network can be 
easily built from a one-to-one alignment, we de-
velop our algorithm based on the one-to-one as-
sumption and use competitive linking algorithm 
(Melamed, 2000) for our word alignment. Firstly, 
an association score is computed for every possi-
ble word pair from the skeleton and sentence to 
be aligned. Then a greedy algorithm is applied to 
select the best word-alignment. In this paper, we 
use a linear combination of multiple association 
scores, as suggested in (Kraif and Chen, 2004). 
As the two sentences to be aligned are in the 
same language, the association scores are com-
puted on the following four clues. They are cog-
nate (S
aboutit?s 5 minutes? to walk .
1), word class (S2), synonyms (S3), and 
position difference (S4). The four scores are line-
arly combined with empirically determined 
weights as shown is Equation 2. 
4
1
( , )j i k k
k
S f e S?
=
= ??                  (2) 
Reordering words: After word alignment, the 
words in all other hypotheses are reordered to 
match the word order of the skeleton. The 
aligned words are reordered according to their 
alignment indices. The unaligned words are reor-
dered in two strategies: moved with its previous 
word or next word. In this work, additional ex-
periments suggested that moving the unaligned 
word with its previous word achieve better per-
formance. In the case that the first word is un-
aligned, it will be moved with its next word. 
Each word is assigned a score based on a simple 
voting scheme. Figure 6 shows an example of 
creating a confusion network. 
Extracting M-best translations: New transla-
tions are extracted from the confusion network. 
We again use beam-search algorithm to derive 
new hypotheses.  The same feature functions 
proposed in Section 3.2 are used to score the par-
tial hypotheses. Moreover, we also use position 
based word probability (i.e. in Figure 6, the 
words in position 5, ?on? scored a probability of 
0.5, and ?? ? scored a probability of 0.25) as a 
feature function. Figure 6 shows some examples 
of new hypotheses generated through confusion 
network regeneration. 
 
 
i 5 minutes ?  walk .   ?  it?s 5 minutes on foot . 
Confusion 
network 
it is 5 minutes on foot .
it?s about 5 minutes? to walk .?i  5 minutes ?  walk . 
 1. it's about five minutes on foot . 
New 2. it about five minutes on foot . 
hypotheses 3. it's about five minutes on walk . 
4. i about 5 minutes to work . 
 
Figure 6: Example of creating a confusion net-
work from the word alignments, and new hy-
potheses generated through the confusion net-
work. The sentence in bold is the alignment ref-
erence. 
4 Rescoring model 
Since the final N+M-best hypotheses are pro-
duced either from different methods or same de-
coder with different models, local feature func-
tions of each hypothesis are not directly compa-
rable, and thus inadequate for rescoring. We 
hence exploit rich global feature functions in the 
rescoring models to compensate the loss of local 
feature functions. We apply the following 10 fea-
ture functions and optimize the weight of each 
feature function using the tool in Moses package. 
? direct and inverse IBM model 1 and 3 
? association score, i.e. hyper-geometric distri-
bution probabilities and mutual information 
? lexicalized word/block reordering rules 
(Chen et al, 2006) 
? 6-gram target LM 
? 8-gram target word-class based LM, word-
classes are clustered by GIZA++ 
? length ratio between source and target sen-
tence 
? question feature (Chen et al, 2005) 
? linear sum of n-grams relative frequencies 
within N-best translations (Chen et al, 2005) 
? n-gram posterior probabilities within the N-
best translations (Zens and Ney, 2006) 
? sentence length posterior probabilities (Zens 
and Ney, 2006) 
108
5 Experiments data Chinese English 
5.1 Tasks 
We carried out two sets of experiments on two 
different datasets. One is in spoken language 
domain while the other is on newswire corpus. 
Both experiments are on Chinese-to-English 
translation. 
Experiments on spoken language domain were 
carried out on the Basic Traveling Expression 
Corpus (BTEC) (Takezawa et al, 2002) Chi-
nese- to-English data augmented with HIT-
corpus1. BTEC is a multilingual speech corpus 
which contains sentences spoken by tourists. 40K 
sentence-pairs are used in our experiment. HIT-
corpus is a balanced corpus and has 500K sen-
tence-pairs in total. We selected 360K sentence-
pairs that are more similar to BTEC data accord-
ing to its sub-topic. Additionally, the English 
sentences of Tanaka corpus2 were also used to 
train our LM. We ran experiments on an 
IWSLT 3  challenge track which uses IWSLT-
20064 DEV clean text set as development set and 
IWSLT-2006 TEST clean text as test set. Table 1 
summarizes the statistics of the training, dev and 
test data for IWSLT task. 
Experiments on newswire domain were car-
ried out on the FBIS5 corpus. We used NIST6 
2002 MT evaluation test set as our development 
set, and the NIST 2003, 2005 test sets as our test 
sets. Table 2 summarizes the statistics of the 
training, dev and test data for NIST task. 
 
data Chinese English
Sentences 406,122 
Words 4,443K 4,591K
 
Train 
Vocabulary 69,989 61,087 
Sentences 489 489?7Dev.  
Words 5,896 45,449 
Sentences 500 500?7Test 
Words 6,296 51,227 
Sentences - 155K Additional 
target data Words - 1.7M 
 
Table 1: Statistics of training, development and 
test data for IWSLT task. 
                                                 
1 http://mitlab.hit.edu.cn/
2 http://www.csse.monash.edu.au/~jwb/tanakacorpus.html 
3 International Workshop for Spoken Language Trans-
lation 
4 http:// www.slc.atr.jp/IWSLT2006/ 
5 LDC2003E14 
6 http://www.nist.gov/speech/tests/mt/ 
Sentences 238,761  
Train Words 7.0M 8.9M 
Vocabulary 56,223 63,941 
Sentences 878 878?4 NIST 02 
(dev) Words 23,248 108,616
Sentences 919 919?4 NIST 03 
(test) Words 25,820 116,547
Sentences 1,082 1,082?4NIST 05 
(test) Words 30,544 141,915
Sentences - 2.2M Additional
target data Words - 61.5M 
 
Table 2: Statistics of training, development and 
test data for NIST task. 
 
Dev set Test set   
System #hypo BLEU NIST BLEU NIST
1-best - 29.98 7.468 29.10 7.103
RESC1 1,200 31.60 7.657 30.42 7.165
RD 1,200 32.46 7.664 30.95 7.175
NE 1,200 32.58 7.660 31.02 7.178
CN 1,200 32.33 7.671 30.82 7.200
RESC2 2,000 31.72 7.659 30.55 7.166
32.98 7.673 31.36 7.202COMB 2,000
 
Table 3: Translation performances (BLEU% and 
NIST scores) of IWSLT task: decoder (1-best), 
rescoring on original 1,200 N-best (RESC1) and 
2,000 N-best hypotheses (RESC2), re-decoding 
(RD), n-gram expansion (NE), confusion net-
work (CN) and combination of all hypotheses 
(COMB). 
5.2 Results 
We set N = 800 and M = 400 for IWSLT task, i.e. 
800 distinct translations for each source input are 
extracted from the decoder and used for regen-
eration; and 400 new hypotheses are generated 
for each regeneration system: re-decoding (RD), 
n-gram expansion (NE) and confusion network 
(CN). System COMB combines the original N-
best and the three regenerated M-best hypotheses 
lists (totally, 2,000 distinct hypotheses: 800 + 
3?400). Then each system computes the 1-best 
translation through rescoring and re-ranking its 
hypotheses list. For comparison purpose, the per-
formance of rescoring on two sets of original N-
best translations are also computed and they are 
applied based on 1,200 (RESC1) and 2,000 
(RESC2) distinct hypotheses extracted from the 
decoder.  For NIST task, we set N = 1,600, and 
M = 800, thus, RESC2 and COMB compute 1-
109
NIST?02 (dev) NIST?03 (test) NIST?05 (test)  
System 
 
#hypo BLEU NIST BLEU NIST BLEU NIST 
1-best 1 27.67 8.498 26.68 8.271 24.82 7.856 
RESC1 2,400 28.13 8.519 27.09 8.312 25.29 7.868 
RD 2,400 28.46 8.518 27.34 8.320 25.54 7.897 
NE 2,400 28.52 8.539 27.47 8.329 25.65 7.907 
CN 2,400 28.40 8.545 27.30 8.332 25.54 7.913 
RESC2 4,000 28.27 8.522 27.21 8.320 25.43 7.875 
COMB 4,000 28.92 8.602 27.78 8.401 26.04 7.994 
 
Table 4: Translation performances (BLEU% and NIST scores) of NIST task: decoder (1-best), rescoring 
on original 2,400 N-best (RESC1) and 4,000 N-best hypotheses (RESC2), re-decoding (RD), n-gram 
expansion (NE), confusion network (CN) and combination of all hypotheses (COMB). 
 
Reference No tax is needed for this item . Thank you . 
RESC2 you don't have to do not need to pay duty on this . thank you . 
1 
COMB (RD) not need to pay duty on this . thank you . 
Reference Certainly . The fitting room is over there . Please come with me . 
RESC2 the fitting room is over there . can you come with me . 
2 
COMB (NE) yes , you can . the fitting room is over there . please come with me . 
Reference OK . I will bring it to you in five minutes . 
RESC2 a good five minutes , we will give you . 
3 
COMB (CN) ok . after five minutes , i will give it to you . 
 
Table 5: Translations output by system RESC2 and COMB on IWSLT task (case-insensitive). 
best from 4,000 (1,600 + 3 800) distinct hy-
potheses. 
?
Our evaluation metrics are BLEU (Papineni et 
al., 2002) and NIST, which are to perform case-
insensitive matching of n-grams up to n = 4. The 
translation performance of IWSLT task and 
NIST task is reported in Tables 3 and 4 respec-
tively. The row ?1-best? reports the scores of the 
translations produced by the decoder. The col-
umn ?#hypo? means the size of the N-best hy-
potheses involved in rescoring. Note that on top 
of the same global feature functions as men-
tioned in Section 4, the local feature functions 
used during decoding were also involved in res-
coring RESC1 and RESC2. 
First of all, we note that both BLEU and NIST 
scores of the first decoding step were improved 
through rescoring. If rescoring was applied after 
regeneration on the N+M best lists, additional 
improvements were gained for all the develop-
ment and test sets on all three regeneration sys-
tems. Absolute improvement on BLEU score of 
0.4-0.6 on IWSLT?06 test set and 0.25-0.35 on 
NIST test sets were obtained when compared 
with system RESC1. Comparing the performance 
of three regeneration methods, we can see that 
re-decoding and confusion network based 
method achieved very similar improvement; 
while n-gram expansion based regeneration ob-
tained slightly better improvement than the other 
two methods. Combining all regenerated hy-
potheses with the original hypotheses further in-
creased the scores on both tasks. Compared with 
RESC2, system COMB obtained absolute im-
provement of 0.81 (31.36 ? 30.55) BLEU score 
on IWSLT?06 test set, 0.57 (27.28 ? 27.21) 
BLEU score on NIST?03 and 0.61 (26.04 ? 25.43) 
BLEU score on NIST?05 respectively. 
We further illustrate the effectiveness of the 
regeneration mechanism using some translation 
examples obtained from system RESC2 and 
COMB as shown in Table 5. 
6 Discussion 
To better interpret the performance improvement; 
first let us check if the regeneration pass has pro-
duced better hypotheses. We computed the oracle 
scores on all four 1,200-best lists in IWSLT task. 
The oracle chooses the translation with the low-
est word error rate (WER) with respect to the 
references in all cases. The results are reported in 
Table 6.  It is worth noticing that the first 800-
best (original N-best) hypotheses are the same in 
110
all four lists, with differences found only in the 
remaining 400 hypotheses (M-best). The consis-
tent improvement of oracle scores shows that the 
tra
theses contain better ones 
than the original ones. 
 
nslation candidates have been really improved. 
From another viewpoint, Table 7 shows the 
number of translations generated by each method 
in the final translation output (translations of 
COMB). After re-ranking N+3M entries, it is 
observed that more than 25% (e.g. for IWSLT?06 
test set, (50+74+39)/500=32.6%; NIST?03 test 
set, (77+85+68)/919=25.1%; NIST?05 test set, 
(95+110+82)/1082=26.5%) of best scored out-
puts were generated by the regeneration pass, 
showing that new generated translations are quite 
often the rescoring winner. This also proved that 
the new-generated hypo
List BLEU NIST WER PER
M  oses 46.10 8.765 36.29 30.94
RD 46.91 8.764 35.29 30.62
NE 46.95 8.811 36.05 30.72
Dev. 
CN 46.85 8.769 36.17 30.83
M  oses 45.09 8.403 37.07 32.04
RD 45.67 8.418 36.50 31.82
NE 45.82 8.481 36.44 31.70
Test 
CN 45.68 8.471 36.55 31.81
 
Table 6: Oracle scores (BLEU%, NIST, WER% 
and PER%) on IWSLT task 1,200-best lists of 
four systems: decoder (Moses), re-decoding 
(RD), n-gram expansion (NE) and confusion 
etwork (CN). 
 
n
# sentence   
Set Tot. Orig. RD NE CN
Dev 489 325 52 76 36IWSLT 
Test 500 337 50 74 39
NIST 02 878 613 92 100 73
NIST 03 919 689 77 85 68
NIST 
NIST 05 1082 795 95 110 82
 
Table 7: Number of translations generated by 
each method in the final translation output of 
system COMB: decoder (Orig.), re-decoding 
(RD), n-gram expansion (NE) and confusion 
network (CN). ?Tot.? is the size of the dev/test 
set. 
ities of words occur in the N-
best translations. 
n, and confusion 
ne
ree methods further im-
pr
the N-
best list through hypotheses regeneration. 
S. 
, pages 351?354. Madonna di 
P. 
ation. Com-
B.
Federico. 2005. The ITC-irst SMT System for 
 
Then, let us consider each single regeneration 
method to understand why regeneration can pro-
duce better hypotheses. Re-decoding may intro-
duce new and better phrase-pairs which are ex-
tracted from the N-best hypotheses to the transla-
tion model thus generate better hypotheses. N-
gram expansion can (almost) fully exploit the 
search space of target strings, which can be gen-
erated by an n-gram LM. As a result, it can pro-
duce alternative translations which contain word 
re-orderings and phrase structures not considered 
by the search algorithm of the decoder (Chen, et 
al., 2007). Confusion network based regeneration 
reinforces the word choice by considering the 
posterior probabil
7 Conclusions 
In this paper, we proposed a novel three-pass 
SMT framework against the typical two-pass 
system. This framework enhanced the quality of 
the translation candidates generated by our pro-
posed regeneration pass and improved the final 
translation performance. Three regeneration 
methods were introduced, namely, re-decoding, 
word-based n-gram expansio
twork based regeneration.  
Experiments were based on the state-of-the-art 
phrase-based decoder and carried out on the 
IWSLT and NIST Chinese-to-English task. We 
showed that all three methods improved the per-
formance with the n-gram expansion method 
achieving the greatest improvement. Moreover, 
the combination of the th
oves the performance. 
We conclude that translation performance can 
be improved by increasing the potential of trans-
lation candidates to contain better translations. 
We have presented an alternative solution to 
ameliorate the quality of translation candidates in 
a way that differs from system combination 
which takes translations from other MT systems. 
We demonstrated that the translation perform-
ance could be self-boosted by expanding 
References 
Bangalore, G. Bordel, and G. Riccardi. 2001. 
Computing consensus translation from multiple 
machine translation systems. In Proceeding of 
IEEE workshop on Automatic Speech Recognition 
and Understanding
Campiglio, Italy. 
F. Brown, V. J. Della Pietra, S. A. Della Pietra & R. 
L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation: Parameter Estim
putational Linguistics, 19(2) 263-312. 
 Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M. 
111
IWSLT-2005. In Proceeding of IWSLT-2005, 
pp.98-104, Pittsburgh, USA, October. 
B. Chen, M. Cettolo and M. Federico. 2006. Reorder-
ing Rules for Phrase-based Statistical Machine 
Translation. In Proceeding of IWSLT-2006, Kyoto, 
Japan. 
B. Chen, M. Federico and M. Cettolo. 2007. Better N-
best Translations through Generative n-gram Lan-
guage Models. In Proceeding of MT Summit XI. 
Copenhagen, Denmark.  
S. F. Chen and J. T. Goodman. 1998. An Empirical 
Study of Smoothing Techniques for Language 
Modeling. Technical Report TR-10-98, Computer 
Science Group, Harvard University. 
P. Koehn, F. J. Och and D. Marcu. 2003. Statistical 
Phrase-based Translation. In Proceedings of 
HLT/NAACL, pp 127-133, Edmonton, Canada. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. 
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin 
and E. Herbst. 2007. Moses: Open Source Toolkit 
for Statistical Machine Translation. In Proceeding 
of ACL-2007, pp. 177-180, Prague, Czech Republic. 
O. Kraif, B. Chen. 2004. Combining clues for lexical 
level aligning using the Null hypothesis approach. 
In Proceeding of COLING-2004, Geneva, pp. 
1261-1264.  
E. Matusov, N. Ueffing, and H. Ney. 2006. Comput-
ing consensus translation from multiple machine 
translation systems using enhanced hypotheses 
alignment. In Proceeding of EACL-2006, Trento, 
Italy.  
I. D. Melamed. 2000. Models of translational equiva-
lence among words. Computational Linguistics, 
26(2), pp. 221-249. 
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL-
2003. Sapporo, Japan. 
F. J. Och and H. Ney. 2003. A Systematic Compari-
son of Various Statistical Alignment Models. 
Computational Linguistics, 29(1), pp. 19-51. 
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceeding of ACL-2002. 
A. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. 
Schwartz and B. Dorr. 2007a. Combining Outputs 
from Multiple Machine Translation Systems.  In 
Proceeding of NAACL-HLT-2007, pp. 228-235. 
Rochester, NY. 
A. Rosti, S. Matsoukas and R. Schwartz. 2007b. Im-
proved Word-Level System Combination for Ma-
chine Translation. In Proceeding of ACL-2007, 
Prague. 
K. C. Sim, W. J. Byrne, M. J.F. Gales, H. Sahbi, and 
P. C. Woodland. 2007. Consensus network decod-
ing for statistical machine translation system com-
bination. In Proceeding of  ICASSP-2007. 
A. Stolcke. 2002. SRILM - an extensible language 
modelling toolkit. In Proceeding of ICSLP-2002. 
901-904. 
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, 
and S. Yamamoto. 2002. Toward a broad-coverage 
bilingual corpus for speech translation of travel 
conversations in the real world. In Proceeding of 
LREC-2002, Las Palmas de Gran Canaria, Spain. 
R. Zens and H. Ney. 2006. N-gram Posterior Prob-
abilities for Statistical Machine Translation. In 
Proceeding of HLT-NAACL Workshop on SMT, pp. 
72-77, NY. 
 
112
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1009?1016
Manchester, August 2008
Linguistically Annotated BTG for Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{dyxiong, mzhang, aaiti}@i2r.a-star.edu.sg
Abstract
Bracketing Transduction Grammar (BTG)
is a natural choice for effective integration
of desired linguistic knowledge into sta-
tistical machine translation (SMT). In this
paper, we propose a Linguistically Anno-
tated BTG (LABTG) for SMT. It conveys
linguistic knowledge of source-side syn-
tax structures to BTG hierarchical struc-
tures through linguistic annotation. From
the linguistically annotated data, we learn
annotated BTG rules and train linguisti-
cally motivated phrase translation model
and reordering model. We also present an
annotation algorithm that captures syntac-
tic information for BTG nodes. The ex-
periments show that the LABTG approach
significantly outperforms a baseline BTG-
based system and a state-of-the-art phrase-
based system on the NISTMT-05 Chinese-
to-English translation task. Moreover, we
empirically demonstrate that the proposed
method achieves better translation selec-
tion and phrase reordering.
1 Introduction
Formal grammar used in statistical machine trans-
lation (SMT), such as Bracketing Transduction
Grammar (BTG) proposed by (Wu, 1997) and the
synchronous CFG presented by (Chiang, 2005),
provides a natural platform for integrating lin-
guistic knowledge into SMT because hierarchical
structures produced by the formal grammar resem-
ble linguistic structures.1 Chiang (2005) attempts
to integrate linguistic information into his formally
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1We inherit the definitions of formal and linguistic from
(Chiang, 2005) which makes a distinction between formally
syntax-based SMT and linguistically syntax-based SMT.
syntax-based system by adding a constituent fea-
ture. Unfortunately, the linguistic feature does not
show significant improvement on the test set. In
this paper, we further this effort by integrating lin-
guistic knowledge into BTG.
We want to augment BTG?s formal structures
with linguistic structures since they are both hier-
archical. In particular, our goal is to learn a more
linguistically meaningful BTG from real-world bi-
texts by projecting linguistic structures onto BTG
formal structures. In doing so, we hope to (1)
maintain the strength of phrase-based approach
since phrases are still used on BTG leaf nodes; (2)
obtain a tight integration of linguistic knowledge in
the translation model; (3) and finally avoid induc-
ing a complicated linguistic synchronous grammar
with expensive computation. The challenge, of
course, is that BTG hierarchical structures are not
always aligned with the linguistic structures in the
syntactic parse trees of source or target language.
Along this line, we propose a novel approach:
Linguistically Annotated BTG (LABTG) for SMT.
The LABTG annotates BTG rules with linguistic
elements that are learned from syntactic parse trees
on the source side through an annotation algo-
rithm, which is capable of labelling both syntactic
and non-syntactic phrases. The linguistic elements
extracted from parse trees capture both internal
lexical content and external context of phrases.
With these linguistic annotations, we expect the
LABTG to address two traditional issues of stan-
dard phrase-based SMT (Koehn et al, 2003) in a
more effective manner. They are (1) phrase trans-
lation: translating phrases according to their con-
texts; (2) phrase reordering: incorporating richer
linguistic features for better reordering.
The proposed LABTG displays two unique
characteristics when compared with BTG-based
SMT (Wu, 1996; Xiong et al, 2006). The first
is that two linguistically-informed sub-models are
introduced for better phrase translation and re-
ordering: annotated phrase translation model and
1009
annotated reordering model. The second is that
our proposed annotation algorithm and scheme are
capable of conveying linguistic knowledge from
source-side syntax structures to BTG structures.
We describe the LABTG model and the annota-
tion algorithm in Section 4. To better explain the
LABTG model, we establish a unified framework
of BTG-based SMT in Section 3. We conduct
a series of experiments to study the effect of the
LABTG in Section 5.
2 Related Work
There have been various efforts to integrate lin-
guistic knowledge into SMT systems, either from
the target side (Marcu et al, 2006; Hassan et al,
2007; Zollmann and Venugopal, 2006), the source
side (Quirk et al, 2005; Liu et al, 2006; Huang
et al, 2006) or both sides (Eisner, 2003; Ding et
al., 2005; Koehn and Hoang, 2007), just to name a
few. LABTG can be considered as, but not limited
to, a new attempt that enriches translation model
with source-side linguistic annotations.
(Huang and Knight, 2006) and (Hassan et al,
2007) introduce relabeling and supertagging on the
target side, respectively. The former re-annotates
syntactified phrases to learn grammatical distinc-
tions while the latter supertags standard plain
phrases, both applied on the target side. The differ-
ence between their work and LABTG is significant
because we annotate standard plain phrases using
linguistic elements on the source side. Compared
with the target side annotation which improves flu-
ency and grammaticality of translation output, lin-
guistic annotation on the source side helps to im-
prove translation adequacy.
Recently, some researchers have extended and
created several variations of BTG/ITG. Zhang et
al. (2005) propose lexicalized ITG for better word
alignment. Xiong et al (2006) demonstrate that
their MEBTG, a BTG variation with MaxEnt-
based reordering model, can improve phrase re-
ordering significantly. Similarly, Setiawan et al
(2007) use an enhanced BTG variation with func-
tion words for reordering. LABTG differs from
these BTG variations in that the latter does not use
any external linguistic knowledge.
Zhang et al (2007) describe a phrase reorder-
ing model based on BTG-style rules which inte-
grates source-side syntactic knowledge. Our an-
notated reordering model of LABTG differs from
their work in two key aspects. Firstly, we al-
low any phrase reorderings while they only reorder
syntactic phrases. In their model, only syntactic
phrases can use linguistic knowledge from parse
trees for reordering while non-syntactic phrases
are combined monotonously with a constant re-
ordering score since no syntactic knowledge can
be used at all. Our proposed LABTG successfully
overcomes this limitation by supporting linguis-
tic annotation on both syntactic and non-syntactic
phrases. Moreover, we show that excluding non-
syntactic phrase from reordering does hurt the
performance. Secondly, we use richer linguistic
knowledge in reordering, including head words
and syntactic labels of context nodes, when com-
pared with their model. Our experiments show that
these additional information can improve reorder-
ing.
3 BTG Based SMT
We establish a unified framework for BTG-based
SMT in this section. There are two kinds of rules
in BTG, lexical rules (denoted as rl) and merging
rules (denoted as rm):
r
l
: A ? x/y
and
r
m
: A ? [A
l
, A
r
]|?A
l
, A
r
?
Lexical rules translate source phrase x into target
phrase y and generate a leaf node A in BTG tree.
Merging rules combine left and right neighboring
phrases A
l
and A
r
into a larger phrase A in an or-
der o ? {straight, inverted}.
We define a BTG derivation D as a sequence
of independent applications of lexical and merging
rules (D = ?rl
1..n
l
, r
m
1..n
m
?). Given a source sen-
tence, the decoding task of BTG-based SMT is to
find a best derivation, which yields the best trans-
lation.
Similar to (Xiong et al, 2006), we can as-
sign a probability to each rule using a log-linear
model with different features and corresponding ?
weights, then multiply them to obtain P (D). For
convenience of notation and keeping in line with
the common understanding of standard phrase-
based model, here we re-organize these features
into translation model (P
T
), reordering model
(P
R
) and target language model (P
L
) as follows
P (D) = P
T
(r
l
1..n
l
) ? P
R
(r
m
1..n
m
)
?
R
?P
L
(e)
?
L
? exp(|e|)
?
w (1)
where exp(|e|) is the word penalty.
1010
The translation model is defined as:
P
T
(r
l
1..n
l
) =
n
l
?
i=1
P (r
l
i
)
P (r
l
) = p(x|y)
?
1
? p(y|x)
?
2
? p
lex
(x|y)
?
3
?p
lex
(y|x)
?
4
? exp(1)
?
5 (2)
where p(?) represent the phrase translation proba-
bilities in both directions, p
lex
(?) denote the lexi-
cal translation probabilities in both directions, and
exp(1) is the phrase penalty.
Similarly, the reordering model is defined on the
merging rules as follows
P
R
(r
m
1..n
m
) =
n
m
?
i=1
P (r
m
i
) (3)
In the original BTGmodel (Wu, 1996), P (rm) was
actually a prior probability which can be set based
on the order preference of the language pairs. In
MEBTG (Xiong et al, 2006), however, the prob-
ability is calculated more sophisticatedly using a
MaxEnt-based classification model with boundary
words as its features.
4 Linguistically Annotated BTG Based
SMT
We extend the original BTG into the linguistically
annotated BTG by adding linguistic annotations
from source-side parse trees to both BTG lexical
rules and merging rules. Before we elaborate how
the LABTG extends the baseline, we introduce an-
notated BTG rules.
In the LABTG, both lexical rules and merging
rules are annotated with linguistic elements as fol-
lows
ar
l
: A
a
? x#a/y
and
ar
m
: A
a
? [A
a
l
l
, A
a
r
r
]|?A
a
l
l
, A
a
r
r
?
The annotation a comprises three linguistic ele-
ments from source-side syntactic parse tree: (1)
head word hw, (2) the part-of-speech (POS) tag
ht of head word and (3) syntactic label sl. In an-
notated lexical rules, the three elements are com-
bined together and then attached to x as an anno-
tation unit. In annotated merging rules, each node
involved in merging is annotated with these three
elements individually.
There are various ways to learn the annotated
rules from training data. The straight-forward way
is to first generate the best BTG tree for each sen-
tence pair using the way of (Wu, 1997), then an-
notate each BTG node with linguistic elements
by projecting source-side syntax tree to BTG tree,
and finally extract rules from these annotated BTG
trees. This way restricts learning space to only the
best BTG trees2, and leads to the loss of many use-
ful annotated rules.
Therefore, we use an alternative way to extract
the annotated rules as illustrated below. Firstly, we
run GIZA++ (Och and Ney, 2000) on the train-
ing corpus in both directions and then apply the
ogrow-diag-finalp refinement rule (Koehn et al,
2003) to obtain many-to-many word alignments.
Secondly, we extract bilingual phrases from the
word-aligned corpus, then annotate their source
sides with linguistic elements to obtain the an-
notated lexical rules.3 Finally, we learn reorder-
ing examples (Xiong et al, 2006), annotate their
two neighboring sub-phrases and whole phrases,
and then generalize them in the annotated merging
rules. Although this alternative way may also miss
reorderings due to word alignment errors, it is still
more flexible and robust than the straight-forward
one, and can learn more annotated BTG rules with-
out constructing BTG trees explicitly.
4.1 LABTG Annotation Algorithm
During the process of rule learning and decod-
ing, we need to annotate bilingual phrases or BTG
nodes generated by the decoder given a source
sentence together with its parse tree. Since both
phrases and BTG nodes can be projected to a span
on the source sentence, we run our annotation al-
gorithm on source-side spans and then assign an-
notation results to the corresponding phrases or
nodes. If the span is exactly covered by a single
subtree in the source-side parse tree, it is called
syntactic span, otherwise non-syntactic span.
One of the challenges in this annotation algorithm
is that BTG nodes (or phrases) are not always cov-
ering syntactic span, in other words, are not always
aligned to constituent nodes in the source-side tree.
To solve this problem, we use heuristic rules to
generate pseudo head word and composite label
which consists of syntactic labels of three relevant
constituents for the non-syntactic span.
The annotation algorithm is shown in Fig. 1.
For a syntactic span, the annotation is trivial. An-
notation elements directly come from the subtree
that exactly covers the span. For a non-syntactic
2Producing BTG forest for each sentence pair is very time-
consuming.
3This makes the number of extracted annotated lexical
rules proportional to that of bilingual phrases.
1011
1: Annotator (span s = ?i, j?, source-side parse tree t)
2: if s is a syntactic span then
3: Find the subtree c in t which exactly covers s
4: s.a := {c.hw, c.ht, c.sl}
5: else
6: Find the smallest subtree c? subsuming s in t
7: if c?.hw ? s then
8: s.a.hw := c?.hw and s.a.ht := c?.ht
9: else
10: Find the word w ? s which is nearest to c?.hw
11: s.a.hw := w and s.a.ht := w.t /*w.t is the POS
tag of w*/
12: end if
13: Find the left context node ln of s in c?
14: Find the right context node rn of s in c?
15: s.a.sl := ln.sl-c?.sl-rn.sl
16: end if
Figure 1: The LABTG Annotation Algorithm.
span, the process is much complicated. Firstly,
we need to locate the smallest subtree c? subsum-
ing the span (line 6). Secondly, we try to identify
the head word/tag of the span (line 7-12) by us-
ing c??s head word hw directly if it is within the
span. Otherwise, the word within the span which
is nearest to hw will be assigned as the head word
of the span. Finally, we determine the composite
label of the span (line 13-15), which is formulated
as L-C-R. L/R refers to the syntactic label of the
left/right context node of s which is a sub-node of
c
?
. There are different ways to define the context
node of a span in the source-side parse tree. It can
be the closest neighboring node or the boundary
node which is the highest leftmost/rightmost sub-
node of c? not overlapping the span. If there is no
such context node (the span s is exactly aligned to
the left/right boundary of c?), L/R will be set to
NULL. C is the label of c?. L, R and C together
define the external syntactic context of s.
Fig. 2 shows a syntactic parse tree for a Chinese
sentence, with head word annotated for each inter-
nal node.4 Some sample annotations are given in
Table 1. We also show different composite labels
for non-syntactic spans with different definitions
of their context nodes. sl
1
is obtained when the
boundary node is defined as the context node while
sl
2
is obtained when the closest neighboring node
is defined as the context node.
4.2 LABTG Model
To better model annotated rules, the LABTG con-
tributes two significant modifications to formula
(1). First is the annotated phrase translation model
4In this paper, we use phrase labels from the Penn Chinese
Treebank (Xue et al, 2005).
IP(??)
?
?
?
?
?
H
H
H
H
H
NP(??)
?
?
H
H
NP(??)
NR
??1
Tibet
NP(??)
?H
NN
??2
financial
NN
??3
work
VP(??)
?
?
?
?
?
H
H
H
H
H
VV
??4
gain
AS
?5
NP(??)
?
?
H
H
ADJP(??)
JJ
??6
remarkable
NP(??)
NN
?7?
achievement
Figure 2: A syntactic parse tree with head word
annotated for each internal node. The superscripts
of leaf nodes denote their surface positions from
left to right.
span hw ht sl
1
(boundary node) sl
2
(neighboring node)
?1, 2? ?? NN NULL-NP-NN NULL-NP-NN
?2, 3? ?? NN NP NP
?2, 4? ?? VV NP-IP-NP NP-IP-AS
?3, 4? ?? VV NP-IP-NP NN-IP-AS
Table 1: Annotation samples according to the tree
shown in Fig. 2. hw/ht represents head word/tag,
respectively. sl means the syntactic label.
with source side linguistically enhanced to replace
the standard phrase translation model, and second
is the additional MaxEnt-based reordering model
that uses linguistic annotations as features. The
LABTG model is formulated as follows
P (D) = P
T
a
(ar
l
1..n
l
) ? P
R
b
(r
m
1..n
m
)
?
R
b
?P
R
a
(ar
m
1..n
m
)
?
R
a
? P
L
(e)
?
L
? exp(|e|)
?
w (4)
Here P
T
a
is the annotated phrase translation
model, P
R
b
is the reordering model from MEBTG
using boundary words as features and P
R
a
is the
annotated reordering model using linguistic anno-
tations of nodes as features.
Annotated Phrase Translation Model The
annotated phrase translation model P
T
a
is sim-
ilar to formula (2) except that phrase transla-
tion probabilities on both directions are p(x#a|y)
and p(y|x#a) respectively, instead of p(x|y) and
p(y|x). By introducing annotations into the trans-
lation model, we integrate linguistic knowledge
into the statistical selection of target equivalents.
Annotated Reordering Model The annotated
reordering model P
R
a
is a MaxEnt-based classi-
fication model which uses linguistic elements of
each annotated node as its features. The model can
be formulated as
P
R
a
(ar
m
) = p
?
(o|A
a
, A
a
l
l
, A
a
r
r
)
1012
=exp(
?
i
?
i
h
i
(o,A
a
, A
a
l
l
, A
a
r
r
))
?
o
exp(
?
i
?
i
h
i
(o,A
a
, A
a
l
l
, A
a
r
r
))
where the functions h
i
? {0, 1} are reordering fea-
tures and ?
i
are weights of these features.
Each merging rule involves 3 nodes
(Aa, Aal
l
, A
a
r
r
) and each node has 3 linguistic
elements (hw, ht, sl). Therefore, the model has 9
features in total. Taking the left node Aal
l
as an
example, the model could use its head word w as
feature as follows
h
i
(o,A
a
, A
a
l
l
, A
a
r
r
) =
{
1, A
a
l
l
.hw = w, o = straight
0, otherwise
4.3 Training
To train the annotated translation model, firstly we
extract all annotated lexical rules from source-side
parsed, word-aligned training data. Then we es-
timate the annotated phrase translation probabili-
ties p(x#a|y) and p(y|x#a) using relative counts
from all collected annotated lexical rules. For ex-
ample, p(y|x#a) can be calculated as follows
p(y|x#a) =
count(x#a, y)
?
y
count(x#a, y)
One might think that linguistic annotations would
cause serious data sparseness problem and the
probabilities should be smoothed. However, ac-
cording to our statistics (described in the next sec-
tion), the differences in annotations for the same
source phrase x are not so diverse. So we take
a direct backoff strategy to map unseen annotated
lexical rules to their un-annotated versions on the
fly during decoding, which is detailed in the next
subsection.
To train the annotated reordering model, we
generate all annotated reordering examples, then
obtain features using linguistic elements of these
examples, and finally estimate feature weights
based on the maximum entropy principle.
4.4 Decoding
A CKY-style decoder with beam search is devel-
oped, similar to (Xiong et al, 2006). Each in-
put source sentence is firstly parsed to obtain its
syntactic tree. Then the CKY-style decoder tries
to generate the best annotated BTG tree using the
trained annotated lexical and merging rules. We
store all annotated lexical rules and their proba-
bilities in a standard phrase table ?, where source
phrases are augmented with annotations. During
the application of annotated lexical rules, we la-
bel each source phrase x with linguistic annota-
tion a through the annotation algorithm given the
source-side parse tree, and retrieve x#a from ?.
In the case of unseen combination x#a, we map
it to x and lookup x in the phrase table so that we
can use the un-annotated lexical rule A ? x/y.
We set p(y|x) = max
a
?
p(y|x#a
?
) and p(x|y) =
max
a
?
p(x#a
?
|y) where (x, a?, y) ? ?. When two
neighboring nodes are merged in a specific order,
the two reordering models, P
R
b
and P
R
a
, will eval-
uate this merging independently with individual
scores. The former uses boundary words as fea-
tures while the latter uses the linguistic elements
as features, annotated on the BTG nodes through
the annotation algorithm according to the source-
side parse tree.
5 Experiments and Analysis
In this section we conducted a number of ex-
periments to demonstrate the competitiveness of
the proposed LABTG based SMT when compared
with two baseline systems: Moses (Koehn et al,
2007), a state-of-the-art phrase-based system and
MEBTG (Xiong et al, 2006), a BTG based sys-
tem. We also investigated the impact of differ-
ent annotation schemes on the LABTG model and
studied the effect of annotated phrase translation
model and annotated reordering model on transla-
tion selection and phrase reordering respectively.
All experiments were carried out on the Chinese-
to-English translation task of the NISTMT-05 with
case-sensitive BLEU scores reported.
The systems were trained on the FBIS cor-
pus. We removed 15,250 sentences, for which
the Chinese parser (Xiong et al, 2005) failed to
produce syntactic parse trees. The parser was
trained on the Penn Chinese Treebank with a F1
score of 79.4%. From the remaining FBIS corpus
(224, 165 sentence pairs), we obtained 4.55M stan-
dard bilingual phrases (including 2.75M source
phrases) for the baseline systems and 4.65M an-
notated lexical rules (including 3.13M annotated
source phrases augmented with linguistic anno-
tations) for the LABTG system using the algo-
rithm mentioned above. These statistics reveal
that there are 1.14 (3.13M/2.75M) annotations per
source phrase, which means our annotation algo-
rithm does not increase the number of extracted
rules exponentially.
We extracted 2.8M reordering examples, from
1013
System BLEU
Moses 0.2386
MEBTG 0.2498
LABTG 0.2667
Table 2: LABTG vs. Moses and MEBTG.
which we generated 114.8K reordering features for
the reordering model P
R
b
(shared by both MEBTG
and LABTG systems) using the right boundary
words of phrases and 85K features for the anno-
tated reordering model P
R
a
(only included in the
LABTG system) using linguistic annotations. We
ran the MaxEnt toolkit (Zhang, 2004) to tune re-
ordering feature weights with iteration number be-
ing set to 100 and Gaussian prior to 1 to avoid over-
fitting.
We built our four-gram language model using
Xinhua section of the English Gigaword corpus
(181.1M words) with the SRILM toolkit (Stolcke,
2002). For the efficiency of minimum-error-rate
training (Och, 2003), we built our development set
(580 sentences) using sentences not exceeding 50
characters from the NIST MT-02 evaluation test
data.
5.1 LABTG vs. phrase-based SMT and
BTG-based SMT
We compared the LABTG system with two base-
line systems. The results are given in Table 2.
The LABTG outperforms Moses and MEBTG by
2.81 and 1.69 absolute BLEU points, respectively.
These significant improvements indicate that BTG
formal structures can be successfully extended
with linguistic knowledge extracted from syntac-
tic structures without losing the strength of phrase-
based method.
5.2 The Effect of Different Annotation
Schemes
A great amount of linguistic knowledge is con-
veyed through the syntactic label sl. To obtain
this label, we tag syntactic BTG node with single
label C from its corresponding constituent in the
source-side parse tree while annotate non-syntactic
BTG node with composite label formulated as L-
C-R. We conducted experiments to study the effect
of different annotation schemes on the LABTG
model by comparing three different annotation
schemes for non-syntactic BTG node: (1) using
single label C from its corresponding smallest sub-
tree c? (C), (2) constructing composite label using
Annotation scheme BLEU
C 0.2626
N-C-N 0.2591
B-C-B 0.2667
Annotating syntactic nodes with com-
posite label
0.2464
Table 3: Comparison of different annotation
schemes.
neighboring node as context node (N-C-N), and (3)
constructing composite label using boundary node
as context node (B-C-B). The results are shown in
Table 3.
On the one hand, linguistic annotation provides
additional information for LABTG, transferring
knowledge from source-side linguistic structures
to BTG formal structures. On the other hand, how-
ever, it is also a constraint on LABTG, guiding the
annotated translation model and reordering model
to the selection of target alernatives and reorder-
ing patterns, respectively. A tight constraint al-
ways means that annotations are too specific, al-
though they incorporate rich knowledge. Too spe-
cific annotations are more sensitive to parse errors,
and easier to make the model lose correct transla-
tions or use wrong reordering patterns. That is the
reason why the annotation scheme ?N-C-N? and
?Annotating syntactic nodes with composite label?
5 both hurt the performance. Conversely, a loose
constraint means that annotations are too generic
and have less knowledge incorporated. The an-
notation scheme ?C? is such a scheme with loose
constraint and less knowledge.
Therefore, an ideal annotation scheme should
not be too specific or too generic. The annota-
tion scheme ?B-C-B? achieves a reasonable bal-
ance between knowledge incorporation and con-
straint, which obtains the best performance. There-
fore we choose boundary node as context node for
label annotation of non-syntactic BTG nodes in ex-
periments described later.
5.3 The Effect of Annotated Translation
Model
To investigate the effect of the annotated transla-
tion model on translation selection, we compared
the standard phrase translation model P
T
used
in MEBTG with the annotated phrase translation
5In this annotation scheme, we produce composite label
L-C-R for both syntactic and non-syntactic BTG nodes. For
syntactic node, sibling node is used as context node while for
non-syntactic node, boundary node is used as context node.
1014
Translation model BLEU
P
T
0.2498
P
T
a
0.2581
P
T
a
(-NULL) 0.2548
Table 4: The effect of annotated translation model.
model P
T
a
. The experiment results are shown in
Table 4. The significant improvement in the BLEU
score indicates that the annotated translation model
helps to select better translation options.
Our study on translation output shows that anno-
tating phrases with source-side linguistic elements
can provide at least two kinds of information for
translation model to improve the adequacy: cate-
gory and context. The category knowledge of a
phrase can be used to select its appropriate trans-
lation related to its category. For example, Chi-
nese phrase ??? can be translated into ?value? if
it is a verb or ?at/on? if it is a proposition. How-
ever, the baseline BTG-based system always se-
lects the proposition translation even if it is a verb
because the language model probability for propo-
sition translation is higher than that of verb trans-
lation. This wrong translation of content words is
similar to the incorrect omission reported in (Och
et al, 2003), which both hurt translation adequacy.
The annotated translation model can avoid wrong
translation by filtering out phrase candidates with
unmatched categories.
The context information (provided by context
node) is also quite useful for translation selection.
Even the ?NULL? context, which we used in label
annotation to indicate that a phrase is located at the
boundary of a constituent, provides some informa-
tion, such as, transitive or intransitive attribute of
a verb phrase. The last row of Tabel 4 shows that
if we remove ?NULL? in label annotation, the per-
formance is degraded. (Huang and Knight, 2006)
also reported similar result by using sisterhood an-
notation on the target side.
5.4 The Effect of Annotated Reordering
Model
To investigate the effect of the annotated reorder-
ing model, we integrate P
R
a
with various settings
in MEBTG while keeping its original phrase trans-
lation model P
T
and reordering model P
R
b
un-
changed. We augment P
R
a
?s feature pool incre-
mentally: firstly using only single labels 6(SL)
6For non-syntactic node, we only use the single label C,
without constructing composite label L-C-R.
Reordering Configuration BLEU
P
R
b
0.2498
P
R
b
+ P
R
a
(SL) 0.2588
P
R
b
+ P
R
a
(+BNL) 0.2627
P
R
b
+ P
R
a
(+BNL+HWT) 0.2652
P
R
b
+ P
R
a
(SL+BNL+HWT): only al-
lowed syntactic phrase reordering
0.2512
Table 5: The effect of annotated reordering model.
as features (132 features in total), then construct-
ing composite labels for non-syntactic phrases
(+BNL) (6.7K features), and finally introducing
head words into the feature pool (+BNL+HWT)
(85K features). This series of experiments demon-
strate the impact and degree of contribution made
by each feature for reordering. We also conducted
experiments to investigate the effect of restrict-
ing reordering to syntactic phrases using the best
reordering feature set (SL+BNL+HWT) for P
R
a
.
The experimental results are presented in Table 2,
from which we have the following observations:
(1) Source-side syntactic labels (SL) capture re-
ordering patterns between source structures and
their target counterparts. Even when the base-
line feature set SL with only 132 features is used
for P
R
a
, the BLEU score improves from 0.2498
to 0.2588. This is because most of the frequent
reordering patterns between Chinese and English
have been captured using syntactic labels. For ex-
ample, the pre-verbal modifier PP in Chinese is
translated into post-verbal counterpart in English.
This reordering can be described by a rule with an
inverted order: V P ? ?PP, V P ?, and captured
by our syntactic reordering features.
(2) Context information, provided by labels of
context nodes (BNL) and head word/tag pairs
(HWT), also improves phrase reordering. Produc-
ing composite labels for non-syntactic BTG nodes
(+BNL) and integrating head word/tag pairs into
P
R
a
as reordering features (+BNL+HWT) are both
effective, indicating that context information com-
plements syntactic label for capturing reordering
patterns.
(3) Restricting phrase reordering to syntactic
phrases is harmful. The BLEU score plummets
from 0.2652 to 0.2512.
6 Conclusions
In this paper, we have presented a Linguistically
Annotated BTG based approach to effectively in-
tegrate linguistic knowledge into SMT by merging
1015
source-side linguistic structures with BTG hierar-
chical structures. The LABTG brings BTG-based
SMT towards linguistically syntax-based SMT and
narrows the linguistic gap between them. Our
experimental results show that the LABTG sig-
nificantly outperforms the state-of-the-art phrase-
based SMT and the baseline BTG-based SMT. The
proposed method also offers better translation se-
lection and phrase reordering by introducing the
annotated phrase translation model and the anno-
tated reordering model with linguistic annotations.
We conclude that (1) source-side syntactic in-
formation can improve translation adequacy; (2)
linguistic annotations of BTG nodes well capture
reordering patterns between source structures and
their target counterparts; (3) integration of linguis-
tic knowledge into SMT should be carefully con-
ducted so that the incorporated knowledge could
not have negative constraints on the model7.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of ACL
2005.
Yuan Ding and Martha Palmer. 2005. Machine Transla-
tion Using Probabilistic Synchronous Dependency Inser-
tion Grammars. In Proceedings of ACL 2005.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of ACL 2003.
Hany Hassan, Khalil Sima?an and Andy Way. 2007. Su-
pertagged Phrase-based Statistical Machine Translation.
In Proceedings of ACL 2007.
Bryant Huang, Kevi Knight. 2006. Relabeling Syntax Trees
to Improve Syntax-Based Machine Translation Quality. In
Proceedings of NAACL-HLT 2006.
Liang Huang, Kevi Knight and Aravind Joshi. 2006. Statisti-
cal Syntax-directed Translation with Extended Domain of
Locality. In Proceedings of AMTA 2006.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings of
HLT/NAACL.
Philipp Koehn, Hieu Hoang. 2007. Factored Translation
Models. In Proceedings of EMNLP 2007.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris
Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst.
2007. Moses: Open Source Toolkit for Statistical Machine
Translation. ACL 2007, demonstration session, Prague,
Czech Republic, June 2007.
7For example, the annotation scheme ?N-C-N? incorpo-
rates rich syntactic knowledge, but also tightens the constraint
on the model, which therefore loses robustness.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Translation.
In Proceedings of ACL-COLING 2006.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation
with Syntactified Target Language Phraases. In Proceed-
ings of EMNLP.
Franz Josef Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of ACL 2000.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of ACL
2003.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin
Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,
Dragomir Radev. 2003. Final Report of Johns Hopkins
2003 SummerWorkshop on Syntax for Statistical Machine
Translation.
Chris Quirk, Arul Menezes and Colin Cherry. 2005. Depen-
dency Treelet Translation: Syntactically Informed Phrasal
SMT. In Proceedings of ACL 2005.
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. Or-
dering Phrases with Function Words. In Proceedings of
ACL 2007.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of International Con-
ference on Spoken Language Processing, volume 2, pages
901-904.
Dekai Wu. 1996. A Polynomial-Time Algorithm for Statisti-
cal Machine Translation. In Proceedings of ACL 1996.
Dekai Wu. 1997. Stochastic Inversion Transduction Gram-
mars and Bilingual Parsing of Parallel Corpora. Computa-
tional Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings of IJCNLP, Jeju Is-
land, Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum En-
tropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of ACL-COLING 2006.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer.
2005. The Penn Chinese TreeBank: Phrase Structure An-
notation of a Large Corpus. Natural Language Engineer-
ing, 11(2):207-238.
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 2007.
Phrase Reordering Model Integrating Syntactic Knowl-
edge for SMT. In Proceedings of EMNLP-CoNLL 2007.
Hao Zhang and Daniel Gildea. 2005. Stochastic Lexicalized
Inversion Transduction Grammar for Alignment. In Pro-
ceedings of ACL 2005.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
Augmented Machine Translation via Chart Parsing. In
NAACL 2006 - Workshop on statistical machine transla-
tion, New York. June 4-9.
1016
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1097?1104
Manchester, August 2008
Grammar Comparison Study for Translational Equivalence 
Modeling and Statistical Machine Translation 
Min Zhang1,  Hongfei Jiang2,  Haizhou Li1,  Aiti Aw1  and  Sheng Li2 
1Institute for Infocomm Research, Singapore 
2Harbin Institute of Technology, China 
{mzhang, hli, aaiti}@i2r.a-star.edu.sg 
{hfjiang, lisheng}@mtlab.hit.edu.cn 
 
Abstract 
This paper presents a general platform, 
namely synchronous tree sequence sub-
stitution grammar (STSSG), for the 
grammar comparison study in Transla-
tional Equivalence Modeling (TEM) and 
Statistical Machine Translation (SMT). 
Under the STSSG platform, we compare 
the expressive abilities of various gram-
mars through synchronous parsing and a 
real translation platform on a variety of 
Chinese-English bilingual corpora. Ex-
perimental results show that the STSSG 
is able to better explain the data in paral-
lel corpora than other grammars. Our 
study further finds that the complexity of 
structure divergence is much higher than 
suggested in literature, which imposes a 
big challenge to syntactic transformation-
based SMT. 
1 Introduction 
Translational equivalence is a mathematical rela-
tion that holds between linguistic expressions 
with the same meaning (Wellington et al, 2006).  
The common explicit representations of this rela-
tion are word alignments, phrase alignments and 
structure alignments between bilingual sentences. 
Translational Equivalence Modeling (TEM) is a 
process to describe and build these alignments 
using mathematical models. Thus, the study of 
TEM is highly relevant to Statistical Machine 
Translation (SMT). 
Grammar is the most important infrastructure 
for TEM and SMT since translation models? ex-
pressive and generative abilities are mainly de-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
termined by the grammar. Many grammars, such 
as finite-state grammars (FSG), bracket/inversion 
transduction grammars (BTG/ITG) (Wu, 1997), 
context-free grammar (CFG), tree substitution 
grammar (TSG) (Comon et al, 2007) and their 
synchronous versions, have been explored in 
SMT. Based on these grammars, a great number 
of SMT models have been recently proposed, 
including string-to-string model (Synchronous 
FSG) (Brown et al, 1993; Koehn et al, 2003), 
tree-to-string model (TSG-string) (Huang et al, 
2006; Liu et al, 2006; Liu et al, 2007), string-to-
tree model (string-CFG/TSG) (Yamada and 
Knight, 2001; Galley et al, 2006; Marcu et al, 
2006), tree-to-tree model (Synchronous 
CFG/TSG, Data-Oriented Translation) (Chiang, 
2005; Cowan et al, 2006; Eisner, 2003; Ding and 
Palmer, 2005; Zhang et al, 2007; Bod, 2007; 
Quirk wt al., 2005; Poutsma, 2000; Hearne and 
Way, 2003) and so on. 
Although many achievements have been ob-
tained by these advances, it is still unclear which 
of these important pursuits is able to best explain 
human translation data, as each has its advan-
tages and disadvantages. Therefore, it has great 
meaning in both theory and practice to do com-
parison studies among these grammars and SMT 
models to see which of them are capable of better 
describing parallel translation data. This is a fun-
damental issue worth exploring in multilingual 
information processing. However, little effort in 
previous work has been put in this point. To ad-
dress this issue, in this paper we define a general 
platform, namely synchronous tree sequence 
substitution grammar (STSSG), for the compari-
son studies. The STSSG can be seen as a gener-
alization of Synchronous TSG (STSG) by replac-
ing elementary tree (a single subtree used in 
STSG) with contiguous tree sequence as the ba-
sic translation unit. As a result, most of previous 
grammars used in SMT can be interpreted as the 
reduced versions of the STSSG. Under the 
STSSG platform, we compare the expressive 
1097
abilities of various grammars and translation 
models through linguistically-based synchronous 
parsing and a real translation platform. By syn-
chronous parsing, we aim to study which gram-
mar can well explain translation data (i.e. transla-
tional equivalence alignment) while by the real 
translation platform, we expect to investigate 
which model can achieve better translation per-
formance. In addition, we also measure the im-
pact of various factors in this study, including the 
genera of corpora (newspaper domain via spoken 
domain), the accuracy of word alignments and 
syntax parsing (automatically vs. manually).  
We report our experimental settings, experi-
mental results and our findings in detail in the 
rest of the paper, which is organized as follows: 
Section 2 reviews previous work. Section 3 
elaborates the general framework while Section 4 
reports the experimental results. Finally, we con-
clude our work in Section 5. 
2 Previous Work 
There are only a few of previous work related to 
the study of translation grammar comparison. 
Fox (2002) is the first to look at how well pro-
posed translation models fit actual translation 
data empirically. She examined the issue of 
phrasal cohesion between English and French 
and discovered that while there is less cohesion 
than one might desire, there is still a large 
amount of regularity in the constructions where 
breakdowns occur. This suggests that reordering 
words by phrasal movement is a reasonable strat-
egy (Fox, 2002). She has also examined the dif-
ferences in cohesion between Treebank-style 
parse trees, trees with flattened verb phrases, and 
dependency structures. Their experimental re-
sults indicate that the highest degree of cohesion 
is present in dependency structures. 
Motivated by the same problem raised by Fox 
(2002), Galley et al (2004) study what rule can 
better explain human translation data. They first 
propose a theory that gives formal semantics to 
word-level alignments defined over parallel cor-
pora, and then use the theory to introduce a linear 
algorithm that is used to derive from word-
aligned, parallel corpora the minimal set of syn-
tactically motivated transformation rules to ex-
plain human translation data. Their basic idea is 
to create transformation rules that condition on 
larger fragments of tree structure. Their experi-
mental results suggest that their proposed rules 
provide a good, realistic indicator of the com-
plexities inherent in translation than SCFG. 
Wellington et al (2006) describes their study 
of the patterns of translational equivalence exhib-
ited by a variety of bilingual/monolingual bitexts. 
They empirically measure the lower bounds on 
alignment failure rates with and without gaps 
under the constraints of word alignment alone or 
with one or both side parse trees. Their study 
finds surprisingly many examples of translational 
equivalence that could not be analyzed using bi-
nary-branching structures without discontinuities. 
Thus, they claim that the complexity of these 
patterns in every bitext is higher than suggested 
in the literature. In addition, they suggest that the 
low coverage rates without gaps under the con-
straints of independently generated monolingual 
parse trees might be the main reason why ?syn-
tactic? constraints have not yet increased the ac-
curacy of SMT systems. However, they find that 
simply allowing a single gap in bilingual phrases 
or other types of constituent can improve cover-
age dramatically. 
DeNeefe et al (2007) compares the strengths 
and weaknesses of a syntax-based MT model 
with a phrase-based MT model from the view-
points of translational equivalence extraction 
methods and coverage. They find that there are 
surprising differences in phrasal coverage ? nei-
ther is merely a superset of the other. They also 
investigate the reason why some phrase pairs are 
not learned by the syntax-based model. They fur-
ther propose several solutions and evaluate on 
the syntax-based extraction techniques in light of 
phrase pairs captured and translation accuracy. 
Finally, significant performance improvement is 
reported using their solutions. 
Different from previous work discussed above, 
this paper mainly focuses on the expressive abil-
ity comparison studies among different gram-
mars and models through synchronous parsing 
and a real SMT platform. Fox (2002), Galley et 
al (2004) and Wellington et al (2006) examine 
TEM only. DeNeefe et al (2007) only compares 
the strengths and weaknesses of a syntax-based 
MT model with a phrase-based MT model. 
3 The General Platform: the STSSG 
In this section, we first define the STSSG plat-
form in Subsection 3.1, and then explain why it 
is a general framework that can cover most of 
previous syntax-based translation grammars and 
models in Subsection 3.2. In Subsection 3.3 and 
3.4, we discuss the STSSG-based SMT and syn-
chronous parsing, which are used to compare 
different grammars and translation models. 
1098
1( )
IT e
1( )
JT f
A
 
 
Figure 1.  A word-aligned parse tree pairs of a Chi-
nese sentence and its English translation 
 
 
 
Figure 2. Two examples of translation rules 
3.1 Definition of the STSSG 
The STSSG is an extension of the STSG by us-
ing tree sequences (rather than elementary trees) 
as the basic translation unit. A STSSG is a septet 
, , , , ,,t t ts s sG N N S S P? ?=< > , where: 
z s?  and t?  are source and target terminal 
alphabets (POSs or lexical words), respec-
tively, and 
z sN  and tN are source and target non-
terminal alphabets (linguistic phrase tag, i.e. 
NP/VP?), respectively, and 
z s sS N?  and t tS N?  are the source and tar-
get start symbols (roots of source and target 
parse trees), and 
z P is a production rule set. 
A grammar rule ir  in the STSSG is an aligned 
tree sequence pair, < s? , t? , A  >, where s? and 
t?  are tree sequences of source side and target 
sides, respectively, and A is the alignments be-
tween leaf nodes of two tree sequences. Here, the 
key concept of ?tree sequence? refers to an or-
dered subtree sequence covering a consecutive 
tree fragment in a complete parse tree. The leaf 
nodes of a subtree in a tree sequence can be ei-
ther non-terminal symbols or terminal symbols. 
Fig. 2 shows two STSSG rules extracted from 
the aligned tree pair shown in Fig. 1, where 1r is 
also a STSG rule.  
In the STSSG, a translational equivalence is 
modeled as a tree sequence pair while MT is 
viewed as a tree sequence substitution process. 
From the definition of ?tree sequence?, we can 
see that a subtree in a tree sequence is a so-called 
elementary tree used in TSG. This suggests that 
SCFG and STSG are only a subset of STSSG 
and SCFG is a subset of STSG. The next subsec-
tion discusses how to configure the STSSG to 
implement the other two simplified grammars. 
This is the reason why we call the STSSG a gen-
eral framework for synchronous grammar-based 
translation modeling. 
It is worth noting that, from rule rewriting 
viewpoint, STSSG can be thought of as a re-
stricted version of synchronous multi-component 
TAGs (Schuler et al, 2000) although TAG is 
more powerful than TSG due to the additional 
operation ?adjunctions?. The synchronous multi-
component TAG can also rewrite several non-
terminals in one step of derivation. The differ-
ence between them is that the rewriting sites (i.e. 
the substitution nodes) must be contiguous in 
STSSG. In addition, STSSG is also related to 
tree automata (Comon et al, 2007). However, the 
discussion on the theoretical relation and com-
parison between them is out of the scope of the 
paper. In this paper, we focus on the comparison 
study of SMT grammars using the STSSG plat-
form. 
3.2 Rule Extraction and Grammar Con-
figuration 
All the STSSG mapping rules are extracted from 
bi-parsed trees. Our rule extraction algorithm is 
an extension of that presented at (Chiang, 2005; 
Liu et al, 2006; Zhang et al, 2007). We modify 
their tree-to-tree/string rule extraction algorithms 
to extract tree-sequence-to-tree-sequence rules. 
Our rules2 are extracted in two steps: 
                                                 
2  We classify the rules into two categories: initial 
rules, whose leaf nodes must be terminals, and ab-
1099
1) Extracting initial rules from bi-parsed trees. 
This is rather straightforward. We first generate 
all fully lexicalized source and target tree se-
quences (whose leaf nodes must be lexical words) 
using a DP algorithm and then iterate over all 
generated source and target sequence pairs. If 
their word alignments are all within the scope of 
the current tree sequence pair, then the current 
tree sequence pair is an initial rule. 
2) Extracting abstract rules from the extracted 
initial rules. The idea behind is that we generate 
an abstract rule from a ?big? initial rule by re-
moving one or more ?small? initial rules from 
the ?big? one, where the ?small? ones must be a 
sub-graph of the ?big? one. Please refer to 
(Chiang, 2005; Liu et al, 2006; Zhang et al, 
2007) for the implementation details. 
As indicated before (Chiang, 2005; Zhang et 
al., 2007), the above scheme generates a very 
large number of rules, which not only makes the 
system too complicated but also introduces too 
many undesirable ambiguities. To control the 
overall model complexity, we introduce the fol-
lowing parameters: 
1) The maximal numbers of trees in the source 
and target tree sequences: s? and t? . 
2) The maximal tree heights in the source and 
target tree sequences: s? and t? . 
3) The maximal numbers of non-terminal leaf 
nodes in the source and target tree sequences: 
s? and t? . 
Now let us see how to implement other mod-
els in relation to STSSG based the STSSG 
through configuring the above parameters. 
1) STSG-based tree-to-tree model (Zhang et 
al., 2007; Bod, 2007) when s? = t? =1. 
2) SCFG-based tree-to-tree model when s? = 
t? =1 and s? = t? =2. 
3) Phrase-based translation model only (no re-
ordering model) when s? = t? =0 and s? = t? =1. 
4) TSG-CFG-based tree-to-string model (Liu 
et al, 2006) when s? = t? =1, t? =2 and ignore 
phrase tags in target side.  
5) CFG-TSG-based string-to-tree model (Gal-
ley et al, 2006) when s? = t? =1and s? =2. 
6) TSSG-CFG-based tree-sequence-to-string 
model (Liu et al, 2007) when t? =2 and ignore 
phrase tags in target side. 
                                                                          
stract rule that having at least one non-terminal leaf 
node. 
From the above definitions, we can see that all 
of previous related models/grammars can be can 
be interpreted as the reduced versions of the 
STSSG. This is the reason why we use the 
STSSG as a general platform for our model and 
grammar comparison studies. 
3.3 Model Training and Decoder for SMT 
We use the tree sequence mapping rules to model 
the translation process. Given the source parse 
tree 1( )
JT f , there are multiple derivations3 that 
could lead to the same target tree 1( )
IT e , the 
mapping probability 1 1( ( ) | ( ))
I JrP T e T f is ob-
tained by summing over the probabilities of all 
derivations. The probability of each derivation?  
is given by the product of the probabilities of all 
the rules ( )ip r  used in the derivation (here we 
assume that a rule is applied independently in a 
derivation). 
1 1 1 1( | ) ( ( ) | ( ))
                  = ( )
i
I J I J
i
r
r rP e f P T e T f
p r
? ??
=
??           (1) 
The model is implemented under log-linear 
framework. We use seven basic features that are 
analogous to the commonly used features in 
phrase-based systems (Koehn, 2004): 1) bidirec-
tional rule mapping probabilities; 2) bidirectional 
lexical translation probabilities; 3) the target lan-
guage model; 4) the number of rules used and 5) 
the number of target words. Besides, we define 
two new features: 1) the number of lexical words 
in a rule to control the model?s preference for 
lexicalized rules over un-lexicalized rules and 2) 
the average tree height in a rule to balance the 
usage of hierarchical rules and more flat rules. 
The overall training process is similar to the 
process in the phrase-based system (koehn et al, 
2007): word alignment, rule extraction, feature 
extraction and probability calculation and feature 
weight tuning. 
Given 1( )
JT f , the decoder is to find the best 
derivation ?  that generates < 1( )JT f , 1( )IT e >.  
1
1
1 1
,
? arg max ( ( ) | ( ))
  arg max ( )
I
I
i
I J
e
i
e r
re P T e T f
p r
? ??
=
? ?              (2) 
By default, same as other SMT decoder, here 
we use Viterbi derivation in Eq (2) instead of the 
                                                 
3 A derivation is a sequence of tree sequence rules that 
maps a source parse tree to its target one. 
1100
summing probabilities in Eq (3). This is to make 
the decoder speed not too slow. The decoder is a 
standard span-based chart parser together with a 
function for mapping the source derivations to 
the target ones. To speed up the decoder, we util-
ize several thresholds to limit the search beams 
for each span, such as the number of rules used 
and the number of hypotheses generated. 
3.4 Synchronous Parsing   
A synchronous parser is an algorithm that can 
infer the syntactic structure of each component 
text in a multitext and simultaneously infer the 
correspondence relation between these structures. 
When a parser?s input can have fewer dimen-
sions than the parser?s grammar, we call it a 
translator. When a parser?s grammar can have 
fewer dimensions than the parser?s input, we call 
it a synchronizer (Melamed, 2004). Therefore, 
synchronous parsing and MT are closed to each 
other. In this paper, we use synchronous parsing 
to compare the ability of different grammars in 
translational equivalence modeling.  
Given a bilingual sentence pair 1
Jf and 1
Ie , the 
synchronous parser is to find a derivation ?  that 
generates < 1( )
JT f , 1( )
IT e >. Our synchronous 
parser is similar to the synchronous CKY parser 
presented at (Melamed, 2004). The difference is 
that we implement it based on our STSSG de-
coder. Therefore, in nature the parser is a stan-
dard synchronous chart parser but constrained by 
the rules of the STSSG grammar. In our imple-
mentation, we simply use our decoder to simu-
late the bilingual parser: 1) for each sentence pair, 
we extract one model; 2) we use the model and 
the decoder to translate the source sentence of 
the given sentence pair; 3) if the target sentence 
is successfully generated by the decoder, then we 
say the symphonious parsing is successful. 
Please note that the synchronous parsing is con-
sidered as successful once the last words in the 
source and target sentences are covered by the 
decoder even if there is no a complete target 
parse tree generated (it may be a tree sequence). 
This is because our study only concerns whether 
all translational equivalences are linked together 
by the synchronous parser correctly. 
4 Experiments 
4.1 Experimental Settings 
Synchronous parsing settings: Our experiments 
of synchronous parsing are carried on three Chi-
nese-to-English bilingual corpora: the FBIS cor-
pus, the IWSLT 2007 training set and the HIT 
Corpus. The FBIS data is a collection of trans-
lated newswire documents published by major 
news agencies from three representative loca-
tions: Beijing, Taipei and Hongkong. The 
IWSLT data is a multilingual speech corpus on 
travel domain while the HIT corpus consists of 
example sentences of a Chinese-English diction-
ary. The first two corpora are sentence-aligned 
while the HIT corpus is a manually bi-parsed 
corpus with manually annotated word alignments. 
We use the three corpora to study whether the 
models? expressive abilities are domain depend-
ent and how the performance of word alignment 
and parsing affect the ability of translation mod-
els. We selected 2000 sentence pairs from each 
individual corpus for the comparison study of 
translational equivalence modeling. Table 1 
gives descriptive statistics of the tree data set. 
 
 Chinese English 
FBIS 48,331 59,788 
IWSLT  17,667 18,427 
HIT 18,215 20,266 
 
Table 1. # of words of experimental data 
for synchronous parsing (there are 2k sen-
tence pairs in each individual corpus) 
 
In the synchronous parsing experiments, we 
compared three synchronous grammars: SCFG, 
STSG and STSSG using the STSSG platform. 
We use the same settings except the following 
parameters (please refer to Subsection 3.2 for 
their definitions): s? = t? =1, s? = t? =2 for 
SCFG ; s? = t? =1 and s? = t? =6 for STSG; 
s? = t? = 4 and s? = t? =6 for STSSG. We iter-
ate over each sentence pair in the three corpora 
with the following process: 
1) to used Stanford parser (Klein and Manning, 
2003) to parse bilingual sentences separately,  
this means that our study is based on the Penn 
Treebank style grammar.  
2) to extract SCFG, STSG and STSSG rules 
form each sentence pair, respectively; 
3) to do synchronous parsing using the exacted 
rules.  
Finally, we can calculate the successful rate of 
the synchronous parsing on each corpus. 
SMT evaluation settings: For the SMT ex-
periments, we trained the translation model on 
the FBIS corpus (7.2M (Chinese)+9.2M(English) 
words) and trained a 4-gram language model on 
1101
the Xinhua portion of the English Gigaword cor-
pus (181M words) using the SRILM Toolkits 
(Stolcke, 2002) with modified Kneser-Ney 
smoothing (Chen and Goodman, 1998). We used 
these sentences with less than 50 characters from 
the NIST MT-2002 test set as our development 
set and the NIST MT-2005 test set as our test set. 
We used the Stanford parser (Klein and Manning, 
2003) to parse bilingual sentences on the training 
set and Chinese sentences on the development 
and test sets. The evaluation metric is case-
sensitive BLEU-4 (Papineni et al, 2002). We 
used GIZA++ and the heuristics ?grow-diag-
final? to generate m-to-n word alignments. For 
the MER training, we modified Koehn?s MER 
trainer (Koehn, 2004) for our STSSG-based sys-
tem. For significance test, we used Zhang et als 
implementation (Zhang et al 2004). We com-
pared four SMT systems: Moses (Koehn et al, 
2007), SCFG-based, STSG-based and STSSG-
based tree-to-tree translation models. For Moses, 
we used its default settings. For the others, we 
implemented them on the STSSG platform by 
adopting the same settings as used in the syn-
chronous parsing. We optimized the decoding 
parameters on the development sets empirically. 
4.2 Experimental Results  
 
 SCFG STSG STSSG 
FBIS 7 (0.35%) 143 (7.15%) 388 (19.4%) 
IWSLT 171 (8.6%) 1179 (58.9%) 1708 (85.4%)
HIT 65 (3.23%) 1133 (56.6%) 1532 (76.6%)
 
Table 2. Successful rates (numbers inside 
bracket) of synchronous parsing over 2,000 
sentence pairs, where the integers outside 
bracket are the numbers of successfully-
parsed sentence pairs 
 
Table 2 reports the experimental results of syn-
chronous parsing. It shows that: 
1) As an extension of STSG/SCFG, STSSG 
outperforms STSG and SCFG consistently in the 
three data sets. The significant difference sug-
gests that the STSSG is much more effective in 
modeling translational equivalences and structure 
divergences. The reason is simply because the 
STSSG uses tree sequences as the basic transla-
tion unit so that it can model non-syntactic 
phrase equivalence with structure information 
and handle structure reordering in a large span.  
2) STSG shows much better performance than 
SCFG. It is mainly due to that STSG allow mul-
tiple level tree nodes operation and reordering in 
a larger span than SCFG. It reconfirms that only 
allowing sibling nodes reordering as done in 
SCFG may be inadequate for translational equiva-
lence modeling (Galley et al, 2004)4.  
3) All the three models on the FBIS corpus 
show much lower performance than that on the 
other two corpora. The main reason, as shown in 
Table 1, is that the sentences in the FBIS corpus 
are much longer than that in the other corpus, so 
their syntactic structures are significantly more 
complicated than the other two. In addition, al-
though tree sequences are utilized, STSSG show 
much lower performance in the FBIS corpus. 
This implies that the complexity of structure di-
vergence between two languages is higher than 
suggested in literature (Fox, 2002; Galley et al, 
2004). Therefore, structure divergence is still a 
big challenge to translational equivalence model-
ing when using syntactic structure mapping. 
4) The HIT corpus does not show better per-
formance than the IWSLT corpus although the 
HIT corpus is manually annotated with parse 
trees and word alignments. In order to study 
whether high performance word alignment and 
parsing results can help synchronous parsing, we 
do several cross validations and report the ex-
perimental results in Table 3. 
 
 Gold Word Alignment 
Automatic 
Word Align-
ment 
 Gold Parse 3.2/56.6/76.6 2.9/57.7/80.9
 Automatic  
Parse 3.2/55.6/76.0 2.9/54.2/78.8
 
Table 3. Successful rates (SCFG/STSG/ 
STSSG)(%) with regards to different word 
alignments and parse trees  on the HIT corpus 
 
Table 3 compares the performance of syn-
chronous parsing on the HIT corpus when using 
gold and automatic parser and word alignment. It 
is surprised that gold word alignments and parse 
trees do not help and even decrease the perform-
ance slightly. Our analysis further finds that 
                                                 
4 This claim is mainly hold for linguistically-informed 
SCFG since formal SCFG and BTG already showed 
much better performance in the formally syntax-based 
translation framework (Chiang, 2005). This is because 
the formal syntax is learned from phrase translational 
equivalences directly without relying on any linguistic 
theory (Chiang, 2005). Thus, it may not suffer from 
the issues of non-isomorphic structure alignment and 
non-syntactic phrase usage heavily (Wellington et al, 
2006). 
1102
more than 90% sentence pairs out of all the sen-
tence pairs that can be successfully bi-parsed are 
in common in the four experiments. This sug-
gests that the STSSG/STSG (SCFG achieves too 
much lower performance) and our rule extraction 
algorithm are robust in dealing with the errors 
introduced by the word alignment and parsing 
programs. If a parser, for example, makes a sys-
tematic error, we expect to learn a rule that can 
nevertheless be systematically used to model cor-
rect translational equivalence. Our error analysis 
on the three corpora shows that most of the fail-
ures of synchronous parsing are due to the struc-
ture divergence (i.e. the nature of non-
isomorphic structure mapping) and the long dis-
tance dependence in the syntactic structures.  
 
 SCFG Moses STSG STSSG
BLEU(%) 22.72 23.86 24.71 26.07 
 
     Table 3. Performance comparison of dif-
ferent grammars on FBIS corpus 
 
Table 3 compares different grammars in terms 
of translation performance. It shows that: 
1) The same as synchronous parsing, the 
STSSG-based model statistically significantly 
outperforms (p < 0.01) previous phrase-based and 
linguistically syntax-based methods. This empiri-
cally verifies the effect of the tree-sequence-based 
grammar for statistical machine translation.  
2) Both STSSG and STSG outperform Moses 
significantly and STSSG clearly outperforms 
STSG, which suggest that: 
z The linguistically motivated structure fea-
tures are still useful for SMT, which can be cap-
tured by the two syntax-based grammars through 
tree node operations. 
z STSSG is much more effective in utiliz-
ing linguistic structures than STSG since it uses 
tree sequence as the basic translation unit. This 
enables STSSG not only to handle structure reor-
derings by tree node operations in a larger span, 
but also to capture non-syntactic phrases with syn-
tactic information, and hence giving the grammar 
more expressive power. 
3) The linguistic-based SCFG shows much 
lower performance. This is largely because SCFG 
only allows sibling nodes reordering and fails to 
utilize both non-syntactic phrases and those syn-
tactic phrases that cannot be covered by a single 
CFG rule. It thereby suggests that SCFG is less 
effective in modelling parse tree structure trans-
fer.  
The above two experimental results show that 
STSSG achieves significant improvements over 
the other two grammars in terms of synchronous 
parsing?s successful rate and translation Bleu 
score. 
5 Conclusions 
Grammar is the fundamental infrastructure in 
translational equivalence modeling and statistical 
machine translation since grammar formalizes 
what kind of rule to be learned from a parallel 
text. In this paper, we first present a general plat-
form STSSG and demonstrate that a number of 
synchronous grammars and SMT models can be 
easily implemented based on the platform. We 
then compare the expressive abilities of different 
grammars on the platform using synchronous 
parsing and statistical machine translation. Our 
experimental results show that STSSG can better 
explain the data in parallel corpora than the other 
two synchronous grammars. We further finds 
that, although syntactic structure features are 
helpful in modeling translational equivalence, the 
complexity of structure divergence is much 
higher than suggested in literature, which im-
poses a big challenge to syntactic transformation-
based SMT. This may explain why traditional 
syntactic constraints in SMT do not yield much 
performance improvement over robust phrase-
substitution models. 
The fundamental assumption underlying much 
recent work on syntax-based modeling, which is 
considered to be one of next technology break-
throughs in SMT, is that translational equiva-
lence can be well modeled by structural trans-
formation. However, as discussed in prior arts 
(Galley et al, 2004) and this paper, linguisti-
cally-informed SCFG is an inadequate model for 
parallel corpora due to its nature that only allow-
ing child-node reorderings. Although STSG 
shows much better performance than SCFG, its 
two major limitations are that it only allows 
structure distortion operated on a single sub-tree 
and cannot model non-syntactic phrases. STSSG 
extends STSG by using tree sequence as the ba-
sic translation unit. This gives the grammar much 
more expressive power.  
There are many open issues in the syntactic 
transformation-based SMT due to the divergence 
nature between bilingual structure mappings. We 
find that structural divergences are more serious 
than suggested in the literature (Fox, 2002; Gal-
lery et al, 2004) or what we expected when sen-
tences are longer. We will continue to investigate 
1103
whether and how parallel corpora can be well 
modeled by syntactic structure mappings.   
References 
Rens Bod. 2007. Unsupervised Syntax-Based Ma-
chine Translation: The Contribution of Discon-
tinuous Phrases. MT-Summmit-07. 51-56.  
Peter F. Brown, S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311. 
S. F. Chen and J. Goodman. 1998. An empirical study 
of smoothing techniques for language modeling. 
Technical Report TR-10-98, Harvard University 
Center for Research in Computing Technology. 
David Chiang. 2005. A hierarchical phrase-based 
model for SMT. ACL-05. 263-270. 
H. Comon, M. Dauchet, R. Gilleron, F. Jacquemard, 
D. Lugiez, S. Tison, and M. Tommasi. 2007. Tree 
automata techniques and applications. Available at: 
http://tata.gforge.inria.fr/. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree trans-
lation. EMNLP-06. 232-241. 
S. DeNeefe, K. Knight, W. Wang and D. Marcu. 2007. 
What Can Syntax-based MT Learn from Phrase-
based MT? EMNLP-CoNLL-07. 755-763 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. ACL-05. 541-548. 
Bonnie J. Dorr (1994). Machine Translation Diver-
gences: A formal description and proposed solu-
tion. Computational Linguistics, 20(4): 597-633 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for MT. ACL-03 (companion volume). 
Heidi J. Fox. 2002. Phrasal Cohesion and Statistical 
Machine Translation. EMNLP-2002. 304-311  
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. 
DeNeefe, W. Wang and I. Thayer. 2006. Scalable 
Inference and Training of Context-Rich Syntactic 
Translation Models. COLING-ACL-06. 961-968 
M. Galley, M. Hopkins, K. Knight and D. Marcu. 
2004. What?s in a translation rule? HLT-NAACL. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06 (poster). 
Mary Hearne and Andy Way. 2003. Seeing the wood 
for the trees: data-oriented translation. MT Sum-
mit IX, 165-172. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. ACL-03. 423-430. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical phrase-based translation. HLT-NAACL-03. 
127-133. 
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. AMTA-04, 115-124. 
Philipp Koehn, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. 
Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. 
Constantin and E. Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07 (poster) 77-180. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-06. 
44-52. 
I. Dan Melamed. 2004. Statistical machine translation 
by parsing. ACL-04. 653-660. 
K. Papineni, Salim Roukos, ToddWard and Wei-Jing 
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. ACL-02. 311-318. 
Arjen Poutsma. 2000. Data-oriented translation. 
COLING-2000. 635-641 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. ACL-05. 271-279. 
William Schuler, David Chiang and Mark Dras. 2000. 
Multi-Component TAG and Notions of Formal 
Power. ACL-2000. 448-455 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Benjamin Wellington, Sonjia Waxmonsky and I. Dan 
Melamed. 2006. Empirical Lower Bounds on the 
Complexity of Translational Equivalence. COL-
ING-ACL-06. 977-984. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377-403. 
K. Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
M. Zhang, H. Jiang, A. Aw, J. Sun, S. Li and C. Tan. 
2007. A Tree-to-Tree Alignment-based Model for 
SMT. MT-Summit-07. 535-542. 
Y. Zhang, S. Vogel and A. Waibel. 2004. Interpreting 
BLEU/NIST scores: How much improvement do 
we need to have a better system? LREC-04.  
1104
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 843?851,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Feature-based Method for Document Alignment in 
Comparable News Corpora 
 
Thuy Vu, Ai Ti Aw, Min Zhang 
Department of Human Language Technology, Institute for Infocomm Research 
1 Fusionopolis Way, #21-01 Connexis, South Tower, Singapore 138632 
{tvu, aaiti, mzhang}@i2r.a-star.edu.sg 
 
 
 
Abstract 
In this paper, we present a feature-based me-
thod to align documents with similar content 
across two sets of bilingual comparable cor-
pora from daily news texts. We evaluate the 
contribution of each individual feature and 
investigate the incorporation of these diverse 
statistical and heuristic features for the task of 
bilingual document alignment. Experimental 
results on the English-Chinese and English-
Malay comparable news corpora show that 
our proposed Discrete Fourier Transform-
based term frequency distribution feature is 
very effective. It contributes 4.1% and 8% to 
performance improvement over Pearson?s 
correlation method on the two comparable 
corpora. In addition, when more heuristic and 
statistical features as well as a bilingual dic-
tionary are utilized, our method shows an ab-
solute performance improvement of 23.2% 
and 15.3% on the two sets of bilingual corpo-
ra when comparing with a prior information 
retrieval-based method.  
1 Introduction 
The problem of document alignment is described 
as the task of aligning documents, news articles 
for instance, across two corpora based on content 
similarity. The groups of corpora can be in the 
same or in different languages, depending on the 
purpose of one?s task. In our study, we attempt to 
align similar documents across comparable cor-
pora which are bilingual, each set written in a 
different language but having similar content and 
domain coverage for different communication 
needs. 
Previous works on monolingual document 
alignment focus on automatic alignment between 
documents and their presentation slides or be-
tween documents and their abstracts. Kan (2007) 
uses two similarity measures, Cosine and Jac-
card, to calculate the candidate alignment score 
in his SlideSeer system, a digital library software 
that retrieves documents and their narrated slide 
presentations. Daum? and Marcu (2004) use a 
phrase-based HMM model to mine the alignment 
between documents and their human-written ab-
stracts. The main purpose of this work is to in-
crease the size of the training corpus for a 
statistical-based summarization system. 
The research on similarity calculation for mul-
tilingual comparable corpora has attracted more 
attention than monolingual comparable corpora. 
However, the purpose and scenario of these 
works are rather varied. Steinberger et al (2002) 
represent document contents using descriptor 
terms of a multilingual thesaurus EUROVOC1, 
and calculate the semantic similarity based on the 
distance between the two documents? representa-
tions. The assignment of descriptors is trained by 
log-likelihood test and computed by ?????, Co-
sine, and Okapi. Similarly, Pouliquen et al 
(2004) use a linear combination of three types of 
knowledge: cognates, geographical place names 
reference, and map documents based on the 
EUROVOC. The major limitation of these works 
is the use of EUROVOC, which is a specific re-
source workable only for European languages. 
Aligning documents across parallel corpora is 
another area of interest. Patry and Langlais (2005) 
use three similarity scores, Cosine, Normalized 
Edit Distance, and Sentence Alignment Score, to 
compute the similarity between two parallel doc-
uments. An Adaboost classifier is trained on a list 
of scored text pairs labeled as parallel or non-
parallel. Then, the learned classifier is used to 
check the correctness of each alignment candidate. 
Their method is simple but effective. However, 
the features used in this method are only suitable 
for parallel corpora as the measurement is mainly 
based on structural similarity. One goal of docu-
ment alignment is for parallel sentence extraction 
for applications like statistical machine transla-
tion. Cheung and Fung (2004) highlight that most 
                                                          
1 EUROVOC is a multilingual thesaurus covering the fields 
in which the European Communities are active.  
843
of the current sentence alignment models are ap-
plicable for parallel documents, rather than com-
parable documents. In addition, they argue that 
document alignment should be done before paral-
lel sentence extraction.  
Tao and Zhai (2005) propose a general method 
to extract comparable bilingual text without us-
ing any linguistic resources. The main feature of 
this method is the frequency correlation of words 
in different languages. They assume that those 
words in different languages should have similar 
frequency correlation if they are actually transla-
tions of each other. The association between two 
documents is then calculated based on this in-
formation using Pearson?s correlation together 
with two monolingual features 25?? , a term 
frequency normalization (Stephan et al, 1994), 
and ???. The main advantages of this approach 
are that it is purely statistical-based and it is lan-
guage-independent. However, its performance 
may be compromised due to the lack of linguistic 
knowledge, particularly across corpora which are 
linguistically very different. Recently, Munteanu 
(2006) introduces a rather simple way to get the 
group of similar content document in multilin-
gual comparable corpus by using the Lemur IR 
Toolkit (Ogilvie and Callan, 2001). This method 
first pushes all the target documents into the da-
tabase of the Lemur, and then uses a word-by-
word translation of each source document as a 
query to retrieve similar content target docu-
ments.  
This paper will leverage on previous work, 
and propose and explore diverse range of fea-
tures in our system. Our document alignment 
system consists of three stages: candidate genera-
tion, feature extraction and feature combination. 
We verify our method on two set of bilingual 
news comparable corpora English-Chinese and 
English-Malay. Experimental results show that 
1) when only using Fourier Transform-based 
term frequency, our method outperforms our re-
implementation of Tao (2005)?s method by 4.1% 
and 8% for the top 100 alignment candidates and, 
2) when using all features, our method signifi-
cantly outperforms our implementation of Mun-
teanu?s (2006) method by 23.2% and 15.3%.  
The paper is organized as follows. In section 
2, we describe the overall architecture of our sys-
tem. Section 3 discusses our improved frequency 
correlation-based feature, while Section 4 de-
scribes in detail the document relationship heu-
ristics used in our model. Section 5 reports the 
experimental results. Finally, we conclude our 
work in section 6. 
2 System Architecture 
Fig 1 shows the general architecture of our doc-
ument alignment system. It consists of three 
components: candidate generation, feature ex-
traction, and feature combination. Our system 
works on two sets of monolingual corpora to de-
rive a set of document alignments that are com-
parable in their content. 
Fig 1. Architecture for Document Alignment Model. 
2.1 Candidate Generation 
Like many other text processing systems, the 
system first defines two filtering criteria to prune 
out ?clearly bad? candidates. This will dramati-
cally reduce the search space. We implement the 
following filers for this purpose: 
Date-Window Filter: As mentioned earlier, 
the data used for the present work are news cor-
pora?a text genre that has very strong links with 
the time element. The published date of docu-
ment is available in data, and can easily be used 
as an indicator to evaluate the relation between 
two articles in terms of time. Similar to Muntea-
nu?s (2006), we aim to constrain the number of 
candidates by assuming that documents with 
similar content should have publication dates 
which are fairly close to each other, even though 
they reside in two different sets of corpora. By 
imposing this constraint, both the complexity and 
the cost in computation can be reduced tremend-
ously as the number of candidates would be sig-
nificantly reduced. For example, when a 1-day 
window size is set, this means that for a given 
source document, the search for its target candi-
dates is set within 3 days of the source document: 
the same day of publication, the day after, and 
the day before. With this filter, using the data of 
one-month in our experiment, a reduction of 90% 
of all possible alignments can be achieved (sec-
tion 5.1). Moreover, with our evaluation data, 
844
after filtering out document pairs using a 1-day 
window size, up to 81.6% for English-Chinese 
and 80.3% for English-Malay of the golden 
alignments are covered. If the window size is 
increased to 5, the coverage is 96.6% and 95.6% 
for two language pairs respectively. 
Title-n-Content Filter: previous date window 
filter constrains the number of candidates based 
purely on temporal information without exploit-
ing any knowledge of the documents? contents. 
The number of candidates to be generated is thus 
dependent on the number of published articles 
per day, instead of the candidates? potential con-
tent similarity. For this reason, we introduce 
another filter which makes use of document titles 
to gauge content-wise cross document similarity. 
As document titles are available in news data, we 
capitalize on words found in these document 
titles, favoring alignment candidates where at 
least one of the title-words in the source docu-
ment has its translation found in the content of 
the other target document. This filter can reduce 
a further 47.9% (English-Chinese) and 26.3% 
(English-Malay) of the remaining alignment can-
didates after applying the date-window filter. 
2.2 Feature Extraction 
The second step extracts all the features for each 
candidate and computes the score for each indi-
vidual feature function. In our model, the feature 
set is composed of the Title-n-Content score 
(???), Linguistic-Independent-Unit score (???), 
and Monolingual Term Distribution similarity 
(???). We will discuss all three features in sec-
tions 3 and 4. 
2.3 Feature Combination 
The final score for each alignment candidate is 
computed by combining all the feature function 
scores into a unique score. In literature, there are 
many methods concerning the estimation of the 
overall score for a given feature set, which vary 
from supervised to unsupervised method. Super-
vised methods such as Support Vector Machine 
(SVM) and Maximum Entropy (ME) estimate 
the weight of each feature based on training data 
which are then used to calculate the final score. 
However, these supervised learning-based me-
thods may not be applicable to our proposed is-
sue as we are motivated to build a language 
independent unsupervised system. We simply 
take a product of all normalized features to ob-
tain one unique score. This is because our fea-
tures are probabilistically independent. In our 
implementation, we normalize the scores to make 
them less sensitive to the absolute value by tak-
ing the logarithm ???. ? as follows: 
 
??????? ? ???
?? ? ??, ? ? ?? ? ??
1, ????
 (1)
 
 
?? ? ?? is a threshold for ? to contribute posi-
tively to the unique score. In our experiment, we 
empirically choose ?  be 2.2 , and the threshold 
for ? is 0.51828 (as ? ? 2.71828). 
3 Monolingual Term Distribution 
3.1 Baseline Model 
The main feature used in Tao and Zhai (2005) is 
the frequency distribution similarity or frequency 
correlation of words in two given corpora. It is 
assumed that frequency distributions of topically-
related words in multilingual comparable corpora 
are often correlated due to the correlated cover-
age of the same events.  
Let ? ? ???, ??, ? , ??? and ? ? ???, ??, ? , ??? 
be the frequency distribution vectors of two 
words ?  and ?  in two documents respectively. 
The frequency correlation of the two words is 
computed by Pearson?s Correlation Coefficient 
in (2). 
 
???, ?? ?
? ????
?
??? ?? ??
?
??? ? ??
?
???
??? ??
??
??? ?
?
?
?? ??
?
??? ?
?
??? ??
??
??? ?
?
?
?? ??
?
??? ?
?
?
 (2)
 
The similarity of two documents is calculated 
with the addition of two features namely Inverse 
Document Frequency (???) and 25?? term fre-
quency normalization shown in the equation (3). 
 
 
????, ??? ? ? ?????? ? ?????? ? ???, ?? ?????,????
25????, ??? ? 25????, ???  
(3)
 
 
Where 25????, ??  is the word frequency 
normalization for word ?  in document? , and 
????????? is the average length of a document. 
 
25????, ?? ? ???
??,??
???,???????????
|?|
??????????
  (4)
 
It is noted that the key feature used by Tao and 
Zhai (2005) is the ???, ?? score which depends 
purely on statistical information. Therefore, our 
motivation is to propose more features to link the 
source and target documents more effectively for 
a better performance.  
3.2 Study on Frequency Correlation 
We further investigate the frequency correlation of 
words from comparable sets of corpora compris-
ing three different languages using the above-
defined model.  
845
 
Fig 2. Sample of frequency correlation for ?Bank Dunia?, ?World Bank?, and ??????. 
 
 
Fig 3. Sample of frequency correlation for ?Dunia?, ?World?, and ????. 
 
 
Fig 4. Sample of frequency correlation for ?Filipina?, ?Information Technology?, and ?????. 
 
Using three months - May to July, 2006 ? of daily 
newspaper in Strait Times2 (in English), Zao Bao3 
(in Chinese), and Berita Harian4 (in Malay), we 
conduct the experiments described in the follow-
ing Fig 2, Fig 3, and Fig 4 showing three different 
cases of term or word correlation. In these figures, 
the ?-axis denotes time and the ?-axis shows the 
frequency distribution of the term or word.  
Multi-word versus Single-word: Fig 2 
illustrates that the distributions for multi-word 
term such as ?World Bank?, ?????(World 
Bank in Chinese)?, and ?Bank Dunia (World 
Bank in Malay)? in the three language corpora 
are almost similar because of the discriminative 
power of that phrase. The phrase has no variance 
and contains no ambiguity. On the other hand, 
the distributions for single words may have much 
less similarity. 
                                                          
2 http://www.straitstimes.com/ an English news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
3 http://www.zaobao.com/ a Chinese news agency in Singa-
pore. Source ? Singapore Press Holdings Ltd. 
4 http://cyberita.asia1.com.sg/ a Malay news agency in Sin-
gapore. Source ? Singapore Press Holdings Ltd. 
Related Common Word: we also investigate 
the similarity in frequency distribution for related 
common single words in the case of ?World?, 
??? (world in Chinese)?, and ?Dunia (world in 
Malay)? as shown in Fig 3. It can be observed 
that the correlation of these common words is not 
as strong as that in the multi-word sample illu-
strated in Fig 2. The reason is that there are many 
variances of these common words, which usually 
do not have high discriminative power due to the 
ambiguities presented within them. Nonetheless, 
among these variances, there is still a small simi-
lar distribution trends that can be detected, which 
may enable us to discover the associations be-
tween them. 
Unrelated Common Word: Fig 4 shows the 
frequency distribution of three unrelated com-
mon words over the same three-month period. 
No correlation in distribution is found among 
them.
0
0.05
0.1
0.15
0.2
1 11 21 31 41 51 61 71 81 91
Bank?Dunia World?Bank ????
0
0.01
0.02
0.03
1 11 21 31 41 51 61 71 81 91
Dunia World ??
0
0.05
0.1
0.15
1 11 21 31 41 51 61 71 81 91
Filipina Information?Technology ???
846
3.3 Enhancement from Baseline Model 
3.3.1 Monolingual Term Correlation 
Due to the inadequacy of the baseline?s purely 
statistical approach, and our studies on the corre-
lations of single, multiple and commonly appear-
ing words, we propose using ?term? or ?multi-
word? instead of ?single-word? or ?word? to cal-
culate the similarity of term frequency 
distribution between two documents. This 
presents us with two main advantages. Firstly, 
the smaller number of terms compared to the 
number of words present in any document would 
imply fewer possible document alignment pairs 
for the system. This increases the computation 
speed remarkably. To extract automatically the 
list of terms in each document, we use the term 
extraction model from Vu et al (2008). In corpo-
ra used in our experiments, the average ratios of 
word/term per document are 556/37, 410/28 and 
384/28 for English, Chinese, and Malay respec-
tively. The other advantage of using terms is that 
terms are more distinctive than words as they 
contain less ambiguity, thus enabling high corre-
lation to be observed when compared with single 
words. 
3.3.2 Bilingual Dictionary Incorporation 
In addition to using terms for the computation, 
we observed from equation (3) that the only mu-
tual feature relating the two documents is the 
frequency distribution coefficient ???, ?? . It is 
likely that the alignment performance could be 
enhanced if more features relating the two doc-
uments are incorporated. 
Following that, we introduce a linguistic fea-
ture, ??????????, ?? , to the baseline model to 
enhance the association between two documents. 
This feature involves the comparison of the 
translations of words within a particular term in 
one language, and the presence of these transla-
tions in the corresponding target language term. 
If more translations obtained from a bilingual 
dictionary of words within a term are found in 
the term extracted from the other language?s 
document, it is more likely that the 2 bilingual 
terms are translations of each other. This feature 
counts the number of word translation found be-
tween the two terms, as described in the follow-
ing. Let ??  and ??  be the term list of ??  and ?? 
respectively, the similarity score in our model is: 
???????, ??? ? ? ?????? ? ?????? ? ???, ?? ?????,????
??????????, ?? ? 25????, ??? ? 25????, ???
(5) 
3.3.3 Distribution Similarity Measurement 
using Monolingual Term 
Finally, we apply the results of time-series re-
search to replace Pearson?s correlation which is 
used in the baseline model, in our calculation of 
the similarity score of two frequency distribu-
tions. A popular technique for time sequence 
matching is to use Discrete Fourier Transform 
(??? ) (Agrawal et al 1993). More recently, 
Klementiev and Roth (2006) also use F-index 
(Hetland, 2004), a score using ???, to calculate 
the time distribution similarity. In our model, we 
assume that the frequency chain of a word is a 
sequence, and calculate ???  score for each 
chain by the following formula: 
?? ? ???. ?
?????
??
???
 (6)
In time series research, it is proven that only 
the first few ?  coefficients of a ???  chain are 
strong and important for comparison (Agrawal et 
al, 1993). Our experiments in section 5 show that 
the best value for ? is 7 for both language pairs. 
???, ?? ?
?
??????? ? ????
?
?
???
?
?
??
 (7)
The ???, ??  in equation (5) is replaced by 
???, ?? in equation (8) to calculate the Monolin-
gual Term Distribution (???) score. 
4 Document Relationship Heuristics 
Besides the ???, we also propose two heuristic-
based features that focus directly on the 
relationship between two multilingual documents, 
namely the Title-n-Content score? ??? , which 
measures the relationship between the title and 
content of a document pair, and Linguistic Inde-
pendent Unit score ? ??? , which make use of 
orthographic similarity between unit of words for 
the different languages.  
4.1 Title-n-Content Score (???) 
Besides being a filter for removing bad align-
ment candidates, ???  is also incorporated as a 
feature in the computation of document align-
ment score. In the corpora used, in most docu-
ments, ?title? does reveal the main topic of a 
document. The use of words in a news title is 
???????, ??? ? ? ?????? ? ??????
????,????
? ???, ?? ? ??????????, ??
? 25????, ??? ? 25????, ??? 
(8)
847
typically concise and conveys the essence of the 
information in the document. Thus, a high ??? 
score would indicate a high likelihood of similar-
ity between two bilingual documents. Therefore, 
we use ??? as a quantitative feature in our fea-
ture set. Function ????, ?? checks whether the 
translation of a word in a document?s title is 
found in the content of its aligned document: 
 
????, ?? ? ?1,     translation of ? is in ?0,     else                                (9)
 
The ??? score of document ??  and ??  is cal-
culated by the following formula: 
 
??????, ??? ? 
? ?????, ???
???T?
? ? ?????, ???
???T?
 (10)
Where ??  and ??  are the content of document 
?? and ??; and ?? and ?? are the set of title words 
of two documents. 
In addition, this method speeds up the align-
ment process without compromising perfor-
mance when compared with the calculation 
based only on contents on both sides. 
4.2 Linguistic Independent Unit (???) 
Linguistic Independent Unit score (LIU) is de-
fined as the piece of information, which is writ-
ten in the same way for different languages. The 
following highlight the number 25, 11, and 50 as 
linguistic-independent-units for the two sen-
tences. 
English: Between Feb 25 and March 11 this 
year, she used counterfeit $50 notes 10 times to 
pay taxi fares ranging from $2.50 to $4.20. 
Chinese:????????????? 2 ?
25 ?? 3? 11 ??? 50 ????????
??? 2? 5?? 4? 2?????? 
5 Experiment and Evaluation 
5.1 Experimental Setup 
The experiments were conducted on two sets of 
comparable corpora namely English-Chinese and 
English-Malay. The data are from three news 
publications in Singapore: the Strait Times (ST, 
English), Lian He Zao Bao (ZB, Chinese), and 
Berita Harian (BH, Malay). Since these languag-
es are from different language families 5 , our 
model can be considered as language indepen-
dent. 
                                                          
5 English is in Indo-European; Chinese is in Sino-Tibetan; 
Malay is in Austronesian family [Wikipedia]. 
The evaluation is conducted based on a set of 
manually aligned documents prepared by a group 
of bilingual students. It is done by carefully read-
ing through each article in the month of June 
(2006) for both sets of corpora and trying to find 
articles of similar content in the other language 
within the given time window. Alignment is 
based on similarity of content where the same 
story or event is mentioned. Any two bilingual 
articles with at least 50% content overlapping are 
considered as comparable. This set of reference 
data is cross-validated between annotators. Table 
1 shows the statistics of our reference data for 
document alignment. 
 
Language pair ST ? ZB ST ? BH 
Distinct source 396 176
Distinct target 437 175
Total alignments 438 183
Table 1. Statistics on evaluation data. 
 
Note that although there are 438 alignments 
for ST-ZB, the number of unique ST articles are 
396, implying that the mapping is not one-to-one. 
5.2 Evaluation Metrics 
Evaluation is performed on two levels to reflect 
performance from two different perspectives. 
?Macro evaluation? is conducted to assess the 
correctness of the alignment candidates given 
their rank among all the alignment candidates. 
?Micro evaluation? concerns about the correctness 
of the aligned documents returned for a given 
source document. 
Macro evaluation: we present the perfor-
mance for macro evaluation using average preci-
sion. It is used to evaluate the performance of a 
ranked list and gives higher score for the list that 
returns more correct alignment in the top. 
Micro evaluation: for micro evaluation, we 
evaluate the F-Score, calculated from recall and 
precision, based on the number of correct align-
ments for the top of alignment candidates for 
each source document. 
5.3 Experiment and Result 
First we implement the method of Tao and Zhai 
(2005) as the baseline. Basically, this method 
does not depend on any linguistic resources and 
calculates the similarity between two documents 
purely by comparing all possible pairs of words. 
In addition to this, we also implement Muntea-
nu?s (2006) method which uses Okapi scoring 
function from the Lemur Toolkit (Ogilvie and 
848
Callan, 2001) to obtain the similarity score. This 
approach relies heavily on bilingual dictionaries. 
To assess performances more fairly, the result 
from baseline method of Tao and Zhai are com-
pared against the results of the following list of 
incremental approaches: the baseline (A); the 
baseline using term instead of word (B); replac-
ing ???, ?? by ???, ?? for ??? feature, with and 
without bilingual dictionaries in (C) and (D) re-
spectively; and including ???  and ???  for our 
final model in (E). Our model is also compared 
our model with results from the implementation 
of Munteanu (2006) using Okapi (F), and the 
results from a combination of our model with 
Okapi (G). Table 2 and Table 3 show the expe-
rimental results for two language pairs English ? 
Chinese (ST-ZB) and English ? Malay (ST-BH), 
respectively. Each row displays the result of each 
experiment at a certain cut-off among the top 
returned alignments. The ?Top? columns reflect 
the cut-off threshold. 
The first three cases (A), (B) and (C), which 
do not rely on linguistic resources, suggest that 
our new features lead to better performance im-
provement over the baseline. It can be seen that 
the use of term and ??? significantly improves 
the performance. The improvement indicated by 
a sharp increase in all cases from (C) to (D) 
shows that dictionaries can indeed help ??? fea-
tures. 
Based on the result of (E), our final model 
significantly outperforms the model of Munteanu  
(F) in both macro and micro evaluation. It is 
noted that our features rely less heavily on dic-
tionaries as it only makes use of this resource to 
translate term words and title words of a docu-
ment while Munteanu (2006) needs to translate 
entire documents, exclude stopword, and relying 
on an IR system. It is also observed that the per-
formance of (G) shows that although the incor-
poration of Okapi score in our final model (E) 
improves the average precision performance of 
ST-ZB slightly, it does not appear to be helpful 
for our ST-BH data. However, Okapi does help 
in the F-Measure on both corpora. 
 
 
Pair? Strait?Times???Zao?Bao?
Level? Top? A? B? C? D? E? F? G?
A
ve
/P
re
ci
si
on
?
M
ac
ro
? 50? 0.042? 0.083? 0.08? 0.559? 0.430? 0.209? 0.508?
100? 0.042? 0.069? 0.083? 0.438? 0.426? 0.194? 0.479?
200? 0.025? 0.069? 0.110? 0.342? 0.396? 0.153? 0.439?
500? 0.025? 0.054? 0.110? 0.270? 0.351? 0.111? 0.376?
F?
M
ea
su
re
?
M
ic
ro
?
1? 0.005? 0.007? 0.009? 0.297? 0.315? 0.157? 0.333?
2? 0.006? 0.005? 0.013? 0.277? 0.286? 0.133? 0.308?
5? 0.005? 0.006? 0.009? 0.200? 0.190? 0.096? 0.206?
10? 0.005? 0.005? 0.007? 0.123? 0.119? 0.063? 0.126?
20? 0.006? 0.008? 0.007? 0.073? 0.074? 0.038? 0.076?
 
Table 2. Performance of Strait Times ? Zao Bao.  
 
Pair? Strait?Times???Berita?Harian?
Level? Top? A? B? C? D? E? F? G?
A
ve
/P
re
ci
si
on
?
M
ac
ro
? 50? 0.000? 0.000? 0.000? 0.514? 0.818? 0.000? 0.782?
100? 0.000? 0.000? 0.080? 0.484? 0.759? 0.052? 0.729?
200? 0.000? 0.008? 0.090? 0.443? 0.687? 0.073? 0.673?
500? 0.005? 0.008? 0.010? 0.383? 0.604? 0.078? 0.591?
F?
M
ea
su
re
?
M
ic
ro
?
1? 0.000? 0.000? 0.005? 0.399? 0.634? 0.119? 0.650?
2? 0.000? 0.004? 0.010? 0.340? 0.515? 0.128? 0.515?
5? 0.002? 0.005? 0.010? 0.205? 0.270? 0.105? 0.273?
10? 0.004? 0.014? 0.013? 0.130? 0.150? 0.076? 0.150?
20? 0.006? 0.017? 0.017? 0.074? 0.078? 0.043? 0.078?
 
Table 3. Performance of Strait Times ? Berita Harian. 
 
 
849
5.4 Discussion 
It can be seen from Table 2 and Table 3 that by 
exploiting the frequency distribution of terms 
using Discrete Fourier Transform instead of 
words on Pearson?s Correlation, performance is 
noticeably improved. Fig 5 shows the incremen-
tal improvement of our model for top-200 and 
top-2 alignments using macro and micro evalua-
tion respectively. The sharp increase can be seen 
in Fig 5 from point (C) onwards. 
 
Fig 5. Step-wise improvement at top-200 for macro 
and top-2 for micro evaluation. 
Fig 6 compares the performance of our system 
with Tao and Zhai (2005) and Munteanu (2006). 
It is shown that our systems outperform these 
two systems under the same experimental 
parameters. Moreover, even without the use of 
dictionaries, our system?s performance on ST-
BH data is much better than Munteanu?s (2006) 
on the same data. 
 
Fig 6. System comparison for ST-ZB and ST-BH at 
top-500 for macro and top-5 for micro evaluation. 
 
We find that dictionary usage contributes 
much more to performance improvement in ST-
BH compared to that in ST-ZB. We attribute this 
to the fact that the feature LIU already contri-
butes markedly to the increase in the perfor-
mance of ST-BH. As a result, it is harder to make 
further improvements even with the application 
of bilingual dictionaries. 
6 Conclusion and Future Work 
In this paper, we propose a feature based model 
for aligning documents from multilingual com-
parable corpora. Our feature set is selected based 
on the need for a method to be adaptable to new 
language-pairs without relying heavily on lin-
guistic resources, unsupervised learning strategy. 
Thus, in the proposed method we make use of 
simple bilingual dictionaries, which are rather 
inexpensive and easily obtained nowadays. We 
also explore diverse features, including Mono-
lingual Term Distribution (??? ), Title-and-
Content (???), and Linguistic Independent Unit 
(???) and measure their contributions in an in-
cremental way. The experiment results show that 
our system can retrieve similar documents from 
two comparable corpora much better than using 
an information retrieval, such as that used by 
Munteanu (2006). It also performs better than a 
word correlation-based method such as Tao?s 
(2005). 
Besides document alignment as an end, there 
are many tasks that can directly benefit from 
comparable corpora with documents that are 
well-aligned. These include sentence alignment, 
term alignment, and machine translation, espe-
cially statistical machine translation. In the future, 
we aim to extract other valuable information 
from comparable corpora which benefits from 
comparable documents. 
Acknowledgements 
We would like to thank the anonymous review-
ers for their many constructive suggestions for 
improving this paper. Our thanks also go to Ma-
hani Aljunied for her contributions to the linguis-
tic assessment in our work.  
References 
Percy Cheung and Pascale Fung. 2004. Sentence 
Alignment in Parallel, Comparable, and Quasi-
comparable Corpora. In Proceedings of 4th Inter-
national Conference on Language Resources and 
Evaluation (LREC). Lisbon, Portugal. 
Hal Daume III and Daniel Marcu. 2004. A Phrase-
Based HMM Approach to Document/Abstract 
Alignment. In Proceedings of Empirical Methods 
in Natural Language Processing (EMNLP). Spain. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
A B C D E
ST?? ZB?A/Prec ST?? ZB?F?Score
ST?? BH?A/Prec ST?? BH?F?Score
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
A/Prec F?Score A/Prec F?Score
ST?? ZB ST?? BH
Tao?and?Zhai?(2005) Our?System?w/o?Dict
Our?System?w?Dict Munteanu?(2006)
850
Min-Yen Kan. 2007. SlideSeer: A Digital Library of 
Aligned Document and Presentation Pairs. In Pro-
ceedings of the Joint Conference on Digital Libra-
ries (JCDL). Vancouver, Canada. 
Soto Montalvo, Raquel Martinez, Arantza Casillas, 
and Victor Fresno. 2006. Multilingual Document 
Clustering: a Heuristic Approach Based on Cog-
nate Named Entities. In Proceedings of the 21st In-
ternational Conference on Computational 
Linguistics and the 44th Annual Meeting of the 
ACL. 
Stephen E. Robertson, Steve Walker, Susan Jones, 
Micheline Hancock-Beaulieu, and Mike Gatford. 
1994. Okapi at TREC-3. In Proceedings of the 
Third Text REtrieval Conference (TREC 1994). 
Gaithersburg, USA. 
Dragos Stefan Munteanu. 2006. Exploiting Compara-
ble Corpora. PhD Thesis. Information Sciences In-
stitute, University of Southern California. USA. 
Ogilvie, P., and Callan, J. 2001. Experiments using 
the Lemur toolkit. In Proceedings of the 10th Text 
REtrieval Conference (TREC). 
Alexandre Patry and Philippe Langlais. 2005. Auto-
matic Identification of Parallel Documents with 
light or without Linguistics Resources. In Proceed-
ings of 18th Annual Conference on Artificial Intel-
ligent. 
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat, 
Emilia Kasper, and Irina Temnikova. 2004. Multi-
lingual and Cross-lingual news topic tracking. In 
Proceedings of the 20th International Conference 
on Computational Linguistics (COLING). 
Ralf Steinberger, Bruno Pouliquen, and Johan Hag-
man. 2002. Cross-lingual Document Similarity 
Calculation Using the Multilingual Thesaurus 
EUROVOC. Computational Linguistics and Intel-
ligent Text Processing. 
Tao Tao and ChengXiang Zhai. 2005. Mining Com-
parable Bilingual Text Corpora for Cross-
Language Information Integration. In Proceedings 
of the 2005 ACM SIGKDD International Confe-
rence on Knowledge Discovery and Data Mining. 
Thuy Vu, Ai Ti Aw and Min Zhang. 2008. Term ex-
traction through unithood and termhood unification. 
In Proceedings of the 3rd International Joint Con-
ference on Natural Language Processing 
(IJCNLP-08). Hyderabad, India. 
ChengXiang Zhai and John Lafferty. 2001. A study of 
smoothing methods for language models applied to 
Ad Hoc information retrieval. In Proceedings of 
the 24th annual international ACM SIGIR confe-
rence on Research and development in information 
retrieval. Louisiana, United States. 
R. Agrawal, C. Faloutsos, and A. Swami. 1993. Effi-
cient similarity search in sequence databases. In 
Proceedings of the 4th International Conference on 
Foundations of Data Organization and Algorithms. 
Chicago, United States. 
Magnus Lie Hetland. 2004. A survey of recent me-
thods for efficient retrieval of similar time se-
quences. In Data Mining in Time Series Databases. 
World Scientific. 
Alexandre Klementiev and Dan Roth. 2006. Weakly 
Supervised Named Entity Transliteration and Dis-
covery from Multilingual Comparable Corpora. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th Annual 
Meeting of the ACL. 
851
  
Name Origin Recognition Using Maximum Entropy Model  
and Diverse Features 
Min Zhang1, Chengjie Sun2, Haizhou Li1, Aiti Aw1, Chew Lim Tan3, Xiaolong Wang2 
1Institute for Infocomm 
Research, Singapore 
{mzhang,hli,aaiti} 
@i2r.a-star.edu.sg 
2Harbin Institute of 
Technology, China 
{cjsun,wangxl} 
@insun.hit.edu.cn 
 
3National University of 
Singapore, Singapore 
tancl@comp.
nus.edu.sg 
Abstract 
Name origin recognition is to identify the 
source language of a personal or location 
name.  Some early work used either rule-
based or statistical methods with single 
knowledge source. In this paper, we cast the 
name origin recognition as a multi-class 
classification problem and approach the 
problem using Maximum Entropy method. 
In doing so, we investigate the use of differ-
ent features, including phonetic rules, n-
gram statistics and character position infor-
mation for name origin recognition. Ex-
periments on a publicly available personal 
name database show that the proposed ap-
proach achieves an overall accuracy of 
98.44% for names written in English and 
98.10% for names written in Chinese, which 
are significantly and consistently better than 
those in reported work.  
1 Introduction 
Many technical terms and proper names, such as 
personal, location and organization names, are 
translated from one language into another with 
approximate phonetic equivalents. The phonetic 
translation practice is referred to as transliteration; 
conversely, the process of recovering a word in its 
native language from a transliteration is called as 
back-transliteration (Zhang et al 2004; Knight 
and Graehl, 1998).  For example, English name 
?Smith? and ????  (Pinyin 1 : Shi-Mi-Si)? in 
                                                 
1 Hanyu Pinyin, or Pinyin in short, is the standard romaniza-
tion system of Chinese. In this paper, Pinyin is given next to 
Chinese form a pair of transliteration and back-
transliteration. In many natural language process-
ing tasks, such as machine translation and cross-
lingual information retrieval, automatic name 
transliteration has become an indispensable com-
ponent.  
Name origin refers to the source language of a 
name where it originates from. For example, the 
origin of the English name ?Smith? and its Chi-
nese transliteration ???? (Shi-Mi-Si)? is Eng-
lish, while both ?Tokyo? and ??? (Dong-Jing)? 
are of Japanese origin. Following are examples of 
different origins of a collection of English-Chinese 
transliterations. 
 
English: Richard-??? (Li-Cha-De) 
Hackensack-????(Ha-Ken-
Sa-Ke) 
Chinese: Wen JiaBao-???(Wen-Jia-
Bao) 
ShenZhen???(Shen-Zhen) 
Japanese: Matsumoto-?? (Song-Ben) 
Hokkaido-???(Bei-Hai-Dao) 
Korean: Roh MooHyun-???(Lu-Wu-
Xuan) 
Taejon-??(Da-Tian) 
Vietnamese: Phan Van Khai-???(Pan-
Wen-Kai) 
Hanoi-??(He-Nei) 
 
In the case of machine transliteration, the name 
origins dictate the way we re-write a foreign word. 
For example, given a name written in English or 
Chinese for which we do not have a translation in 
                                                                            
Chinese characters in round brackets for ease of reading. 
56
  
a English-Chinese dictionary, we first have to de-
cide whether the name is of Chinese, Japanese, 
Korean or some European/English origins. Then 
we follow the transliteration rules implied by the 
origin of the source name. Although all English 
personal names are rendered in 26 letters, they 
may come from different romanization systems. 
Each romanization system has its own rewriting 
rules. English name ?Smith? could be directly 
transliterated into Chinese as ????(Shi-Mi-Si)? 
since it follows the English phonetic rules, while 
the Chinese translation of Japanese name ?Koi-
zumi? becomes ???(Xiao-Quan)? following the 
Japanese phonetic rules. The name origins are 
equally important in back-transliteration practice. 
Li et al (2007) incorporated name origin recogni-
tion to improve the performance of personal name 
transliteration. Besides multilingual processing, 
the name origin also provides useful semantic in-
formation (regional and language information) for 
common NLP tasks, such as co-reference resolu-
tion and name entity recognition. 
Unfortunately, little attention has been given to 
name origin recognition (NOR) so far in the litera-
ture. In this paper, we are interested in two kinds 
of name origin recognition: the origin of names 
written in English (ENOR) and the origin of 
names written in Chinese (CNOR). For ENOR, 
the origins include English (Eng), Japanese (Jap), 
Chinese Mandarin Pinyin (Man) and Chinese Can-
tonese Jyutping (Can). For CNOR, they include 
three origins: Chinese (Chi, for both Mandarin and 
Cantonese), Japanese and English (refer to Latin-
scripted language). 
Unlike previous work (Qu and Grefenstette, 
2004; Li et al, 2006; Li et al, 2007) where NOR 
was formulated with a generative model, we re-
gard the NOR task as a classification problem. We 
further propose using a discriminative learning 
algorithm (Maximum Entropy model: MaxEnt) to 
solve the problem. To draw direct comparison, we 
conduct experiments on the same personal name 
corpora as that in the previous work by Li et al 
(2006). We show that the MaxEnt method effec-
tively incorporates diverse features and outper-
forms previous methods consistently across all test 
cases. 
The rest of the paper is organized as follows: in 
section 2, we review the previous work. Section 3 
elaborates our proposed approach and the features. 
Section 4 presents our experimental setup and re-
ports our experimental results. Finally, we con-
clude the work in section 5. 
2 Related Work 
Most of previous work focuses mainly on ENOR 
although same methods can be extended to CNOR. 
We notice that there are two informative clues that 
used in previous work in ENOR. One is the lexical 
structure of a romanization system, for example, 
Hanyu Pinyin, Mandarin Wade-Giles, Japanese 
Hepbrun or Korean Yale, each has a finite set of 
syllable inventory (Li et al, 2006). Another is the 
phonetic and phonotactic structure of a language, 
such as phonetic composition, syllable structure. 
For example, English has unique consonant 
clusters such as /str/ and /ks/ which Chinese, 
Japanese and Korean (CJK) do not have. 
Considering the NOR solutions by the use of these 
two clues, we can roughly group them into two 
categories: rule-based methods (for solutions 
based on lexical structures) and statistical methods 
(for solutions based on phonotactic structures). 
Rule-based Method  
Kuo and Yang (2004) proposed using a rule-
based method to recognize different romanization 
system for Chinese only. The left-to-right longest 
match-based lexical segmentation was used to 
parse a test word. The romanization system is con-
firmed if it gives rise to a successful parse of the 
test word. This kind of approach (Qu and Grefen-
stette, 2004) is suitable for romanization systems 
that have a finite set of discriminative syllable in-
ventory, such as Pinyin for Chinese Mandarin. For 
the general tasks of identifying the language origin 
and romanization system, rule based approach 
sounds less attractive because not all languages 
have a finite set of discriminative syllable inven-
tory. 
Statistical Method 
1) N-gram Sum Method (SUM): Qu and Gre-
fenstette (2004) proposed a NOR identifier using a 
trigram language model (Cavnar and Trenkle, 
1994) to distinguish personal names of three lan-
guage origins, namely Chinese, Japanese and Eng-
lish. In their work, the training set includes 11,416 
Chinese name entries, 83,295 Japanese name en-
tries and 88,000 English name entries. However, 
the trigram is defined as the joint probabil-
57
  
ity 1 2( )i i ip c c c? ? for 3-character 1 2i i ic c c? ?  rather than 
the commonly used conditional probabil-
ity 1 2( | )i i ip c c c? ? . Therefore, the so-called trigram 
in Qu and Grefenstette (2004) is basically a sub-
string unigram probability, which we refer to as 
the n-gram (n-character) sum model (SUM) in this 
paper. Suppose that we have the unigram count 
1 2( )i i iC c c c? ? for character substring 1 2i i ic c c? ? , the 
unigram is then computed as: 
1 2
1 2
1 2
1 2,
( )
( )
( )
i i i
i i i
i i i
i i ii c c c
C c c c
p c c c
C c c c
? ?
? ?
? ?
? ?
= ?           (1) 
which is the count of character substring 1 2i i ic c c? ?  
normalized by the sum of all 3-character string 
counts in the name list for the language of interest.  
For origin recognition of Japanese names, this 
method works well with an accuracy of 92%. 
However, for English and Chinese, the results are 
far behind with a reported accuracy of 87% and 
70% respectively. 
2) N-gram Perplexity Method (PP): Li et al 
(2006) proposed using n-gram character perplexity 
cPP  to identify the origin of a Latin-scripted name. 
Using bigram, the cPP is defined as: 
1
1 log ( | )
2
Nc
i i 1ic
p c cN
cPP
?
=
? ?
=   (2) 
where cN is the total number of characters in the 
test name, ic is the i
th character in the test name. 
1( | )i ip c c ? is the bigram probability which is 
learned from each name list respectively. As a 
function of model, cPP  measures how good the 
model matches the test data. Therefore, cPP can be 
used to measure how good a test name matches a 
training set. A test name is identified to belong to 
a language if the language model gives rise to the 
minimum perplexity. Li et al (2006) shown that 
the PP method gives much better performance 
than the SUM method. This may be due to the fact 
that the PP measures the normalized conditional 
probability rather than the sum of joint probability. 
Thus, the PP method has a clearer mathematical 
interpretation than the SUM method. 
The statistical methods attempt to overcome the 
shortcoming of rule-based method, but they suffer 
from data sparseness, especially when dealing 
with a large character set, such as in Chinese (our 
experiments will demonstrate this point empiri-
cally). In this paper, we propose using Maximum 
Entropy (MaxEnt) model as a general framework 
for both ENOR and CNOR. We explore and inte-
grate multiple features into the discriminative clas-
sifier and use a common dataset for benchmarking. 
Experimental results show that the MaxEnt model 
effectively incorporates diverse features to demon-
strate competitive performance.   
3 MaxEnt Model and Features 
3.1 MaxEnt Model for NOR 
The principle of maximum entropy (MaxEnt) 
model is that given a collection of facts, choose a 
model consistent with all the facts, but otherwise 
as uniform as possible (Berger et al, 1996). Max-
Ent model is known to easily combine diverse fea-
tures. For this reason, it has been widely adopted 
in many natural language processing tasks. The 
MaxEnt model is defined as: 
( , )
1
1
( | ) j i
K
f c x
i j
j
p c x
Z
?
=
= ?           (3) 
      ( , )
1 1 1
( | ) j i
KN N
f c x
i j
i i j
Z p c x ?
= = =
= =? ??          (4) 
where ic is the outcome label, x is the given obser-
vation, also referred to as an instance. Z is a nor-
malization factor. N  is the number of outcome 
labels, the number of language origins  in our case. 
1 2, , , Kf f fL are feature functions and 
1 2, , , K? ? ?L are the model parameters. Each pa-
rameter corresponds to exactly one feature and can 
be viewed as a ?weight? for the corresponding fea-
ture.  
In the NOR task, c is the name origin label; x is 
a personal name, if is a feature function. All fea-
tures used in the MaxEnt model in this paper are 
binary. For example: 
 
1,    " "& (" ")
( , )
0,  j
if c Eng x contains str
f c x
otherwise
=?
= ??
 
In our implementation, we used Zhang?s maxi-
mum entropy package2. 
3.2 Features 
Let us use English name ?Smith? to illustrate the 
features that we define. All characters in a name 
                                                 
2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html 
58
  
are first converted into upper case for ENOR be-
fore feature extraction. 
N-gram Features: N-gram features are de-
signed to capture both phonetic and orthographic 
structure information for ENOR and orthographic 
information only for CNOR. This is motivated by 
the facts that: 1) names written in English but from 
non-English origins follow different phonetic rules 
from the English one; they also manifest different 
character usage in orthographic form; 2) names 
written in Chinese follows the same pronunciation 
rules (Pinyin), but the usage of Chinese characters 
is distinguishable between different language ori-
gins as reported in Table 2 of (Li et al, 2007).  
The N-gram related features include: 
1) FUni: character unigram <S, M, I, T, H> 
2) FBi: character bigram <SM, MI, IT, TH> 
3) FTri: character trigram <SMI, MIT, ITH > 
Position Specific n-gram Features: We in-
clude position information into the n-gram fea-
tures. This is mainly to differentiate surname from 
given name in recognizing the origin of CJK per-
sonal names written in Chinese. For example, the 
position specific n-gram features of a Chinese 
name ????(Wen-Jia-Bao)? are as follows: 
1) FPUni: position specific unigram  
<0?(Wen), 1?(Jia), 2?(Bao)> 
2) FPBi: position specific bigram  
<0??(Wen-Jia), 1??(Jia-Bao)> 
3) FPTri: position specific trigram  
<0???(Wen-Jia-Bao)> 
Phonetic Rule-based Features: These features 
are inspired by the rule-based methods (Kuo and 
Yang, 2004; Qu and Grefenstette, 2004) that check 
whether an English name is a sequence of sylla-
bles of CJK languages in ENOR task. We use the 
following two features in ENOR task as well. 
1) FMan: a Boolean feature to indicate 
whether a name is a sequence of Chinese 
Mandarin Pinyin.   
2) FCan: a Boolean feature to indicate whether 
a name is a sequence of Cantonese Jyutping. 
Other Features:  
1) FLen: the number of Chinese characters in a 
given name. This feature is for CNOR only.  
The numbers of Chinese characters in per-
sonal names vary with their origins. For ex-
ample, Chinese and Korean names usually 
consist of 2 to 3 Chinese characters while 
Japanese names can have up to 4 or 5 Chi-
nese characters 
2) FFre: the frequency of n-gram in a given 
name. This feature is for ENOR only. In 
CJK names, some consonants or vowels 
usually repeat in a name as the result of the 
regular syllable structure. For example, in 
the Chinese name ?Zhang Wanxiang?, the 
bigram ?an? appears three times 
Please note that the trigram and position spe-
cific trigram features are not used in CNOR due to 
anticipated data sparseness in CNOR3.  
4 Experiments 
We conduct the experiments to validate the effec-
tiveness of the proposed method for both ENOR 
and CNOR tasks. 
4.1 Experimental Setting 
 
Origin #  entries Romanization System 
Eng4 88,799 English 
Man5 115,879 Pinyin 
Can 115,739 Jyutping 
Jap6 123,239 Hepburn 
 
Table 1: DE: Latin-scripted personal name corpus for 
ENOR 
 
 
Origin #  entries 
Eng7 37,644 
Chi8 29,795 
Jap9 33,897 
 
Table 2: DC: Personal name corpus written in Chinese 
characters for CNOR 
 
                                                 
3 In the test set of CNOR, 1080 out of 2980 names of Chinese 
origin do not consist of any bigrams learnt from training data, 
while 2888 out of 2980 names do not consist of any learnt 
trigrams. This is not surprising as most of Chinese names only 
have two or three Chinese characters and in our open testing, 
the train set is exclusive of all entries in the test set.  
4 http://www.census.gov/genealogy/names/ 
5 http://technology.chtsai.org/namelist/  
6 http://www.csse.monash.edu.au/~jwb/enamdict_doc.html 
7 Xinhua News Agency (1992)  
8 http://www.ldc.upenn.edu LDC2005T34 
9 www.cjk.org 
59
  
Datasets: We prepare two data sets which are col-
lected from publicly accessible sources: DE and DC 
for the ENOR and CNOR experiment respectively. 
DE is the one used in (Li et al, 2006), consisting of 
personal names of Japanese (Jap), Chinese (Man), 
Cantonese (Can) and English (Eng) origins. DC 
consists of personal names of Japanese (Jap), Chi-
nese (Chi, including both Mandarin and Canton-
ese) and English (Eng) origins. Table 1 and Table 
2 list their details. In the experiments, 90% of en-
tries in Table 1 (DE) and Table 2 (DC) are ran-
domly selected for training and the remaining 10% 
are kept for testing for each language origin. Col-
umns 2 and 3 in Tables 7 and 8 list the numbers of 
entries in the training and test sets.  
 
Evaluation Methods: Accuracy is usually used to 
evaluate the recognition performance (Qu and 
Gregory, 2004; Li et al, 2006; Li et al, 2007). 
However, as we know, the individual accuracy 
used before only reflects the performance of recall 
and does not give a whole picture about a multi-
class classification task. Instead, we use precision 
(P), recall (R) and F-measure (F) to evaluate the 
performance of each origin. In addition, an overall 
accuracy (Acc) is also given to describe the whole 
performance. The P, R, F and Acc are calculated 
as following: 
 
#        
#          
correctly recognized entries of the given origin
P
entries recognized as the given origin by the system
=
 
 
#        
#      
correctly recognized entries of the given origin
R
entries of the given origin
=
 
 
2PR
F
P R
=
+
     #     
#   
all correctly recognized entries
Acc
all entries
=
 
4.2 Experimental Results and Analysis 
Table 3 reports the experimental results of ENOR. 
It shows that the MaxEnt approach achieves the 
best result of 98.44% in overall accuracy when 
combining all the diverse features as listed in Sub-
section 3.2. Table 3 also measures the contribu-
tions of different features for ENOR by gradually 
incorporating the feature set. It shows that:  
1) All individual features are useful since the 
performance increases consistently when 
more features are being introduced. 
2) Bigram feature presents the most informa-
tive feature that gives rise to the highest 
performance gain, while the trigram feature  
further boosts performance too. 
3) MaxEnt method can integrate the advan-
tages of previous rule-based and statistical 
methods and easily integrate other features. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
    
R(
%)
 
F 
Ac
c(%
) 
Eng 91.40 80.76 85.75
Man 83.05 81.90 82.47
Can 81.13 82.76 81.94
FUni 
Jap 87.31 94.11 90.58
85.29
Eng 97.54 91.10 94.21
Man 97.51 98.10 97.81
Can 97.68 98.05 97.86
+FBi 
Jap 94.62 98.24 96.39
96.72
Eng 97.71 93.79 95.71
Man 98.94 99.37 99.16
Can 99.12 99.19 99.15
+FTri 
Jap 96.19 98.52 97.34
97.97
Eng 97.53 94.64 96.06
Man 99.21 99.43 99.32
Can 99.41 99.24 99.33
+FPUni 
Jap 96.48 98.49 97.47
98.16
Eng 97.68 94.98 96.31
Man 99.32 99.50 99.41
Can 99.53 99.34 99.44
+FPBi 
Jap 96.59 98.52 97.55
98.28
Eng 97.62 94.97 96.27
Man 99.34 99.58 99.46
Can 99.63 99.37 99.50
+FPTri 
Jap 96.61 98.45 97.52
98.30
Eng 97.74 95.06 96.38
Man 99.37 99.59 99.48
Can 99.61 99.41 99.51
+FFre 
Jap 96.66 98.56 97.60
98.35
Eng 97.82 95.11 96.45
Man 99.52 99.68 99.60
Can 99.71 99.59 99.65
 + FMan 
+ FCan 
Jap 96.69 98.59 97.63
98.44
 
Table 3: Contribution of each feature for ENOR 
 
 
60
  
Features Eng Jap Man Can 
FMan -0.357 0.069 0.072 -0.709 
FCan -0.424 -0.062 -0.775 0.066 
 
Table 4: Features weights in ENOR task. 
 
F
ea
tu
re
 
O
ri
gi
n 
P(
%
) 
R(
%
) 
F 
   A
cc(
%
) 
Eng 97.89 98.43 98.16
Chi 95.80 95.03 95.42FUni 
Jap 96.96 97.05 97.00
96.97 
Eng 96.99 98.27 97.63
Chi 96.86 92.11 94.43+FBi 
Jap 95.04 97.73 96.36
96.28 
Eng 97.35 98.38 97.86
Chi 97.29 95.00 96.13+FLen 
Jap 96.78 97.64 97.21
97.14 
Eng 97.74 98.65 98.19
Chi 97.65 96.34 96.99+FPUni 
Jap 97.91 98.05 97.98
97.77 
Eng 97.50 98.43 97.96
Chi 97.61 96.04 96.82+FPBi 
Jap 97.59 97.94 97.76
97.56 
Eng 98.08 99.04 98.56
Chi 97.57 96.88 97.22
FUni 
+FLen 
+ 
FPUni Jap 98.58 98.11 98.34
98.10 
 
Table 5: Contribution of each feature for CNOR 
 
Table 4 reports the feature weights of two fea-
tures ?FMan? and ?FCan? with regard to different 
origins in ENOR task. It shows that ?FCan? has 
positive weight only for origin ?Can? while 
?FMan? has positive weights for both origins 
?Man? and ?Jap?, although the weight for ?Man? 
is higher. This agrees with our observation that the 
two features favor origins ?Man? or ?Can?. The 
feature weights also reflect the fact that some 
Japanese names can be successfully parsed by the 
Chinese Mandarin Pinyin system due to their simi-
lar syllable structure. For example, the Japanese 
name ?Tanaka Miho? is also a sequence of Chi-
nese Pinyin: ?Ta-na-ka Mi-ho?.  
Table 5 reports the contributions of different 
features in CNOR task by gradually incorporating 
the feature set. It shows that:  
1) Unigram features are the most informative 
2) Bigram features degrade performance. This 
is largely due to the data sparseness prob-
lem as discussed in Section 3.2.   
3) FLen is also useful that confirms our intui-
tion about name length. 
Finally the combination of the above three use-
ful features achieves the best performance of 
98.10% in overall accuracy for CNOR as in the 
last row of Table 5. 
In Tables 3 and 5, the effectiveness of each fea-
ture may be affected by the order in which the fea-
tures are incorporated, i.e., the features that are 
added at a later stage may be underestimated. 
Thus, we conduct another experiment using "all-
but-one" strategy to further examine the effective-
ness of each kind of features. Each time, one type 
of the n-gram (n=1, 2, 3) features (including or-
thographic n-gram, position-specific and n-gram 
frequency features) is removed from the whole 
feature set. The results are shown in Table 6. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
 
R(
%)
 
F 
Ac
c(%
) 
Eng 97.81 95.01 96.39
Man 99.41 99.58 99.49
Can 99.53 99.48 99.50
w/o 
Uni-
gram 
Jap 96.63 98.52 97.57
98.34
Eng 97.34 95.17 96.24
Man 99.30 99.48 99.39
Can 99.54 99.33 99.43
w/o Bi-
gram 
Jap 96.73 98.32 97.52
98.26
Eng 97.57 94.10 95.80
Man 98.98 99.23 99.10
Can 99.20 99.08 99.14
w/o 
Tri-
gram 
Jap 96.06 98.42 97.23
97.94
 
Table 6: Effect of n-gram feature for ENOR 
 
Table 6 reveals that removing trigram features 
affects the performance most. This suggests that 
trigram features are much more effective for 
ENOR than other two types of features. It also 
shows that trigram features in ENOR does not suf-
fer from the data sparseness issue. 
As observed in Table 5, in CNOR task, 93.96% 
61
  
accuracy is obtained when removing unigram fea-
tures, which is much lower than 98.10% when bi-
gram features are removed. This suggests that uni-
gram features are very useful in CNOR, which is 
mainly due to the data sparseness problem that 
bigram features may have encountered. 
4.3 Model Complexity and Data Sparseness 
Table 7 (ENOR) and Table 8 (CNOR) compare 
our MaxEnt model with the SUM model (Qu and 
Gregory, 2004) and the PP model (Li et al, 2006). 
All the experiments are conducted on the same 
data sets as described in section 4.1. Tables 7 and 
8 show that the proposed MaxEnt model outper-
forms other models. The results are statistically 
significant ( 2? test with p<0.01) and consistent 
across all tests. 
Model Complexity: 
We look into the complexity of the models and 
their effects. Tables 7 and 8 summarize the overall 
accuracy of three models. Table 9 reports the 
numbers of parameters in each of the models. We 
are especially interested in a comparison between 
the MaxEnt and PP models because their perform-
ance is close.  We observe that, using trigram fea-
tures, the MaxEnt model has many more parame-
ters than the PP model does. Therefore, it is not 
surprising if the MaxEnt model outperforms when 
more training data are available. However, the ex-
periment results also show that the MaxEnt model 
consistently outperforms the PP model even with 
the same size of training data. This is largely at-
tributed to the fact that MaxEnt incorporates more 
robust features than the PP model does, such as 
rule-based, length of names features.  
One also notices that PP clearly outperforms 
SUM by using the same number of parameters in 
ENOR and shows comparable performance in 
CNOR tasks. Note that SUM and PP are different 
in two areas: one is the PP model employs word 
length normalization while SUM doesn?t; another 
that the PP model uses n-gram conditional prob-
ability while SUM uses n-character joint probabil-
ity. We believe that the improved performance of 
PP model can be attributed to the effect of usage 
of conditional probability, rather than length nor-
malization since length normalization does not 
change the order of probabilities. 
Data Sparesness: 
We understand that we can only assess the ef-
fectiveness of a feature when sufficient statistics is 
available. In CNOR (see Table 8), we note that the 
Chinese transliterations of English origin use only 
377 Chinese characters, so data sparseness is not a 
big issue. Therefore, bigram SUM and bigram PP 
methods easily achieve good performance for Eng-
lish origin. However, for Japanese origin (repre-
sented by 1413 Chinese characters) and Chinese 
origin (represented by 2319 Chinese characters), 
the data sparseness becomes acute and causes per-
formance degradation in SUM and PP models. We 
are glad to find that MaxEnt still maintains a good 
performance benefiting from other robust features. 
Table 10 compares the overall accuracy of the 
three methods using unigram and bigram features 
in CNOR task, respectively. It shows that the 
MaxEnt method achieves best performance. An-
other interesting finding is that unigram features 
perform better than bigram features for PP and  
MaxEnt models, which shows that  data sparseness 
remains an issue even for MaxEnt model.  
5 Conclusion 
We propose using MaxEnt model to explore di-
verse features for name origin recognition. Ex-
periment results show that our method is more ef-
fective than previously reported methods. Our 
contributions include: 
1) Cast the name origin recognition problem as 
a multi-class classification task and propose 
a MaxEnt solution to it; 
2) Explore and integrate diverse features for 
name origin recognition and propose the 
most effective feature sets for ENOR and 
for CNOR 
In the future, we hope to integrate our name 
origin recognition method with a machine translit-
eration engine to further improve transliteration 
performance. We also hope to study the issue of 
name origin recognition in context of sentence and 
use contextual words as additional features. 
References 
Adam L. Berger, Stephen A. Della Pietra and Vincent J. 
Della Pietra. 1996. A Maximum Entropy Approach 
to Natural Language Processing. Computational Lin-
guistics. 22(1):39?71. 
William B. Cavnar and John M. Trenkle. 1994. Ngram 
based text categorization. In 3rd Annual Symposium 
62
  
on Document Analysis and Information Retrieval, 
275?282. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics. 24(4), 
599-612. 
Jin-Shea Kuo and Ying-Kuei Yan. 2004. Generating 
Paired Transliterated-Cognates Using Multiple Pro-
nunciation Characteristics from Web Corpora. PA-
CLIC 18, December 8th-10th, Waseda University, 
Tokyo, Japan, 275?282. 
Haizhou Li, Shuanhu Bai and Jin-Shea Kuo. 2006. 
Transliteration. Advances in Chinese Spoken Lan-
guage Processing. World Scientific Publishing Com-
pany, USA, 341?364. 
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo and Minghui 
Dong. 2007. Semantic Transliteration of Personal 
Names. ACL-2007. 120?127. 
Xinhua News Agency. 1992. Chinese Transliteration of 
Foreign Personal Names. The Commercial Press  
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in 
Latin script via language identification and corpus 
validation. ACL-2004. 183?190. 
Min Zhang, Jian Su and Haizhou Li. 2004. Direct Or-
thographical Mapping for Machine Translation. 
COLING-2004. 716-722. 
 
Trigram SUM Trigram PP MaxEnt Origin # training 
entries 
# test 
entries P (%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 79,920 8,879 94.66 72.50 82.11 95.84 94.72 95.28 97.82 95.11 96.45
Man 104,291 11,588 86.79 94.87 90.65 98.99 98.33 98.66 99.52 99.68 99.60
Can 104,165 11,574 90.03 93.87 91.91 96.17 99.67 97.89 99.71 99.59 99.65
Jap 110,951 12,324 89.17 92.84 90.96 98.20 96.29 97.24 96.69 98.59 97.63
Overall Acc (%) 89.57 97.39 98.44 
Table 7: Benchmarking different methods in ENOR task 
Bigram SUM  Bigram PP  MaxEnt Origin # training 
entries 
# test 
entries P(%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 37,644 3,765 95.94 98.65 97.28 97.58 97.61 97.60 98.08 99.04 98.56 
Chi 29,795 2,980 96.26 87.35 91.59 95.10 87.35 91.06 97.57 96.88 97.22 
Jap 33,897 3,390 93.01 97.67 95.28 90.94 97.43 94.07 98.58 98.11 98.34 
Overall Acc (%) 95.00 94.53 98.10 
Table 8: Benchmarking different methods in CNOR task 
# of parameters for ENOR # of parameters for CNOR 
Methods 
Trigram Unigram Bigram 
MaxEnt  124,692 13,496  182,116 
PP 16,851 4,045 86,490 
SUM  16,851 4,045 86,490 
 
Table 9: Numbers of parameters used in different methods 
 
 SUM PP MaxEnt 
Unigram Features 90.55 97.09 98.10 
Bigram Features 95.00 94.53 97.56 
 
Table 10: Overall accuracy using unigram and bigram features in CNOR task 
63
Refinements in BTG-based Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace
Singapore 119613
{dyxiong, mzhang, aaiti}@i2r.a-star.edu.sg
Haitao Mi, Qun Liu and Shouxun Lin
Key Lab of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing China, 100080
{htmi, liuqun, sxlin}@ict.ac.cn
Abstract
Bracketing Transduction Grammar (BTG)
has been well studied and used in statistical
machine translation (SMT) with promising
results. However, there are two major issues
for BTG-based SMT. First, there is no effec-
tive mechanism available for predicting or-
ders between neighboring blocks in the orig-
inal BTG. Second, the computational cost is
high. In this paper, we introduce two re-
finements for BTG-based SMT to achieve
better reordering and higher-speed decod-
ing, which include (1) reordering heuristics
to prevent incorrect swapping and reduce
search space, and (2) special phrases with
tags to indicate sentence beginning and end-
ing. The two refinements are integrated into
a well-established BTG-based Chinese-to-
English SMT system that is trained on large-
scale parallel data. Experimental results on
the NIST MT-05 task show that the proposed
refinements contribute significant improve-
ment of 2% in BLEU score over the baseline
system.
1 Introduction
Bracket transduction grammar was proposed by Wu
(1995) and firstly employed in statistical machine
translation in (Wu, 1996). Because of its good trade-
off between efficiency and expressiveness, BTG re-
striction is widely used for reordering in SMT (Zens
et al, 2004). However, BTG restriction does not
provide a mechanism to predict final orders between
two neighboring blocks.
To solve this problem, Xiong et al (2006)
proposed an enhanced BTG with a maximum en-
tropy (MaxEnt) based reordering model (MEBTG).
MEBTG uses boundary words of bilingual phrases
as features to predict their orders. Xiong et
al. (2006) reported significant performance im-
provement on Chinese-English translation tasks in
two different domains when compared with both
Pharaoh (Koehn, 2004) and the original BTG us-
ing flat reordering. However, error analysis of the
translation output of Xiong et al (2006) reveals
that boundary words predict wrong swapping, espe-
cially for long phrases although the MaxEnt-based
reordering model shows better performance than
baseline reordering models.
Another big problem with BTG-based SMT is the
high computational cost. Huang et al (2005) re-
ported that the time complexity of BTG decoding
with m-gram language model is O(n3+4(m?1)). If a
4-gram language model is used (common in many
current SMT systems), the time complexity is as
high as O(n15). Therefore with this time complexity
translating long sentences is time-consuming even
with highly stringent pruning strategy.
To speed up BTG decoding, Huang et al (2005)
adapted the hook trick which changes the time
complexity from O(n3+4(m?1)) to O(n3+3(m?1)).
However, the implementation of the hook trick with
pruning is quite complicated. Another method to in-
crease decoding speed is cube pruning proposed by
Chiang (2007) which reduces search space signifi-
cantly.
In this paper, we propose two refinements to ad-
dress the two issues, including (1) reordering heuris-
505
tics to prevent incorrect swapping and reduce search
space using swapping window and punctuation re-
striction, and (2) phrases with special tags to indicate
beginning and ending of sentence. Experimental re-
sults show that both refinements improve the BLEU
score significantly on large-scale data.
The above refinements can be easily implemented
and integrated into a baseline BTG-based SMT sys-
tem. However, they are not specially designed for
BTG-based SMT and can also be easily integrated
into other systems with different underlying trans-
lation strategies, such as the state-of-the-art phrase-
based system (Koehn et al, 2007), syntax-based sys-
tems (Chiang et al, 2005; Marcu et al, 2006; Liu et
al., 2006).
The rest of the paper is organized as follows. In
section 2, we review briefly the core elements of
the baseline system. In section 3 we describe our
proposed refinements in detail. Section 4 presents
the evaluation results on Chinese-to-English trans-
lation based on these refinements as well as results
obtained in the NIST MT-06 evaluation exercise. Fi-
nally, we conclude our work in section 5.
2 The Baseline System
In this paper, we use Xiong et al (2006)?s sys-
tem Bruin as our baseline system. Their system has
three essential elements which are (1) a stochastic
BTG, whose rules are weighted using different fea-
tures in log-linear form, (2) a MaxEnt-based reorder-
ing model with features automatically learned from
bilingual training data, (3) a CKY-style decoder us-
ing beam search similar to that of Wu (1996). We
describe the first two components briefly below.
2.1 Model
The translation process is modeled using BTG rules
which are listed as follows
A ? [A1, A2] (1)
A ? ?A1, A2? (2)
A ? x/y (3)
The lexical rule (3) is used to translate source phrase
x into target phrase y and generate a block A. The
two rules (1) and (2) are used to merge two consec-
utive blocks into a single larger block in a straight or
inverted order.
To construct a stochastic BTG, we calculate rule
probabilities using the log-linear model (Och and
Ney, 2002). For the two merging rules (1) and (2),
the assigned probability Prm(A) is defined as fol-
lows
Prm(A) = ??? ? 4?LMpLM (A1,A2) (4)
where ?, the reordering score of block A1 and
A2, is calculated using the MaxEnt-based reordering
model (Xiong et al, 2006) described in the next sec-
tion, ?? is the weight of ?, and 4pLM (A1,A2) is the
increment of language model score of the two blocks
according to their final order, ?LM is its weight.
For the lexical rule (3), it is applied with a proba-
bility Prl(A)
Prl(A) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3
?plex(y|x)?4 ? exp(1)?5 ? exp(|y|)?6
?p?LMLM (y) (5)
where p(?) are the phrase translation probabilities
in both directions, plex(?) are the lexical translation
probabilities in both directions, exp(1) and exp(|y|)
are the phrase penalty and word penalty, respec-
tively and ?s are weights of features. These features
are commonly used in the state-of-the-art systems
(Koehn et al, 2005; Chiang et al, 2005).
2.2 MaxEnt-based Reordering Model
The MaxEnt-based reordering model is defined on
two consecutive blocks A1 and A2 together with
their order o ? {straight, inverted} according to
the maximum entropy framework.
? = p?(o|A1, A2) = exp(
?
i ?ihi(o,A1, A2))?
o exp(
?
i ?ihi(o,A1, A2))(6)
where the functions hi ? {0, 1} are model features
and ?i are weights of the model features trained au-
tomatically (Malouf, 2002).
There are three steps to train a MaxEnt-based re-
ordering model. First, we need to extract reordering
examples from unannotated bilingual data, then gen-
erate features from these examples and finally esti-
mate feature weights.
506
For extracting reordering examples, there are two
points worth mentioning:
1. In the extraction of useful reordering examples,
there is no length limitation over blocks com-
pared with extracting bilingual phrases.
2. When enumerating all combinations of neigh-
boring blocks, a good way to keep the number
of reordering examples acceptable is to extract
smallest blocks with the straight order while
largest blocks with the inverted order .
3 Refinements
In this section we describe two refinements men-
tioned above in detail. First, we present fine-
grained reordering heuristics using swapping win-
dow and punctuation restriction. Secondly, we inte-
grate special bilingual phrases with sentence begin-
ning/ending tags.
3.1 Reordering Heuristics
We conduct error analysis of the translation out-
put of the baseline system and observe that Bruin
sometimes incorrectly swaps two large neighboring
blocks on the target side. This happens frequently
when inverted order successfully challenges straight
order by the incorrect but strong support from the
language model and the MaxEnt-based reordering
model. The reason is that only boundary words
are used as evidences by both language model and
MaxEnt-based reordering model when the decoder
selects which merging rule (straight or inverted) to
be used 1. However, statistics show that bound-
ary words are not reliable for predicting the right
order between two larger neighboring blocks. Al-
Onaizan and Papineni (2006) also proved that lan-
guage model is insufficient to address long-distance
word reordering. If a wrong inverted order is se-
lected for two large consecutive blocks, incorrect
long-distance swapping happens.
Yet another finding is that many incorrect swap-
pings are related to punctuation marks. First, the
source sequence within a pair of balanced punctua-
tion marks (quotes and parentheses) should be kept
1In (Xiong et al, 2006), the language model uses the left-
most/rightmost words on the target side as evidences while the
MaxEnt-based reordering model uses the boundary words on
both sides.
Chinese: ?? : ??????????
?????????????????
????
Bruin: urgent action , he said : ?This is a very
serious situation , we can only hope that there
will be a possibility .?
Bruin+RH: he said : ?This is a very serious sit-
uation , we can only hope that there will be the
possibility to expedite action .?
Ref: He said: ?This is a very serious situa-
tion. We can only hope that it is possible to
speed up the operation.?
Figure 1: An example of incorrect long-distance
swap. The underlined Chinese words are incorrectly
swapped to the beginning of the sentence by the
original Bruin. RH means reordering heuristics.
within the punctuation after translation. However,
it is not always true when reordering is involved.
Sometime the punctuation marks are distorted with
the enclosed words sequences being moved out.
Secondly, it is found that a series of words is fre-
quently reordered from one side of a structural mark,
such as commas, semi-colons and colons, to the
other side of the mark for long sentences contain-
ing such marks. Generally speaking, on Chinese-
to-English translation, source words are translated
monotonously relative to their adjacent punctuation
marks, which means their order relative to punctua-
tion marks will not be changed. In summary, punctu-
ation marks place a strong constraint on word order
around them.
For example, in Figure 1, Chinese words ???
??? are reordered to sentence beginning. That is
an incorrect long-distance swapping, which makes
the reordered words moved out from the balanced
punctuation marks ??? and ???, and incorrectly
precede their previous mark ???.
These incorrect swappings definitely jeopardize
the quality of translation. Here we propose two
straightforward but effective heuristics to control
and adjust the reordering, namely swapping window
and punctuation restriction.
Swapping Window (SW): It constrains block
swapping in the following way
ACTIVATE A ? ?A1, A2? IF |A1s|+ |A2s| < sws
507
where |Ais| denotes the number of words on the
source side Ais of block Ai, sws is a pre-defined
swapping window size. Any inverted reordering be-
yond the pre-defined swapping window size is pro-
hibited.
Punctuation Restriction (PR): If two neighbor-
ing blocks include any of the punctuation marks p ?
{? ? ? ? ? ? ? ? ? ? ? ?}, the two
blocks will be merged with straight order.
Punctuation marks were already used in pars-
ing (Christine Doran, 2000) and statistical machine
translation (Och et al, 2003). In (Och et al,
2003), three kinds of features are defined, all re-
lated to punctuation marks like quotes, parentheses
and commas. Unfortunately, no statistically signifi-
cant improvement on the BLEU score was reported
in (Och et al, 2003). In this paper, we consider
this problem from a different perspective. We em-
phasize that words around punctuation marks are
reordered ungrammatically and therefore we posi-
tively use punctuation marks as a hard decision to
restrict such reordering around punctuations. This
is straightforward but yet results in significant im-
provement on translation quality.
The two heuristics described above can be used
together. If the following conditions are satisfied,
we can activate the inverted rule:
|A1s|+ |A2s| < sws && P
?
(A1s
?
A2s) = ?
where P is the set of punctuation marks mentioned
above.
The two heuristics can also speed up decoding be-
cause decoding will be monotone within those spans
which are not in accordance with both heuristics.
For a sentence with n words, the total number of
spans is O(n2). If we set sws = m (m < n),
then the number of spans with monotone search is
O((n?m)2). With punctuation restriction, the non-
monotone search space will reduce further.
3.2 Phrases with Sentence Beginning/Ending
Tags
We observe that in a sentence some phrases are more
likely to be located at the beginning, while other
phrases are more likely to be at the end. This kind of
location information with regard to the phrase posi-
tion could be used for reordering. A straightforward
way to use this information is to mark the begin-
ning and ending of word-aligned sentences with ?s?
and ?/s? respectively. This idea is borrowed from
language modeling (Stolcke, 2002). The corre-
sponding tags at the source and target sentences are
aligned to each other, i.e, the beginning tag of source
sentences is aligned to the beginning tag of target
sentences, similarly for the ending tag. Figure 2
shows a word-aligned sentence pair annotated with
the sentence beginning and ending tag.
During training, the sentence beginning and end-
ing tags (?s? and ?/s?) are treated as words. There-
fore the phrase extraction and MaxEnt-based re-
ordering training algorithm need not to be modified.
Phrases with the sentence beginning/ending tag will
be extracted and MaxEnt-based reordering features
with such tags will also be generated. For example,
from the word-aligned sentence pair in Figure 2, we
can extract tagged phrases like
?s??? ||| ?s? Tibet ?s
?? ?/s? ||| achievements ?/s?
and generate MaxEnt-based reordering features with
tags like
hi(o, b1, b2) =
{ 1, b2.t1 = ?/s?, o = s
0, otherwise
where b1, b2 are blocks, t1 denotes the last source
word, o = s means the order between two blocks
is straight. To avoid wrong alignments, we remove
tagged phrases where only the beginning/ending tag
is extracted on either side of the phrases, such as
?s? ||| ?s? Those?
?/s? ||| ?/s?
During decoding, we first annotate source sen-
tences with the beiginning/ending tags, then trans-
late them as what Bruin does. Note that phrases
with sentence beginning/ending tags will be used in
the same way as ordinary phrases without such tags
during decoding. With the additional support of lan-
guage model and MaxEnt-based reordering model,
we observe that phrases with such tags are always
moved to the beginning or ending of sentences cor-
rectly.
508
?s? ?? ?? ?? ?? ?? ?? ?/s?
?s? Tibet ?s financial work has gained remarkable achievements ?/s?
Figure 2: A word-aligned sentence pair annotated with the sentence beginning and ending tag.
4 Evaluation
In this section, we report the performance of the en-
hanced Bruin on the NIST MT-05 and NIST MT-06
Chinese-to-English translation tasks. We describe
the corpus, model training, and experiments related
to the refinements described above.
4.1 Corpus
The bilingual training data is derived from the fol-
lowing various sources: the FBIS (LDC2003E14),
Hong Kong Parallel Text (Hong Kong News and
Hong Kong Hansards, LDC2004T08), Xinhua News
(LDC2002E18), Chinese News Translation Text
Part1 (LDC2005T06), Translations from the Chi-
nese Treebank (LDC2003E07), Chinese English
News Magazine (LDC2005E47). It contains 2.4M
sentence pairs in total (68.1M Chinese words and
73.8M English words).
For the efficiency of minimum-error-rate training,
we built our development set using sentences not ex-
ceeding 50 characters from the NIST MT-02 evalu-
ation test data (580 sentences).
4.2 Training
We use exactly the same way and configuration de-
scribed in (He et al, 2006) to preprocess the training
data, align words and extract phrases.
We built two four-gram language models using
Xinhua section of the English Gigaword corpus
(181.1M words) and the English side of the bilin-
gual training data described above respectively. We
applied modified Kneser-Ney smoothing as imple-
mented in the SRILM toolkit (Stolcke, 2002).
The MaxEnt-based reordering model is trained
using the way of (Xiong et al, 2006). The difference
is that we only use lexical features generated by tail
words of blocks, instead of head words, removing
features generated by the combination of two bound-
ary words.
Bleu(%) Secs/sent
Bruin 29.96 54.3
sws RH1 RH12 RH1 RH12
5 29.65 29.95 42.6 41.2
10 30.55 31.27 46.2 41.8
15 30.26 31.40 48.0 42.2
20 30.19 31.42 49.1 43.2
Table 1: Effect of reordering heuristics. RH1 de-
notes swapping window while RH12 denotes swap-
ping window with the addition of punctuation re-
striction.
4.3 Translation Results
Table 1 compares the BLEU scores 2 and the speed
in seconds/sentence of the baseline system Bruin
and the enhanced system with reordering heuristics
applied. The second row gives the BLEU score and
the average decoding time of Bruin. The rows be-
low row 3 show the BLEU scores and speed of the
enhanced Bruin with different combinations of re-
ordering heuristics. We can clearly see that the re-
ordering heuristics proposed by us have a two-fold
effect on the performance: improving the BLEU
score and decreasing the average decoding time.
The example in Figure 1 shows how reordering
heuristics prevent incorrect long-distance swapping
which is not in accordance with the punctuation re-
striction.
Table 1 also shows that a 15-word swapping win-
dow is an inflexion point with the best tradeoff be-
tween the decoding time and the BLEU score. We
speculate that in our corpus most reorderings hap-
pen within a 15-word window. We use the FBIS
corpus to testify this hypothesis. In this corpus, we
extract all reordering examples using the algorithm
of Xiong et al (2006). Figure 3 shows the reorder-
ing length distribution curve in this corpus. Accord-
2In this paper, all BLEU scores are case-sensitive and evalu-
ated on the NIST MT-05 Chinese-to-English translation task if
there is no special note.
509
0 10 20 30 40 50 60 70 80
0
5
10
15
20
25
Pe
rce
nt 
(%
)
Reordering Length
Figure 3: Reordering length distribution. The hor-
izontal axis (reordering length) indicates the num-
ber of words on the source side of two neighboring
blocks which are to be swapped. The vertical axis
represents what proportion of reorderings with a cer-
tain length is likely to be in all reordering examples
with an inverted order.
Bleu(%)
Without Special Phrases 31.40
With Special Phrases 32.01
Table 2: Effect of integrating special phrases with
the sentence beginning/ending tag.
ing to our statistics, reorderings within a window
not exceeding 15 words have a very high proportion,
97.29%. Therefore we set sws = 15 for later exper-
iments.
Table 2 shows the effect of integrating special
phrases with sentence beginning/ending tags into
Bruin. As special phrases accounts for only 1.95%
of the total phrases used, an improvement of 0.6%
in BLEU score is well worthwhile. Further, the im-
provement is statistically significant at the 99% con-
fidence level according to Zhang?s significant tester
(Zhang et al, 2004). Figure 4 shows several exam-
ples translated with special phrases integrated. We
can see that phrases with sentence beginning/ending
tags are correctly selected and located at the right
place.
Table 3 shows the performance of two systems on
the NIST MT-05 Chinese test data, which are (1)
System Refine MT-05 MT-06
Bruin - 29.96 -
EBruin RH 31.40 30.22
EBruin RH+SP 32.01 -
Table 3: Results of different systems. The refine-
ments RH, SP represent reordering heuristics and
special phrases with the sentence beginning/ending
tag, respectively.
Bruin, trained on the large data described above; and
(2) enhanced Bruin (EBruin) with different refine-
ments trained on the same data set. This table also
shows the evaluation result of the enhanced Bruin
with reordering heuristics, obtained in the NIST MT-
06 evaluation exercise. 3
5 Conclusions
We have described in detail two refinements for
BTG-based SMT which include reordering heuris-
tics and special phrases with tags. The refinements
were integrated into a well-established BTG-based
system Bruin introduced by Xiong et al (2006). Re-
ordering heuristics proposed here achieve a twofold
improvement: better reordering and higher-speed
decoding. To our best knowledge, we are the first
to integrate special phrases with the sentence be-
ginning/ending tag into SMT. Experimental results
show that the above refinements improve the base-
line system significantly.
For further improvements, we will investigate
possible extensions to the BTG grammars, e.g.
learning useful nonterminals using unsupervised
learning algorithm.
Acknowledgements
We would like to thank the anonymous review-
ers for useful comments on the earlier version of
this paper. The first author was partially sup-
ported by the National Science Foundations of
China (No. 60573188) and the High Technology
Research and Development Program of China (No.
2006AA010108) while he studied in the Institute of
Computing Technology, Chinese Academy of Sci-
ences.
3Full results are available at http://www.nist.gov/
speech/tests/mt/doc/mt06eval official results.html.
510
With Special Phrases Without Special Phrases
?s? Japan had already pledged to provide 30 mil-
lion US dollars of aid due to the tsunami victims of
the country . ?/s?
originally has pledged to provide 30 million US
dollars of aid from Japan tsunami victimized coun-
tries .
?s? the results of the survey is based on the re-
sults of the chiefs of the Ukrainian National 50.96%
cast by chiefs . ?/s?
is based on the survey findings Ukraine 50.96% cast
by the chiefs of the chiefs of the country .
?s? and at the same time , the focus of the world have
been transferred to other areas . ?/s?
and at the same time , the global focus has shifted
he.
Figure 4: Examples translated with special phrases integrated. The bold underlined words are special phrases
with the sentence beginning/ending tag.
References
Yaser Al-Onaizan, Kishore Papineni. 2006. Distortion
Models for Statistical Machine Translation. In Pro-
ceedings of ACL-COLING 2006.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, Michael Subotin. 2005. The
Hiero Machine Translation System: Extensions, Eval-
uation, and Analysis. In Proceedings of HLT/EMNLP,
pages 779?786, Vancouver, October 2005.
David Chiang. 2007. Hierarchical Phrase-based Transla-
tion. In computational linguistics, 33(2).
Christine Doran. 2000. Punctuation in a Lexicalized
Grammar. In Proceedings of Workshop TAG+5, Paris.
Zhongjun He, Yang Liu, Deyi Xiong, Hongxu Hou, Qun
Liu. 2006. ICT System Description for the 2006
TC-STAR Run #2 SLT Evaluation. In Proceedings of
TC-STAR Workshop on Speech-to-Speech Translation,
Barcelona, Spain.
Liang Huang, Hao Zhang and Daniel Gildea. 2005. Ma-
chine Translation as Lexicalized Parsing with Hooks.
In Proceedings of the 9th International Workshop
on Parsing Technologies (IWPT-05), Vancouver, BC,
Canada, October 2005.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115?124.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL
2007, demonstration session, Prague, Czech Republic,
June 2007.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. In Proceedings of ACL-COLING 2006.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of CoNLL-2002.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phraases. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, Dragomir Radev. 2003. Final
Report of Johns Hopkins 2003 Summer Workshop on
Syntax for Statistical Machine Translation.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of IJCAL 1995, pages 1328-1334, Montreal, August.
511
Dekai Wu. 1996. A Polynomial-Time Algorithm for Sta-
tistical Machine Translation. In Proceedings of ACL
1996.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for Sta-
tistical Machine Translation. In Proceedings of ACL-
COLING 2006, pages 521?528.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Re-
ordering Constraints for Phrase-Based Statistical Ma-
chine Translation. In Proceedings of CoLing 2004,
Geneva, Switzerland, pp. 205-211.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of LREC 2004, pages 2051? 2054.
512
Term Extraction Through Unithood And Termhood Unification 
Thuy VU, Ai Ti AW, Min ZHANG  
Department of Language Technology, Institute for Infocomm Research 
21 Heng Mui Keng Terrace, Singapore 119613 
{tvu, aaiti, mzhang}@i2r.a-star.edu.sg 
Abstract 
Term Extraction (TE) is an important com-
ponent of many NLP applications. In gen-
eral, terms are extracted for a given text 
collection based on global context and fre-
quency analysis on words/phrases associa-
tion. These extracted terms represent effec-
tively the text content of the collection for 
knowledge elicitation tasks. However, they 
fail to dictate the local contextual informa-
tion for each document effectively. In this 
paper, we refine the state-of-the-art C/NC-
Value term weighting method by consider-
ing both termhood and unithood measures, 
and use the former extracted terms to direct 
the local term extraction for each document. 
We performed the experiments on Straits 
Times year 2006 corpus and evaluated our 
performance using Wikipedia termbank.  
The experiments showed that our model 
outperforms C/NC-Value method for global 
term extraction by 24.4% based on term 
ranking. The precision for local term ex-
traction improves by 12% when compared 
to pure linguistic based extraction method. 
1 Introduction 
Terminology Extraction (TE) is a subtask of in-
formation extraction. The goal of TE is to auto-
matically extract relevant terms from a given cor-
pus. These extracted terms are used in a variety of 
NLP tasks such as information retrieval, text min-
ing, document summarization etc. In our applica-
tion scenario, we are interested in terms whose 
constituent words have strong collocation relations 
and can be translated to another language in stable 
single word or multi-word translation equivalents. 
Thus, we define ?term? as a word/phrase that car-
ries a special meaning. 
A general TE consists of two steps. The first 
step makes use of various degrees of linguistic fil-
tering (e.g., part-of-speech tagging, phrase chunk-
ing etc.), through which candidates of various lin-
guistic patterns are identified (e.g. noun-noun, ad-
jective-noun-noun combinations etc.). The second 
step involves the use of frequency- or statistical-
based evidence measures to compute weights indi-
cating to what degree a candidate qualifies as a 
terminological unit. There are many methods in 
literature trying to improve this second step. Some 
of them borrowed the metrics from Information 
Retrieval to evaluate how important a term is 
within a document or a corpus. Those metrics are 
Term Frequency/Inverse Document Frequency 
(TF/IDF), Mutual Information, T-Score, Cosine, 
and Information Gain. There are also other works 
(Nakagawa and Mori, 2002; Frantzi and 
Ananiadou, 1998) that introduced better method to 
weigh the term candidates. 
Currently, the C/NC method (Frantzi and 
Ananiadou, 1998) is widely considered as the 
state-of-the-art model for TE. Although this 
method was first applied on English, it also per-
formed well on other languages such as Japanese 
(Hideki Mima and Sophia Ananiadou, 2001), Slo-
vene (?pela Vintar, 2004), and other domains such 
as medical corpus (Frantzi and Ananiadou, 1998), 
and computer science (E. Milios et al 2003). 
In terminology research, a term is evaluated us-
ing two types of feature: termhood1 and unithood 
                                                 
1 Termhood refers to a degree of linguistic unit. It considers a 
term as a linguistic unit representative for the document con-
tent. 
631
2(Kyo Kageura, 1996). In C/NC method, the fea-
tures used to compute the term weight are based on 
termhood only. In this paper, we introduce a uni-
thood feature, T-Score, to the C/NC method. Ex-
periment results show that by incorporating T-
Score into C/NC to derive a new weight, 
NTCValue , it gives a better ranking of the global 
terms and outperforms C/NC method by 24.4%. 
On the other hand, C/NC method extracts term 
candidates using linguistic patterns and derives 
their weights based on distribution of terms over 
all documents. The extracted terms thus represent 
global content of the corpus, and do not represent 
well the contextual information for each individual 
document. So, we propose a method to enrich the 
local terms through a Term Re-Extraction Model 
(TREM). Experiment results show that the preci-
sion for local TE has been improved significantly, 
by 12% when compared to pure linguistic based 
extraction method. 
In the following sections, we introduce the state-
of-the-art method, the C/NC Value method. We 
then introduce our proposed methods, the 
NTCValue method on section 3, the Term Re-
Extraction Model (TREM) on section 4 followed 
by the experiment results and conclusion. 
2 The C/NC value Method 
C/NC method uses a combination of linguistic and 
statistical information to evaluate the weight of a 
term. This method has two steps: candidate 
extraction and term weighting by C/NC value. 
2.1 Term Candidate Extraction 
This method uses 3 linguistic patterns to extract the 
term candidates: 
? (Noun+Noun);  
? (Adj|Noun)+Noun;  
? (Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|
Noun)*)Noun. 
The term candidates are passed to the second step. 
2.2 Term Weighting 
2.2.1 CValue 
CValue  is calculated based on the frequency of 
term and its subterms. 
                                                 
2 Unithood refers to a degree of strength or stability of syn-
tagmatic combinations or collocations. 
( ) ( ) ( ) ( )???
?
???
? ??= ?
? aTba
bf
TP
afaaCValue
1
log2  
Where, ( )af  is the frequency of term a  with a  
words, aT  is the set of extracted candidate terms that 
contain a  and ( )aTP  is the total number of longer 
candidate terms that contain a . The for-
mula ( ) ( )?? aTba bfTP
1
 will have value 0 when aT is 
empty. 
2.2.2 NC Value 
NCValue combines the context information of 
a term together with the CValue. The weight of a 
context word3 b is defined by the number of terms ( )bt in which it appears over the total number of 
terms considered, n . aC  is the set of distinct con-
text words and ( )bfa is the frequency of b  as con-
text word of a . 
( ) ( )
n
bt
bweight =  
( ) ( )?
?
?=
aCb
a bweightbfNValue  
( ) ( ) ( )aNValueaCValueaNCValue ?+?= 2.08.0
 
From the above formula, we find that 
NCValue is mainly weighted by CValue .It treats 
the term candidate as a linguistic unit and evaluates 
its weight based on characteristics of the termhood, 
i.e. frequency and context word of the term candi-
date. The performance can be improved if feature 
measuring the adhesion of words within the term is 
incorporated.  
3 Enhancement on Global TE: the 
NTCValue 
Theoretically, the C/NC method can be improved 
by adding unithood feature to the term weighting 
formula. Based on the comparison of (Evert, S and 
B. Krenn, 2001), we explore T-Score, a 
competitive metric to evaluate the association 
between two words, as a unithood feature. 
                                                 
3 All experiments in this paper use the length of context is 3. 
632
3.1 T-Score 
The T-Score is used to measure the adhesion 
between two words in a corpus. It is defined by the 
following formula (Manning and Schuetze, 1999): 
( ) ( ) ( ) ( )( )
N
wwP
wPwPwwP
wwTS
ji
jiji
ji ,
.,
,
?=  
Where, ( )ji wwP , is the probability of bi-gram 
jiww  in the corpus, ( )wP  is the probability of 
word w  in the corpus, and N  is the total number 
of words in the corpus. The adhesion is a type of 
unithood feature since it is used to evaluate the 
intrinsic strength between two words of a term. 
3.2 Incorporate T-Score within C/NC value 
As discussed in 2.2, the most influential feature in 
the C/NC method is the term frequency. Our idea 
here is to combine the frequency with T-Score, a 
unithood feature. Taking the example in Table 1, 
the candidates have similar rank in the output using 
C/NC termhood approach. 
 
massive tidal waves 
gigantic tidal waves 
killer tsunami tidal waves 
deadly tidal waves 
huge tidal waves 
giant tidal waves 
tsunamis tidal waves  
Table 1. Example of similar terms 4 
To give better ranking and differentiation, we 
introduce T-Score to measure the adhesion be-
tween the words within the term. We use the 
minimum T-Score of all bi-grams in term a , ( )aTSmin , as a weighted parameter for the term 
besides the term frequency. For a 
term nwwwa .... 21= , the ( )aTSmin  is defined as: ( ) ( ){ } ( )1...1,,minmin 1 ?== + niwwTSaTS ii  
Term ( )?TSmin
massive tidal waves 4.56 
gigantic tidal waves 2.44 
killer tsunami tidal waves 3.99 
deadly tidal waves 3.15 
huge tidal waves 2.20 
                                                 
4 The italic means a week adhesion. 
giant tidal waves 1.35 
tsunamis tidal waves  5.06 
Table 2. Term with Minimum T-Score value 
Table 2 shows the ( )aTSmin  of the different 
terms in table 1. Since ( )aTSmin can have a nega-
tive value, we only considered those terms with ( ) 0min >aTS  and combined it with the term fre-
quency. We redefine CValue to TCValue by re-
placing ( )af  using ( )aF , as follows: 
( ) ( ) ( )( ) ( )( ) ( )??
?
>+?
?=
0minifmin2ln
0minif
aTSaTSaf
aTSaf
aF
( ) ( ) ( ) ( )???
?
???
? ??= ?
? aTba
bF
TP
aFaaTCValue
1
log2  
The final weight, defined as NTCValue, is com-
puted using the same parameter as NCValue .  ( ) ( ) ( )aNValueaTCValueaNTCValue ?+?= 2.08.0
 
4 Enhancement on Local Terms: Term 
Re-Extraction Method (TREM) 
The extracted term candidates are ranked globally 
with best global terms promoted due to their dis-
tinguishing power. However, preliminary investi-
gation on using linguistic patterns for extracting 
global term candidates for identifying term candi-
dates of each document does not perform satisfac-
tory, as high rank global terms do not reconcile 
well with the local term candidates identified using 
the linguistic patterns. A re-extraction process is 
thus evolved to derive local terms of a document 
from global terms using the NTCValue of the 
global terms. 
4.1 Local Term Candidate Extraction 
A string (or term candidate) extracted based on 
linguistic pattern follows the maximum matching 
algorithm. As long as the longest string whose 
part-of-speech tag satisfies the linguistic pattern, it 
will be extracted. For this reason, some noises are 
extracted together with these candidates. Table 3 
shows some examples of noisy term candidates.  
Strait Times yesterday 
THE World Cup 
gross domestic product growth forecast 
senior vice-president of DBS Vickers security 
on-line 
Table 3. Examples of noisy candidates. 
633
Our intention here is to reduce the noise and also 
mine more good terms embedded within the noise 
by using the global terms. We favor recall over 
precision to get as many local terms as possible. 
The examples in table 3 show the problem in de-
tecting term candidate?s boundary using linguistic 
patterns. The ?Strait Times yesterday? is a bad 
term identified by linguistic patterns because all 
three words are tagged as ?noun?. The second one 
is caused by an error of the POS tagger. Because of 
capitalization, the word ?THE? is being tagged 
wrongly as a ?proper-noun? (NNP/NNPS), and not 
determiner (DT). Similarly, ?gross domestic prod-
uct growth forecast? and ?senior vice-president of 
DBS Vickers security on-line? are complex noun-
phrases that are not symbolized good terms in the 
document. The more expressive terms would be 
?gross domestic product?, ?DBS Vickers security?, 
etc. 
Our proposed algorithm utilizes the term weight 
from section 3.2 to do term re-extraction for each 
document through dynamic programming theory 
(Viterbi algorithm) to resolve the above problem.  
4.2 Proposed algorithm 
The algorithm for term re-extraction is outlined 
in Figure 1.  
Algorithm: Term re-extraction for a document 
Input: L ? global term list with NTCValue  
  T ? input for TREM nwww ...T 21=  
1: For 2=i  ? n   
2:  If ( ) L...T 1,1 ?= ii ww  
3:   ( ) ( )iNTCiMaxNTC ,1T,1 =  
4:  Else ( ) 0,1 =iMaxNTC  
5:  End If 
6:  For 1=j  ? 1?i  
7:   If ( ) LT ?= ++ ijij ww ...1,1  
8:    ( ) max,1 =iMaxNTC  
( ) ( ) ( ){ }iMaxNTCNTCjMaxNTC ij ,1;,1 ,1++ T  
9:   End If 
10:  End For 
11: End For 
Output: Updated term list for a document 
Figure 1. Term Re-Extraction Algorithm 
 
 
Where, ji,T  is the word chain formed by the 
words from i  to j  of the term nwww ...T 21= ; ( )iMaxNTC ,1  is the maximum NTCValue value 
from 1 to i  of the term nwww ...T 21= ; and ( )iNTC ,1T  is the NTCValue of ji,T . 
5 Experiments and Evaluations 
5.1 Term Bank Collection 
Term boundary is one of the main issues in termi-
nology research. In our experiments, we consider a 
term based on the resources from Wikipedia. In 
each Wikipedia article, the editor annotated the key 
terminologies through the use of hyperlinks. We 
extracted the key terms for each article based on 
this markup. The entire Wikipedia contains about 
1,910,974 English articles and 8,964,590 key terms. 
These terms are considered as Wikipedia term-
bank and we use it to evaluate our performance. 
An extracted term is considered correct if and only 
if it is in the term-bank. 
5.2 Corpus Collection 
To evaluate the model, we use the corpus collected 
from Straits Times in year 2006. We separate the 
data into 12 months as showed in Table 4.  
Month Total articles Total words 
1 3,134 1,844,419 
2 3,151 1,824,970 
3 3,622 2,098,459 
4 3,369 1,969,684 
5 3,395 1,957,962 
6 3,187 1,781,664 
7 3,253 1,818,606 
8 3,497 1,927,180 
9 3,463 1,853,902 
10 3,499 1,870,417 
11 3,493 1,845,254 
12 3,175 1,711,168 
Table 4. Evaluation data from Straits Times. 
5.3 NTCValue Evaluation 
We evaluate the performance of global ranked 
terms using average-precision. A higher average-
precision would mean that the list contains more 
good terms in higher rank. The average precision ( ).PAve  of a term-list { }LtttL ,...,, 21=  with 
634
cL as the list of all correct terms in L  ( )LLc ? , is 
calculated by the following formula: 
( ) ? ?
?? ??
??
???
? ??
???
??=
Lk ki
ik
c
r
k
r
L
LP
1 1
11
Ave  
Where: 
??
?
?
?=
ci
ci
i Lt
Lt
r
0
1
 
Table 5 shows the comparison result of the ori-
gin NCValue  and our NTCValue  on the ranking 
of global terms. The experiment is conducted on 
the data described in section 5.2. We evaluate the 
performance based on 8 different levels of top 
ranking terms. 
Each cell in Table 5 contains a couple of ( ).PAve  for NCValue  and NTCValue  
( )NTCValueNCValue / respectively. The 
( ).PAve  decreases gradually when we relax the 
threshold for the evaluation . The result shows that 
the term ranking using NTCValue  improves the 
performance significantly. 
 
Number of top high term 01 02 03 04 05 06 
50 0.70/0.77 0.57/0.81 0.52/0.80 0.51/0.78 0.55/0.80 0.67/0.69
100 0.60/0.73 0.59/0.77 0.51/0.79 0.50/0.74 0.57/0.78 0.64/0.70
200 0.55/0.70 0.56/0.75 0.53/0.78 0.49/0.72 0.55/0.77 0.62/0.69
500 0.53/0.67 0.54/0.70 0.54/0.71 0.48/0.68 0.53/0.71 0.57/0.65
1000 0.51/0.62 0.52/0.66 0.52/0.66 0.47/0.64 0.51/0.65 0.53/0.60
5000 0.48/0.58 0.49/0.61 0.49/0.62 0.45/0.60 0.49/0.61 0.49/0.56
10000 0.43/0.52 0.44/0.55 0.44/0.56 0.42/0.54 0.44/0.56 0.44/0.50
All_terms 0.38/0.47 0.39/0.49 0.40/0.50 0.37/0.48 0.39/0.49 0.38/0.45
Number of top high term 07 08 09 10 11 12 
50 0.67/0.67 0.65/0.70 0.49/0.65 0.62/0.71 0.65/0.76 0.63/0.86
100 0.64/0.71 0.62/0.74 0.47/0.66 0.59/0.74 0.59/0.76 0.61/0.82
200 0.65/0.72 0.59/0.75 0.48/0.68 0.55/0.72 0.56/0.73 0.58/0.77
500 0.62/0.71 0.56/0.70 0.50/0.66 0.52/0.66 0.54/0.67 0.55/0.69
1000 0.59/0.66 0.54/0.66 0.50/0.64 0.49/0.64 0.51/0.64 0.54/0.65
5000 0.54/0.60 0.51/0.62 0.49/0.60 0.46/0.61 0.48/0.60 0.51/0.61
10000 0.46/0.53 0.46/0.55 0.45/0.55 0.43/0.56 0.44/0.55 0.46/0.55
All_terms 0.40/0.47 0.40/0.50 0.40/0.50 0.38/0.49 0.38/0.48 0.39/0.48
Table 5. Performance of NTCValue with C/NC value. 
 
Method Without TREM TREM+NC TREM+NTC 
Month Precision No. terms Precision No. terms Precision No. terms 
1  44.98  23915  50.81  34910  50.85  34998  
2  44.74  23772  50.22  34527  50.33  34657  
3  44.39  28772  49.58  41691  49.59  41778  
4  42.89  25857  48.78  38564  48.91  38589  
5  44.67  25787  50.44  38252  50.38  38347  
6  46.58  23293  51.80  33574  51.91  33651  
7  46.35  23638  51.31  33990  51.35  34041  
8  46.50  25869  51.91  37896  51.96  37973  
9  46.16  25276  51.34  36632  51.39  36731  
10  45.79  24987  50.99  36082  51.05  36179  
11  45.28  24661  50.43  35894  50.54  35906  
12  45.67  22745  50.73  32594  50.73  32673  
Table 6. Term Re-Extraction evaluation result. 
635
5.4 TREM Evaluation 
We evaluate TREM based on the term bank de-
scribed in section 5.1. Let iM  be the number of 
extracted terms for article i , iN  be the number of 
extracted terms in the term bank for article i , and 
n is the total articles in the test corpus. The accu-
racy is evaluated by the following formula: 
?
=
=
n
i i
i
M
N
P
1
 
Table 6 shows the result of TREM. From the re-
sults, we can find that the accuracy has improved 
significantly after the re-extraction process. On top 
of that, the results of TREM based on NTCValue  
is also slightly better than using NCValue . More-
over, the number of correct terms extracted by 
TREM using NTCValue is higher than us-
ing NCValue . 
6 Conclusions and Future Works 
We introduce a term re-extraction process (TREM) 
using Viterbi algorithm to augment the local TE 
for each document in a corpus. The results in Table 
6 show that TREM improves the precision of terms 
in local documents and also increases the number 
of correct terms extracted. We also propose a 
method to combine the C/NC value with T-Score. 
The results of our method, NTCValue , show that 
the motivation to combine the termhood features 
used in C/NC method, with T-Score, a unithood 
feature, improves the term ranking result. Results 
on Table 6 also show that NTCValue gives a bet-
ter result than the origin NCValue for TREM. 
In Table 5, the average scores for ?All Term? 
are 38.8% and 48.3% for NCValue  and 
NTCValue respectively. Therefore, NTCValue 
method improves global TE by 24.4% when com-
pared to the origin NCValue method. With the 
same calculation, we also conclude that TREM 
outperforms the linguistic pattern method by 12% 
(average scores are 50.7% and 45.3% for TREM 
and TREM-NTC respectively).  
In the future, we will focus on improving the 
performance of TREM by using more features, 
besides the weighting score. 
 
References 
C. Manning and H. Schuetze. 1999. Foundations of Sta-
tistical Natural Language Processing. MIT Press 
Cambridge, Massachusetts. 
E. Milios, Y. Zhang, B. He, L. Dong. 2003. Automatic 
Term Extraction and Document Similarity in Special 
Text Corpora. Proceedings of the 6th Conference of 
the Pacific Association for Computational Linguistics 
(PACLing'03), Halifax, Nova Scotia, Canada, pp. 
275-284. 
Evert, S. and B. Krenn. 2001. Methods for Qualitative 
Evaluation of Lexical Association Measures. Pro-
ceedings of the 39th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 369 ? 381. 
Hideki Mima, Sophia Ananiadou. 2001. An Application 
and Evaluation of the C/NC-Value Approach for the 
Automatic Term Recognition of Multi-Word Units in 
Japanese. International Journal on Terminology. 
Hiroshi Nakagawa, Tatsunori Mori. 2000. Automatic 
Term Recognition based on Statistics of Compound 
Nouns. Terminology, Vol.6, No.2, pp.195 ? 210. 
Hiroshi Nakagawa, Tatsunori Mori. 2002. A Simple but 
Powerful Automatic Term Extraction Method. 2nd 
International Workshop on Computational Terminol-
ogy, ACL. 
Katerine T. Frantzi, Sophia Ananiadou, and Junichi 
Tsujii. 1998. The C-Value/NC-Value Method of 
Automatic Recognition for Multi-word terms. Journal 
on Research and Advanced Technology for Digital 
Libraries. 
Kyo Kageura. 1996. Methods of Automatic Term Rec-
ognition - A Review. Terminology, 3(2): 259 ? 289, 
1996. 
?pela Vintar. 2004. Comparative Evaluation of C-value 
in the Treatment of Nested Terms. Memura 2004 ? 
Methodologies and Evaluation of Multiword Units in 
Real-World Applications. Proceedings of the Interna-
tional Conference on Language Resources and 
Evaluation 2004, pp. 54-57. 
636
Fast Computing Grammar-driven Convolution Tree Kernel for
Semantic Role Labeling
Wanxiang Che1?, Min Zhang2, Ai Ti Aw2, Chew Lim Tan3, Ting Liu1, Sheng Li1
1School of Computer Science and Technology
Harbin Institute of Technology, China 150001
{car,tliu}@ir.hit.edu.cn, lisheng@hit.edu.cn
2Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{mzhang,aaiti}@i2r.a-star.edu.sg
3School of Computing
National University of Singapore, Singapore 117543
tancl@comp.nus.edu.sg
Abstract
Grammar-driven convolution tree kernel
(GTK) has shown promising results for se-
mantic role labeling (SRL). However, the
time complexity of computing the GTK is
exponential in theory. In order to speed
up the computing process, we design two
fast grammar-driven convolution tree kernel
(FGTK) algorithms, which can compute the
GTK in polynomial time. Experimental re-
sults on the CoNLL-2005 SRL data show
that our two FGTK algorithms are much
faster than the GTK.
1 Introduction
Given a sentence, the task of semantic role labeling
(SRL) is to analyze the propositions expressed by
some target verbs or nouns and some constituents
of the sentence. In previous work, data-driven tech-
niques, including feature-based and kernel-based
learning methods, have been extensively studied for
SRL (Carreras and Ma`rquez, 2005).
Although feature-based methods are regarded as
the state-of-the-art methods and achieve much suc-
cess in SRL, kernel-based methods are more effec-
tive in capturing structured features than feature-
based methods. In the meanwhile, the syntactic
structure features hidden in a parse tree have been
suggested as an important feature for SRL and need
to be further explored in SRL (Gildea and Palmer,
2002; Punyakanok et al, 2005). Moschitti (2004)
?The work was mainly done when the author was a visiting
student at I2R
and Che et al (2006) are two reported work to use
convolution tree kernel (TK) methods (Collins and
Duffy, 2001) for SRL and has shown promising re-
sults. However, as a general learning algorithm, the
TK only carries out hard matching between two sub-
trees without considering any linguistic knowledge
in kernel design. To solve the above issue, Zhang
et al (2007) proposed a grammar-driven convolu-
tion tree kernel (GTK) for SRL. The GTK can uti-
lize more grammatical structure features via two
grammar-driven approximate matching mechanisms
over substructures and nodes. Experimental results
show that the GTK significantly outperforms the
TK (Zhang et al, 2007). Theoretically, the GTK
method is applicable to any problem that uses syn-
tax structure features and can be solved by the TK
methods, such as parsing, relation extraction, and so
on. In this paper, we use SRL as an application to
test our proposed algorithms.
Although the GTK shows promising results for
SRL, one big issue for the kernel is that it needs ex-
ponential time to compute the kernel function since
it need to explicitly list all the possible variations
of two sub-trees in kernel calculation (Zhang et al,
2007). Therefore, this method only works efficiently
on such kinds of datasets where there are not too
many optional nodes in production rule set. In order
to solve this computation issue, we propose two fast
algorithms to compute the GTK in polynomial time.
The remainder of the paper is organized as fol-
lows: Section 2 introduces the GTK. In Section 3,
we present our two fast algorithms for computing
the GTK. The experimental results are shown in Sec-
tion 4. Finally, we conclude our work in Section 5.
781
2 Grammar-driven Convolution Tree
Kernel
The GTK features with two grammar-driven ap-
proximate matching mechanisms over substructures
and nodes.
2.1 Grammar-driven Approximate Matching
Grammar-driven Approximate Substructure
Matching: the TK requires exact matching between
two phrase structures. For example, the two phrase
structures ?NP?DT JJ NN? (NP?a red car) and
?NP?DT NN? (NP?a car) are not identical, thus
they contribute nothing to the conventional kernel
although they share core syntactic structure property
and therefore should play the same semantic role
given a predicate. Zhang et al (2007) introduces
the concept of optional node to capture this phe-
nomenon. For example, in the production rule
?NP?DT [JJ] NP?, where [JJ] denotes an optional
node. Based on the concept of optional node, the
grammar-driven approximate substructure matching
mechanism is formulated as follows:
M(r1, r2) =
?
i,j
(IT (T ir1 , T jr2)? ?
ai+bj
1 ) (1)
where r1 is a production rule, representing a two-
layer sub-tree, and likewise for r2. T ir1 is the ith vari-
ation of the sub-tree r1 by removing one ore more
optional nodes, and likewise for T jr2 . IT (?, ?) is a bi-
nary function that is 1 iff the two sub-trees are iden-
tical and zero otherwise. ?1 (0 ? ?1 ? 1) is a small
penalty to penalize optional nodes. ai and bj stand
for the numbers of occurrence of removed optional
nodes in subtrees T ir1 and T jr2 , respectively.
M(r1, r2) returns the similarity (i.e., the kernel
value) between the two sub-trees r1 and r2 by sum-
ming up the similarities between all possible varia-
tions of the sub-trees.
Grammar-driven Approximate Node Match-
ing: the TK needs an exact matching between two
nodes. But, some similar POSs may represent simi-
lar roles, such as NN (dog) and NNS (dogs). Zhang
et al (2007) define some equivalent nodes that can
match each other with a small penalty ?2 (0 ? ?2 ?
1). This case is called node feature mutation. The
approximate node matching can be formulated as:
M(f1, f2) =
?
i,j
(If (f i1, f j2 )? ?ai+bj2 ) (2)
where f1 is a node feature, f i1 is the ith mutation of
f1 and ai is 0 iff f i1 and f1 are identical and 1 oth-
erwise, and likewise for f2 and bj . If (?, ?) is a func-
tion that is 1 iff the two features are identical and
zero otherwise. Eq. (2) sums over all combinations
of feature mutations as the node feature similarity.
2.2 The GTK
Given these two approximate matching mecha-
nisms, the GTK is defined by beginning with the
feature vector representation of a parse tree T as:
??(T ) = (#subtree1(T ), . . . ,#subtreen(T ))
where #subtreei(T ) is the occurrence number of
the ith sub-tree type (subtreei) in T . Now the GTKis defined as follows:
KG(T1, T2) = ???(T1),??(T2)?
=?i #subtreei(T1) ?#subtreei(T2)=?i((
?
n1?N1 I
?
subtreei(n1))
? (?n2?N2 I
?
subtreei(n2)))
=?n1?N1
?
n2?N2 ?
?(n1, n2)
(3)
where N1 and N2 are the sets of nodes in trees T1
and T2, respectively. I ?subtreei(n) is a function that
is ?a1 ??b2 iff there is a subtreei rooted at node n and
zero otherwise, where a and b are the numbers of
removed optional nodes and mutated node features,
respectively. ??(n1, n2) is the number of the com-
mon subtrees rooted at n1 and n2, i.e.,
??(n1, n2) =
?
i
I ?subtreei(n1) ? I ?subtreei(n2) (4)
??(n1, n2) can be further computed by the follow-
ing recursive rules:
R-A: if n1 and n2 are pre-terminals, then:
??(n1, n2) = ??M(f1, f2) (5)
where f1 and f2 are features of nodes n1 and n2
respectively, and M(f1, f2) is defined in Eq. (2),
which can be computed in linear time O(n), where
n is the number of feature mutations.
R-B: else if both n1 and n2 are the same non-terminals, then generate all variations of sub-trees
of depth one rooted at n1 and n2 (denoted by Tn1
782
and Tn2 respectively) by removing different optionalnodes, then:
??(n1, n2) = ??
?
i,j IT (T in1 , T jn2)? ?
ai+bj
1
??nc(n1,i)k=1 (1 + ??(ch(n1, i, k), ch(n2, j, k)))
(6)
where T in1 , T jn2 , IT (?, ?), ai and bj have been ex-
plained in Eq. (1). nc(n1, i) returns the number
of children of n1 in its ith subtree variation T in1 .
ch(n1, i, k) is the kth child of node n1 in its ith vari-
ation subtree T in1 , and likewise for ch(n2, j, k). ?
(0 < ? < 1) is the decay factor.
R-C: else ??(n1, n2) = 0
3 Fast Computation of the GTK
Clearly, directly computing Eq. (6) requires expo-
nential time, since it needs to sum up all possible
variations of the sub-trees with and without optional
nodes. For example, supposing n1 = ?A?a [b] c
[d]?, n2 = ?A?a b c?. To compute the Eq. (6), we
have to list all possible variations of n1 and n2?s sub-
trees, n1: ?A?a b c d?, ?A?a b c?, ?A?a c d?, ?A?a
c?; n2: ?A?a b c?. Unfortunately, Zhang et al
(2007) did not give any theoretical solution for the
issue of exponential computing time. In this paper,
we propose two algorithms to calculate it in polyno-
mial time. Firstly, we recast the issue of computing
Eq. (6) as a problem of finding common sub-trees
with and without optional nodes between two sub-
trees. Following this idea, we rewrite Eq. (6) as:
??(n1, n2) = ?? (1 +
lm?
p=lx
?p(cn1 , cn2)) (7)
where cn1 and cn2 are the child node sequences of
n1 and n2, ?p evaluates the number of common
sub-trees with exactly p children (at least including
all non-optional nodes) rooted at n1 and n2, lx =
max{np(cn1), np(cn2)} and np(?) is the number of
non-optional nodes, lm = min{l(cn1), l(cn2)}and
l(?) returns the number of children.
Now let?s study how to calculate ?p(cn1 , cn2) us-
ing dynamic programming algorithms. Here, we
present two dynamic programming algorithms to
compute it in polynomial time.
3.1 Fast Grammar-driven Convolution Tree
Kernel I (FGTK-I)
Our FGTK-I algorithm is motivated by the string
subsequence kernel (SSK) (Lodhi et al, 2002).
Given two child node sequences sx = cn1 andt = cn2 (x is the last child), the SSK uses the fol-lowing recursive formulas to evaluate the ?p:
??0(s, t) = 1, for all s, t,
??p(s, t) = 0, ifmin(|s|, |t|) < p, (8)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (9)
??p(sx, t) = ????p(sx, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?|t|?j+2)),(10)
p = 1, . . . , n? 1,
?p(sx, t) = ?p(s, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?2)). (11)
where ??p is an auxiliary function since it is only
the interior gaps in the subsequences that are penal-
ized; ? is a decay factor only used in the SSK for
weighting each extra length unit. Lodhi et al (2002)
explained the correctness of the recursion defined
above.
Compared with the SSK kernel, the GTK has
three different features:
f1: In the GTK, only optional nodes can be
skipped while the SSK kernel allows any node skip-
ping;
f2: The GTK penalizes skipped optional nodes
only (including both interior and exterior skipped
nodes) while the SSK kernel weights the length of
subsequences (all interior skipped nodes are counted
in, but exterior nodes are ignored);
f3: The GTK needs to further calculate the num-
ber of common sub-trees rooted at each two match-
ing node pair x and t[j].
To reflect the three considerations, we modify the
SSK kernel as follows to calculate the GTK:
?0(s, t) = opt(s)? opt(t)? ?|s|+|t|1 , for all s, t, (12)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (13)
?p(sx, t) = ?1 ??p(sx, t)? opt(x)
+
?
j:tj=x
(?p?1(s, t[1 : j ? 1])? ?|t|?j (14)
?opt(t[j + 1 : |t|])???(x, t[j])).
where opt(w) is a binary function, which is 0 if
non-optional nodes are found in the node sequence
w and 1 otherwise (f1); ?1 is the penalty to penalize
skipped optional nodes and the power of ?1 is the
number of skipped optional nodes (f2); ??(x, t[j])
is defined in Eq. (7) (f3). Now let us compare
783
the FGTK-I and SSK kernel algorithms. Based on
Eqs. (8), (9), (10) and (11), we introduce the opt(?)
function and the penalty ?1 into Eqs. (12), (13) and
(14), respectively. opt(?) is to ensure that in the
GTK only optional nodes are allowed to be skipped.
And only those skipped optional nodes are penal-
ized with ?1. Please note that Eqs. (10) and (11)
are merged into Eq. (14) because of the different
meaning of ? and ?1. From Eq. (8), we can see
that the current path in the recursive call will stop
and its value becomes zero once non-optional node
is skipped (when opt(w) = 0).
Let us use a sample of n1 = ?A?a [b] c [d]?, n2 =
?A?a b c? to exemplify how the FGTK-I algorithm
works. In Eq. (14)?s vocabulary, we have s = ?a [b]
c?, t = ?a b c?, x = ?[d]?, opt(x) = opt([d]) = 1,
p = 3. Then according to Eq (14), ?p(cn1 , cn2) can
be calculated recursively as Eq. (15) (Please refer to
the next page).
Finally, we have ?p(cn1 , cn2) = ?1 ???(a, a)?
??(b, b)???(c, c)
By means of the above algorithm, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |2) (Lodhi et
al., 2002). This means that the worst case complex-
ity of the FGTK-I is O(p?3|N1| ? |N2|2), where ? is
the maximum branching factor of the two trees.
3.2 Fast Grammar-driven Convolution Tree
Kernel II (FGTK-II)
Our FGTK-II algorithm is motivated by the partial
trees (PTs) kernel (Moschitti, 2006). The PT kernel
algorithm uses the following recursive formulas to
evaluate ?p(cn1 , cn2):
?p(cn1 , cn2) =
|cn1 |?
i=1
|cn2 |?
j=1
??p(cn1 [1 : i], cn2 [1 : j]) (16)
where cn1 [1 : i] and cn2 [1 : j] are the child sub-sequences of cn1 and cn2 from 1 to i and from 1to j, respectively. Given two child node sequences
s1a = cn1 [1 : i] and s2b = cn2 [1 : j] (a and b are
the last children), the PT kernel computes ??p(?, ?) as
follows:
??p(s1a, s2b) =
{
?2??(a, b)Dp(|s1|, |s2|) if a = b
0 else (17)
where ??(a, b) is defined in Eq. (7) and Dp is recur-
sively defined as follows:
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?Dp(k, l ? 1) + ?Dp(k ? 1, l) (18)
??2Dp(k ? 1, l ? 1)
D1(k, l) = 1, for all k, l (19)
where ? used in Eqs. (17) and (18) is a factor to
penalize the length of the child sequences.
Compared with the PT kernel, the GTK has two
different features which are the same as f1 and f2
when defining the FGTK-I.
To reflect the two considerations, based on the PT
kernel algorithm, we define another fast algorithm
of computing the GTK as follows:
?p(cn1 , cn2 ) =
? |cn1 |
i=1
? |cn2 |
j=1 ??p(cn1 [1 : i], cn2 [1 : j])
?opt(cn1 [i+ 1 : |cn1 |])?opt(cn2 [j + 1 : |cn2 |])
??|cn1 |?i+|cn2 |?j1
(20)
??p(s1a, s2b) =
{ ??(a, b)Dp(|s1|, |s2|) if a = b
0 else (21)
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?1Dp(k, l ? 1)? opt(s2[l]) (22)
+?1Dp(k ? 1, l)? opt(s1[k])
??21Dp(k ? 1, l ? 1)? opt(s1[k])? opt(s2[l])
D1(k, l) = ?k+l1 ? opt(s1[1 : k])? opt(s2[1 : l]), (23)
for all k, l
??p(s1, s2) = 0, if min(|s1|, |s2|) < p (24)
where opt(w) and ?1 are the same as them in the
FGTK-I.
Now let us compare the FGTK-II and the PT al-
gorithms. Based on Eqs. (16), (18) and (19), we in-
troduce the opt(?) function and the penalty ?1 into
Eqs. (20), (22) and (23), respectively. This is to
ensure that in the GTK only optional nodes are al-
lowed to be skipped and only those skipped optional
nodes are penalized. In addition, compared with
Eq. (17), the penalty ?2 is removed in Eq. (21) in
view that our kernel only penalizes skipped nodes.
Moreover, Eq. (24) is only for fast computing. Fi-
nally, the same as the FGTK-I, in the FGTK-II the
current path in a recursive call will stop and its value
becomes zero once non-optional node is skipped
(when opt(w) = 0). Here, we still can use an ex-
ample to derivate the process of the algorithm step
by step as that for FGTK-I algorithm. Due to space
limitation, here, we do not illustrate it in detail.
By means of the above algorithms, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |) (Moschitti,
784
?p(cn1 , cn2 ) = ?p(?a [b] c [d]? , ?a b c?)
= ?1 ??p(?a [b] c?, ?a b c?) + 0 //Since x * t, the second term is 0
= ?1 ? (0 + ?p?1(?a [b]?, ?a b?)? ?3?31 ???(c, c)) //Since opt(?c?) = 0, the first term is 0
= ?1 ???(c, c)? (0 + ?p?2(?a?, ?a b?)? ?2?21 ???(b, b)) //Since p? 1 > |?a?|,?p?2(?a?, ?a b?) = 0
= ?1 ???(c, c)? (0 + ??(a, a)???(b, b)) //?p?2(?a?, ?a?) = ??(a, a)
(15)
2006). This means that the worst complexity of the
FGTK-II is O(p?2|N1| ? |N2|). It is faster than the
FGTK-I?s O(p?3|N1| ? |N2|2) in theory. Please note
that the average ? in natural language parse trees is
very small and the overall complexity of the FGTKs
can be further reduced by avoiding the computation
of node pairs with different labels (Moschitti, 2006).
4 Experiments
4.1 Experimental Setting
Data: We use the CoNLL-2005 SRL shared task
data (Carreras and Ma`rquez, 2005) as our experi-
mental corpus.
Classifier: SVM (Vapnik, 1998) is selected as our
classifier. In the FGTKs implementation, we mod-
ified the binary Tree Kernels in SVM-Light Tool
(SVM-Light-TK) (Moschitti, 2006) to a grammar-
driven one that encodes the GTK and the two fast dy-
namic algorithms inside the well-known SVM-Light
tool (Joachims, 2002). The parameters are the same
as Zhang et al (2007).
Kernel Setup: We use Che et al (2006)?s hybrid
convolution tree kernel (the best-reported method
for kernel-based SRL) as our baseline kernel. It is
defined as Khybrid = ?Kpath + (1 ? ?)Kcs (0 ?
? ? 1)1. Here, we use the GTK to compute the
Kpath and the Kcs.
In the training data (WSJ sections 02-21), we get
4,734 production rules which appear at least 5 times.
Finally, we use 1,404 rules with optional nodes for
the approximate structure matching. For the node
approximate matching, we use the same equivalent
node sets as Zhang et al (2007).
4.2 Experimental Results
We use 30,000 instances (a subset of the entire train-
ing set) as our training set to compare the different
kernel computing algorithms 2. All experiments are
1Kpath and Kcs are two TKs to describe predicate-
argument link features and argument syntactic structure fea-
tures, respectively. For details, please refer to (Che et al, 2006).
2There are about 450,000 identification instances are ex-
tracted from training data.
conducted on a PC with CPU 2.8GH and memory
1G. Fig. 1 reports the experimental results, where
training curves (time vs. # of instances) of five
kernels are illustrated, namely the TK, the FGTK-
I, the FGTK-II, the GTK and a polynomial kernel
(only for reference). It clearly demonstrates that our
FGTKs are faster than the GTK algorithm as ex-
pected. However, the improvement seems not so
significant. This is not surprising as there are only
30.4% rules (1,404 out of 4,734)3 that have optional
nodes and most of them have only one optional
node4. Therefore, in this case, it is not time con-
suming to list all the possible sub-tree variations and
sum them up. Let us study this issue from computa-
tional complexity viewpoint. Suppose all rules have
exactly one optional node. This means each rule can
only generate two variations. Therefore computing
Eq. (6) is only 4 times (2*2) slower than the GTK
in this case. In other words, we can say that given
the constraint that there is only one optional node
in one rule, the time complexity of the GTK is also
O(|N1| ? |N2|) 5, where N1 and N2 are the numbers
of tree nodes, the same as the TK.
12000
6000
8000
10000
Train
ing T
ime (
S) GTKFGTK-I
2000
4000Tra
ining
 Time
 (S)
FGTK-IITKPoly
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 1: Training time comparison among different
kernels with rule set having less optional nodes.
Moreover, Fig 1 shows that the FGTK-II is faster
than the FGTK-I. This is reasonable since as dis-
3The percentage is even smaller if we consider all produc-
tion (it becomes 14.4% (1,404 out of 9,700)).
4There are 1.6 optional nodes in each rule averagely.
5Indeed it is O(4 ? |N1| ? |N2|). The parameter 4 is omitted
when discussing time complexity.
785
cussed in Subsection 3.2, the FGTK-I?s time com-
plexity is O(p?3|N1| ? |N2|2) while the FGTK-II?s is
O(p?2|N1| ? |N2|).
40000
45000
20000
25000
30000
35000
Train
ing T
ime (
S) GTKFGTK-I
0
5000
10000
15000Trai
ning 
Time
 (S)
FGTK-IITKPoly
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 2: Training time comparison among different
kernels with rule set having more optional nodes.
To further verify the efficiency of our proposed
algorithm, we conduct another experiment. Here we
use the same setting as that in Fig 1 except that we
randomly add more optional nodes in more produc-
tion rules. Table 1 reports the statistics on the two
rule set. Similar to Fig 1, Fig 2 compares the train-
ing time of different algorithms. We can see that
Fig 2 convincingly justify that our algorithms are
much faster than the GTK when the experimental
data has more optional nodes and rules.
Table 1: The rule set comparison between two ex-
periments.
# rules # rule with at
least optional
nodes
# op-
tional
nodes
# average op-
tional nodes per
rule
Exp1 4,734 1,404 2,242 1.6
Exp2 4,734 4,520 10,451 2.3
5 Conclusion
The GTK is a generalization of the TK, which can
capture more linguistic grammar knowledge into the
later and thereby achieve better performance. How-
ever, a biggest issue for the GTK is its comput-
ing speed, which needs exponential time in the-
ory. Therefore, in this paper we design two fast
grammar-driven convolution tree kennel (FGTK-I
and II) algorithms which can compute the GTK in
polynomial time. The experimental results show that
the FGTKs are much faster than the GTK when data
set has more optional nodes. We conclude that our
fast algorithms enable the GTK kernel to easily scale
to larger dataset. Besides the GTK, the idea of our
fast algorithms can be easily used into other similar
problems.
To further our study, we will use the FGTK algo-
rithms for other natural language processing prob-
lems, such as word sense disambiguation, syntactic
parsing, and so on.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152?164.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006, Sydney, Australia, July.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of NIPS-
2001.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of ACL-2002, pages 239?246.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines: Methods, Theory and
Algorithms. Kluwer Academic Publishers.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of
ACL-2004, pages 335?342.
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling case.
In Proceedings of the HHLT-NAACL-2006, June.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role la-
beling. In Proceedings of IJCAI-2005.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley.
Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for semantic
role classification. In Proceedings of ACL-2007, pages
200?207.
786
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 33?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Phrase-based Statistical Model for SMS Text Normalization 
AiTi Aw, Min Zhang, Juan Xiao, Jian Su 
Institute of Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
{aaiti,mzhang,stuxj,sujian}@i2r.a-star.edu.sg 
Abstract 
Short Messaging Service (SMS) texts be-
have quite differently from normal written 
texts and have some very special phenom-
ena. To translate SMS texts, traditional 
approaches model such irregularities di-
rectly in Machine Translation (MT). How-
ever, such approaches suffer from 
customization problem as tremendous ef-
fort is required to adapt the language 
model of the existing translation system to 
handle SMS text style. We offer an alter-
native approach to resolve such irregulari-
ties by normalizing SMS texts before MT. 
In this paper, we view the task of SMS 
normalization as a translation problem 
from the SMS language to the English 
language 1  and we propose to adapt a 
phrase-based statistical MT model for the 
task. Evaluation by 5-fold cross validation 
on a parallel SMS normalized corpus of 
5000 sentences shows that our method can 
achieve 0.80702 in BLEU score against 
the baseline BLEU score 0.6958. Another 
experiment of translating SMS texts from 
English to Chinese on a separate SMS text 
corpus shows that, using SMS normaliza-
tion as MT preprocessing can largely 
boost SMS translation performance from 
0.1926 to 0.3770 in BLEU score. 
1 Motivation 
SMS translation is a mobile Machine Translation 
(MT) application that translates a message from 
one language to another. Though there exists 
many commercial MT systems, direct use of 
such systems fails to work well due to the special 
phenomena in SMS texts, e.g. the unique relaxed 
and creative writing style and the frequent use of 
unconventional and not yet standardized short-
forms. Direct modeling of these special phenom-
ena in MT requires tremendous effort. Alterna-
tively, we can normalize SMS texts into 
                                                          
1 This paper only discusses English SMS text normalization. 
grammatical texts before MT.  In this way, the 
traditional MT is treated as a ?black-box? with 
little or minimal adaptation. One advantage of 
this pre-translation normalization is that the di-
versity in different user groups and domains can 
be modeled separately without accessing and 
adapting the language model of the MT system 
for each SMS application. Another advantage is 
that the normalization module can be easily util-
ized by other applications, such as SMS to 
voicemail and SMS-based information query. 
In this paper, we present a phrase-based statis-
tical model for SMS text normalization. The 
normalization is visualized as a translation prob-
lem where messages in the SMS language are to 
be translated to normal English using a similar 
phrase-based statistical MT method (Koehn et al, 
2003). We use IBM?s BLEU score (Papineni et 
al., 2002) to measure the performance of SMS 
text normalization. BLEU score computes the 
similarity between two sentences using n-gram 
statistics, which is widely-used in MT evalua-
tion. A set of parallel SMS messages, consisting 
of 5000 raw (un-normalized) SMS messages and 
their manually normalized references, is con-
structed for training and testing. Evaluation by 5-
fold cross validation on this corpus shows that 
our method can achieve accuracy of 0.80702 in 
BLEU score compared to the baseline system of 
0.6985. We also study the impact of our SMS 
text normalization on the task of SMS transla-
tion. The experiment of translating SMS texts 
from English to Chinese on a corpus comprising 
402 SMS texts shows that, SMS normalization as 
a preprocessing step of MT can boost the transla-
tion performance from 0.1926 to 0.3770 in 
BLEU score. 
The rest of the paper is organized as follows. 
Section 2 reviews the related work. Section 3 
summarizes the characteristics of English SMS 
texts. Section 4 discusses our method and Sec-
tion 5 reports our experiments. Section 6 con-
cludes the paper. 
2 Related Work 
There is little work reported on SMS normaliza-
tion and translation. Bangalore et al (2002) used 
33
a consensus translation technique to bootstrap 
parallel data using off-the-shelf translation sys-
tems for training a hierarchical statistical transla-
tion model for general domain instant messaging 
used in Internet chat rooms. Their method deals 
with the special phenomena of the instant mes-
saging language (rather than the SMS language) 
in each individual MT system.  Clark (2003) 
proposed to unify the process of tokenization, 
segmentation and spelling correction for nor-
malization of general noisy text (rather than SMS 
or instant messaging texts) based on a noisy 
channel model at the character level. However, 
results of the normalization are not reported. Aw 
et al (2005) gave a brief description on their in-
put pre-processing work for an English-to-
Chinese SMS translation system using a word-
group model. In addition, in most of the com-
mercial SMS translation applications 2 , SMS 
lingo (i.e., SMS short form) dictionary is pro-
vided to replace SMS short-forms with normal 
English words. Most of the systems do not han-
dle OOV (out-of-vocabulary) items and ambigu-
ous inputs. Following compares SMS text 
normalization with other similar or related appli-
cations. 
2.1 SMS Normalization versus General 
Text Normalization 
General text normalization deals with Non-
Standard Words (NSWs) and has been well-
studied in text-to-speech (Sproat et al, 2001) 
while SMS normalization deals with Non-Words 
(NSs) or lingoes and has seldom been studied 
before. NSWs, such as digit sequences, acronyms, 
mixed case words (WinNT, SunOS), abbrevia-
tions and so on, are grammatically correct in lin-
guistics. However lingoes, such as ?b4? (before) 
and ?bf? (boyfriend), which are usually self-
created and only accepted by young SMS users, 
are not yet formalized in linguistics. Therefore, 
the special phenomena in SMS texts impose a 
big challenge to SMS normalization. 
2.2 SMS Normalization versus Spelling 
Correction Problem 
Intuitively, many would regard SMS normaliza-
tion as a spelling correction problem where the 
lingoes are erroneous words or non-words to be 
replaced by English words. Researches on spell-
ing correction centralize on typographic and 
cognitive/orthographic errors (Kukich, 1992) and 
use approaches (M.D. Kernighan, Church and 
                                                          
2 http://www.etranslator.ro and http://www.transl8bit.com 
Gale, 1991) that mostly model the edit operations 
using distance measures (Damerau 1964; Leven-
shtein 1966), specific word set confusions (Gold-
ing and Roth, 1999) and pronunciation modeling 
(Brill and Moore, 2000; Toutanova and Moore, 
2002). These models are mostly character-based 
or string-based without considering the context. 
In addition, the author might not be aware of the 
errors in the word introduced during the edit op-
erations, as most errors are due to mistype of 
characters near to each other on the keyboard or 
homophones, such as ?poor? or ?pour?.  
In SMS, errors are not isolated within word 
and are usually not surrounded by clean context. 
Words are altered deliberately to reflect sender?s 
distinct creation and idiosyncrasies. A character 
can be deleted on purpose, such as ?wat? (what) 
and ?hv? (have).  It also consists of short-forms 
such as ?b4? (before), ?bf? (boyfriend). In addi-
tion, normalizing SMS text might require the 
context to be spanned over more than one lexical 
unit such as ?lemme? (let me), ?ur? (you are) etc. 
Therefore, the models used in spelling correction 
are inadequate for providing a complete solution 
for SMS normalization. 
2.3 SMS Normalization versus Text Para-
phrasing Problem 
Others may regard SMS normalization as a para-
phrasing problem. Broadly speaking, paraphrases 
capture core aspects of variability in language, 
by representing equivalencies between different 
expressions that correspond to the same meaning. 
In most of the recent works (Barzilay and 
McKeown, 2001; Shimohata, 2002), they are 
acquired (semi-) automatically from large com-
parable or parallel corpora using lexical and 
morpho-syntactic information. 
Text paraphrasing works on clean texts in 
which contextual and lexical-syntactic features 
can be extracted and used to find ?approximate 
conceptual equivalence?. In SMS normalization, 
we are dealing with non-words and ?ungram-
matically? sentences with the purpose to normal-
ize or standardize these words and form better 
sentences. The SMS normalization problem is 
thus different from text paraphrasing. On the 
other hand, it bears some similarities with MT as 
we are trying to ?convert? text from one lan-
guage to another. However, it is a simpler prob-
lem as most of the time; we can find the same 
word in both the source and target text, making 
alignment easier. 
 
34
3 Characteristics of English SMS  
Our corpus consists of 55,000 messages collected 
from two sources, a SMS chat room and corre-
spondences between university students. The 
content is mostly related to football matches, 
making friends and casual conversations on 
?how, what and where about?. We summarize 
the text behaviors into two categories as below. 
3.1 Orthographic Variation 
The most significant orthographic variant in 
SMS texts is in the use of non-standard, self-
created short-forms. Usually, sender takes advan-
tage of phonetic spellings, initial letters or num-
ber homophones to mimic spoken conversation 
or shorten words or phrases (hw vs. homework or 
how, b4 vs. before, cu vs. see you, 2u vs. to you, 
oic vs. oh I see, etc.) in the attempt to minimize 
key strokes. In addition, senders create a new 
form of written representation to express their 
oral utterances. Emotions, such as ?:(? symboliz-
ing  sad, ?:)? symbolizing smiling, ?:()? symbol-
izing shocked, are representations of body 
language. Verbal effects such as ?hehe? for 
laughter and emphatic discourse particles such as 
?lor?, ?lah?, ?meh? for colloquial English are 
prevalent in the text collection. 
The loss of ?alpha-case? information posts an-
other challenge in lexical disambiguation and 
introduces difficulty in identifying sentence 
boundaries, proper nouns, and acronyms. With 
the flexible use of punctuation or not using punc-
tuation at all, translation of SMS messages with-
out prior processing is even more difficult. 
3.2 Grammar Variation 
SMS messages are short, concise and convey 
much information within the limited space quota 
(160 letters for English), thus they tend to be im-
plicit and influenced by pragmatic and situation 
reasons. These inadequacies of language expres-
sion such as deletion of articles and subject pro-
noun, as well as problems in number agreements 
or tenses make SMS normalization more chal-
lenging. Table 1 illustrates some orthographic 
and grammar variations of SMS texts. 
3.3 Corpus Statistics  
We investigate the corpus to assess the feasibility 
of replacing the lingoes with normal English 
words and performing limited adjustment to the 
text structure. Similarly to Aw et al (2005), we 
focus on the three major cases of transformation 
as shown in the corpus: (1) replacement of OOV 
words and non-standard SMS lingoes; (2) re-
moval of slang and (3) insertion of auxiliary or 
copula verb and subject pronoun.  
 
Phenomena Messages 
1. Dropping ??? at 
the end of 
question 
btw, wat is ur view 
(By the way, what is your 
view?) 
2. Not using any 
punctuation at 
all 
Eh speak english mi malay 
not tt good  
(Eh, speak English! My Ma-
lay is not that good.) 
3. Using spell-
ing/punctuation 
for emphasis 
goooooood Sunday morning 
!!!!!!  
(Good Sunday morning!) 
4. Using phonetic 
spelling 
dat iz enuf  
(That is enough) 
5. Dropping 
vowel 
i hv cm to c my luv. 
(I have come to see my love.)
6. Introducing 
local flavor 
yar lor where u go juz now  
(yes, where did you go just 
now?) 
7.  Dropping verb 
I hv 2 go. Dinner w parents.  
(I have to go. Have dinner 
with parents.) 
 
Table 1. Examples of SMS Messages 
 
 
Transformation Percentage (%) 
Insertion 8.09 
Deletion 5.48 
Substitution 86.43 
 
Table 2. Distribution of Insertion, Deletion and 
Substitution Transformation. 
 
Substitution  Deletion Insertion 
u -> you m are 
2 ? to lah am 
n ? and t is 
r ? are ah you 
ur ?your leh to 
dun ? don?t 1 do 
man ? manches-
ter 
huh a 
no ? number one in 
intro ? introduce lor yourself 
wat ? what ahh will 
 
Table 3. Top 10 Most Common Substitu-
tion, Deletion and Insertion 
 
Table 2 shows the statistics of these transfor-
mations based on 700 messages randomly se-
lected, where 621 (88.71%) messages required 
35
If we include the word ?null? in the English 
vocabulary, the above model can fully address 
the deletion and substitution transformations, but 
inadequate to address the insertion transforma-
tion. For example, the lingoes ?duno?, ?ysnite? 
have to be normalized using an insertion trans-
formation to become ?don?t know? and ?yester-
day night?. Moreover, we also want the 
normalization to have better lexical affinity and 
linguistic equivalent, thus we extend the model 
to allow many words to many words alignment, 
allowing a sequence of SMS words to be normal-
ized to a sequence of contiguous English words. 
We call this updated model a phrase-based nor-
malization model.  
normalization with a total of 2300 transforma-
tions. Substitution accounts for almost 86% of all 
transformations. Deletion and substitution make 
up the rest. Table 3 shows the top 10 most com-
mon transformations. 
4 SMS Normalization  
We view the SMS language as a variant of Eng-
lish language with some derivations in vocabu-
lary and grammar. Therefore, we can treat SMS 
normalization as a MT problem where the SMS 
language is to be translated to normal English. 
We thus propose to adapt the statistical machine 
translation model (Brown et al, 1993; Zens and 
Ney, 2004) for SMS text normalization. In this 
section, we discuss the three components of our 
method: modeling, training and decoding for 
SMS text normalization. 
4.2 Phrase-based Model 
Given an English sentence e  and SMS sentence 
s , if we assume that e  can be decomposed into 
 phrases with a segmentation T , such that 
each phrase e  in  can be corresponded with 
one phrase s  in 
K
k
k
e
s , we have e e  
and 
1 1
N
k Ke e  ? ?=
1 1
M
k Ks s s  s= ? ? . The channel model can be 
rewritten in equation (3).  
4.1 Basic Word-based Model  
The SMS normalization model is based on the 
source channel model (Shannon, 1948). Assum-
ing that an English sentence e, of length N is 
?corrupted? by a noisy channel to produce a 
SMS message s, of length M, the English sen-
tence e, could be recovered through a posteriori 
distribution for a channel target text given the 
source text P s , and a prior distribution for 
the channel source text . 
( | )e
( )P e
 
 { }
{ }
1
1
1 1 1
1 1 1
? arg max ( | )
arg max ( | ) ( )
N
N
N N M
e
M N N
e
e P e s
P s e P e
=
= i          (1) 
 
{ }
1 1 1 1
1 1 1
1 1 1
1 1 1
( | ) ( , | )
( | ) ( | , )
( | ) ( | )
max ( | ) ( | )
M N M N
T
N M N
T
N K K
T
N K K
T
P s e P s T e
P T e P s T e
P T e P s e
P T e P s e
=
=
=
?
?
?
?
i
 i
 i
    (3) 
 
This is the basic function of the channel model 
for the phrase-based SMS normalization model, 
where we used the maximum approximation for 
the sum over all segmentations. Then we further 
decompose the probability 1 1( | )
K KP s e  using a 
phrase alignment  as done in the previous 
word-based model. 
A
 
Assuming that one SMS word is mapped ex-
actly to one English word in the channel model 
 under an alignment , we need to con-
sider only two types of probabilities: the align-
ment probabilities denoted by P m  and the 
lexicon mapping probabilities denoted by 
(Brown et al 1993). The channel 
model can be written as in the following equation 
where m is the position of a word in 
( | )P s e
( |m aP s e
A
( | )ma
)
m
s and  its 
alignment in . 
ma
e
 
 
{ }
1 1 1 1
1 1 1
1
( | ) ( , | )
( | ) ( | , )
( | ) ( | )
m
M N M N
A
N M N
A
M
m m a
A m
P s e P s A e
P A e P s A e
P m a P s e
=
=
=
? ?? ??
?
?
? ?
i
i ??
  (2) 
{ }
{ }
{ }
1
1 1 1 1
1 1 1
1
1
1
1
( | ) ( , | )
( | ) ( | , )
( | ) ( | , )
( | ) ( | )
k
k
K K K K
A
K K K
A
K
ak
k k a
kA
K
k k a
kA
P s e P s A e
P A e P s A e
P k a P s s e
P k a P s e
?
=
=
=
=
? ?= ? ?? ?
? ?? ? ?? ?
?
?
? ?
? ?







   
   i
   i
  i
 (4) 
 
We are now able to model the three transfor-
mations through the normalization pair ( , )
kk a
s e    , 
36
with the mapping probability . The fol-
lowings show the scenarios in which the three 
transformations occur. 
( | )
kk a
P s e 
kk a
s e<  
kk a
s e=  
| ) (k ka P s
?
?  i
( |kP s e 
)
)k ke 
}1
1
1
1
1 1
| )
)
) ( |
) (
M
K
k
N K
n k
P s e
P s
P s
?
=
?
= =
???
???
?
? ?
i i
i
1 )
N
 
Insertion  
Deletion 
ka
e  = null 
Substitution  
 
The statistics in our training corpus shows that 
by selecting appropriate phrase segmentation, the 
position re-ordering at the phrase level occurs 
rarely. It is not surprising since most of the Eng-
lish words or phrases in normal English text are 
replaced with lingoes in SMS messages without 
position change to make SMS text short and con-
cise and to retain the meaning. Thus we need to 
consider only monotone alignment at phrase 
level, i.e., k , as in equation (4). In addition, 
the word-level reordering within phrase is 
learned during training. Now we can further de-
rive equation (4) as follows: 
ka= 
{ }1 1
1
1
( | ) ( | )
( | )
k
K
K K
a
kA
K
k k
k
P s e P k e
P s e
=
=
? ? ?
?
? ?
?


  
 
 (5) 
?
?
The mapping probability is esti-
mated via relative frequencies as follows: 
)k
 
'
'
( ,( | )
( ,
k
k k
k k
s
N s eP s e
N s
= ?

                            (6) 
Here, denotes the frequency of the 
normalization pair . 
( , )k kN s e 
( , )k ks e 
Using a bigram language model and assuming 
Bayes decision rule, we finally obtain the follow-
ing search criterion for equation (1). 
 {
1
1
1
1 1 1
1
1
,
? arg max ( ) (
arg max ( |
      max ( | )
arg max ( | | )
N
N
N
N N N
e
N
n n
e n
N
k kT
n n k k
e T
e P e
P e e
P T e e
P e e e
=
=
?? ??
?
?
?
?
i
 
 
(7) 
????
The alignment process given in equation (8) is 
different from that of normalization given in 
equation (7) in that, here we have an aligned in-
put sentence pair, s and . The alignment 
process is just to find the alignment segmentation 
,? ,k ks e ks e? < > =<   
( , )k kP s e 
between the two sen-
tences that maximizes the joint probability. 
Therefore, in step (2) of the EM algorithm given 
at Figure 1, only the joint probabilities 
are involved and updated.  
???
 
For the above equation, we assume the seg-
mentation probability ( |P T e to be constant. 
Finally, the SMS normalization model consists of 
two sub-models: a word-based language model 
(LM), characterized by 1( | )n nP e e ?
)k
 and a phrase-
based lexical mapping model (channel model), 
characterized by ( |kP s e
)ke 
)ke 
  . 
,?
arg m
s ek k 1
ax ( ,
K
k
k
P s
=
?  ? < > 
1
M
1
Ne
1,k k K=>
4.3 Training Issues 
For the phrase-based model training, the sen-
tence-aligned SMS corpus needs to be aligned 
first at the phrase level. The maximum likelihood 
approach, through EM algorithm and Viterbi 
search (Dempster et al, 1977) is employed to 
infer such an alignment. Here, we make a rea-
sonable assumption on the alignment unit that a 
single SMS word can be mapped to a sequence 
of contiguous English words, but not vice verse. 
The EM algorithm for phrase alignment is illus-
trated in Figure 1 and is formulated by equation 
(8). 
 
 
 
The Expectation-Maximization Algorithm 
 
(1) Bootstrap initial alignment using ortho-
graphic similarities 
(2)  Expectation: Update the joint probabili-
ties  ( ,kP s
(3)  Maximization: Apply the joint probabili-
ties to get new alignment using 
Viterbi search algorithm 
( ,kP s
(4)  Repeat (2) to (3) until alignment con-
verges 
(5) Derive normalization pairs from final 
alignment 
 
Figure 1. Phrase Alignment Using EM Algorithm 
 
, 1? | , )k k
M N
s e ke s e? < > =  (8) 1
 
Since EM may fall into local optimization, in 
order to speed up convergence and find a nearly 
global optimization, a string matching technique 
is exploited at the initialization step to identify 
the most probable normalization pairs. The or-
37
thographic similarities captured by edit distance 
and a SMS lingo dictionary3  which contains the 
commonly used short-forms are first used to es-
tablish phrase mapping boundary candidates. 
Heuristics are then exploited to match tokens 
within the pairs of boundary candidates by trying 
to combine consecutive tokens within the bound-
ary candidates if the numbers of tokens do not 
agree. 
Finally, a filtering process is carried out to 
manually remove the low-frequency noisy 
alignment pairs. Table 4 shows some of the ex-
tracted normalization pairs. As can be seen from 
the table, our algorithm discovers ambiguous 
mappings automatically that are otherwise miss-
ing from most of the lingo dictionary. 
 
( , )s e   log ( | )P s e   
(2, 2) 0 
(2, to) -0.579466 
(2, too) -0.897016 
(2, null) -2.97058 
(4, 4) 0 
(4, for) -0.431364 
(4, null) -3.27161 
(w, who are) -0.477121 
(w, with) -0.764065 
(w, who) -1.83885 
(dat, that) -0.726999 
(dat, date) -0.845098 
(tmr, tomorrow) -0.341514 
 
Table 4. Examples of normalization pairs 
 
Given the phrase-aligned SMS corpus, the 
lexical mapping model, characterized by 
( | )k kP s e  , is easily to be trained using equation 
(6). Our n-gram LM 1( | )n nP e e ? is trained on 
English Gigaword provided by LDC using 
SRILM language modeling toolkit (Stolcke, 
2002). Backoff smoothing (Jelinek, 1991) is used 
to adjust and assign a non-zero probability to the 
unseen words to address data sparseness. 
4.4 Monotone Search  
Given an input , the search, characterized in 
equation (7), is to find a sentence e that maxi-
s
mizes  using the normalization 
model. In this paper, the maximization problem 
in equation (7) is solved using a monotone search, 
implemented as a Viterbi search through dy-
namic programming. 
( | ) ( )P s e P ei
5 Experiments 
The aim of our experiment is to verify the effec-
tiveness of the proposed statistical model for 
SMS normalization and the impact of SMS nor-
malization on MT. 
A set of 5000 parallel SMS messages, which 
consists of raw (un-normalized) SMS messages 
and reference messages manually prepared by 
two project members with inter-normalization 
agreement checked, was prepared for training 
and testing. For evaluation, we use IBM?s BLEU 
score (Papineni et al, 2002) to measure the per-
formance of the SMS normalization. BLEU score 
measures the similarity between two sentences 
using n-gram statistics with a penalty for too 
short sentences, which is already widely-used in 
MT evaluation.  
 
Setup BLEU score (3-gram) 
Raw SMS without 
Normalization 0.5784 
Dictionary Look-up 
plus Frequency 0.6958 
Bi-gram Language 
Model Only 0.7086 
 
Table 5. Performance of different set-
ups of the baseline experiments on the 
5000 parallel SMS messages 
5.1 Baseline Experiments: Simple SMS 
Lingo Dictionary Look-up and Using 
Language Model Only 
The baseline experiment is to moderate the texts 
using a lingo dictionary comprises 142 normali-
zation pairs, which is also used in bootstrapping 
the phrase alignment learning process.  
Table 5 compares the performance of the dif-
ferent setups of the baseline experiments. We 
first measure the complexity of the SMS nor-
malization task by directly computing the simi-
larity between the raw SMS text and the 
normalized English text. The 1st row of Table 5 
reports the similarity as 0.5784 in BLEU score, 
which implies that there are quite a number of 
English word 3-gram that are common in the raw 
and normalized messages. The 2nd experiment is 
carried out using only simple dictionary look-up. 
                                                          
3 The entries are collected from various websites such as 
http://www.handphones.info/sms-dictionary/sms-lingo.php, 
and http://www.funsms.net/sms_dictionary.htm, etc.  
38
Lexical ambiguity is addressed by selecting the 
highest-frequency normalization candidate, i.e., 
only unigram LM is used. The performance of 
the 2nd experiment is 0.6958 in BLEU score. It 
suggests that the lingo dictionary plus the uni-
gram LM is very useful for SMS normalization. 
Finally we carry out the 3rd experiment using 
dictionary look-up plus bi-gram LM. Only a 
slight improvement of 0.0128 (0.7086-0.6958) is 
obtained. This is largely because the English 
words in the lingo dictionary are mostly high-
frequency and commonly-used. Thus bi-gram 
does not show much more discriminative ability 
than unigram without the help of the phrase-
based lexical mapping model. 
Experimental result analysis reveals that the 
strength of our model is in its ability to disam-
biguate mapping as in ?2? to ?two? or ?to? and 
?w? to ?with? or ?who?. Error analysis shows 
that the challenge of the model lies in the proper 
insertion of subject pronoun and auxiliary or 
copula verb, which serves to give further seman-
tic information about the main verb, however this 
requires significant context understanding. For 
example, a message such as ?u smart? gives little 
clues on whether it should be normalized to ?Are 
you smart?? or ?You are smart.? unless the full 
conversation is studied. 
 
Takako w r u? 
Takako who are you? 
Im in ns, lik soccer, clubbin hangin w frenz! 
Wat bout u mee? 
I'm in ns, like soccer, clubbing hanging with 
friends!  What about you? 
fancy getting excited w others' boredom 
Fancy getting excited with others' boredom 
If u ask me b4 he ask me then i'll go out w u all 
lor. N u still can act so real. 
If you ask me before he asked me then I'll go 
out with you all.  And you still can act so real. 
Doing nothing, then u not having dinner w us? 
Doing nothing, then you do not having dinner 
with us? 
Aiyar sorry lor forgot 2 tell u... Mtg at 2 pm. 
Sorry forgot to tell you...  Meeting at two pm. 
tat's y I said it's bad dat all e gals know u... 
Wat u doing now? 
That's why I said it's bad that all the girls know 
you...  What you doing now? 
 
5.2 Using Phrase-based Model 
We then conducted the experiment using the pro-
posed method (Bi-gram LM plus a phrase-based 
lexical mapping model) through a five-fold cross 
validation on the 5000 parallel SMS messages. 
Table 6 shows the results. An average score of 
0.8070 is obtained. Compared with the baseline 
performance in Table 5, the improvement is very 
significant. It suggests that the phrase-based 
lexical mapping model is very useful and our 
method is effective for SMS text normalization. 
Figure 2 is the learning curve. It shows that our 
algorithm converges when training data is 
increased to 3000 SMS parallel messages. This 
suggests that our collected corpus is representa-
tive and enough for training our model. Table 7 
illustrates some examples of the normalization 
results. 
  
5-fold cross validation BLEU score (3-gram)
Setup 1 0.8023 
Setup 2 0.8236 
Setup 3 0.8071 
Setup 4 0.8113 
Setup 5 0.7908 
Ave. 0.8070 
 
Table 7. Examples of Normalization Results 
5.3 Effect on English-Chinese MT 
An experiment was also conducted to study the 
effect of normalization on MT using 402 mes-
sages randomly selected from the text corpus. 
We compare three types of SMS message: raw 
SMS messages, normalized messages using sim-
ple dictionary look-up and normalized messages 
using our method. The messages are passed to 
two different English-to-Chinese translation sys-
tems provided by Systran4 and Institute for Info-
comm Research5(I2R) separately to produce three 
sets of translation output. The translation quality 
is measured using 3-gram cumulative BLEU 
score against two reference messages. 3-gram is 
 
Table 6. Normalization results for 5-
fold cross validation test 
0.7
0.72
0.74
0.76
0.78
0.8
0.82
1000 2000 3000 4000 5000
BLEU
 Figure 2. Learning Curve  
                                                          
4 http://www.systranet.com/systran/net 
5 http://nlp.i2r.a-star.edu.sg/techtransfer.html 
39
used as most of the messages are short with aver-
age length of seven words. Table 8 shows the 
details of the BLEU scores. We obtain an aver-
age of 0.3770 BLEU score for normalized mes-
sages against 0.1926 for raw messages. The 
significant performance improvement suggests 
that preprocessing of normalizing SMS text us-
ing our method before MT is an effective way to 
adapt a general MT system to SMS domain. 
 
 I2R Systran Ave. 
Raw Message 0.2633 0.1219 0.1926 
Dict Lookup 0.3485 0.1690 0.2588 
Normalization 0.4423 0.3116 0.3770 
 
Table 8. SMS Translation BLEU score with or 
without SMS normalization 
6 Conclusion 
In this paper, we study the differences among 
SMS normalization, general text normalization, 
spelling check and text paraphrasing, and inves-
tigate the different phenomena of SMS messages. 
We propose a phrase-based statistical method to 
normalize SMS messages. The method produces 
messages that collate well with manually normal-
ized messages, achieving 0.8070 BLEU score 
against 0.6958 baseline score. It also signifi-
cantly improves SMS translation accuracy from 
0.1926 to 0.3770 in BLEU score without adjust-
ing the MT model. 
This experiment results provide us with a good 
indication on the feasibility of using this method 
in performing the normalization task. We plan to 
extend the model to incorporate mechanism to 
handle missing punctuation (which potentially 
affect MT output and are not being taken care at 
the moment),  and making use of pronunciation 
information to handle OOV caused by the use of 
phonetic spelling. A bigger data set will also be 
used to test the robustness of the system leading 
to a more accurate alignment and normalization.  
References  
A.T. Aw, M. Zhang, Z.Z. Fan, P.K. Yeo and J. Su. 
2005. Input Normalization for an English-to-
Chinese SMS Translation System. MT Summit-
2005  
S. Bangalore, V. Murdock and G. Riccardi. 2002. 
Bootstrapping Bilingual Data using Consensus 
Translation for a Multilingual Instant Messaging 
System. COLING-2002 
R. Barzilay and K. R. McKeown. 2001. Extracting 
paraphrases from a parallel corpus. ACL-2001 
E. Brill and R. C. Moore. 2000. An Improved Error 
Model for Noisy Channel Spelling Correction. 
ACL-2000 
P. F. Brown, S. D. Pietra, V. D. Pietra and R. Mercer. 
1993. The Mathematics of Statistical Machine 
Translation: Parameter Estimation. Computational 
Linguistics: 19(2) 
A. Clark. 2003. Pre-processing very noisy text. In 
Proceedings of Workshop on Shallow Processing 
of Large Corpora, Lancaster, 2003  
F. J. Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communica-
tions ACM 7, 171-176 
A.P. Dempster, N.M. Laird and D.B. Rubin. 1977. 
Maximum likelihood from incomplete data via the 
EM algorithm, Journal of the Royal Statistical So-
ciety, Series B, Vol. 39, 1-38 
A. Golding and D. Roth. 1999. A Winnow-Based Ap-
proach to Spelling Correction. Machine Learning 
34: 107-130 
F. Jelinek. 1991. Self-organized language modeling 
for speech recognition. In A. Waibel and K.F. Lee, 
editors, Readings in Speech Recognition, pages 
450-506. Morgan Kaufmann, 1991 
M. D. Kernighan, K Church and W. Gale. 1990. A 
spelling correction program based on a noisy 
channel model. COLING-1990 
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys, 
24(4):377-439  
K. A. Papineni, S. Roukos, T. Ward and W. J. Zhu. 
2002. BLEU : a Method for Automatic Evaluation 
of Machine Translation. ACL-2002 
P. Koehn, F.J. Och and D. Marcu. 2003. Statistical 
Phrase-Based Translation. HLT-NAACL-2003 
C. Shannon. 1948. A mathematical theory of commu-
nication. Bell System Technical Journal 27(3): 
379-423 
M. Shimohata and E. Sumita 2002. Automatic Para-
phrasing Based on Parallel Corpus for Normaliza-
tion. LREC-2002 
R. Sproat, A. Black, S. Chen, S. Kumar, M. Ostendorf 
and C. Richards. 2001. Normalization of Non-
Standard Words. Computer Speech and Language, 
15(3):287-333 
A. Stolcke. 2002. SRILM ? An extensible language 
modeling toolkit. ICSLP-2002 
K. Toutanova and R. C. Moore. 2002. Pronunciation 
Modeling for Improved Spelling Correction. ACL-
2002 
R. Zens and H. Ney. 2004. Improvements in Phrase-
Based Statistical MT. HLT-NAALL-2004 
40
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200?207,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Grammar-driven Convolution Tree Kernel for Se-
mantic Role Classification 
 
Min ZHANG1     Wanxiang CHE2     Ai Ti AW1     Chew Lim TAN3     
Guodong ZHOU1,4     Ting LIU2     Sheng LI2     
 
1Institute for Infocomm Research   
{mzhang, aaiti}@i2r.a-star.edu.sg 
2Harbin Institute of Technology 
{car, tliu}@ir.hit.edu.cn   
lisheng@hit.edu.cn 
3National University of Singapore 
tancl@comp.nus.edu.sg 
4 Soochow Univ., China 215006 
gdzhou@suda.edu.cn 
 
 
 
 
 
Abstract 
Convolution tree kernel has shown promis-
ing results in semantic role classification. 
However, it only carries out hard matching, 
which may lead to over-fitting and less ac-
curate similarity measure. To remove the 
constraint, this paper proposes a grammar-
driven convolution tree kernel for semantic 
role classification by introducing more lin-
guistic knowledge into the standard tree 
kernel. The proposed grammar-driven tree 
kernel displays two advantages over the pre-
vious one: 1) grammar-driven approximate 
substructure matching and 2) grammar-
driven approximate tree node matching. The 
two improvements enable the grammar-
driven tree kernel explore more linguistically 
motivated structure features than the previ-
ous one. Experiments on the CoNLL-2005 
SRL shared task show that the grammar-
driven tree kernel significantly outperforms 
the previous non-grammar-driven one in 
SRL. Moreover, we present a composite 
kernel to integrate feature-based and tree 
kernel-based methods. Experimental results 
show that the composite kernel outperforms 
the previously best-reported methods. 
1 Introduction 
Given a sentence, the task of Semantic Role Label-
ing (SRL) consists of analyzing the logical forms 
expressed by some target verbs or nouns and some 
constituents of the sentence. In particular, for each 
predicate (target verb or noun) all the constituents in 
the sentence which fill semantic arguments (roles) 
of the predicate have to be recognized. Typical se-
mantic roles include Agent, Patient, Instrument, etc. 
and also adjuncts such as Locative, Temporal, 
Manner, and Cause, etc. Generally, semantic role 
identification and classification are regarded as two 
key steps in semantic role labeling. Semantic role 
identification involves classifying each syntactic 
element in a sentence into either a semantic argu-
ment or a non-argument while semantic role classi-
fication involves classifying each semantic argument 
identified into a specific semantic role. This paper 
focuses on semantic role classification task with the 
assumption that the semantic arguments have been 
identified correctly. 
Both feature-based and kernel-based learning 
methods have been studied for semantic role classi-
fication (Carreras and M?rquez, 2004; Carreras and 
M?rquez, 2005). In feature-based methods, a flat 
feature vector is used to represent a predicate-
argument structure while, in kernel-based methods, 
a kernel function is used to measure directly the 
similarity between two predicate-argument struc-
tures. As we know, kernel methods are more effec-
tive in capturing structured features. Moschitti 
(2004) and Che et al (2006) used a convolution 
tree kernel (Collins and Duffy, 2001) for semantic 
role classification. The convolution tree kernel 
takes sub-tree as its feature and counts the number 
of common sub-trees as the similarity between two 
predicate-arguments. This kernel has shown very 
200
promising results in SRL. However, as a general 
learning algorithm, the tree kernel only carries out 
hard matching between any two sub-trees without 
considering any linguistic knowledge in kernel de-
sign. This makes the kernel fail to handle similar 
phrase structures (e.g., ?buy a car? vs. ?buy a red 
car?) and near-synonymic grammar tags (e.g., the 
POS variations between ?high/JJ degree/NN? 1 and 
?higher/JJR degree/NN?) 2. To some degree, it may 
lead to over-fitting and compromise performance. 
This paper reports our preliminary study in ad-
dressing the above issue by introducing more lin-
guistic knowledge into the convolution tree kernel. 
To our knowledge, this is the first attempt in this 
research direction. In detail, we propose a gram-
mar-driven convolution tree kernel for semantic 
role classification that can carry out more linguisti-
cally motivated substructure matching. Experimental 
results show that the proposed method significantly 
outperforms the standard convolution tree kernel on 
the data set of the CoNLL-2005 SRL shared task. 
The remainder of the paper is organized as fol-
lows: Section 2 reviews the previous work and Sec-
tion 3 discusses our grammar-driven convolution 
tree kernel. Section 4 shows the experimental re-
sults. We conclude our work in Section 5. 
2 Previous Work 
Feature-based Methods for SRL: most features 
used in prior SRL research are generally extended 
from Gildea and Jurafsky (2002), who used a linear 
interpolation method and extracted basic flat fea-
tures from a parse tree to identify and classify the 
constituents in the FrameNet (Baker et al, 1998). 
Here, the basic features include Phrase Type, Parse 
Tree Path, and Position. Most of the following work 
focused on feature engineering (Xue and Palmer, 
2004; Jiang et al, 2005) and machine learning 
models (Nielsen and Pradhan, 2004; Pradhan et al, 
2005a). Some other work paid much attention to the 
robust SRL (Pradhan et al, 2005b) and post infer-
ence (Punyakanok et al, 2004). These feature-
based methods are considered as the state of the art 
methods for SRL. However, as we know, the stan-
dard flat features are less effective in modeling the 
                                                          
1 Please refer to http://www.cis.upenn.edu/~treebank/ for the 
detailed definitions of the grammar tags used in the paper. 
2 Some rewrite rules in English grammar are generalizations of 
others: for example, ?NP? DET JJ NN? is a specialized ver-
sion of ?NP? DET NN?. The same applies to POS. The stan-
dard convolution tree kernel is unable to capture the two cases. 
syntactic structured information. For example, in 
SRL, the Parse Tree Path feature is sensitive to 
small changes of the syntactic structures. Thus, a 
predicate argument pair will have two different 
Path features even if their paths differ only for one 
node. This may result in data sparseness and model 
generalization problems. 
Kernel-based Methods for SRL: as an alternative, 
kernel methods are more effective in modeling 
structured objects. This is because a kernel can 
measure the similarity between two structured ob-
jects using the original representation of the objects 
instead of explicitly enumerating their features. 
Many kernels have been proposed and applied to 
the NLP study. In particular, Haussler (1999) pro-
posed the well-known convolution kernels for a 
discrete structure. In the context of it, more and 
more kernels for restricted syntaxes or specific do-
mains (Collins and Duffy, 2001; Lodhi et al, 2002; 
Zelenko et al, 2003; Zhang et al, 2006) are pro-
posed and explored in the NLP domain. 
Of special interest here, Moschitti (2004) proposed 
Predicate Argument Feature (PAF) kernel for SRL 
under the framework of convolution tree kernel. He 
selected portions of syntactic parse trees as predicate-
argument feature spaces, which include salient sub-
structures of predicate-arguments, to define convo-
lution kernels for the task of semantic role classifi-
cation. Under the same framework, Che et al (2006) 
proposed a hybrid convolution tree kernel, which 
consists of two individual convolution kernels: a Path 
kernel and a Constituent Structure kernel. Che et al 
(2006) showed that their method outperformed PAF 
on the CoNLL-2005 SRL dataset.  
The above two kernels are special instances of 
convolution tree kernel for SRL. As discussed in 
Section 1, convolution tree kernel only carries out 
hard matching, so it fails to handle similar phrase 
structures and near-synonymic grammar tags. This 
paper presents a grammar-driven convolution tree 
kernel to solve the two problems 
3 Grammar-driven Convolution Tree 
Kernel 
3.1 Convolution Tree Kernel 
In convolution tree kernel (Collins and Duffy, 
2001), a parse tree T  is represented by a vector of 
integer counts of each sub-tree type (regardless of 
its ancestors): ( )T? = ( ?, # subtreei(T), ?), where 
201
# subtreei(T) is the occurrence number of the ith 
sub-tree type (subtreei) in T. Since the number of 
different sub-trees is exponential with the parse tree 
size, it is computationally infeasible to directly use 
the feature vector ( )T? . To solve this computa-
tional issue, Collins and Duffy (2001) proposed the 
following parse tree kernel to calculate the dot 
product between the above high dimensional vec-
tors implicitly. 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
 ( ) ( )
 ( , )
(( ) ( ))
i isubtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
=< >
=
= ?
?? ? ?
? ?
 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively, and ( )
isubtree
I n  is a function that is 
1 iff the subtreei occurs with root at node n and zero 
otherwise, and 1 2( , )n n?  is the number of the com-
mon subtrees rooted at n1 and n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
1 2( , )n n? can be further computed efficiently by the 
following recursive rules: 
Rule 1: if the productions (CFG rules) at 1n  and 
2n  are different, 1 2( , ) 0n n? = ; 
Rule 2: else if both 1n  and 2n  are pre-terminals 
(POS tags), 1 2( , ) 1n n ?? = ? ; 
Rule 3: else,  
1( )
1 2 1 21
( , ) (1 ( ( , ), ( , )))nc n
j
n n ch n j ch n j?
=
? = + ?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and ? (0< ? <1) is the decay 
factor in order to make the kernel value less vari-
able with respect to the subtree sizes. In addition, 
the recursive Rule 3 holds because given two 
nodes with the same children, one can construct 
common sub-trees using these children and com-
mon sub-trees of further offspring. The time com-
plexity for computing this kernel is 1 2(| | | |)O N N? . 
3.2 Grammar-driven Convolution Tree 
Kernel 
This Subsection introduces the two improvements 
and defines our grammar-driven tree kernel. 
 
Improvement 1: Grammar-driven approximate 
matching between substructures. The conven-
tional tree kernel requires exact matching between 
two contiguous phrase structures. This constraint 
may be too strict. For example, the two phrase 
structures ?NP?DT JJ NN? (NP?a red car) and 
?NP?DT NN? (NP->a car) are not identical, thus 
they contribute nothing to the conventional kernel 
although they should share the same semantic role 
given a predicate. In this paper, we propose a 
grammar-driven approximate matching mechanism 
to capture the similarity between such kinds of 
quasi-structures for SRL. 
First, we construct reduced rule set by defining 
optional nodes, for example, ?NP->DT [JJ] NP? or 
?VP-> VB [ADVP]  PP?, where [*] denotes op-
tional nodes. For convenience, we call ?NP-> DT 
JJ NP? the original rule and ?NP->DT [JJ] NP? the 
reduced rule. Here, we define two grammar-driven 
criteria to select optional nodes: 
1) The reduced rules must be grammatical. It 
means that the reduced rule should be a valid rule 
in the original rule set. For example, ?NP->DT [JJ] 
NP? is valid only when ?NP->DT NP? is a valid 
rule in the original rule set while ?NP->DT [JJ 
NP]? may not be valid since ?NP->DT? is not a 
valid rule in the original rule set. 
2) A valid reduced rule must keep the head 
child of its corresponding original rule and has at 
least two children. This can make the reduced rules 
retain the underlying semantic meaning of their 
corresponding original rules. 
Given the reduced rule set, we can then formu-
late the approximate substructure matching mecha-
nism as follows: 
11 2 1 2,
( , ) ( ( , ) )
a bi ji j
T r ri j
M r r I T T ?
+
= ??              (1)  
where 1r is a production rule, representing a sub-tree 
of depth one3, and 1
i
rT is the i
th variation of the sub-
tree 1r by removing one ore more optional nodes
4, 
and likewise for 2r and 2
j
rT . ( , )TI ? ? is a function 
that is 1 iff the two sub-trees are identical and zero 
otherwise. 1? (0? 1? ?1) is a small penalty to penal-
                                                          
3 Eq.(1) is defined over sub-structure of depth one. The ap-
proximate matching between structures of depth more than one 
can be achieved easily through the matching of sub-structures 
of depth one in the recursively-defined convolution kernel. We 
will discuss this issue when defining our kernel. 
4 To make sure that the new kernel is a proper kernel, we have 
to consider all the possible variations of the original sub-trees. 
Training program converges only when using a proper kernel. 
202
ize optional nodes and the two parameters ia  and 
jb stand for the numbers of occurrence of removed 
optional nodes in subtrees 1
i
rT and 2
j
rT , respectively. 
1 2( , )M r r returns the similarity (ie., the kernel 
value) between the two sub-trees 1r and 2r  by sum-
ming up the similarities between all possible varia-
tions of the sub-trees 1r and 2r . 
Under the new approximate matching mecha-
nism, two structures are matchable (but with a small 
penalty 1? ) if the two structures are identical after 
removing one or more optional nodes. In this case, 
the above example phrase structures ?NP->a red 
car? and ?NP->a car? are matchable with a pen-
alty 1?  in our new kernel. It means that one co-
occurrence of the two structures contributes 1?  to 
our proposed kernel while it contributes zero to the 
traditional one. Therefore, by this improvement, our 
method would be able to explore more linguistically 
appropriate features than the previous one (which is 
formulated as 1 2( , )TI r r ). 
Improvement 2: Grammar-driven tree nodes ap-
proximate matching. The conventional tree kernel 
needs an exact matching between two (termi-
nal/non-terminal) nodes. But, some similar POSs 
may represent similar roles, such as NN (dog) and 
NNS (dogs). In order to capture this phenomenon, 
we allow approximate matching between node fea-
tures. The following illustrates some equivalent 
node feature sets:  
? JJ, JJR, JJS 
? VB, VBD, VBG, VBN, VBP, VBZ 
? ?? 
where POSs in the same line can match each other 
with a small penalty 0? 2? ?1. We call this case 
node feature mutation. This improvement further 
generalizes the conventional tree kernel to get bet-
ter coverage. The approximate node matching can 
be formulated as: 
21 2 1 2,
( , ) ( ( , ) )
a bi ji j
fi j
M f f I f f ?
+
= ??           (2) 
where 1f is a node feature, 1
if is the ith mutation 
of 1f and ia is 0 iff 1
if and 1f are identical and 1 oth-
erwise, and likewise for 2f . ( , )fI ? ? is a function 
that is 1 iff the two features are identical and zero 
otherwise. Eq. (2) sums over all combinations of 
feature mutations as the node feature similarity. 
The same as Eq. (1), the reason for taking all the 
possibilities into account in Eq. (2) is to make sure 
that the new kernel is a proper kernel.  
The above two improvements are grammar-
driven, i.e., the two improvements retain the under-
lying linguistic grammar constraints and keep se-
mantic meanings of original rules. 
 
The Grammar-driven Kernel Definition: Given 
the two improvements discussed above, we can de-
fine the new kernel by beginning with the feature 
vector representation of a parse tree T as follows: 
( )T? =? (# subtree1(T), ?, # subtreen(T))       
where # subtreei(T) is the occurrence number of the 
ith sub-tree type (subtreei) in T. Please note that, 
different from the previous tree kernel, here we 
loosen the condition for the occurrence of a subtree 
by allowing both original and reduced rules (Im-
provement 1) and node feature mutations (Im-
provement 2). In other words, we modify the crite-
ria by which a subtree is said to occur. For example, 
one occurrence of the rule ?NP->DT JJ NP? shall 
contribute 1 times to the feature ?NP->DT JJ NP? 
and 1?  times to the feature ?NP->DT NP? in the 
new kernel while it only contributes 1 times to the 
feature ?NP->DT JJ NP? in the previous one. Now 
we can define the new grammar-driven kernel 
1 2( , )GK T T as follows: 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
( ) ( )
 ( , )
(( ) ( ))
i i
G
subtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
? ?=< >
? ?=
?= ?
?? ? ?
? ?
 (3) 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively. ( )
isubtree
I n?  is a function that is 
1 2
a b? ?? iff the subtreei occurs with root at node n 
and zero otherwise, where a and b are the numbers 
of removed optional nodes and mutated node fea-
tures, respectively. 1 2( , )n n??  is the number of the 
common subtrees rooted at n1 and n2, i.e. , 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? ? ?? = ??         (4) 
Please note that the value of 1 2( , )n n?? is no longer 
an integer as that in the conventional one since op-
tional nodes and node feature mutations are consid-
ered in the new kernel. 1 2( , )n n??  can be further 
computed by the following recursive rules:  
 
203
============================================================================ 
Rule A: if 1n and 2n are pre-terminals, then: 
1 2 1 2( , ) ( , )n n M f f??? = ?                          (5) 
where 1f and 2f are features of nodes 1n and 2n re-
spectively, and 1 2( , )M f f  is defined at Eq. (2).  
Rule B: else if both 1n and 2n are the same non-
terminals, then generate all variations of the subtrees 
of depth one rooted by 1n and 2n (denoted by 1nT  
and 2nT  respectively) by removing different optional 
nodes, then: 
 
1
1
1 2 1 2,
( , )
1 21
( , ) ( ( , )
   (1 ( ( , , ), ( , , )))
a bi ji j
T n ni j
nc n i
k
n n I T T
ch n i k ch n j k
? ?
+
=
?? = ? ?
?? + ?
?
?
(6) 
 
where  
? 1inT and 2jnT stand for the ith and jth variations in 
sub-tree set 1nT and 2nT , respectively. 
? ( , )TI ? ? is a function that is 1 iff the two sub-
trees are identical and zero otherwise.  
? ia and jb stand for the number of removed op-
tional nodes in subtrees 1
i
nT and 2
j
nT , respectively. 
? 1( , )nc n i returns the child number of 1n in its ith 
subtree variation 1
i
nT . 
? 1( , , )ch n i k  is the kth child of node 1n  in its ith 
variation subtree 1
i
nT , and likewise for 2( , , )ch n j k . 
? Finally, the same as the previous tree kernel, 
? (0< ? <1) is the decay factor (see the discussion 
in Subsection 3.1). 
 
Rule C: else 1 2( , ) 0n n?? =  
  
============================================================================ 
 
Rule A accounts for Improvement 2 while Rule 
B accounts for Improvement 1. In Rule B, Eq. (6) 
is able to carry out multi-layer sub-tree approxi-
mate matching due to the introduction of the recur-
sive part while Eq. (1) is only effective for sub-
trees of depth one. Moreover, we note that Eq. (4) 
is a convolution kernel according to the definition 
and the proof given in Haussler (1999), and Eqs (5) 
and (6) reformulate Eq. (4) so that it can be com-
puted efficiently, in this way, our kernel defined by 
Eq (3) is also a valid convolution kernel. Finally, 
let us study the computational issue of the new 
convolution tree kernel. Clearly, computing Eq. (6) 
requires exponential time in its worst case. How-
ever, in practice, it may only need  1 2(| | | |)O N N? . 
This is because there are only 9.9% rules (647 out 
of the total 6,534 rules in the parse trees) have op-
tional nodes and most of them have only one op-
tional node. In fact, the actual running time is even 
much less and is close to linear in the size of the 
trees since 1 2( , ) 0n n?? =  holds for many node 
pairs (Collins and Duffy, 2001). In theory, we can 
also design an efficient algorithm to compute Eq. 
(6) using a dynamic programming algorithm (Mo-
schitti, 2006). We just leave it for our future work. 
3.3 Comparison with previous work 
In above discussion, we show that the conventional 
convolution tree kernel is a special case of the 
grammar-driven tree kernel. From kernel function 
viewpoint, our kernel can carry out not only exact 
matching (as previous one described by Rules 2 
and 3 in Subsection 3.1) but also approximate 
matching (Eqs. (5) and (6) in Subsection 3.2). From 
feature exploration viewpoint, although they ex-
plore the same sub-structure feature space (defined 
recursively by the phrase parse rules), their feature 
values are different since our kernel captures the 
structure features in a more linguistically appropri-
ate way by considering more linguistic knowledge 
in our kernel design. 
Moschitti (2006) proposes a partial tree (PT) 
kernel which can carry out partial matching be-
tween sub-trees. The PT kernel generates a much 
larger feature space than both the conventional and 
the grammar-driven kernels. In this point, one can 
say that the grammar-driven tree kernel is a spe-
cialization of the PT kernel. However, the impor-
tant difference between them is that the PT kernel 
is not grammar-driven, thus many non-
linguistically motivated structures are matched in 
the PT kernel. This may potentially compromise 
the performance since some of the over-generated 
features may possibly be noisy due to the lack of 
linguistic interpretation and constraint. 
Kashima and Koyanagi (2003) proposed a con-
volution kernel over labeled order trees by general-
izing the standard convolution tree kernel. The la-
beled order tree kernel is much more flexible than 
the PT kernel and can explore much larger sub-tree 
features than the PT kernel. However, the same as 
the PT kernel, the labeled order tree kernel is not 
grammar-driven. Thus, it may face the same issues 
204
(such as over-generated features) as the PT kernel 
when used in NLP applications. 
 Shen el al. (2003) proposed a lexicalized tree 
kernel to utilize LTAG-based features in parse 
reranking. Their methods need to obtain a LTAG 
derivation tree for each parse tree before kernel 
calculation. In contrast, we use the notion of op-
tional arguments to define our grammar-driven tree 
kernel and use the empirical set of CFG rules to de-
termine which arguments are optional. 
4 Experiments 
4.1 Experimental Setting 
Data: We use the CoNLL-2005 SRL shared task 
data (Carreras and M?rquez, 2005) as our experi-
mental corpus. The data consists of sections of the 
Wall Street Journal part of the Penn TreeBank 
(Marcus et al, 1993), with information on predi-
cate-argument structures extracted from the Prop-
Bank corpus (Palmer et al, 2005). As defined by 
the shared task, we use sections 02-21 for training, 
section 24 for development and section 23 for test. 
There are 35 roles in the data including 7 Core 
(A0?A5, AA), 14 Adjunct (AM-) and 14 Reference 
(R-) arguments. Table 1 lists counts of sentences 
and arguments in the three data sets. 
  
 Training Development Test
Sentences 39,832 1,346 2,416
Arguments 239,858 8,346 14,077
Table 1: Counts on the data set 
 
We assume that the semantic role identification 
has been done correctly. In this way, we can focus 
on the classification task and evaluate it more accu-
rately. We evaluate the performance with Accu-
racy. SVM (Vapnik, 1998) is selected as our classi-
fier and the one vs. others strategy is adopted and 
the one with the largest margin is selected as the 
final answer. In our implementation, we use the bi-
nary SVMLight (Joachims, 1998) and modify the 
Tree Kernel Tools (Moschitti, 2004) to a grammar-
driven one. 
 
Kernel Setup: We use the Constituent, Predicate, 
and Predicate-Constituent related features, which 
are reported to get the best-reported performance 
(Pradhan et al, 2005a), as the baseline features. We 
use Che et al (2006)?s hybrid convolution tree ker-
nel (the best-reported method for kernel-based 
SRL) as our baseline kernel. It is defined as 
(1 )  (0 1)hybrid path csK K K? ? ?= + ? ? ? (for the de-
tailed definitions of pathK and csK , please refer to 
Che et al (2006)). Here, we use our grammar-
driven tree kernel to compute pathK and csK , and we 
call it grammar-driven hybrid tree kernel while Che 
et al (2006)?s is non-grammar-driven hybrid convo-
lution tree kernel.  
We use a greedy strategy to fine-tune parameters. 
Evaluation on the development set shows that our 
kernel yields the best performance when ? (decay 
factor of tree kernel), 1? and 2? (two penalty factors 
for the grammar-driven kernel), ? (hybrid kernel 
parameter) and c (a SVM training parameter to 
balance training error and margin) are set to 0.4, 
0.6, 0.3, 0.6 and 2.4, respectively. For other parame-
ters, we use default setting. In the CoNLL 2005 
benchmark data, we get 647 rules with optional 
nodes out of the total 6,534 grammar rules and de-
fine three equivalent node feature sets as below: 
? JJ, JJR, JJS 
? RB, RBR, RBS 
? NN, NNS, NNP, NNPS, NAC, NX 
 
Here, the verb feature set ?VB, VBD, VBG, VBN, 
VBP, VBZ? is removed since the voice information 
is very indicative to the arguments of ARG0 
(Agent, operator) and ARG1 (Thing operated). 
 
Methods Accuracy (%) 
 Baseline: Non-grammar-driven 85.21 
 +Approximate Node Matching 86.27 
 +Approximate Substructure 
Matching 
87.12 
 Ours: Grammar-driven Substruc-
ture and Node Matching 
87.96 
Feature-based method with poly-
nomial kernel (d = 2) 
89.92 
 
Table 2: Performance comparison 
4.2 Experimental Results 
Table 2 compares the performances of different 
methods on the test set. First, we can see that the 
new grammar-driven hybrid convolution tree kernel 
significantly outperforms ( 2? test with p=0.05) the 
205
non-grammar one with an absolute improvement of 
2.75 (87.96-85.21) percentage, representing a rela-
tive error rate reduction of 18.6% (2.75/(100-85.21)) 
. It suggests that 1) the linguistically motivated 
structure features are very useful for semantic role 
classification and 2) the grammar-driven kernel is 
much more effective in capturing such kinds of fea-
tures due to the consideration of linguistic knowl-
edge. Moreover, Table 2 shows that 1) both the 
grammar-driven approximate node matching and the 
grammar-driven approximate substructure matching 
are very useful in modeling syntactic tree structures 
for SRL since they contribute relative error rate re-
duction of 7.2% ((86.27-85.21)/(100-85.21)) and 
12.9% ((87.12-85.21)/(100-85.21)), respectively; 2) 
the grammar-driven approximate substructure 
matching is more effective than the grammar-driven 
approximate node matching. However, we find that 
the performance of the grammar-driven kernel is 
still a bit lower than the feature-based method. This 
is not surprising since tree kernel methods only fo-
cus on modeling tree structure information. In this 
paper, it captures the syntactic parse tree structure 
features only while the features used in the feature-
based methods cover more knowledge sources.  
In order to make full use of the syntactic structure 
information and the other useful diverse flat fea-
tures, we present a composite kernel to combine the 
grammar-driven hybrid kernel and feature-based 
method with polynomial kernel: 
(1 )      (0 1)comp hybrid polyK K K? ? ?= + ? ? ?  
Evaluation on the development set shows that the 
composite kernel yields the best performance when 
? is set to 0.3. Using the same setting, the system 
achieves the performance of 91.02% in Accuracy 
in the same test set. It shows statistically significant 
improvement (?2 test with p= 0.10) over using the 
standard features with the polynomial kernel (? = 0, 
Accuracy = 89.92%) and using the grammar-driven 
hybrid convolution tree kernel (? = 1, Accuracy = 
87.96%). The main reason is that the tree kernel 
can capture effectively more structure features 
while the standard flat features can cover some 
other useful features, such as Voice, SubCat, which 
are hard to be covered by the tree kernel. The ex-
perimental results suggest that these two kinds of 
methods are complementary to each other. 
In order to further compare with other methods, 
we also do experiments on the dataset of English 
PropBank I (LDC2004T14). The training, develop-
ment and test sets follow the conventional split of 
Sections 02-21, 00 and 23. Table 3 compares our 
method with other previously best-reported methods 
with the same setting as discussed previously. It 
shows that our method outperforms the previous 
best-reported one with a relative error rate reduction 
of 10.8% (0.97/(100-91)). This further verifies the 
effectiveness of the grammar-driven kernel method 
for semantic role classification. 
  
Method Accuracy (%)
Ours (Composite Kernel)      91.97 
Moschitti (2006): PAF kernel only    87.7 
Jiang et al (2005): feature based    90.50 
Pradhan et al (2005a): feature based    91.0 
 
Table 3: Performance comparison between our 
method and previous work 
 
Training Time Method 
  4 Sections  19 Sections
Ours: grammar-
driven tree kernel 
~8.1 hours ~7.9 days 
Moschitti (2006): 
non-grammar-driven 
tree kernel 
~7.9 hours ~7.1 days 
 
Table 4: Training time comparison 
 
Table 4 reports the training times of the two ker-
nels. We can see that 1) the two kinds of convolu-
tion tree kernels have similar computing time. Al-
though computing the grammar-driven one requires 
exponential time in its worst case, however, in 
practice, it may only need 1 2(| | | |)O N N?  or lin-
ear and 2) it is very time-consuming to train a SVM 
classifier in a large dataset.  
5 Conclusion and Future Work 
In this paper, we propose a novel grammar-driven 
convolution tree kernel for semantic role classifica-
tion. More linguistic knowledge is considered in 
the new kernel design. The experimental results 
verify that the grammar-driven kernel is more ef-
fective in capturing syntactic structure features than 
the previous convolution tree kernel because it al-
lows grammar-driven approximate matching of 
substructures and node features. We also discuss 
the criteria to determine the optional nodes in a 
206
CFG rule in defining our grammar-driven convolu-
tion tree kernel. 
The extension of our work is to improve the per-
formance of the entire semantic role labeling system 
using the grammar-driven tree kernel, including all 
four stages: pruning, semantic role identification, 
classification and post inference. In addition, a 
more interesting research topic is to study how to 
integrate linguistic knowledge and tree kernel 
methods to do feature selection for tree kernel-
based NLP applications (Suzuki et al, 2004). In 
detail, a linguistics and statistics-based theory that 
can suggest the effectiveness of different substruc-
ture features and whether they should be generated 
or not by the tree kernels would be worked out. 
References  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The 
Berkeley FrameNet Project. COLING-ACL-1998  
Xavier Carreras and Llu?s M?rquez. 2004. Introduction to 
the CoNLL-2004 shared task: Semantic role labeling. 
CoNLL-2004  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction to 
the CoNLL-2005 shared task: Semantic role labeling. 
CoNLL-2005  
Eugene Charniak. 2000. A maximum-entropy-inspired 
parser. In Proceedings ofNAACL-2000 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A hybrid convolution tree kernel for semantic 
role labeling. COLING-ACL-2006(poster) 
Michael Collins and Nigel Duffy. 2001. Convolution 
kernels for natural language. NIPS-2001 
 Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics, 
28(3):245?288 
David Haussler. 1999. Convolution kernels on discrete 
structures. Technical Report UCSC-CRL-99-10 
Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument 
interdependence. IJCAI-2005 
T. Joachims. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kashima H. and Koyanagi T. 2003. Kernels for Semi-
Structured Data. ICML-2003 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello 
Cristianini and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learn-
ing Research, 2:419?444 
Mitchell P. Marcus, Mary Ann Marcinkiewicz  and Bea-
trice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational 
Linguistics, 19(2):313?330 
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. ACL-2004 
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling 
case. HLT-NAACL-2006 (short paper)  
Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing 
weak learners in semantic parsing. EMNLP-2004 
Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. 
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1) 
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne 
Ward, James H. Martin and Daniel Jurafsky. 2005a. 
Support vector learning for semantic argument classi-
fication. Journal of Machine Learning 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James 
Martin and Daniel Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. ACL-2005 
Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi-
mak. 2004. Semantic role labeling via integer linear 
programming inference. COLING-2004 
Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005. 
The necessity of syntactic parsing for semantic role 
labeling. IJCAI-2005 
Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using 
LTAG based features in parse reranking. EMNLP-03 
Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004. 
Convolution kernels with feature selection for Natu-
ral Language processing tasks. ACL-2004 
Vladimir N. Vapnik. 1998. Statistical Learning Theory. 
Wiley 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP-2004 
Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-
ardella. 2003. Kernel methods for relation extraction. 
Machine Learning Research, 3:1083?1106 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. COLING-ACL-2006 
207
Proceedings of ACL-08: HLT, pages 559?567,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Tree Sequence Alignment-based Tree-to-Tree Translation Model 
 
 
Min Zhang1  Hongfei Jiang2  Aiti Aw1  Haizhou Li1  Chew Lim Tan3 and Sheng Li2
1Institute for Infocomm Research 2Harbin Institute of Technology 3National University of Singapore
mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg 
aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn  
hli@i2r.a-star.edu.sg   
 
  
Abstract 
This paper presents a translation model that is 
based on tree sequence alignment, where a tree 
sequence refers to a single sequence of sub-
trees that covers a phrase. The model leverages 
on the strengths of both phrase-based and lin-
guistically syntax-based method. It automati-
cally learns aligned tree sequence pairs with 
mapping probabilities from word-aligned bi-
parsed parallel texts. Compared with previous 
models, it not only captures non-syntactic 
phrases and discontinuous phrases with lin-
guistically structured features, but also sup-
ports multi-level structure reordering of tree 
typology with larger span. This gives our 
model stronger expressive power than other re-
ported models. Experimental results on the 
NIST MT-2005 Chinese-English translation 
task show that our method statistically signifi-
cantly outperforms the baseline systems.  
1 Introduction 
Phrase-based modeling method (Koehn et al, 
2003; Och and Ney, 2004a) is a simple, but power-
ful mechanism to machine translation since it can 
model local reorderings and translations of multi-
word expressions well. However, it cannot handle 
long-distance reorderings properly and does not 
exploit discontinuous phrases and linguistically 
syntactic structure features (Quirk and Menezes, 
2006). Recently, many syntax-based models have 
been proposed to address the above deficiencies 
(Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and 
Palmer, 2005; Quirk et al 2005; Cowan et al, 
2006; Zhang et al, 2007; Bod, 2007; Yamada and 
Knight, 2001; Liu et al, 2006; Liu et al, 2007; 
Gildea, 2003; Poutsma, 2000; Hearne and Way, 
2003). Although good progress has been reported, 
the fundamental issues in applying linguistic syn-
tax to SMT, such as non-isomorphic tree align-
ment, structure reordering and non-syntactic phrase 
modeling, are still worth well studying. 
In this paper, we propose a tree-to-tree transla-
tion model that is based on tree sequence align-
ment. It is designed to combine the strengths of 
phrase-based and syntax-based methods. The pro-
posed model adopts tree sequence 1  as the basic 
translation unit and utilizes tree sequence align-
ments to model the translation process. Therefore, 
it not only describes non-syntactic phrases with 
syntactic structure information, but also supports 
multi-level tree structure reordering in larger span. 
These give our model much more expressive 
power and flexibility than those previous models. 
Experiment results on the NIST MT-2005 Chinese-
English translation task show that our method sig-
nificantly outperforms Moses (Koehn et al, 2007), 
a state-of-the-art phrase-based SMT system, and 
other linguistically syntax-based methods, such as 
SCFG-based and STSG-based methods (Zhang et 
al., 2007). In addition, our study further demon-
strates that 1) structure reordering rules in our 
model are very useful for performance improve-
ment while discontinuous phrase rules have less 
contribution and 2) tree sequence rules are able to 
model non-syntactic phrases with syntactic struc-
ture information, and thus contribute much to the 
performance improvement, but those rules consist-
ing of more than three sub-trees have almost no 
contribution.  
The rest of this paper is organized as follows: 
Section 2 reviews previous work. Section 3 elabo-
                                                          
1 A tree sequence refers to an ordered sub-tree sequence that 
covers a phrase or a consecutive tree fragment in a parse tree. 
It is the same as the concept ?forest? used in Liu et al(2007).  
559
rates the modelling process while Sections 4 and 5 
discuss the training and decoding algorithms. The 
experimental results are reported in Section 6. Fi-
nally, we conclude our work in Section 7. 
2 Related Work 
Many techniques on linguistically syntax-based 
SMT have been proposed in literature. Yamada 
and Knight (2001) use noisy-channel model to 
transfer a target parse tree into a source sentence. 
Eisner (2003) studies how to learn non-isomorphic 
tree-to-tree/string mappings using a STSG. Ding 
and Palmer (2005) propose a syntax-based transla-
tion model based on a probabilistic synchronous 
dependency insertion grammar. Quirk et al (2005) 
propose a dependency treelet-based translation 
model. Cowan et al (2006) propose a feature-
based discriminative model for target language 
syntactic structures prediction, given a source 
parse tree. Huang et al (2006) study a TSG-based 
tree-to-string alignment model. Liu et al (2006) 
propose a tree-to-string model. Zhang et al 
(2007b) present a STSG-based tree-to-tree transla-
tion model. Bod (2007) reports that the unsuper-
vised STSG-based translation model performs 
much better than the supervised one. The motiva-
tion behind all these work is to exploit linguistical-
ly syntactic structure features to model the 
translation process. However, most of them fail to 
utilize non-syntactic phrases well that are proven 
useful in the phrase-based methods (Koehn et al, 
2003). 
The formally syntax-based model for SMT was 
first advocated by Wu (1997). Xiong et al (2006) 
propose a MaxEnt-based reordering model for 
BTG (Wu, 1997) while Setiawan et al (2007) pro-
pose a function word-based reordering model for 
BTG. Chiang (2005)?s hierarchal phrase-based 
model achieves significant performance improve-
ment. However, no further significant improve-
ment is achieved when the model is made sensitive 
to syntactic structures by adding a constituent fea-
ture (Chiang, 2005). 
In the last two years, many research efforts were 
devoted to integrating the strengths of phrase-
based and syntax-based methods. In the following, 
we review four representatives of them.   
1) Hassan et al (2007) integrate supertags (a 
kind of lexicalized syntactic description) into the 
target side of translation model and language mod-
el under the phrase-based translation framework, 
resulting in good performance improvement. How-
ever, neither source side syntactic knowledge nor 
reordering model is further explored.  
2) Galley et al (2006) handle non-syntactic 
phrasal translations by traversing the tree upwards 
until a node that subsumes the phrase is reached. 
This solution requires larger applicability contexts 
(Marcu et al, 2006). However, phrases are utilized 
independently in the phrase-based method without 
depending on any contexts.  
3) Addressing the issues in Galley et al (2006), 
Marcu et al (2006) create an xRS rule headed by a 
pseudo, non-syntactic non-terminal symbol that 
subsumes the phrase and its corresponding multi-
headed syntactic structure; and one sibling xRS 
rule that explains how the pseudo symbol can be 
combined with other genuine non-terminals for 
acquiring the genuine parse trees. The name of the 
pseudo non-terminal is designed to reflect the full 
realization of the corresponding rule. The problem 
in this method is that it neglects alignment consis-
tency in creating sibling rules and the naming me-
chanism faces challenges in describing more 
complicated phenomena (Liu et al, 2007).  
4) Liu et al (2006) treat all bilingual phrases as 
lexicalized tree-to-string rules, including those 
non-syntactic phrases in training corpus. Although 
the solution shows effective empirically, it only 
utilizes the source side syntactic phrases of the in-
put parse tree during decoding. Furthermore, the 
translation probabilities of the bilingual phrases 
and other tree-to-string rules are not compatible 
since they are estimated independently, thus hav-
ing different parameter spaces. To address the 
above problems, Liu et al (2007) propose to use 
forest-to-string rules to enhance the expressive 
power of their tree-to-string model. As is inherent 
in a tree-to-string framework, Liu et al?s method 
defines a kind of auxiliary rules to integrate forest-
to-string rules into tree-to-string models. One prob-
lem of this method is that the auxiliary rules are 
not described by probabilities since they are con-
structed during decoding, rather than learned from 
the training corpus. So, to balance the usage of dif-
ferent kinds of rules, they use a very simple feature 
counting the number of auxiliary rules used in a 
derivation for penalizing the use of forest-to-string 
and auxiliary rules. 
In this paper, an alternative solution is presented 
to combine the strengths of phrase-based and syn-
560
1( )
IT e
1( )
JT f
A
 
 
Figure 1: A word-aligned parse tree pairs of a Chi-
nese sentence and its English translation  
 
 
 
Figure 2: Two Examples of tree sequences 
 
 
 
Figure 3: Two examples of translation rules 
tax-based methods. Unlike previous work, our so-
lution neither requires larger applicability contexts 
(Galley et al, 2006), nor depends on pseudo nodes 
(Marcu et al, 2006) or auxiliary rules (Liu et al, 
2007). We go beyond the single sub-tree mapping 
model to propose a tree sequence alignment-based 
translation model. To the best of our knowledge, 
this is the first attempt to empirically explore the 
tree sequence alignment based model in SMT.  
3 Tree Sequence Alignment Model 
3.1 Tree Sequence Translation Rule   
The leaf nodes of a sub-tree in a tree sequence can 
be either non-terminal symbols (grammar tags) or 
terminal symbols (lexical words). Given a pair of 
source and target parse trees 1( )
JT f and 1( )
IT e  in 
Fig. 1, Fig. 2 illustrates two examples of tree se-
quences derived from the two parse trees. A tree 
sequence translation rule r  is a pair of aligned tree 
sequences r =< 2
1
( )jjTS f , 21( )
i
iTS e , A%  >, where: 
z 2
1
( )jjTS f is a source tree sequence, covering 
the span [ 1 2,j j ] in 1( )
JT f , and 
z 2
1
( )iiTS e is a target one, covering the span 
[ 1 2,i i ] in 1( )
IT e , and 
z A% are the alignments between leaf nodes of 
two tree sequences, satisfying the following 
condition: 1 2 1 2( , ) :i j A i i i j j j? ? ? ? ? ? ?% . 
Fig. 3 shows two rules extracted from the tree pair 
shown in Fig. 1, where r1 is a tree-to-tree rule and 
r2 is a tree sequence-to-tree sequence rule. Ob-
viously, tree sequence rules are more powerful 
than phrases or tree rules as they can capture all 
phrases (including both syntactic and non-syntactic 
phrases) with syntactic structure information and 
allow any tree node operations in a longer span. 
We expect that these properties can well address 
the issues of non-isomorphic structure alignments, 
structure reordering, non-syntactic phrases and 
discontinuous phrases translations. 
3.2 Tree Sequence Translation Model 
Given the source and target sentences 1
Jf and 1
Ie  
and their parse trees 1( )
JT f and 1( )
IT e , the tree 
sequence-to-tree sequence translation model is 
formulated as: 
1 1
1 1
1 1 1 1 1 1
( ), ( )
1 1
( ), ( )
1 1 1
1 1 1 1
( | ) ( , ( ), ( ) | )
( ( ( ) | )
( ( ) | ( ), )
( | ( ), ( ), ))
                
                      
                      
J I
J I
I J I I J J
T f T e
J J
T f T e
I J J
I I J J
r r
r
r
r
P e f P e T e T f f
P T f f
P T e T f f
P e T e T f f
=
=
?
?
?
? (1) 
In our implementation, we have: 
561
1) 1 1( ( ) | ) 1
J JrP T f f ? since we only use the best 
source and target parse tree pairs in training. 
2) 1 1 1 1( | ( ), ( ), ) 1
I I J JrP e T e T f f ? since we just 
output the leaf nodes of 1( )
IT e to generate 1
Ie  
regardless of source side information. 
Since 1( )
JT f contains the information of 1
Jf , 
now we have: 
1 1 1 1 1
1 1
( | ) ( ( ) | ( ), )
                 ( ( ) | ( ))
I J I J J
I J
r r
r
P e f P T e T f f
P T e T f
=
=
           (2) 
By Eq. (2), translation becomes a tree structure 
mapping issue. We model it using our tree se-
quence-based translation rules. Given the source 
parse tree 1( )
JT f , there are multiple derivations 
that could lead to the same target tree 1( )
IT e , the 
mapping probability 1 1( ( ) | ( ))
I JrP T e T f is obtained 
by summing over the probabilities of all deriva-
tions. The probability of each derivation? is given 
as the product of the probabilities of all the rules 
( )ip r  used in the derivation (here we assume that 
a rule is applied independently in a derivation). 
2 2
1 1
1 1 1 1( | ) ( ( ) | ( ))
     = ( : ( ), ( ), )
i
I J I J
i j
i i j
r
r rP e f P T e T f
p r TS e TS f A
? ??
=
< >?? %    (3) 
Eq. (3) formulates the tree sequence alignment-
based translation model. Figs. 1 and 3 show how 
the proposed model works. First, the source sen-
tence is parsed into a source parse tree. Next, the 
source parse tree is detached into two source tree 
sequences (the left hand side of rules in Fig. 3). 
Then the two rules in Fig. 3 are used to map the 
two source tree sequences to two target tree se-
quences, which are then combined to generate a 
target parse tree. Finally, a target translation is 
yielded from the target tree.  
Our model is implemented under log-linear 
framework (Och and Ney, 2002). We use seven 
basic features that are analogous to the commonly 
used features in phrase-based systems (Koehn, 
2004): 1) bidirectional rule mapping probabilities; 
2) bidirectional lexical rule translation probabilities; 
3) the target language model; 4) the number of 
rules used and 5) the number of target words. In 
addition, we define two new features: 1) the num-
ber of lexical words in a rule to control the model?s 
preference for lexicalized rules over un-lexicalized 
rules and 2) the average tree depth in a rule to bal-
ance the usage of hierarchical rules and flat rules. 
Note that we do not distinguish between larger (tal-
ler) and shorter source side tree sequences, i.e. we 
let these rules compete directly with each other. 
4 Rule Extraction 
Rules are extracted from word-aligned, bi-parsed 
sentence pairs 1 1( ), ( ),
J IT f T e A< > , which are 
classified into two categories: 
z initial rule, if all leaf nodes of the rule are 
terminals (i.e. lexical word), and 
z abstract rule, otherwise, i.e. at least one leaf 
node is a non-terminal (POS or phrase tag). 
Given an initial rule 2 2
1 1
( ), ( ),j ij iTS f TS e A< >% , 
its sub initial rule is defined as a triple 
4 4
3 3
?( ), ( ),j ij iTS f TS e A< >  if and only if: 
z 4 4
3 3
?( ), ( ),j ij iTS f TS e A< > is an initial rule. 
z 3 4 3 4( , ) :i j A i i i j j j? ? ? ? ? ? ?% , i.e. 
A? A? %  
z 4
3
( )jjTS f is a sub-graph of 21( )
j
jTS f while  
4
3
( )iiTS e  is a sub-graph of 21( )
i
iTS e . 
Rules are extracted in two steps: 
1) Extracting initial rules first. 
2) Extracting abstract rules from extracted ini-
tial rules with the help of sub initial rules. 
It is straightforward to extract initial rules. We 
first generate all fully lexicalized source and target 
tree sequences using a dynamic programming algo-
rithm and then iterate over all generated source and 
target tree sequence pairs 2 2
1 1
( ), ( )j ij iTS f TS e< > . If 
the condition ? ( , )i j? 1 2 1 2:A i i i j j j? ? ? ? ? ? ? 
is satisfied, the triple 2 2
1 1
( ), ( ),j ij iTS f TS e A< >% is 
an initial rule, where A%  are alignments between 
leaf nodes of 2
1
( )jjTS f  and 21( )
i
iTS e . We then de-
rive abstract rules from initial rules by removing 
one or more of its sub initial rules. The abstract 
rule extraction algorithm presented next is imple-
mented using dynamic programming. Due to space 
limitation, we skip the details here. In order to con-
trol the number of rules, we set three constraints 
for both finally extracted initial and abstract rules:  
1) The depth of a tree in a rule is not greater 
562
than h . 
2) The number of non-terminals as leaf nodes is 
not greater than c . 
3) The tree number in a rule is not greater than d. 
In addition, we limit initial rules to have at most 
seven lexical words as leaf nodes on either side. 
However, in order to extract long-distance reorder-
ing rules, we also generate those initial rules with 
more than seven lexical words for abstract rules 
extraction only (not used in decoding). This makes 
our abstract rules more powerful in handling 
global structure reordering. Moreover, by configur-
ing these parameters we can implement other 
translation models easily: 1) STSG-based model  
when 1d = ; 2) SCFG-based model when 1d =  
and 2h = ; 3) phrase-based translation model only 
(no reordering model) when 0c =  and 1h = . 
 
Algorithm 1: abstract rules extraction 
Input: initial rule set inir  
Output: abstract rule set absr  
1: for each i inir r? , do 
2:    put all sub initial rules of ir  into a set subiniir
3:    for each subset subiniir? ? do 
4:          if there are spans overlapping between 
any two rules in the subset ?  then 
5:                    continue   //go to line 3 
6:           end if  
7:           generate an abstract rule by removing 
the portions covered by ?  from ir  and 
co-indexing the pairs of non-terminals 
that rooting the removed source and 
target parts 
8:           add them into the abstract rule set absr  
9:     end do 
10: end do  
 
5 Decoding 
Given 1( )
JT f , the decoder is to find the best deri-
vation ?  that generates < 1( )JT f , 1( )IT e >.  
1
1
1 1
,
? arg max ( ( ) | ( ))
  arg max ( )
I
I
i
I J
e
i
e r
re P T e T f
p r
? ??
=
? ?              (4) 
Algorithm 2: Tree Sequence-based Decoder 
 Input: 1( )
JT f   Output: 1( )
IT e  
 Data structures: 
1 2[ , ]h j j    To store translations to a span 1 2[ , ]j j  
1: for s = 0 to J -1 do      // s: span length 
2:     for 1j = 1 to J - s , 2j = 1j + s  do  
3:          for each rule r spanning 1 2[ , ]j j  do  
4:               if r  is an initial rule then 
5:                    insert r into 1 2[ , ]h j j  
6:               else 
7:      generate new translations from 
r by replacing non-terminal leaf 
nodes of r with their correspond-
ing spans? translations that are al-
ready translated in previous steps 
8:      insert them into 1 2[ , ]h j j  
9:  end if 
10: end for 
11: end for 
12: end for 
13: output the hypothesis with the highest score  
in [1, ]h J  as the final best translation 
 
The decoder is a span-based beam search to-
gether with a function for mapping the source deri-
vations to the target ones. Algorithm 2 illustrates 
the decoding algorithm. It translates each span ite-
ratively from small one to large one (lines 1-2).  
This strategy can guarantee that when translating 
the current span, all spans smaller than the current 
one have already been translated before if they are 
translatable (line 7). When translating a span, if the 
usable rule is an initial rule, then the tree sequence 
on the target side of the rule is a candidate transla-
tion (lines 4-5). Otherwise, we replace the non-
terminal leaf nodes of the current abstract rule 
with their corresponding spans? translations that 
are already translated in previous steps (line 7). To 
speed up the decoder, we use several thresholds to 
limit search beams for each span:  
1) ? , the maximal number of rules used 
2) ? , the minimal log probability of rules 
3) ? , the maximal number of translations yield  
It is worth noting that the decoder does not force 
a complete target parse tree to be generated. If no 
rules can be used to generate a complete target 
parse tree, the decoder just outputs whatever have 
563
been translated so far monotonically as one hy-
pothesis. 
6 Experiments 
6.1 Experimental Settings 
We conducted Chinese-to-English translation ex-
periments. We trained the translation model on the 
FBIS corpus (7.2M+9.2M words) and trained a 4-
gram language model on the Xinhua portion of the 
English Gigaword corpus (181M words) using the 
SRILM Toolkits (Stolcke, 2002) with modified 
Kneser-Ney smoothing. We used sentences with 
less than 50 characters from the NIST MT-2002 
test set as our development set and the NIST MT-
2005 test set as our test set. We used the Stanford 
parser (Klein and Manning, 2003) to parse bilin-
gual sentences on the training set and Chinese sen-
tences on the development and test sets. The 
evaluation metric is case-sensitive BLEU-4 (Papi-
neni et al, 2002). We used GIZA++ (Och and Ney, 
2004) and the heuristics ?grow-diag-final? to gen-
erate m-to-n word alignments. For the MER train-
ing (Och, 2003), we modified Koehn?s MER 
trainer (Koehn, 2004) for our tree sequence-based 
system. For significance test, we used Zhang et als 
implementation (Zhang et al 2004). 
We set three baseline systems: Moses (Koehn et 
al., 2007), and SCFG-based and STSG-based tree-
to-tree translation models (Zhang et al, 2007). For 
Moses, we used its default settings. For the 
SCFG/STSG and our proposed model, we used the 
same settings except for the parameters d and h  
( 1d = and 2h = for the SCFG; 1d = and 6h = for 
the STSG; 4d =  and 6h = for our model). We 
optimized these parameters on the training and de-
velopment sets: c =3, ? =20, ? =-100 and ? =100. 
6.2 Experimental Results   
We carried out a number of experiments to ex-
amine the proposed tree sequence alignment-based 
translation model. In this subsection, we first re-
port the rule distributions and compare our model 
with the three baseline systems. Then we study the 
model?s expressive ability by comparing the con-
tributions made by different kinds of rules, includ-
ing strict tree sequence rules, non-syntactic phrase 
rules, structure reordering rules and discontinuous 
phrase rules2. Finally, we investigate the impact of 
maximal sub-tree number and sub-tree depth in our 
model. All of the following discussions are held on 
the training and test data. 
 
 
Rule 
 Initial Rules  Abstract Rules  
L P U Total 
BP 322,965 0 0  322,965
TR 443,010 144,459 24,871  612,340
TSR 225,570 103,932 714  330,216
 
Table 1: # of rules used in the testing ( 4d = , h =  6) 
(BP: bilingual phrase (used in Moses), TR: tree rule (on-
ly 1 tree), TSR: tree sequence rule (> 1 tree), L: fully 
lexicalized, P: partially lexicalized, U: unlexicalized) 
 
Table 1 reports the statistics of rules used in the 
experiments. It shows that:  
1) We verify that the BPs are fully covered by 
the initial rules (i.e. lexicalized rules), in which the 
lexicalized TSRs model all non-syntactic phrase 
pairs with rich syntactic information. In addition, 
we find that the number of initial rules is greater 
than that of bilingual phrases. This is because one 
bilingual phrase can be covered by more than one 
initial rule which having different sub-tree struc-
tures. 
2) Abstract rules generalize initial rules to un-
seen data and with structure reordering ability. The 
number of the abstract rule is far less than that of 
the initial rules. This is because leaf nodes of an 
abstract rule can be non-terminals that can 
represent any sub-trees using the non-terminals as 
roots.   
Fig. 4 compares the performance of different 
models. It illustrates that: 
1) Our tree sequence-based model significantly 
outperforms (p < 0.01) previous phrase-based and 
linguistically syntax-based methods. This empirical-
ly verifies the effect of the proposed method. 
2) Both our method and STSG outperform Mos-
es significantly. Our method also clearly outper-
forms STSG. These results suggest that: 
z The linguistically motivated structure features 
are very useful for SMT, which can be cap-
                                                          
2 To be precise, we examine the contributions of strict tree 
sequence rules and single tree rules separately in this section. 
Therefore, unless specified, the term ?tree sequence rules? 
used in this section only refers to the strict tree sequence rules, 
which must contain at least two sub-trees on the source side. 
564
tured by the two syntax-based models through 
tree node operations. 
z Our model is much more effective in utilizing 
linguistic structures than STSG since it uses 
tree sequence as basic translation unit. This 
allows our model not only to handle structure 
reordering by tree node operations in a larger 
span, but also to capture non-syntactic phras-
es, which circumvents previous syntactic 
constraints, thus giving our model more ex-
pressive power. 
3) The linguistically motivated SCFG shows 
much lower performance. This is largely because 
SCFG only allows sibling nodes reordering and fails 
to utilize both non-syntactic phrases and those syn-
tactic phrases that cannot be covered by a single 
CFG rule. It thereby suggests that SCFG is less 
effective in modelling parse tree structure transfer 
between Chinese and English when using Penn 
Treebank style linguistic grammar and under word-
alignment constraints. However, formal SCFG 
show much better performance in the formally syn-
tax-based translation framework (Chiang, 2005). 
This is because the formal syntax is learned from 
phrases directly without relying on any linguistic 
theory (Chiang, 2005). As a result, it is more ro-
bust to the issue of non-syntactic phrase usage and 
non-isomorphic structure alignment.  
24.71
26.07
23.86
22.72
21.5
22.5
23.5
24.5
25.5
26.5
SCFG Moses STSG Ours
BL
EU
(%
)
 
Figure 4: Performance comparison of different methods 
 
Rule  
Type 
TR 
(STSG) 
TR 
+TSR_L 
TR+TSR_L
+TSR_P 
TR 
+TSR 
BLEU(%) 24.71 25.72 25.93 26.07 
 
Table 2: Contributions of TSRs (see Table 1 for the de-
finitions of the abbreviations used in this table) 
 
Table 2 measures the contributions of different 
kinds of tree sequence rules. It suggests that: 
1) All the three kinds of TSRs contribute to the 
performance improvement and their combination 
further improves the performance. It suggests that 
they are complementary to each other since the 
lexicalized TSRs are used to model non-syntactic 
phrases while the other two kinds of TSRs can ge-
neralize the lexicalized rules to unseen phrases. 
2)  The lexicalized TSRs make the major con-
tribution since they can capture non-syntactic 
phrases with syntactic structure features. 
 
Rule Type BLEU (%) 
TR+TSR 26.07 
(TR+TSR) w/o SRR 24.62 
(TR+TSR) w/o DPR 25.78 
 
Table 3: Effect of Structure Reordering Rules (SRR: 
refers to the structure reordering rules that have at least 
two non-terminal leaf nodes with inverted order in the 
source and target sides, which are usually not captured 
by phrase-based models. Note that the reordering be-
tween lexical words and non-terminal leaf nodes is not 
considered here) and Discontinuous Phrase Rules (DPR: 
refers to these rules having at least one non-terminal 
leaf node between two lexicalized leaf nodes) in our 
tree sequence-based model ( 4d =  and 6h = ) 
 
Rule Type # of rules # of rules overlapped 
(Intersection) 
SRR 68,217 18,379 (26.9%) 
DPR 57,244 18,379 (32.1%) 
 
Table 4: numbers of SRR and DPR rules 
 
Table 3 shows the contributions of SRR and 
DPR. It clearly indicates that SRRs are very effec-
tive in reordering structures, which improve per-
formance by 1.45 (26.07-24.62) BLEU score. 
However, DPRs have less impact on performance 
in our tree sequence-based model. This seems in 
contradiction to the previous observations3 in lite-
rature. However, it is not surprising simply be-
cause we use tree sequences as the basic translation 
units. Thereby, our model can capture all phrases. 
In this sense, our model behaves like a phrase-
based model, less sensitive to discontinuous phras-
                                                          
3 Wellington et al (2006) reports that discontinuities are very 
useful for translational equivalence analysis using binary-
branching structures under word alignment and parse tree 
constraints while they are almost of no use if under word 
alignment constraints only. Bod (2007) finds that discontinues 
phrase rules make significant performance improvement in 
linguistically STSG-based SMT models. 
565
es (Wellington et al, 2006). Our additional expe-
riments also verify that discontinuous phrase rules 
are complementary to syntactic phrase rules (Bod, 
2007) while non-syntactic phrase rules may com-
promise the contribution of discontinuous phrase 
rules. Table 4 reports the numbers of these two 
kinds of rules. It shows that around 30% rules are 
shared by the two kinds of rule sets. These over-
lapped rules contain at least two non-terminal leaf 
nodes plus two terminal leaf nodes, which implies 
that longer rules do not affect performance too 
much. 
 
22.07
25.28
26.1425.94 26.02 26.07
21.5
22.5
23.5
24.5
25.5
26.5
1 2 3 4 5 6
BL
EU
(%
)
 
Figure 5: Accuracy changing with different max-
imal tree depths ( h = 1 to 6 when 4d = ) 
 
22.72
24.71
26.0526.03 26.07
25.74
25.2925.2825.2624.78
21.5
22.5
23.5
24.5
25.5
26.5
1 2 3 4 5
B
LE
U
(%
)
 
Figure 6: Accuracy changing with the different maximal 
number of trees in a tree sequence ( d =1 to 5), the upper 
line is for 6h =  while the lower line is for 2h = .  
 
Fig. 5 studies the impact when setting different 
maximal tree depth ( h ) in a rule on the perfor-
mance. It demonstrates that:  
1) Significant performance improvement is 
achieved when the value of h  is increased from 1 
to 2. This can be easily explained by the fact that 
when h = 1, only monotonic search is conducted, 
while h = 2 allows non-terminals to be leaf nodes, 
thus introducing preliminary structure features to 
the search and allowing non-monotonic search. 
2) Internal structures and large span (due to h  
increasing) are also useful as attested by the gain 
of 0.86 (26.14-25.28) Blue score when the value of 
h  increases from 2 to 4. 
Fig. 6 studies the impact on performance by set-
ting different maximal tree number (d) in a rule. It 
further indicates that: 
1) Tree sequence rules (d >1) are useful and 
even more helpful if we limit the tree depth to no 
more than two (lower line, h=2). However, tree 
sequence rules consisting of more than three sub-
trees have almost no contribution to the perform-
ance improvement. This is mainly due to data 
sparseness issue when d >3. 
2) Even if only two-layer sub-trees (lower line) 
are allowed, our method still outperforms STSG 
and Moses when d>1. This further validates the 
effectiveness of our design philosophy of using 
multi-sub-trees as basic translation unit in SMT. 
7 Conclusions and Future Work 
In this paper, we present a tree sequence align-
ment-based translation model to combine the 
strengths of phrase-based and syntax-based me-
thods. The experimental results on the NIST MT-
2005 Chinese-English translation task demonstrate 
the effectiveness of the proposed model. Our study 
also finds that in our model the tree sequence rules 
are very useful since they can model non-syntactic 
phrases and reorderings with rich linguistic struc-
ture features while discontinuous phrases and tree 
sequence rules with more than three sub-trees have 
less impact on performance. 
There are many interesting research topics on 
the tree sequence-based translation model worth 
exploring in the future. The current method ex-
tracts large amount of rules. Many of them are re-
dundant, which make decoding very slow. Thus, 
effective rule optimization and pruning algorithms 
are highly desirable. Ideally, a linguistically and 
empirically motivated theory can be worked out, 
suggesting what kinds of rules should be extracted 
given an input phrase pair. For example, most 
function words and headwords can be kept in ab-
stract rules as features. In addition, word align-
ment is a hard constraint in our rule extraction. We 
will study direct structure alignments to reduce the 
impact of word alignment errors. We are also in-
terested in comparing our method with the forest-
to-string model (Liu et al, 2007). Finally, we 
would also like to study unsupervised learning-
based bilingual parsing for SMT.  
566
 References  
Rens Bod. 2007. Unsupervised Syntax-Based Machine 
Translation: The Contribution of Discontinuous 
Phrases. MT-Summmit-07. 51-56. 
David Chiang. 2005. A hierarchical phrase-based mod-
el for SMT. ACL-05. 263-270. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree transla-
tion. EMNLP-06. 232-241. 
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency in-
sertion grammars. ACL-05. 541-548. 
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule? HLT-
NAACL-04. 
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang and I. Thayer. 2006. Scalable Infe-
rence and Training of Context-Rich Syntactic 
Translation Models. COLING-ACL-06. 961-968 
Daniel Gildea. 2003. Loosely Tree-Based Alignment for 
Machine Translation. ACL-03. 80-87. 
Jonathan Graehl and Kevin Knight. 2004. Training tree 
transducers. HLT-NAACL-2004. 105-112. 
Mary Hearne and Andy Way. 2003. Seeing the wood for 
the trees: data-oriented translation. MT Summit IX, 
165-172. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06 (poster). 
Dan Klein and Christopher D. Manning. 2003. Accurate 
Unlexicalized Parsing. ACL-03. 423-430. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statistic-
al phrase-based translation. HLT-NAACL-03. 127-
133. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. AMTA-04, 115-124 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07 (poster) 77-180. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-06. 
44-52. 
I. Dan Melamed. 2004. Statistical machine translation 
by parsing. ACL-04. 653-660. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statistical 
machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz J. Och and Hermann Ney. 2004a. The alignment 
template approach to statistical machine translation. 
Computational Linguistics, 30(4):417-449. 
Kishore Papineni, Salim Roukos, ToddWard and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. ACL-02. 311-318. 
Arjen Poutsma. 2000. Data-oriented translation. 
COLING-2000. 635-641 
Chris Quirk and Arul Menezes. 2006. Do we need 
phrases? Challenging the conventional wisdom in 
SMT. COLING-ACL-06. 9-16. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. ACL-05. 271-279. 
Stefan Riezler and John T. Maxwell III. 2006. Gram-
matical Machine Translation. HLT-NAACL-06. 
248-255. 
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. 
Ordering Phrases with Function Words. ACL-7. 
712-719. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. ICSLP-02. 901-904. 
Benjamin Wellington, Sonjia Waxmonsky and I. Dan 
Melamed. 2006. Empirical Lower Bounds on the 
Complexity of Translational Equivalence. COLING-
ACL-06. 977-984. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):377-403. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
SMT. COLING-ACL-06. 521? 528. 
Kenji Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree Align-
ment-based Model for Statistical Machine Transla-
tion. MT-Summit-07. 535-542. 
Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement 
do we need to have a better system? LREC-04. 2051-
2054. 
567
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 149?152,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Linguistically Annotated Reordering Model
for BTG-based Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{dyxiong, mzhang, aaiti, hli}@i2r.a-star.edu.sg
Abstract
In this paper, we propose a linguistically anno-
tated reordering model for BTG-based statis-
tical machine translation. The model incorpo-
rates linguistic knowledge to predict orders for
both syntactic and non-syntactic phrases. The
linguistic knowledge is automatically learned
from source-side parse trees through an an-
notation algorithm. We empirically demon-
strate that the proposed model leads to a sig-
nificant improvement of 1.55% in the BLEU
score over the baseline reordering model on
the NIST MT-05 Chinese-to-English transla-
tion task.
1 Introduction
In recent years, Bracketing Transduction Grammar
(BTG) proposed by (Wu, 1997) has been widely
used in statistical machine translation (SMT). How-
ever, the original BTG does not provide an effec-
tive mechanism to predict the most appropriate or-
ders between two neighboring phrases. To address
this problem, Xiong et al (2006) enhance the BTG
with a maximum entropy (MaxEnt) based reorder-
ing model which uses boundary words of bilingual
phrases as features. Although this model outper-
forms previous unlexicalized models, it does not uti-
lize any linguistically syntactic features, which have
proven useful for phrase reordering (Wang et al,
2007). Zhang et al (2007) integrates source-side
syntactic knowledge into a phrase reordering model
based on BTG-style rules. However, one limita-
tion of this method is that it only reorders syntac-
tic phrases because linguistic knowledge from parse
trees is only carried by syntactic phrases as far as re-
ordering is concerned, while non-syntactic phrases
are combined monotonously with a flat reordering
score.
In this paper, we propose a linguistically anno-
tated reordering model for BTG-based SMT, which
is a significant extension to the work mentioned
above. The new model annotates each BTG node
with linguistic knowledge by projecting source-side
parse trees onto the corresponding binary trees gen-
erated by BTG so that syntactic features can be used
for phrase reordering. Different from (Zhang et al,
2007), our annotation algorithm is able to label both
syntactic and non-syntactic phrases. This enables
our model to reorder any phrases, not limited to syn-
tactic phrases. In addition, other linguistic informa-
tion such as head words, is also used to improve re-
ordering.
The rest of the paper is organized as follows. Sec-
tion 2 briefly describes our baseline system while
Section 3 introduces the linguistically annotated re-
ordering model. Section 4 reports the experiments
on a Chinese-to-English translation task. We con-
clude in Section 5.
2 Baseline SMT System
The baseline system is a phrase-based system which
uses the BTG lexical rules (A ? x/y) to translate
source phrase x into target phrase y and the BTG
merging rules (A ? [A,A]|?A,A?) to combine two
neighboring phrases with a straight or inverted or-
der. The BTG lexical rules are weighted with several
features, such as phrase translation, word penalty
and language models, in a log-linear form. For the
merging rules, a MaxEnt-based reordering model
using boundary words of neighboring phrases as fea-
tures is used to predict the merging order, similar to
(Xiong et al, 2006). We call this reordering model
149
boundary words based reordering model (BWR). In
this paper, we propose to incorporate a linguistically
annotated reordering model into the log-linear trans-
lation model, so as to strengthen the BWR?s phrase
reordering ability. We train all the model scaling fac-
tors on the development set to maximize the BLEU
score. A CKY-style decoder is developed to gener-
ate the best BTG binary tree for each input sentence,
which yields the best translation.
3 Linguistically Annotated Reordering
Model
The linguistically annotated reordering
model (LAR) is a MaxEnt-based classifica-
tion model which predicts the phrase order
o ? {inverted, straight} during the application
of merging rules to combine their left and right
neighboring phrases Al and Ar into a larger phrase
A. 1 The model can be formulated as
LAR = exp(
?
i ?ihi(o,Al, Ar, A))?
o? exp(
?
i ?ihi(o?, Al, Ar, A))
(1)
where the functions hi ? {0, 1} are reordering fea-
tures and ?i are weights of these features. We define
the features as linguistic elements which are anno-
tated for each BTG node through an annotation al-
gorithm, which comprise (1) head word hw, (2) the
part-of-speech (POS) tag ht of head word and (3)
syntactic label sl.
Each merging rule involves 3 nodes (A,Al, Ar)
and each node has 3 linguistic elements (hw, ht, sl).
Therefore, the model has 9 features in total. Taking
the left node Al as an example, the model could use
its head word w as feature as follows
hi(o,A,Al, Ar) =
{ 1, Al.hw = w, o = straight
0, otherwise
3.1 Annotation Algorithm
There are two steps to annotate a phrase or a BTG
node using source-side parse tree information: (1)
determining the span on the source side which is
exactly covered by the node or the phrase, then
(2) annotating the span according to the source-side
parse tree. If the span is exactly covered by a sin-
gle subtree in the source-side parse tree, it is called
1Each phrase is also a node in the BTG tree generated by the
decoder.
1: Annotator (span s = ?i, j?, source-side parse tree t)
2: if s is a syntactic span then
3: Find the subtree c in t which exactly covers s
4: s.{ } := {c.hw, c.ht, c.sl}
5: else
6: Find the smallest subtree c? subsuming s in t
7: if c?.hw ? s then
8: s.hw := c?.hw and s.ht := c?.ht
9: else
10: Find the word w ? s which is nearest to c?.hw
11: s.hw := w and s.ht := w.t /*w.t is the POS
tag of w*/
12: end if
13: Find the left boundary node ln of s in c?
14: Find the right boundary node rn of s in c?
15: s.sl := ln.sl-c?.sl-rn.sl
16: end if
Figure 1: The Annotation Algorithm.
syntactic span, otherwise it is non-syntactic span.
One of the challenges in this annotation algorithm
is that phrases (BTG nodes) are not always cover-
ing syntactic span, in other words, they are not al-
ways aligned to all constituent nodes in the source-
side tree. To solve this problem, we use heuristic
rules to generate pseudo head word and composite
label which consists of syntactic labels of three rel-
evant constituents for the non-syntactic span. In this
way, our annotation algorithm is capable of labelling
both syntactic and non-syntactic phrases and there-
fore providing linguistic information for any phrase
reordering.
The annotation algorithm is shown in Fig. 1. For
a syntactic span, the annotation is trivial. Annotation
elements directly come from the subtree that covers
the span exactly. For a non-syntactic span, the pro-
cess is much complicated. Firstly, we need to locate
the smallest subtree c? subsuming the span (line 6).
Secondly, we try to identify the head word/tag of the
span (line 7-12) by using its head word directly if it
is within the span. Otherwise, the word within the
span which is nearest to hw will be assigned as the
head word of the span. Finally, we determine the
composite label of the span (line 13-15), which is
formulated as L-C-R. L/R means the syntactic label
of the left/right boundary node of s which is the
highest leftmost/rightmost sub-node of c? not over-
lapping the span. If there is no such boundary node
150
IP(??)
??
??
?
HH
HH
H
NP(??)
?? HH
NP(??)
NR
??1
Tibet
NP(??)
? H
NN
??2
financial
NN
??3
work
VP(??)
??
??
?
HH
HH
H
VV
??4
gain
AS
?5
NP(??)
?? HH
ADJP(??)
JJ
??6
remarkable
NP(??)
NN
??7
achievement
Figure 2: A syntactic parse tree with head word annotated
for each internal node. The superscripts of leaf nodes
denote their surface positions from left to right.
span hw ht sl
?1, 2? ?? NN NULL-NP-NN
?2, 3? ?? NN NP
?2, 4? ?? VV NP-IP-NP
?3, 4? ?? VV NP-IP-NP
Table 1: Annotation samples according to the tree shown
in Fig. 2. hw/ht represents the head word/tag, respec-
tively. sl means the syntactic label.
(the span s is exactly aligned to the left/right bound-
ary of c?), L/R will be set to NULL. C is the label of
c?. L, R and C together define the external syntactic
context of s.
Fig. 2 shows a syntactic parse tree for a Chinese
sentence, with head word annotated for each internal
node. Some sample annotations are given in Table 1.
3.2 Training and Decoding
Training an LAR model takes three steps. Firstly, we
extract annotated reordering examples from source-
side parsed, word-aligned bilingual data using the
annotation algorithm and the reordering example
extraction algorithm of (Xiong et al, 2006). We
then generate features using linguistic elements of
these examples and finally estimate feature weights.
This training process flexibly learns rich syntactic
reordering information without explicitly construct-
ing BTG tree or forest for each sentence pair.
During decoding, each input source sentence is
firstly parsed to obtain its syntactic tree. Then the
CKY-style decoder tries to generate the best BTG
tree using the lexical and merging rules. When two
neighboring nodes are merged in a specific order, the
two embedded reordering models, BWR and LAR,
evaluate this merging independently with individual
scores. The former uses boundary words as features
while the latter uses the linguistic elements as fea-
tures, annotated on the BTG nodes through the anno-
tation algorithm according to the source-side parse
tree.
4 Experiments
All experiments in this section were carried out on
the Chinese-to-English translation task of the NIST
MT-05. The baseline system and the new system
with the LAR model were trained on the FBIS cor-
pus. We removed 15,250 sentences, for which the
Chinese parser (Xiong et al, 2005) failed to pro-
duce syntactic parse trees. The parser was trained
on the Penn Chinese Treebank with a F1 score of
79.4%. The remaining FBIS corpus (224,165 sen-
tence pairs) was used to obtain standard bilingual
phrases for the systems.
We extracted 2.8M reordering examples from
these sentences. From these examples, we gener-
ated 114.8K reordering features for the BWR model
using the right boundary words of phrases and 85K
features for the LAR model using linguistic annota-
tions. We ran the MaxEnt toolkit (Zhang, 2004) to
tune reordering feature weights with iteration num-
ber being set to 100 and Gaussian prior to 1 to avoid
overfitting.
We built our four-gram language model using
Xinhua section of the English Gigaword corpus
(181.1M words) with the SRILM toolkit (Stol-
cke, 2002). For the efficiency of minimum-error-
rate training (Och, 2003), we built our development
set (580 sentences) using sentences not exceeding
50 characters from the NIST MT-02 evaluation test
data.
4.1 Results
We compared various reordering configurations in
the baseline system and new system. The base-
line system only has BWR as the reordering model,
while the new system employs two reordering mod-
els: BWR and LAR. For the linguistically anno-
tated reordering model LAR, we augment its feature
pool incrementally: firstly using only single labels
151
2(SL) as features (132 features in total), then con-
structing composite labels for non-syntactic phrases
(+BNL) (6.7K features), and finally introducing
head words and their POS tags into the feature pool
(+BNL+HWT) (85K features). This series of exper-
iments demonstrate the impact and degree of con-
tribution made by each feature for reordering. We
also conducted experiments to investigate the ef-
fect of restricting reordering to syntactic phrases in
the new system using the best reordering feature
set (SL+BNL+HWT) for LAR. The experimental
results (case-sensitive BLEU scores together with
confidence intervals) are presented in Table 2, from
which we have the following observations:
(1) The LAR model improves the performance
statistically significantly. Even we only use the base-
line feature set SL with only 132 features for the
LAR, the BLEU score improves from 0.2497 to
0.2588. This is because most of the frequent reorder-
ing patterns between Chinese and English have been
captured using syntactic labels. For example, the
pre-verbal modifier PP in Chinese is translated into
post-verbal counterpart in English. This reordering
can be described by a rule with an inverted order:
V P ? ?PP, V P ?, and captured by our syntactic
reordering features.
(2) Context information, provided by labels of
boundary nodes (BNL) and head word/tag pairs
(HWT), also improves phrase reordering. Produc-
ing composite labels for non-syntactic BTG nodes
(+BNL) and integrating head word/tag pairs into
the LAR as reordering features (+BNL+HWT) are
both effective, indicating that context information
complements syntactic label for capturing reorder-
ing patterns.
(3) Restricting phrase reordering to syntactic
phrases is harmful. The BLEU score plummets from
0.2652 to 0.2512.
5 Conclusion
In this paper, we have presented a linguistically an-
notated reordering model to effectively integrate lin-
guistic knowledge into phrase reordering by merg-
ing source-side parse trees with BTG binary trees.
Our experimental results show that, on the NIST
2For non-syntactic node, we only use the single label C,
without constructing composite label L-C-R.
Reordering Configuration BLEU (%)
BWR 24.97 ? 0.90
BWR + LAR (SL) 25.88 ? 0.95
BWR + LAR (+BNL) 26.27 ? 0.98
BWR + LAR (+BNL+HWT) 26.52 ? 0.96
Only allowed SPs reordering 25.12 ? 0.87
Table 2: The effect of the linguistically annotated reorder-
ing model. BWR denotes the boundary word based re-
ordering model while LAR denotes the linguistically an-
notated reordering model. (SL) is the baseline feature set,
(+BNL) and (+BNL+HWT) are extended feature sets for
the LAR. SP means syntactic phrase.
MT-05 task of Chinese-to-English translation, the
proposed reordering model leads to BLEU improve-
ment of 1.55%. We believe that our linguistically
annotated reordering model can be further improved
by using better annotation which transfers more
knowledge (morphological, syntactic or semantic)
to the model.
References
Franz Josef Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Proceedings of ACL 2003.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of International Conference on
Spoken Language Processing, volume 2, pages 901-904.
Chao Wang, Michael Collins and Philipp Koehn. 2007. Chi-
nese Syntactic Reordering for Statistical Machine Transla-
tion. In Proceedings of EMNLP-CoNLL 2007.
Dekai Wu. 1997. Stochastic Inversion Transduction Grammars
and Bilingual Parsing of Parallel Corpora. Computational
Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings of IJCNLP, Jeju Island,
Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum
Entropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of ACL-COLING 2006.
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 2007.
Phrase Reordering Model Integrating Syntactic Knowledge
for SMT. In Proceedings of EMNLP-CoNLL 2007.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
152
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 157?160,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Exploiting N-best Hypotheses for SMT Self-Enhancement 
 
Boxing Chen, Min Zhang, Aiti Aw and Haizhou Li 
Department of Human Language Technology 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, 119613, Singapore 
{bxchen, mzhang, aaiti, hli}@i2r.a-star.edu.sg 
 
 
 
Abstract 
Word and n-gram posterior probabilities esti-
mated on N-best hypotheses have been used to 
improve the performance of statistical ma-
chine translation (SMT) in a rescoring frame-
work. In this paper, we extend the idea to 
estimate the posterior probabilities on N-best 
hypotheses for translation phrase-pairs, target 
language n-grams, and source word re-
orderings. The SMT system is self-enhanced 
with the posterior knowledge learned from N-
best hypotheses in a re-decoding framework. 
Experiments on NIST Chinese-to-English task 
show performance improvements for all the 
strategies. Moreover, the combination of the 
three strategies achieves further improvements 
and outperforms the baseline by 0.67 BLEU 
score on NIST-2003 set, and 0.64 on NIST-
2005 set, respectively. 
1 Introduction 
State-of-the-art Statistical Machine Translation 
(SMT) systems usually adopt a two-pass search 
strategy. In the first pass, a decoding algorithm is 
applied to generate an N-best list of translation 
hypotheses; while in the second pass, the final 
translation is selected by rescoring and re-ranking 
the N-best hypotheses through additional feature 
functions. In this framework, the N-best hypothe-
ses serve as the candidates for the final translation 
selection in the second pass. 
These N-best hypotheses can also provide useful 
feedback to the MT system as the first decoding 
has discarded many undesirable translation candi-
dates. Thus, the knowledge captured in the N-best 
hypotheses, such as posterior probabilities for 
words, n-grams, phrase-pairs, and source word re-
orderings, etc. is more compatible with the source 
sentences and thus could potentially be used to 
improve the translation performance. 
Word posterior probabilities estimated from the 
N-best hypotheses have been widely used for con-
fidence measure in automatic speech recognition 
(Wessel, 2002) and have also been adopted into 
machine translation. Blatz et al (2003) and Uef-
fing et al (2003) used word posterior probabilities 
to estimate the confidence of machine translation. 
Chen et al (2005), Zens and Ney (2006) reported 
performance improvements by computing target n-
grams posterior probabilities estimated on the N-
best hypotheses in a rescoring framework. Trans-
ductive learning method (Ueffing et al, 2007) 
which repeatedly re-trains the generated source-
target N-best hypotheses with the original training 
data again showed translation performance im-
provement and demonstrated that the translation 
model can be reinforced from N-best hypotheses.  
In this paper, we further exploit the potential of 
the N-best hypotheses and propose several 
schemes to derive the posterior knowledge from 
the N-best hypotheses, in an effort to enhance the 
language model, translation model, and source 
word reordering under a re-decoding framework of 
any phrase-based SMT system. 
2 Self-Enhancement with Posterior 
Knowledge 
The self-enhancement system structure is shown in 
Figure 1. Our baseline system is set up using 
Moses (Koehn et al, 2007), a state-of-the-art 
phrase-base SMT open source package. In the fol-
lowings, we detail the approaches to exploiting the 
three different kinds of posterior knowledge, 
namely, language model, translation model and 
word reordering. 
157
2.1 Language Model 
We consider self-enhancement of language model 
as a language model adaptation problem similar to 
(Nakajima et al, 2002). The original monolingual 
target training data is regarded as general-domain 
data while the test data as a domain-specific data. 
Obviously, the real domain-specific target data 
(test data) is unavailable for training. In this work, 
the N-best hypotheses of the test set are used as a 
quasi-corpus to train a language model. This new 
language model trained on the quasi-corpus is then 
used together with the language model trained on 
the general-domain data (original training data) to 
produce a new list of N-best hypotheses under our 
self-enhancement framework. The feature function 
of the language model 1 1( , )
J I
LMh f e  is a mixture 
model of the two language models as in Equation 1. 
1 1 1 1 2 1( , ) ( ) ( )
J I I I
LM TLM QLMh f e h e h e? ?= +      (1) 
where 1
Jf is the source language words string, 
1
Ie is  the target language words string, TLM is the 
language model trained on target training data, and 
QLM is on the quasi-corpus of N-best hypotheses. 
The mixture model exploits multiple language 
models with weights 1?  and 2?  being optimized 
together with other feature functions. The proce-
dure for self-enhancement of the language model is 
as follows. 
1. Run decoding and extract N-best hypotheses. 
2. Train a new language model (QLM) on the N-
best hypotheses. 
3. Optimize the weights of the decoder which uses 
both original LM (TLM) and the new LM 
(QLM). 
4. Repeat step 1-3 for a fixed number of iterations. 
2.2 Translation Model 
In general, we can safely assume that for a given 
source input, phrase-pairs that appeared in the N-
best hypotheses are better than those that did not. 
We call the former ?good phrase-pairs? and the 
later ?bad phrase-pairs? for the given source input. 
Hypothetically, we can reinforce the translation 
model by appending the ?good phrase-pairs? to the 
original phrase table and changing the probability 
space of the translation model, as phrase-based 
translation probabilities are estimated using rela-
tive frequencies. The new direct phrase-based 
translation probabilities are computed as follows:   
( , ) ( , )( | )
( ) ( )
train nbest
train nbest
N f e N f ep e f
N f N f
+= +
% %% %%% % %       (2) 
where f%  is the source language phrase, e%  is  the 
target language phrase, (.)trainN is the frequencies 
observed in the training data, and (.)nbestN  is the 
frequencies observed in the N-best hypotheses. For 
those phrase-pairs that did not appear in the N-best 
hypotheses list (?bad phrase-pairs?), ( , )nbestN f e% %  
equals 0, but the marginal count of f%  is increased 
by ( )nbestN f% , in this way the phrase-based transla-
tion probabilities of ?bad phrase-pairs? degraded 
when compared with the corresponding probabili-
ties in the original translation model, and that of 
?good phrase-pairs? increased, hence improve the 
translation model. 
The procedure for translation model self-
enhancement can be summarized as follows. 
1. Run decoding and extract N-best hypotheses. 
2. Extract ?good phrase-pairs? according to the 
hypotheses? phrase-alignment information and 
append them to the original phrase table to gen-
erate a new phrase table. 
3. Score the new phrase table to create a new 
translation model. 
4. Optimize the weights of the decoder with the 
above new translation model. 
5. Repeat step 1-4 for a fixed number of iterations. 
2.3 Word Reordering 
Some previous work (Costa-juss? and Fonollosa, 
2006; Li et al, 2007) have shown that reordering a 
source sentence to match the word order in its cor-
 
Figure 1: Self-enhancement system structure, where 
TM is translation model, LM is language model, and 
RM is reordering model. 
158
responding target sentence can produce better 
translations for a phrase-based SMT system. We 
bring this idea forward to our word reordering self-
enhancement framework, which similarly trans-
lates a source sentence (S) to target sentence (T) in 
two stages: S S T?? ? , where S ?  is the reor-
dered source sentence.  
The phrase-alignment information in each hy-
pothesis indicates the word reordering for source 
sentence. We select the word reordering with the 
highest posterior probability as the best word reor-
dering for a given source sentence. Word re-
orderings from different phrase segmentation but 
with same word surface order are merged. The 
posterior probabilities of the word re-orderings are 
computed as in Equation 3. 
1
1 1
( )( | )
J
J J
hyp
N rp r f
N
=                        (3) 
where 1( )
JN r  is the count of word reordering 1
Jr , 
and hypN  is the number of N-best hypotheses.  
The words of the source sentence are then reor-
dered according to their indices in the best selected 
word reordering 1
Jr . The procedure for self-
enhancement of word reordering is as follows. 
1. Run decoding and extract N-best hypotheses. 
2. Select the best word re-orderings according to 
the phrase-alignment information. 
3. Reorder the source sentences according to the 
selected word reordering. 
4. Optimize the weights of the decoder with the 
reordered source sentences. 
5. Repeat step 1-4 for a fixed number of iterations. 
3 Experiments and Results 
Experiments on Chinese-to-English NIST transla-
tion tasks were carried out on the FBIS1 corpus. 
We used NIST 2002 MT evaluation test set as our 
development set, and the NIST 2003, 2005 test sets 
as our test sets as shown in Table 1. 
We determine the number of iteration empiri-
cally by setting it to 10. We then observe the 
BLEU score on the development set for each itera-
tion. The iteration number which achieved the best 
BLEU score on development set is selected as the 
iteration number of iterations for the test set.  
 
                                                          
1 LDC2003E14 
#Running words Data set type 
Chinese English 
parallel 7.0M 8.9M train 
monolingual - 61.5M 
NIST 02 dev 23.2K 108.6K 
NIST 03 test 25.8K 116.5K 
NIST 05 test 30.5K 141.9K 
Table 1: Statistics of training, dev and test sets. Evalua-
tion sets of NIST campaigns include 4 references: total 
numbers of running words are provided in the table. 
 
System #iter. NIST 02 NIST 03 NIST 05
Base - 27.67 26.68 24.82 
TM 4 27.87 26.95 25.05 
LM 6 27.96 27.06 25.07 
WR 6 27.99 27.04 25.11 
Comb 7 28.45 27.35 25.46 
Table 2: BLEU% scores of five systems: decoder (Base), 
self-enhancement on translation model (TM), language 
model (LM), word reordering (WR) and the combina-
tion of TM, LM and WR (Comb). 
 
Further experiments also suggested that, in this 
experiment scenario, setting the size of N-best list 
to 3,000 arrives at the greatest performance im-
provements. Our evaluation metric is BLEU (Pap-
ineni et al, 2002). The translation performance is 
reported in Table 2, where the column ?#iter.? re-
fers to the iteration number where the system 
achieved the best BLEU score on development set. 
Compared with the baseline (?Base? in Table 2), 
all three self-enhancement methods (?TM?, ?LM?, 
and ?WR? in Table 2) consistently improved the 
performance. In general, absolute gains of 0.23- 
0.38 BLEU score were obtained for each method 
on two test sets. While comparing the performance 
among all three methods, we can see that they 
achieved very similar improvement. Combining 
the three methods showed further gains in BLEU 
score. Totally, the combined system outperformed 
the baseline by 0.67 BLEU score on NIST?03, and 
0.64 on NIST?05 test set, respectively. 
4 Discussion 
As posterior knowledge applied in our models are 
posterior probabilities, the main difference be-
tween our work and all previous work is the use of 
knowledge source, where we derive knowledge 
from the N-best hypotheses generated from previ-
ous iteration. 
159
Comparing the work of (Nakajima et al, 2002), 
there is a slight difference between the two models. 
Nakajima et al used only 1-best hypothesis, while 
we use N-best hypotheses of test set as the quasi-
corpus to train the language model. 
In the work of  (Costa-juss? and Fonollosa, 2006;  
Li et al, 2007) which similarly translates a source 
sentence (S) to target sentence (T) in two stages: 
S S T?? ? , they derive S ? from training data; 
while we obtain S ?  based on the occurrence fre-
quency, i.e. posterior probability of each source 
word reordering in the N-best hypotheses list. 
An alternative solution for enhancing the trans-
lation model is through self-training (Ueffing, 
2006; Ueffing et al, 2007) which re-trains the 
source-target N-best hypotheses together with the 
original training data, and thus differs from ours in 
the way of new phrase pairs extraction. We only 
supplement those phrase-pairs appeared in the N-
best hypotheses to the original phrase table. Fur-
ther experiment showed that improvement ob-
tained by self-training method is not as consistent 
on both development and test sets as that by our 
method. One possible reason is that in self-training, 
the entire translation model is adjusted with the 
addition of new phrase-pairs extracted from the 
source-target N-best hypotheses, and hence the 
effect is less predictable. 
5 Conclusions 
To take advantage of the N-best hypotheses, we 
proposed schemes in a re-decoding framework and 
made use of the posterior knowledge learned from 
the N-best hypotheses to improve a phrase-based 
SMT system. The posterior knowledge include 
posterior probabilities for target n-grams, transla-
tion phrase-pairs and source word re-orderings, 
which in turn improve the language model, transla-
tion model, and word reordering respectively. 
Experiments were based on the state-of-the-art 
phrase-based decoder and carried out on NIST 
Chinese-to-English task. It has been shown that all 
three methods improved the performance. More-
over, the combination of all three strategies outper-
forms each individual method and significantly 
outperforms the baseline. We demonstrated that 
the SMT system can be self-enhanced by exploit-
ing useful feedback from the N-best hypotheses 
which are generated by itself. 
References 
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. 
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2003. 
Confidence estimation for machine translation. Final 
report, JHU/CLSP Summer Workshop. 
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M. 
Federico. 2005. The ITC-irst SMT System for 
IWSLT-2005. In Proceeding of IWSLT-2005, pp.98-
104, Pittsburgh, USA, October. 
M. R. Costa-juss?, J. A. R. Fonollosa. 2006. Statistical 
Machine Reordering. In Proceeding of EMNLP 2006. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, 
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. 
Herbst. 2007. Moses: Open Source Toolkit for Statis-
tical Machine Translation. In Proceedings of ACL-
2007, pp. 177-180, Prague, Czech Republic. 
C.-H. Li, M. Li, D. Zhang, M. Li, M. Zhou and Y. Guan. 
2007.  A Probabilistic Approach to Syntax-based Re-
ordering for Statistical Machine Translation. In Pro-
ceedings of ACL-2007. Prague, Czech Republic. 
H. Nakajima, H. Yamamoto, T. Watanabe. 2002.  Lan-
guage model adaptation with additional text gener-
ated by machine translation. In Proceedings of 
COLING-2002. Volume 1, Pages: 1-7. Taipei. 
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu, 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceeding of ACL-2002, pp. 
311-318. 
N. Ueffing. 2006. Using Monolingual Source-Language 
Data to Improve MT Performance. In Proceedings of 
IWSLT 2006. Kyoto, Japan. November 27-28. 
N. Ueffing, K. Macherey, and H. Ney. 2003. Confi-
dence Measures for Statistical Machine Translation. 
In Proceeding of MT Summit IX, pages 394?401, 
New Orleans, LA, September. 
N. Ueffing, G. Haffari, A. Sarkar. 2007. Transductive 
learning for statistical machine translation. In Pro-
ceedings of ACL-2007, Prague. 
F. Wessel. 2002. Word Posterior Probabilities for Large 
Vocabulary Continuous Speech Recognition. Ph.D. 
thesis, RWTH Aachen University. Aachen, Germany, 
January. 
R. Zens and H. Ney. 2006. N-gram Posterior Probabili-
ties for Statistical Machine Translation. In Proceed-
ings of the HLT-NAACL Workshop on SMT, pp. 72-
77, NY. 
160
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 172?180,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Forest-based Tree Sequence to String Translation Model 
 
Hui Zhang1, 2   Min Zhang1   Haizhou Li1   Aiti Aw1   Chew Lim Tan2 
1Institute for Infocomm Research                    2National University of Singapore                    
zhangh1982@gmail.com   {mzhang, hli, aaiti}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg  
 
 
 
 
Abstract 
This paper proposes a forest-based tree se-
quence to string translation model for syntax- 
based statistical machine translation, which 
automatically learns tree sequence to string 
translation rules from word-aligned source-
side-parsed bilingual texts. The proposed 
model leverages on the strengths of both tree 
sequence-based and forest-based translation 
models. Therefore, it can not only utilize forest 
structure that compactly encodes exponential 
number of parse trees but also capture non-
syntactic translation equivalences with linguis-
tically structured information through tree se-
quence. This makes our model potentially 
more robust to parse errors and structure di-
vergence. Experimental results on the NIST 
MT-2003 Chinese-English translation task 
show that our method statistically significantly 
outperforms the four baseline systems. 
1 Introduction 
Recently syntax-based statistical machine trans-
lation (SMT) methods have achieved very prom-
ising results and attracted more and more inter-
ests in the SMT research community. Fundamen-
tally, syntax-based SMT views translation as a 
structural transformation process. Therefore, 
structure divergence and parse errors are two of 
the major issues that may largely compromise 
the performance of syntax-based SMT (Zhang et 
al., 2008a; Mi et al, 2008).  
Many solutions have been proposed to address 
the above two issues. Among these advances, 
forest-based modeling (Mi et al, 2008; Mi and 
Huang, 2008) and tree sequence-based modeling 
(Liu et al, 2007; Zhang et al, 2008a) are two 
interesting modeling methods with promising 
results reported. Forest-based modeling aims to 
improve translation accuracy through digging the 
potential better parses from n-bests (i.e. forest) 
while tree sequence-based modeling aims to 
model non-syntactic translations with structured 
syntactic knowledge. In nature, the two methods 
would be complementary to each other since 
they manage to solve the negative impacts of 
monolingual parse errors and cross-lingual struc-
ture divergence on translation results from dif-
ferent viewpoints. Therefore, one natural way is 
to combine the strengths of the two modeling 
methods for better performance of syntax-based 
SMT. However, there are many challenges in 
combining the two methods into a single model 
from both theoretical and implementation engi-
neering viewpoints. In theory, one may worry 
about whether the advantage of tree sequence has 
already been covered by forest because forest 
encodes implicitly a huge number of parse trees 
and these parse trees may generate many differ-
ent phrases and structure segmentations given a 
source sentence. In system implementation, the 
exponential combinations of tree sequences with 
forest structures make the rule extraction and 
decoding tasks much more complicated than that 
of the two individual methods.  
In this paper, we propose a forest-based tree 
sequence to string model, which is designed to 
integrate the strengths of the forest-based and the 
tree sequence-based modeling methods. We pre-
sent our solutions that are able to extract transla-
tion rules and decode translation results for our 
model very efficiently. A general, configurable 
platform was designed for our model. With this 
platform, we can easily implement our method 
and many previous syntax-based methods by 
simple parameter setting. We evaluate our 
method on the NIST MT-2003 Chinese-English 
translation tasks. Experimental results show that 
our method significantly outperforms the two 
individual methods and other baseline methods. 
Our study shows that the proposed method is 
able to effectively combine the strengths of the 
forest-based and tree sequence-based methods, 
and thus having great potential to address the 
issues of parse errors and non-syntactic transla-
172
tions resulting from structure divergence. It also 
indicates that tree sequence and forest play dif-
ferent roles and make contributions to our model 
in different ways. 
The remainder of the paper is organized as fol-
lows. Section 2 describes related work while sec-
tion 3 defines our translation model. In section 4 
and section 5, the key rule extraction and decod-
ing algorithms are elaborated. Experimental re-
sults are reported in section 6 and the paper is 
concluded in section 7. 
2 Related work  
As discussed in section 1, two of the major chal-
lenges to syntax-based SMT are structure diver-
gence and parse errors. Many techniques have 
been proposed to address the structure diver-
gence issue while only fewer studies are reported 
in addressing the parse errors in the SMT re-
search community. 
To address structure divergence issue, many 
researchers (Eisner, 2003; Zhang et al, 2007) 
propose using the Synchronous Tree Substitution 
Grammar (STSG) grammar in syntax-based 
SMT since the STSG uses larger tree fragment as 
translation unit. Although promising results have 
been reported, STSG only uses one single sub-
tree as translation unit which is still committed to 
the syntax strictly. Motivated by the fact that 
non-syntactic phrases make non-trivial contribu-
tion to phrase-based SMT, the tree sequence-
based translation model is proposed (Liu et al, 
2007; Zhang et al, 2008a) that uses tree se-
quence as the basic translation unit, rather than 
using single sub-tree as in the STSG. Here, a tree 
sequence refers to a sequence of consecutive 
sub-trees that are embedded in a full parse tree. 
For any given phrase in a sentence, there is at 
least one tree sequence covering it. Thus the tree 
sequence-based model has great potential to ad-
dress the structure divergence issue by using tree 
sequence-based non-syntactic translation rules. 
Liu et al (2007) propose the tree sequence con-
cept and design a tree sequence to string transla-
tion model. Zhang et al (2008a) propose a tree 
sequence-based tree to tree translation model and 
Zhang et al (2008b) demonstrate that the tree 
sequence-based modelling method can well ad-
dress the structure divergence issue for syntax-
based SMT. 
To overcome the parse errors for SMT, Mi et 
al. (2008) propose a forest-based translation 
method that uses forest instead of one best tree as 
translation input, where a forest is a compact rep-
resentation of exponentially number of n-best 
parse trees. Mi and Huang (2008) propose a for-
est-based rule extraction algorithm, which learn 
tree to string rules from source forest and target 
string. By using forest in rule extraction and de-
coding, their methods are able to well address the 
parse error issue. 
From the above discussion, we can see that 
traditional tree sequence-based method uses sin-
gle tree as translation input while the forest-
based model uses single sub-tree as the basic 
translation unit that can only learn tree-to-string 
(Galley et al 2004; Liu et al, 2006) rules. There-
fore, the two methods display different strengths, 
and which would be complementary to each 
other. To integrate their strengths, in this paper, 
we propose a forest-based tree sequence to string 
translation model.  
3 Forest-based tree sequence to string 
model  
In this section, we first explain what a packed 
forest is and then define the concept of the tree 
sequence in the context of forest followed by the 
discussion on our proposed model. 
3.1 Packed Forest 
A packed forest (forest in short) is a special kind 
of hyper-graph (Klein and Manning, 2001; 
Huang and Chiang, 2005), which is used to rep-
resent all derivations (i.e. parse trees) for a given 
sentence under a context free grammar (CFG). A 
forest F is defined as a triple ? ?, ?, ? ?, where 
? is non-terminal node set, ?  is hyper-edge set 
and ? is leaf node set (i.e. all sentence words). A 
forest F satisfies the following two conditions: 
 
1) Each node ?  in ?  should cover a phrase, 
which is a continuous word sub-sequence in ?. 
2) Each hyper-edge ?  in ?  is defined as 
?? ? ?? ??? ? ??, ??? ? ?? ? ??, ?? ? ?? , 
where ?? ? ?? ???  covers a sequence of conti-
nuous and non-overlap phrases, ??  is the father 
node of the children sequence ?? ??? ???. The 
phrase covered by ??  is just the sum of all the 
phrases covered by each child node ??. 
 
We here introduce another concept that is used 
in our subsequent discussions. A complete forest 
CF is a general forest with one additional condi-
tion that there is only one root node N in CF, i.e., 
all nodes except the root N in a CF must have at 
least one father node. 
Fig. 1 is a complete forest while Fig. 7 is a 
non-complete forest due to the virtual node 
?VV+VV? introduced in Fig. 7. Fig. 2 is a hyper-
edge (IP => NP VP) of Fig. 1, where NP covers 
173
the phrase ?Xinhuashe?, VP covers the phrase 
?shengming youguan guiding? and IP covers the 
entire sentence. In Fig.1, only root IP has no fa-
ther node, so it is a complete forest. The two 
parse trees T1 and T2 encoded in Fig. 1 are 
shown separately in Fig. 3 and Fig. 41.  
Different parse tree represents different deri-
vations and explanations for a given sentence. 
For example, for the same input sentence in Fig. 
1, T1 interprets it as ?XNA (Xinhua News 
Agency) declares some regulations.? while T2 
interprets it as ?XNA declaration is related to 
some regulations.?.  
 
 
 
Figure 1. A packed forest for sentence ????
/Xinhuashe ?? /shengming ?? /youguan ??
/guiding? 
             
Figure 2.  A hyper-edge used in Fig. 1 
 
       
 
Figure 3. Tree 1 (T1)            Figure 4. Tree 2 (T2) 
3.2 Tree sequence in packed forest 
Similar to the definition of tree sequence used in 
a single parse tree defined in Liu et al (2007) 
and Zhang et al (2008a), a tree sequence in a 
forest also refers to an ordered sub-tree sequence 
that covers a continuous phrase without overlap-
ping. However, the major difference between 
                                                          
1 Please note that a single tree (as T1 and T2 shown in Fig. 
3 and Fig. 4) is represented by edges instead of hyper-edges. 
A hyper-edge is a group of edges satisfying the 2nd condi-
tion as shown in the forest definition. 
them lies in that the sub-trees of a tree sequence 
in forest may belongs to different single parse 
trees while, in a single parse tree-based model, 
all the sub-trees in a tree sequence are committed 
to the same parse tree.  
The forest-based tree sequence enables our 
model to have the potential of exploring addi-
tional parse trees that may be wrongly pruned out 
by the parser and thus are not encoded in the for-
est. This is because that a tree sequence in a for-
est allows its sub-trees coming from different 
parse trees, where these sub-trees may not be 
merged finally to form a complete parse tree in 
the forest. Take the forest in Fig. 1 as an exam-
ple, where ((VV shengming) (JJ youguan)) is a 
tree sequence that all sub-trees appear in T1 
while ((VV shengming) (VV youguan)) is a tree 
sequence whose sub-trees do not belong to any 
single tree in the forest. But, indeed the two sub-
trees (VV shengming) and (VV youguan) can be 
merged together and further lead to a complete 
single parse tree which may offer a correct inter-
pretation to the input sentence (as shown in Fig. 
5). In addition, please note that, on the other 
hand, more parse trees may introduce more noisy 
structures. In this paper, we leave this problem to 
our model and let the model decide which sub-
structures are noisy features. 
 
          
 
 Figure 5. A parse tree that was wrongly 
pruned out 
 
            
    Figure 6. A tree sequence to string rule 
 
174
A tree-sequence to string translation rule in a 
forest is a triple <L, R, A>, where L is the tree 
sequence in source language, R is the string con-
taining words and variables in target language, 
and A is the alignment between the leaf nodes of 
L and R. This definition is similar to that of (Liu 
et al 2007, Zhang et al 2008a) except our tree-
sequence is defined in forest. The shaded area of 
Fig. 6 exemplifies a tree sequence to string trans-
lation rule in the forest.  
3.3 Forest-based tree-sequence to string 
translation model 
Given a source forest F and target translation TS 
as well as word alignment A, our translation 
model is formulated as: 
  
 Pr??, ??, ?? ? ? ? ????????????? ?,???????, ??,??  
 
By the above Eq., translation becomes a tree 
sequence structure to string mapping issue. Giv-
en the F, TS and A, there are multiple derivations 
that could map F to TS under the constraint A. 
The mapping probability Pr??, ??, ??  in our 
study is obtained by summing over the probabili-
ties of all derivations ?. The probability of each 
derivation ?? is given as the product of the prob-
abilities of all the rules ( )ip r  used in the deriva-
tion (here we assume that each rule is applied 
independently in a derivation). 
Our model is implemented under log-linear 
framework (Och and Ney, 2002). We use seven 
basic features that are analogous to the common-
ly used features in phrase-based systems (Koehn, 
2003): 1) bidirectional rule mapping probabilities, 
2) bidirectional lexical rule translation probabili-
ties, 3) target language model, 4) number of rules 
used and 5) number of target words. In addition, 
we define two new features: 1) number of leaf 
nodes in auxiliary rules (the auxiliary rule will be 
explained later in this paper) and 2) product of 
the probabilities of all hyper-edges of the tree 
sequences in forest. 
4 Training  
This section discusses how to extract our transla-
tion rules given a triple ? ?, ??, ? ? . As we 
know, the traditional tree-to-string rules can be 
easily extracted from ? ?, ??, ? ? using the algo-
rithm of Mi and Huang (2008)2. We would like 
                                                          
2 Mi and Huang (2008) extend the tree-based rule extraction 
algorithm (Galley et al, 2004) to forest-based by introduc-
ing non-deterministic mechanism. Their algorithm consists 
of two steps, minimal rule extraction and composed rule 
generation. 
to leverage on their algorithm in our study. Un-
fortunately, their algorithm is not directly appli-
cable to our problem because tree rules have only 
one root while tree sequence rules have multiple 
roots. This makes the tree sequence rule extrac-
tion very complex due to its interaction with for-
est structure. To address this issue, we introduce 
the concepts of virtual node and virtual hyper-
edge to convert a complete parse forest ?  to a 
non-complete forest ? which is designed to en-
code all the tree sequences that we want. There-
fore, by doing so, the tree sequence rules can be 
extracted from a forest in the following two 
steps: 
1) Convert the complete parse forest ? into a 
non-complete forest ?  in order to cover those 
tree sequences that cannot be covered by a single 
tree node. 
2) Employ the forest-based tree rule extraction 
algorithm (Mi and Huang, 2008) to extract our 
rules from the non-complete forest. 
To facilitate our discussion, here we introduce 
two notations:  
? Alignable: A consecutive source phrase is 
an alignable phrase if and only if it can be 
aligned with at least one consecutive target 
phrase under the word-alignment con-
straint. The covered source span is called 
alignable span. 
? Node sequence: a sequence of nodes (ei-
ther leaf or internal nodes) in a forest cov-
ering a consecutive span. 
Algorithm 1 illustrates the first step of our rule 
extraction algorithm, which is a CKY-style Dy-
namic Programming (DP) algorithm to add vir-
tual nodes into forest. It includes the following 
steps: 
1) We traverse the forest to visit each span in 
bottom-up fashion (line 1-2), 
1.1) for each span [u,v] that is covered by 
single tree nodes3, we put these tree 
nodes into the set NSS(u,v) and go 
back to step 1 (line 4-6). 
1.2) otherwise we concatenate the tree se-
quences of sub-spans to generate the 
set of tree sequences covering the cur-
rent larger span (line 8-13). Then, we 
prune the set of node sequences (line 
14). If this span is alignable, we 
create virtual father nodes and corres-
ponding virtual hyper-edges to link 
the node sequences with the virtual 
father nodes (line 15-20). 
                                                          
3 Note that in a forest, there would be multiple single tree 
nodes covering the same span as shown Fig.1.  
175
2) Finally we obtain a forest with each align-
able span covered by either original tree 
nodes or the newly-created tree sequence 
virtual nodes. 
Theoretically, there is exponential number of 
node sequences in a forest. Take Fig. 7 as an ex-
ample. The NSS of span [1,2] only contains ?NP? 
since it is alignable and covered by the single 
tree node NP. However, span [2,3] cannot be 
covered by any single tree node, so we have to 
create the NSS of span[2,3] by concatenating the 
NSSs of span [2,2] and span [3,3]. Since NSS of 
span [2,2] contains 4 element {?NN?, ?NP?, 
?VV?, ?VP?} and NSS of span [3, 3] also con-
tains 4 element {?VV?, ?VP?, ?JJ?, ?ADJP?}, 
NSS of span [2,3] contains 16=4*4 elements. To 
make the NSS manageable, we prune it with the 
following thresholds: 
? each node sequence should contain less 
than n nodes 
? each node sequence set should contain less 
than m node sequences 
? sort node sequences according to their 
lengths and only keep the k shortest ones 
Each virtual node is simply labeled by the 
concatenation of all its children?s labels as 
shown in Fig. 7. 
 
Algorithm 1. add virtual nodes into forest 
Input: packed forest F, alignment A 
Notation:  
   L: length of source sentence 
   NSS(u,v): the set of node sequences covering span [u,v] 
  VN(ns): virtual father node for node sequence ns. 
Output: modified forest F with virtual nodes 
 
 
1. for length := 0 to L - 1 do 
2.      for start := 1 to L - length do 
3.          stop := start + length 
4.          if span[start, stop] covered by tree nodes then 
5.                for each node n of span [start, stop] do 
6.                    add n into NSS(start, stop) 
7.          else  
8.                for pivot := start to stop - 1 
9.                     for each ns1 in NSS(start, pivot) do 
10.                          for each ns2 in NSS(pivot+1, stop) do 
11.                               create ?? ?? ?1? ?  ?2?  
12.                                if ns is not in NSS(start, stop) then 
13.                                      add ns into NSS(start, stop) 
14.                do pruning on NSS(start, stop) 
15.                if the span[start, stop] is alignable then 
16.                    for each ns of NSS(start, stop) do 
17.                   if node VN(ns) is not in F then 
18.                                add node VN(ns) into F 
19.                          add a hyper-edge h into F,  
20.                          let lhs(h) := VN(ns), rhs(h) := ns 
 
Algorithm 1 outputs a non-complete forest CF 
with each alignable span covered by either tree 
nodes or virtual nodes. Then we can easily ex-
tract our rules from the CF using the tree rule 
extraction algorithm (Mi and Huang, 2008). 
Finally, to calculate rule feature probabilities 
for our model, we need to calculate the fractional 
counts (it is a kind of probability defined in Mi 
and Huang, 2008) of each translation rule in a 
parse forest. In the tree case, we can use the in-
side-outside-based methods (Mi and Huang 
2008) to do it. In the tree sequence case, since 
the previous method cannot be used directly, we 
provide another solution by making an indepen-
dent assumption that each tree in a tree sequence 
is independent to each other. With this assump-
tion, the fractional counts of both tree and tree 
sequence can be calculated as follows: 
 
???? ? ?????????????????   
 
???????? ? ? ????
????????????
? ? ????
??????
? ? ????
??????????????
 
 
where ???? is the fractional counts to be calcu-
lated for rule r, a frag is either lhs(r) (excluding 
virtual nodes and virtual hyper-edges) or any tree 
node in a forest, TOP is the root of the forest, 
??. ? and ??.) are the outside and inside probabil-
ities of nodes, ?????. ? returns the root nodes of a 
tree sequence fragment, ???????. ?  returns the 
leaf nodes of a tree sequence fragment, ???? is 
the hyper-edge probability. 
 
 
 
              Figure 7. A virtual node in forest 
5 Decoding  
We benefit from the same strategy as used in our 
rule extraction algorithm in designing our decod-
ing algorithm, recasting the forest-based tree se-
quence-to-string decoding problem as a forest-
based tree-to-string decoding problem. Our de-
coding algorithm consists of four steps: 
1) Convert the complete parse forest to a non-
complete one by introducing virtual nodes. 
176
2) Convert the non-complete parse forest into 
a translation forest4 ?? by using the translation 
rules and the pattern-matching algorithm pre-
sented in Mi et al (2008). 
3) Prune out redundant nodes and add auxil-
iary hyper-edge into the translation forest for 
those nodes that have either no child or no father. 
By this step, the translation forest ?? becomes a 
complete forest.  
4) Decode the translation forest using our 
translation model and a dynamic search algo-
rithm. 
The process of step 1 is similar to Algorithm 1 
except no alignment constraint used here. This 
may generate a large number of additional virtual 
nodes; however, all redundant nodes will be fil-
tered out in step 3. In step 2, we employ the tree-
to-string pattern match algorithm (Mi et al, 
2008) to convert a parse forest to a translation 
forest. In step 3, all those nodes not covered by 
any translation rules are removed. In addition, 
please note that the translation forest is already 
not a complete forest due to the virtual nodes and 
the pruning of rule-unmatchable nodes. We, 
therefore, propose Algorithm 2 to add auxiliary 
hyper-edges to make the translation forest com-
plete.  
In Algorithm 2, we travel the forest in bottom-
up fashion (line 4-5). For each span, we do: 
1) generate all the NSS for this span (line 7-12)  
2) filter the NSS to a manageable size (line 13) 
3) add auxiliary hyper-edges for the current 
span (line 15-19) if it can be covered by at least 
one single tree node, otherwise go to step 1 . This 
is the key step in our Algorithm 2. For each tree 
node and each node sequences covering the same 
span (stored in the current NSS), if the tree node 
has no children or at least one node in the node 
sequence has no father, we add an auxiliary hy-
per-edge to connect the tree node as father node 
with the node sequence as children. Since Algo-
rithm 2 is DP-based and traverses the forest in a 
bottom-up way, all the nodes in a node sequence 
should already have children node after the lower 
level process in a small span. Finally, we re-build 
the NSS of current span for upper level NSS 
combination use (line 20-22). 
 
 In Fig. 8, the hyper-edge ?IP=>NP VV+VV 
NP? is an auxiliary hyper-edge introduced by 
Algorithm 2. By Algorithm 2, we convert the 
translation forest into a complete translation for-
est. We then use a bottom-up node-based search 
                                                          
4 The concept of translation forest is proposed in Mi et 
al. (2008). It is a forest that consists of only the hyper-
edges induced from translation rules. 
algorithm to do decoding on the complete trans-
lation forest. We also use Cube Pruning algo-
rithm (Huang and Chiang 2007) to speed up the 
translation process. 
 
 
 
Figure 8. Auxiliary hyper-edge in a translation 
forest 
 
Algorithm 2. add auxiliary hyper-edges into mt forest F 
Input:  mt forest F 
Output: complete forest F with auxiliary hyper-edges 
 
1. for i := 1 to L do 
2.      for each node n of span [i, i] do 
3.          add n into NSS(i, i) 
4. for length := 1 to L - 1 do 
5.      for start := 1 to L - length do 
6.          stop := start + length 
7.          for pivot := start to stop-1 do 
8.               for each ns1 in NSS (start, pivot) do 
9.                    for each ns2 in NSS (pivot+1,stop) do 
10.                 create ?? ?? ?1? ?  ?2? 
11.                          if ns is not in NSS(start, stop) then 
12.                                add ns into NSS (start, stop) 
13.           do pruning on NSS(start, stop) 
14.           if there is tree node cover span [start, stop] then 
15.         for each tree node n of span [start,stop] do 
16.                      for each ns of NSS(start, stop) do 
17.                     if node n have no children or  
there is node in ns with no father  
then 
18.                                add auxiliary hyper-edge h into F 
19.                                let lhs(h) := n, rhs(h) := ns 
20.          empty NSS(start, stop) 
21.          for each node n of span [start, stop] do 
22.                 add n into NSS(start, stop) 
6 Experiment 
6.1 Experimental Settings 
We evaluate our method on Chinese-English 
translation task. We use the FBIS corpus as train-
ing set, the NIST MT-2002 test set as develop-
ment (dev) set and the NIST MT-2003 test set as 
test set. We train Charniak?s parser (Charniak 
2000) on CTB5 to do Chinese parsing, and modi-
fy it to output packed forest. We tune the parser 
on section 301-325 and test it on section 271-
300. The F-measure on all sentences is 80.85%. 
A 3-gram language model is trained on the Xin-
177
hua portion of the English Gigaword3 corpus and 
the target side of the FBIS corpus using the 
SRILM Toolkits (Stolcke, 2002) with modified 
Kneser-Ney smoothing (Kenser and Ney, 1995). 
GIZA++ (Och and Ney, 2003) and the heuristics 
?grow-diag-final-and? are used to generate m-to-
n word alignments. For the MER training (Och, 
2003), Koehn?s MER trainer (Koehn, 2007) is 
modified for our system. For significance test, 
we use Zhang et al?s implementation (Zhang et 
al, 2004). Our evaluation metrics is case-
sensitive BLEU-4 (Papineni et al, 2002). 
For parse forest pruning (Mi et al, 2008), we 
utilize the Margin-based pruning algorithm pre-
sented in (Huang, 2008). Different from Mi et al 
(2008) that use a static pruning threshold, our 
threshold is sentence-depended. For each sen-
tence, we compute the Margin between the n-th 
best and the top 1 parse tree, then use the Mar-
gin-based pruning algorithm presented in 
(Huang, 2008) to do pruning. By doing so, we 
can guarantee to use at least all the top n best 
parse trees in the forest. However, please note 
that even after pruning there is still exponential 
number of additional trees embedded in the for-
est because of the sharing structure of forest. 
Other parameters are set as follows: maximum 
number of roots in a tree sequence is 3, maxi-
mum height of a translation rule is 3, maximum 
number of leaf nodes is 7, maximum number of 
node sequences on each span is 10, and maxi-
mum number of rules extracted from one node is 
10000. 
6.2 Experimental Results 
We implement our proposed methods as a gen-
eral, configurable platform for syntax-based 
SMT study. Based on this platform, we are able 
to easily implement most of the state-of-the-art 
syntax-based x-to-string SMT methods via sim-
ple parameter setting. For training, we set forest 
pruning threshold to 1 best for tree-based me-
thods and 100 best for forest-based methods. For 
decoding, we set: 
1) TT2S: tree-based tree-to-string model by 
setting the forest pruning threshold to 1 best and 
the number of sub-trees in a tree sequence to 1. 
2) TTS2S: tree-based tree-sequence to string 
system by setting the forest pruning threshold to 
1 best and the maximum number of sub-trees in a 
tree sequence to 3. 
3) FT2S: forest-based tree-to-string system by 
setting the forest pruning threshold to 500 best, 
the number of sub-trees in a tree sequence to 1. 
4) FTS2S: forest-based tree-sequence to string 
system by setting the forest pruning threshold to 
500 best and the maximum number of sub-trees 
in a tree sequence to 3. 
 
Model BLEU(%) 
Moses 25.68 
TT2S 26.08 
TTS2S 26.95 
FT2S 27.66 
FTS2S 28.83 
 
Table 1. Performance Comparison 
 
We use the first three syntax-based systems 
(TT2S, TTS2S, FT2S) and Moses (Koehn et al, 
2007), the state-of-the-art phrase-based system, 
as our baseline systems. Table 1 compares the 
performance of the five methods, all of which are 
fine-tuned.  It shows that: 
1) FTS2S significantly outperforms (p<0.05) 
FT2S. This shows that tree sequence is very use-
ful to forest-based model. Although a forest can 
cover much more phrases than a single tree does, 
there are still many non-syntactic phrases that 
cannot be captured by a forest due to structure 
divergence issue. On the other hand, tree se-
quence is a good solution to non-syntactic trans-
lation equivalence modeling. This is mainly be-
cause tree sequence rules are only sensitive to 
word alignment while tree rules, even extracted 
from a forest (like in FT2S), are also limited by 
syntax according to grammar parsing rules. 
2) FTS2S shows significant performance im-
provement (p<0.05) over TTS2S due to the con-
tribution of forest. This is mainly due to the fact 
that forest can offer very large number of parse 
trees for rule extraction and decoder. 
3) Our model statistically significantly outper-
forms all the baselines system. This clearly de-
monstrates the effectiveness of our proposed 
model for syntax-based SMT. It also shows that 
the forest-based method and tree sequence-based 
method are complementary to each other and our 
proposed method is able to effectively integrate 
their strengths. 
4) All the four syntax-based systems show bet-
ter performance than Moses and three of them 
significantly outperforms (p<0.05) Moses. This 
suggests that syntax is very useful to SMT and 
translation can be viewed as a structure mapping 
issue as done in the four syntax-based systems. 
Table 2 and Table 3 report the distribution of 
different kinds of translation rules in our model 
(training forest pruning threshold is set to 100 
best) and in our decoding (decoding forest prun-
ing threshold is set to 500 best) for one best 
translation generation. From the two tables, we 
can find that: 
178
Rule Type Tree 
to String 
Tree Sequence 
to String 
L 4,854,406 20,526,674 
P 37,360,684 58,826,261 
U 3,297,302 3,775,734 
All 45,512,392 83,128,669 
 
Table 2. # of rules extracted from training cor-
pus. L means fully lexicalized, P means partially 
lexicalized, U means unlexicalized. 
 
Rule Type Tree 
to String 
Tree Sequence 
to String 
L 10,592 1,161 
P 7,132 742 
U 4,874 278 
All 22,598 2,181 
 
Table 3. # of rules used to generate one-best 
translation result in testing 
 
1) In Table 2, the number of tree sequence 
rules is much larger than that of tree rules al-
though our rule extraction algorithm only ex-
tracts those tree sequence rules over the spans 
that tree rules cannot cover. This suggests that 
the non-syntactic structure mapping is still a big 
challenge to syntax-based SMT. 
2) Table 3 shows that the tree sequence rules 
is around 9% of the tree rules when generating 
the one-best translation. This suggests that 
around 9% of translation equivalences in the test 
set can be better modeled by tree sequence to 
string rules than by tree to string rules. The 9% 
tree sequence rules contribute 1.17 BLEU score 
improvement (28.83-27.66 in Table 1) to FTS2S 
over FT2S.  
3) In Table 3, the fully-lexicalized rules are 
the major part (around 60%), followed by the 
partially-lexicalized (around 35%) and un-
lexicalized (around 15%). However, in Table 2, 
partially-lexicalized rules extracted from training 
corpus are the major part (more than 70%). This 
suggests that most partially-lexicalized rules are 
less effective in our model. This clearly directs 
our future work in model optimization. 
 
BLEU (%)    
N-best \ model FT2S FTS2S 
100 Best 27.40 28.61 
500 Best  27.66 28.83 
2500 Best  27.66 28.96 
5000 Best  27.79 28.89 
 
Table 4. Impact of the forest pruning  
 
Forest pruning is a key step for forest-based 
method. Table 4 reports the performance of the 
two forest-based models using different values of 
the forest pruning threshold for decoding. It 
shows that: 
1) FTS2S significantly outperforms (p<0.05) 
FT2S consistently in all test cases. This again 
demonstrates the effectiveness of our proposed 
model. Even if in the 5000 Best case, tree se-
quence is still able to contribute 1.1 BLEU score 
improvement (28.89-27.79). It indicates the ad-
vantage of tree sequence cannot be covered by 
forest even if we utilize a very large forest.  
2) The BLEU scores are very similar to each 
other when we increase the forest pruning thre-
shold. Moreover, in one case the performance 
even drops. This suggests that although more 
parse trees in a forest can offer more structure 
information, they may also introduce more noise 
that may confuse the decoder. 
7 Conclusion   
In this paper, we propose a forest-based tree-
sequence to string translation model to combine 
the strengths of forest-based methods and tree-
sequence based methods. This enables our model 
to have the great potential to address the issues 
of structure divergence and parse errors for syn-
tax-based SMT. We convert our forest-based tree 
sequence rule extraction and decoding issues to 
tree-based by introducing virtual nodes, virtual 
hyper-edges and auxiliary rules (hyper-edges). In 
our system implementation, we design a general 
and configurable platform for our method, based 
on which we can easily realize many previous 
syntax-based methods. Finally, we examine our 
methods on the FBIS corpus and the NIST MT-
2003 Chinese-English translation task. Experi-
mental results show that our model greatly out-
performs the four baseline systems. Our study 
demonstrates that forest-based method and tree 
sequence-based method are complementary to 
each other and our proposed method is able to 
effectively combine the strengths of the two in-
dividual methods for syntax-based SMT. 
Acknowledgement  
We would like to thank Huang Yun for preparing 
the pictures in this paper; Run Yan for providing 
the java version modified MERT program and 
discussion on the details of MOSES; Mi Haitao 
for his help and discussion on re-implementing 
the FT2S model; Sun Jun and Xiong Deyi for 
their valuable suggestions. 
179
References  
Eugene Charniak. 2000. A maximum-entropy inspired 
parser. NAACL-00. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Da-
niel Marcu. 2004. What?s in a translation rule? 
HLT-NAACL-04. 273-280. 
Liang Huang. 2008. Forest Reranking: Discriminative 
Parsing with Non-Local Features. ACL-HLT-08. 
586-594 
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. IWPT-05. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language 
models. ACL-07. 144?151 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06. (poster) 
Reinhard Kenser and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. 
ICASSP-95. 181-184 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. IWPT-2001. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical phrase-based translation. HLT-NAACL-03. 
127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07. 177-180. (poster) 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation 
Rules. ACL-07. 704-711. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. ACL-HLT-08. 192-199. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-08. 206-214. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statis-
tical machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Alignment 
Models. Computational Linguistics. 29(1) 19-51.  
Kishore Papineni, Salim Roukos, ToddWard and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. ACL-02. 311-
318. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree 
Alignment-based Model for Statistical Machine 
Translation. MT-Summit-07. 535-542. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan, Sheng Li. 2008a. A Tree Sequence 
Alignment-based Tree-to-Tree Translation Model. 
ACL-HLT-08. 559-567. 
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, 
Sheng Li. 2008b. Grammar Comparison Study for 
Translational Equivalence Modeling and Statistic-
al Machine Translation. COLING-08. 1097-1104. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? 
LREC-04. 2051-2054. 
 
180
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 315?323,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Syntax-Driven Bracketing Model for Phrase-Based Translation
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 South Connexis, Singapore 138632
{dyxiong, mzhang, aaiti, hli}@i2r.a-star.edu.sg
Abstract
Syntactic analysis influences the way in
which the source sentence is translated.
Previous efforts add syntactic constraints
to phrase-based translation by directly
rewarding/punishing a hypothesis when-
ever it matches/violates source-side con-
stituents. We present a new model that
automatically learns syntactic constraints,
including but not limited to constituent
matching/violation, from training corpus.
The model brackets a source phrase as
to whether it satisfies the learnt syntac-
tic constraints. The bracketed phrases are
then translated as a whole unit by the de-
coder. Experimental results and analy-
sis show that the new model outperforms
other previous methods and achieves a
substantial improvement over the baseline
which is not syntactically informed.
1 Introduction
The phrase-based approach is widely adopted in
statistical machine translation (SMT). It segments
a source sentence into a sequence of phrases, then
translates and reorder these phrases in the target.
In such a process, original phrase-based decod-
ing (Koehn et al, 2003) does not take advan-
tage of any linguistic analysis, which, however,
is broadly used in rule-based approaches. Since
it is not linguistically motivated, original phrase-
based decoding might produce ungrammatical or
even wrong translations. Consider the following
Chinese fragment with its parse tree:
Src: [? [[7? 11?]NP [?? [? [?? ?]NP
]PP ]VP ]IP ]VP
Ref: established July 11 as Sailing Festival day
Output: [to/? [?[set up/?? [for/? naviga-
tion/??]] on July 11/7?11?? knots/?]]
The output is generated from a phrase-based sys-
tem which does not involve any syntactic analy-
sis. Here we use ?[]? (straight orientation) and
???? (inverted orientation) to denote the common
structure of the source fragment and its transla-
tion found by the decoder. We can observe that
the decoder inadequately breaks up the second NP
phrase and translates the two words ???? and
??? separately. However, the parse tree of the
source fragment constrains the phrase ??? ??
to be translated as a unit.
Without considering syntactic constraints from
the parse tree, the decoder makes wrong decisions
not only on phrase movement but also on the lex-
ical selection for the multi-meaning word ???1.
To avert such errors, the decoder can fully respect
linguistic structures by only allowing syntactic
constituent translations and reorderings. This, un-
fortunately, significantly jeopardizes performance
(Koehn et al, 2003; Xiong et al, 2008) because by
integrating syntactic constraint into decoding as a
hard constraint, it simply prohibits any other use-
ful non-syntactic translations which violate con-
stituent boundaries.
To better leverage syntactic constraint yet still
allow non-syntactic translations, Chiang (2005)
introduces a count for each hypothesis and ac-
cumulates it whenever the hypothesis exactly
matches syntactic boundaries on the source side.
On the contrary, Marton and Resnik (2008) and
Cherry (2008) accumulate a count whenever hy-
potheses violate constituent boundaries. These
constituent matching/violation counts are used as
a feature in the decoder?s log-linear model and
their weights are tuned via minimal error rate
training (MERT) (Och, 2003). In this way, syn-
tactic constraint is integrated into decoding as a
soft constraint to enable the decoder to reward hy-
potheses that respect syntactic analyses or to pe-
1This word can be translated into ?section?, ?festival?,
and ?knot? in different contexts.
315
nalize hypotheses that violate syntactic structures.
Although experiments show that this con-
stituent matching/violation counting feature
achieves significant improvements on various
language-pairs, one issue is that matching syn-
tactic analysis can not always guarantee a good
translation, and violating syntactic structure does
not always induce a bad translation. Marton and
Resnik (2008) find that some constituency types
favor matching the source parse while others
encourage violations. Therefore it is necessary to
integrate more syntactic constraints into phrase
translation, not just the constraint of constituent
matching/violation.
The other issue is that during decoding we are
more concerned with the question of phrase co-
hesion, i.e. whether the current phrase can be
translated as a unit or not within particular syntac-
tic contexts (Fox, 2002)2, than that of constituent
matching/violation. Phrase cohesion is one of
the main reasons that we introduce syntactic con-
straints (Cherry, 2008). If a source phrase remains
contiguous after translation, we refer this type of
phrase bracketable, otherwise unbracketable. It
is more desirable to translate a bracketable phrase
than an unbracketable one.
In this paper, we propose a syntax-driven brack-
eting (SDB) model to predict whether a phrase
(a sequence of contiguous words) is bracketable
or not using rich syntactic constraints. We parse
the source language sentences in the word-aligned
training corpus. According to the word align-
ments, we define bracketable and unbracketable
instances. For each of these instances, we auto-
matically extract relevant syntactic features from
the source parse tree as bracketing evidences.
Then we tune the weights of these features us-
ing a maximum entropy (ME) trainer. In this way,
we build two bracketing models: 1) a unary SDB
model (UniSDB) which predicts whether an inde-
pendent phrase is bracketable or not; and 2) a bi-
nary SDB model(BiSDB) which predicts whether
two neighboring phrases are bracketable. Similar
to previous methods, our SDB model is integrated
into the decoder?s log-linear model as a feature so
that we can inherit the idea of soft constraints.
In contrast to the constituent matching/violation
counting (CMVC) (Chiang, 2005; Marton and
Resnik, 2008; Cherry, 2008), our SDB model has
2Here we expand the definition of phrase to include both
syntactic and non-syntactic phrases.
the following advantages
? The SDB model automatically learns syntac-
tic constraints from training data while the
CMVC uses manually defined syntactic con-
straints: constituency matching/violation. In
our SDB model, each learned syntactic fea-
ture from bracketing instances can be consid-
ered as a syntactic constraint. Therefore we
can use thousands of syntactic constraints to
guide phrase translation.
? The SDB model maintains and protects the
strength of the phrase-based approach in a
better way than the CMVC does. It is able to
reward non-syntactic translations by assign-
ing an adequate probability to them if these
translations are appropriate to particular syn-
tactic contexts on the source side, rather than
always punish them.
We test our SDB model against the baseline
which doest not use any syntactic constraints on
Chinese-to-English translation. To compare with
the CMVC, we also conduct experiments using
(Marton and Resnik, 2008)?s XP+. The XP+ ac-
cumulates a count for each hypothesis whenever
it violates the boundaries of a constituent with a
label from {NP, VP, CP, IP, PP, ADVP, QP, LCP,
DNP}. The XP+ is the best feature among all fea-
tures that Marton and Resnik use for Chinese-to-
English translation. Our experimental results dis-
play that our SDB model achieves a substantial
improvement over the baseline and significantly
outperforms XP+ according to the BLEU metric
(Papineni et al, 2002). In addition, our analysis
shows further evidences of the performance gain
from a different perspective than that of BLEU.
The paper proceeds as follows. In section 2 we
describe how to learn bracketing instances from
a training corpus. In section 3 we elaborate the
syntax-driven bracketing model, including feature
generation and the integration of the SDB model
into phrase-based SMT. In section 4 and 5, we
present our experiments and analysis. And we fi-
nally conclude in section 6.
2 The Acquisition of Bracketing
Instances
In this section, we formally define the bracket-
ing instance, comprising two types namely binary
bracketing instance and unary bracketing instance.
316
We present an algorithm to automatically ex-
tract these bracketing instances from word-aligned
bilingual corpus where the source language sen-
tences are parsed.
Let c and e be the source sentence and the
target sentence, W be the word alignment be-
tween them, T be the parse tree of c. We
define a binary bracketing instance as a tu-
ple ?b, ?(ci..j), ?(cj+1..k), ?(ci..k)? where b ?
{bracketable, unbracketable}, ci..j and cj+1..k
are two neighboring source phrases and ?(T, s)
(?(s) for short) is a subtree function which returns
the minimal subtree covering the source sequence
s from the source parse tree T . Note that ?(ci..k)
includes both ?(ci..j) and ?(cj+1..k). For the two
neighboring source phrases, the following condi-
tions are satisfied:
?eu..v, ep..q ? e s.t.
?(m,n) ? W, i ? m ? j ? u ? n ? v (1)
?(m,n) ? W, j + 1 ? m ? k ? p ? n ? q (2)
The above (1) means that there exists a target
phrase eu..v aligned to ci..j and (2) denotes a tar-
get phrase ep..q aligned to cj+1..k. If eu..v and
ep..q are neighboring to each other or all words be-
tween the two phrases are aligned to null, we set
b = bracketable, otherwise b = unbracketable.
From a binary bracketing instance, we derive a
unary bracketing instance ?b, ?(ci..k)?, ignoring
the subtrees ?(ci..j) and ?(cj+1..k).
Let n be the number of words of c. If we ex-
tract all potential bracketing instances, there will
be o(n2) unary instances and o(n3) binary in-
stances. To keep the number of bracketing in-
stances tractable, we only record 4 representa-
tive bracketing instances for each index j: 1) the
bracketable instance with the minimal ?(ci..k), 2)
the bracketable instance with the maximal ?(ci..k),
3) the unbracketable instance with the minimal
?(ci..k), and 4) the unbracketable instance with the
maximal ?(ci..k).
Figure 1 shows the algorithm to extract brack-
eting instances. Line 3-11 find all potential brack-
eting instances for each (i, j, k) ? c but only keep
4 bracketing instances for each index j: two min-
imal and two maximal instances. This algorithm
learns binary bracketing instances, from which we
can derive unary bracketing instances.
1: Input: sentence pair (c, e), the parse tree T of c and the
word alignment W between c and e
2: < := ?
3: for each (i, j, k) ? c do
4: if There exist a target phrase eu..v aligned to ci..j and
ep..q aligned to cj+1..k then
5: Get ?(ci..j), ?(cj+1..k), and ?(ci..k)
6: Determine b according to the relationship between
eu..v and ep..q
7: if ?(ci..k) is currently maximal or minimal then
8: Update bracketing instances for index j
9: end if
10: end if
11: end for
12: for each j ? c do
13: < := < ? {bracketing instances from j}
14: end for
15: Output: bracketing instances <
Figure 1: Bracketing Instances Extraction Algo-
rithm.
3 The Syntax-Driven Bracketing Model
3.1 The Model
Our interest is to automatically detect phrase
bracketing using rich contextual information. We
consider this task as a binary-class classification
problem: whether the current source phrase s is
bracketable (b) within particular syntactic contexts
(?(s)). If two neighboring sub-phrases s1 and s2
are given, we can use more inner syntactic con-
texts to complete this binary classification task.
We construct the syntax-driven bracketing
model within the maximum entropy framework. A
unary SDB model is defined as:
PUniSDB(b|?(s), T ) =
exp(?i ?ihi(b, ?(s), T )?
b exp(
?
i ?ihi(b, ?(s), T )
(3)
where hi ? {0, 1} is a binary feature function
which we will describe in the next subsection, and
?i is the weight of hi. Similarly, a binary SDB
model is defined as:
PBiSDB(b|?(s1), ?(s2), ?(s), T ) =
exp(?i ?ihi(b, ?(s1), ?(s2), ?(s), T )?
b exp(
?
i ?ihi(b, ?(s1), ?(s2), ?(s), T )
(4)
The most important advantage of ME-based
SDB model is its capacity of incorporating more
fine-grained contextual features besides the binary
feature that detects constituent boundary violation
or matching. By employing these features, we
can investigate the value of various syntactic con-
straints in phrase translation.
317
jingfang
police
yi fengsuo
block
le baozha
bomb
xianchang
scene
NN NN
NP
VP
ASVVADNN
ADVP
VP
NP
IP
s
s1 s2
Figure 2: Illustration of syntax-driven features
used in SDB. Here we only show the features for
the source phrase s. The triangle, rounded rect-
angle and rectangle denote the rule feature, path
feature and constituent boundary matching feature
respectively.
3.2 Syntax-Driven Features
Let s be the source phrase in question, s1 and s2
be the two neighboring sub-phrases. ?(.) is the
root node of ?(.). The SDB model exploits various
syntactic features as follows.
? Rule Features (RF)
We use the CFG rules of ?(s), ?(s1) and
?(s2) as features. These features capture
syntactic ?horizontal context? which demon-
strates the expansion trend of the source
phrase s, s1 and s2 on the parse tree.
In figure 2, the CFG rule ?ADVP?AD?,
?VP?VV AS NP?, and ?VP?ADVP
VP? are used as features for s1, s2 and s
respectively.
? Path Features (PF)
The tree path ?(s1)..?(s) connecting ?(s1)
and ?(s), ?(s2)..?(s) connecting ?(s2)
and ?(s), and ?(s)..? connecting ?(s) and
the root node ? of the whole parse tree are
used as features. These features provide
syntactic ?vertical context? which shows the
generation history of the source phrases on
the parse tree.
(a) (b) (c)
Figure 3: Three scenarios of the relationship be-
tween phrase boundaries and constituent bound-
aries. The gray circles are constituent boundaries
while the black circles are phrase boundaries.
In figure 2, the path features are ?ADVP
VP?, ?VP VP? and ?VP IP? for s1, s2 and s
respectively.
? Constituent Boundary Matching Features
(CBMF)
These features are to capture the relationship
between a source phrase s and ?(s) or
?(s)?s subtrees. There are three different
scenarios3: 1) exact match, where s exactly
matches the boundaries of ?(s) (figure 3(a)),
2) inside match, where s exactly spans a
sequence of ?(s)?s subtrees (figure 3(b)), and
3) crossing, where s crosses the boundaries
of one or two subtrees of ?(s) (figure 3(c)).
In the case of 1) or 2), we set the value of
this feature to ?(s)-M or ?(s)-I respectively.
When s crosses the boundaries of the sub-
constituent ?l on s?s left, we set the value to
?(?l)-LC; If s crosses the boundaries of the
sub-constituent ?r on s?s right, we set the
value to ?(?r)-RC; If both, we set the value
to ?(?l)-LC-?(?r)-RC.
Let?s revisit the Figure 2. The source
phrase s1 exactly matches the constituent
ADVP, therefore CBMF is ?ADVP-M?. The
source phrase s2 exactly spans two sub-trees
VV and AS of VP, therefore CBMF is
?VP-I?. Finally, the source phrase s cross
boundaries of the lower VP on the right,
therefore CBMF is ?VP-RC?.
3.3 The Integration of the SDB Model into
Phrase-Based SMT
We integrate the SDB model into phrase-based
SMT to help decoder perform syntax-driven
phrase translation. In particular, we add a
3The three scenarios that we define here are similar to
those in (Lu? et al, 2002).
318
new feature into the log-linear translation model:
PSDB(b|T, ?(.)). This feature is computed by the
SDB model described in equation (3) or equation
(4), which estimates a probability that a source
span is to be translated as a unit within partic-
ular syntactic contexts. If a source span can be
translated as a unit, the feature will give a higher
probability even though this span violates bound-
aries of a constituent. Otherwise, a lower proba-
bility is given. Through this additional feature, we
want the decoder to prefer hypotheses that trans-
late source spans which can be translated as a unit,
and avoids translating those which are discontinu-
ous after translation. The weight of this new fea-
ture is tuned via MERT, which measures the extent
to which this feature should be trusted.
In this paper, we implement the SDB model in a
state-of-the-art phrase-based system which adapts
a binary bracketing transduction grammar (BTG)
(Wu, 1997) to phrase translation and reordering,
described in (Xiong et al, 2006). Whenever a
BTG merging rule (s ? [s1 s2] or s ? ?s1 s2?)
is used, the SDB model gives a probability to the
span s covered by the rule, which estimates the
extent to which the span is bracketable. For the
unary SDB model, we only consider the features
from ?(s). For the binary SDB model, we use all
features from ?(s1), ?(s2) and ?(s) since the bi-
nary SDB model is naturally suitable to the binary
BTG rules.
The SDB model, however, is not only limited
to phrase-based SMT using BTG rules. Since it
is applied on a source span each time, any other
hierarchical phrase-based or syntax-based system
that translates source spans recursively or linearly,
can adopt the SDB model.
4 Experiments
We carried out the MT experiments on Chinese-
to-English translation, using (Xiong et al, 2006)?s
system as our baseline system. We modified the
baseline decoder to incorporate our SDB mod-
els as descried in section 3.3. In order to com-
pare with Marton and Resnik?s approach, we also
adapted the baseline decoder to their XP+ feature.
4.1 Experimental Setup
In order to obtain syntactic trees for SDB models
and XP+, we parsed source sentences using a lex-
icalized PCFG parser (Xiong et al, 2005). The
parser was trained on the Penn Chinese Treebank
with an F1 score of 79.4%.
All translation models were trained on the FBIS
corpus. We removed 15,250 sentences, for which
the Chinese parser failed to produce syntactic
parse trees. To obtain word-level alignments, we
ran GIZA++ (Och and Ney, 2000) on the remain-
ing corpus in both directions, and applied the
?grow-diag-final? refinement rule (Koehn et al,
2005) to produce the final many-to-many word
alignments. We built our four-gram language
model using Xinhua section of the English Gi-
gaword corpus (181.1M words) with the SRILM
toolkit (Stolcke, 2002).
For the efficiency of MERT, we built our de-
velopment set (580 sentences) using sentences not
exceeding 50 characters from the NIST MT-02 set.
We evaluated all models on the NIST MT-05 set
using case-sensitive BLEU-4. Statistical signif-
icance in BLEU score differences was tested by
paired bootstrap re-sampling (Koehn, 2004).
4.2 SDB Training
We extracted 6.55M bracketing instances from our
training corpus using the algorithm shown in fig-
ure 1, which contains 4.67M bracketable instances
and 1.89M unbracketable instances. From ex-
tracted bracketing instances we generated syntax-
driven features, which include 73,480 rule fea-
tures, 153,614 path features and 336 constituent
boundary matching features. To tune weights of
features, we ran the MaxEnt toolkit (Zhang, 2004)
with iteration number being set to 100 and Gaus-
sian prior to 1 to avoid overfitting.
4.3 Results
We ran the MERT module with our decoders to
tune the feature weights. The values are shown
in Table 1. The PSDB receives the largest feature
weight, 0.29 for UniSDB and 0.38 for BiSDB, in-
dicating that the SDB models exert a nontrivial im-
pact on decoder.
In Table 2, we present our results. Like (Mar-
ton and Resnik, 2008), we find that the XP+ fea-
ture obtains a significant improvement of 1.08
BLEU over the baseline. However, using all
syntax-driven features described in section 3.2,
our SDB models achieve larger improvements
of up to 1.67 BLEU. The binary SDB (BiSDB)
model statistically significantly outperforms Mar-
ton and Resnik?s XP+ by an absolute improvement
of 0.59 (relatively 2%). It is also marginally better
than the unary SDB model.
319
Features
System P (c|e) P (e|c) Pw(c|e) Pw(e|c) Plm(e) Pr(e) Word Phr. XP+ PSDB
Baseline 0.041 0.030 0.006 0.065 0.20 0.35 0.19 -0.12 ? ?
XP+ 0.002 0.049 0.046 0.044 0.17 0.29 0.16 0.12 -0.12 ?
UniSDB 0.023 0.051 0.055 0.012 0.21 0.20 0.12 0.04 ? 0.29
BiSDB 0.016 0.032 0.027 0.013 0.13 0.23 0.08 0.09 ? 0.38
Table 1: Feature weights obtained by MERT on the development set. The first 4 features are the phrase
translation probabilities in both directions and the lexical translation probabilities in both directions. Plm
= language model; Pr = MaxEnt-based reordering model; Word = word bonus; Phr = phrase bonus.
BLEU-n n-gram Precision
System 4 1 2 3 4 5 6 7 8
Baseline 0.2612 0.71 0.36 0.18 0.10 0.054 0.030 0.016 0.009
XP+ 0.2720** 0.72 0.37 0.19 0.11 0.060 0.035 0.021 0.012
UniSDB 0.2762**+ 0.72 0.37 0.20 0.11 0.062 0.035 0.020 0.011
BiSDB 0.2779**++ 0.72 0.37 0.20 0.11 0.065 0.038 0.022 0.014
Table 2: Results on the test set. **: significantly better than baseline (p < 0.01). + or ++: significantly
better than Marton and Resnik?s XP+ (p < 0.05 or p < 0.01, respectively).
5 Analysis
In this section, we present analysis to perceive the
influence mechanism of the SDB model on phrase
translation by studying the effects of syntax-driven
features and differences of 1-best translation out-
puts.
5.1 Effects of Syntax-Driven Features
We conducted further experiments using individ-
ual syntax-driven features and their combinations.
Table 3 shows the results, from which we have the
following key observations.
? The constituent boundary matching feature
(CBMF) is a very important feature, which
by itself achieves significant improvement
over the baseline (up to 1.13 BLEU). Both
our CBMF and Marton and Resnik?s XP+
feature focus on the relationship between a
source phrase and a constituent. Their signifi-
cant contribution to the improvement implies
that this relationship is an important syntactic
constraint for phrase translation.
? Adding more features, such as path feature
and rule feature, achieves further improve-
ments. This demonstrates the advantage of
using more syntactic constraints in the SDB
model, compared with Marton and Resnik?s
XP+.
BLEU-4
Features UniSDB BiSDB
PF + RF 0.2555 0.2644*@@
PF 0.2596 0.2671**@@
CBMF 0.2678** 0.2725**@
RF + CBMF 0.2737** 0.2780**++@@
PF + CBMF 0.2755**+ 0.2782**++@?
RF + PF + CBMF 0.2762**+ 0.2779**++
Table 3: Results of different feature sets. * or **:
significantly better than baseline (p < 0.05 or p <
0.01, respectively). + or ++: significantly better
than XP+ (p < 0.05 or p < 0.01, respectively).
@?: almost significantly better than its UniSDB
counterpart (p < 0.075). @ or @@: significantly
better than its UniSDB counterpart (p < 0.05 or
p < 0.01, respectively).
? In most cases, the binary SDB is constantly
significantly better than the unary SDB, sug-
gesting that inner contexts are useful in pre-
dicting phrase bracketing.
5.2 Beyond BLEU
We want to further study the happenings after we
integrate the constraint feature (our SDB model
and Marton and Resnik?s XP+) into the log-linear
translation model. In particular, we want to inves-
tigate: to what extent syntactic constraints change
translation outputs? And in what direction the
changes take place? Since BLEU is not sufficient
320
System CCM Rate (%)
Baseline 43.5
XP+ 74.5
BiSDB 72.4
Table 4: Consistent constituent matching rates re-
ported on 1-best translation outputs.
to provide such insights, we introduce a new sta-
tistical metric which measures the proportion of
syntactic constituents 4 whose boundaries are con-
sistently matched by decoder during translation.
This proportion, which we call consistent con-
stituent matching (CCM) rate , reflects the ex-
tent to which the translation output respects the
source parse tree.
In order to calculate this rate, we output transla-
tion results as well as phrase alignments found by
decoders. Then for each multi-branch constituent
cji spanning from i to j on the source side, we
check the following conditions.
? If its boundaries i and j are aligned to phrase
segmentation boundaries found by decoder.
? If all target phrases inside cji ?s target span 5
are aligned to the source phrases within cji
and not to the phrases outside cji .
If both conditions are satisfied, the constituent cji
is consistently matched by decoder.
Table 4 shows the consistent constituent match-
ing rates. Without using any source-side syntac-
tic information, the baseline obtains a low CCM
rate of 43.53%, indicating that the baseline de-
coder violates the source parse tree more than it
respects the source structure. The translation out-
put described in section 1 is actually generated by
the baseline decoder, where the second NP phrase
boundaries are violated.
By integrating syntactic constraints into decod-
ing, we can see that both Marton and Resnik?s
XP+ and our SDB model achieve a significantly
higher constituent matching rate, suggesting that
they are more likely to respect the source struc-
ture. The examples in Table 5 show that the de-
coder is able to generate better translations if it is
4We only consider multi-branch constituents.
5Given a phrase alignment P = {cgf ? eqp}, if the seg-
mentation within cji defined by P is cji = cj1i1 ...c
jk
ik , and
cjrir ? evrur ? P, 1 ? r ? k, we define the target span of c
j
i
as a pair where the first element is min(eu1 ...euk ) and the
second element is max(ev1 ...evk ), similar to (Fox, 2002).
CCM Rates (%)
System <6 6-10 11-15 16-20 >20
XP+ 75.2 70.9 71.0 76.2 82.2
BiSDB 69.3 74.7 74.2 80.0 85.6
Table 6: Consistent constituent matching rates for
structures with different spans.
faithful to the source parse tree by using syntactic
constraints.
We further conducted a deep comparison of
translation outputs of BiSDB vs. XP+ with re-
gard to constituent matching and violation. We
found two significant differences that may explain
why our BiSDB outperforms XP+. First, although
the overall CCM rate of XP+ is higher than that
of BiSDB, BiSDB obtains higher CCM rates for
long-span structures than XP+ does, which are
shown in Table 6. Generally speaking, viola-
tions of long-span constituents have a more neg-
ative impact on performance than short-span vio-
lations if these violations are toxic. This explains
why BiSDB achieves relatively higher precision
improvements for higher n-grams over XP+, as
shown in Table 3.
Second, compared with XP+ that only punishes
constituent boundary violations, our SDB model
is able to encourage violations if these violations
are done on bracketable phrases. We observed in
many cases that by violating constituent bound-
aries BiSDB produces better translations than XP+
does, which on the contrary matches these bound-
aries. Still consider the example shown in section
1. The following translations are found by XP+
and BiSDB respectively.
XP+: [to/? ?[set up/?? [for the/? [naviga-
tion/?? section/?]]] on July 11/7?11??]
BiSDB: [to/? ?[[set up/?? a/?] [marine/??
festival/?]] on July 11/7?11??]
XP+ here matches all constituent boundaries while
BiSDB violates the PP constituent to translate the
non-syntactic phrase ??? ??. Table 7 shows
more examples. From these examples, we clearly
see that appropriate violations are helpful and even
necessary for generating better translations. By
allowing appropriate violations to translate non-
syntactic phrases according to particular syntac-
tic contexts, our SDB model better inherits the
strength of phrase-based approach than XP+.
321
Src: [[? [???????]NP ]PP [?? [??]NP [????]NP ]VP ]VP
Ref: show their loving hearts to people in the Indian Ocean disaster areas
Baseline: ?love/?? [for the/? ?[people/?? [to/?? [own/?? a report/??]]]? ?in/?? the Indian Ocean/?
???]?
XP+: ?[contribute/?? [its/?? [part/?? love/??]]] [for/? ?the people/?? ?in/?? the Indian Ocean/?
????]?
BiSDB: ?[[[contribute/?? its/??] part/??] love/??] [for/? ?the people/?? ?in/?? the Indian Ocean?
????]?
Src: [???? [?]ADVP [?? [[???]QP ??]NP [???]PP]VP]IP [?]PU [????...]IP
Ref: The Pentagon has dispatched 20 airplanes to South Asia, including...
Baseline: [[The Pentagon/???? has sent/???] [?[to/? [[South Asia/?? ,/?] including/????]] [20/?
? plane/???]?]]
XP+: [The Pentagon/???? [has/? [sent/?? [[20/?? planes/???] [to/? South Asia/??]]]]] [,/?
[including/????...]]
BiSDB: [The Pentagon/???? [has sent/??? [[20/?? planes/???] [to/? South Asia/??]]] [,/? [in-
cluding/????...]]
Table 5: Translation examples showing that both XP+ and BiSDB produce better translations than the
baseline, which inappropriately violates constituent boundaries (within underlined phrases).
Src: [[? [[[????????]NP [??]ADJP [??]NP]NP ?]LCP]PP ??]VP
Ref: said after a brief discussion with Powell at the US State Department
XP+: [?after/? ??[a brief/?? meeting/??] [with/? Powell/??]? [in/? the US State Department/???
??]? said/??]
BiSDB: ?said after/??? ?[a brief/?? meeting/??] ? with Powell/??? [at/? the State Department of the
United States/?????]???
Src: [? [[?? [??????]NP]VP]IP]PP [??? [??????]NP]VP
Ref: took a key step towards building future democratic politics
XP+: ?[a/? [key/??? step/???]] ?forward/?? [to/? [a/?? [future/?? political democracy/???
?]]]??
BiSDB: ?[made a/??? [key/??? step/???]] [towards establishing a/??? ?democratic politics/???
? in the future/???]?
Table 7: Translation examples showing that BiSDB produces better translations than XP+ via appropriate
violations of constituent boundaries (within double-underlined phrases).
6 Conclusion
In this paper, we presented a syntax-driven brack-
eting model that automatically learns bracketing
knowledge from training corpus. With this knowl-
edge, the model is able to predict whether source
phrases can be translated together, regardless of
matching or crossing syntactic constituents. We
integrate this model into phrase-based SMT to
increase its capacity of linguistically motivated
translation without undermining its strengths. Ex-
periments show that our model achieves substan-
tial improvements over baseline and significantly
outperforms (Marton and Resnik, 2008)?s XP+.
Compared with previous constituency feature,
our SDB model is capable of incorporating more
syntactic constraints, and rewarding necessary vi-
olations of the source parse tree. Marton and
Resnik (2008) find that their constituent con-
straints are sensitive to language pairs. In the fu-
ture work, we will use other language pairs to test
our models so that we could know whether our
method is language-independent.
References
Colin Cherry. 2008. Cohesive Phrase-based Decoding
for Statistical Machine Translation. In Proceedings
of ACL.
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of ACL, pages 263?270.
David Chiang, Yuval Marton and Philip Resnik. 2008.
Online Large-Margin Training of Syntactic and
Structural Translation Features. In Proceedings of
EMNLP.
Heidi J. Fox 2002. Phrasal Cohesion and Statistical
Machine Translation. In Proceedings of EMNLP,
pages 304?311.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of HLT-NAACL.
322
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Eval-
uation. In International Workshop on Spoken Lan-
guage Translation.
Yajuan Lu?, Sheng Li, Tiezhun Zhao and Muyun Yang.
2002. Learning Chinese Bracketing Knowledge
Based on a Bilingual Language Model. In Proceed-
ings of COLING.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrase-Based Transla-
tion. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of
ACL 2000.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatically
Evaluation of Machine Translation. In Proceedings
of ACL.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
Yueliang Qian. 2005. Parsing the Penn Chinese
Treebank with Semantic Knowledge. In Proceed-
ings of IJCNLP, Jeju Island, Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Max-
imum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proceedings of
ACL-COLING 2006.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Linguistically Annotated BTG for Statistical
Machine Translation. In Proceedings of COLING
2008.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
323
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 941?948,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Comparative Study of Hypothesis Alignment and its Improvement 
for Machine Translation System Combination 
Boxing Chen*, Min Zhang, Haizhou Li and Aiti Aw 
 
Institute for Infocomm Research 
1 Fusionopolis Way, 138632 Singapore 
{bxchen, mzhang, hli, aaiti}@i2r.a-star.edu.sg 
 
 
Abstract 
Recently confusion network decoding shows 
the best performance in combining outputs 
from multiple machine translation (MT) sys-
tems. However, overcoming different word 
orders presented in multiple MT systems dur-
ing hypothesis alignment still remains the 
biggest challenge to confusion network-based 
MT system combination. In this paper, we 
compare four commonly used word align-
ment methods, namely GIZA++, TER, CLA 
and IHMM, for hypothesis alignment. Then 
we propose a method to build the confusion 
network from intersection word alignment, 
which utilizes both direct and inverse word 
alignment between the backbone and hypo-
thesis to improve the reliability of hypothesis 
alignment. Experimental results demonstrate 
that the intersection word alignment yields 
consistent performance improvement for all 
four word alignment methods on both Chi-
nese-to-English spoken and written language 
tasks. 
1 Introduction 
Machine translation (MT) system combination 
technique leverages on multiple MT systems to 
achieve better performance by combining their 
outputs. Confusion network based system com-
bination for machine translation has shown 
promising advantage compared with other tech-
niques based system combination, such as sen-
tence level hypothesis selection by voting and 
source sentence re-decoding using the phrases or 
translation models that are learned from the 
source sentences and target hypotheses pairs 
(Rosti et al, 2007a; Huang and Papineni, 2007). 
In general, the confusion network based sys-
tem combination method for MT consists of four 
steps: 1) Backbone selection: to select a back-
bone (also called ?skeleton?) from all hypotheses. 
The backbone defines the word orders of the fi-
nal translation. 2) Hypothesis alignment: to build 
word-alignment between backbone and each hy-
pothesis. 3) Confusion network construction: to 
build a confusion network based on hypothesis 
alignments. 4) Confusion network decoding: to 
decode the best translation from a confusion 
network. Among the four steps, the hypothesis 
alignment presents the biggest challenge to the 
method due to the varying word orders between 
outputs from different MT systems (Rosti et al 
2007). Many techniques have been studied to 
address this issue. Bangalore et al (2001) used 
the edit distance alignment algorithm which is 
extended to multiple strings to build confusion 
network, it only allows monotonic alignment. 
Jayaraman and Lavie (2005) proposed a heuris-
tic-based matching algorithm which allows non-
monotonic alignments to align the words be-
tween the hypotheses. More recently, Matusov et 
al. (2006, 2008) used GIZA++ to produce word 
alignment for hypotheses pairs. Sim et al (2007), 
Rosti et al (2007a), and Rosti et al (2007b) used 
minimum Translation Error Rate (TER) (Snover 
et al, 2006) alignment to build the confusion 
network. Rosti et al (2008) extended TER algo-
rithm which allows a confusion network as the 
reference to compute word alignment. Karakos et 
al. (2008) used ITG-based method for hypothesis 
alignment. Chen et al (2008) used Competitive 
Linking Algorithm (CLA) (Melamed, 2000) to 
align the words to construct confusion network. 
Ayan et al (2008) proposed to improve align-
ment of hypotheses using synonyms as found in 
WordNet (Fellbaum, 1998) and a two-pass 
alignment strategy based on TER word align-
ment approach. He et al (2008) proposed an 
IHMM-based word alignment method which the 
parameters are estimated indirectly from a varie-
ty of sources. 
Although many methods have been attempted, 
no systematic comparison among them has been 
reported. A through and fair comparison among 
them would be of great meaning to the MT sys-
941
tem combination research. In this paper, we im-
plement a confusion network-based decoder. 
Based on this decoder, we compare four com-
monly used word alignment methods (GIZA++, 
TER, CLA and IHMM) for hypothesis alignment 
using the same experimental data and the same 
multiple MT system outputs with similar features 
in terms of translation performance. We conduct 
the comparison study and other experiments in 
this paper on both spoken and newswire do-
mains: Chinese-to-English spoken and written 
language translation tasks. Our comparison 
shows that although the performance differences 
between the four methods are not significant, 
IHMM consistently show slightly better perfor-
mance than other methods. This is mainly due to 
the fact the IHMM is able to explore more know-
ledge sources and Viterbi decoding used in 
IHMM allows more thorough search for the best 
alignment while other methods has to use less 
optimal greedy search.  
In addition, for better performance, instead of 
only using one direction word alignment (n-to-1 
from hypothesis to backbone) as in previous 
work, we propose to use more reliable word 
alignments which are derived from the intersec-
tion of two-direction hypothesis alignment to 
construct confusion network. Experimental re-
sults show that the intersection word alignment-
based method consistently improves the perfor-
mance for all four methods on both spoken and 
written language tasks. 
This paper is organized as follows. Section 2 
presents a standard framework of confusion net-
work based machine translation system combina-
tion. Section 3 introduces four word alignment 
methods, and the algorithm of computing inter-
section word alignment for all four word align-
ment methods. Section 4 describes the experi-
ments setting and results on two translation tasks. 
Section 5 concludes the paper. 
2 Confusion network based system 
combination 
In order to compare different hypothesis align-
ment methods, we implement a confusion net-
work decoding system as follows: 
Backbone selection: in the previous work, 
Matusov et al (2006, 2008) let every hypothesis 
play the role of the backbone (also called ?skele-
ton? or ?alignment reference?) once. We follow 
the work of (Sim et al, 2007; Rosti et al, 2007a; 
Rosti et al, 2007b; He et al, 2008) and choose 
the hypothesis that best agrees with other hypo-
theses on average as the backbone by applying 
Minimum Bayes Risk (MBR) decoding (Kumar 
and Byrne, 2004).  TER score (Snover et al 
2006) is used as the loss function in MBR decod-
ing. Given a hypothesis set H, the backbone can 
be computed using the following equation, where  
( , )TER ? ?  returns the TER score of two hypothes-
es. 
 
?
?arg min ( , )b
E H E H
E TER E E
? ?
= ?           (1) 
Hypothesis alignment: all hypotheses are 
word-aligned to the corresponding backbone in a 
many-to-one manner. We apply four word 
alignment methods: GIZA++-based, TER-based, 
CLA-based, and IHMM-based word alignment 
algorithm. For each method, we will give details 
in the next section. 
Confusion network construction: confusion 
network is built from one-to-one word alignment; 
therefore, we need to normalize the word align-
ment before constructing the confusion network.  
The first normalization operation is removing 
duplicated links, since GIZA++ and IHMM-
based word alignments could be n-to-1 mappings 
between the hypothesis and backbone. Similar to 
the work of (He et al, 2008), we keep the link 
which has the highest similarity measure 
( , )j iS e e?  based on surface matching score, such 
as the length of maximum common subsequence 
(MCS) of the considered word pair. 
2 ( ( , ))
( , )
( ) ( )
j i
j i
j i
len MCS e e
S e e
len e len e
??
? =
? +
          (2) 
where ( , )j iMCS e e?  is the maximum common 
subsequence of word je?  and ie ; (.)len  is a 
function to compute the length of letter sequence. 
The other hypothesis words are set to align to the 
null word. For example, in Figure 1, 1e? and 3e?  
are aligned to the same backbone word 
2e , we 
remove the link between 
2e  and 3e?  if 
3 2 1 2( , ) ( , )S e e S e e? ?< , as shown in Figure 1 (b). 
The second normalization operation is reorder-
ing the hypothesis words to match the word order 
of the backbone. The aligned words are reor-
dered according to their alignment indices. To 
reorder the null-aligned words, we need to first 
insert the null words into the proper position in 
the backbone and then reorder the null-aligned 
hypothesis words to match the nulls on the back-
bone side. Reordering null-aligned words varies 
based to the word alignment method in the pre-
942
vious work. We reorder the null-aligned word 
following the approach of Chen et al (2008) 
with some extension. The null-aligned words are 
reordered with its adjacent word: moving with its 
left word (as Figure 1 (c)) or right word (as Fig-
ure 1 (d)). However, to reduce the possibility of 
breaking a syntactic phrase, we extend to choose 
one of the two above operations depending on 
which one has the higher likelihood with the cur-
rent null-aligned word. It is implemented by 
comparing two association scores based on co-
occurrence frequencies. They are association 
score of the null-aligned word and its left word, 
or the null-aligned word and its right word. We 
use point-wise mutual information (MI) as Equa-
tion 3 to estimate the likelihood. 
 11
1
( )
( , ) log
( ) ( )
i i
i i
i i
p e e
MI e e
p e p e
+
+
+
? ?
? ? =
? ?
              (3) 
where 1( )i ip e e +? ?  is the occurrence probability of 
bigram 1i ie e +? ?  observed in the hypothesis list; 
( )ip e?  and 1( )ip e +?  are probabilities of hypothe-
sis word ie?  and 1ie +?  respectively. 
In example of Figure 1, we choose (c) 
if 2 3 3 4( , ) ( , )MI e e MI e e? ? ? ?> , otherwise, word is 
reordered as (d). 
a 
1e  2e  3e  
 
    
 
1e?  2e?  3e?  4e?  
b 
1e  2e  3e  
 
    
 
1e?  2e?  3e?  4e?  
c 
1e  2e  3e  
 
4e?  1e?  2e?  3e?  
d 
 
1e  2e  3e  
3e?  4e?  1e?  2e?  
 
Figure 1: Example of alignment normalization. 
 
Confusion network decoding: the output 
translations for a given source sentence are ex-
tracted from the confusion network through a 
beam-search algorithm with a log-linear combi-
nation of a set of feature functions. The feature 
functions which are employed in the search 
process are:  
? Language model(s), 
? Direct and inverse IBM model-1, 
? Position-based word posterior probabili-
ties (arc scores of the confusion network), 
? Word penalty, 
? N-gram frequencies (Chen et al, 2005), 
? N-gram posterior probabilities (Zens and 
Ney, 2006). 
The n-grams used in the last two feature func-
tions are collected from the original hypotheses 
list from each single system. The weights of fea-
ture functions are optimized to maximize the 
scoring measure (Och, 2003). 
3 Word alignment algorithms 
We compare four word alignment methods 
which are widely used in confusion network 
based system combination or bilingual parallel 
corpora word alignment. 
3.1 Hypothesis-to-backbone word align-
ment 
GIZA++: Matusov et al (2006, 2008) proposed 
using GIZA++ (Och and Ney, 2003) to align 
words between the backbone and hypothesis. 
This method uses enhanced HMM model boot-
strapped from IBM Model-1 to estimate the 
alignment model. All hypotheses of the whole 
test set are collected to create sentence pairs for 
GIZA++ training. GIZA++ produces hypothesis-
backbone many-to-1 word alignments. 
TER-based: TER-based word alignment 
method (Sim et al, 2007; Rosti et al, 2007a; 
Rosti et al, 2007b) is an extension of multiple 
string matching algorithm based on Levenshtein 
edit distance (Bangalore et al, 2001). The TER 
(translation error rate) score (Snover et al, 2006) 
measures the ratio of minimum number of string 
edits between a hypothesis and reference where 
the edits include insertions, deletions, substitu-
tions and phrase shifts. The hypothesis is modi-
fied to match the reference, where a greedy 
search is used to select the set of shifts because 
an optimal sequence of edits (with shifts) is very 
expensive to find. The best alignment is the one 
that gives the minimum number of translation 
edits.  TER-based method produces 1-to-1 word 
alignments. 
CLA-based: Chen et al (2008) used competi-
tive linking algorithm (CLA) (Melamed, 2000) 
to build confusion network for hypothesis rege-
neration. Firstly, an association score is com-
puted for every possible word pair from the 
backbone and hypothesis to be aligned. Then a 
greedy algorithm is applied to select the best 
word alignment. We compute the association 
score from a linear combination of two clues: 
943
surface similarity computed as Equation (2) and 
position difference based distortion score by fol-
lowing (He et al, 2008). CLA works under a 1-
to-1 assumption, so it produces 1-to-1 word 
alignments. 
IHMM-based: He et al (2008) propose an 
indirect hidden Markov model (IHMM) for hy-
pothesis alignment. Different from traditional 
HMM, this model estimates the parameters indi-
rectly from various sources, such as word seman-
tic similarity, surface similarity and distortion 
penalty, etc. For fair comparison reason, we also 
use the surface similarity computed as Equation 
(2) and position difference based distortion score 
which are used for CLA-based word alignment. 
IHMM-based method produces many-to-1 word 
alignments. 
3.2 Intersection word alignment and its ex-
pansion 
In previous work, Matusov et al (2006, 2008) 
used both direction word alignments to compute 
so-called state occupation probabilities and then 
compute the final word alignment. The other 
work usually used only one direction word 
alignment (many/1-to-1 from hypothesis to 
backbone). In this paper, we use more reliable 
word alignments which are derived from the in-
tersection of both direct (hypothesis-to-backbone) 
and inverse (backbone-to-hypothesis) word 
alignments with heuristic-based expansion which 
is widely used in bilingual word alignment. The 
algorithm includes two steps: 
1) Generate bi-directional word alignments. It 
is straightforward for GIZA++ and IHMM to 
generate bi-directional word alignments. This is 
simply achieved by switching the parameters of 
source and target sentences. Due to the nature of 
greedy search in TER, the bi-directional TER-
based word alignments by switching the parame-
ters of source and target sentences are not neces-
sary exactly the same. For example, in Figure 2, 
the word ?shot? can be aligned to either ?shoot? 
or ?the? as the edit cost of word pair (shot, shoot) 
and (shot, the) are the same when compute the 
minimum-edit-distance for TER score. 
 
 
I shot  killer 
I shoot the killer 
a 
 
I shoot the killer 
I  shot killer 
b 
Figure 2: Example of two directions TER-based 
word alignments. 
 
For CLA word alignment, if we use the same 
association score, direct and inverse CLA word 
alignments should be exactly the same. There-
fore, we use different functions to compute the 
surface similarities, such as using maximum 
common subsequence (MCS) to compute inverse 
word alignment, and using longest matched pre-
fix (LMP) for computing direct word alignment, 
as in Equation (4). 
2 ( ( , ))
( , )
( ) ( )
j i
j i
j i
len LMP e e
S e e
len e len e
??
? =
? +
         (4) 
2) When two word alignments are ready, we 
start from the intersection of the two word 
alignments, and then continuously add new links 
between backbone and hypothesis if and only if 
both of the two words of the new link are un-
aligned and this link exists in the union of two 
word alignments. If there are more than two links 
share a same hypothesis or backbone word and 
also satisfy the constraints, we choose the link 
that with the highest similarity score. For exam-
ple, in Figure 2, since MCS-based similarity 
scores ( , ) ( , )S shot shoot S shot the> , we 
choose alignment (a). 
4  Experiments and results 
4.1 Tasks and single systems 
Experiments are carried out in two domains. One 
is in spoken language domain while the other is 
on newswire corpus. Both experiments are on 
Chinese-to-English translation. 
Experiments on spoken language domain were 
carried out on the Basic Traveling Expression 
Corpus (BTEC) (Takezawa et al, 2002) Chi-
nese- to-English data augmented with HIT-
corpus1. BTEC is a multilingual speech corpus 
which contains sentences spoken by tourists. 
40K sentence-pairs are used in our experiment. 
HIT-corpus is a balanced corpus and has 500K 
sentence-pairs in total. We selected 360K sen-
tence-pairs that are more similar to BTEC data 
according to its sub-topic. Additionally, the Eng-
lish sentences of Tanaka corpus2 were also used 
to train our language model. We ran experiments 
on an IWSLT challenge task which uses IWSLT-
20063 DEV clean text set as development set and 
IWSLT-2006 TEST clean text as test set. 
                                                 
1 http://mitlab.hit.edu.cn/ 
2 http://www.csse.monash.edu.au/~jwb/tanakacorpus.html 
3 http:// www.slc.atr.jp/IWSLT2006/ 
944
Experiments on newswire domain were car-
ried out on the FBIS4 corpus. We used NIST5 
2002 MT evaluation test set as our development 
set, and the NIST 2005 test set as our test set.  
Table 1 summarizes the statistics of the train-
ing, dev and test data for IWSLT and NIST tasks. 
 
task data Ch En 
 
 
 
IWSLT 
Train Sent. 406K 
Words 4.4M 4.6M 
Dev Sent. 489 489?7 
Words 5,896 45,449 
Test Sent. 500 500?7 
Words 6,296 51,227 
Add. Words - 1.7M 
 
 
 
NIST 
Train Sent. 238K 
Words 7.0M 8.9M 
Dev 
2002 
Sent. 878 878?4 
Words 23,248 108,616 
Test 
2005 
Sent. 1,082 1,082?4 
Words 30,544 141,915 
Add. Words - 61.5M 
 
Table 1: Statistics of training, dev and test data 
for IWSLT and NIST tasks. 
 
In both experiments, we used four systems, as 
listed in Table 2,  they are phrase-based system 
Moses (Koehn et al, 2007), hierarchical phrase-
based system (Chiang, 2007), BTG-based lexica-
lized reordering phrase-based system (Xiong et 
al., 2006) and a tree sequence alignment-based 
tree-to-tree translation system (Zhang et al, 
2008). Each system for the same task is trained 
on the same data set. 
4.2 Experiments setting 
For each system, we used the top 10 scored hy-
potheses to build the confusion network. Similar 
to (Rosti et al, 2007a), each word in the hypo-
thesis is assigned with a rank-based score of 
1/ (1 )r+ , where r is the rank of the hypothesis. 
And we assign the same weights to each system. 
For selecting the backbone, only the top hypo-
thesis from each system is considered as a candi-
date for the backbone. 
Concerning the four alignment methods, we 
use the default setting for GIZA++; and use tool-
kit TERCOM (Snover et al, 2006) to compute 
the TER-based word alignment, and also use the 
default setting. For fair comparison reason, we 
                                                 
4 LDC2003E14 
5 http://www.nist.gov/speech/tests/mt/ 
decide to do not use any additional resource, 
such as target language synonym list, IBM model 
lexicon; therefore, only surface similarity is ap-
plied in IHMM-based and CLA-based methods. 
We compute the distortion model by following 
(He et al, 2008) for IHMM and CLA-based me-
thods. The weights for each model are optimized 
on held-out data. 
 
 System Dev Test 
 
IWSLT
Sys1 30.75 27.58 
Sys2 30.74 28.54 
Sys3 29.99 26.91 
Sys4 31.32 27.48 
 
NIST 
Sys1 25.64 23.59 
Sys2 24.70 23.57 
Sys3 25.89 22.02 
Sys4 26.11 21.62 
 
Table 2: Results (BLEU% score) of single sys-
tems involved to system combination. 
4.3 Experiments results 
Our evaluation metric is BLEU (Papineni et al, 
2002), which are to perform case-insensitive 
matching of n-grams up to n = 4.  
Performance comparison of four methods: 
the results based on direct word alignments are 
reported in Table 3, row Best is the best single 
systems? scores; row MBR is the scores of back-
bone; GIZA++, TER, CLA, IHMM stand for 
scores of systems for four word alignment me-
thods. 
z MBR decoding slightly improves the per-
formance over the best single system for both 
tasks. This suggests that the simple voting strate-
gy to select backbone is workable. 
z For both tasks, all methods improve the per-
formance over the backbone. For IWSLT test set, 
the improvements are from 2.06 (CLA, 30.88-
28.82) to 2.52 BLEU-score (IHMM, 31.34-
28.82). For NIST test set, the improvements are 
from 0.63 (TER, 24.31-23.68) to 1.40 BLEU-
score (IHMM, 25.08-23.68). This verifies that 
the confusion network decoding is effective in 
combining outputs from multiple MT systems 
and the four word-alignment methods are also 
workable for hypothesis-to-backbone alignment. 
z For IWSLT task where source sentences are 
shorter (12-13 words per sentence in average), 
the four word alignment methods achieve similar 
performance on both dev and test set. The big-
gest difference is only 0.46 BLEU score (30.88 
for CLA, vs. 31.34 for IHMM). For NIST task 
945
where source sentences are longer (26-28 words 
per sentence in average), the difference is more 
significant. Here IHMM method achieves the 
best performance, followed by GIZA++, CLA 
and TER. IHMM is significantly better than TER 
by 0.77 BLEU-score (from 24.31 to 25.08, 
p<0.05). This is mainly because IHMM exploits 
more knowledge source and Viterbi decoding 
allows more thorough search for the best align-
ment while other methods use less optimal gree-
dy search. Another reason is that TER uses hard 
matching in computing edit distance. 
 
 method Dev Test 
 
 
IWSLT 
Best 31.32 28.54 
MBR 31.40 28.82 
GIZA++ 34.16 31.06 
TER 33.92 30.96 
CLA 33.85 30.88 
IHMM 34.35 31.34 
 
 
NIST 
Best 26.11 23.59 
MBR 26.36 23.68 
GIZA++ 27.58 24.88 
TER 27.15 24.31 
CLA 27.44 24.51
IHMM 27.76 25.08
 
Table 3: Results (BLEU% score) of combined 
systems based on direct word alignments. 
 
Performance improvement by intersection 
word alignment: Table 4 reports the perfor-
mance of the system combinations based on in-
tersection word alignments. It shows that: 
z Comparing Tables 3 and 4, we can see that 
the intersection word alignment-based expansion 
method improves the performance in all the dev 
and test sets for both tasks by 0.2-0.57 BLEU-
score and the improvements are consistent under 
all conditions. This suggests that the intersection 
word alignment-based expansion method is more 
effective than the commonly used direct word-
alignment-based hypothesis alignment method in 
confusion network-based MT system combina-
tion. This is because intersection word align-
ments are more reliable compared with direct 
word alignments, and so for heuristic-based ex-
pansion which is based on the aligned words 
with higher scores. 
z TER-based method achieves the biggest 
performance improvement by 0.4 BLEU-score in 
IWSLT and 0.57 in NIST. Our statistics shows 
that the TER-based word alignment generates 
more inconsistent links between the two-
directional word alignments than other methods. 
This may give the intersection with heuristic-
based expansion method more room to improve 
performance. 
z On the contrast, CLA-based method obtains 
relatively small improvement of 0.26 BLEU-
score in IWSLT and 0.21 in NIST. The reason 
could be that the similarity functions used in the 
two directions are more similar. Therefore, there 
are not so many inconsistent links between the 
two directions. 
z Table 5 shows the number of links modified 
by intersection operation and the BLEU-score 
improvement. We can see that the more the mod-
ified links, the bigger the improvement.  
 
 method Dev Test 
 
 
IWSLT
MBR 31.40 28.82
GIZA++ 34.38 31.40
TER 34.17 31.36
CLA 34.03 31.14
IHMM 34.59 31.74
 
 
NIST 
MBR 26.36 23.68
GIZA++ 27.80 25.11
TER 27.58 24.88
CLA 27.64 24.72
IHMM 27.96 25.37
 
Table 4: Results (BLEU% score) of combined 
systems based on intersection word alignments. 
 
 
 
system 
IWSLT NIST 
Inc. Imp. Inc. Imp.
CLA 1.2K 0.26 9.2K 0.21 
GIZA++ 3.2K 0.36 25.5K 0.23 
IHMM 3.7K 0.40 21.7K 0.29 
TER 4.3K 0.40 40.2K 0.57 
#total links 284K 1,390K 
 
Table 5: Number of modified links and absolute 
BLEU(%) score improvement on test sets. 
 
Effect of fuzzy matching in TER: the pre-
vious work on TER-based word alignment uses 
hard match in counting edits distance. Therefore, 
it is not able to handle cognate words match, 
such as in Figure 2, original TER script count the 
edit cost of (shoot, shot) equals to word pair 
(shot, the). Following (Leusch et al, 2006), we 
modified the TER script to allow fuzzy matching: 
change the substitution cost from 1 for any word 
pair to 
946
 ( , ) 1 ( , )sub j i j iCOST e e S e e? ?= ?              (5) 
which ( , )j iS e e?  is the similarity score based on 
the length of longest matched prefix (LMP) 
computed as in Equation (4).  As a result, the 
fuzzy matching reports 
( , ) 1 (2 3) /(5 4) 1/ 3SubCost shoot shot = ? ? + =  and 
( , ) 1 (2 0) /(5 3) 1SubCost shoot the = ? ? + =  while in 
original TER, both of the two scores are equal to 
1. Since cost of word pair (shoot, shot) is smaller 
than that of word pair (shot, the), word ?shot? 
has higher chance to be aligned to ?shoot? (Fig-
ure 2 (a)) instead of ?the? (Figure 2 (b)). This 
fuzzy matching mechanism is very useful to such 
kind of monolingual alignment task as in hypo-
thesis-to-backbone word alignment since it can 
well model word variances and morphological 
changes. 
Table 6 summaries the results of TER-based 
systems with or without fuzzy matching. We can 
see that the fuzzy matching improves the per-
formance for all cases. This verifies the effect of 
fuzzy matching for TER in monolingual word 
alignment. In addition, the improvement in NIST 
test set (0.36 BLEU-score for direct alignment 
and 0.21 BLEU-score for intersection one) are 
more than that in IWSLT test set (0.15 BLEU-
score for direct alignment and 0.11 BLEU-score 
for intersection one). This is because the sen-
tences of IWSLT test set are much shorter than 
that of NIST test set. 
 
TER-based 
systems 
IWSLT NIST 
Dev Test Dev Test 
Direct align 
+fuzzy match 
33.92 
34.14
30.96 
31.11 
27.15 
27.53
24.31 
24.67
Intersect align 
    +fuzzy match 
34.17 
34.40
31.36 
31.47 
27.58 
27.79
24.88 
25.09
 
Table 6: Results (BLEU% score) of TER-based 
combined systems with or without fuzzy match. 
5 Conclusion 
Confusion-network-based system combination 
shows better performance than other methods in 
combining multiple MT systems? outputs, and 
hypothesis alignment is a key step. In this paper, 
we first compare four word alignment methods 
for hypothesis alignment under the confusion 
network framework. We verify that the confu-
sion network framework is very effective in MT 
system combination and IHMM achieves the best 
performance. Moreover, we propose an intersec-
tion word alignment-based expansion method for 
hypothesis alignment, which is more reliable as it 
leverages on both direct and inverse word align-
ment. Experimental results on Chinese-to-
English spoken and newswire domains show that 
the intersection word alignment-based method 
yields consistent improvements across all four 
word alignment methods. Finally, we evaluate 
the effect of fuzzy matching for TER. 
Theoretically, confusion network decoding is 
still a word-level voting algorithm although it is 
more complicated than other sentence-level vot-
ing algorithms. It changes lexical selection by 
considering the posterior probabilities of words 
in hypothesis lists. Therefore, like other voting 
algorithms, its performance strongly depends on 
the quality of the n-best hypotheses of each sin-
gle system. In some extreme cases, it may not be 
able to improve BLEU-score (Mauser et al, 
2006; Sim et al, 2007). 
 
References  
N. F. Ayan. J. Zheng and W. Wang. 2008. Improving 
Alignments for Better Confusion Networks for 
Combining Machine Translation Systems. In Pro-
ceedings of COLING 2008, pp. 33?40. Manchester, 
Aug. 
S. Bangalore, G. Bordel, and G. Riccardi. 2001. 
Computing consensus translation from multiple 
machine translation systems. In Proceeding of 
IEEE workshop on Automatic Speech Recognition 
and Understanding, pp. 351?354. Madonna di 
Campiglio, Italy. 
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M. 
Federico. 2005. The ITC-irst SMT System for 
IWSLT-2005. In Proceeding of IWSLT-2005, 
pp.98-104, Pittsburgh, USA, October. 
B. Chen, M. Zhang, A. Aw and H. Li. 2008. Regene-
rating Hypotheses for Statistical Machine Transla-
tion. In: Proceeding of COLING 2008. pp105-112. 
Manchester, UK. Aug. 
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228. 
C. Fellbaum. editor. 1998. WordNet: An Electronic 
Lexical Database. MIT Press. 
X. He, M. Yang, J. Gao, P. Nguyen, R. Moore, 2008. 
Indirect-HMM-based Hypothesis Alignment for 
Combining Outputs from Machine Translation 
Systems. In Proceeding of EMNLP. Hawaii, US, 
Oct. 
F. Huang and K. Papinent. 2007. Hierarchical System 
Combination for Machine Translation. In Proceed-
ings of the 2007 Joint Conference on Empirical 
Methods in Natural Language Processing and 
947
Computational Natural Language Learning 
(EMNLP-CoNLL?2007), pp. 277 ? 286, Prague, 
Czech Republic, June. 
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching. 
In Proceeding of EAMT. pp.143?152. 
D. Karakos, J. Eisner, S. Khudanpur, and M. Dreyer. 
2008. Machine Translation System Combination 
using ITG-based Alignments. In Proceeding of 
ACL-HLT 2008, pp. 81?84. 
O. Kraif, B. Chen. 2004. Combining clues for lexical 
level aligning using the Null hypothesis approach. 
In: Proceedings of COLING 2004, Geneva, Au-
gust, pp. 1261-1264.  
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and 
E. Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In Proceedings of 
ACL-2007. pp. 177-180, Prague, Czech Republic. 
S. Kumar and W. Byrne. 2004. Minimum Bayes Risk 
Decoding for Statistical Machine Translation. In    
Proceedings of HLT-NAACL 2004, May 2004, 
Boston, MA, USA. 
G. Leusch, N. Ueffing and H. Ney. 2006. CDER: Ef-
ficient MT Evaluation Using Block Movements. In 
Proceedings of EACL. pp. 241-248. Trento Italy. 
E. Matusov, N. Ueffing, and H. Ney. 2006. Compu-
ting consensus translation from multiple machine 
translation systems using enhanced hypotheses 
alignment. In Proceeding of EACL, pp. 33-40, 
Trento, Italy, April. 
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D. 
Dechelotte, M. Federico, M. Kolss, Y. Lee, J. B. 
Marino, M. Paulik, S. Roukos, H. Schwenk, and H. 
Ney. System Combination for Machine Translation 
of Spoken and Written Language. IEEE Transac-
tions on Audio, Speech and Language Processing, 
volume 16, number 7, pp. 1222-1237, September. 
A. Mauser, R. Zens, E. Matusov, S. Hasan, and H. 
Ney. 2006. The RWTH Statistical Machine Trans-
lation System for the IWSLT 2006 Evaluation. In 
Proceeding of IWSLT 2006, pp. 103-110, Kyoto, 
Japan, November. 
I. D. Melamed. 2000. Models of translational equiva-
lence among words. Computational Linguistics, 
26(2), pp. 221-249. 
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL-
2003. Sapporo, Japan. 
F. J. Och and H. Ney. 2003. A systematic comparison 
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19-51. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 
2002. BLEU: a method for automatic evaluation of 
machine translation. In Proceeding of ACL-2002, 
pp. 311-318. 
A. I. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. 
Schwartz and B. Dorr. 2007a. Combining Outputs 
from Multiple Machine Translation Systems.  In 
Proceeding of NAACL-HLT-2007, pp. 228-235. 
Rochester, NY. 
A. I. Rosti, S. Matsoukas and R. Schwartz. 2007b. 
Improved Word-Level System Combination for 
Ma-chine Translation. In Proceeding of ACL-2007, 
Prague. 
A. I. Rosti, B. Zhang, S. Matsoukas, and R. Schwartz. 
2008. Incremental Hypothesis Alignment for 
Building Confusion Networks with Application to 
Machine Translation System Combination, In Pro-
ceeding of the Third ACL Workshop on Statistical 
Machine Translation, pp. 183-186. 
K. C. Sim, W. J. Byrne, M. J.F. Gales, H. Sahbi, and 
P. C. Woodland. 2007. Consensus network decod-
ing for statistical machine translation system com-
bination. In Proceeding of  ICASSP-2007. 
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. 
Makhoul. 2006. A study of translation edit rate 
with targeted human annotation. In Proceeding of 
AMTA. 
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, 
and S. Yamamoto. 2002. Toward a broad-coverage 
bilingual corpus for speech translation of travel 
conversations in the real world. In Proceeding of 
LREC-2002, Las Palmas de Gran Canaria, Spain. 
D. Xiong, Q. Liu and S. Lin. 2006. Maximum Entro-
py Based Phrase Reordering Model for Statistical 
Machine Translation. In Proceeding of ACL-2006. 
pp.521-528.  
R. Zens and H. Ney. 2006. N-gram Posterior Prob-
abilities for Statistical Machine Translation. In 
Proceeding of HLT-NAACL Workshop on SMT, pp. 
72-77, NY. 
M. Zhang, H. Jiang, A. Aw, H. Li, C. L. Tan, and S. 
Li. 2008. A Tree Sequence Alignment-based Tree-
to-Tree Translation Model. In Proceeding of ACL-
2008. Columbus, US. June. 
Y. Zhang, S. Vogel, and A. Waibel 2004. Interpreting 
BLEU/NIST scores: How much improvement do 
we need to have a better system? In Proceedings of 
LREC 2004, pp. 2051-2054. 
                                                 
*  The first author has moved to National Research 
Council, Canada. His current email address is: Box-
ing.Chen@nrc.ca. 
948
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 21?24,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
MARS: Multilingual Access and Retrieval System with Enhanced 
Query Translation and Document Retrieval 
 
 
Lianhau Lee, Aiti Aw, Thuy Vu, Sharifah Aljunied Mahani, Min Zhang, Haizhou Li 
Institute for Infocomm Research 
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632 
{lhlee, aaiti, tvu, smaljunied, mzhang, hli} 
@i2r.a-star.edu.sg 
 
  
 
Abstract 
In this paper, we introduce a multilingual ac-
cess and retrieval system with enhanced query 
translation and multilingual document retrieval, 
by mining bilingual terminologies and aligned 
document directly from the set of comparable 
corpora which are to be searched upon by us-
ers. By extracting bilingual terminologies and 
aligning bilingual documents with similar con-
tent prior to the search process provide more 
accurate translated terms for the in-domain 
data and support multilingual retrieval even 
without the use of translation tool during re-
trieval time. This system includes a user-
friendly graphical user interface designed to 
provide navigation and retrieval of information 
in browse mode and search mode respectively.  
1 Introduction 
Query translation is an important step in the 
cross-language information retrieval (CLIR). 
Currently, most of the CLIR system relies on 
various kinds of dictionaries, for example Word-
Nets (Luca and Nurnberger, 2006; Ranieri et al, 
2004), in query translation. Although dictionaries 
can provide effective translation on common 
words or even phrases, they are always limited in 
the coverage. Hence, there is a need to expand 
the existing collections of bilingual terminologies 
through various means. 
Recently, there has been more and more re-
search work focus on bilingual terminology ex-
traction from comparable corpora. Some promis-
ing results have been reported making use of sta-
tistics, linguistics (Sadat et al, 2003), translitera-
tion (Udupa et al, 2008), date information (Tao 
and Zhai, 2005) and document alignment ap-
proach (Talvensaari et al, 2007). 
In this paper, we introduce our Multilingual 
Access and Retrieval System ? MARS which 
addresses the query translation issue by using in-
domain bilingual terminologies extracted directly 
from the comparable corpora which are to be 
accessed by users. And at the same time, bilin-
gual documents are paired up prior to the search 
process based on their content similarities to 
overcome the limitation of traditional keyword 
matching based on the translated terms. These 
would provide better retrieval experiences as not 
only more accurate in-domain translated term 
will be used to retrieve the documents but also 
provide a new perspective of multilingual infor-
mation retrieval to process the time-consuming 
multilingual document matching at the backend. 
The following sections of this paper will de-
scribe the system architecture and the proposed 
functionalities of the MARS system. 
2 MARS System 
The MARS system is designed to enhance query 
translation and document retrieval through min-
ing the underlying multilingual structures of 
comparable corpora via a pivot language. There 
are three reasons for using a pivot language. 
Firstly, it is appropriate to use a universal lan-
guage among potential users of different native 
languages. Secondly, it reduces the backend data 
processing cost by just considering the pair-wise 
relationship between the pivot language and any 
other languages. Lastly, the dictionary resources 
between the pivot language and all the other lan-
guages are more likely to be available than oth-
erwise. 
There are two main parts in this system, 
namely data processing and user interface. The 
data processing is an offline process to mine the 
underlying multilingual structure of the compa-
21
rable corpora to support retrieval. The structure 
of the comparable corpora is presented visually 
in the user interface under browse mode and 
search mode to facilitate navigation and retrieval 
of information respectively. 
3 Data Processing  
For demo purpose, three different language 
newspapers from the year 1995 to 2006 pub-
lished by Singapore Press Holding (SPH), 
namely Strait Times1 (English), ZaoBao2 (Chi-
nese) and Berita Harian3  (Malay), are used as 
comparable corpora. In these particular corpora, 
English is chosen as the pivot language and noun 
terms are chosen as the basic semantic unit as 
they represent a huge amount of significant in-
formation. Our strategy is to organize and ma-
nipulate the corpora in three levels of abstraction 
? clusters, documents and terms. And our key 
task over here is to find the underlying associa-
tions of documents or terminologies in each level 
across different languages. 
First, monolingual documents are grouped into 
clusters by k-means algorithm using simple word 
vectors. Then, monolingual noun terms are ex-
tracted from each cluster using linguistic patterns 
and filtered by occurrence statistics globally 
(within cluster) and locally (within document), so 
that they are good representatives for cluster as a 
whole as well as individual documents (Vu et al, 
2008). The extracted terms are then used in 
document clustering in a new cycle and the 
whole process is repeated until the result con-
verges. 
Next, cluster alignment is carried out between 
the pivot language (English) and the other lan-
guages (Chinese, Malay). Clusters can be con-
ceptualized as the collection of documents with 
the same themes (e.g. finance, politics or sports) 
and their alignments as the correspondents in the 
other languages. Since there may be overlaps 
among themes, e.g. finance and economy, each 
cluster is allowed to align to more than one clus-
ter with varying degree of alignment score. 
After that, document alignment is carried out 
between aligned cluster pairs (Vu et al, 2009). 
Note that the corpora are comparable, thus the 
aligned document pairs are inherently compara-
                                                 
1 http://www.straitstimes.com/ an English news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
2 http://www.zaobao.com/ a Chinese news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
3 http://cyberita.asia1.com.sg/ a Malay news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
ble, i.e. they are similar in contents but not iden-
tical as translation pairs. Also as important to 
note that, document alignment harvested over 
here is independent of user query. In other 
words, document alignment is not simply deter-
mined by mere occurrence of certain keyword 
and its absence does not hinder documents to be 
aligned. Hence mining of document alignment 
beforehand improves document retrieval after-
ward. 
Finally, term alignment is likewise generated 
between aligned document pairs. The aligned 
terms are expected to be in-domain translation 
pairs since they are both derived from documents 
of similar contents, and thus they have similar 
contexts. By making use of the results provided 
by each other, document alignment and term 
alignment can be improved over iterations. 
All the mentioned processes are done offline 
and the results are stored in a relational database 
which will handle online queries generated in the 
user interface later on. 
4 User Interface  
As mentioned, there are two modes provided in 
the user interface to facilitate navigation and re-
trieval of information, namely browse mode and 
search mode. Both modes can be switched sim-
ply by clicking on the respective tabs in the user 
interface. In the following, the functionalities of 
the browse mode and the search mode will be 
explained in details. 
4.1 Browse Mode 
Browse mode provides a means to navigate 
through the complex structures underneath an 
overwhelming data with an easily-understood, 
user-friendly graphical interface. In the figure 1, 
the graph in the browse mode gives an overall 
picture of the distribution of documents in vari-
ous clusters and among the different language 
collections. The outer circles represent the lan-
guage repositories and the inner circles represent 
the clusters. The sizes of the clusters are depend-
ing on the number of contained documents and 
the color represents the dominant theme. The 
labels of the highlighted clusters, characterized 
by a set of five distinguished words, are shown in 
the tooltips next to them. By clicking on a clus-
ter, the links depicting the cluster alignments will 
show up. The links to the clusters in the other 
languages are all propagated through the pivot 
language. 
22
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1 Browse mode in the MARS System 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 2 Search mode in the MARS System 
23
The right hand side of the browse panel pro-
vides the detail information about the selected 
cluster using three sub-panels, i.e. top, middle 
and bottom. The top panel displays a list of ex-
tracted terms from the selected cluster. User may 
narrow down the list of interested terms by using 
the search-text column on top. By clicking on a 
term in the list, its translations in other lan-
guages, if any, will be displayed in the middle 
sub-panel and the document containing the term 
will be listed in the bottom sub-panel. The 
?Search? buttons next to the term translations 
provide a short-cut to jump to the search mode 
with the corresponding term translation being cut 
and pasted over. Last but not least, user may 
simply click on any document listed in the bot-
tom sub-panel to read the content of the docu-
ment and its aligned documents in a pop-up win-
dow. 
4.2 Search Mode 
Search mode provides a means for comprehen-
sive information retrieval. Refer to the figure 2, 
user may enter query in any of the selected lan-
guages to search for documents in all languages. 
The main difference is that query translation is 
done via bilingual terms extracted via the term 
alignment technology discussed earlier. For each 
retrieved document, documents with similar con-
tent in the other languages are also provided to 
supplement the searched results. This enables 
documents which are potentially relevant to the 
users be retrieved as some of these retrieved 
documents may not contain the translated terms 
at all. 
On top of the query translation, other informa-
tion such as related terms and similar terms to 
the query are shown at the tab panel on the right. 
Related terms are terms that correlate statistically 
with the query term and they are arranged by 
cluster, separated by dotted line in the list. Simi-
lar terms are longer terms that contains the query 
term in itself. Both the related terms and the 
similar terms provide user additional hints and 
guides to improve further queries. 
5 Conclusion  
The MARS system is developed to enable user to 
better navigate and search information from mul-
tilingual comparable corpora in a user-friendly 
graphical user interface. Query translation and 
document retrieval is enhanced by utilizing the 
in-domain bilingual terminologies and document 
alignment acquired from the comparable corpora 
itself, without limited by dictionaries and key-
word matching. 
Currently, the system only support simple 
query. Future work will improve on this to allow 
more general query. 
References  
Ernesto William De Luca, and Andreas Nurnberger. 
2006. A Word Sense-Oriented User Interface 
for Interactive Multilingual Text Retrieval, In 
Proceedings of the Workshop Information Re-
trieval, Hildesheim.  
M. Ranieri, E. Pianta, and L. Bentivogli. 2004. 
Browsing Multilingual Information with the 
MultiSemCor Web Interface, In Proceedings of 
the LREC-2004 Workshop ?The amazing utility of 
parallel and comparable corpora?, Lisban, Portu-
gal. 
Fatiha Sadat, Masatoshi Yoshikawa, Shunsuke Ue-
mura. 2003. Learning bilingual translations 
from comparable corpora to cross-language 
information retrieval: hybrid statistics-based 
and linguistics-based approach, In Proceedings 
of the 6th international workshop on Information 
Retrieval with Asian Languages, vol. 1: pp. 57-64. 
 Raghavendra Udupa, K. Saravanan, A. Kumaran, 
Jagadeesh Jagarlamudi. 2008. Mining named en-
tity transliteration equivalents from compara-
ble corpora. In Proceedings of the 17th ACM con-
ference on Information and knowledge manage-
ment. 
Tao Tao, and ChengXiang Zhai. 2005. Mining com-
parable bilingual text corpora for cross-
language information integration. In Proceed-
ings of the 11th ACM SIGKDD international con-
ference on Knowledge discovery in data mining. 
Tuomas Talvensaari, Jorma Laurikkala, Kalervo Jar-
velin, Martti Juhola, Heikki Keskustalo. 2007. 
Creating and exploiting a comparable corpus 
in cross-language information retrieval. ACM 
Transactions on Information System (TOIS), vol. 
25(1):  Article No 4. 
Thuy Vu, Aiti Aw, Min Zhang. 2008. Term extrac-
tion through unithood and termhood unifica-
tion. In Proceedings of the 3rd International Joint 
Conference on Natural Language Processing 
(IJCNLP-08), Hyderabad, India. 
Thuy Vu, Aiti Aw, Min Zhang. 2009. Feature-based 
Method for Document Alignment in Compara-
ble News Corpora. In Proceedings of the 12th 
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL-09), 
Athens, Greece. 
24
Coling 2010: Poster Volume, pages 639?646,
Beijing, August 2010
EM-based Hybrid Model for Bilingual Terminology Extraction 
from Comparable Corpora 
 
Lianhau Lee, Aiti Aw, Min Zhang, Haizhou Li 
Institute for Inforcomm Research 
{lhlee, aaiti, mzhang, hli}@i2r.a-star.edu.sg 
 
 
Abstract 
In this paper, we present an unsuper-
vised hybrid model which combines sta-
tistical, lexical, linguistic, contextual, 
and temporal features in a generic EM-
based framework to harvest bilingual 
terminology from comparable corpora 
through comparable document align-
ment constraint. The model is configur-
able for any language and is extensible 
for additional features. In overall, it pro-
duces considerable improvement in per-
formance over the baseline method. On 
top of that, our model has shown prom-
ising capability to discover new bilin-
gual terminology with limited usage of 
dictionaries. 
1 Introduction 
Bilingual terminology extraction or term align-
ment has been well studied in parallel corpora. 
Due to the coherent nature of parallel corpora, 
various statistical methods, like EM algorithm 
(Brown et. al., 1993) have been proven to be 
effective and have achieved excellent perform-
ance in term of precision and recall. The limita-
tion of parallel corpora in all domains and lan-
guages has led some researchers to explore 
ways to automate the parallel sentence extrac-
tion process from non-parallel corpora 
(Munteanu and Marcu, 2005; Fung and Cheung, 
2004) before proceeding to the usual term 
alignment extraction using the existing tech-
niques for parallel corpora. Nevertheless, the 
coverage is limited since parallel sentences in 
non-parallel corpora are minimal. 
Meanwhile, some researchers have started to 
exploit comparable corpora directly in a new 
manner. The motivations for such an approach 
are obvious: comparable corpora are abundantly 
available, from encyclopedia to daily newspa-
pers, and the human effort is reduced in either 
generating or collecting these corpora. If bilin-
gual terminology can be extracted directly from 
these corpora, evolving or emerging terminol-
ogies can be captured much faster than lexicog-
raphy and this would facilitate many tasks and 
applications in accessing cross-lingual informa-
tion. 
There remain challenges in term alignment 
for comparable corpora. The structures of texts, 
paragraphs and sentences can be very different. 
The similarity of content in two documents var-
ies through they talk about the same subject 
matter. Recent research in using transliteration 
(Udupa et. al., 2008; Knight and Graehl, 1998), 
context information (Morin et. al., 2007; Cao 
and Li, 2002; Fung, 1998), part-of-speech tag-
ging, frequency distribution (Tao and Zhai, 
2005) or some hybrid methods (Klementiev and 
Roth, 2006; Sadat et. al., 2003) have shone 
some light in dealing with comparable corpora. 
In particular, context information seems to be 
popular since it is ubiquitous and can be re-
trieved from corpora easily. 
In this paper, we propose an EM-based hy-
brid model for term alignment to address the 
issue. Through this model, we hope to discover 
new bilingual terminology from comparable 
corpora without supervision. In the following 
sections, the model will be explained in details. 
639
2 System Architecture 
It is expensive and challenging to extract bilin-
gual terminologies from a given set of compa-
rable corpora if they are noisy with very diverse 
topics. Thus the first thing we do is to derive the 
document association relationship between two 
corpora of different languages. To do this, we 
adopt the document alignment approach pro-
posed by Vu et. al. (2009) to harvest compara-
ble news document pairs. Their approach is re-
lying on 3 feature scores, namely Title-n-
Content (TNC), Linguistic Independent Unit 
(LIU), and Monolingual Term Distribution 
(MTD). In the nutshell, they exploit common 
words, numbers and identical strings in titles 
and contents as well as their distribution in time 
domain. Their method is shown to be superior 
to Tao and Zai (2005) which simply make use 
of frequency correlation of words. 
After we have retrieved comparable docu-
ment pairs, we tokenize these documents with 
prominent monolingual noun terms found 
within. We are interested only in noun terms 
since they are more informative and more im-
portantly they are more likely not to be covered 
by dictionary and we hope to find their transla-
tions through comparable bilingual corpora. We 
adopt the approach developed by Vu et. al. 
(2008). They first use the state-of-the-art C/NC-
Value method (Frantzi and Ananiadou, 1998) to 
extract terms based on the global context of the 
corpus, follow by refining the local terms for 
each document with a term re-extraction process 
(TREM) using Viterbi algorithm. 
 
 
Figure 1. The procedure of bilingual terminol-
ogy extraction from comparable documents.  
 
After these preprocesses, we have a set of 
comparable bilingual document pairs and a set 
of prominent monolingual noun terms for each 
monolingual document. The aim of our term 
alignment model is to discover new bilingual 
terminology formed from these monolingual 
terms across aligned document pairs (Figure.1). 
Like other approaches to comparable corpora, 
there exist many challenges in aligning bilingual 
terms due to the presence of noises and the sig-
nificant text-structure disparity across the com-
parable bilingual documents. To overcome this, 
we propose using both corpus-driven and non-
corpus-driven information, from which we draw 
various features and derive our hybrid model. 
These features are used to make initial guess on 
the alignment score of term pair candidates. Fig-
ure 2 shows the overall process of our term 
alignment model on comparable corpora. This 
model is language independent and it comprises 
several main components: 
? EM algorithm 
? Term alignment initialization 
? Mutual information (MI) & TScore res-
coring 
 
Figure 2. Term alignment model.  D = docu-
ment alignment score, L = lexical similarity, N 
= named entity similarity, C = context similar-
ity, T = temporal similarity, R = related term 
similarity. 
640
3 EM Algorithm 
We make two assumptions on the preprocesses 
that the extracted monolingual terms are good 
representatives of their source documents, and 
the document alignment scores derived from 
document alignment process are good indicators 
of how well the contents of various documents 
align. Hence, the logical implication suggests 
that the extracted terms from both well aligned 
documents could well be candidates of aligned 
term pairs. 
By reformulating the state-of-the-art EM-
based word alignment framework IBM model 1 
(Brown et. al., 1993), we can derive a term 
alignment model easily. In IBM word alignment 
model 1, the task is to find word alignment by 
using parallel sentences. In the reformulated 
model for term alignment, parallel sentences are 
replaced by comparable documents, character-
ized by document alignment score and their rep-
resentative monolingual terms. 
The significant advantage over the original 
IBM model 1 is the relaxation of parallel sen-
tences or parallel corpora, by incorporating an 
additional feature of document alignment score. 
We initialize the term alignment score of the 
corresponding term pair candidates with the 
document alignment score to reflect the confi-
dence level of document alignment. Other than 
that, we also employ a collection of feature 
similarity score: lexical similarity, named entity 
similarity, context similarity, temporal similar-
ity, and related term similarity, to term align-
ment initialization. We will explain this further 
in the next section. 
As we know, IBM model 1 will converge to 
the global maximum regardless of the initial 
assignment. This is truly good news for parallel 
corpora, but not for comparable corpora which 
contains a lot of noises. To prevent IBM model 
1 from overfitting, we choose to run ten itera-
tions (each iteration consists of one E-step and 
one M-step) for each cycle of EM in both e-f 
and f-e directions.  
After each cycle of EM process, we simply 
filter off the weak term alignment pairs of both 
directions with a high threshold (0.8) and popu-
late the lexicon database with the remaining 
pairs and use it to start another cycle of EM. 
The process repeats until no new term align-
ment pair is found. The EM algorithm for term 
alignment is shown as follow: 
 
Figure 3. EM algorithm for e-f direction, where 
e[k] = k-th aligned source document, f[k] = k-th 
aligned target document, e[k,i] = i-th term in 
e[k], f[k,j] = j-th term in f[k], a[i,j,k] = probabil-
ity of alignment from f[k,j] to e[k,i], t(f|e) = 
probability of alignment from term e to term f. 
4 Term Alignment Initialization 
We retrieve term alignment candidates by pair-
ing all possible combinations of extracted 
monolingual source terms and target terms 
across the aligned document pairs. Before each 
cycle of EM, we assign an initial term align-
ment score, t(f|e) to each of these term pair can-
didates. Basically, we initialize the term align-
ment score t(f|e) based on document alignment 
score (D), lexical similarity (L), named entity 
similarity (N), context similarity (C), temporal 
similarity (T), and related term similarity (R). 
The similarity calculations of the corpus-driven 
features (D, C, T, R) are derived directly from 
the corpus and require limited lexical resource. 
The non-corpus-driven features (L, N) make use 
of a small word based bilingual dictionary to 
measure their lexical relevancy. That makes our 
model not resource-demanding and it shows that 
our model can work under limited resource 
condition. 
All the above features contribute to the term 
alignment score t(f|e) independently, and we 
formulate their cumulative contributions as the 
following: 
Initialize t(f|e). 
for (iteration = 1 to 10) 
E step 
kjiallfor
ikejkft
ikejkft
kjia
i
,,,
]),[|],[(
]),[|],[(
],,[ ?=
 
M step 
),(,
),(
),(
)|(
),(],,,[),(
],[
,],[
:,,
feallfor
fetcount
fetcount
eft
feallforkjiafetcount
f
fjkf
eike
kji
?
?
=
=
==
 
End for.
641
)|()|()|()|(
)|()|()|(
,:),(
efRefTefCefN
efLEFDeft
FfEeFE
????
???
???
?= ?
??   
where, 
e = source term 
f  = target term 
E  = source document 
F   = target document 
D   = document alignment score 
L   = lexical similarity 
N   = named entity similarity 
C  = context similarity 
T   = temporal similarity 
R   = related term similarity 
  
 (1) 
 
This formula allows us to extend the model with 
additional features without affecting the existing 
configuration. 
4.1 Document Alignment Score (D) 
As explained in the Section 3, the relaxation on 
the requirement of parallel corpora in the new 
EM model leads to the incorporation of 
document alignment score. To indicate the 
confidence level of document alignment, we 
credit every aligned term pair candidate formed 
across the aligned documents with the 
corresponding document alignment score.  
Although it is not necessary, document 
alignment score is first normalized to the range 
of [0,1], with 1 indicates parallel alignment. 
4.2 Lexical Similarity (L) 
We design a simple lexical similarity measure-
ment of two terms based on word translation. 
Term pairs that share more than 50% of word 
translation pairs will be credited with lexical 
similarity of L0, where L0 is configurable con-
tribution weightage of lexical similarity. This 
provides us a primitive hint on term alignment 
without resorting to exhaustive dictionary 
lookup. 
??
? ?=
otherwise
efTifL
efL W
,1
5.0)|(,
)|( 0  
where L0 > 1 and  TW(f|e) is word translation 
score.  
(2)
 
4.3 Named Entity Similarity (N) 
Named entity similarity is a measure of prede-
fined category membership likelihood, such as 
person, location and organization. Term pairs 
that belong to the same NE categories will be 
credited with named entity similarity of N0, 
where N0 is a configurable weightage of named 
entity similarity. We use this similarity score to 
discover bilingual terms of same NE categories, 
yet not covered by bilingual dictionary. 
??
?=
otherwise
matchcategoriesNEifN
efN
,1
,
)|( 0  
where N0 > 1. 
(3)
 
4.4 Context Similarity (C) 
We assume that terms with similar contexts are 
likely to have similar meaning. Thus, we make 
use of context similarity to measure semantic 
similarity. Here, only k nearest content words 
(verbs, nouns, adjectives and adverbs) before or 
after the terms within the sentence boundary are 
considered as its contexts. The following shows 
the calculation of context similarity of two 
terms based on cosine similarity between their 
context frequency vectors before scaling to the 
range of [1, C0], where C0 is a configurable con-
tribution weightage of context similarity. As 
shown in the formula, the t(f?|e?) accounts for 
the translation probability from the source con-
text word to the target context word, hence the 
cosine similarity calculation is carried out in the 
target language domain. 
??
?
??
???
+=
)('
2
)('
2
)('
)('
0
)'()'(
)'|'()'()'(
)1(
1)|(
fcontextfecontexte
fcontextf
econtexte
ffreqefreq
eftffreqefreq
C
efC
 
where C0 > 1. 
(4)
  
4.5 Temporal Similarity (T) 
In temporal similarity, we make use of date in-
formation which is available in some corpus 
(e.g. news). We assume aligned terms are syn-
chronous in time, this is especially true for com-
parable news corpora (Tao and Zai, 2005). We 
642
use Discrete Fourier Transform (DFT) to trans-
form the distribution function of a term in dis-
crete time domain to a representative function in 
discrete frequency domain, which is usually 
known as ?spectrum?. We then calculate the 
power spectrum, which is defined as magnitude 
square of a spectrum. Power spectrum is sensi-
tive to the relative spacing in time (or frequency 
component), yet invariant to the shifting in time, 
thus it is most suitably to be used for pattern 
matching of time distribution. The temporal 
similarity is calculated based on cosine similar-
ity between the power spectrums of the two 
terms before scaling to the range of [1, T0], 
where T0 is a configurable contribution weight-
age of temporal similarity. 
 ( ) 1)(),(cos)1()|( 0 +?= kPkPineTefT fe             (5) 
where T0 > 1 and 
( )
2
1
0
2
2
22
)(
|)}({|)(
)()(
)()(
)(),(cos
?
??
?
?
=
??=
=
=
N
n
kn
N
i
x
xx
kk
k
enonFunctionDistributi
nonFunctionDistributiDFTkP
kvku
kvku
kvkuine
?
 
4.6 Related Term Similarity (R) 
Related terms are terms that correlate statisti-
cally in the same documents and they can be 
found by using mutual information or t-test in 
the monolingual corpus. Basically, related term 
similarity is a measure of related term likeli-
hood. Aligned terms are assumed to have simi-
lar related terms, hence related term similarity 
contributes to semantic similarity. The related 
term similarity is calculated based on weighted 
contribution from the related terms of the source 
term before scaling to the range of [1, R0], 
where R0 is a configurable contribution weight-
age of related terms similarity. 
 
1)|()1()|( 0 +?= efyRsimilaritRefR          (6) 
 where R0 > 1 and 
? ?
?
? ?
?=
Ff eRe
eRe
efvote
efvote
efyRsimilarit
)('
)('
)'|(
)'|(
)|(  
? ?
?
? ???
???
?
?
?
?
=
Ff
eeR
eRe
eeR
eRe
efvote
eeMIfew
efvote
eeMIfew
efvote
}]'{)'([
)("
}]'{)'([
)("
)"|(
)",(),"(
)"|(
)",(),"(
)'|(
 
???
?
???
?=
??
? ???=
)"()(
)",(
log)",(
,1
)()"(,5.1
),"(
epep
eep
eeMI
otherwise
fReTrif
few
 
 vote(f|e?) is initialized to 1 before it is com-
puted iteratively until it converges. R(e) is the 
set of related term of e and Tr(e) is the set of 
translated term of e. 
5 MI & TScore Rescoring 
We design the MI & TScore rescoring process 
to enhance the alignment score t(f|e) of e-f term 
pairs that have significant co-occurrence fre-
quencies in aligned document pairs, based on 
pointwise mutual information and TScore (or 
commonly known as t-test) of the terms. By 
using both measures concurrently, the associa-
tion relationship of a term pair can be assumed 
with higher confidence. On top of that, the asso-
ciation of a term pair can also be suggested by a 
much higher TScore value alone. In this rescor-
ing process, we scale up the alignment score 
t(f|e) of any term pair which is strongly associ-
ated by a constant factor. The following shows 
the mathematical expressions of what has been 
described, with M0 as the configurable scaling 
factor. 
 
Rescoring condition: 
andfeTScoreif 5.2),({[ ?          (7) 
)]','(6.0),(
)()'(
)()'(:)','(
feMIMaxfeMI
ffreqffreqor
efreqefreqfe = =
??  
thenfeTScoreor }5),( ?  
0)|()|( MefTefT ?=  
where M0 > 1 and 
N
fep
fpepfep
feTScore
2
),(
)()(),(
),(
?=  
),( feirNumberOfPaN =  
643
6 Experiment and Evaluation 
We conduct the experiment on articles from 
three newspapers of different languages pub-
lished by Singapore Press Holding (SPH), 
namely Straits Times1 (English), ZaoBao2 (Chi-
nese) and Berita Harian3 (Malay), in June 2006. 
There are 3187 English articles, 4316 Chinese 
articles and 1115 Malay articles. English is cho-
sen to be the source language and the remaining 
two languages as target languages. To analyze 
the effect of the quality of comparable docu-
ment in our term alignment model, we prepare 
two different input sets of document alignment, 
namely golden document alignment and auto-
mated document alignment for each source-
target language pair. The former is retrieved by 
linguistic experts who are requested to read the 
contents of the articles in the source and the tar-
get languages, and then match the articles with 
similar contents (e.g. news coverage on same 
story), while the latter is generated using unsu-
pervised method proposed by Vu et. al. (2009), 
mentioned in Section 2. 
In both cases of document alignments, only 
monolingual noun terms extracted automatically 
by program (Vu et. al., 2008) will be used as 
basic semantic unit. There are 23,107 unique 
English noun terms, 31,944 unique Chinese 
noun terms and 8,938 unique Malay noun terms 
extracted in overall. In average, there are 17.3 
noun term tokens extracted for each English 
document, 16.9 for Chinese document and 13.0 
for Malay document. Also note that the term 
alignment reference list is constructed based on 
these extracted monolingual terms under the 
constraints of document alignment. In other 
words, the linguistic experts are requested to 
match the extracted terms across aligned docu-
ment pairs (for both golden document alignment 
and automated document alignment sets respec-
tively). The numbers of comparable document 
pairs and the corresponding unique term align-
ment reference pairs are shown in Table 2. 
                                                 
1 http://www.straitstimes.com/ an English news 
agency in Singapore. Source ? Singapore Press 
Holdings Ltd. 
2 http://www.zaobao.com/ a Chinese news agency in 
Singapore. Source ? Singapore Press Holdings Ltd. 
3 http://cyberita.asia1.com.sg/ a Malay news agency 
in Singapore. Source ? Singapore Press Holdings 
Ltd. 
In the experiment, we will conduct the named 
entity recognition (NER) by using the devel-
oped system from the Stanford NLP Group, for 
English, and an in-house engine, for Chinese. 
Currently, there is no available NER engine for 
Malay.  
 
Dictionary E-C C-E E-M M-E 
Entry 23,979 71,287 28,496 18,935 
Table 1. Statistics of dictionaries, where E = English, 
C = Chinese, M = Malay. 
 
GoldenDocAlign AutomatedDocAlign 
Corpus Doc 
Align  
Term 
Align Ref 
Doc 
Align 
Term 
Align Ref 
ST-ZB 90 313 899 777 
ST-BH 42 113 475 358 
Table 2. Statistics of comparable document align-
ment pairs and term alignment reference pairs. 
 
For baseline, we make use of IBM model 1, 
modified in the same way which has been de-
scribed in the section 3, except that we treat all 
comparable documents as parallel sentences, i.e. 
document alignment score is 1. Precision and 
recall are used to evaluate the performance of 
the system. To achieve high precision, high 
thresholds are used in the system and they are 
kept constant throughout the experiments for 
consistency. To evaluate the capability of dis-
covering new bilingual terminology, we design 
a novelty metric, which is the ratio of the num-
ber of correct out-of-dictionary term alignment 
over the total number of correct term alignment. 
 
C
N
Novelty
G
C
Recall
T
C
Precision ===         (8) 
where, 
C = total number of correct term alignment result. 
T = total number of term alignment result. 
G = total number of term alignment reference. 
N     = total number of correct term alignment result 
that are out-of-dictionary. 
 
Table 3 shows the evaluation result of term 
alignment using EM algorithm with incremental 
feature setting. The particular order of setting is 
due to the implementation sequences and it is 
not expected to affect the result of analysis. 
We observe that the precision, recall and 
novelty of the system are comparatively higher 
when the golden document alignment is used 
instead of the automated document alignment.  
644
Table 3. Performance of term alignment using EM algorithm with incremental feature setting, where D = 
document alignment, L = lexical similarity, R = related term similarity, M = MI & TScore rescoring, N = 
named entity similarity, C = context similarity, T = temporal similarity.
 
This is expected since the golden document 
alignment provides document pairs with 
stronger semantic bonding. This also suggests 
that improving on the document alignment 
would further improve the term alignment re-
sult. 
It is noteworthy observation that the imple-
mented features improve the system precision 
and recall under various scenarios, although the 
degree of improvement varies from case to case. 
This shows the effectiveness of these features in 
the model.  
On the other hand, the novelty of the system 
is around 40%+ and 50%+ for ST-ZB and ST-
BH respectively (except for the automated 
document alignment in ST-BH scenarios). This 
suggests that the system can discover quite a 
large percentage of the correct bilingual termi-
nologies that do not exist in the lexicon initially. 
Compared with the baseline IBM model 1, 
there is an increase of 14.5% in precision, 
3.51% in recall and 2.9% in novelty for ST-ZB, 
using the golden document alignment. For ST-
BH, there is an even larger increase: 50% in 
precision, 7.96% in recall and 60% in novelty. 
7 Conclusion 
We have proposed an unsupervised EM-based 
hybrid model to extract bilingual terminology 
from comparable corpora through document 
alignment constraint. Our strategy is to make 
use of various information (corpus-driven and 
non-corpus-driven) to make initial guess on the 
semantic bonding of the term alignment candi-
dates before subjecting them to document 
alignment constraint through EM algorithm. 
The hybrid model allows inclusion of additional 
features without reconfigurations on existing 
features, this make it practically attractive. 
Moreover, the proposed system can be easily 
deployed in any language with minimal con-
figurations. 
We have successfully conducted the experi-
ments in English-Chinese and English-Malay 
comparable news corpora. The features em-
ployed in the model have shown incremental 
improvement in performance over the baseline 
method. In particular, the system shows im-
provement in the capability to discover new bi-
lingual terminology from comparable corpora 
even with limited usage of dictionaries. 
From the experiments, we have found that the 
quality of comparable bilingual documents is a 
GoldenDocAlign AutomatedDocAlign corpora Setting 
Precision Recall Novelty Precision Recall Novelty 
IBM 1 75.0% 1.92%  50.0% 22.2% 0.26% 50.0% 
(D) 75.0% 1.92% 50.0% 22.2% 0.26% 50.0% 
(D,L) 81.8% 2.88% 55.6% 33.3% 0.52% 25.0% 
(D,L,R) 81.8% 2.88% 55.6% 33.3% 0.52% 25.0% 
(D,L,R,M) 78.6% 3.51% 63.6% 35.7% 0.64% 40.0% 
(D,L,R,M,N) 88.2% 4.79% 53.3% 35.7% 0.64% 40.0% 
(D,L,R,M,N,C) 89.5% 5.43% 52.9% 33.3% 0.64% 40.0% 
ST-ZB 
(D,L,R,M,N,C,T) 89.5% (17/19) 
5.43% 
(17/313) 
52.9% 
(9/17) 
37.5% 
(6/16) 
0.77% 
(6/777) 
16.7%   
(1/6) 
IBM 1 33.3% 0.89% 0.00% 33.3% 0.78% 0.00% 
(D) 33.3% 0.89% 0.00% 33.3% 0.78% 0.00% 
(D,L) 75.0% 5.31% 50.0% 50.0% 1.94% 0.00% 
(D,L,R) 75.0% 5.31% 50.0% 50.0% 1.94% 0.00% 
(D,L,R,M) 75.0% 5.31% 50.0% 54.5% 2.33% 0.00% 
(D,L,R,M,N) 75.0% 5.31% 50.0% 54.5% 2.33% 0.00% 
(D,L,R,M,N,C) 83.3% 8.85% 60.0% 50.0% 1.94% 0.00% 
ST-BH 
(D,L,R,M,N,C,T) 83.3% (10/12) 
8.85% 
(10/113) 
60.0% 
(6/10) 
50.0% 
(5/10) 
1.94% 
(5/258) 
0.00% 
(0/5) 
645
major limiting factor to achieve good perform-
ance. In future, we want to explore ways to im-
prove on this. 
References 
R. Agrawal, C. Faloutsos, and A. Swami. 1993. Effi-
cient similarity search in sequence databases. In 
Proceedings of the 4th International Conference 
on Foundations of Data Organization and Algo-
rithms. Chicago, United States. 
P. F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2): 263-312. 
Yunbo Cao and Hang Li. 2002. Base Noun Phrase 
Translation Using Wed Data and the EM Algo-
rithm, Computational Linguistics, pp.1-7. 
Pascale Fung, 1998. A statistical view on bilingual 
lexicon extraction: From parallel corpora to non-
parallel corpora. Proceedings of AMTA, pp.1-17.  
Pascale Fung and Percy Cheung. 2004. Mining Very-
Non-Parallel Corpora: Parallel Sentence and 
Lexicon Extraction via Bootstrapping and EM, 
Proceedings of EMNLP, pp.57-63. 
Alexandre Klementiev and Dan Roth, 2006. Weakly 
Supervised Named Entity Transliteration and Dis-
covery from Multilingual Comparable Corpora. 
Computational Linguistics, pp. 817-824. 
K. Knight and J. Graehl. 1998. Machine translitera-
tion, Computational Linguistics, 24(4): 599-612.  
E. Morin, B. Daille, K. Takeuchi, K. Kageura. 2007. 
Bilingual Terminology Mining ? Using Brain, not 
brawn comparable corpora, Proceedings of ACL. 
Dragos Stefan Munteanu and Daniel Marcu. 2005. 
Improving Machine Translation Performance by 
Exploiting Non-Parallel Corpora. Computational 
Linguistics, 31(4): 477-504. 
Fatiha Sadat, Masatoshi Yoshikawa, Shunsuke Ue-
mura, 2003. Learning Bilingual Translations from 
Comparable Corpora to Cross-Language Infor-
mation Retrieval: Hybrid Statistics-based and 
Linguistics-based Approach. Proceedings of ACL, 
vol.11, pp.57-64. 
Tao Tao and Chengxiang Zhai. 2005. Mining com-
parable bilingual text corpora for cross-language 
information integration, Proceedings of ACM. 
Raghavendra Udupa, K. Saravanan, A. Kumaran, 
Jagadeesh Jagarlamudi. 2008. Mining named en-
tity transliteration equivalents from comparable 
corpora, Proceedings of ACM. 
Thuy Vu, Aiti Aw, Min Zhang, 2008. Term extrac-
tion through unithood and termhood unification. 
Proceedings of IJCNLP-08, Hyderabad, India. 
Thuy Vu, Aiti Aw, Min Zhang, 2009. Feature-based 
Method for Document Alignment in Comparable 
News Corpora. Proceedings of EACL-09, Athens, 
Greece. 
 
646
Linguistically Annotated Reordering:
Evaluation and Analysis
Deyi Xiong?
Institute for Infocomm Research
Min Zhang??
Institute for Infocomm Research
Aiti Aw?
Institute for Infocomm Research
Haizhou Li?
Institute for Infocomm Research
Linguistic knowledge plays an important role in phrase movement in statistical machine trans-
lation. To efficiently incorporate linguistic knowledge into phrase reordering, we propose a new
approach: Linguistically Annotated Reordering (LAR). In LAR, we build hard hierarchical skele-
tons and inject soft linguistic knowledge from source parse trees to nodes of hard skeletons during
translation. The experimental results on large-scale training data show that LAR is comparable
to boundary word-based reordering (BWR) (Xiong, Liu, and Lin 2006), which is a very compet-
itive lexicalized reordering approach. When combined with BWR, LAR provides complementary
information for phrase reordering, which collectively improves the BLEU score significantly.
To further understand the contribution of linguistic knowledge in LAR to phrase reordering,
we introduce a syntax-based analysis method to automatically detect constituent movement in
both reference and system translations, and summarize syntactic reordering patterns that are
captured by reordering models. With the proposed analysis method, we conduct a comparative
analysis that not only provides the insight into how linguistic knowledge affects phrase move-
ment but also reveals new challenges in phrase reordering.
1. Introduction
The phrase-based approach is a widely accepted formalism in statistical machine trans-
lation (SMT). It segments the source sentence into a sequence of phrases (not necessarily
syntactic phrases), then translates and reorders these phrases in the target. The reason
for the popularity of phrasal SMT is its capability of non-compositional translations and
? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: dyxiong@i2r.a-star.edu.sg.
?? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: mzhang@i2r.a-star.edu.sg.
? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: aaiti@i2r.a-star.edu.sg.
? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: hli@i2r.a-star.edu.sg.
Submission received: 24 October 2008; revised submission received: 12 March 2010; accepted for publication:
21 April 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
local word reorderings within phrases. Unfortunately, reordering at the phrase level is
still problematic for phrasal SMT. The default distortion-based reordering model simply
penalizes phrase movement according to the jump distance, without considering any
linguistic contexts (morphological, lexical, or syntactic) around phrases.
In order to utilize lexical information for phrase reordering, Tillman (2004) and
Koehn et al (2005) propose lexicalized reordering models which directly condition
phrase movement on phrases themselves. One problem with such lexicalized reordering
models is that they are restricted only to reorderings of phrases seen in training data.
To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words
of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering
evidence. Although these lexicalized reordering models significantly outperform the
distortion-based reordering model as reported, only using lexical information (e.g.,
boundary words) is not adequate to move phrases to appropriate positions.
Consider the following Chinese example with its English translation:
[VP [PP(while)(develop)(related)(legislation)] [VP [VV
(consider)] [NP [DNP [NP(this) (referendum)] [DEG(of)]] [NP
(results)]]]]1
consider the results of this referendum while developing related legislation
In this example, boundary words and are able to decide that the translation of the
PP phrase ... should be postponed until some phrase that succeeds it is translated.
But they cannot provide further information about exactly which succeeding phrase
should be translated first. If high-level linguistic knowledge, such as the syntactic
context VP?PP VP, is given, the position of the PP phrase can be easily determined
since the pre-verbal modifier PP in Chinese is frequently translated into a post-verbal
counterpart in English.
In this article, we focus on linguistically motivated phrase reordering, which in-
tegrates high-level linguistic knowledge in phrase reordering. We adopt a two-step
strategy. In the first step, we establish a hierarchical skeleton in phrasal SMT by in-
corporating Bracketing Transduction Grammar (BTG) (Wu 1997) into phrasal SMT. In
the second step, we inject soft linguistic information into nodes of the skeleton.
There are two significant advantages to using BTG in phrasal SMT. First, BTG is able
to generate hierarchical structures.2 This not only enhances phrasal SMT?s capability
for hierarchical and long-distance reordering but also establishes a platform for phrasal
SMT to incorporate knowledge from linguistic structure. Second, phrase reordering is
restricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight
or inverted) of nodes in any binary branching structure, it is broadly verified that the
ITG constraint has good coverage of word reorderings on various language pairs (Wu,
Carpuat, and Shen 2006). This makes phrase reordering in phrasal SMT a more tractable
task.
After enhancing phrasal SMT with a hard hierarchical skeleton, we further inject
soft linguistic information into the nodes of the skeleton. We annotate each BTG node
1 In this article, we use Penn Chinese Treebank phrase labels (Xue et al 2000).
2 Chiang (2005) also generates hierarchical structures in phrasal SMT. One difference is that Chiang?s
hierarchical grammar is lexicon-sensitive because the model requires at least one pair of aligned words in
each rule except for the ?glue rule.? The other difference is that his grammar allows multiple
nonterminals. These two differences make Chiang?s grammar more expressive than the BTG but at the
cost of learning a larger model.
536
Xiong et al Linguistically Annotated Reordering
with syntactic and lexical elements by projecting the source parse tree onto the BTG
binary tree. The challenge, of course, is that BTG hierarchical structures are not always
aligned with the linguistic structures in the source language parse tree. To address this
issue, we propose an annotation algorithm. The algorithm is able to label any BTG
nodes during decoding with very little overhead, regardless of whether the BTG nodes
are aligned with syntactic constituent nodes in the source parse tree. The annotated
linguistic elements are then used to guide phrase reordering under the ITG constraint.
We call this two-step phrase reordering strategy linguistically annotated reorder-
ing (LAR) (Xiong et al 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reorder-
ing strategy based on BTG. However, they use boundary words as reordering features
at the second step. To distinguish this from our work, we call their approach boundary
word?based reordering (BWR). LAR and BWR can be considered as two reordering
variants for BTG-based phrasal SMT, which have similar training procedures. Further-
more, they can be combined.
We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al 2002).
The BLEU scores show that LAR is comparable to BWR and significantly improves
phrase reordering when combined with BWR.
We want to further study what happens when we combine BWR with LAR. In
particular, we want to investigate to what extent the integrated linguistic knowledge
(from LAR) changes phrase movement in an actual SMT system, and in what direction
the change takes place. The investigations will enable us to have a better understanding
of the relationship between phrase movement and linguistic context, and therefore to
explore linguistic knowledge more effectively in phrasal SMT.
Because syntactic constituents are often moved together across languages during
translation (Fox 2002), we particularly study how linguistic knowledge affects syntactic
constituent movement. To that end, we introduce a syntax-based analysis method. We
parse source sentences, and align the parse trees with reference translations as well as
system translations. We then summarize syntactic reordering patterns using context-
free grammar (CFG) rules from the obtained tree-to-string alignments. The extracted
reordering patterns clearly show the trace of syntactic constituent movement in both
reference translations and system translations.
With the proposed analysis method, we analyze the combination of BWR and LAR
vs. BWR alone. There are essentially three issues that are addressed in this syntax-based
comparative analysis.
1. The first issue concerns syntactic constituent movement in human/
machine translations. Fox (2002) investigates syntactic constituent
movement in human translations. We study syntactic constituent
movement in both human translations and machine translations that are
generated by an actual SMT system and compare them.
2. The second issue concerns the change of phrase movement after rich
linguistic knowledge is integrated into phrase reordering. To gain a better
insight into this issue, we study phrase movement patterns for 13 specific
syntactic constituents.
3. The last issue concerns which constituents remain difficult to reorder even
though rich linguistic knowledge is employed.
The rest of the article is structured as follows. Section 2 introduces background
information about BTG-based phrasal SMT and phrase reordering under the ITG
537
Computational Linguistics Volume 36, Number 3
constraint. Section 3 describes algorithms which extract training instances for reorder-
ing models of BWR and LAR. Section 4 introduces the BWR model as our baseline
reordering model. Section 5 describes LAR and the combination of LAR with BWR.
Section 6 elaborates the syntax-based analysis method. Section 7 reports our evaluation
results on large-scale data. Section 8 demonstrates our analysis results and addresses the
various issues discussed above. Section 9 discusses related work. And finally, Section 10
summarizes our main conclusions.
2. Background
2.1 BTG-Based Phrasal SMT
We establish a unified framework for BTG-based phrasal SMT in this section. There
are two kinds of rules in BTG, lexical rules (denoted as rl) and merging rules (denoted
as rm):3
rl : Ap ? x/y
rm : Ap ? [Al, Ar]|?Al, Ar? (1)
A lexical rule translates a source phrase x into a target phrase y and generates a leaf
node A in the BTG tree. Merging rules combine left and right neighboring phrases Al
and Ar into a larger phrase Ap in an order o ? {straight, inverted}. In this article, we use
?[]? to denote a straight order and ???? an inverted order.
We define a BTG derivation D as a sequence of independent applications of lexical
and merging rules (D = ?rl1..nl , r
m
1..nm?). Given a source sentence, the decoding task of
BTG-based SMT is to find a best derivation, which yields the best translation.4
We assign a probability to each rule using a log-linear model with different features
and corresponding weights ?, then multiply them to obtain P(D). To keep in line with
the common understanding of standard phrasal SMT (Koehn, Och, and Marcu 2003),
here we re-organize these features into a translation model (PT), a reordering model
(PR), and a target language model (PL) as follows:
P(D) = PT(r
l
1..nl
) ? PR(rm1..nm )
?R ? PL(e)?L ? exp(|e|)?w (2)
where exp(|e|) is the word penalty.
The translation model is defined as
PT(r
l
1..nl
) =
nl
?
i=1
P(rli)
P(rl) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3 ? plex(y|x)?4 ? exp(1)?5 (3)
where p(?) represents the phrase translation probabilities in both directions, plex(?) de-
notes the lexical translation probabilities in both directions, and exp(1) is the phrase
penalty.
3 The subscripts l, r, p of A do not mean that we categorize A into three different nonterminals. We use them
to represent the left node, right node, and parent node.
4 In this article, we use c to denote a source sentence and e a target sentence.
538
Xiong et al Linguistically Annotated Reordering
Similarly, the reordering model is defined on the merging rules as follows:
PR(r
m
1..nm ) =
nm
?
i=1
P(rmi ) (4)
One of the most important and challenging tasks in building a BTG-based phrasal SMT
system is to define P(rm).
2.2 Reordering Under the ITG Constraint
Under the ITG constraint, three nodes {Al, Ar, Ap} are involved when we consider the
order o between the two children {Al, Ar} in any binary subtrees. Therefore it is natural
to define the ITG reordering P(rm) as a function as follows:
P(rm) = f (Al, Ar, Ap, o) (5)
where o ? {straight, inverted}.
Based on this function, various reordering models are built according to different
assumptions. For example, the flat reordering model in the original BTG (Wu 1996)
assigns prior probabilities for the straight and inverted order assuming the order is
highly related to the properties of language pairs. It is formulated as
P(rm) =
{
ps, o = straight
1 ? ps, o = inverted
(6)
Supposing French and English are the source and target language, respectively, the
value of ps can be set as high as 0.8 to prefer monotone orientations because the two
languages have similar word orders in most cases.
The main problem of the flat reordering model is also the problem of the standard
distortion model (Koehn, Och, and Marcu 2003): Neither model considers linguistic
contexts. To be context-dependent, the ITG reordering might directly model the condi-
tional probability P(o|Al, Ar). This probability could be calculated using the maximum
likelihood estimate (MLE) by taking counts from training data, in the manner of the
lexicalized reordering model (Tillman 2004; Koehn et al 2005):
P(o|Al, Ar) =
Count(o, Al, Ar)
Count(Al, Ar)
(7)
Unfortunately this lexicalized reordering method usually leads to a serious data sparse-
ness problem under the ITG constraint because Al and Ar become larger and larger due
to the merging rules, and are finally unseen in the training data.
To avoid the data sparseness problem yet be contextually informative, attributes
of Al and Ar, instead of nodes themselves, are used as reordering evidence in a new
perspective of the ITG reordering (Xiong, Liu, and Lin 2006). The new perspective treats
the ITG reordering as a binary-classification problem where the possible order straight
or inverted between two children nodes is the target class which the reordering model
predicts given Al, Ar, and Ap.
539
Computational Linguistics Volume 36, Number 3
3. Reordering Example Extraction
Because we consider the ITG reordering as a classification problem, we need to obtain
training instances to build a classifier. Here we refer to a training instance as a reorder-
ing example, which is formally defined as a triple of (o, bl, br) where bl and br are two
neighboring blocks and o ? {straight, inverted} is the order between them.
The block is a pair of aligned source phrase and target phrase
b = (ci2i1 , e
j2
j1
) (8)
where b must be consistent with the word alignment M
?(i, j) ? M, i1 ? i ? i2 ? j1 ? j ? j2 (9)
By this, we require that no words inside the source phrase ci2i1 are aligned to words
outside the target phrase ej2j1 and that no words outside the source phrase are aligned to
words inside the target phrase. This definition is similar to that of the bilingual phrase
except that there is no length limitation over blocks. Figure 1 shows a word alignment
matrix between a Chinese sentence and English sentence. In the matrix, each block can
be represented as a rectangle, for example, blocks (c44, e
4
4), (c
5
4, e
5
4), (c
7
4, e
9
4) in red rectangles,
and (c32, e
3
3), (c
3
1, e
3
1) in blue rectangles.
In this section, we discuss two algorithms for extracting reordering examples from
word-aligned bilingual data. The first algorithm AExtractor (described in Section 3.1)
extracts reordering examples directly from word alignments by extending the bilin-
gual phrase extraction algorithm. The second algorithm TExtractor (described in Sec-
tion 3.2) extracts reordering examples from BTG-style trees which are built from word
alignments.
Figure 1
A word alignment matrix between a Chinese sentence and English sentence. Bold dots represent
junctions which connect two neighboring blocks. Red and blue rectangles are blocks which are
connected by junction J2.
540
Xiong et al Linguistically Annotated Reordering
3.1 AExtractor: Extracting Reordering Examples from Word Alignments
Before we describe this algorithm, we introduce the concept of junction in the word
alignment matrix. We define a junction as a vertex shared by two neighboring blocks.
There are two types of junctions: a straight junction, which connects two neighboring
blocks in a straight order (e.g., black dots J1 ? J4 in Figure 1) and an inverted junction,
which connects two neighboring blocks in an inverted order (e.g., the red dot J5 in
Figure 1).
The algorithm for AExtractor is shown in Figure 2. This completes three sub-tasks
as follows.
1. Find blocks (lines 4 and 5). This is similar to the standard phrase extraction
algorithm (Och 2002) except that we find blocks with arbitrary length.
2. Detect junctions and store blocks in the arrays of detected junctions (lines 7
and 8). Junctions that are included the current block can be easily detected
by looking at the previous and next blocks. A junction can connect
multiple blocks on its left and right sides. For example, the second
junction J2 in Figure 1 connects two blocks on the left side and three blocks
on the right side. To store these blocks, we maintain two arrays (left and
right) for each junction.
3. Select block pairs from each detected junction as reordering examples
(lines 12?16). This is the most challenging task for AExtractor. Because a
junction may have n blocks on its left side and m blocks on its right side,
we will obtain nm reordering examples if we enumerate all block pairs.
This will quickly increase the number of reordering examples, especially
Figure 2
AExtractor.
541
Computational Linguistics Volume 36, Number 3
those with the straight order. To keep the number of reordering examples
tractable, we define various selection rules r to heuristically select special
block pairs as reordering examples.
We define four selection rules as follows.
1. strINV: We select the smallest blocks (in terms of the target length) for
straight junctions, and the largest blocks for inverted junctions. Take the
straight junction J2 in Figure 1 as an example, the extracted reordering
example is (straight, | five,| flights).
2. STRinv: We select the largest blocks (in terms of the target length) for
straight junctions, and the smallest blocks for inverted junctions. Still
taking the straight junction J2 as an example, this time the extracted
reordering example is (straight, | The last five,|
flights all fail due to accidents).
3. RANDOM: For any junction, we randomly select one block pair from its
arrays.
4. COMBO: For each junction, we first select two block pairs using selection
rule strINV and STRinv. If there are unselected blocks, we randomly select
one block pair from the remaining blocks.
3.2 TExtractor: Extracting Reordering Examples from BTG-Style Trees
A potential problem for AExtractor is caused by the use of heuristic selection rules:
keeping some block pairs as reordering examples while abandoning other block pairs.
The kept block pairs are not necessarily the best training instances for tuning an ITG
order predictor. To avoid this problem we can extract reordering examples from the
BTG trees of sentence pairs. Reordering examples extracted in this way are naturally
suitable for BTG order prediction.
There are various ways to build BTG trees over sentence pairs. One can use BTG to
produce bilingual parses of sentence pairs, similar to the approaches proposed by Wu
(1997) and Zhang and Gildea (2005) but using the more sophisticated reordering models
BWR or LAR. After parsing, reordering examples can be extracted from bilingual parse
trees and a better reordering model is therefore induced from the extracted reordering
examples. Using the better reordering model, the bilingual sentences are parsed again.
This procedure is run iteratively until no performance gain is obtained in terms of
translation or parsing accuracy. Formally, we can use expectation-maximization (EM)
training in this procedure. In the expectation step, we first estimate the likelihood of all
BTG trees of sentence pairs with the current BTG model. Then we extract reordering
examples and collect counts for them, weighted with the probability of the BTG tree
where they occur. In the maximization step, we can train a more accurate reordering
model with updated reordering examples. Unfortunately, this method is at high com-
putational cost.
Instead, here we adopt a less expensive alternative method to produce BTG trees
over sentence pairs. Supposing we have word alignments produced by GIZA++, we
then use the shift-reduce algorithm (SRA) introduced by Zhang, Gildea, and Chiang
(2008) to decompose word alignments into hierarchical trees. The SRA can guarantee
that each node is a bilingual phrase in the decomposition tree. If the fan-out of a node
is larger than two, we binarize it from left to right: for two neighboring child nodes, if
542
Xiong et al Linguistically Annotated Reordering
they are also neighboring on both the source and target sides, we combine them and
create a new node to dominate them. In this way, we can transform the decomposition
tree into a BTG-style tree. Note that not all multi-branching nodes can be binarized. We
extract reordering examples only from binary nodes.
Figure 3 shows the BTG-style tree which is built from the word alignment in Figure 1
according to the method mentioned here. From this tree, we can easily extract four re-
ordering examples in a straight order and one reordering example in an inverted order.
4. Boundary Word-Based Reordering
Following the binary-classification perspective of the ITG reordering, Xiong, Liu, and
Lin (2006) propose a reordering model which exploits the maximum entropy (MaxEnt)
classifier for BTG order prediction
PRb (r
m) = P?(o|Al, Ar, Ap) =
exp(
?
i ?ihi(o, Al, Ar, Ap))
?
o? exp(
?
i ?ihi(o
?, Al, Ar, Ap))
(10)
where the functions hi ? {0, 1} are reordering features and the ?i are the weights of these
features.
Xiong, Liu, and Lin (2006) define reordering features using the boundary words of
the source/target sides of both children {Al, Ar}. Supposing that we have a reordering
example (inverted, 7 15 | on July 15, | held its presidential and
parliament elections), leftmost/rightmost source words {, 15,,} and target
words {on, 15, held, elections} will be extracted as boundary words. Each boundary word
will form a reordering feature as follows
hi(o, Al, Ar, Ap) =
{
1, fn = bval, o = inverted
0, otherwise
where fn denotes the feature name, and bval is the corresponding boundary word.
There are two reasons why boundary words are used as important clues for
reordering:
1. Phrases frequently cohere across languages (Fox 2002). In cohesive phrase
movement, boundary words directly interact with the external contexts of
Figure 3
The BTG-style tree built from the word alignment in Figure 1. We use ([i, j], [p, q]) to denote a tree
node, where i, j and p, q are the beginning and ending indices in the source and target language,
respectively.
543
Computational Linguistics Volume 36, Number 3
phrases. This suggests that boundary words might contain information for
phrase reordering.
2. The quantitative analysis in Xiong, Liu, and Lin (2006, page 525) further
shows that boundary words indeed contain information for order
prediction.
To train a BWR model, we follow three steps. First, we extract reordering examples
from word-aligned bilingual data as described in the last section, then generate reorder-
ing features using boundary words from the reordering examples, and finally estimate
feature weights.
5. Linguistically Annotated Reordering
In order to employ more linguistic knowledge in the ITG reordering, we annotate each
BTG node involved in reordering using linguistic elements from the source-side parse
trees. The linguistic elements include: (1) the head word hw, (2) the part-of-speech (POS)
tag ht of the head word, and (3) its syntactic category sc. In this section, we describe the
annotation algorithm and the LAR model, as well as the combination of LAR and BWR.
5.1 Annotation Algorithm
There are two steps to annotating a BTG node using source-side parse tree information:
(1) determining the sequence on the source side which is exactly covered by the node,
then (2) annotating the sequence according to the source-side parse tree. If the sequence
is exactly covered by a single subtree in the source-side parse tree, it is called a syntactic
sequence, otherwise it is a non-syntactic sequence. One of the challenges in this an-
notation is that phrases (BTG nodes) do not always cover syntactic sequences; in other
words, they are not always aligned to constituent nodes in the source-side tree. To solve
this problem, we generate a pseudo head word and composite category which consists
of the syntactic categories of three relevant constituents for the non-syntactic sequences.
In this way, our annotation is capable of labelling both syntactic and non-syntactic
phrases and therefore providing linguistic information for any phrase reordering.
The annotation algorithm is shown in Figure 4. For a syntactic sequence, the an-
notation is trivial. Annotation elements directly come from the subtree that covers the
sequence exactly. For a non-syntactic sequence, the process is more complicated. Firstly,
we need to locate the smallest subtree c? covering the sequence (line 6). Secondly, we
try to identify the head word/tag of the sequence (lines 7?12) by using its head word
directly if it is within the sequence. Otherwise, the word within the sequence which is
nearest to hw will be assigned as the head word of the sequence. Finally, we determine
the composite category of the sequence (lines 13?15), which is formulated as L-C-R.
L/R refers to the syntactic category of the left/right boundary node of s, which is the
highest leftmost/rightmost sub-node of c? not overlapping the sequence. If there is no
such boundary node (the sequence s is exactly aligned to the left/right boundary of c?),
L/R will be set to NULL. C is the syntactic category of c?. L, R, and C together describe
the external syntactic context of s. The composite category we define for non-syntactic
phrases is similar to the CCG-style category in Zollmann, Venugopal, and Vogel (2008).
Figure 5 shows a syntactic parse tree for a Chinese sentence, with the head word
annotated for each internal node. Some sample annotations are given in Table 1.
544
Xiong et al Linguistically Annotated Reordering
Figure 4
The Annotation Algorithm.
Figure 5
A syntactic parse tree with the head word annotated for each internal node. The superscripts on
leaf nodes denote their surface positions from left to right.
5.2 Reordering Model
The linguistically annotated reordering model PRa is a MaxEnt-based classification
model, which can be formulated as
PRa (r
m) = p?(o|A
ap
p , A
al
l , A
ar
r ) =
exp(
?
i ?ihi(o, A
ap
p , A
al
l , A
ar
r ))
?
o? exp(
?
i ?ihi(o
?, A
ap
p , A
al
l , A
ar
r ))
(11)
where the feature functions hi ? {0, 1} are defined using annotated linguistic elements
of each BTG node. Here we use the superscripts al, ar, and ap to stress that the BTG nodes
are linguistically annotated.
545
Computational Linguistics Volume 36, Number 3
Table 1
Annotation samples according to the tree shown in Figure 5.
sequence hw ht sc
?1, 2?  NN NULL-NP-NN
?2, 3?  NN NP
?2, 4?  VV NP-IP-NP
?3, 4?  VV NP-IP-NP
hw/ht = the head word/tag, respectively; sc = syntactic category.
Each merging rule involves three nodes (A
ap
p , A
al
l , A
ar
r ) and each node has three
linguistic elements (hw, ht, sc). Therefore, the model has nine features in total. Taking
the left node Aall as an example, the model could use its head word w as a feature as
follows:
hi(o, A
ap
p , A
al
l , A
ar
r ) =
{
1, Aall .hw = w, o = straight
0, otherwise
Training an LAR model also takes three steps. Firstly, we extract annotated reorder-
ing examples from source-side parsed, word-aligned bilingual data using the reordering
example extraction algorithm and the annotation algorithm. We then generate features
using the linguistic elements of these examples. Finally we tune feature weights to build
the MaxEnt model.
5.3 Combining LAR and BWR
LAR and BWR can be combined at two different levels:
1. Feature level. Because both LAR and BWR are trained under the
maximum entropy principle, we can combine linguistically annotated
features from LAR and boundary word features from BWR together and
train a single MaxEnt model. We call this method All-in-One combination.
2. Model level. We can also train two reordering models separately and
integrate them into BTG-based SMT
P(D) = PT(rl1..nl ) ?PRb (r
m
1..nm )
?Rb ?
PRa (r
m
1..nm )
?Ra ?PL(e)?L ? exp(|e|)?w (12)
where PRb is the BWR reordering model and PRa is the LAR reordering
model. We call this combination BWR+LAR.
We will empirically compare these two combination methods in Section 7.4.
6. A New Syntax-Based Reordering Analysis Method
In order to understand the influence of linguistic knowledge on phrase reordering, we
propose a syntax-based method to analyze phrase reordering. In this analysis method,
546
Xiong et al Linguistically Annotated Reordering
we leverage the alignments between source-side parse trees and reference/system
translations to summarize syntactic reordering patterns and calculate syntax-based
measures of precision and recall for each syntactic constituent.
6.1 Overview
The alignment between a source parse tree and a target string is a collection of rela-
tionships between parse tree nodes and their corresponding target spans.5 A syntactic
reordering pattern (SRP) is defined as
?? ? ?1...?n ? [i1]...[in]?
The first part of an SRP is a CFG structure on the source side and the second part
[i1]...[in] indicates the order of target spans ?T1 ...?
T
n of nonterminals ?1...?n on the target
side.6
Let?s take the VP structure VP ? PP1VP2 as an example to explain how the pre-
cision and recall can be obtained. On the target side, the order of PPT1 and VP
T
2
might be [1][2] or [2][1]. Therefore we have two syntactic reordering patterns for this
structure:
?VP ? PP1VP2 ? [1][2]? and ?VP ? PP1VP2 ? [2][1]?
Suppose that the two reordering patterns occur a times in the alignments between
source parse trees and reference translations, b times in the alignments between source
parse trees and system translations, and c times in both alignments. Then the reordering
precision/recall for this structure is c/b and c/a, respectively. We can further calculate
the F1-score as 2 ? c/(a + b). These syntax-based metrics intuitively show how well the
reordering model can reorder this structure. By summarizing all reordering patterns of
all constituents, we can obtain an overall precision, recall, and F1-score for the tested
reordering model.
This new syntax-based analysis for reordering is motivated in part by recent work
which transforms the order of nodes in the source-side parse tree before translation
(Xia and McCord 2004; Collins, Koehn, and Kucerova 2005; Li et al 2007; Wang,
Collins, and Koehn 2007). Here we focus on the order transformation of syntactic con-
stituents performed by reordering models during translation. In addition to aligning
parse trees with reference translations, we also align parse trees with system transla-
tions so that we can learn the movement of syntactic constituents carried out by the
reordering models and investigate the performance of the reordering models by com-
paring both alignments.
For notational convenience, we denote syntactic reordering patterns that are ex-
tracted from the alignments between source parse trees and reference translations as
REF-SRP and those from the alignments between source parse trees and system trans-
lations as SYS-SRP. We refer to those present in both alignments under some conditions
5 We adopt the definition of span from Fox (2002): Given a node n that covers a word sequence sp...si...sq
and a word alignment matrix M, the target words aligned to n are {ti : ti ? M(si )}. We define the target
span of node n as nT = (min({ti}), max({ti})). Note that nT may contain words that are not in {ti}.
6 Please note that the order of structures may not be defined in some cases (see Section 6.3).
547
Computational Linguistics Volume 36, Number 3
that will be described in Section 6.4 as MATCH-SRP. To conduct a thorough analysis on
the reorderings, we carry out the following steps on the test corpus (source sentences +
reference translations):
1. Parse source sentences.
2. Generate word alignments between source sentences and reference
translations as well as word alignments between source sentences and
system translations.
3. According to the word alignments of Step 2, for each multi-branching
node ? ? ?1...?n in the source parse tree generated in Step 1, find the
target spans ?T1 ...?
T
n and their order [i1]...[in] in the reference and system
translations, respectively.
4. Generate REF-SRPs, SYS-SRPs, and MATCH-SRPs according to the target
orders generated in Step 3 for each multi-branching node.
5. Summarize all SRPs and calculate the precision and recall as described
above.
We further elaborate Steps 2?4 in the Sections 6.2?6.4.
6.2 Generating Word Alignments
To obtain word alignments between source sentences and multiple reference transla-
tions, we pair the source sentences with each of the reference translations and include
the created sentence pairs in our bilingual training corpus. Then we run GIZA++ on the
new corpus in both directions, and apply the ?grow-diag-final? refinement rule (Koehn
et al 2005) to produce the final word alignments.
To obtain word alignments between source sentences and system translations, we
store the word alignments within each phrase pair in our phrase table. When we output
the system translation for a source sentence, we trace back the original source phrase
for each target phrase in the system translation. This will generate a phrase alignment
between the source sentence and system translation. Given the phrase alignment and
word alignments within the phrase stored in the phrase table, we can easily obtain word
alignments between the whole source sentence and system translation.
6.3 Generating Target Spans and Orders
Given the source parse tree and the word alignment between a source sentence and
a reference/system translation, for each multi-branching node ? ? ?1...?n, we firstly
determine the target span ?Ti for each child node ?i following Fox (2002). If one child
node is aligned to NULL, we define a special target span for it. The order for this special
target span will remain the same as the child node occurring in ?1...?n.
Two target spans may overlap with each other because of inherent divergences
between two languages or noise in the word alignment. When this happens on two
neighboring nodes ?i and ?i+1, we combine these two nodes together and redefine a
target span ?Ti&i+1 for the combined node. This process will be repeated until no more
neighboring nodes can be combined. For example, the target span of nodes a and b in
548
Xiong et al Linguistically Annotated Reordering
Figure 6
An example source parse tree with the word alignment between the source sentence and the
target translation. Dotted lines show the word alignment.
Figure 6 overlap ((1, 3) vs. (2, 2)). Therefore these two nodes are to be combined into a
new node, whose target span is (1, 3).
After performing all necessary node combinations, if there are no more overlaps, we
call the multi-branching node reorderable, otherwise non-reorderable. To get a clearer
picture of the reorderable nodes, we divided them into two categories:
 fully reorderable if all target spans of child nodes don?t overlap;
 partially reorderable if some child nodes are combined due to
overlapping.
In Figure 6, both nodes a and c are fully reorderable nodes.7 Node d is a partially
reorderable node. Node g is a non-reorderable node because (1) the target spans of its
child nodes d and f overlap, and (2) child nodes d and f cannot be combined because
they are not neighbors.
Because we have multiple reference translations for each source sentence, we can
define multiple orders for {?Ti }n1. If one node is non-reorderable in all reference trans-
lations, we call it REF-non-reorderable, otherwise REF-reorderable. To specify the
reorderable attribute of a node in the system translation, we prefix ?SYS-? to {non-
reorderable, reorderable, fully-reorderable, partially-reorderable}.
6.4 Generating SRPs
After we obtain the orders of the child nodes for each multi-branching node, we gener-
ate REF-SRPs and SYS-SRPs from the fully/partially reorderable nodes. We obtain the
7 Their target translations are interrupted by the other node?s translation. We will discuss this situation in
Section 8.5.
549
Computational Linguistics Volume 36, Number 3
MATCH-SRP for each multi-branching node by comparing the obtained SYS-SRP with
the REF-SRPs for this node under the following conditions:
1. Because we have multiple reference translations, we may have different
REF-SRPs. We compare the SYS-SRP with the REF-SRP where the
reference translation for this node (the sequence within the target span of
the node defined by the REF-SRP) has the shortest Levenshtein distance
(Navarro 2001) to that of the system translation.
2. If there are combined nodes in SYS/REF-SRPs, they are treated as a unit
when comparing, without considering the order within each combined
node. If the order of the SYS-SRP and the selected REF-SRP matches, we
have one MATCH-SRP for the node.
Let?s give an example to explain these conditions. Suppose that we are processing
the structure VP ? PP1ADVP2VP3. We obtain four REF-SRPs from four different ref-
erence translations and one SYS-SRP from the system output. Here we only show the
orders:
Ref.a : [3][1][2]
Ref.b : [3][2][1]
Ref.c&d : [2][3][1]
SYS : [3][1&2]
References c and d have the same order. Therefore we have three different REF-SRPs
for this structure. In the SYS-SRP, PP1 and ADVP2 are combined and moved to the right
side of VP3. Supposing that the system translation for this structure has the shortest edit
distance to that of Reference b, we use the order of Reference b to compare the system
order. In the Reference b order, both PP1 and ADVP2 are also moved to the right side of
VP3. Therefore the two orders of Reference b and SYS match. We have one matched SRP
for this structure.
7. Evaluation
Our system is a BTG-based phrasal SMT system, developed following Section 2. We
integrate the boundary word?based reordering model and the linguistically annotated
reordering model into our system according to our reordering configuration. We car-
ried out various experiments to evaluate the reordering example extraction algorithms
of Section 3, the linguistically annotated reordering model vs. boundary word?based
reordering model, and the effects of linguistically annotated features on the Chinese-to-
English translation task of the NIST MT-05 using large scale training data.
7.1 Experimental Setup
We ran GIZA++ (Och and Ney 2000) on the parallel corpora (consisting of 101.93M
Chinese words and 112.78M English words) listed in Table 2 in both directions and
then applied the ?grow-diag-final? refinement rule (Koehn, Och, and Marcu 2003) to
550
Xiong et al Linguistically Annotated Reordering
Table 2
Corpora used.
Corpus LDC catalog Chinese words English words
United Nations LDC2004E12 68.63M 76.99M
Hong Kong News LDC2004T08 15.07M 15.89M
Sinorama Magazine LDC2005T10 10.26M 9.64M
FBIS LDC2003E14 7.09M 9.28M
Xinhua LDC2002E18 0.40M 0.43M
Chinese News Translation LDC2005T06 0.28M 0.31M
Chinese Treebank LDC2003E07 0.10M 0.13M
Multiple Translation Chinese LDC2004T07 0.10M 0.11M
Total ?? 101.93M 112.78M
obtain many-to-many word alignments. From the word-aligned corpora, we extracted
bilingual phrases.
We used all corpora listed in Table 2 except for the United Nations corpus to train
our reordering models, which consist of 33.3M Chinese words and 35.79M English
words. We ran the reordering example extractor AExtractor and TExtractor of Section 3
on the chosen word-aligned corpora. We then extracted boundary word features from
the reordering examples. To extract linguistically annotated features, we parsed the
Chinese side of the chosen parallel text using a Chinese parser (Xiong, Liu, and Lin
2005) which was trained on the Penn Chinese Treebank with an F1-score of 79.4%. We
ran the off-the-shelf MaxEnt toolkit8 to tune the reordering feature weights with the
iteration number set to 100 and Gaussian prior to 1 to avoid overfitting.
We built our 4-gram language model using the SRILM toolkit (Stolcke 2002), which
was trained on the Xinhua section of the English Gigaword corpus (181.1M words).
We selected 580 short sentences (not exceeding 50 characters per sentence) from the
NIST MT-02 evaluation test data as our development set (18 words/31 characters per
sentence). The NIST MT-05 test set includes 1,082 sentences with an average of 27.4
words/47.6 characters per sentence. The reference corpus for the NIST MT-05 test set
contains four translations per source sentence. Both the development and test sets were
also parsed using the parser mentioned above.
Our evaluation metric is the case-insensitive BLEU-4 (Papineni et al 2002) using the
shortest reference sentence length for the brevity penalty. The model feature weights are
tuned on the development set to maximize BLEU using MERT (Och 2003). Statistical
significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn
2004).
7.2 Bias in AExtractor
As described in Section 3, AExtractor selectively extracts reordering examples. This
selective extraction raises three questions:
1. Is it necessary to extract all reordering examples?
8 Available at: http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
551
Computational Linguistics Volume 36, Number 3
2. If it is not necessary, do the heuristic selection rules impose any bias on
the reordering model? For example, if we use the strINV selection rule,
meaning that we always extract the largest block pairs for inverted
reordering examples, does the reordering model prefer swappings on
larger blocks to those on smaller blocks?
3. Does the bias have a strong impact on the performance in terms of BLEU
score?
The answer to the first question is no. Firstly, it is practically undesirable to extract
all reordering examples because even a very small training set will produce millions of
reordering examples if we enumerate all block pair combinations. Secondly, extracting
all reordering examples introduces a great amount of noise into training and therefore
undermines the final reordering model. In Table 3, we show the number of reorder-
ing examples extracted using different extraction algorithms and selection rules. The
AExtractor with the COMBO selection rule extracts the largest number of reordering
examples. However, it does not obtain the highest BLEU score compared with other
selection rules which extract a smaller number of reordering examples. This empirically
suggests that there is no need to extract all reordering examples.
To answer the second question, we trace the best BTG trees produced on the test set
by our system. The BWR reordering model is trained on reordering examples which are
extracted using different selection rules. Then we calculate the average number of words
on the target side which are covered by binary nodes in a straight order. We refer to this
number as straight average length. Similarly, inverted average length is calculated on
all binary nodes in an inverted order. The third and fourth columns of Table 3 show the
two average variables. Comparing these average numbers, we clearly observe that two
selection rules indeed impose noticeable bias on the reordering model.
 The strINV selection rule, which always extracts the largest block pairs for
inverted reordering examples, has the largest inverted average length.
This indicates that the strINV rule biases the reordering model towards
larger swappings.
 On the contrary, the STRinv selection rule, which extracts the largest block
pairs for straight reordering examples and smallest pairs for inverted
reordering examples, has the largest straight average length and a
Table 3
Comparison of reordering example extraction algorithms and selection rules. We only use BWR
as the reordering model for this comparison.
Ext. Alg. Reordering Straight Inverted BLEU
(Sel. rule) Examples Avg. Len. Avg. Len.
AExtractor (strINV) 10.06M 15.8 14.5 32.37
AExtractor (STRinv) 10.06M 17.3 12.8 32.47
AExtractor (RANDOM) 10.06M 14.7 11.8 32.24
AExtractor (COMBO) 23.27M 13.8 13.5 32.10
TExtractor 14.30M 15.0 14.1 29.95
552
Xiong et al Linguistically Annotated Reordering
relatively much smaller inverted average length. This suggests that the
STRinv rule makes the reordering model prefer smaller swappings.
Note that the selection rules RANDOM and COMBO do not impose bias on the length
of extracted reordering examples compared with strINV and STRinv. The latter two se-
lection rules have special preferences on the length of reordering examples and transfer
these preferences to the reordering models as shown in Table 3.
Because the preference for reordering larger/smaller blocks is imposed by the
reordering example extraction algorithm with special selection rules, one might wonder
whether we can allow the decoder to decide its own reordering preference. We add two
new features to our translation model: reordering count penalty (rc) and reordering
length penalty (rl). We accumulate rc whenever two neighboring BTG nodes are re-
ordered. And at the same time we add the number of words which are covered by these
two neighboring nodes to rl. Their weights are tuned using MERT to maximize BLEU
score on the development set with other model feature weights. These two features
are similar to the widely used word/phrase penalty features. Tuning the weights of
the word/phrase penalty features, we can allow the decoder to favor shorter or longer
phrases. Similarly, with the two new features rc and rl, we can allow the decoder to
favor shorter or longer reorderings.
We conducted experiments using reordering examples which are extracted with the
RANDOM and COMBO selection rules because these two rules do not impose bias on
the length of reordering examples. Observing the optimized weights of rc and rl on
the development set, we find that the decoder rewards larger rc but smaller rl. This
means that the decoder prefers shorter reorderings to longer reorderings. However, the
BLEU scores on the test set are 31.89 and 32.0 for RANDOM and COMBO, respectively,
which are worse than the BLEU scores of RANDOM and COMBO without using rc
and rl in Table 3, and also worse than the performance of strINV and STRinv which
impose preferences on reordering examples. This seems to suggest that the preference
for shorter/longer reorderings imposed by the reordering example extraction algorithm
is better than that decided by the decoder itself.
Finally, for the last question, we observe from Table 3 that BLEU scores are not that
much different although we have quite the opposite bias imposed by different selection
rules. The changes in BLEU score, which happen when we shift from one selection rule
to the other, are limited to a maximum of 1.2%. Among the four selection rules, the
STRinv rule achieves the highest BLEU score. The reason might be that the bias towards
smaller swappings imposed by this rule helps the decoder to reduce incorrect long-
distance swappings (Xiong et al 2008b).
7.3 AExtractor vs. TExtractor
We further compared the two algorithms for reordering example extraction. In Table 3,
we find that TExtractor significantly underperforms in comparison to AExtractor. This
is because the transformation from decomposition trees to BTG trees is not complete.
Many crossing links due to errors and noise in word alignments generated by GIZA++
make it impossible to build BTG nodes over the corresponding words. It would be better
to use alignments induced by the ITG and EM procedure described in Section 3.2 but
this has a very high cost.
Given the comparison in Table 3, we use AExtractor with the STRinv selection rule
to extract reordering examples for both BWR and LAR in all experiments described
below.
553
Computational Linguistics Volume 36, Number 3
7.4 LAR vs. BWR
Table 4 shows the results of the different integration of BWR and LAR into our systems.
Only using LAR achieves a BLEU score of 32.17, which is comparable to that of BWR.
This suggests that LAR is promising given that:
 LAR uses many fewer features than BWR does. According to our statistics,
LAR contains only 166.1k linguistically annotated features whereas BWR
has 451.4k boundary word features.
 Syntactic divergences between the source and target languages as well as
parse errors prevent the effective use of syntactic knowledge for phrase
reordering (see the in-depth analysis in Section 8.2.2).
Although BWR marginally outperforms LAR (32.47 vs. 32.17), simple boundary
word features are not adequate to move phrases to appropriate positions because
they cannot recognize syntactic contexts which are very relevant to phrase reordering.
Therefore the best way to reorder a phrase is to combine BWR and LAR so that we
can use syntactic information on the one hand and not worry too much about syntactic
divergences on the other hand.
As described in Section 5.3, we can combine BWR and LAR at two levels: the feature
level and the model level. When we combine them at the model level, we achieve an
absolute improvement of 0.83 and 1.13 BLEU points over BWR and LAR, respectively,
which are both statistically significant (p < 0.01). This shows that LAR and BWR are
complementary to each other and in particular that using linguistic knowledge can
significantly improve a very competitive lexicalized reordering model (BWR).
The other combination method All-in-One (at the feature level) also obtains signif-
icant improvements over BWR and LAR but marginally underperforms compared to
BWR+LAR. In our later experiments we use the combination method BWR+LAR.
7.5 Varying Training Data Size
To investigate how LAR improves BWR when we vary our training data size, we carried
out experiments on three different training data sets: FBIS (7.09M Chinese words, 9.28M
English words); Large1, which includes all corpora listed in Table 2 except for the United
Nations corpus (33.3M Chinese words, 35.79M English words); and Large2, which
Table 4
BLEU scores for LAR, BWR, and their combinations.
Reordering Configuration BLEU
BWR 32.47
LAR 32.17
All-in-One 33.03**++
BWR+LAR 33.30**++
** = Significantly better than BWR (p < 0.01); ++ = significantly better than LAR (p < 0.01).
554
Xiong et al Linguistically Annotated Reordering
Table 5
BLEU scores on different training data sets. Large1 refers to the corpora listed in Table 2 except
for the United Nations corpus. Large2 includes all corpora listed in Table 2.
Training Data BWR BWR+LAR Improvement
FBIS 24.97 26.52 1.55
Large1 29.96 30.78 0.82
Large2 32.47 33.30 0.83
consists of Large1 and the United Nations corpus (101.93M Chinese words, 112.78
English words). The language model remains the same for these three data sets because
it is trained on a much larger data set (181.1M words).
Table 5 shows the results. We observe that BWR+LAR is able to achieve a larger
improvement of 1.55 BLEU points over BWR on smaller training data. When we enlarge
the training data set from FBIS to Large1, both BWR and BWR+LAR improve quite a
bit. The difference between them is narrower, 0.82 BLEU points, but still significant.
When we continue to use more training data (Large2), the improvement obtained by
integrating LAR becomes stable at the 0.8 level.
7.6 Effects of Linguistically Annotated Features
We conducted further experiments to evaluate the effects of individual linguistically an-
notated features. Using the reordering configuration of BWR+LAR, we augment LAR?s
feature pool incrementally: firstly using only syntactic categories9(sc) as features (170
features in total), then constructing composite categories (cc) for non-syntactic phrases
(sc + cc) (8.6K features), and finally introducing head words and their POS tags into
the feature pool (sc + cc + hw + ht) (166.1K features). This series of experiments demon-
strates the impact and degree of contribution made by each feature for reordering.
The experimental results are presented in Table 6, from which we have the following
observations:
1. Syntactic category alone improves the performance statistically
significantly. The baseline feature set sc with only 170 features improves
the BLEU score from 32.47 to 32.87.
2. Other linguistic information, provided by the categories of boundary
nodes (cc) and head word/tag pairs (hw + ht), also improves phrase
reordering. Producing composite categories for non-syntactic BTG nodes
and integrating head word/tag pairs into LAR as reordering features are
both effective, indicating that context information complements syntactic
category for capturing reordering patterns.
9 For a non-syntactic node, we only use the single category C, without constructing the composite category
L-C-R.
555
Computational Linguistics Volume 36, Number 3
Table 6
The effect of the linguistically annotated reordering model. (sc) is the baseline feature set,
(sc + cc) and (sc + cc + hw + ht) are extended feature sets for LAR.
Reordering Configuration BLEU
BWR 32.47
BWR + LAR (sc) 32.87*
BWR + LAR (sc + cc) 33.06**
BWR + LAR (sc + cc + hw + ht) 33.30**++
* = almost significantly better than BWR (p < 0.075); ** = significantly better than BWR (p < 0.01); ++ =
significantly better than BWR + LAR (sc) (p < 0.01).
8. Analysis
We first obtain system translations of the test corpus. We generate word alignments
between source sentences and system/reference translations as described in Section 6.2.
Then we follow the analysis steps of Section 6.1 to investigate syntactic constituent
movement in the reference translations and system translations which are generated
using two different reordering configurations: BWR+LAR vs. BWR. In LAR, we use the
best reordering feature set (sc + cc + hw + ht).
8.1 Syntactic Constituent Movement: Overview
If a syntactic constituent is fully reorderable or partially reorderable, it is considered to
be movable as a unit. To denote the proportion of syntactic constituents to be moved as
a unit, we introduce two variables REF-R-rate and SYS-R-rate, which are defined as
SYS-R-rate =
count(SYS-reorderable nodes)
count(multi-branching nodes)
(13)
REF-R-rate =
count(REF-reorderable nodes)
count(multi-branching nodes)
(14)
Table 7 shows the statistics of REF/SYS-reorderable nodes on the test corpus. From
this table, we have the following observations:
1. A large number of nodes are REF-reorderable, accounting for 79.82% of all
the multi-branching nodes. This number shows that, in reference
translations, a majority of syntactic constituent movement across
Chinese?English can be performed by directly permuting constituents in a
sub-tree.
2. The R-rates of BWR and BWR+LAR are 77.46% and 81.79%, respectively.
The R-rate of BWR+LAR is obviously higher than that of BWR, which
suggests that BWR+LAR tends towards moving more syntactic
constituents together than BWR does. We will discuss this further later.
556
Xiong et al Linguistically Annotated Reordering
Table 7
Statistics of multi-branching and REF/SYS-reorderable nodes per sentence.
BWR BWR+LAR
multi-branching node 18.68
REF-reorderable node 14.91
REF-R-rate 79.82%
SYS-fully-reorderable node 13.16 14.01
SYS-partially-reorderable node 1.31 1.26
SYS-R-rate 77.46% 81.79%
8.2 Syntactic Constituent Movement among Multiple Reference Translations
8.2.1 Differences in Movement Orientation. Because each source sentence is translated by
four different human experts, we would like to analyze the differences among reference
translations, especially on the orders of constituents being translated. Table 8 shows
the overall distribution over the number of different orders for each multi-branching
constituent among the reference translations.
In most cases (75.4%), four reference translations have completely the same order
for syntactic constituents. This makes it easier for our analysis to compare the system
order with the reference order. However, there are 22% cases where two different orders
are provided, which shows the flexibility of translation. According to our study, noun
phrases taking DNP or CP modifiers, as well as DNPs and CPs themselves, are more
likely to be translated in two different orders. Table 9 shows the percentages in which
two different orders for these constituents are observed in the reference corpus.
DNP and CP are always used as pre-modifiers of noun phrases in Chinese. They
often include the particle word  (of ) at the ending position. The difference is that
DNP constructs a phrasal modifier whereas CP constructs a relative-clause modifier.
There is no fixed reordering pattern for DNP and CP and therefore for NP which takes
DNP/CP as a pre-modifier. In the DNP ? NP DEG structure, the DEG () can be
Table 8
Distribution of number of different orders by which syntactic constituents are translated in
references.
Number of different orders 1 2 3 4
Percentage 75.40 22 2.33 0.33
Table 9
Two-order translation distribution of 4 NP-related constituents.
Constituent 2-order translation percentage
NP ? DNP NP 16.93
NP ? CP NP 9.43
CP ? IP DEC 24.79
DNP ? NP DEG 34.58
557
Computational Linguistics Volume 36, Number 3
translated into ?s or of, which are both appropriate in most cases, depending on the
translator?s preference. If the former is chosen, the order of DNP and therefore the
order for NP ? DNP NP will both be straight: [1][2]. Otherwise, the two orders will be
inverted: [2][1]. Similarly, there are also different translation patterns for CP ? IP DEC
and NP ? CP NP. CP can be translated into ?that + clause? or adjective-like phrases
in English. Figure 7 shows an example where the CP constituent is translated into an
adjective-like phrase. Although the ?that + clause? must be placed behind the noun
phrase which it modifies, the order for adjective-like phrases is flexible (see Figure 7).
For those constituents with different reference orders, we compare the order of
the system translation to that of the reference translation which has the shortest edit
distance to the system translation as described herein so that we can take into account
the potential influence of different translations on the order of syntactic constituents.
8.2.2 REF-Non-Reorderable Constituents. We also study REF-R-rates for the 13 most fre-
quent constituents listed in Table 10. We find that two constituents, VP1 ? PP VP2 and
NP1 ? CP NP2, have the lowest REF-R-rates, 58.20% and 61.77%, respectively. This
means that about 40% of them are REF-non-reorderable. In order to understand the
reasons why they are non-reorderable in reference translations, we further investigate
REF-non-reorderable cases for the constituent type VP1 ? PP VP2 and roughly classify
the reasons into three categories as follows.
1. Outside interruption. The reordering of PP and VP2 is interrupted by
other constituents outside VP1. For example, the Chinese sentence [NP
/somebody ] [VP1 [PP.../when...] [VP2 [/say NP[...] ] ] ] is translated
into when..., somebody said .... Here the translation of the first NP which is
outside VP1 is inserted between the translations of PP and VP2 and
therefore interrupts their reordering. Outside interruption accounts for
21.65% of REF-non-reorderable cases.
2. Inside interruption. The reordering of PP and VP2 is interrupted by the
combination of PP?s subnodes with VP2?s subnodes. Inside interruption
accounts for 48.45% of REF-non-reorderable cases, suggesting that it is the
major factor which decreases the reorderability of VP ? PP VP. Because
both PP and VP have their own complex sub-structures, the inside
Figure 7
An example of the translation of NP ? CP NP. This constituent can be translated in two
different orders: 1) the recently adopted statistical method (straight order); 2) the statistical
method recently adopted (inverted order).
558
Xiong et al Linguistically Annotated Reordering
Table 10
F1-scores ( BWR+LAR vs. BWR) for the 13 most frequent constituents in the test corpus.
Constituents indicated in bold have relatively lower F1 score for reordering.
Type Constituent Percent. (%) SYS-R-rate (%) F1-score (%)
BWR BWR+LAR BWR BWR+LAR
VP
VP ? VV NP 8.12 79.22 84.10 76.97 80.53
VP ? ADVP VP 4.30 63.45 65.86 70.83 73.67
VP ? PP VP 1.87 60.32 70.37 39.29 40.33
VP ? VV IP 1.82 79.35 86.14 77.16 82.26
NP
NP ? NN NN 6.88 84.68 85.18 76.17 79.10
NP ? NP NP 5.12 82.13 84.93 69.25 72.17
NP ? DNP NP 2.14 69.75 74.83 56.68 56.61
NP ? CP NP 2.12 59.67 73.43 48.75 54.48
Misc.
IP ? NP VP 6.78 71.99 79.80 63.22 65.79
PP ? P NP 3.63 80.63 85.95 82.75 84.93
CP ? IP DEC 3.51 83.94 87.89 69.91 72.24
QP ? CD CLP 2.74 66 65 67.52 68.47
DNP ? NP DEG 2.43 85.98 89.84 67.5 68.75
interruption is very complicated and includes a variety of cases, some of
which are quite unexpected. Here we show two frequent examples of
inside interruption:
a. The preposition in the PP and the verb word/phrase of VP2 are
aligned to only one target word or one continuous phrase. For
example,.../pressure,.../be confident of,.../
suffer from, and so on. This is caused by the lexical divergence
problem.
b. The PP is first combined with the verb word of VP2 in an inverted
order, then combined with the remainder of VP2 in a straight order.
For example, [PP [P] [omission1]] [VP [VV	] [omission2]]
might be translated into learned from omission1 that omission2.
3. Parse error. This accounts for 29.90% of REF-non-reorderable cases.
Although these reasons are summarized from our analysis on the constituent type
VP ? PP VP, they can be used to explain other REF-non-reorderable constituents, such
as NP ? CP NP.
8.3 Syntactic Constituent Movement in System Translations
8.3.1 Overall Reordering Precision and Recall of Syntactic Constituents. By summarizing all
syntactic reordering patterns (REF-SRP, SYS-SRP, and Match-SRP) for all constituents,
we can calculate the overall reordering precision and recall of syntactic constituents.
Table 11 shows the results for both BWR+LAR and BWR, where BWR+LAR clearly
outperforms BWR.
559
Computational Linguistics Volume 36, Number 3
Table 11
Syntactic reordering precision and recall of BWR+LAR vs. BWR on the test corpus.
Precision Recall F1
BWR 70.89 68.79 69.83
BWR+LAR 71.32 73.08 72.19
8.3.2 The Effect of Linguistic Knowledge on Phrase Movement. To understand the change
in phrase movement caused by linguistic knowledge, we further investigate how well
BWR and BWR+LAR reorder certain constituents, especially those with high distribu-
tion probability. Table 10 lists the 13 most frequent constituents, which jointly account
for 51.46% of all multi-branching constituents. Except for NP ? DNP NP, the reorder-
ing F1 score of all these constituents in BWR+LAR is better than that in BWR.
Our hypothesis for the phrase movement change in BWR+LAR is that the integrated
linguistic knowledge makes phrase movement in BWR+LAR pay more respect to syn-
tactic constituent boundaries. The overall R-rates of BWR+LAR vs. BWR described in
Section 8.1 indicate that BWR+LAR tends towards moving more syntactic constituents
together than BWR does. We want to know whether this is also true for a specific
constituent type. The fourth and fifth columns in Table 10 present the R-rate for each
individual constituent type that we have analyzed. It is obvious that the R-rate of
BWR+LAR is much higher than that of BWR for almost all constituents. This indicates
that higher R-rate is one of the reasons for the higher performance of BWR+LAR.
To gain a more concrete understanding of this change, we show two examples for
the reordering of VP ? PP VP in Figure 8. In both examples, BWR fails to move the PP
constituent to the right of the VP constituent, whereas BWR+LAR does it successfully.
By tracing the binary BTG trees generated by the decoder, we find that BWR generated
a very different BTG tree from the source parse tree whereas the BTG tree in BWR+LAR
almost matches the source parse tree. In the first example, BWR combines the VP phrase
Figure 8
Two examples for the translation of VP ? PP VP. Square brackets indicate combinations in a
straight order and angular brackets represent combinations in an inverted order.
560
Xiong et al Linguistically Annotated Reordering

 with  and then combines . The preposition word  is combined with
the NP phrase NHK, which makes the translation of NHK interrupt the reordering of
VP ? PP VP in this example. The BWR tree in the second example is even worse. The
non-syntactic phrase   in the VP phrase is first combined with  ,
which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of
the VP phrase is then merged. This merging process continues regardless of the source
parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples
suggests that reordering models should respect syntactic structures in order to capture
reorderings under these structures.
Our observation on phrase movement change resonates with the recent efforts in
phrasal SMT that allow the decoder to prefer translations which show more respect
for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto,
Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other
words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early
syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has
receded in more powerful syntax-based models (Galley et al 2004; Chiang 2005) and
non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008)
and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses
which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose
this as a hard constraint on the ITG constraint to allow reorderings which respect the
source parse tree. They all report significant improvements on different language pairs,
which indicates that syntactic cohesion is very useful for phrasal SMT. Our analysis
demonstrates that linguistically annotated reordering provides an alternative way to
incorporate syntactic cohesion into phrasal SMT.
8.4 Challenges in Phrase Reordering and Suggestions
We highlight three constituent types in Table 10 (indicated in bold) which are much
more difficult to reorder, as indicated by their relatively lower F1 scores. The lower F1
scores indicate that BWR+LAR is not fully sufficient for reordering these constituents
although it performs much better than BWR. We find two main reasons for the lower F1
scores and provide suggestions accordingly as follows.
1. Constrained decoding. We observe that in reorderable constituents which
involve long-distance reorderings, their boundaries are easily violated by
phrases outside them. To prohibit boundary violations, we propose
constrained decoding. In constrained decoding, we define special zones
in source sentences. Reorderings and translations within the zones cannot
be interrupted by fragments outside the zones. We can also define other
constrained operations on the zones. For example, we can prohibit
swappings in any zones which contain punctuation (Xiong et al 2008b).
The beginning and ending positions of a zone are automatically learned.
To be more flexible, they are not necessarily constituent boundaries.
Constrained decoding is different from both soft constraints (Cherry 2008;
Marton and Resnik 2008) and hard constraints (Yamamoto, Okuma, and
Sumita 2008). It can be considered as in between both of these because it is
harder than the former but softer than the latter.
2. Integrating special reordering rules. Some constituents are indeed
non-reorderable as we discussed in Section 8.2.2. Inside or outside
561
Computational Linguistics Volume 36, Number 3
interruptions have to be allowed to obtain fluent translations for these
constituents. However, the allowance of interruptions is sometimes
beyond the representability of BTG rules. For example, to solve the lexical
divergence problem, bilingual rules with aligned lexicons have to be
introduced. To capture reorderings of these constituents, we propose to
integrate special reordering rules with richer contextual information into
BTG to extend BTG?s ability to deal with interruptions. Completely
replacing BTG with richer formalisms, such as hierarchical phrase
(Chiang 2005) and tree-to-string (Liu, Liu, and Lin 2006) or string-to-tree
(Marcu et al 2006), introduces a huge extra cost. Instead, integrating a
small number of reordering rules into BTG to model reorderings of
non-reorderable constituents would be more desirable.
8.5 Discussion
In the definition of syntactic reordering patterns, we only consider the relative order
of individual constituents on the target side. We do not consider whether or not they
remain contiguous on the target side. It is possible that other words are inserted be-
tween spans of two contiguous constituents. We use the term gap to refer to when
this happens. The absence of a gap in the definition of syntactic reordering patterns
may produce more matched SRPs and therefore lead to higher precision and recall.
Table 12 shows the revised overall precision and recall of syntactic reordering patterns
when we also compare gaps. The revised results show that BWR+LAR still significantly
outperforms BWR. This also applies to the 13 constituents identified in Table 10. The
analysis results obtained before are still valid when we consider gaps.
9. Related Work
9.1 Linguistically Motivated Phrase Reordering
There are various approaches which are devoted to incorporating linguistic knowledge
into phrase reordering. Generally, these approaches can be roughly divided into three
categories: (1) reordering the source language in a preprocessing step before decoding
begins; (2) estimating phrase movement with reordering models; and (3) capturing
reorderings by synchronous grammars. The preprocessing approach applies manual or
automatically extracted reordering knowledge from linguistic structures to transform
the source language sentence into a word order that is closer to the target sentence.
The second reordering approach moves phrases under certain reordering constraints
and estimates the probabilities of movement with linguistic information. In the third
Table 12
Revised overall precision and recall of BWR+LAR vs. BWR on the test corpus when we consider
the gap in syntactic reordering patterns.
Precision Recall F1
BWR (gap) 46.28 44.91 45.58
BWR+LAR (gap) 48.80 50 49.39
562
Xiong et al Linguistically Annotated Reordering
approach, reordering knowledge is included in synchronous rules. The last two cate-
gories reorder the source sentence during decoding, which distinguishes them from the
first approach. Note that some researchers integrate multiple reordering approaches in
one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni
2008).
9.1.1 The Preprocessing Approach. In early work, Brown et al (1992) describe an approach
to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a
preprocessing approach which automatically learns reordering patterns based on CFG
productions. Since then, the preprocessing approach seems to have been more popular.
Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types
of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees
using fine-grained human-written rules, mostly concentrating on VP and NP structures.
Li et al (2007) improve the preprocessing approach by generating n-best reordered
source sentences with reordering knowledge automatically learned from the alignments
between source parse trees and target translations. The approach proposed in Li et al
also enhances the connection between the preprocessing and decoding by adding a
source reordering probability feature. Other approaches introduced in Nie?n and Ney
(2001), Popovic? and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological,
POS, and chunk knowledge in the preprocessing approach, respectively.
9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM con-
straint (Zens and Ney 2003), the early work uses a distortion-based reordering model
to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG
constraint, the corresponding model is the flat model which assigns a prior probability
to the straight or inverted order (Wu 1996). These two models don?t respect the content
of phrases which are moved. To address this issue, lexicalized reordering models which
are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn
et al 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and
Lin (2006) introduce a more flexible reordering model under the ITG constraint using
discriminative features which are automatically learned from a training corpus. Zhang
et al (2007) propose a model for syntactic phrase reordering which uses syntactic
knowledge from source parse trees. Our reordering approach is most similar to those in
Xiong, Liu, and Lin (2006) and Zhang et al but extends them further by using syntactic
knowledge and allowing non-syntactic phrase reordering.
9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use
synchronous grammars to capture reorderings between two languages. Chiang (2005)
introduces formal synchronous grammars for phrase-based translation. In his work,
hierarchical reordering knowledge is included in synchronous rules which are automat-
ically learned from word-aligned corpus. In linguistically syntax-based models, string-
to-tree (Marcu et al 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and
Lin 2006), and tree-to-tree (Zhang et al 2008) translation rules, just to name a few, are
explored. Linguistical reordering knowledge is naturally included in these syntax-based
translation rules.
9.2 Automatic Analysis of Reordering
Although there is a variety of work on phrase reordering, automatic analysis of phrase
reordering is not widely explored in the SMT literature. Chiang et al (2005) propose
563
Computational Linguistics Volume 36, Number 3
an automatic method to compare different system outputs in a fine-grained manner
with regard to reordering. In their method, common word n-grams occurring in both
reference translations and system translations are extracted and generalized to part-of-
speech tag sequences. A recall is calculated for each certain tag sequence to indicate the
ability of reordering models to capture this tag sequence in system translations. Popovic
et al (2006) use the relative difference between WER (word error rate) and PER (position
independent word error rate) to indicate reordering errors. The larger the difference, the
more reordering errors there are.
Callison-Burch et al (2007) propose a constituent-based evaluation that is very simi-
lar to our method in Steps (1)?(3). They also parse the source sentence and automatically
align the parse tree with the reference/system translations. The difference is that they
highlight constituents from the parse tree to enable human evaluation of the translations
of these constituents, rather than automatically analyzing constituent movement. They
use this method for human evaluation in the shared translation task of the 2007 and
2008 ACL Workshop on Statistical Machine Translation.
Fox (2002) systematically studies syntactic cohesion between French and English
using human translations and alignments. Compared with her work, our analysis here
includes, but is not limited to, an investigation of syntactic cohesion in an actual MT
system.
10. Conclusion
We have presented a novel linguistically motivated phrase reordering approach:
Linguistically Annotated Reordering. The LAR approach incorporates soft linguistic
knowledge from the source parse tree into hard hierarchical skeletons generated by
BTG in phrasal SMT. To automatically learn reordering features, we have introduced
algorithms for reordering example extraction and linguistic annotation. We have also
proposed a new syntax-based analysis method to detect syntactic constituent movement
in human/machine translations.
We have conducted experiments on large-scale training data to evaluate LAR and
BWR as well as the reordering example extraction algorithms. Our evaluation results
show that:
1. Extracting reordering examples directly from word alignments is much
better than from BTG-style trees which are built from word alignments.
2. Selection rules which bias the reordering model towards smaller
swappings improve translation quality.
3. BWR+LAR significantly outperforms BWR, which suggests that the
integration of linguistic knowledge improves reordering; and tuning two
separate reordering models is better than the All-in-One combination
method.
We have further analyzed the outputs of BWR+LAR vs. BWR using the proposed
syntax-based analysis method. Our analysis results show that:
1. BWR+LAR achieves a significantly higher reordering precision and recall
than BWR does with regard to syntactic constituent movement.
564
Xiong et al Linguistically Annotated Reordering
2. For most reorderable constituents, integrating source-side linguistic
knowledge into the reordering model can significantly improve
reorderings by guiding reordering models to prefer hypotheses that pay
more respect to constituent boundaries.
3. For non-reorderable constituents or constituents involving long-distance
reorderings, integrating source-side linguistic knowledge into the
reordering model is not sufficient to avoid illegal boundary violations or to
capture reordering patterns.
To avoid illegal boundary violations in long-span constituents, we suggest con-
strained decoding, which protects special zones in the source sentence from being
interrupted by phrases outside the zones. Beginning and ending positions of the zones
are automatically learned using lexical and syntactic knowledge. To capture complex re-
orderings which cross constituent boundaries, phrasal SMT should integrate reordering
rules with richer contextual information.
Acknowledgments
We would like to thank the three anonymous
reviewers for their helpful comments and
suggestions.
References
Al-Onaizan, Yaser and Kishore Papineni.
2006. Distortion models for statistical
machine translation. In Proceedings
of the 21st International Conference on
Computational Linguistics and 44th
Annual Meeting of the Association for
Computational Linguistics, pages 529?536,
Sydney.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, John D. Lafferty,
and Robert L. Mercer. 1992. Analysis,
statistical transfer, and synthesis in
machine translation. In Proceedings
of the Fourth International Conference on
Theoretical and Methodological Issues in
Machine Translation, pages 83?100,
Montreal.
Callison-Burch, Chris, Cameron Fordyce,
Philipp Koehn, Christof Monz, and Josh
Schroeder. 2007. (Meta-) evaluation of
machine translation. In Proceedings of the
Second Workshop on Statistical Machine
Translation, pages 136?158, Prague.
Cherry, Colin. 2008. Cohesive phrase-based
decoding for statistical machine
translation. In Proceedings of ACL-08: HLT,
pages 72?80, Columbus, OH.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics, pages 263?270,
Ann Arbor, MI.
Chiang, David, Adam Lopez, Nitin
Madnani, Christof Monz, Philip Resnik,
and Michael Subotin. 2005. The hiero
machine translation system: Extensions,
evaluation, and analysis. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 779?786,
Vancouver.
Collins, Michael, Philipp Koehn, and Ivona
Kucerova. 2005. Clause restructuring for
statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 531?540, Ann Arbor, MI.
Eisner, Jason. 2003. Learning non-isomorphic
tree mappings for machine translation. In
The Companion Volume to the Proceedings of
41st Annual Meeting of the Association for
Computational Linguistics, pages 205?208,
Sapporo.
Fox, Heidi. 2002. Phrasal cohesion and
statistical machine translation. In
Proceedings of the 2002 Conference on
Empirical Methods in Natural Language
Processing, pages 304?311,
Philadelphia, PA.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics:
HLT-NAACL 2004, pages 273?280,
Boston, MA.
Ge, Niyu, Abe Ittycheriah, and Kishore
Papineni. 2008. Multiple reorderings in
phrase-based machine translation. In
Proceedings of the ACL-08: HLT Second
Workshop on Syntax and Structure in
565
Computational Linguistics Volume 36, Number 3
Statistical Translation (SSST-2), pages 61?68,
Columbus, OH.
Huang, Liang, Kevi Knight, and Aravind
Joshi. 2006. Statistical syntax-directed
translation with extended domain of
locality. In Proceedings of the 7th Conference
of the Association for Machine Translation
of the Americas, pages 66?73,
Cambridge, MA.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In
Proceedings of EMNLP 2004, pages 388?395,
Barcelona.
Koehn, Philipp, Amittai Axelrod, Alexandra
Birch Mayne, Chris Callison-Burch,
Miles Osborne, and David Talbot. 2005.
Edinburgh system description for the
2005 IWSLT speech translation
evaluation. In Proceedings of the
International Workshop on Spoken
Language Translation 2005, pages 78?85,
Pittsburgh, PA.
Koehn, Philipp, Franz Joseph Och, and
Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings of
the 2003 Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 58?54, Edmonton.
Kumar, Shankar and William Byrne. 2005.
Local phrase reordering models for
statistical machine translation. In
Proceedings of Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing,
pages 161?168, Vancouver.
Li, Chi-Ho, Minghui Li, Dongdong Zhang,
Mu Li, Ming Zhou, and Yi Guan. 2007. A
probabilistic approach to syntax-based
reordering for statistical machine
translation. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 720?727,
Prague.
Lin, Dekang. 2004. A path-based transfer
model for machine translation. In
Proceedings of the 20th International
Conference on Computational Linguistics
(Coling 2004), pages 625?630, Geneva.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006.
Tree-to-string alignment template for
statistical machine translation. In
Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 609?616,
Sydney.
Marcu, Daniel, Wei Wang, Abdessamad
Echihabi, and Kevin Knight. 2006. SPMT:
Statistical machine translation with
syntactified target language phrases. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 44?52, Sydney.
Marton, Yuval and Philip Resnik. 2008. Soft
syntactic constraints for hierarchical
phrased-based translation. In Proceedings
of ACL-08: HLT, pages 1003?1011,
Columbus, OH.
Navarro, Gonzalo. 2001. A guided tour to
approximate string matching. ACM
Computing Surveys, 33(1):31?88.
Nie?n, Sonja and Hermann Ney. 2001.
Morpho-syntactic analysis for
reordering in statistical machine
translation. In Proceedings of MT Summit
VIII, pages 247?252, Santiago de
Compostela.
Och, Franz Josef. 2002. Statistical Machine
Translation: From Single-Word Models to
Alignment Templates. Ph.D. thesis, RWTH
Aachen University, Germany.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 160?167, Sapporo.
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models. In
Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics,
pages 440?447, Hong Kong.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of 40th
Annual Meeting of the Association for
Computational Linguistics, pages 311?318,
Philadelphia, PA.
Popovic, Maja, Adria` de Gispert, Deepa
Gupta, Patrik Lambert, Hermann Ney,
Jose? B. Marin?o, Marcello Federico, and
Rafael Banchs. 2006. Morpho-syntactic
information for automatic error analysis
of statistical machine translation output.
In Proceedings on the Workshop on
Statistical Machine Translation, pages 1?6,
New York, NY.
Popovic?, Maja and Hermann Ney. 2006.
Pos-based word reorderings for statistical
machine translation. In Proceedings of the
Fifth International Conference on Language
Resources and Evaluation (LREC 2006),
pages 1278?1283, Genoa.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translations: Syntactically informed
phrasal smt. In Proceedings of the 43rd
566
Xiong et al Linguistically Annotated Reordering
Annual Meeting of the ACL, pages 271?279,
Ann Arbor, MI.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit.
In Proceedings of the 7th International
Conference on Spoken Language
Processing (ICSLP 2002), pages 901?904,
Denver, CO.
Tillman, Christoph. 2004. A unigram
orientation model for statistical machine
translation. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL
2004): Short Papers, pages 101?104,
Boston, MA.
Wang, Chao, Michael Collins, and Philipp
Koehn. 2007. Chinese syntactic reordering
for statistical machine translation. In
Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL),
pages 737?745, Prague.
Wu, Dekai. 1996. A polynomial-time
algorithm for statistical machine
translation. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 152?158,
Santa Cruz, CA.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Wu, Dekai, Marine Carpuat, and Yihai Shen.
2006. Inversion transduction grammar
coverage of Arabic-English word
alignment for tree-structured statistical
machine translation. In Proceeding
of the IEEE/ACL 2006 Workshop on
Spoken Language Technology (SLT 2006),
pages 234?237, Aruba.
Xia, Fei and Michael McCord. 2004.
Improving a statistical MT system
with automatically learned rewrite
patterns. In Proceedings of the 20th
International Conference on Computational
Linguistics (Coling 2004), pages 508?514,
Geneva.
Xiong, Deyi, Qun Liu, and Shouxun Lin.
2005. Parsing the penn chinese treebank
with semantic knowledge. In Proceedings of
The 2nd International Joint Conference on
Natural Language Processing (IJCNLP-05),
pages 70?81, Jeju Island.
Xiong, Deyi, Qun Liu, and Shouxun Lin.
2006. Maximum entropy based phrase
reordering model for statistical machine
translation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 521?528, Sydney.
Xiong, Deyi, Min Zhang, Aiti Aw, and
Haizhou Li. 2008a. A linguistically
annotated reordering model for
BTG-based statistical machine
translation. In Proceedings of ACL-08:
HLT, Short Papers, pages 149?152,
Columbus, OH.
Xiong, Deyi, Min Zhang, Aiti Aw, Haitao Mi,
Qun Liu, and Shouxun Lin. 2008b.
Refinements in BTG-based statistical
machine translation. In Proceedings
of the Third International Joint Conference
on Natural Language Processing,
pages 505?512, Hyderabad.
Xue, Nianwen, Fei Xia, Shizhe Huang, and
Anthony Kroch. 2000. The bracketing
guidelines for the Penn Chinese treebank
(3.0). Technical report IRCS 00-07,
University of Pennsylvania Institute for
Research in Cognitive Science,
Philadelphia.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics,
pages 523?530, Toulouse.
Yamamoto, Hirofumi, Hideo Okuma, and
Eiichiro Sumita. 2008. Imposing
constraints from the source tree on ITG
constraints for SMT. In Proceedings of the
ACL-08: HLT Second Workshop on Syntax
and Structure in Statistical Translation
(SSST-2), pages 1?9, Columbus, OH.
Zens, Richard, and Hermann Ney. 2003.
A comparative study on reordering
constraints in statistical machine
translation. In Proceedings of the 41st
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 144?151, Sapporo, Japan.
Zhang, Dongdong, Mu Li, Chi-Ho Li, and
Ming Zhou. 2007. Phrase reordering
model integrating syntactic knowledge
for SMT. In Proceedings of the 2007 Joint
Conference on Empirical Methods in
Natural Language Processing and
Computational Natural Language
Learning (EMNLP-CoNLL), pages 533?540,
Prague.
Zhang, Hao and Daniel Gildea. 2005.
Stochastic lexicalized inversion
transduction grammar for alignment. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 475?482, Ann Arbor, MI.
567
Computational Linguistics Volume 36, Number 3
Zhang, Hao, Daniel Gildea, and David
Chiang. 2008. Extracting synchronous
grammar rules from word-level
alignments in linear time. In Proceedings of
the 22nd International Conference on
Computational Linguistics (Coling 2008),
pages 1081?1088, Manchester.
Zhang, Min, Hongfei Jiang, Aiti Aw, Haizhou
Li, Chew Lim Tan, and Sheng Li. 2008. A
tree sequence alignment-based tree-to-tree
translation model. In Proceedings of ACL-08:
HLT, pages 559?567, Columbus, OH.
Zhang, Yuqi, Richard Zens, and Hermann
Ney. 2007. Chunk-level reordering of
source language sentences with
automatically learned rules for statistical
machine translation. In Proceedings
of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in
Statistical Translation, pages 1?8,
Rochester, NY.
Zollmann, Andreas, Ashish Venugopal, and
Stephan Vogel. 2008. The CMU
syntax-augmented machine translation
system: SAMT on hadoop with N-best
alignments. In Proceedings of International
Workshop on Spoken Language Translation
(IWSLT), pages 18?25, Honolulu, HI.
568
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 73?78,
Baltimore, Maryland USA, June 23-24, 2014.
c
?2014 Association for Computational Linguistics
A Rule-Augmented Statistical Phrase-based Translation System
Cong Duy Vu Hoang
?
, AiTi Aw
?
and Nhung T. H. Nguyen
??
?
Human Language Technology Dept.
Institute for Infocomm Research (I
2
R), A*STAR, Singapore
{cdvhoang, aaiti}@i2r.a-star.edu.sg
?
School of Information Science
Japan Advanced Institute of Science and Technology (JAIST), Japan
nthnhung@jaist.ac.jp
Abstract
Interactive or Incremental Statistical Ma-
chine Translation (IMT) aims to provide a
mechanism that allows the statistical mod-
els involved in the translation process to be
incrementally updated and improved. The
source of knowledge normally comes from
users who either post-edit the entire trans-
lation or just provide the translations for
wrongly translated domain-specific termi-
nologies. Most of the existing work on
IMT uses batch learning paradigm which
does not allow translation systems to make
use of the new input instantaneously. We
introduce an adaptive MT framework with
a Rule Definition Language (RDL) for
users to amend MT results through trans-
lation rules or patterns. Experimental re-
sults show that our system acknowledges
user feedback via RDL which improves
the translations of the baseline system on
three test sets for Vietnamese to English
translation.
1 Introduction
In current Statistical Machine Translation (SMT)
framework, users are often seen as passive con-
tributors to MT performance. Even if there is a
collaboration between the users and the system, it
is carried out in a batch learning paradigm (Ortiz-
Martinez et al., 2010), where the training of the
SMT system and the collaborative process are car-
ried out in different stages. To increase the produc-
tivity of the whole translation process, one has to
incorporate human correction activities within the
translation process. Barrachina et al. (2009) pro-
posed an iterative process in which the translator
activity is used by the system to compute its best
?
Work done during an internship at I
2
R, A*STAR.
(or n-best) translation suffix hypotheses to com-
plete the prefix. Ortiz-Martinez et al. (2011) pro-
posed an IMT framework that includes stochas-
tic error-correction models in its statistical formal-
ization to address the prefix coverage problems
in Barrachina et al. (2009). Gonzalez-Rubio et
al. (2013) proposed a similar approach with a spe-
cific error-correction model based on a statistical
interpretation of the Levenshtein distance (Leven-
shtein, 1966). On the other hand, Ortiz-Martinez
et al. (2010) presented an IMT system that is able
to learn from user feedback by incrementally up-
dating the statistical models used by the system.
The key aspect of this proposed system is the use
of HMM-based alignment models trained by an in-
cremental EM algorithm.
Here, we present a system similar to Ortiz-
Martinez et al. (2010). Instead of updating the
translation model given a new sentence pair, we
provide a framework for users to describe trans-
lation rules using a Rule Definition Language
(RDL). Our RDL borrows the concept of the rule-
based method that allows users to control the
translation output by writing rules using their lin-
guistic and domain knowledge. Although statis-
tical methods pre-dominate the machine transla-
tion research currently, rule-based methods are
still promising in improving the translation qual-
ity. This approach is especially useful for low
resource languages where large training corpus
is not always available. The advantage of rule-
based methods is that they can well handle par-
ticular linguistic phenomena which are peculiar to
languages and domains. For example, the TCH
MT system at IWSLT 2008 (Wang et al., 2008)
used dictionary and hand-crafted rules (e.g. regu-
lar expression) to process NEs. Their experiments
showed that handling NE separately (e.g., person
name, location name, date, time, digit) results in
translation quality improvement.
In this paper, we present an adaptive and in-
73
Figure 1: The proposed rule-augmented SMT
framework.
teractive MT system that allows users to correct
the translation and integrate the adaptation into
the next translation cycle. Our experiments show
that the system is specifically effective in han-
dling translation errors related to out of vocabulary
words (OOVs), language expressions, name enti-
ties (NEs), abbreviations, terminologies, idioms,
etc. which cannot be easily addressed in the ab-
sence of in-domain parallel data.
2 System Overview
Figure 1 shows the translation and interactive pro-
cess of our system. The system is trained with a
batch of parallel texts to create a baseline model.
Users improve the translation by adding RDL
rules to change or correct the unsatisfactory trans-
lation. New RDL rules are tested in a working
environment before uploading to the production
environment where they would be used by subse-
quent translation requests.
In our system, RDL Management checks, vali-
dates and indexes the translation rules. The Rule-
Augmented Decoder has two components: (1) the
RDL Matcher to find applicable RDL rules for a
given source text to create dynamic translation hy-
potheses; and (2) the Augmented Decoder to pro-
duce the final consensus translation using both dy-
namic hypotheses and static hypotheses from the
baseline model.
3 Rule Definition Language (RDL)
The Rule Definition Language (RDL) comprises a
RDL grammar, a RDL parser and a RDL matching
algorithm.
3.1 RDL Grammar
Our RDL grammar is represented with a Backus-
Naur Form (BNF)s syntax. The major feature of
Node Type Description
Token Any string of characters in the defined
basic processing unit of the language.
String A constant string of characters.
Identifier A term represents a pre-defined role
(e.g. integer, date, sequence, . . . ).
Meta-node A term executes a specific function
(e.g. casing, selection/option, con-
nection).
Context cue A term describes source context?s ex-
istence.
Function A term executes a pre-defined task.
Table 1: A brief description of RDL nodes.
Figure 2: An Example of RDL Rule.
RDL grammar is the support of pre-defined identi-
fiers and meta-operators which go beyond the nor-
mal framework of regular expression. We also
included a set of pre-defined functions to further
constraint the application and realization of the
rules. This framework allows us to incorporate
semantic information into the rule definition and
derive translation hypotheses using both semantic
and lexical information. A RDL rule is identified
by a unique rule ID and five constituents, includ-
ing Source pattern, rule Condition, Target transla-
tion, Reordering rule and user ConFidence. The
source pattern and target translation can be con-
structed using different combination of node types
as described in Table 1. The rules can be further
conditioned by using some pre-defined functions
and the system allows users to reorder the transla-
tion of the target node. Figure 2 gives an example
of a RDL rule where identifier @Num is used.
3.2 RDL Parsing and Indexing
The RDL Parser checks the syntax of the rules
before indexing and storing them into the rule
database. We utilize the compiler generator (WoB
et al., 2003) to generate a RDL template parser and
then embed all semantic parsing components into
the template to form our RDL Parser.
As rule matching is performed during transla-
tion, searching of the relevant rules have to be very
fast and efficient. We employed the modified ver-
sion of an inverted index scheme (Zobel and Mof-
fat, 2006) for our rule indexing. The algorithm is
74
Figure 3: A linked item chain for a rule source
(@a @b [c] [?d e?] [?f g h?] (?i? | ?j k?)).
represented in Algorithm 1.
Data: ruleID & srcPatn
Result: idxTbl
// To build data structure ? Forward Step
doForward(srcPatn, linkedItmChain);
// To create index table ? Backward Step
doBackward(linkedItmChain, ruleID, idxTbl);
Algorithm 1: Algorithm for RDL rule indexing.
The main idea of the rule indexing algorithm is
to index all string-based nodes in the source pat-
tern of the RDL rule. Each node is represented
using 3-tuple. They are ruleID, number of nodes
in source pattern and all plausible positions of the
node during rule matching. The indexing is car-
ried out via a Forward Step and Backward Step.
The Forward Step builds a linked item chain which
traverses all possible position transitions from one
node to another as illustrated in Figure 3. Note that
S and E are the Start and End Node. The link indi-
cates the order of transition from a node to another.
The numbers refer to the possible positions of an
item in source. The Backward Step starts at the
end of the source pattern; traverses back the link
to index each node using the 3-tuple constructed
in the Forward Step. This data structure allows us
to retrieve, add or update RDL rules efficiently and
incrementally without re-indexing.
3.3 RDL Matching Algorithm
Each word in the source string will be matched
against the index table to retrieve relevant RDL
rules during decoding. The aim is to retrieve all
RDL rules in which the word is used as part of
the context in the source pattern. We sort all the
rules based on the word positions recorded dur-
ing indexing, match their source patterns against
the input string within the given span, check the
conditions and generate the hypotheses if the rules
fulfill all the constraints.
4 Rule-Augmented Decoder
The rule-augmented decoder integrates the dy-
namic hypotheses generated during rule match-
ing with the baseline hypotheses during decoding.
Given a sentence f from a source language F, the
fundamental equation of SMT (Brown et al., 1993)
to translate it into a target sentence e of a target
language E is stated in Equation 1.
e
best
= argmax
e
P
r
(e|f)
= argmax
e
P
r
(f |e)P
r
(e)
= argmax
e
N
?
n=1
?
n
h
n
(e, f)
(1)
Here, P
r
(f |e) is approximated by a translation
model that represents the correlation between the
source and the target sentence and P
r
(e) is ap-
proximated by a language model presenting the
well-formedness of the candidate translation e.
Most of the SMT systems follow a log-linear ap-
proach (Och and Ney, 2002), where direct mod-
elling of the posterior probabilityP
r
(f |e) of Equa-
tion 1 is used. The decoder searches for the best
translation given a set of model h
m
(e, f) by max-
imizing the log-linear feature score (Och and Ney,
2004) as in Equation 1.
For each hypothesis generated by the RDL rule,
an appropriate feature vector score is needed to en-
sure that it will not disturb the probability distribu-
tion of each model and contributes to hypothesis
selection process of SMT decoder.
4.1 Model Score Estimation
The aim of the RDL implementation is to address
the translation of language-specific expressions
(such as date-time, number, title, etc.) and do-
main-specific terminologies. Sometimes, transla-
tion rules and bilingual phrases can be easily ob-
served and obtained from experienced translators
or linguists. However, it is difficult to estimate the
probability of the RDL rules manually to reflect
the correct word or phrase distribution in real data.
Many approaches have been proposed to solve the
OOV problem and estimate word translation prob-
abilities without using parallel data. Koehn et
al. (2000) estimated word translation probabilities
from unrelated monolingual corpora using the EM
algorithm. Habash et al. (2008) presented differ-
ent techniques to extend the phrase table for on-
line handling of OOV. In their approach, the ex-
tended phrases are added to the baseline phrase
75
table with a default weight. Arora et al. (2008)
extended the phrase table by adding new phrase
translations for all source language words that do
not have a single-word entry in the original phrase-
table, but appear in the context of larger phrases.
They adjusted the probabilities of each entry in the
extended phase table.
We performed different experiments to estimate
the lexical translation feature vector for each dy-
namic hypothesis generated by our RDL rules. We
obtain the best performance by estimating the fea-
ture vector score using the baseline phrase table
through context approximation. For each hypoth-
esis generated by the RDL rule, we retrieve en-
tries from the phrase table which have at least one
similar word with the source of the generated hy-
pothesis. We sort the entries based on the sim-
ilarities between the generated and retrieved hy-
potheses using both source and target phrase. The
medium score of the sorted list is assigned to the
generated hypothesis.
5 System Features
The main features of our system are (1) the flexi-
bilities provided to the user to create different lev-
els of translation rules, from simple one-to-one
bilingual phrases to complex generalization rules
for capturing the translation of specific linguis-
tic phenomena; and (2) the ability to validate and
manage translation rules online and incrementally.
5.1 RDL Rule Management
Our system framework is language independent
and has been implemented on a Vietnamese to En-
glish translation project. Figure 4 shows the RDL
Management Screen where a user can add, mod-
ify or delete a translation rule using RDL. A RDL
rule can be created using nodes. Each node can
be defined using string or system predefined meta-
identifiers with or without meta-operators as de-
scribed in Table 1. Based on the node type selected
by the user, the system further restricts the user to
appropriate conditions and translation functions.
The user can define the order of the translation out-
put of each node and at the same time, inform the
system whether to use a specific RDL exclusively
during decoding, in which any phrases from the
baseline phrase table overlapping with that span
will be ignored
1
. The system also provides an edi-
1
Similar to Moses XML markup exclusive feature
http://www.statmt.org/moses/?n=Moses.
Figure 4: RDL Management screen with identi-
fiers & meta-functions supported.
tor for expert users to code the rules using the RDL
controlled language. Each rule is validated by the
RDL parser (discussed in section 3.2), which will
display errors or warning messages when an in-
valid syntax is encountered.
5.2 RDL Rule Validation
Our decoder manages two types of phrase table.
One is the static phrase-table obtained through
the SMT training in parallel texts; the other is
the dynamic table that comprises of the hypothe-
ses generated on-the-fly during RDL rule match-
ing. To ensure only fully tested rules are used in
the production environment, the system supports
two types of dynamic phrase table. The work-
ing phrase-table holds the latest updates made by
the users. The users can test the translation with
these latest modifications using a specific transla-
tion protocol. When users are satisfied with these
modifications, they can perform an operation to
upload the RDL rules to the production phrase-
table, where the RDLs are used for all translation
AdvancedFeatures#ntoc9
76
Named Entity Category Number of Rules
Date-time 120
Measurement 92
Title 13
Designation 12
Number 19
Terminology 178
Location 13
Organization 48
Total 495
Table 2: Statistics of created RDL rules for
Vietnamese-to-English NE Translation.
requests. Uploaded rules can be deleted, modified
and tested again in the working environment be-
fore updated to the production environment. Fig-
ure 5b and Figure 5c show the differences in trans-
lation output before and after applied the RDL rule
in Figure 5a.
6 A Case Study for Vietnamese?English
Translation
We performed an experiment using the proposed
RDL framework for a Vietnamese to English
translation system. As named entity (NE) con-
tributes to most of the OOV occurrences and im-
pacts the system performance for out-of-domain
test data in our system, we studied the NE usage
in a large Vietnamese monolingual corpus com-
prising 50M words to extract RDL rules. We cre-
ated RDL rules for 8 popular NE types including
title, designation, date-time, measurement, loca-
tion, organization, number and terminology. We
made use of a list of anchor words for each NE
category and compiled our RDL rules based on
these anchor words. As a result, we compiled a
total of 495 rules for 8 categories and it took about
3 months for the rule creation. Table 2 shows the
coverage of our compiled rules.
6.1 Experiment & Results
Our experiments were performed on a training set
of about 875K parallel sentences extracted from
web news and revised by native linguists over 2
years. The corpus has 401K and 225K unique En-
glish and Vietnamese tokens. We developed 1008
and 2548 parallel sentences, each with 4 refer-
ences, for development and testing, respectively.
All the reference sentences are created and revised
by different native linguists at different times. We
also trained a very large English language model
using data from Gigaword, Europarl and English
Figure 5: Translation Demo with RDL rules.
Data Set nS nT nMR
TrainFull (VN) 875,579 28,251,775 627,125
TrainFull (EN) 875,579 20,191,526 -
Test1 (VN) 1009 34,717 737
Test1 (4 refs) (EN) 1009 ?25,713 -
Test2 (VN) 1033 29,546 603
Test2 (4 refs) (EN) 1033 ?22,717 -
Test3 (VN) 506 16,817 344
Test3 (4 refs) (EN) 506 ?12,601 -
Dev (VN) 1008 34,803 -
Dev (4 refs) (EN) 1008 ?25,631 -
Table 3: Statistics of Vietnamese-to-English paral-
lel data. nS, nT, and nMR are number of sentence
pairs and tokens, and count of matched rules, re-
spectively.
web texts of Vietnamese authors to validate the
impact of RDL rules on large-scale and domain-
rich corpus. The experimental results show that
created RDL rules improve the translation perfor-
mance on all 3 test sets. Table 3 and Table 4 show
respective data statistics and results of our evalua-
tion. More specifically, the BLEU scores increase
3%, 3.6% and 1.4% on the three sets, respectively.
7 Conclusion
We have presented a system that provides a con-
trol language (Kuhn, 2013) specialized for MT for
users to create translation rules. Our RDL differs
from Moses?s XML mark-up in that it offers fea-
77
Data Set System BLEU NIST METEOR
Set 1 Baseline 39.21 9.2323 37.81
+RDL (all) 39.51 9.2658 37.98
Set 2 Baseline 40.25 9.5174 38.24
+RDL (all) 40.61 9.6092 38.84
Set 3 Baseline 36.77 8.6953 37.65
+RDL (all) 36.91 8.7062 37.69
Table 4: Experimental results with RDL rules.
tures that go beyond the popular regular expres-
sion framework. Without restricting the mark-up
on the source text, we allow multiple translations
to be specified for the same span or overlapping
span.
Our experimental results show that RDL
rules improve the overall performance of the
Vietnamese-to-English translation system. The
framework will be tested for other language pairs
(e.g. Chinese-to-English, Malay-to-English) in
the near future. We also plan to explore advanced
methods to identify and score ?good? dynamic
hypotheses on-the-fly and integrate them into cur-
rent SMT translation system (Simard and Foster,
2013).
Acknowledgments
We would like to thank the reviewers of the paper
for their helpful comments.
References
Paul M. Sumita E. Arora, K. 2008. Translation
of unknown words in phrase-based statistical ma-
chine translation for languages of rich morphol-
ogy. In In Proceedings of the Workshop on Spoken
Language Technologies for Under-Resourced Lan-
guages, SLTU 2008.
Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,
Antonio Lagarda, Hermann Ney, Jes?us Tom?as, En-
rique Vidal, and Juan-Miguel Vilar. 2009. Sta-
tistical approaches to computer-assisted translation.
Comput. Linguist., 35(1):3?28, March.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Jes?us Gonz?alez-Rubio, Daniel Ort??z-Martinez, Jos?e-
Miguel Bened??, and Francisco Casacuberta. 2013.
Interactive machine translation using hierarchical
translation models. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 244?254, Seattle, Washington,
USA, October.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of
ACL: Short Papers, HLT-Short ?08, pages 57?60,
Stroudsburg, PA, USA.
Philipp Koehn and Kevin Knight. 2000. Estimating
word translation probabilities from unrelated mono-
lingual corpora using the em algorithm. In Proceed-
ings of the Seventeenth National Conference on Ar-
tificial Intelligence and Twelfth Conference on Inno-
vative Applications of Artificial Intelligence, pages
711?715. AAAI Press.
Tobias Kuhn. 2013. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics.
VI Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet
Physics Doklady, 10:707.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In In Proceedings of
ACL, pages 295?302, Stroudsburg, PA, USA.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449, Decem-
ber.
Daniel Ortiz-Martinez, Ismael Garcia-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for inter-
active statistical machine translation. In In Proceed-
ings of NAACL, HLT ?10, pages 546?554, Strouds-
burg, PA, USA.
Daniel Ortiz-Mart??nez, Luis A. Leiva, Vicent Alabau,
Ismael Garc??a-Varea, and Francisco Casacuberta.
2011. An interactive machine translation system
with online learning. In In Proceedings of ACL:
Systems Demonstrations, HLT ?11, pages 68?73,
Stroudsburg, PA, USA.
Michel Simard and George Foster. 2013. Pepr: Post-
edit propagation using phrase-based statistical ma-
chine translation. Proceedings of the XIV Machine
Translation Summit, pages 191?198.
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu,
Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008.
The tch machine translation system for iwslt 2008.
In In Proceedings of IWSLT 2008, Hawaii, USA.
Albrecht WoB, Markus Loberbauer, and Hanspeter
Mossenbock. 2003. Ll(1) conflict resolution in a
recursive descent compiler generator. In Modular
Programming Languages, volume 2789 of Lecture
Notes in Computer Science, pages 192?201.
Justin Zobel and Alistair Moffat. 2006. Inverted files
for text search engines. ACM Comput. Surv., 38,
July.
78
