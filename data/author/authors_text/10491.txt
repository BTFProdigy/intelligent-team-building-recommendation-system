Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 569?576
Manchester, August 2008
A Syntactic Time-Series Model for Parsing Fluent and Disfluent Speech ?
Tim Miller
Department of Computer Science
and Engineering
University of Minnesota
tmill@cs.umn.edu
William Schuler
Department of Computer Science
and Engineering
University of Minnesota
schuler@cs.umn.edu
Abstract
This paper describes an incremental ap-
proach to parsing transcribed spontaneous
speech containing disfluencies with a Hier-
archical Hidden Markov Model (HHMM).
This model makes use of the right-corner
transform, which has been shown to in-
crease non-incremental parsing accuracy
on transcribed spontaneous speech (Miller
and Schuler, 2008), using trees trans-
formed in this manner to train the HHMM
parser. Not only do the representations
used in this model align with structure in
speech repairs, but as an HMM-like time-
series model, it can be directly integrated
into conventional speech recognition sys-
tems run on continuous streams of audio.
A system implementing this model is eval-
uated on the standard task of parsing the
Switchboard corpus, and achieves an im-
provement over the standard baseline prob-
abilistic CYK parser.
1 Introduction
Disfluency is one obstacle preventing speech
recognition systems from being able to recog-
nize spontaneous speech. Perhaps the most chal-
lenging aspect of disfluency recognition is the
phenomenon of speech repair, which involves a
speaker realizing a mistake, cutting off the flow
of speech, and then continuing on, possibly re-
tracing and replacing part of the utterance to that
point. This paper will describe a system which ap-
plies a syntactic model of speech repair to a time-
?The authors would like to thank the anonymous review-
ers for their input. This research was supported by National
Science Foundation CAREER/PECASE award 0447685. The
views expressed are not necessarily endorsed by the sponsors.
?c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
series parsing model, and evaluate that system on
the standard Switchboard corpus parsing task.
The speech repair terminology used here fol-
lows that of Shriberg (1994). A speech repair con-
sists of a reparandum, an interruption point, and
the alteration. The reparandum contains the words
that the speaker means to replace, including both
words that are in error and words that will be re-
traced. The interruption point is the point in time
where the stream of speech is actually stopped, and
the repairing of the mistake can begin. The alter-
ation contains the words that are meant to replace
the words in the reparandum.
2 Background
Historically, research in speech repair has focused
on acoustic cues such as pauses and prosodic con-
tours for detecting repairs, which could then be ex-
cised from the text for improved transcription. Re-
cent work has also looked at the possible contribu-
tion of higher-level cues, including syntactic struc-
ture, in the detection of speech repair. Some of this
work is inspired by Levelt?s (1983) investigation of
the syntactic and semantic variables in speech re-
pairs, particularly his well-formedness rule, which
states that the reparandum and alteration of a repair
typically have the same consitituent label, similar
to coordination.
Hale et al (2006) use Levelt?s well-formedness
rule to justify an annotation scheme where unfin-
ished categories (marked X-UNF) have the UNF
label appended to all ancestral category labels
all the way up to the top-most constituent be-
neath an EDITED label (EDITED labels denoting
a reparandum). They reason that this should pre-
vent grammar rules of finished constituents in the
corpus from corrupting the grammar of unfinished
constituents. While this annotation proves helpful,
it also leads to the unfortunate result that a large
reparandum requires several special repair rules to
be applied, even though the actual error is only
569
happening at one point.
Intuitively, though, it seems that an error is only
occurring at the end of the reparandum, and that
until that point only fluent grammar rules are be-
ing applied by the speaker. This intuition has also
been confirmed by empirical studies (Nakatani and
Hirschberg, 1994), which show that there is no ob-
vious error signal in speech up until the moment of
interruption. Although speakers may retrace much
of the reparandum for clarity or other reasons, ide-
ally the reparandum contains nothing but standard
grammatical rules until the speech is interrupted.1
Another recent approach to this problem (John-
son and Charniak, 2004) uses a tree-adjoining
grammar (TAG) approach to define a mapping be-
tween a source sentence possibly containing a re-
pair, and a target, fluent sentence. The use of
the TAG channel model is justified by the putative
crossing dependencies seen in repairs like . . . a
flight to Boston, uh, I mean, to Denver. . . where
there is repetition from the reparandum to the re-
pair. Essentially, this model is building up the
reparandum and alteration in tandem, based on
these crossing dependencies. While this is an inter-
esting model, it focuses on detection and removal
of EDITED sections, and subsequent parsing of
cleaned up speech. As such, it introduces chal-
lenges for integrating the system into a real-time
speech recognizer.
Recent work by Miller and Schuler (2008)
showed how a probabilistic grammar trained on
trees modified by use of the right-corner transform
can improve parsing accuracy over an unmodified
grammar when tested on the Switchboard corpus.
The approach described here builds on that work in
using right-corner transformed trees, and extends it
by mapping them to a time-series model to do pars-
ing directly in a model of the sort used in speech
recognition. This system is shown to be more ac-
curate than a baseline CYK parser when used to
parse the Switchboard corpus. The remainder of
this section will review the right-corner transform,
followed by Section 3, which will step through an
extended example giving details about the trans-
form process and its applicability to the problem
of processing speech repairs.
1One objection to this claim is the case of multiple nested
repairs. In this case, though, we presume that all reparanda
were originally intended by the speaker to be fluent at the time
of generation.
2.1 Right-corner transform
The right-corner transform rewrites syntax trees,
turning all right branching structure into left
branching structure, and leaving left branching
structure as is. As a result, constituent structure
can be explicitly built from the bottom up dur-
ing incremental recognition. This arrangement is
well-suited to recognition of speech with errors,
because it allows constituent structure to be built
up using fluent speech rules until the moment of
interruption, at which point a special repair rule
may be applied.
Before transforming the trees in the gram-
mar into right-corner trees, trees are binarized in
the same manner as Johnson (1998b) and Klein
and Manning (2003).2 Binarized trees are then
transformed into right-corner trees using trans-
form rules similar to those described by John-
son (1998a). In this transform, all right branch-
ing sequences in each tree are transformed into
left branching sequences of symbols of the form
A
1
/A
2
, denoting an incomplete instance of cate-
gory A
1
lacking an instance of category A
2
to the
right.3
Rewrite rules for the right-corner transform are
shown below, first flattening right-branching struc-
ture:
A
1
?
1
A
2
?
2
A
3
a
3
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
A
3
a
3
A
1
?
1
A
2
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
. . .
then replacing it with left-branching structure:
A
1
A
1
/A
2
:?
1
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
3
A
1
/A
2
:?
1
?
2
. . .
Here, the first two rewrite rules are applied iter-
atively (bottom-up on the tree) to flatten all right
branching structure, using incomplete constituents
2For the details of the particular binarization process used
here, see Miller and Schuler (2008).
3Here, all A
i
denote nonterminal symbols, and all ?
i
de-
note subtrees; the notation A
1
:?
1
indicates a subtree ?
1
with
label A
1
; and all rewrites are applied recursively, from leaves
to root.
570
to record the original nonterminal ordering. The
third rule is then applied to generate left branching
structure, preserving this ordering.
Because this process turns right expansion into
left expansion (leaving center expansion as the
only stack consumer), right-corner transformed
trees also require less stack memory than ordi-
nary phrase structure trees. This key property of
the right-corner transform is exploited in the map-
ping of transformed training trees to a time-series
model. This property will be examined further in
Section 5.
3 Speech Repair Example
A substantial example of a speech repair from the
Switchboard corpus can be seen in Figures 1 and 2,
in which the same repair and surrounding context
is shown after the preliminary binarization pro-
cess, and after the right-corner transform. Fig-
ure 1 also shows, in brackets, the augmented an-
notation described above from Hale et al (2006).
This scheme consisted of adding -X to an EDITED
label which produced a category X (daughter an-
notation), as well as propagating the -UNF label at
the right corner of the tree up through every parent
below the EDITED root.
3.1 Re-annotation of speech repair
Before applying the right-corner transform, some
corpus pre-processing steps are applied ? Fig-
ure 1 shows an example tree after these changes.
These steps aim to improve on the default annota-
tion of repair in the Switchboard corpus by mak-
ing the representation more in line with linguistic
as well as processing desiderata.
The standard annotation practice of having the
EDITED label as the category at the root of a
reparandum does not represent any proposed lin-
guistic phenomenon, and it conflates speech re-
pairs of different categories. As a result, the parser
is unable to make use of information about which
syntactic category the reparandum was originally
intended to be. This information would be useful
in the example discussed here, since an unfinished
NP is followed by a completed NP. The daughter
annotation used by Hale et al fixes this annota-
tion to allow the parser to have access to this infor-
mation. The annotation scheme in this paper also
makes use of the daughter annotation.
In addition, trees are modified to reduce the ar-
ity of speech repair rules, and to change the model
of how repairs occur. The default Switchboard
has, e.g. an NP reparandum (headed by EDITED)
and the NP alteration represented as siblings in the
syntax tree. This annotation seems to implicitly
model all repairs as stopping the process of pro-
ducing the current constituent, then starting a new
one. In contrast, the representation here models re-
pairs of this type as a speaker generating a partial
noun phrase, realizing an error, and then continu-
ing to generate the same noun phrase. This rep-
resentation scheme thus represents the beginning
of an effort to distinguish between repairs that are
changing the direction of the utterance and those
that are just fixing a local mistake.
3.2 Right-corner transform model of speech
repair
The right corner transform is then applied to this
example in Figure 2. The resulting tree represen-
tation is especially valuable because it models hu-
man production of speech repairs well, by not ap-
plying any special rule until the moment of inter-
ruption.
In the example in Figure 1, there is an unfinished
constituent (PP-UNF) at the end of the reparan-
dum. This standard annotation is deficient because
even if an unfinished consituent like PP-UNF is
correctly recognized, and the speaker is essentially
in an error state, there may be several partially
completed constituents above ? in Figure 1, the
NP, PP, and NP above the PP-UNF. These con-
stituents need to be completed, but using the stan-
dard annotation there is only one chance to make
use of the information about the error that has oc-
curred ? the ?NP ? NP PP-UNF? rule. Thus, by
the time the error section is completed, there is no
information by which a parsing algorithm could
choose to reduce the topmost NP to EDITED (or
EDITED-NP) other than independent rule proba-
bilities.
The approach used by Hale et al (2006) works
because the information about the transition to an
?error state? is propagated up the tree, in the form
of the -UNF tags. As the parsing chart is filled
in from the bottom up, each rule applied is essen-
tially coming out of a special repair rule set, and
so at the top of the tree the EDITED hypothesis is
much more likely. However, this requires that sev-
eral fluent speech rules from the data set be modi-
fied for use in a special repair grammar, which not
only reduces the amount of available training data,
571
SNP
CC
and
NP
EDITED-NP
NP
DT
the
NN
JJ
first
NN
kind
PP[-UNF]
IN
of
NP[-UNF]
NP
invasion
PP-UNF
of
NP
NP
DT
the
NN
JJ
first
NN
type
PP
IN
of
NP
privacy
VP
. . .
Figure 1: Binarized tree repair structure, with the -UNF propagation as in Hale et al (2006) shown in
brackets.
S/VP
S/VP
S/S
CC
and
NP
NP/NP
NP/PP
NP/NP
EDITED-NP
NP/PP
NP/NP
NP/PP
NP
NP/NN
NP/NN
DT
the
JJ
first
NN
kind
IN
of
NP
invasion
PP-UNF
of
NP
NP/NN
NP/NN
DT
the
JJ
first
NN
type
IN
of
NP
privacy
VBD
. . .
Figure 2: Right-corner transformed tree with repair structure
but violates our intuition that reparanda are usually
fluent up until the actual edit occurs.
The right corner transform works in a different
way, by building up constituent structure from left
to right. In Figure 2, the same repair is shown
as it appears in the training data for this system.
With this representation, the problem noticed by
Hale and colleagues has been solved in a different
way, by incrementally building up left-branching
rather than right-branching structure, so that only
a single special error rule is required at the end of
the constituent. As seen in the figure, all of the
structure beneath the EDITED-NP label is built us-
ing rules from the fluent grammar. It is only at
one point, when the PP-UNF is found, that a re-
pair rule is applied and the EDITED-NP section is
found. The next step in the process is that the NP
essentially restarts (the NP/NP label), and the sub-
sequent words start to build up what will be the NP
alteration in a fluent manner.
To summarize, while the -UNF propagation
scheme often requires the entire reparandum to
be generated from a speech repair rule set, this
scheme only requires one special rule, where the
moment of interruption actually occurred. This re-
duces the number of special speech repair rules
that need to be learned and saves more potential
examples of fluent speech rules, and therefore po-
tentially makes better use of limited data.
4 Mapping to an HHMM
This section describes how a corpus of trees trans-
formed as above can be mapped to a time-series
model called a Hierarchical Hidden Markov Model
(HHMM) in order to incorporate parsing into
speech decoding. This suggests that this approach
can be used in applications using streaming speech
input, unlike other parsing approaches which are
cubic time on input length at best, and require in-
put to be pre-segmented.
This section will begin by showing how HH-
MMs can model linguistic structure by extending
standard Hidden Markov Models (HMMs) used in
speech recognition, and will follow with a descrip-
tion of how right-corner transformed trees can be
mapped to this model topology.
572
4.1 Hierarchical HMMs
In general, the hidden state in an HMM can be as
simple or complex as necessary. This can include
factorizing the hidden state into any number of in-
terdependent random variables modeling the sub-
states of the complex hidden state. A Hierarchi-
cal Hidden Markov Model is essentially an HMM
with a specific factorization that is useful in many
domains ? the hidden state at each time step is
factored into d random variables which function as
a stack, and d additional boolean random variables
which regulate the operations of the stack through
time. The boolean random variables are typically
marginalized out when performing inference on a
sequence.
While the vertical direction of the hidden sub-
states (at a fixed t) represents a stack at a sin-
gle point in time, the horizontal direction of the
hidden sub-states (at a fixed d) can be viewed as
simple HMMs at depth d, taking direction from
the HMM above them and controlling those be-
low them. This interpretation will be useful when
formally defining the transitions between the stack
elements at different time steps below.
Formally, HMMs characterize speech or text as
a sequence of hidden states q
t
(which may con-
sist of speech sounds, words, and/or other hypoth-
esized syntactic or semantic information), and ob-
served states o
t
at corresponding time steps t (typ-
ically short, overlapping frames of an audio sig-
nal, or words or characters in a text processing
application). A most likely sequence of hidden
states q?
1..T
can then be hypothesized given any se-
quence of observed states o
1..T
, using Bayes? Law
(Equation 2) and Markov independence assump-
tions (Equation 3) to define a full P(q
1..T
| o
1..T
)
probability as the product of a Language Model
(?
L
) prior probability and an Observation Model
(?
O
) likelihood probability:
q?
1..T
= argmax
q
1..T
P(q
1..T
| o
1..T
) (1)
= argmax
q
1..T
P(q
1..T
) ? P(o
1..T
| q
1..T
) (2)
def
= argmax
q
1..T
T
?
t=1
P
?
L
(q
t
| q
t-1
)?P
?
O
(o
t
| q
t
) (3)
Language model transitions P
?
L
(q
t
| q
t?1
) over
complex hidden states q
t
can be modeled us-
ing synchronized levels of stacked-up compo-
nent HMMs in a Hierarchic Hidden Markov
Model (HHMM) (Murphy and Paskin, 2001).
HHMM transition probabilities are calculated in
two phases: a ?reduce? phase (resulting in an in-
termediate, marginalized state f
t
), in which com-
ponent HMMs may terminate; and a ?shift? phase
(resulting in a modeled state q
t
), in which untermi-
nated HMMs transition, and terminated HMMs are
re-initialized from their parent HMMs. Variables
over intermediate f
t
and modeled q
t
states are fac-
tored into sequences of depth-specific variables ?
one for each of D levels in the HMM hierarchy:
f
t
= ?f
1
t
. . . f
D
t
? (4)
q
t
= ?q
1
t
. . . q
D
t
? (5)
Transition probabilities are then calculated as a
product of transition probabilities at each level, us-
ing level-specific ?reduce? ?F and ?shift? ?Q mod-
els:
P
?
L
(q
t
|q
t-1
) =
?
f
t
P(f
t
|q
t-1
)?P(q
t
|f
t
q
t-1
) (6)
def
=
?
f
1..D
t
D
?
d=1
P
?F(f
d
t
| f
d+1
t
q
d
t-1
q
d-1
t-1
)?
P
?Q(q
d
t
|f
d+1
t
f
d
t
q
d
t-1
q
d-1
t
) (7)
with fD+1
t
and q0
t
defined as constants.
Shift and reduce probabilities are now defined
in terms of finitely recursive FSAs with probabil-
ity distributions over transition, recursive expan-
sion, and final-state status of states at each hier-
archy level. In simple HHMMs, each interme-
diate state variable is a boolean switching vari-
able fd
t
? {0,1} and each modeled state variable
is a syntactic, lexical, or phonetic state qd
t
. The
intermediate variable fd
t
is true (equal to 1) with
probability 1 if there is a transition at the level im-
mediately below d and the stack element qd
t?1
is a
final state, and false (equal to 0) with probability 1
otherwise:4
P
?F(f
d
t
| f
d+1
t
q
d
t?1
q
d?1
t?1
)
def
=
{
if fd+1
t
=0 : [f
d
t
=0]
if fd+1
t
=1 : P
?F-Reduce(f
d
t
| q
d
t?1
, q
d?1
t?1
)
(8)
where fD+1 = 1 and q0
t
= ROOT.
Shift probabilities at each level are defined us-
ing level-specific transition ?Q-Trans and expan-
4Here [?] is an indicator function: [?] = 1 if ? is true, 0
otherwise.
573
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9 t=10 t=11 t=12 t=13
and the
first
kind of
invasion
of the
first
type of
privacy
. . .
? ? ? ? ? ? ? ?
NP/NN
NP/NN
? ?
? ?
NP/NN
NP/NN
NP/PP
NP/NP
NP/PP
NP/NP
NP/NP
NP/NP
NP/PP
NP/NP
?
S/S S/S S/S S/S S/S S/S S/S S/S S/S S/S S/S
S/VP
Figure 3: Sample tree from Figure 2 mapped to qd
t
variable positions of an HHMM at each stack depth d
(vertical) and time step t (horizontal). Values for final-state variables fd
t
are not shown. Note that some
nonterminal labels have been omitted; labels for these nodes can be reconstructed from their children.
This includes the EDITED-NP nonterminal that occurs as a child of the NP/NP at t=8, d=2, indicated in
boldface.
sion ?Q-Expand models:
P
?Q(q
d
t
| f
d+1
t
f
d
t
q
d
t?1
q
d?1
t
)
def
=
?
?
?
if fd+1
t
=0, f
d
t
=0 : [q
d
t
= q
d
t?1
]
if fd+1
t
=1, f
d
t
=0 : P
?Q-Trans(q
d
t
| q
d
t?1
q
d?1
t
)
if fd+1
t
=1, f
d
t
=1 : P
?Q-Expand(q
d
t
| q
d?1
t
)
(9)
where fD+1 = 1 and q0
t
= ROOT. This model
is conditioned on final-state switching variables at
and immediately below the current HHMM level.
If there is no final state immediately below the
current level (the first case above), it determinis-
tically copies the current HHMM state forward to
the next time step. If there is a final state imme-
diately below the current level (the second case
above), it transitions the HHMM state at the cur-
rent level, according to the distribution ?Q-Trans.
And if the state at the current level is final (the
third case above), it re-initializes this state given
the state at the level above, according to the distri-
bution ?Q-Expand. The overall effect is that higher-
level HMMs are allowed to transition only when
lower-level HMMs terminate. An HHMM there-
fore behaves like a probabilistic implementation of
a pushdown automaton (or ?shift-reduce? parser)
with a finite stack, where the maximum stack depth
is equal to the number of levels in the HHMM hi-
erarchy.
4.2 Mapping trees to HHMM derivations
Any tree can now be mapped to an HHMM deriva-
tion by aligning the nonterminals with qd
t
cate-
gories. First, it is necessary to define rightward
depth d, right index position t, and final (right)
child status f , for every nonterminal node A in a
tree, where:
? d is defined to be the number of right branches
between node A and the root,
? t is defined to be the number of words beneath
or to the left of node A, and
? f is defined to be 0 if node A is a left (or
unary) child, 1 otherwise.
Any binary-branching tree can then be annotated
with these values and rewritten to define labels and
final-state values for every combination of d and t
covered by the tree. This process simply copies
stacked up constituents over multiple time steps,
while other constituents are being recognized. Co-
ordinates d, t ? D,T that are not covered by the
tree are assigned label ???, and f = 1. The result-
ing label and final-state values at each node now
define a value of qd
t
and fd
t+1
for each depth d and
time step t of the HHMM (see Figure 3). Prob-
abilities for HHMM models ?Q-Expand, ?Q-Trans,
and ?F-Reduce can then be estimated from these val-
ues directly. Like the right-corner transform, this
mapping is reversible, so q and f values can be
taken from a hypothesized most likely sequence
and mapped back to trees (which can then undergo
the reverse of the right-corner transform to become
ordinary phrase structure trees).
5 HHMM Application to Speech Repair
While the HHMM parser described above can pro-
duce the same output as a standard probablistic
CYK parser (the most likely parse tree), the differ-
ent parsing strategy of an HHMM parser and the
close connection of this system with a probabilis-
tic model of semantics present potential benefits to
the recognition of disfluent speech.
574
First, by using a depth-limited stack, this model
better adheres to psycholinguistically observed
short term memory limits that the human parser
and generator are likely to obey (Cowan, 2001;
Miller, 1956). The use of a depth-limited stack
is enabled by the right-corner transform?s prop-
erty of transforming right expansions to left ex-
pansions, which minimizes stack usage. Corpus
studies (Schuler et al, 2008) suggest that broad
coverage parsing can be achieved via this trans-
form using only four stack elements. In practical
terms this means that the model is less likely than
a standard CYK parser to spend time and probabil-
ity mass on analyses that conflict with the memory
limits humans appear to be constrained by when
generating and understanding speech.
Second, this model is part of a more general
framework that incorporates a model of referential
semantics into the parsing model of the HHMM
(Schuler et al, in press). While the framework
evaluated in this paper models only the syntactic
contribution to speech repair, there are some cases
where syntactic cues are not sufficient to distin-
guish disfluent from fluent utterances. In many
of these cases, semantic information is the only
way to tell that an utterance contains a repair.5 A
recognition system that incorporates referential se-
mantics with syntax should improve the accuracy
of speech repair recognition as it has been shown
to increase recognition of entities in fluent speech
recognition.
Finally, the semantic model just described, as
well as the mechanics of the HHMM parser on
a right-corner transformed grammar, combine to
form a model that accounts for two previously
distant aspects of speech processing: referential
semantics and speech repair. From the genera-
tive view of language processing, the model starts
with a desired referent, and based on that refer-
ent selects the appropriate syntactic structures, and
within those it selects the appropriate lexical items
to unambiguously describe the referent. In the se-
mantic sense, then, the model is operating in a
top-down fashion, with the referent being the driv-
ing force for the generation of syntax and words.
However, since the model is also working in a left-
5For example, the sentence ?The red. . . uh. . . blue box? is
more likely to be considered a repair in a context with sin-
gle colored boxes, whereas the sentence ?The big. . . uh. . . blue
box? is less likely to be considered a repair in the same con-
text, although the two sentences have the same syntactic struc-
ture.
to-right fashion on a highly left-branching gram-
mar, there is also a bottom-up composition of con-
stituents, which models the phenomenon of speech
repair naturally and accurately.
6 Evaluation
The evaluation of this system was performed on
the Switchboard corpus of transcribed conversa-
tional speech, using the mrg annotations in directo-
ries 2 and 3 for training, and the files sw4004.mrg
to sw4153.mrg in directory 4 for evaluation, fol-
lowing Johnson and Charniak (2004). In addition
to testing the HHMM parser on the Switchboard
corpus, the experiment testing a CYK parser from
Miller and Schuler (2008) was replicated, with
slightly better results due to a change in the evalu-
ation script6 and small changes in the binarization
process (both of these changes affect the baseline
and test systems).
The input to the system consists of the terminal
symbols from the trees in the corpus section men-
tioned above. The terminal symbol strings are first
pre-processed by stripping punctuation and empty
categories, which could not be expected from the
output of a speech recognizer. In addition, any in-
formation about repair is stripped from the input,
including partial words, repair symbols,7 and in-
terruption point information. While an integrated
system for processing and parsing speech may use
both acoustic and syntactic information to find re-
pairs, and thus may have access to some of this
information about where interruptions occur, this
testing paradigm is intended to evaluate the use of
the right-corner transform in a time-series model
on parsing speech repair. To make a fair compari-
son to the CYK baseline of Hale et al (2006), the
recognizer was given correct part-of-speech tags as
input along with words.
The results presented here use two standard met-
rics for assessing accuracy of transcribed speech
with repairs. The first metric, Parseval F-measure,
takes into account precision and recall of all non-
terminal (and non pre-terminal) constituents in a
hypothesized tree relative to the gold standard. The
second metric, EDIT-finding F, measures precision
and recall of the words tagged as EDITED in the
hypothesized tree relative to those tagged EDITED
6Specifically, we switched to using the evalb tool created
by Sekine and Collins (1997).
7The Switchboard corpus has special terminal symbols in-
dicating e.g. the start and end of the reparandum.
575
in the gold standard. F score is defined as usual,
2pr/(p + r) for precision p and recall r.
Table 1 below shows the results of experiments
using the model of speech repair described in this
paper. The ?Baseline? result shows the accuracy of
the binarized grammar at parsing the Switchboard
test set. The ?RCT? result shows the accuracy
of parsing when the right-corner transform is per-
formed on the trees in the training set prior to train-
ing. Finally, the ?HHMM+RCT? results shows the
accuracy of the HHMM parser system described
in this paper, trained on right-corner trees mapped
to the random variables at each time step. ?CYK?
and ?TAG? lines show relevant results from related
work.
System Parseval F EDIT F
Baseline 63.43 41.82
CYK (H06) 71.16 41.7
RCT 73.21 61.03
HHMM+RCT 77.15 68.03
TAG-based model (JC04) ? 79.7
Table 1: Summary of results.
These results show an improvement over the
standard CYK parsing algorithm, in both overall
parsing accuracy and EDIT-finding accuracy. This
shows that the HHMM parser, which is more ap-
plicable to speech input due to its asymptotic linear
time complexity, does not need to sacrifice any ac-
curacy to do so, and indeed improves on accuracy
for both metrics under consideration.
7 Conclusion
The work described here has extended previous
work for recognizing disfluent speech to an incre-
mental model, moving in a direction that holds
promise for eventual direct implementation in a
speech recognizer.
Extending this model to actual speech adds
some complexity, since disfluency phenomena are
difficult to detect in an audio signal. However,
there are also advantages in this extension, since
the extra phonological variables and acoustic ob-
servations contain information that can be useful
in the recognition of disfluency phenomena.
References
Cowan, Nelson. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Hale, John, Izhak Shafran, Lisa Yung, Bonnie Dorr,
Mary Harper, Anna Krasnyanskaya, Matthew Lease,
Yang Liu, Brian Roark, Matthew Snover, and Robin
Stewart. 2006. PCFGs with syntactic and prosodic
indicators of speech repairs. In Proceedings of the
45th Annual Conference of the Association for Com-
putational Linguistics (COLING-ACL).
Johnson, Mark and Eugene Charniak. 2004. A tag-
based noisy channel model of speech repairs. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ?04), pages
33?39, Barcelona, Spain.
Johnson, Mark. 1998a. Finite state approximation of
constraint-based grammars using left-corner gram-
mar transforms. In Proceedings of COLING/ACL,
pages 619?623.
Johnson, Mark. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Levelt, William J.M. 1983. Monitoring and self-repair
in speech. Cognition, 14:41?104.
Miller, Tim and William Schuler. 2008. A unified syn-
tactic model for parsing fluent and disfluent speech.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL ?08).
Miller, George A. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81?97.
Murphy, Kevin P. and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840.
Nakatani, C. and J. Hirschberg. 1994. A corpus-based
study of repair cues in spontaneous speech. The
Journal of the Acoustic Society of America, 95:1603?
1616.
Schuler, William, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2008. Toward a
psycholinguistically-motivated model of language.
In Proceedings of COLING, Manchester, UK.
Schuler, William, Stephen Wu, and Lane Schwartz. in
press. A framework for fast incremental interpre-
tation during speech decoding. Computational Lin-
guistics.
Sekine, Satoshi and Michael Collins. 1997. Evalb
bracket scoring program.
Shriberg, Elizabeth. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California at Berkeley.
576
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 785?792
Manchester, August 2008
Toward a Psycholinguistically-Motivated Model of Language Processing
William Schuler
Computer Science and Engineering
University of Minnesota
schuler@cs.umn.edu
Samir AbdelRahman
Department of Computer Science
Cairo University
s.abdelrahman@fci-cu.edu.eg
Tim Miller
Computer Science and Engineering
University of Minnesota
tmill@cs.umn.edu
Lane Schwartz
Computer Science and Engineering
University of Minnesota
lschwar@cs.umn.edu
Abstract
Psycholinguistic studies suggest a model
of human language processing that 1) per-
forms incremental interpretation of spo-
ken utterances or written text, 2) preserves
ambiguity by maintaining competing anal-
yses in parallel, and 3) operates within
a severely constrained short-term memory
store ? possibly constrained to as few
as four distinct elements. This paper de-
scribes a relatively simple model of lan-
guage as a factored statistical time-series
process that meets all three of the above
desiderata; and presents corpus evidence
that this model is sufficient to parse natu-
rally occurring sentences using human-like
bounds on memory.
1 Introduction
Psycholinguistic studies suggest a model of human
language processing with three important proper-
ties. First, eye-tracking studies (Tanenhaus et al,
1995; Brown-Schmidt et al, 2002) suggest that hu-
mans analyze sentences incrementally, assembling
and interpreting referential expressions even while
they are still being pronounced. Second, humans
appear to maintain competing analyses in paral-
lel, with eye gaze showing significant attention to
competitors (referents of words with similar pre-
fixes to the correct word), even relatively long af-
ter the end of the word has been encountered, when
attention to other distractor referents has fallen off
(Dahan and Gaskell, 2007). Preserving ambigu-
ity in a parallel, non-deterministic search like this
may account for human robustness to missing, un-
known, mispronounced, or misspelled words. Fi-
nally, studies of short-term memory capacity sug-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
gest human language processing operates within a
severely constrained short-term memory store ?
possibly restricted to as few as four distinct ele-
ments (Miller, 1956; Cowan, 2001).
The first two observations may be taken to
endorse existing probabilistic beam-search mod-
els which maintain multiple competing analyses,
pruned by contextual preferences and dead ends
(e.g. Roark, 2001). But the last observation on
memory bounds imposes a restriction that until
now has not been evaluated in a corpus study. Can
a simple, useful human-like processing model be
defined using these constraints? This paper de-
scribes a relatively simple model of language as a
factored statistical time-series process that meets
all three of the above desiderata; and presents
corpus evidence that this model is sufficient to
parse naturally occurring sentences using human-
like bounds on memory.
The remainder of this paper is organized as fol-
lows: Section 2 describes some current approaches
to incremental parsing; Section 3 describes a statis-
tical framework for parsing using a bounded stack
of explicit constituents; Section 4 describes an ex-
periment to estimate the level of coverage of the
Penn Treebank corpus that can be achieved with
various stack memory limits, using a set of re-
versible tree transforms, and gives accuracy results
of a bounded-memory model trained on this cor-
pus.
2 Background
Much work on cognitive modeling in psycholin-
guistics is centered on modeling the concepts to
which utterances refer. Coarsely, these concepts
may correspond to activation patterns among neu-
rons in specific regions of the brain. In some the-
ories, a short-term memory store of several unre-
lated concepts may be retained by organizing the
activation of these concepts into compatible pat-
terns, only a few of which can be reliably main-
785
tained (Smolensky and Legendre, 2006). Activa-
tion is then theorized to spread through and among
these groups of concepts in proportion to some
learned probability that the concepts will be rel-
evant (Anderson and Reder, 1999), with the most
active concepts corresponding to the most likely
linguistic analyses. Competition between rival ac-
tivated groups of concepts (corresponding to in-
complete linguistic analyses) has even been linked
to reading delays (Hale, 2003).
This competition among mutually-exclusive
variously-activated short term memory stores of
concepts, essentially a weighted disjunction over
conjunctions of concepts, can be modeled in lan-
guage understanding as simple Viterbi decoding of
a factored HMM-like time-series model (Schuler
et al, in press). In this model, concepts (corre-
sponding to vectors of individuals in a first-order
world model) are introduced and composed (via
set operations like intersection) in each hypothe-
sized short-term memory store, using the elements
of the memory store as a stack. These vectors of
individuals can be considered a special case of vec-
tors of concept elements proposed by Smolensky,
with set intersection a special case of tensor prod-
uct in the composition model. Referents in this
kind of incremental model can be constrained by
? but still distinguished from ? higher-level ref-
erents while they are still being recognized.
It is often assumed that this semantic con-
cept composition proceeds isomorphically with
the composition of syntactic constituents (Frege,
1892). This parallel semantic and syntactic com-
position is considered likely to be performed in
short-term memory because it has many of the
characteristics of short-term memory processes,
including nesting limits (Miller and Chomsky,
1963) and susceptibility to degradation due to in-
terruption. Ericsson and Kintch (1995) propose a
theory of long-term working memory that extends
short-term memory, but only for inter-sentential
references, which do seem to be retained across
interruptions in reading. But while the relation-
ship between competing probability distributions
in such a model and experimental reading times
has been evaluated (e.g. by Hale), the relationship
between the syntactic demands on a short-term
memory store and observations of human short-
term memory limits is still largely untested. Sev-
eral models have been proposed to perform syntac-
tic analysis using a bounded memory store.
For example, Marcus (1980) proposed a deter-
ministic parser with an explicit four-element work-
ing memory store in order to model human parsing
limitations. But this model only stores complete
constituents (whereas the model proposed in this
paper stores incompletely recognized constituents,
in keeping with the Tanenhaus et al findings). As
a result, the Marcus model relies on a suite of spe-
cialized memory operations to compose complete
constituents out of complete constituents, which
are not independently cognitively motivated.
Cascaded finite-state automata, as in FASTUS
(Hobbs et al, 1996), also make use of a bounded
stack, but stack levels in such systems are typically
dedicated to particular syntactic operations: e.g.
a word group level, a phrasal level, and a clausal
level. As a result, some amount of constituent
structure may overflow its dedicated level, and be
sacrificed (for example, prepositional phrase at-
tachment may be left underspecified).
Finite-state equivalent parsers (and thus,
bounded-stack parsers) have asymptotically linear
run time. Other parsers (Sagae and Lavie, 2005)
have achieved linear runtime complexity with
unbounded stacks in incremental parsing by
using a greedy strategy, pursuing locally most
probable shift or reduce operations, conditioned
on multiple surrounding words. But without an
explicit bounded stack it is difficult to connect
these models to concepts in a psycholinguistic
model.
Abney and Johnson (1991) explore left-corner
parsing as a memory model, but again only in
terms of (complete) syntactic constituents. The
approach explored here is similar, but the trans-
form is reversed to allow the recognizer to store
recognized structure rather than structures being
sought, and the transform is somewhat simpli-
fied to allow more structure to be introduced into
syntactic constituents, primarily motivated by a
need to keep track of disconnected semantic con-
cepts rather than syntactic categories. Without this
link to disconnected semantic concepts, the syntax
model would be susceptible to criticism that the
separate memory levels could be simply chunked
together through repeated use (Miller, 1956).
Roark?s (2001) top-down parser generates trees
incrementally in a transformed representation re-
lated to that used in this paper, but requires dis-
tributions to be maintained over entire trees rather
than stack configurations. This increases the beam
786
width necessary to avoid parse failure. Moreover,
although the system is conducting a beam search,
the objects in this beam are growing, so the recog-
nition complexity is not linear, and the connection
to a bounded short-term memory store of uncon-
nected concepts becomes somewhat complicated.
The model described in this paper is arguably
simpler than many of the models described above
in that it has no constituent-specific mechanisms,
yet it is able to recognize the rich syntactic struc-
tures found in the Penn Treebank, and is still
compatible with the psycholinguistic notion of a
bounded short-term memory store of conceptual
referents.
3 Bounded-Memory Parsing with a Time
Series Model
This section describes a basic statistical framework
? a factored time-series model ? for recogniz-
ing hierarchic structures using a bounded store of
memory elements, each with a finite number of
states, at each time step. Unlike simple FSA com-
pilation, this model maintains an explicit represen-
tation of active, incomplete phrase structure con-
stituents on a bounded stack, so it can be readily
extended with additional variables that depend on
syntax (e.g. to track hypothesized entities or rela-
tions). These incomplete constituents are related
to ordinary phrase structure annotations through a
series of bidirectional tree transforms. These trans-
forms:
1. binarize phrase structure trees into linguisti-
cally motivated head-modifier branches (de-
scribed in Section 3.1);
2. transform right-branching sequences to left-
branching sequences (described in Sec-
tion 3.2); and
3. align transformed trees to an array of random
variable values at each depth and time step of
a probabilistic time-series model (described
in Section 3.3).
Following these transforms, a model can be trained
from example trees, then run as a parser on unseen
sentences. The transforms can then be reversed to
evaluate the output of the parser. This representa-
tion will ultimately be used to evaluate the cover-
age of a bounded-memory model on a large corpus
of tree-annotated sentences, and to evaluate the ac-
curacy of a basic (unsmoothed, unlexicalized) im-
plementation of this model in Section 4.
It is important to note that these transformations
are not postulated to be part of the human recog-
nition process. In this model, sentences can be
recognized and interpreted entirely in right-corner
form. The transforms only serve to connect this
process to familiar representations of phrase struc-
ture.
3.1 Binary branching structure
This paper will attempt to draw conclusions about
the syntactic complexity of natural language, in
terms of stack memory requirements in incremen-
tal (left-to-right) recognition. These requirements
will be minimized by recognizing trees in a right-
corner form, which accounts partially recognized
phrases and clauses as incomplete constituents,
lacking one instance of another constituent yet to
come.
In particular, this study will use the trees in the
Penn Treebank Wall Street Journal (WSJ) corpus
(Marcus et al, 1994) as a data set. In order to
obtain a linguistically plausible right-corner trans-
form representation of incomplete constituents, the
corpus is subjected to another, pre-process trans-
form to introduce binary-branching nonterminal
projections, and fold empty categories into non-
terminal symbols in a manner similar to that pro-
posed by Johnson (1998b) and Klein and Manning
(2003). This binarization is done in such a way
as to preserve linguistic intuitions of head projec-
tion, so that the depth requirements of right-corner
transformed trees will be reasonable approxima-
tions to the working memory requirements of a hu-
man reader or listener.
3.2 Right-Corner Transform
Phrase structure trees are recognized in this frame-
work in a right-corner form that can be mapped to
and from ordinary phrase structure via reversible
transform rules, similar to those described by
Johnson (1998a). This transformed grammar con-
strains memory usage in left-to-right traversal to a
bound consistent with the psycholinguistic results
described above.
This right-corner transform is simply the left-
right dual of a left-corner transform (Johnson,
1998a). It transforms all right branching sequences
in a phrase structure tree into left branching se-
quences of symbols of the form A
1
/A
2
, denoting
an incomplete instance of category A
1
lacking an
instance of category A
2
to the right. These incom-
plete constituent categories have the same form
787
a) binarized phrase structure tree:
S
NP
NP
JJ
strong
NN
demand
PP
IN
for
NP
NPpos
NNP
NNP
new
NNP
NNP
york
NNP
city
POS
?s
NNS
JJ
general
NNS
NN
obligation
NNS
bonds
VP
VBN
VBN
propped
PRT
up
NP
DT
the
NN
JJ
municipal
NN
market
b) result of right-corner transform:
S
S/NN
S/NN
S/NP
S/VP
NP
NP/NNS
NP/NNS
NP/NNS
NP/NP
NP/PP
NP
NP/NN
JJ
strong
NN
demand
IN
for
NPpos
NPpos/POS
NNP
NNP/NNP
NNP/NNP
NNP
new
NNP
york
NNP
city
POS
?s
JJ
general
NN
obligation
NNS
bonds
VBN
VBN/PRT
VBN
propped
PRT
up
DT
the
JJ
municipal
NN
market
Figure 1: Trees resulting from a) a binarization of a sample phrase structure tree for the sentence Strong
demand for New York City?s general obligations bonds propped up the municipal market, and b) a right-
corner transform of this binarized tree.
and much of the same meaning as non-constituent
categories in a Combinatorial Categorial Grammar
(Steedman, 2000).
Rewrite rules for the right-corner transform are
shown below, first to flatten out right-branching
structure:1
1The tree transforms presented in this paper will be de-
fined in terms of destructive rewrite rules applied iteratively
to each constituent of a source tree, from leaves to root, and
from left to right among siblings, to derive a target tree. These
rewrites are ordered; when multiple rewrite rules apply to the
same constituent, the later rewrites are applied to the results
of the earlier ones. For example, the rewrite:
A
0
. . . A
1
?
2
?
3
. . .
?
A
0
. . . ?
2
?
3
. . .
could be used to iteratively eliminate all binary-branching
nonterminal nodes in a tree, except the root. In the notation
used in this paper, Roman uppercase letters (A
i
) are variables
matching constituent labels, Roman lowercase letters (a
i
) are
variables matching terminal symbols, Greek lowercase letters
A
1
?
1
A
2
?
2
A
3
a
3
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
A
3
a
3
A
1
?
1
A
2
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
. . .
then to replace it with left-branching structure:
(?
i
) are variables matching entire subtree structure, Roman
letters followed by colons, followed by Greek letters (A
i
:?
i
)
are variables matching the label and structure, respectively, of
the same subtree, and ellipses (. . . ) are taken to match zero
or more subtree structures, preserving the order of ellipses in
cases where there are more than one (as in the rewrite shown
above).
788
A1
A
1
/A
2
:?
1
A
2
/A
3
?
2
?
3
. . .
?
A
1
A
1
/A
3
A
1
/A
2
:?
1
?
2
?
3
. . .
Here, the first two rewrite rules are applied iter-
atively (bottom-up on the tree) to flatten all right
branching structure, using incomplete constituents
to record the original nonterminal ordering. The
third rule is then applied to generate left-branching
structure, preserving this ordering. Note that the
last rewrite above leaves a unary branch at the left-
most child of each flattened node. This preserves
the nodes at which the original tree was not right-
branching, so the original tree can be reconstructed
when the right-corner transform concatenates mul-
tiple right-branching sequences into a single left-
branching sequence.
An example of a right-corner transformed tree
is shown in Figure 1(b). An important property of
this transform is that it is reversible. Rewrite rules
for reversing a right-corner transform are simply
the converse of those shown above. The correct-
ness of this can be demonstrated by dividing a
tree into maximal sequences of right branches (that
is, maximal sequences of adjacent right children).
The first two ?flattening? rewrites of the right-
corner transform, applied to any such sequence,
will replace the right-branching nonterminal nodes
with a flat sequence of nodes labeled with slash
categories, which preserves the order of the non-
terminal category symbols in the original nodes.
Reversing this rewrite will therefore generate the
original sequence of nonterminal nodes. The final
rewrite similarly preserves the order of these non-
terminal symbols while grouping them from the
left to the right, so reversing this rewrite will re-
produce the original version of the flattened tree.
3.3 Hierarchic Hidden Markov Models
Right-corner transformed phrase structure trees
can then be mapped to random variable positions
in a Hierarchic Hidden Markov Model (Murphy
and Paskin, 2001), essentially a Hidden Markov
Model (HMM) factored into some fixed number of
stack levels at each time step.
HMMs characterize speech or text as a sequence
of hidden states q
t
(in this case, stacked-up syn-
tactic categories) and observed states o
t
(in this
case, words) at corresponding time steps t. A
most likely sequence of hidden states q?
1..T
can
then be hypothesized given any sequence of ob-
served states o
1..T
, using Bayes? Law (Equation 2)
and Markov independence assumptions (Equa-
tion 3) to define a full P(q
1..T
| o
1..T
) probabil-
ity as the product of a Transition Model (?
A
)
prior probability P(q
1..T
)
def
=
?
t
P
?
A
(q
t
| q
t-1
) and
an Observation Model (?
B
) likelihood probability
P(o
1..T
| q
1..T
)
def
=
?
t
P
?
B
(o
t
| q
t
):
q?
1..T
= argmax
q
1..T
P(q
1..T
| o
1..T
) (1)
= argmax
q
1..T
P(q
1..T
)?P(o
1..T
| q
1..T
) (2)
def
= argmax
q
1..T
T
?
t=1
P
?
A
(q
t
| q
t-1
)?P
?
B
(o
t
| q
t
) (3)
Transition probabilities P
?
A
(q
t
| q
t-1
) over com-
plex hidden states q
t
can be modeled using syn-
chronized levels of stacked-up component HMMs
in a Hierarchic Hidden Markov Model (HHMM)
(Murphy and Paskin, 2001). HHMM transition
probabilities are calculated in two phases: a re-
duce phase (resulting in an intermediate, marginal-
ized state f
t
), in which component HMMs may ter-
minate; and a shift phase (resulting in a modeled
state q
t
), in which unterminated HMMs transition,
and terminated HMMs are re-initialized from their
parent HMMs. Variables over intermediate f
t
and
modeled q
t
states are factored into sequences of
depth-specific variables ? one for each of D levels
in the HMM hierarchy:
f
t
= ?f
1
t
. . . f
D
t
? (4)
q
t
= ?q
1
t
. . . q
D
t
? (5)
Transition probabilities are then calculated as a
product of transition probabilities at each level, us-
ing level-specific reduce ?R and shift ?S models:
P
?
A
(q
t
|q
t-1
) =
?
f
t
P(f
t
|q
t-1
)?P(q
t
|f
t
q
t-1
) (6)
def
=
?
f
1..D
t
D
?
d=1
P
?R(f
d
t
|f
d+1
t
q
d
t-1
q
d-1
t-1
)?
P
?S(q
d
t
|f
d+1
t
f
d
t
q
d
t-1
q
d-1
t
) (7)
with fD+1
t
and q0
t
defined as constants. In Viterbi
decoding, the sums are replaced with argmax oper-
ators. This decoding process preserves ambiguity
by maintaining competing analyses of the entire
memory store. A graphical representation of an
HHMM with three levels is shown in Figure 3.
Shift and reduce probabilities can then be de-
fined in terms of finitely recursive Finite State Au-
tomata (FSAs) with probability distributions over
789
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9 t=10 t=11 t=12 t=13 t=14 t=15
strong
dem
and
for
new
york
city ?s
general
obligations
bonds
propped
up the
m
unicipal
m
arket
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ?
NNP/NNP
NNP/NNP
NPpos/POS
? ? ? ?
VBN/PRT
? ? ?
?
NP/NN
NP/PP
NP/NP
NP/NP
NP/NP
NP/NP
NP/NNS
NP/NNS
NP/NNS
S/VP
S/VP
S/NP
S/NN
S/NN
Figure 2: Sample tree from Figure 1 mapped to qd
t
variable positions of an HHMM at each stack depth
d (vertical) and time step t (horizontal). This tree uses only two levels of stack memory. Values for
final-state variables fd
t
are not shown. Note that some nonterminal labels have been omitted; labels for
these nodes can be reconstructed from their children.
transition, recursive expansion, and final-state sta-
tus of states at each hierarchy level. In simple HH-
MMs, each intermediate variable is a boolean vari-
able over final-state status fd
t
? {0,1} and each
modeled state variable is a syntactic, lexical, or
phonetic state qd
t
. The intermediate variable fd
t
is
true or false (equal to 1 or 0 respectively) accord-
ing to ?F-Reduce if there is a transition at the level
immediately below d, and false (equal to 0) with
probability 1 otherwise:2
P
?R(f
d
t
| f
d+1
t
q
d
t-1
q
d-1
t-1
)
def
=
{
if fd+1
t
=0 : [f
d
t
=0]
if fd+1
t
=1 : P
?F-Reduce(f
d
t
| q
d
t-1
, q
d-1
t-1
)
(8)
where fD+1
t
= 1 and q0
t
= ROOT.
Shift probabilities over the modeled variable qd
t
at each level are defined using level-specific tran-
sition ?Q-Trans and expansion ?Q-Expand models:
P
?S(q
d
t
| f
d+1
t
f
d
t
q
d
t-1
q
d-1
t
)
def
=
?
?
?
if fd+1
t
=0, f
d
t
=0 : [q
d
t
= q
d
t-1
]
if fd+1
t
=1, f
d
t
=0 : P
?Q-Trans(q
d
t
| q
d
t-1
q
d-1
t
)
if fd+1
t
=1, f
d
t
=1 : P
?Q-Expand(q
d
t
| q
d-1
t
)
(9)
where fD+1
t
= 1 and q0
t
= ROOT. This model
is conditioned on final-state switching variables at
and immediately below the current FSA level. If
there is no final state immediately below the cur-
rent level (the first case above), it deterministically
copies the current FSA state forward to the next
time step. If there is a final state immediately be-
low the current level (the second case above), it
2Here [?] is an indicator function: [?] = 1 if ? is true, 0
otherwise.
. . .
. . .
. . .
. . .
f
3
t?1
f
2
t?1
f
1
t?1
q
1
t?1
q
2
t?1
q
3
t?1
o
t?1
f
3
t
f
2
t
f
1
t
q
1
t
q
2
t
q
3
t
o
t
Figure 3: Graphical representation of a Hierarchic
Hidden Markov Model. Circles denote random
variables, and edges denote conditional dependen-
cies. Shaded circles are observations.
transitions the FSA state at the current level, ac-
cording to the distribution ?Q-Trans. And if the state
at the current level is final (the third case above),
it re-initializes this state given the state at the level
above, according to the distribution ?Q-Expand. The
overall effect is that higher-level FSAs are allowed
to transition only when lower-level FSAs termi-
nate. An HHMM therefore behaves like a prob-
abilistic implementation of a pushdown automaton
(or shift?reduce parser) with a finite stack, where
the maximum stack depth is equal to the number
of levels in the HHMM hierarchy.
Figure 2 shows the transformed tree from Fig-
ure 1 aligned to HHMM depth levels and time
steps. Because it uses a bounded stack, recognition
in this model is asymptotically linear (Murphy and
Paskin, 2001).
This model recognizes right-corner transformed
trees constrained to a stack depth corresponding to
observed human short term memory limits. This
790
HHMM depth limit sentences coverage
no memory 127 0.32%
1 memory element 3,496 8.78%
2 memory elements 25,909 65.05%
3 memory elements 38,902 97.67%
4 memory elements 39,816 99.96%
5 memory elements 39,832 100.00%
TOTAL 39,832 100.00%
Table 1: Percent coverage of right-corner trans-
formed treebank sections 2?21 with punctuation
omitted, using HHMMs with depth limits D from
zero to five.
is an attractive model of human language process-
ing because the incomplete syntactic constituents
it stores at each stack depth can be directly associ-
ated with (incomplete) semantic referents, e.g. by
adding random variables over environment or dis-
course referents at each depth and time step. If
these referents are calculated incrementally, recog-
nition decisions can be informed by the values of
these variables in an interactive model of language,
following Tanenhaus et al (1995). The corpus re-
sults described in the next section suggest that a
large majority of naturally occurring sentences can
be recognized using only three or four stack mem-
ory elements via this transform.
4 Empirical Results
In order to evaluate the coverage of this bounded-
memory model, Sections 2?21 of the Penn Tree-
bank WSJ corpus were transformed and mapped
to HHMM variables as described in Section 3.3. In
order to counter possible undesirable effects of an
arbitrary branching analysis of punctuation, punc-
tuation was removed. Coverage results on this cor-
pus are shown in Table 1.
Experiments training on transformed trees from
Sections 2?21 of the WSJ Treebank, evaluating
reversed-transformed output sequences from Sec-
tion 22 (development set) and Section 23 (test set),
show an accuracy (F score) of 82.1% and 80.1%
respectively.3 Although they are lower than those
for state-of-the-art parsers, these results suggest
that the bounded-memory parser described here is
doing a reasonably good job of modeling syntac-
tic dependencies, and therefore may have some
3Using unsmoothed relative frequency estimates from the
training set, a depth limit of D = 3, beam with of 2000, and
no lexicalization.
promise as a psycholinguistic model.
Although recognition in this system is linear, it
essentially works top-down, so it has larger run-
time constants than a bottom-up CKY-style parser.
The experimental system described above runs at
a rate of about 1 sentence per second on a 64-
bit 2.6GHz dual core desktop with a beam width
of 2000. In comparison, the Klein and Manning
(2003) CKY-style parser runs at about 5 sentences
per second on the same machine. On sentences
longer than 40 words, the HHMM and CKY-style
parsers are roughly equivalent, parsing at the rate
of .21 sentences per second, versus .24 for the
Klein and Manning CKY.
But since it is linear, the HHMM parser can be
directly integrated with end-of-sentence detection
(e.g. deciding whether ?.? is a sentence delimiter
based on whether the words preceding it can be
reduced as a sentence), or with n-gram language
models (if words are observations, this is simply
an autoregressive HMM topology). The use of
an explicit constituent structure in a time series
model also allows integration with models of dy-
namic phenomena such as semantics and corefer-
ence which may depend on constituency. Finally,
as a linear model, it can be directly applied to
speech recognition (essentially replacing the hid-
den layer of a conventional word-based HMM lan-
guage model).
5 Conclusion
This paper has described a basic incremental pars-
ing model that achieves worst-case linear time
complexity by enforcing fixed limits on a stack
of explicit (albeit incomplete) constituents. Ini-
tial results show a use of only three to four levels
of stack memory within this framework provides
nearly complete coverage of the large Penn Tree-
bank corpus.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their input. This research was
supported by National Science Foundation CA-
REER/PECASE award 0447685. The views ex-
pressed are not necessarily endorsed by the spon-
sors.
791
References
Abney, Steven P. and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
Anderson, J.R. and L.M. Reder. 1999. The fan effect:
New results and new theories. Journal of Experi-
mental Psychology: General, 128(2):186?197.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference res-
olution in the wild: Online circumscription of
referential domains in a natural interactive problem-
solving task. In Proceedings of the 24th Annual
Meeting of the Cognitive Science Society, pages
148?153, Fairfax, VA, August.
Cowan, Nelson. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Dahan, Delphine and M. Gareth Gaskell. 2007. The
temporal dynamics of ambiguity resolution: Evi-
dence from spoken-word recognition. Journal of
Memory and Language, 57(4):483?501.
Ericsson, K. Anders and Walter Kintsch. 1995.
Long-term working memory. Psychological Review,
102:211?245.
Frege, Gottlob. 1892. Uber sinn und bedeutung.
Zeitschrift fur Philosophie und Philosophischekritik,
100:25?50.
Hale, John. 2003. Grammar, Uncertainty and Sen-
tence Processing. Ph.D. thesis, Cognitive Science,
The Johns Hopkins University.
Hobbs, Jerry R., Douglas E. Appelt, John Bear,
David Israel, Megumi Kameyama, Mark Stickel, and
Mabry Tyson. 1996. Fastus: A cascaded finite-state
transducer for extracting information from natural-
language text. In Finite State Devices for Natural
Language Processing, pages 383?406. MIT Press,
Cambridge, MA.
Johnson, Mark. 1998a. Finite state approximation of
constraint-based grammars using left-corner gram-
mar transforms. In Proceedings of COLING/ACL,
pages 619?623.
Johnson, Mark. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Marcus, Mitch. 1980. A theory of syntactic recognition
for natural language. MIT Press.
Miller, George and Noam Chomsky. 1963. Finitary
models of language users. In Luce, R., R. Bush, and
E. Galanter, editors, Handbook of Mathematical Psy-
chology, volume 2, pages 419?491. John Wiley.
Miller, George A. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81?97.
Murphy, Kevin P. and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840.
Roark, Brian. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Sagae, Kenji and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies (IWPT?05).
Schuler, William, Stephen Wu, and Lane Schwartz. in
press. A framework for fast incremental interpre-
tation during speech decoding. Computational Lin-
guistics.
Smolensky, Paul and Ge?raldine Legendre. 2006.
The Harmonic Mind: From Neural Computation to
Optimality-Theoretic GrammarVolume I: Cognitive
Architecture. MIT Press.
Steedman, Mark. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Tanenhaus, Michael K., Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. Inte-
gration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
792
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 737?745,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Word Buffering Models for Improved Speech Repair Parsing?
Tim Miller
University of Minnesota ? Twin Cities
tmill@cs.umn.edu
Abstract
This paper describes a time-series model
for parsing transcribed speech containing
disfluencies. This model differs from pre-
vious parsers in its explicit modeling of a
buffer of recent words, which allows it to
recognize repairs more easily due to the
frequent overlap in words between errors
and their repairs. The parser implement-
ing this model is evaluated on the stan-
dard Switchboard transcribed speech pars-
ing task for overall parsing accuracy and
edited word detection.
1 Introduction
Speech repair is a phenomenon in spontaneous
speech where a speaker interrupts the flow of
speech (at what?s called the interruption point),
backtracks some number of words (the reparan-
dum), and continues the utterance with material
meant to replace the reparandum (the alteration).1
The utterance can be rendered syntactically cor-
rect by excising all the words that the speaker
skipped over when backtracking. Speech with re-
pair is difficult for machines to process because in
addition to detecting repair, a system must know
what words are meant to be excised, and parsing
systems must determine how to form a grammat-
ical structure out of the set of words comprising
both the error speech and the correct speech.
Recent approaches to syntactic modeling of
speech with repairs have shown that significant
gains in parsing accuracy can be achieved by mod-
eling the syntax of repairs (Hale et al, 2006;
Core and Schubert, 1999). In addition, others
have shown that a parser based on a time-series
model that explicitly represents the incomplete
?This research was supported by NSF CAREER award
0447685. The views expressed are not necessarily endorsed
by the sponsors .
1This terminology follows Shriberg (1994).
constituents in fluent and disfluent speech can also
improve parsing accuracy (Miller and Schuler,
2008). However, these parsing approaches are still
not as accurate at detecting reparanda as classifica-
tion systems which use a variety of features to de-
tect repairs (Charniak and Johnson, 2001; Johnson
and Charniak, 2004; Heeman and Allen, 1999).
One highly salient feature which classification
systems use to detect repair is the repetition of
words between the error and the repair. Johnson
and Charniak report that 60% of words in the al-
terations are copies of words in reparanda in the
Switchboard corpus. Typically, this information
is not available to a parser trained on context-free
grammars.
Meanwhile, psycholinguistic models suggest
that the human language system makes use of
buffers both to keep track of recent input (Bad-
deley et al, 1998) and to smooth out generation
(Levelt, 1989). These buffers are hypothesized
to contain representations of recent phonological
events, suggesting that there is a short window
where new input might be compared to recent in-
put. This could be represented as a buffer which
predicts or detects repeated input in certain con-
strained circumstances.
This paper describes a hybrid parsing sys-
tem operating on transcribed speech which com-
bines an incremental parser implemented as a
probabilistic time-series model, as in Miller and
Schuler, with a buffer of recent words meant to
loosely model something like a phonological loop,
which should better account for word repetition ef-
fects in speech repair.
2 Background
This work uses the Switchboard corpus (Godfrey
et al, 1992) for both training and testing. This
corpus contains transcribed and syntactically an-
notated conversations between human interlocu-
tors. The reparanda in speech repairs are ulti-
737
mately dominated by the EDITED label, and in
cases where the reparandum ends with an unfin-
ished constituent, the lowest constituent label is
augmented with the -UNF tag. These annotations
provide necessary but not sufficient information
for parsing speech with repairs, and thus many im-
provements in performing this task come as the re-
sult of modifying these annotations in the training
data.
As mentioned above, both Hale and colleagues
(2006) and Miller and Schuler (2008) showed
that speech repairs contain syntactic regularities,
which can improve the parsing of transcribed
speech with repairs when modeled properly. Hale
et al used ?daughter annotation?, which adds the
label of an EDITED node?s child to the EDITED
label itself, and ?-UNF propagation?, which la-
bels every node between an original -UNF node
and the EDITED with an -UNF tag. Miller and
Schuler used a ?right-corner transform? to convert
standard phrase structure trees of the Penn Tree-
bank into ?right-corner trees?, which have highly
left-branching structure and non-standard tree cat-
egories representing incomplete constituents be-
ing recognized. These trees can be mapped into a
fixed-depth Hierarchical Hidden Markov Model to
achieve improved parsing and reparandum-finding
results over standard CYK parsers.
Work by Johnson and Charniak (2004; 2001)
uses much of the same structure, but is not a pars-
ing approach per se. In earlier work, they used a
boosting algorithm using word identity and cate-
gory features to classify individual words as part
of a reparandum or not, and achieved very im-
pressive accuracy. More recent work uses a tree-
adjoining grammar (TAG) to model the overlap in
words and part-of-speech tags between reparan-
dum and alteration as context sensitive syntax
trees. A parser is then used to rank the multiple
outputs of the TAG model with reparandum words
removed.
Another approach that makes use of the corre-
spondence between words in the reparandum and
alteration is Heeman and Allen (1999). This ap-
proach uses several sources of evidence, including
word and POS correspondence, to predict repair
beginnings and correct them (by predicting how
far back they are intended to retrace). This model
includes random variables between words that cor-
respond to repair state, and in a repair state, allows
words in the reparandum to ?license? words in the
. . .
. . .
. . .
. . .
f
3
t?1
f
2
t?1
f
1
t?1
q
1
t?1
q
2
t?1
q
3
t?1
o
t?1
f
3
t
f
2
t
f
1
t
q
1
t
q
2
t
q
3
t
o
t
Figure 1: Graphical representation of the depen-
dency structure in a standard Hierarchic Hidden
Markov Model with D = 3 hidden levels that
can be used to parse syntax. Circles denote ran-
dom variables, and edges denote conditional de-
pendencies. Shaded circles denote variables with
observed values.
alteration with high probability, accounting for the
high percentage of copied words and POS tags be-
tween reparandum and alteration.
3 Model Description
This work is based on a standard Hierarchical Hid-
den Markov Model parser (Schuler, 2009), with
the addition of two new random variables for
tracking the state of speech repair. The HHMM
framework is a desirable starting point for this
work for two reasons: First, its definition in terms
of a graphical model makes it easy to think about
and to add new random variables. Second, the
HHMM parser operates incrementally in a left-to-
right fashion on word input, which allows this sys-
tem to run in a single pass, conditioning current
words on a hypothesized buffer and interruption
point variable. The incremental nature of this sys-
tem is a constraint that other systems are not bound
by, but makes this model more psycholinguisti-
cally plausible. In comparison, a CYK parsing
framework attempting to use the same probabilis-
tic model of word dependency between reparanda
and alterations would need to do a second pass af-
ter obtaining the most likely parses, in order to tell
if a particular word?s generation probability in a
specific parse is influenced by a recent repair.
The graphical model representation of this
framework is illustrated in Figures 1 and 4. The
original model, shown in Figure 1, has complex
variables Q and F broken down into several qd
t
and fd
t
for time step t and depth d. These ran-
738
dom variables will be explained shortly, but for
now suffice it to say that in this work they are un-
altered from the original HHMM parsing frame-
work, while those labeled I and B (Figure 4) are
additions specific to the system described in this
paper. This section will next describe the stan-
dard HHMM parsing framework, before describ-
ing how this work augments it.
3.1 Right-corner Transform
The HHMM parser consists of stacks of a fixed
depth, which contain hypotheses of constituents
that are being processed. In order to minimize
the number of stack levels needed in processing,
the phrase structure trees in the training set are
modified using a ?right-corner transform?, which
converts right expansion in trees to left expansion,
leaving heavily left-branching structure requiring
little depth. The right-corner transform used in
this paper is simply the left-right dual of a left-
corner transform (Johnson, 1998a).
The right-corner transform can be defined as
a recursive algorithm on phrase-structure trees in
Chomsky Normal Form (CNF). Trees are con-
verted to CNF first by binarizing using stan-
dard linguistically-motivated techniques (Klein
and Manning, 2003; Johnson, 1998b). Remaining
unbinarized structure is binarized in a brute force
fashion, creating right-branching structure by cre-
ating a single node which dominates the two right-
most children of a ?super-binary? tree, with the la-
bel being the concatenation of its children?s labels
(see Figure 2).
Taking this CNF phrase structure tree as input,
the right-corner transform algorithm keeps track
of two separate trees, the original and the new
right-corner tree it is building. This process be-
gins at the right-most preterminal of the original
tree, and works its way up along the right ?spine?,
while building its way down a corresponding left
spine of the new right-corner tree. The trees be-
low shows the first step of the algorithm, with the
tree on the left being disassembled, the tree on the
right being built from its parts, and the working
positions in the trees shown in bold.
A
B
b
X
Y:? Z
z
A
A/Z
?
Z
z
The bottom right corner of the original tree is
made the top right corner of the new tree, and the
left corner of the new tree is made the newworking
position and given a ?slash? category A/Z. The
?slash? category label A/Z represents a tree that
is the start of a constituent of type A that needs
a right-child of type Z in order to complete. The
new right-corner of the original tree is the parent
(X) of the previous right corner, and its subtree is
now added to the right-corner derivation:
A
B
b
X
Y:?
A
A/Z
A/X
?
Y:?
Z
z
After the first step, the subtrees moved over to
the right-corner tree may have more complex sub-
structure than a single word (in this case, ? rep-
resents that possibly complex structure). After be-
ing attached to the right-corner tree in the correct
place, the algorithm is recursively applied to that
now right-branching substructure.
Again, the left child is given a new slash cat-
egory: The ?active constituent? (the left side of a
slash category) is inherited from the root, and the
?awaited constituent? (the right side of a slash cat-
egory) is taken from the constituent label of the
right-corner it came from.
This algorithm proceeds iteratively up the right
spine of the original tree, moving structure to the
right-corner tree and recursively transforming it as
it is added. The final step occurs when the original
root (A in this case) is reduced to having a single
child, in which case its child is added as a child
of the leftmost current branch of the right-corner
tree, and it is transformed recursively.
Figures 2 and 3 show an example tree from
the Switchboard corpus before and after the right-
corner transform is applied.
739
SINTJ
so
INTJ S
INTJ
uh
S
NP
you
VP
VBP
live
PP
IN
in
NP
dallas
Figure 2: Input to the right-corner transform. This
tree also shows an example of the ?brute-force? bi-
narization done on super-binary branches that can-
not be otherwise be binarized with linguistically-
motivated rules.
S
S/NP
S/PP
S/VP
S/S
S/INTJ S
INTJ
so
INTJ
uh
NP
you
VBP
live
IN
in
NP
dallas
Figure 3: Right-corner transformed version of the
tree in Figure 2.
3.2 Hierarchical Hidden Markov Model
A Hierarchical Hidden Markov Model is essen-
tially an HMM with a specific factorization that
is useful in many domains ? the hidden state at
each time step is factored into d random variables
which function as a stack, and d additional ran-
dom variables which regulate the operations of the
stack through time. For the model of speech repair
presented here, an interruption point is identified
by one of these regulator variables firing earlier
than it would in fluent speech. This concept will
be formalized below. The stack regulating random
variables are typically marginalized out when per-
forming inference on a sequence.
While the vertical direction of the hidden sub-
states (at a fixed t) represents a stack at a sin-
gle point in time, the horizontal direction of the
hidden sub-states (at a fixed d) can be viewed as
a simple HMM at depth d, expanding the state
from the HMM above it across multiple time steps
and causing the HMM below it to expand its own
states. This interpretation will be useful when for-
mally defining the transitions between the stack el-
ements at different time steps below.
Formally, HMMs characterize speech or text as
a sequence of hidden states q
t
(which may con-
sist of speech sounds, words, and/or other hypoth-
esized syntactic or semantic information), and ob-
served states o
t
at corresponding time steps t (typ-
ically short, overlapping frames of an audio sig-
nal, or words or characters in a text processing
application). A most likely sequence of hidden
states q?
1..T
can then be hypothesized given any se-
quence of observed states o
1..T
, using Bayes? Law
(Equation 2) and Markov independence assump-
tions (Equation 3) to define a full P(q
1..T
| o
1..T
)
probability as the product of a Language Model
(?
L
) prior probability and an Observation Model
(?
O
) likelihood probability:
q?
1..T
= argmax
q
1..T
P(q
1..T
| o
1..T
) (1)
= argmax
q
1..T
P(q
1..T
) ? P(o
1..T
| q
1..T
) (2)
def
= argmax
q
1..T
T
?
t=1
P
?
L
(q
t
| q
t?1
)?P
?
O
(o
t
| q
t
)
(3)
Language model transitions P
?
L
(q
t
| q
t?1
) over
complex hidden states q
t
can be modeled us-
ing synchronized levels of stacked-up compo-
nent HMMs in a Hierarchic Hidden Markov
Model (HHMM) (Murphy and Paskin, 2001).
HHMM transition probabilities are calculated in
two phases: a ?reduce? phase (resulting in an in-
termediate, marginalized state f
t
), in which com-
ponent HMMs may terminate; and a ?shift? phase
(resulting in a modeled state q
t
), in which unter-
minated HMMs transition, and terminated HMMs
are re-initialized from their parent HMMs. Vari-
ables over intermediate f
t
and modeled q
t
states
are factored into sequences of depth-specific vari-
ables ? one for each of D levels in the HMM hi-
erarchy:
f
t
= ?f
1
t
. . . f
D
t
? (4)
q
t
= ?q
1
t
. . . q
D
t
? (5)
Transition probabilities are then calculated as a
product of transition probabilities at each level, us-
ing level-specific ?reduce? ?F and ?shift? ?Q mod-
740
els:
P
?
L
(q
t
|q
t?1
) =
?
f
t
P(f
t
|q
t?1
)?P(q
t
|f
t
q
t?1
) (6)
def
=
?
f
1..D
t
D
?
d=1
P
?F(f
d
t
| f
d+1
t
q
d
t?1
q
d?1
t?1
)?
P
?Q(q
d
t
|f
d+1
t
f
d
t
q
d
t?1
q
d?1
t
)
(7)
with fD+1
t
and q0
t
defined as constants.
Shift and reduce probabilities are now defined
in terms of finitely recursive FSAs with probabil-
ity distributions over transition, recursive expan-
sion, and final-state status of states at each hierar-
chy level. In the HHMM used in this paper, each
intermediate state variable is a reduction state vari-
able fd
t
? G ? {0,1} (where G is the set of all
nonterminal symbols from the original grammar),
representing a reduction to the final syntactic state
in G, a horizontal transition to a new awaited cate-
gory, or a top-down transition to a new active cat-
egory. Each modeled state variable is a syntactic
element (qd
t
? G ? G) with an active and awaited
category represented with the slash notation.
The intermediate variable fd
t
is probabilistically
determined given a reduction at the stack level be-
low, but is deterministically 0 in the case of a non-
reduction at the stack level below. 2
P
?F(f
d
t
| f
d+1
t
q
d
t?1
q
d?1
t?1
)
def
=
{
if fd+1
t
/? G : [f
d
t
=0]
if fd+1
t
? G : P
?F-Reduce(f
d
t
| q
d
t?1
, q
d?1
t?1
)
(8)
where fD+1 ? G and q0
t
= ROOT.
Shift probabilities at each level are defined
using level-specific transition ?Q-T and expan-
sion ?Q-E models:
P
?Q(q
d
t
| f
d+1
t
f
d
t
q
d
t?1
q
d?1
t
)
def
=
?
?
?
if fd+1
t
/?G, f
d
t
/?G : [q
d
t
= q
d
t?1
]
if fd+1
t
?G, f
d
t
/?G : P
?Q-T(q
d
t
| f
d+1
t
f
d
t
q
d
t?1
q
d?1
t
)
if fd+1
t
?G, f
d
t
?G : P
?Q-E(q
d
t
| q
d?1
t
)
(9)
where fD+1 ? G and q0
t
= ROOT. This model
is conditioned on final-state switching variables at
and immediately below the current HHMM level.
If there is no final state immediately below the cur-
rent level (the first case above), it deterministically
2Here [?] is an indicator function: [?] = 1 if ? is true, 0
otherwise.
copies the current HHMM state forward to the
next time step. If there is a final state immediately
below the current level (the second case above),
it transitions the HHMM state at the current level,
according to the distribution ?Q-T. And if the state
at the current level is final (the third case above), it
re-initializes this state given the state at the level
above, according to the distribution ?Q-E. The
overall effect is that higher-level HMMs are al-
lowed to transition only when lower-level HMMs
terminate. An HHMM therefore behaves like a
probabilistic implementation of a pushdown au-
tomaton (or ?shift-reduce? parser) with a finite
stack, where the maximum stack depth is equal to
the number of levels in the HHMM hierarchy.
All of the probability distributions defined
above can be estimated by training on a corpus of
right-corner transformed trees, by mapping tree el-
ements onto the random variables in the HHMM
and computing conditional probability tables at
each random variable. This process is described in
more detail in other work (Schuler et al, in press).
3.3 Interruption Point and Word Buffer
This paper expands upon this standard HHMM
parsing model by adding two new sub-models to
the hidden variables described above, an interrup-
tion point (I) variable, and a word buffer (B) .
This model is illustrated in Figure 4, which takes
Figure 1 as a starting point and adds random vari-
ables just mentioned.
Buffers are hypothesized to be used in the hu-
man language system to smooth out delivery of
speech (Levelt, 1989). In this work, a buffer of
that sort is placed between the syntax generating
elements and the observed evidence (words). Its
role in this model is not to smooth the flow of
speech, but to keep a short memory that enables
the speaker to conveniently and helpfully restart
when a repair is produced. This in turn gives as-
sistance to a listener trying to understand what the
speaker is saying, since the listener also has the
last few words in memory.
The I variable implements a state machine that
keeps track of the repair status at each time point.
The domain of this variable is {0,1,ET}, where
1 indicates the first word of an alteration, ET in-
dicates editing terms in between reparandum and
alteration, and 0 indicating no repair.3
3Actually, 0 can occur during an alteration, but in those
cases that fact is indicated by the state of the buffer.
741
f3
t?2
f
2
t?2
f
1
t?2
q
1
t?2
q
2
t?2
q
3
t?2
o
t?2
Q
t?2
F
t?2
i
t?2
b
t?2
f
3
t?1
f
2
t?1
f
1
t?1
q
1
t?1
q
2
t?1
q
3
t?1
o
t?1
Q
t?1
F
t?1
i
t?1
b
t?1
f
3
t
f
2
t
f
1
t
q
1
t
q
2
t
q
3
t
o
t
Q
t
F
t
i
t
b
t
Figure 4: Extended HHMM parsing model with variables for interruption points (I) and a modeled word
buffer (B). Arrows within and between complex hidden variables F andQ have been removed for clarity.
The value of I is deterministically constrained
in this work by its inputs, but it can be conceived
as a conditional probability P(i
t
| i
t?1
, q
t
, q
t?1
, r
t
)
to allow footholds for future research.4 While
depending formally on many values, in practice
its dependencies are highly context-dependent and
constrained:
P(i
t
| i
t?1
, q
t
, q
t?1
, q
t
)
def
=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if i
t?1
=1 : [i
t
=0]
if i
t?1
=ET ? (INTJ ? PRN) ? q
t
: [i
t
=ET]
if i
t?1
=ET : [i
t
=1]
if i
t?1
=0 ? EDITED ? (q
t?1
? f
t
)
?(INTJ ? PRN) ? q
t
: [i
t
=ET]
if i
t?1
=0 ? EDITED ? (q
t?1
? f
t
) : [i
t
=1]
if i
t?1
=0 : [i
t
=0]
These conditions are meant to be evaluated in
a short-circuiting fashion, i.e., the first condition
which is true starting from the top is applied. The
default (last) case is most common, going from
non-repair to non-repair state. When the syntax
generated something with the category EDITED
at the last time step (as evidenced by either the
modeled state variable q
t?1
or the reduction state
variable f
t
depending on the length of the reparan-
dum), the interruption point variable is triggered to
change, either to ET if an interjection (INTJ) or
4Most obviously, this variable could be made prior to its
conditions to be their cause, if a suitable model for the causa-
tion of interruption points was designed using prosodic cues.
For this work, it is simply an intermediary that is not strictly
necessary but makes the model design more intuitive.
parenthetical (PRN) followed, otherwise to 1 for
the first word of an alteration. The ET state con-
tinues as long as the syntax at the current level is
generating something containing INTJ or PRN.
The random variable for the word buffer is more
complex, containing at each time step t an integer
index for keeping track of a current position in the
buffer (c
t
? ?0, 1, . . . , n? 1? for buffer size n),
and an array of several recently generated words
(~w
t
). This can be represented as the following con-
ditional probability:
P(b
t
| b
t?1
, i
t
, q
t
) = P(c
t
| c
t?1
, i
t
)?
P(~w
t
| ~w
t?1
, c
t
) (10)
The operation of the buffer is governed by four
cases:
Case 1: During normal operation (i.e. for fluent
speech), the interruption point variable is 0 and
at the previous time step the buffer index points
at the end of the buffer (i
t
=0 ? c
t?1
=n?1). In
this simple case, the buffer pointer remains point-
ing at the end position in the buffer (c
t
=n? 1),
and the last n? 1 items in the buffer are determin-
istically copied backwards one position. A new
word is generated probabilistically to occupy the
last position in the buffer (where c
t
is pointing).
This probability is estimated empirically using the
same model used in a standard HHMM to gener-
ate words, by conditioning the word on the deepest
non-empty q
t
value in the stack.
Case 2: When an editing term is being gener-
ated, (i
t
=ET), the buffer is not in use. Practi-
742
cally, this means that the value of the index c and
all wj are just copied over from time t?1 to time
t. This makes sense psycholinguistically, because
a buffer used to smooth speech rates would by def-
inition not be used when speech is interrupted by
a repair. It also makes sense from a purely engi-
neering point of view, since words used as editing
terms are usually stock phrases and filled pauses
that are not likely to have much predictive value
for the alteration, and are thus not worth keeping in
the buffer. The probability of the actual observed
word is modeled the same way word probabilities
are modeled in a standard HHMM, conditioned on
the deepest non-empty q
t
value, and ignoring the
buffer.
Case 3: The alteration case applies to the first
word after the reparandum and optional editing
terms (i
t
=1). In this case, the index c
t
for the cur-
rent position of the buffer is obtained by subtract-
ing a number of words to replace, with that num-
ber drawn from a prior distribution. This distribu-
tion is based on the function f(k) = 1.22 ? 0.45k.
This function was taken from Shriberg (1996),
where it was estimated based on several differ-
ent training corpora, and provided a remarkable
fit to all of them. Since this model uses a fixed
size buffer, the values are precomputed and renor-
malized to form a probability distribution. With
a buffer size of only n = 4, approximately 96%
of the probability mass of the original function is
accounted for.
After the indices are computed, the buffer at po-
sition c
t
is given a word value. The model first
decides whether to substitute or copy the previous
word over. The probability governing this decision
is also determined empirically, by computing how
often the first word in a alteration in the Switch-
board training set is a copy of the first word it is
meant to replace. If the copy operation is selected,
the word is added to the buffer without further di-
luting its probability. If, however, the substitution
operation was selected, the word is added to the
buffer with probability distributed across all pos-
sible words.
Case 4: The final case to account for
is alterations of length greater than one
(i
t
=0 ? c
t?1
6= n?1). This occurs when the
current index was moved back more than one
position, and so even though i is set to 0, the
current index into the buffer is not pointing at the
end. In this case, again the index c
t
is selected
according to a prior probability distribution. The
value selected from the distribution corresponds
to different actions that may be selected when
retracing the words in the reparandum to generate
the alteration.
The first option is that the current index remains
in place, which corresponds to an insertion oper-
ation, where the alteration is given an extra word
relative to the reparandum at its current position.
Following an insertion, a new word is generated
and placed in the buffer at the current index, with
probability conditioned on the syntax at the most
recent time step. The second option is to continue
the alignment, moving the current index forward
one position in the buffer, and then either perform-
ing a substitution or copy operation in alignment
with a word from the alteration. Word probabil-
ities for the copy and substitution operations are
generated in the same way as for the first word of
an alteration. Finally, the current index may skip
forward more than one value, performing a dele-
tion operation. Deletion skips over words in the
reparandum that do not correspond to words in the
alteration. After the deletion moves the current in-
dex pointer forward, a word is again either copied
or substituted against the newly aligned word.
The prior probability distributions over align-
ment operations is estimated from data in the
Switchboard in a similar manner to Johnson and
Charniak (2004). Briefly, using the disfluency-
annotated section of the Switchboard corpus (.dps
files), a list of reparanda and alterations corre-
sponding to one another are compiled. For each
pair, the minimal cost alignment is computed,
where a copy operation has cost 0, substitution
has cost 4, and deletion and insertion each have
cost 7. Using these alignments, probabilities are
computed using relative frequency counts for both
the first word of an alteration, and for subsequent
operations. Copy and substitution are the most fre-
quent operations (copying gives information about
the repair itself, while substitution can correct the
reason for the error), insertion is somewhat less
frequent (presumably for specifying further infor-
mation), and deletion is relatively rare (usually a
repair is not made to remove information).
4 Evaluation
This model was evaluated on the Switchboard
corpus (Godfrey et al, 1992) of conversational
telephone speech between two human interlocu-
743
System Precision Recall F-Score
Plain CYK 18.01 17.73 17.87
Hale et al CYK 40.90 35.41 37.96
Hale et al Lex. n/a n/a 70.0
TAG 82.0 77.8 79.7
Plain HHMM 43.90 47.36 45.57
HHMM-Back 44.12 57.49 49.93
HHMM-Retrace 48.82 59.41 53.59
Table 1: Table of results of edit-finding accuracy.
Italics indicate reported, rather than reproduced,
results.
System Configuration Parseval-F Edited-F
Plain CYK 71.03 17.9
Hale et al CYK 68.47 37.96
Hale et al Lex. 80.16 70.0
Plain HHMM 74.23 45.57
HHMM-Back 74.58 49.93
HHMM-Retrace 74.23 53.59
Table 2: Table of parsing results.
tors. The input to this system is the gold standard
word transcriptions, segmented into individual ut-
terances. The standard train/test breakdown was
used, with sections 2 and 3 used for training, and
subsections 0 and 1 of section 4 used for testing.
Several held-out sentences from the end of section
4 were used during development.
For training, the data set was first standardized
by removing punctuation, empty categories, ty-
pos, all categories representing repair structure,
and partial words ? anything that would be diffi-
cult or impossible to obtain reliably with a speech
recognizer.
The two metrics used here are the standard Par-
seval F-measure, and Edit-finding F. The first takes
the F-score of labeled precision and recall of the
non-terminals in a hypothesized tree relative to the
gold standard tree. The second measure marks
words in the gold standard as edited if they are
dominated by a node labeled EDITED, and mea-
sures the F-score of the hypothesized edited words
relative to the gold standard.
Results are shown in Tables 1 and 2. Table 1
shows detailed results on edited word finding, with
two test systems and several related approaches.
The first two lines show results from a re-
implementation of Hale et al parsers. In both
those cases, gold standard part-of-speech (POS)
tags were supplied to the parser. The follow-
ing two lines are reported results of a lexicalized
parser from Hale et al and the TAG system of
Johnson and Charniak. The final three lines are
evaluations of HHMM systems. The first is an
implementation of Miller and Schuler, run with-
out gold standard POS tags as input. The second
HHMM result is a systemmuch like that described
in this paper, but designed to approximate the best
result that can come from simply trying to match
the first word of an alteration with a recent word.
Levelt (1989) notes that in over 90% of repairs, the
first word of the alteration is either identical or a
member of the same category as the first word of
the reparandum, and this clue is enough for listen-
ers to understand what the alteration is meant to
replace. This implementation keeps the I variable
to model repair state, but rather than a modeled
buffer being part of the hidden state, it keeps an
observed buffer that simply tracks the last n words
seen (n = 4 in this experiment). This buffer is
used only to generate the first word of a repair, and
only when the syntactic state allows the word. Fi-
nally, the system described in Section 3 is shown
on the final line.
Table 2 shows overall parsing accuracy results,
with the same set of systems, with the exception
of the TAG system which did not report parsing
results.
5 Discussion and Conclusion
These results first show that the main contribution
of this paper, a model for a buffer of recent words
which influences speech repairs, results in drastic
improvements in the ability of an HHMM system
to discover edited words. This model does this in
a single pass through the observed words, incre-
mentally forming hypotheses about the state of the
syntactic process as well as the state of repair, just
as humans must recognize spontaneous speech.
Another interesting result is the relative effec-
tiveness of a buffer that is not modeled, but rather
just a collection of words used to condition the first
words of repair (?HHMM-Back?). While this re-
sult is superior to the plain HHMM system, it still
falls well short of the retracing model using a mod-
eled buffer. This suggests that, though one word
is sufficient to align a reparandum and alteration
when the existence of a repair is given, more in-
formation is often necessary when the task is not
just alignment of repair but also detection of re-
744
pair. A model that takes into account information
sources that identify the existence of repair, such
as prosodic cues (Hale et al, 2006; Lickley, 1996),
may thus result in improved performance for the
simpler unmodeled buffer.
These results also confirm that parsing sponta-
neous speech with an HHMM can be far superior
to a CKY parser, even when the CKY parser is
given the advantage of correct POS tags as input.
Second, even the baseline HHMM system also
improves over the CYK parser in finding edited
words, again without the advantage of correct POS
tags as input.
In conclusion, the model described here uses a
buffer inspired by the phonological loop used in
the human auditory system to keep a short mem-
ory of recent input. This model, when used to as-
sist in the detection and correction of repair, re-
sults in a large increase in accuracy in detection
of repair over other most basic parsing systems.
This system does not reach the performance lev-
els of lexicalized parsers, nor multi-pass classifi-
cation systems. Future work will explore ways to
apply additional features of these systems or other
sources of information to account for the remain-
der of the performance gap.
References
Alan Baddeley, Susan Gathercole, and Costanza Pa-
pagno. 1998. The phonological loop as a language
learning device. Psychological Review, 105(1):158?
173, January.
Eugene Charniak and Mark Johnson. 2001. Edit de-
tection and parsing for transcribed speech. In 2nd
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 118?
126.
Mark G. Core and Lenhart K. Schubert. 1999. A syn-
tactic framework for speech repairs and other disrup-
tions. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics (ACL
99).
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proc. ICASSP,
pages 517?520.
John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr,
Mary Harper, Anna Krasnyanskaya, Matthew Lease,
Yang Liu, Brian Roark, Matthew Snover, and Robin
Stewart. 2006. PCFGs with syntactic and prosodic
indicators of speech repairs. In Proceedings of the
45th Annual Conference of the Association for Com-
putational Linguistics (COLING-ACL).
Peter A. Heeman and James F. Allen. 1999. Speech
repairs, intonational phrases, and discourse markers:
Modeling speakers? utterances in spoken dialogue.
Computational Linguistics, 25:527?571.
Mark Johnson and Eugene Charniak. 2004. A tag-
based noisy channel model of speech repairs. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL ?04),
pages 33?39, Barcelona, Spain.
Mark Johnson. 1998a. Finite state approximation of
constraint-based grammars using left-corner gram-
mar transforms. In Proceedings of COLING/ACL,
pages 619?623.
Mark Johnson. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Willem J.M. Levelt. 1989. Speaking: From Intention
to Articulation. MIT Press.
R. J. Lickley. 1996. Juncture cues to disfluency. In
Proceedings of The Fourth International Conference
on Spoken Language Processing (ICSLP ?96), pages
2478?2481.
Tim Miller and William Schuler. 2008. A syntac-
tic time-series model for parsing fluent and dis-
fluent speech. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING?08).
Kevin P. Murphy and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840.
William Schuler, Samir AbdelRahman, TimMiller, and
Lane Schwartz. in press. Broad-coverage incremen-
tal parsing using human-like memory constraints.
Computational Linguistics.
William Schuler. 2009. Parsing with a bounded
stack using a model-based right-corner transform.
In Proceedings of the North American Association
for Computational Linguistics (NAACL ?09), Boul-
der, Colorado.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California at Berkeley.
Elizabeth Shriberg. 1996. Disfluencies in Switch-
board. In Proceedings of International Conference
on Spoken Language Processing.
745
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 656?664,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improved Syntactic Models for Parsing Speech with Repairs?
Tim Miller
Department of Computer Science and Engineering
University of Minnesota, Twin Cities
tmill@cs.umn.edu
Abstract
This paper introduces three new syntactic
models for representing speech with repairs.
These models are developed to test the intu-
ition that the erroneous parts of speech repairs
(reparanda) are not generated or recognized as
such while occurring, but only after they have
been corrected. Thus, they are designed to
minimize the differences in grammar rule ap-
plications between fluent and disfluent speech
containing similar structure. The three models
considered in this paper are also designed to
isolate the mechanism of impact, by systemat-
ically exploring different variables.
1 Introduction
Recent work in recognition of speech with repairs
has shown that syntactic cues to speech repair can
improve both overall parsing accuracy and detection
of repaired sections (Hale et al, 2006; Miller and
Schuler, 2008; Johnson and Charniak, 2004). These
techniques work by explictly modeling the structure
of speech repair, specifically the tendency of repairs
to follow unfinished constituents of the same cate-
gory. This is the essence of what was termed the
well-formedness rule by Willem Levelt (1983) in his
psycholinguistic studies of repair.
The work presented here uses the same motiva-
tions as those cited above (to be described in more
detail below), in that it attempts to model the syn-
tactic structure relating unfinished erroneous con-
?This research was supported by NSF CAREER award
0447685. The views expressed are not necessarily endorsed by
the sponsors.
stituents to the repair of those constituents. How-
ever, this work attempts to improve on those mod-
els by focusing on the generative process used by a
speaker in creating the repair. This is done first by
eschewing any labels representing the presence of
an erroneous constituent while processing the text.
This modeling representation reflects the intuition
that speakers do not intend to generate erroneous
speech ? they intend their speech to be fluent, or
a correction to an error, and can stop very quickly
when an error is noticed. This corresponds to Lev-
elt?s Main Interruption Rule, which states that a
speaker will ?Stop the flow of speech immediately
upon detecting the occasion of repair.? Rather than
attempting to recognize a special syntactic category
called EDITED during the processing phase, this
work introduces the REPAIRED category to signal
the ending of a repaired section only.
The second part of the modeling framework is
the use of a right-corner transform on training data,
which converts phrase-structure trees into heavily
left-branching structures. This transformation has
been shown to represent the structure of unfinished
constituents like those seen in speech repair in a nat-
ural way, leading to improved detection of speech
repair (Miller and Schuler, 2008).
Combining these two modeling techniques in a
bottom-up parsing framework results in a parsing
architecture that is a reasonable approximation to
the sequential processing that must be done by the
human speech processor when recognizing spoken
language with repairs. This parser also recognizes
sentences containing speech repair with better accu-
racy than the previous models on which it is based.
656
Therefore, these syntactic models hold promise for
integration into systems for processing of streaming
speech.
1.1 Speech Repair Terminology
A speech repair occurs when a speaker decides to
interrupt the flow of speech and restart part or all
of an utterance. Typically speech repair structure
(Shriberg, 1994) is considered to contain a reparan-
dum, or the part of the utterance to be replaced, and
an alteration, which is meant to replace the reparan-
dum section. There are also frequently editing terms
(for example, ?uh? and ?um?) between the reparan-
dum and alteration, which may be used to signal the
repair, or to indicate that the speaker is thinking, or
just to maintain control of the dialogue.
1.2 Related Work
This work is related to that of Hale et al(2006) in
that it attempts to model the syntactic structure of
speech repair. In that paper speech repair detec-
tion accuracy was increased by explicitly account-
ing for the relation between reparanda category and
alteration category. This was done by so-called
?daughter annotation,? which expanded the set of
EDITED categories by appending the category be-
low the EDITED label to the end of the EDITED
label ? for example, a noun phrase (NP) reparanda
would be of type EDITED-NP. In addition, this ap-
proach made edit detection easier by propagating the
-UNF label attached to the rightmost unfinished con-
stituent up to the EDITED label. These two changes
in combination allow the parser to better recognize
when a reparandum has occurred, and to make sib-
lings of reparanda and alterations with the same ba-
sic category label.
Another model of speech repair that explicitly
models the structure of speech repair is that of John-
son and Charniak (2004). That model has a differ-
ent approach than the context-free parsing approach
done in the present work. Instead, they run a tree-
adjoining grammar (TAG) parser which traces the
overlapping words and part-of-speech tags that oc-
cur in the reparandum and alteration of a speech re-
pair. This approach is highly accurate at detecting
speech repairs, and allows for downstream process-
ing of cleaned up text to be largely free of speech
repair, but due to its TAG component it may present
difficulties incorporating into an architecture that
operates on streaming text or speech.
This work is also similar in aim to a component of
the parsing and language modeling work of Roark
and Johnson (1999), which used right-binarization
in order to delay decision-making about constituents
as much as possible. For example, the rule
NP ? DT NN
might be right-binarized as two rules:
NP ? DT NP -DT
and
NP -DT ? NN
The result of this binarization is that when predicting
the noun phrase (NP) rule, a top-down parser is de-
laying making any commitments about the category
following the determiner (DT). This delay in predic-
tion means that the parser does not need to make
any predictions about whether the next word will
be, e.g., a common noun (NN), plural noun (NNS),
or proper noun (NNP), until it sees the actual next
word.
Similarly, the model presented in this work aims
to delay the decision to create a speech repair as
much as possible. This is done here by eliminating
the EDITED category (representing a reparandum)
during processing, replacing it with a REPAIRED
category which represents the alteration of a speech
repair, and by eliminating implicit cues about repair
happening before a decision to repair should be nec-
essary.
Finally, this work is most directly related to that
of Miller and Schuler (2008). In that work, the au-
thors used a right-corner transform to turn standard
phrase-structure trees into highly left-branching
trees with sub-tree category labels representing in-
complete but in-progress constituent structure. That
structure was shown to have desirable properties in
the representation of repair in syntax trees, and this
work leverages that insight, while attempting to im-
prove the input representation such that the right-
corner representation does not require the parser to
make any assumptions or decisions earlier than nec-
essary.
657
2 Syntactic Model
This section will first describe the default represen-
tation scheme for speech repair in the Switchboard
corpus and the standard representation after applica-
tion of a right-corner transform, and then describe
why there are shortcomings in both of these repre-
sentations. Descriptions of several alternative mod-
els follow, with an explanation of how each of them
is meant to address the shortcomings seen in previ-
ous representations. These models are then evalu-
ated in Section 3.
2.1 Standard Repair Annotation
The standard representation of speech repair in the
Switchboard corpus makes use of one new category
label (EDITED), to represent a reparandum, and a
new dash-tag (-UNF), representing the lowest unfin-
ished constituent in a phrase. An example tree with
both EDITED and -UNF tags is shown in Figure 1.
SBAR
WHNP-2
DT
that
S
EDITED
S
NP-SBJ
PRP
you
VP-UNF
MD
could
NP-SBJ
PRP
you
VP
MD
could
VP
VB
use
PP-PRP
IN
for
NP
NN
landfill
Figure 1: A fragment of a standard phrase-structure tree
from the development set, containing both an EDITED
constituent and an -UNF tag.
This sentence contains a restarted sentence (S)
constituent, in which the speaker started by saying
?you could?, then decided to restart the phrase, in
this case without changing the first two words. One
important thing to notice is that the EDITED label
contains no information about the structure beneath
it. As a result, a parser trained on this default anno-
tation has no information about the attempted con-
stituent type, which, in the case of restarts would ob-
viously be beneficial. As described above, the work
by Hale et al using daughter annotation was meant
to overcome this shortcoming.
Another shortcoming of this annotation scheme
to consider is that the EDITED tag is not meaning-
ful with respect to constituent structure. Attempt-
ing to learn from this structure, for example a prob-
abilistic context-free grammar, will result in the rule
that a sentence (S) consists of a reparandum, a noun
phrase, and a verb phrase, which is an odd way of
thinking about both constituent structure and mean-
ing. A more intuitive understanding might be that a
sentence may consist of a noun phrase followed by a
verb phrase, and during the production of that rule,
an interruption may occur which causes the rule to
restart.
2.2 Right-Corner Transform
The work described above by Miller and Schuler
(2008) uses a right-corner transform. This transform
turns right-branching structure into left-branching
structure, using category labels that use a ?slash? no-
tation ?/? to represent an incomplete constituent of
type ? ?looking for? a constituent of type ? in order
to complete itself. Figure 2 shows the right-corner
transformed tree from above.
This transform first requires that trees be bina-
rized. This binarization is done in a similar way to
Johnson (1998) and Klein and Manning (2003).
Rewrite rules for the right-corner transform are as
follows, first flattening right-branching structure:1
A1
?1 A2
?2 A3
a3
?
A1
A1/A2
?1
A2/A3
?2
A3
a3
(1)
A1
?1 A2
A2/A3
?2
. . .
?
A1
A1/A2
?1
A2/A3
?2
. . . (2)
then replacing it with left-branching structure:
1Here, all Ai denote nonterminal symbols, and ?i denote
subtrees ; the notation A1:?0 indicates a subtree ?0 with la-
bel A1; and all rewrites are applied recursively, from leaves to
root. In trees containing repairs, the symbol ET represents any
number of editing terms and the sub-structure within them.
658
SS/NP
S/PP
S/VP
S/VP
S/S
S/S
? ? ? WHNP
that
EDITED-S
S/VP
NP
you
VP-UNF
could
NP
you
MD
could
VB
use
IN
for
NP
landfill
Figure 2: Right-corner transformed tree fragment.
A1
A1/A2:?1 A2/A3
?2
?3 . . . ?
A1
A1/A3
A1/ A2:?1 ?2
?3 . . .
(3)
This representation has interesting properties,
which work well for speech repair. First, the left-
branching structure of a repair results in reparanda
that only require one special repair rule application,
at the last word in the reparandum. Second, the ex-
plicit representation of incomplete constituents al-
lows many reparanda to seamlessly integrate with
the rest of the parse tree, with the EDITED label
essentially acting as an instruction to the parser to
maintain the current position in the unfinished con-
stituent. This subtle second point is illustrated in the
tree in Figure 2. After the EDITED section is de-
tected, it combines with a category label S/S to form
another sub-tree with category label S/S, essentially
acting as a null op in a state machine looking to com-
plete a phrase of type S.
This representation also contains problems, how-
ever. First, note that the (bottom-up) parser uses one
set of rules to combine the reparandum with the cur-
rent state of the recognition, and another set of rules
when combining the alteration with the previous in-
put. While it is a benefit of this approach that both
rule sets are made up of fluent speech rules, their
way of combining nonetheless requires an early pre-
monition of the repair to occur. If anything, the re-
pair should require special rule applications, but in
this representation it is still the case that the reparan-
dum looks different and the alteration looks ?nor-
mal.?
A better model of repair from a recognition per-
spective would recognize the reparandum as flu-
ent, since they are recognized as such in real time,
and then, when noticing the repeated words, declare
these new words to be a repair section, and retroac-
tively declare the original start of the phrase to be
a reparandum. It is this conception of a recognition
model that forms part of the basis for a new syntactic
model of speech repair in Section 2.3.
A second problem with this representation is ev-
ident in certain multi-word repairs such as the one
in Figure 2 that require an extra right branch off of
the main left branching structure of the tree. As a
result, a multi-word reparandum structure requires
an extra unary rule application at the left-corner of
the sub-tree, in this case S/VP, relative to the inline
structure of the fluent version of that phrase. This
extra rule will often be nearly deterministic, but in
some cases it may not be, which would result essen-
tially in a penalty for starting speech repairs. This
may act to discourage short repairs and incentivize
longer reparanda, across which the penalty would
be amortized. This incentive is exactly backwards,
since reparanda tend to be quite short.
The next section will show how the two issues
mentioned above can be resolved by making mod-
659
ifications to the original structure of trees containing
repairs.
2.3 Modified Repair Annotation
The main model introduced in this paper works by
turning the original repair into a right-branching
structure as much as possible. As a result, the
right-corner transformed representation has very flat
structure, and, unlike the standard right-corner trans-
formed representation described above, does not re-
quire a second level of depth in the tree with differ-
ent rule applications. This can also be an important
consideration for speech, since there are parsers that
can operate in asymptotically linear time by using
bounded stacks, and flat tree structure minimizes the
amount of stack space required.
This model works by using an ?interruption?
model for the way a repair begins. The interrup-
tion model works on restarted constituents, by mov-
ing the repaired constituent (the alteration) to be
the right-most child of the original EDITED con-
stituent. The EDITED label is then removed, and
a new REPAIRED label is added. This of course
makes the detection of EDITED sections possible
only retrospectively, by noting a REPAIRED section
of a certain syntactic category, and tracing back in
the tree to find the closest ancestor of the same cate-
gory.
This can be illustrated schematically by the fol-
lowing rewrite rule:
A0
EDITED
A1
?0 A2
?1
ET
. . .
A1:?2
?
A0
A1
?0 A2
?1 REPAIRED-A1
ET
. . .
A1:?2
(4)
Figure 3 shows how the example tree from Fig-
ure 1 looks when transformed in this manner. The
result of these transformations may appear odd, but
it is important to note that it is merely an intermedi-
ate stage between the ?standard? representation with
an EDITED label, representing the post-recognition
understanding of the sentence, and the right-corner
representation in which recognition actually occurs.
This right-corner representation can be seen in Fig-
ure 2.3.
This representation is notable in that it looks ex-
actly the same after the first word of the repair
(?you?) as the later incarnation of the same word in
the alteration. After the second word (?could?), the
repair is initiated, and here a repair rule is initiated.
It should be noted, however, that strictly speaking
the only reason the REPAIRED category needs to
exist is to keep track of edits for the purpose of eval-
uating the parser. It serves only a processing pur-
pose, telling the parser to reset what it is looking for
in the incoming word stream.
WHSBAR
WHNP
DT
that
S
NP
PRP
you
VP
MD
could
REPAIRED-S
S
NP
PRP
you
VP
MD
could
VP
VB
use
PP
IN
for
NP
NN
landfill
Figure 3: REPAIRED-INT transformation
The next model attempts to examine the im-
pact of two different factors in the REPAIRED-INT
representation above. That representation had the
side effect of creating special rules off of the alter-
ation (REPAIRED) node, and it is difficult to as-
sign praise or blame to the performance results of
that model without distinguishing the main modi-
fication from the side effects. This can be recti-
fied by proposing another model that similarly elim-
inates the EDITED label for reparanda, and uses
a new label REPAIRED for the alteration, but that
660
SS/NP
S/PP
S/VP
S/VP
S/REPAIRED-S
S/VP
S/S
WHNP
that
NP
you
MD
could
NP
you
MD
could
VB
use
IN
for
NP
landfill
Figure 4: REPAIRED-INT + right-corner transformation
does not satisfy the desire to have reparanda occur
inline using the ?normal? rule combinations. This
model does, however, still have special rules that
the REPAIRED label will generate. Thus, if this
model performs equally well (or equally as poorly)
as REPAIRED-INT, then it is likely due to the model
picking up strong signals about an alteration rule
set. This modification involves rewriting the origi-
nal phrase structure tree as follows:
A0
EDITED
A1:?0
ET
. . .
A1:?1 ?
A0
A1
A1:?0 ET
. . .
REPAIRED-A1
A1:?1
(5)
A tree with this annotation scheme can be seen in
Figure 5, and its right-corner counterpart is shown
in Figure 6.
The final modification to examine acts effectively
as another control to the previous two annotation
schemes. The two modifications above are essen-
tially performing two operations, first acting to bina-
rize speech repairs by lumping a category of type X
with a category of type EDITED-X, and then explic-
itly marking the repair but not the reparandum. This
modification tests whether simply adding an extra
layer of structure can improve performance while re-
taining the standard speech repair annotation includ-
ing the EDITED category label. This modification
will be denoted EDITED-BIN.
EDITED-BIN trees are created using the follow-
ing rewrite rule:
WHSBAR
WHNP
DT
that
S
S
NP
PRP
you
VP-UNF
MD
could
REPAIRED-S
NP
PRP
you
VP
MD
could
VP
VB
use
PP
IN
for
NP
NN
landfill
Figure 5: REPAIRED-BIN transformation
S
S/NP
S/PP
S/VP
S/VP
S/REPAIRED-S
S/S
WHNP
that
S
S/VP
NP
you
VP-UNF
could
NP
you
MD
could
VB
use
IN
for
NP
landfill
Figure 6: REPAIRED-BIN + right-corner transformation
A0
EDITED
A1:?0
ET
. . .
A1:?1 ?
A0
A1
EDITED-A1
A1:?0
ET
. . .
A1:?1
(6)
After this transform, the tree would look identical
to the REPAIRED-BIN tree in Figure 5, except the
node labeled ?REPAIRED-S? is labeled ?S?, and its
left sibling is labeled ?EDITED-S? instead of ?S.?
An EDITED-BIN tree after right-corner transforma-
tions is shown in Figure 7. This explicit binariza-
tion of speech repairs may be effective in its own
right, because without it, a ?brute force? binariza-
tion must be done to format the tree before apply-
ing the right-corner transform, and that process in-
661
volves joining chains of categories with underscores
into right-branching super-categories. This process
can result in reparanda categories in unpredictable
places in the middle of lengthy super-categories,
making data sparse and less reliable.
S
S/NP
S/PP
S/VP
S/VP
S/S
S/S
WHNP
that
EDITED-S
S/VP
NP
you
VP-UNF
could
NP
you
MD
could
VB
use
IN
for
NP
landfill
Figure 7: EDITED-BIN + right-corner transformation
3 Evaluation
The evaluation of this model was performed using a
probabilistic CYK parser2. This parser operates in
a bottom-up fashion, building up constituent struc-
ture from the words it is given as input. This parsing
architecture is a good match for the structure gen-
erated by the right-corner transform because it does
not need to consider any categories related to speech
repair until the repaired section has been completed.
Moreover, the structure of the trees means that the
parser is also building up structure from left to right.
That mode of operation is useful for any model
which purports to be potentially extensible to speech
recognition or to model the human speech proces-
sor. In contrast, top-down parsers require exhaustive
searches, meaning that they need to explore interpre-
tations containing disfluency, even in the absence of
syntactic cues for its existence.
These experiments used the Switchboard corpus
(Godfrey et al, 1992), a syntactically-annotated cor-
pus of spontaneous dialogues between human inter-
locutors. This corpus is annotated for phrase struc-
ture in much the same way as the Penn Treebank
2The specific parser used is the Stanford parser described in
Klein and Manning(2003), but run in ?vanilla PCFG? mode.
Wall Street Journal corpus, with the addition of sev-
eral speech-specific categories as described in Sec-
tion 2.1. For training, trees in sections 2 and 3 of
this corpus were transformed as described in Sec-
tion 2, and rule probabilities were estimated in the
usual way. For testing, trees in section 4, subsec-
tions 0 and 1, were used. Data from the tail end of
section 4 (subsections 3 and 4) was used during de-
velopment of this work.
Before doing any training or testing, all trees in
the data set were stripped of punctuation, empty
categories, typos, all categories representing repair
structure, and partial words ? anything that would
be difficult or impossible to obtain reliably with
a speech recognizer. A baseline parser was then
trained and tested using the split described above,
achieving standard results as seen in the table be-
low. For a fair comparison to the evaluation in Hale
et al (2006), the parser was given part-of-speech
tags along with each word as input. The structure
obtained by the parser was then in the right-corner
format. For standardized scoring, the right-corner
transform, binarization, and augmented repair anno-
tation were undone, so that comparison was done
against the nearly pristine test corpus. Several test
configurations were then evaluated, and compared
to three baseline approaches.
The two metrics used here are the standard Parse-
val F-measure, and Edit-finding F. The first takes the
F-score of labeled precision and recall of the non-
terminals in a hypothesized tree relative to the gold
standard tree. The second measure marks words in
the gold standard as edited if they are dominated by
a node labeled EDITED, and measures the F-score
of the hypothesized edited words relative to the gold
standard (recall in this case is percentage of actual
edited words that were hypothesized as edited, and
precision is percentage of hypothesized edited words
that were actually edited).
The first three lines in the table refer to baseline
approaches to compare against. ?Plain? refers to a
configuration with no modifications other than the
removal of repair cues. The next result shown is a
reproducton of the results from Hale et al (2006)
(described in section 1.2)3. The next line (?Standard
3The present work compares to the standard CYK parsing
result from that paper, and not the result from a heavily opti-
mized parser using lexicalization.
662
Right Corner?) is a reproduction of the results from
Miller and Schuler (2008).
The following three lines contain the three ex-
perimental configurations. First, the configuration
denoted EDITED-BIN refers to the simple bina-
rized speech repair described in Section 2.3 (Equa-
tion 6). REPAIRED-BIN refers to the binarized
speech repair in which the labels are basically re-
versed from EDITED-BIN (Equation 5). Finally,
REPAIRED-INT refers to the speech repair type
where the REPAIRED category may be a child of
a non-identity category, representing an interruption
of the outermost desired constituent (Equation 4).
System Configuration Parseval-F Edited-F
Baseline 71.03 17.9
Hale et al 68.47?? 37.9??
Standard Right Corner 71.21?? 30.6??
EDITED-BIN 69.77?? ?? 38.9?? ??
REPAIRED-BIN 71.37? 31.6?? ??
REPAIRED-INT 71.77?? 39.2?? ??
Table 1: Table of parsing results. Star (?) indicates sig-
nificance relative to the ?Standard Right Corner? baseline
(p < 0.05), dagger (?) indicates significance relative to
the ?Baseline? labeled result (p < 0.05). Double star and
dagger indicate highly significant results (p < 0.001).
Significance results were obtained by perform-
ing a two-tailed paired Student?s t-test on both the
Parseval-F and Edit-F per-sentence results. This
methodology is not perfect, since it fails to account
for the ease of recognition of very short sentences
(which are common in a speech corpus like Switch-
board), and thus slightly underweights performance
on longer sentences. This is also the explanation
for the odd effect where the ?REPAIRED-BIN? and
?REPAIRED-INT? results achieve significance over
the ?Standard Right Corner? result, but not over the
?Baseline? result. However, the simplest alternative
? weighting each sentence by its length ? is probably
worse, since it makes the distributions being com-
pared in the t-test broadly distributed collections of
unlike objects, and thus hard to interpret meaning-
fully.
These results show a statistically significant im-
provement over previous work in overall parsing ac-
curacy, and obvious (as well as statistically signif-
icant) gains in accuracy recognizing edited words
(reparanda) with a parser. The REPAIRED-INT
approach, which makes repair structure even more
highly left-branching than the standard right-corner
transform, proved to be the most accurate approach.
The superior performance according to the EDIT-
F metric by REPAIRED-INT over REPAIRED-BIN
suggests that the improvement of REPAIRED-INT
over a baseline is not due simply to a new category.
The EDITED-BIN approach, while lowering overall
accuracy slightly, does almost as well on EDITED-F
as REPAIRED-INT, despite having a very different
representation of repair. This suggests that there are
elements of repair that this modification recognizes
that the others do not. This possibility will be ex-
plored in future work.
Another note of interest regards the recovery of
reparanda in the REPAIRED-INT case. As men-
tioned in Section 2.3, the EDITED section can be
found by tracing upwards in the tree from a RE-
PAIRED node of a certain type, to find an non-
repaired ancestor of the same type. This makes an
assumption that repairs are always maximally local,
which probably does not hurt accuracy, since most
repairs actually are quite short. However, this as-
sumption is obviously not true in the general case,
since in Figure 3 for example, the repair could trace
all the way back to the S label at the root of the tree
in the case of a restarted sentence. It is even possible
that this implicit incentive to short repairs is respon-
sible for some of the accuracy gains by discounting
long repairs. In any case, future work will attempt to
maintain the motivation behind the REPAIRED-INT
modification while relaxing hard assumptions about
repair distance.
4 Conclusion
This paper introduced three potential syntactic rep-
resentations for speech with repairs, based on the
idea that errors are not recognized as such until a
correction is begun. The main result is a new rep-
resentation, REPAIRED-INT, which, when trans-
formed via the right-corner transform, makes a very
attractive model for speech with repairs. This rep-
resentation leads to a parser that improves on other
parsing approaches in both overall parsing accu-
racy and accuracy recognizing words that have been
edited.
663
References
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech corpus
for research and development. In Proc. ICASSP, pages
517?520.
John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr, Mary
Harper, Anna Krasnyanskaya, Matthew Lease, Yang
Liu, Brian Roark, Matthew Snover, and Robin Stew-
art. 2006. PCFGs with syntactic and prosodic indica-
tors of speech repairs. In Proceedings of the 45th An-
nual Conference of the Association for Computational
Linguistics (COLING-ACL).
Mark Johnson and Eugene Charniak. 2004. A tag-based
noisy channel model of speech repairs. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL ?04), pages 33?
39, Barcelona, Spain.
Mark Johnson. 1998. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430.
Willem J.M. Levelt. 1983. Monitoring and self-repair in
speech. Cognition, 14:41?104.
Tim Miller and William Schuler. 2008. A unified syn-
tactic model for parsing fluent and disfluent speech. In
Proceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?08).
Brian Roark and Mark Johnson. 1999. Efficient proba-
bilistic top-down and left-corner parsing. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics (ACL 99).
Elizabeth Shriberg. 1994. Preliminaries to a Theory of
Speech Disfluencies. Ph.D. thesis, University of Cali-
fornia at Berkeley.
664
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 105?108,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Unified Syntactic Model for Parsing Fluent and Disfluent Speech?
Tim Miller
University of Minnesota
tmill@cs.umn.edu
William Schuler
University of Minnesota
schuler@cs.umn.edu
Abstract
This paper describes a syntactic representation
for modeling speech repairs. This representa-
tion makes use of a right corner transform of
syntax trees to produce a tree representation
in which speech repairs require very few spe-
cial syntax rules, making better use of training
data. PCFGs trained on syntax trees using this
model achieve high accuracy on the standard
Switchboard parsing task.
1 Introduction
Speech repairs occur when a speaker makes a mis-
take and decides to partially retrace an utterance in
order to correct it. Speech repairs are common in
spontaneous speech ? one study found 30% of dia-
logue turns contained repairs (Carletta et al, 1993)
and another study found one repair every 4.8 sec-
onds (Blackmer and Mitton, 1991). Because of the
relatively high frequency of this phenomenon, spon-
taneous speech recognition systems will need to be
able to deal with repairs to achieve high levels of
accuracy.
The speech repair terminology used here follows
that of Shriberg (1994). A speech repair consists of
a reparandum, an interruption point, and the alter-
ation. The reparandum contains the words that the
speaker means to replace, including both words that
are in error and words that will be retraced. The in-
terruption point is the point in time where the stream
of speech is actually stopped, and the repairing of
the mistake can begin. The alteration contains the
?This research was supported by NSF CAREER award
0447685. The views expressed are not necessarily endorsed by
the sponsors.
words that are meant to replace the words in the
reparandum.
Recent advances in recognizing spontaneous
speech with repairs (Hale et al, 2006; Johnson and
Charniak, 2004) have used parsing approaches on
transcribed speech to account for the structure in-
herent in speech repairs at the word level and above.
One salient aspect of structure is the fact that there
is often a good deal of overlap in words between
the reparandum and the alteration, as speakers may
trace back several words when restarting after an er-
ror. For instance, in the repair . . . a flight to Boston,
uh, I mean, to Denver on Friday . . . , there is an exact
match of the word ?to? between reparandum and re-
pair, and a part of speech match between the words
?Boston? and ?Denver?.
Another sort of structure in repair is what Lev-
elt (1983) called the well-formedness rule. This
rule states that the constituent started in the reparan-
dum and repair are ultimately of syntactic types that
could be grammatically joined by a conjunction. For
example, in the repair above, the well-formedness
rule says that the repair is well formed if the frag-
ment . . . a flight to Boston and to Denver. . . is gram-
matical. In this case the repair is well formed since
the conjunction is grammatical, if not meaningful.
The approach described here makes use of a trans-
form on a tree-annotated corpus to build a syntactic
model of speech repair which takes advantage of the
structure of speech repairs as described above, while
also providing a representation of repair structure
that more closely adheres to intuitions about what
happens when speakers make repairs.
105
2 Speech repair representation
The representational scheme used for this work
makes use of a right-corner transform, a way of
rewriting syntax trees that turns all right recursion
into left recursion, and leaves left recursion as is.
As a result, constituent structure is built up dur-
ing recognition in a left-to-right fashion, as words
are read in. This arrangement is well-suited to
recognition of speech with repairs, because it al-
lows for constituent structure to be built up using
fluent speech rules up until the moment of interrup-
tion, at which point a special repair rule may be ap-
plied. This property will be examined further in sec-
tion 2.3, following a technical description of the rep-
resentation scheme.
2.1 Binary branching structure
In order to obtain a linguistically plausible right-
corner transform representation of incomplete con-
stituents, the Switchboard corpus is subjected to a
pre-process transform to introduce binary-branching
nonterminal projections, and fold empty categories
into nonterminal symbols in a manner similar to that
proposed by Johnson (1998b) and Klein and Man-
ning (2003). This binarization is done in in such
a way as to preserve linguistic intuitions of head
projection, so that the depth requirements of right-
corner transformed trees will be reasonable approx-
imations to the working memory requirements of a
human reader or listener.
Trees containing speech repairs are reduced in ar-
ity by merging repair structure lower in the tree,
when possible. As seen in the left tree below, 1 re-
pair structure is annotated in a flat manner, which
can lead to high-arity rules which are sparsely repre-
sented in the data set, and thus difficult to learn. This
problem can be mitigated by using the rewrite rule
shown below, which turns an EDITED-X constituent
into the leftmost child of a tree of type X, as long as
the original flat tree had X following an EDITED-
X constituent and possibly some editing term (ET)
categories. The INTJ category (?uh?,?um?,etc.) and
the PRN category (?I mean?, ?that is?, etc.) are con-
sidered to be editing term categories when they lie
1Here, all Ai denote nonterminal symbols, and all ?i denote
subtrees; the notation A1:?1 indicates a subtree ?1 with label
A1; and all rewrites are applied recursively, from leaves to root.
between EDITED-X and X constituents.
A0
EDITED
A1:?1
ET* A1:?2 ?3 ?
A0
A1
EDITED-A1
A1:?1
ET* A1:?2
?3
2.2 Right-corner transform
Binarized trees2 are then transformed into right-
corner trees using transform rules similar to those
described by Johnson(1998a). This right-corner
transform is simply the left-right dual of a left-
corner transform. It transforms all right recursive
sequences in each tree into left recursive sequences
of symbols of the form A1/A2, denoting an incom-
plete instance of category A1 lacking an instance of
category A2 to the right.
Rewrite rules for the right-corner transform are
shown below:
A1
?1 A2
?2 A3:?3
?
A1
A1/A2
?1
A2/A3
?2
A3:?3
A1
A1/A2:?1 A2/A3
?2
?3 . . . ?
A1
A1/A3
A1/A2:?1 ?2
?3 . . .
Here, the first rewrite rule is applied iteratively
(bottom-up on the tree) to flatten all right recursion,
using incomplete constituents to record the original
nonterminal ordering. The second rule is then ap-
plied to generate left recursive structure, preserving
this ordering.
The incomplete constituent categories created by
the right corner transform are similar in form and
meaning to non-constituent categories used in Com-
binatorial Categorial Grammars (CCGs) (Steedman,
2000). Unlike CCGs, however, a right corner trans-
formed grammar does not allow backward function
application, composition, or raising. As a result, it
does not introduce spurious ambiguity between for-
ward and backward operations, but cannot be taken
to explicitly encode argument structure, as CCGs
can.
2All super-binary branches remaining after the above pre-
process are ?nominally? decomposed into right-branching struc-
tures by introducing intermediate nodes with labels concate-
nated from the labels of its children, delimited by underscores
106
EDITED [-NP]
NP [-UNF]
NP
DT
the
JJ
first
NN
kind
PP [-UNF]
IN
of
NP [-UNF]
NN
invasion
PP-UNF
IN
of
Figure 1: Standard tree repair structure, with -UNF prop-
agation as in (Hale et al, 2006) shown in brackets.
EDITED-NP
NP/PP
NP/NP
NP/PP
NP
NP/NN
NP/NN
DT
the
JJ
first
NN
kind
IN
of
NP
invasion
PP-UNF
of
Figure 2: Right-corner transformed tree with repair struc-
ture
2.3 Application to speech repair
An example speech repair from the Switchboard cor-
pus can be seen in Figures 1 and 2, in which the same
repair fragment is shown in a standard state such as
might be used to train a probabilistic context free
grammar, and after the right-corner transform. Fig-
ure 1 also shows, in brackets, the augmented anno-
tation used by Hale et al(2006). This scheme con-
sisted of adding -X to an EDITED label which pro-
duced a category X, as well as propagating the -UNF
label at the right corner of the tree up through every
parent below the EDITED root.
The standard annotation (without -UNF propaga-
tion) is deficient because even if an unfinished con-
stituent like PP-UNF is correctly recognized, and the
speaker is essentially in an error state, there may be
several partially completed constituents above ? in
Figure 1, the NP, PP, and NP above the PP-UNF.
These constituents need to be completed, but using
the standard annotation there is only one chance to
make use of the information about the error that has
occurred ? the NP ? NP PP-UNF rule. Thus, by the
time the error section is completed, there is no infor-
mation by which a parsing algorithm could choose
to reduce the topmost NP to EDITED other than in-
dependent rule probabilities.
The approach used by (Hale et al, 2006) works
because the information about the transition to an er-
ror state is propagated up the tree, in the form of the
-UNF tags. As the parsing chart is filled in bottom
up, each rule applied is essentially coming out of a
special repair rule set, and so at the top of the tree
the EDITED hypothesis is much more likely. How-
ever, this requires that several fluent speech rules
from the data set be modified for use in a special
repair grammar, which not only reduces the amount
of available training data, but violates our intuition
that most reparanda are fluent up until the actual edit
occurs.
The right corner transform model works in a dif-
ferent way, by building up constituent structure from
left to right. In Figure 2, the same fragment is
shown as it appears in the training data for this sys-
tem. With this representation, the problem noticed
by Hale and colleagues (2006) has been solved in
a different way, by incrementally building up left-
branching rather than right-branching structure, so
that only a single special error rule is required at the
end of the constituent. Whereas the -UNF propa-
gation scheme often requires the entire reparandum
to be generated from a speech repair rule set, this
scheme only requires one special rule, where the
moment of interruption actually occurred.
This is not only a pleasing parsimony, but it re-
duces the number of special speech repair rules that
need to be learned and saves more potential exam-
ples of fluent speech rules, and therefore potentially
makes better use of limited data.
3 Evaluation
The evaluation of this system was performed on
the Switchboard corpus, using the mrg annotations
in directories 2 and 3 for training, and the files
sw4004.mrg to sw4153.mrg in directory 4 for evalu-
ation, following Johnson and Charniak(2004).
The input to the system consists of the terminal
symbols from the trees in the corpus section men-
tioned above. The terminal symbol strings are first
pre-processed by stripping punctuation and other
107
System Parseval F EDIT F
Baseline 60.86 42.39
CYK (H06) 71.16 41.7
RCT 68.36 64.41
TAG-based model (JC04) ? 79.7
Table 1: Baseline results are from a standard CYK parser
with binarized grammar. We were unable to find the cor-
rect configuration to match the baseline results from Hale
et al RCT results are on the right-corner transformed
grammar (transformed back to flat treebank-style trees
for scoring purposes). CYK and TAG lines show relevant
results from related work.
non-vocalized terminal symbols, which could not
be expected from the output of a speech recognizer.
Crucially, any information about repair is stripped
from the input, including partial words, repair sym-
bols 3, and interruption point information. While an
integrated system for processing and parsing speech
may use both acoustic and syntactic information to
find repairs, and thus may have access to some of
this information about where interruptions occur,
this experiment is intended to evaluate the use of the
right corner transform and syntactic information on
parsing speech repair. To make a fair comparison to
the CYK baseline of (Hale et al, 2006), the recog-
nizer was given correct part-of-speech tags as input
along with words.
The results presented here use two standard met-
rics for assessing accuracy of transcribed speech
with repairs. The first metric, Parseval F-measure,
takes into account precision and recall of all non-
terminal (and non pre-terminal) constituents in a hy-
pothesized tree relative to the gold standard. The
second metric, EDIT-finding F, measures precision
and recall of the words tagged as EDITED in the
hypothesized tree relative to those tagged EDITED
in the gold standard. F score is defined as usual,
2pr/(p + r) for precision p and recall r.
The results in Table 1 show that this system per-
forms comparably to the state of the art in over-
all parsing accuracy and reasonably well in edit de-
tection. The TAG system (Johnson and Charniak,
2004) achieves a higher EDIT-F score, largely as a
result of its explicit tracking of overlapping words
3The Switchboard corpus has special terminal symbols indi-
cating e.g. the start and end of the reparandum.
between reparanda and alterations. A hybrid system
using the right corner transform and keeping infor-
mation about how a repair started may be able to
improve EDIT-F accuracy over this system.
4 Conclusion
This paper has described a novel method for pars-
ing speech that contains speech repairs. This system
achieves high accuracy in both parsing and detecting
reparanda in text, by making use of transformations
that create incomplete categories, which model the
reparanda of speech repair well.
References
Elizabeth R. Blackmer and Janet L. Mitton. 1991. Theo-
ries of monitoring and the timing of repairs in sponta-
neous speech. Cognition, 39:173?194.
Jean Carletta, Richard Caley, and Stephen Isard. 1993.
A collection of self-repairs from the map task cor-
pus. Technical report, Human Communication Re-
search Centre, University of Edinburgh.
John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr, Mary
Harper, Anna Krasnyanskaya, Matthew Lease, Yang
Liu, Brian Roark, Matthew Snover, and Robin Stew-
art. 2006. PCFGs with syntactic and prosodic indica-
tors of speech repairs. In Proceedings of the 45th An-
nual Conference of the Association for Computational
Linguistics (COLING-ACL).
Mark Johnson and Eugene Charniak. 2004. A tag-based
noisy channel model of speech repairs. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL ?04), pages 33?
39, Barcelona, Spain.
Mark Johnson. 1998a. Finite state approximation of
constraint-based grammars using left-corner grammar
transforms. In Proceedings of COLING/ACL, pages
619?623.
Mark Johnson. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430.
William J.M. Levelt. 1983. Monitoring and self-repair in
speech. Cognition, 14:41?104.
Elizabeth Shriberg. 1994. Preliminaries to a Theory of
Speech Disfluencies. Ph.D. thesis, University of Cali-
fornia at Berkeley.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
108
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 277?280,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Parsing Speech Repair without Specialized Grammar Symbols?
Tim Miller
University of Minnesota
tmill@cs.umn.edu
Luan Nguyen
University of Minnesota
lnguyen@cs.umn.edu
William Schuler
University of Minnesota
schuler@cs.umn.edu
Abstract
This paper describes a parsing model for
speech with repairs that makes a clear sep-
aration between linguistically meaningful
symbols in the grammar and operations
specific to speech repair in the operation of
the parser. This system builds a model of
how unfinished constituents in speech re-
pairs are likely to finish, and finishes them
probabilistically with placeholder struc-
ture. These modified repair constituents
and the restarted replacement constituent
are then recognized together in the same
way that two coordinated phrases of the
same type are recognized.
1 Introduction
Speech repair is a phenomenon in spontaneous
spoken language in which a speaker decides to
interrupt the flow of speech, replace some of the
utterance (the ?reparandum?), and continues on
(with the ?alteration?) in a way that makes the
whole sentence as transcribed grammatical only
if the reparandum is ignored. As Ferreira et al
(2004) note, speech repairs1 are the most disrup-
tive type of disfluency, as they seem to require
that a listener first incrementally build up syntac-
tic and semantic structure, then subsequently re-
move it and rebuild when the repair is made. This
difficulty combines with their frequent occurrence
to make speech repair a pressing problem for ma-
chine recognition of spontaneous speech.
This paper introduces a model for dealing with
one part of this problem, constructing a syntac-
tic analysis based on a transcript of spontaneous
spoken language. The model introduced here dif-
fers from other models attempting to solve the
?This research was supported by NSF CAREER award
0447685. The views expressed are not necessarily endorsed
by the sponsors .
1Ferreira et al use the term ?revisions?.
same problem, by completely separating the fluent
grammar from the operations of the parser. The
grammar thus has no representation of disfluency
or speech repair, such as the ?EDITED? category
used to represent a reparandum in the Switchboard
corpus, as such categories are seemingly at odds
with the typical nature of a linguistic constituent.
Rather, the approach presented here uses a
grammar that explicitly represents incomplete
constituents being processed, and repair is rep-
resented by rules which allow incomplete con-
stituents to be prematurely merged with existing
structure. While this model is interesting for its
elegance in representation, there is also reason
to hypothesize improved performance, since this
processing model requires no additional grammar
symbols, and only one additional operation to ac-
count for speech repair, and thus makes better use
of limited data resources.
2 Background
Previous work on parsing of speech with repairs
has shown that syntactic cues can be used to in-
crease accuracy of detection of reparanda, which
can increase overall parsing accuracy. The first
source of structure used to recognize repair is what
Levelt (1983) called the ?Well-formedness Rule.?
This rule essentially states that a speech repair acts
like a conjunction; that is, the reparandum and the
alteration must be of the same syntactic category.
Of course, the reparandum is often unfinished, so
the Well-formedness Rule allows for the reparan-
dum category to be inferred.
This source of structure has been used by two
related approaches, that of Hale et al (2006) and
Miller (2009). Hale and colleagues exploit this
structure by adding contextual information to the
standard reparandum label ?EDITED?. In their
terminology, daughter annotation takes the (pos-
sibly unfinished) constituent label of the reparan-
dum and appends it to the EDITED label. This
277
allows a learned probabilistic context-free gram-
mar to represent the likelihood of a reparandum of
a certain type being a sibling with a finished con-
stituent of the same type.
Miller?s approach exploited the same source of
structure, but changed the representation to use
a REPAIRED label for alterations instead of an
EDITED label for reparanda. The rationale for
that change is the fact that a speech repair does not
really begin until the interruption point, at which
point the alteration is started and the reparandum
is retroactively labelled as such. Thus, the argu-
ment goes, no special syntactic rules or symbols
should be necessary until the alteration begins.
3 Model Description
3.1 Right-corner transform
This work first uses a right-corner transform,
which turns right-branching structure into left-
branching structure, using category labels that use
a ?slash? notation ?/? to represent an incomplete
constituent of type ? ?looking for? a constituent
of type ? in order to complete itself.
This transform first requires that trees be bina-
rized. This binarization is done in a similar way to
Johnson (1998) and Klein and Manning (2003).
Rewrite rules for the right-corner transform are
as follows, first flattening right-branching struc-
ture:2
A
1
?
1
A
2
?
2
A
3
a
3
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
A
3
a
3
A
1
?
1
A
2
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
. . .
then replacing it with left-branching structure:
A
1
A
1
/A
2
:?
1
A
2
/A
3
?
2
?
3
. . .
?
A
1
A
1
/A
3
A
1
/ A
2
:?
1
?
2
?
3
. . .
One problem with this notation is the represen-
tation given to unfinished constituents, as seen in
Figures 1 and 2. The standard representation of
2Here, all A
i
denote nonterminal symbols, and ?
i
denote
subtrees; the notationA
1
:?
0
indicates a subtree ?
0
with label
A
1
; and all rewrites are applied recursively, from leaves to
root.
S
. . . EDITED
PP
IN
as
NP-UNF
DT
a
PP
IN
as
NP
NP
DT
a
NN
westerner
PP-LOC
IN
in
NP
NNP
india
. . .
Figure 1: Section of interest of a standard phrase
structure tree containing speech repair with unfin-
ished noun phrase (NP).
PP
PP/NP
PP/PP
PP/NP
PP/PP
EDITEDPP
EDITEDPP/NP-UNF
IN
as
NP-UNF
DT
a
IN
as
NP
NP/NN
DT
a
NN
westerner
IN
in
NP
india
Figure 2: Right-corner transformed version of the
fragment above. This tree requires several special
symbols to represent the reparandum that starts
this fragment.
an unfinished constituent in the Switchboard cor-
pus is to append the -UNF label to the lowest un-
finished constituent (see Figure 1). Since one goal
of this work is separation of linguistic knowledge
from language processing mechanisms, the -UNF
tag should not be an explicit part of the gram-
mar. In theory, the incomplete category notation
induced by the right-corner transform is perfectly
suited to this purpose. For instance, the category
NP-UNF is a stand in category for several incom-
plete constituents, for example NP/NN, NP/NNS,
etc. However, since the sub-trees with -UNF la-
bels in the original corpus are by definition unfin-
ished, the label to the right of the slash (NN in
this case) is not defined. As a result, transformed
trees with unfinished structure have the represen-
tation of Figure 2, which gives away the positive
benefits of the right-corner transform in represent-
ing repair by propagating a special repair symbol
(EDITED) through the grammar.
3.2 Approximating unfinished constituents
It is possible to represent -UNF categories as stan-
dard unfinished constituents, and account for un-
finished constituents by having the parser prema-
278
turely end the processing of a given constituent.
However, in the example given above, this would
require predicting ahead of time that the NP-UNF
was only missing a common noun ? NN (for ex-
ample). This problem is addressed in this work
by probabilistically filling in placeholder final cat-
egories of unfinished constituents in the standard
phrase structure trees, before applying the right-
corner transform.
In order to fill in the placeholder with realistic
items, phrase completions are learned from cor-
pus statistics. First, this algorithm identifies an
unfinished constituent to be finished as well as its
existing children (in the continuing example, NP-
UNF with child labelled DT). Next, the corpus is
searched for fluent subtrees with matching root la-
bels and child labels (NP and DT), and a distri-
bution is computed of the actual completions of
those subtrees. In the model used in this work,
the most common completions are NN, NNS, and
NNP. The original NP-UNF subtree is then given a
placeholder completion by sampling from the dis-
tribution of completions computed above.
After this addition is complete, the UNF and
EDITED labels are removed from the reparandum
subtree, and if a restarted constituent of the same
type is a sibling of the reparandum (e.g. another
NP), the two subtrees are made siblings under a
new subtree with the same category label (NP).
See Figure 3 for a simple visual example of how
this works.
S
. . . EDITED
PP
IN
as
NP
DT
a
NN
eli
PP
IN
as
NP
NP
DT
a
NN
westerner
PP-LOC
IN
in
NP
NNP
india
. . .
Figure 3: Same tree as in Figure 1, with the un-
finished noun phrase now given a placeholder NN
completion (both bolded).
Next, these trees are modified using the right-
corner transform as shown in Figure 4. This tree
still contains placeholder words that will not be
in the text stream of an observed input sentence.
Thus, in the final step of the preprocessing algo-
rithm, the finished category label and the place-
holder right child are removed where found in a
right-corner tree. This results in a right-corner
transformed tree in which a unary child or right
PP
PP/NNP
PP/PP
PP/NP
PP/PP
PP
PP/NN
PP/NP
IN
as
DT
a
NN
eli
IN
as
NP
NP/NN
DT
a
NN
westerner
IN
in
NNP
india
Figure 4: Right-corner transformed tree with
placeholder finished phrase.
PP
PP/NNP
PP/PP
PP/NP
PP/PP
PP/NN
PP/NP
IN
as
DT
a
IN
as
NP
NP/NN
DT
a
NN
westerner
IN
in
NNP
india
Figure 5: Final right-corner transformed state af-
ter excising placeholder completions to unfinished
constituents. The bolded label indicates the signal
of an unfinished category reparandum.
child subtree having an unfinished constituent type
(a slash category, e.g. PP/NN in Figure 5) at its
root represents a reparandum with an unfinished
category. The tree then represents and processes
the rest of the repair in the same way as a coordi-
nation.
4 Evaluation
This model was evaluated on the Switchboard cor-
pus (Godfrey et al, 1992) of conversational tele-
phone speech between two human interlocuters.
The input to this system is the gold standard
word transcriptions, segmented into individual ut-
terances. For comparison to other similar systems,
the system was given the gold standard part of
speech for each input word as well. The standard
train/test breakdown was used, with sections 2 and
3 used for training, and subsections 0 and 1 of sec-
tion 4 used for testing. Several sentences from the
end of section 4 were used during development.
For training, the data set was first standardized
by removing punctuation, empty categories, ty-
pos, all categories representing repair structure,
279
and partial words ? anything that would be diffi-
cult or impossible to obtain reliably with a speech
recognizer.
The two metrics used here are the standard Par-
seval F-measure, and Edit-finding F. The first takes
the F-score of labeled precision and recall of the
non-terminals in a hypothesized tree relative to the
gold standard tree. The second measure marks
words in the gold standard as edited if they are
dominated by a node labeled EDITED, and mea-
sures the F-score of the hypothesized edited words
relative to the gold standard.
System Configuration Parseval-F Edited-F
Baseline CYK 71.05 18.03
Hale et al 68.48 37.94
Plain RC Trees 69.07 30.89
Elided RC Trees 67.91 24.80
Merged RC Trees 68.88 27.63
Table 1: Results
Results of the testing can be seen in Ta-
ble 1. The first line (?Baseline CYK?) indi-
cates the results using a standard probabilistic
CYK parser, trained on the standardized input
trees. The following two lines are results from re-
implementations of the systems from Hale et al
(2006) andMiller (2009). The line marked ?Elided
trees? gives current results. Surprisingly, this re-
sult proves to be lower than the previous results.
Two observations in the output of the parser on
the development set gave hints as to the reasons
for this performance loss.
First, repairs using the slash categories (for un-
finished reparanda) were rare (relative to finished
reparanda). This led to the suspicion that there
was a state-splitting phenomenon, where cate-
gories previously lumped together as EDITED-NP
were divided into several unfinished categories
(NP/NN, NP/NNS, etc.). To test this suspicion, an-
other experiment was performed where all unary
child and right child subtrees with unfinished cat-
egory labels X/Y were replaced with EDITED-X.
This result is shown in line five of Table 1. This
result improves on the elided version, and sug-
gests that the state-splitting effect is most likely
one cause of decreased performance.
The second effect in the parser output was the
presence of several very long reparanda (more
than ten words), which are highly unlikely in nor-
mal speech. This phenomenon does not occur
in the ?Plain RC Trees? condition. One explana-
tion for this effect is that plain RC trees use the
EDITED label in each rule of the reparandum (see
Figure 2 for a short real-world example). This
essentially creates a reparandum rule set, mak-
ing expansion of a reparandum difficult due to the
likelihood of a long chain eventually requiring a
reparandum rule that was not found in the train-
ing data, or was not learned correctly in the much
smaller set of reparandum-specific training data.
5 Conclusion and Future Work
In conclusion, this paper has presented a new
model for speech containing repairs that enforces
a clean separation between linguistic categories
and parsing operations. Performance was below
expectations, but analysis of the interesting rea-
sons for these results suggests future directions. A
model which explicitly represents the distance that
a speaker backtracks when making a repair would
prevent the parser from hypothesizing the unlikely
reparanda of great length.
References
Fernanda Ferreira, Ellen F. Lau, and Karl G.D. Bai-
ley. 2004. Disfluencies, language comprehension,
and Tree Adjoining Grammars. Cognitive Science,
28:721?749.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proc. ICASSP,
pages 517?520.
John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr,
Mary Harper, Anna Krasnyanskaya, Matthew Lease,
Yang Liu, Brian Roark, Matthew Snover, and Robin
Stewart. 2006. PCFGs with syntactic and prosodic
indicators of speech repairs. In Proceedings of the
45th Annual Conference of the Association for Com-
putational Linguistics (COLING-ACL).
Mark Johnson. 1998. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Willem J.M. Levelt. 1983. Monitoring and self-repair
in speech. Cognition, 14:41?104.
Tim Miller. 2009. Improved syntactic models for pars-
ing speech with repairs. In Proceedings of the North
American Association for Computational Linguis-
tics, Boulder, CO.
280
Broad-Coverage Parsing Using Human-Like
Memory Constraints
William Schuler?
University of Minnesota
Samir AbdelRahman??
Cairo University
Tim Miller?
University of Minnesota
Lane Schwartz?
University of Minnesota
Human syntactic processing shows many signs of taking place within a general-purpose
short-term memory. But this kind of memory is known to have a severely constrained storage
capacity ? possibly constrained to as few as three or four distinct elements. This article describes
a model of syntactic processing that operates successfully within these severe constraints, by
recognizing constituents in a right-corner transformed representation (a variant of left-corner
parsing) and mapping this representation to random variables in a Hierarchical Hidden Markov
Model, a factored time-series model which probabilistically models the contents of a bounded
memory store over time. Evaluations of the coverage of this model on a large syntactically
annotated corpus of English sentences, and the accuracy of a bounded-memory parsing strategy
based on this model, suggest this model may be cognitively plausible.
1. Introduction
It is an interesting possibility that human syntactic processingmay occur entirely within
a general-purpose short-term memory. Like other short-term memory processes, syn-
tactic processing is susceptible to degradation if short-term memory capacity is loaded,
for example, when readers are asked to retain lists of words while reading (Just
and Carpenter 1992); and memory of words and syntax degrades over time within
and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse
information about referents from other sentences (Ericsson and Kintsch 1995). But
short-term memory is known to have severe capacity limitations of perhaps no more
than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem
? Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455.
E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu.
?? Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street,
Postal Code: 12613, Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg.
Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for
publication: 27 May 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
too austere to process the rich tree-like phrase structure commonly invoked to explain
word-order regularities in natural language.
This article aims to show that they are not. The article describes a comprehension
model, based on a right-corner transform?a reversible tree transform related to the
left-corner transform of Johnson (1998a)?that associates familiar phrase structure trees
with the contents of a memory store of three to four partially completed constituents
over time. Coverage results on the large syntactically annotated Penn Treebank corpus
show a vast majority of naturally occurring sentences can be recognized using a mem-
ory store containing a maximum of only three incomplete constituents, and nearly all
sentences can be recognized using four, consistent with estimates of human short-term
memory capacity.
This transform reduces memory usage in incremental (left to right) processing
by transforming right-branching constituent structures into left-branching structures,
allowing child constituents to be composed with parent constituents before either have
been completely recognized. But because this composition identifies an incomplete
child as the awaited portion of an incomplete parent, it implicitly predicts that this
child constituent will be the rightmost (i.e., last) child of the parent, before this child
has been completely recognized. Parsing accuracy results on the Penn Treebank
using a Hierarchical Hidden Markov Model (Murphy and Paskin 2001)?essentially a
probabilistic pushdown automaton with a bounded pushdown store?show that this
prediction can be reliably learned from training data.
The remainder of this article is organized as follows: Section 2 describes some
related models of human syntactic processing using a bounded memory store; Section 3
describes a Hierarchical Hidden Markov Model (HHMM) framework for statistical
parsing using this bounded store of incomplete constituents; Section 4 describes the
right-corner transform and how it relates conventional phrase structure to incomplete
constituents in a bounded memory store; Section 5 describes an experiment to estimate
the level of coverage of the Penn Treebank corpus that can be achieved using this
transform with various memory limits, given a linguistically motivated binarization of
this corpus; and Section 6 gives accuracy results of this bounded-memorymodel trained
on this corpus, given that some amount of incremental prediction (as described earlier)
must be involved.
2. Bounded-Memory Parsing
One of the earliest bounded-memory parsing models is that of Marcus (1980). This
model maintains a bounded store of complete but unattached constituents as a buffer,
and operates on them using a variety of specialized memory manipulation operations,
deferring certain attachment decisions until the contents of this buffer indicate it is safe
to do so. (In contrast, the model described in this article maintains a store of incom-
plete constituents using ordinary stack-like push and pop operations, defined to allow
constituents to be composed before being completely recognized.) The Marcus parser
provides a bounded-memory explanation for human difficulties in processing garden
path sentences: for example, the horse raced past the barn fell, with intended interpretation
[NP the horse [RC (which was) raced past the barn]] fell (Bever 1970), in which raced seems like
the main verb of the sentence until the word fell is encountered. But this explanation due
to memory exhaustion is not compatible with observations of unproblematic parsing
of sentences such as these when contextual information is provided in advance: for
example, two men on horseback had a race; one went by the meadow, and the other went by the
barn (Crain and Steedman 1985).
2
Schuler et al Parsing Using Human-Like Memory Constraints
Ades and Steedman (1982) introduce the idea of composing incomplete constituents
to reduce storage demands in incremental processing using Combinatorial Catego-
rial Grammar (CCG), avoiding the need to maintain large buffers of complete but
unattached constituents. The right-corner transform described in this article composes
incomplete constituents in very much the same way, but CCG is essentially a compe-
tence model, in that it seeks to unify lexical category representations used in processing
with learned generalizations about argument structure, whereas the model described
herein is exclusively a performance model, allowing generalizations about lexical ar-
gument structures to be learned in some other representation, then combined with
probabilistic information about parsing strategies to yield a set of derived incomplete
constituents. As a result, the model described in this article has a freer hand to satisfy
strict working memory bounds, which may not permit some of the alternative compo-
sition operations proposed in the CCG account, thought to be associated with available
prosody and quantifier scope analyses.1
Johnson-Laird (1983) and Abney and Johnson (1991) propose a pure processing
account of memory capacity limits in parsing ordinary phrase structure trees. The
Johnson-Laird and Abney and Johnson models adopt a left-corner parsing strategy, of
which the right-corner transform introduced in this article is a variant, in order to bring
memory usage for most parsable sentences to within seven or so active or awaited
phrase structure constituents. This account may be used to explain human processing
difficulties in processing triply center-embedded sentences like the rat that the cat that the
dog chased killed ate the malt, with intended interpretation [NP the rat that [NP the cat that [NP
the dog] chased] killed] ate the malt (Chomsky and Miller 1963). But this explanation does
not account for examples of triply center-embedded sentences that typically do not
cause processing problems: [NP that [NP the food that [NP John] ordered] tasted good] pleased
him (Gibson 1991). Moreover, the apparent competition between comprehension of
center-embedded object relatives and retention of unrelated words in general-purpose
memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at
least, can be) used to store incomplete constituents during comprehension. This would
predict three or four elements of reliable storage, rather than seven (Cowan 2001).
The transform-based model described in this article exploits a conception of chunking
(Miller 1956) to combine pairs of active and awaited constituents from the Abney
and Johnson analysis, connected by recognized structure, in order to operate within
estimates of human short-term memory bounds.
Because of these counterexamples to the memory-exhaustion explanation of garden
path and center-embedding difficulties, recent work has turned to explanations other
than memory exhaustion for these phenomena. Lewis and Vasishth (2005) attribute
processing errors to activation interference among stored constituents that have sim-
ilar syntactic and semantic roles. Hale?s surprisal (2001) and entropic model (2006)
link human processing difficulties to significant changes in the relative probability of
competing hypotheses in incremental parsing, such that if activation is taken to be a
mechanism for probability estimation, processing difficulties may be ascribed to the
relatively slow speed of activation change within the brain (or to collapsing activation
when probabilities grow too small, as in the case of garden path sentences). These
models explain many processing difficulties without invoking memory limits, and are
1 The lack of support for some of these available scope analyses may not necessarily be problematic for the
present model. The complexity of interpreting nested raised quantifiers may place them beyond the
capability of human interactive incremental interpretation, but not beyond the capability of post hoc
interpretation (understood after the listener has had time to think about it).
3
Computational Linguistics Volume 36, Number 1
compatible with brain imaging evidence of increased cortical activity and recruitment
of auxiliary brain areas during periods of increased uncertainty in sentence processing
(Just and Varma 2007). But if interference or changing activation is posited as the source
of processing difficulty, and delays are not linked to memory exhaustion per se, then
these theories do not explain how (or whether) syntactic processing operates within
general-purpose short-term memory.
Toward this end, this article will specifically evaluate the claim that syntactic
processing can be performed entirely within general-purpose short-term memory by
using this memory to store unassimilated incomplete syntactic constituents, derived
through a right-corner transform from basic properties of phrase structure trees. As
a probabilistic incremental parser, the model described in this article is compatible
with surprisal-based explanations of processing difficulties; it is, however, in some
sense orthogonal, because it models a different dimension of resource allocation. The
surprisal framework models allocation of processing resources (in this case, activation)
among disjunctions of competing hypotheses, which are maintained for some amount
of time in parallel, whereas the framework described here can be taken to model the
allocation of processing resources (in this case, memory elements) among conjunctions
of incompletely recognized constituents within each competing hypothesis.2 Thus, in
this view, there are twoways to simultaneously activatemultiple concepts: disjunctively
(sharing activation among competing hypotheses) and conjunctively (sharing activation
among unassimilated constituents within a hypothesis). But only the inner conjunctive
allocation corresponds to the familiar discretely bounded store of short-term memory
as described by Miller (1956); the outer disjunctive allocation treats activation as a
continuous resource in which like-valued pockets expand and contract as they are
reinforced or contradicted by incoming observations. Indeed, it would be surprising
if these two dimensions of resource allocation did not exist: the former, because it
would contradict years of observations about the behavior of short-term memory; and
the latter, because it would require neural activation spreading to be instantaneous
and uniform, contradicting most neuropsychological evidence. Levy (2008) compares
the allocation of activation in this kind of framework to the distributed allocation
of resources in a particle filter (Gordon, Salmond, and Smith 1993), an approximate
inference technique for probabilistic time-series models in which particles in a (typically
fixed) reservoir are assigned randomly sampled hypotheses from learned transition
probabilities, essentially functioning as units of activation. The model described in this
paper qualifies this analogy by positing that each individual particle in this reservoir
endorses a coherent hypothesis about the contents of a three- to four-element memory
store at any given time, rather than about an entire unbounded phrase structure tree.3
2 Probability distributions in entropy-based models like Hale?s are typically assumed to be defined over
sets of hypotheses pursued in parallel, but other interpretations (for example, lookahead-based
deterministic models) are possible. The model described in this article is also compatible with
deterministic parsing frameworks, in which case it models allocation of processing resources among
incompletely-recognized constituents within a single non-competing hypothesis.
3 Pure connectionist models of syntactic processing (Elman 1991; Berg 1992; Rohde 2002) attempt to unify
storage of constituent structure with that of ambiguous alternative analyses, but the memory demands of
systems based on this approach typically do not scale well to broad-coverage parsing. Recent results for
using self-organizing maps as a unified memory resource are encouraging (Mayberry and Miikkulainen
2003), but are still limited to parsing relatively short travel planning queries with limited syntactic
complexity. Hybrid systems that generate explicit alternative hypotheses with explicit stacked-up
constituents, and use connectionist models for probability estimation over these hypotheses (Henderson
2004) typically achieve better performance in practice.
4
Schuler et al Parsing Using Human-Like Memory Constraints
Previous memory-based explanations of problematic sentences (explaining garden
path effects as exceeding a bound of four complete but unattached constituents, or ex-
plaining center embedding difficulties as exceeding a bound of seven active or awaited
constituents) have been shown to underestimate human sentence processing capacity
when equally complex but unproblematic sentences were examined. The hypothesis
advanced in this article, that human sentence processing uses general-purpose short-
term memory to store incomplete constituents as defined by a right-corner transform,
leaves the explanation of several negative examples of unparsable garden path and cen-
ter embedding sentences to orthogonal models of surprisal or interference. But in order
to determine whether this right-corner memory hypothesis still underestimates human
sentence processing capacity, a corpus study was performed on two complementary
corpora of transcribed spontaneous speech and newspaper text, manually annotated
with phrase structure trees (Marcus, Santorini, and Marcinkiewicz 1993). These spon-
taneous speech and newspaper text corpora contain only attested positive examples
of parsable sentences, but they may be considered complementary for this purpose
because the complexity of spontaneous speech may somewhat understate human recog-
nition capacity (potentially limiting it to the cost of spontaneously generating sentences
in an unusual social context), and the complexity of newspaper text may somewhat
overstate human recognition capacity (though it is composed and edited to be readable,
it is still composed and edited off-line), so results from these corpora may be taken
together to suggest generous and conservative upper bounds on human processing
capacity.
3. Bounded-Memory Parsing with a Time Series Model
The framework adopted in this article is a factored HMM-like time series model, which
maintains a probability distribution over the contents of a bounded set of random
variables over time, corresponding to hypothesized stores of memory elements. The
random variables in this store may be understood as simultaneous activations in a cog-
nitive model (similar to the superimposed roles described by Smolensky and Legendre
[2006]), and the probability distribution over these stores may be thought of as compet-
ing pockets of activation, as described in the previous section. Some of these variables
persist as elements of the short-term memory store, and some are transient as results of
hypothesized compositions, which are estimated and immediately discarded or folded
into the persistent store according to the dependencies in the model. The variables
have values or contents (or fillers)?in this case incomplete constituent categories?that
change over time, and although these values may be uncertain, the set of hypothesized
contents of this memory store at any given point in time are collectively constrained to
form a coherent (but possibly incomplete) syntactic analysis of a sentence.
The particular model used here is an HHMM (Murphy and Paskin 2001), which
mimics a bounded-memory pushdown automaton (PDA), supporting simple push and
pop operations on a bounded stack-like memory store. A time-series model is used
here instead of an explicit stack machine, first because the probability model is well
defined on a bounded memory store, and second because the plasticity of the random
variables that mimic stack behavior in this model makes the model cross-linguistically
attractive. By evoking additional random variables and dependencies, the model can
be defined (or presumably, trained) to mimic other types of automata, such as extended
pushdown automata (EPDAs) recognizing tree-adjoining languages with crossed and
nested dependencies, as have been hypothesized for languages like Dutch (Shieber
5
Computational Linguistics Volume 36, Number 1
1985). However, the remainder of this article will only discuss random variables and
dependencies necessary to mimic a bounded stack pushdown automaton.
3.1 Hierarchical HMMs
Hierarchical Hidden Markov Models (Murphy and Paskin 2001) are essentially Hidden
Markov Models factored into some fixed number of stack-like elements at each time
step.
HMMs characterize speech or text as sequences of hidden states qt (which may
consist of phones, words, or other hypothesized syntactic or semantic information), and
observed states ot at corresponding time steps t (typically short, overlapping frames
of an audio signal, or words or characters in a text processing application). A most
likely sequence of hidden states q?1..T can then be hypothesized given any sequence of
observed states o1..T:
q?1..T = argmax
q1..T
P(q1..T | o1..T ) (1)
= argmax
q1..T
P(q1..T ) ? P(o1..T | q1..T ) (2)
def
= argmax
q1..T
T
?
t=1
P?A (qt | qt?1) ? P?B (ot | qt) (3)
using Bayes? Law (Equation 2) and Markov independence assumptions (Equation 3)
to define a full P(q1..T | o1..T ) probability as the product of a Language Model (?A)
prior probability P(q1..T )
def
=
?
t P?A (qt | qt?1) and anObservation Model (?B) likelihood
probability P(o1..T | q1..T )
def
=
?
t P?B (ot | qt) (Baker 1975; Jelinek, Bahl, and Mercer 1975).
Language model transitions P?A (qt | qt?1) over complex hidden states qt can be
modeled using synchronized levels of stacked-up component HMMs in an HHMM,
analogous to a shift-reduce parser or pushdown automaton with a bounded stack.
HHMM transition probabilities are calculated in two phases: a ?reduce? phase (result-
ing in an intermediate, transient final-state variable ft), modeling whether component
HMMs terminate; and a ?shift? phase (resulting in a persistent modeled state qt), in
which unterminated HMMs transition and terminated HMMs are re-initialized from
their parent HMMs. Variables over intermediate ft and modeled qt states are factored
into sequences of depth-specific variables?one for each of D levels in the HHMM
hierarchy:
ft = ? f 1t . . . f
D
t ? (4)
qt = ?q1t . . . q
D
t ? (5)
Transition probabilities are then calculated as a product of transition probabilities at
each level, using level-specific ?reduce? ?F,d and ?shift? ?Q,d models:
P?A (qt | qt?1) =
?
ft
P( ft | qt?1) ? P(qt | ft qt?1) (6)
6
Schuler et al Parsing Using Human-Like Memory Constraints
Figure 1
Graphical representation of a Hierarchical Hidden Markov Model. Circles denote random
variables, and edges denote conditional dependencies. Shaded circles denote variables with
observed values.
def
=
?
f 1t..f
D
t
D
?
d=1
P?F,d ( f
d
t | f
d+1
t q
d
t?1q
d?1
t?1 ) ?
D
?
d=1
P?Q,d (q
d
t | f
d+1
t f
d
t q
d
t?1q
d?1
t ) (7)
with f D+1t and q
0
t defined as constants. In these equations, probabilities are marginalized
or summed over all combinations of intermediate variables f 1t ... f
D
t , so only the memory
store contents q1t ...q
D
t persist across time steps.
4 A graphical representation of anHHMM
with three depth levels is shown in Figure 1.
The independence assumptions in this model can be psycholinguistically moti-
vated. Independence across time points t (Equation 3) arise naturally from causality:
Any change to a memory store configuration to generate a configuration at time step
t+ 1 should depend only on the current memory store configuration at time step t;
memory operations should not be able to peek backward or forward in time to consult
past or future memory stores. Independence across depth levels d (Equation 7) arise
naturally from uncertainty about the structure between incomplete constituent chunks
(this property of right-corner transform categories is elaborated in Section 4).5
Shift and reduce probabilities can now be defined in terms of finitely recursive
HMMs with probability distributions over recursive expansion, transition, and reduc-
tion of states at each depth level. In the version of HHMMs used in this paper, each
modeled variable is a syntactic state qdt ? G?G (describing an incomplete constituent
consisting of an active grammatical category from domain G and an awaited grammat-
ical category from domain G?for example, an incomplete constituent S/NP consisting
of an active sentence S awaiting a noun phrase constituent NP); and each intermediate
4 In Viterbi decoding, probabilities over intermediate variables may be maximized rather than
marginalized, but in any case the intermediate variables do not persist.
5 Also, the fact that this is a generative model, in which observations are conditioned on hypotheses, then
flipped using Bayes? Law (Equation 2)?as opposed to a discriminative or conditional model, in which
hypotheses are conditioned directly on observations?is also appealing as a human model, in that it
allows the same architecture to be used for both recognition and generation. This is a desirable property
for modeling split utterances, in which interlocutors complete one another?s sentences (Lerner 1991;
Helasvuo 2004).
7
Computational Linguistics Volume 36, Number 1
variable is a reduction or non-reduction state f dt ? G?{1, 0} (indicating, respectively, a
reduction of incomplete constituent qdt?1 to a complete right child constituent of some
grammatical category from domainG, or a non-reduction of qdt?1 as a unary or left child,
as defined in Section 4). An intermediate variable f dt at depth d may indicate reduction
or non-reduction according to ?F-Reduction,d if there is a reduction at the depth level
immediately below d, but must indicate non-reduction ( f dt = 0) with probability 1 if
there is no reduction below:6
P?F,d ( f
d
t | f
d+1
t q
d
t?1q
d?1
t?1 )
def
=
{
if f d+1t ?G : [ f
d
t = 0]
if f d+1t ?G : P?F-Reduction,d ( f
d
t | q
d
t?1, q
d?1
t?1 )
(8)
where f D+1t = 1 and q
0
t = ROOT.
Shift probabilities over the modeled variable qdt at each level are defined using level-
specific transition ?Q-Transition,d and expansion ?Q-Expansion,d models:
P?Q,d (q
d
t | f
d+1
t f
d
t q
d
t?1q
d?1
t )
def
=
?
?
?
?
?
if f d+1t ?G, f
d
t ?G : [q
d
t = q
d
t?1]
if f d+1t ?G, f
d
t ?G : P?Q-Transition,d (q
d
t | f
d+1
t f
d
t q
d
t?1q
d?1
t )
if f d+1t ?G, f
d
t ?G : P?Q-Expansion,d (q
d
t | q
d?1
t )
(9)
where f D+1t = 1 and q
0
t = ROOT. This model is conditioned on final-state intermediate
variables f dt and f
d+1
t at and immediately below each HHMM level. If there is no re-
duction immediately below a given level (the first case provided), it deterministically
copies the current HHMM state forward to the next time step. If there is a reduction
immediately below the current level but no reduction at the current level (the second
case provided), it transitions the HHMM state at the current level, according to the
distribution ?Q-Transition,d. And if there is a reduction at the current level (the third case
above), it re-initializes this state given the state at the level above, according to the
distribution ?Q-Expansion,d.
Models ?F-Reduction,d, ?Q-Transition,d, and ?Q-Expansion,d are defined directly from train-
ing examples, for example (in the experiments described in this article), using relative
frequency estimation. The overall effect is that higher-level HMMs are allowed to
transition only when lower-level HMMs terminate. An HHMM therefore behaves like
a probabilistic implementation of a shift?reduce parser or pushdown automaton with a
bounded stack, where the maximum stack depth is equal to the number of depth levels
in the HHMM hierarchy.
4. Right-Corner Transform and Incomplete Constituents
The model described in this article recognizes trees in a right-corner transformed
representation to minimize usage of a bounded short-term memory store. This right-
corner transform is a variant of the left-corner transform described by Johnson (1998a),
but whereas the left-corner transform changes left-branching structure into right-
branching structure, the right-corner transform changes right-branching structure into
6 Here [?] is an indicator function: [?] = 1 if ? is true, 0 otherwise.
8
Schuler et al Parsing Using Human-Like Memory Constraints
left-branching structure. Recognition using this transformed grammar, extracted from
a transformed corpus, is similar to recognition using a left-corner parsing strategy (Aho
and Ullman 1972). This kind of strategy was shown to reduce memory requirements
for parsing sentences with mainly left- or right-recursive phrase structure to fewer than
seven active or awaited constituent categories (Abney and Johnson 1991). This is within
Miller?s (1956) estimate of human short-term memory capacity (if memory elements
store individual categories), whereas parsing heavily center-embedded sentences
(known to be difficult for human readers) would require seven or more elements at the
frontier of this capacity.
But recent research suggests that human memory capacity may be limited to as few
as three or four distinct items (Cowan 2001), with longer estimates of seven or more
possibly due to the human capacity to chunk remembered items into associated groups
(Miller 1956). The right-corner strategy described in this paper therefore assumes
constituent categories can similarly be chunked into incomplete constituents A/B formed
by pairing an active category Awith an awaited category B somewhere along the active
category?s right progeny (so, for example, a transitive verb may become an incomplete
constituent VP/NP consisting of an active verb phrase lacking an awaited noun phrase
yet to come).7 These chunked incomplete constituent categories A and B are naturally
related through fixed contiguous phrase structure between them, established during
the course of parsing prior to the beginning of B, and these incomplete constituents can
be composed with other incomplete constituents B/C to form similarly related category
pairs A/C.
These chunks are not only contiguous sections of phrase structure trees, they also
have contiguous string yields, so they correspond to the familiar notion of text chunks
used in shallow parsing approaches (Hobbs et al 1996). For example, a hypothesized
memory store may contain incomplete constituents S/NP (a sentence without a noun
phrase), followed by NP/NN (a noun phrase lacking a common noun), with cor-
responding string yields demand for bonds propped up and the municipal, respectively,
forming a complete contiguous segmentation of a sentence at any point in processing.
Although these two chunks could be composed into an incomplete constituent S/NN,
doing so at this point would close off the possibility of introducing another constituent
between these two, containing the recognized noun phrase as a left child (e.g., demand
for bonds propped up [NP [NP the municipal bonds]?s prices]).
This conception of chunking applied to right-branching progeny in phrase structure
trees does not have the power to eliminate the bounds of a memory store, however. In a
larger cognitive model, syntactic processing is assumed to occur as part of an interactive
semantic interpretation process, in which referents of constituents are calculated as
these constituents are recognized, and are used to constrain subsequent processing
decisions (Tanenhaus et al 1995; Brown-Schmidt, Campana, and Tanenhaus 2002).8
The chunked category pairs A and B in these incomplete constituents A/B result from
successful compositions of other such constituents earlier in the recognition process,
which means that the relationship between the referents of A and B is known and fixed
7 Incomplete constituents may also be defined through a left-corner transform, but left-corner transformed
categories are incomplete in the other direction?a goal category yet to come lacking an
already-recognized constituent?so stored incomplete constituent categories resulting from a left-corner
transform would have the character of future goal events, rather than remembered past events. This is
discussed in greater detail in Section 4.4.
8 This can be implemented in a time-series model by factoring the model to include additional random
variables over referents, as described in Schuler, Wu, and Schwartz (2009).
9
Computational Linguistics Volume 36, Number 1
in any hypothesized incomplete constituent. But syntactic and semantic relationships
between chunks in a hypothesized memory store are unspecified. Chunking beyond
the level of incomplete constituents would therefore involve grouping referents whose
interrelations have not necessarily been established by the parser. Because the set
of referents is presumably much larger than the set of syntactic categories, one may
assume there are real barriers to reliably chunking them in the absence of these fixed
relationships.
There certainly may be cases where syntactically unconnected referents (belonging
to different incomplete constituents) could be grouped together as chunks. But for
simplicity, this article will assume a very strict condition that only a single incomplete
constituent can be stored in each short-term memory element. Experimental results
described in Section 5 suggest that a vast majority of English sentences can be recog-
nized within these human-like memory bounds, even with this strict condition on
chunking. If parsing can be performed in bounded memory under such strict condi-
tions, it can reasonably be assumed to operate at least as well under more permissive
circumstances, where some amount of syntactically-unrelated referential chunking is
allowed.
Several existing incremental systems are organized around a left-corner parsing
strategy (Roark 2001; Henderson 2004). But these systems generally keep large numbers
of constituents open for modifier attachment in each hypothesis. This allows modifiers
to be attached as right children of any such open constituent. But if any number of
open constituents are allowed, then either the assumption that stored elements have
fixed syntactic (and semantic) internal structure will be violated, or the assumption that
syntax operates within a boundedmemory store will be violated, both of which are psy-
cholinguistically attractive as simplifying assumptions. The HHMM model described
in this article upholds both the fixed-element and bounded-memory assumptions by
hypothesizing fixed reductions of right child constituents into incomplete parents in the
same memory element, to make room for new constituents that may be introduced at a
later time. These in-element reductions are defined naturally on phrase structure trees
as the result of aligning right-corner transformed constituent structures to sequences of
random variables in a factored time-series model. The success of this predictive strategy
in corpus-based coverage and accuracy results described in Sections 5 and 6 suggests
that it may be plausible as a cognitive model.
Other accounts may model reductions in bounded memory as occurring as soon
as possible, by maintaining the option of undoing them when necessary (Stevenson
1998). This seems unattractive in the context of an interactive semantic model, however,
where syntactic constituents and semantic referents are composed in tandem, because
potentially very rich referential constraints introduced by composing a child constituent
into a parent would have to be systematically undone. An interesting possibility might
be that the appearance of syntactic restructuring may arise from a collection of hypoth-
esized stores of syntactically fixed incomplete constituents, pursued in parallel. The
results presented in this article suggest that this mechanism is possible, but these two
possibilities might be very difficult to distinguish empirically.
There is also a tradition of defining incomplete constituents as head-driven?
introduced in parsing only at the point in incremental recognition at which they can
be associated with a head word (Gibson 1991: Pritcher 1991: Gorrell 1995). In typically
head-initial languages such as English, incomplete constituents derived from these
head-driven models resemble those derived from a right-corner transform. But head-
driven incomplete constituents do not appear to obey general-purpose memory bounds
in head-final languages such as Japanese, and do not appear to obey attachment prefer-
10
Schuler et al Parsing Using Human-Like Memory Constraints
ences predicted by a head-driven account (Kamide and Mitchell 1999), favoring a pre-
head attachment account, as a right-corner transform would predict.
4.1 Tree Transforms Using Rewrite Rules
The incomplete constituents used in the present model are defined in terms of tree
transforms, which consist of recursive operations that change tree structures into other
tree structures. These transforms are not cognitive processes?syntax in this model is
learned and used entirely as time-series probabilities over random variable values in
the memory store. The role of these transforms is as a means to associate sequences of
configurations of incomplete constituents in a memory store with linguistically familiar
phrase structure representations, such as those studied in competence models or found
in annotated corpora.
The transforms presented in this article will be defined in terms of destructive rewrite
rules applied iteratively to each constituent of a source tree, from leaves to root, and from
left to right among siblings, to derive a target tree. These rewrites are ordered; when
multiple rewrite rules apply to the same constituent, the later rewrites are applied to
the results of the earlier ones.9 For example, the rewrite
A0
. . . A1
?2 ?3
. . . ?
A0
. . . ?2 ?3 . . .
could be used to iteratively eliminate all binary-branching nonterminal nodes in a tree,
except the root.
In the notation used in this article,
 Roman uppercase letters (Ai) are variables matching constituent labels,
 Roman lowercase letters (ai) are variables matching terminal symbols,
 Greek lowercase letters (?i) are variables matching entire subtree structure,
 Roman letters followed by colons, followed by Greek letters (Ai:?i) are
variables matching the label and structure, respectively, of the same
subtree, and
 ellipses (. . . ) are taken to match zero or more subtree structures,
preserving the order of ellipses in cases where there are more than one (as
in the rewrite shown herein).
Many of the transforms used in this article are reversible, meaning that the result
of applying a transform to a tree, then applying the reverse of that transform to the
resulting tree, will be the original tree itself. In general, a transform can be reversed
if the direction of its rewrite rules is reversed, and if each constituent in a target tree
9 The appropriate analogy here is to a Unix sed script, made sensitive to the beginning and end brackets of
a constituent and those of its children.
11
Computational Linguistics Volume 36, Number 1
matches a unique rewrite rule in the reversed transform. The fact that not all rewrites
can be unambiguously matched to HHMM output means that parse accuracy must be
evaluated on partially-binarized gold-standard trees, in order to remove the effect of
this ambiguous matching from the evaluation. This will be discussed in greater detail
in Section 6.
4.2 Right-Corner Transform Using Rewrite Rules
Rewrite rules for the right-corner transform are shown here, first to flatten out right-
recursive structure,
A1
?1 A2
?2 A3
a3
?
A1
A1/A2
?1
A2/A3
?2
A3
a3
,
A1
?1 A2
A2/A3
?2
. . .
?
A1
A1/A2
?1
A2/A3
?2
. . .
then to replace it with left-recursive structure,
A1
A1/A2:?1 A2/A3
?2
?3 . . . ?
A1
A1/A3
A1/A2:?1 ?2
?3 . . .
Here, the first two rewrite rules are applied iteratively (bottom-up on the tree) to flatten
all right recursion, using incomplete constituents to record the original nonterminal
ordering. The second rule is then applied to generate left-recursive structure, preserving
this ordering. Note that the last rewrite leaves a unary branch at the leftmost child of
each flattened node. This preserves the simple category labels of nodes that correspond
directly to nodes in the original tree, so the original tree can be reconstructed when
the right-corner transform concatenates multiple right-recursive sequences into a single
left-recursive sequence.
An example of a right-corner transformed tree is shown in Figure 2(c). An important
property of this transform is that it is reversible. Rewrite rules for reversing a right-
corner transform are simply the converse of those shown here. The correctness of this
can be demonstrated by dividing a tree into maximal sequences of right-recursive
branches (that is, maximal sequences of adjacent right children). The first two ?flatten-
ing? rewrites of the right-corner transform, applied to any such sequence, will replace
the right-branching nonterminal nodes with a flat sequence of nodes labeled with
slash categories, which preserves the order of the nonterminal category symbols in the
original nodes. Reversing this rewrite will therefore generate the original sequence of
nonterminal nodes. The final rewrite similarly preserves the order of these nonterminal
symbols while grouping them from the left to the right, so reversing this rewrite will
reproduce the flattened tree.
12
Schuler et al Parsing Using Human-Like Memory Constraints
Figure 2
A sample phrase structure tree (a) as it appears in the Penn Treebank, (b) after it has been
binarized, and (c) after it has been right-corner transformed.
4.3 Mapping Trees to HHMMDerivations
Any tree can be mapped to an HHMM derivation by aligning the nonterminals with qdt
categories. First, it is necessary to define rightward depth d, right index position t, and
final (rightmost) child status f dt+1, for every nonterminal node A in a tree, where
 d is defined to be the number of right branches between node A and the
root,
13
Computational Linguistics Volume 36, Number 1
 t is defined to be the number of words beneath or to the left of node A, and
 f dt+1 is defined to be 0 if A is a left child, 1 if A is a unary child, and A if A is
right.
Any right-corner transformed tree can then be annotated with these values and rewrit-
ten to define labels and final-state values for every combination of d and t covered by
the tree. This is done using the rewrite rule
d, t,A0, 0
d, t,A1, 1
? d, t,A1, 1
to replace unary branches with f dt+1 flags, and
d, t,A0, f
d
t+1
d, t?,A1, f
d
t?+1 d+1, t,A2,A2
?
d, t,A0, f
d
t+1
d, t?1,A1, 0
d, t?+1,A1, 0
d, t?,A1, f
d
t?+1
d+1, t,A2,A2
to copy stacked-up left child constituents over multiple time steps, while lower-level
(right child) constituents are being recognized. The dashed line on the right side of the
rewrite rule represents the variable number of time steps for a stacked-up higher-level
constituent (as seen, for example, in time steps 4?7 at depth 1 in Figure 3). Coordinates
d, t ? D, and T that have f dt+1=1 are assigned label ???, and coordinates not covered by
the tree are assigned label ??? and f dt+1=1.
The resulting label and final-state values at each node now define a value of qdt
and f dt for each depth d and time step t of the HHMM (see Figure 3). Probabilities for
HHMMmodels ?Q-Expansion,d, ?Q-Transition,d, and ?F-Reduction,d can then be estimated from
these values directly. Like the right-corner transform, this mapping is reversible, so qdt
and f dt values can be taken from a hypothesized most likely sequence and mapped back
Figure 3
Sample tree from Figure 2 mapped to qdt variable positions of an HHMM at each stack depth d
(vertical) and time step t (horizontal). This tree uses only two levels of stack memory. Values for
final-state variables f dt are not shown. Note that the mapping transform omits some nonterminal
labels; labels for these nodes can be reconstructed from their children.
14
Schuler et al Parsing Using Human-Like Memory Constraints
to trees (which can then undergo the reverse of the right-corner transform to become
ordinary phrase structure trees). Inspection of this rewrite rule will reveal the reverse of
this transform simply involves deleting unary-branching sequences that differ only in
the value of t and restoring unary branches when f dt+1=1.
This alignment of right-corner transformed trees also has the interesting property
that the categories on the stack at any given time step represent a segmentation of the
input up to that time step. For example, in Figure 3 at t = 12 the stack contains a sentence
lacking a verb phrase: S/VP (strong demand for . . . bonds), followed by a verb projection
lacking a particle: VBN/PRT (propped).
4.4 Comparison with Left-Corner Transform
A right-corner transform is used in this study, rather than a left-corner transform,
mainly because the right-corner version coincides with an intuition about how incom-
plete constituents might be stored in human memory. Stacked-up constituents in the
right-corner form correspond to chunks of words that have been encountered, rather
than hypothesized goal constituents. Intuitively, in the right-corner view, after a sen-
tence has been recognized, the stack memory contains a complete sentential constituent
(and some associated referent). In the left corner view, on the other hand, the stackmem-
ory after a sentence has been recognized contains only the lower-rightmost constituent
in the corresponding phrase structure tree (see Figure 4). This is because a time-order
Figure 4
A left-corner transformed version of the tree (a) and memory store (b) from Figures 2 and 3.
15
Computational Linguistics Volume 36, Number 1
alignment of a left-corner tree to elements in a bounded memory store corresponds to
a top-down traversal of the tree, whereas a time-order alignment of a right-corner tree
to elements in a bounded memory store corresponds to a bottom-up traversal of the
tree. If referential semantics are assumed to be calculated in tandem (as suggested by
the Tanenhaus et al [1995] results), a top-down traversal through time requires some
effort to reconcile with the traditional compositional semantic notion that the meanings
of constituents are composed from the meanings of their parts (Frege 1892).
4.5 Comparison with CCG
The incomplete constituent categories generated in the right-corner transform have the
same form and much of the same meaning as non-constituent categories in a CCG
(Steedman 2000).10 Both CCG operations of forward function application:
A1  A1/A2 A2
and forward function composition:
A1/A3  A1/A2 A2/A3
appear in the branching structure of right-corner transformed trees. Nested operations
can also occur in CCG derivations:
A1/A2  A1/A2/A3 A3
as well as in right-corner transformed trees (using underscore delimiters to denote
sequences of constituent categories, described in Section 5.1):
A1/A2  A1/A3 A2 A3
There are also correlates of type-raising (unary branches introduced by the right-corner
transform operations described in Section 4):
A1/A2  A3
But, importantly, the right-corner transform generates no correlates to the CCG
operations of backward function application or composition:
A1  A2 A1\A2
A1\A3  A2\A3 A1\A2
This has two consequences. First, right-corner transform models do not introduce am-
biguity between type-raised forward and backward operations, as CCG derivations do.
Second, because leftward dependencies (as between a verb and its subject in English)
cannot be incorporated into lexical categories, right-corner transform models cannot be
taken to explicitly encode argument structure, as CCGs are. The right-corner transform
model described in this article is therefore perhaps better regarded as a performance
model of processing, given subcategorizations specified in some other grammar (such
as in this case the Treebank grammar), rather than a constraint on grammar itself.
10 In fact, one of the original motivations for CCG as a model of language was to minimize stack usage in
incremental processing (Ades and Steedman 1982).
16
Schuler et al Parsing Using Human-Like Memory Constraints
4.6 Comparison with Cascaded FSAs in Information Extraction
Hierarchies of weighted finite-state automata (FSA)?equivalent HMMs may also be
viewed as probabilistic implementations of cascaded FSAs, used for modeling syntax
in information extraction systems such as FASTUS (Hobbs et al 1996). Indeed, the left-
branching sequences of transformed constituents recognized by this model (as shown
in Figure 3) bear a strong resemblance to the flattened phrase structure representations
recognized by cascaded FSA systems, in that most phrases are consolidated to flat
sequences at one hierarchy level. This flat structure is desirable in cascaded FSA systems
because it allows information to be extracted from noun or verb phrases using straight-
forward pattern matching rules, implemented as FSA-equivalent regular expressions.
Like FASTUS, this system produces layers of flat phrases that can be searched
using regular expression pattern-matching rules. It also has a fixed number of levels
and linear-time recognition complexity. But unlike FASTUS, the model described here
can produce?and can be trained on?complete phrase structure trees (accessible by
reversing the transforms described previously).
5. Coverage
The coverage of this model was evaluated on the large Penn Treebank corpus of
syntactically annotated sentences from the Switchboard corpus of transcribed speech
(Godfrey, Holliman, and McDaniel 1992) and the Wall Street Journal (Marcus, Santorini,
and Marcinkiewicz 1993). These sentences were right-corner transformed and mapped
to a time-aligned bounded memory store as described in Section 4 to determine the
amount of memory each sentence would require.
5.1 Binary Branching Structure
In order to obtain a linguistically plausible right-corner transform representation of
incomplete constituents, the corpus is subjected to another pre-process transform to
introduce binary-branching nonterminal projections, and fold empty categories into
nonterminal symbols in amanner similar to that proposed by Johnson (1998b) and Klein
and Manning (2003). This binarization is done in such a way as to preserve linguistic
intuitions of head projection, so that the depth requirements of right-corner transformed
trees will be reasonable approximations to the working memory requirements of a
human reader or listener.
First, ordinary phrases and clauses are decomposed into head projections, each
consisting of one subordinate head projection and one argument or modifier, for
example:
A0
. . . VB:?1 NP:?2 . . .
?
A0
. . . VB
VB:?1 NP:?2
. . .
The selection of head constituents is done using rewrite rules similar to the Magerman-
Black head rules (Magerman 1995). Any new constituent created by this process is
17
Computational Linguistics Volume 36, Number 1
assigned the label of the subordinate head projection. The subordinate projection may
be the left or complete list of head-projection rewrite rules is provided in Appendix A.11
Conjunctions are decomposed into purely right-branching structures using non-
terminals appended with a ?-LIST? suffix:
A0
. . . A1:?1 CC A1:?2
?
A0
. . . A1-LIST
A1:?1 CC A1:?2
A0
. . . A1:?1 A1-LIST:?2
?
A0
. . . A1-LIST
A1:?1 A1-LIST:?2
This right-branching decomposition of conjoined lists is motivated by the general
preference in English toward right branching structure, and the distinction of right
children as ?-LIST? categories is motivated by the asymmetry of conjunctions such as
and and or generally occurring only between constituents at the end of a list, not at the
beginning. (Thus, in decomposing coffee, tea or milk, the words tea or milk form an NP-
LIST constituent, whereas the words coffee, tea do not.)
Empty constituents are removed outright, along with any unary projections that
may arise from this removal. In the case of empty constituents representing traces, the
extracted category label is annotated onto the lowest nonterminal dominating the trace
using the suffix ?-extrX,? where ?X? is the category of the extracted constituent. To
preserve grammaticality, this annotation is then passed up the tree and eliminated when
awh-, topicalized, or othermoved constituent is encountered, in amanner similar to that
used in Head-driven Phrase Structure Grammar (Pollard and Sag 1994), but this does
not affect branching structure.
Together these rewrites remove about 65% of super-binary branches from the un-
processed Treebank. All remaining super-binary branches are ?nominally? decomposed
into right-branching structures by introducing intermediate nodes, each with a label
concatenated from the labels of its children, delimited by underscores:
A0
. . . A1:?1 A2:?2
?
A0
. . . A1 A2
A1:?1 A2:?2
This decomposition is ?nominal? in that the concatenated labels leave the resulting bi-
nary branches just as complex as the original n-ary branches prior to this decomposition.
It is equivalent to leaving super-binary branches intact and using dot rules in parsing
11 Although it is possible that in some cases these rules may generate counterintuitive branching patterns,
inspection of transformed trees during this experiment showed no unusual branching structure, except in
the case of noun sequences in base noun phrases (e.g. [general obligation] bonds or general [obligation
bonds]), which were left flat in the Treebank. Correct binarization of these structures would require
extensive annotator effort. However, because base noun phrases are often very small, and seldom contain
any sub-structure, it seems safe to assume that structural errors in these base noun phrases would not
drastically alter coverage results reported in this section.
18
Schuler et al Parsing Using Human-Like Memory Constraints
(Earley 1970). This decomposition therefore does nothing to reduce sparse data effects
in statistical parsing.
5.2 Coverage Results
Sections 2 and 3 (the standard training set) of the Penn Treebank Switchboard corpus
were binarized as described in Section 5.1, then right-corner transformed and mapped
to elements in a boundedmemory store as described in Section 4. Punctuation added by
transcribers was removed. Coverage of this corpus, in sentences, for a recognizer using
right-corner transform chunking with one to five levels of stack memory, is shown in
Table 1. These results show that a simple syntax-based chunking into incomplete con-
stituents, using the right-corner transform defined in Section 4 of this article, allows a
vast majority of Switchboard sentences (over 99%) to be recognized using three or fewer
elements of memory, with no sentences requiring more than five elements, essentially
as predicted by studies of human short-term memory.
Although spontaneous speech is arguably more natural test data than prepared
speech or edited text, it is possible that coverage results on these data may under-
estimate processing requirements, due to the preponderance of very short sentences
and sentence fragments in spontaneous speech (for example, nearly 30% of sentences in
the Switchboard corpus are only one word long). It may also be argued that coverage
results on this corpus more accurately reflect the complexity of speech planning under
somewhat awkward social circumstances (being asked to start a conversation with
a stranger), which may be more cognitively demanding than recognition. For these
reasons, the right-corner transform chunking was also evaluated on Sections 2?21 (the
standard training set) of the Penn Treebank Wall Street Journal (WSJ) text corpus (see
Table 2, column 1).
The WSJ text corpus results appear to show substantially higher memory
requirements than Switchboard, with only 93% of sentences recognizable using three or
fewer memory elements. But much of this increase is due to arguably arbitrary treebank
conventions in annotating punctuation (for example, commas between phrases are
attached to the leftmost phrase: ([Pierre Vinken . . . [61 years old] ,] joined . . . ) which
can lead to psycholinguistically implausible analyses in which phrases (in this case
61 years old) are center-embedded by lone punctuation marks on one side or the other.
In general, branching structure for punctuation can be difficult to motivate on linguistic
grounds, because punctuation marks do not have lexical projections or argument
structure in most linguistic theories. In spoken language, punctuation corresponds to
Table 1
Percent coverage of right-corner transformed Switchboard Treebank Sections 2?3.
memory capacity (right-corner, no punct) sentences coverage
no stack memory 26,201 28.38%
1 stack element 53,740 58.21%
2 stack elements 85,068 92.14%
3 stack elements 91,890 99.53%
4 stack elements 92,315 99.99%
5 stack elements 92,328 100.00%
TOTAL 92,328 100.00%
19
Computational Linguistics Volume 36, Number 1
Table 2
Percent coverage of left- and right-corner transformed WSJ Treebank Sections 2?21.
memory capacity right-corner, with punct right-corner, no punct left-corner, no punct
sentences coverage sentences coverage sentences coverage
no stack elements 35 0.09% 127 0.32% 127 0.32%
1 stack elements 3,021 7.57% 3,550 8.90% 4,284 10.74%
2 stack elements 21,916 54.95% 25,948 65.06% 26,750 67.07%
3 stack elements 37,203 93.28% 38,948 97.66% 38,853 97.42%
4 stack elements 39,703 99.54% 39,866 99.96% 39,854 99.93%
5 stack elements 39,873 99.97% 39,883 100.00% 39,883 100.00%
6 stack elements 39,883 100.00% - - - -
TOTAL 39,883 100.00% 39,883 100.00% 39,883 100.00%
pauses or patterns of inflection, distributed throughout an utterance. It therefore seems
questionable to account for punctuation marks in a psycholinguistic model as explicit
composable concepts in a memory store. In order to counter possible undesirable
effects of an arbitrary branching analysis of punctuation, a second evaluation of the
model was performed on a version of the WSJ corpus with punctuation removed.
An analysis (Table 2, column 2) of the Penn Treebank WSJ corpus Sections 2?21
without punctuation, using the right-corner transformed trees just described, shows
that 97.66% of trees can be recognized using three hidden levels, and 99.96% can be
recognized using four, and again (similar to the Switchboard results), no sentences
require more than five remembered incomplete constituents. Table 2, column 3, shows
similar results for a left-corner transformed corpus, using left-right reflections of the
rewrite rules presented in Section 4.
Cowan (2001) documents empirically observed short-term memory limits of about
four elements across awide variety of tasks. It is therefore not surprising to find a similar
limit in the memory required to parse the Treebank, assuming elements corresponding
to right-corner-transformed incomplete constituents.
As the table shows, some quintuply center-embedded constituents were found in
both corpora, suggesting that a three- to four-element limit may be soft, and can be
relaxed for short durations. Indeed, all quintuply embedded constituents were only a
few words long. Interestingly, many of the most heavily embedded words seemed to
strongly co-occur, which may suggest that these words arise from fixed expressions and
are not compositional. For example, Figure 5 shows one of the 13 phrase structure trees
in the Switchboard corpus which require five stack elements in right-corner parsing.
The complete sentence is:
So if there?s no one else around and I have a chance to listen to something I?ll turn that on.
If the construction there ?s NP AP in this sentence is parsed non-compositionally as a
single expression (and thus is rendered left-branching by the right-corner transform as
defined in Section 4), the sentence could be parsed using only four memory elements.
Even constrained to only four center embeddings, the existence of such sentences
confounds explanations of the center-embedding difficulties as directly arising from
stack limits in a left-corner (or right-corner) parser (Abney and Johnson 1991). It is
also interesting to note that three of the incomplete constituents in this example are
recursively nested or self-embedded instances of sentential projections, essentially with
20
Schuler et al Parsing Using Human-Like Memory Constraints
Figure 5
A phrase structure tree requiring five stack elements. Categories in bold will be incomplete at a
point after recognizing so if there?s no . . .
the same category, similar to the center-embedded constructions which human readers
found difficult to process. This suggests that restrictions on self-embedding of identical
constituent categories would also fail to predict readability.
Instead, these data seem to argue in favor of an explanation due to probability:
Although the five-element sentences found in the Treebank use mostly common phrase
structure rules, problematic center-embedded sentences like the salmon the man the dog
chased smoked fell may cause difficulty simply because they are examples of an unusual
construction: a nested object relative clause. The fact that this is an unusual construction
may in turn be a result of the fact that speakers tend to avoid nesting object relative
clauses because they can lead to memory exhaustion, though such constructions may
become readable with practice.
6. In-Element Composition Ambiguity and Parsing Accuracy
The right-corner transform described in Section 4 saves memory because it transforms
any right-branching sequence with left-child subtrees into a left-branching sequence of
incomplete constituents, with the same sequence of subtrees as right children. The left-
branching sequences of siblings resulting from this transform can then be composed
bottom-up through time by replacing each left child category with the category of the
resulting parent, within the same memory element (or depth level). For example, in
Figure 6(a) a left-child category NP/NP at time t = 4 is composed with a noun new of
category NP/NNP (a noun phrase lacking a proper noun yet to come), resulting in a
new parent category NP/NNP at time t = 5 replacing the left child category NP/NP in
the topmost d = 1 memory element.
This in-element composition preserves elements of the bounded memory store for
use in processing descendants of this composed constituent, yielding the human-like
memory demands reported in Section 5. But whenever an in-element composition like
this is hypothesized, it isolates an intermediate constituent (in this example, the noun
phrase new york city) from subsequent composition. Allowing access to this intermediate
constituent?for example, to allow new york city to become a modifier of bonds, which
itself becomes an argument of for?requires an analysis in which the intermediate
21
Computational Linguistics Volume 36, Number 1
Figure 6
Alternative analyses of strong demand for new york city ...: (a) using in-element composition,
compatible with strong demand for new york city is ... (in which the demand is for the city); and (b)
using cross-element (or delayed) composition, compatible with either strong demand for new york
city is ... (in which the demand is for the city) or strong demand for new york city bonds is ... (in
which a forthcoming referent?in this case, bonds?is associated with the city, and is in
demand). In-element composition (a) saves memory but closes off access to the noun phrase
headed by city, and so is not incompatible with the ...bonds completion. Cross-element
composition (b) requires more memory, but allows access to the noun phrase headed by city, so
is compatible with either completion. This ambiguity is introduced at t = 4 and propagated until
at least t = 7. An ordinary, non-right-corner stack machine would exclusively use analysis (b),
avoiding ambiguity.
constituent is stored in a separate memory element, shown in Figure 6(b). This creates
a local ambiguity in the parser (in this case, from time step t = 4) that may have to be
propagated across several words before it can be resolved (in this case, at time step
t = 7). This is essentially an ambiguity between arc-eager (in-element) and arc-standard
(cross-element) composition strategies, as described by Abney and Johnson (1991). In
contrast, an ordinary (purely arc-standard) parser with an unbounded stack would only
hypothesize analysis (b), avoiding this ambiguity.12
The right-corner HHMM approach described in this article relies on a learned
statistical model to predict when in-element (arc-eager) compositions will occur, in
addition to hypothesizing parse trees. The model encodes a mixed strategy: with some
probability arc-eager or arc-standard for each possible expansion. Accuracy results on
a right-corner HHMM model trained on the Penn Wall Street Journal Treebank suggest
that this kind of optionally arc-eager strategy can be reliably statistically learned.
6.1 Evaluation
In order to determinewhether amemory-preserving parsing strategy, like the optionally
arc-eager strategy, can be reliably learned, a baseline Cocke-Kasami-Younger (CKY)
parser and bounded-memory right-corner HHMM parser were evaluated on the stan-
dard Penn Treebank WSJ Section 23 parsing task, using the binarized tree set described
in Section 5.2 (WSJ Sections 2?21) as training data. Training examples requiring more
12 It is important to note that neither the right-corner nor left-corner parsing strategy by itself creates this
ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce
memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as
that of Henderson (2004) adopt an arc-standard strategy, essentially always choosing analysis (b), and
thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must
maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in
short-term memory (Resnik 1992).
22
Schuler et al Parsing Using Human-Like Memory Constraints
than four stack elements were excluded from training, in order to avoid generating
inconsistent model probabilities (e.g., from expansions that could not be re-composed
within the bounded memory store).
Most likely sequences of HHMM stack configurations are evaluated by reversing
the binarization, right-corner, and time-series mapping transforms described in Sec-
tions 4 and 5. But some of the binarization rewrites cannot be completely reversed,
because they cannot be unambiguously matched to output trees. Automatically derived
lexical projections below the annotated phrase level (e.g., binarizations of base noun
phrases) can be completely reversed, because the derived categories are character-
istically labeled with terminal symbols. So, too, can the conjunction and ?nominal?
binarizations described in Section 5.1, because they can be identified by characteristic
?-LIST? and underscore delimiters. But automatically derived projections above the
annotated phrase level cannot be reliably identified in parser output (for example, an
intermediate projection ?S PP S?may or may not be annotated in the corpus). In order
to isolate the evaluation from the effects of these ambiguous matchings, the evaluation
was performed using trees in a partially binarized format, obtained by reversing only
those rewrites that result in unambiguous matches. Evaluating on this partially bina-
rized data does not seem to unfairly increase parsing performance compared to other
published results?quite the contrary: an evaluation using the state-of-the-art Charniak
(2000) parser scores about half a point worse on labeled F-score (89.3% vs. 89.9%) when
its hypotheses and gold standard trees are converted into this format.13
Both CKY baseline and HHMM test systems were run with a simple part of speech
(POS) model using relative frequency estimates from the training set, backed off to a
discriminative (decision tree) model conditioned on the last five letters of each word,
normalized over unigram POS probabilities. The CKY baseline andHHMMresults were
obtained by training and evaluating on binarized trees, which is a necessary condition
for the right-corner transform. The CKY baseline results appear to be better than those
for a baseline probabilistic context-free grammar (PCFG) system reported by Klein and
Manning (2003) using no modifications to the corpus, and no parent or sibling condi-
tioning (see Table 3, top) because the binarization process allows the parser to avoid
some sparse data effects due to large flat branching structures in the Treebank, resulting
in improved parsing accuracy. Klein and Manning note that applying linguistically
motivated binarization transforms can yield substantial improvements in accuracy?as
much as nine points, in their study (in comparison, binarization only seems to improve
accuracy by about seven points above an unmodified baseline in the present study). But
the Klein and Manning results for binarization are provided only for models already
augmented with Markov dependencies (that is, conditioning on parent and sibling
categories, analogous to HHMM dependencies), so it was not possible to compare to
a binarized and un-Markovized benchmark.
The results for HHMM parsing, training, and evaluating on these same binarized
trees (modulo right-corner and variable-mapping transforms) were substantially bet-
ter than binarized CKY, most likely due to the expanded HHMM dependencies on
previous (qdt?1) and parent (q
d?1
t ) variables at each q
d
t . For example, binarized PCFG
probabilities may be defined in terms of three category symbols A, B, and C: P(A 
B C |A); whereas some of the HHMM probabilities are defined in terms of five category
13 This is presumably because the probability that a human annotator will annotate phrase structure
brackets at a particular projection or not is something existing parsers learn and exploit to improve their
accuracy. But it is not clear that this distinction is linguistically motivated.
23
Computational Linguistics Volume 36, Number 1
Table 3
Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure
(% of sentences yielding no tree output) results for basic CKY parser and HHMM parser on
unmodified and binarized WSJ Sections 22 (sentences 1?393: ?devset?) and 23?24 (all sentences).
Results are shown with and without punctuation, compared to Klein and Manning 2003
(KM?03) using baseline and parent+sibling (par+sib) conditioning, and Roark 2001 (R?01) using
parent+sibling conditioning. Baseline CKY and test (parent+sibling) cases for the HHMM
system start out at a higher accuracy than for the Klein-Manning system because the HHMM
system requires binarization of trees, which removes some data sparsity in the raw Treebank
annotation, whereas the Klein-Manning results are computed prior to binarization. Because it is
incremental, the parser occasionally eliminates all continuable analyses from the beam, and
therefore fails to find a parse. HHMM parse failures are accounted as zeros in the recall statistics,
but are also listed separately, because in principle it might be possible to recover useful syntactic
structure from partial sequences.
with punctuation: (?40 wds) LR LP F-score sentence error
failure reduction
KM?03: unmodified, devset ? ? 72.6 0
KM?03: par+sib, devset ? ? 77.4 0 17.5%
CKY: binarized, devset 80.3 79.9 80.1 0.8
HHMM: par+sib, devset 84.1 83.5 83.8 0.5 18.6%
CKY: binarized, sect 23 78.8 79.4 79.1 0.1
HHMM: par+sib, sect 23 83.4 83.7 83.5 0.1 21.1%
no punctuation: (?120 wds) LR LP F fail
R?01: par+sib, sect 23?24 75.2 77.4 ? 0.1
HHMM: par+sib, sect 23?24 77.2 78.3 77.7 0.0
labels: P(A/B |C/D, E) (transitioning from incomplete constituent C/D to incomplete
constituent A/B in the context of an expanding category E). This increases the number
of free parameters (estimated conditional probabilities) in the model,14 but apparently
not to the point of sparsity; this is similar to the effect of horizontal Markovization (con-
ditioning on the sibling category immediately previous to an expanded category) and
vertical Markovization (conditioning on the parent of an expanded category) commonly
used in PCFG parsing models (Collins 1999).
The improvement due to HHMM parsing over the PCFG baseline (18.6% reduction
in error) is comparable to that reported by Klein and Manning for parent and sibling
dependencies (first-order vertical and horizontal Markovization) over a baseline PCFG
without binarization (17.5% reduction in error). However, because it is not possible
to run the HHMM parser without binarization, and because Klein and Manning do
not report results for binarization transforms in the absence of parent and sibling
Markovization, it is potentially misleading to compare the results directly. For example,
it is possible that the binarization transforms described here may have performance-
optimizing effects that are latent in the binarized PCFG, but are brought out in HHMM
parsing.
Results on Section 23 of this corpus show close to 84% recall and precision, compa-
rable to that reported for state-of-the-art cubic-time parsers (with no constant bounds
14 Without punctuation, the HHMMmodel has 50,429 free parameters (including both Q and F models),
whereas the binarized PCFG has 12,373.
24
Schuler et al Parsing Using Human-Like Memory Constraints
on processing storage) using similar configurations of conditioning information, that is,
without lexicalization or smoothing.
Roark (2001) describes a similar incremental parser based on left-corner trans-
formed grammars, and also reports results for parsing with and without parent and
sibling Markovization. Again the performance is comparable under similar conditions
(Table 3, bottom).
This system was run with a beam width of 2,000 hypotheses. This beam width
was selected in order to compare the performance of the bounded-memory model,
which predicts in-element or cross-element composition, with that of conventional
broad-coverage parsers, which also maintain large beams. With better modeling and
vastly more data from which to learn, it is possible that the human processor may
need to maintain far fewer alternative analyses, or perhaps only one, conditioned on
a lookahead window of observations (Henderson 2004).15
These experiments used a maximum stack depth of four, and conditioned expan-
sion and transition probabilities for each qdt on only the portion of the parent category
following the slash (that is, only A2 of A1/A2), in order to avoid sparse data effects.
Examples requiring more than four stack elements were excluded from training. This
is because in the basic relative frequency estimation used here, training examples are
depth-specific. Because the (unpunctuated) training set contains only about a dozen
sentences requiring more than four depth levels, each occupying that level for only a
few words, the data on which the fifth level of this model would be trained are very
sparse. Models at greater stack depths, and models depending on complete parent cate-
gories (or grandparent categories, etc., as in state-of-the-art parsers) could be developed
using smoothing and backoff techniques or feature-based log-linear models, but this is
left for later work (see Section 7).
7. Conclusion
This article has described a model of human syntactic processing that recognizes com-
plete phrase structure trees using only a small store of memory elements of limited
complexity. Sequences of hypothesized contents of this memory store can be mapped to
and from conventional phrase structure trees using a reversible right-corner transform.
If this syntactic processing model is combined with a bounded-memory interpreter
(Schuler, Wu, and Schwartz 2009), however, allowing the contents of this store to be
incrementally interpreted within the same bounded memory, it stands to reason that
complete, explicit phrase structure trees would not need to be constructed at any time
in processing, in keeping with experimental results showing similar lack of retention of
words and syntactic structure during human processing (Sachs 1967; Jarvella 1971).
Initial results show the use of a memory store consisting of only three to four mem-
ory elements within this framework provides nearly complete coverage of the Penn
Treebank Switchboard and WSJ corpora, consistent with recent estimates of general-
purpose short-term memory capacity. This suggests that, unlike some earlier mod-
els, the hypothesis that human sentence processing uses general-purpose short-term
15 Although, if most competing analyses are unconscious, they would be difficult to detect. Formally, the
competing pockets of activation hypothesized in a parallel-processing version of this model could be
arbitrarily small and numerous, but it seems unlikely that very small pockets of activation would persist
for very long (just as low probability analyses would be unlikely to remain on the HHMM beam). This
possibility is discussed in the particle filter account of Levy (2008).
25
Computational Linguistics Volume 36, Number 1
memory to store incomplete constituents, as defined by a right-corner transform, does
not seem to substantially underestimate human processing capacity. Moreover, despite
additional predictions that must take place within this model to manage parsing in such
close quarters, preliminary accuracy results for an unlexicalized, un-smoothed version
of this model, using only a four-element memory store, show close to 84% recall and
precision on the standard parsing evaluation. This result is comparable to that reported
for state-of-the-art cubic-time parsers (with no constant bounds on processing storage)
using similar configurations of conditioning information, namely, without lexicalization
or smoothing.
This model does not attempt to derive processing difficulties frommemory bounds,
following evidence that garden path and center-embedding processing difficulties are
caused by interference or local probability estimates rather than encounters with mem-
ory capacity limits. But this does not mean that memory store capacity and probabilistic
explanations of processing difficulty are completely independent. Probability estima-
tion seems likely to be dependent on structural information from the memory store (for
example, incomplete object relative clauses seem to be very improbable in the context
of other incomplete object relative clauses). As hypotheses use more elements in the
memory store, the distribution over these hypotheses will tend to become broader,
taxing the reservoir of activation capacity, and making it more likely for low proba-
bility hypotheses to disappear, increasing the incidence of garden path errors. Further
investigations into how the memory store elements are allocated in various syntactic
contexts may allow these apparently disparate dimensions of processing capacity to be
unified.
The model described here may be promising as an engineering tool as well. But
to achieve competitive performance with unconstrained state-of-the-art parsers will
require the development of additional approximation algorithms beyond the scope of
this article. This is because most modern parsers are lexicalized, incorporating head-
word dependencies into parsing decisions, and employing finely tuned smoothing and
backoff techniques to integrate these potentially sparse head-word dependencies with
denser unlexicalized models. The bounded-memory right-corner HHMM described
in this article can also be lexicalized in this way, but because head word dependencies
are most straightforwardly defined in terms of top-down PCFG-like dependency
structures, this lexicalization requires the introduction of additional formal machinery
to transform PCFG probabilities into right-corner form (Schuler 2009). In other
words, rather than transforming a training set of trees and mapping them to a time
series model, it is necessary to transform a consistent probabilistically weighted
grammar (in some sense, an infinite set of trees) into appropriately weighted and
consistent right-corner PCFG and HHMM models. This requires the introduction of
an approximate inference algorithm, similar to that used in value iteration (Bellman
1957), which estimates probabilities of infinite left-recursive or right-recursive chains
by exploiting the fact that increasingly longer chains of events contribute exponentially
decreasing probability mass. On top of this, preserving head-word dependencies in
incremental processing also requires the introduction of a framework for storing head
words of modifier constituents that precede the head word of a parent constituent;
including some mechanism to ensure that probability assignments are fairly distributed
among competing hypotheses (e.g., by marginalizing over possible head words) in
cases where the calculation of accurate dependency probabilities must be deferred
until the head word of the parent constituent is encountered. For these reasons, a
complete lexicalized model is considered beyond the scope of this article, and is left for
future work.
26
Schuler et al Parsing Using Human-Like Memory Constraints
Appendix A: Head Transform Rules
The experiments described in this article used a binarization process that included the
following rewrite rules, designed to binarize flat Treebank constituents into linguisti-
cally motivated head projections:
1. NP: right-binarize basal NPs as much as possible; then left-binarize NPs
after left context reduced to nil:
A0=NP|WHNP
. . . A1=[A-Z]*:?1 A2=NN[A-Z]*:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=NP
A1=NN[A-Z]*|NP:?1 A2=PP|S|VP|WHSBAR:?2 . . .
?
A0
A1
A1:?1 A2:?2
. . .
2. VP: left-binarize basal VPs as much as possible; then right-binarize VPs
after right context reduced to nil:
A0=VP|SQ
. . . A1=VB[A-Z]*|BES:?1 A2=[A-Z]*:?2 . . .
?
A0
. . . A1
A1:?1 A2:?2
. . .
A0=VP
. . . A1=ADVP|RB[A-Z]*|PP:?1 A2=VB[A-Z]*|VP:?2
?
A0
. . . A2
A1:?1 A2:?2
3. ADJP: right-binarize basal ADJPs as much as possible; then left-binarize
ADJPs after left context reduced to nil:
A0=ADJP[A-Z]*
. . . A1=RB[A-Z]*:?1 A2=JJ[A-Z]*:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=ADJP
A1=JJ[A-Z]*|ADJP:?1 A2=PP|S:?2 . . .
?
A0
A1
A1:?1 A2:?2
. . .
4. ADVP: right-binarize basal ADVPs as much as possible; then left-binarize
ADVPs after left context reduced to nil:
A0=ADVP
. . . A1=RB[A-Z]*:?1 A2=RB[A-Z]*:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=ADVP
A1=RB[A-Z]*|ADVP:?1 A2=PP|S:?2 . . .
?
A0
A1
A1:?1 A2:?2
. . .
27
Computational Linguistics Volume 36, Number 1
5. PP: left-binarize PPs as much as possible; then right-binarize PPs after
right context reduced to nil:
A0=PP|SBAR
. . . A1=IN|TO:?1 A2=[A-Z]*:?2 . . .
?
A0
. . . A1
A1:?1 A2:?2
. . .
A0=PP
. . . A1=ADVP|RB|PP:?1 A2=PP:?2
?
A0
. . . A2
A1:?1 A2:?2
6. S: group subject NP and predicate VP of a sentence; then group modifiers
to right and left:
A0=S[A-Z]*
. . . A1=NP:?1 A2=VP:?2 . . .
?
A0
. . . S
A1:?1 A2:?2
. . .
A0=S[A-Z]*
. . . A1=ADVP|RB[A-Z]*|PP:?1 A2=VB[A-Z]*|VP:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=S[A-Z]*
. . . A1=ADVP|RB[A-Z]*|PP:?1 A2=A0:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=S[A-Z]*
. . . A1=A0:?1 A2=ADVP|RB[A-Z]*|PP:?2 . . .
?
A0
. . . A1
A1:?1 A2:?2
. . .
Acknowledgments
The authors would like to thank
the anonymous reviewers for their input.
This research was supported by National
Science Foundation CAREER/PECASE
award 0447685 and by NASA award
NNX08AC36A. The views expressed are not
necessarily endorsed by the sponsors.
References
Abney, Steven P. and Mark Johnson. 1991.
Memory requirements and local
ambiguities of parsing strategies.
J. Psycholinguistic Research, 20(3):233?250.
Ades, Anthony E. and Mark Steedman. 1982.
On the order of words. Linguistics and
Philosophy, 4:517?558.
Aho, Alfred V. and Jeffery D. Ullman. 1972.
The Theory of Parsing, Translation and
Compiling; Volume. I: Parsing. Prentice-Hall,
Englewood Cliffs, NJ.
Baker, James. 1975. The Dragon system: an
overview. IEEE Transactions on Acoustics,
Speech and Signal Processing, 23(1):24?29.
Bellman, Richard. 1957. Dynamic
Programming. Princeton University Press,
Princeton, NJ.
Berg, George. 1992. A connectionist parser
with recursive sentence structure and
lexical disambiguation. In Proceedings of the
Tenth National Conference on Artificial
Intelligence, pages 32?37, San Jose, CA.
Bever, Thomas G.?1970. The cognitive basis
for linguistic structure. In J. R?. Hayes,
editor, Cognition and the Development of
Language. Wiley, New York, pages 279?362.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference
resolution in the wild: Online
circumscription of referential domains in a
28
Schuler et al Parsing Using Human-Like Memory Constraints
natural interactive problem-solving task.
In Proceedings of the 24th Annual Meeting of
the Cognitive Science Society, pages 148?153,
Fairfax, VA.
Charniak, Eugene. 2000. A maximum-
entropy inspired parser. In Proceedings
of the First Meeting of the North American
Chapter of the Association for Computational
Linguistics (ANLP-NAACL?00),
pages 132?139, Seattle, WA.
Chomsky, Noam and George A. Miller.
1963. Introduction to the formal
analysis of natural languages.
In Handbook of Mathematical Psychology.
Wiley, New York, pages 269?321.
Collins, Michael. 1999. Head-driven
statistical models for natural language parsing.
Ph.D. thesis, University of Pennsylvania.
Cowan, Nelson. 2001. The magical
number 4 in short-term memory:
A reconsideration of mental storage
capacity. Behavioral and Brain Sciences,
24:87?185.
Crain, Stephen and Mark Steedman.
1985. On not being led up the garden path:
The use of context by the psychological
syntax processor. In D. R. Dowty,
L. Karttunen, and A. M. Zwicky, editors,
Natural Language Parsing: Psychological,
Computational, and Theoretical Perspectives,
number 1 in Studies in Natural Language
Processing. Cambridge University
Press, Cambridge, pages 320?358.
Earley, Jay. 1970. An efficient context-free
parsing algorithm. CACM, 13(2):94?102.
Elman, Jeffrey L. 1991. Distributed
representations, simple recurrent
networks, and grammatical structure.
Machine Learning, 7:195?225.
Ericsson, K. Anders and Walter Kintsch.
1995. Long-term working memory.
Psychological Review, 102:211?245.
Frege, Gottlob. 1892. Uber sinn
und bedeutung. Zeitschrift fur Philosophie
und Philosophischekritik, 100:25?50.
Gibson, Edward. 1991. A computational theory
of human linguistic processing: Memory
limitations and processing breakdown.
Ph.D. thesis, Carnegie Mellon University.
Godfrey, John J., Edward C. Holliman,
and Jane McDaniel. 1992. Switchboard:
Telephone speech corpus for research
and development. In Proceedings of
ICASSP, pages 517?520, San Francisco, CA.
Gordon, N. J., D. J. Salmond, and A. F. M.
Smith. 1993. Novel approach to nonlinear/
non-gaussian bayesian state estimation.
IEE Proceedings F (Radar and Signal
Processing), 140(2):107?113.
Gorrell, Paul. 1995. Syntax and Parsing.
Cambridge University Press, Cambridge.
Hale, John. 2001. A probabilistic earley parser
as a psycholinguistic model. In Proceedings
of the Second Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 159?166, Pittsburgh, PA.
Hale, John. 2006. Uncertainty about the
rest of the sentence. Cognitive Science,
30(4):609?642.
Helasvuo, Marja-Liisa. 2004. Shared
syntax: the grammar of co-constructions.
Journal of Pragmatics, 36:1315?1336.
Henderson, James. 2004. Lookahead
in deterministic left-corner parsing.
In Proceedings Workshop on Incremental
Parsing: Bringing Engineering and
Cognition Together, pages 26?33, Barcelona.
Hobbs, Jerry R., Douglas E. Appelt, John Bear,
David Israel, Megumi Kameyama, Mark
Stickel, and Mabry Tyson. 1996. Fastus:
A cascaded finite-state transducer for
extracting information from natural-
language text. In Yves Schabes, editor,
Finite State Devices for Natural Language
Processing. MIT Press, Cambridge, MA,
pages 383?406.
Jarvella, Robert J. 1971. Syntactic
processing of connected speech. Journal
of Verbal Learning and Verbal Behavior,
10:409?416.
Jelinek, Frederick, Lalit R. Bahl, and Robert L.
Mercer. 1975. Design of a linguistic
statistical decoder for the recognition
of continuous speech. IEEE Transactions
on Information Theory, 21:250?256.
Johnson, Mark. 1998a. Finite state
approximation of constraint-based
grammars using left-corner grammar
transforms. In Proceedings of COLING/ACL,
pages 619?623, Montreal.
Johnson, Mark. 1998b. PCFG models
of linguistic tree representation.
Computational Linguistics, 24:613?632.
Johnson-Laird, P. N. 1983. Mental Models:
Towards a Cognitive Science of Language,
Inference and Consciousness. Harvard
University Press, Cambridge, MA.
Just, Marcel Adam and Patricia A.
Carpenter. 1992. A capacity theory
of comprehension: Individual differences
in working memory. Psychological Review,
99:122?149.
Just, Marcel Adam and Sashank Varma.
2007. The organization of thinking:
What functional brain imaging
reveals about the neuroarchitecture of
complex cognition. Cognitive, Affective,
& Behavioral Neuroscience, 7:153?191.
29
Computational Linguistics Volume 36, Number 1
Kamide, Yuki and Don C. Mitchell. 1999.
Incremental pre-head attachment in
Japanese parsing. Language and
Cognitive Processes, 14:631?662.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo.
Lerner, Gene H. 1991. On the syntax of
sentences in progress. Language in
Society, 20:441?458.
Levy, Roger. 2008. Modeling the effects of
memory on human online sentence
processing with particle filters. In
Proceedings of NIPS, pages 937?944,
Vancouver.
Lewis, Richard L. and Shravan Vasishth.
2005. An activation-based model of
sentence processing as skilled
memory retrieval. Cognitive Science,
29(3):375?419.
Magerman, David. 1995. Statistical decision-
tree models for parsing. In Proceedings of the
33rd Annual Meeting of the Association
for Computational Linguistics (ACL?95),
pages 276?283, Cambridge, MA.
Marcus, Mitch. 1980. Theory of Syntactic
Recognition for Natural Language. MIT Press,
Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: the
Penn Treebank. Computational Linguistics,
19(2):313?330.
Mayberry, III, Marshall R. and Risto
Miikkulainen. 2003. Incremental
nonmonotonic parsing through semantic
self-organization. In Proceedings of the
25th Annual Conference of the Cognitive
Science Society, pages 798?803,
Boston, MA.
Miller, George A. 1956. The magical number
seven, plus or minus two: Some limits
on our capacity for processing information.
Psychological Review, 63:81?97.
Murphy, Kevin P. and Mark A. Paskin. 2001.
Linear time inference in hierarchical
HMMs. In Proceedings of NIPS,
pages 833?840, Vancouver.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure
Grammar. Chicago: University of Chicago
Press and Stanford: CSLI Publications.
Pritchett, Bradley L. 1991. Head position
and parsing ambiguity. Journal of
Psycholinguistic Research, 20:251?270.
Resnik, Philip. 1992. Left-corner parsing and
psychological plausibility. In Proceedings
of COLING, pages 191?197, Nantes.
Roark, Brian. 2001. Probabilistic top-down
parsing and language modeling.
Computational Linguistics, 27(2):249?276.
Rohde, Douglas L. T. 2002. A connectionist
model of sentence comprehension and
production. Ph.D. thesis, Computer Science
Department, Carnegie Mellon University.
Sachs, Jacqueline. 1967. Recognition memory
for syntactic and semantic aspects of
connected discourse. Perception and
Psychophysics, 2:437?442.
Schuler, William. 2009. Parsing with a
bounded stack using a model-based
right-corner transform. In Proceedings
of the North American Association for
Computational Linguistics (NAACL ?09),
pages 344?352, Boulder, CO.
Schuler, William, Stephen Wu, and
Lane Schwartz. 2009. A framework for
fast incremental interpretation during
speech decoding. Computational
Linguistics, 35(3):313?343.
Shieber, Stuart. 1985. Evidence
against the context-freeness of natural
language. Linguistics and Philosophy,
8:333?343.
Smolensky, P. and G. Legendre. 2006. The
Harmonic Mind: From Neural Computation
to Optimality-Theoretic Grammar. MIT Press,
Cambridge, MA.
Steedman, Mark. 2000. The Syntactic
Process. MIT Press/Bradford Books,
Cambridge, MA.
Stevenson, Suzanne. 1998. Parsing as
incremental restructuring. In J. D. Fodor
and F. Ferreira, editors, Reanalysis in
Sentence Processing. Kluwer Academic,
Boston, MA, pages 327?363.
Tanenhaus, Michael K., Michael J.
Spivey-Knowlton, Kathy M. Eberhard,
and Julie E. Sedivy. 1995. Integration of
visual and linguistic information in
spoken language comprehension. Science,
268:1632?1634.
30
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169?1178,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Pronoun Anaphora Resolution System based on
Factorial Hidden Markov Models
Dingcheng Li
University of Minnesota,
Twin Cities, Minnesosta
lixxx345@umn.edu
Tim Miller
University of Wisconsin
Milwaukee, Wisconsin
tmill@cs.umn.edu
William Schuler
The Ohio State University
Columbus, Ohio
schuler@ling.osu.edu
Abstract
This paper presents a supervised pronoun
anaphora resolution system based on factorial
hidden Markov models (FHMMs). The ba-
sic idea is that the hidden states of FHMMs
are an explicit short-term memory with an an-
tecedent buffer containing recently described
referents. Thus an observed pronoun can find
its antecedent from the hidden buffer, or in
terms of a generative model, the entries in the
hidden buffer generate the corresponding pro-
nouns. A system implementing this model is
evaluated on the ACE corpus with promising
performance.
1 Introduction
Pronoun anaphora resolution is the task of find-
ing the correct antecedent for a given pronominal
anaphor in a document. It is a subtask of corefer-
ence resolution, which is the process of determin-
ing whether two or more linguistic expressions in
a document refer to the same entity. Adopting ter-
minology used in the Automatic Context Extraction
(ACE) program (NIST, 2003), these expressions
are called mentions. Each mention is a reference
to some entity in the domain of discourse. Men-
tions usually fall into three categories ? proper men-
tions (proper names), nominal mentions (descrip-
tions), and pronominal mentions (pronouns). There
is a great deal of related work on this subject, so
the descriptions of other systems below are those
which are most related or which the current model
has drawn insight from.
Pairwise models (Yang et al, 2004; Qiu et al,
2004) and graph-partitioning methods (McCallum
and Wellner, 2003) decompose the task into a col-
lection of pairwise or mention set coreference de-
cisions. Decisions for each pair or each group
of mentions are based on probabilities of features
extracted by discriminative learning models. The
aforementioned approaches have proven to be fruit-
ful; however, there are some notable problems. Pair-
wise modeling may fail to produce coherent parti-
tions. That is, if we link results of pairwise deci-
sions to each other, there may be conflicting corefer-
ences. Graph-partitioning methods attempt to recon-
cile pairwise scores into a final coherent clustering,
but they are combinatorially harder to work with in
discriminative approaches.
One line of research aiming at overcoming the
limitation of pairwise models is to learn a mention-
ranking model to rank preceding mentions for a
given anaphor (Denis and Baldridge, 2007) This ap-
proach results in more coherent coreference chains.
Recent years have also seen the revival of in-
terest in generative models in both machine learn-
ing and natural language processing. Haghighi
and Klein (2007), proposed an unsupervised non-
parametric Bayesian model for coreference resolu-
tion. In contrast to pairwise models, this fully gener-
ative model produces each mention from a combina-
tion of global entity properties and local attentional
state. Ng (2008) did similar work using the same un-
supervised generative model, but relaxed head gen-
eration as head-index generation, enforced agree-
ment constraints at the global level, and assigned
salience only to pronouns.
Another unsupervised generative model was re-
cently presented to tackle only pronoun anaphora
1169
resolution (Charniak and Elsner, 2009). The
expectation-maximization algorithm (EM) was ap-
plied to learn parameters automatically from the
parsed version of the North American News Cor-
pus (McClosky et al, 2008). This model generates a
pronoun?s person, number and gender features along
with the governor of the pronoun and the syntactic
relation between the pronoun and the governor. This
inference process allows the system to keep track of
multiple hypotheses through time, including multi-
ple different possible histories of the discourse.
Haghighi and Klein (2010) improved their non-
parametric model by sharing lexical statistics at the
level of abstract entity types. Consequently, their
model substantially reduces semantic compatibility
errors. They report the best results to date on the
complete end-to-end coreference task. Further, this
model functions in an online setting at mention level.
Namely, the system identifies mentions from a parse
tree and resolves resolution with a left-to-right se-
quential beam search. This is similar to Luo (2005)
where a Bell tree is used to score and store the
searching path.
In this paper, we present a supervised pro-
noun resolution system based on Factorial Hidden
Markov Models (FHMMs). This system is moti-
vated by human processing concerns, by operating
incrementally and maintaining a limited short term
memory for holding recently mentioned referents.
According to Clark and Sengul (1979), anaphoric
definite NPs are much faster retrieved if the an-
tecedent of a pronoun is in immediately previous
sentence. Therefore, a limited short term memory
should be good enough for resolving the majority of
pronouns. In order to construct an operable model,
we also measured the average distance between pro-
nouns and their antecedents as discussed in next sec-
tions and used distances as important salience fea-
tures in the model.
Second, like Morton (2000), the current sys-
tem essentially uses prior information as a dis-
course model with a time-series manner, using a
dynamic programming inference algorithm. Third,
the FHMM described here is an integrated system,
in contrast with (Haghighi and Klein, 2010). The
model generates part of speech tags as simple struc-
tural information, as well as related semantic in-
formation at each time step or word-by-word step.
While the framework described here can be ex-
tended to deeper structural information, POS tags
alone are valuable as they can be used to incorpo-
rate the binding features (described below).
Although the system described here is evaluated
for pronoun resolution, the framework we describe
can be extended to more general coreference resolu-
tion in a fairly straightforward manner. Further, as
in other HMM-based systems, the system can be ei-
ther supervised or unsupervised. But extensions to
unsupervised learning are left for future work.
The final results are compared with a few super-
vised systems as the mention-ranking model (De-
nis and Baldridge, 2007) and systems compared in
their paper, and Charniak and Elsner?s (2009) unsu-
pervised system, emPronouns. The FHMM-based
pronoun resolution system does a better job than the
global ranking technique and other approaches. This
is a promising start for this novel FHMM-based pro-
noun resolution system.
2 Model Description
This work is based on a graphical model framework
called Factorial Hidden Markov Models (FHMMs).
Unlike the more commonly known Hidden Markov
Model (HMM), in an FHMM the hidden state at
each time step is expanded to contain more than one
random variable (as shown in Figure 1). This al-
lows for the use of more complex hidden states by
taking advantage of conditional independence be-
tween substates. This conditional independence al-
lows complex hidden states to be learned with lim-
ited training data.
2.1 Factorial Hidden Markov Model
Factorial Hidden Markov Models are an extension
of HMMs (Ghahramani and Jordan, 1997). HMMs
represent sequential data as a sequence of hidden
states generating observation states (words in this
case) at corresponding time steps t. A most likely
sequence of hidden states can then be hypothesized
given any sequence of observed states, using Bayes
Law (Equation 2) and Markov independence as-
sumptions (Equation 3) to define a full probability as
the product of a Transition Model (?T ) prior prob-
ability and an Observation Model (?O) likelihood
1170
probability.
h?1..T
def= argmax
h1..T
P(h1..T | o1..T ) (1)
def= argmax
h1..T
P(h1..T ) ? P(o1..T |h1..T ) (2)
def= argmax
h1..T
T
?
t=1
P?T (ht |ht?1) ? P?O(ot |ht)
(3)
For a simple HMM, the hidden state corresponding
to each observation state only involves one variable.
An FHMM contains more than one hidden variable
in the hidden state. These hidden substates are usu-
ally layered processes that jointly generate the ev-
idence. In the model described here, the substates
are also coupled to allow interaction between the
separate processes. As Figure 1 shows, the hidden
states include three sub-states, op, cr and pos which
are short forms of operation, coreference feature and
part-of-speech. Then, the transition model expands
the left term in (3) to (4).
P?T (ht |ht?1)
def= P(opt | opt?1, post?1)
?P(crt | crt?1, opt?1)
?P(post | opt, post?1)
(4)
The observation model expands from the right
term in (3) to (5).
P?O(ot |ht)
def= P(ot | post, crt) (5)
The observation state depends on more than one hid-
den state at each time step in an FHMM. Each hid-
den variable can be further split into smaller vari-
ables. What these terms stand for and the motiva-
tions behind the above equations will be explained
in the next section.
2.2 Modeling a Coreference Resolver with
FHMMs
FHMMs in our model, like standard HMMs, can-
not represent the hierarchical structure of a syntac-
tic phrase. In order to partially represent this in-
formation, the head word is used to represent the
whole noun phrase. After coreference is resolved,
the coreferring chain can then be expanded to the
whole phrase with NP chunker tools.
In this system, hidden states are composed of
three main variables: a referent operation (OP),
coreference features (CR) and part of speech tags
(POS) as displayed in Figure 1. The transition model
is defined as Equation 4.
opt-1=
copy
post-1=
VBZ
ot-1=loves
et-1=
per,org
gt-1=
neu,fem
crt-1
opt=
old
post=
PRP
ot=them
gt=
fem,neu
crt
ht-1 ht
et=
org,per
nt-1=
plu,sing
nt=
sing,plu
it-1=
-,2
it=
0,2
Figure 1: Factorial HMM CR Model
The starting point for the hidden state at each time
step is the OP variable, which determines which
kind of referent operations will occur at the current
word. Its domain has three possible states: none,
new and old.
The none state indicates that the present state will
not generate a mention. All previous hidden state
values (the list of previous mentions) will be passed
deterministically (with probability 1) to the current
time step without any changes. The new state signi-
fies that there is a new mention in the present time
step. In this event, a new mention will be added to
the entity set, as represented by its set of feature val-
ues and position in the coreference table. The old
state indicates that there is a mention in the present
time state and that this mention refers back to some
antecedent mention. In such a case, the list of enti-
ties in the buffer will be reordered deterministically,
moving the currently mentioned entity to the top of
the list.
Notice that opt is defined to depend on opt?1
and post?1. This is sometimes called a switching
FHMM (Duh, 2005). This dependency can be use-
ful, for example, if opt?1 is new, in which case opt
has a higher probability of being none or old. If
1171
post?1 is a verb or preposition, opt has more proba-
bility of being old or new.
One may wonder why opt generates post, and
not the other way around. This model only roughly
models the process of (new and old) entity genera-
tion, and either direction of causality might be con-
sistent with a model of human entity generation,
but this direction of causality is chosen to represent
the effect of semantics (referents) generating syn-
tax (POS tags). In addition, this is a joint model in
which POS tagging and coreference resolution are
integrated together, so the best combination of those
hidden states will be computed in either case.
2.3 Coreference Features
Coreference features for this model refer to features
that may help to identify co-referring entities.
In this paper, they mainly include index (I),
named entity type (E), number (N) and gender (G).
The index feature represents the order that a men-
tion was encountered relative to the other mentions
in the buffer. The latter three features are well
known and described elsewhere, and are not them-
selves intended as the contribution of this work. The
novel aspect of this part of the model is the fact that
the features are carried forward, updated after ev-
ery word, and essentially act as a discourse model.
The features are just a shorthand way of represent-
ing some well known essential aspects of a referent
(as pertains to anaphora resolution) in a discourse
model.
Features Values
I positive integers from 1. . .n
G male, female, neutral, unknown
N singular, plural, unknown
E person, location, organization,
GPE, vehicle,
company, facility
Table 1: Coreference features stored with each mention.
Unlike discriminative approaches, generative
models like the FHMM described here do not have
access to all observations at once. This model must
then have a mechanism for jointly considering pro-
nouns in tandem with previous mentions, as well as
the features of those mentions that might be used to
find matches between pronouns and antecedents.
Further, higher order HMMs may contain more
accurate information about observation states. This
is especially true for coreference resolution because
pronouns often refer back to mentions that are far
away from the present state. In this case, we would
need to know information about mentions which are
at least two mentions before the present one. In
this sense, a higher order HMM may seem ideal
for coreference resolution. However, higher order
HMMs will quickly become intractable as the order
increases.
In order to overcome these limitations, two strate-
gies which have been discussed in the last section
are taken: First, a switching variable called OP is
designed (as discussed in last section); second, a
memory of recently mentioned entities is maintained
to store features of mentions and pass them forward
incrementally.
OP is intended to model the decision to use the
current word to introduce a new referent (new), refer
to an antecedent (old), or neither (none). The entity
buffer is intended to model the set of ?activated? en-
tities in the discourse ? those which could plausibly
be referred to with a pronoun. These designs allow
similar benefits as longer dependencies of higher-
order HMMs but avoid the problem of intractability.
The number of mentions maintained must be limited
in order for the model to be tractable. Fortunately,
human short term memory faces effectively similar
limitations and thus pronouns usually refer back to
mentions not very far away.
Even so, the impact of the size of the buffer on
decoding time may be a concern. Since the buffer of
our system will carry forward a few previous groups
of coreference features plus op and pos, the compu-
tational complexity will be exorbitantly high if we
keep high beam size and meanwhile if each feature
interacts with others. Luckily, we have successfully
reduced the intractability to a workable system in
both speed and space with following methods. First,
we estimate the size of buffer with a simple count
of average distances between pronouns and their an-
tecedents in the corpus. It is found that about six is
enough for covering 99.2% of all pronouns.
Secondly, the coreference features we have used
have the nice property of being independent from
one another. One might expect English non-person
entities to almost always have neutral gender, and
1172
thus be modeled as follows:
P(et, gt | et?1, gt?1) = P(gt | gt?1, et) ? P(et | et?1)
(6)
However, a few considerations made us reconsider.
First, exceptions are found in the corpus. Personal
pronouns such as she or he are used to refer to coun-
try, regions, states or organizations. Second, existing
model files made by Bergsma (2005) include a large
number of non-neutral gender information for non-
person words. We employ these files for acquiring
gender information of unknown words. If we use
Equation 6, sparsity and complexity will increase.
Further, preliminary experiments have shown mod-
els using an independence assumption between gen-
der and personhood work better. Thus, we treat each
coreference feature as an independent event. Hence,
we can safely split coreference features into sepa-
rate parts. This way dramatically reduces the model
complexity. Thirdly, our HMM decoding uses the
Viterbi algorithm with A-star beam search.
The probability of the new state of the coreference
table P(crt | crt?1, opt) is defined to be the product
of probabilities of the individual feature transitions.
P(crt | crt?1, opt) = P(it | it?1, opt)?
P(et | et?1, it, opt)?
P(gt | gt?1, it, opt)?
P(nt |nt?1, it, opt)
(7)
This supposes that the features are conditionally in-
dependent of each other given the index variable, the
operator and previous instance. Each feature only
depends on the operator and the corresponding fea-
ture at the previous state, with that set of features
re-ordered as specified by the index model.
2.4 Feature Passing
Equation 7 is correct and complete, but in fact the
switching variable for operation type results in three
different cases which simplifies the calculation of
the transition probabilities for the coreference fea-
ture table.
Note the following observations about corefer-
ence features: it only needs a probabilistic model
when opt is old ? in other words, only when the
model must choose between several antecedents to
re-refer to. gt, et and nt are deterministic except
when opt is new, when gender, entity type, and num-
ber information must be generated for the new entity
being introduced.
When opt is none, all coreference variables (en-
tity features) will be copied over from the previous
time step to the current time step, and the probabil-
ity of this transition is 1.0. When opt is new, it is
changed deterministically by adding the new entity
to the first position in the list and moving every other
entity down one position. If the list of entities is
full, the least recently mentioned entity will be dis-
carded. The values for the top of the feature lists
gt, et, and nt will then be generated from feature-
specific probability distributions estimated from the
training data. When opt is old, it will probabilisti-
cally select a value 1 . . . n, for an entity list contain-
ing n items. The selected value will deterministi-
cally order the gt, nt and et lists. This distribution
is also estimated from training data, and takes into
account recency of mention. The shape of this dis-
tribution varies slightly depending on list size and
noise in the training data, but in general the probabil-
ity of a mention being selected is directly correlated
to how recently it was mentioned.
With this understanding, coreference table tran-
sition probabilities can be written in terms of only
their non-deterministic substate distributions:
P(crt | crt?1, old) = Pold(it | it?1)?
Preorder(et | et?1, it)?
Preorder(gt | gt?1, it)?
Preorder(nt |nt?1, it)
(8)
where the old model probabilistically selects the an-
tecedent and moves it to the top of the list as de-
scribed above, thus deciding how the reordering will
take place. The reorder model actually implements
the list reordering for each independent feature by
moving the feature value corresponding to the se-
lected entity in the index model to the top of that
feature?s list. The overall effect is simply the prob-
abilistic reordering of entities in a list, where each
entity is defined as a label and a set of features.
P(crt | crt?1, new) = Pnew(it | it?1)?
Pnew(gt | gt?1)?
Pnew(nt |nt?1)?
Pnew(et | et?1)
(9)
where the new model probabilistically generates a
1173
feature value based on the training data and puts it
at the top of the list, moves every other entity down
one position in the list, and removes the final item if
the list is already full. Each entity in i takes a value
from 1 to n for a list of size n. Each g can be one of
four values ? male, female, neuter and unknown; n
one of three values ? plural, singular and unknown
and e around eight values.
Note that post is used in both hidden states and
observation states. While it is not considered a
coreference feature as such, it can still play an im-
portant role in the resolving process. Basically, the
system tags parts of speech incrementally while si-
multaneously resolving pronoun anaphora. Mean-
while, post?1 and opt?1 will jointly generate opt.
This point has been discussed in Section 2.2.
Importantly, the pos model can help to imple-
ment binding principles (Chomsky, 1981). It is
applied when opt is old. In training, pronouns
are sub-categorised into personal pronouns, reflex-
ive and other-pronoun. We then define a vari-
able loct whose value is how far back in the list
of antecedents the current hypothesis must have
gone to arrive at the current value of it. If we
have the syntax annotations or parsed trees, then,
the part of speech model can be defined when
opt is old as Pbinding(post | loct, sloct). For ex-
ample, if post ? ref lexive, P(post | loct, sloct)
where loct has smaller values (implying closer men-
tions to post) and sloct = subject should have
higher values since reflexive pronouns always re-
fer back to subjects within its governing domains.
This was what (Haghighi and Klein, 2009) did and
we did this in training with the REUTERS cor-
pus (Hasler et al, 2006) in which syntactic roles
are annotated. We finally switched to the ACE
corpus for the purpose of comparison with other
work. In the ACE corpus, no syntactic roles are
annotated. We did use the Stanford parser to ex-
tract syntactic roles from the ACE corpus. But
the result is largely affected by the parsing accu-
racy. Again, for a fair comparison, we extract simi-
lar features to Denis and Baldridge (2007), which is
the model we mainly compare with. They approx-
imate syntactic contexts with POS tags surround-
ing the pronoun. Inspired by this idea, we success-
fully represent binding features with POS tags be-
fore anaphors. Instead of using P(post | loct, sloct),
we train P(post | loct, posloct) which can play
the role of binding. For example, suppose the
buffer size is 6 and loct = 5, posloct = noun.
Then, P(post = ref lexive | loct, posloct) is usu-
ally higher than P(post = pronoun | loct, posloct),
since the reflexive has a higher probability of refer-
ring back to the noun located in position 5 than the
pronoun.
In future work expanding to coreference resolu-
tion between any noun phrases we intend to inte-
grate syntax into this framework as a joint model of
coreference resolution and parsing.
3 Observation Model
The observation model that generates an observed
state is defined as Equation 5. To expand that equa-
tion in detail, the observation state, the word, de-
pends on its part of speech and its coreference fea-
tures as well. Since FHMMs are generative, we can
say part of speech and coreference features generate
the word.
In actual implementation, the observed model will
be very sparse, since crt will be split into more vari-
ables according to how many coreference features it
is composed of. In order to avoid the sparsity, we
transform the equation with Bayes? law as follows.
P?O(ot |ht) =
P (ot) ? P(ht | ot)
?
o? P (o?)P(ht | o?)
(10)
= P (ot) ? P(post, crt | ot)?
o? P (o?)P(post, crt | o?)
(11)
We define pos and cr to be independent of each
other, so we can further split the above equation as:
P?O(ot |ht)
def= P (ot) ? P(post | ot) ? P(crt | ot)?
o? P (o?) ? P(post | o?) ? P(crt | o?)
(12)
where P(crt | ot) = P(gt | ot)P(nt | ot)P(et | ot) and
P(crt | o?) = P(gt | o?)P(nt | o?)P(et | o?).
This change transforms the FHMM to a hybrid
FHMM since the observation model no longer gen-
erates the data. Instead, the observation model gen-
erates hidden states, which is more a combination
of discriminative and generative approaches. This
way facilitates building likelihood model files of fea-
tures for given mentions from the training data. The
1174
hidden state transition model represents prior proba-
bilities of coreference features associated with each
while this observation model factors in the probabil-
ity given a pronoun.
3.1 Unknown Words Processing
If an observed word was not seen in training, the
distribution of its part of speech, gender, number and
entity type will be unknown. In this case, a special
unknown words model is used.
The part of speech of unknown words
P(post |wt = unkword) is estimated using a
decision tree model. This decision tree is built
by splitting letters in words from the end of the
word backward to its beginning. A POS tag is
assigned to the word after comparisons between
the morphological features of words trained from
the corpus and the strings concatenated from the
tree leaves are made. This method is about as
accurate as the approach described by Klein and
Manning (2003).
Next, a similar model is set up for estimating
P(nt |wt = unkword). Most English words have
regular plural forms, and even irregular words have
their patterns. Therefore, the morphological features
of English words can often be used to determine
whether a word is singular or plural.
Gender is irregular in English, so model-based
predictions are problematic. Instead, we follow
Bergsma and Lin (2005) to get the distribution of
gender from their gender/number data and then pre-
dict the gender for unknown words.
4 Evaluation and Discussion
4.1 Experimental Setup
In this research, we used the ACE corpus (Phase 2) 1
for evaluation. The development of this corpus in-
volved two stages. The first stage is called EDT (en-
tity detection and tracking) while the second stage
is called RDC (relation detection and characteriza-
tion). All markables have named entity types such
as FACILITY, GPE (geopolitical entity), PERSON,
LOCATION, ORGANIZATION, PERSON, VEHI-
CLE and WEAPONS, which were annotated in the
first stage. In the second stage, relations between
1See http://projects.ldc.upenn.edu/ace/
annotation/previous/ for details on the corpus.
named entities were annotated. This corpus include
three parts, composed of different genres: newspa-
per texts (NPAPER), newswire texts (NWIRE) and
broadcasted news (BNEWS). Each of these is split
into a train part and a devtest part. For the train
part, there are 76, 130 and 217 articles in NPA-
PER, NWIRE and BNEWS respectively while for
the test part, there are 17, 29 and 51 articles respec-
tively. Though the number of articles are quite dif-
ferent for three genres, the total number of words are
almost the same. Namely, the length of NPAPER
is much longer than BNEWS (about 1200 words,
800 word and 500 words respectively for three gen-
res). The longer articles involve longer coreference
chains. Following the common practice, we used
the devtest material only for testing. Progress during
the development phase was estimated only by using
cross-validation on the training set for the BNEWS
section. In order to make comparisons with publica-
tions which used the same corpus, we make efforts
to set up identical conditions for our experiments.
The main point of comparison is Denis and
Baldridge (2007), which was similar in that it de-
scribed a new type of coreference resolver using
simple features.
Therefore, similar to their practice, we use all
forms of personal and possessive pronouns that were
annotated as ACE ?markables?. Namely, pronouns
associated with named entity types could be used in
this system. In experiments, we also used true ACE
mentions as they did. This means that pleonastics
and references to eventualities or to non-ACE enti-
ties are not included in our experiments either. In
all, 7263 referential pronouns in training data set
and 1866 in testing data set are found in all three
genres. They have results of three different systems:
SCC (single candidate classifier), TCC (twin candi-
date classifier) and RK (ranking). Besides the three
and our own system, we also report results of em-
Pronouns, which is an unsupervised system based
on a recently published paper (Charniak and Elsner,
2009). We select this unsupervised system for two
reasons. Firstly, emPronouns is a publicly available
system with high accuracy in pronoun resolution.
Secondly, it is necessary for us to demonstrate our
system has strong empirical superiority over unsu-
pervised ones. In testing, we also used the OPNLP
Named Entity Recognizer to tag the test corpus.
1175
During training, besides coreference annotation
itself, the part of speech, dependencies between
words and named entities, gender, number and index
are extracted using relative frequency estimation to
train models for the coreference resolution system.
Inputs for testing are the plain text and the trained
model files. The entity buffer used in these exper-
iments kept track of only the six most recent men-
tions. The result of this process is an annotation
of the headword of every noun phrase denoting it
as a mention. In addition, this system does not
do anaphoricity detection, so the antecedent oper-
ation for non-anaphora pronoun it is set to be none.
Finally, the system does not yet model cataphora,
about 10 cataphoric pronouns in the testing data
which are all counted as wrong.
4.2 Results
The performance was evaluated using the ratio of
the number of correctly resolved anaphors over the
number of all anaphors as a success metrics. All the
standards are consistent with those defined in Char-
niak and Elsner (2009).
During development, several preliminary experi-
ments explored the effects of starting from a simple
baseline and adding more features. The BNEWS
corpus was employed in these development exper-
iments. The baseline only includes part of speech
tags, the index feature and and syntactic roles. Syn-
tactic roles are extracted from the parsing results
with Stanford parser. The success rate of this base-
line configuration is 0.48. This low accuracy is par-
tially due to the errors of automatic parsing. With
gender and number features added, the performance
jumped to 0.65. This shows that number and gen-
der agreements play an important role in pronoun
anaphora resolution. For a more standard compari-
son to other work, subsequent tests were performed
on the gold standard ACE corpus (using the model
as described with named entity features instead of
syntactic role features). As shown in Denis and
Baldridge (2007), they employ all features we use
except syntactic roles. In these experiments, the sys-
tem got better results as shown in Table 2.
The result of the first one is obtained by running
the publicly available system emPronouns2. It is a
2the available system in fact only includes the testing part.
Thus, it may be unfair to compare emPronouns this way with
System BNEWS NPAPER NWIRE
emPronouns 58.5 64.5 60.6
SCC 62.2 70.7 68.3
TCC 68.6 74.7 71.1
RK 72.9 76.4 72.4
FHMM 74.9 79.4 74.5
Table 2: Accuracy scores for emPronouns, the single-
candidate classifier (SCC), the twin-candidate classifier
(TCC), the ranker and FHMM
high-accuracy unsupervised system which reported
the best result in Charniak and Elsner (2009).
The results of the other three systems are those
reported by Denis and Baldridge (2007). As Table 2
shows, the FHMM system gets the highest average
results.
The emPronouns system got the lowest results
partially due to the reason that we only directly
run the existing system with its existing model files
without retraining. But the gap between its results
and results of our system is large. Thus, we may
still say that our system probably can do a better job
even if we train new models files for emPronouns
with ACE corpus.
With almost exactly identical settings, why does
our FHMM system get the highest average results?
The convincing reason is that FHMM is strongly in-
fluenced by the sequential dependencies. The rank-
ing approach ranks a set of mentions using a set of
features, and it also maintains the discourse model,
but it is not processing sequentially. The FHMM
system always maintain a set of mentions as well
as a first-order dependencies between part of speech
and operator. Therefore, context can be more fully
taken into consideration. This is the main reason that
the FHMM approach achieved better results than the
ranking approach.
From the result, one point we may notice is that
NPAPER usually obtains higher results than both
BNEWS and NWIRE for all systems while BNEWS
lower than other two genres. In last section, we
mention that articles in NPAPER are longer than
other genres and also have denser coreference chains
while articles in BENEWS are shorter and have
sparer chains. Then, it is not hard to understand
why results of NPAPER are better while those of
other systems.
1176
BNEWS are poorer.
In Denis and Baldridge (2007), they also reported
new results with a window of 10 sentences for RK
model. All three genres obtained higher results than
those when with shorter ones. They are 73.0, 77.6
and 75.0 for BNEWS,NPAPER and NWIRE respec-
tively. We can see that except the one for NWIRE,
the results are still poorer than our system. For
NWIRE, the RK model got 0.5 higher. The average
of the RK is 75.2 while that of the FHMM system is
76.3, which is still the best.
Since the emPronoun system can output sample-
level results, it is possible to do a paired Student?s
t-test. That test shows that the improvement of our
system on all three genres is statistically significant
(p < 0.001). Unfortunately, the other systems only
report overall results so the same comparison was
not so straightforward.
4.3 Error Analysis
After running the system on these documents, we
checked which pronouns fail to catch their an-
tecedents. There are a few general reasons for er-
rors.
First, pronouns which have antecedents very far
away cannot be caught. Long-distance anaphora res-
olution may pose a problem since the buffer size
cannot be too long considering the complexity of
tracking a large number of mentions through time.
During development, estimation of an acceptable
size was attempted using the training data. It was
found that a mention distance of fourteen would ac-
count for every case found in this corpus, though
most cases fall well short of that distance. Future
work will explore optimizations that will allow for
larger or variable buffer sizes so that longer distance
anaphora can be detected.
A second source of error is simple misjudgments
when more than one candidate is waiting for selec-
tion. A simple case is that the system fails to distin-
guish plural personal nouns and non-personal nouns
if both candidates are plural. This is not a problem
for singular pronouns since gender features can tell
whether pronouns are personal or not. Plural nouns
in English do not have such distinctions, however.
Consequently, demands and Israelis have the same
probability of being selected as the antecedents for
they, all else being equal. If demands is closer to
they, demands will be selected as the antecedent.
This may lead to the wrong choice if they in fact
refers to Israelis. This may require better measures
of referent salience than the ?least recently used?
heuristic currently implemented.
Third, these results also show difficulty resolv-
ing coordinate noun phrases due to the simplistic
representation of noun phrases in the input. Con-
sider this sentence: President Barack Obama and
his wife Michelle Obama visited China last week.
They had a meeting with President Hu in Beijing.
In this example, the pronoun they corefers with the
noun phrase President Barack Obama and his wife
Michelle Obama. The present model cannot repre-
sent both the larger noun phrase and its contained
noun phrases. Since the noun phrase is a coordinate
one that includes both noun phrases, the model can-
not find a head word to represent it.
Finally, while the coreference feature annotations
of the ACE are valuable for learning feature mod-
els, the model training may still give some mislead-
ing results. This is brought about by missing fea-
tures in the training corpus and by the data sparsity.
We solved the problem with add-one smoothing and
deleted interpolation in training models besides the
transformation in the generation order of the obser-
vation model.
5 Conclusion and Future Work
This paper has presented a pronoun anaphora resolu-
tion system based on FHMMs. This generative sys-
tem incrementally resolves pronoun anaphora with
an entity buffer carrying forward mention features.
The system performs well and outperforms other
available models. This shows that FHMMs and
other time-series models may be a valuable model
to resolve anaphora.
Acknowledgments
We would like to thank the authors and maintainers
of ranker models and emPronouns. We also would
like to thank the three anonymous reviewers. The
final version is revised based on their valuable com-
ments. Thanks are extended to Shane Bergsma, who
provided us the gender and number data distribution.
In addition, Professor Jeanette Gundel and our lab-
mate Stephen Wu also gave us support in paper edit-
ing and in theoretical discussion.
1177
References
S Bergsma. 2005. Automatic acquisition of gender
information for anaphora resolution. page 342353.
Springer.
Eugene Charniak and Micha Elsner. 2009. Em works
for pronoun anaphora resolution. In Proceedings of
the Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-09),
Athens, Greece.
Noam Chomsky. 1981. Lectures on government and
binding. Foris, Dordercht.
H.H. Clark and CJ Sengul. 1979. In search of refer-
ents for nouns and pronouns. Memory & Cognition,
7(1):35?41.
P. Denis and J. Baldridge. 2007. A ranking approach to
pronoun resolution. In Proc. IJCAI.
Kevin Duh. 2005. Jointly labeling multiple sequences:
a factorial HMM approach. In ACL ?05: Proceedings
of the ACL Student Research Workshop, pages 19?24,
Ann Arbor, Michigan.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29:1?
31.
A. Haghighi and D. Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the 45th annual meeting on Associ-
ation for Computational Linguistics, page 848.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3-
Volume 3, pages 1152?1161. Association for Compu-
tational Linguistics.
A. Haghighi and D. Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393. Associa-
tion for Computational Linguistics.
L. Hasler, C. Orasan, and K. Naumann. 2006. NPs
for events: Experiments in coreference annotation. In
Proceedings of the 5th edition of the International
Conference on Language Resources and Evaluation
(LREC2006), pages 1167?1172. Citeseer.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan.
X Luo. 2005. On coreference resolution performance
metrics. pages 25?32. Association for Computational
Linguistics Morristown, NJ, USA.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In IJCAI Workshop on In-
formation Integration on the Web. Citeseer.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. BLLIP North American News Text, Complete.
Linguistic Data Consortium. LDC2008T13.
T.S. Morton. 2000. Coreference for NLP applications.
In Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.
V. Ng. 2008. Unsupervised models for coreference reso-
lution. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 640?
649. Association for Computational Linguistics.
US NIST. 2003. The ACE 2003 Evaluation Plan. US Na-
tional Institute for Standards and Technology (NIST),
Gaithersburg, MD.[online, pages 2003?08.
L. Qiu, M.Y. Kan, and T.S. Chua. 2004. A public ref-
erence implementation of the rap anaphora resolution
algorithm. Arxiv preprint cs/0406031.
X. Yang, J. Su, G. Zhou, and C.L. Tan. 2004. Im-
proving pronoun resolution by incorporating corefer-
ential information of candidates. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 127. Association for Com-
putational Linguistics.
1178
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 81?86,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Descending-Path Convolution Kernel for Syntactic Structures
Chen Lin
1
, Timothy Miller
1
, Alvin Kho
1
, Steven Bethard
2
,
Dmitriy Dligach
1
, Sameer Pradhan
1
and Guergana Savova
1
,
1
Children?s Hospital Boston Informatics Program and Harvard Medical School
{firstname.lastname}@childrens.harvard.edu
2
Department of Computer and Information Sciences, University of Alabama at Birmingham
bethard@cis.uab.edu
Abstract
Convolution tree kernels are an efficient
and effective method for comparing syntac-
tic structures in NLP methods. However,
current kernel methods such as subset tree
kernel and partial tree kernel understate the
similarity of very similar tree structures.
Although soft-matching approaches can im-
prove the similarity scores, they are corpus-
dependent and match relaxations may be
task-specific. We propose an alternative ap-
proach called descending path kernel which
gives intuitive similarity scores on compa-
rable structures. This method is evaluated
on two temporal relation extraction tasks
and demonstrates its advantage over rich
syntactic representations.
1 Introduction
Syntactic structure can provide useful features for
many natural language processing (NLP) tasks
such as semantic role labeling, coreference resolu-
tion, temporal relation discovery, and others. How-
ever, the choice of features to be extracted from a
tree for a given task is not always clear. Convolu-
tion kernels over syntactic trees (tree kernels) offer
a potential solution to this problem by providing
relatively efficient algorithms for computing sim-
ilarities between entire discrete structures. These
kernels use tree fragments as features and count
the number of common fragments as a measure of
similarity between any two trees.
However, conventional tree kernels are sensitive
to pattern variations. For example, two trees in Fig-
ure 1(a) sharing the same structure except for one
terminal symbol are deemed at most 67% similar
by the conventional tree kernel (PTK) (Moschitti,
2006). Yet one might expect a higher similarity
given their structural correspondence.
The similarity is further attenuated by trivial
structure changes such as the insertion of an ad-
jective in one of the trees in Figure 1(a), which
would reduce the similarity close to zero. Such
an abrupt attenuation would potentially propel a
model to memorize training instances rather than
generalize from trends, leading towards overfitting.
In this paper, we describe a new kernel over
syntactic trees that operates on descending paths
through the tree rather than production rules as
used in most existing methods. This representation
is reminiscent of Sampson?s (2000) leaf-ancestor
paths for scoring parse similarities, but here it is
generalized over all ancestor paths, not just those
from the root to a leaf. This approach assigns more
robust similarity scores (e.g., 78% similarity in the
above example) than other soft matching tree ker-
nels, is faster than the partial tree kernel (Moschitti,
2006), and is less ad hoc than the grammar-based
convolution kernel (Zhang et al, 2007).
2 Background
2.1 Syntax-based Tree Kernels
Syntax-based tree kernels quantify the similarity
between two constituent parses by counting their
common sub-structures. They differ in their defini-
tion of the sub-structures.
Collins and Duffy (2001) use a subset tree (SST)
representation for their sub-structures. In the SST
representation, a subtree is defined as a subgraph
with more than one node, in which only full pro-
duction rules are expanded. While this approach is
widely used and has been successful in many tasks,
the production rule-matching constraint may be un-
necessarily restrictive, giving zero credit to rules
that have only minor structural differences. For
example, the similarity score between the NPs in
Figure 1(b) would be zero since the production rule
is different (the overall similarity score is above-
zero because of matching pre-terminals).
The partial tree kernel (PTK) relaxes the defi-
nition of subtrees to allow partial production rule
81
a)
NP
DT
a
NN
cat
NP
DT
a
NN
dog
b)
NP
DT
a
NN
cat
NP
DT
a
JJ
fat
NN
cat
c)
S
ADVP
RB
here
NP
PRP
she
VP
VBZ
comes
S
NP
PRP
she
VP
VBZ
comes
ADVP
RB
here
Figure 1: Three example tree pairs.
matching (Moschitti, 2006). In the PTK, a subtree
may or may not expand any child in a production
rule, while maintaining the ordering of the child
nodes. Thus it generates a very large but sparse
feature space. To Figure 1(b), the PTK generates
fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a]
[NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among
others, for the second tree. This allows for partial
matching ? substructure (ii) ? while also generating
some fragments that violate grammatical intuitions.
Zhang et al (2007) address the restrictiveness
of SST by allowing soft matching of production
rules. They allow partial matching of optional
nodes based on the Treebank. For example, the
rule NP ? DT JJ NN indicates a noun phrase
consisting of a determiner, adjective, and common
noun. Zhang et al?s method designates the JJ as
optional, since the Treebank contains instances of
a reduced version of the rule without the JJ node
(NP ? DT NN ). They also allow node match-
ing among similar preterminals such as JJ, JJR, and
JJS, mapping them to one equivalence class.
Other relevant approaches are the spectrum tree
(SpT) (Kuboyama et al, 2007) and the route kernel
(RtT) (Aiolli et al, 2009). SpT uses a q-gram
? a sequence of connected vertices of length q ?
as their sub-structure. It observes grammar rules
by recording the orientation of edges: a?b?c is
different from a?b?c. RtT uses a set of routes as
basic structures, which observes grammar rules by
NP
DT
a
NN
cat
l=0: [NP],[DT],[NN]
l=1: [NP-DT],[NP-NN],
[DT-a],[NN-cat]
l=2: [NP-DT-a],[NP-NN-cat]
Figure 2: A parse tree (left) and its descending
paths according to Definition 1 (l - length).
recording the index of a neighbor node.
2.2 Temporal Relation Extraction
Among NLP tasks that use syntactic informa-
tion, temporal relation extraction has been draw-
ing growing attention because of its wide applica-
tions in multiple domains. As subtasks in Tem-
pEval 2007, 2010 and 2013, multiple systems
were built to create labeled links from events
to events/timestamps by using a variety of fea-
tures (Bethard and Martin, 2007; Llorens et al,
2010; Chambers, 2013). Many methods exist for
synthesizing syntactic information for temporal
relation extraction, and most use traditional tree
kernels with various feature representations. Mir-
roshandel et al (2009) used the path-enclosed tree
(PET) representation to represent syntactic informa-
tion for temporal relation extraction on the Time-
Bank (Pustejovsky et al, 2003) and the AQUAINT
TimeML corpus
1
. The PET is the smallest subtree
that contains both proposed arguments of a relation.
Hovy et al (2012) used bag tree structures to rep-
resent the bag of words (BOW) and bag of part of
speech tags (BOP) between the event and time in
addition to a set of baseline features, and improved
the temporal linking performance on the TempEval
2007 and Machine Reading corpora (Strassel et
al., 2010). Miller at al. (2013) used PET tree, bag
tree, and path tree (PT, which is similar to a PET
tree with the internal nodes removed) to represent
syntactic information and improved the temporal
relation discovery performance on THYME data
2
(Styler et al, 2014). In this paper, we also use
syntactic structure-enriched temporal relation dis-
covery as a vehicle to test our proposed kernel.
3 Methods
Here we decribe the Descending Path Kernel
(DPK).
1
http://www.timeml.org
2
http://thyme.healthnlp.org
82
Definition 1 (Descending Path): Let T be a
parse tree, v any non-terminal node in T , dv a
descendant of v, including terminals. A descending
path is the sequence of indexes of edges connecting
v and dv, denoted by [v ? ? ? ? ? dv]. The length l
of a descending path is the number of connecting
edges. When l = 0, a descending path is the non-
terminal node itself, [v]. Figure 2 illustrates a parse
tree and its descending paths of different lengths.
Suppose that all descending paths of a tree T are
indexed 1, ? ? ? , n, and path
i
(T ) is the frequency
of the i-th descending path in T . We represent T as
a vector of frequencies of all its descending paths:
?(T ) = (path
1
(T ), ? ? ? , path
n
(T )).
The similarity between any two trees T
1
and T
2
can be assessed via the dot product of their respec-
tive descending path frequency vector representa-
tions: K(T
1
, T
2
) = ??(T
1
),?(T
2
)?.
Compared with the previous tree kernels, our
descending path kernel has the following advan-
tages: 1) the sub-structures are simplified so that
they are more likely to be shared among trees,
and therefore the sparse feature issues of previous
kernels could be alleviated by this representation;
2) soft matching between two similar structures
(e.g., NP?DT JJ NN versus NP?DT NN) have
high similarity without reference to any corpus or
grammar rules;
Following Collins and Duffy (2001), we derive
a recursive algorithm to compute the dot product
of the descending path frequency vector represen-
tations of two trees T
1
and T
2
:
K(T
1
, T
2
) = ??(T
1
),?(T
2
)?
=
?
i
path
i
(T
1
) ? path
i
(T
2
)
=
?
n
1
?N
1
?
n
2
?N
2
?
i
I
path
i
(n
1
) ? I
path
i
(n
2
)
=
?
n
1
?N
1
n
2
?N
2
C(n
1
, n
2
)
(1)
where N
1
and N
2
are the sets of nodes in T
1
and
T
2
respectively, i indexes the set of possible paths,
I
path
i
(n) is an indicator function that is 1 iff the
descending path
i
is rooted at node n or 0 other-
wise. C(n
1
, n
2
) counts the number of common
descending paths rooted at nodes n
1
and n
2
:
C(n
1
, n
2
) =
?
i
I
path
i
(n
1
) ? I
path
i
(n
2
)
C(n
1
, n
2
) can be computed in polynomial time by
the following recursive rules:
Rule 1: If n
1
and n
2
have different labels (e.g.,
?DT? versus ?NN?), then C(n1, n2) = 0;
Rule 2: Else if n
1
and n
2
have the same labels
and are both pre-terminals (POS tags), then
C(n
1
, n
2
) = 1 +
{
1 if term(n
1
) = term(n
2
)
0 otherwise.
where term(n) is the terminal symbol under n;
Rule 3: Else if n
1
and n
2
have the same labels
and they are not both pre-terminals, then:
C(n
1
, n
2
) = 1 +
?
n
i
?children(n
1
)
n
j
?children(n
2
)
C(n
i
, n
j
)
where children(m) are the child nodes of m.
As in other tree kernel approaches (Collins and
Duffy, 2001; Moschitti, 2006), we use a discount
parameter ? to control for the disproportionately
large similarity values of large tree structures.
Therefore, Rule 2 becomes:
C(n
1
, n
2
) = 1 +
{
? if term(n
1
) = term(n
2
)
0 otherwise.
and Rule 3 becomes:
C(n
1
, n
2
) = 1 + ?
?
n
i
?children(n
1
)
n
j
?children(n
2
)
C(n
i
, n
j
)
Note that Eq. (1) is a convolution kernel under
the kernel closure properties described in Haus-
sler (1999). Rules 1-3 show the equivalence be-
tween the number of common descending paths
rooted at nodes n
1
and n
2
, and the number of
matching nodes below n
1
and n
2
.
In practice, there are many non-matching nodes,
and most matching nodes will have only a few
matching children, so the running time, as in SST,
will be approximated by the number of matching
nodes between trees.
3.1 Relationship with other kernels
For a given tree, DPK will generate significantly
fewer sub-structures than PTK, since it does not
consider all ordered permutations of a production
rule. Moreover, the fragments generated by DPK
are more likely to be shared among different trees.
For the number of corpus-wide fragments, it is
83
Kernel ID #Frag Sim N(Sim)
SST a 9 3 0.50
O
(
?|N
1
||N
2
|
)
b 15 2 0.25
c 63 7 0.20
DPK a 11 7 0.78
O
(
?
2
|N
1
||N
2
|
)
b 13 9 0.83
c 31 22 0.83
PTK a 20 10 0.67
O
(
?
3
|N
1
||N
2
|
)
b 36 15 0.65
c 127 34 0.42
Table 1: Comparison of the worst case computa-
tional complexicity (? - the maximum branching
factor) and kernel performance on the 3 examples
from Figure 1. #Frag is the number of fragments,
N(Sim) is the normalized similarity. Please see
the online supplementary note for detailed frag-
ments of example (a).
possible that DPK? SST? PTK. In Table 1, given
? = 1, we compare the performance of 3 kernels
on the three examples in Figure 1. Note that for
more complicated structures, i.e., examples b and
c, DPK generates fewer fragments than SST and
PTK, with more shared fragments among trees.
The complexity for all three kernels are at least
O
(
|N
1
||N
2
|
)
since they share the pairwise summa-
tion at the end of Equation 1. SST, due to its re-
quirement of exact production rule matching, only
takes one pass in the inner loop which adds a factor
of ? (the maximum branching factor of any pro-
duction rule). DPK does a pairwise summation
of children, which adds a factor of ?
2
to the com-
plexity. Finally, the efficient algorithm for PTK
is proved by Moschitti (2006) to contain a con-
stant factor of ?
3
. Table 1 orders the tree kernels
according by their listed complexity.
It may seem that the value of DPK is strictly in its
ability to evaluate all paths, which is not explicitly
accounted for by other kernels. However, another
view of the DPK is possible by thinking of it as
cheaply calculating rule production similarity by
taking advantage of relatively strict English word
ordering. Like SST and PTK, the DPK requires
the root category of two subtrees to be the same
for the similarity to be greater than zero. Unlike
SST and PTK, once the root category comparison
is successfully completed, DPK looks at all paths
that go through it and accumulates their similarity
scores independent of ordering ? in other words, it
will ignore the ordering of the children in its pro-
duction rule. This means, for example, that if the
rule production NP? NN JJ DT were ever found
in a tree, to DPK it would be indistinguishable from
the common production NP? DT JJ NN, despite
having inverted word order, and thus would have
a maximal similarity score. SST and PTK would
assign this pair a much lower score for having com-
pletely different ordering, but we suggest that cases
such as these are very rare due to the relatively
strict word ordering of English. In most cases, the
determiner of a noun phrase will be at the front, the
nouns will be at the end, and the adjectives in the
middle. So with small differences in production
rules (one or two adjectives, extra nominal modifier,
etc.) the PTK will capture similarity by compar-
ing every possible partial rule completion, but the
DPK can obtain higher and faster scores by just
comparing one child at a time because the ordering
is constrained by the language. This analysis does
lead to a hypothesis for the general viability of the
DPK, suggesting that in languages with freer word
order it may give inflated scores to structures that
are syntactically dissimilar if they have the same
constituent components in different order.
Formally, Moschitti (2006) showed that SST is
a special case of PTK when only the longest child
sequence from each tree is considered. On the other
end of the spectrum, DPK is a special case of PTK
where the similarity between rules only considers
child subsequences of length one.
4 Evaluation
We applied DPK to two published temporal relation
extraction systems: (Miller et al, 2013) in the
clinical domain and Cleartk-TimeML (Bethard,
2013) in the general domain respectively.
4.1 Narrative Container Discovery
The task here as described by Miller et al (2013) is
to identify the CONTAINS relation between a time
expression and a same-sentence event from clinical
notes in the THYME corpus, which has 78 notes
of 26 patients. We obtained this corpus from the
authors and followed their linear composite kernel
setting:
K
C
(s
1
, s
2
) = ?
P
?
p=1
K
T
(t
p
1
, t
p
2
)+K
F
(f
1
, f
2
) (2)
where s
i
is an instance object composed of flat fea-
tures f
i
and a syntactic tree t
i
. A syntactic tree t
i
84
can have multiple representations, as in Bag Tree
(BT), Path-enclosed Tree (PET), and Path Tree
(PT). For the tree kernel K
T
, subset tree (SST) ker-
nel was applied on each tree representation p. The
final similarity score between two instances is the
? -weighted sum of the similarities of all representa-
tions, combined with the flat feature (FF) similarity
as measured by a feature kernel K
F
(linear or poly-
nomial). Here we replaced the SST kernel with
DPK and tested two feature combinations FF+PET
and FF+BT+PET+PT. To fine tune parameters, we
used grid search by testing on the default develop-
ment data. Once the parameters were tuned, we
tested the system performance on the testing data,
which was set up by the original system split.
4.2 Cleartk-TimeML
We tested one sub-task from TempEval-2013 ?
the extraction of temporal relations between an
event and time expression within the same sen-
tence. We obtained the training corpus (Time-
Bank + AQUAINT) and testing data from the au-
thors (Bethard, 2013). Since the original features
didn?t contain syntactic features, we created a PET
tree extractor for this system. The kernel setting
was similar to equation (2), while there was only
one tree representation, PET tree, P=1. A linear
kernel was used as K
F
to evaluate the exact same
flat features as used by the original system. We
used the built-in cross validation to do grid search
for tuning the parameters. The final system was
tested on the testing data for reporting results.
4.3 Results and Discussion
Results are shown in Table 2. The top section
shows THYME results. For these experiments,
the DPK is superior when a syntactically-rich PET
representation is used. Using the full feature set of
Miller et al (2013), SST is superior to DPK and
obtains the best overall performance. The bottom
section shows results on TempEval-2013 data, for
which there is little benefit from either tree kernel.
Our experiments with THYME data show that
DPK can capture something in the linguistically
richer PET representation that the SST kernel can-
not, but adding BT and PT representations decrease
the DPK performance. As a shallow representation,
BT does not have much in the way of descending
paths for DPK to use. PT already ignores the pro-
duction grammar by removing the inner tree nodes.
DPK therefore cannot get useful information and
may even get misleading cues from these two rep-
Features K
T
P R F
THYME
FF+PET DPK 0.756 0.667 0.708
SST 0.698 0.630 0.662
FF+BT+ DPK 0.759 0.626 0.686
PET+PT SST 0.754 0.711 0.732
TempEval
FF+PET DPK 0.328 0.263 0.292
SST 0.325 0.263 0.290
FF - 0.309 0.266 0.286
Table 2: Comparison of tree kernel performance
for temporal relation extraction on THYME and
TempEval-2013 data.
resentations. These results show that, while DPK
should not always replace SST, there are represen-
tations in which it is superior to existing methods.
This suggests an approach in which tree representa-
tions are matched to different convolution kernels,
for example by tuning on held-out data.
For TempEval-2013 data, adding syntactic fea-
tures did not improve the performance significantly
(comparing F-score of 0.290 with 0.286 in Ta-
ble 3). Probably, syntactic information is not a
strong feature for all types of temporal relations on
TempEval-2013 data.
5 Conclusion
In this paper, we developed a novel convolution
tree kernel (DPK) for measuring syntactic similar-
ity. This kernel uses a descending path represen-
tation in trees to allow higher similarity scores on
partially matching structures, while being simpler
and faster than other methods for doing the same.
Future work will explore 1) a composite kernel
which uses DPK for PET trees, SST for BT and PT,
and feature kernel for flat features, so that different
tree kernels can work with their ideal syntactic rep-
resentations; 2) incorporate dependency structures
for tree kernel analysis 3) applying DPK to other
relation extraction tasks on various corpora.
6 Acknowledgements
Thanks to Sean Finan for technically supporting the
experiments. The project described was supported
by R01LM010090 (THYME) from the National
Library Of Medicine.
85
References
Fabio Aiolli, Giovanni Da San Martino, and Alessan-
dro Sperduti. 2009. Route kernels for trees. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 17?24. ACM.
Steven Bethard and James H Martin. 2007. Cu-tmp:
temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
129?132. Association for Computational Linguis-
tics.
Steven Bethard. 2013. Cleartk-timeml: A minimalist
approach to TempEval 2013. In Second Joint Con-
ference on Lexical and Computational Semantics (*
SEM), volume 2, pages 10?14.
Nate Chambers. 2013. Navytime: Event and time or-
dering from raw text. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 73?77, Atlanta, Georgia, USA, June. Associa-
tion for Computational Linguistics.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Neural Information
Processing Systems.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia in Santa Cruz.
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Pat-
wardhan, and Chris Welty. 2012. When did that
happen?: linking events and relations to timestamps.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 185?193. Association for Compu-
tational Linguistics.
Tetsuji Kuboyama, Kouichi Hirata, Hisashi Kashima,
Kiyoko F Aoki-Kinoshita, and Hiroshi Yasuda.
2007. A spectrum tree kernel. Information and Me-
dia Technologies, 2(1):292?299.
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. Tipsem (english and spanish): Evaluating
CRFs and semantic roles in TempEval-2. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 284?291. Association for
Computational Linguistics.
Timothy Miller, Steven Bethard, Dmitriy Dligach,
Sameer Pradhan, Chen Lin, and Guergana Savova.
2013. Discovering temporal narrative containers
in clinical text. In Proceedings of the 2013 Work-
shop on Biomedical Natural Language Processing,
pages 18?26, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for
classifying temporal relations between events. Proc.
of the PACLIC23, pages 355?364.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318?329.
Springer.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The TimeBank corpus. In Cor-
pus linguistics, volume 2003, page 40.
Geoffrey Sampson. 2000. A proposal for improving
the measurement of parse accuracy. International
Journal of Corpus Linguistics, 5(1):53?68.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA machine reading
program-encouraging linguistic and reasoning re-
search with a series of reading tasks. In LREC.
William Styler, Steven Bethard, Sean Finan, Martha
Palmer, Sameer Pradhan, Piet de Groen, Brad Er-
ickson, Timothy Miller, Lin Chen, Guergana K.
Savova, and James Pustejovsky. 2014. Temporal
annotations in the clinical domain. Transactions
of the Association for Computational Linguistics,
2(2):143?154.
Min Zhang, Wanxiang Che, Ai Ti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for seman-
tic role classification. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 200?207.
86
Temporal Annotation in the Clinical Domain
William F. Styler IV1, Steven Bethard2, Sean Finan3, Martha Palmer1,
Sameer Pradhan3, Piet C de Groen4, Brad Erickson4, Timothy Miller3,
Chen Lin3, Guergana Savova3 and James Pustejovsky5
1 Department of Linguistics, University of Colorado at Boulder
2 Department of Computer and Information Sciences, University of Alabama at Birmingham
3 Children?s Hospital Boston Informatics Program and Harvard Medical School
4 Mayo Clinic College of Medicine, Mayo Clinic, Rochester, MN
5 Department of Computer Science, Brandeis University
Abstract
This article discusses the requirements of
a formal specification for the annotation of
temporal information in clinical narratives.
We discuss the implementation and extension
of ISO-TimeML for annotating a corpus of
clinical notes, known as the THYME cor-
pus. To reflect the information task and the
heavily inference-based reasoning demands
in the domain, a new annotation guideline
has been developed, ?the THYME Guidelines
to ISO-TimeML (THYME-TimeML)?. To
clarify what relations merit annotation, we
distinguish between linguistically-derived and
inferentially-derived temporal orderings in the
text. We also apply a top performing Temp-
Eval 2013 system against this new resource to
measure the difficulty of adapting systems to
the clinical domain. The corpus is available to
the community and has been proposed for use
in a SemEval 2015 task.
1 Introduction
There is a long-standing interest in temporal reason-
ing within the biomedical community (Savova et al.,
2009; Hripcsak et al., 2009; Meystre et al., 2008;
Bramsen et al., 2006; Combi et al., 1997; Keravnou,
1997; Dolin, 1995; Irvine et al., 2008; Sullivan et
al., 2008). This interest extends to the automatic ex-
traction and interpretation of temporal information
from medical texts, such as electronic discharge sum-
maries and patient case summaries. Making effective
use of temporal information from such narratives is
a crucial step in the intelligent analysis of informat-
ics for medical researchers, while an awareness of
temporal information (both implicit and explicit) in a
text is also necessary for many data mining tasks.
It has also been demonstrated that the temporal in-
formation in clinical narratives can be usefully mined
to provide information for some higher-level tempo-
ral reasoning (Zhao et al., 2005). Robust temporal
understanding of such narratives, however, has been
difficult to achieve, due to the complexity of deter-
mining temporal relations among events, the diver-
sity of temporal expressions, and the interaction with
broader computational linguistic issues.
Recent work on Electronic Health Records (EHRs)
points to new ways to exploit and mine the informa-
tion contained therein (Savova et al., 2009; Roberts
et al., 2009; Zheng et al., 2011; Turchin et al., 2009).
We target two main use cases for extracted data. First,
we hope to enable interactive displays and summaries
of the patient?s records to the physician at the time of
visit, making a comprehensive review of the patient?s
history both faster and less prone to oversights. Sec-
ond, we hope to enable temporally-aware secondary
research across large databases of medical records
(e.g., ?What percentage of patients who undergo pro-
cedure X develop side-effect Y within Z months??).
Both of these applications require the extraction of
time and date associations for critical events and the
relative ordering of events during the patient?s period
of care, all from the various records which make up a
patient?s EHR. Although we have these two specific
applications in mind, the schema we have developed
is generalizable and could potentially be embedded
in a wide variety of biomedical use cases.
Narrative texts in EHRs are temporally rich doc-
uments that frequently contain assertions about the
timing of medical events, such as visits, laboratory
values, symptoms, signs, diagnoses, and procedures
(Bramsen et al., 2006; Hripcsak et al., 2009; Zhou
et al., 2008). Temporal representation and reason-
ing in the medical record are difficult due to: (1) the
diversity of time expressions; (2) the complexity of
determining temporal relations among events (which
are often left to inference); (3) the difficulty of han-
dling the temporal granularity of an event; and (4)
143
Transactions of the Association for Computational Linguistics, 2 (2014) 143?154. Action Editor: Ellen Riloff.
Submitted 9/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
general issues in natural language processing (e.g.,
ambiguity, anaphora, ellipsis, conjunction). As a re-
sult, the signals used for reconstructing a timeline can
be both domain-specific and complex, and are often
left implicit, requiring significant domain knowledge
to accurately detect and interpret.
In this paper, we discuss the demands on accurately
annotating such temporal information in clinical
notes. We describe an implementation and extension
of ISO-TimeML (Pustejovsky et al., 2010), devel-
oped specifically for the clinical domain, which we
refer to as the ?THYME Guidelines to ISO-TimeML?
(?THYME-TimeML?), where THYME stands for
?Temporal Histories of Your Medical Events?. A sim-
plified version of these guidelines formed the basis
for the 2012 i2b2 medical-domain temporal relation
challenge (Sun et al., 2013a).
This is being developed in the context of the
THYME project, whose goal is to both create ro-
bust gold standards for semantic information in clini-
cal notes, as well as to develop state-of-the-art algo-
rithms to train and test on this dataset.
Deriving timelines from news text requires the con-
crete realization of context-dependent assumptions
about temporal intervals, orderings and organization,
underlying the explicit signals marked in the text
(Pustejovsky and Stubbs, 2011). Deriving patient
history timelines from clinical notes also involves
these types of assumptions, but there are special de-
mands imposed by the characteristics of the clinical
narrative. Due to both medical shorthand practices
and general domain knowledge, many event-event
relations are not signaled in the text at all, and rely
on a shared understanding and common conceptual
models of the progressions of medical procedures
available only to readers familiar with language use
in the medical community.
Identifying these implicit relations and temporal
properties puts a heavy burden on the annotation
process. As such, in the THYME-TimeML guideline,
considerable effort has gone into both describing and
proscribing the annotation of temporal orderings that
are inferable only through domain-specific temporal
knowledge.
Although the THYME guidelines describe a num-
ber of departures from the ISO-TimeML standard for
expediency and ease of annotation, this paper will
focus on those differences specifically motivated by
the needs of the clinical domain, and on the conse-
quences for systems built to extract temporal data in
both the clinical and general domain.
2 The Nature of Clinical Documents
In the THYME corpus, we have been examining
1,254 de-identified1 notes from a large healthcare
practice (the Mayo Clinic), representing two distinct
fields within oncology: brain cancer, and colon can-
cer. To date, we have principally examined two dif-
ferent general types of clinical narrative in our EHRs:
clinical notes and pathology reports.
Clinical notes are records of physician interactions
with a patient, and often include multiple, clearly
delineated sections detailing different aspects of the
patient?s care and present illness. These notes are
fairly generic across institutions and specialities, and
although some terms and inferences may be specific
to a particular type of practice (such as oncology),
they share a uniform structure and pattern. The ?His-
tory of Present Illness?, for example, summarizes the
course of the patient?s chief complaint, as well as the
interventions and diagnostics which have been thus
far attempted. In other sections, the doctor may out-
line her current plan for the patient?s treatment, then
later describe the patient?s specific medical history,
allergies, care directives, and so forth.
Most critically for temporal reasoning, each clin-
ical note reflects a single time in the patient?s treat-
ment history at which all of the doctor?s statements
are accurate (the DOCTIME), and each section tends
to describe events of a particular timeframe. For
example, ?History of Present illness? predominantly
describes events occuring before DOCTIME, whereas
?Medications? provides a snapshot at DOCTIME and
?Ongoing Care Orders? discusses events which have
not yet occurred.2
Clinical notes contain rich temporal information
and background, moving fluidly from prior treat-
ments and symptoms to present conditions to future
interventions. They are also often rich with hypo-
thetical statements (?if the tumor recurs, we can...?),
each of which can form its own separate timeline.
By constrast, pathology notes are quite different.
Such notes are generated by a medical pathologist
1Although most patient information was removed, dates
and temporal information were not modified according to this
project?s specific data use agreement.
2One complication is the propensity of doctors and automated
systems to later update sections in a note without changing the
timestamp or metadata. We have added a SECTIONTIME to keep
these updated sections from affecting our overall timeline.
144
upon receipt and analysis of specimens (ranging from
tissue samples from biopsy to excised portions of
tumor or organs). Pathology notes provide crucial
information to the patient?s doctor confirming the
malignancy (cancer) in samples, describing surgi-
cal margins (which indicate whether a tumor was
completely excised), and classifying and ?staging? a
tumor, describing the severity and spread of the can-
cer. Because the information in such notes pertains
to samples taken at a single moment in time, they are
temporally sparse, seldom referring to events before
or after the examination of the specimen. However,
they contain critical information about the state of
the patient?s illness and about the cancer itself, and
must be interpreted to understand the history of the
patient?s illness.
Most importantly, in all EHRs, we must contend
with the results of a fundamental tension in mod-
ern medical records: hyper-detailed records provide
a crucial defense against malpractice litigation, but
including such detail takes enormous time, which
doctors seldom have. Given that these notes are writ-
ten by and for medical professionals (who form a
relatively insular speech community), a great many
non-standard expressions, abbreviations, and assump-
tions of shared knowledge are used, which are simul-
taneously concise and detail-rich for others who have
similar backgrounds.
These time-saving devices can range from tempo-
rally loaded acronyms (e.g., ?qid?, Latin for quater in
die, ?four times daily?), to assumed orderings (a diag-
nostic test for a disorder is assumed to come before
the procedure which treats it), and even to completely
implicit events and temporal details. For example,
consider the sentence in (1).
(1) Colonoscopy 3/12/10, nodule biopsies negative
We must understand that during the colonoscopy,
the doctor obtained biopsies of nodules, which were
packaged and sent to a pathologist, who reviewed
them and determined them to be ?negative? (non-
cancerous).
In such documents, we must recover as much tem-
poral detail as possible, even though it may be ex-
pressed in a way which is not easily understood out-
side of the medical community, let alone by linguists
or automated systems. We must also be aware of the
legal relevance of some events (e.g., ?We discussed
the possible side effects?), even when they may not
seem relevant to the patient?s actual care.
Finally, each specialty and note type has separate
conventions. Within colon cancer notes, the Amer-
ican Joint Committee on Cancer (AJCC) Staging
Codes (e.g., T4N1, indicating the nature of the tumor,
lymph node and metastasis involvement) are metic-
ulously recorded, but are largely absent in the brain
cancer notes which make up the second corpus in
our project. So, although clinical notes share many
similarities, annotators without sufficient domain ex-
pertise may require additional training to adapt to the
inferences and nuances of a new clinical subdomain.
3 Interpreting ?Event? and Temporal
Expressions in the Clinical Domain
Much prior work has been done on standardizing
the annotation of events and temporal expressions
in text. The most widely used approach is the ISO-
TimeML specification (Pustejovsky et al., 2010), an
ISO standard that provides a common framework for
annotating and analyzing time, events, and event rela-
tions. As defined by ISO-TimeML, an EVENT refers
to anything that can be said ?to obtain or hold true, to
happen or to occur?. This is a broad notion of event,
consistent with Bach?s use of the term ?eventuality?
(Bach, 1986) as well as the notion of fluents in AI
(McCarthy, 2002).
Because the goals of the THYME project involve
automatically identifying the clinical timeline for
a patient from clincal records, the scope of what
should be admitted into the domain of events is inter-
preted more broadly than in ISO-TimeML3. Within
the THYME-TimeML guideline, an EVENT is any-
thing relevant to the clinical timeline, i.e., anything
that would show up on a detailed timeline of the pa-
tient?s care or life. The best single-word syntactic
head for the EVENT is then used as its span. For
example, a diagnosis would certainly appear on such
a timeline, as would a tumor, illness, or procedure.
On the other hand, entities that persist throughout
the relevant temporal period of the clinical timeline
(endurants in ontological circles) would not be con-
sidered as event-like. This includes the patient, other
humans mentioned (the patient?s mother-in-law or
the doctor), organizations (the emergency room),
non-anatomical objects (the patient?s car), or indi-
vidual parts of the patient?s anatomy (an arm is not
an EVENT unless missing or otherwise notable).
To meet our explicit goals, the THYME-TimeML
guideline introduces two additional levels of interpre-
3Our use of the term ?EVENT? corresponds with the less
specific ISO-TimeML term ?Eventuality?
145
tation beyond that specified by ISO-TimeML: (i) a
well-defined task; and (ii) a clearly identified domain.
By focusing on the creation of a clinical timeline
from clinical narrative, the guideline imposes con-
straints that cannot be assumed for a broadly defined
and domain independent annotation schema.
Some EVENTs annotated under our guideline are
considered meaningful and eventive mostly by virtue
of a specific clinical or legal value. For example,
AJCC Staging Codes (discussed in Section 2) are
eventive only in the sense of the code being assigned
to a tumor at a given moment in the patient?s care.
However, they are of such critical importance and
informative value to doctors that we have chosen to
annotate them specifically so that they will show up
on the patient?s timeline in a clinical setting.
Similarly, because of legal pressures to establish in-
formed consent and patient knowledge of risk, entire
paragraphs of clinical notes are dedicated to docu-
menting the doctor?s discussion of risks, plans, and
alternative strategies. As such, we annotate verbs of
discussion (?We talked about the risks of this drug?),
consent (?She agreed with the current plan?), and
comprehension (?Mrs. Larsen repeated the potential
side effects back to me?), even though they are more
relevant to legal defense than medical treatment.
It is also because of this grounding in clinical lan-
guage that entities and other non-events are often
interpreted in terms of their associated eventive prop-
erties. There are two major types for which this is a
significant shift in semantic interpretation:
(2) a Medication as Event:
Orders: Lariam twice daily.
b Disorder as Event:
Tumor of the left lung.
In both these cases, entities which are not typically
marked as events are identified as such, because they
contribute significant information to the clinical time-
line being constructed. In (2a), for example, the
TIMEX3 ?twice daily? is interpreted as scoping over
the eventuality of the patient taking the medication,
not the prescription event. In sentence (2b), the ?tu-
mor? is interpreted as a stative eventuality of the
patient having a tumor located within an anatomical
region, rather than an entity within an entity.
Within the medical domain, these eventive inter-
pretations of medications, growths and status codes
are unambiguous and consistent. Doctors in clini-
cal notes (unlike in biomedical research texts) do
not discuss medications without an associated (im-
plicit) administering EVENT (though some mentions
may be hypothetical, generic or negated). Similarly,
mentions of symptoms or disorders reflect occur-
rences in a patient?s life, rather than abstract entities.
With these interpretations in mind, we can safely in-
fer, for instance, that all UMLS (Unified Medical
Language System, (Bodenreider, 2004)) entities of
the types Disorder, Chemical/Drug, Procedure and
Sign/Symptom will be EVENTs.
In general, in the medical domain, it is essential to
read ?between the lines? of the shorthand expressions
used by the doctors, and recognize implicit events
that are being referred to by specific anatomical sites
or medications.
4 Modifications to ISO-TimeML for the
Clinical Domain
Overall, we have found that the specification required
for temporal annotation in the clinical domain does
not require substantial modification from existing
specifications for the general domain. The clinical
domain includes no shortage of inferences, short-
hands, and unusual use of language, but the structure
of the underlying timeline is not unique.
As a result of this, we have been able to adopt most
of the framework from ISO-TimeML, adapting the
guidelines where needed, as well as reframing the
focus of what gets annotated. This is reflected in a
comprehensive guideline, incorporating the specific
patterns and uses of events and temporal expressions
as seen in clinical data. This approach allows the
resulting annotations to be interoperable with exist-
ing solutions, while still accommodating the major
differences in the nature of the texts. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org4
Our extensions of the ISO-TimeML specification
to the clinical domain are intended to address specific
constructions, meanings, and phenomena in medical
texts. Our schema differs from ISO-TimeML in a
few notable ways.
EVENT Properties We have both simplified the
ISO-TimeML coding of EVENTs, and extended it to
meet the needs of the clinical domain and the specific
language goals of the clinical narrative.
4Access to the corpus will require a data use agreement.
More information about this process is available from the corpus
website.
146
Consider, for example, how modal subordination is
handled in ISO-TimeML. This involves the semantic
characterization of an event as ?likely?, ?possible?, or
as presented by observation, evidence, or hearsay. All
of these are accounted for compositionally in ISO-
TimeML within the SLINK (Subordinating Link)
relation (Pustejovsky et al., 2005). While accept-
ing ISO-TimeML?s definition of event modality, we
have simplified the annotation task within the cur-
rent guideline, so that EVENTs now carry attributes
for ?contextual modality?, ?contextual aspect? and
?permanence?.
Contextual modality allows the values ACTUAL,
HYPOTHETICAL, HEDGED, and GENERIC. ACTUAL
covers EVENTs which have actually happened, e.g.,
?We?ve noted a tumor?. HYPOTHETICAL covers con-
ditionals and possibilities, e.g., ?If she develops a
tumor?. HEDGED is for situations where doctors
proffer a diagnosis, but do so cautiously, to avoid
legal liability for an incorrect diagnosis or for over-
looking a correct one. For example:
(3) a. The signal in the MRI is not inconsistent
with a tumor in the spleen.
b. The rash appears to be measles, awaiting
antibody test to confirm.
These HEDGED EVENTs are more real than a hypo-
thetical diagnosis, and likely merit inclusion on a
timeline as part of the diagnostic history, but must
not be conflated with confirmed fact. These (and
other forms of uncertainty in the medical domain)
are discussed extensively in (Vincze et al., 2008). In
contrast, GENERIC EVENTs do not refer to the pa-
tient?s illness or treatment, but instead discuss illness
or treatment in general (often in the patient?s specific
demographic). For example:
(4) In other patients without significant comor-
bidity that can tolerate adjuvant chemother-
apy, there is a benefit to systemic adjuvant
chemotherapy.
These sections would be true if pasted into any pa-
tient?s note, and are often identical chunks of text
repeatedly used to justify a course of action or treat-
ment as well as to defend against liability.
Contextual Aspect (to distinguish from grammati-
cal aspect), allows the clinically-necessary category,
INTERMITTENT. This serves to distinguish intermit-
tent EVENTs (such as vomiting or seizures) from
constant, more stative EVENTs (such as fever or sore-
ness). For example, the bolded EVENT in (5a) would
be marked as INTERMITTENT, while that in (5b)
would not:
(5) a She has been vomiting since June.
b She has had swelling since June.
In the first case, we assume that her vomiting has
been intermittent, i.e., there were several points since
June in which she was not actively vomiting. In the
second case, unless made otherwise explicit (?she has
had occasional swelling?), we assume that swelling
was a constant state. This property is also used when
a particular instance of an EVENT is intermittent,
even though it generally would not be:
(6) Since starting her new regime, she has had occa-
sional bouts of fever, but is feeling much better.
The permanence attribute has two values, FINITE
and PERMANENT. Permanence is a property of dis-
eases themselves, roughly corresponding to the med-
ical concept of ?chronic? vs. ?acute? disease, which
marks whether a disease is persistent following diag-
nosis. For example, a (currently) uncurable disease
like Multiple Sclerosis would be classed as PERMA-
NENT, and thus, once mentioned in a patient?s note,
will be assumed to persist through the end of the
patient?s timeline. This is compared with FINITE
disorders like ?Influenza? or ?fever?, which, if not
mentioned in subsequent notes, should be considered
cured and no longer belongs on the patient?s time-
line. Because it requires domain-specific knowledge,
although present in the specification, Permanence
is not currently annotated. However, annotators are
trained on the basic idea and told about subsequent
axiomatic assignment. The addition of this property
to our schema is designed to relieve annotators of any
feeling of obligation to express this inferred informa-
tion in some other way.
TIMEX3 Types Temporal expressions (TIMEX3s)
in the clinical domain function the same as in the gen-
eral linguistic community, with two notable excep-
tions. ISO-TimeML SETs (statements of frequency)
occur quite frequently in the medical domain, par-
ticularly with regard to medications and treatments.
Medication sections within notes often contain long
lists of medications, each with a particular associated
set (?Claritin 30mg twice daily?), and further tempo-
ral specification is not uncommon (e.g., ?three times
per day at meals?, ?once a week at bedtime?).
The second major change for the medical domain
is a new type of TIMEX3 which we call PREPOS-
TEXP. This covers temporally complex terms like
147
?preoperative?, ?postoperative?, and ?intraoperative?.
These temporal expressions designate a span of time
bordered, usually only on one side, by the incorpo-
rated event (an operation, in the previous EVENTs).
In many cases, the referent is clear:
(7) She underwent hemicolectomy last week, and
had some postoperative bleeding.
Here we understand that ?postoperative? refers to
?the period of time following the hemicolectomy?. In
these cases, the PREPOSTEXP makes explicit a tempo-
ral link between the bleeding and the hemicolectomy.
In other cases, no clear referent is present:
(8) Patient shows some post-procedure scarring.
In these situations, where no procedure is mentioned
(or the reference is never explicitly resolved), we
treat the PREPOSTEXP as a narrative container (see
Section 5), covering the span of time following the
unnamed procedure.
Finally, it is worth noting that the process of nor-
malizing those TIMEX3s is significantly more com-
plex relative to the general domain, because many
temporal expressions are anchored not to dates or
times, but to other EVENTs (whose dates are often
not mentioned or not known by the physician). As
we move towards a complete system, we are working
to expand the ISO-TimeML system for TIMEX3 nor-
malization to allow some value to be assigned to a
phrase like ?in the months after her hemicolectomy?
when no referent date is present. ISO-TimeML, in
discussion with ISO TC 37SC 4, plans to reference
to such TIMEX3s in a future release of the standard.
5 Temporal Ordering and Narrative
Containers
The semantic content and informational impact of
a timeline is encoded in the ordering relations that
are identified between the temporal and event expres-
sions present in clinical notes. ISO-TimeML speci-
fies the standard thirteen ?Allen relations? from the
interval calculus (Allen, 1983), which it refers to as
TLINK values. For unguided, general-purpose annota-
tion, the number of relations that could be annotated
grows quadratically with the number of events and
times, and the task quickly becomes unmanageable.
There are, however, strategies that we can adopt to
make this labeling task more tractable. Temporal
ordering relations in text are of three kinds:
1. Relations between two events
2. Relations between two times
3. Relations between a time and an event.
ISO-TimeML, as a formal specification of the tem-
poral information conveyed in language, makes no
distinction between these ordering types. Humans,
however, do make distinctions, based on local tempo-
ral markers and the discourse relations established in
a narrative (Miltsakaki et al., 2004; Poesio, 2004).
Because of the difficulty of humans capturing ev-
ery relationship present in the note (and the disagree-
ment which arises when annotators attempt to do so),
it is vital that the annotation guidelines describe an
approach that reduces the number of relations that
must be considered, but still results in maximally in-
formative temporal links. We have found that many
of the weaknesses in prior annotation approaches
stem from interaction between two competing goals:
? The guideline should specify certain types of an-
notations that should be performed;
? The guideline should not force annotations to be
performed when they need not be.
Failing in the first goal will result in under-annotation
and the neglect of relations which provide necessary
information for inference and analysis. Failure in the
second goal results in over-annotation, creating com-
plex webs of temporal relations which yield mostly
inferable information, but which complicate annota-
tion and adjudication considerably.
Our method of addressing both goals in tempo-
ral relations annotation is that of the narrative con-
tainer, discussed in Pustejovsky and Stubbs (2011).
A narrative container can be thought of as a temporal
bucket into which an EVENT or series of EVENTs
may fall, or a natural cluster of EVENTs around a
given time or situation. These narrative containers
are often represented (or ?anchored?) by dates or
other temporal expressions (within which a variety
of different EVENTs occur), although they can also
be anchored to more abstract concepts (?recovery?
which might involve a variety of EVENTs) or even
durative EVENTs (many other EVENTs can occur dur-
ing a surgery). Rather than marking every possible
TLINK between each EVENT, we instead try to link
all EVENTs to their narrative containers, and then
link those containers so that the contained EVENTs
can be linked by inference.
First, annotators assign each event to one of four
broad narrative containers: before the DOCTIME, be-
fore and overlapping the DOCTIME, just overlapping
the DOCTIME or after the DOCTIME. This narrative
148
container is identified by the EVENT attribute Doc-
TimeRel. After the assignment of DocTimeRel, the
remainder of the narrative container relations must
be specified using temporal links (TLINKs). There
are five different temporal relations used for such
TLINKs: BEFORE, OVERLAP, BEGINS-ON, ENDS-ON
and CONTAINS5. Due to our narrative container ap-
proach, CONTAINS is the most frequent relation by a
large margin.
EVENTs serving as narrative container anchors are
not tagged as containers per-se. Instead, annotators
use the narrative container idea to help them visu-
alize the temporal relations within a document, and
then make a series of CONTAINS TLINK annotations
which establish EVENTs and TIMEX3s as anchors,
and specify their contents. If the annotators do their
jobs correctly, properly implementing DocTimeRel
and creating accurate TLINKs, a good understanding
of the narrative containers present in a document will
naturally emerge from the annotated text.
The major advantage introduced with narrative
containers is this: a narrative event is placed within a
bounding temporal interval which is explicitly men-
tioned in the text. This allows EVENTs within sep-
arate containers to be linked by post-hoc inference,
temporal reasoning, and domain knowledge, rather
than by explicit (and time-consuming) one-by-one
temporal relations annotation.
A secondary advantage is that this approach works
nicely with the general structure of story-telling in
both the general and clinical domains, and provides a
compelling and useful metaphor for interpreting time-
lines. Often, especially in clinical histories, doctors
will cluster discussions of symptoms, interventions
and diagnoses around a given date (e.g. a whole para-
graph starting ?June 2009:?), a specific hospitaliza-
tion (?During her January stay at Mercy?), or a given
illness or treatment (?While she underwent Chemo?).
Even when specific EVENTs are not explicitly or-
dered within a cluster (often because the order can be
easily inferred with domain knowledge), it is often
quite easy to place the EVENTs into containers, and
just a few TLINKs can order the containers relative to
one another with enough detail to create a clinically
useful understanding of the overall timeline.
Narrative containers also allow the inference of re-
lations between sub-events within nested containers:
5This is a subset of the ISO-TimeML TLINK types, excluding
those seldom occurring in medical records, like ?simultaneous?
as well as inverse relations like ?during? or ?after?.
(9) December 19th: The patient underwent an MRI
and EKG as well as emergency surgery. Dur-
ing the surgery, the patient experienced mild
tachycardia, and she also bled significantly
during the initial incision.
1. December 19th CONTAINS MRI
2. December 19th CONTAINS EKG
3. December 19th CONTAINS surgery
a. surgery CONTAINS tachycardia
b. surgery CONTAINS incision
c. incision CONTAINS bled
Through our container nesting, we can automatically
infer that ?bled? occurred on December 19th (because
?19th? CONTAINS ?surgery? which CONTAINS ?inci-
sion? which CONTAINS ?bled?). This also allows the
capture of EVENT/sub-event relations, and the rapid
expression of complex temporal interactions.
6 Explicit vs. Inferable Annotation
Given a specification language, there are essentially
two ways of introducing the elements into the docu-
ment (data source) being annotated:6
? Manual annotation: Elements are introduced into
the document directly by the human annotator fol-
lowing the guideline.
? Automatic (inferred) annotation: Elements are cre-
ated by applying an automated procedure that in-
troduces new elements that are derivable from the
human annotations.
As such, there is a complex interaction between spec-
ification and guideline, and we focus on how the
clinical annotation task has helped shape and refine
the annotation guidelines. It is important to note that
an annotation guideline does not necessarily force
the markup of certain elements in a text, even though
the specification language (and the eventual goal of
the project) might require those annotations to exist.
In some cases, these added annotations are derived
logically from human annotations. Explicitly marked
temporal relations can be used to infer others that are
not marked but exist implicitly through closure. For
instance, given EVENTs A, B and C and TLINKs ?A
BEFORE B? and ?B BEFORE C?, the TLINK ?A BE-
FORE C? can be automatically inferred. Repeatedly
applying such inference rules allows all inferable
6We ignore the application of automatic techniques, such as
classifiers trained on external datasets, as our focus here is on
the preparation of the gold standard used for such classifiers.
149
TLINKs to be generated (Verhagen, 2005). We can
use this idea of closure to show our annotators which
annotations need not be marked explicitly, saving
time and effort. We have also incorporated these clo-
sure rules into our inter-annotator agreement (IAA)
calculation for temporal relations, described further
in Section 7.2.
The automatic application of rules following the
annotation of the text is not limited to the marking
of logically inferable relations or EVENTs. In the
clinical domain, the combination of within-group
shared knowledge and pressure towards concise writ-
ing leads to a number of common, inferred relations.
Take, for example, the sentence:
(10) Jan 2013: Colonoscopy, biopsies. Pathology
showed adenocarcinoma, resected at Mercy.
Diagnosis T3N1 Adenocarcinoma.
In this sentence, only the CONTAINS relations be-
tween ?Jan 2013? and the EVENTs (in bold) are
explicitly stated. However, based on the known
progression-of-care for colon cancer, we can infer
that the colonoscopy occurs first, biopsies occur dur-
ing the colonoscopy, pathology happens afterwards,
a diagnosis (here, adenocarcinoma) is returned after
pathology, and resection of the tumor occurs after
diagnosis. The presence of the AJCC staging infor-
mation in the final sentence (along with the confir-
mation of the adenocarcinoma diagnosis) implies a
post-surgical pathology exam of the resected spec-
imen, as the AJCC staging information cannot be
determined without this additional examination.
These inferences come naturally to domain ex-
perts but are largely inaccessible to people outside
the medical community without considerable anno-
tator training. Making explicit our understanding of
these ?understood orderings? is crucial; although they
are not marked by human annotators in our schema,
the annotators often found it initially frustrating to
leave these (purely inferential) relations unstated. Al-
though many of our (primarily linguistically trained)
annotators learned to see these patterns, we chose to
exclude them from the manual task since newer an-
notators with varying degrees of domain knowledge
may struggle if asked to manually annotate them.
Similar unspoken-but-understood orderings are
found throughout the clinical domain. As mentioned
in Section 3, both Permanence and Contextual As-
pect:Intermittent are properties of symptoms and dis-
eases themselves, rather than of the patient?s particu-
lar situation. As such, these properties could easily
Annotation Type Raw Count
EVENT 15,769
TIMEX3 1,426
LINK 7935
Total 25,130
Table 1: Raw Frequency of Annotation Types
TLINK Type Raw Count % of TLINKs
CONTAINS 5,112 64.42%
OVERLAP 1,205 15.19%
BEFORE 1,004 12.65%
BEGINS-ON 488 6.15%
ENDS-ON 126 1.59%
Total 7,935 100.00%
Table 2: Relative Frequency of TLINK types
be identified and marked across a medical ontology,
and then be automatically assigned to EVENTs rec-
ognized as specific medical named entities.
Finally, due to the peculiarities of EHR systems,
some annotations must be done programatically. Ex-
act dates of patient visit (or of pathology/radiology
consult) are often recorded as metadata on the EHR
itself, rather than within the text, making the canoni-
cal DOCTIME (or time of automatic section modifi-
cations) difficult to access in de-identified plaintext
data, but easy to find automatically.
7 Results
We report results on the annotations from the here-
released subset of the THYME colon cancer corpus,
which includes clinical notes and pathology reports
for 35 patients diagnosed with colon cancer for a
total of 107 documents. Each note was annotated
by a pair of graduate or undergraduate students in
Linguistics at the University of Colorado, then adju-
dicated by a domain expert. These clinical narratives
were sampled from the EHRs of a major healthcare
center (the Mayo Clinic). They were deidentified for
all patient-sensitive information; however, original
dates were retained.
7.1 Descriptive Statistics
Table 1 presents the raw counts for events, temporal
expressions and links in the adjudicated gold anno-
tations. Table 2 presents the number and percentage
of TLINKs by type in the adjudicated relations gold
annotations.
150
Annotation Type F1-Score Alpha
EVENT 0.8038 0.7899
TIMEX3 0.8047 0.6705
LINK: Participants only 0.5012 0.4999
LINK: Participants+type 0.4506 0.4503
LINK: CONTAINS 0.5630 0.5626
Table 3: IAA (F1-Score and Alpha) by annotation type
EVENT Property F1-Score Alpha
DocTimeRel 0.7189 0.6889
Cont.Aspect 0.9947 0.9930
Cont.Modality 0.9547 0.9420
Table 4: IAA (F1-Score and Alpha) for EVENT properties
7.2 Inter-annotator Agreement
We report inter-annotator agreement (IAA) results
on the THYME corpus. Each note was annotated by
two independent annotators. The final gold standard
was produced after disagreement adjudication by a
third annotator was performed.
We computed the IAA as F1-score and Krippen-
dorff?s Alpha (Krippendorff, 2012) by applying clo-
sure, using explicitly marked temporal relations to
identify others that are not marked but exist implicitly.
In the computation of the IAA, inferred-only TLINKs
do not contribute to the score, matched or unmatched.
For instance, if both annotators mark A BEFORE B
and B BEFORE C, to prevent artificially inflating the
agreement score, the inferred A BEFORE C is ignored.
Likewise, if one annotator marked A BEFORE B and
B BEFORE C and the other annotator did not, the
inferred A BEFORE C is not counted. However, if
one annotator did explicitly mark A BEFORE C, then
an equivalent inferred TLINK would be used to match
it. EVENT and TIMEX3 IAA was generated based
on exact and overlapping spans, respectively. These
results are reported in Table 3.
The THYME corpus also differs from ISO-
TimeML in terms of EVENT properties, with the
addition of DocTimeRel, ContextualModality and
ContextualAspect. IAA for these properties is in
Table 4.
7.3 Baseline Systems
To get an idea of how much work will be neces-
sary to adapt existing temporal information extrac-
tion systems to the clinical domain, we took the freely
available ClearTK-TimeML system (Bethard, 2013),
TempEval 2013 THYME Corpus
P R F1 P R F1
TIMEX3 83.2 71.7 77.0 59.3 42.8 49.7
EVENT 81.4 76.4 78.8 78.9 23.9 36.6
DocTimeRel - - - 47.4 47.4 47.4
LINK7 28.6 30.9 26.6 22.7 18.6 20.4
EVENT-TIMEX3 - - - 32.3 60.7 42.1
EVENT-EVENT - - - 7.0 3.0 4.2
Table 5: Performance of ClearTK-TimeML models, as
reported in the TempEval 2013 competition, and as applied
to the THYME Corpus development set.
which was among the top performing systems in
TempEval 2013 (UzZaman et al., 2013), and eval-
uated its performance on the THYME corpus.
ClearTK-TimeML uses support vector machine
classifiers trained on the TempEval 2013 training
data, employing a small set of features including
character patterns, tokens, stems, part-of-speech tags,
nearby nodes in the constituency tree, and a small
time word gazetteer. For EVENTs and TIMEX3s,
the ClearTK-TimeML system could be applied di-
rectly to the THYME corpus. For DocTimeRels, the
relation for an EVENT was taken from the TLINK
between that EVENT and the document creation time,
after mapping INCLUDES to OVERLAP. EVENTs
with no such TLINK were assumed to have a Doc-
TimeRel of OVERLAP. For other temporal relations,
INCLUDES was mapped to CONTAINS.
Results of this system on TempEval 2013 and the
THYME corpus are shown in Table 5. For time ex-
pressions, performance when moving to the clinical
data degrades about 25%, from F1 of 77.0 to 49.7.
For events, the degradation is much larger, about
40%, from 78.8 to 36.6, most likely because of the
large number of clinical symptoms, diseases, disor-
ders, etc. which have never been observed by the
system during training. Temporal relations are a bit
more difficult to compare because TempEval lumped
DocTimeRel and other temporal relations together
and had several differences in their evaluation met-
ric7. However, we at least can see that performance
of the ClearTK-TimeML system on temporal rela-
tions is low on clinical text, achieving only F1 of
20.4.
These results suggest that clinical narratives do
7The TempEval 2013 evaluation metric penalized systems
for parts of the text that were not examined by annotators, and
used different variants of closure-based precision and recall.
151
indeed present new challenges for temporal informa-
tion extraction systems, and that having access to
domain specific training data will be crucial for ac-
curate extraction in the clinical domain. At the same
time, it is encouraging that we were able to apply
existing ISO-TimeML-based systems to our corpus,
despite the several extensions to ISO-TimeML that
were necessary for clinical narratives.
8 Discussion
CONTAINS plays a large role in the THYME cor-
pus, representing 66% of TLINK annotations made,
compared with only 14.6% for OVERLAP, the second
most frequent type. We also see that BEFORE links
are relatively less common than OVERLAP and CON-
TAINS, illustrating that much of the temporal ordering
on the timeline is accomplished by using many ver-
tical links (CONTAINS, OVERLAP) to build contain-
ers, and few horizontal links (BEFORE, BEGINS-ON,
ENDS-ON) to order them.
IAA on EVENTs and Temporal Expressions is
strong, although differentiating implicit EVENTs
(which should not be marked) from explicit, mark-
able EVENTs remains one of the biggest sources of
disagreement. When compared to the data from the
2012 i2b2 challenge (Sun et al., 2013b), our IAA
figures are quite similar. Even with our more com-
plex schema, we achieved an F1-score of 0.8038 for
EVENTs (compared to the i2b2 score of 0.87 for par-
tial match). For TIMEX3s, our F1-score was 0.8047,
compared to an F1-score of 0.89 for i2b2.
TLINKing medical EVENTs remains a very diffi-
cult task. By using our narrative container approach
to constrain the number of necessary annotations and
by eliminating often-confusing inverse relations (like
?after? and ?during?) (neither of which were done for
the i2b2 data), we were able to significantly improve
on the i2b2 TLINK span agreement F1-score of 0.39,
achieving an agreement score of 0.5012 for all LINKs
across our corpus. The majority of remaining an-
notator disagreement comes from different opinions
about whether any two EVENTs require an explicit
TLINK between them or an inferred one, rather than
what type of TLINK it would be (e.g. BEFORE vs.
CONTAINS). Although our results are still signifi-
cantly higher than the results reported for i2b2, and
in line with previously reported general news figures,
we are not satisfied. Improving IAA is an important
goal for future work, and with further training, speci-
fication, experience, and standardization, we hope to
clarify contexts for explicit TLINKS.
News-trained temporal information extraction sys-
tems see a significant drop in performance when ap-
plied to the clinical texts of the THYME corpus. But
as the corpus is an extension of ISO-TimeML, future
work will be able to train ISO-TimeML compliant
systems on the annotations of the THYME corpus to
reduce or eliminate this performance gap.
Some applications that our work may enable in-
clude (1) better understanding of event semantics,
such as whether a disease is chronic or acute and
its usual natural history, (2) typical event duration
for these events, (3) the interaction of general and
domain-specific events and their importance in the fi-
nal timeline, and, more generally, (4) the importance
of rough temporality and narrative containers as a
step towards finer-grained timelines.
We have several avenues of ongoing and future
work. First, we are working to demonstrate the utility
of the THYME corpus for training machine learning
models. We have designed support vector machine
models with constituency tree kernels that were able
to reach an F1-score of 0.737 on an EVENT-TIMEX3
narrative container identification task (Miller et al.,
2013), and we are working on training models to
identify events, times and the remaining types of
temporal relations. Second, as per our motivating
use cases, we are working to integrate this annotation
data with timeline visualization tools and to use these
annotations in quality-of-care research. For example,
we are using temporal reasoning built on this work to
investigate the liver toxicity of methotrexate across
a large corpus of EHRs (Lin et al., under review)].
Finally, we plan to explore the application of our
notion of an event (anything that should be visible on
a domain-appropriate timeline) to other domains. It
should transfer naturally to clinical notes about other
(non-cancer) conditions, and even to other types of
clinical notes, as certain basic events should always
be included in a patient?s timeline. Applying our
notion of event to more distant domains, such as legal
opinions, would require first identifying a consensus
within the domain about which events must appear
on a timeline.
9 Conclusion
Much of the information in clinical notes critical to
the construction of a detailed timeline is left implicit
by the concise shorthand used by doctors. Many
events are referred to only by a term such as ?tu-
152
mor?, while properties of the event itself, such as
?intermittent?, may not be specified. In addition, the
ordering of events on a timeline is often left to the
reader to infer, based on domain-specific knowledge.
It is incumbent upon the annotation guideline to in-
dicate that only informative event orderings should
be annotated, while leaving domain-specific order-
ings to post-annotation inference. This document
has detailed our approach to adapting the existing
ISO-TimeML standard to this recovery of implicit
information, and defining guidelines that support an-
notation within this complex domain. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org, and the full
corpus has been proposed for use in a SemEval 2015
shared task.
Acknowledgments
The project described is supported by Grant Num-
ber R01LM010090 and U54LM008748 from the Na-
tional Library Of Medicine. The content is solely the
responsibility of the authors and does not necessarily
represent the official views of the National Library
Of Medicine or the National Institutes of Health.
We would also like to thank Dr. Piet C. de Groen
and Dr. Brad Erickson at the Mayo Clinic, as well as
Dr. William F. Styler III, for their contributions to the
schema and to our understanding of the intricacies of
clinical language.
References
James F Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Emmon Bach. 1986. The algebra of events. Linguistics
and philosophy, 9(1):5?16.
Steven Bethard. 2013. Cleartk-timeml: A minimalist ap-
proach to tempeval 2013. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
10?14, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): integrating biomedical
terminology. Nucleic acids research, 32(Database
issue):D267?D270, January.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Finding temporal order
in discharge summaries. In AMIA Annual Symposium
Proceedings, volume 2006, page 81. American Medical
Informatics Association.
Carlo Combi, Yuval Shahar, et al. 1997. Temporal reason-
ing and temporal data maintenance in medicine: issues
and challenges. Computers in biology and medicine,
27(5):353?368.
Robert H Dolin. 1995. Modeling the temporal complex-
ities of symptoms. Journal of the American Medical
Informatics Association, 2(5):323?331.
George Hripcsak, Nicholas D Soulakis, Li Li, Frances P
Morrison, Albert M Lai, Carol Friedman, Neil S Cal-
man, and Farzad Mostashari. 2009. Syndromic surveil-
lance using ambulatory electronic health records. Jour-
nal of the American Medical Informatics Association,
16(3):354?361.
Ann K Irvine, Stephanie W Haas, and Tessa Sullivan.
2008. Tn-ties: A system for extracting temporal infor-
mation from emergency department triage notes. In
AMIA Annual Symposium proceedings, volume 2008,
page 328. American Medical Informatics Association.
Elpida T Keravnou. 1997. Temporal abstraction of med-
ical data: Deriving periodicity. In Intelligent Data
Analysis in Medicine and Pharmacology, pages 61?79.
Springer.
Klaus H. Krippendorff. 2012. Content Analysis: An
Introduction to Its Methodology. SAGE Publications,
Inc, third edition edition, April.
Chen Lin, Elizabeth Karlson, Dmitriy Dligach, Mon-
ica Ramirez, Timothy Miller, Huan Mo, Natalie
Braggs, Andrew Cagan, Joshua Denny, and Guer-
gana. Savova. under review. Automatic identification
of methotrexade-induced liver toxicity in rheumatoid
arthritis patients from the electronic medical records.
Journal of the Medical Informatics Association.
John McCarthy. 2002. Actions and other events in sit-
uation calculus. In Proceedings of the International
conference on Principles of Knowledge Representation
and Reasoning, pages 615?628. Morgan Kaufmann
Publishers; 1998.
Ste?phane M Meystre, Guergana K Savova, Karin C Kipper-
Schuler, John F Hurdle, et al. 2008. Extracting infor-
mation from textual documents in the electronic health
record: a review of recent research. Yearb Med Inform,
35:128?44.
Timothy Miller, Steven Bethard, Dmitriy Dligach, Sameer
Pradhan, Chen Lin, and Guergana Savova. 2013. Dis-
covering temporal narrative containers in clinical text.
In Proceedings of the 2013 Workshop on Biomedical
Natural Langua ge Processing, pages 18?26, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
153
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bon-
nie Webber. 2004. The penn discourse treebank. In In
Proceedings of LREC 2004.
Massimo Poesio. 2004. Discourse annotation and seman-
tic annotation in the gnome corpus. In In Proceedings
of the ACL Workshop on Discourse Annotation.
James Pustejovsky and Amber Stubbs. 2011. Increasing
informativeness in temporal annotation. In Proceedings
of the 5th Linguistic Annotation Workshop, pages 152?
160. Association for Computational Linguistics.
James Pustejovsky, Robert Knippen, Jessica Littman, and
Roser Sauri. 2005. Temporal and event information in
natural language text. Language Resources and Evalu-
ation, 39(2-3):123?164.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent
Romary. 2010. Iso-timeml: An international standard
for semantic annotation. In Proceedings of the Seventh
International Conference on Language Resources and
Evaluation (LREC 2010), Valletta, Malta.
Angus Roberts, Robert Gaizauskas, Mark Hepple, George
Demetriou, Yikun Guo, and Ian Roberts. 2009. Build-
ing a semantically annotated corpus of clinical texts.
Journal of biomedical informatics, 42(5):950?966.
Guergana Savova, Steven Bethard, Will Styler, James Mar-
tin, Martha Palmer, James Masanz, and Wayne Ward.
2009. Towards temporal relation discovery from the
clinical narrative. In AMIA Annual Symposium Pro-
ceedings, volume 2009, page 568. American Medical
Informatics Association.
Tessa Sullivan, Ann Irvine, and Stephanie W Haas. 2008.
It?s all relative: usage of relative temporal expressions
in triage notes. Proceedings of the American Society
for Information Science and Technology, 45(1):1?8.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013a.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013b.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association, 20(5):806?813.
Alexander Turchin, Maria Shubina, Eugene Breydo,
Merri L Pendergrass, and Jonathan S Einbinder. 2009.
Comparison of information content of structured and
narrative text data sources on the example of medica-
tion intensification. Journal of the American Medical
Informatics Association, 16(3):362?370.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
James Allen, Marc Verhagen, and James Pustejovsky.
2013. Semeval-2013 task 1: Tempeval-3: Evaluating
time expressions, events, and temporal relations. In Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval 2013), pages 1?9, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2):211?241.
Veronika Vincze, Gyrgy Szarvas, Richrd Farkas, Gyrgy
Mra, and Jnos Csirik. 2008. The bioscope corpus:
biomedical texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics, 9(Suppl 11):1?
9.
Ying Zhao, George Karypis, and Usama M. Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discovery,
10:141?168.
Jiaping Zheng, Wendy W Chapman, Rebecca S Crowley,
and Guergana K Savova. 2011. Coreference resolution:
A review of general methodologies and applications in
the clinical domain. Journal of biomedical informatics,
44(6):1113?1122.
Li Zhou, Simon Parsons, and George Hripcsak. 2008. The
evaluation of a temporal reasoning system in processing
clinical discharge summaries. Journal of the American
Medical Informatics Association, 15(1):99?106.
154
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 27?35,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
HHMM Parsing with Limited Parallelism
Tim Miller
Department of Computer Science
and Engineering
University of Minnesota, Twin Cities
tmill@cs.umn.edu
William Schuler
University of Minnesota, Twin Cities
and The Ohio State University
schuler@ling.ohio-state.edu
Abstract
Hierarchical Hidden Markov Model
(HHMM) parsers have been proposed as
psycholinguistic models due to their broad
coverage within human-like working
memory limits (Schuler et al, 2008) and
ability to model human reading time
behavior according to various complexity
metrics (Wu et al, 2010). But HHMMs
have been evaluated previously only with
very wide beams of several thousand
parallel hypotheses, weakening claims to
the model?s efficiency and psychological
relevance. This paper examines the effects
of varying beam width on parsing accu-
racy and speed in this model, showing that
parsing accuracy degrades gracefully as
beam width decreases dramatically (to 2%
of the width used to achieve previous top
results), without sacrificing gains over a
baseline CKY parser.
1 Introduction
Probabilistic parsers have been successful at ac-
curately estimating syntactic structure from free
text. Typically, these systems work by consider-
ing entire sentences (or utterances) at once, using
dynamic programming to obtain globally optimal
solutions from locally optimal sub-parses.
However, these methods usually do not attempt
to conform to human-like processing constraints,
e.g. leading to center embedding and garden path
effects (Chomsky and Miller, 1963; Bever, 1970).
For systems prioritizing accurate parsing perfor-
mance, there is little need to produce human-like
errors. But from a human modeling perspective,
the success of globally optimized whole-utterance
models raises the question of how humans can ac-
curately parse linguistic input without access to
this same global optimization. This question cre-
ates a niche in computational research for models
that are able to parse accurately while adhering as
closely as possible to human-like psycholinguistic
constraints.
Recent work on incremental parsers includes
work on Hierarchical Hidden Markov Model
(HHMM) parsers that operate in linear time by
maintaining a bounded store of incomplete con-
stituents (Schuler et al, 2008). Despite this seem-
ing limitation, corpus studies have shown that
through the use of grammar transforms, this parser
is able to cover nearly all sentences contained in
the Penn Treebank (Marcus et al, 1993) using a
small number of unconnected memory elements.
But this bounded-memory parsing comes at a
price. The HHMM parser obtains good coverage
within human-like memory bounds only by pur-
suing an ?optionally arc-eager? parsing strategy,
nondeterministically guessing which constituents
can be kept open for attachment (occupying an ac-
tive memory element), or closed for attachment
(freeing a memory element for subsequent con-
stituents). Although empirically determining the
number of parallel competing hypotheses used in
human sentence processing is difficult, previous
results in computational models have shown that
human-like behavior can be elicited at very low
levels of parallelism (Boston et al, 2008b; Brants
and Crocker, 2000), suggesting that large num-
bers of active hypotheses are not needed. Previ-
ously, the HHMM parser has only been evaluated
on large beam widths, leaving this aspect of its
psycholinguistic plausibility untested.
In this paper, the performance of an HHMM
parser will be evaluated in two experiments that
27
vary the amount of parallelism allowed during
parsing, measuring the degree to which this de-
grades the system?s accuracy. In addition, the
evaluation will compare the HHMM parser to an
off-the-shelf probabilistic CKY parser to evaluate
the actual run time performance at various beam
widths. This serves two purposes, evaluating one
aspect of the plausibility of this parsing frame-
work as a psycholinguistic model, and evaluating
its potential utility as a tool for operating on un-
segmented text or speech.
2 Related Work
There are several criteria a parser must meet
in order to be plausible as a psycholinguistic
model of the human sentence-processing mecha-
nism (HSPM).
Incremental operation is perhaps the most obvi-
ous. The HSPM is able to process sentences in-
crementally, meaning that at each point in time of
processing input, it has some hypothesis of the in-
terpretation of that input, and each subsequent unit
of input serves to update that hypothesis.
The next criterion for psycholinguistic plausi-
bility is processing efficiency. The HSPM not
only operates incrementally, but in standard op-
eration it does not lag behind a speaker, even if,
for example, the speaker continues speaking at ex-
tended length without pause. Standard machine
approaches, such as chart parsers based on the
CKY algorithm, operate in worst-case cubic run
time on the length of input. Without knowing
where an utterance or sentence might end, such an
algorithm will take more time with each succes-
sive word and will eventually fall behind.
The third criterion is a reasonable limiting of
memory resources. This constraint means that the
HSPM, while possibly considering multiple hy-
potheses in parallel, is not limitlessly so, as evi-
denced by the existence of garden path sentences
(Bever, 1970; Lewis, 2000). If this were not the
case, garden-path sentences would not cause prob-
lems, as reaching the disambiguating word would
simply result in a change in the favored hypothe-
sis. In fact, garden path sentences typically cannot
be understood on a first pass and must be reread,
indicating that the correct analysis is attainable
and yet not present in the set of parallel hypotheses
of the first pass.
While parsers meeting these three criteria can
claim to not violate any psycholinguistic con-
straints, there has been much recent work in
testing psycholinguistically-motivated parsers to
make forward predictions about human sentence
processing, in order to provide positive evidence
for certain probabilistic parsing models as valid
psycholinguistic models of sentence processing.
This work has largely focused on correlating mea-
sures of parsing difficulty in computational models
with delays in reading time in human subjects.
Hale (2001) introduced the surprisal metric for
probabilistic parsers, which measures the log ra-
tio of the total probability mass at word t ? 1
and word t. In other words, it measures how
much probability was lost in incorporating the
next word into the current hypotheses. Boston et
al. (2008a) show that surprisal is a significant pre-
dictor of reading time (as measured in self-paced
reading experiments) using a probabilistic depen-
dency parser. Roark et al (2009) dissected parsing
difficulty metrics (including surprisal and entropy)
to separate out the effects of syntactic and lexical
difficulties, and showed that these new metrics are
strong predictors of reading difficulty.
Wu et al (2010) evaluate the same Hierarchical
Hidden Markov Model parser used in this work in
terms of its ability to reproduce human-like results
for various complexity metrics, including some of
those mentioned above, and introduce a new met-
ric called embedding difference. This metric is
based on the idea of embedding depth, which is
the number of elements in the memory store re-
quired to hold a given hypothesis. Using more
memory elements corresponds to center embed-
ding in phrase structure trees, and presumably cor-
relates to some degree with complexity. Average
embedding for a time step is computed by com-
puting the weighted average number of required
memory elements (weighted by probability) for all
hypotheses on the beam. Embedding difference is
simply the change in this value when the next word
is encountered.
Outside of Wu et al, the most similar work
from a modeling perspective is an incremen-
tal parser implemented using Cascaded Hidden
Markov Models (CHMMs) (Crocker and Brants,
2000). This model is superficially similar to the
Hierarchical Hidden Markov Models described
below in that it relies on multiple levels of interde-
pendent HMMs to account for hierarchical struc-
ture in an incremental model. Crocker and Brants
use the system to parse ambiguous sentences (such
28
as the athlete realized his goals were out of reach)
and examine the relative probabilities of two plau-
sible analyses at each time step. They then show
that the shifting of these two probabilities is con-
sistent with empirical evidence about how humans
perceive these sentences word by word.
However, as will be described below, the
HHMM has advantages over the CHMM from
a psycholinguistic modeling perspective. The
HHMM uses a limited memory store contain-
ing only four elements which is consistent with
many estimates of human short term memory lim-
its (Cowan, 2001; Miller, 1956). In addition to
modeling memory limits, the limited store acts as
a fixed-depth stack that ensures linear asymptotic
parsing time, and a grammar transform allows for
wide coverage of speech and newspaper corpora
within that limited memory store (Schuler et al,
2010).
3 Hierarchical Hidden Markov Model
Parser
Hidden Markov Models (HMMs) have long been
used to successfully model sequence data in which
there is a latent (hidden) variable at each time step
that generates the observed evidence at that time
step. These models are used for such applications
as part-of-speech tagging, and speech recognition.
Hierarchical Hidden Markov Models (HH-
MMs) are an extension of HMMs which can rep-
resent sequential data containing hierarchical rela-
tions. In HHMMs, complex hidden variables may
output evidence for several time steps in sequence.
This process may recurse, though a finite depth
is required to make any guarantees about perfor-
mance. Murphy and Paskin (2001) showed that
this model could be framed as a Dynamic Bayes
Network (DBN), so that inference is linear on the
length of the input sequence.
In the HHMM parser used here, the complex
hidden variables are syntactic states that gener-
ate sub-sequences of other syntactic states, even-
tually generating pre-terminals and words. This
section will describe how the trees must be trans-
formed, and then mapped to HHMM states. This
section will then continue with a formal definition
of an HHMM, followed by a description of how
this model can parse natural language, and finally
a discussion of what different aspects of the model
represent in terms of psycholinguistic modeling.
3.1 Right-Corner Transform
In order to parse with an HHMM, phrase struc-
ture trees need to be mapped to a hierarchical se-
quence of states of nested HMMs. Since Mur-
phy and Paskin showed that the run time complex-
ity of the HHMM is exponential on the depth of
the nested HMMs, it is important to minimize the
depth of the model for optimal performance. In
order to do this, a tree transformation known as
a right-corner transform is applied to the phrase
structure trees comprising the training data, to
transform right-expanding sequences of complete
constituents into left-expanding sequences of in-
complete constituents A?/A?, consisting of an in-
stance of an active constituent A? lacking an in-
stance of an awaited constituent A? yet to be rec-
ognized. This transform can be defined as a syn-
chronous grammar that maps every context-free
rule expansion in a source tree (in Chomsky Nor-
mal Form) to a corresponding expansion in a right-
corner transformed tree:1
? Beginning case: the top of a right-expanding
sequence in an ordinary phrase structure tree
is mapped to the bottom of a left-expanding
sequence in a right-corner transformed tree:
A?
A??0
?
A??1
?
?
A?
A?/A??1
A??0
?
?
(1)
? Middle case: each subsequent branch in
a right-expanding sequence of an ordinary
phrase structure tree is mapped to a branch in
a left-expanding sequence of the transformed
tree:
A?
? A???
A????0
?
A????1
?
?
A?
A?/A????1
A?/A???
?
A????0
?
?
(2)
? Ending case: the bottom of a right-expanding
sequence in an ordinary phrase structure tree
1Here, ? and ? are tree node addresses, consisting of se-
quences of zeros, representing left branches, and ones, repre-
senting right branches, on a path from the root of the tree to
any given node.
29
a) A?
A?0
a ?
00 A?01
A?010
a ?
01
00
a ?
01
01
A?011
a ?
01
10
a ?
01
11
A?1
A?10
A?100
a ?
10
00
a ?
10
01
A?101
a ?
10
10
a ?
10
11
a ?
11
b) A?
A?/A?11
A?/A?1
A?0
A?0/A?0111
A?0/A?011
A?0/A?01
a ?
00
A?010
A?010/A?0101
a ?
01
00 a ?
01
01
a ?
01
10 a ?
01
11
A?10
A?10/A?1011
A?10/A?101
A?100
A?100/A?1001
a ?
10
00 a ?
10
01
a ?
10
10 a ?
10
11
a ?
11
Figure 1: Sample right-corner transform of
schematized tree before (a) and after (b) applica-
tion of transform.
is mapped to the top of a left-expanding se-
quence in a right-corner transformed tree:
A?
? A???
a???
?
A?
A?/A???
?
A???
a???
(3)
The application of this transform is exemplified in
Figure 1.
3.2 Hierarchical Hidden Markov Models
Right-corner transformed trees are mapped to ran-
dom variables in a Hierarchical Hidden Markov
Model (Murphy and Paskin, 2001).
A Hierarchical Hidden Markov Model
(HHMM) is essentially a factored version of
a Hidden Markov Model (HMM), configured to
recognize bounded recursive structures (i.e. trees).
Like HMMs, HHMMs use Viterbi decoding to
obtain sequences of hidden states s?1..T given
sequences of observations o1..T (words or audio
features), through independence assumptions
in a transition model ?A and an observation
model ?B (Baker, 1975; Jelinek et al, 1975):
s?1..T
def= argmax
s1..T
T
?
t=1
P?A(st | st?1) ? P?B(ot | st)
(4)
HHMMs then factor the hidden state transition?A
into a reduce and shift phase (Equation 5), then
into a bounded set of depth-specific operations
(Equation 6):
P?A(st|st?1) =
?
rt
P?R(rt|st?1)?P?S(st|rt st?1)
(5)
def=
?
r1..Dt
D
?
d=1
P?R,d(r
d
t | rd+1t sdt?1sd?1t?1 )?
P?S,d(s
d
t |rd+1t rdt sdt?1sd?1t )
(6)
which allow depth-specific variables to reduce
(through ?R-Rdn,d), transition (?S-Trn,d), and ex-
pand (?S-Exp,d) like tape symbols in a pushdown
automaton with a bounded memory store, depend-
ing on whether the variable below has reduced
(rdt ?RG) or not (rdt 6?RG):2
P?R,d(r
d
t | rd+1t sdt?1sd?1t?1 )
def=
{
if rd+1t 6?RG : Jrdt =r?K
if rd+1t ?RG : P?R-Rdn,d(rdt | rd+1t sdt?1 sd?1t?1 )
(7)
P?S,d(s
d
t | rd+1t rdt sdt?1sd?1t )
def=
?
?
?
if rd+1t 6?RG, rdt 6?RG : Jsdt =sdt?1K
if rd+1t ?RG, rdt 6?RG : P?S-Trn,d(sdt | rd+1t rdt sdt?1sd?1t )
if rd+1t ?RG, rdt ?RG : P?S-Exp,d(sdt | sd?1t )
(8)
where s0t = s? and rD+1t = r? for constants s?
(an incomplete root constituent), r? (a complete
lexical constituent) and r? (a null state resulting
from reduction failure) s.t. r??RG and r? 6?RG.
Right-corner transformed trees, as exemplified
in Figure 1(b), can then be aligned to HHMM
states as shown in Figure 2, and used to train an
HHMM as a parser.
Parsing with an HHMM simply involves pro-
cessing the input sequence, and estimating a most
likely hidden state sequence given this observed
input. Since the output is to be the best possible
parse, the Viterbi algorithm is used, which keeps
track of the highest probability state at each time
step, where the state is the store of incomplete syn-
tactic constituents being processed. State transi-
tions are computed using the models above, and
each state at each time step keeps a back pointer to
the state it most probably came from. Extracting
the highest probability parse requires extracting
2Here, J?K is an indicator function: J?K = 1 if ? is true, 0
otherwise.
30
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7
a
?0101
a
?0110
a
?0111
a
?1000
a
?1001
a
?1010
? ? ? ? ? ?
? ? ? ?
A
?100 /A
?1001
A
?10 /A
?101
A
?10 /A
?1011
?
A
?0 /A
?011
A
?0 /A
?0111
A
? /A
?1
A
? /A
?1
A
? /A
?1
A
? /A
?1
Figure 2: Mapping of schematized right-corner
tree into HHMM memory elements.
the most likely sequence, deterministically map-
ping that sequence back to a right-corner tree, and
reversing the right-corner transform to produce an
ordinary phrase structure tree.
Unfortunately exact inference is not tractable
with this model and dataset. The state space is
too large to manage for both space and time rea-
sons, and thus approximate inference is carried
out, through the use of a beam search. At each
time step, only the top N most probable hypoth-
esized states are maintained. Experiments de-
scribed in (Schuler, 2009) suggest that there does
not seem to be much lost in going from exact in-
ference using the CKY algorithm to a beam search
with a relatively large width. However, the op-
posite experiment, examining the effect of going
from a relatively wide beam to a very narrow beam
has not been thoroughly studied in this parsing ar-
chitecture.
4 Optionally Arc-eager Parsing
The right-corner transform described in Sec-
tion 3.1 saves memory because it transforms any
right-expanding sequence with left-child subtrees
into a left-expanding sequence of incomplete con-
stituents, with the same sequence of subtrees as
right children. The left-branching sequences of
siblings resulting from this transform can then be
composed bottom-up through time by replacing
each left child category with the category of the
resulting parent, within the same memory element
(or depth level). For example, in Figure 3(a) a
left-child category NP/NP at time t=4 is composed
with a noun new of category NP/NNP (a noun
phrase lacking a proper noun yet to come), result-
ing in a new parent category NP/NNP at time t=5
replacing the left child category NP/NP in the top-
most d=1 memory element.
This in-element composition preserves ele-
ments of the bounded memory store for use in pro-
cessing descendants of this composed constituent,
yielding the human-like memory demands re-
ported in (Schuler et al, 2008). But whenever
an in-element composition like this is hypothe-
sized, it isolates an intermediate constituent (in
this example, the noun phrase ?new york city?)
from subsequent composition. Allowing access
to this intermediate constituent ? for example,
to allow ?new york city? to become a modifier
of ?bonds?, which itself becomes an argument of
?for? ? requires an analysis in which the interme-
diate constituent is stored in a separate memory
element, shown in Figure 3(b). This creates a lo-
cal ambiguity in the parser (in this case, from time
step t=4) that may have to be propagated across
several words before it can be resolved (in this
case, at time step t=7). This is essentially an am-
biguity between arc-eager (in-element) and arc-
standard (cross-element) composition strategies,
as described by Abney and Johnson (1991). In
contrast, an ordinary (purely arc-standard) parser
with an unbounded stack would only hypothesize
analysis (b), avoiding this ambiguity.3
The right-corner HHMM approach described
in this paper relies on a learned statistical model
to predict when in-element (arc-eager) compo-
sitions will occur, in addition to hypothesizing
parse trees. The model encodes a mixed strategy:
with some probability arc-eager or arc-standard
for each possible expansion. Accuracy results on
a right-corner HHMM model trained on the Penn
Wall Street Journal Treebank suggest that this kind
of optionally arc-eager strategy can be reliably sta-
tistically learned.
By placing firm limits on the number of open
incomplete constituents in working memory, the
Hierarchical HMM parser maintains parallel hy-
potheses on the beam which predict whether each
constituent will host a subsequent attachment or
not. Empirical results described in the next section
3It is important to note that neither the right-corner nor
left-corner parsing strategy by itself creates this ambiguity.
The ambiguity arises from the decision to use this option-
ally arc-eager strategy to reduce memory store allocation in
a bounded memory parser. Implementations of left-corner
parsers such as that of Henderson (2004) adopt a arc-standard
strategy, essentially always choosing analysis (b) above, and
thus do not introduce this kind of local ambiguity. But in
adopting this strategy, such parsers must maintain a stack
memory of unbounded size, and thus are not attractive as
models of human parsing in short-term memory (Resnik,
1992).
31
a)
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7
strong
dem
and
for
new
york
city
? ? ? ? ? ?
? ? ? ? ? ?
?
NP/NN
NP/PP
NP/NP
NP/NNP
NP/NNP
NP(dem
.)
b)
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7
strong
dem
and
for
new
york
city
? ? ? ? ? ?
? ? ? ?
NNP/NNP
NNP/NNP
NP(city)
?
NP/NN
NP/PP
NP/NP
NP/NP
NP/NP
NP(dem
.)/NP(?)
Figure 3: Alternative analyses of ?strong demand for new york city ...?: a) using in-element composition,
compatible with ?strong demand for new york city is ...? (in which the demand is for the city); and b)
using cross-element (or delayed) composition, compatible with either ?strong demand for new york city
is ...? (in which the demand is for the city) or ?strong demand for new york city bonds is ...? (in which a
forthcoming referent ? in this case, bonds ? is associated with the city, and is in demand). In-element
composition (a) saves memory but closes off access to the noun phrase headed by ?city?, and so is not
incompatible with the ?...bonds? completion. Cross-element composition (b) requires more memory,
but allows access to the noun phrase headed by ?city?, so is compatible with either completion. This
ambiguity is introduced at t=4 and propagated until at least t=7. An ordinary, non-right-corner stack
machine would exclusively use analysis (b), avoiding ambiguity.
show that this added demand on parallelism does
not substantially degrade parsing accuracy, even at
very narrow beam widths.
5 Experimental Evaluation
The parsing model described in Section 3 has
previously been evaluated on the standard task
of parsing the Wall Street Journal section of the
Penn Treebank. This evaluation was optimized
for accuracy results, and reported a relatively wide
beam width of 2000 to achieve its best results.
However, most psycholinguistic models of the hu-
man sentence processing mechanism suggest that
if the HSPM does work in parallel, it does so with
a much lower number of concurrent hypotheses
(Boston et al, 2008b). Viewing the HHMM pars-
ing framework as a psycholinguistic model, a nec-
essary (though not sufficient) condition for it being
a valid model is that it be able to maintain rela-
tively accurate parsing capabilities even at much
lower beam widths.
Thus, the first experiments in this paper evalu-
ate the degradation of parsing accuracy depending
on beam width of the HHMM parser. Experiments
were conducted again on the WSJ Penn Treebank,
using sections 02-21 to train, and section 23 as the
test set. Punctuation was included in both train-
ing and testing. A set of varied beam widths were
considered, from a high of 2000 to a low of 15.
This range was meant to roughly correspond to
the range of parallelism used in other similar ex-
periments, using 2000 as a high end due to its us-
age in previous parsing experiments. However, it
should be noted that in fact the highest value of
2000 is already an approximate search ? prelim-
inary experiments showed that exhaustive search
with the HHMM would require more than 100000
elements per time step (exact values may be much
higher but could not be collected because they ex-
hausted system memory).
The HHMM parser was compared to a custom
built (though standard) probabilistic CKY parser
implementation trained on the CNF trees used as
input to the right-corner transform, so that the
CKY parser was able to compete on a fair foot-
ing. The accuracy results of these experiments are
shown in Figure 4.
These results show fairly graceful decline in
parsing accuracy with a beam width starting at
2000 elements down to about 50 beam elements.
This beam width is much less than 1% of the ex-
haustive search, though it is around 1% of what
might be considered the highest reasonable beam
width for efficient parsing. The lowest beam
widths attempted, 15, 20, and 25, result in ac-
curacy below that of the CKY parser. The low-
est beam width attempted, 15, shows the sharpest
decline in accuracy, putting the HHMM system
nearly 8 points below the CKY parser in terms of
accuracy.
This compares reasonably well to results by
32
 70
 72
 74
 76
 78
 80
 82
 84
 0  100  200  300  400  500
La
be
le
d 
F-
Sc
or
e
Beam Width
Figure 4: Plot of parsing accuracy (labeled F-
score) vs. beam widths for an HHMM parser
(curved line). Top line is HHMM accuracy with
beam width of 2000 (upper bound). The bottom
line is CKY parser results. Points correspond to
beam widths of 15, 20, 25, 50, 100, 250, and 500.
Brants and Crocker (2000) showing that an in-
cremental chart-parsing algorithm can parse accu-
rately with pruning down to 1% of normal memory
usage. While that parsing algorithm is difficult to
compare directly to this HHMM parser, the reduc-
tion in beam width in this system to 50 beam el-
ements from an already approximated 2000 beam
elements shows similar robustness to approxima-
tion. Accuracy comparisons should be taken with
a grain of salt due to additional annotations per-
formed to the Treebank before training, but the
HHMM parser with a beam width of 50 obtains
approximately the same accuracy as the Brants
and Crocker incremental CKY parser pruning to
3% of chart size. At 1% pruning, Brants and
Crocker achieved around 75% accuracy, which
falls between the HHMM parser at beam widths
of 20 and 25.
Results by Boston et al (2008b) are also dif-
ficult to compare directly due to a difference in
parsing algorithm and different research priority
(that paper was attempting to correlate parsing dif-
ficulty with reading difficulty). However, that pa-
per showed that a dependency parser using less
than ten beam elements (and as few as one) was
just as capable of predicting reading difficulty as
the parser using 100 beam elements.
A second experiment was conducted to eval-
uate the HHMM for its time efficiency in pars-
ing. This experiment is intended to address two
questions: Whether this framework is efficient
 0
 2
 4
 6
 8
 10
 12
 14
 10  20  30  40  50  60  70
Se
co
nd
s 
pe
r s
en
te
nc
e
Sentence Length
CKY
HHMM
Figure 5: Plot of parsing time vs. sentence length
for HHMM and CKY parsers.
enough to be considered a viable psycholinguis-
tic model, and whether its parsing time and accu-
racy remain competitive with more standard cu-
bic time parsing technologies at low beam widths.
To evaluate this aspect, the HHMM parser was
run at low beam widths on sentences of varying
lengths. The baseline was the widely-used Stan-
ford parser (Klein and Manning, 2003), run in
?vanilla PCFG? mode. This parser was used rather
than the custom-built CKY parser from the pre-
vious experiment, to avoid the possibility that its
implementation was not efficient enough to pro-
vide a realistic test. The HHMMparser was imple-
mented as described in the previous section. These
experiments were run on a machine with a single
2.40 GHz Celeron CPU, with 512 MB of RAM. In
both implementations the parser timing includes
only time spent actually parsing sentences, ignor-
ing the overhead incurred by reading in model files
or training.
Figure 5 shows a plot of parsing time versus
sentence length for the HHMM parser for a beam
width of 20. Sentences shorter than 10 words were
not included for visual clarity (both parsers are ex-
tremely fast at that length). At this beam width,
the performance of the HHMM parser (labeled F-
score) was 74.03%, compared to 71% for a plain
CKY parser. As expected, the HHMM parsing
time increases linearly with sentence length, while
the CKY parsing time increases super-linearly.
(However, due to high constants in the run time
complexity of the HHMM, it was not a priori clear
that the HHMM would be faster for any sentence
of reasonable length.)
33
The results of this experiment show that the
HHMM parser is indeed competitive with a proba-
bilistic CKY parser, in terms of parsing efficiency,
even while parsing with higher accuracy. At sen-
tences longer that 26 words (including punctua-
tion), the HHMM parser is faster than the CKY
parser. This advantage is clear for segmented text
such as the Wall Street Journal corpus. However,
this advantage is compounded when considering
unsegmented or ambiguously segmented text such
as transcribed speech or less formal written text, as
the HHMM parser can also make decisions about
where to put sentence breaks, and do so in linear
time.4
6 Conclusion and Future Work
This paper furthers the case for the HHMM as a
viable psycholinguistic model of the human pars-
ing mechanism by showing that performance de-
grades gracefully as parallelism decreases, provid-
ing reasonably accurate parsing even at very low
beam widths. In addition, this work shows that
an HHMM parser run at low beam widths is com-
petitive in speed with parsers that don?t work in-
crementally, because of its asymptotically linear
runtime.
This is especially surprising given that the
HHMM uses parallel hypotheses on the beam to
predict whether constituents will remain open for
attachment or not. Success at low beam widths
suggests that this optionally arc-eager prediction
is something that is indeed relatively predictable
during parsing, lending credence to claims of psy-
cholinguistic relevance of HHMM parsing.
Future work should explore further directions
in improving parsing performance at low beam
widths. The lowest beam value experiments
presented here generally parsed fairly accurately
when they completed, but were already encounter-
ing problems with unparseable sentences that neg-
atively affected parser accuracy. The large accu-
racy decrease between beam sizes of 20 and 15 is
likely to be mostly due to the lack of any correct
analysis on the beam when the sentence is com-
pleted.
It should be noted, however, that no adjustments
were made to the parser?s syntactic model with
these beam variations. This syntactic model was
optimized for accuracy at the standard beam width
4It does this probabilistically as a side effect of the pars-
ing, by choosing an analysis in which r0t ? RG (for any t).
of 2000, and thus contains some state splittings
that are beneficial at wide beam widths, but at
low beam widths are redundant and prevent oth-
erwise valid hypotheses from being maintained on
the beam. For applications in which speed is a
priority, future research can evaluate tradeoffs in
accuracy that occur at different beam widths with
a coarser-grained syntactic representation that al-
lows for more variation of hypotheses even on
very small beams.
Acknowledgments
This research was supported by National Science
Foundation CAREER/PECASE award 0447685.
The views expressed are not necessarily endorsed
by the sponsors.
References
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
James Baker. 1975. The Dragon system: an overivew.
IEEE Transactions on Acoustics, Speech and Signal
Processing, 23(1):24?29.
Thomas G. Bever. 1970. The cognitive basis for lin-
guistic structure. In J. ?R. Hayes, editor, Cognition
and the Development of Language, pages 279?362.
Wiley, New York.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
Umesh Patil, and Shravan Vasishth. 2008a. Parsing
costs as predictors of reading difficulty: An evalua-
tion using the Potsdam Sentence Corpus. Journal of
Eye Movement Research, 2(1):1?12.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
and Shravan Vasishth. 2008b. Surprising parser ac-
tions and reading difficulty. In Proceedings of ACL-
08: HLT, Short Papers, pages 5?8, Columbus, Ohio,
June. Association for Computational Linguistics.
Thorsten Brants and Matthew Crocker. 2000. Prob-
abilistic parsing and psychological plausibility. In
Proceedings of COLING ?00, pages 111?118.
Noam Chomsky and George A. Miller. 1963. Intro-
duction to the formal analysis of natural languages.
In Handbook of Mathematical Psychology, pages
269?321. Wiley.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Matthew Crocker and Thorsten Brants. 2000. Wide-
coverage probabilistic sentence processing. Journal
of Psycholinguistic Research, 29(6):647?669.
34
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
James Henderson. 2004. Lookahead in determinis-
tic left-corner parsing. In Proc. Workshop on Incre-
mental Parsing: Bringing Engineering and Cogni-
tion Together, pages 26?33, Barcelona, Spain.
Frederick Jelinek, Lalit R. Bahl, and Robert L. Mercer.
1975. Design of a linguistic statistical decoder for
the recognition of continuous speech. IEEE Trans-
actions on Information Theory, 21:250?256.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan.
Richard L. Lewis. 2000. Falsifying serial and paral-
lel parsing models: Empirical conundrums and an
overlooked paradigm. Journal of Psycholinguistic
Research, 29:241?248.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
George A. Miller. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81?97.
Kevin P. Murphy and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840, Vancouver, BC, Canada.
Philip Resnik. 1992. Left-corner parsing and psy-
chological plausibility. In Proceedings of COLING,
pages 191?197, Nantes, France.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
Proceedings of the 2009 Conference on Empirical
Methods in Natural Langauge Processing, pages
324?333.
William Schuler, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2008. Toward a
psycholinguistically-motivated model of language.
In Proceedings of COLING, pages 785?792,
Manchester, UK, August.
William Schuler, Samir AbdelRahman, TimMiller, and
Lane Schwartz. 2010. Broad-coverage incremen-
tal parsing using human-like memory constraints.
Computational Linguistics, 36(1).
William Schuler. 2009. Parsing with a bounded
stack using a model-based right-corner transform.
In Proceedings of the North American Association
for Computational Linguistics (NAACL ?09), pages
344?352, Boulder, Colorado.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an
incremental right-corner parser. In Proceedings of
the 49th Annual Conference of the Association for
Computational Linguistics.
35
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 73?81,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Active Learning for Coreference Resolution
Timothy A. Miller and Dmitriy Dligach and Guergana K. Savova
Children?s Hospital Boston
and Harvard Medical School
300 Longwood Ave.
Enders 141
Boston, MA 02115, USA
{Timothy.Miller,Dmitriy.Dligach,Guergana.Savova}@childrens.harvard.edu
Abstract
Active learning can lower the cost of anno-
tation for some natural language processing
tasks by using a classifier to select informa-
tive instances to send to human annotators. It
has worked well in cases where the training in-
stances are selected one at a time and require
minimal context for annotation. However,
coreference annotations often require some
context and the traditional active learning ap-
proach may not be feasible. In this work we
explore various active learning methods for
coreference resolution that fit more realisti-
cally into coreference annotation workflows.
1 Introduction
Coreference resolution is the task of deciding which
entity mentions in a text refer to the same entity.
Solving this problem is an important part of the
larger task of natural language understanding in gen-
eral. The clinical domain offers specific tasks where
it is easy to see that correctly resolving coreference
is important. For example, one important task in the
clinical domain is template filling for the Clinical El-
ements Model (CEM).1 This task involves extracting
various pieces of information about an entity and fit-
ting the information into a standard data structure
that can be reasoned about. An example CEM tem-
plate is that for Disease with attributes for Body Lo-
cation, Associated Sign or Symptom, Subject, Nega-
tion, Uncertainty, and Severity. Since a given entity
may have many different attributes and relations, it
1http://intermountainhealthcare.org/cem
may be mentioned multiple times in a text. Coref-
erence resolution is important for this task because
it must be known that all the attributes and relations
apply to the same entity so that a single CEM tem-
plate is filled in for an entity, rather than creating a
new template for each mention of the entity.
2 Background
2.1 Coreference Resolution
Space does not permit a thorough review of coref-
erence resolution, but recent publications covered
the history and current state of the art for both the
general domain and the clinical domain (Ng, 2010;
Pradhan et al, 2011; Zheng et al, 2011).
The system used here (Zheng et al, 2012) is
an end-to-end coreference resolution system, mean-
ing that the algorithm receives no gold standard in-
formation about mentions, named entity types, or
any linguistic information. The coreference res-
olution system is a module of the clinical Tex-
tual Analysis and Knowledge Extraction System
(cTAKES) (Savova et al, 2010) that is trained on
clinical data. It takes advantage of named entity
recognition (NER) and categorization to detect en-
tity mentions, and uses several cTAKES modules
as feature generators, including the NER module,
a constituency parser module, and a part of speech
tagging module.
The system architecture is based on the pairwise
discriminative classification approach to the coref-
erence resolution problem. In that paradigm, pairs
of mentions are classified as coreferent or not, and
then some reconciliation must be done on all of the
73
links so that there are no conflicts in the clusters.
The system uses support vector machines (SVMs)
as the pairwise classifiers, and conflicts are avoided
by only allowing an anaphor to link with one an-
tecedent, specifically that antecedent the classifier
links with the highest probability.
There are separate pairwise classifiers for named
entity and pronominal anaphor types. In the domain
of clinical narratives, person mentions and personal
pronouns in particular are not especially challeng-
ing ? the vast majority of person mentions are the
patient. In addition, pronoun mentions, while im-
portant, are relatively rare. Thus we are primarily
interested in named entity coreference classification,
and we use that classifier as the basis of the work de-
scribed here.
The feature set of this system is similar to that
used by Ng and Cardie (2002). That system in-
cludes features based on surface form of the men-
tions, shallow syntactic information, and lexical se-
mantics from WordNet. The system used here has
a similar feature set but uses Unified Medical Lan-
guage System (UMLS)2 semantic features as it is
intended for clinical text, and also incorporates sev-
eral syntactic features extracted from constituency
parses extracted from cTAKES.
To generate training data for active learning simu-
lations, mention detection is run first (cTAKES con-
tains a rule-based NER system) to find named en-
tities and a constituency parser situates entities in
a syntax tree). For each entity found, the system
works backwards through all other mentions within
a ten sentence window. For each candidate anaphor-
antecedent pair, a feature vector is extracted using
the features briefly described above.
2.2 Active Learning
Active Learning (AL) is a popular approach to se-
lecting unlabeled data for annotation (Settles, 2010)
that can potentially lead to drastic reductions in the
amount of annotation that is necessary for train-
ing an accurate statistical classifier. Unlike passive
learning, where the data is sampled for annotation
randomly, AL delegates data selection to the clas-
sifier. AL is an iterative process that operates by
first training a classifier on a small sample of the
2http://www.nlm.nih.gov/research/umls/
data known as the seed examples. The classifier
is subsequently applied to a pool of unlabeled data
with the purpose of selecting additional examples
the classifier views as informative. The selected data
is annotated and the cycle is repeated, allowing the
learner to quickly refine the decision boundary be-
tween classes. One common approach to assessing
the informativeness is uncertainty sampling (Lewis
and Gale, 1994; Schein and Ungar, 2007), in which
the learner requests a label for the instance it is most
uncertain how to label. In this work, we base our
instance selection on the distance to the SVM de-
cision boundary (Tong and Koller, 2002), assuming
that informative instances tend to concentrate near
the boundary.
Most AL work focuses on instance selection
where the unit of selection is one instance repre-
sented as a feature vector. In this paper we also
attempt document selection, where the unit of se-
lection is a document, typically containing multi-
ple coreference pairs each represented as a feature
vector. The most obvious way to extend a sin-
gle instance informativeness metric to the document
scenario is to aggregate the informativeness scores.
Several uncertainty metrics have been proposed that
follow that route to adapt single instance selection
to multiple instance scenarios (Settles et al, 2008;
Tomanek et al, 2009). We borrow some of these
metrics and propose several new ones.
To the best of our knowledge only one work
exists that explores AL for coreference resolution.
Gasperin (2009) experiments with an instance based
approach in which batches of anaphoric pairs are se-
lected on each iteration of AL. In these experiments,
AL did not outperform the passive learning baseline,
probably due to selecting batches of large size.
3 Active Learning Configurations
3.1 Instance Selection
The first active learning model we considered selects
individual training instances ? putatively coreferent
mention pairs. This method is quite easy to simu-
late, and follows naturally from most of the theo-
retical active learning literature, but it has the draw-
back of being seemingly unrealistic as an annotation
paradigm. That is, since coreference can span across
an entire document, it is probably not practical to
74
have a human expert annotate only a single instance
at a time when a given instance may require many
sentences of reading in order to contextualize the in-
stance and properly label it. Moreover, even if such
an annotation scheme proved viable, it may result
in an annotated corpus that is only valuable for one
type of coreference system architecture.
Nonetheless, active learning for coreference at the
instance level is still useful. First, since this method
most closely follows the successful active learning
literature by using the smallest discrete problems, it
can serve as a proof of concept for active learning
in the coreference task ? if it does not work well at
this level, it probably will not work at the document
level. Previous results (Gasperin, 2009) have shown
that certain multiple instance methods do not work
for coreference resolution, so testing on smaller se-
lection sizes first can ensure that active learning is
even viable at that scale. In addition, though in-
stance selection may not be feasible for real world
annotations, individual instances and metrics for se-
lecting them are usually used as building blocks for
more complex methods. In order for this to be pos-
sible it must be shown that the instances themselves
have some value.
3.2 Document Selection
Active learning with document selection is a much
more realistic representation of conventional anno-
tation methods. Conventionally, a set of documents
is selected, and each document is annotated exhaus-
tively for coreference (Pradhan et al, 2011; Savova
et al, 2011). Document selection fits into this work-
flow very naturally, by selecting the next document
to annotate exhaustively based on some metric of
which document has the best instances. In theory,
this method can save annotation time by only anno-
tating the most valuable documents.
Document selection is somewhat similar to the
concept of batch-mode active learning, wherein
multiple instances are selected at once, though
batch-mode learning is usually intended to solve a
different problem, that of an asymmetry between
classifier training speed and annotation speed (Set-
tles, 2010). A more important difference is that doc-
ument selection requires that all of the instances in
the batch must come from the same document. Thus,
one might expect a priori that document selection
for active learning will not perform as well as in-
stance selection. However, it is possible that even
smaller gains will be valuable for improving annota-
tion time, and the more robust nature of a corpus an-
notated in such a way will make the long term bene-
fits worthwhile.
In this work, we propose several metrics for se-
lecting documents to annotate, all of which are
based on instance level uncertainty. In the fol-
lowing descriptions, D is the set of documents, d
is a single document, d? is the selected document,
Instances(d) is a function which returns the set of
pair instances in document d, i is an instance, dist(i)
is a function which returns the distance of instance i
from the classification boundary, and I is the indica-
tor function, which takes the value 1 if its argument
is true and 0 otherwise. Note that high uncertainty
occurs when Abs(dist(i)) approaches 0.
? Best instance ? This method uses the un-
certainty sampling criteria on instances, and
selects the document containing the in-
stance the classifier is least certain about.
d? = argmin
d?D
[mini?Instances(d)Abs(dist(i))]
? Highest average uncertainty ? This method
computes the average uncertainty of all
instances in a document, and selects the
document with the highest average uncertainty.
d? = argmin
d?D
1
|Instances(d)|
?
i?Instances(d)Abs(dist(i))
? Least bad example ? This method uses
uncertainty sampling criteria to find the
document whose most certain example is
least certain, in other words the document
whose most useless example is least useless.
d? = argmin
d?D
maxi?Instances(d)Abs(dist(i))
? Narrow band ? This method creates an un-
certainty band around the discriminating
boundary and selects the document with
the most examples inside that narrow band.
d? = argmax
d?D
?
i?Instances(d) I(Abs(dist(i) < 0.2))
? Smallest spread ? This method computes the
distance between the least certain and most
certain instances and selects the document
minimizing that distance.
75
d? = argmin
d?D
[maxi?Instances(d)(Abs(dist(i)))?
mini?Instances(d)(Abs(dist(i)))]
? Most positives ? This method totals the
number of positive predicted instances
in each document and selects the doc-
ument with the most positive instances.
d? = argmax
d?D
?
i?Instances(d) I(dist(i) > 0)
? Positive ratio ? This method calculates
the percentage of positive predicted in-
stances in each document and selects the
document with the highest percentage.
d? = argmax
d?D
?
i?Instances(d) I(dist(i)>0)
|Instances(d)|
Many of these are straightforward adaptations of
the instance uncertainty criteria, but others deserve
a bit more explanation. The most positives and pos-
itive ratio metrics are based on the observation that
the corpus is somewhat imbalanced ? for every posi-
tive instance there are roughly 20 negative instances.
These metrics try to account for the possibility that
instance selection focuses on positive instances. The
average uncertainty is an obvious attempt to turn in-
stance metrics into document metrics, but narrow
band and smallest spread metrics attempt to do the
same thing while accounting for skew in the distri-
bution of ?good? and ?bad? instances.
3.3 Document-Inertial Instance Selection
One of the biggest impracticalities of instance se-
lection is that labeling any given instance may re-
quire reading a fair amount of the document, since
the antecedent and anaphor can be quite far apart.
Thus, any time savings accumulated by only anno-
tating an instance is reduced since the reading time
per instance is probably increased.
It is also possible that document selection goes
too far in the other direction, and requires too
many useless instances to be annotated to achieve
gains. Therefore, we propose a hybrid method of
document-inertial instance selection which attempts
to combine aspects of instance selection and docu-
ment selection.
This method uses instance selection criteria to se-
lect new instances, but will look inside the current
document for a new instance within an uncertainty
threshold rather than selecting the most uncertain in-
stance in the entire training set. Sticking with the
same document for several instances in a row can
potentially solve the real world annotation problem
that marking up each instance requires some knowl-
edge of the document context. Instead, the context
learned by selecting one instance can be retained if
useful for annotating the next selected instance from
the same document.
This also preserves one of the biggest advantages
of instance selection, that of re-training the model
after every selected instance. In batch-mode selec-
tion and document selection, many instances are se-
lected according to criteria based on the same model
starting point. As a result, the selected instances
may be redundant and document scores based on
accumulated instance scores may not reflect reality.
Re-training the model between selected instances
prevents redundant instances from being selected.
4 Evaluation
Evaluations of the active learning models described
above took place in a simulation context. In active
learning simulations, a labeled data set is used, and
the unlabeled pool is simulated by ignoring or ?cov-
ering? the labels for part of the data until the selec-
tion algorithm selects a new instance for annotation.
After selection the next data point is simply put into
the training data and its label is uncovered.
The data set used was the Ontology Development
and Information Extraction (ODIE) corpus (Savova
et al, 2011) used in the 2011 i2b2/VA Challenge on
coreference resolution.3 We used a set of 64 docu-
ments from the training set of the Mayo Clinic notes
for our simulations.
Instances were created by using the training
pipeline from the coreference system described in
Section 2.1. As previously mentioned, this work
uses the named entity anaphor classifier as it con-
tains the most data points. This training set resulted
in 6820 instances, with 311 positive instances and
6509 negative instances. Baseline ten-fold cross val-
idation performance on this data set using an SVM
with RBF kernel is an F-score of 0.48.
Simulations are performed using ten fold cross-
validation. First, each data point is assigned to one
3https://www.i2b2.org/NLP/Coreference/
76
of ten folds (this is done randomly to avoid any auto-
correlation issues). Then, for each iteration, one fold
is made the seed data, another fold is the validation
data, and the remainder are the unlabeled pool. Ini-
tially the labeled training data contains only the seed
data set. The model is trained on the labeled train-
ing data, tested on the validation set, then used to
select the next data point from the pool data set. The
selected data point is then removed from the pool
and added to the training data with its gold stan-
dard label(s), and the process repeats until the pool
of unlabeled data is empty. Performance is averaged
across folds to minimize the effects of randomness
in seed and validation set selection. Typically, active
learning is compared to a baseline of passive learn-
ing where the next data point to be labeled is selected
from the unlabeled pool data set randomly.
4.1 Instance Selection Experiments
Instance selection simulations follow the general
template above, with each instance (representing
a putative antecedent-anaphor pair) randomly as-
signed to a fold. After scoring on the validation set,
uncertainty sampling is used to select a single in-
stance from the unlabeled pool, and that instance is
added to the training set.
Figure 1 shows the results of active learning using
uncertainty selection on instances versus using pas-
sive learning (random selection). This makes it clear
that if the classifier is allowed to choose the data, top
performance can be achieved much faster than if the
data is presented in random order. Specifically, the
performance for uncertainty selection levels off at
around 500 instances into the active learning, out of
a pool set of around 5500 instances. In contrast, the
passive learning baseline takes basically the entire
dataset to reach the same performance.
This is essentially a proof of concept that there is
such a thing as a ?better? or ?worse? instance when
it comes to training a classifier for coreference. We
take this as a validation for attempting a document
selection experiment, with many metrics using in-
stance uncertainty as a building block.
4.2 Document Selection Experiments
Document selection follows similarly to the instance
selection above. The main difference is that instead
of assigning pair vectors to folds, we assign docu-
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Active vs. Passive Learning on Pairwise Named Entity Coreference
Number of instances
F?
sc
or
e
 
 
Random (Passive)
Uncertainty Sampling
Figure 1: Instance selection simulation results. The x-
axis is number of instances and the y-axis is ten-fold av-
eraged f-score of the pairwise named entity classifier.
ments to folds. To make a selection, each instance is
labeled according to the model, document level met-
rics described in Section 3.2 are computed per docu-
ment, and the document is selected which optimizes
the metric being evaluated. All of that document?s
instances and labels are added to the training data,
and the process repeats as before.
The results of these experiments are divided into
two plots for visual clarity. Figure 2 shows the
results of these experiments, roughly divided into
those that work as well as a random baseline (left)
and those that seem to work worse than a random
baseline (right). The best performing metrics (on
the left side of the figure) are Positive Ratio, Least
Worst,Highest Average, and Narrow Band, although
none of these performs noticeably better than ran-
dom. The remaining metrics (on the right) seem
to do worse than random, taking more instances to
reach the peak performance near the end.
The performance of document selection suggests
that it may not be a viable means of active learn-
ing. This may be due to a model of data distribution
in which useful instances are distributed very uni-
formly throughout the corpus. In this case, an aver-
age document will only have 8?10 useful instances
and many times as many that are not useful.
This was investigated by follow-up experiments
on the instance selection which kept track of which
77
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Least worst
Highest average
Pos/neg ratio
Narrow Band
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Best example
Most positives
Smallest spread
Figure 2: Two sets of document selection experiments.
document each instance came from. The experi-
ments tracked the first 500 instances only, which is
roughly the number of instances shown in Figure 1
to reach peak performance. Figure 3 (left) shows
a histogram with document indices on the x-axis
and normalized instance counts on the y-axis. The
counts are normalized by total number of document
vectors. In other words, we wanted to show whether
there was a distinction between ?good? documents
containing lots of good instances and ?bad? docu-
ments with few good instances.
The figure shows a few spikes, but most docu-
ments have approximately 10% of their instances
sampled, and all but one document has at least one
instance selected. Further investigation shows that
the spikes in the figure are from shorter documents.
Since shorter documents have few instances overall
but always at least one positive instance, they will be
biased to have a higher ratio of positive to negative
instances. If positive instances are more uncertain
(which may be the case due to the class imbalance),
then shorter documents will have more selected in-
stances per unit length.
We performed another follow-up experiment
along these lines using the histogram as a measure
of document value. In this experiment, we took the
normalized histogram, selected documents from it in
order of normalized number of items selected, and
used that as a document selection technique. Ob-
viously this would be ?cheating? if used as a metric
for document selection, but it can serve as a check on
the viability of document selection. If the results are
better than passive document selection, then there is
some hope that a document level metric based on the
uncertainty of its instances can be successful.
In fact, the right plot on Figure 3 shows that the
?cheating? method of document selection still does
not look any better than random document selection.
4.3 Document-Inertial Instance Selection
Experiments
The experiments for document-inertial instance se-
lection were patterned after the instance selection
paradigm. However, each instance was bundled with
metadata representing the document from which it
came. In the first selection, the algorithm selects the
most uncertain instance, and the document it comes
from is recorded. For subsequent selections, the
document which contained the previously selected
instance is given priority when looking for a new
instance. Specifically, each instance in that docu-
ment is classified, and the confidence is compared
against a threshold. If the document contains in-
stances meeting the threshold, the most uncertain in-
stance was selected. After each instance, the model
is retrained as in normal instance selection, and the
new model is used in the next iteration of the selec-
tion algorithm. For these experiments, the threshold
is set at 0.75, where the distance between the classi-
fication boundary and the margin is 1.0.
Figure 4 shows the performance of this algorithm
compared to passive and uncertainty sampling. Per-
78
0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Normalized document selection counts
Document index
%
 o
f v
ec
to
rs
 s
el
ec
te
d
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Cheating
Figure 3: Left: Percentage of instances selected from each document. Right: Performance of a document selection
algorithm that can ?cheat? and select the document with the highest proportion of good instances.
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Active vs. Passive Learning on Pairwise Named Entity Coreference
Number of instances
F?
sc
or
e
 
 
Random (Passive)
Uncertainty Sampling
Sticky Instance Sampling
Figure 4: Document-inertial instance selection results.
formance using this algorithm is clearly better than
passive learning and is similar to standard uncer-
tainty selection ignoring document constraints.
5 Discussion and Conclusion
The results of these experiments paint a complex
picture of the way active learning works for this do-
main and model combination. The first experiments
with uncertainty selection indicate that the number
of instances required to achieve classifier perfor-
mance can be compressed. Selecting and training
on all the good instances first leads to much faster
convergence to the asymptotic performance of the
classifier given the features and data set.
Attempting to extend this result to document se-
lection met with mediocre results. Even the best per-
forming of seven attempted algorithms seems to be
about the same as random document selection. One
can interpret these results in different ways.
The most pessimistic interpretation is that docu-
ment selection simply requires too many useless in-
stances to be annotated, good instances are spread
too evenly, and so document selection will never be
meaningfully faster than random selection. This in-
terpretation seems to be supported by experiments
showing that even if document selection uses a
?cheating? algorithm to select the documents with
the highest proportion of good instances it still does
not beat a passive baseline.
One can also interpret these results to inspire fur-
ther work, first by noting that all of the selection
techniques attempt to build on the instance selec-
tion metrics. While our document selection metrics
were more sophisticated than simply taking the n-
best instances, Settles (2010) notes that some suc-
cessful batch mode techniques explicitly account for
diversity in the selections, which we do not. In ad-
dition, one could argue that our experiments were
unduly constrained by the small number of docu-
ments available in the unlabeled pool, and that with
a larger unlabeled pool, one would eventually en-
counter documents with many good instances. This
may be true, but may be difficult in practice as clin-
ical notes often need to be manually de-identified
79
before any research use, and so it is not simply a
matter of querying all records in an entire electronic
medical record system.
The document-inertial instance selection showed
that the increase in training speed can be main-
tained without switching documents for every in-
stance. This suggests that while good training in-
stances may be uniformly distributed, it is usually
possible to find multiple good enough instances in
the current document, and they can be found despite
not selecting instances in the exact best order that
plain instance selection would suggest.
Future work is mainly concerned with real world
applicability. Document level active learning can
probably be ruled out as being non-beneficial despite
being the easiest to work into annotation work flows.
Instance level selection is very efficient in achieving
classifier performance but the least practical.
Document-inertial seems to provide some com-
promise. It does not completely solve the prob-
lems of instance selection, however, as annotation
will still not be complete if done exactly as simu-
lated here. In addition, the assumption of savings
is based on a model that each instance takes a con-
stant amount of time to annotate. This assumption is
probably true for tasks like word sense disambigua-
tion, where an annotator can be presented one in-
stance at a time with little context. However, a better
model of annotation for tasks like coreference is that
there is a constant amount of time required for read-
ing and understanding the context of a document,
then a constant amount of time on top of that per
instance.While modeling annotation time may pro-
vide some insight, it will probably be most effective
to undertake empirical annotation experiments to in-
vestigate whether document-inertial instance selec-
tion actually provides a valuable time savings.
The final discussion point is that of producing
complete document annotations. For coreference
systems following the pairwise discriminative ap-
proach as in that described in Section 2.1, a corpus
annotated instance by instance is useful. However,
many recent approaches do some form of document-
level clustering or explicit coreference chain build-
ing, and are not natively able to handle incompletely
annotated documents.4
4Other recent unsupervised graphical model approaches us-
Future work will investigate this issue by quan-
tifying the value of complete gold standard annota-
tions versus the partial annotations that may be pro-
duced using document-inertial instance selection.
One way of doing this is in simulation, by training
a model on the 500 good instances that document-
inertial instance selection selects, and then classify-
ing the rest of the training instances using that model
to create a ?diluted? gold standard. Then, a model
trained on the diluted gold standard will be used
to classify the validation set and performance com-
pared to the version trained on the full gold standard
corpus. Similar experiments can be performed using
other systems. The logic here is that if an instance
was not in the top 10% of difficult instances it can be
classified with high certainty. The fact that positive
instances are rare and tend to be most uncertain is a
point in favor of this approach ? after all, high accu-
racy can be obtained by guessing in favor of negative
once the positive instances are labeled. On the other
hand, if document-inertial instance selection simply
amounts to labeling of positive instances, it may not
result in substantial time savings.
In conclusion, this work has shown that instance
selection works for coreference resolution, intro-
duced several metrics for document selection, and
proposed a hybrid selection approach that preserves
the benefits of instance selection while offering the
potential of being applicable to real annotation. This
work can benefit the natural language processing
community by providing practical methods for in-
creasing the speed of coreference annotation.
Acknowledgments
The project described was supported by award
number NLM RC1LM010608, the Strategic Health
IT Advanced Research Projects (SHARP) Program
(90TR002) administered by the Office of the Na-
tional Coordinator for Health Information Technol-
ogy, and Integrating Informatics and Biology to the
Bedside (i2b2) NCBO U54LM008748. The content
is solely the responsibility of the authors and does
not necessarily represent the official views of the
NLM/NIH/ONC.
ing Gibbs sampling (Haghighi and Klein, 2007) may be able to
incorporate partially annotated documents in semi-supervised
training.
80
References
Caroline Gasperin. 2009. Active learning for anaphora
resolution. In Proceedings of the NAACL HLT Work-
shop on Active Learning for Natural Language Pro-
cessing, pages 1?8.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
848?855.
David D. Lewis andWilliam A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 3?12.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL).
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the 15th Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27.
Guergana K. Savova, James J. Masanz, Philip V. Ogren,
Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-
Schuler, and Christopher G. Chute. 2010. Mayo
clinical text analysis and knowledge extraction sys-
tem (cTAKES): architecture, component evaluation
and applications. J Am Med Inform Assoc, 17(5):507?
513.
Guergana K. Savova, Wendy W. Chapman, Jiaping
Zheng, and Rebecca S. Crowley. 2011. Anaphoric
relations in the clinical narrative: corpus creation. J
Am Med Inform Assoc, 18:459?465.
A.I. Schein and L.H. Ungar. 2007. Active learning for
logistic regression: an evaluation. Machine Learning,
68(3):235?265.
B. Settles, M. Craven, and S. Ray. 2008. Multiple-
instance active learning. Advances in Neural Informa-
tion Processing Systems (NIPS), 20:1289?1296.
Burr Settles. 2010. Active learning literature survey.
Technical report, University of Wisconsin?Madison.
Katrin Tomanek, Florian Laws, Udo Hahn, and Hinrich
Schu?tze. 2009. On proper unit selection in active
learning: co-selection effects for named entity recog-
nition. In HLT ?09: Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 9?17, Morristown, NJ, USA.
Association for Computational Linguistics.
S. Tong and D. Koller. 2002. Support vector machine
active learning with applications to text classification.
The Journal of Machine Learning Research, 2:45?66.
Jiaping Zheng, Wendy Webber Chapman, Rebecca S.
Crowley, and Guergana K. Savova. 2011. Coreference
resolution: A review of general methodologies and ap-
plications in the clinical domain. Journal of Biomedi-
cal Informatics, 44:1113?1122.
Jiaping Zheng, Wendy W Chapman, Timothy A Miller,
Chen Lin, Rebecca S Crowley, and Guergana K
Savova. 2012. A system for coreference resolution for
the clinical narrative. Journal of the American Medi-
cal Informatics Association.
81
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 18?26,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discovering Narrative Containers in Clinical Text
Timothy A. Miller1, Steven Bethard2, Dmitriy Dligach1,
Sameer Pradhan1, Chen Lin1, and Guergana K. Savova1
1 Children?s Hospital Informatics Program, Boston Children?s Hospital and Harvard Medical School
firstname.lastname@childrens.harvard.edu
2 Center for Computational Language and Education Research, University of Colorado Boulder
steven.bethard@colorado.edu
Abstract
The clinical narrative contains a great deal
of valuable information that is only under-
standable in a temporal context. Events,
time expressions, and temporal relations
convey information about the time course
of a patient?s clinical record that must be
understood for many applications of inter-
est. In this paper, we focus on extracting
information about how time expressions
and events are related by narrative con-
tainers. We use support vector machines
with composite kernels, which allows for
integrating standard feature kernels with
tree kernels for representing structured
features such as constituency trees. Our
experiments show that using tree kernels
in addition to standard feature kernels im-
proves F1 classification for this task.
1 Introduction
Clinical narratives are a rich source of unstruc-
tured information that hold great potential for im-
pacting clinical research and clinical care. These
narratives consist of unstructured natural language
descriptions of various stages of clinical care,
which makes them information dense but chal-
lenging to use computationally. Information ex-
tracted from these narratives is already being used
for clinical research tasks such as automatic phe-
notype classification for collecting disease cohorts
retrospectively (Ananthakrishnan et al, 2013),
which can in turn be used for a variety of studies,
including pharmacogenomics (Lin et al, 2012;
Wilke et al, 2011). Future applications may use
information extracted from the clinical narrative at
the point of care to assist physicians in decision-
making in a real time fashion.
One of the most interesting and challenging as-
pects of clinical text is the pervasiveness of tempo-
rally grounded information. This includes a num-
ber of clinical concepts which are events with fi-
nite time spans (e.g., surgery or x-ray), time ex-
pressions (December, postoperatively), and links
that relate events to times or other events. For ex-
ample, surgery last May relates the time last May
with the event surgery via the CONTAINS relation,
while Vicodin after surgery relates the medication
event Vicodin with the procedure event surgery via
the AFTER relation. There are many potential ap-
plications of clinical information extraction that
are only possible with an understanding of the or-
dering and duration of the events in a clinical en-
counter.
In this work we focus on extracting a particu-
lar temporal relation, CONTAINS, that holds be-
tween a time expression and an event expression.
This level of representation is based on the compu-
tational discourse model of narrative containers
(Pustejovsky and Stubbs, 2011), which are time
expressions or events which are central to a sec-
tion of a text, usually manifested by being rela-
tive hubs of temporal relation links. We argue that
containment relations are useful as an intermediate
level of granularity between full temporal relation
extraction and ?coarse? temporal bins (Raghavan
et al, 2012) like before admission, on admission,
and after admission. Correctly extracting CON-
TAINS relations will, for example, allow for more
accurate placement of events on a timeline, to
the resolution possible by the number of time ex-
pressions in the document. We suspect that this
finer grained information will also be more useful
for downstream applications like coreference, for
which coarse information was found to be useful.
The approach we develop is a supervised machine
18
learning approach in which pairs of time expres-
sions and events are classified as CONTAINS or
not. The specific approach is a support vector ma-
chine using both standard feature kernels and tree
kernels, a novel approach to this problem in this
domain that has shown promise on other relation
extraction tasks.
This work makes use of a new corpus we devel-
oped as part of the THYME1 project (Temporal
History of Your Medical Events) focusing on tem-
poral events and relations in clinical text. This cor-
pus consists of clinical and pathology notes on col-
orectal cancer from Mayo Clinic. Gold standard
annotations include Penn Treebank-style phrase
structure in addition to clinically relevant temporal
annotations like clinical events, temporal expres-
sions, and various temporal relations.
2 Background and Related Work
2.1 Annotation Methodology
The THYME annotation guidelines2 detail the ex-
tension of TimeML (Pustejovsky et al, 2003b)
to the annotations of events, temporal expres-
sions and temporal relations in the clinical do-
main. In summary, an EVENT is anything that is
relevant to the clinical timeline. Temporal expres-
sions (TIMEX3s) in the clinical domain are simi-
lar to those in the general domain with two excep-
tions. First, TimeML sets and frequencies occur
much more often in the clinical domain, especially
with regard to medications and treatments (Clar-
itin 30mg twice daily). The second deviation is a
new type of TIMEX3 ? PREPOSTEXP which covers
temporally complex terms like preoperative, post-
operative, and intraoperative.
EVENTs and TIMEX3s are ordered on a timeline
through temporal TLINKs which range from fairly
coarse (the relation to document time creation) to
fairly granular (the explicit pairwise TLINKs be-
tween EVENTs and/or TIMEX3s). Of note for this
work, the CONTAINS relation between a TIMEX3
and an EVENT means that the span of the EVENT
is completely within the span of the TIMEX3. The
interannotator agreement F1-score for CONTAINS
for the set of documents used here was 0.60.
2.2 Narrative Containers
One relatively new concept for marking temporal
relations is that of narrative containers, as in Puste-
1http://clear.colorado.edu/TemporalWiki
2Annotation guidelines are posted on the THYME wiki.
jovsky and Stubbs (2011). Narrative containers
are time spans which are central to the discourse
and often subsume multiple events and time ex-
pressions. They are often anchored by a time ex-
pression, though more abstract events may also act
as anchors. Using the narrative container frame-
work significantly reduces the number of explicit
TLINK annotations yet retains a relevant degree of
granularity enabling inferencing.
Consider the following clinical text example
with DocTime of February 8.
The patient recovered well after her ini-
tial first surgery on December 16th to
remove the adenocarcinoma, although
on the evening of January 3rd she was
admitted with a fever and treated with
antibiotics.
There are three narrative containers in this snip-
pet ? (1) the broad period leading up to the docu-
ment creation time which includes the events of re-
covered and adenocarcinoma, (2) December 16th,
which includes the events of surgery and remove,
and (3) January 3rd, which includes the events of
admitted, fever, and treated.
Using only the relation to the document creation
time would provide too coarse of a timeline result-
ing in collapsing the three narrative containers (the
coarse time bins of Raghavan et al (2012) would
collapse all events into the before admission cat-
egory). On the other hand, marking explicit links
between every pair of events and temporal expres-
sions would be tedious and redundant. In this ex-
ample, there is no need to explicitly mark that, for
instance, fever was AFTER surgery, because we
know that the fever happened on January 3rd and
that the surgery happened on December 16th, and
that January 3rd is AFTER December 16th. With
the grouping of EVENTs in this way, we can infer
the links between them and reduce annotator ef-
fort. Narrative containers strike the right balance
between parsimony and expressiveness.
2.3 Related Work
Of course, the possibility of annotating temporal
containment relations was allowed by even the ear-
liest versions of the TimeML specification using
TLINKs with the relation type INCLUDES. How-
ever, TimeML is a specification not a guideline,
and as such, the way in which temporal relations
have been annotated has varied widely and no
19
corpus has previously been annotated with narra-
tive containers in mind. In the TimeBank corpus
(Pustejovsky et al, 2003a), annotators annotated
only a sparse, mostly disconnected graph of the
temporal relations that seemed salient to them. In
TempEval 2007 and 2010 (Verhagen et al, 2007;
Verhagen et al, 2010), annotators annotated only
relations in specific constructions ? e.g. all pairs
of events and times in a sentence ? and used a re-
stricted set of relation types that excluded the IN-
CLUDES relation. TempEval 2013 (UzZaman et
al., 2013) allowed INCLUDES relations, but again
only in particular constructions or when the rela-
tion seemed salient to the annotators. The 2012
i2b2 Challenge3, which provided TimeML anno-
tations on clinical data, annotated the INCLUDES
relation, but merged it with other relations for the
evaluation due to low inter-annotator agreement.
Since no narrative container-annotated corpora
exist, there are also no existing models for extract-
ing narrative container relations. However, we
can draw on the various methods applied to re-
lated temporal relation tasks. Most relevant is the
work on linking events to timestamps. This was
one of the subtasks in TempEval 2007 and 2010,
and systems used a variety of features including
words, part-of-speech tags, and the syntactic path
between the event and the time (Bethard and Mar-
tin, 2007; Llorens et al, 2010). Syntactic path
features were also used in the 2012 i2b2 Chal-
lenge, where they provided gains especially for
intra-sentential temporal links (Xu et al, 2013).
Recent research has also looked to syntac-
tic tree kernels for temporal relation extraction.
Mirroshandel et al (2009) used a path-enclosed
tree (i.e., selecting only the sub-tree containing
the event and time), and used various weighting
scheme variants of this approach on the Time-
Bank (Pustejovsky et al, 2003a) and Opinion4
corpora. Hovy et al (2012) used a flat tree struc-
ture for each event-time pair, including only token-
based information (words, part of speech tags) be-
tween the event and time, and found that adding
such tree kernels on top of a baseline set of fea-
tures improved event-time linking performance on
the TempEval 2007 and Machine Reading cor-
pora (Strassel et al, 2010). While Mirroshandel et
al. saw improvements using a representation with
syntactic structure, Hovy et al used the flat tree
3http://i2b2.org/NLP/TemporalRelations
4Also known as the AQUAINT TimeML corpus ?
http://www.timeml.org
structure because they found that ?using a full-
parse syntactic tree as input representation did not
help performance.? Thus, it remains an open ques-
tion exactly where and when syntactic tree kernels
will help temporal relation extraction.
3 Methods
Inspired by this prior work, we treat the narrative
container extraction task as a within-sentence rela-
tion extraction task between time and event men-
tions. For each sentence, this approach iterates
over every gold standard annotated EVENT, pair-
ing it with each TIMEX3 in the sentence, and uses
a supervised machine learning algorithm to clas-
sify each pair as related by the CONTAINS relation
or not. Training examples are generated in the
same way, with pairs corresponding to annotated
links marked as positive examples and all others
marked as negative. We investigate a variety of
features for the classifier as well as a variety of
tree kernel combinations.
This straightforward approach does not address
all relation pairs, setting aside event-event rela-
tions and inter-sentential relations, which are both
likely to require different approaches.
3.1 SVM with Tree Kernels
The machine learning approach we use is support
vector machine (SVM) with standard feature ker-
nels, tree kernels, and composite kernels that com-
bine the two. SVMs are used extensively for clas-
sification tasks in natural language processing, due
to robust performance and widely available soft-
ware packages. We take advantage of the ability
in SVMs to represent structured features such as
trees using convolution kernels (Collins and Duffy,
2001), also known as tree kernels. This kernel
computes similarity between two tree structures
by computing the number of common sub-trees,
with a weight parameter to discount the influence
of larger structural similarities. The specific for-
malism we use is sometimes called a subset tree
kernel (Moschitti, 2006), which checks for simi-
larity on subtrees of all sizes, as long as each sub-
tree has its production rule completely expanded.
A useful property of kernels is that a linear com-
bination of two kernels is guaranteed to be a ker-
nel (Cristianini and Shawe-Taylor, 2000). In ad-
dition, the product of two kernels is also a ker-
nel. This means that it is simple to combine tradi-
tional feature-based kernels used in SVMs (linear,
20
polynomial, radial basis function) with tree ker-
nels representing structural information. This ap-
proach of using composite kernels has been widely
used in the task of relation extraction where syn-
tactic information is presumed to be useful, but is
hard to represent as traditional numeric features.
We investigate a few different composite ker-
nels here, including a linear combination:
KC(o1, o2) = ? ?KT (t1, t2) +KF (f1, f2) (1)
where a composite kernel KC operates on objects
oj composed of features fj and tree tj , by adding
a tree kernel KT weighted by ? to a feature kernel
KF . We also use a composite kernel that takes the
product of kernels:
KC(o1, o2) = KT (t1, t2) ?KF (f1, f2) (2)
Sometimes it is beneficial to make use of multi-
ple syntactic ?views? of the same instance. Below
we will describe many different tree representa-
tions, and the tree kernel framework allows them
to all be used simultaneously, by simply summing
the similarities of the different representations and
taking the combined sum as the tree kernel value:
KT ({t
1
1, t
2
1 . . . , t
N
1 }, {t
1
2, t
2
2, . . . , t
N
2 }) =
N?
i=1
KT (t
i
1, t
i
2) (3)
where i indexes the N different tree views. In all
kernel combinations we compute the normalized
version of both the feature and tree kernels so that
they can be combined on an even footing.
The actual implementations we use for train-
ing are the SVM-LIGHT-TK package (Mos-
chitti, 2006), which is a tree kernel extension to
SVMlight (Joachims, 1999). At test time, we
use the SVM-LIGHT-TK bindings of the ClearTK
toolkit (Ogren et al, 2009) in a module built on
top of Apache cTAKES (Savova et al, 2010), to
take advantage of the pre-processing stages.
3.2 Flat Features
The flat features developed for the standard fea-
ture kernel include the text of each argument as a
whole, the tokens of each argument represented as
a bag of words, the first and last word of each ar-
gument, and the preceding and following words of
each argument as bags of words. The token con-
text between arguments is also represented using
the text span as a whole, the first and last words,
the set of words represented as a bag of words, and
the distance between the arguments. In addition,
part of speech (POS) tag features are extracted for
each mention, with separate bag of POS tag fea-
tures for each argument. The POS features are
generated by the cTAKES POS tagger.
We also include semantic features of each argu-
ment. For event mentions, we include a feature
marking the contextual modality, which can take
on the possible values Actual, Hedged, Hypothet-
ical, or Generic, which is part of the gold stan-
dard annotations. This feature was included as it
was presumed that actual events are more likely
to have definite time spans, and thus be related
to times, than hypothetical or generic mentions of
events. For time mentions we include a feature for
the time class, with possible values of Date, Time,
Duration, Quantifier, Set, or Prepostexp. The time
class feature was used as it was hypothesized that
dates and times are more likely to contain events
than sets (e.g., once a month).
3.3 Tree Kernel Representations
We leverage existing tree kernel representations
for this work, using some directly and others as
starting point to a domain-specific representation.
First, we take advantage of the (relatively) flat
structured tree kernel representations of Hovy et
al. (2012). This representation uses lexical items
such as POS tags rather than constituent struc-
ture, but places them into an ordered tree struc-
ture, which allows tree kernels to use them as a
bag of items while also taking advantage of order-
ing structure when it is useful. Figure 1 shows an
example tree for an event-time pair for which a re-
lation exists, where the lexical information used is
POS tag information for each term (the represen-
tation that Hovy et al found most useful). We also
used a version of this representation where the sur-
face form is used instead of the POS tag.
While Hovy et al showed positive results using
this representation over just standard features, it is
still somewhat constrained in its ability to repre-
sent long distance relations. This is because the
subset tree kernel compares only complete rule
productions, and with long distance relations a flat
tree structure will have a production that is too big
to learn. Alternatively, tree kernel representations
can be based on constituent structure, as is com-
mon in the relation extraction literature. This will
21
BOP
Event-Actual
TOK
VBN
TOK
TO
TOK
VB
TOK
NN
Timex-Date
TOK
JJ
TOK
NN
BOW
Event-Actual
TOK
scheduled
TOK
to
TOK
undergo
TOK
surgery
Timex-Date
TOK
next
TOK
week
Figure 1: Two trees indicating the flat tree kernel
representation. Above is the bag of POS tags ver-
sion; below is the bag of words version.
hopefully allow for the representation of longer
distance relations by taking advantage of syntactic
sub-structure with smaller productions. The rep-
resentations used here are known as Feature Trees
(FT), Path Trees (PT) and Path-Enclosed Trees
(PET).
The Feature Tree representation takes the en-
tire syntax tree for the sentence containing both
arguments and inserts semantic information about
those arguments. That information includes the ar-
gument type (EVENT or TIMEX) as an additional
tree node above the constituent enclosing the argu-
ment. We also append semantic class information
to the argument (contextual modality for events,
time class for times), as in the flat features.
The Feature Tree representation is not com-
monly used, as it includes an entire sentence
around the arguments of interest, and that may in-
clude a great deal of unrelated structure that adds
noise to the classifier. Here we include it in an at-
tempt to get to the root of an apparent discrepancy
in the tree kernel literature, as explained in Sec-
tion 2, in which Hovy et al (2012) report a nega-
tive result and Mirroshandel et al (2009) report a
positive result for using constituency structure in
tree kernels for temporal relation extraction.
The Path Tree representation uses a sub-tree of
the whole constituent tree, but removes all nodes
that are not along the path between the two argu-
ments. Path information has been used in standard
feature kernels (Pradhan et al, 2008), with each
individual path being a possible boolean feature.
VP
Arg1-Event-Actual
arg1
S
VP
VP
Arg2-Timex-Date
arg2
Figure 2: Path Tree (PT) representation
Another representation making use of the path tree
takes contiguous subsections of the path tree, or
?path n-grams,? in an attempt to combat the spar-
sity of using the whole path (Zheng et al, 2012).
By using the path representation with a tree ker-
nel, the model should get the benefit of all different
sizes of path n-grams, up to the size of the whole
path. This representation is augmented by adding
in argument nodes with event and time features, as
in the Feature Tree. Unlike the Feature Tree and
the PET below, the Path Tree representation does
not include word nodes, because the important as-
pect of this representation is the labels of the nodes
on the path between arguments. Figure 2 shows an
example of what this representation looks like.
The Path-Enclosed Tree representation is based
on the smallest sub-tree that encloses the two pro-
posed arguments. This is a representation that has
shown value in other work using tree kernels for
relation extraction (Zhang et al, 2006; Mirroshan-
del et al, 2009). The information contained in
the PET representation is a superset of that con-
tained in the Path Tree representation, since it in-
cludes the full path between arguments as well
as the structure between arguments and the ar-
gument text. This means that it can take into
account path information while also considering
constituent structure between arguments that may
play a role in determining whether the two ar-
guments are related. For example, temporal cue
words like after or during may occur between ar-
guments and will not be captured by Path Trees.
Like the PT representation, the PET representa-
tion is augmented with the semantic information
specified above in the Feature Tree representation.
Figure 3 shows an example of this representation.
22
VP
Arg1-Event-Actual
VBN
scheduled
VP
TO
to
VP
VB
undergo
NP
NN
surgery
Arg2-Timex-Date
NP
JJ
next
NN
week
Figure 3: Path-Enclosed Tree representation
4 Evaluation
The corpus we used for evaluations was described
in Section 2. There are 78 total notes in the corpus,
with three notes for each of 26 patients. The data
is split into training (50%), development (25%),
and test (25%) sections based on patient number,
so that each patient?s notes are all in the same
section. The combined training and development
set used for final training consists of 4378 sen-
tences with 49,050 tokens, and 7372 events, 849
time expressions, and 2287 CONTAINS relations.
There were 774 positive instances of CONTAINS
in the training data, with 1513 negative instances.
For constituent structure and features we use the
gold standard treebank and event and time features
from our corpus. Preliminary work suggests that
automatic parses from cTAKES do not harm per-
formance very much, but the focus of this work is
on the relation extraction so we use gold standard
parses. All preliminary experiments were done us-
ing the development set for testing.
We designed a set of experiments to exam-
ine several hypotheses regarding extraction of the
CONTAINS relation and the efficacy of different
tree kernel representations. The first two config-
urations test simple rule-based baseline systems,
CLOSEST-P and CLOSEST-R, for distance-related
decision rule systems meant to optimize precision
and recall, respectively. CLOSEST-P hypothesizes
a CONTAINS link between every TIMEX3 and the
closest annotated EVENT, which will make few
links overall. CLOSEST-R hypothesizes a CON-
TAINS link between every EVENT and the closest
TIMEX3, which will make many more links.
The next configuration, Flat Features, uses the
token and part of speech features along with ar-
gument semantics features, as described in Sec-
tion 3. While this feature set may not seem ex-
haustive, in preliminary work many traditional re-
lation extraction features were tried and found to
not have much effect. This particular configura-
tion was tested because it is most comparable to
the bag of word and bag of POS kernels from
Hovy et al (2012), and should help show whether
the tree kernel is providing anything over an equiv-
alent set of basic features.
We then examine several composite kernels, all
using the same feature kernel, but using different
tree kernel-based representations. First, we use a
composite kernel which uses the bag of word and
bag of POS tree views, as in Hovy et al (2012).
Next, we add in two additional tree views to the
tree kernel, Path-Enclosed Tree and Path Tree,
which are intended to examine the effect of using
traditional syntax, and the long distance features
that they enable. The final experimental config-
uration replaces the PET and PT representations
from the last configuration with the Feature Tree
representation. This tests the hypothesis that the
difference between positive results for tree kernels
in this task (as in, say, Mirroshandel et al (2009))
and negative results reported by Hovy et al (2012)
is the difference between using a full-parse tree
and using standard sub-tree representations.
For the rule-based systems, there are no param-
eters to tune. Our machine-learning systems are
based on support vector machines (SVM), which
require tuning of several parameters, including
kernel type (linear, polynomial, and radial basis
function), the parameters for each kernel, and c,
the cost of misclassification. Tree kernels intro-
duce an additional parameter ? for weighting large
structures, and the use of a composite kernel in-
troduces parameters for which kernel combination
operator to use, and how to weight the different
kernels for the sum operator.
For each machine learning configuration, we
performed a large grid search over the combined
parameter space, where we trained on the train-
ing set and tested on the development set. For
the final experiments, the parameters were chosen
that optimized the F1 score on the development
set. Qualitatively, the parameter tuning strongly
favored configurations which combined the ker-
nels using the sum operator, and recall and pre-
cision were strongly correlated with the SVM pa-
rameter c. Using these parameters, we then trained
23
on the combined training and development sets
and tested on the official test set.
4.1 Evaluation Metrics
The state of evaluating temporal relations has been
evolving over the past decade. This is partially
due to the inferential properties of temporal rela-
tions, because it is possible to define the same set
of relations using different set of axioms. To take
a very simple example, given a gold set of rela-
tions A<B and B<C, and given the system output
A<B, A<C and B<C, if one were to compute a
plain precision/recall metric, then the axiom A<C
would be counted against the system, when one
can easily infer from the gold set of relations that
it is indeed correct. With more relations the infer-
ence process becomes more complex.
Recently there has been some work trying
to address the shortcomings of the plain F1
score (Muller and Tannier, 2004; Setzer et al,
2006; UzZaman and Allen, 2011; Tannier and
Muller, 2008; Tannier and Muller, 2011). How-
ever, the community has not yet come to a consen-
sus on the best evaluation approach. Two recent
evaluations, TempEval-3 (UzZaman et al, 2013)
and the 2012 i2b2 Challenge (Sun et al, 2013),
used an implementation of the proposal by (Uz-
Zaman and Allen, 2011). However, as described
in Cherry et al (2013), this algorithm, which uses
a greedy graph minimization approach, is sensi-
tive to the order in which the temporal relations
are presented to the scorer. In addition, the scorer
is not able to give credit for non-redundant, non-
minimum links (Cherry et al, 2013) as with the
the case of the relation A<C mentioned earlier.
Considering that the measures for evaluating
temporal relations are still evolving, we decided to
use plain F-score, with recall and precision scores
also reported. This score is computed across all
intra-sentential EVENT-TIMEX3 pairs in the gold
standard, where precision = # correct predictions# predictions ,
recall = # correct predictions# gold standard relations , and F1 score =
2?precision?recall
precision+recall .
4.2 Experimental Results
Results are shown in Table 1. Rule-based base-
lines perform reasonably well, but are heavily bi-
ased in terms of precision or recall. The ma-
chine learning baseline cannot even obtain the
same performance as the CLOSEST-R rule-based
system, though it is more balanced in terms of pre-
System Precision Recall F1
CLOSEST-P 0.754 0.537 0.627
CLOSEST-R 0.502 0.947 0.656
Flat Features (FF) 0.705 0.593 0.645
FF+Bag Trees (BT) 0.649 0.728 0.686
FF+BT+PET+PT 0.770 0.707 0.737
FF+BT+FT 0.691 0.691 0.691
Table 1: Table of results of main experiments.
cision and recall. Using a composite kernel which
adds in the flat token-based tree kernels improves
performance over the standard feature kernel by
4.1 points. Adding in the Path Tree and Path-
Enclosed Tree constituency-based trees along with
the flat trees improves F1 score to our best result
of 73.7. Finally, replacing PT and PET representa-
tions with the Feature Tree representation does not
offer any performance improvement over the Flat
Features + Bag Trees configuration.
4.3 Error Analysis
We performed error analysis on the outputs of the
best-performing system (FF+BT+PET+PT in Ta-
ble 1). First, we note that the parameter search
was optimized for F1. This resulted in the highest-
scoring configuration using a composite kernel
with the sum operator, polynomial kernel for the
secondary kernel, ? = 0.5, tree kernel weight (T )
of 0.1, and c = 10.0. This high value of c and low
value of T results in higher precision and lower
recall, but there were configurations with lower c
and higher T which made the opposite tradeoff,
with only marginally worse F1-score. For the pur-
poses of error analysis, however, this configuration
leads to a focus on false negatives.
First, the false positives contained many rela-
tions that were legitimately ambiguous or possible
annotator errors. An example ambiguous case is
She is currently being treated on the Surgical Ser-
vice for..., in which the system generates the re-
lation CONTAINS(currently, treated), but the gold
standard labels as OVERLAP. This example is am-
biguous because it is not clear from just the lin-
guistic context whether the treatment is wholly
contained in the small time window denoted by
currently, or whether it started a while ago or will
continue into the future. There are many similar
cases where the event is a disease/disorder type,
and the specific nature of the disease is impor-
tant to understanding whether this is a CONTAINS
24
or OVERLAP relation, specifically understanding
whether the disease is chronic or more acute.
Another source of false positives were where
the event and time were clearly related, but not
with CONTAINS. In the example reports that she
has been having intermittent bleeding since May
of 1998, the term since clearly indicates that this
is a BEGINS-ON relation between bleeding and
May of 1998. This is a case where having other
temporal relation classifiers may be useful, as they
can compete and the relation can be assigned to
whichever classifier is more confident.
False negatives frequently occurred in contexts
where the event and time were far apart. Syn-
tactic tree kernels were introduced to help im-
prove recall on longer-distance relations, and were
successful up to a limit. However, certain ex-
amples are so far apart that the algorithm may
have had difficulty sorting noise from important
structure. For example, the system did not find
the CONTAINS(October 27, 2010, oophorectomy)
relation in the sentence:
October 27, 2010, Dr. XXX performed
exploratory laparotomy with an trans-
verse colectomy and Dr. YYY performed
a total abdominal hysterectomy with a
bilateral salpingo-oophorectomy.
Here, while the date may be part of the same sen-
tence as the event, the syntactic relation between
the pair is not what makes the relation; the date is
acting as a kind of discourse marker that indicates
that the following events are contained. This sug-
gests that discourse-level features may be useful
even for the intra-sentential classification task.
Other false negatives occurred where there was
syntactic complexity, even on shorter examples.
The subset tree kernel used here matches com-
plete rule productions, and across complex struc-
ture with large productions, the chances of finding
similarity decreases substantially. Thus, events
within coordination or separated from the time by
clause breaks are more difficult to relate to the
time due to the multiple different ways of relating
these different syntactic elements.
Finally, there are some examples where the an-
chor of a narrative container is an event with mul-
tiple sub-events. In these cases, the system per-
forms well at relating a time expression to the an-
chor event, but may miss the sub-events that are
farther away. This is a case where having an event-
event TLINK classifier, then applying determinis-
tic closure rules, would allow a combined system
to link the sub-events to the time expression.
5 Discussion and Conclusion
In this paper we have developed a system for auto-
matically identifying CONTAINS relations in clini-
cal text. The experiments show first that a machine
learning approach that intelligently integrates con-
stituency information can greatly improve perfor-
mance over rule-based baselines. We also show
that the tree kernel approach, which can model se-
quence better than a bag of tokens-style approach,
is beneficial even when it uses the same features.
Finally, the experiments show that choosing the
correct representation is important for tree kernel
approaches, and specifically that using a full parse
tree may give inferior performance compared to
sub-trees focused on the structure of interest.
In general, there is much work to be done in the
area of representing temporal information in clin-
ical records. Many of the inputs to the algorithm
described in this paper need to be extracted auto-
matically, including time expressions and events.
Work on relations will focus on adding features
to represent discourse information and richer rep-
resentation of event semantics. Discourse infor-
mation may help with the longer-distance errors,
where the time expression acts almost as a topic
for an extended description of events. Better un-
derstanding of event semantics, such as whether
a disease is chronic or acute, or typical duration
for a treatment, may help constrain relations. In
addition, we will explore the effectiveness of us-
ing dependency tree structure, which has been use-
ful in the domain of extracting relations from the
biomedical literature (Tikk et al, 2013).
Acknowledgements
The work described was supported by Tempo-
ral History of Your Medical Events (THYME)
NLM R01LM010090 and Integrating Informat-
ics and Biology to the Bedside (i2b2) NCBO
U54LM008748. Thanks to the anonymous re-
viewers for thorough and insightful comments.
References
Naushad UzZaman, Hector Llorens, et al 2013. Semeval-
2013 task 1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Second Joint Confer-
ence on Lexical and Computational Semantics (*SEM),
25
Volume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Computa-
tional Linguistics.
Ashwin N Ananthakrishnan, Tianxi Cai, et al 2013. Improv-
ing case definition of Crohn?s disease and ulcerative col-
itis in electronic medical records using natural language
processing: a novel informatics approach. Inflammatory
bowel diseases.
Steven Bethard and James H. Martin. 2007. CU-TMP: Tem-
poral relation classification using syntactic and semantic
features. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007), pages
129?132.
Colin Cherry, Xiaodan Zhu, et al 2013. A la recherche du
temps perdu: extracting temporal relations from medical
text in the 2012 i2b2 NLP challenge. Journal of the Amer-
ican Medical Informatics Association, March.
Michael Collins and Nigel Duffy. 2001. Convolution kernels
for natural language. In Neural Information Processing
Systems.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Dirk Hovy, James Fan, et al 2012. When did that happen?:
linking events and relations to timestamps. In Proceedings
of the 13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 185?193.
Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector Learn-
ing. Universita?t Dortmund.
Chen Lin, Helena Canhao, et al 2012. Feature engineering
and selection for rheumatoid arthritis disease activity clas-
sification using electronic medical records. In Proceed-
ings of ICML Workshop on Machine Learning for Clinical
Data.
Hector Llorens, Estela Saquete, and Borja Navarro. 2010.
TIPSem (english and spanish): Evaluating crfs and seman-
tic roles in tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 284?291.
Association for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for clas-
sifying temporal relations between events. Proc. of the
PACLIC23, pages 355?364.
Alessandro Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In Ma-
chine Learning: ECML 2006, pages 318?329. Springer.
Philippe Muller and Xavier Tannier. 2004. Annotating and
measuring temporal relations in texts. In Proceedings of
the 20th international conference on Computational Lin-
guistics, COLING ?04, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Philip V. Ogren, Philipp G. Wetzler, and Steven J. Bethard.
2009. ClearTK: a framework for statistical natural lan-
guage processing. In Unstructured Information Manage-
ment Architecture Workshop at the Conference of the Ger-
man Society for Computational Linguistics and Language
Technology, 9.
Sameer S Pradhan, Wayne Ward, and James H Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics, 34(2):289?310.
James Pustejovsky and Amber Stubbs. 2011. Increasing in-
formativeness in temporal annotation. In Proceedings of
the 5th Linguistic Annotation Workshop, pages 152?160.
James Pustejovsky, Patrick Hanks, et al 2003a. The time-
bank corpus. In Corpus linguistics, volume 2003, page 40.
James Pustejovsky, Jose? Casta no, et al 2003b. Timeml:
Robust specification of event and temporal expressions in
text. In Fifth International Workshop on Computational
Semantics (IWCS-5).
Preethi Raghavan, Eric Fosler-Lussier, and Albert M Lai.
2012. Temporal classification of medical events. In Pro-
ceedings of the 2012 Workshop on Biomedical Natural
Language Processing, pages 29?37. Association for Com-
putational Linguistics.
Guergana K. Savova, James J. Masanz, et al 2010. Mayo
clinical text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and appli-
cations. J Am Med Inform Assoc, 17(5):507?513.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple. 2006.
The role of inference in the temporal annotation and anal-
ysis of text. Language Resources and Evaluation, 39(2-
3):243?265, February.
Stephanie Strassel, Dan Adams, et al 2010. The DARPA
machine reading program - encouraging linguistic and rea-
soning research with a series of reading tasks. In Proceed-
ings of the Seventh International Conference on Language
Resources and Evaluation (LREC?10), Valletta, Malta.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informatics
Association, April.
Xavier Tannier and Philippe Muller. 2008. Evaluation met-
rics for automatic temporal annotation of texts. Proceed-
ings of the Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08).
Xavier Tannier and Philippe Muller. 2011. Evaluating tem-
poral graphs built from texts via transitive reduction. J.
Artif. Int. Res., 40(1):375413, January.
Domonkos Tikk, Ille?s Solt, et al 2013. A detailed error anal-
ysis of 13 kernel methods for protein-protein interaction
extraction. BMC bioinformatics, 14(1):12.
Naushad UzZaman and James Allen. 2011. Temporal eval-
uation. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Lan-
guage Technologies, page 351?356.
Marc Verhagen, Robert Gaizauskas, et al 2007. Semeval-
2007 task 15: Tempeval temporal relation identification.
In Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 75?80.
Marc Verhagen, Roser Sauri, et al 2010. Semeval-2010 task
13: Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Uppsala,
Sweden, July. Association for Computational Linguistics.
RA Wilke, H Xu, et al 2011. The emerging role of electronic
medical records in pharmacogenomics. Clinical Pharma-
cology & Therapeutics, 89(3):379?386.
Yan Xu, Yining Wang, et al 2013. An end-to-end system to
identify temporal relation in discharge summaries: 2012
i2b2 challenge. Journal of the American Medical Infor-
matics Association : JAMIA.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring syn-
tactic features for relation extraction using a convolution
tree kernel. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of the
ACL, pages 288?295.
Jiaping Zheng, Wendy W Chapman, et al 2012. A sys-
tem for coreference resolution for the clinical narrative.
Journal of the American Medical Informatics Association,
19:660?667.
26
