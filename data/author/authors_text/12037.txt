Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 619?627,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
EEG responds to conceptual stimuli and corpus semantics
Brian Murphy
CIMeC, University of Trento
Rovereto 38068, Italy
brian.murphy@unitn.it
Marco Baroni
CIMeC, University of Trento
Rovereto 38068, Italy
marco.baroni@unitn.it
Massimo Poesio
CIMeC, University of Trento
Rovereto 38068, Italy
massimo.poesio@unitn.it
Abstract
Mitchell et al (2008) demonstrated that
corpus-extracted models of semantic
knowledge can predict neural activation
patterns recorded using fMRI. This
could be a very powerful technique for
evaluating conceptual models extracted
from corpora; however, fMRI is expensive
and imposes strong constraints on data
collection. Following on experiments
that demonstrated that EEG activation
patterns encode enough information to
discriminate broad conceptual categories,
we show that corpus-based semantic rep-
resentations can predict EEG activation
patterns with significant accuracy, and
we evaluate the relative performance of
different corpus-models on this task.
1 Introduction
Models of semantic relatedness induced from cor-
pus data have proven effective in a number of em-
pirical tasks (Sahlgren, 2006) and there is increas-
ing interest in whether distributional information
extracted from corpora correlates with aspects
of speakers? semantic knowledge: see Lund and
Burgess (1996), Landauer and Dumais (1997), Al-
muhareb (2006), Pad?o and Lapata (2007), Schulte
im Walde (2008), among many others. For this
purpose, corpus models have been tested on data-
sets that are based on semantic judgements (met-
alinguistic or meta-cognitive intuitions about syn-
onymy, semantic distance, category-membership)
or behavioural experiments (semantic priming,
property generation, free association). While all
these data are valuable, they are indirect reflec-
tions of semantic knowledge, and when the pre-
dictions they make diverge from those of corpora,
interpretation is problematic: is the corpus model
missing essential aspects of semantics, or are non-
semantic factors biasing the data elicited from in-
formants?
Reading semantic processes and representations
directly from the brain would be an ideal way to
get around these limitations. Until recently, anal-
ysis of linguistic quantities using neural data col-
lected with EEG (measurement at the scalp of volt-
ages induced by neuronal firing) or fMRI (mea-
surement of changes of oxygen concentrations in
the brain tied to cognitive processes) had neither
the advantages of corpora (scale) nor of infor-
mants (finer grained judgements).
However, some clear patterns of differential ac-
tivity have been found for broad semantic classes.
Viewing images of natural (typically animals and
plants) and non-natural (typically artefacts like
tools or vehicles) objects elicits different loci of
activity in fMRI (Martin and Chao, 2001) and
EEG (Kiefer, 2001), that persist across partici-
pants. Differences have also been found in re-
sponse to auditorily or visually presented words of
different lexical classes, such as abstract/concrete,
and verb/noun (Pulverm?uller, 2002). But interpre-
tation of such group results remains somewhat dif-
ficult, as they may be consistent with more than
one distinction: the natural/artefactual division
just mentioned, may rather be between living/non-
living entities, dynamic/static entities, or be based
on embodied experience (e.g. manipulable or not).
More recently, however, machine learning and
other numerical techniques have been successfully
applied to extract semantic information from neu-
ral data in a more discriminative fashion, down
to the level of individual concepts. The work
presented here builds on two strands of previ-
ous work: Murphy et al (2008) use EEG data
to perform semantic categorisation on single stim-
uli; and Mitchell et al (2008) introduce an fMRI-
based method that detects word level distinctions
by learning associations between features of neu-
ral activity and semantic features derived from a
619
corpus. We combine these innovations by intro-
ducing a method that extracts featural represen-
tations from the EEG signal, and uses corpus-
based models to predict word level distinctions in
patterns of EEG activity. The proposed method
achieves a performance level significantly above
chance (also when distinguishing between con-
cepts from the same semantic category, e.g., dog
and cat), and approaching that achieved with
fMRI.
The paper proceeds as follows. The next section
describes a simple behavioural experiment where
Italian-speaking participants had to name photo-
graphic images of mammals and tools while their
EEG activity was being recorded, and continues
to detail how the rich and multidimensional sig-
nals collected were reduced to a small set of op-
timally informative features using a new method.
Section 3 describes a series of corpus-based se-
mantic models derived from both a raw-text web
corpus, and from various parsings of a conven-
tional corpus. In Section 4 we describe the train-
ing of a series of linear models, that each learn
the associations between a set of corpus semantic
features and an individual EEG activity feature.
By combining these models it is possible to pre-
dict the EEG activity pattern for a single unseen
word, and compare this to the observed pattern
for the corresponding concept. Results (Section
5) show that these predictions succeed at a level
significantly above chance, both for coarser dis-
tinctions between words in different superordinate
categories (e.g., differentiating between drill and
gorilla), and, at least for the model based on the
larger web corpus, for those within the same cate-
gory (e.g., drill vs spanner, koala vs gorilla).
2 Neural Activation Data
2.1 Data collection
EEG data was gathered from native speakers of
Italian during a simple behavioural experiment at
the CIMeC/DiSCoF laboratories at Trento Univer-
sity. Seven participants (five male and two fe-
male; age range 25-33; all with college educa-
tion) performed a silent naming task. Each of them
was presented
1
on screen with a series of contrast-
normalised greyscale photographs of tools (gar-
den and work tools) and land mammals (exclud-
ing emotionally valent domesticated animals and
1
Using the E-Prime software package: http://www.
pstnet.com/e-prime/.
~1.5s0.5s 0.5s 2s
Figure 1: Presentation of image stimuli
predators), for which they had to think of the most
appropriate name (see figure 1). They were not ex-
plicitly asked to group the entities into superordi-
nate categories, or to concentrate on their seman-
tic properties, but completing the task involved re-
solving each picture to its corresponding concept.
Images remained on screen until a keyboard re-
sponse was received from the participant to indi-
cate a suitable label had been found, and presenta-
tions were interleaved with three second rest peri-
ods. Thirty stimuli in each of the two classes were
each presented six times, in random order, to give
a total of 360 image presentations in the session.
Response rates were over 95%, and a post-session
questionnaire determined that participants agreed
on image labels in approximately 90% of cases.
English terms for the concepts used are listed be-
low.
Mammals anteater, armadillo, badger, beaver, bi-
son, boar, camel, chamois, chimpanzee, deer,
elephant, fox, giraffe, gorilla, hare, hedge-
hog, hippopotamus, ibex, kangaroo, koala,
llama, mole, monkey, mouse, otter, panda,
rhinoceros, skunk, squirrel, zebra
Tools Allen key, axe, chainsaw, craft knife, crow-
bar, file, garden fork, garden trowel, hack-
saw, hammer, mallet, nail, paint brush, paint
roller, pen knife, pick axe, plaster trowel,
pliers, plunger, pneumatic drill, power drill,
rake, saw, scissors, scraper, screw, screw-
driver, sickle, spanner, tape measure
The EEG signals were recorded at 500Hz from
64 scalp locations based on the 10-20 standard
620
montage.
2
The EEG recording computer and stim-
ulus presentation computer were synchronised by
means of parallel port transmitted triggers. Af-
ter the experiment, pre-processing of the recorded
signals was carried out using the EEGLAB pack-
age (Delorme and Makeig, 2003): signals were
band-pass filtered at 1-50Hz to remove slow drifts
and high-frequency noise, and then down-sampled
to 120Hz. An ICA decomposition was subse-
quently applied (Makeig et al, 1996), and signal
components due to eye-movements were manually
identified and removed.
As a preliminary test to verify that the recorded
signals included category specific patterns, we
applied a discriminative classification technique
based on source-separation, similar to that de-
scribed in Murphy et al (2008). This found that
the categories of mammals and tools could be dis-
tinguished with an accuracy ranging from 57% to
80% (mean of 72% over the seven participants).
2.2 Feature extraction
The features extracted are metrics of signal power
at a particular scalp location, in a particular fre-
quency band, and at a particular time latency rel-
ative to the presentation of each image stimulus.
Termed Event Related Synchronisation (ERS) or
Event Related Spectral Perturbation (ERSP), such
frequency-specific changes in signal amplitude are
known to correlate with a wide range of cogni-
tive functions (Pfurtscheller and Lopes da Silva,
1999), and have specifically been shown to be sen-
sitive to category distinctions during the process-
ing of linguistic and visual stimuli (Murphy et al,
2008; Gilbert et al, 2009).
Feature extraction and selection is performed
individually on a per-participant basis. As a first
step all signal channels are z-score normalised
to control for varying conductivity at each elec-
trode site, and a Laplacian sharpening is applied
to counteract the spatial blurring of signals caused
by the skull, and so minimise redundancy of infor-
mation between channels.
For each stimulus presentation, 14,400 signal
power features are extracted: 64 electrode chan-
nels by 15 frequency bands (of width 3.3Hz, be-
tween 1 and 50Hz) by 15 time intervals (of length
67ms, in the first second after image presentation).
A z-score normalisation is carried out across all
2
Using a Brain Vision BrainAmp system: http://
www.brainproducts.com/.
Figure 2: Mean rank of selected features in the
time/frequency space (left panel) and on the scalp
(right panel) for participant E
stimulus presentations to equalise variance across
frequencies and times: to control both for the low-
pass filtering action of the skull, and for the re-
duced synchronicity of activity at increasing laten-
cies. For each stimulus a mean is then taken over
each of six presentations to arrive at a more reli-
able power estimate for each feature.
3
The feature ranking method used in Mitchell et
al. (2008) evaluates the extent to which the rela-
tionship among stimuli is stable across across pre-
sentations, using a correlational measure,
4
but pre-
liminary analyses with this selection method on
EEG features proved disappointing. Here, two ad-
ditional ranking criteria are used: each feature is
evaluated for its noisiness (the amount of power
variation seen across presentations of the same
stimulus), and for its distinctiveness (the amount
of variation in power estimates across different
stimuli). A combination of these three strategies
is used to rank the features by their informative-
ness, and the top 50 features are then selected for
each participant.
5
A qualitative evaluation of the feature selec-
tion strategy can be carried out by examining
the distribution of features selected. Figure 2
shows the distribution of selected features over the
time/frequency spectrum (left panel), and over the
scalp (right panel - viewed from above, with the
nose pointing upwards). The distribution seen is
3
Stimulus power features are isolated by band-pass filter-
ing for the required frequencies, cropping following the rel-
evant time interval relative to each image presentation, and
then taking the variance of the resulting signal, which is pro-
portional to power.
4
See the associated supplementary materials of Mitchell
et al (2008) for details: http://www.sciencemag.
org/cgi/content/full/320/5880/1191/DC1.
5
Several combinations of these parameters (selection
thresholds of 5, 20, 50, 100, 200 features; ranking criteria in
isolation and in combination) were investigated - the one cho-
sen gave highest overall performance with the web-derived
corpus model: 50 features, combined ranking criteria.
621
Figure 3: First two components of principal com-
ponents analysis of selected features for partici-
pant E (crosses: mammals; circles: tools)
plausible in reference to previous work: lower fre-
quencies (Pfurtscheller and Lopes da Silva, 1999),
latencies principally in the first few hundred mil-
liseconds (Kiefer, 2001), and activity in the visual
centres at the rear of the head, as well as parietal
areas (Pulverm?uller, 2005). A principal compo-
nents analysis can also be performed on the se-
lected features to see if they reflect any plausi-
ble semantic space. As figure 3 shows, the fea-
ture selection stage has captured quite faithfully
the mammal/tool distinction in a totally unsuper-
vised fashion.
3 Corpus-based semantic models
Data from linguistics (Pustejovsky, 1995; Fill-
more, 1982) and neuroscience (Barsalou, 1999;
Barsalou, 2003; Pulverm?uller, 2005) underline
how certain verbs, by emphasising typical ways in
which we interact with entities and how they be-
have, are pivotal in the representation of concrete
nominal concepts. Following these traditions,
Mitchell et al (2008) use 25 manually picked
verbs as their corpus-based features.
Here that approach is replicated by translating
these verbs into Italian. Mitchell et al (2008) se-
lected verbs that denote our interaction with ob-
jects and living things, such as smell and ride.
While the translations are not completely faithful
(because frequent verbs of this sort tend to span
different sets of senses in the two languages), the
aim was to respect the same principle when build-
ing the Italian list. The full list, with our back
translations into English, is presented in Table 1.
We refer to this set as the ?Mitchell? verbs.
alzare ?raise? annusare ?smell/sniff?
aprire ?open? ascoltare ?listen?
assaggiare ?taste? avvicinare ?near?
cavalcare ?ride? correre ?run/flow?
dire ?say/tell? entrare ?enter?
guidare ?drive? indossare ?wear?
maneggiare ?handle? mangiare ?eat?
muovere ?move? pulire ?clean?
puzzare ?stink? riempire ?fill?
rompere ?break? sentire ?feel/hear?
sfregare ?rub? spingere ?push?
temere ?fear? toccare ?touch?
vedere ?see?
Table 1: The ?Mitchell? verbs, with English trans-
lations
As in Mitchell et al (2008), in order to find
a corpus large enough to provide reliable co-
occurrence statistics for our target concepts and
the 25 verbs, we resorted to the Web, queried us-
ing the Yahoo! API.
6
In particular, we represent
each concept by a vector that records how many
times it co-occurred with each target verb within
a span of 5 words left and right, according to Ya-
hoo! counts. We refer to this corpus-based model
as the yahoo-mitchell model below.
While manual verb picking has proved effec-
tive for Mitchell and colleagues (and for us, as we
will see in a moment), ultimately what we are in-
terested in is discovering the most distinctive fea-
tures of each conceptual category. We are there-
fore interested in more systematic approaches to
inducing corpus-based concept descriptions, and
in which of these approaches works best for this
task. The alternative models we consider were
not extracted from the Web, but from an existing
corpus, so that we could rely on pre-existing lin-
guistic annotation (POS tagging, lemmatization,
dependency paths), and perform more flexible,
annotation-aware queries to collect co-occurrence
statistics.
More specifically, we used the la Repub-
blica/SSLMIT corpus
7
, that contains about 400
million tokens of newspaper text. From this, we
extracted four models where nominal concepts are
represented in terms of patterns of co-occurrence
with verbs (we collected statistics for the top
20,000 most common nouns in the corpus, includ-
ing the concepts used as stimuli in the silent nam-
6
http://developer.yahoo.com/search/
7
http:://sslmit.unibo.it/repubblica/
622
ing experiment, and the top 5,000 verbs). We first
re-implemented a ?classic? window-based word
space model (Sahlgren, 2006), referred to below
as repubblica-window, where each noun lemma is
represented by its co-occurrence with verb lem-
mas within the maximum span of a sentence, with
no more than one other intervening noun. The
repubblica-position model is similar, but it also
records the position of the verb with respect to
the noun (so that X-usare ?X-use? and usare-X
?use-X? count as different features), analogously
to the seminal HAL model (Lund and Burgess,
1996). It has been shown that models that take
the syntactic relation between a target word and
a collocate feature into account can outperform
?flat? models in some tasks (Pad?o and Lapata,
2007). The next two models are based on the de-
pendency parse of the la Repubblica corpus docu-
mented by Lenci (2009). We only counted as col-
locates those verbs that were linked to nouns by
a direct path (such as subject and object) or via
preposition-mediated paths (e.g., tagliare con for-
bici ?to cut with scissors?), and where the paths
were among the top 30 most frequent in the cor-
pus. In the repubblica-depfilter model, we record
co-occurrence with verbs that are linked to the
nouns by one of the top 30 paths, but we do
not preserve the paths themselves in the features.
This is analogous to the model proposed by Pad?o
and Lapata (2007). In the repubblica-deppath
model, we preserve the paths as part of the fea-
tures (so that subj-uccidere ?subj-kill? and obj-
uccidere count as different features), analogously
to Lin (1998), Curran and Moens (2002) and oth-
ers. For all models, following standard practice in
computational linguistics (Evert, 2005), we trans-
form raw co-occurrence counts into log-likelihood
ratios.
Following the evaluation paradigm of Mitchell
et al (2008), linear models trained on corpus-
based features are used to predict the pattern of
EEG activity for unseen concepts. This only
works if we have a very limited number of fea-
tures (or else we would have more parameters to
estimate than data-points to estimate them). The
Repubblica-based models have thousands of fea-
tures (one per verb collocate, or verb+path collo-
cate). We adopt two strategies to select a reduced
number of features. In the topfeat versions, we
first pick the 50 features that have the highest asso-
ciation with each of the target concepts. We then
count in how many of these concept-specific top
lists a feature occurs, and we pick the 25 features
that occur in the largest number of them. The intu-
ition is that this should give us a good trade-off be-
tween how characteristic the features are (we only
use features that are highly associated with some
of our concepts), and their generalization capabili-
ties (we pick features that are associated with mul-
tiple concepts). Randomly selected examples of
the features extracted in this way for the various
Repubblica models are reported in Table 2.
repubblica-window repubblica-position
abbattere ?demolish? X-ferire ?X-wound?
afferrare ?seize? X-usare ?X-use?
impugnare ?grasp? dipingere-X ?paint-X?
tagliare ?cut? munire-X ?supply-X?
trovare ?find? tagliare-X ?cut-X?
repubblica-depfilter repubblica-deppath
abbattere ?demolish? con+tagliare ?with+cut?
correre ?run? obj+abbattere ?obj+demolish?
parlare ?speak? obj+uccidere ?obj+kill?
saltare ?jump? intr-subj+vivere ?intr-subj+live?
tagliare ?cut? tr-subj+aprire ?tr-subj+open?
Table 2: Examples of top features from the la Re-
pubblica models
Alternatively, instead of feature selection we
perform feature reduction by means of a Singular
Value Decomposition (SVD) of the noun-by-verb
matrix. We apply the SVD to matrices that include
the top 20,000 most frequent nouns in the cor-
pus (including our target concepts) since the qual-
ity of the resulting reduced model should improve
if we can exploit richer patterns of correlations
among the columns ? verbs ? across rows ? nouns
(Landauer and Dumais, 1997; Sch?utze, 1997). In
the svd versions of our models, we pick as fea-
tures the top 25 left singular vectors, weighted
by the corresponding singular values. These fea-
tures do not have a straightforward interpretation,
but they tend to group verb meanings that belong
to broad semantic domains. For example, among
the original verbs that are most strongly correlated
with one of the top singular vectors of repubblica-
window we find giocare ?play?, vincere ?win? and
perdere ?lose?. Another singular vector is asso-
ciated with ammontare ?amount?, costare ?cost?,
pagare ?pay?, etc. One of the top singular vec-
tors of repubblica-deppath is strongly correlated
with in+scendere ?descend into?, in+mettere ?put
into?, in+entrare ?enter into?, though not all sin-
gular vectors are so clearly characterized by the
verbs they correlate with.
623
None of the la Repubblica models had full cov-
erage of our concept stimulus set (see the second
column of Table 3 below), because our extraction
method missed some multi-word units, and fea-
ture selection led to losing some more items due
to data sparseness (e.g., some target words had no
collocates connected by the dependency paths we
selected). The experiments reported in the next
section used all the target concepts available in
each model, but a replication using the 50 concepts
that were common to all models obtained results
that are comparable. For a direct comparison be-
tween Yahoo! and la Repubblica derived features,
we tried collecting statistics for the Mitchell verbs
from Repubblica as well, but the resulting model
was extremely sparse, and we do not report its per-
formance here.
Finally, it is important to note that any repre-
sentation yielded by a corpus semantic model does
not characterise a concept directly, but is rather an
aggregate of the various senses and usages of the
noun chosen to represent it. This obvious limita-
tion will persist until comprehensive, robust and
computationally efficient word-sense disambigua-
tion techniques become available. However these
models are designed to extract semantic (as op-
posed to syntactic or phonological) properties of
words, and as noted in the introduction, have been
demonstrated to correlate with behavioural effects
of conceptual processing.
4 Predicting EEG patterns using
corpus-based models
In Section 2.2 above we showed how we extracted
features summarizing the spatial, temporal and
frequency distribution of the EEG signal collected
while participants were processing each of the tar-
get concepts. In Section 3, we described various
ways to obtain a compact representation of the
same concepts in terms of corpus-derived features.
We will now discuss the method we employed to
verify whether the corpus-derived features can be
used to predict the EEG patterns ? that is whether
semantics can be used to predict neural activity.
Our hope is that a good corpus-based model will
provide a decomposition of concepts into mean-
ingful properties, corresponding to coherent sub-
patterns of activation in the brain, and thus capture
generalizations across concepts. For example, if
a concept is particularly visually evocative (e.g.,
zebra), we might expect it to be strongly associ-
ated with the verb see, while also causing partic-
ular activation of the vision centres of the brain.
Similarly, concepts with strong associations with
a particular sound (e.g., cuckoo) might be seman-
tically associated with hear while also dispropor-
tionately activating auditory areas of the brain. It
should thus be possible to learn a model of corpus-
to-EEG-pattern correspondences on training data,
and use it to predict the EEG activation patterns of
unseen concepts.
We follow the paradigm proposed by Mitchell et
al. (2008) for fMRI data. For each participant and
selected EEG feature, we train a model where the
level of activation of the latter in response to dif-
ferent concepts is approximated by a linear com-
bination of the corpus features:
~
f = C
~
? + ~
where
~
f is the vector of activations of a specific
EEG feature for different concepts, the matrix C
contains the values of the corpus features for the
same concepts (row-normalised to z-scores),
~
? is
the weight we must learn for each corpus feature,
and~ is a vector of error terms. We use the method
of least squared errors to learn the weights that
maximize the fit of the model. We can then predict
the activation of an EEG feature in response to a
new concept that was not in the training data by a
~
?-weighted sum of the values of each corpus fea-
ture for the new concept. In some cases collinear-
ity in the corpus data (regular linear relationships
among the corpus-feature columns) prevented the
estimation procedure from finding a solution. In
such cases (due to the small number of data, rel-
ative to the number of unknowns), the least in-
formative corpus-features (those that correlated on
average most highly with other features) were iter-
atively removed until a solution was reached. All
models were trained with between 23 and 25 cor-
pus features.
Again following Mitchell and colleagues, we
adopt a leave-2-out paradigm in which a linear
model for each EEG feature is trained in turn on
all concepts minus 2. For each of the 2 left out
concepts, we predict the EEG activation pattern
using the trained linear model and their corpus
features, as just described. We then try to cor-
rectly match the predicted and observed activa-
tions, by measuring the Euclidean distance be-
tween the model-generated EEG activity (a vec-
tor of estimated power levels for the n EEG fea-
624
tures selected) and the corresponding EEG activ-
ity recorded in the experiment (other distance met-
rics gave similar results to the ones reported here).
Given the 2 left-out concepts a and b, we com-
pute 2 matched distances (i.e., distance between
predicted and observed pattern for a, and the same
for b) and 2 mismatched distances (predicted a and
observed b; predicted b and observed a). If the av-
erage of the matched distances is lower than the
average of the mismatched distances, we consider
the prediction successful ? otherwise we count is
as a failed prediction. At chance levels, expected
matching accuracy is 50%.
5 Results
Table 3 shows the comparative results for all the
corpus models introduced in Section 3. The third
column (all) shows the overall accuracy in cor-
rectly matching predicted to observed EEG ac-
tivity patterns, and so successfully distinguishing
word meanings. The significance of the figures is
indicated with the conventional annotation (calcu-
lated using a one-way two-sided t-test across the
individual participant accuracy figures against an
expected population mean of 50%).
8
The second
column shows the coverage of each model of the
60 mammal and tool concepts used, which ranged
from full (for the yahoo-mitchell model) to 51 con-
cepts (for the depfilter-topfeat model). The corre-
sponding number of matching comparisons over
which accuracy was calculated ranged from 1770
down to 1225.
As suggested by previous work (Murphy et al,
2008), and illustrated by figure 3, coarse distinc-
tions between words in different superordinate cat-
egories (e.g., hammer vs armadillo; giraffe vs
nail) may be easier to detect than those among
concepts within the same category (e.g., ham-
mer vs nail; giraffe vs armadillo). The fourth
and fifth columns give these accuracies, and while
between-category discriminations do prove more
reliable, they indicate that, for the top rated model
at least, finer within-category distinctions are also
being captured. Figures from the top two perform-
ing models are given for individual participants in
tables 4 and 5.
8
On average, the difference seen between matched and
mismatched pairs was small, at about 3% of the distance
between observed and predicted representations, and was
marginally bigger for correct than for incorrect predictions
(p < 0.01).
part. overall within between
A 54 53 55
B 54 47 60
C 62 56 67
D 61 56 67
E 68 58 78
F 52 54 51
G 57 51 63
Table 4: Accuracy levels for individual participant
sessions, yahoo-mitchell web corpus
part. overall within between
A 49 52 46
B 59 57 60
C 60 60 59
D 50 45 55
E 56 53 58
F 64 64 65
G 52 49 55
Table 5: Accuracy levels for individual participant
sessions, repubblica-window-svd
6 Discussion
Our results show that corpus-extracted conceptual
models can be used to distinguish between the
EEG activation levels associated with conceptual
categories to a degree that is significantly above
chance. Though category specific patterns are de-
tectable in the EEG signal alone (as illustrated by
the PCA analysis in figure 3), on that basis we can-
not be sure that semantics is being detected. Some
other property of the stimuli that co-varies with the
semantic classes of interest could be responsible,
such as visual complexity, conceptual familiarity,
lexical frequency, or phonological form. Only by
cross-training with individual corpus features and
showing that these hold a predictive relationship to
neural activity have we been able to establish that
EEG patterns encode semantics.
Present evidence indicates that fMRI may pro-
vide richer data for training such models than EEG
(Mitchell and colleagues obtain an average accu-
racy of 77%, and 65% for the within category set-
ting). However, fMRI has several clear disadvan-
tages as a tool for language researchers. First of
all, the fine spatial resolution it provides (down
to 2-3mm), while of great interest to neuroscien-
tists, is not in itself linguistically informative. Its
coarse temporal resolution (of the order of several
seconds), makes it ill-suited to analysing on-line
linguistic processes. EEG on the other hand, de-
spite its low spatial resolution (several centime-
tres), gives millisecond-level temporal resolution,
625
model coverage all within cat between cat
yahoo-mitchell 100 58.3** (5.7) 53.6* (3.7) 63.0** (8.9)
repubblica-window-svd 96.7 55.7* (5.6) 54.3 (6.5) 56.9* (5.9)
repubblica-window-topfeat 93.3 52.1 (4.3) 48.7 (3.6) 55.4 (7.0)
repubblica-deppath-svd 93.3 51.4 (8.7) 49.0 (8.0) 54.0 (10.0)
repubblica-depfilter-topfeat 85.0 51.1 (9.6) 49.3 (9.6) 53.1 (10.0)
repubblica-position-topfeat 93.3 50.0 (5.2) 46.0 (4.7) 53.6 (8.0)
repubblica-deppath-topfeat 86.7 49.9 (9.0) 47.0 (9.3) 52.4 (9.6)
repubblica-position-svd 96.7 49.4 (10.2) 46.6 (9.8) 52.3 (11.3)
repubblica-depfilter-svd 93.3 48.9 (11.1) 47.1 (8.9) 50.6 (12.9)
Table 3: Comparison across corpus models, with percentage concept coverage, mean cross-subject per-
centage prediction accuracy and standard deviation; ?p < 0.05, ? ? p < 0.01
enabling the separate analysis of sequential cogni-
tive processes and states (e.g., auditory process-
ing, word comprehension, semantic representa-
tion). fMRI is also prohibitively expensive for
most researchers (ca. 300 euros per hour at cost
price), compared to EEG (ca. 30 euros per hour).
Finally, there is no prospect of fMRI being minia-
turised, while wearable EEG systems are already
becoming commercially available, making exper-
imentation in more ecological settings a possibil-
ity (e.g., playing with a child, meeting at a desk,
walking around). In short, while EEG can be used
to carry out systematic investigations of categori-
cal distinctions, doing so with fMRI would be pro-
hibitively expensive.
Present results indicate that distinctions be-
tween categories are easier than distinctions be-
tween category elements; and that selecting the
conceptual features by hand gives better results
than discovering them automatically. Both of
these results however may be due to limitations
of the current method. One limitation is that we
have been using the same set of features for all
concepts, which is likely to blur the distinctions
between members of a category more than those
between categories. A second limitation of our
present methodology is that it is constrained to use
very small numbers of semantic features, which
limits its applicability. For example it is hard to
conceive of a small set of verbs, or other parts-of-
speech, whose co-occurrence patterns could suc-
cessfully characterise the full range of meaning
found in the human lexicon. Even the more eco-
nomical corpus-extracted conceptual models tend
to run in the hundreds of features (Almuhareb,
2006). We are currently working on variations in
the method that will address these shortcomings.
The web-based model with manually picked
features outperformed all la Repubblica-based
models. However, the results attained with
repubblica-window-svd are encouraging, espe-
cially considering that we are reporting results for
an EEG feature configuration optimised for the
web data (see footnote 5), and that la Repubblica
is several orders of magnitude smaller than the
web. That data sparseness might be the main is-
sue with la Repubblica models is suggested by
the fact that repubblica-window-svd is the least
sparse of them, since it does not filter data by posi-
tion or dependency path, and compresses informa-
tion from many verbs via SVD. In future research,
we plan to extract richer models from larger cor-
pora. And as the discriminative accuracy of cross-
training techniques improves, further insights into
the relative validity of corpus representations will
be attainable. One research aim is to see if individ-
ual corpus semantic properties are encoded neu-
rally, so providing strong evidence for a particular
model. These techniques may also prove more ob-
jective and reliable in evaluating representations of
abstract concepts, for which it is more difficult to
collect reliable judgements from informants.
References
A. Almuhareb. 2006. Attributes in lexical acquisition.
Dissertation, University of Essex.
L. Barsalou. 1999. Perceptual symbol systems. Be-
havioural and Brain Sciences, 22:577?660.
L. Barsalou. 2003. Situated simulation in the human
conceptual system. Language and Cognitive Pro-
cesses, 18:513?562.
J.R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Proceedings of
SIGLEX, pages 59?66.
A. Delorme and S. Makeig. 2003. Eeglab: an open
source toolbox for analysis of single-trial dynamics
includingindependent component analysis. Journal
of Neuroscience Methods, 134:9?21.
626
S. Evert. 2005. The statistics of word cooccurrences.
Dissertation, Stuttgart University.
Ch. J. Fillmore. 1982. Frame semantics. In Linguis-
tic Society of Korea, editor, Linguistics in the Morn-
ing Calm, pages 111?138. Hanshin, Seoul.
J. Gilbert, L. Shapiro, and G. Barnes. 2009. Processing
of living and nonliving objects diverges in the visual
processing system: evidence from meg. In Proceed-
ings of the Cognitive Neuroscience Society Annual
Meeting.
M. Kiefer. 2001. Perceptual and seman-
tic sources of category-specific effects in object
categorization:event-related potentials during pic-
ture and word categorization. Memory and Cogni-
tion, 29(1):100?116.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211?240.
A. Lenci. 2009. Argument alternations in italian verbs:
a computational study. In Atti del XLII Congresso
Internazionale di Studi della Societ`a di Linguistica
Italiana.
D. Lin. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL98, Montreal,
Canada.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28:203?208.
S. Makeig, A.J. Bell, T. Jung, and T.J. Sejnowski.
1996. Independent component analysis of elec-
troencephalographic data. In in Advances in Neu-
ral Information Processing Systems, pages 145?151.
MIT Press.
A. Martin and L. Chao. 2001. Semantic memory and
the brain: structure and processes. Current Opinions
in Neurobiology, 11:194?201.
T. Mitchell, S. Shinkareva, A. Carlson, K. Chang,
V. Malave, R. Mason, and M. Just. 2008. Predicting
human brain activity associated with the meanings
of nouns. Science, 320:1191?1195.
B. Murphy, M. Dalponte, M. Poesio, and L. Bruz-
zone. 2008. Distinguishing concept categories from
single-trial electrophysiological activity. In Pro-
ceedings of the Annual Meeting of the Cognitive Sci-
ence Society.
S. Pad?o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
G. Pfurtscheller and F. Lopes da Silva. 1999. Event-
related EEG/MEG synchronization and desynchro-
nization: Basic principles. Clinical Neurophysiol-
ogy, 110:1842?1857.
F. Pulverm?uller. 2002. The neuroscience of language:
on brain circuits of words and serial order. Cam-
bridge University Press, Cambridge.
F. Pulverm?uller. 2005. Brain mechanisms linking lan-
guage and action. Nature Reviews Neuroscience,
6:576?582.
J. Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge.
M. Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Dissertation, Stockholm
University.
S. Schulte im Walde. 2008. Theoretical adequacy, hu-
man data and classification approaches in modelling
word properties, word relatedness and word classes.
Habilitation, Saarland University.
H. Sch?utze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford.
627
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 489?499,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Interpretable Semantic Vectors from a Joint Model of Brain- and Text-
Based Meaning
Alona Fyshe
1
, Partha P. Talukdar
1
, Brian Murphy
2
, Tom M. Mitchell
1
1
Machine Learning Department, Carnegie Mellon University
2
School of Electronics, Electrical Engineering and Computer Science
Queen?s University Belfast
[afyshe,partha.talukdar,tom.mitchell]@cs.cmu.edu
brian.murphy@qub.ac.uk
Abstract
Vector space models (VSMs) represent
word meanings as points in a high dimen-
sional space. VSMs are typically created
using a large text corpora, and so repre-
sent word semantics as observed in text.
We present a new algorithm (JNNSE) that
can incorporate a measure of semantics
not previously used to create VSMs: brain
activation data recorded while people read
words. The resulting model takes advan-
tage of the complementary strengths and
weaknesses of corpus and brain activation
data to give a more complete representa-
tion of semantics. Evaluations show that
the model 1) matches a behavioral mea-
sure of semantics more closely, 2) can
be used to predict corpus data for unseen
words and 3) has predictive power that
generalizes across brain imaging technolo-
gies and across subjects. We believe that
the model is thus a more faithful represen-
tation of mental vocabularies.
1 Introduction
Vector Space Models (VSMs) represent lexical
meaning by assigning each word a point in high di-
mensional space. Beyond their use in NLP appli-
cations, they are of interest to cognitive scientists
as an objective and data-driven method to discover
word meanings (Landauer and Dumais, 1997).
Typically, VSMs are created by collecting word
usage statistics from large amounts of text data and
applying some dimensionality reduction technique
like Singular Value Decomposition (SVD). The
basic assumption is that semantics drives a per-
son?s language production behavior, and as a result
co-occurrence patterns in written text indirectly
encode word meaning. The raw co-occurrence
statistics are unwieldy, but in the compressed
VSM the distance between any two words is con-
ceived to represent their mutual semantic similar-
ity (Sahlgren, 2006; Turney and Pantel, 2010), as
perceived and judged by speakers. This space then
reflects the ?semantic ground truth? of shared lex-
ical meanings in a language community?s vocab-
ulary. However corpus-based VSMs have been
criticized as being noisy or incomplete representa-
tions of meaning (Glenberg and Robertson, 2000).
For example, multiple word senses collide in the
same vector, and noise from mis-parsed sentences
or spam documents can interfere with the final se-
mantic representation.
When a person is reading or writing, the se-
mantic content of each word will be necessarily
activated in the mind, and so in patterns of ac-
tivity over individual neurons. In principle then,
brain activity could replace corpus data as input
to a VSM, and contemporary imaging techniques
allow us to attempt this. Functional Magnetic Res-
onance Imaging (fMRI) and Magnetoencephalog-
raphy (MEG) are two brain activation recording
technologies that measure neuronal activation in
aggregate, and have been shown to have a pre-
dictive relationship with models of word mean-
ing (Mitchell et al, 2008; Palatucci et al, 2009;
Sudre et al, 2012; Murphy et al, 2012b).
1
If brain activation data encodes semantics, we
theorized that including brain data in a model of
semantics could result in a model more consistent
with semantic ground truth. However, the inclu-
sion of brain data will only improve a text-based
model if brain data contains semantic information
not readily available in the corpus. In addition,
if a semantic test involves another subject?s brain
activation data, performance can improve only if
the additional semantic information is consistent
across brains. Of course, brains differ in shape,
size and in connectivity, so additional information
encoded in one brain might not translate to an-
1
For more details on fMRI and MEG, see Section 4.2
489
other. Furthermore, different brain imaging tech-
nologies measure very different correlates of neu-
ronal activity. Due to these differences, it is possi-
ble that one subject?s brain activation data cannot
improve a model?s performance on another sub-
ject?s brain data, or for brain data collected using
a different recording technology. Indeed, inter-
subject models of brain activation is an open re-
search area (Conroy et al, 2013), as is learning the
relationship between recording technologies (En-
gell et al, 2012; Hall et al, 2013). Brain data
can also be corrupted by many types of noise (e.g.
recording room interference, movement artifacts),
another possible hindrance to the use of brain data
in VSMs.
VSMs are interesting from both engineering
and scientific standpoints. In this work we fo-
cus on the scientific question: Can the inclusion
of brain data improve semantic representations
learned from corpus data? What can we learn from
such a model? From an engineering perspective,
brain activation data will likely never replace text
data. Brain activation recordings are both expen-
sive and time consuming to collect, whereas tex-
tual data is vast and much of it is free to download.
However, from a scientific perspective, combining
text and brain data could lead to more consistent
semantic models, in turn leading to a better un-
derstanding of semantics and semantic modeling
generally.
In this paper, we leverage both kinds of data to
build a hybrid VSM using a new matrix factor-
ization method (JNNSE). Our hypothesis is that
the noise of brain and corpus derived statistics
will be largely orthogonal, and so the two data
sources will have complementary strengths as in-
put to VSMs. If this hypothesis is correct, we
should find that the resulting VSM is more suc-
cessful in modeling word semantics as encoded in
human judgements, as well as separate corpus and
brain data that was not used in the derivation of the
model. We will show that our method:
1. creates a VSM that is more correlated to an
independent measure of word semantics.
2. produces word vectors that are more pre-
dictable from the brain activity of different
people, even when brain data is collected
with a different recording technology.
3. predicts corpus representations of withheld
words more accurately than a model that does
not combine data sources.
4. directly maps semantic concepts onto the
brain by jointly learning neural representa-
tions.
Together, these results suggest that corpus and
brain activation data measure semantics in com-
patible and complimentary ways. Our results
are evidence that a joint model of brain- and
text-based semantics may be closer to seman-
tic ground truth than text-only models. Our
findings also indicate that there is additional se-
mantic information available in brain activation
data that is not present in corpus data, and that
there are elements of semantics currently lack-
ing in text-based VSMs. We have made avail-
able the top performing VSMs created with brain
and text data (http://www.cs.cmu.edu/
?
afyshe/papers/acl2014/).
In the following sections we will review NNSE,
and our extension, JNNSE. We will describe the
data used and the experiments to support our posi-
tion that brain data is a valuable source of semantic
information that compliments text data.
2 Non-Negative Sparse Embedding
Non-Negative Sparse Embedding (NNSE) (Mur-
phy et al, 2012a) is an algorithm that produces
a latent representation using matrix factorization.
Standard NNSE begins with a matrix X ? R
w?c
made of c corpus statistics for w words. NNSE
solves the following objective function:
argmin
A,D
w
?
i=1
?
?
X
i,:
?A
i,:
?D
?
?
2
+ ?
?
?
A
?
?
1
(1)
subject to: D
i,:
D
T
i,:
? 1,? 1 ? i ? ` (2)
A
i,j
? 0, 1 ? i ? w, 1 ? j ? ` (3)
The solution will find a matrix A ? R
w?`
that is
sparse, non-negative, and represents word seman-
tics in an `-dimensional latent space. D ? R
`?c
gives the encoding of corpus statistics in the la-
tent space. Together, they factor the original cor-
pus statistics matrix X in a way that minimizes
the reconstruction error. TheL
1
constraint encour-
ages sparsity in A; ? is a hyperparameter. Equa-
tion 2 constrains D to eliminate solutions where
A is made arbitrarily small by making D arbi-
trarily large. Equation 3 ensures that A is non-
negative. We may increase ` to give more dimen-
sional space to represent word semantics, or de-
crease ` for more compact representations.
490
The sparse and non-negative representation in
A produces a more interpretable semantic space,
where interpretability is quantified with a behav-
ioral task (Chang et al, 2009; Murphy et al,
2012a). To illustrate the interpretability of NNSE,
we describe a word by selecting the word?s top
scoring dimensions, and selecting the top scoring
words in those dimensions. For example, the word
chair has the following top scoring dimensions:
1. chairs, seating, couches;
2. mattress, futon, mattresses;
3. supervisor, coordinator, advisor.
These dimensions cover two of the distinct mean-
ings of the word chair (furniture and person of
power).
NNSE?s sparsity constraint dictates that each
word can have a non-zero score in only a few di-
mensions, which aligns well to previous feature
elicitation experiments in psychology. In feature
elicitation, participants are asked to name the char-
acteristics (features) of an object. The number of
characteristics named is usually small (McRae et
al., 2005), which supports the requirement of spar-
sity in the learned latent space.
3 Joint Non-Negative Sparse Embedding
We extend NNSEs to incorporate an additional
source of data for a subset of the words in X ,
and call the approach Joint Non-Negative Sparse
Embeddings (JNNSEs). The JNNSE algorithm
is general enough to incorporate any new infor-
mation about the a word w, but for this study
we will focus on brain activation recordings of
a human subject reading single words. We
will incorporate either fMRI or MEG data, and
call the resulting models JNNSE(fMRI+Text) and
JNNSE(MEG+Text) and refer to them generally
as JNNSE(Brain+Text). For clarity, from here
on, we will refer to NNSE as NNSE(Text), or
NNSE(Brain) depending on the single source of
input data used.
Let us order the rows of the corpus data X so
that the first 1 . . . w
?
rows have both corpus statis-
tics and brain activation recordings. Each brain
activation recording is a row in the brain data ma-
trix Y ? R
w
?
?v
where v is the number of features
derived from the recording. For MEG recordings,
v =sensors ? time points= 306? 150. For fMRI
v = grey-matter voxels =' 20, 000 depending on
the brain anatomy of each individual subject. The
new objective function is:
argmin
A,D
(c)
,D
(b)
w
?
i=1
?
?
X
i,:
?A
i,:
?D
(c)
?
?
2
+
w
?
?
i=1
?
?
Y
i,:
?A
i,:
?D
(b)
?
?
2
+ ?
?
?
A
?
?
1
(4)
subject to: D
(c)
i,:
D
(c)
i,:
T
? 1, ? 1 ? i ? ` (5)
D
(b)
i,:
D
(b)
i,:
T
? 1,? 1 ? i ? ` (6)
A
i,j
? 0, 1 ? i ? w, 1 ? j ? `
(7)
We have introduced an additional constraint on the
rows 1 . . . w
?
, requiring that some of the learned
representations in A also reconstruct the brain ac-
tivation recordings (Y ) through representations in
D
(b)
? R
`?v
. Let us use A
?
to refer to the brain-
constrained rows of A. Words that are close in
?brain space? must have similar representations in
A
?
, which can further percolate to affect the rep-
resentations of other words in A via closeness in
?corpus space?.
With A or D fixed, the objective function for
NNSE(Text) and JNNSE(Brain+Text) is convex.
However, we are solving forA andD, so the prob-
lem is non-convex. To solve for this objective, we
use the online algorithm of Section 3 from Mairal
et al (Mairal et al, 2010). This algorithm is
guaranteed to converge, and in practice we found
that JNNSE(Brain+Text) converged as quickly as
NNSE(Text) for the same `. We used the SPAMS
package
2
to solve, and set ? = 0.025. This al-
gorithm was a very easy extension to NNSE(Text)
and required very little additional tuning.
We also consider learning shared representa-
tions in the case where data X and Y contain the
effects of known disjoint features. For example,
when a person reads a word, the recorded brain
activation data Y will contain the physiological
response to viewing the stimulus, which is unre-
lated to the semantics of the word. These sig-
nals can be attributed to, for example, the num-
ber of letters in the word and the number of white
pixels on the screen (Sudre et al, 2012). To ac-
count for such effects in the data, we augment
A
?
with a set of n fixed, manually defined fea-
tures (e.g. word length) to create A
?
percept
?
R
w?(`+n)
. D
(b)
? R
(`+n)?v
is used withA
?
percept
,
2
SPAMS Package: http://spams-devel.gforge.inria.fr/
491
to reconstruct the brain data Y . More gener-
ally, one could instead allocate a certain num-
ber of latent features specific to X or Y, both of
which could be learned, as explored in some re-
lated work (Gupta et al, 2013). We use 11 per-
ceptual features that characterize the non-semantic
features of the word stimulus (for a list, see sup-
plementary material at http://www.cs.cmu.
edu/
?
afyshe/papers/acl2014/).
The JNNSE algorithm is advantageous in that
it can handle partially paired data. That is, the
algorithm does not require that every row in X
also have a row in Y . Fully paired data is a re-
quirement of many other approaches (White et al,
2012; Jia and Darrell, 2010). Our approach al-
lows us to leverage the semantic information in
corpus data even for words without brain activa-
tion recordings.
JNNSE(Brain+Text) does not require brain data
to be mapped to a common average brain, which
is often the case when one wants to generalize be-
tween human subjects. Such mappings can blur
and distort data, making it less useful for subse-
quent prediction steps. We avoid these mappings,
and instead use the fact that similar words elicit
similar brain activation within a subject. In the
JNNSE algorithm, it is this closeness in ?brain
space? that guides the creation of the latent space
A. Leveraging intra-subject distance measures
to study inter-subject encodings has been studied
previously (Kriegeskorte et al, 2008a; Raizada
and Connolly, 2012), and has even been used
across species (humans and primates) (Kriegesko-
rte et al, 2008b).
Though we restrict ourselves to using one sub-
ject per JNNSE(Brain+Text) model, the JNNSE
algorithm could easily be extended to include
data from multiple brain imaging experiments by
adding a new squared loss term for additional
brain data.
3.1 Related Work
Perhaps the most well known related approach
to joining data sources is Canonical Correlation
Analysis (CCA) (Hotelling, 1936), which has been
applied to brain activation data in the past (Rus-
tandi et al, 2009). CCA seeks two linear trans-
formations that maximally correlate two data sets
in the transformed form. CCA requires that the
data sources be paired (all rows in the corpus data
must have a corresponding brain data), as corre-
lation between points is integral to the objective.
To apply CCA to our data we would need to dis-
card the vast majority of our corpus data, and use
only the 60 rows of X with corresponding rows
in Y. While CCA holds the input data fixed and
maximally correlates the transformed form, we
hold the transformed form fixed and seek a solu-
tion that maximally correlates the reconstruction
(AD
(c)
or A
?
D
(b)
) with the data (X and Y respec-
tively). This shift in error compensation is what
allows our data to be only partially paired. While
a Bayesian formulation of CCA can handle miss-
ing data, our model has missing data for> 97% of
the full w ? (v + c) brain and corpus data matrix.
To our knowledge, this extreme amount of missing
data has not been explored with Bayesian CCA.
One could also use a topic model style formula-
tion to represent this semantic representation task.
Supervised topic models (Blei and McAuliffe,
2007) use a latent topic to generate two observed
outputs: words in a document and a categorical la-
bel for the document. The same idea could be ap-
plied here: the latent semantic representation gen-
erates the observed brain activity and corpus statis-
tics. Generative and discriminative models both
have their own strengths and weaknesses, gener-
ative models being particularly strong when data
sources are limited (Ng and Jordan, 2002). Our
task is an interesting blend of data-limited and
data-rich problem scenarios.
In the past, various pieces of additional informa-
tion have been incorporated into semantic models.
For example, models with behavioral data (Sil-
berer and Lapata, 2012) and models with visual
information (Bruni et al, 2011; Silberer et al,
2013) have both shown to improve semantic rep-
resentations. Other works have correlated VSMs
built with text or images with brain activation
data (Murphy et al, 2012b; Anderson et al, 2013).
To our knowledge, this work is the first to integrate
brain activation data into the construction of the
VSM.
4 Data
4.1 Corpus Data
The corpus statistics used here are the download-
able vectors from Fyshe et al (2013)
3
. They
are compiled from a 16 billion word subset of
ClueWeb09 (Callan and Hoy, 2009) and contain
two types of corpus features: dependency and doc-
ument features, found to be complimentary for
3
http://www.cs.cmu.edu/
?
afyshe/papers/
conll2013/
492
most tasks. Dependency statistics were derived
by dependency parsing the corpus and compil-
ing counts for all dependencies incident on the
word. Document statistics are word-document
co-occurrence counts. Count thresholding was
applied to reduce noise, and positive pointwise-
mutual-information (PPMI) (Church and Hanks,
1990) was applied to the counts. SVD was ap-
plied to the document and dependency statistics
and the top 1000 dimensions of each type were
retained. We selected the rows corresponding to
noun-tagged words (approx. 17000 words).
4.2 Brain Activation Data
We have MEG and fMRI data at our disposal.
MEG measures the magnetic field caused by many
thousands of neurons firing together, and has good
time resolution (1000 Hz) but poor spatial reso-
lution. fMRI measures the change in blood oxy-
genation that results from differential neural ac-
tivity, and has good spatial resolution but poor
time resolution (0.5-1 Hz). We have fMRI data
and MEG data for 18 subjects (9 in each imaging
modality) viewing 60 concrete nouns (Mitchell et
al., 2008; Sudre et al, 2012). The 60 words span
12 word categories (animals, buildings, tools, in-
sects, body parts, furniture, building parts, uten-
sils, vehicles, objects, clothing, food). Each of the
60 words was presented with a line drawing, so
word ambiguity is not an issue. For both record-
ing modalities, all trials for a particular word were
averaged together to create one training instance
per word, with 60 training instances in all for each
subject and imaging modality. More preprocess-
ing details appear in the supplementary material.
5 Experimental Results
Here we explore several variations of JNNSE and
NNSE formulations. For a comparison of the
models used, see Table 1.
5.1 Correlation to Behavioral Data
To test if our joint model of Brain+Text is closer
to semantic ground truth we compared the latent
representation A learned via JNNSE(Brain+Text)
or NNSE(Text) to an independent behavioral mea-
sure of semantics. We collected behavioral data
for the 60 nouns in the form of answers to 218
semantic questions. Answers were gathered with
Mechanical Turk. The full list of questions ap-
pear in the supplementary material. Some exam-
ple questions are:?Is it alive??, and ?Can it bend??.
Mechanical Turk users were asked to respond to
each question for each word on a scale of 1-5. At
least 3 respondents answered each question and
the median score was used. This gives us a se-
mantic representation of each of the 60 words in
a 218-dimensional behavioral space. Because we
required answers to each of the questions for all
words, we do not have the problems of sparsity
that exist for feature production norms from other
studies (McRae et al, 2005). In addition, our an-
swers are ratings, rather than binary yes/no an-
swers.
For a given value of ` we solve the NNSE(Text)
and JNNSE(Brain+Text) objective function as de-
tailed in Equation 1 and 4 respectively. We com-
pared JNNSE(Brain+Text) and NNSE(Text) mod-
els by measuring the correlation of all pairwise
distances in JNNSE(Brain+Text) and NNSE(Text)
space to the pairwise distances in the 218-
dimensional semantic space. Distances were
calculated using normalized Euclidean distance
(equivalent in rank-ordering to cosine distance,
but more suitable for sparse vectors). Figure 1
shows the results of this correlation test. The er-
ror bars for the JNNSE(Brain+Text) models rep-
resent a 95% confidence interval calculated using
the standard error of the mean (SEM) over the 9
person-specific JNNSE(Brain+Text) models. Be-
cause there is only one NNSE(Text) model for
each dimension setting, no SEM can be calculated,
but it suffices to show that the NNSE(Text) corre-
lation does not fall into the 95% confidence inter-
val of the JNNSE(Brain+Text) models. The SVD
matrix for the original corpus data has correlation
0.4279 to the behavioral data, also below the 95%
confidence interval for all JNNSE models. The re-
sults show that a model that incorporates brain ac-
tivation data is more faithful to a behavioral mea-
sure of semantics.
5.2 Word Prediction from Brain Activation
We now show that the JNNSE(Brain+Text) vec-
tors are more consistent with independent sam-
ples of brain activity collected from different sub-
jects, even when recorded using different record-
ing technologies. As previously mentioned, be-
cause there is a large degree of variation between
brains and because MEG and fMRI measure very
different correlates of neuronal activity, this type
of generalization has proven to be very challeng-
ing and is an open research question in the neuro-
science community.
The output A of the JNNSE(Brain+Text) or
493
Table 1: A Comparison of the models explored in this paper, and the data upon which they operate.
Model Name Section(s) Text Data Brain Data Withheld Data
NNSE(Text) 2, 5 X x -
NNSE(Brain) 2, 5.2.1, 5.3 x X -
JNNSE(Brain+Text) 3, 5 X X -
JNNSE(Brain+Text): Dropout task 5.2.2 X X subset of brain data
JNNSE(Brain+Text): Predict corpus 5.3 X X subset of text data
250 500 10000.4
0.42
0.44
0.46
0.48
0.5
Correlation of Semantic Question Distances to JNNSE(fMRI)
Number of Latent Dimensions
Cor
rela
tion
 
 JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 1: Correlation of JNNSE(Brain+Text) and
NNSE(Text) models with the distances in a se-
mantic space constructed from behavioral data.
Error bars indicate SEM.
NNSE(Text) algorithm can be used as a VSM,
which we use for the task of word prediction from
fMRI or MEG recordings. A JNNSE(Brain+Text)
created with a particular human subject?s data is
never used in the prediction framework with that
same subject. For example, if we use fMRI data
from subject 1 to create a JNNSE(fMRI+Text), we
will test it with the remaining 8 fMRI subjects, but
all 9 MEG subjects (fMRI and MEG subjects are
disjoint).
Let us call the VSM learned with
JNNSE(Brain+Text) or NNSE(Text) the se-
mantic vectors. We can train a weight matrix W
that predicts the semantic vector a of a word from
that word?s brain activation vector x: a = Wx.
W can be learned with a variety of methods, we
will use L
2
regularized regression. One can also
train regressors that predict the brain activation
data from the semantic vector: x = Wa, but we
have found this to give lower predictive accuracy.
Note that we must re-train our weight matrix W
for each subject (instead of re-using D
(b)
from
Equation 4) because testing always occurs on a
different subject, and the brain activation data is
not inter-subject aligned.
We train ` independent L
2
regularized regres-
sors to predict the `-dimensional vectors a =
{a
1
. . . a
`
}. The predictions are concatenated
to produce a predicted semantic vector: a? =
{a?
1
, . . . , a?
`
}. We assess word prediction perfor-
mance by testing if the model can differentiate be-
tween two unseen words, a task named 2 vs. 2 pre-
diction (Mitchell et al, 2008; Sudre et al, 2012).
We choose the assignment of the two held out se-
mantic vectors (a
(1)
,a
(2)
) to predicted semantic
vectors (a?
(1)
, a?
(2)
) that minimizes the sum of the
two normalized Euclidean distances. 2 vs. 2 ac-
curacy is the percentage of tests where the correct
assignment is chosen.
The 60 nouns fall into 12 word categories.
Words in the same word category (e.g. screw-
driver and hammer) are closer in semantic space
than words in different word categories, which
makes some 2 vs. 2 tests more difficult than oth-
ers. We choose 150 random pairs of words (with
each word represented equally) to estimate the dif-
ficulty of a typical word pair, without having to
test all
(
60
2
)
word pairs. The same 150 random
pairs are used for all subjects and all VSMs. Ex-
pected chance performance on the 2 vs. 2 test is
50%.
Results for testing on fMRI data in the
2 vs. 2 framework appear in Figure 2.
JNNSE(fMRI+Text) data performed on aver-
age 6% better than the best NNSE(Text), and
exceeding even the original SVD corpus represen-
tations while maintaining interpretability. These
results generalize across brain activity recording
types; JNNSE(MEG+Text) performs as well as
JNNSE(fMRI+Text) when tested on fMRI data.
The results are consistent when testing on MEG
data: JNNSE(MEG+Text) or JNNSE(fMRI+Text)
outperforms NNSE(Text) (see Figure 3).
494
250 500 1000
64
66
68
70
72
74
Number of Latent Dimensions
2 vs
. 2 A
ccu
racy
2 vs. 2 Acc. for JNNSE and NNSE, tested on fMRI data
 
 
JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 2: Average 2 vs. 2 accuracy for
NNSE(Text) and JNNSE(Brain+Text), tested on
fMRI data. Models created with one subject?s
fMRI data were not used to compute 2 vs. 2 ac-
curacy for that same subject.
250 500 1000
66
68
70
72
74
76
78
80
82
Number of Latent Dimensions
2 vs
. 2 A
ccu
racy
2 vs. 2 Acc. for JNNSE and NNSE, tested on MEG data
 
 
JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 3: Average 2 vs. 2 accuracy for
NNSE(Text) and JNNSE(Brain+Text), tested on
MEG data. Models created with one subject?s
MEG data were not used to compute 2 vs. 2 ac-
curacy for that same subject.
NNSE(Text) performance decreases as the
number of latent dimension increases. This im-
plies that without the regularizing effect of brain
activation data, the extra NNSE(Text) dimensions
are being used to overfit to the corpus data, or
possibly to fit semantic properties not detectable
with current brain imaging technologies. How-
ever, when brain activation data is included, in-
creasing the number of latent dimensions strictly
increases performance for JNNSE(fMRI+Text).
JNNSE(MEG+Text) has peak performance with
500 latent dimensions, with ? 1% decrease in
performance at 1000 latent dimensions. In previ-
ous work, the ability to decode words from brain
activation data was found to improve with added
latent dimensions (Murphy et al, 2012a). Our
results may differ because our words are POS
tagged, and we included only nouns for the final
NNSE(Text) model. We found that with the orig-
inal ? = 0.05 setting from Murphy et al (Mur-
phy et al, 2012a) produced vectors that were too
sparse; four of the 60 test words had all-zero vec-
tors (JNNSE(Brain+Text) models did have any all-
zero vectors). To improve the NNSE(Text) vectors
for a fair comparison, we reduced ? = 0.025, un-
der which NNSE(Text) did not produce any all-
zero vectors for the 60 words.
Our results show that brain activation data con-
tributes additional information, which leads to an
increase in performance for the task of word pre-
diction from brain activation data. This suggests
that corpus-only models may not capture all rel-
evant semantic information. This conflicts with
previous studies which found that semantic vec-
tors culled from corpus statistics contain all of the
semantic information required to predict brain ac-
tivation (Bullinaria and Levy, 2013).
5.2.1 Prediction from a Brain-only Model
How much predictive power does the corpus data
provide to this word prediction task? To test
this, we calculated the 2 vs. 2 accuracy for a
NNSE(Brain) model trained on brain activation
data only. We train NNSE(Brain) with one sub-
ject?s data and use the resulting vectors to calculate
2 vs. 2 accuracy for the remaining subjects. We
have brain data for only 60 words, so using ` ? 60
latent dimensions leads to an under-constrained
system and a degenerate solution wherein only one
latent dimension is active for any word (and where
the brain data can be perfectly reconstructed). The
degenerate solution makes it impossible to gen-
eralize across words and leads to performance at
chance levels. An NNSE(MEG) trained on MEG
data gave maximum 2 vs. 2 accuracy of 67% when
` = 20. The reduced performance may be due to
the limited training data and the low SNR of the
data, but could also be attributed to the lack of cor-
pus information, which provides another piece of
semantic information.
495
5.2.2 Effect on Rows Without Brain Data
It is possible that some JNNSE(Brain+Text) di-
mensions are being used exclusively to fit brain
activation data, and not the semantics represented
in both brain and corpus data. If a particular
dimension j is solely used for brain data, the
sparsity constraint will favor solutions that sets
A
(i,j)
= 0 for i > w
?
(no brain data constraint),
and A
(i,j)
> 0 for some 0 ? i ? w
?
(brain data
constrained). We found that there were no such
dimensions in the JNNSE(Brain+Text). In fact for
the ` = 1000 JNNSE(Brain+Text), all latent di-
mensions had greater than ? 25% non-zero en-
tries, which implies that all dimensions are being
shared between the two data inputs (corpus and
brain activation), and are used to reconstruct both.
To test that the brain activation data is truly in-
fluencing rows of A not constrained by brain acti-
vation data, we performed a dropout test. We split
the original 60 words into two 30 word groups (as
evenly as possible across word categories). We
trained JNNSE(fMRI+Text) with 30 words, and
tested word prediction with the remaining 8 sub-
jects and the other 30 words. Thus, the training
and testing word sets are disjoint. Because of the
reduced size of the training data, we did see a drop
in performance, but JNNSE(fMRI+Text) vectors
still gave word prediction performance 7% higher
than NNSE(Text) vectors. Full results appear in
the supplementary material.
5.3 Predicting Corpus Data
Here we ask: can an accurate latent representa-
tion of a word be constructed using only brain
activation data? This task simulates the scenario
where there is no reliable corpus representation of
a word, but brain data is available. This scenario
may occur for seldom-used words that fall below
the thresholds used for the compilation of corpus
statistics. It could also be useful for acronym to-
kens (lol, omg) found in social media contexts
where the meaning of the token is actually a full
sentence.
We trained a JNNSE(fMRI+Text) with brain
data for all 60 words, but withhold the corpus data
for 30 of the 60 words (as evenly distributed as
possible amongst the 12 word categories). The
brain activation data for the 30 withheld words
will allow us to create latent representations in
A for withheld words. Simultaneously, we will
learn a mapping from the latent representation to
the corpus data (D
(c)
). This task cannot be per-
Table 2: Mean rank accuracy over 30 words
using corpus representations predicted by a
JNNSE(MEG+Text) model trained with some
rows of the corpus data withheld. Significance
is calculated using Fisher?s method to combine p-
values for each of the subject-dependent models.
Latent Dim size Rank Accuracy p-value
250 65.30 < 10
?19
500 67.37 < 10
?24
1000 63.47 < 10
?15
formed with a NNSE(Text) model because one
cannot learn a latent representation of a word with-
out data of some kind. This further emphasizes the
impact of brain imaging data, which will allow us
to generalize to previously unseen words in corpus
space.
We use the latent representations in A for each
of the words without corpus data and the mapping
to corpus space D
(c)
to predict the withheld cor-
pus data in X . We then rank the withheld rows of
X by their distance to the predicted row of X and
calculate the mean rank accuracy of the held out
words. Results in Table 2 show that we can recre-
ate the withheld corpus data using brain activation
data. Peak mean rank accuracy (67.37) is attained
at ` = 500 latent dimensions. This result shows
that neural semantic representations can create a
latent representation that is faithful to unseen cor-
pus statistics, providing further evidence that the
two data sources share a strong common element.
How much power is the remaining corpus data
supplying in scenarios where we withhold cor-
pus data? To answer this question, we trained an
NNSE(Brain) model on 30 words of brain activa-
tion, and then trained a regressor to predict cor-
pus data from those latent brain-only representa-
tions. We use the trained regressor to predict the
corpus data for the remaining 30 words. Peak per-
formance is attained at ` = 10 latent dimensions,
giving mean rank accuracy of 62.37, significantly
worse than the model that includes both corpus
and brain activation data (67.37).
5.4 Mapping Semantics onto the Brain
Because our method incorporates brain data into
an interpretable semantic model, we can directly
map semantic concepts onto the brain. To do
this, we examined the mappings from the latent
space to the brain space via D
(b)
. We found that
the most interpretable mappings come from mod-
496
!"#$%&'()
(a) D
(b)
matrix, subject P3, dimension with top words bath-
room, balcony, kitchen. MNI coordinates z=-12 (left) and z=-18
(right). Fusiform is associated with shelter words.
!"#$%&'$()*+
!(&%&'$()*+
(b) D
(b)
matrix; subject P1; dimension with top words ankle,
elbow, knee. MNI coordinates z=60 (left) and z=54 (right). Pre-
and post-central areas are activated for body part words.
!"#$%&'(#)*+"#,$%
(c) D
(b)
matrix; subject P1; dimension with top scoring words
buffet, brunch, lunch. MNI coordinates z=30 (left) and z=24
(right). Pars opercularis is believed to be part of the gustatory
cortex, which responds to food related words.
Figure 4: The mappings (D
(b)
) from latent se-
mantic space (A) to brain space (Y ) for fMRI and
words from three semantic categories. Shown are
representations of the fMRI slices such that the
back of the head is at the top of the image, the
front of the head is at the bottom.
els where the perceptual features had been scaled
down (divided by a constant factor), which en-
courages more of the data to be explained by
the semantic features in A. Figure 4 shows the
mappings (D
(b)
) for dimensions related to shel-
ter, food and body parts. The red areas align
with areas of the brain previously known to be
activated by the corresponding concepts (Mitchell
et al, 2008; Just et al, 2010). Our model
has learned these mappings in an unsupervised
setting by relating semantic knowledge gleaned
from word usage to patterns of activation in the
brain. This illustrates how the interpretability of
JNNSE can allow one to explore semantics in
the human brain. The mappings for one subject
are available for download (http://www.cs.
cmu.edu/
?
afyshe/papers/acl2014/).
6 Future Work and Conclusion
We are interested in pursuing many future projects
inspired by the success of this model. We would
like to extend the JNNSE algorithm to incorporate
data from multiple subjects, multiple modalities
and multiple experiments with non-overlapping
words. Including behavioral data and image data
is another possibility.
We have explored a model of semantics that in-
corporates text and brain activation data. Though
the number of words for which we have brain acti-
vation data is comparatively small, we have shown
that including even this small amount of data has
a positive impact on the learned latent representa-
tions, including for words without brain data. We
have provided evidence that the latent representa-
tions are closer to the neural representation of se-
mantics, and possibly, closer to semantic ground
truth. Our results reveal that there are aspects of
semantics not currently represented in text-based
VSMs, indicating that there may be room for im-
provement in either the data or algorithms used to
create VSMs. Our findings also indicate that using
the brain as a semantic test can separate models
that capture this additional semantic information
from those that do not. Thus, the brain is an im-
portant source of both training and testing data.
Acknowledgments
This work was supported in part by NIH un-
der award 5R01HD075328-02, by DARPA under
award FA8750-13-2-0005, and by a fellowship to
Alona Fyshe from the Multimodal Neuroimag-
ing Training Program funded by NIH awards
T90DA022761 and R90DA023420.
References
Andrew J Anderson, Elia Bruni, Ulisse Bordignon,
Massimo Poesio, and Marco Baroni. 2013. Of
words , eyes and brains : Correlating image-based
distributional semantic models with neural represen-
tations of concepts. In Proceedings of the Confer-
ence on Empirical Methods on Natural Language
Processing.
David M Blei and Jon D. McAuliffe. 2007. Supervised
topic models. In Advances in Neural Information
Processing Systems, pages 1?22.
497
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proceedings of the EMNLP 2011 Geometrical Mod-
els for Natural Language Semantics (GEMS).
John A Bullinaria and Joseph P Levy. 2013. Limiting
factors for mapping corpus-based semantic repre-
sentations to brain activity. PloS one, 8(3):e57191,
January.
Jamie Callan and Mark Hoy. 2009. The ClueWeb09
Dataset.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M Blei. 2009. Reading
Tea Leaves : How Humans Interpret Topic Models.
In Advances in Neural Information Processing Sys-
tems, pages 1?9.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Bryan R Conroy, Benjamin D Singer, J Swaroop Gun-
tupalli, Peter J Ramadge, and James V Haxby. 2013.
Inter-subject alignment of human cortical anatomy
using functional connectivity. NeuroImage, 81:400?
11, November.
Andrew D Engell, Scott Huettel, and Gregory Mc-
Carthy. 2012. The fMRI BOLD signal tracks elec-
trophysiological spectral perturbations, not event-
related potentials. NeuroImage, 59(3):2600?6,
February.
Alona Fyshe, Partha Talukdar, Brian Murphy, and Tom
Mitchell. 2013. Documents and Dependencies : an
Exploration of Vector Space Models for Semantic
Composition. In Computational Natural Language
Learning, Sofia, Bulgaria.
Arthur M Glenberg and David a Robertson. 2000.
Symbol Grounding and Meaning: A Compari-
son of High-Dimensional and Embodied Theories
of Meaning. Journal of Memory and Language,
43(3):379?401, October.
Sunil Kumar Gupta, Dinh Phung, Brett Adams, and
Svetha Venkatesh. 2013. Regularized nonnegative
shared subspace learning. Data Mining and Knowl-
edge Discovery, 26(1):57?97.
Emma L Hall, Si?an E Robson, Peter G Morris, and
Matthew J Brookes. 2013. The relationship be-
tween MEG and fMRI. NeuroImage, November.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321?377.
Yangqing Jia and Trevor Darrell. 2010. Factorized La-
tent Spaces with Structured Sparsity. In Advances in
Neural Information Processing Systems, volume 23.
Marcel Adam Just, Vladimir L Cherkassky, Sandesh
Aryal, and Tom M Mitchell. 2010. A neuroseman-
tic theory of concrete noun representation based on
the underlying brain codes. PloS one, 5(1):e8622,
January.
Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-
dettini. 2008a. Representational similarity analysis
- connecting the branches of systems neuroscience.
Frontiers in systems neuroscience, 2(November):4,
January.
Nikolaus Kriegeskorte, Marieke Mur, Douglas A Ruff,
Roozbeh Kiani, Jerzy Bodurka, Hossein Esteky,
Keiji Tanaka, and Peter A Bandettin. 2008b. Match-
ing Categorical Object Representations in Inferior
Temporal Cortex of Man and Monkey. Neuron,
60(6):1126?1141.
TK Landauer and ST Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological review, 1(2):211?240.
Julien Mairal, Francis Bach, J Ponce, and Guillermo
Sapiro. 2010. Online learning for matrix factor-
ization and sparse coding. The Journal of Machine
Learning Research, 11:19?60.
Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior research methods, 37(4):547?59,
November.
Tom M Mitchell, Svetlana V Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L Malave, Robert A
Mason, and Marcel Adam Just. 2008. Pre-
dicting human brain activity associated with the
meanings of nouns. Science (New York, N.Y.),
320(5880):1191?5, May.
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012a. Learning Effective and Interpretable Se-
mantic Models using Non-Negative Sparse Embed-
ding. In Proceedings of Conference on Computa-
tional Linguistics (COLING).
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012b. Selecting Corpus-Semantic Models for Neu-
rolinguistic Decoding. In First Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 114?123, Montreal, Quebec, Canada.
Andrew Y. Ng and Michael I. Jordan. 2002. On dis-
criminative vs. generative classifiers: A compari-
son of logistic regression and naive bayes. In Ad-
vances in neural information processing systems,
volume 14.
Mark Palatucci, Geoffrey Hinton, Dean Pomerleau,
and Tom M Mitchell. 2009. Zero-Shot Learning
with Semantic Output Codes. Advances in Neural
Information Processing Systems, 22:1410?1418.
Rajeev D S Raizada and Andrew C Connolly. 2012.
What Makes Different People?s Representations
Alike : Neural Similarity Space Solves the Problem
of Across-subject fMRI Decoding. Journal of Cog-
nitive Neuroscience, 24(4):868?877.
498
Indrayana Rustandi, Marcel Adam Just, and Tom M
Mitchell. 2009. Integrating Multiple-Study
Multiple-Subject fMRI Datasets Using Canonical
Correlation Analysis. In MICCAI 2009 Workshop:
Statistical modeling and detection issues in intra-
and inter-subject functional MRI data analysis.
Magnus Sahlgren. 2006. The Word-Space Model Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words. Doctor
of philosophy, Stockholm University.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of Semantic Representation with Vi-
sual Attributes. In Association for Computational
Linguistics 2013, Sofia, Bulgaria.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila
Wehbe, Alona Fyshe, Riitta Salmelin, and Tom
Mitchell. 2012. Tracking Neural Coding of Per-
ceptual and Semantic Features of Concrete Nouns.
NeuroImage, 62(1):463?451, May.
Peter D Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning : Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Martha White, Yaoliang Yu, Xinhua Zhang, and Dale
Schuurmans. 2012. Convex multi-view subspace
learning. In Advances in Neural Information Pro-
cessing Systems, pages 1?14.
499
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 114?123,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Selecting Corpus-Semantic Models for Neurolinguistic Decoding
Brian Murphy
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
brianmurphy@cmu.edu
Partha Talukdar
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
ppt@cs.cmu.edu
Tom Mitchell
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
tom.mitchell@cs.cmu.edu
Abstract
Neurosemantics aims to learn the mapping
between concepts and the neural activity
which they elicit during neuroimaging ex-
periments. Different approaches have been
used to represent individual concepts, but
current state-of-the-art techniques require
extensive manual intervention to scale to
arbitrary words and domains. To over-
come this challenge, we initiate a system-
atic comparison of automatically-derived
corpus representations, based on various
types of textual co-occurrence. We find
that dependency parse-based features are
the most effective, achieving accuracies
similar to the leading semi-manual ap-
proaches and higher than any published
for a corpus-based model. We also find
that simple word features enriched with
directional information provide a close-to-
optimal solution at much lower computa-
tional cost.
1 Introduction
The cognitive plausibility of computational
models of word meaning has typically been
tested using behavioural benchmarks, such as
identification of synonyms among close asso-
ciates (the TOEFL task for language learners,
see e.g. Landauer and Dumais, 1997); emulating
elicited judgments of pairwise similarity (such as
Rubenstein and Goodenough, 1965); judgments
of category membership (e.g. Battig and Mon-
tague, 1969); and word priming effects (Lund
and Burgess, 1996). Mitchell et al (2008) in-
troduced a new task in neurosemantic decoding
? using models of semantics to learn the map-
ping between concepts and the neural activity
which they elicit during neuroimaging experi-
ments. This was achieved with a linear model
which used training data to find neural basis im-
ages that correspond to the assumed semantic
dimensions (for instance, one such basis image
might be the activity of the brain for words rep-
resenting animate concepts), and subsequently
used these general patterns and known seman-
tic dimensions to infer the fMRI activity that
should be elicited by an unseen stimulus con-
cept. Follow-on work has experimented with
other neuroimaging modalities (Murphy et al,
2009), and with a range of semantic models in-
cluding elicited property norms (Chang et al,
2011), corpus derived models (Devereux and
Kelly, 2010; Pereira et al, 2011) and structured
ontologies (Jelodar et al, 2010).
The current state-of-the-art performance on
this task is achieved using models that are hand-
tailored in some respect, whether using manual
annotation tasks (Palatucci et al, 2009), use of
a domain-appropriate curated corpus (Pereira
et al, 2011), or selection of particular collocates
to suit the concepts to be described (Mitchell
et al, 2008). While these approaches are clearly
very successful, it is questionable whether they
are a general solution to describe the vari-
ous parts-of-speech and semantic domains that
make up a speaker?s vocabulary. The Mitchell
et al (2008) 25-verb model would probably have
to be extended to describe the lexicon at large,
and it is unclear whether such a compact model
could be maintained. While Wikipedia (Pereira
et al, 2011) has very broad and increasing cov-
114
erage, it is possible that it will remain inad-
equate for specialist vocabularies, or for less-
studied languages. And while the method used
by Palatucci et al (2009) distributes the anno-
tation task efficiently by crowd-sourcing, it still
requires that appropriate questions are compiled
by researchers, a task that is both difficult to
perform in a systematic way, and which may not
generalize to more abstract concepts.
In this paper we examine a representative set
of corpus-derived models of meaning, that re-
quire no manual intervention, and are applicable
to any syntactic and semantic domain. We con-
centrate on which types of basic corpus pattern
perform well on the neurosemantic decoding
task: LSA-style word-region co-occurrences,
and various HAL-style word-collocate features
including raw tokens, POS tags, and a full de-
pendency parse. Otherwise a common feature
extraction and preprocessing pipeline is used: a
co-occurrence frequency cutoff, application of a
frequency normalization weighting, and dimen-
sionality reduction with SVD.
The following section describes how the brain
activity data was gathered and processed; the
construction of several corpus-derived models
of meaning; and the regression-based meth-
ods used to predict one from the other, evalu-
ated with a brain-image matching task (Mitchell
et al, 2008). In section 3 we report the re-
sults, and in the Conclusion we discuss both the
practical implications, and what this works sug-
gests for the cognitive plausibility of distribu-
tional models of meaning.
2 Methods
2.1 Brain activity features
The dataset used here is that described in detail
in (Mitchell et al, 2008) and released publicly1
in conjunction with the NAACL 2010 Work-
shop on Computational Neurolinguistics (Mur-
phy et al, 2010). Functional MRI (fMRI) data
was collected from 9 participants while they per-
formed a property generation task. The stimuli
were line-drawings, accompanied by their text
1http://www.cs.cmu.edu/afs/cs/project/theo-
73/www/science2008/data.html
label, of everyday concrete concepts, with 5 ex-
emplars of each of 12 semantic classes (mam-
mals, body parts, buildings, building parts,
clothes, furniture, insects, kitchen utensils, mis-
cellaneous functional artifacts, work tools, veg-
etables, and vehicles). Stimuli remained on
screen for three seconds, and each was each pre-
sented six times, in random order, to give a total
of 360 image presentations in the session.
The fMRI images were recorded with 3.0T
scanner at 1 second intervals, with a spatial reso-
lution of 3x3x6mm. The resulting data was pre-
processed with the SPM package (Friston et al,
2007); the blood-oxygen-level response was ap-
proximated by taking a boxcar average over a
sequence of brain images in each trial; percent
signal change was calculated relative to rest pe-
riods, and the data from each of the six repeti-
tions of each stimulus were averaged to yield a
single brain image for each concept. Finally, a
grey-matter anatomical mask was used to select
only those voxels (three-dimensional pixels) that
overlap with cortex, yielding approximately 20
thousand features per participant.
2.2 Models of semantics
Our objective is to compare current semantic
representations that get state-of-the-art perfor-
mance on the neuro-semantics task with repre-
sentative distributional models of semantics that
can be derived from arbitrary corpora, using
varying degrees of linguistic preprocessing. A
series of candidate models were selected to rep-
resent the variety of ways in which basic textual
features can be extracted and represented, in-
cluding token co-occurrence in a small local win-
dow, dependency parses of whole sentences, and
document co-occurrence, among others. Other
parameters were kept fixed in a way that the
literature suggests would be neutral to the var-
ious models, and so allow a fair comparison
among them (Sahlgren, 2006; Bullinaria and
Levy, 2007; Turney and Pantel, 2010).
All textual statistics were gathered from a set
of 50m English-language web-page documents
consisting of 16 billion words. Where a fixed
text window was used, we chose an extent of
?4 lower-case tokens either side of the target
115
word of interest, which is in the mid-range of
optimal values found by various authors (Lund
and Burgess, 1996; Rapp, 2003; Sahlgren, 2006).
Positive pointwise-mutual-information (1,2) was
used as an association measure to normalize
the observed co-occurrence frequency p(w, f) for
the varying frequency of the target word p(w)
and its features p(f). PPMI up-weights co-
occurrences between rare words, yielding posi-
tive values for collocations that are more com-
mon than would be expected by chance (i.e. if
word distributions were independent), and dis-
cards negative values that represent patterns of
co-occurrences that are rarer than one would ex-
pect by chance. It has been shown to perform
well generally, with both word- and document-
level statistics, in raw and dimensionality re-
duced forms (Bullinaria and Levy, 2007; Turney
and Pantel, 2010).2
PPMIwf =
{
PMIwf if PMIwf > 0
0 otherwise
(1)
PMIwf = log
(
p(w, f)
p(w)p(f)
)
(2)
A frequency threshold is commonly applied
for three reasons: low-frequency co-occurrence
counts are more noisy; PMI is positively bi-
ased towards hapax co-occurrences; and due
to Zipfian distributions a cut-off dramatically
reduces the amount of data to be processed.
Many authors use a threshold of approximately
50-100 occurrences for word-collocate models
(Lund and Burgess, 1996; Lin, 1998; Rapp,
2003). Since Bullinaria and Levy (2007) find
improving performance with models using pro-
gressively lower cutoffs we explored two cut-offs
of 20 and 50 which equate to low co-occurrences
thresholds of 0.00125 or 0.003125 per million re-
spectively; for the word-region model we chose
a threshold of 2 occurrences of a target term in
a document, to keep the input features to a rea-
sonable dimensionality (Bradford, 2008).
After applying these operations to the input
data from each model, the resulting dimension-
2Preliminary analyses confirmed that PPMI per-
formed as well or better than alternatives including log-
likelihood, TF-IDF, and log-entropy.
ality ranged widely, from about 500 thousand,
to tens of millions. A singular value decompo-
sition (SVD) was applied to identify the 1000
dimensions within each model with the great-
est explanatory power, which also has the ef-
fect of combining similar dimensions (such as
synonyms, inflectional variants, topically simi-
lar documents) into common components, and
discarding more noisy dimensions in the data.
Again there is variation in the number of di-
mension that authors use: here we experiment
with 300 and 1000. For decomposition we used
a sparse SVD method, the Implicitly Restarted
Arnoldi Method (Lehoucq et al, 1998; Jones
et al, 2001), which was coherent with the PPMI
normalization used, since a zero value repre-
sented both negative target-feature associations,
and those that were not observed or fell below
the frequency cut-off. We also streamlined the
task by reducing the input data C (of n target
words by m co-occurrence features) to a square
matrix CCT of size n ? n, taking advantage of
the equality of their left singular vectors U. For
SVD to generalize well over the many input fea-
tures, it is also important to have more training
cases that the small set of 60 concrete nouns
used in our evaluation task. Consequently we
gathered all statistics over a set of the 40,000
most frequent word-forms found in the Ameri-
can National Corpus (Nancy Ide and Keith Su-
derman, 2006), which should approximate the
scale and composition of the vocabulary of a
university-educated speaker of English (Nation
and Waring, 1997), and over 95% of tokens typ-
ically encountered in English.
2.2.1 Hand-tailored benchmarks
The state-of-the-art models on this brain ac-
tivity prediction task are both hand-tailored.
Mitchell et al (2008) used a model of seman-
tics based on co-occurrence in the Google 1T 5-
gram corpus of English (Brants and Franz, 2006)
with a small set of 25 Verbs chosen to rep-
resent everyday sensory-motor interaction with
concrete objects, such as see, move, listen. We
recreated this using our current parameters (web
document corpus, co-occurrence frequency cut-
off, PPMI normalization). The second hand-
116
tailored dataset we used was a set of Elicited
Properties inspired by the 20 Questions game,
and gathered using Mechanical Turk (Palatucci
et al, 2009; Palatucci, 2011). Multiple infor-
mants were asked to answer one or more of 218
questions ?related to size, shape, surface prop-
erties, and typical usage? such as Do you see
it daily?, Is it wild?, Is it man-made? with a
scalar response ranging from 1 to 5. The re-
sulting responses were then averaged over infor-
mants, and then the values of each question were
grouped into 5 bins, giving all dimensions simi-
lar mean and variance.
2.2.2 Word-Region Model
Latent Semantic Analysis (Deerwester et al,
1990; Landauer and Dumais, 1997), and its
probabilistic cousins (Blei et al, 2003; Grif-
fiths et al, 2007), express the meaning of a
word as a distribution of co-occurrence across
a set of documents, or other text-regions such
as paragraphs. This word-region matrix in-
stantiates the assumption that words that share
a topical domain (such as medicine, entertain-
ment, philosophy) would be expected to appear
in similar sub-sets of text-regions. In such a
model, the nearest neighbors of a target word
are syntagmatically related (i.e. appear along-
side each other), and for judge might include
lawyer, court, crime, or prison.
The Document model used here is loosely
based on LSA, taking the frequency of occur-
rence of each of our 40,000 vocabulary words
in each of 50 million documents as its input
data, and it follows Bullinaria and Levy (2007);
Turney and Pantel (2010) in using PPMI as a
normalization function. We have not investi-
gated variations on the decomposition algorithm
in any detail, such as those using non-negative
matrix factorization, probabilistic LSA or LDA
topic models, as the objective in this paper is
to provide a direct comparison between the dif-
ferent types of basic collocation information en-
coded in corpora, rather than evaluate the best
algorithmic means for discovering latent dimen-
sions in those co-occurrences. Nor have we eval-
uated performance on a more structured corpus
input (Pereira et al, 2011). However prelimi-
nary tests with the Wikipedia corpus, and with
LDA, using the Gensim package (Rehurek and
Sojka, 2010) yielded similar performances.
2.2.3 Word-Collocate Models
Word-collocate models make a complemen-
tary assumption to that of the document model:
that words with closely-related categorical or
taxonomic properties should appear in the same
position of similar sentences. In a basic word-
collocate model, based on a word-word co-
occurrence matrix, the nearest neighbors of
judge might be athlete, singer, or fire-fighter,
reflecting paradigmatic relatedness (i.e. substi-
tutability). Word-collocate models are further
differentiated by the amount of linguistic anno-
tation attached to word features, ranging from
simple word-form features in a fixed-width win-
dow around the concept word, to more elaborate
word sequence patterns and parses including
parts of speech and dependency relation tags.
Among these alternatives, we might expect a
dependency model to be the most powerful. In-
tuitively, the information that John is sentient
is similarly encoded in the text John likes cake
and John seems to really really like cake, and a
suitably effective parser should be able to gen-
eralize over this variation, to extract the same
dependency relationship of John-subject-like. In
contrast a narrow window-based model might
exclude informative features (such as like in the
second example), while including presumably
uninformative ones (such as really). However
parsers have the disadvantage of being computa-
tionally expensive (meaning that they typically
are applied to smaller corpora) and usually in-
troduce some noise through their errors. Conse-
quently, simpler window-based models have of-
ten been found to be as effective.
The most basic model considered is the
Word-Form model, in which all lower-case to-
kens (word forms and punctuation) found within
four positions left and right of the target word
are recorded, yielding simple features such as
{john, likes}. It may also be termed a ?flat?
model in contrast to those which assign a vari-
able weight to collocates, progressively lower as
one moves further than the target position (e.g.
117
Lund et al, 1995). We did not use a stop-list, as
Bullinaria and Levy (2007) found co-occurrence
with very high frequency words also to be infor-
mative for semantic tasks. We also expect that
the subsequent steps of normalizing with PPMI,
reduction with SVD, and use of regularised re-
gression should be able to recognize when such
high-frequency words are not informative and
then discount these, without the need for such
assumptions upfront.
The Stemmed model is a slight variation on
the Word-Form model, where the same statistics
are aggregated after applying Lancaster stem-
ming (Paice, 1990; Loper and Bird, 2002).
The Directional model, inspired by Schu?tze
and Pedersen (1993), is also derived from the
word-form model, but differentiates between co-
occurrence to the left or to the right of the target
word, with features such as {john L, cake R}.
The Part-of-Speech model (Kanejiya et al,
2003; Widdows, 2003) replaces each lower-
case word-token with its part-of-speech disam-
biguated form (e.g. likes VBZ, cake NN ). These
annotations were extracted from the full depen-
dency parse described below.
The Sequence model draws on a range of
work that uses word sequence patterns (Lin and
Pantel, 2001; Almuhareb and Poesio, 2004; Ba-
roni et al, 2010), and may also be considered an
approximation of models that use shallow syn-
tactic analysis (Grefenstette, 1994; Curran and
Moens, 2002). All distinct token sequences up
to length 4 either side of the target word were
counted.
Finally the Dependency model uses a full
dependency parse, which might be considered
the most informed representation of the word-
collocate relationships instantiated in corpus
sentences, and this approach has been used by
several authors (Lin, 1998; Pado? and Lapata,
2007; Baroni and Lenci, 2010). The features
used are pairs of dependency relation and lex-
eme corresponding to each edge linked to a tar-
get word of interest (e.g. likes subj ). The parser
used here was Malt, which achieves accuracies of
85% when deriving labelled dependencies on En-
glish text (Hall et al, 2007). The features pro-
duced by this module are much more limited,
to those words that have a direct dependency
relation with the word of interest.
2.3 Linear Learning Model
A linear regression model will allow us to eval-
uate how well a given model of word semantics
can be used to predict brain activity. We fol-
low the analysis in Mitchell et al (2008) and
subsequently adopted by several other research
groups (see Murphy et al, 2010). For each par-
ticipant and selected fMRI feature (i.e. each
voxel, which records the time-course of neural
activity at a fixed location in the brain), we train
a model where the level of activation of the latter
(the blood oxygenation level) in response to dif-
ferent concepts is approximated by a regularised
linear combination of their semantic features:
f = C? + ?||?||2 (3)
where f is the vector of activations of a spe-
cific fMRI feature for different concepts, the ma-
trix C contains the values of the semantic fea-
tures for the same concepts, ? is the vector of
weights we must learn for each of those (corpus-
derived) features, and ? tunes the degree of reg-
ularisation. We can illustrate this with a toy
example, containing several stimulus concepts
and their attributes on three semantic dimen-
sions: cat (+animate, -big, +moving); phone
(-animate, -big, -moving); elephant (+animate,
+big, +moving); skate-board (-animate, -big,
+moving). After training over all the voxels in
our fMRI data with this simple semantic model,
we can derive whole brain images that are typ-
ical of each of the semantic dimensions. The
power of the model is its ability to predict ac-
tivity for concepts that were not in the training
set ? for instance the brain activation elicited by
the word car might be approximated by combin-
ing the images see for -animate, +big, +moving,
even though this combination of properties was
not observed during training.
The linear model was estimated with a
least squared errors method and L2 regularisa-
tion, selecting the lambda parameter from the
range 0.0001 to 5000 using Generalized Cross-
Validation (see Hastie et al, 2011, p.244). The
118
activation of each fMRI voxel in response to a
new concept that was not in the training data
was predicted by a ?-weighted sum of the val-
ues on each semantic dimension, building a pic-
ture of expected the global neural activity re-
sponse for an arbitrary concept. Again follow-
ing Mitchell et al (2008) we use a leave-2-out
paradigm in which a linear model for each neu-
ral feature is trained in turn on all concepts mi-
nus 2, having selected the 500 most stable voxels
in the training set using the same correlational
measure across stimulus presentations. For each
of the 2 left-out concepts, we predict the global
neural activation pattern, as just described. We
then try to correctly match the predicted and
observed activations, by measuring the cosine
distance between the model-generated estimate
of fMRI activity and the that observed in the ex-
periment. If the sum of the matched cosine dis-
tances is lower than the sum of the mismatched
distances, we consider the prediction successful
? otherwise as failed. At chance levels, expected
matching accuracy is 50%, and significant per-
formance above chance can be estimated using
the binomial test, once variance had been veri-
fied over independent trials (i.e. where no single
stimulus concept is shared between pairs).
3 Results
Table 1 shows the main results of the leave-
two-out brain-image matching task. They show
the mean classification performance over 1770
word pairs (60 select 2) by 9 participants. All of
these classification accuracies are highly signif-
icant at p  0.001 over test trials (binomial,
chance 50%, n=1770*9) and p < 0.001 over
words (binomial, chance 50%, n=60). There
were some significant differences between mod-
els when making inferences over trials, but for
the small set of words used here it is not possible
to make firm conclusions about the superiority
of one model over the other, that could be confi-
dently expected to generalize to other stimuli or
experiments. However, we do achieve classifica-
tion accuracies that are as high, or higher than
any previously published (Palatucci et al, 2009;
Pereira et al, 2011), while models based on very
Semantic Models Features Accuracy
25 Verbs 25 78.5
Elicited Properties 218 83.5
Document (f2) 1000 76.2
Word Form 1000 80.0
Stemmed 1000 76.2
Direction 1000 80.2
Part-of-Speech 1000 80.0
Sequence 1000 78.5
Dependency 1000 83.1
Table 1: Brain activity prediction accuracy on leave-
2-out pair-matching task. A frequency cutoff of 20
was used for all 1000 dimensional models.
Semantic Models 300 Feats. 1000 Feats.
Document (f2) 79.9 76.2
Word Form 78.1 80.0
Stemmed 77.9 76.2
Direction 80.0 80.2
Part-of-Speech 77.9 80.0
Sequence 72.9 78.5
Dependency 81.6 83.1
Table 2: Effect of SVD dimensionality in the leave-
2-out pair-matching setting; frequency cutoff of 20.
different basic features (directional word-forms;
dependency relations; document co-occurrence)
yield very similar performance.
3.1 Effect of Number of Dimensions
Here we evaluate what effect the number of SVD
dimensions used has on the final performance
of various semantic models. Experimental re-
sults comparing 300 and 1000 dimensions are
presented in Table 2, all based on a frequency
cutoff of 20. We observe that performance im-
proves in 5 out of 7 semantic models compared,
with the highest performance achieved by the
Dependency model when 1000 SVD dimensions
were used.
3.2 Effect of Frequency Cutoff
In this section, we evaluate what effect frequency
cutoff has on the brain prediction accuracy of
various semantic models. From the results in
Table 3, we observe only marginal changes as
the frequency cutoff varied from 20 to 50. This
suggests that the semantic models of this set of
119
Semantic Models Cutoff = 50 Cutoff = 20
Document (f2) 79.9 79.9
Word Form 78.5 78.1
Stemmed 78.2 77.9
Direction 80.8 80.0
Part-of-Speech 77.5 77.9
Sequence 74.4 72.9
Dependency 81.3 81.6
Table 3: Effect of frequency cutoff in the leave-2-out
pair-matching setting; 300 SVD dimensions.
words are not very sensitive to variations in the
frequency cutoff under current experimental set-
tings, and do not benefit clearly from the de-
crease in sparsity and increase in noise that a
lower threshold produces.
3.3 Information Overlap Analysis
To verify that the models are in fact substan-
tially different, we performed a follow-on analy-
sis that measured the informational overlap be-
tween the corpus-derived models. Given two
models A and B, both with dimensionality 40
thousand words by 300 SVD dimensions, we can
evaluate the extent to which A (used as the
predictor semantic representation) contains the
information encoded in B (the explained rep-
resentation). As shown in (4), for each SVD
component c, we take the left singular vector
bc as a dependent variable and fit it with a lin-
ear model, using the matrix A (all left singu-
lar vectors) as independent variables. The ex-
plained variance for this column is weighted by
its squared singular value s2c in B, and the sum of
these component-wise variances gives the total
variance explained R2A?B.
R2A?B =
300?
c=1
s2c?
s2c
RA?bc (4)
Figure 1 indicates that the first three models,
which are all derived from token occurrences in a
?4 window, are close to identical. The sequence
and document models are relatively dissimilar,
and the dependency model occupies a middle
ground, with some similarity to all the models.
It is also interesting to note that the among the
first cluster of word-form derived models, the
Figure 1: Informational Overlap between Corpus-
Derived Datasets, in R2
directional one has the highest similarity to the
dependency model.
4 Conclusion
The main result of this study was that we
achieved classification accuracies as high as
any published, and within a fraction of a per-
centage point of the human benchmark 20
Questions data, using completely unsupervised,
data-driven models of semantics based on a large
random sample of web-text. The most linguisti-
cally informed among the models (and so, per-
haps the most psychologically plausible), based
on dependency parses, is the most successful.
Still the performance of sometimes radically dif-
ferent models, from Document-based (syntag-
matic) and Word-Form-based (paradigmatic), is
surprisingly similar. One reason for this may be
that we have reached a ceiling in performance
on the fMRI data, due to its inherent noise ? in
this regard it is interesting to note that an at-
tempt to classify individual concepts using this
data directly, without an intervening model of
semantics, also achieves about 80% (though on a
different task, Shinkareva et al, 2008). Another
possible explanation is that both methods reveal
equivalent sets of underlying semantic dimen-
sions, but figure 1 suggests not. Alternatively,
it may be that the small set of 60 words exam-
ined here may be as well-distinguished by means
120
of their taxonomic differences, as by their top-
ical differences, a suggestion supported by the
results in Pereira et al (2011, see Figure 2A).
From the perspective of computational effi-
ciency however, some of the models have clearer
advantages. The Dependency and Part-of-
Speech models are processing-intensive, since
the broad vocabulary considered requires that
the very large quantities of text pass through
a parsing or tagging pipeline (though these
tasks can be parallelized). The Sequence and
Document models conversely require very large
amounts of memory to store all their features
during SVD. In comparison, the Direction model
is impressive, as it achieves close to optimal per-
formance, despite being very cheap to produce
in terms of processor time and memory foot-
print. Its relatively superior performance may
be due to the relatively fixed word-order of En-
glish, making it a good approximation of a De-
pendency model. For instance, given the nar-
row ?4 token windows used here, the Direction
features shaky Left and donate Right (relative
to a target noun) are probably nearly identical
to the Dependency features shaky Adj and do-
nate Subj. The Sequence model might also be
seen as an approximate Dependency model, but
one with the addition of more superficial colloca-
tions such as ?fish and chips? or ?Judge Judy?,
which are less relevant to our semantic task.
The evidence for the influence of the scal-
ing parameters (number of SVD dimensions,
frequency cutoff) is mixed: cut-off appears to
have little effect either way, and increasing the
number of dimensions can help or hinder (com-
pare the Sequence and Document models). We
can speculate that the Document model is al-
ready ?saturated? with 300 dimensions/topics,
but that the other models based on properties
have a higher inherent dimensionality. It may
also be a lower cut-off and higher dimensional-
ity would show clearer benefits over a larger set
of semantic/syntactic domains, including lower-
frequency words (the lowest frequency work in
the set of 60 used here was igloo, which has an
incidence of 0.3 per million words in the ANC).
PPMI appears to be both effective, and par-
simonious with assumptions one might make
about conceptual representations, where it
would be cognitively onerous and unnecessary
to encode all negative features (such as the facts
that dogs do not have wheels, are not commu-
nication events, and do not belong in the avi-
ation domain). But while SVD is certainly ef-
fective in dealing with the pervasive synonymy
and polysemy seen in corpus-feature sets, it is
less clear that it reveals psychologically plausi-
ble dimensions of meaning. Alternatives such as
non-negative matrix factorization (Lee and Se-
ung, 1999) or Latent Dirichlet Allocation (Blei
et al, 2003) might extract more readily inter-
pretable dimensions; or alternative regularisa-
tion methods such as Elastic Nets, Lasso (Hastie
et al, 2011), or Network Regularisation (Sandler
et al, 2009) might even be capable of identifying
meaningful clusters of features when learning di-
rectly on co-occurrence data. Finally, we should
consider whether more derived datasets could be
used as input data in place of the basic corpus
features used here, such as the full facts learned
by the NELL system (Carlson et al, 2010), or
crowd-sourced data which can be easily gathered
for any word (e.g. association norms, Kiss et al,
1973), though different algorithmic means would
be needed to deal with their extreme degree of
sparsity.
The results also suggest a series of follow-on
analyses. A priority should be to test these
models against a wider range of neuroimaging
data modalities (e.g. MEG, EEG) and stim-
ulus sets, including abstract kinds (see Mur-
phy et al 2012, for a preliminary study), and
parts-of-speech beyond nouns. It may be that a
putative complementarity between word-region
and word-collocate models is only revealed when
we look at a broader sample of the human
lexicon. And beyond establishing what infor-
mational content is required to make semantic
distinctions, other factorisation methods (e.g.
sparse or non-negative decompositions) could be
applied to yield more interpretable dimensions.
Other classification tasks might also be more
sensitive for detecting differences between mod-
els, such as the test of word identification among
a set by rank accuracy, as used in (Shinkareva
et al, 2008).
121
References
Almuhareb, A. and Poesio, M. (2004). Attribute-
based and value-based clustering: An evaluation.
In Proceedings of EMNLP, pages 158?165.
Baroni, M. and Lenci, A. (2010). Distributional
Memory : A General Framework for Corpus-Based
Semantics. Computational Linguistics, 36(4):673?
721.
Baroni, M., Murphy, B., Barbu, E., and Poesio, M.
(2010). Strudel: A corpus-based semantic model
based on properties and types. Cognitive Science,
34(2):222?254.
Battig, W. F. and Montague, W. E. (1969). Cate-
gory Norms for Verbal Items in 56 Categories: A
Replication and Extension of the Connecticut Cat-
egory Norms. Journal of Experimental Psychology
Monographs, 80(3):1?46.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003).
Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3(4-5):993?1022.
Bradford, R. B. (2008). An empirical study of re-
quired dimensionality for large-scale latent seman-
tic indexing applications. Proceeding of the 17th
ACM conference on Information and knowledge
mining CIKM 08, pages 153?162.
Brants, T. and Franz, A. (2006). Web 1T 5-gram
Version 1.
Bullinaria, J. A. and Levy, J. P. (2007). Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510?526.
Carlson, A., Betteridge, J., Kisiel, B., Settles, B.,
Jr., E. R. H., and Mitchell, T. M. (2010). To-
ward an Architecture for Never-Ending Language
Learning. Artificial Intelligence, 2(4):3?3.
Chang, K.-m. K., Mitchell, T., and Just, M. A.
(2011). Quantitative modeling of the neural repre-
sentation of objects: how semantic feature norms
can account for fMRI activation. NeuroImage,
56(2):716?727.
Curran, J. R. and Moens, M. (2002). Improvements
in automatic thesaurus extraction. In SIGLEX,
pages 59?66.
Deerwester, S., Dumais, S., Landauer, T., Furnas,
G., and Harshman, R. (1990). Indexing by La-
tent Semantic Analysis. Journal of the American
Society of Information Science, 41(6):391 ? 407.
Devereux, B. and Kelly, C. (2010). Using fMRI ac-
tivation to conceptual stimuli to evaluate meth-
ods for extracting conceptual representations from
corpora. In Murphy, B., Korhonen, A., and
Chang, K. K.-M., editors, 1st Workshop on Com-
putational Neurolinguistics.
Friston, K. J., Ashburner, J. T., Kiebel, S. J.,
Nichols, T. E., and Penny, W. D. (2007). Statis-
tical Parametric Mapping: The Analysis of Func-
tional Brain Images, volume 8. Academic Press.
Grefenstette, G. (1994). Explorations in Automatic
Thesaurus Discovery. Kluwer, Dordrecht.
Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B.
(2007). Topics in semantic representation. Psy-
chological Review, 114(2):211?244.
Hall, J., Nilsson, J., Nivre, J., Eryigit, G., Megyesi,
B., Nilsson, M., and Saers, M. (2007). Single Malt
or Blended? A Study in Multilingual Parser Op-
timization. CoNLL Shared Task Session, pages
933?939.
Hastie, T., Tibshirani, R., and Friedman, J. (2011).
The Elements of Statistical Learning, volume 18 of
Springer Series in Statistics. Springer, 5th edition.
Jelodar, A. B., Alizadeh, M., and Khadivi, S. (2010).
WordNet Based Features for Predicting Brain Ac-
tivity associated with meanings of nouns. In Mur-
phy, B., Korhonen, A., and Chang, K. K.-M., ed-
itors, 1st Workshop on Computational Neurolin-
guistics, pages 18?26.
Jones, E., Oliphant, T., Peterson, P., and Et Al.
(2001). SciPy: Open source scientific tools for
Python.
Kanejiya, D., Kumar, A., and Prasad, S. (2003).
Automatic evaluation of students? answers using
syntactically enhanced LSA. Building educational
applications, NAACL, 2:53?60.
Kiss, G. R., Armstrong, C., Milroy, R., and Piper, J.
(1973). An associative thesaurus of English and its
computer analysis. In Aitken, A. J., Bailey, R. W.,
and Hamilton-Smith, N., editors, The Computer
and Literary Studies. Edinburgh University Press.
Landauer, T. and Dumais, S. (1997). A solution to
Plato?s problem: the latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Lee, D. D. and Seung, H. S. (1999). Learning the
parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788?91.
Lehoucq, R. B., Sorensen, D. C., and Yang, C.
(1998). Arpack users? guide: Solution of large
scale eigenvalue problems with implicitly restarted
Arnoldi methods. SIAM.
122
Lin, D. (1998). Automatic Retrieval and Clustering
of Similar Words. In COLING-ACL, pages 768?
774.
Lin, D. and Pantel, P. (2001). DIRT ? discovery
of inference rules from text. Proceedings of the
seventh ACM SIGKDD international conference
on Knowledge discovery and data mining KDD 01,
datamining:323?328.
Loper, E. and Bird, S. (2002). {NLTK}: The natu-
ral language toolkit. In ACL Workshop, volume 1,
pages 63?70. Association for Computational Lin-
guistics.
Lund, K. and Burgess, C. (1996). Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28:203?208.
Lund, K., Burgess, C., and Atchley, R. (1995). Se-
mantic and associative priming in high dimen-
sional semantic space. In Proceedings of the 17th
Cognitive Science Society Meeting, pages 660?665.
Mitchell, T. M., Shinkareva, S. V., Carlson, A.,
Chang, K.-M., Malave, V. L., Mason, R. A., and
Just, M. A. (2008). Predicting Human Brain Ac-
tivity Associated with the Meanings of Nouns. Sci-
ence, 320:1191?1195.
Murphy, B., Baroni, M., and Poesio, M. (2009). EEG
responds to conceptual stimuli and corpus seman-
tics. In Proceedings of EMNLP, pages 619?627.
ACL.
Murphy, B., Korhonen, A., and Chang, K. K.-
M., editors (2010). Proceedings of the 1st Work-
shop on Computational Neurolinguistics, NAACL-
HLT, Los Angeles. ACL.
Murphy, B., Talukdar, P., and Mitchell, T. (2012).
Comparing Abstract and Concrete Conceptual
Representations using Neurosemantic Decoding.
In NAACL Workshop on Cognitive Modelling and
Computational Linguistics.
Nancy Ide and Keith Suderman (2006). The Amer-
ican National Corpus First Release. Proceedings
of the Fifth Language Resources and Evaluation
Conference (LREC).
Nation, P. and Waring, R. (1997). Vocabulary size,
text coverage and word lists. In Schmitt, N. and
McCarthy, M., editors, Vocabulary Description ac-
quisition and pedagogy, pages 6?19. Cambridge
University Press.
Pado?, S. and Lapata, M. (2007). Dependency-based
construction of semantic space models. Computa-
tional Linguistics, 33(2):161?199.
Paice, C. D. (1990). Another stemmer. SIGIR Fo-
rum, 24(3):56?61.
Palatucci, M., Hinton, G., Pomerleau, D., and
Mitchell, T. M. (2009). Zero-Shot Learning with
Semantic Output Codes. Advances in Neural In-
formation Processing Systems, 22:1?9.
Palatucci, M. M. (2011). Thought Recognition: Pre-
dicting and Decoding Brain Activity Using the
Zero-Shot Learning Model. PhD thesis, Carnegie
Mellon University.
Pereira, F., Detre, G., and Botvinick, M. (2011).
Generating Text from Functional Brain Images.
Frontiers in Human Neuroscience, 5:1?11.
Rapp, R. (2003). Word Sense Discovery Based on
Sense Descriptor Dissimilarity. Proceedings of the
Ninth Machine Translation Summit, pp:315?322.
Rehurek, R. and Sojka, P. (2010). Software Frame-
work for Topic Modelling with Large Corpora. In
New Challenges, LREC 2010, pages 45?50. ELRA.
Rubenstein, H. and Goodenough, J. B. (1965). Con-
textual correlates of synonymy. Communications
of the ACM, 8(10):627?633.
Sahlgren, M. (2006). The Word-Space Model: Using
distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Dissertation, Stock-
holm University.
Sandler, T., Talukdar, P. P., Ungar, L. H., and
Blitzer, J. (2009). Regularized Learning with Net-
works of Features. Advances in Neural Informa-
tion Processing Systems 21, 4:1401?1408.
Schu?tze, H. and Pedersen, J. (1993). A Vector Model
for syntagmatic and paradigmatic relatedness. In
Making Sense of Words Proceedings of the 9th
Annual Conference of the University of Waterloo
Centre for the New OED and Text Research, pages
104?113.
Shinkareva, S. V., Mason, R. A., Malave, V. L.,
Wang, W., Mitchell, T. M., and Just, M. A.
(2008). Using fMRI Brain Activation to Iden-
tify Cognitive States Associated with Perception
of Tools and Dwellings. PloS ONE, 3(1).
Turney, P. D. and Pantel, P. (2010). From Frequency
to Meaning: Vector Space Models of Semantics.
Artificial Intelligence, 37(1):141?188.
Widdows, D. (2003). Unsupervised methods for de-
veloping taxonomies by combining syntactic and
statistical information. In NAACL, pages 197?
204. Association for Computational Linguistics.
123
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 36?44,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Detecting Semantic Category in
Simultaneous EEG/MEG Recordings
Brian Murphy
Centre for Mind/Brain Sciences
University of Trento
corso Bettini 31,
38068 Rovereto, Italy
brian.murphy@unitn.it
Massimo Poesio
Centre for Mind/Brain Sciences
University of Trento
corso Bettini 31,
38068 Rovereto, Italy
massimo.poesio@unitn.it
Abstract
Electroencephalography (EEG) and magne-
toencephalography (MEG) are closely related
neuroimaging technologies that both measure
summed electrical activity of synchronous
sources of neural activity. However they dif-
fer in the portions of the brain to which they
are more sensitive, in the frequency bands they
can detect, and to the amount of noise to which
they are subject. Since semantic representa-
tions are thought to be widely distributed in
the brain, this preliminary study considered
if the broader coverage offered by simulta-
neous EEG/MEG recordings would increase
sensitivity to these cognitive states. The re-
sults showed that MEG data allowed stim-
uli in two semantic categories (mammals and
tools) to be distinguished more accurately, de-
spite some experimental settings that were op-
timised for EEG. The addition of EEG data
did not prove informative, indicating that it
may be redundant relative to MEG, even when
using dimensionality reduction techniques to
combat overfitting.
1 Introduction
Electroencephalography (EEG) and magnetoen-
cephalography (MEG) are similar methods for
recording activity in the brain. Both detect signals
that are produced by the mixing of neural sources,
where each source represents macro-scale synchro-
nisation between the firing of individual neurons.
The sum of these activities induce voltages at the
scalp that are recorded with EEG, and magnetic
fields that are detected with MEG. But the signals
yielded by each technique are not identical for sev-
eral reasons. EEG signals are heavily attenuated
and filtered (both in time in space) by the passage
through skull and tissue. As a result, MEG signals
are less noisy, have finer spatial resolution, capture
a wider range of frequencies, and so have the po-
tential to be more informative. Further, the signal
footprint of MEG and EEG signals on the brain is
not the same: EEG sensors are more sensitive to
currents that are radial to the scalp and so predomi-
nantly detect activity in the at the top of gyri and the
bottom of sulci (the top and bottom of folds in the
surface of the brain); while MEG is more sensitive
to currents that are tangential to the scalp, and so
detects more activity in the side walls of sulci. The
high spatial resolution of MEG means that it cannot
see as deeply into the brain as EEG can. Finally,
MEG sensors of different types (in this case mag-
netometers and planar gradiometers) are sensitive to
magnetic fields of different orientations (see Figure
1): planar gradiometers are most sensitive to current
generators of a particular orientation directly under
the sensor position; magnetometers record genera-
tors that are tangential and peripheral to the sensor
area.
The distribution of sensor coverage may be im-
portant for the decoding of semantic categories in
particular. Neuroimaging evidence suggests that se-
mantic representations may be widely distributed in
the brain. For example there are well-established
differences in neural activity in the fusiform gyrus
that correspond to higher level categories (natural
vs non-natural kinds; people vs places - see e.g.
Chao et al, 2002); there is also evidence that the
36
Figure 1: Schematic from above of selective sensitivity
of three co-located MEG sensors
Left and centre panels show perpendicular planar gradiometers;
right panel shows magnetometer. A co-located EEG electrode
would be most sensitive to currents perpendicular to the scalp.
Image courtesy of Elekta AB.
meaning of bodily actions is encoded in the motor-
cortex (Pulvermu?ller, 2005); and concepts associ-
ated with eating (e.g. foodstuffs) seem to be repre-
sented at least in part by activations in gustatory cor-
tex (Mitchell et al, 2008; Just et al, 2010). Hence
a wide coverage of sensors that are sensitive to dif-
ferent but overlapping portions of brain tissue may
provide a fuller description of semantic memories.
Given the fact that it has been possible to decode
conceptual categories and language semantics from
EEG signals (Murphy et al, 2008, 2009), the ques-
tion is if MEG signals can be shown to be more in-
formative. Similar studies on lower-level tasks typi-
cally used in brain-computer interfaces suggests that
it may be: Hill et al (2006) find that there is a
modest increase in the decoding accuracy on imag-
ined motor activity with MEG, relative to EEG, and
Waldert et al (2008) have similar findings detecting
the direction of hand movements.
A related question is whether the information sup-
plied by EEG and MEG is complementary, and if
so how best it should be combined. This depends
critically on the number of signals used: raising the
number of input signals increases the information
supplied to the machine learning methods, but in-
teracts with their tendency to overfit, if the number
of descriptive dimensions (recorded signals) is of a
similar order of magnitude to the number of training
cases (experimental trials in which a stimulus is pre-
sented). This is often the case with data from neu-
roimaging experiments, as there are practical limita-
tions on the number of data points that can be col-
lected: individual stimuli must usually be separated
by several seconds so that neural signals can return
to baseline between each, and participants can usu-
ally only be expected to perform a task at full at-
tention for 60 minutes or so, in such experimental
environments.
To investigate this question, we replicated an ex-
isting EEG experiment (Murphy et al, 2010). In that
experiment participants had been presented with im-
ages of animals and tools, while EEG activity was
recorded at 64 standard 10-10 locations, and sin-
gle trials (stimulus presentations) could be classi-
fied as representing the category of animal or tool
with an average accuracy of 72% over all seven
participants. The classification methods used were
an adaptive time/frequency window optimisation
(Dalponte et al, 2007), a supervised spatial com-
ponent signal decomposition (Common Spatial Pat-
terns, Koles et al, 1990) that yielded measures of
neural activity based on signal power, and a support-
vector machine (Boser et al, 1992).
The replication experiment reported here was car-
ried out with two participants, and used the same
task and materials, while simultaneously recording
with a 306-channel MEG system (204 gradiometers,
102 magnetometers) and a high-density 124-channel
EEG system. This data was then analysed using the
same machine learning methods as previously, but
varying the number and type of input signals, and
using dimensionality reduction to address increased
dimensionality.
2 Methods
2.1 Experiment and Materials
Two male native speakers of Italian took part in
the study, aged 30 and 47. Both were right-handed
with corrected or normal vision. Participants in this
study receive compensation of 7 euros per hour. The
experiment is conducted under the approval of the
ethics committee at the University of Trento, and
participants gave informed consent.
The participants were asked to perform a silent
naming task on grey-scale images of 30 land-
mammals and 30 work tools. Each stimulus was
presented between four and six times, in randomised
order.1 The participants sat in a relaxed upright posi-
1Participant 1 saw 264 stimulus trials (144 mammal and 120
tool trials); participant 2 saw 360 (180 in each class).
37
tion 1.5m from a projector screen in moderate light-
ing conditions. Images were presented on a medium
grey background and fell within a 6 degree viewing
angle. The task duration was split into five blocks
and the participants were given the choice to pause
between each. The cumulative task time did not ex-
ceed 45 minutes.
Each trial began with the presentation of a fixa-
tion cross for 0.25s, followed by the stimulus image,
a further fixation cross for 0.75s and a blank screen
for 1s. Participants were instructed to silently name
the object represented in their native tongue (Ital-
ian), using the first appropriate label that came to
mind, and to press the keyboard space-bar with the
left-hand to indicate they had found an appropriate
word. If the participant could not think of a suitable
label, they were asked not to make a response. The
image remained on the screen until the participant
responded, or until a time-out of three seconds was
reached. The participants were asked to keep still
during the task, and to avoid eye-movements and fa-
cial muscle activity in particular, except during the
blank period.
The materials were chosen to represent well-
defined semantic categories and to minimise non-
semantic, associative confounds. The set of 30 land
mammals were chosen to be both non-domesticated
and non-threatening, to avoid emotional valence
whether positive (e.g. pets) or negative (e.g. preda-
tors). Thirty hardware and garden implements were
chosen as genuine work tools. Appropriate pho-
tographs were sourced from the internet, and nor-
malised visually: each image file measured 300 pix-
els square; the image proper was converted to grey-
scale, superimposed on a homogeneous light-grey
background and had maximal horizontal and vertical
dimensions of 250 pixels; image contrast was nor-
malised. The concepts represented are listed below.
Land Mammals ant-eater, armadillo, badger,
beaver, bison, boar, camel, chamois, chim-
panzee, deer, elephant, fox, giraffe, gorilla,
hare, hedgehog, hippopotamus, ibex, kan-
garoo, koala, llama, mole, monkey, mouse,
otter, panda, rhinoceros, skunk, squirrel, zebra
(Italian formichiere, armadillo, tasso, castoro,
bisonte, cinghiale, cammello, camoscio, scim-
panz, cervo, elefante, volpe, giraffa, gorilla,
coniglio, riccio, ippopotamo, stambecco,
canguro, koala, lama, talpa, scimmia, topo,
lontra, panda, rinoceronte, puzzola, scoiattolo,
zebra)
Work Tools Allen key, axe, chainsaw, craft-knife,
crowbar, file, garden fork, garden trowel, hack-
saw, hammer, mallet, nail, paint brush, paint
roller, penknife, pick-axe, plaster trowel, pliers,
plunger, pneumatic drill, power-drill, rake, saw,
scissors, scraper, screw, screwdriver, sickle,
spanner, tape-measure (Italian brugola, ascia,
motosega, taglierino, piede di porco, lima,
forcone, paletta, seghetto, martello, mazza,
chiodo, pennello, rullo, coltellino svizzero, pic-
cone, cazzuola, pinza, stura lavandini, martello
pneumatico, trapano, rastrello, sega, forbici,
spatola, vite, cacciavite, falce, chiave inglese,
metro)
2.2 Neural Recordings
The experiment was conducted at the LNiF imag-
ing laboratories at the University of Trento, using a
306-sensor Elekta Neuromag system (2 planar gra-
diometers and 1 magnetometer at each of 102 sensor
locations). A dense-coverage 124-electrode EEG
cap was used also, using a right mastoid reference
and forehead ground. Both sets of signals were
recorded simultaneously at 1000Hz in a magneti-
cally shielded room. At the start of the session the
relative positions of the MEG and EEG sensors were
determined using a Polyhemus 3-D digitisation sys-
tem.
Data preprocessing was conducted using the
MNE, FieldTrip and EEGLAB packages.2 The data
was band-pass filtered at 1-50Hz to remove slow
drifts in the signal and high-frequency noise, and
then down-sampled to 125Hz. Eye and muscle arte-
facts were not removed, but these lie outside the
range of frequencies that were considered in the
analysis described below.
2Martinos Centre for Biomedical Imaging
(http://www.nmr.mgh.harvard.edu/martinos/); Don-
ders Institute for Brain, Cognition and Behaviour
(http://www.ru.nl/neuroimaging/fieldtrip); and Schwartz Center
for Computational Neuroscience (http://sccn.ucsd.edu/eeglab/)
respectively.
38
2.3 Analysis
The analysis method first applies a time/frequency
filter to select an information-rich band and inter-
val for the distinction of interest; a supervised de-
composition to extract components of whole-scalp
synchronous activity that are sensitive to this class
distinction (Common Spatial Patterns, or CSP ? see
Parra et al, 2005; Model and Zibulevsky, 2006;
Philiastides et al, 2006 for examples of other ap-
plications to cognitive neuroscience); and a gen-
eral purpose machine learning algorithm (Support-
Vector Machine or SVM) that uses the resulting
measures of signal power to predict the semantic
class of each trial. Individual trial epochs are arbi-
trarily allocated to one of k interlaced partitions of
equal size in a k-fold training/evaluation procedure.
The time/frequency filter applied here was
adopted from the earlier experiment, as it had been
found to provide optimal separation between trials
of the two classes over the participants of that study.
Using this common window (4-18Hz, 95-360ms af-
ter image onset) allows direct comparison between
the informativity of each type of sensor, or combina-
tion of sensor types. However this may disadvantage
MEG, since it is more sensitive to higher frequency
activity (> 50Hz), which at least one study has
found to vary systematically with semantic classes
(Tanji et al, 2005).
The decomposition method used, CSP (Koles
et al, 1990), extracts spatial components of elec-
trophysiological activity (linear combinations of raw
signals) that correspond to synchronous neural sub-
assemblies. It is a supervised technique that yields
signals whose level of activity (measured as signal
power) is modulated by the binary class distinction
of interest ? that is signals that show high power
when processing mammal concepts, and low power
when processing tool concepts, or vice-versa. CSP
identifies C components (where C is the number of
input channels) that are ranked by their sensitivity
to the class-separation of interest, in terms of op-
timal variance for the two populations of signals
(i.e., high variance between classes and low vari-
ance within classes). In this case we selected the
first and the last rows of this matrix (Ramoser et al,
2000) as the components that are most representa-
tive for the classes mammals and tools, respectively.
This procedure can be interpreted as extracting the
event-related spectral activity (i.e. the relative event-
related synchronisation) of two synchronous neural
structures which have been found to have an op-
timally differential response to the semantic cate-
gories of interest.
The final categorisation step is based on a
Support-Vector Machine (SVM) classifier (Boser
et al, 1992; Vapnik, 1998). The input for each
trial consisted of two measures of neural activity
extracted from the category-sensitive signal compo-
nents: the variance of the waveform, which is pro-
portional to signal power. The features were further
normalised by taking the log, and scaling to a range
of -1 to +1 across all trials. The SVM implementa-
tion used was LIBSVM (Chang and Lin, 2001), and
default parameters were used (radial basis function
kernel, cost parameter of 1, and a gamma value of
the inverse of the number of data-points).3 Test and
training data were kept strictly separate at all stages
of analysis.
In the results that follow here, these techniques
were first applied as before to replicate the previ-
ous experiment, but then also with an additional
step of dimensionality reduction to address the over-
fitting we expected given the dramatically larger
number of input channels (up to 430 if all EEG
and MEG channels were used, compared to 64
channels in the previous experiment). The signal
recorded in any individual channel will be com-
prised of a mix of genuine neural activity (both
relevant and irrelevant to our classification task),
systematic noise sources (e.g. 50Hz electrical line
noise, eye-movement artefacts, heart-beat artefacts),
and additional random noise. And as EEG and MEG
channels record activity from partially overlapping
portions of brain tissue, there is considerable re-
dundancy between neighbouring channels. Princi-
ple Components Analysis (PCA) is a dimensionality
reduction technique that addresses both these issues,
grouping redundant activity into the first (strongest)
3No optimisation of SVM parameters was attempted, as ex-
tensive parameter testing in the earlier experiment did not yield
any improvements in classification performance. We believe
that this is because CSP is in itself a powerful data-mining tech-
nique, that here typically yields two simple clusters of data cor-
responding to each semantic category. We expect a simple lin-
ear classifier would have similar performance on this task.
39
components, and relegating random noise to the last
components. Where PCA was used, it was ap-
plied directly before the CSP-based extraction of
category-specific sources.
3 Results
In the previous EEG experiment, the classification
accuracy averaged 72%, but varied substantially
from one participant to the next, ranging from 56%
to 80%. First we wanted to establish how repre-
sentative these two new simultaneous MEG/EEG
sessions had been, by replicating the EEG-based
analysis. To do this, an arbitrary subset of the 60
EEG channels were selected (taking roughly every
second channel among the total of 124), the stan-
dard time/frequency filter window was applied, and
the resulting data was classified using a 5-fold test-
training procedure.4 The first participant?s data was
typical of the previous cohort, classifying with accu-
racy of 70% (in this session, accuracy over 61% is
significant at p < 0.05, using a one-sided binomial
test, n = 264, p = 0.54), while the second partici-
pant?s data only achieved 52% accuracy (accuracy
over 56% significant at p< 0.05, n= 360, p= 0.5).
To get a first impression of the relative informa-
tivity of each signal type, the same procedure was
performed with subsets of 60 MEG channels: mag-
netometers alone yielded markedly higher results
(78% and 61% for participants 1 and 2 respectively),
while planar gradiometers alone gave marginally
lower results (67% and 48% respectively).
Next, to examine the effect of increasing the
amount of input data, we performed these analyses
using all available channels of each type. In one case
(participant 1, magnetometers) there was a drop in
5% points, and another (participant 2, magnetome-
ters) an increase of 3% points, but generally this had
little effect on results, indicating that in most cases
any increase in available information was offset by
overfitting.
These results are summarised in the first two
columns of in Tables 1 and 2. The tables also show
4In each test/training partition, the labelled training data
alone was used to derive two category specific scalp-maps.
These scalp-maps were used to extract signal components and
resulting signal power measures for all trials. The data was then
partitioned again along the same folds for SVM training and
prediction.
Table 1: Classification accuracy, participant 1
Type (available signals) 60 ch. all ch. 60 cp.
EEG (124) 70% 69% 76%
Magnetometers (102) 78% 73% 78%
Gradiometers (204) 67% 66% 71%
Mag.+Grad. (306) 72% 63% 77%
EEG+Mag. (224) 68% 67% 77%
EEG+Grad. (328) 69% 54% 73%
EEG+Mag.+Grad. (430) 72% 55% 77%
ch: raw channel input; cp: PCA component input
significance: 61% at p < 0.05; 65% at p < 0.001
Table 2: Classification accuracy, participant 2
Type (available signals) 60 ch. all ch. 60 cp.
EEG (124) 52% 50% 52%
Magnetometers (102) 61% 64% 68%
Gradiometers (204) 48% 51% 60%
Mag.+Grad. (306) 63% 50% 56%
EEG+Mag. (224) 56% 53% 58%
EEG+Grad. (328) 52% 53% 62%
EEG+Mag.+Grad. (430) 58% 51% 55%
ch: raw channel input; cp: PCA component input
significance: 56% at p < 0.05; 59% at p < 0.001
the results for all possible combinations of the three
signal types, and it is apparent that the effect of over-
fitting is more pronounced for these larger signal
sets. And though the base level of classification ac-
curacy is very different for these two participants,
both show a similar pattern with respect to signal
type and dimensionality: magnetometers are most
informative for these semantic distinctions, and all
signal types are vulnerable to overfitting effects.
To combat overfitting, we repeated these analy-
ses with dimensionality reduction. Since PCA is
an unsupervised technique, the components were
derived and extracted in one step over the whole
data set. The first (strongest) 60 components were
then taken as input to the same analysis procedure
as before (CSP-derived signal power estimates fed
to the SVM), to give a global description of whole
scalp neural activity, presumably with reduced re-
dundancy and noise. As can be seen in the final
columns of Tables 1 and 2, this resulted in optimal
classification accuracy in almost all cases, both rel-
ative to the full collections of signals, and the 60
40
channel subsets.
A serious limitation of these results however is the
arbitrary selection of signal subsets. While much of
the information recorded between signals is likely
redundant, it could be that the random inclusion or
exclusion of one channel or component could dra-
matically affect accuracy, if that signal was particu-
larly informative, or particularly subject to spurious
noise. So to have a more comprehensive view, we
conducted an exhaustive parameter search through
possible subsets of each combination of signal type,
increasing set size in steps of five, and calculating
average classification accuracy with a moving win-
dow of nine points. The results are illustrated in Fig-
ures 2 (using the raw signals as input) and 3 (using
PCA components of each signal set), and show the
average prediction performance across both experi-
mental participants.
Several things stand out when considering the dif-
ference between the classification performance us-
ing raw signals directly, and dimensionality reduced
sets. In the PCA case, the classification accuracy
levels start higher, rise faster, and peak earlier in
almost all cases. In absolute terms optimum per-
formance is little changed for magnetometer and
EEG signals alone (peaking just above 70% and
60% respectively), while gradiometers seem to ben-
efit somewhat (by about 3% points). But the PCA
lines are also smoother, reflecting more stability in
classification, and so more independence from par-
ticular parameter settings.
Common to both plots is that magnetometers are
the most informative type, followed consecutively
by gradiometers and EEG channels. In terms of mu-
tual redundancy, the information encoded in EEG
channels seems to largely be a subset of that en-
coded by gradiometers (gradiometer performance is
not improved by the addition of EEG channels). The
interaction of magnetometer data and these signal
types is more complex ? magnetometer performance
is reduced by the addition of either or both EEG and
gradiometer channels.
4 Conclusion
This paper reports only two sessions of simultane-
ous MEG/EEG recording, and there were some clear
differences in the results for each participant, so the
conclusions must be considered tentative. Neverthe-
less they suggest that EEG data are to a large ex-
tent redundant with respect to MEG signals. MEG
magnetometers in particular can lead to substantially
higher classification accuracy, with smaller numbers
of channels, than EEG alone. In the case of the sec-
ond participant, prediction with EEG signals did not
approach significance, while MEG signals allowed
highly significant (p 0.001) performance. We be-
lieve that this advantage is due to the lack of attenu-
ation and higher spatial resolution inherent in MEG,
allowing it to pick out individual neural sources with
more precision.
Regardless of the signal types chosen, the high
dimensionality of the data posed challenges. Any
arbitrary subset of channels may leave informative
aspects of brain-activity undetected and this led to
fluctuating results; but including large numbers of
channels invariably leads to overfitting, and conse-
quent falls in classification accuracy. In light of
this, a reduction in dimensions that kept most of the
global signal intact (in this case a principle com-
ponents analysis) proved very effective in prevent-
ing overfitting, giving reliably superior performance
with lower numbers of channels.
While MEG signals proved more informative,
there was not always a dramatic difference in perfor-
mance (peak performance in participant 1 was sim-
ilar for MEG or EEG; for participant 2 there was
a ca. 15% point difference). However this study
used a time interval and frequency band in the sig-
nal that had been optimised for EEG, so it may be
that considering a wider range of frequencies, higher
in the spectrum, could allow MEG to achieve bet-
ter results. Also, though steps were taken to avoid
it, slight movements by the participants relative to
the MEG apparatus will have compromised the reli-
ability of its signals (EEG does not suffer from the
same problem as electrodes are placed directly on
the scalp). This could be addressed in future studies
with continuous head tracking and correction.
Finally, several variations could be tried to im-
prove the overall classification performance of the
system. The spatial decomposition used (CSP) is
particularly prone to overfitting (Parra et al, 2005),
and could be replaced with less aggressive tech-
niques like Linear Discriminant Analysis. Princi-
ple component analysis is a rather brittle technique
41
Figure 2: Classification accuracy taking subsets of raw signals from sensors of different types, 9-point smoothed
Figure 3: Classification accuracy taking subsets of PCA components derived from raw signals from sensors of different
types, 9-point smoothed
42
which is heavily biased towards the few strongest
sources in a system, and so independent component
analysis (ICA) may be a more effective choice for
dimensionality reduction (Makeig et al, 1996). And
data from the various sensor types could be com-
bined in other ways, using an ensemble of classi-
fiers, each based on different subsets of signals, or
by taking more than one class-sensitive component
per category.
Acknowledgements
We are very grateful to Elena Betta, Gianpaolo De-
marchi and Gianpiero Monittola at the LNiF labs
for assistance in MEG data collection. The work
described here was funded by CIMeC, the Au-
tonomous Province of Trento, and the Fondazione
Cassa Risparmio Trento e Rovereto.
References
Boser, B. E.; I. M. Guyon; and V. N. Vapnik (1992):
A training algorithm for optimal margin classi-
fiers. In: 5th Annual ACM Workshop on COLT,
ed. D. Haussler. ACM Press, Pittsburgh, pp. 144?
152.
Chang, Chih-Chung and Chih-Jen Lin (2001): LIB-
SVM: a library for support vector machines.
Chao, Linda L.; Jill Weisberg; and Alex Martin
(2002): Experience-dependent modulation of cat-
egory related cortical activity. Cerebral Cortex,
12:545?551.
Dalponte, Michele; Francesca Bovolo; and Lorenzo
Bruzzone (2007): Automatic selection of fre-
quency and time intervals for classification of
EEG signals. Electronics Letters, 43:1406?1408.
Hill, N.J.; T.N. Lal; M. Schroder; T. Hinterberger;
G. Widman; C.E. Elger; B. Scholkopf; and N. Bir-
baumer (2006): Classifying event-related desyn-
chronization in EEG, ECoG and MEG signals.
Lecture Notes in Computer Science, 4174:404.
Just, M.A.; V.L. Cherkassky; S. Aryal; and T.M.
Mitchell (2010): A neurosemantic theory of con-
crete noun representation based on the underlying
brain codes. PLoS ONE, 5.
Koles, Zoltan J.; Michael S. Lazar; and Steven Z.
Zhou (1990): Spatial patterns underlying popula-
tion differences in the background EEG. Brain
Topography, 2(4):275?284.
Makeig, Scott; Anthony J. Bell; Tzyy-ping Jung;
and Terrence J. Sejnowski (1996): Independent
Component Analysis of Electroencephalographic
Data. In: Advances in Neural Information Pro-
cessing Systems. MIT Press, vol. 8, pp. 145?151.
Mitchell, Tom M.; Svetlana V. Shinkareva; Andrew
Carlson; Kai-Min Chang; Vicente L. Malave;
Robert A. Mason; and Marcel Adam Just (2008):
Predicting Human Brain Activity Associated with
the Meanings of Nouns. Science, 320:1191?1195.
Model, Dmitri and Michael Zibulevsky (2006):
Learning Subject-Specific Spatial and Temporal
Filters for Single-Trial EEG Classification. Neu-
roImage, 32(4):1631?1641.
Murphy, Brian; Marco Baroni; and Massimo Poesio
(2009): EEG responds to conceptual stimuli and
corpus semantics. In: Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing. The Association for Computational
Linguistics, pp. 619?627.
Murphy, Brian; Michele Dalponte; Massimo Poe-
sio; and Lorenzo Bruzzone (2008): Distinguish-
ing Concept Categories from Single-Trial Elec-
trophysiological Activity. In: Proceedings of the
Annual Meeting of the Cognitive Science Society.
Murphy, Brian; Massimo Poesio; Francesca Bovolo;
Michele Dalponte; Lorenzo Bruzzone; and Heba
Lakany (2010): EEG decoding of semantic cate-
gory reveals distributed representations for single
concepts. Brain and Language, under review.
Parra, Lucas C.; Clay D. Spence; Adam D. Ger-
son; and Paul Sajda (2005): Recipes for the linear
analysis of EEG. NeuroImage, 28:326?341.
Philiastides, M.G.; R. Ratcliff; and P. Sajda (2006):
Neural representation of task difficulty and de-
cision making during perceptual categorization:
a timing diagram. Journal of Neuroscience,
26(35):8965.
Pulvermu?ller, Friedemann (2005): Brain mecha-
nisms linking language and action. Nature Re-
views Neuroscience, 6:576?582.
Ramoser, H.; J. M. Gerking; and Gert Pfurtscheller
(2000): Optimal spatial filtering of single
43
trial EEG during imagined hand movement.
IEEE Transactions on Rehabilitation Engineer-
ing, 8(4):441?446.
Tanji, Kazuyo; Kyoko Suzuki; Arnaud Delorme;
and Nobukazu Shamoto, Hiroshiand Nakasato
(2005): High-Frequency gamma-Band Activity
in the Basal Temporal Cortex during Picture-
Naming and Lexical-Decision Tasks. Journal of
Neuroscience, 25(13):3287?3293.
Vapnik, V. N. (1998): Statistical Learning Theory.
Wiley.
Waldert, S.; H. Preissl; E. Demandt; C. Braun;
N. Birbaumer; A. Aertsen; and C. Mehring
(2008): Hand movement direction decoded from
MEG and EEG. Journal of Neuroscience,
28(4):1000.
44
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 22?29,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
PaddyWaC: A Minimally-Supervised Web-Corpus of Hiberno-English
Brian Murphy
Centre for Mind/Brain Sciences,
University of Trento
38068 Rovereto (TN), Italy
brian.murphy@unitn.it
Egon Stemle
Centre for Mind/Brain Sciences,
University of Trento
38068 Rovereto (TN), Italy
egon.stemle@unitn.it
Abstract
Small, manually assembled corpora may be avail-
able for less dominant languages and dialects,
but producing web-scale resources remains a chal-
lenge. Even when considerable quantities of text
are present on the web, finding this text, and distin-
guishing it from related languages in the same region
can be difficult. For example less dominant vari-
ants of English (e.g. New Zealander, Singaporean,
Canadian, Irish, South African) may be found under
their respective national domains, but will be par-
tially mixed with Englishes of the British and US
varieties, perhaps through syndication of journalism,
or the local reuse of text by multinational compa-
nies. Less formal dialectal usage may be scattered
more widely over the internet through mechanisms
such as wiki or blog authoring. Here we automati-
cally construct a corpus of Hiberno-English (English
as spoken in Ireland) using a variety of methods: fil-
tering by national domain, filtering by orthographic
conventions, and bootstrapping from a set of Ireland-
specific terms (slang, place names, organisations).
We evaluate the national specificity of the resulting
corpora by measuring the incidence of topical terms,
and several grammatical constructions that are par-
ticular to Hiberno-English. The results show that
domain filtering is very effective for isolating text
that is topic-specific, and orthographic classification
can exclude some non-Irish texts, but that selected
seeds are necessary to extract considerable quanti-
ties of more informal, dialectal text.
1 Introduction
For less dominant language variants, corpora are usu-
ally painstakingly constructed by hand. This results in
high quality collections of text, classified and balanced
by genre, register and modality. But the process is time-
consuming and expensive, and results in relatively small
resources. For example the International Corpus of En-
glish (ICE) project (Greenbaum, 1996) has already re-
sulted in the publication of corpora covering ten dialects
of English, following a common schema, but the indi-
vidual corpora are limited to approximately one million
words.
An alternative is to use automatic methods to harvest
corpora from the Web. Identification of major languages
is a robust technology, and where the regional boundaries
of a language or dialect correspond closely to a national
top-level internet domain, very large collections (of sev-
eral billion words) can now can be produced easily, with
close to no manual intervention (Baroni et al, 2009).
These methods can also deal with some issues of text
quality found on the web, successfully extracting coher-
ent pieces of running text from web pages (i.e. discard-
ing menu text, generic headings, copyright and other le-
gal notices), reducing textual duplication, and identifying
spam, portal pages and other files that do not contain lin-
guistically interesting text.
Corpora of minor languages that lack their own do-
main, but that have clear orthographic differences from
more dominant neighbouring languages can be collected
automatically by using a small set of seed documents,
from which language-specific search terms can be ex-
tracted (Scannell, 2007). These methods, combined with
automated language identification methods, can quickly
produce large, clean collections with close to no manual
intervention.
However for language variants that do not have their
own domain (e.g. Scots, Bavarian), it is less clear
that such web corpora can be automatically constructed.
Smaller or politically less dominant countries that do
have their own domain (e.g. Belgium, New Zealand),
may also find the language of their ?national? web
strongly influenced by other language varieties, for ex-
ample through syndication of journalistic articles, or ma-
terials published by foreign companies.
In this paper we use minimally supervised methods
(Baroni and Bernardini, 2004; Baroni et al, 2009) to
quickly and cheaply build corpora of Hiberno-English
(English as spoken in Ireland), which are many times
larger than ICE-Ireland, the largest published collection
22
currently available (Kallen and Kirk, 2007). We investi-
gate several combinations of strategies (based on domain
names, and on regional variations in vocabulary and or-
thography) to distinguish text written in this minor lan-
guage variant from related dominant variants (US and UK
English). We validate the specificity of the resulting cor-
pora by measuring the incidence of Ireland-specific lan-
guage, both topically (the frequency with which Irish re-
gions and organisations are mentioned), and structurally,
by the presence of grammatical constructions that are par-
ticular to Hiberno-English. We also compare our cor-
pus to another web-corpus of Hiberno-English that is in
development (Cr?bad?n, Scannell, personal communica-
tion) that relies on domain filtering of crawled web-pages.
The results show that filtering by national domain is
very effective in identifying text that deals with Irish top-
ics, but that the grammar of the resulting text is largely
standard. Using a set of seed terms tailored to the lan-
guage variant (Irish slang, names of Ireland-based organ-
isations, loanwords from Irish Gaelic), yields text which
is much more particular to Hiberno-English usage. At the
same time, such tailored seed terms increase the danger
of finding ?non-authentic? uses of Irishisms (sometimes
termed paddywhackery or oirish), either in fictional di-
alogues, or in documents discussing distinctive patterns
in Irish English. The application of a British/American
spelling filter has less clear effects, increasing topical
incidence slightly, while reducing structural incidences
somewhat.
The paper proceeds as follows: in the next section we
introduce Hiberno-English, situating it relative to other
variants of English, and concentrating on the characteris-
tic features that will be used as metrics of ?Irishness? of
text retrieved from the Web. Next we describe the process
by which several candidate corpora of Hiberno-English
were constructed (section 3), and the methods we used
to quantify incidence of distinctive usage (section 4). In
the final two sections we compare the incidence of these
markers with those found in corpora of other variants
of English (UK, US), Scannell?s IE-domain filtered cor-
pus, and a hand-crafted corpus of Hiberno-English (ICE-
Ireland), and reflect on the wider applicability of these
methods to variants of other languages and orthographies.
2 Structures and Lexicon of
Hiberno-English
Hiberno-English differs in a range of ways from other
varieties of English. In broad terms it can be grouped
with British English, in that its lexicon, grammar and or-
thographic conventions are more similar to that of Great
Britain, than to that of North America. For example with
lexical variants such as bumper/fender, rubbish bin/trash
can, lift/elevator and zed/zee it shares the former British
usage rather than the latter American usage, though there
are exceptions (in Irish usage the North Americans term
truck is replacing the British lorry). Similarly in syntax
it tends to follow British conventions, for instance He?s
familiar with X rather than X is familiar to him, write to
me rather than write me and the acceptability of singu-
lar verbal marking with group subjects, as in the team are
pleased ? though there are counterexamples again, in that
Irish English tends to follow American dialects in dis-
pensing with the shall/will distinction. Most obviously,
Irish writing uses British spellings rather than American
spellings.
However, there are still dialectal differences between
Irish and British English. Beyond the usual regional dif-
ferences that one might find between the words used in
different parts of England, the English spoken in Ireland
is particularly influenced by the Irish language (Gaelic,
Gaeilge) (Kirk and Kallen, 2007). While English is the
first language of the overwhelming majority of residents
of Ireland (estimates of Irish mother-tongue speakers are
of the order of 50,000, or about 1% of the population),
Irish retains status as the first official language of the Re-
public of Ireland, maintained as a core subject at all levels
of school education, and through state-maintained radio
and television channels. As recently as the early 19th
century, Irish was the majority language, and so many
traces of it remain in modern Hiberno-English, in the
form of Irish loan-words (e.g. sl?n ?goodbye?, gaelscoil
?Irish (speaking) school?), Anglicizations (e.g. ?gansey?,
jumper, from Irish geansa?), and composites (e.g. ?jack-
een?, a pejorative term for Dubliners, combining the Irish
diminutive -?n with the English ?Jack?).
In this paper we take a series of characteristic terms
and structures from Hiberno-English, mostly inspired by
(Kirk and Kallen, 2007), and use them as markers of the
Irishness of the text we assemble from the web. While
there are many more interesting grammatical differences
between Hiberno-English and other variants (e.g. per-
fective use of the simple present: I know that family for
years), we restrict ourselves to those that can be automat-
ically identified in a corpus through searching of plain
text, or of shallow syntactic patterns (parts of speech).
The first marker we use is to measure the incidence of
a set of terms that are topically related to Ireland: proper
names of Ireland-based organisations, and geographical
terms. The method for assembling this list is described in
section 4.
The most simple structure that we use as a marker of
Hiberno-English is the contraction I amn?t (I?m not or I
ain?t in other varieties). The next is the ?after? perfec-
tive, which often expresses immediacy, and a negative
outcome:
(1) I?m after losing my wallet
?I just lost my wallet?
23
A further structure that is novel from the point of view
of other variants of English is a particular use of verbs
that take a complement that expresses a question (most
commonly ask, wonder, see and know), without the use
of a complementizer such as if or whether and with an
inversion of subject-verb order (typical of interrogatives):
(2) I wonder is he coming?
?I wonder if/whether he is coming?
Finally we consider the expanded usage of reflexive pro-
nouns in Hiberno-English, where they may be used for
emphasis, in any argument position, and without be-
ing anaphorically bound, as is usually required. Here
we limit ourselves to subject position reflexives, which
can be identified from word order patterns, without any
deeper semantic analysis:
(3) himself is in big trouble
?he is in big trouble?
With the exception of the amn?t contraction, all of these
phenomena are demonstrated by (Kirk and Kallen, 2007)
to be common in the ICE-Ireland corpus, though some-
what less common in Northern Irish portion of that col-
lection, and to be very rare or completely absent in
the ICE-GB corpus of the English of Britain (Nelson
et al, 2002). Significantly, these constructions are found
predominantly in the spoken language portion of the
ICE-Ireland corpus, suggesting that speakers are perhaps
aware that they are not ?standard? English, and so not
considered appropriate in the written register.
3 Constructing a Web-Corpus of
Hiberno-English
Within the WaCky initiative (Web-as-Corpus kool ynitia-
tive) (Baroni and Bernardini, 2006) a community of lin-
guists and information technology specialists developed
a set of tools to selectively crawl sections of the Web, and
then process, index and search the resulting data. Contri-
butions like BootCaT (Baroni and Bernardini, 2004), an
iterative procedure to bootstrap specialised corpora and
terms from the Web, have been successfully used in a
range of projects: first in the construction of the WaCky
corpora, a collection of very large (>1 billion words) cor-
pora of English (ukWaC), German (deWaC) and Italian
(itWaC); and subsequently by other groups, e.g. noWaC
and jpWaC (Baroni et al, 2009; Guevara, 2010; Erjavec
et al, 2008).
Here we use BootCaT to build seven prototype corpora
of Hiberno-English, and evaluate the dialect-specificity
of each by measuring the incidence of proper terms and
constructions that are associated with this language vari-
ant. Additionally, we use ukWaC as the de-facto stan-
dard British English Web corpus, and construct a medium
size web-corpus of the US domain to represent Ameri-
can usage. Each corpus is preprocessed and formatted for
the IMS Open Corpus Workbench (CWB, (Christ, 1994;
Web, 2008)), a generic query engine for large text corpora
that was developed for applications in computational lex-
icography.
BootCaT first takes a set of manually assembled seed
terms, these (possibly multi-word) terms are randomly
combined, and then are used as search queries with a
Web search engine; the HTML documents of the top re-
sults are downloaded and cleaned to extract running text
and discard all web-markup. Preprocessing and format-
ting for the CWB consists of tokenising, lemmatising,
and part-of-speech tagging the corpus, and then convert-
ing the result into CWB?s internal format; we replicated
the processing stages employed for ukWaC.
The construction of the nine corpora differs on three
dimensions:
Seeds: two seed sets were used namely, an Hiberno-
English one (IEs), and the original ukWaC list of
mid-frequency terms (UKs) from the British Na-
tional Corpus (Burnard, 1995); the Irish seeds were
used in pairs and triples to attempt to vary the degree
of regional specificity.
TLDs: two types of top-level internet domain (TLD) re-
strictions were imposed during (or after) the con-
struction of the corpora; either no restriction was im-
posed (.ALL), or a corpus was filtered by a specific
national TLD (e.g. .ie).
Spelling: two types of spelling filter were imposed;
either none, or an ?orthographic convention fac-
tor? (OCF) was calculated to detect American and
British spellings, and a corpus was filtered accord-
ingly (BrEn).
The IE seeds contained 81 seed terms, gathered using
one author?s native intuition, and words indicated as be-
ing specific to Irish English by the Oxford English Dic-
tionary, and from various Web pages about Hiberno-
English. 76 single-word and 5 two-word terms were used
falling into three main categories: Irish place names, re-
gional variant terms (mostly slang), and load words from
Irish Gaelic (many being state institutions). The full list-
ing of terms is given here:
Place names: Dublin, Galway, Waterford, Drogheda, Antrim, Derry,
Kildare, Meath, Donegal, Armagh, Wexford, Wicklow,
Louth, Kilkenny, Westmeath, Offaly, Laois, Belfast, Cavan,
Sligo, Roscommon, Monaghan, Fermanagh, Carlow, Longford,
Leitrim, Navan, Ennis, Tralee, Leinster, Connaught, Munster, Ul-
ster
Regional variants: banjaxed (wrecked), craic (fun), fecking (variant
of fucking), yoke (thing), yer man/one/wan (that man/woman),
culchie (country dweller), da (father), footpath (pavement),
24
gaff (home), gobshite (curse), gurrier (young child), jack-
een (Dubliner), jacks (toilet), janey mac (exclamation), jaysus
(variant of exclamation ?jesus?), kip (sleep; hovel), knacker
(Traveller, gypsy), knackered (wrecked), langer (penis; id-
iot), langers/langered (drunk), scallion (spring onion), skanger
(disgusting person), strand (beach, seaside), scuttered (drunk),
boreen (small road), gob (mouth; spit), eejit (variant of idiot),
lough (lake), fooster (dawdle), barmbrack (traditional Hallow?een
cake), shebeen (unlicensed bar), bogman (contry dweller), old
one (old lady), quare (variant queer), gansey (pullover)
Loan words: garda, garda? (police), taoiseach (prime minister), d?il
(parliament), Sl?inte (?cheers?), Gaeltacht (Irish speaking areas),
Seanad (senate), T?naiste (deputy prime minister), ceol ((tradi-
tional Irish) music), sl?n (?goodbye?), gr? (affection, love for),
gaelscoil (Irish speaking school)
These seed terms were combined into a set of 3000 3-
tuple (3T) and a set of 3000 2-tuple (2T) search queries,
i.e. two-word terms were enclosed in inverted commas to
form one single term for the search engine. For 3T this re-
sulted in over 80% 3-tuples with 3 single-word terms, and
slightly over 17% with 2 single-word terms, and the re-
maining percentages for 3-tuples with 1 single-word and
no single-word terms; for 2T this resulted in almost 88%
2-tuples with 2 single-word terms, almost 12% with only
1 single-word terms, and less than 1% with no single-
word terms. The UK seeds were the original ones used
during the construction of the ukWaC corpus and they
were combined into 3000 3-tuple search queries.
No TLD restriction means that the search engine was
not instructed to return search results within a specific
domain, and hence, documents originate from typical
English-language domains (.com, .ie, .uk, etc.) but also
from .de and potentially any other. A restriction meant
that the documents could only originate from one TLD.
No spelling filter means that nothing was done. The
OCF indicates the degree to which terms within a docu-
ment are predominantly spelled according to one prede-
fined word list relative to another. The number of term
intersections with each list is counted and OCF is calcu-
lated as the difference between counts over their sum. To
simplify matters, we utilised a spell-checker to return the
list of known words from a document, this corresponds to
checking a document for spelling errors and only keeping
the non-erroneous words. In our case we used an en_GB
dictionary, an en_US one, and the two together. The three
lists yield the needed numbers of words only known by
one of the two dictionaries, and, hence unknown by the
other dictionary, and the ratio in the range of [?1,+1] can
be calculated.
The search engine we used for all queries was Yahoo
(Yahoo! Inc., 1995); for all search queries English results
were requested, that is we relied on the search engine?s
built-in language identification algorithm1, and from all
1This restriction is very effective at distinguishing non-English from
English content, but returns content from any English variant.
search queries the top 10 results were used. Cleaning
of the Web pages (termed boilerplate removal) was ac-
complished by BootCaT?s implementation of the BTE
method (Finn et al, 2001); it strives to extract the main
body of a Web page, that is the largest contiguous text
area with the least amount of intervening non-text ele-
ments (HTML tags), and discards the rest.
Several corpora were constructed from the Irish seeds
using 2- or 3-tuple search terms: either without restrict-
ing the TLDs; subsequent restriction to the .ie TLD; or
subsequent filtering according to spelling. Corpora were
also constructed with the search engine instructed to di-
rectly return documents from the .us or the .ie TLD, re-
spectively, where the latter one was later also filtered ac-
cording to spelling. The ukWaC corpus is restricted to
the .uk TLD.
4 Evaluating Variety Specificity of the
Corpus
To evaluate the dialectal specificity of the text in each pu-
tative corpus of Hiberno-English, we measured the inci-
dence of several characteristic terms and structures. The
same phenomena were counted in corpora of US and UK
English (identified as that found under the .us and .uk
TLDs respectively) to establish baseline frequencies. All
corpora were HTML-cleaned, lemmatised and part-of-
speech tagged using the same methods described above,
and searches were made with identical, case-insensitive,
queries in the CQP language.
First we quantified topical specificity by searching
for a set of Irish geographical terms (towns, counties,
regions), and Ireland-based organisations (companies,
NGOs, public-private bodies), to identify text which is
?about Ireland?. There were 80 terms, evenly split be-
tween the two categories. In this list we avoided proper
names which are orthographically identical to content
words (e.g. Down, Cork, Clones, Trim, Limerick, Mal-
low, Mayo), given names (Clare, Kerry, Tyrone), place
names found in other territories (Baltimore, Skibbereen,
Newbridge, Westport, Passage West), or names that
might be found as common noun-phrases (e.g. Horse
Racing Ireland, Prize Bond Company, Electricity Supply
Board). While political terms might have been appropri-
ate markers (e.g. the political party Fianna F?il; the par-
liamentary speaker the Ceann Comhairle), the seed terms
we used contained many governmental institutions, and
so this could be considered an unfairly biased diagnostic
marker. The full list of terms is given below.
Topical terms: ActionAid, Aer, Aer, Allied, An, Arklow, Athlone,
Athy, Balbriggan, Ballina, Ballinasloe, Bantry, Bord, Bord, Bord,
Buncrana, Bundoran, Bus, Carrick-on-Suir, Carrickmacross,
Cashel, Castlebar, Christian, Clonakilty, Clonmel, Cobh, Coillte,
Comhl(?|a)mh, Connacht, C(?|o)ras, Donegal, Dublin, Dublin,
Dungarvan, Eircom, EirGrid, Enniscorthy, Fermoy, Fyffes, Glan-
25
bia, Gorta, Grafton, Greencore, Iarnr(?|o)d, IONA, Irish, Irish,
Irish, Kerry, Kilkee, Kilrush, Kinsale, Laois, Leixlip, Let-
terkenny, Listowel, Listowel, Loughrea, Macroom, Mullingar,
Naas, Nenagh, Oxfam, Paddy, Portlaoise, Radi(o|?), Ryanair,
Telif(?|i)s, Templemore, Thurles, Tipperary, Tramore, Trinity,
Tr(?|o)caire, Tuam, Tullamore, Tullow, Vhi, Waterford, Youghal
For the structural markers we used more conservative
query patterns where appropriate, to minimise false pos-
itives. For this reason the incidence figures given here
should be considered lower estimates of the frequency of
these structures, but they allow us to establish an inde-
pendent metric with a minimum of manual intervention.
As mentioned above, for the emphatic use of reflex-
ives, we searched only in the subject verb configuration,
even though these are possible in other argument posi-
tions also (e.g. I saw himself in the pub yesterday). The
query was restricted to reflexive pronouns (other than it-
self ) found at the start of a sentence, or immediately after
a conjunction, and directly before a finite verb (other than
have or be). The CQP query (4) yields examples such as
(5)-(7).
(4) [pos="CC" | pos="SENT"] [lemma=".+self" &
lemma!="itself"] [pos="VV[ZD]?"];
(5) ... more commonplace or didactic, less
imaginative? Himself added, "You are a romantic
idiot, and I love you more than...
(6) ... Instruments in Lansing, Michigan, where Val
and Don and myself taught bouzouki, mandolin,
guitar and fiddle workshops. It is a...
(7) ... game of crazy golf, except this time it was
outdoor. Conor and myself got bored straight away
so we formed our own game while Mike ...
For the ?after? perfective construction, we searched for a
pattern of a personal pronoun (i.e. not including it, this,
that), the lexeme after, and a gerund form of a common
verb (other than have, be). The query (8) allowed for
a modal auxiliary, and for intervening adverbs, as illus-
trated in (9)-(11).
(8) [pos="PP" & word!="it" %c & word!="that" %c &
word!="this" %c] [pos="RB.*"]* [lemma="be"]
[pos="RB.*"]* [word="after"] [pos="RB.*"]*
[pos="V[VH]G"]
(9) ... the holy angels on your head, young fellow. I
hear tell you?re after winning all in the sports
below; and wasn?t it a shame I didn?t ...
(10) ... MICHAEL ? Is the old lad killed surely?
PHILLY. I?m after feeling the last gasps quitting
his heart. MICHAEL ? Look at ...
(11) ... placards with the words ?Blind as a Batt? and
?Batman you are after robbing us?. They came
from as far away as Wexford and called ...
The use of embedded inversions in complements was
queried for the same four verbs identified by (Kirk and
Kallen, 2007): ask, see, wonder and know. Other verbs
were considered, by expansion from these four via Levin
verb classes (Levin, 1993), but preliminary results gave
many false positives. The query used search for one of
these four verbs, followed by a form of the verb be, and
then a personal pronoun specific to the subject position
(12). Examples of the instances extracted are given be-
low (13)-(15).
(12) [pos="VV.*" & lemma="(ask|know|see|wonder)"
%c] [lemma="be"] [word="(I|he|she|we|they)" %c];
(13) ... but that is the reality. I remember as a young
child being asked was I a Protestant or a Catholic:
that?s the worst thing ...
(14) ... unless I get 170+, there isn?t a chance. And then
I wonder am I mad even applying for medicine.
Anyway anyone else who?s...
(15) There was the all important question and she was
dying to know was he a married man or a widower
who had lost his wife or some ...
Finally, examples of the amn?t contraction (17)-(19) were
extracted with the simple case-insensitive query (16).
(16) "am" "n?t";
(17) Hi I?m relatively new to CCTV but work in IT and
so amn?t 100 % lost ! Anyway, I have already set
up a personal ...
(18) ... and plaster, with some pride.) It was he did that,
and amn?t I a great wonder to think I ?ve traced
him ten days with ...
(19) ?I will indeed Mrs. R, thanks very much, sure
amn?t I only parchin?? Ye needn?t have gone to the
trouble of ...
It should be noted that these structural usages differ in the
degree to which they are perceived as distinctive. While
speakers of Irish English may not be aware that amn?t
and the embedded inversion construction are dialectally
restricted, many do know that the after and reflexive con-
structions are particular to Ireland. Hence by searching
for these constructions our evaluation is biased towards
colloquial language and consciously dialectal usage.
26
5 Results
As can be seen in the first two rows of table 1, consider-
ably large Irish corpora were gathered with ease, and even
after applying several subsequent filtering strategies, the
smallest corpus was several times the size of the manually
assembled ICE-Ireland corpus.
Figure 1 (left panel) further shows that the strategy of
searching by random seed combinations yielded pages
in many domains, with a considerable proportion being
in the .ie domain, but by no means the majority. This
suggests that Ireland specific usage of English is not re-
stricted to the national internet domain, i.e. the .ie TLD.
The relative proportion of .ie domain pages (see right
panel of same figure) was increased by selecting only
pages which had predominantly British orthography, sug-
gesting that this has some efficacy in eliminating texts
written in American English.
Table 1 also shows the absolute incidence of each
of the five characteristic phenomena considered. All
matches returned by the CQP search queries were man-
ually evaluated, to ensure that they were authentic ex-
amples of the constructions in question (for the larger
ukWaC corpus only a random sample were examined).
Numbers of false positives that were excluded are shown
in brackets, such as the examples from ukWaC below:
(20) ... just as they were after receiving secret briefings
from Health Commission Wales officers.
(21) All I know is they?re getting cold.
The bars in sets one and two show figures for the man-
ually compiled ICE-Ireland corpus, and the Cr?bad?n
web-corpus. The ICE-Ireland numbers differ somewhat
from those reported in that paper (Kirk and Kallen, 2007),
since we used more selective search strategies (note that
the cut-off reported relative incidences reach about 21 per
mil. tokens), which would miss some examples such as
those below which have the after construction without a
personal pronoun, and have the non-reflexive use in ob-
ject position, respectively:
(22) There?s nothing new after coming in anyway so
(23) Again it?s up to yourself which type of pricing
policy you use
It should also be noted that ICE-Ireland, following the
standard scheme for the International Corpus of English
project (Greenbaum, 1996), is biased towards spoken lan-
guage, with written text only making up only 40% of the
total text.
The relative incidence (per million tokens) of Ireland-
specific topics and constructions is summarised in figure
2. The bars in sets three and four demonstrate that these
same characteristics, very common in Hiberno-English as
evidenced by the ICE-Ireland, appear to be exceedingly
rare in UK and US English. Unsurprisingly, web authors
in the US and UK domains do not write often about Irish
places and organisations. But constructions that are pu-
tatively exclusive to Hiberno-English are seldom found.
Those that are found might be explained by the effect
of language contact with Irish immigrants to those coun-
tries, and the fact that text by Irish authors may be found
in these domains, whether those people are resident in
those countries or not. For instance in the example below,
the given name Ronan suggests that the author might be
of Irish extraction:
(24) At about that point Cardinal Cormac of
Westminster walked right past us and Ronan and
myself went to say hello to him and tell him we
were up here from his diocese.
The sets headed ?.ie? show the figures for the corpora we
constructed by querying seed terms within the Irish na-
tional domain. The incidence of characteristic features
of Hiberno-English grammar are higher than those seen
in the US and UK domains, similar to that seen in the
Cr?bad?n corpus, and lower than in the ICE-Ireland cor-
pus, perhaps reflecting the fact that these constructions
are less common in written Hiberno-English. Subsequent
filtering out of pages with dominance of American En-
glish spelling (?.ie, BrEn?) does not have much effect on
the numbers.
The ?Irish Seeds (IEs)? bars show that the use of tai-
lored seed terms returns text which has a similar topical
specificity to that in the .ie domain generally, but which
shows more structural characteristics of Hiberno-English.
These results can also be improved upon, first by concen-
trating on the .ie domain portion of the tailored-seeds ex-
tracted pages (?Irish Seeds (IEs), IE Dom (.ie)?) which
boosts topical specificity. Filtering instead by orthogra-
phy (?IEs, BrEn?) seems to strike a happy medium, in-
creasing incidence in all categories.
However returning to table 1, it is apparent that there
are many false positives among the constructions found
using Irish seed terms. This was caused by the search
strategy retrieving a small number of pages on the topic of
Hiberno-English, that contained many constructed exam-
ples of the structures of interest. The same corpora con-
tained smaller numbers of examples from theatre scripts
and other fiction.
6 Discussion
The results show us that our methods can be effective in
extracting text that is both specific to Irish topics, and in-
cludes instances of constructions that are particular to the
variety of English spoken in Ireland. The incidences rel-
ative to corpus size are not as high as those seen in the
27
Table 1: Corpora sizes, incidences of Ireland terms and constructions; absolute numbers (false positives in brackets)
IC
E
-I
re
la
nd
C
ru
ba
da
n
uk
W
aC
U
K
s,
3T
,.
us
U
K
s,
3T
,.
ie
U
K
s,
3T
,.
ie
,B
rE
n
IE
s,
3T
,.
A
L
L
IE
s,
3T
,.
A
L
L
,.
ie
IE
s,
3T
,.
A
L
L
,B
rE
n
IE
s,
2T
,.
A
L
L
IE
s,
2T
,.
ie
Size (in 106 Tokens) 1.1 46.3 2119.9 74.7 17.8 15.0 25.2 2.6 17.3 18.4 6.4
Size (in 103 Docs) 0.5 43.0 2692.6 4.6 2.0 1.6 3.4 0.7 2.5 7.3 2.3
Ireland Terms 194 17330 12743 82 14199 13802 23527 7264 22071 12454 9935
"after" Construction 7 (-4) 12 (2) 48 (72) 1 (2) 11 (1) 7 (1) 26 (50) 2 (1) 11 (47) 14 (38) 9 (1)
"amn?t" Construction 0 (0) 0 (0) 32 (0) 0 (0) 0 (0) 0 (0) 5 (45) 1 (1) 2 (43) 6 (36) 0 (0)
embedded Inversions 24 (-18) 18 (5) 42 (309) 0 (15) 5 (2) 5 (0) 20 (4) 2 (1) 17 (2) 4 (1) 5 (0)
Subject Reflexives 22 (-19) 33 (0) 1797 (115) 35 (8) 15 (1) 10 (0) 39 (0) 2 (0) 30 (0) 17 (3) 8 (1)
Figure 1: Domain composition of Irish-Seed based Corpora
Top?Level Domains
Numb
er of D
ocum
ents (T
otal:3
382)
0
500
1000
1500
com ie uk org net edu au info Others Top?Level Domains
Numb
er of D
ocum
ents (T
otal:2
485)
0
200
400
600
800
1000
1200
com ie uk org net info au ca Others
Figure 2: Relative Incidences of Ireland terms and constructions, per million words (grey bars indicating the original counts before
manual inspection), in each copus
ICE-Ir
eland Cruba
dan ukWa
C
UKs, 
3T, .u
s
UKs, 
3T, .ie
UKs, 
3T, .ie
, BrEn
IEs, 3
T, .AL
L
IEs, 3
T, .AL
L, .ie
IEs, 3
T, .AL
L, BrE
n
IEs, 2
T, .AL
L
IEs, 2
T, .ie
0
1
2
3
4
5
6
7 "after" Construction"amn't" Constructionembedded InversionsSubject RefelxivesIreland Terms (right Scale)
0
5
10
15
20
25
30
35
40
28
manually constructed ICE-Ireland corpus. We can specu-
late on the reasons for this. It may be in part due to ?pollu-
tion? of our corpus with non-Irish English, via syndicated
journalism (e.g. some Irish newspapers are repackaging
of British newspapers with added Irish content), or via
multinational organisations with bases in Ireland. In our
view the main explanatory factor is that of modality and
register. The ICE-Ireland corpus is predominantly spoken
(~60%), with many texts coming from informal settings
(unscripted speeches, face to face and telephone conver-
sations). One reading of the figures which is consistent
with this viewpoint is that the .ie domain corpora contain
proportionally more high register, edited text (e.g. from
governmental and commercial organisations, for which
the use of the .ie domain may be an important part of cor-
porate identity), and that the tailored-seed corpora con-
tain more text contributed by individuals (forums, blogs,
etc), for whom domain endings are of little consequence.
Nevertheless, the use of Hiberno-English specific seed
terms did reveal higher incidences of distinctive Irish us-
ages than simple domain filtering.
But despite these lower incidences, in absolute terms
our corpora provide many more examples of Hiberno-
English than that were hitherto available. For example
the ICE-Ireland corpus contains a total of seven examples
of the ?after? construction, while with our Irish-seeds de-
rived corpus, and using a fairly restrictive query pattern,
we isolated 26 examples of this structure. Further the
size of these pilot corpora were kept intentionally lim-
ited, a small fraction of the approximately 150 million .ie
domain pages indexed by Google. Much larger corpora
could be constructed with relative ease, by using a larger
seed set, or with an interactive seed-discovery method,
where the text from the first round of web-harvesting
could be analysed to identify further terms that are com-
paratively specific to Hiberno-English (relative to corpora
of other varieties of English), in a similar fashion to the
methods discussed in (Scannell, 2007).
In terms of wider implications, the fact that seeds tai-
lored to a particular region and language variant is as ef-
fective as filtering by domain, is encouraging for dialects
and minority languages that lack a dedicated internet do-
main. This suggest that for less-dominant language vari-
ants without distinctive established orthographies (e.g.
Scots, Andalusian, Bavarian), large corpora displaying
characteristic features of that variant can be constructed
in a simple automatic manner with minimal supervision
(a small set of seeds provided by native speakers). Our
methods might also prove useful for dialects in which a
standard variant is dominant in the written language (e.g.
Arabic, Chinese). One might expect that the written Ara-
bic in the .ma (Morocco) domain would differ little from
that in the .qa domain (Qatar) despite the large differences
in vernacular speech. Similarly the grammar and vocabu-
lary of Chinese written in Mainland Chinese, Taiwanese,
Hong Kong and Singaporese domains (ignoring orthog-
raphy) might be less representative of the variation in ev-
eryday language. The use of regional slang and proper
names may help one to collect more examples of this
more natural language usage, and less of the dominant
standard variant.
References
Baroni, M. and Bernardini, S. (2004). BootCaT: Bootstrapping
corpora and terms from the web. In (ELRA), E. L. R. A.,
editor, Proceedings of LREC 2004, Lisbon: ELDA., pages
1313?1316.
Baroni, M. and Bernardini, S., editors (2006). Wacky! Working
papers on the Web as Corpus.
Baroni, M., Bernardini, S., Ferraresi, A., and Zanchetta, E.
(2009). The WaCky wide web: a collection of very large
linguistically processed web-crawled corpora. Language Re-
sources and Evaluation, 43(3):209?226.
Burnard, L. (1995). Users Reference Guide, British National
Corpus, Version 1.0. Oxford University Computing Ser-
vices/British National Corpus Consortium, Oxford.
Christ, O. (1994). A Modular and Flexible Architecture for an
Integrated Corpus Query System. In Papers in Computa-
tional Lexicography (COMPLEX ?94), pages 22?32.
Erjavec, I. S., Erjavec, T., and Kilgarriff, A. (2008). A web cor-
pus and word sketches for Japanese. Information and Media
Technologies, 3:529?551.
Finn, A., Kushmerick, N., and Smyth, B. (2001). Fact or fic-
tion: Content classification for digital libraries.
Greenbaum, S. (1996). Comparing English Worldwide.
Clarendon Press.
Guevara, E. (2010). NoWaC: a large web-based corpus for Nor-
wegian. In Proceedings of the Sixth Web as Corpus Work-
shop (WAC6), pages 1?7. The Association for Computational
Linguistics.
Kallen, J. and Kirk, J. (2007). ICE-Ireland: Local variations on
global standards. In Beal, J. C., Corrigan, K. P., and Moisl,
H. L., editors, Creating and Digitizing Language Corpora:
Synchronic Databases, volume 1, pages 121?162. Palgrave
Macmillan, London.
Kirk, J. and Kallen, J. (2007). Assessing Celticity in a Corpus
of Irish Standard English. In The Celtic languages in con-
tact: papers from the workshop within the framework of the
XIII International Congress of Celtic Studies, Bonn, 26-27
July 2007, page 270.
Levin, B. (1993). English Verb Classes and Alternations. Uni-
versity of Chicago Press, Chicago.
Nelson, G., Wallis, S., and Aarts, B. (2002). Exploring natural
language: working with the British component of the Inter-
national Corpus of English. John Benjamins.
Scannell, K. (2007). The Cr?bad?n project: Corpus building
for under-resourced languages. In Fairon, C., Naets, H., Kil-
garriff, A., and de Schryver, G.-M., editors, Building and Ex-
ploring Web Corpora: Proceedings of the 3rd Web as Corpus
Workshop, volume 4, pages 5?15.
Web (2008). The IMS Open Corpus Workbench (CWB).
Yahoo! Inc. (1995). The Yahoo! Internet search engine.
29
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 84?93,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Documents and Dependencies: an Exploration of
Vector Space Models for Semantic Composition
Alona Fyshe, Partha Talukdar, Brian Murphy and Tom Mitchell
Machine Learning Department &
Center for the Neural Basis of Cognition
Carnegie Mellon University, Pittsburgh
{afyshe|ppt|bmurphy|tom.mitchell}@cs.cmu.edu
Abstract
In most previous research on distribu-
tional semantics, Vector Space Models
(VSMs) of words are built either from
topical information (e.g., documents in
which a word is present), or from syntac-
tic/semantic types of words (e.g., depen-
dency parse links of a word in sentences),
but not both. In this paper, we explore the
utility of combining these two representa-
tions to build VSM for the task of seman-
tic composition of adjective-noun phrases.
Through extensive experiments on bench-
mark datasets, we find that even though
a type-based VSM is effective for seman-
tic composition, it is often outperformed
by a VSM built using a combination of
topic- and type-based statistics. We also
introduce a new evaluation task wherein
we predict the composed vector represen-
tation of a phrase from the brain activity of
a human subject reading that phrase. We
exploit a large syntactically parsed corpus
of 16 billion tokens to build our VSMs,
with vectors for both phrases and words,
and make them publicly available.
1 Introduction
Vector space models (VSMs) of word semantics
use large collections of text to represent word
meanings. Each word vector is composed of fea-
tures, where features can be derived from global
corpus co-occurrence patterns (e.g. how often a
word appears in each document), or local corpus
co-occurrence patterns patterns (e.g. how often
two words appear together in the same sentence,
or are linked together in dependency parsed sen-
tences). These two feature types represent dif-
ferent aspects of word meaning (Murphy et al,
2012c), and can be compared with the paradig-
matic/syntagmatic distinction (Sahlgren, 2006).
Global patterns give a more topic-based mean-
ing (e.g. judge might appear in documents also
containing court and verdict). Certain local pat-
terns give a more type-based meaning (e.g. the
noun judge might be modified by the adjective
harsh, or be the subject of decide, as would related
and substitutable words such as referee or con-
ductor). Global patterns have been used in Latent
Semantic Analysis (Landauer and Dumais, 1997)
and LDA Topic models (Blei et al, 2003). Local
patterns based on word co-occurrence in a fixed
width window were used in Hyperspace Analogue
to Language (Lund and Burgess, 1996). Subse-
quent models added increasing linguistic sophisti-
cation, up to full syntactic and dependency parses
(Lin, 1998; Pado? and Lapata, 2007; Baroni and
Lenci, 2010).
In this paper we systematically explore the util-
ity of a global, topic-based VSM built from what
we call Document features, and a local, type-based
VSM built from Dependency features. Our Doc-
ument VSM represents each word w by a vector
where each feature is a specific document, and the
feature value is the number of mentions of word
w in that document. Our Dependency VSM rep-
resents word w with a vector where each feature
is a dependency parse link (e.g., the word w is the
subject of the verb ?eat?), and the feature value is
the number of instances of this dependency fea-
ture for word w across a large text corpus. We
also consider a third Combined VSM in which
the word vector is the concatenation of its Doc-
ument and Dependency features. All three mod-
els subsequently normalize frequencies using pos-
itive pointwise mutual-information (PPMI), and
84
are dimensionality reduced using singular value
decomposition (SVD). This is the first systematic
study of the utility of Document and Dependency
features for semantic composition. We construct
all three VSMs (Dependencies, Documents, Com-
bined) using the same text corpus and preprocess-
ing pipeline, and make the resulting VSMs avail-
able for download (http://www.cs.cmu.
edu/?afyshe/papers/conll2013/). To
our knowledge, this is the first freely available
VSM that includes entries for both words and
adjective-noun phrases, and it is built from a much
larger corpus than previously shared resources (16
billion words, 50 million documents). Our main
contributions include:
? We systematically study complementarity of
topical (Document) and type (Dependency)
features in Vector Space Model (VSM)
for semantic composition of adjective-noun
phrases. To the best of our knowledge, this is
one of the first studies of this kind.
? Through extensive experiments on standard
benchmark datasets, we find that a VSM built
from a combination of topical and type fea-
tures is more effective for semantic compo-
sition, compared to a VSM built from Docu-
ment and Dependency features alone.
? We introduce a novel task: to predict the vec-
tor representation of a composed phrase from
the brain activity of human subjects reading
that phrase.
? We explore two composition methods, addi-
tion and dilation, and find that while addition
performs well on corpus-only tasks, dilation
performs best on the brain activity task.
? We build our VSMs, for both phrases and
words, from a large syntactically parsed text
corpus of 16 billion tokens. We also make
the resulting VSM publicly available.
2 Related Work
Mitchell and Lapata (2010) explored several
methods of combining adjective and noun vec-
tors to estimate phrase vectors, and compared
the similarity judgements of humans to the sim-
ilarity of their predicted phrase vectors. They
found that for adjective-noun phrases, type-based
models outperformed Latent Dirichlet Allocation
(LDA) topic models. For the type-based mod-
els, multiplication performed the best, followed
by weighted addition and a dilation model (for de-
tails on composition functions see Section 4.2).
However, Mitchell and Lapata did not combine
the topic- and type-based models, an idea we ex-
plore in detail in this paper.
Baroni and Zamparelli (2010) extended the typ-
ical vector representation of words. Their model
used matrices to represent adjectives, while nouns
were represented with column vectors. The vec-
tors for nouns and adjective-noun phrases were
derived from local word co-occurrence statistics.
The matrix to represent the adjective was esti-
mated with partial least squares regression where
the product of the learned adjective matrix and
the observed noun vector should equal the ob-
served adjective-noun vector. Socher et al (2012)
also extended word representations beyond sim-
ple vectors. Their model assigns each word a vec-
tor and a matrix, which are composed via an non-
linear function (e.g. tanh) to create phrase rep-
resentations consisting of another vector/matrix
pair. This process can proceed recursively, follow-
ing a parse tree to produce a composite sentence
meaning. Other general semantic composition
frameworks have been suggested, e.g. (Sadrzadeh
and Grefenstette, 2011) who focus on the opera-
tional nature of composition, rather than the rep-
resentations that are supplied to the framework.
Here we focus on creating word representations
that are useful for semantic composition.
Turney (2012) published an exploration of the
impact of domain- and function-specific vector
space models, analogous to the topic and type
meanings encoded by our Document and Depen-
dency models respectively. In Turney?s work,
domain-specific information was represented by
noun token co-occurrence statistics within a lo-
cal window, and functional roles were repre-
sented by generalized token/part-of-speech co-
occurrence patterns with verbs - both of which
are relatively local and shallow when compared
with this work. Similar local context-based fea-
tures were used to cluster phrases in (Lin and Wu,
2009). Though the models discussed here are
not entirely comparable to it, a recent comparison
suggested that broader, deeper features such as
ours may result in representations that are superior
for tasks involving neural activation data (Murphy
et al, 2012b).
85
In contrast to the composite model in (Griffiths
et al, 2005), in this paper we explore the com-
plementarity of semantics captured by topical in-
formation and syntactic/semantic types. We fo-
cus on learning VSMs (involving both words and
phrases) for semantic composition, and use more
expressive dependency-based features in our type-
based VSM. A comparison of vector-space repre-
sentations was recently published (Blacoe and La-
pata, 2012), in which the authors compared sev-
eral methods of combining single words vectors
to create phrase vectors. They found that the best
performance for adjective-noun composition used
point-wise multiplication and a model based on
type-based word co-occurrence patterns.
3 Creating a Vector-Space
To create the Dependency vectors, a 16 billion
word subset of ClueWeb09 (Callan and Hoy,
2009) was dependency parsed using the Malt
parser (Hall et al, 2007). Dependency statistics
were then collected for a predetermined list of
target words and adjective-noun phrases, and for
arbitrary adjective-noun phrases observed in the
corpus. The list was composed of the 40 thou-
sand most frequent single tokens in the Ameri-
can National Corpus (Ide and Suderman, 2006),
and a small number of words and phrases used
as stimuli in our brain imaging experiments. Ad-
ditionally, we included any phrase found in the
corpus whose maximal token span matched the
PoS pattern J+N+, where J and N denote adjec-
tive and noun PoS tags respectively. For each
unit (i.e., word or phrase) in this augmented list,
counts of all unit-external dependencies incident
on the head word were aggregated across the cor-
pus, while unit-internal dependencies were ig-
nored. Each token was appended with its PoS tag,
and the dependency edge label was also included.
This resulted in the extraction of 498 million de-
pendency tuples. For example, the dependency tu-
ple (a/DT, NMOD, 27-inch/JJ television/NN,14),
indicates that a/DT was found as a child of 27-
inch/JJ television/NN with a frequency of 14 in
the corpus.
To create Document vectors, word-document
co-occurrence counts were taken from the same
subset of Clueweb, which covered 50 million doc-
uments. We applied feature-selection for compu-
tational efficiency reasons, ranking documents by
the number of target word/phrase types they con-
tained and choosing the top 10 million.
A series of three additional filtering steps
selected target words/phrases, and Docu-
ment/Dependency features for which there was
adequate data.1 First, a co-occurrence frequency
cut-off was used to reduce the dimensionality
of the matrices, and to discard noisy estimates.
A cutoff of 20 was applied to the dependency
counts, and of 2 to document counts. Positive
pointwise-mutual-information (PPMI) was used
as an association measure to normalize the
observed co-occurrence frequency for the varying
frequency of the target word and its features,
and to discard negative associations. Second, the
target list was filtered to the 57 thousand words
and phrases which had at least 20 non-?stop
word? Dependency co-occurrence types, where
a ?stop word? was one of the 100 most frequent
Dependency features observed (so named be-
cause the dependencies were largely incident on
function words). Third, features observed for
no more than one target were removed, as were
empty target entries. The result was a Document
co-occurrence matrix of 55 thousand targets by
5.2 million features (total 172 million non-zero
entries), and a Dependency matrix of 57 thousand
targets by 1.25 million features (total 35 million
non-zero entries).
A singular value decomposition (SVD) matrix
factorization was computed separately on the De-
pendency and Document statistics matrices, with
1000 latent dimensions retained. For this step
we used Python/Scipy implementation of the Im-
plicitly Restarted Arnoldi method (Lehoucq et al,
1998; Jones et al, 2001). This method is com-
patible with PPMI normalization, since a zero
value represents both negative target-feature asso-
ciations, and those that were not observed or fell
below the frequency cut-off. To combine Docu-
ment and Dependency information, we concate-
nate vectors.
4 Experiments
To evaluate how Document and Dependency di-
mensions can interact and compliment each other,
1In earlier experiments with more than 500 thousand
phrasal entries, we found that the majority of targets were
dominated by non-distinctive stop word co-occurrences, re-
sulting in semantically vacuous representations.
86
Table 1: The nearest neighbors of three queries under three VSMs: all 2000 dimensions (Deps & Docs);
1000 Document dimensions (Docs); 1000 Dependency dimensions (Deps).
Query Deps & Docs Docs Deps
beautiful/JJ wonderful/JJ wonderful/JJ lovely/JJ
lovely/JJ fantastic/JJ gorgeous/JJ
excellent/JJ unspoiled/JJ wonderful/JJ
dog/NN cat/NN dogs/NNS cat/NN
dogs/NNS vet/NN the/DT dog/NN
pet/NN leash/NN dogs/NNS
bad/JJ publicity/NN negative/JJ publicity/NN fast/JJ cash/NN loan/NN negative/JJ publicity/NN
bad/JJ press/NN small/JJ business/NN loan/NN bad/JJ press/NN
unpleasantness/NN important/JJ cities/NNS unpleasantness/NN
Concrete Cats Mixed Cats Concrete Sim Mixed Sim Mixed Related0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1Performance of Documents and Dependency Dimensions for Single Word Tasks
Task
Per
form
anc
e
 
 Docs onlyDeps onlyDocs & Deps
Figure 1: Performance of VSMs for single word
behavioral tasks as we vary Document and Depen-
dency inclusion.
we can perform a qualitative comparison between
the nearest neighbors (NNs) of words and phrases
in the three VSMs ? Dependency, Document, and
Combined (Dependency & Document). Results
appear in Table 1. Note that single words and
phrases can be neighbors of each other, demon-
strating that our VSMs can generalize across syn-
tactic types. In the Document VSM, we get more
topically related words as NNs (e.g., vet and leash
for dog); and in the Dependency VSM, we see
words that might substitute for one another in a
sentence (e.g., gorgeous for beautiful). The two
feature sets can work together to up-weight the
most suitable NNs (as in beautiful), or help to
drown out noise (as in the NNs for bad publicity
in the Document VSM).
4.1 Judgements of Word Similarity
As an initial test of the informativeness of Doc-
ument and Dependency features, we evaluate
the representation of single words. Behavioral
judgement benchmarks have been widely used to
evaluate vector space representations (Lund and
Burgess, 1996; Rapp, 2003; Sahlgren, 2006).
Here we used five such tests. Two tests are catego-
rization tests, where we evaluate how well an au-
tomatic clustering of our word vectors correspond
to pre-defined word categories. The first ?Con-
crete Categories? test-set consists of 82 nouns,
each assigned to one of 10 concrete classes (Battig
and Montague, 1969). The second ?Mixed Cat-
egories? test-set contains 402 nouns in a range
of 21 concrete and abstract classes from Word-
Net (Almuhareb and Poesio, 2004; Miller et al,
1990). Both categorization tests were performed
with the Cluto clustering package (Karypis, 2003)
using cosine distances. Success was measured as
percentage purity over clusters based on their plu-
rality class, with chance performance at 10% and
5% respectively for the ?Concrete Categories? and
?Mixed Categories? tests.
The remaining three tests use group judgements
of similarity: the ?Concrete Similarity? set of
65 concrete word pairs (Rubenstein and Goode-
nough, 1965); and two variations on the Word-
Sim353 test-set (Finkelstein et al, 2002), par-
titioned into subsets corresponding to strict at-
tributional similarity (?Mixed Similarity?, 203
noun pairs), and broader topical ?relatedness?
(?Mixed Relatedness?, 252 noun pairs) (Agirre et
al., 2009). Performance on these benchmarks is
Spearman correlation between the aggregate hu-
man judgements and pairwise cosine distances of
word vectors in a VSM.
The results in Figure 1 show that the Depen-
dency VSM substantially outperforms the Docu-
ment VSM when predicting human judgements of
strict attributional (categorial) similarity (?Simi-
larity? as opposed to ?Relatedness?) for concrete
nouns. Conversely the Document VSM is compet-
87
Figure 2: The performance of three phrase representations for predicting the behavioral phrasal similar-
ity scores from Mitchell and Lapata (2010). The highest correlation is 0.5033 and uses 25 Document
dimensions, 600 Dependency dimensions and the addition composition function.
itive for less concrete word types, and for judge-
ments of broader topical relatedness.
4.2 Judgements of Phrase Similarity
We also evaluated our system on behavioral data
of phrase similarity judgements gathered from 18
human informants. The adjective-noun phrase
pairs are divided into 3 groups: high, medium
and low similarity (Mitchell and Lapata, 2010).
For each pair of phrases, informants rated phrase
similarity on a Likert scale of 1-7. There are 36
phrase pairs in each of the three groups for a to-
tal of 108 phrase pairs. Not all of the phrases oc-
curred frequently enough in our corpus to pass our
thresholds, and so were omitted from our analy-
sis. In several cases we also used pluralizations
of the test phrases (e.g.?dark eyes?) where the
singular form was not found in our VSM. After
these changes we were left with 28, 24 and 28
in the high, medium and low groups respectively.
In total we have 80 observed vectors for the 108
phrase pairs. These adjective-noun phrases were
included in the list of targets, so their statistics
were gathered in the same way as for single words.
This does not impact results for composed vectors,
as all of the single words in the phrases do appear
in our VSMs. A full list of the phrase pairs can be
found in Mitchell and Lapata (2010).
To evaluate, we used three different representa-
tions of phrases. For phrase pairs that passed our
thresholds, we can test the similarity of observed
representations by comparing the VSM represen-
tation of the phrase (no composition function).
For all 108 phrase pairs we can test the composed
phrase representations, derived by applying addi-
tion and dilation operations to word vectors. Mul-
tiplication is not used as SVD representations in-
clude negative values, and so the product of two
negative values would be positive.
Addition is the element-wise sum of two se-
mantic feature vectors saddi = sadji +snouni , where
snouni , sadji , and saddi are the ith element of the
noun, adjective, and predicted phrase vectors, re-
spectively. Dilation of two semantic feature vec-
tors sadj and snoun is calculated by first decom-
posing the noun into a component parallel to the
adjective (x) and a component perpendicular to
the adjective (y) so that snoun = x + y. Dilation
then enhances the adjective component by multi-
plying it by a scalar (?): sdilate = ?x+y. This can
be viewed as taking the representation of the noun,
and up-weighting the elements it shares with the
adjective, which is coherent with the notion of co-
composition (Pustejovsky, 1995). Previous work
(Mitchell and Lapata, 2010) tuned the ? parame-
ter (? = 16.7). We use that value here, though
further optimization might increase performance.
For our evaluation we calculated the cosine dis-
tance between pairs of phrases in the three dif-
ferent representation spaces: observed, addition
and dilation. Results for a range of dimension-
ality settings appear in Figure 2. In the observed
space, we maximized performance when we in-
88
cluded all 1000 of the Document and 350 Depen-
dency dimensions. For consistency the y axis in
Figure 2 extends only to 100 Document dimen-
sions: changes beyond 100 dimensions for ob-
served vectors were minimal. By design, SVD
will tend to use lower dimensions to represent the
strongest signals in the input statistics, which typ-
ically originate in the types of targets that are most
frequent ? in this case single words. We have ob-
served that less frequent and noisier counts, as
might be found for many phrases, are displaced
to the higher dimensions. Consistent with this ob-
servation, maximum performance occurs using a
high number of dimensions (correlation of 0.37 to
human judgements of phrase similarity).
Interestingly, using the single word vectors to
predict the phrase vectors via the addition function
gives the best correlation of any of the represen-
tations, outperforming even the observed phrase
representations. When using 25 Document di-
mensions and 600 Dependency dimensions the
correlation is 0.52, compared to the best per-
formance of 0.51 using Dependency dimensions
only. We speculate that the advantage of com-
posed vectors over observed vectors is due to
sparseness and resulting noise/variance in the ob-
served phrase vectors, as phrases are necessarily
less frequent than their constituent words.
The dilation composition function performs
slightly worse than addition, but shows best per-
formance at the same point as addition. Here, the
highest correlation (0.46) is substantially lower
than that attained by addition, and uses 25 dimen-
sions of the Document, and 600 dimensions of the
Dependency VSM.
To summarize, without documents, {observed,
addition and dilation} phrase vectors have maxi-
mal correlations {0.37, 0.51 and 0.46}. With doc-
uments, {observed, addition and dilation} phrase
vectors have maximal correlations {0.37, 0.52 and
0.50}. Our results using the addition function
(0.52) outperform the results in two previous stud-
ies (Mitchell and Lapata, 2010; Blacoe and Lap-
ata, 2012): (0.46 and 0.48 respectively). This is
evidence that a VSM built from a larger corpus,
and with both Document and Dependency infor-
mation can yield superior results.
4.3 Composed vs Observed Phrase Vectors
Next we tested how well our representations and
semantic composition functions could predict the
observed vector statistics for phrases from the
vectors of their component words. Again, we
explored addition and dilation composition func-
tions. For testing we have 13, 575 vectors for
which both the adjective and noun passed our
thresholds. We predicted a composed phrase vec-
tor using the statistics of the single words and
one of the two composition functions (addition
or dilation). We then sorted the list of observed
phrase vectors by their distance to the composed
phrase vector and recorded the position of the
corresponding observed vector in the list. From
this we calculated percentile rank, the percent of
phrases that are further from the predicted vec-
tor than the observed vector. Percentile rank is:
100 ? (1 ? ?rank/N) where ?rank is the aver-
age position of the correct observed vector in the
sorted list and N = 13, 575 is the size of the list.
Figure 3 shows the changes in percentile rank
in response to varying dimensions of Documents
and Dependencies for the addition function. Di-
lation results are not shown, but the pattern of
performance is very similar. In general, when
one includes more Document dimensions, the per-
centile rank increases. For both the dilation and
addition composition functions the peak perfor-
mance is with 750 Dependency dimensions and
1000 Document dimensions. Dilation?s peak per-
formance is 97.87; addition peaks at 98.03 per-
centile rank. As in Section 4.2, we see that the
accurate representation of phrases requires higher
SVD dimensions.
To evaluate when composition fails, we ex-
amined the cases where the percentile rank was
< 25%. Amongst these words we found an over-
representation of operational adjectives like ?bet-
ter? and ?more?. As observed previously, it is
possible that such adjectives could be better rep-
resented with a matrix or function (Socher et al,
2012; Baroni and Zamparelli, 2010). Composi-
tion may also be failing when the adjective-noun
phrase is non-compositional (e.g. lazy susan); fil-
tering such phrases could improve performance.
4.4 Brain Activity Data
Here we explore the relationship between the neu-
ral activity observed when a person reads a phrase,
89
100 250 500 750 100093
93.5
94
94.5
95
95.5
96
96.5
97
97.5
98
Number of Dependency Dimensions
Per
cen
tile 
Ran
k
Percentile Rank for Varing Doc. and Dep. Dimensions (Addition)
 
 
0 Doc Dims25501005007501000
Figure 3: The percentile rank of observed phrase
vectors compared to vectors created using the ad-
dition composition function.
and our predicted composed VSM for that phrase.
We collected brain activity data using Magnetoen-
cephalography (MEG). MEG is a brain imaging
method with much higher temporal resolution (1
ms) than fMRI (?2 sec). Since words are natu-
rally read at a rate of about 2 per second, MEG is a
better candidate for capturing the fast dynamics of
semantic composition in the brain. Some previous
work has explored adjective-noun composition in
the brain (Chang et al, 2009), but used fMRI and
corpus statistics based only on co-occurrence with
5 hand-selected verbs.
Our MEG data was collected while 9 partici-
pants viewed 38 phrases, each repeated 20 times
(randomly interleaved). The stimulus nouns were
chosen because previous research had shown them
to be decodable from MEG recordings, and the ad-
jectives were selected to modulate their most de-
codable semantic properties (e.g. edibility, ma-
nipulability) (Sudre et al, 2012). The 8 adjec-
tives selected are (?big?, ?small?, ?ferocious?,
?gentle?, ?light?, ?heavy?, ?rotten?, ?tasty?), and
the 6 nouns are (?dog?, ?bear?, ?tomato?, ?car-
rot?, ?hammer?, ?shovel?). The words ?big? and
?small? are paired with every noun, ?ferocious?
and ?gentle? with animals, ?light? and ?heavy?
with tools and ?rotten? and ?tasty? with foods.
We also included the words ?the? and the word
?thing? as semantically neutral fillers, to present
each of the words in a condition without seman-
tic modulation. In total there are 38 phrases (e.g.
?rotten carrot?, ?big hammer?).
In the MEG experiment, the adjective and
paired noun were each shown for 500ms, with a
300ms interval between them, and there were 3
Figure 4: Results for predicting composed phrase
vectors (addition [4a] and dilation [4b]) from
MEG recordings. Results shown are the aver-
age over 9 subjects viewing 38 adjective-noun
phrases. This is the one task on which dilation
outperforms addition.
(a) Addition composition function results.
(b) Dilation composition function results.
seconds in total time between the onset of subse-
quent phrases. Data was preprocessed to maxi-
mize the signal/noise ratio as is common practice
? see Gross et al, (2012). The 20 repeated trials
for each phrase were averaged together to create
one average brain image per phrase.
To determine if the recorded MEG data can be
used to predict our composed vector space rep-
resentations, we devised the following classifica-
tion framework.2 The training data is comprised
of the averaged MEG signal for each of the 38
phrases for one subject, and the labels are the 38
phrases. We use our VSMs and composition func-
tions to form a mapping of the 38 phrases to com-
2Predicting brain activity from VSM representations is
also possible, but provides additional challenges, as parts of
the observed brain activity are not driven by semantics.
90
posed semantic feature vectors w ? {s1 . . . sm}.
The mapping allows us to use Zero Shot Learn-
ing (Palatucci et al, 2009) to predict novel phrases
(not seen during training) from a MEG record-
ing. This is a particularly attractive characteris-
tic for the task of predicting words, as there are
many words and many more phrases in the En-
glish language, and one cannot hope to collect
MEG recordings for all of them.
Formally, let us define the semantic represen-
tation of a phrase w as semantic feature vector
~sw = {s1...sm}, where the semantic space has
dimensionm that varies depending on the number
of Document and/or Dependency dimensions we
include. We utilize the mapping w ? {s1 . . . sm}
to train m independent functions f1(X) ?
s?1, . . . , fm(X) ? s?m where s? represents the
value of a predicted composed semantic feature.
We combine the output of f1 . . . fm to create the
final predicted semantic vector ~s? = {s?1 . . . s?m}.
We use cosine distance to quantify the distance be-
tween true and predicted semantic vectors.
To measure performance we use the 2 vs. 2 test.
For each test we withhold two phrases and train
regressors on the remaining 36. We use the re-
gressors f and MEG data from the two held out
phrases to create two predicted semantic vectors.
We then choose the assignment of predicted se-
mantic vectors (~s?i and ~s?j) to true semantic vec-
tors (~si and ~sj) that minimizes the sum of cosine
distances. If we choose the correct assignment
(~s?i 7? ~si and ~s?j 7? ~sj) we mark the test as cor-
rect. 2 vs. 2 accuracy is the number of 2 vs. 2
tests with correct assignments divided by the total
number of tests. There are (38 choose 2) = 703
distinct 2 vs. 2 tests, and we evaluate on the subset
for which neither the adjective nor noun are shared
(540 pairs). Chance performance is 0.50.
For each f we trained a regressor with L2
penalty. We tune the regularization parame-
ter with leave-one-out-cross-validation on training
data. We train regressors using the first 800 ms of
MEG signal after the noun stimulus appears, when
we assume semantic composition is taking place.
Results appear in Figure 4. The best perfor-
mance (2 vs. 2 accuracy of 0.9440) is achieved
with dilation, 800 dimensions of Dependencies
and zero Document dimensions. When we use
the addition composition function, optimal per-
formance is 0.9212, at 600 Dependency and zero
Document dimensions. Note, however, that the
parameter search here was much coarser that in
Sections 4.2 and 4.3, due to the computation re-
quired. We used a finer grid around the peaks in
performance for addition and dilation and found
minimal improvement (?0.5%) with the addition
of a small number of Document dimensions.
It is intriguing that this neurosemantic task is
the only task for which dilation outperforms addi-
tion. All other composition tasks explored in this
study were concerned with matching composed
word vectors to observed or composed word vec-
tors, whereas here we are interested in matching
composed word vectors to observed brain activity.
Perhaps the brain works in a manner more akin to
the emphasis of elements as modeled by dilation,
rather than a summing of features. Further work
is required to fully understand this phenomenon,
but this is surely a thought-provoking result.3
5 Conclusion
We have performed a systematic study of comple-
mentarity of topical (Document) and type (Depen-
dency) features in Vector Space Model (VSM) for
semantic composition of adjective-noun phrases.
To the best of our knowledge, this is one of the
first such studies of this kind. Through experi-
ments on multiple real world benchmark datasets,
we demonstrated the benefit of combining topic-
and type-based features in a VSM. Additionally,
we introduced a novel task of predicting vec-
tor representations of composed phrases from the
brain activity of human subjects reading those
phrases. We exploited a large syntactically parsed
corpus to build our VSM models, and make them
publicly available. We hope that the findings and
resources from this paper will serve to inform fu-
ture work on VSMs and semantic composition.
Acknowledgment
We are thankful to the anonymous reviewers for their con-
structive comments. We thank CMUs Parallel Data Labo-
ratory (PDL) for making the OpenCloud cluster available,
Justin Betteridge (CMU) for his help with parsing the corpus,
and Yahoo! for providing the M45 cluster. This research has
been supported in part by DARPA (under contract number
FA8750-13-2-0005), NIH (NICHD award 1R01HD075328-
01), Keck Foundation (DT123107), NSF (IIS0835797), and
Google. Any opinions, findings, conclusions and recommen-
dations expressed in this paper are the authors and do not
necessarily reflect those of the sponsors.
3No pun intended.
91
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kraval-
ova, and Marius Pas. 2009. A study on similarity and
relatedness using distributional and WordNet-based ap-
proaches. Proceedings of NAACL-HLT 2009.
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: An evalua-
tion. In Proceedings of EMNLP, pages 158?165.
Marco Baroni and Alessandro Lenci. 2010. Distributional
memory: A general framework for corpus-based seman-
tics. Computational Linguistics, 36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1183?1193. Association
for Computational Linguistics.
W F Battig and W E Montague. 1969. Category Norms
for Verbal Items in 56 Categories: A Replication and Ex-
tension of the Connecticut Category Norms. Journal of
Experimental Psychology Monographs, 80(3):1?46.
William Blacoe and Mirella Lapata. 2012. A Comparison of
Vector-based Representations for Semantic Composition.
In Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 546?556, Jeju
Island, Korea.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003.
Latent Dirichlet Allocation. Journal of Machine Learning
Research, 3(4-5):993?1022.
Jamie Callan and Mark Hoy. 2009. The ClueWeb09 Dataset.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
Kai-min Chang, Vladimir L. Cherkassky, Tom M Mitchell,
and Marcel Adam Just. 2009. Quantitative modeling of
the neural representation of adjective-noun phrases to ac-
count for fMRI activation. In Proceedings of the Annual
Meeting of the ACL and the 4th IJCNLP of the AFNLP,
pages 638?646.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: the concept revisited.
ACM Transactions on Information Systems, 20(1):116?
131.
Thomas L Griffiths, Mark Steyvers, David M Blei, and
Joshua B Tenenbaum. 2005. Integrating topics and syn-
tax. Advances in neural information processing systems,
17.
Joachim Gross, Sylvain Baillet, Gareth R. Barnes,
Richard N. Henson, Arjan Hillebrand, Ole Jensen, Karim
Jerbi, Vladimir Litvak, Burkhard Maess, Robert Oost-
enveld, Lauri Parkkonen, Jason R. Taylor, Virginie van
Wassenhove, Michael Wibral, and Jan-Mathijs Schoffe-
len. 2012. Good-practice for conducting and reporting
MEG research. NeuroImage, October.
J Hall, J Nilsson, J Nivre, G Eryigit, B Megyesi, M Nilsson,
and M Saers. 2007. Single Malt or Blended? A Study
in Multilingual Parser Optimization. In Proceedings of
the CoNLL Shared Task Session of EMNLPCoNLL 2007,
volume s. 19-33, pages 933?939. Association for Compu-
tational Linguistics.
Nancy Ide and Keith Suderman. 2006. The American Na-
tional Corpus First Release. Proceedings of the Fifth Lan-
guage Resources and Evaluation Conference (LREC).
Eric Jones, Travis Oliphant, Pearu Peterson, and others.
2001. SciPy: Open source scientific tools for Python.
George Karypis. 2003. CLUTO: A Clustering Toolkit.
Technical Report 02-017, Department of Computer Sci-
ence, University of Minnesota.
T Landauer and S Dumais. 1997. A solution to Plato?s prob-
lem: the latent semantic analysis theory of acquisition, in-
duction, and representation of knowledge. Psychological
Review, 104(2):211?240.
R B Lehoucq, D C Sorensen, and C Yang. 1998. Arpack
users? guide: Solution of large scale eigenvalue problems
with implicitly restarted Arnoldi methods. SIAM.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for
discriminative learning. In Proceedings of the ACL.
Dekang Lin. 1998. Automatic Retrieval and Clustering of
Similar Words. In COLING-ACL, pages 768?774.
K Lund and C Burgess. 1996. Producing high-dimensional
semantic spaces from lexical co-occurrence. Behavior
Research Methods, Instruments, and Computers, 28:203?
208.
George A Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1990. Introduction
to WordNet: an on-line lexical database. International
Journal of Lexicography, 3(4):235?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive science,
34(8):1388?429, November.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012a.
Comparing Abstract and Concrete Conceptual Represen-
tations using Neurosemantic Decoding. In NAACL Work-
shop on Cognitive Modelling and Computational Linguis-
tics.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012b.
Selecting Corpus-Semantic Models for Neurolinguistic
Decoding. In First Joint Conference on Lexical and Com-
putational Semantics (*SEM), pages 114?123, Montreal,
Quebec, Canada.
Brian Murphy, Partha Pratim Talukdar, and Tom Mitchell.
2012c. Learning Effective and Interpretable Semantic
Models using Non-Negative Sparse Embedding. In Inter-
national Conference on Computational Linguistics (COL-
ING 2012), Mumbai, India.
S Pado? and M Lapata. 2007. Dependency-based construc-
tion of semantic space models. Computational Linguis-
tics, 33(2):161?199.
92
Mark Palatucci, Geoffrey Hinton, Dean Pomerleau, and
Tom M Mitchell. 2009. Zero-Shot Learning with Se-
mantic Output Codes. Advances in Neural Information
Processing Systems, 22:1410?1418.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge.
Reinhard Rapp. 2003. Word Sense Discovery Based on
Sense Descriptor Dissimilarity. Proceedings of the Ninth
Machine Translation Summit, pp:315?322.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627?633, October.
Mehrnoosh Sadrzadeh and Edward Grefenstette. 2011. A
Compositional Distributional Semantics Two Concrete
Constructions and some Experimental Evaluations. Lec-
ture Notes in Computer Science, 7052:35?47.
Magnus Sahlgren. 2006. The Word-Space Model: Using dis-
tributional analysis to represent syntagmatic and paradig-
matic relations between words in high-dimensional vector
spaces. Dissertation, Stockholm University.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic Compositionality
through Recursive Matrix-Vector Spaces. In Conference
on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila We-
hbe, Alona Fyshe, Riitta Salmelin, and Tom Mitchell.
2012. Tracking Neural Coding of Perceptual and Seman-
tic Features of Concrete Nouns. NeuroImage, 62(1):463?
451, May.
Peter D Turney. 2012. Domain and Function : A Dual-Space
Model of Semantic Relations and Compositions. Journal
of Artificial Intelligence Research, 44:533?585.
93
