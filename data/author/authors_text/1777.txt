Proceedings of NAACL HLT 2007, Companion Volume, pages 49?52,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Document Similarity Measures to Distinguish  
Native vs. Non-Native Essay Writers 
Olga Gurevich 
Educational Testing Service 
Rosedale & Carter Roads,  
Turnbull 11R 
Princeton, NJ 08541 
ogurevich@ets.org 
Paul Deane 
Educational Testing Service 
Rosedale & Carter Roads,  
Turnbull 11R 
Princeton, NJ 08541 
pdeane@ets.org 
 
Abstract 
The ability to distinguish statistically dif-
ferent populations of speakers or writers 
can be an important asset in many NLP 
applications.  In this paper, we describe a 
method of using document similarity 
measures to describe differences in be-
havior between native and non-native 
speakers of English in a writing task.1 
1 Introduction 
The ability to distinguish statistically different 
populations of speakers or writers can be an impor-
tant asset in many NLP applications.  In this paper, 
we describe a method of using document similarity 
measures to describe differences in behavior be-
tween native and non-native speakers of English in 
a prompt response task. 
We analyzed results from the new TOEFL inte-
grated writing task, described in the next section.  
All task participants received the same set of 
prompts and were asked to summarize them.  The 
resulting essays are all trying to express the same 
?gist? content, so that any measurable differences 
between them must be due to differences in indi-
vidual language ability and style.  Thus the task is 
uniquely suited to measuring differences in linguis-
tic behavior between populations. 
Our measure of document similarity, described 
in section 3, is a combination of word overlap and 
syntactic similarity, also serving as a measure of 
syntactic variability.  The results demonstrate sig-
nificant differences between native and non-native 
                                                        
1 This research was funded while the first author was a Re-
search Postdoctoral Fellow at ETS in Princeton, NJ. 
speakers that cannot be attributed to any demo-
graphic factor other than the language ability itself. 
2 TOEFL Integrated Writing Task and 
Scoring 
The Test of English as a Foreign Language 
(TOEFL) is administered to foreign students wish-
ing to enroll in US or Canadian universities.  It 
aims to measure the extent to which a student has 
acquired English; thus native speakers should on 
average perform better on the test regardless of 
their analytical abilities.  The TOEFL now includes 
a writing component, and pilot studies were con-
ducted with native as well as non-native speakers. 
One of the writing components is an Integrated 
Writing Task.  Students first read an expository 
passage, which remains on the screen throughout 
the task.  Students then hear a segment of a lecture 
concerning the same topic.  However, the lecture 
contradicts and complements the information con-
tained in the reading.  The lecture is heard once; 
students then summarize the lecture and the read-
ing and describe any contradictions between them. 
The resulting essays are scored by human raters 
on a scale of 0 to 5, with 5 being the best possible 
score2.  The highest-scoring essays express ideas 
from both the lecture and the reading using correct 
grammar; the lowest-scoring essays rely on only 
one of the prompts for information and have 
grammatical problems; and the scores in between 
show a combination of both types of deficiencies. 
The test prompt contained passages about the 
advantages and disadvantages of working in 
groups; the reading was 260 words long, the lec-
ture 326 words.  540 non-native speakers and 950 
                                                        
2 Native speaker essays were initially scored with possible 
half-grades such as 2.5.  For purposes of comparison, these 
were rounded down to the nearest integer. 
49
native speakers were tested by ETS in 2004.  ETS 
also collected essential demographic data such as 
native language, educational level, etc., for each 
student.  For later validation, we excluded 1/3 of 
each set, selected at random, thus involving 363 
non-native speakers and 600 native speakers. 
Percent score frequencies
0
5
10
15
20
25
30
35
1 2 3 4 5
Sco re
Non-native
Native
 
Figure 1.  Relative score distributions. 
 
Among the non-native speakers, the most 
common score was 1 (see Fig. 1 for a histogram).  
By contrast, native speaker scores centered around 
3 and showed a normal-type distribution.  The dif-
ference in distributions confirms that the task is 
effective at separating non-native speakers by skill 
level, and is easier for native speakers.  The poten-
tial sources of difficulty include comprehension of 
the reading passage, listening ability and memory 
for the lecture, and the analytical ability to find 
commonalities and differences between the content 
of the reading and the lecture. 
3 Document Similarity Measure 
Due to the design of the TOEFL task, the content 
of the student essays is highly constrained.  The 
aim of the computational measures is to extract 
grammatical and stylistic differences between dif-
ferent essays.  We do this by comparing the essays 
to the reading and lecture prompts.  Our end goal is 
to determine to what extent speakers diverge from 
the prompts while retaining the content.   
The prediction is that native speakers are much 
more likely to paraphrase the prompts while keep-
ing the same gist, whereas non-native speakers are 
likely to either repeat the prompts close to verba-
tim, or diverge from them in ways that do not pre-
serve the gist.  This intuition conforms to previous 
studies of native vs. non-native speakers? text 
summarization (cf. Campbell 1987), although we 
are not aware of any related computational work. 
We begin by measuring lexico-grammatical 
similarity between each essay and the two prompts.  
An essay is represented as a set of features derived 
from its lexico-grammatical content, as described 
below.  The resulting comparison measure goes 
beyond simple word or n-gram overlap by provid-
ing a measure of structural similarity as well.  In 
essence, our method measures to what extent the 
essay expresses the content of the prompt in the 
same words, used in the same syntactic positions. 
3.1 C-rater tuples 
In order to get a measure of syntactic similarity, we 
relied on C-rater (Leacock & Chodorow 2003), an 
automatic scoring engine developed at ETS.  C-
rater includes several basic NLP components, in-
cluding POS tagging, morphological processing, 
anaphora resolution, and shallow parsing.  The 
parsing produces tuples for each clause, which de-
scribe each verb and its syntactic arguments (1). 
(1) CLAUSE: the group spreads responsibil-
ity for a decision to all the members 
TUPLE: :verb: spread :subj: the group :obj: 
responsible :pp.for: for a decide :pp.to: to all 
C-rater does not produce full-sentence trees or 
prepositional phrase attachment.  However, the 
tuples are reasonably accurate on non-native input. 
3.2 Lexical and Syntactic Features 
C-rater produces tuples for each document, often 
several per sentence.  For the current experiment, 
we used the main verb, its subject and object.  We 
then converted each tuple into a set of features, 
which included the following: 
? The verb, subject (pro)noun, and object 
(pro)noun as individual words; 
? All of the words together as a single feature; 
? The verb, subject, and object words with 
their argument roles. 
Each document can now be represented as a set 
of tuple-derived features, or feature vectors. 
3.3 Document Comparison 
Two feature vectors derived from tuples can be 
compared using a cosine measure (Salton 1989).  
The closer to 1 the cosine, the more similar the two 
feature sets.  To compensate for different frequen-
cies of the features and for varying document 
lengths, the feature vectors are weighted using 
standard tf*idf techniques. 
50
In order to estimate the similarity between two 
documents, we use the following procedure.  For 
each tuple vector in Document A, we find the tuple 
in Document B with the maximum cosine to the 
tuple in Document A.  The maximum cosine val-
ues for each tuple are then averaged, resulting in a 
single scalar value for Document A.  We call this 
measure Average Maximum Cosine (AMC). 
We calculated AMCs for each student response 
versus the reading, the lecture, and the reading + 
lecture combined.  This procedure was performed 
for both native and non-native essays.  A detailed 
examination of the resulting trends is in section 4. 
3.4 Other Measures of Document Similarity 
We also performed several measures of document 
similarity that did not include syntactic features. 
Content Vector Analysis 
The student essays and the prompts were compared 
using Content Vector Analysis (CVA), where each 
document was represented as a vector consisting of 
the words in it (Salton 1989).  The tf*idf-weighted 
vectors were compared by a cosine measure. 
For non-native speakers, there was a noticeable 
trend.  At higher score levels (where the score is 
determined by a human rater), student essays 
showed more similarity to both the reading and the 
lecture prompts.  Both the reading and lecture 
similarity trends were significant (linear trend; F= 
MSlinear trend/MSwithin-subjects=63 for the reading; F=71 
for the lecture at 0.05 significance level3).  Thus, 
the rate of vocabulary retention from both prompts 
increases with higher English-language skill level. 
Native speakers showed a similar pattern of in-
creasing cosine similarity between the essay and 
the reading (F=35 at 0.05 significance for the 
trend), and the lecture (F=35 at the 0.05 level). 
BLEU score 
In order to measure the extent to which whole 
chunks of text from the prompt are reproduced in 
the student essays, we used the BLEU score, 
known from studies of machine translation (Pap-
ineni et al 2002).  We used whole essays as sec-
tions of text rather than individual sentences. 
For non-native speakers, the trend was similar 
to that found with CVA: at higher score levels, the 
                                                        
3 All statistical calculations were performed as ANOVA-style 
trend analyses using SPSS. 
overlap between the essays and both prompts in-
creased (F=52.4 at the 0.05 level for the reading; 
F=53.6 for the lecture). 
Native speakers again showed a similar pattern, 
with a significant trend towards more similarity to 
the reading (F=35.6) and the lecture (F=31.3).  
These results were confirmed by a simple n-gram 
overlap measure. 
4 Results 
4.1 Overall similarity to reading and lecture 
The AMC similarity measure, which relies on syn-
tactic as well as lexical similarity, produced some-
what different results from simpler bag-of-word or 
n-gram measures.  In particular, there was a differ-
ence in behavior between native and non-native 
speakers: non-native speakers showed increased 
structural similarity to the lecture with increasing 
scores, but native speakers did not.   
For non-native speakers, the trend of increased 
AMC between the essay and the lecture was sig-
nificant (F=10.9).  On the other hand, there was no 
significant increase in AMC between non-native 
essays and the reading (F=3.4).  Overall, for non-
native speakers the mean AMC was higher for the 
reading than for the lecture (0.114 vs. 0.08). 
Native speakers, by contrast, showed no sig-
nificant trends for either the reading or the lecture.  
Overall, the average AMCs for the reading and the 
lecture were comparable (0.08 vs. 0.075). 
We know from results of CVA and BLEU 
analyses that for both groups of speakers, higher-
scoring essays are more lexically similar to the 
prompts.  Thus, the lack of a trend for native 
speakers must be due to lack of increase in struc-
tural similarity between higher-scoring essays and 
the prompts.  Since better essays are presumably 
better at expressing the content of the prompts, we 
can hypothesize that native speakers paraphrase the 
content more than non-native speakers. 
4.2 Difference between lecture and reading 
The most informative measure of speaker behavior 
was the difference between the Average Maximum 
Cosine with the reading and the lecture, calculated 
by subtracting the lecture AMC from the reading 
AMC.  Here, non-native speakers showed a sig-
nificant downward linear trend with increasing 
51
score (F=6.5; partial eta-squared 0.08), whereas the 
native speakers did not show any trend (F=1.5).  
The AMC differences are plotted in Figure 3. 
AMC difference between reading and 
lecture
-0.05
0
0.05
0.1
0.15
0 1 2 3 4 5
Score
Non-native
Native
 
Figure 2 - AMC difference between reading and 
lecture 
 
Non-native speakers with lower scores rely 
mostly on the reading to produce their response, 
whereas speakers with higher scores rely some-
what more on the lecture than on the reading.  By 
contrast, native speakers show no correlation be-
tween score and reading vs. lecture similarity.  
Thus, there is a significant difference in the overall 
distribution and behavior between native and non-
native speaker populations.  This difference also 
shows that human raters rely on information other 
than simple verbatim similarity to the lecture in 
assigning the overall scores. 
4.3 Other parameters of variation 
For non-native speakers, the best predictor of the 
human-rated score is the difference in AMC be-
tween the reading and the lecture. 
As demonstrated in the previous section, the 
AMC difference does not predict the score for na-
tive speakers.  We analyzed native speaker demo-
graphic data in order to find any other possible 
predictors.  The students? overall listening score, 
their status as monolingual vs. bilingual, their par-
ents? educational levels all failed to predict the es-
say scores.  
5 Discussion and Future Work 
The Average Maximum Cosine measure as de-
scribed in this paper successfully characterizes the 
behavior of native vs. non-native speaker popula-
tions on an integrated writing task.  Less skillful 
non-native speakers show a significant trend of 
relying on the easier, more available prompt (the 
reading) than on the harder prompt (the lecture), 
whereas more skillful readers view the lecture as 
more relevant and rely on it more than on the read-
ing.  This difference can be due to better listening 
comprehension for the lecture and/or better mem-
ory.  By contrast, native speakers rely on both the 
reading and the lecture about the same, and show 
no significant trend across skill levels.  Native 
speakers seem to deviate more from the structure 
of the original prompts while keeping the same 
content, signaling better paraphrasing skills. 
While not a direct measure of gist similarity, 
this technique represents a first step toward detect-
ing paraphrases in written text.  In the immediate 
future, we plan to extend the set of features to in-
clude non-verbatim similarity, such as synonyms 
and words derived by LSA-type comparison (Lan-
dauer et al 1998).  In addition, the syntactic fea-
tures will be expanded to include frequent 
grammatical alternations such as active / passive. 
A rather simple measure such as AMC has al-
ready revealed differences in population distribu-
tions for native vs. non-native speakers.  
Extensions of this method can potentially be used 
to determine if a given essay was written by a na-
tive or a non-native speaker.  For instance, a statis-
tical classifier can be trained to distinguish feature 
sets characteristic for different populations.  Such a 
classifier can be useful in a number of NLP-related 
fields, including information extraction, search, 
and, of course, educational measurement. 
References 
Campbell, C. 1987. Writing with Others? Words: Native 
and Non-Native University Students? Use of Infor-
mation from a Background Reading Text in Aca-
demic Compositions.  Technical Report, UCLA 
Center for Language Education and Research. 
Landauer, T.; Foltz, P. W; and Laham. D. 1998. Intro-
duction to Latent Semantic Analysis. Discourse 
Processes 25: 259-284. 
Leacock, C., & Chodorow, M. 2003. C-rater: Scoring of 
short-answer questions. Computers and the Humani-
ties, 37(4), 389-405. 
Papineni, K; Roukos, S.; Ward, T. and Zhu, W-J. 2002.  
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation.  ACL ?02, p. 311-318. 
Salton, G. 1989. Automatic Text Processing: The Trans-
formation, Analysis, and Retrieval of Information by 
Computer.  Reading, MA: Addison-Weley. 
52
Proceedings of the 43rd Annual Meeting of the ACL, pages 605?613,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Nonparametric Method for Extraction of Candidate Phrasal Terms 
Paul Deane 
Center for Assessment, Design and Scoring 
Educational Testing Service 
pdeane@ets.org 
 
 
Abstract 
This paper introduces a new method for 
identifying candidate phrasal terms (also 
known as multiword units) which applies a 
nonparametric, rank-based heuristic measure. 
Evaluation of this measure, the mutual rank 
ratio metric, shows that it produces better 
results than standard statistical measures when 
applied to this task.  
1 Introduction 
The ordinary vocabulary of a language like 
English contains thousands of phrasal terms -- 
multiword lexical units including compound 
nouns, technical terms, idioms, and fixed 
collocations. The exact number of phrasal terms is 
difficult to determine, as new ones are coined 
regularly, and it is sometimes difficult to determine 
whether a phrase is a fixed term or a regular, 
compositional expression. Accurate identification 
of phrasal terms is important in a variety of 
contexts, including natural language parsing, 
question answering systems, information retrieval 
systems, among others. 
Insofar as phrasal terms function as lexical units, 
their component words tend to cooccur more often, 
to resist substitution or paraphrase, to follow fixed 
syntactic patterns, and to display some degree of 
semantic noncompositionality (Manning, 
1999:183-186). However, none of these 
characteristics are amenable to a simple 
algorithmic interpretation. It is true that various 
term extraction systems have been developed, such 
as Xtract (Smadja 1993), Termight (Dagan & 
Church 1994), and TERMS (Justeson & Katz 
1995) among others (cf. Daille 1996, Jacquemin & 
Tzoukermann 1994, Jacquemin, Klavans, & 
Toukermann 1997, Boguraev & Kennedy 1999, 
Lin 2001). Such systems typically rely on a 
combination of linguistic knowledge and statistical 
association measures. Grammatical patterns, such 
as adjective-noun or noun-noun sequences are 
selected then ranked statistically, and the resulting 
ranked list is either used directly or submitted for 
manual filtering. 
The linguistic filters used in typical term 
extraction systems have no obvious connection 
with the criteria that linguists would argue define a 
phrasal term (noncompositionality, fixed order, 
nonsubstitutability, etc.). They function, instead, to 
reduce the number of a priori improbable terms 
and thus improve precision. The association 
measure does the actual work of distinguishing 
between terms and plausible nonterms. A variety 
of methods have been applied, ranging from simple 
frequency (Justeson & Katz 1995),  modified 
frequency measures such as c-values (Frantzi, 
Anadiou & Mima 2000, Maynard & Anadiou 
2000) and standard statistical significance tests 
such as the t-test, the chi-squared test, and log-
likelihood (Church and Hanks 1990, Dunning 
1993), and information-based methods, e.g. 
pointwise mutual information (Church & Hanks 
1990).  
Several studies of the performance of lexical 
association metrics suggest significant room for 
improvement, but also variability among tasks.  
One series of studies (Krenn 1998, 2000; Evert 
& Krenn 2001, Krenn & Evert 2001; also see Evert 
2004) focused on the use of association metrics to 
identify the best candidates in particular 
grammatical constructions, such as adjective-noun 
pairs or verb plus prepositional phrase 
constructions, and compared the performance of 
simple frequency to several common measures (the 
log-likelihood, the t-test, the chi-squared test, the 
dice coefficient, relative entropy and mutual 
information). In Krenn & Evert 2001, frequency 
outperformed mutual information though not the t-
test, while in Evert and Krenn 2001, log-likelihood 
and the t-test gave the best results, and mutual 
information again performed worse than 
frequency. However, in all these studies 
performance was generally low, with precision 
falling rapidly after the very highest ranked 
phrases in the list. 
By contrast, Schone and Jurafsky (2001) 
evaluate the identification of phrasal terms without 
grammatical filtering on a 6.7 million word extract 
from the TREC databases, applying both WordNet 
and online dictionaries as gold standards. Once 
again, the general level of performance was low, 
with precision falling off rapidly as larger portions 
605
of the n-best list were included, but they report 
better performance with statistical and information 
theoretic measures (including mutual information) 
than with frequency. The overall pattern appears to 
be one where lexical association measures in 
general have very low precision and recall on 
unfiltered data, but perform far better when 
combined with other features which select 
linguistic patterns likely to function as phrasal 
terms. 
The relatively low precision of lexical 
association measures on unfiltered data no doubt 
has multiple explanations, but a logical candidate 
is the failure or inappropriacy of underlying 
statistical assumptions. For instance, many of the 
tests assume a normal distribution, despite the 
highly skewed nature of natural language 
frequency distributions, though this is not the most 
important consideration except at very low n (cf. 
Moore 2004, Evert 2004, ch. 4). More importantly, 
statistical and information-based metrics such as 
the log-likelihood and mutual information measure 
significance or informativeness relative to the 
assumption that the selection of component terms 
is statistically independent. But of course the 
possibilities for combinations of words are 
anything but random and independent. Use of 
linguistic filters such as "attributive adjective 
followed by noun" or "verb plus modifying 
prepositional phrase" arguably has the effect of 
selecting a subset of the language for which the 
standard null hypothesis -- that any word may 
freely be combined with any other word -- may be 
much more accurate. Additionally, many of the 
association measures are defined only for bigrams, 
and do not generalize well to phrasal terms of 
varying length.  
The purpose of this paper is to explore whether 
the identification of candidate phrasal terms can be 
improved by adopting a heuristic which seeks to 
take certain of these statistical issues into account. 
The method to be presented here, the mutual rank 
ratio, is a nonparametric rank-based approach 
which appears to perform significantly better than 
the standard association metrics. 
The body of the paper is organized as follows: 
Section 2 will introduce the statistical 
considerations which provide a rationale for the 
mutual rank ratio heuristic and outline how it is 
calculated. Section 3 will present the data sources 
and evaluation methodologies applied in the rest of 
the paper. Section 4 will evaluate the mutual rank 
ratio statistic and several other lexical association 
measures on a larger corpus than has been used in 
previous evaluations. As will be shown below, the 
mutual rank ratio statistic recognizes phrasal terms 
more effectively than standard statistical measures. 
2 Statistical considerations 
2.1 Highly skewed distributions 
As first observed e.g. by Zipf (1935, 1949) the 
frequency of words and other linguistic units tend 
to follow highly skewed distributions in which 
there are a large number of rare events. Zipf's 
formulation of this relationship for single word 
frequency distributions (Zipf's first law) postulates 
that the frequency of a word is inversely 
proportional to its rank in the frequency 
distribution, or more generally if we rank words by 
frequency and assign rank z, where the function 
fz(z,N) gives the frequency of rank z for a sample 
of size N, Zipf's first law states that: 
  fz(z,N) = C
z?
 
where C is a normalizing constant and ? is a free 
parameter that determines the exact degree of 
skew; typically with single word frequency data, ? 
approximates 1 (Baayen 2001: 14). Ideally, an 
association metric would be designed to maximize 
its statistical validity with respect to the 
distribution which underlies natural language text  
-- which is if not a pure Zipfian distribution at least 
an LNRE (large number of rare events, cf. Baayen 
2001) distribution with a very long tail, containing 
events which differ in probability by many orders 
of magnitude. Unfortunately, research on LNRE 
distributions focuses primarily on unigram 
distributions, and generalizations to bigram and n-
gram distributions on large corpora are not as yet 
clearly feasible (Baayen 2001:221). Yet many of 
the best-performing lexical association measures, 
such as the t-test, assume normal distributions, (cf. 
Dunning 1993) or else (as with mutual 
information) eschew significance testing in favor 
of a generic information-theoretic approach. 
Various strategies could be adopted in this 
situation: finding a better model of the 
distribution,or adopting a nonparametric method.   
2.2 The independence assumption 
Even more importantly, many of the standard 
lexical association measures measure significance 
(or information content) against the default 
assumption that word-choices are statistically 
independent events. This assumption is built into 
the highest-performing measures as observed in 
Evert & Krenn 2001, Krenn & Evert 2001 and 
Schone & Jurafsky 2001. 
This is of course untrue, and justifiable only as a 
simplifying idealization in the absence of a better 
model. The actual probability of any sequence of 
words is strongly influenced by the base 
grammatical and semantic structure of language, 
particularly since phrasal terms usually conform to 
606
the normal rules of linguistic structure. What 
makes a compound noun, or a verb-particle 
construction, into a phrasal term is not deviation 
from the base grammatical pattern for noun-noun 
or verb-particle structures, but rather a further 
pattern (of meaning and usage and thus heightened 
frequency) superimposed on the normal linguistic 
base. There are, of course, entirely aberrant phrasal 
terms, but they constitute the exception rather than 
the rule. 
This state of affairs poses something of a 
chicken-and-the-egg problem, in that statistical 
parsing models have to estimate probabilities from 
the same base data as the lexical association 
measures, so the usual heuristic solution as noted 
above is to impose a linguistic filter on the data, 
with the association measures being applied only 
to the subset thus selected. The result is in effect a 
constrained statistical model in which the 
independence assumption is much more accurate. 
For instance, if the universe of statistical 
possibilities is restricted to the set of sequences in 
which an adjective is followed by a noun, the null 
hypothesis that word choice is independent -- i.e., 
that any adjective may precede any noun -- is a 
reasonable idealization. Without filtering, the 
independence assumption yields the much less 
plausible null hypothesis that any word may appear 
in any order. 
It is thus worth considering whether there are 
any ways to bring additional information to bear on 
the problem of recognizing phrasal terms without 
presupposing statistical independence.  
2.3 Variable length; alternative/overlapping 
phrases 
Phrasal terms vary in length. Typically they 
range from about two to six words in length, but 
critically we cannot judge whether a phrase is 
lexical without considering both shorter and longer 
sequences. 
That is, the statistical comparison that needs to 
be made must apply in principle to the entire set of 
word sequences that must be distinguished from 
phrasal terms, including longer sequences, 
subsequences, and overlapping sequences, despite 
the fact that these are not statistically independent 
events. Of the association metrics mentioned thus 
far, only the C-Value method attempts to take 
direct notice of such word sequence information, 
and then only as a modification to the basic 
information provided by frequency. 
Any solution to the problem of variable length 
must enable normalization allowing direct 
comparison of phrases of different length. Ideally, 
the solution would also address the other issues -- 
the independence assumption and the skewed 
distributions typical of natural language data. 
 
2.4 Mutual expectation 
An interesting proposal which seeks to overcome 
the variable-length issue is the mutual expectation 
metric presented in Dias, Guillor?, and Lopes 
(1999) and implemented in the SENTA system 
(Gil and Dias 2003a). In their approach, the 
frequency of a phrase is normalized by taking into 
account the relative probability of each word 
compared to the phrase.  
Dias, Guillor?, and Lopes take as the foundation 
of their approach the idea that the cohesiveness of 
a text unit can be measured by measuring how 
strongly it resists the loss of any component term. 
This is implemented by considering, for any n-
gram, the set of [continuous or discontinuous]  
(n-1)-grams which can be formed by deleting one 
word from the n-gram. A normalized expectation 
for the n-gram is then calculated as follows: 
 
1 2
1 2
([ , ... ])
([ , ... ])
n
n
p w w w
FPE w w w
 
 
where [w1, w2 ... wn] is the phrase being evaluated 
and FPE([w1, w2 ... wn]) is: 
 
1 2 1
1
^1 ([ , ... ]) [ ... .... ]
n
n i n
i
p w w w p w w w
n
=
? ?? ?
+? ?? ?? ?? ??  
 
where wi is the term omitted from the n-gram. 
 
They then calculate mutual expectation as the 
product of the probability of the n-gram and its 
normalized expectation. 
 This statistic is of interest for two reasons: 
first, it provides a single statistic that can be 
applied to n-grams of any length; second, it is not 
based upon the independence assumption. The core 
statistic, normalized expectation, is essentially 
frequency with a penalty if a phrase contains 
component parts significantly more frequent than 
the phrase itself. 
 It is of course an empirical question how 
well mutual expectation performs (and we shall 
examine this below) but mutual expectation is not 
in any sense a significance test. That is, if we are 
examining a phrase like the east end, the 
conditional probability of east given [__ end] or of 
end given [__ east] may be relatively low (since 
other words can appear in that context) and yet the 
phrase might still be very lexicalized if the 
association of both words with this context were 
significantly stronger than their association for 
607
other phrases. That is, to the extent that phrasal 
terms follow the regular patterns of the language, a 
phrase might have a relatively low conditional 
probability (given the wide range of alternative 
phrases following the same basic linguistic 
patterns) and thus have a low mutual expectation 
yet still occur far more often than one would 
expect from chance. 
   In short, the fundamental insight -- assessing 
how tightly each word is bound to a phrase -- is 
worth adopting. There is, however, good reason to 
suspect that one could improve on this method by  
assessing relative statistical significance for each 
component word without making the independence 
assumption. In the heuristic to be outlined below, a 
nonparametric method is proposed. This method is 
novel: not a modification of mutual expectation, 
but a new technique based on ranks in a Zipfian 
frequency distribution. 
2.5 Rank ratios and mutual rank ratios 
This technique can be justified as follows. For 
each component word in the n-gram, we want to 
know whether the n-gram is more probable for that 
word than we would expect given its behavior with 
other words. Since we do not know what the 
expected shape of this distribution is going to be, a 
nonparametric method using ranks is in order, and 
there is some reason to think that frequency rank 
regardless of n-gram size will be useful. In 
particular, Ha, Sicilia-Garcia, Ming and Smith 
(2002) show that Zipf's law can be extended to the 
combined frequency distribution of n-grams of 
varying length up to rank 6, which entails that the 
relative rank of words in such a combined 
distribution provide a useful estimate of relative 
probability. The availability of new techniques for 
handling large sets of n-gram data (e.g. Gil & Dias 
2003b) make this a relatively feasible task. 
Thus, given a phrase like east end, we can rank 
how often __ end appears with east in comparison 
to how often other phrases appear with east.That 
is, if {__ end, __side, the __, toward the __, etc.} is 
the set of (variable length) n-gram contexts 
associated with east (up to a length cutoff), then 
the actual rank of __ end is the rank we calculate 
by ordering all contexts by the frequency with 
which the actual word appears in the context. 
We also rank the set of contexts associated with 
east by their overall corpus frequency. The 
resulting ranking is the expected rank of __ end 
based upon how often the competing contexts 
appear regardless of which word fills the context. 
The rank ratio (RR) for the word given the 
context can then be defined as: 
 
RR(word,context)  = ( )( )
,
,
ER word context
AR word context
 
 
where ER is the expected rank and AR is the actual 
rank. A normalized, or mutual rank ratio for the n-
gram can then be defined as 
 
2 11, [__ .... ] 2, [ __ ... ] , [ 1, 2... _]( )* ( )...* ( )n nw w w w n w wn RR w RR w RR w
 
The motivation for this method is that it attempts 
to address each of the major issues outlined above 
by providing a nonparametric metric which does 
not make the independence assumption and allows 
scores to be compared across n-grams of different 
lengths. 
    A few notes about the details of the method are 
in order. Actual ranks are assigned by listing all the 
contexts associated with each word in the corpus, 
and then ranking contexts by word, assigning the 
most frequent context for word n the rank 1, next 
next most frequent rank 2, etc. Tied ranks are 
given the median value for the ranks occupied by 
the tie, e.g., if two contexts with the same 
frequency would occupy ranks 2 and 3, they are 
both assigned rank 2.5. Expected ranks are 
calculated for the same set of contexts using the 
same algorithm, but substituting the unconditional 
frequency of the (n-1)-gram for the gram's 
frequency with the target word.1 
3 Data sources and methodology 
The Lexile Corpus is a collection of documents 
covering a wide range of reading materials such as 
a child might encounter at school, more or less 
evenly divided by Lexile (reading level) rating to 
cover all levels of textual complexity from 
kindergarten to college. It contains in excess of 
400 million words of running text, and has been 
made available to the Educational Testing Service 
under a research license by Metametrics 
Corporation. 
This corpus was tokenized using an in-house 
tokenization program, toksent,  which treats most 
punctuation marks as separate tokens but  makes 
single tokens out of common abbreviations, 
numbers like 1,500, and words like o'clock. It 
should be noted that some of the association 
measures are known to perform poorly if 
punctuation marks and common stopwords are 
                                                     
1
 In this study the rank-ratio method was tested for 
bigrams and trigrams only, due to the small number of  
WordNet gold standard items greater than two words in 
length. Work in progress will assess the metrics' 
performance on n-grams of orders four through six.  
608
included; therefore, n-gram sequences containing 
punctuation marks and the 160 most frequent word 
forms were excluded from the analysis so as not to 
bias the results against them. Separate lists of 
bigrams and trigrams were extracted and ranked 
according to several standard word association 
metrics. Rank ratios were calculated from a 
comparison set consisting of all contexts derived 
by this method from bigrams and trigrams, e.g., 
contexts of the form word1__, ___word2, 
___word1 word2, word1 ___ word3, and word1 
word2 ___.2 
Table 1 lists the standard lexical association 
measures tested in section four3. 
The logical evaluation method for phrasal term 
identification is to rank n-grams using each metric 
and then compare the results against a gold 
standard containing known phrasal terms. Since 
Schone and Jurafsky (2001) demonstrated similar 
results whether WordNet or online dictionaries 
were used as a gold standard, WordNet was 
selected. Two separate lists were derived 
containing two- and three-word phrases. The 
choice of WordNet as a gold standard tests ability 
to predict general dictionary headwords rather than 
technical terms, appropriate since the source 
corpus consists of nontechnical text. 
Following Schone & Jurafsky (2001), the bigram 
and trigram lists were ranked by each statistic then 
scored against the gold standard, with results 
evaluated using a figure of merit (FOM) roughly 
characterizable as the area under the precision-
recall curve. The formula is: 
1
1 k
i
i
P
K
=
?  
where Pi (precision at i) equals i/Hi, and Hi is the 
number of n-grams into the ranked n-gram list 
required to find the ith correct phrasal term. 
It should be noted, however, that one of the most 
pressing issues with respect to phrasal terms is that 
they display the same skewed, long-tail 
distribution as ordinary words, with a large 
                                                     
2
 Excluding the 160 most frequent words prevented 
evaluation of a subset of phrasal terms such as verbal 
idioms like act up or go on. Experiments with smaller 
corpora during preliminary work indicated that this 
exclusion did not appear to bias the results. 
3
 Schone & Jurafsky's results indicate similar results 
for log-likelihood & T-score, and strong parallelism 
among information-theoretic measures such as Chi-
Squared, Selectional Association (Resnik 1996), 
Symmetric Conditional Probability (Ferreira and Pereira 
Lopes, 1999) and the Z-Score (Smadja 1993). Thus it 
was not judged necessary to replicate results for all 
methods covered in Schone & Jurafsky (2001). 
proportion of the total displaying very low 
frequencies. This can be measured by considering  
 
Table 1. Some Lexical Association Measures 
the overlap between WordNet and the Lexile 
corpus. A list of 53,764 two-word phrases were 
extracted from WordNet, and 7,613 three-word 
phrases. Even though the Lexile corpus is quite 
large -- in excess of 400 million words of running 
text -- only 19,939 of the two-word phrases and 
                                                     
4
 Due to the computational cost of calculating C-
Values over a very large corpus, C-Values were 
calculated over bigrams and trigrams only. More 
sophisticated versions of the C-Value method such as 
NC-values were not included as these incorporate 
linguistic knowledge and thus fall outside the scope of 
the study. 
METRIC FORMULA 
Frequency 
(Guiliano, 1964) x yf  
Pointwise 
Mutual 
Information 
[PMI] 
(Church & 
Hanks, 1990) 
 
( )xy x y2log /P P P  
True Mutual 
Information 
[TMI] 
 (Manning, 
1999) 
 
( )xy 2 xy x ylog /P P P P  
Chi-Squared 
( 2? ) 
(Church and 
Gale, 1991) { }{ }
,
,
2( )
i X X
Y Y
i j i j
i j
j
f ?
??
?
??
 
T-Score 
(Church & 
Hanks, 1990) 
1 2
2 2
1 2
1 2
x x
s s
n n
?
+
 
C-Values4 
(Frantzi, 
Anadiou & 
Mima 2000) 
2 is not nested
2
log ( )
log ( )
1
( )
( ) a
a
b T
a
f
f
f b
P T
? ?
? ?
?
?
?
?
? ?? ?? ?? ?? ?? ?? ?? ?? ??
 
where ? is the candidate string 
f(?) is its frequency in the corpus 
T? is the set of candidate terms that   
     contain ? 
P(T?) is the number of these  
     candidate terms 
609
1,700 of the three-word phrases are attested in the 
Lexile corpus. 14,045 of the 19,939 attested two-
word phrases occur at least 5 times, 11,384 occur 
at least 10 times, and only 5,366 occur at least 50 
times; in short, the strategy of cutting off the data 
at a threshold sacrifices a large percent of  total 
recall. Thus one of the issues that needs to be 
addressed is the accuracy with which lexical 
association measures can be extended to deal with 
relatively sparse data, e.g., phrases that appear less 
than ten times in the source corpus. 
A second question of interest is the effect of 
filtering for particular linguistic patterns. This is 
another method of prescreening the source data 
which can improve precision but damage recall. In 
the evaluation bigrams were classified as N-N and 
A-N sequences using a dictionary template, with 
the expected effect. For instance, if the WordNet 
two word phrase list is limited only to those which 
could be interpreted as noun-noun or adjective 
noun sequences, N>=5, the total set of WordNet 
terms that can be retrieved is reduced to 9,757..  
4 Evaluation 
Schone and Jurafsky's (2001) study examined 
the performance of various association metrics on  
a corpus of 6.7 million words with a cutoff of 
N=10. The resulting n-gram set had a maximum 
recall of 2,610 phrasal terms from the WordNet 
gold standard, and found the best figure of merit 
for any of the association metrics even with 
linguistic filterering to be 0.265. On the 
significantly larger Lexile corpus N must be set 
higher (around N=50) to make the results 
comparable. The statistics were also calculated for 
N=50, N=10 and N=5 in order to see what the 
effect of including more (relatively rare) n-grams 
would be on the overall performance for each 
statistic. Since many of the statistics are defined 
without interpolation only for bigrams, and the 
number of WordNet trigrams at N=50 is very 
small, the full set of scores were only calculated on 
the bigram data. For trigrams, in addition to rank 
ratio and frequency scores, extended pointwise 
mutual  information and true mutual information 
scores were calculated using the formulas log 
(Pxyz/PxPy Pz)) and Pxyz log (Pxyz/PxPy Pz)). Also, 
since the standard lexical association metrics 
cannot be calculated across different n-gram types, 
results for bigrams and trigrams are presented 
separately for purposes of comparison. 
The results are are shown in Tables 2-5. Two 
points should should be noted in particular. First, 
the rank ratio statistic outperformed the other 
association measures tested across the board. Its 
best performance, a score of 0.323 in the part of 
speech filtered condition with N=50, outdistanced 
METRIC POS Filtered Unfiltered 
RankRatio 0.323 0.196 
Mutual 
Expectancy 
0.144 0.069 
TMI 0.209 0.096 
PMI 0.287 0.166 
Chi-sqr 0.285 0.152 
T-Score 0.154 0.046 
C-Values 0.065 0.048 
Frequency 0.130 0.044 
Table 2. Bigram Scores for Lexical Association  
Measures with N=50 
METRIC POS Filtered Unfiltered 
RankRatio 0.218 0.125 
MutualExpectation 0.140 0.071 
TMI 0.150 0.070 
PMI 0.147 0.065 
Chi-sqr 0.145 0.065 
T-Score 0.112 0.048 
C-Values 0.096 0.036 
Frequency 0.093 0.034 
Table 3. Bigram Scores for Lexical Association  
Measures with N=10 
METRIC POS Filtered Unfiltered 
RankRatio 0.188 0.110 
Mutual 
Expectancy 
0.141 0.073 
TMI 0.131 0.063 
PMI 0.108 0.047 
Chi-sqr 0.107 0.047 
T-Score 0.098 0.043 
C-Values 0.084 0.031 
Frequency 0.081 0.021 
Table 4. Bigram Scores for Lexical Association  
Measures with N=5 
METRIC N=50 N=10 N=5 
RankRatio 0.273 0.137 0.103 
PMI 0.219 0.121 0.059 
TMI 0.137 0.074 0.056 
Frequency 0.089 0.047 0.035 
    
Table 5. Trigram scores for Lexical Association 
Measures at N=50, 10 and 5 without linguistic 
filtering. 
610
the best score  in Schone & Jurafsky's study 
(0.265), and when large numbers of rare bigrams 
were included, at N=10 and N=5, it continued to 
outperform the other measures. Second, the results 
were generally consistent with those reported in 
the literature, and confirmed Schone & Jurafsky's 
observation that the information-theoretic 
measures (such as mutual information and chi-
squared) outperform frequency-based measures 
(such as the T-score and raw frequency.)5 
4.1 Discussion  
One of the potential strengths of this method is 
that is allows for a comparison between n-grams of 
varying lengths. The distribution of scores for the 
gold standard bigrams and trigrams appears to bear 
out the hypothesis that the numbers are comparable 
across n-gram length. Trigrams constitute 
approximately four percent of the gold standard 
test set, and appear in roughly the same percentage 
across the rankings; for instance, they consistute 
3.8% of the top 10,000 ngrams ranked by mutual 
rank ratio. Comparison of trigrams with their 
component bigrams also seems consistent with this 
hypothesis; e.g., the bigram Booker T. has a higher 
mutual rank ratio than the trigram Booker T. 
Washington, which has a higher rank that the 
bigram T. Washington. These results suggest that it 
would be worthwhile to examine how well the 
method succeeds at ranking n-grams of varying 
lengths, though the limitations of the current 
evaluation set to bigrams and trigrams prevented a 
full evaluation of its effectiveness across n-grams 
of varying length. 
The results of this study appear to support the 
conclusion that the Mutual Rank Ratio performs 
notably better than other association measures on 
this task. The performance is superior to the next-
best measure when N is set as low as 5 (0.110 
compared to 0.073 for Mutual Expectation and 
0.063 for true mutual information and less than .05 
for all other metrics). While this score is still fairly 
low, it indicates that the measure performs 
relatively well even when large numbers of low-
probability n-grams are included. An examination 
of the n-best list for the Mutual Rank ratio at N=5 
supports this contention.  
The top 10 bigrams are:  
                                                     
5
 Schone and Jurafsky's results differ from Krenn & 
Evert (2001)'s results, which indicated that frequency 
performed better than the statistical measures in almost 
every case. However, Krenn and Evert's data consisted 
of n-grams preselected to fit particular collocational 
patterns. Frequency-based metrics seem to be 
particularly benefited by linguistic prefiltering. 
Julius Caesar, Winston Churchill, potato chips, peanut 
butter, Frederick Douglass, Ronald Reagan, Tia 
Dolores, Don Quixote, cash register, Santa Claus 
     At ranks 3,000 to 3,010, the bigrams are:  
Ted Williams, surgical technicians, Buffalo Bill, drug 
dealer, Lise Meitner, Butch Cassidy, Sandra Cisneros, 
Trey Granger,  senior prom, Ruta Skadi 
At ranks 10,000 to 10,010, the bigrams are: 
egg beater, sperm cells, lowercase letters, methane gas, 
white settlers, training program, instantly recognizable, 
dried beef, television screens, vienna sausages 
In short, the n-best list returned by the mutual 
rank ratio statistic appears to consist primarily of 
phrasal terms far down the list, even when N is as 
low as 5. False positives are typically: (i) 
morphological variants of established phrases; (ii) 
bigrams that are part of longer phrases, such as 
cream sundae (from ice cream sundae); (iii) 
examples of highly productive constructions such 
as an artist, three categories or January 2. 
The results for trigrams are relatively sparse and 
thus less conclusive, but are consistent with the 
bigram results: the mutual rank ratio measure 
performs best, with top ranking elements 
consistently being phrasal terms.  
Comparison with the n-best list for other metrics 
bears out the qualitative impression that the rank 
ratio is performing better at selecting phrasal terms 
even without filtering. The top ten bigrams for the 
true mutual information metric at N=5 are: 
a little, did not, this is, united states, new york, know 
what, a good, a long, a moment, a small 
Ranks 3000 to 3010 are: 
waste time, heavily on, earlier than, daddy said, ethnic 
groups, tropical rain, felt sure, raw materials, gold 
medals, gold rush 
Ranks 10,000 to 10,010 are: 
quite close, upstairs window, object is, lord god, private 
schools, nat turner, fire going, bering sea,little higher, 
got lots 
The behavior is consistent with known weaknesses 
of true mutual information -- its tendency to 
overvalue frequent forms.  
Next, consider the n-best lists for log-
likelihood at N=5. The top ten n-grams are: 
sheriff poulson, simon huggett, robin redbreast, eric 
torrosian, colonel hillandale, colonel sapp, nurse 
leatheran, st. catherines, karen torrio, jenny yonge 
N-grams 3000 to 3010 are: 
comes then, stuff who, dinner get, captain see, tom see, 
couple get, fish see, picture go, building go, makes will, 
pointed way 
611
N-grams 10000 to 10010 are: 
sayings is, writ this, llama on, undoing this, dwahro did, 
reno on, squirted on, hardens like, mora did, millicent 
is, vets did 
Comparison thus seems to suggest that if anything 
the quality of the mutual rank ratio results are 
being understated by the evaluation metric, as the 
metric is returning a large number of phrasal terms 
in the higher portion of the n-best list that are 
absent from the gold standard. 
Conclusion 
This study has proposed a new method for 
measuring strength of lexical association for 
candidate phrasal terms based upon the use of 
Zipfian ranks over a frequency distribution 
combining n-grams of varying length. The method 
is related in general philosophy of Mutual 
Expectation, in that it assesses the strenght of 
connection for each word to the combined phrase; 
it differs by adopting a nonparametric measure of 
strength of association. Evaluation indicates that 
this method may outperform standard lexical 
association measures, including mutual 
information, chi-squared, log-likelihood, and the 
T-score. 
References  
Baayen, R. H. (2001) Word Frequency Distributions. 
Kluwer: Dordrecht. 
Boguraev, B. and  C. Kennedy (1999). Applications 
of Term Identification Technology: Domain 
Description and Content Characterization. Natural 
Language Engineering 5(1):17-44. 
Choueka, Y. (1988). Looking for needles in a 
haystack or locating interesting collocation 
expressions in large textual databases. Proceedings 
of the RIAO, pages 38-43. 
Church, K.W., and  P. Hanks (1990). Word 
association norms, mutual information, and 
lexicography. Computational Linguistics 16(1):22-
29. 
Dagan, I. and K.W. Church (1994). Termight: 
Identifying and translating technical terminology. 
ACM International Conference Proceeding 
Series: Proceedings of the fourth conference 
on Applied natural language processing, pages 
39-40. 
Daille, B. 1996. "Study and Implementation of 
Combined Techniques from Automatic Extraction 
of Terminology". Chap. 3 of "The Balancing Act": 
Combining Symbolic and Statistical Approaches to 
Kanguage (Klavans, J., Resnik, P.  (eds.)), pages 
49-66.  
Dias, G., S. Guillor?, and J.G. Pereira Lopes (1999), 
Language independent automatic acquisition of 
rigid multiword units from unrestricted text 
corpora. TALN, p. 333-338. 
Dunning, T. (1993). Accurate methods for the 
statistics of surprise and coincidence. 
Computational Linguistics 19(1): 65-74. 
Evert, S. (2004). The Statistics of Word 
Cooccurrences: Word Pairs and Collocations. Phd 
Thesis, Institut f?r maschinelle 
Sprachverarbeitung, University of Stuttgart. 
Evert, S. and B. Krenn. (2001). Methods for the 
Qualitative Evaluation of Lexical Association 
Measures. Proceedings of the 39th Annual Meeting 
of the Association for Computational Linguistics, 
pages 188-195. 
Ferreira da Silva, J. and G. Pereira Lopes (1999). A 
local maxima method and a fair dispersion 
normalization for extracting multiword units from 
corpora. Sixth Meeting on Mathematics of 
Language, pages 369-381. 
Frantzi, K., S. Ananiadou, and H. Mima. (2000). 
Automatic recognition of multiword terms: the C-
Value and NC-Value Method. International 
Journal on Digital Libraries 3(2):115-130. 
Gil, A. and G. Dias. (2003a). Efficient Mining of 
Textual Associations. International Conference on 
Natural Language Processing and Knowledge 
Engineering. Chengqing Zong (eds.) pages 26-29.  
Gil, A. and G. Dias (2003b). Using masks, suffix 
array-based data structures, and multidimensional 
arrays to compute positional n-gram statistics from 
corpora. In Proceedings of the Workshop on 
Multiword Expressions of the 41st Annual Meeting 
of the Association of Computational Linguistics, 
pages 25-33. 
Ha, L.Q., E.I. Sicilia-Garcia, J. Ming and F.J. Smith. 
(2002), "Extension of Zipf's law to words and 
phrases", Proceedings of the 19th International 
Conference on Computational Linguistics 
(COLING'2002), pages 315-320.  
Jacquemin, C. and E. Tzoukermann. (1999). NLP for 
Term Variant Extraction: Synergy between 
Morphology, Lexicon, and Syntax. Natural 
Language Processing Information Retrieval, pages 
25-74. Kuwer, Boston, MA, U.S.A. 
Jacquemin, C., J.L. Klavans and E. Tzoukermann 
(1997). Expansion of multiword terms for indexing 
and retrieval using morphology and syntax. 
Proceedings of the 35th Annual Meeting of the 
Association for Computational Linguistics, pages 
24-31. 
612
Johansson, C. 1994b, Catching the Cheshire Cat, In 
Proceedings of COLING 94, Vol. II, pages 1021 -
1025. 
Johansson, C. 1996. Good Bigrams. In Proceedings 
from the 16th International Conference on 
Computational Linguistics (COLING-96), pages 
592-597. 
Justeson, J.S. and S.M. Katz (1995). Technical 
terminology: some linguistic properties and an 
algorithm for identification in text. Natural 
Language Engineering 1:9-27. 
Krenn, B. 1998. Acquisition of Phraseological Units 
from Linguistically Interpreted Corpora. A Case 
Study on German PP-Verb Collocations. 
Proceedings of ISP-98, pages 359-371. 
Krenn, B. 2000. Empirical Implications on Lexical 
Association Measures. Proceedings of The Ninth 
EURALEX International Congress. 
Krenn, B. and S. Evert. 2001. Can we do better than 
frequency? A case study on extracting PP-verb 
collocations. Proceedings of the ACL Workshop 
on Collocations, pages 39-46. 
Lin, D. 1998. Extracting Collocations from Text 
Corpora. First Workshop on Computational 
Terminology, pages 57-63  
Lin, D. 1999. Automatic Identification of Non-
compositional Phrases, In Proceedings of The 37th 
Annual Meeting of the Association For 
Computational Lingusitics, pages 317-324. 
Manning, C.D. and H. Sch?tze. (1999). Foundations 
of Statistical Natural Language Processing. MIT 
Press, Cambridge, MA, U.S.A. 
Maynard, D. and S. Ananiadou. (2000). Identifying 
Terms by their Family and Friends. COLING 
2000, pages 530-536. 
Pantel, P. and D. Lin. (2001). A Statistical Corpus-
Based Term Extractor. In: Stroulia, E. and Matwin, 
S. (Eds.) AI 2001, Lecture Notes in Artificial 
Intelligence, pages 36-46. Springer-Verlag. 
Resnik, P. (1996). Selectional constraints: an 
information-theoretic model and its computational 
realization. Cognition 61: 127-159. 
Schone, P. and D. Jurafsky, 2001. Is Knowledge-
Free Induction of Multiword Unit Dictionary 
Headwords a Solved Problem? Proceedings of 
Empirical Methods in Natural Language 
Processing, pages 100-108. 
Sekine, S., J. J. Carroll, S. Ananiadou, and J. Tsujii. 
1992. Automatic  Learning for Semantic 
Collocation. Proceedings of the 3rd Conference on 
Applied Natural Language Processing, pages 104-
110. 
Shimohata, S., T. Sugio, and J. Nagata. (1997). 
Retrieving collocations by co-occurrences and 
word order constraints. Proceedings of the 35th 
Annual Meeting of the Association for 
Computational Linguistics, pages 476-481. 
Smadja, F. (1993). Retrieving collocations from text: 
Xtract. Computational Linguistics, 19:143-177. 
Thanapoulos, A., N. Fakotakis and G. Kokkinkais. 
2002. Comparaitve Evaluation of Collocation 
Extraction Metrics. Proceedings of the LREC 2002 
Conference, pages 609-613. 
Zipf, P. (1935). Psychobiology of Language. 
Houghton-Mifflin, New York, New York. 
Zipf, P.(1949). Human Behavior and the Principle of 
Least Effort. Addison-Wesley, Cambridge, Mass. 
 
613
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 57?64,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
The Web is not a PERSON, Berners-Lee is not an ORGANIZATION, and
African-Americans are not LOCATIONS:
An Analysis of the Performance of Named-Entity Recognition
Robert Krovetz
Lexical Research
Hillsborough, NJ 08844
rkrovetz@lexicalresearch.com
Paul Deane Nitin Madnani
Educational Testing Service
Princeton, NJ 08541
{pdeane,nmadnani}@ets.org
Abstract
Most work on evaluation of named-entity
recognition has been done in the context of
competitions, as a part of Information Extrac-
tion. There has been little work on any form of
extrinsic evaluation, and how one tagger com-
pares with another on the major classes: PER-
SON, ORGANIZATION, and LOCATION.
We report on a comparison of three state-of-
the-art named entity taggers: Stanford, LBJ,
and IdentiFinder. The taggers were compared
with respect to: 1) Agreement rate on the clas-
sification of entities by class, and 2) Percent-
age of ambiguous entities (belonging to more
than one class) co-occurring in a document.
We found that the agreement between the tag-
gers ranged from 34% to 58%, depending on
the class and that more than 40% of the glob-
ally ambiguous entities co-occur within the
same document. We also propose a unit test
based on the problems we encountered.
1 Introduction
Named-Entity Recognition (NER) has been an im-
portant task in Computational Linguistics for more
than 15 years. The aim is to recognize and clas-
sify different types of entities in text. These might
be people?s names, or organizations, or locations, as
well as dates, times, and currencies. Performance
assessment is usually made in the context of In-
formation Extraction, of which NER is generally a
component. Competitions have been held from the
earliest days of MUC (Message Understanding Con-
ference), to the more recent shared tasks in CoNLL.
Recent research has focused on non-English lan-
guages such as Spanish, Dutch, and German (Meul-
der et al, 2002; Carreras et al, 2003; Rossler, 2004),
and on improving the performance of unsupervised
learning methods (Nadeau et al, 2006; Elsner et al,
2009).
There are no well-established standards for eval-
uation of NER. Since criteria for membership in the
classes can change from one competition to another,
it is often not possible to compare performance di-
rectly. Moreover, since some of the systems in the
competition may use proprietary software, the re-
sults in a competition might not be replicable by
others in the community; however, this applies to
the state of the art for most NLP applications rather
than just NER.
Our work is motivated by a vocabulary as-
sessment project in which we needed to identify
multi-word expressions and determine their asso-
ciation with other words and phrases. However,
we found that state-of-the-art software for named-
entity recognition was not reliable; false positives
and tagging inconsistencies significantly hindered
our work. These results led us to examine the state-
of-the-art in more detail.
The field of Information Extraction (IE) has been
heavily influenced by the Information Retrieval (IR)
community when it comes to evaluation of system
performance. The use of Recall and Precision met-
rics for evaluating IE comes from the IR commu-
nity. However, while the IR community regularly
conducts a set of competitions and shared tasks us-
ing standardized test collections, the IE community
does not. Furthermore, NER is just one component
57
of an IE pipeline and any proposed improvements
to this component must be evaluated by determining
whether the performance of the overall IE pipeline
has improved. However, most, if not all, NER eval-
uations and shared tasks only focus on intrinsic NER
performance and ignore any form of extrinsic eval-
uation. One of the contributions of this paper is
a freely available unit test based on the systematic
problems we found with existing taggers.
2 Evaluation Methodology
We compared three state-of-the-art NER taggers:
one from Stanford University (henceforth, Stanford
tagger), one from the University of Illinois (hence-
forth, the LBJ tagger) and BBN IdentiFinder (hence-
forth, IdentiFinder).
The Stanford Tagger is based on Conditional Ran-
dom Fields (Finkel et al, 2005). It was trained on
100 million words from the English Gigawords cor-
pus. The LBJ Tagger is based on a regularized av-
erage perceptron (Ratinov and Roth, 2009). It was
trained on a subset of the Reuters 1996 news cor-
pus, a subset of the North American News Corpus,
and a set of 20 web pages. The features for both
these taggers are based on local context for a target
word, orthographic features, label sequences, and
distributional similarity. Both taggers include non-
local features to ensure consistency in the tagging of
identical tokens that are in close proximity. Identi-
Finder is a state-of-the-art commercial NER tagger
that uses Hidden Markov Models (HMMs) (Bikel et
al., 1999).
Since we did not have gold standard annotations
for any of the real-world data we evaluated on, we
instead compared the three taggers along two dimen-
sions:
? Agreement on classification. How well do
the taggers work on the three most diffi-
cult classes: PERSON, ORGANIZATION, and
LOCATION and, more importantly, to what
extent does one tagger agree with another?
What types of mistakes do they make system-
atically?1
1Although one could draw a distinction between named en-
tity identification and classification, we focus on the final output
of the taggers, i.e., classified named entities.
? Ambiguity in discourse. Although entities
can potentially have more than one entity clas-
sification, such as Clinton (PERSON or LO-
CATION), it would be surprising if they co-
occurred in a single discourse unit such as a
document. How frequently does each tagger
produce multiple classifications for the same
entity in a single document?
We first compared the two freely available, aca-
demic taggers (Stanford and LBJ) on a corpus of
425 million words that is used internally at the Ed-
ucational Testing Service. Note that we could not
compare these two taggers to IdentiFinder on this
corpus since IdentiFinder is not available for public
use without a license.
Next, we compared all three taggers on the Amer-
ican National Corpus. The American National Cor-
pus (ANC) has recently released a copy which is
tagged by IdentiFinder.2 Since the ANC is a pub-
licly available corpus, we tagged it using both the
Stanford and LBJ taggers and could then compare
all three taggers along the two intended dimensions.
We found that the public corpus had many of the
same problems as the ones we found with our in-
ternally used corpus. Some of these problems have
been discussed before (Marrero et al, 2009) but not
in sufficient detail.
The following section describes our evaluation of
the Stanford and LBJ taggers on the internal ETS
corpus. Section 4 describes a comparison of all three
taggers on the American National Corpus. Section 5
describes the unit test we propose. In Section 6, we
propose and discuss the viability of the ?one named-
entity tag per discourse? hypothesis. In Section 7,
we highlight the problems we find during our com-
parisons and propose a methodology for improved
intrinsic evaluation for NER. Finally, we conclude
in Section 8.
3 Comparing Stanford and LBJ
In this section, we compare the two academic tag-
gers in terms of classification agreement by class
and discourse ambiguity on the ETS SourceFinder
corpus, a heterogeneous corpus containing approx-
imately 425 million words, and more than 270, 000
2http://www.anc.org/annotations.html
58
Person Organization Location
Stanford LBJ Stanford LBJ Stanford LBJ
Shiloh A.sub.1 RNA Santa Barbara Hebrew The New Republic
Yale What Arnold FIGURE ASCII DNA
Motown Jurassic Park NaCl Number: Tina Mom
Le Monde Auschwitz AARGH OMITTED Jr. Ph.D
Drosophila T. Rex Drosophila Middle Ages Drosophila Drosophila
Table 1: A sampling of false positives for each class as tagged by the Stanford and LBJ taggers
Common Entities Percentage
Person 548,864 58%
Organization 249,888 34%
Location 102,332 37%
Table 2: Agreement rate by class between the Stanford and LBJ taggers
articles. The articles were extracted from a set of
60 different journals, newspapers and magazines fo-
cused on both literary and scientific topics.
Although Named Entity Recognition is reported
in the literature to have an accuracy rate of 85-95%
(Finkel et al, 2005; Ratinov and Roth, 2009), it was
clear by inspection that both the Stanford and the
LBJ tagger made a number of mistakes. The ETS
corpus begins with an article about Tim Berners-
Lee, the man who created the World Wide Web.
At the beginning of the article, ?Tim? as well as
?Berners-Lee? are correctly tagged by the Stanford
tagger as belonging to the PERSON class. But
later in the same article, ?Berners-Lee? is incorrectly
tagged as ORGANIZATION. The LBJ tagger makes
many mistakes as well, but they are not necessarily
the same mistakes as the mistakes made by the Stan-
ford tagger. For example, the LBJ tagger sometimes
classifies ?The Web? as a PERSON, and the Stan-
ford tagger classifies ?Italian? as a LOCATION.3
Table 1 provides an anecdotal list of the ?entities?
that were misclassified by the two taggers.4
Both taggers produced about the same number
of entities overall: 1.95 million for Stanford, and
3?Italian? is classified primarily as MISC by the LBJ tagger.
These terms are sometimes called Gentilics or Demonyms.
4Both taggers can use a fourth class MISC in addition to
the standard entity classes PERSON, ORGANIZATION, and
LOCATION. We ran Stanford without the MISC class and LBJ
with MISC. However, the problems highlighted in this paper
remain equally prevalent even without this discrepancy.
1.8 million for LBJ. The agreement rate between
the taggers is shown in Table 2. We find that the
highest rate of agreement is for PERSONS, with
an agreement rate of 58%. The agreement rate on
LOCATIONS is 37%, and the agreement rate on
ORGANIZATIONS is 34%. Even on cases where
the taggers agree, the classification can be incorrect.
Both taggers classify ?African Americans? as LO-
CATIONS.5 Both treat ?Jr.? as being part of a per-
son?s name, as well as being a LOCATION (in fact,
the tagging of ?Jr.? as a LOCATION is more fre-
quent in both).
For our second evaluation criterion, i.e., within-
discourse ambiguity, we determined the percent-
age of globally ambiguous entities (entities that had
more than one classification across the entire corpus)
that occurred with multiple taggings within a single
document. This analysis showed that the problems
described above are not anecdotal. Table 3 shows
that at least 40% of the entities that have more than
one classification co-occur within a document. This
is true for both taggers and all of the named entity
classes.6
5The LBJ tagger classifies the majority of instances of
?African American? as MISC.
6The LBJ tagger also includes the class MISC. We looked at
the co-occurrence rate between the different classes and MISC,
and we found that the majority of each group co-occurred within
a document there as well.
59
Stanford LBJ
Overlap Co-occurrence Overlap Co-occurrence
Person-Organization 98,776 40% 58,574 68%
Person-Location 72,296 62% 55,376 69%
Organization-Location 80,337 45% 64,399 63%
Table 3: Co-occurrence rates between entities with more than one tag for Stanford and LBJ taggers
Stanford-BBN LBJ-BBN
Common Entities Percentage Common Entities Percentage
Person 8034 28% 27,687 53%
Organization 12533 50% 21,777 51%
Location(GPE) 3289 28% 5475 47%
Table 4: Agreement rate by class between the Stanford (and LBJ) and BBN IdentiFinder taggers on the ANC Corpus
4 Comparing All 3 Taggers
A copy of the American National Corpus was re-
cently released with a tagging by IdentiFinder. We
tagged the corpus with the Stanford and LBJ tagger
to see how the results compared.
We found many of the same problems with the
American National Corpus as we found with the
SourceFinder corpus used in the previous section.
The taggers performed very well for entities that
were common in each class, but we found misclas-
sifications even for terms at the head of the Zipfian
curve. Terms such as ?Drosophila? and ?RNA? were
classified as a LOCATION. ?Affymetrix? was clas-
sified as a PERSON, LOCATION, and ORGANI-
ZATION.
Table 4 shows the agreement rate between the
Stanford and IdentiFinder taggers as well as that be-
tween the LBJ and IdentiFinder taggers. A sample
of terms that were classified as belonging to more
than one class, across all 3 taggers, is given in Table
5.
All taggers differ in how the entities are tok-
enized. The Stanford tagger tags each component
word of the multi-word expressions separately. For
example, ?John Smith? is tagged as John/PERSON
and Smith/PERSON. But it would be tagged as
[PER John Smith] by the LBJ tagger, and similarly
by IdentiFinder. This results in a higher overlap be-
tween classes in general, and there is a greater agree-
ment rate between LBJ and IdentiFinder than be-
tween Stanford and either one.
The taggers also differ in the number of entities
that are recognized overall, and the percentage that
are classified in each category. IdentiFinder recog-
nizes significantly more ORGANIZATION entities
than Stanford and LBJ. IdentiFinder also uses a GPE
(Geo-Political Entity) category that is not found in
the other two. This splits the LOCATION class. We
found that many of the entities that were classified as
LOCATION by the other two taggers were classified
as GPE by IdentiFinder.
Although the taggers differ in tokenization as well
as categories, the results on ambiguity in a discourse
support our findings on the larger corpus. The re-
sults are shown in Table 6. For both the Stanford and
LBJ tagger, between 42% and 58% of the entities
with more than one classification co-occur within a
document. For IdentiFinder, the co-occurrence rate
was high for two of the groupings, but significantly
less for PERSON and GPE.
5 Unit Test for NER
We created a unit test based on our experiences in
comparing the different taggers. We were particular
about choosing examples that test the following:
1. Capitalized, upper case, and lower case ver-
sions of entities that are true positives for PER-
SON, ORGANIZATION, and LOCATION (for
a variety of frequency ranges).
2. Terms that are entirely in upper case that are not
named entities (such as RNA and AAARGH).
60
Person/Organization Person/Location Organization/Location
Bacillus Bacillus Affymetrix
Michelob Aristotle Arp2/3
Phenylsepharose ArrayOligoSelector ANOVA
Synagogue Auschwitz Godzilla
Transactionalism Btk:ER Macbeth
Table 5: A sampling of terms that were tagged as belonging to more than one class in the American National Corpus
Stanford LBJ IdentiFinder
Overlap Co-occurrence Overlap Co-occurrence Overlap Co-occurrence
Person-Org 5738 53% 2311 58% 8379 57%
Person-Loc(GPE) 4126 58% 3283 43% 2412 22%
Org-Loc(GPE) 5109 57% 4592 50% 4093 60%
Table 6: Co-occurrence rates between entities with more than one tag for the American National Corpus
3. Terms that contain punctuation marks such as
hyphens, and expressions (such as ?A.sub.1?)
that are clearly not named entities.
4. Terms that contain an initial, such as ?T. Rex?,
?M.I.T?, and ?L.B.J.?
5. Acronym forms such as ETS and MIT, some
with an expanded form and some without.
6. Last names that appear in close proximity to the
full name (first and last). This is to check on the
impact of discourse and consistency of tagging.
7. Terms that contain a preposition, such as ?Mas-
sachusetts Institute of Technology?. This is in-
tended to test for correct extent in identifying
the entity.
8. Terms that are a part of a location as well as an
organization. For example, ?Amherst, MA? vs.
?Amherst College?.
An excerpt from this unit test is shown in Table 7.
We provide more information about the full unit test
at the end of the paper.
6 One Named-Entity Tag per Discourse
Previous papers have noted that it would be unusual
for multiple occurrences of a token in a document to
be classified as a different type of entity (Mikheev
et al, 1999; Curran and Clark, 2003). The Stan-
ford and LBJ taggers have features for non-local de-
pendencies for this reason. The observation is sim-
ilar to a hypothesis proposed by Gale, Church, and
Yarowsky with respect to word-sense disambigua-
tion and discourse (Gale et al, 1992). They hypoth-
esized that when an ambiguous word appears in a
document, all subsequent instances of that word in
the document will have the same sense. This hy-
pothesis is incorrect for word senses that we find in
a dictionary (Krovetz, 1998) but is likely to be cor-
rect for the subset of the senses that are homony-
mous (unrelated in meaning). Ambiguity between
named entities is similar to homonymy, and for most
entities it is unlikely that they would co-occur in a
document.7 However, there are cases that are excep-
tions. For example, Finkel et al (2005) note that in
the CoNLL dataset, the same term can be used for a
location and for the name of a sports team. Ratinov
and Roth (2009) note that ?Australia? (LOCATION)
can occur in the same document as ?Bank of Aus-
tralia? (ORGANIZATION).
Existing taggers treat the non-local dependencies
as a way of dealing with the sparse data problem,
and as a way to resolve tagging differences by look-
ing at how often one token is classified as one type
7Krovetz (1998) provides some examples where different
named entities co-occur in a discourse, such as ?New York?
(city) and ?New York? (state). However, these are both in the
same class (LOCATION) and are related to each other.
61
This is not a Unit Test
(a tribute to Rene Magritte and RMS)
Although we created this test with humor, we intend it as a serious
test of the phenomena we encountered. These problems include
ambiguity between entities (such as Bill Clinton and Clinton,
Michigan), uneven treatment of variant forms (MIT, M.I.T., and
Massachusetts Institute of Technology - these should all be
labeled the same in this text - are they?), and frequent false
positives such as RNA and T. Rex.
...
Table 7: Excerpt from a Unit test for Named-Entity Recognition
versus another. We propose that these dependencies
can be used in two other aspects: (a) as a source
of error in evaluation and, (b) as a way to identify
semantically related entities that are systematic ex-
ceptions. There is a grammar to named entity types.
?Bank of Australia? is a special case of Bank of
[LOCATION]. The same thing is true for ?China
Daily? as a name for a newspaper. We propose that
co-occurrences of different labels for particular in-
stances can be used to create such a grammar; at the
very least, particular types of co-occurrences should
be treated as an exception to what is otherwise an
indication of a tagging mistake.
7 Discussion
The Message Understanding Conference (MUC) has
guidelines for named-entity recognition. But the
guidelines are just that. We believe that there should
be standards. Without such standards it is difficult
to determine which tagger is correct, and how the
accuracy varies between the classes.
We propose that the community focus on four
classes: PERSON, ORGANIZATION, LOCA-
TION, and MISC. This does not mean that the other
classes are not important. Rather it is recognition of
the following facts:
? These classes are more difficult than dates,
times, and currencies.
? There is widespread disagreement between tag-
gers on these classes, and evidence that they are
misclassifying unique entities a significant per-
centage of the time.
? We need at least one class for handling terms
that do not fit into the first three classes.
? The first three classes have important value in
other areas of NLP.
Although we recognize that an extrinsic evalu-
ation of named entity recognition would be ideal,
we also realize that intrinsic evaluations are valu-
able in their own right. We propose that the exist-
ing methodology for intrinsically evaluating named
entity taggers can be improved in the following man-
ner:
1. Create test sets that are organized across a va-
riety of domains. It is not enough to work with
newswire and biomedical text.
2. Use standardized sets that are designed to test
different types of linguistic phenomena, and
make it a de facto norm to use more than one
set as part of an evaluation.
3. Report accuracy rates separately for the three
major classes. Accuracy rates should be further
broken down according to the items in the unit
test that are designed to assess mistakes: or-
thography, acronym processing, frequent false
positives, and knowledge-based classification.
4. Establish a way for a tagging system to express
uncertainty about a classification.
62
The approach taken by the American National
Corpus is a good step in the right direction. Like
the original Brown Corpus and the British National
Corpus, it breaks text down according to informa-
tional/literary text types, and spoken versus written
text. The corpus also includes text that is drawn from
the literature of science and medicine. However, the
relatively small number of files in the corpus makes
it difficult to assess accuracy rates on the basis of re-
peated occurrences within a document, but with dif-
ferent tags. Because there are hundreds of thousands
of files in the internal ETS corpus, there are many
opportunities for observations. The tagged version
of the American National Corpus has about 8800
files. This is one of the biggest differences between
the evaluation on the corpus we used internally at
ETS and the American National Corpus.
The use of a MISC class is needed for reasons
that are independent of certainty. This is why we
propose a goal of allowing systems to express this
aspect of the classification. We suggest a meta-tag of
a question-mark. The meta-tag can be applied to any
class. Entities for which the system is uncertain can
then be routed for active learning. This also allows a
basic separation of entities into those for which the
system is confident of its classification, and those for
which it is not.
8 Conclusion
Although Named Entity Recognition has a reported
accuracy rate of more than 90%, the results show
they make a significant number of mistakes. The
high accuracy rates are based on inadequate meth-
ods for testing performance. By considering only
the entities where both taggers agree on the classifi-
cation, it is likely that we can obtain improved accu-
racy. But even so, there are cases where both taggers
agree yet the agreement is on an incorrect tagging.
The unit test for assessing NER performance is
freely available to download.8
As with Information Retrieval test collections, we
hope that this becomes one of many, and that they be
adopted as a standard for evaluating performance.
8http://bit.ly/nertest
Acknowledgments
This work has been supported by the Institute
for Education Sciences under grant IES PR/Award
Number R305A080647. We are grateful to Michael
Flor, Jill Burstein, and anonymous reviewers for
their comments.
References
Daniel M. Bikel, Richard M. Schwartz, and Ralph M.
Weischedel. 1999. An Algorithm that Learns What?s
in a Name. Machine Learning, 34:211?231.
Xavier Carreras, Llus Mrquez, and Llus Padr. 2003.
Named entity recognition for Catalan using Spanish
resources. In Proceedings of EACL.
James R. Curran and Stephen Clark. 2003. Language
Independent NER using a Maximum Entropy Tagger.
In Proceeding of the 7th Conference on Computational
Natural Language Learning (CoNLL), pages 164?167.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured Generative Models for Unsupervised
Named-Entity Clustering. In Proceedings of NAACL,
pages 164?172.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of ACL, pages 363?370.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One Sense per Discourse. In Pro-
ceedings of the ARPA Workshop on Speech and Natu-
ral Language Processing, pages 233?237.
Robert Krovetz. 1998. More than One Sense Per Dis-
course. In Proceedings of the ACL-SIGLEX Work-
shop: SENSEVAL-1.
Monica Marrero, Sonia Sanchez-Cuadrado, Jorge Morato
Lara, and George Andreadakis. 2009. Evaluation of
Named Entity Extraction Systems. Advances in Com-
putational Linguistics, Research in Computing Sci-
ence, 41:47?58.
Fien De Meulder, V Eronique Hoste, and Walter Daele-
mans. 2002. A Named Entity Recognition System for
Dutch. In Computational Linguistics in the Nether-
lands, pages 77?88.
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named Entity Recognition Without Gazetteers. In
Proceedings of EACL, pages 1?8.
David Nadeau, Peter D. Turney, and Stan Matwin. 2006.
Unsupervised Named-Entity Recognition: Generating
Gazetteers and Resolving Ambiguity. In Proceedings
of the Canadian Conference on Artificial Intelligence,
pages 266?277.
63
L. Ratinov and D. Roth. 2009. Design Challenges
and Misconceptions in Named Entity Recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL), pages
147?155.
Marc Rossler. 2004. Adapting an NER-System for Ger-
man to the Biomedical Domain. In Proceedings of
the International Joint Workshop on Natural Language
Processing in Biomedicine and its Applications, pages
92?95.
64
Proceedings of the First Workshop on Argumentation Mining, pages 69?78,
Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational Linguistics
Applying Argumentation Schemes for Essay Scoring    Yi Song   Michael Heilman   Beata Beigman Klebanov   Paul Deane Educational Testing Service Princeton, NJ, USA  {ysong, mheilman, bbeigmanklebanov, pdeane}@ets.org    Abstract 
Under the framework of the argumentation scheme theory (Walton, 1996), we developed annotation protocols for an argumentative writing task to support identification and classification of the arguments being made in essays. Each annotation protocol defined ar-gumentation schemes (i.e., reasoning pat-terns) in a given writing prompt and listed questions to help evaluate an argument based on these schemes, to make the argument structure in a text explicit and classifiable. We report findings based on an annotation of 600 essays. Most annotation categories were applied reliably by human annotators, and some categories significantly contributed to essay score. An NLP system to identify sen-tences containing scheme-relevant critical questions was developed based on the human annotations.   1. Introduction In this paper, we analyze the structure of argu-ments as a first step in analyzing their quality.  Argument structure plays a critical role in identi-fying relevant arguments based on their content, so it seems reasonable to focus first on identify-ing characteristic patterns of argumentation and the ways in which such arguments are typically developed when they are explicitly stated. It is worthwhile to classify the arguments in a text and to identify their structure when they are ex-tended to include whole text segments (Walton, 1996; Walton, Reed, and Macagno, 2008), but it is not clear how far human annotation can go in analyzing argument structure.  An analysis of the effectiveness and full com-plexity of argument structure is different than the identification of generic elements that might compose an argument, such as claims (e.g., a thesis sentence), main reasons (e.g., supporting topic sentences), evidence (e.g., elaborating 
segments), and other components, such as the introduction and conclusion (Burstein, Kukich, Wolff, Lu, Chodorow, Braden-Harder, & Harris, 1998; Burstein, Marcu, and Knight, 2003; Pendar & Cotos, 2008). In contrast, here we focus on analyzing specific types of arguments, what the literature terms argumentation schemes (Walton, 1996). Argumentation schemes include schemat-ic content and take into account a pattern of pos-sible argumentation moves in a larger persuasive dialog. Understanding these argumentation schemes is important for understanding the logic behind an argument. Critical questions associat-ed with a particular argumentation scheme pro-vide a normative standard that can be used to evaluate the relevance of an argument?s justifica-tory structure (van Eemeren and Grootendorst, 1992; Walton, 1996; Walton et al., 2008).   We aimed to lay foundations for the automat-ed analysis of argumentation schemes, such as the identification and classification of the argu-ments in an essay. Specifically, we developed annotation protocols for writing prompts in an argument analysis task from a graduate school admissions test. The task was designed to assess how well a student analyzes someone else?s ar-gument, which is provided by the prompt.  The student must critically evaluate the logical soundness of the given argument. The annotation categories were designed to map student re-sponses to the scheme-relevant critical questions. We examined whether this approach provides a useful framework for describing argumentation and whether human annotators can apply it relia-bly and consistently. Furthermore, we have be-gun work on automating the annotation process by developing a system to predict whether sen-tences contain scheme-relevant critical questions. 2. Theoretical Framework As Nussbaum (2011) notes, there have been crit-ical advances in the study of informal argument, 
69
  
which takes place within a social context involv-ing dialog among people with different beliefs, most notably the development of theories that provide relatively rich schemata for classifying informal arguments, such as Walton (1996).  An argumentation scheme is defined as ?a more or less conventionalized way of represent-ing the relation between what is stated in the ar-gument and what is stated in the standpoint? (van Eemeren and Grootendorst, 1992, p. 96). It is a strategic pattern of argumentation linking prem-ises to a conclusion and illustrating how the con-clusion is derived from the premises. This ?in-ternal structure? of argumentation reflects justifi-catory standards that can be used to help evaluate the reasonableness of an argument (van Eemeren and Grootendorst, 2004). Argumentation schemes should be distinguished from the kinds of structures postulated in Mann and Thompson?s (1988) Rhetorical Structure Theory (RST) be-cause they focus on relations inherent in the meaning of the argument, regardless of whether they are explicitly realized in the discourse. Consider, for instance, argument from conse-quences, which applies when the primary claim argues for or against a proposed policy (i.e., course of action) by citing positive or negative consequences that would follow if the policy were adopted (Walton, 1996). Elaborations of an argument from consequences are designed to defend against possible objections. For instance, an opponent could claim that the claimed conse-quences are not probable; or that they are not desirable; or that they are less important than other, undesirable consequences. Thus a sophis-ticated writer, in elaborating an argument from consequences, may provide information to rein-force the idea that the argued consequences are probable, desirable, and more important than any possible undesired effects. These moves corre-spond to what the literature calls critical ques-tions, which function as a standard for evaluating the reasonableness of an argument based on its argumentation schemes (Walton, 1996). Walton and his colleagues (2008) analyzed over 60 argumentation schemes, and identified critical questions associated with certain schemes as the logical moves in argumentative discourse. The range of possible moves is quite large, espe-cially when people use multiple schemes. There have been several efforts to annotate corpora with argumentation scheme information to sup-port future machine learning efforts (Mochales and Ieven, 2009; Palau and Moens, 2009; Rienks, Heylen, and Van der Weijden, 2005; 
Verbree, Rienks, and Heylen, 2006), to support argument representation (Atkinson, Bench-Capon, and McBurney, 2006; Rahwan, Banihashemi, Reed, Walton, and Abdallah, 2010), and to teach argumentative writing (Fer-retti, Lewis, and Andrews-Weckerly, 2009; Nussbaum and Schraw, 2007; Nussbaum and Edwards, 2011; Song and Ferretti, 2013). In ad-dition, Feng and Hirsh (2011) used the argumen-tation schemes to reconstruct the implicit parts (i.e., unstated assumptions) of the argument structure. In many previous studies, the data sets on argumentation schemes were relatively small and the inter-rater agreement was not measured.  We are particularly interested in exploring the relationship between the use of scheme-relevant critical questions and essay quality, as measured by holistic essay scores. The difference between an expert and a novice is that the expert knows which critical questions should be asked when the dynamic of the argument requires them, while the novice misses the essential moves to ask critical questions that help evaluate if the argument is valid or reasonable. Often, students presume information and fail to ask questions that would reveal potential fallacies. For exam-ple, they might use quotations from books, ar-guments from TV programs, or opinions posted online without evaluating whether the infor-mation is adequately supported by evidence. Critically evaluating arguments is considered an important skill in college and graduate school. For example, a widely accepted graduate admis-sions test has a task to assess students? critical thinking and analytical writing skills. In this ar-gument analysis task, students should demon-strate skills in critiquing other people?s argu-ments, such as identifying unwarranted assump-tions or discussing what specific evidence is need to support the argument. They must com-municate their evaluation of the arguments clear-ly to the audience. To accomplish this task suc-cessfully, students need to evaluate the argu-ments against appropriate criteria. Therefore, their essays could be analyzed using an annota-tion approach based on the theory of argumenta-tion schemes and critical questions.  Our research questions were as follows:   1. Can this scheme-based annotation approach be applied consistently by annotators to a corpus of argumentative essays? 2. Do annotation categories based on the theo-ry of argumentation schemes contribute 
70
  
significantly to the prediction of essay scores? 3. Can we use NLP techniques to train an au-tomated classifier for distinguishing sen-tences that raise critical questions from sen-tences that contain no critical questions? 3  Development of Annotation Protocols Although Walton?s argumentation schemes pro-vided a good framework for analyzing argu-ments, it was challenging to apply them in some cases of argument essays because various inter-pretations could be made on some argument structures. For instance, people were often con-fused with argument from consequences, argu-ment from correlation to cause, and argument from cause to effect because all these three types of arguments indicate a causal relationship. While it is good that Walton tried to identify var-iations of a causal relationship, a side effect is that some schemes are not so distinguishable from each other, especially for someone who is not an expert in logic. This ambiguity makes it difficult to apply his theory directly to annota-tion. Thus, we modified Walton?s schemes and created new schemes when necessary to achieve exclusive annotation categories and capture the features in the argument analysis task. In this paper, we illustrate our annotation pro-tocols on a policy argument because over half of the argument analysis prompts for the assess-ment we are working with deal with policy is-sues (i.e., issues involve the possibility of putting a practice into place). Here, we use the ?Patriot Car? prompt as an example.   The following appeared in a memo-randum from the new president of the Patriot car manufacturing company.   "In the past, the body styles of Patriot cars have been old-fashioned, and our cars have not sold as well as have our competitors' cars. But now, since many regions in this country report rapid in-creases in the numbers of newly licensed drivers, we should be able to increase our share of the market by selling cars to this growing population. Thus, we should discontinue our oldest models and con-centrate instead on manufacturing sporty cars. We can also improve the success of our marketing campaigns by switching our advertising to the Youth Advertising 
agency, which has successfully promoted the country's leading soft drink."  Test takers are asked to analyze the reasoning in the argument, consider any assumptions, and discuss how well any evidence that is mentioned supports the conclusion. The prompt states that the new president of the Patriot car manufacturing company pointed out a problem that the body styles of Patriot cars have been old-fashioned and their cars have not sold as well as their competitors? cars. The president proposed a plan to discontinue their oldest mod-els and to concentrate on manufacturing sporty cars. He believed that this plan will lead to an increase in their market share (i.e., the goal). This is a policy issue because it involves whether the plan of discontinuing oldest car models and manufacturing sporty cars should be put into place. This prompt shows a typical pattern of many argument analysis prompts about policy issues: (1) a problem is stated; (2) a plan is pro-posed; and (3) a desirable goal will be achieved if the plan is implemented. Thus, we created a policy scheme that includes these three major components (i.e., problem, plan, and goal), and a causal relationship that bridges the plan to the goal in the policy scheme. Therefore, a causal scheme appears in a policy argument to represent the causal relationship from the proposed plan to the goal. This part is different from Walton?s analysis. He uses the argument from conse-quences scheme for policy arguments, but it cre-ated confusions when applying it to annotation, especially when students unconsciously use the word ?cause? to introduce a potential conse-quence that follows a policy. In addition, our causal scheme combines the argument from cor-relation to cause scheme and the argument from cause to effect scheme specified by Walton.  Accordingly, we revised or re-arranged some of the critical questions in Walton?s theory. For example, challenges to arguments that use a poli-cy scheme fall into the following six categories: (a) problem; (b) goal; (c) plan implementation; (d) plan definition; (e) side effect; and (f) alterna-tive plan. When someone writes that the presi-dent should re-evaluate whether this is really a problem, it matches the question in the ?prob-lem? category; when someone questions if there  is an alternative plan that could also help achieve the goal and is better than the plan proposed by the president, it should be categorized as a chal-lenge in ?alternative plan.? We call these ?specif-ic questions? because they are attached to a par-
71
  
ticular prompt. In other words, specific questions are content dependent. Each category also in-cludes one or more ?general questions? that can be asked for any argument using the same argu-mentation scheme, and in this case, it is the poli-cy scheme.  We have developed annotation protocols for various argumentation schemes. Table 1 includes part of the annotation protocols (i.e., scheme, category, and general critical questions) for three argumentation schemes: the policy argument scheme, the causal argument scheme, and the argument from a sample scheme. This study fo-cuses on these three argumentation schemes and 16 associated categories.  4  Application of the Annotation Ap-proach This section focuses on applying the annotation approach and the following research question: Can this scheme-based annotation approach be applied consistently by raters to a corpus of ar-gumentative essays?  4.1  Annotation Rules 
The first step of the annotation is reading the en-tire essay. It is important to understand the writ-er?s major arguments and the organization of the essay. Next, the annotator will identify and high-light any text segment (e.g., paragraph, sentence, or clause) that addresses a critical question. Usu-ally, the minimal text segment is at the sentence-level, but it could be the case that the selection is at the phrase-level when a sentence includes multiple points that match more than one critical question. Thirdly, for a highlighted unit, the an-notator will choose a topic, a category, and a se-cond topic, if applicable. Only one category label can be assigned to each selected text unit. ?Generic? information will not be selected or assigned an annotation label. Generic infor-mation includes restatements of the text in the prompt, general statements that do not address any specific questions, rhetoric attacks, and irrel-evant information. Note that this notion of gener-ic information is related to ?shell language,? as described by Madnani et al (2012).  However, our definition here focuses more closely on sen-tences that do not raise critical questions.  Sur-face errors (e.g., grammar and spelling) can be 
Scheme Category Critical Question 
Policy 
Problem Is this really a problem? Is the problem well-defined? Goal How desirable is this goal? Are there specific conflicting goals we do not wish to sacrifice? Plan Implementation Is it practically possible to carry out this plan?  Plan Definition Is the plan well defined? Side Effects Are there negative side effects that should be taken into account if we carry out our plan? Alternative plan Are there better alternatives that could achieve the goal? 
Causal 
Causal Mechanism Is there really a correlation? Is the correlation merely a coincidence (invalid causal relationship)? Are there alternative causal factors? Causal Efficacy Is the causal mechanism strong enough to produce the desired effects? Applicability Does this causal mechanism apply? Intervening Factors Are there intervening factors that could undermine the causal mechanism? 
Sample 
Significance Are the patterns we see in the sample clear-cut enough (and in the right direction) to support the desired inference? Representativeness Is there any reason to think that this sample might not be representative of the group about which we wish to make an inference? Stability Is there any reason to think this pattern will be stable across all the circumstances about which we wish to make an inference?  Sample Size Is there any reason to think that the sample may not be large enough and reliable enough to support the inference we wish to draw? Validity Is the sample measured in a way that will give valid information on the population attributes about which we wish to make inferences?  Alternatives Are there external considerations that could invalidate the claims? Table 1: Annotation protocols for three types of argumentation schemes 
72
  
ignored if they do not prevent people from un-derstanding the meaning of the essay. Here is an example of annotated text.  As stated by the president, there is a rap-id increase in the number of newly li-censed drivers which would be a market-able target.  [However, there was no con-crete evidence that these newly licensed drivers favored sporty cars over other model types.]Causal Applicability [On a similar note, there was no anecdotal evidence demonstrating that lack of sales was con-tributed to the old-fashion body styles of the Patriot cars.]Causal Mechanism [There could be numerous other factors contrib-uting to their lack of sales:  prices are not competitive, safety ratings are not as high, features are not as appealing.  The best way to tackle this problem is to send out researches and surveys to get the opinions of consumers.]Causal Mechanism 4.2  Annotation Tool The annotation interface includes the following elements: 1. the original writing prompt; 2. topics that the prompt addresses; 3. categories associated with critical questions relevant to that type of argument; 4. general critical questions that can be used across prompts that possess the same argu-mentation scheme; and 5. specific critical questions for this particular prompt.   The annotators highlight text segments to be an-notated and then clicked a button to choose a topic (e.g., body style versus advertising agency in the Patriot Car prompt) and a category to iden-tify which critical questions were addressed.  4.3  Data and Annotation Procedures In this section, we report our annotation on two selected argument analysis prompts in an as-sessment for graduate school admissions. The actual prompts are not included here because they may be used in future tests. Both prompts deal with policy issues and are involved in causal reasoning, but the second prompt also has a sam-ple scheme (see Table 1). For each prompt, we randomly selected 300 essays to annotate. These essays were written between 2008 and 2010.  
Four annotators with linguistics backgrounds who were not co-authors of the paper received training on the annotation approach. Training focused on the application to specific prompts because each prompt had a specific annotation protocol that covers the argumentation schemes and how they relate to the prompt?s topics. The first author delivered the training sessions, and helped resolve differences of opinion during practice annotation rounds. After training and practice, the annotators annotated 20 pilot essays for a selected prompt to test their agreement. This pilot stage gave us another chance to find and clarify any confusion about the annotation categories. After that, the annotators worked on the sampled set of 300 essays, and these annota-tions were then used for analyses. For each prompt, 40 essays were randomly selected, and all 4 annotators annotated these 40 essays to check the inter-annotator agreement.  For the experiments described later that involve the mul-tiply-annotated set, we used the annotations from the annotator who seemed most consistent. 4.4  Inter-Annotator Agreement To compute human-human agreement, we auto-matically split the essays into sentences.  For each sentence, we computed the annotations that overlapped with at least part of the tence.  Then, for each category, we computed human-human agreement across all sentences about whether that category should be marked or not.  We also created a ?Generic? label, as dis-cussed in section 4.1, for sentences that were not marked by any of the other labels. We computed two inter-annotator agreement statistics. Our primary statistic is Cohen?s kappa between pairs of raters. Four annotators generat-ed 6 pairs of kappa values, and in this report we only report the average kappa value for each an-notation category. As an alternative statistic, we computed Krippendorff?s alpha, a chance-corrected statistic for calculating the inter-annotator agreement between multiple coders (four annotators in our case), which is similar to multi kappa (Krippendorff, 1980). Table 2 shows the kappa and alpha values for each annotation category, excluding those that were rare. To identify rare categories, we aver-aged the numbers of sentences annotated under a category among four annotators, which indicated how many sentences were annotated under this category in 40 essays.  If the number was lower than 10, which means that no more than one sen-tence was annotated in every four essays, then 
73
  
the category was considered rare. Most rare cate-gories had low inter-rater agreement, which is not surprising.  It is not realistic to require anno-tators to always agree about rare categories. From Table 2, we can see that the kappa value and the alpha value on the same category were close. The inter-annotator agreement on the ?ge-neric? category varied little across the two prompts (kappa: 0.572-0.604; alpha: 0.571-0.603), which indicates that the annotators had a fairly good agreement on this category. The an-notators had good agreements on most of the commonly used categories (kappa ranged from 0.549 to 0.848, and alpha ranged from 0.537 to 0.843) except the ?plan definition? under the pol-icy scheme in prompt B (both kappa and alpha values were below 0.400). The major reason for this disagreement is that one annotator marked a significantly higher number of sentences (more than double) for this category than others did.  
 Table 2: Inter-annotator agreement 5  Essay Score and Annotation Features This section explores the second research ques-tion: Do annotation categories based on the theo-ry of argumentation schemes contribute signifi-cantly to the prediction of essay scores?  An-swering this question would tell us whether we capture an important construct of the argument analysis task by recognizing these argumentation features. Specifically, we tested whether these features add predictive value to a model based 
the state-of-the-art e-rater essay scoring system (Burstein, Tetreault, and Madnani, 2013). To explore the relationship between annota-tion categories and essay quality, we ran a multi-ple regression analysis for each prompt. Essay quality was the dependent variable and was measured by a final human score, on a scale from 0 to 6. The independent variables were nine high-level e-rater features and the annotation categories relevant to a prompt (Prompt A: 10 categories; Prompt B 16 categories). The e-rater features were designed to measure different as-pects of writing (grammar, mechanics, style, us-age, word choice, word length, sentence variety, development, and organization). We computed the percentage of sentences that were marked as belonging to each category (i.e., the number of sentences in a category divided by the total num-ber of sentences) to factor out essay length. Note that the generic category was negatively correlated with the essay score in both prompts, since it included responses judged irrelevant to the scheme-relevant critical questions. In other words, the generic responses are the parts of the text that do not present specific critical evalua-tions of the arguments in a given prompt. For the purposes of our evaluation, we used the inverse feature labeled ?all critical questions?: the pro-portion of the text that actually raises some criti-cal question (i.e., is not generic), regardless of scheme. We believe this formulation more trans-parently expresses the underlying mechanism relating the feature to essay quality. For each prompt, we split the 300 essays into two data sets: the training set and the testing set. The testing set had the 40 essays that were anno-tated by all four annotators, and the training set had the remaining 260. We trained three models with stepwise regression on the training set and evaluated them on the testing set:  1. A model that included only the e-rater fea-tures to examine how well the e-rater mod-el works (?baseline?) 2. A model with the baseline features and all the annotation category percentage varia-bles except for the "generic" category vari-able (?baseline + categories?) 3. A model with the baseline features and a feature corresponding to the inverse of the "generic" category (?baseline + all critical questions?).  Table 3 presents the Pearson correlation coef-ficient r values for comparing model predictions 
Prompt Category Kappa Alpha 
Prompt A     Generic 0.572 0.571  Policy : Problem 0.644 0.640  Policy : Side Effects 0.612 0.609  Policy : Alternative Plan 0.665 0.666  Causal : Causal Mechanism 0.680 0.676  Causal : Applicability 0.557 0.555 Prompt B     Generic 0.604 0.603  Policy : Problem 0.848 0.843  Policy : Plan Definition 0.346 0.327  Causal : Causal Mechanism 0.620 0.622  Causal : Applicability 0.767 0.769  Sample : Validity 0.549 0.537 
74
  
to human scores for each of the models. In prompt A, three annotation categories (causal mechanism, applicability, and alternative plan) were selected by the stepwise regression because they significantly contributed to the essay score above the nine e-rater features. This model showed higher test set correlations than the base-line model (? r = .014). The model with the gen-eral argument feature (?all critical questions?) showed a similar increase (? r = .014).   Training Set r Testing Set r Testing Set ? r Prompt A    baseline	 ? .838 .852 --- baseline + specific categories	 ? .852 .866 .014 baseline +  all critical questions	 ? .858 .866 .014  Prompt B    baseline	 ? .818 .761 --- baseline + specific categories	 ? .835 .817 .056 baseline +  all critical questions	 ? .845 .821 .060  Table 3: Performance of essay scoring models with and without argumentation features  Similar observations apply to prompt B. The causal mechanism category added prediction significantly above e-rater with an increase (? r = .056). The model containing the general argu-ment feature (?all critical questions?) performed slightly better (? r = .060). These results suggest that annotation catego-ries based on argumentation schemes contribute additional useful information about essay quality to a strong baseline essay scoring model.  In the next section, we report on preliminary experi-ments testing whether these annotations can be automated, which would almost certainly be nec-essary for practical applications. 6  Argumentation Schemes NLP System We developed an NLP system for automatically identifying the presence of scheme-relevant criti-cal questions in essays, and we evaluated this system with annotated data from the two selected argument prompts. This addresses the third re-search question: Can we use NLP techniques to train an automated classifier for distinguishing 
sentences that raise critical questions from sen-tences that contain no critical questions? 6.1  Modeling In this initial development of the NLP system, we focused on the task of predicting whether a sentence raises any critical questions or none (i.e., generic vs. nongeneric). As such, the task was binary classification at the level of the sen-tence. The system we developed uses the SKLL tool1 to fit L2-penalized logistic regression mod-els with the following features:  ? Word n-grams: Binary indicators for the presence of contiguous subsequences of n words in the sentence. The value of n ranged from 1 to 3. These features had value 1 if a particular n-gram was present in a sentence and 0 otherwise. ? word n-grams of the previous and next sen-tences: These are analogous to the word n-gram features for the current sentence. ? sentence length bins: Binary indicators for whether the sentence is longer than 2t word tokens, where t  ranges from 1 to 10. ? sentence position: The sentence number di-vided by the number of sentences in text. ? part of speech tags: Binary indicators for the presence of words with various parts of speech, as predicted by NLTK 2.0.4. ? prompt overlap: Three features based on lex-ical overlap between the sentence and the prompt for the essay: a) the Jaccard similari-ty between the sets of word n-grams in the sentence and prompt (n = 1, 2, 3), b) the Jac-card similarity between the sets of word uni-grams (i.e., just n = 1) in the sentence and prompt, and c) the Jaccard similarity be-tween the sets of ?content? word unigrams in the sentence and prompt (for this, content words were defined as word tokens that con-tained only numbers and letters and did not appear in NLTK?s English stopword list). 6.2  Experiments For these experiments, we used the training and testing sets described in Section 5. We trained models on the training data for each prompt in-dividually and on the combination of the training data for both prompts. To measure generalization across prompts, we tested these models on the testing data for each prompt and on the combina-                                                1 https://github.com/EducationalTestingService/skll 
75
  
tion of the testing data for the two prompts. We evaluated performance in terms of unweighted Cohen?s kappa. The results are in Table 4.  Training Testing Kappa combined combined .438 Prompt A  .350 Prompt B  .346 combined Prompt A .379 Prompt A  .410 Prompt B  .217 combined Prompt B .498 Prompt A  .285 Prompt B  .478  Table 4: Performance of the NLP Model  The model trained on data from both prompts performed relatively well compared to the other models.  For the testing data for prompt B, the combined model outperformed the model trained on just data from prompt B.  However, the prompt-specific model for prompt A slightly outperformed the combined model on the testing data for prompt A. Although the performance of models trained with data from one prompt and tested with data from another prompt did not perform as well, there is evidence of some generalization across prompts. The model trained on data from prompt B and tested on data from prompt A had kappa = 0.217; the model trained on data from prompt A and tested on data from prompt B had kappa = 0.285. Of course, these human-machine agree-ment values were somewhat lower than human-human agreement values (0.572 and 0.604, re-spectively), leaving substantial room for im-provement in future work. We also examined the most strongly weighted features in the combined model.  We observed that multiple hedge words (e.g., ?perhaps?, ?may?) had positive weights, which associated with the ?generic? class.  We also observed that words related to argumentation (e.g., ?conclu-sions?, ?questions?) had negative weights, which associated them with the nongeneric class, as one would expect.  One issue of concern is that some words related to the specific topics discussed in the prompts received high weights as well, which may limit generalizability.  
7  Conclusion Our research focused on identification and classi-fication of argumentation schemes in argumenta-tive text. We developed annotation protocols that capture various argumentation schemes. The an-notation categories corresponded to scheme-relevant critical questions, and for text segments that do not contain any critical questions, we as-signed a ?generic? category. In this paper, we reported the results based on an annotation of a large pool of student essays (both high-quality and low-quality essays). Results showed that most of the common annotation categories (e.g. causal mechanism, alternative plan) can be ap-plied reliably by the four annotators. However, the annotation work is labor-intensive. People need to receive sufficient train-ing to apply the approach consistently. They must not only identify meaningful chunks of tex-tual information but also assign the right annota-tion category label for the selected text. Despite these complexities, it is a worthwhile investiga-tion. Developing a systematic classification of argument structures not only plays a critical role in this project, but also has a potential contribu-tion to other assessments on argumentation skills aligned with the Common Core State Standards. This work would help improve the current auto-mated scoring techniques for argumentative es-says because this annotation approach takes into account the argument structure and its content.  We ran regression analyses and found that manual annotations grounded in the argumenta-tion schemes theory predict essay quality. Our data showed that features based on manual ar-gument scheme annotations significantly con-tributed to models of essay scores for both prompts. This is probably because our approach focused on the core of argumentation, rather than surface or word-level features (e.g., mechanics, grammar, usage, style, essay organization, and vocabulary) examined by the baseline model. Furthermore, we have implemented an auto-mated system for predicting the human annota-tions. This system focused only on predicting whether or not a sentence raises any critical questions (i.e., generic vs. nongeneric). In the future, we plan to test whether features based on automated annotations make contributions to essay scoring models that are similar to the con-tributions of manual annotations.  We also plan to work on detecting specific critical questions and adding additional features, such as features from Feng and Hirst (2011). 
76
  
Acknowledgements  We would like to thank Keelan Evanini, Jill Burstein, Aoife Cahill, and the anonymous re-viewers of this paper for their helpful comments.  We would also like to thank Michael Flor for helping set up the annotation interface, and Melissa Lopez, Matthew Mulholland, Patrick Houghton, and Laura Ridolfi for annotating the data. References Katie Atkinson, Trevor Bench-Capon, and Peter McBurney. 2006. Computational representation of practical argument. Synthese, 152: 157-206. Burstein, Jill, Karen Kukich, Susanne Wolff, Chi Lu, Martin Chodorow, Lisa Braden-Harder, and Mary Dee Harris. 1998. "Automated scoring using a hybrid feature identification technique." In Pro-ceedings of the 17th international conference on Computational linguistics-Volume 1, pp. 206-210. Association for Computational Linguistics. Jill Burstein, Daniel Marcu, and Kevin Knight. 2003. Finding the WRITE stuff: Automatic identification of discourse structure in student essays. IEEE Transactions on Intelligent Systems, 18(1): 32-39.  Jill Burstein, Joel Tetreault, and Nitin Madnani. 2013. The e-rater automated essay scoring system. In Sermis, M. D. and Burstein, J. (eds.), Handbook of Automated Essay Evaluation: Current Applications and New Directions (pp. 55-67). New York: Routledge.  Vanessa W. Feng and Graeme Hirst. 2011.  Classify-ing arguments by scheme.  Proceedings of the 49th Annual Meeting of the Association for Computa-tional Linguistics, Portland, OR.   Ralph P. Ferretti, William E. Lewis, and Scott An-drews-Weckerly. 2009. Do goals affect the struc-ture of students? argumentative writing strategies? Journal of Educational Psychology, 101: 577-589. Klaus Krippendorff. 1980. Content Analysis: An In-troduction to its Methodology. Beverly Hills, CA : Sage Publications.Mann, William C., and Sandra A. Thompson. 1988. "Rhetorical structure theory: Toward a functional theory of text organization." Text 8(3): 243-281. Nitin Madnani, Michael Heilman, Joel Tetreault, and Martin Chodorow.  2012.  Identifying High Level Organizational Elements in Argumentative Dis-course. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.  (pp. 20-28).  Association for Com-putational Linguistics. 
Raquel Mochales and Asgje Ieven. 2009. Creating an argumentation corpus: do theories apply to real ar-guments?: a case study on the legal argumentation of the ECHR. In ICAIL ?09: Proceedings of the 12th International Conference on Artificial Intelli-gence and Law.  Michael Nussbaum. 2011. Argumentation, dialogue theory, and probability modeling: alternative frameworks for argumentation research in educa-tion. Educational Psychologist, 46: 84-106.  Nussbaum, E. M. and Edwards, O.V. (2011). Critical questions and argument stratagems: A framework for enhancing and analyzing students? reasoning practices. Journal of the Learning Sciences, 20, 443-488. Palau, R.M. and Moens, M. F. 2009. Automatic ar-gument detection and its role in law and the seman-tic web. In Proceedings of the 2009 conference on law, ontologies and the semantic web. IOS Press, Amsterdam, The Netherlands.Pendar, Nick, and Elena Cotos. 2008. "Automatic identification of discourse moves in scientific article introductions." In Proceedings of the Third Workshop on Innova-tive Use of NLP for Building Educational Applica-tions, pp. 62-70. Association for Computational Linguistics. Rahwan, I., Banihashemi, B., Reed, C. Walton, D., and Abdallah, S. (2010). Representing and classi-fying arguments on the semantic web. The Knowledge Engineering Review. Rienks, R., Heylen, D., and Van der Weijden, E. 2005. Argument diagramming of meeting conver-sations. In A. Vinciarelli, J. Odobez (Ed.), Pro-ceedings of Multimodal Multiparty Meeting Pro-cessing, Workshop at the 7th International Confer-ence on Multimodal Interfaces (pp. 85?92). Trento, Italy.  Yi Song and Ralph P. Ferretti. 2013. Teaching critical questions about argumentation through the revising process: Effects of strategy instruction on college students? argumentative essays. Reading and Writ-ing: An Interdisciplinary Journal, 26(1): 67-90. Stephen E. Toulmin. 1958. The uses of argument. Cambridge University Press, Cambridge, UK. Frans H. van Eemeren and Rob Grootendorst. 1992. Argumentation, communication, and fallacies: A pragma-dialectical perspective. Mahwah, NJ: Erl-baum. Frans H. van Eemeren and Rob Grootendorst. 2004. A systematic theory of argumentation: A pragma-dialectical approach. Cambridge, UK: Cambridge University Press. Verbree, D., Rienks, H., and Heylen, D. (2006). First Steps Towards the Automatic Construction of Ar-gument-Diagrams from Real Discussions. In Pro-
77
  
ceedings of the 2006 conference on Computational Models of Argument: Proceedings of COMMA 2006. IOS Press, Amsterdam, The Netherlands. Douglas N. Walton. 1996. Argumentation schemes for presumptive reasoning. Mahwah, NJ: Lawrence Erlbaum. Douglas N. Walton, Chris Reed, and Fabrizio Macagno. 2008. Argumentation schemes. New York, NY: Cambridge University Press. 
78
