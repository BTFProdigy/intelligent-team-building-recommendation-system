Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Annotation Graphs and Servers and Multi-Modal Resources: 
Infrastructure for Interdisciplinary Education, Research and 
Development 
Christopher Cieri 
University of Pennsylvania 
Linguistic Data Consortium 
3615 Market Street 
Philadelphia, PA. 19104-2608 USA 
ccieri@ldc.upenn.edu 
Steven Bird  
University of Pennsylvania 
Linguistic Data Consortium 
3615 Market Street 
Philadelphia, PA. 19104-2608 USA 
sb@ldc.upenn.edu 
 
 
Abstract 
Annotation graphs and annotation 
servers offer infrastructure to support 
the analysis of human language 
resources in the form of time-series 
data such as text, audio and video. This 
paper outlines areas of common need 
among empirical linguists and 
computational linguists. After 
reviewing examples of data and tools 
used or under development for each of 
several areas, it proposes a common 
framework for future tool development, 
data annotation and resource sharing 
based upon annotation graphs and 
servers. 
1 Introduction 
Despite different methodologies, goals and 
traditions, researchers in a variety of specialties 
in linguistics and computational linguistics share 
a core of assumptions and needs. Research 
communities in empirical linguistics, natural 
language processing, speech recognition, 
information retrieval and language teaching 
have a common need for language resources 
such as observations of linguistic performance, 
annotations encoding human judgment, 
standards for maintaining consistency among 
distributed resources and processes for 
extracting relevant observations. Where needs 
overlap, there is the opportunity to reuse 
existing resources and coordinate new initiatives 
so that communities share the burden of 
development while benefiting from the results. 
Where computational linguistics interacts with 
other areas of language research and teaching, 
there are additional opportunities for symbiosis. 
Natural language technology may offer greater 
access and robustness to empirical linguistic 
research that in turn may offer new data 
necessary to develop new technologies. This 
paper discusses common infrastructure for the 
annotation of linguistic data and the application 
of that infrastructure to several traditionally very 
diverse fields of inquiry.  
2 Common Assumptions, Needs and 
Goals in Natural Language Studies 
Human language resources, expensive to 
create and maintain, are in increasing demand 
among a growing number of research 
communities. One solution to this expanding 
need is to reannotate and reuse language 
resources created for other purposes. The now 
classic example is that of the Switchboard-1 
Corpus (ISBN: 1-58563-121-3), a collection of 
2400 two-sided telephone conversations among 
543 U.S. speakers, created by Texas Instruments 
in 1991. Although collected for speaker 
identification and topic spotting research, 
Switchboard has been widely used to support 
large vocabulary conversational speech 
recognition. It has been extensively corrected 
twice, once at Penn and NIST, and once at 
Mississippi State. Two excerpts have been 
published as test corpora for government-
sponsored projects. At least 6 other annotations 
have been created at various times and more-or-
less widely distributed among research sites: 
part-of-speech annotation (Penn); syntactic 
structure annotation (Penn); dysfluency 
annotation (Penn); partial phonetic transcription 
(independently at UCLA and at Berkeley); and 
discourse function annotation (Colorado). These 
annotations use different ?editions? of the 
underlying corpus and have sometimes silently 
introduced their own corrections or modified the 
data format to suit their needs. Thus the 
Colorado discourse function annotation was 
based on phrase structures introduced by the 
Penn dysfluency annotation, which in turn was 
based on the Penn/NIST corrections, which in 
turn were based on the original TI transcriptions 
of the underlying (and largely unchanging) 
audio files. Switchboard and its derivatives 
remain in active use worldwide, and new 
derivatives continue to be produced, along with 
(published and unpublished) corrections of old 
ones. This worsens the already acute problem of 
establishing and maintaining coherent relations 
among the derivatives in common use today. 
The Switchboard-1 case is by no means 
isolated (Graff & Bird 2000). The Topic 
Detection and Tracking Corpus, TDT-2 (ISBN: 
1-58563-157-4) was created in 1998 by LDC 
and contains newswire and more than 600 hours 
of transcribed broadcast news from 8 English 
and 3 Chinese sources sampled daily over six 
months with annotations to indicate story 
boundaries and relevance of those stories to 100 
randomly selected topics. Since its release, 
TDT-2 has been used as training, development-
test and evaluation data in the TDT evaluations; 
the audio has been used in TREC SDR 
evaluations (Garofalo, Auzanne and Voorhees 
2000), TDT text has been partially re-annotated 
for entity detection in the Automatic Content 
Extraction project (Przybocki 2000) and 
portions have been used for the Center for 
Spoken Language Processing?s workshops in 
Novel Information Detection (Allan et. al. 
1999), Mandarin-English Information (Meng et. 
al. 2000) and Audio-Visual Speech Recognition 
(Chalapati 2000). 
Switchboard and TDT are just two examples 
of a growing trend toward reannotation and 
reuse of language resources, a trend that is not 
limited to language engineering. Miller and 
Walker (2001) have demonstrated the value of 
the CALLHOME German corpus (ISBN: 1-
58563-117-5), developed to support speech 
recognition research, for language teaching. 
Deckert & Yaeger-Dror (2000) have used 
Switchboard to study regional syntactic 
variation in American English. 
Reannotation and reuse of linguistic data 
highlight the need for common infrastructure to 
support resource development across disciplines 
and specialties. 
3 Overlaps between Human Language 
Technology and Other Linguistic 
Research 
Many specialties in empirical linguistics and 
language engineering require large volumes of 
language data and tools for browsing and 
searching the data efficiently. The sections that 
follow provide examples of recent efforts to 
address emerging needs for language resources.  
Interlinear Texts and Linguistic Exploration  
Interlinear text is a product of linguistic 
fieldwork often in low-density languages. The 
physical appearance of interlinear text typically 
consists of a main text line annotated with 
linguistic transcriptions and analyses, such as 
morphological representations, glosses at 
various levels, part-of-speech tags, and a free 
translation at the sentence level. Fragments of 
these annotation lines are vertically aligned with 
the corresponding fragments of text. Phrasal 
translations and footnotes are often presented on 
other lines. Interlinear texts come in many forms 
and can be represented digitally in many ways, 
e.g. plain text with hard spacing, tables, special 
markup, and special-purpose data structures. 
There are various methods for linking to audio 
data and lexical entries, and for including 
footnotes and other marginalia. This diversity of 
form presents problems for general-purpose 
software for searching, exchanging, displaying 
and enriching interlinear texts. Nonetheless 
interlinear text is a precious resource with 
multiple uses in natural language processing. Its 
various components can be used in the 
development of lexical and morphological 
resources, can support tagging and parsing and 
can provide training material for machine 
translation. Maeda and Bird (2000, 2001) 
demonstrated a tool for creating interlinear text. 
A screenshot appears in Figure 1. 
Figure 1: Interlinear text tool   using the AG 
Toolkit 
Sociolinguistic Annotation  
The quantitative analysis of linguistic 
variation begins with empirical observation and 
statistical description of linguistic behavior. 
Although general computer technology 
encourages the collection, annotation, analysis 
and discussion of linguistic behavior wholly 
within the digital domain, few tools exist to help 
the sociolinguist in this effort. The project on 
Data and Annotations for Sociolinguistics 
(DASL) is investigating best practices via a case 
study of well-documented sociolinguistic 
phenomena in several large speech corpora: 
TIMIT , Switchboard-1, CallHome and Hub-4. 
Researchers are currently annotating the corpora 
for t/d deletion, the process by which [t] and [d] 
sometimes fail to be realized under certain 
phonological, morphological and social 
conditions. The case study is also a means to 
address broader questions: How do the specified 
corpora compare with the interview data 
typically used in sociolinguistics? Will the study 
of corpus data reveal new patterns not evident in 
the more common studies conducted within the 
framework of the speech community? Can 
empirical research on language variation be 
organized on a large scale with teams of non-
specialist annotators? 
All of the data used in DASL were originally 
created to support human language technology 
development; the datasets are currently being 
reannotated to support empirical studies of 
linguistic variation. A custom annotation tool 
allows users to query each corpus for tokens of 
potential interest greatly reducing effort relative 
to traditional approaches. Annotators can read or 
listen to each token, access demographic data 
and encode their observations in formats 
compatible with other analytical software used 
in the community. The web-based interface in 
Figure 2 promotes multi-site annotation and the 
study of inter-annotator consistency (Cieri and 
Strassel, 2001). 
Authoring Resources and Tools for Language 
Learning 
Although current information technology 
encourages new approaches in computer assisted 
language learning and teaching, progress in this 
area is hampered by an inadequate supply of 
language resources. The SMART (Source Media 
Authoring Resources and Tools) pilot project is 
addressing this problem by providing 
appropriately licensed data and software 
resources for preparing language-learning 
material. The Linguistic Data Consortium, a 
partner in this effort, is contributing several of 
its large data sets including conversational and 
broadcast data in Arabic, English, French and 
German. The language resources overlap almost 
completely with those used in language 
engineering. SMART is building upon the 
distribution model established in LDC Online, a 
service that provides network-based access to 
hundreds of gigabytes of text and audio data and 
annotations. Audio data are available digitally in 
files corresponding to a conversation, broadcast 
or other linguistic event. To facilitate searching, 
LDC Online includes, according to their 
availability, human- and machine-generated 
Figure 2: Sociolinguistic Annotation Tool 
transcripts time-aligned to permit more fine-
grained access. For example, where a time-
aligned transcript of a conversation exists, users 
may extract, reformat and play any segment 
specified by the time stamps in the transcript. 
SMART is building upon this foundation by 
providing additional data resources, browsing 
and search customized to the needs of language 
teachers and additional output formats to 
accommodate courseware authoring tools 
available in the commercial market. 
SMART promises to benefit a wide range of 
language teachers and learners but only to the 
extent that its resources are readily available. 
The volume of SMART data exceeds that which 
can be easily transferred over a network. Even 
small video clips consume hundreds of megabits 
of bandwidth. Instead SMART data will be 
delivered via servers that maintain raw data and 
associated annotations, permit browsing and 
queries and allow the user to specify the format 
and granularity of the response. The user will 
have the option of downloading the data for 
local use or adding annotations that may be kept 
privately or made public via the annotation 
server. The technology of the annotation server 
coupled with the extensibility of annotation 
graphs described below will enables nearly 
unconstrained access to SMART data. 
These efforts to support interlinear text, 
sociolinguistic annotation and multimodal data 
in language teaching each require flexible access 
to signal data and associated annotations. The 
sections that follow describe an architecture that 
provides such access. 
4 Annotation Graphs, Annotation 
Servers and a Query Language: 
Common Infrastructure for 
Coordinated Research, Resource 
Development 
Storing and serving large amounts of 
annotated data via the web requires 
interoperable data representations and tools 
along with methods for handling external 
formats and protocols for querying and 
delivering annotations. Annotation graphs were 
presented by Bird and Liberman (1999) as a 
general purpose model for representing and 
manipulating annotations of time series data, 
regardless of their physical storage format.  An 
annotation graph is a labeled, directed, acyclic 
graph with time offsets on some of its nodes. 
The formalism is illustrated below by 
application to the TIMIT Corpus (Garofolo et al 
1986). The original TIMIT word file contains 
starting and ending offsets (in 16KHz samples) 
and transcripts of each word in the audio file  
 
train/dr1/fjsp0/sa1.wrd: 
 2360    5200   she 
 5200    9680   had  
 9680   11077   your 
11077   16626   dark 
16626   22179   suit 
22179   24400   in 
24400   30161   greasy 
30161   36150   wash 
36720   41839   water 
41839   44680   all 
44680   49066   year  
 
The phone file provides the same information 
for each sound in the audio file. This is the 
phonetic transcription for ?she had?.  
 
train/dr1/fjsp0/sa1.phn: 
    0    2360    h# 
 2360    3720    sh 
 3720    5200    iy 
 5200    6160    hv 
 6160    8720    ae 
 8720    9680    dcl 
 9680   10173    y 
10173   11077    axr 
11077   12019    dcl 
12019   12257    d 
 
A section of the corresponding annotation 
graph appears in Figure 3. Each node displays 
the node identifier and the time offset.  The arcs 
are decorated with type and label information. 
Type W is for words and the type P is for 
phonetic transcriptions. 
Figure 3: A TIMIT annotation graph 
Since an annotation graph is just a set of 
(timed) nodes, arcs and labels, it can be trivially 
represented using three relational tables: 
 
Time:       Arc:               Label: 
 N     T      A   X   Y  T      A  L 
--------     -------------     ------- 
 0     0      1   0   1  P      1  h# 
 1  2360      2   1   2  P      2  sh 
 2  3270      3   2   3  P      3  iy 
 3  5200      4   3   4  P      4  hv 
 4  6160      5   4   5  P      5  ae 
 5  8720      6   5   6  P      6  dcl 
 6  9680      7   6   7  P      7  y 
 7 10173      8   7   8  P      8  axr 
 8 11077      9   8   9  P      9  dcl 
 9 12019     10   9  10  P     10  d 
 
10 12257     19   3   6  W     18  she 
14 16626     20   6   8  W     19  had 
17 22179     21   8  14  W     20  your 
             22  14  17  W     21  dark  
                               22  suit 
 
A large amount of annotation can be 
efficiently represented and indexed in this 
manner.  This brings us to the question of 
converting (or loading) existing data into such a 
database. The LDC's catalog alone includes 
nearly 200 publications, where each typically 
has its own format (often more than one). The 
sheer quantity and diversity of the data presents 
a significant challenge to the conversion 
process.  In addition, some corpora exist in 
multiple versions, or include uncorrected, 
corrected and re-corrected parts. 
The Annotation Graph Toolkit, version 1.0, 
contains a complete implementation of the 
annotation graph model, import filters for 
several formats, loading/storing data to an 
annotation server (MySQL), application 
programming interfaces in C++ and Tcl/tk, and 
example annotation tools for dialogue, ethology 
and interlinear text.  The supported formats are: 
xlabel, TIMIT, BAS Partitur, Penn Treebank, 
Switchboard, LDC Callhome, CSV and AIF 
level 0.  Future work will provide Python and 
Perl interfaces, more supported formats, a query 
language and interpreter, and a multi-channel 
transcription tool.  All software is distributed 
under an open source license, and is available 
from http://www.ldc.upenn.edu/AG/. 
Given that the annotation data can be stored 
in a relational database, it can be queried 
directly in SQL.  More convenient, a domain-
specific query language will be developed (see 
Cassidy and Bird 2000 and the work cited 
there). Query expressions will be transmitted 
over the web in the form of a CGI request, and 
translated into SQL by the annotation server. 
The resulting annotation data will be returned in 
the form of an XML document.  An example for 
the TIMIT database, using the language 
proposed by Cassidy and Bird (2000), will serve 
to illustrate: 
Find word arcs spanning a sequence of 
segments beginning with hv and containing ae: 
http://BASE-URL/cgi-bin/query? 
X.[].Y<timit/word; 
X.[:hv].[]*.[:ae].[]*.Y<-timit/ph 
Executed on the above annotation data, this 
query would return the XML document in 
Figure 4. 
Neither the query nor the returned document 
are intended for human consumption. A client-
side annotation tool will initiate queries and 
display annotation content on behalf of an end-
user.  
<?xml version="1.0"?> 
<!DOCTYPE AGSet SYSTEM "ag.dtd"> 
<AGSet id="Timit" version="1.0" xmlns="http://www.ldc.upenn.edu/atlas/ag/"  
        xmlns:xlink="http://www.w3.org/1999/xlink" 
        xmlns:dc="http://purl.org/DC/documents/rec-dces-19990702.htm"> 
<Timeline id="T1"> 
<Signal id="S1" mimeClass="audio" mimeType="wav" encoding="wav" 
        unit="16kHz" xlink:href="TIMIT/train/dr1/fjsp0/sa1.wav"/> 
</Timeline>       
<AG id="t1" type="transcription" timeline="T1"> 
<Anchor id="A3" offset="5200" unit="16kHz"/> 
<Anchor id="A6" offset="9680" unit="16kHz"/> 
<Annotation id="Ann10" type="W" start="A3" end="A6"> 
<Feature name="label">had</Feature> 
</Annotation> 
</AG> 
</AGSet> 
Figure 4: Document returned by AG query 
This annotation tool and server are integrated 
using the model shown below. A simplified 
client-server model, working at the level of 
annotation files is already available with the 
current distribution of the Annotation Graph 
Toolkit. Significantly, a networked annotation 
tool is identical to a standalone version, except 
that the AG library fetches its data from a 
remote server instead of local disk.  
 
The annotation graph formalism, annotation 
servers and the emerging query language will 
provide basic infrastructure to store, process and 
deliver essentially arbitrary amounts and types 
of signal annotations for a wide variety of 
research and teaching tasks including those 
described above. This infrastructure will enable 
reuse of existing resources and coordinated 
development of new resources both within and 
across research communities working with 
annotated linguistic datasets. 
5 Remaining Challenges to Language 
Resource Development 
We have described a process whereby 
annotated data in a variety of formats can be 
loaded into a central database server that 
interacts directly with annotation tools. The 
Annotation Graph Toolkit, version 1.0, is the 
first implementation of this architecture. As the 
toolkit undergoes future development, it will 
need to deal continually with conversion issues. 
Annotation data will continue to be created and 
manipulated by multiple tools and to be stored 
in incompatible file formats.  Data will continue 
to be mapped between different formats so that 
appropriate tools can be used, and appropriately 
managed to keep inconsistencies from arising.  
There will still be times when we need to trace 
the provenance of a particular item, back 
through a history involving several formats. 
These will always be hard problems; the 
proposed infrastructure will address them but no 
infrastructure is likely to eliminate conversion, 
integrity and provenance issues. 
Annotation graphs focus on the problems of 
dealing with time series. They do not directly 
address paradigmatic data such as lexicons and 
demographic tables. One should note however, 
that time series data and paradigmatic data can 
be united efficiently. As already mentioned, 
annotation graphs may be stored trivially in 
relational tables, technology routinely  used for 
paradigmatic data. In this way, conventional 
?joins? of relational table can convolve time-
series annotations with paradigms (e.g. texts 
with dictionaries or utterances with speaker 
demographics). 
Through judicious compromises - such as 
one-time computer-assisted conversion of 
legacy annotation data and creating once-off 
interfaces to existing useful tools - and through 
the judicious combination of simple and well-
supported formalisms and technologies as 
described above, we believe that the 
management problems can be substantially 
reduced in scale and severity. 
We can illustrate the advantages of AG with 
a example of the annotation of the Switchboard 
corpus for ?t/d deletion. Switchboard contains 
two-channel audio of thousands of 5-minute 
conversations among pairs of speakers that have 
been transcribed with the transcripts time-
aligned to the audio. A single utterance is 
written: 
 
274.35 279.50 A.119 Uh, he, 
uh, carves out different figures 
in the, in the plants, 
 
giving the start and stop time of the utterance, 
channel, speaker ID and the transcript of the 
utterance. This can be converted trivially into 
AG format as above. 
Figure 5:  Interactions among annotation tools 
and the annotation server 
The DASL tool concordances audio 
transcripts and identifies utterances in which the 
target phenomenon (eg. ?t/d deletion) may 
occur. A line of the concordance file contains 
two IDs one to identify the utterance within the 
concordance, the other to link back to the 
original corpus. The <annotate> tags identify a 
potential environment for the phenomenon 
under study.  
 
<sample id="1" senid="10194">uh 
he uh carves out <annotate> 
different figures </annotate> in 
the in the p[lants]- plants 
shrubs </sample> 
 
The link between the concordance and the 
original corpus is maintained through a table 
containing: Sentence_ID, File_ID, Start_Time, 
Stop_Time, Channel and Speaker. 
 
10194 2141 274.35  279.50 A 1139 
 
Speakers? demographic data appears in 
another table containing: Speaker_ID, Sex, Age, 
Region, Education_Level 
 
1139, MALE, 50, NORTHERN, 2 
 
The DASL interface embeds the concordance 
results in a template containing input fields for 
each parameter to be annotated (see Figure 2). 
The linguist?s annotation of the utterance can be 
stored in AG formalism as in Figure 5. Note that 
although AGs provide an elegant and general 
solution to the annotation of time series data, 
they do not remove the need to deal with the ad 
hoc formats one may encounter in various 
corpora. Nor do they remove the need to track 
the relations among elements in time-series data 
and paradigmatic material. 
6 Conclusions  
Researchers in human language share 
assumptions and needs within and across 
research communities. Each group feels an acute 
need for language resources including data, 
annotations, formats and processes. This paper 
has summarized some common needs and 
described an architecture for encoding 
annotations and delivering them via annotation 
servers using SQL or a custom query language. 
Much of the architecture discussed has already 
been created and is available in the Annotation 
Graphic Toolkit. Other components, especially 
the query language, are currently under 
development. It is hoped that tools based on 
annotations graphs and annotation servers will 
encourage greater levels of resource sharing and 
the coordination of future resource development. 
Figure 5: A sociolinguistic annotation in AG format 
<?xml version="1.0"?> 
<!DOCTYPE AGSet SYSTEM "http://www.ldc.upenn.edu/AG/doc/xml/ag.dtd"> 
<AGSet id="DASL" version="1.0" 
xmlns="http://www.ldc.upenn.edu/atlas/ag/" xmlns:xlink="http://www.w3.org/1999/xlink" 
xmlns:dc="http://purl.org/DC/documents/rec-dces-19990702.htm"> 
<Metadata></Metadata> 
<Timeline id="DASL:Timeline1"> <Signal id="DASL:Timeline1:Signal1" mimeClass="audio" 
mimeType="wav" encoding="mu-law" unit="8kHz" xlink:type="simple"  
xlink:href="LDC93S7:sw2141.wav"> 
</Signal></Timeline>  
<AG id="DASL:AG1" timeline="DASL:Timeline1"> 
<Anchor id="DASL:AG1:Anchor1" offset="274.595" signals="DASL:Timeline1:Signal1"></Anchor> 
<Anchor id="DASL:AG1:Anchor2" offset="280.671" signals="DASL:Timeline1:Signal1"></Anchor> 
<Annotation id="DASL:AG1:Annotation1" type="csv" start="DASL:AG1:Anchor1" 
 end="DASL:AG1:Anchor2"> 
<Feature name="td">Deleted</Feature> <Feature name="Morphological">Monomorpheme</Feature> 
<Feature name="EPreceding">AlveolarNasal</Feature> <Feature name="EFollowing">Obstruent</Feature> 
<Feature name="Same_Prec_Foll">N/A</Feature>  <Feature name="Stress">Unstressed</Feature> 
<Feature name="Cluster_complexity">Two_elements</Feature> 
<Feature name="Sentence_id">1</Feature> <Feature name="Corpus_name">swb</Feature>  
<Feature name="WPreceding">uh he uh carves out </Feature> 
<Feature name="WMatched">different figures</Feature>  
<Feature name="WFollowing"> in the in the p[lants]- plants shrubs</Feature> 
<Feature name="File_name">/speech/swb0/sw2141.wav</Feature>  
<Feature name="Speech_channel">1</Feature> <Feature name="Speaker_id">1139</Feature> 
<Feature name="Sex">MALE</Feature>   <Feature name="Birth_year">1956</Feature> 
<Feature name="Dialect">NORTHERN</Feature> <Feature name="Edu">2</Feature>  
</Annotation></AG></AGSet> 
References 
James Allan, et. al., (1999) Topic Based Novelty 
Detection 1999 Summer Workshop at CLSP Final 
Report, http://www.clsp.jhu.edu/ws99/final/Topic-
based.pdf 
Steven Bird & Mark Liberman (2001) A Formal 
Framework for Linguistic Annotation, Speech 
Communication 33(1,2) pp 23-60, 
http://arxiv.org/abs/cs/0010033 
Steven Bird, Peter Buneman & Wang-Chiew Tan 
(2000) Towards a query language for annotation 
graphs, Proceedings of the Second International 
Conference on Language Resources and 
Evaluation, pp. 807-814.  
Steve Cassidy & Steven Bird (2000) Querying 
databases of annotated speech, Proceedings of the 
Eleventh Australasian Database Conference, 
http://www.ldc.upenn.edu/Papers/ADC2000/adc0
0.pdf 
Center for Language and Speech Processing  (2000) 
Summer Workshop Pages, 
http://www.clsp.jhu.edu/workshops/. 
Chalapati, Neti, et. al. (2000) Audio Visual Speech 
Recognition, Summer Workshop at CLSP Final 
Report, 
http://www.clsp.jhu.edu/workshops/ws2000/final_
reports/avsr/. 
Lea Christiansen, Christopher Cieri, Kathleen Egan, 
Anita Kulman, Milton Paul (2001) Getting 
SMART about Authoring, Presented at CALICO 
2001: Computer Aided Language Instruction 
Conference, Orlando, University of Central 
Florida. 
Christopher Cieri and Stephanie Strassel (2001) 
DASL Project Pages, 
http://www.ldc.upenn.edu/Projects/DASL. 
Deckert & Yaeger-Dror (2000) Dialect variation in 
negation strategies in the LDC Switchboard 
corpus Corpus Linguistics Conference 2, Boston 2, 
pp. 49-59. 
Garofalo, John, Cedric Auzanne and Ellen Voorhees 
(2000) The TREC Spoken Document Retrieval 
Track: A Success Story, 
http://www.nist.gov/speech/tests/sdr/sdr2000/pape
rs/01plenary1.pdf 
Garofalo, John S., Lori F. Lamel, William M. Fisher, 
Jonathan G. Fiscus, David S. Pallett, and Nancy L. 
Dahlgren, "The DARPA TIMIT Acoustic-
Phonetic Continuous Speech Corpus CDROM" 
(printed documentation; available on request from 
the LDC). 
David Graff, Steven Bird (2000) Many uses, many 
annotations for large speech corpora, Proceedings 
of the Second 2nd Language Resources and 
Evaluation Conference, Athens, Greece, pp. 427-
433. 
http://www.ldc.upenn.edu/Papers/LREC2000/mult
iuse.pdf. 
Kazuaki Maeda and Steven Bird (2000) A Formal 
Framework for Interlinear Text, Web-Based 
Language Documentation and Description 
Workshop, University of Pennsylvania, 
Philadelphia, December 2000 
http;//www.ldc.upenn.edu/exploration/expl2000/. 
Kazuaki Maeda and Steven Bird (2001), Annotation 
Tools Based on the Annotation Graph API, 
Proceedings of this workshop. 
Meng, Helen, et. al., (2000) Mandarin English 
Information (MEI) : Investigation Translingual 
Speech Retrieval, 1999 Summer Workshop at 
CLSP Final Report 
http://www.clsp.jhu.edu/ws2000/final_reports/mei
/ws00mei.pdf 
David Miller and Kevin Walker (2001) Telephone 
Speech in the Foreign Language Classroom: 
Applications Methods and Technology, Presented 
at CALICO 2001: Computer Aided Language 
Instruction Conference, Orlando, University of 
Central Florida. 
Przybocki, Mark (2000) Automatic Content 
Extraction Web Page, 
http://www.nist.gov/speech/tests/ace/ 
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 55?62,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Basic Language Resources for Diverse Asian Languages:
A Streamlined Approach for Resource Creation
Heather Simpson, Kazuaki Maeda, Christopher Cieri
Linguistic Data Consortium
University of Pennsylvania
3600 Market St., Suite 810
Philadelphia, PA 19104, USA
{hsimpson, maeda, ccieri}@ldc.upenn.edu
Abstract
The REFLEX-LCTL (Research on En-
glish and Foreign Language Exploitation-
Less Commonly Taught Languages) pro-
gram, sponsored by the United States
government, was an effort in simultane-
ous creation of basic language resources
and technologies for under-resourced lan-
guages, with the aim to enrich sparse ar-
eas in language technology resources and
encourage new research. We were tasked
to produce basic language resources for 8
Asian languages: Bengali, Pashto, Pun-
jabi, Tamil, Tagalog, Thai, Urdu and
Uzbek, and 5 languages from Europe and
Africa, and distribute them to research and
development also funded by the program.
This paper will discuss the streamlined ap-
proach to language resource development
we designed to support simultaneous cre-
ation of multiple resources for multiple lan-
guages.
1 Introduction
Over the past decade, the scope of interest in lan-
guage resource creation has increased across mul-
tiple disciplines. The differing approaches of these
disciplines are reflected in the terms used for newly
targeted groups of languages. Less commonly
targeted languages (LCTLs) research may focus
on development of basic linguistic technologies
or language-aware applications. The REFLEX-
LCTL (Research on English and Foreign Language
Exploitation-Less Commonly Taught Languages)
program, sponsored by the United States govern-
ment, was an effort in simultaneous creation of ba-
sic language resources and technologies for LCTLs,
namely languages which have large numbers of
speakers, but are nonetheless infrequently studied
by language learners or researchers in the U.S.
Under the REFLEX-LCTL program, we pro-
duced ?language packs? of basic language resources
for 13 such languages. This paper focuses on the
resources created for the 8 Asian languages: Ben-
gali, Pashto, Punjabi, Tamil, Tagalog, Thai, Urdu
and Uzbek.1
Our approach to language pack creation was to
maximize our resources. We accomplished this in a
number of ways. We engaged in thorough planning
to identify required tasks and their interdependen-
cies, and the human and technical resources needed
to accomplish them. We focused intensely on iden-
tifying available digital resources at the start of
language pack creation, so we could immediately
begin assessment of their usability, and work on
filling the resource gaps identified. We developed
annotation tools usable for multiple tasks and all
languages and designed to make manual annota-
tion more efficient. We developed standards for
data representations, to support efficient creation,
processing, and end use.
2 Planning For Basic Language
Resources Creation
2.1 Language Selection
The selection of REFLEX-LCTL target languages
was based on a number of criteria, while operating
within a fixed budget. The most basic criterion
was that the language be spoken by large num-
ber of native speakers, but poorly represented in
terms of available language resources. The Indic
languages (Bengali, Punjabi, Urdu) were chosen
to give researchers the opportunity to experiment
with bootstrapping techniques with resources in re-
lated languages. In order to maximize the useful-
ness and generality of our methods, the project
adopted the additional goals of variation in the ex-
pected availability of raw resources and also varia-
tion in linguistic characteristics both within the set
of selected languages and in comparison to more
well-resourced languages. Though we are focus-
ing, in this paper, on the Asian languages, LCTL
languages are linguistically and geographically di-
verse, representing major language families and
major geographical regions.
The following short descriptions of the Asian
LCTL languages are intended to provide some con-
text for the language pack discussions. The lan-
1The other languages were: Amazigh (Berber),
Hungarian, Kurdish, Tigrinya and Yoruba.
55
guage demographic information is taken from Eth-
nologue (Gordon, 2005).
2.2 Bengali
Bengali is spoken mostly in Bangladesh and India.
The language pack for Bengali was the first com-
plete language pack we created, and it served as
a pilot language pack for the rest of the project.
There were a relatively large number of raw mate-
rials and existing lexicons to support our lexicon
development, and our language pack included the
largest lexicon among the Asian language packs.
2.3 Urdu
Urdu, spoken primarily in Pakistan and part of
India, is closely related to Hindi. Urdu is tra-
ditionally written using Perso-Arabic script, and
has vocabulary borrowed from Arabic and Persian.
This was a language which had a large amount of
available digital resources in comparison with other
LCTLs, but did not meet our original expectations
for raw digital text.
2.4 Thai
Thai is a Tai-Kadai language spoken by more than
20 million people, mainly in Thailand. Thai was
another language which was relatively rich in avail-
able digital language resources. The Thai language
pack includes the largest amount of monolingual
text and found parallel text among the language
packs. For Thai, tokenization, or word segmenta-
tion, was probably the most challenging aspect of
the resource creation effort. For the initial version
of the language pack, we used a tokenization tool
adopted from an opensource software package.
2.5 Tamil
Tamil is a Dravidian language with more than 60
million speakers in India, Sri Lanka, Singapore and
other countries. We benefited from having local
language experts available for consultation on this
language pack
2.6 Punjabi
Punjabi, also written as Panjabi, is an Indo-
European language spoken in both India and Pak-
istan. Ethnologue and ISO 639-3 distinguish three
variations of Punjabi: Eastern, Mirpur and West-
ern, and the Eastern variation has the largest pop-
ulation of speakers. We were able to obtain rela-
tively large amounts of monolingual text and ex-
isting parallel text.
2.7 Tagalog
Tagalog is an Austronesian language spoken by 15
million people, primarily in the Philippines. The
monolingual text we produced is the smallest (774
K words) among the eight Asian language packs,
due in part to the prevalence of English in for-
mal communication mediums such as news publi-
cations.
2.8 Pashto
Pashto an Indo-European language spoken primar-
ily in Afghanistan and parts of Pakistan. It is one
of the two official languages in Afghanistan. Eth-
nologue and ISO 639-3 distinguish three varieties
of Pashto: Northern, Central and Southern. Ma-
jor sources of data for this language pack included
BBC, Radio Free America and Voice of America.
2.9 Uzbek
Uzbek is primarily spoken in Uzbekistan and in
other Asian republics of the former Soviet Union.
The creation of the language pack for Uzbek, which
is a Turkic language, and the official language of
Uzbekistan, was outsourced to BUTE (Budapest
University of Technology and Economics) in Hun-
gary. Even though the Uzbek government officially
decided to use a Latin script in 1992, the Cyrillic
alphabet used between the 1940?s and 1990?s are
still commonly found. Our language pack contains
all resources in Latin script and includes an en-
coding converter for converting between the Latin
script and the Cyrillic script.
2.10 Designing Language Packs
Within the REFLEX-LCTL program, a language
pack is a standardized package containing the fol-
lowing language resources:
? Documentation
? Grammatical Sketch
? Data
? Monolingual Text
? Parallel Text
? Bilingual Lexicon
? Named Entity Annotated Text
? POS Tagged Text
? Tools
? Tokenizer
? Sentence Segmenter
? Character Encoding Conversion Tool
? Name Transliterators
? Named Entity Tagger
? POS Tagger
? Morphological Analyzer
Grammatical sketches are summaries (approxi-
mately 50 pages) of the features of the written lan-
guage. The primary target audience are language
engineers with a basic grounding in linguistic con-
cepts.
56
Monolingual text is the foundation for all other
language pack resources. We provided monolingual
text in both tokenized and non-tokenized format.
Parallel text is an important resource for devel-
opment of machine translation technologies, and
allows inductive lexicon creation. The bilingual
lexicons also support a variety of language tech-
nologies. The named entity annotations and part
of speech tagged text can be used to create auto-
matic taggers.
The language packs also include basic data pro-
cessing and conversion tools, such as tokenizers,
sentence segmenters, character encoding convert-
ers and name transliterators, as well as more ad-
vanced tools, such as POS taggers, named entity
taggers, and morphological analyzers.
These language packs include 6 of the 9 text re-
sources and tools in 4 of the 15 text-based modules
listed in the current BLARK matrix (ELDA, 2008).
When we had a relatively stable definition of the
deliverables for language pack, we were able to be-
gin planning for the downstream processes
3 Standards for Data
Representation
An important step in planning was to define stan-
dards for language pack data representation, to
allow all downstream processes to run more effi-
ciently.
3.1 Language Codes
We decided to use the ISO 639-32 (also Ethnologue,
15th edition (Gordon, 2005)) three-letter language
codes throughout the language packs. For exam-
ple, the language code is stored in every text data
file in the language packs. The ISO 639-3 lan-
guage codes for our eight languages are as follows:
Urdu (URD), Thai (THA), Bengali (BEN), Tamil
(TAM), Punjabi (PAN), Tagalog (TGL), Pashto
(PBU) and Uzbek (UZN). When there were multi-
ple ISO 639-3 codes for a target language, the code
for the sublanguage for which the majority of the
written text can be obtained was used.
3.2 File Formats
One of the first tasks in planning for this data
creation effort was to define file formats for the
monolingual text, parallel text, lexicons and an-
notation files. This designing process was led by
us and a group of experts selected from the re-
search sites participating in the REFLEX-LCTL
program. The requirements included the follow-
ing:
? Monolingual and parallel text files should be
able to represent sentence segmentation, and
2See http://www.sil.org/iso639-3/ for more de-
tails
both tokenized and non-tokenized text.
? For parallel text, the text and the target lan-
guage and the translations in English should
be stored in separate aligned files.
? Unique IDs should be assigned to sen-
tences/segments, so that the segment-level
mapping in parallel text is clear.
? Annotation files should be in a stand-off for-
mat: i.e., annotations should be separate from
the source text files.
? Lexicon files should be able to represent at the
minimum of word, stem, part-of-speech, gloss
and morphological analysis.
? File formats should be XML-based.
? Files should be encoded in UTF-8 (UNI-
CODE).
After several cycles of prototyping and exchang-
ing feedback, we settled on the following original
file formats named ?LCTL text format? (LTF - file
name extension: .ltf.xml), ?LCTL annotation for-
mat? (LAF - file name extension: .laf.xml), and
?LCTL lexicon format? (LLF - file name exten-
sion .llf.xml. Appendix A shows the DTD for LTF
format.
3.3 Evaluation and Training Data
Partition
To support evaluation of language technologies
based on the data included in the language packs,
we designated approximately 10% of our primary
data as the evaluation data and the rest as the
training data. Any data that was included as ?as-
is?, (e.g. found parallel text), was included in the
training partition.
3.4 Directory Structure and File Naming
Conventions
Giving all language packs a consistent design and
structure allows users to navigate the contents with
ease. As such, we defined the directory structure
within each language pack to be the following.
The top directory was named as follows.
LCTL_{Language}_{Version}/
For example, the version 1.0 of the Urdu lan-
guage pack would have the top directory named
LCTL Urdu v1.0.
The top directory name is also used as the official
title for the package, so the full name rather than
the language code was used for maximum clarity
for users not familiar with the ISO coding.
Under the main directory, the following subdi-
rectories are defined:
57
Documentation/
Grammatical_Sketch/
Tools/
Lexicon/
Monolingual_Text/
Parallel_Text/
Named_Entity_Annotations/
POS_Tagged_Text/
The Parallel Text directory was divided into
?Found? and ?Translation? directories. The Found
directory contains parallel text that was available
as raw digital text, which we processed into our
standardized formats. The Translation directory
contains manually translated text, created by our
translators or subcontractors as well as part of
found parallel text which we were able to align
at the sentence-level. The data directories (e.g.,
Monolingual Text, Parallel Text, Named Entity
Annotation, POS Tagged Text) were further di-
vided into evaluation data (?Eval?) and training
data (?Train?) directories as requested by the pro-
gram.
We used the following format for text corpora
file names wherever possible:
{SRC}_{LNG}_{YYYYMMDD}.{SeqID}
{SRC} is a code assigned for the source; {LNG}
is the ISO 639-3 language code; {YYYYMMDD}
is the publication/posting date of the document;
and {SeqID} is a unique ID within the documents
from the same publication/posting date.
4 Building Technical Infrastructure
In creating the language resources included in the
LCTL language packs, we developed a variety of
software tools designed for humans, including na-
tive speaker informants without technical exper-
tise, to provide data needed for the resource cre-
ation efforts as efficiently as possible. In particular,
the following tools played crucial roles in the cre-
ation of language packs.
4.1 Annotation Collection Kit Interface
(ACK)
In order to facilitate efficient annotation of a vari-
ety of tasks and materials, we created a web-based
judgment/annotation tool, named the Annotation
Collection Kit interface (ACK). ACK allows a task
manager to create annotation ?kits,? which consist
of question text and predefined list and/or free-
form answer categories. Any UTF-8 text may be
specified for questions or answers. ACK is ideal
for remote creation of multiple types of text-based
annotation, by allowing individual ?kits? to be up-
loaded onto a specific server URL which any re-
mote user can access. In fact, using this tool
we were able to support native speaker annota-
tors working on part-of-speech (POS) annotation
in Thailand.
When annotators make judgments in ACK, they
are stored in a relational database. The results can
be downloaded in CSV (comma-separated value) or
XML format, so anyone with secure access to the
server can easily access the results.
ACK was designed so that anyone with even a
basic knowledge of a scripting language such as
Perl or Python would be able to create the ACK
annotation kits, which are essentially sets of data
corresponding to a sets of annotation decisions in
the form of radio buttons, check boxes, pull-down
menus, or comment fields. Indeed some of the lin-
guists on the LCTL project created their own ACK
kits when needed. Although they are limited in
scope, creative use of ACK kits can yield a great
deal of helpful types of annotation.
For example, for POS annotation, the annota-
tors were given monolingual text from our corpus,
word by word, in order, and asked to select the
correct part of speech for that word in context.
We also used ACK to add/edit glosses and part
of speech tags for lexicon entries, to perform mor-
phological tagging, and various other tasks that
required judgment from native speakers.
Figure 1: ACK - Annotation Collection Kit
Figure 1 shows a screen shot of ACK.
4.2 Named Entity Annotation Tool
For named entity annotation task, we chose to em-
ploy very simple annotation guidelines, to facili-
tate maximum efficiency and accuracy from native-
speaker annotators regardless of linguistic training.
We used an named entity (NE) annotation tool
called SimpleNET, which we previously developed
for the named entity annotation task for another
project. SimpleNET requires almost no training
in tool usage, and annotations can be made either
with the keyboard or the mouse. The NE anno-
tated text in the LCTL language packs was created
with this tool. This tool is written in Python us-
ing the QT GUI toolkit, which allows the display
58
of bidirectional text.
Figure 2: SimpleNET Annotation Tool
Figure 2 shows a screen shot of SimpleNET.
4.3 Named Entity Taggers and POS
Taggers
We created common development frameworks for
creating named entity taggers and part-of-speech
taggers for the LCTL languages. These frame-
works allowed us to create taggers for any new
language given enough properly-formatted training
data and test data. Included are core code written
in Java as well as data processing utilities written
in Perl and Python. The framework for creating
POS taggers was centered around the MALLET
toolkit (McCallum, 2002).3
4.4 Data Package Creation and
Distribution
As per LDC?s usual mechanisms for small corpora,
language packs were to be packaged as a tar gzip
(.tgz) file, and distributed to the REFLEX-LCTL
participating research sites. The distribution of the
completed languages packs were handled by our se-
cure web downloading system. Access instructions
were sent to the participating research sites, and
all downloads were logged for future reference.
5 Steps for Creating Each
Language Pack
5.1 Identifying Local Language Experts
and Native Speakers
An intermediate step between planning and cre-
ation was to identify and contact any available lo-
cal experts in the targeted languages, and recruit
additional native speakers to serve as annotators
and language informants. Annotators were not
necessarily linguists or other language experts, but
3We thank Partha Pratim Talukdar for providing
frameworks for creating taggers.
they were native speakers with reading and writing
proficiencies who received training as needed from
in-house language experts for creating our anno-
tated corpora, and helped to identify and evaluate
harvestable online resources.
Intensive recruiting efforts were conducted for
native speakers of each language. Our recruiting
strategy utilized such resources as online discussion
boards and student associations for those language
communities, and we were also able to capitalize
on the diversity of the student/staff body at the
University of Pennsylvania by recruiting through
posted signs on campus.
5.2 Identifying Available Language
Resources
The first step in creating each language pack was
to identify resources that are already available. To
this end we implemented a series of ?Harvest Fes-
tivals?; intensive sessions where our entire team,
along with native speaker informants, convened
to search the web for available resources. Avail-
able resources were immediately documented on
a shared and group editable internal wiki page.
By bringing together native speakers, linguists,
programmers, information managers and projects
managers in the same room, we were able to min-
imize communications latency, brainstorm as a
group, and quickly build upon each other?s efforts.
This approach was generally quite successful, es-
pecially for the text corpora and lexicons, and
brought us some of our most useful data.
5.3 Investigating Intellectual Property
Rights
As soon as Raw digital resources were identi-
fied, our local intellectual property rights special-
ist began investigation into their usability for the
REFLEX-LCTL language packs. It was necessary
to contact many individual data providers to ob-
tain an agreement, so the process was quite lengthy
and in some cases permission was not granted un-
til shortly before the package was scheduled for
release to the REFLEX community. Our general
practice was to process all likely candidate data
pools and remove data as necessary in later stages,
thus ensuring that IPR was not a bottleneck in
language pack creation. The exception to this was
for large data sources, where removal would have
significantly affected the quantity of data in the
deliverable.
5.4 Creating Basic Text Processing Tools
The next step was to create the language-specific
basic data processing tools, such as encoding con-
verter, sentence segmenter and tokenizer.
The goal for this project was to include whatever
encoding converters were needed to convert all of
59
the raw text and lexical resources collected or cre-
ated into the standard encoding selected for that
target language.
Dividing text into individual sentences is a nec-
essary first step for many processes including the
human translation that dominated much of our ef-
fort. Simple in principle, LCTL sentence segmen-
tation can prove tantalizingly complex. Our goal
was to produce a sentence segmenter that accepts
text in our standard encoding as input and outputs
segmented sentences in the same encoding.
Word segmentation, or tokenization, is also chal-
lenging for languages such as Thai. For Thai,
our approach was to utilize an existing opensource
word segmentation tool, and enhancing it by using
a larger word list than the provided one.
We designed the basic format conversion tools,
such as the text-to-LTF converter, to be able to
just plug in language-specific tokenizers and seg-
menters.
5.5 Creating Monolingual Text
The monolingual text corpora in the languages
packs were primarily created by identifying and
harvesting available resources from the Internet,
such as news, newsgroups and weblogs in the tar-
get language. Once the IPR expert determined
that we can use the resources for the REFLEX-
LCTL program, we harvested the document files ?
recent documents as archived documents. The har-
vested files were then analyzed and the files that
actually have usable contents, such as news arti-
cles and weblog postings were kept and converted
into the LCTL Text format. The formatting pro-
cess was typically done in the following steps: 1)
convert the harvested document or article in html
or other web format to a plain text format, strip-
ping html tags, advertisements, links and other
non-contents; 2) convert the plain text files into
UTF-8, 2) verify the contents with native speak-
ers, and if necessary, further remove non-contents,
or divide a file into multiple files; 3) convert the
plain text files into the LCTL Text format, apply-
ing sentence segmentation and tokenization. Each
document file is assigned a unique document ID.
Essential information about the document such as
the publication date was kept in the final files.
5.6 Creating Parallel Text
Each language pack contains at least 250,000 words
of parallel text. Part of this data was found re-
sources harvested from online resources, such as
bilingual news web site. The found parallel doc-
uments were converted into the LTF format, and
aligned at the sentence level, producing segment-
mapping tables between the LTF files in the LCTL
language and the LTF files in English.
The rest of this data was created by manually
translating documents in the LCTL language into
English, or documents in English into the LCTL
language. A subset of the monolingual text corpus
was selected for translation into English.
In addition, about 65,000 words of English
source text were selected as the ?Common English
Subset? for translation into each LCTL language.
Having the same set of parallel documents for all
languages will facilitate comparison between any
or all of the diverse LCTL languages. The Com-
mon Subset included : newswire text, weblogs, a
phrasebook and an elicitation corpus. The phrase-
book contained common phrases used in daily life,
such as ?I?m here?, and ?I have to go?. The
elicitation corpus, provided by Carnegie Mellon
University (Alvarez et al, 2006), included expres-
sions, such as ?male name 1 will write a book
for female name 1, where male name 1 and fe-
male name 1 are common names in the LCTL lan-
guage. The set of elicitation expressions is designed
to elicit lexical distinctions which differ across lan-
guages.
The manual translation tasks were outsourced
to translation agencies or independent translators.
Since the translators were instructed to translate
text which had already been sentence-segmented,
the creation of sentence-level mappings was trivial.
However, we found that it was important to create
a sentence-numbered template for the translators
to use, otherwise we were likely to receive trans-
lations where the source text sentence boundaries
were not respected.
5.7 Creating Lexicons
Bilingual lexicons are also an important resource
that can support a variety of human language tech-
nologies, such as machine translation and informa-
tion extraction. The goal for this resource was a
lexicon of at least 10,000 lemmas with glosses and
parts of speech for each language. For most of the
languages, we were able to identify existing lexi-
cons, either digital or printed, to consult with and
extract information for a subset of the lexical en-
tries; however, in all cases we needed to process
them substantially before they could be used effi-
ciently. We performed quality checking, normaliz-
ing, added parts of speech and glosses, added en-
tries and removed irrelevant entries.
5.8 Creating Annotated Corpora
A subset of the target language text in each lan-
guage pack received up to three types of annota-
tions: part-of-speech tags, morphological analysis,
and named entity tags. Named entity annotations
were created for all language packs.
Annotations were created by native speakers us-
ing the annotation tools discussed in section 4.
60
5.9 Creating Morphological Analyzers
To address the requirement to include some kind
of tool for morphological analysis in each language
pack, we created either a morphological analyzer
implementing hand-written rules or a stemmer us-
ing an unsupervised statistical approach, such as
the approach described in (Hammarstrom, 2006).
5.10 Creating Named Entity Taggers
We created a named entity tagger for each lan-
guage pack using our common development frame-
work for named entity taggers4.3. The tagger was
created using the named entity annotated text we
created for the language packs.
5.11 Creating Part-of-Speech Taggers
Similarly, we created a POS tagger for each lan-
guage pack using our common development frame-
work for POS taggers (See Section 4.3). We coor-
dinated the POS tag sets for the taggers and lexi-
cons.
5.12 Creating Name Transliterators
A transliterator that converts the language?s na-
tive script into Latin script is a desired resource.
For some languages, this is not a straightforward
task. For example, not all vowels are explicitly rep-
resented in Bengali script, and there can be mul-
tiple pronunciations possible for a given Bengali
character. Names, especially names foreign to the
target language exhibit a wide variety of spelling,
and in HLTs, make up a large percentage of the
out-of-vocabulary terms. We focused on creating
a transliterator to for romanization of names in the
LCTL languages. This resource was generally cre-
ated by the programming team with consultation
from native speakers.
5.13 Writing Grammatical Sketches
The grammatical sketches provide an outline of the
features of the written language, to provide the
language engineers with description of challenges
specific to the languages in creating language tech-
nologies. These sketches were written mainly by
senior linguists in our group, for readers who do
not necessarily have training in linguistics. The
format of these documents was either html or pdf.
6 Summary of Completed
Language Packs
Table 1 summarizes the contents of the 8 Asian
language packs .4 All of the language packs have
already been distributed to REFLEX-LCTL par-
ticipating research sites. The packs continue to
be used to develop and test language technologies.
For example, the Urdu pack was used to support a
4The numbers represent the number of tokens.
task in the 2006 NIST Open MT Evaluation cam-
paign (of Standards and Technology, 2009). Once
a language pack has been used for evaluation it will
be placed into queue for general release.
7 Conclusion
We have developed an efficient approach for creat-
ing basic text language resources for diverse lan-
guages. Our process integrated the efforts of soft-
ware programmers, native speakers, language spe-
cialists, and translation agencies to identify and
built on already available resources, and create new
resources as efficiently as possible.
Using our streamlined processes, we were able
to complete language packs for eight diverse Asian
languages. We hope that the completed resources
will provide valuable support for research and tech-
nology development for these languages.
We faced various challenges at the beginning of
the project which led us to revisions of our meth-
ods, and some of these challenges would surely be
encountered during a similar effort. We hope that
our approach as described here will be of service to
future endeavors in HLT development for under-
resourced languages.
References
Alison Alvarez, Lori S. Levin, Robert E. Fred-
erking, Simon Fung, and Donna Gates. 2006.
The MILE corpus for less commonly taught lan-
guages. In Proceedings of HLT-NAACL 2006.
ELDA. 2008. BLARK Resource/Modules
Matrix. From Evaluations and Language Re-
sources Distribution Agency (ELDA) web site
http://www.elda.org/blark/matrice res mod.php
, accessed on 2/23/2008.
Raymond G. Gordon, Jr., editor. 2005. Eth-
nologue: Languages of the World, Fifteenth
edition, Online version. SIL International.
http://www.ethnologue.com/.
Harald Hammarstrom, 2006. Poor Man?s Stem-
ming: Unsupervised Recognition of Same-Stem
Words. Springer Berlin / Heidelberg.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
National Institute of Standards and
Technology. 2009. NIST Open
Machine Translation Evaluation.
http://www.itl.nist.gov/iad/mig/tests/mt/,
accessed on June 7, 2009.
61
Urdu Thai Bengali Tamil Punjabi Tagalog Pashto Uzbek
Mono Text 14,804 39,700 2,640 1,112 13,739 774 5,958 790
Parallel Text (L ? E) 1,300 694 237 308 203 180 206
Parallel Text (Found) 947 1,496 243 230
Parallel Text (E ? L) 65 65 65 65 65 65 65 65
Lexicon 26 232 482 10 108 18 10 25
Encoding Converter Yes Yes Yes Yes Yes Yes Yes Yes
Sentence Segmenter Yes Yes Yes Yes Yes Yes Yes Yes
Word Segmenter Yes Yes Yes Yes Yes Yes Yes Yes
POS Tagger Yes Yes Yes Yes Yes Yes Yes Yes
POS Tagged Text 5 5 59
Morphological Analyzer Yes Yes Yes Yes Yes Yes Yes Yes
Morph-Tagged Text 11 144
NE Annotated Text 233 218 138 132 157 136 165 93
Named Entity Tagger Yes Yes Yes Yes Yes Yes Yes Yes
Name Transliterator Yes Yes Yes Yes Yes Yes Yes Yes
Descriptive Grammar Yes Yes Yes Yes Yes Yes Yes Yes
Table 1: Language Packs for Asian Languages (Data Volume in 1000 Words)
A DTD for LTF Files
<!ELEMENT LCTL_TEXT (DOC+) >
<!ATTLIST LCTL_TEXT lang CDATA #IMPLIED
source_file CDATA #IMPLIED
source_type CDATA #IMPLIED
author CDATA #IMPLIED
encoding CDATA #IMPLIED >
<!ELEMENT DOC (HEADLINE|DATELINE|AUTHORLINE|TEXT)+ >
<!ATTLIST DOC id ID #REQUIRED
lang CDATA #IMPLIED
>
<!ELEMENT HEADLINE (SEG+) >
<!ELEMENT DATELINE (#PCDATA) >
<!ELEMENT AUTHORLINE (#PCDATA) >
<!ELEMENT TEXT (P|SEG)+ >
<!ELEMENT P (SEG+) >
<!ELEMENT SEG (ORIGINAL_TEXT?, TOKEN*) >
<!ATTLIST SEG id ID #REQUIRED
start_token IDREF #IMPLIED
end_token IDREF #IMPLIED
start_char CDATA #IMPLIED
end_char CDATA #IMPLIED
>
<!ELEMENT ORIGINAL_TEXT (#PCDATA) >
<!ELEMENT TOKEN (#PCDATA) >
<!ATTLIST TOKEN id ID #REQUIRED
attach (LEFT|RIGHT|BOTH)
#IMPLIED
pos CDATA #IMPLIED
morph CDATA #IMPLIED
gloss CDATA #IMPLIED
start_char CDATA #IMPLIED
end_char CDATA #IMPLIED
>
62
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 93?100,
Dublin, Ireland, August 23rd 2014.
Intellectual Property Rights Management with Web Service Grids 
Christopher Cieri email: ccieri Denise DiPersio email: dipersio  Linguistic Data Consortium 3600 Market Street Philadelphia, PA. 10104 email: @ldc.upenn.edu 
Abstract This paper enumerates the ways in which configurations of web services may complicate is-sues of licensing language resources, whether data or tools. It details specific licensing chal-lenges within the context of the US Language Application (LAPPS) Grid, sketches a solution under development and highlights ways in which that approach may be extended for other web service configurations.   1 Introduction Growing interest in web service architectures raises questions about how such uses of language tech-nologies and other resources interact with licensing constraints, including those that were imagined at an earlier time when resources and tools were more likely controlled by individual user organizations. Research communities that depend upon language resources (LRs) have become accustomed to, if not delighted with, the need to agree to certain limitations on the use of such resources. However, histori-cally, negotiations concerning the use of LRs occur relatively infrequently. Even the largest data cen-ters produce only a few new resources each month, generally under one of a small number of familiar license types. Once the resource has been acquired, integrating it in a local workflow requires time, creating a natural brake on the need to acquire new resources.  Grid infrastructure, on the other hand, promises the ability to very rapidly build pipelines from existing services and resources. The common vision of web service architectures is that they reduce the burden of tool integration by presenting the tools as services and coordinating their input and output requirements. However, absent a similar mechanism for coordinating the licenses that constrain LR use, Grid operators risk creating infrastruc-ture that simultaneously ameliorates the tool integration problem while exacerbating the licensing problem. In the sections that follow, we describe an approach that addresses the general problem of documenting, communicating and partially enforcing licensing constraints within a service grid.  2 Web Section Complexities Human Language Technology related web services, singly and in various configurations and clusters, constitute a new ecosystem in which LRs, specifically data sets and tools, may be implemented and combined in ways not necessarily contemplated by existing intellectual property law and contracts that apply to the web services? constituent components. For example, traditional licenses may permit dis-tribution from a data center to a licensed user organization and processing by either, but may prohibit distribution from the user to any additional parties. Even if it were clear that this constraint were in-tended only to block redistribution to unlicensed users, it is not clear whether all copyright holders would agree that moving the same data over the web to be processed by web services should be al-lowed. Another example involves the attribution and license requirements of shared software. In the past, licenses were typically described in a document included with the software source code. Attribu-tion requirements were satisfied by listing software authors? names in similar documents or by dis-playing them in a header presented when the software was invoked at the commend line. However, users of web services may never see a source code repository or a command line. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0  93
Service grids, as just one of multiple possible configurations of web services, have different stake-holder types: grid operators who maintain the software and servers that allow service registration and discovery; two types of service providers, those who provide access to data and those who provide access to software; and of course users. In addition to multiple stakeholder types, such Grids also have multiple instances of users, data providers and software providers. Where grids have been federated, there are also multiple grid operators. Each of these stakeholders may have different desires relative to the behavior of web services, most importantly where intellectual property protection is concerned. Beyond the obvious conflict that users typically want fewer restrictions on resources than providers, grid operators or service providers may require compensation, grid operators may wish to track and record user behavior, service providers may demand attribution, limit use of their services to the non-commercial sectors and may wish to exploit data that passes through their service nodes for purposes of further system development or evaluation. In addition to multiple stakeholder types and multiple instances of each, grids also combine these data and software services in various combinations that affect licensing in a variety of ways. Figure 1 summarizes three simple cases. In the first, users direct data they own or control through an external service controlled by a second party. In the second case, both the data and the processing are con-trolled by a single entity who is not the user. In the third case, one external party controls the data while another controls the software. In each of these cases, the interaction of multiple parties may complicate licensing by introducing new and idiosyncratic constraints.   
 Figure 1: Simple Configurations of Web Services Figure 2 sketches more complex cases in which data, controlled by the user or not, passes through multiple services controlled by independent parties. Examples of the first two use cases might include speech translation, configured as speech transcription followed by translation of the transcript text, in which the input speech is controlled by the users (e.g. in voicemail transcription) or is controlled by an independent party (e.g. translation of broadcast news). In the third case, not only are there multiple services operating on the same data, but these services are configured as generic engines that require models provided by other parties to operate on specific languages. One example might be language identification engines that accept new models in order to recognize new languages.   
 Figure 2: More Complex Web Service Configurations Web service grids further complicate licensing because no provider or user controls the entire eco-system. Grid operators, software service providers, data providers and users may all be distinct from 94
one another. Indeed in the future imagined by grid proponents, there are many software and data pro-viders and even more users on any one grid, and many grids federated to permit users of one to access services on another. In such an environment, it is clear that each user, provider and grid operator may act independently and sometimes at cross-purposes to others.  3 Approaches to Grid Licensing One can imagine multiple approaches to harmonizing this ecosystem. First, one might choose to try to constrain service and data providers insisting that as a condition of participation in the grid, they must agree to make their services available to specific classes of users under pre-defined terms. The NICT Language Grid did this by establishing a Service Grid Agreement. However, it is equally possi-ble to construct a grid in which service providers are not the owners or developers of all the software they use to provide services. For example, within the US NSF-funded Language Application Grid, two of the principle service providers, Brandeis University and Vassar College, have created services based on third party software such as NLTK (Bird, Klein, Loper 2009) and the Stanford Toolkit (Manning et al., 2014). In this case, the service providers do not own the tools and thus cannot enter into agreements about the terms under which the software underlying their service may be used. As a solution, one might imagine providing software services under whatever licenses the underlying soft-ware has imposed and then constrain the users to comply with these terms. A third alternative would be to assume that all parties take responsibility for their own actions during grid operations and to im-pose no controls over either providers or users. The fourth option is of course to constrain both provid-ers and users. In this paper we will argue for this last hybrid. Descriptions of the licensing approaches used by existing grids are scant, only rarely presented in published works, occasionally described on project web pages and sometimes left to be understood from the licenses used. Piperidis (2012) describes META-SHARE as a membership based infrastruc-ture in which resources are available under one of four license types. While META-SHARE encour-ages within-network sharing with the fewest constraints possible, the four license types permit a range of constraints including those expressed by the Creative Commons1 licenses and also allow for fees. META-SHARE servers harvest licensing elements from contributed services and present them to users as a table, in fact the model for our Table 2 below. The Language Grid2 developed by NICT, includes license text, where available, in the description of each resource it provides. The Language Grid also provides a tool for composing workflows. Upon execution of a given workflow, the Grid displays the sequence of licenses that affect the use of the workflow component tools and data. Bosca et al. (2012) describe LinguaGrid3 as ?open to different operators (Universities, Research institutes, Companies) with configurable services access policies: free, restricted to registered users, research or commercial licensing?. LinguaGrid is built upon the grid infrastructure developed by NICT and presumably uses the same approach to license management. CLARIN4 documentation describes a rich set of licensing options for service providers. Many have equivalents in the Creative Common licenses though CLARIN enriches this set by allowing providers to require that published papers based on CLARIN resources are reported to the providers and that derived resources are deposited to CLARIN, a specific variant of the share-alike constraint. CLARIN also provides a legal help desk to answer questions about licensing among other issues. The LAPPS Grid has developed an explicit model for license management that is membership based, allows for a wide range of license types and fees, presents a summary of license constraints and actual licenses prior to workflow execution and even prevents the subset of license violations that can be detected at execution time. Grid architecture constitutes a new approach to combining LRs. Providing clear documentation of the terms under which providers and users operate offers peace-of-mind to resource providers and clarity to service users either of which group may otherwise opt out of an initiative whose risks are incompletely understood. Furthermore, it is probable that for service providers who do not own the underlying data or software, imposing constraints on users may be not only a wise idea, but also a le-gal or contractual obligation. It is important to note that many license terms constrain behavior that                                                 1 http://creativecommons.org/ 2 http://langrid.org/en/index.html 3 http://www.linguagrid.org/ 4 http://www.clarin.eu/ 95
may occur long after web services have run, for example commercial use of output. Therefore, grid operators are not in a position to strictly enforce license terms. They may however, block obvious and immediate violations of licenses, make users aware of constraints that affect behavior and secure their agreement to relevant terms. 4 Dimensions of Constraints on Language Resource Use The licenses that constrain the behavior of language resource users, and thus grid users, vary along a number of dimensions, the first of which pertains to the object being licensed. Software licenses typ-ically constrain the use of software and derivative works. Data licenses similarly constrain the use of the data and derivative works. However, derivative work, to the extent that the term is defined at all, seems to refer to other data in the case of data licenses, and to other software in the case of software licenses. Importantly, none of the software licenses reviewed for this paper made a clear attempt to constrain the use of their output, which is often data, while many data licenses do constrain the use of processed data. The LRs used in web services may be owned by the user, may be in the public domain or may be copyrighted by someone other than the user. Copyrighted LRs may be constrained as to use or as to user. The commonest use constraints typically prevent commercial use and the creation or commercial use of derivative works. They may prevent distribution of the LR or derivative works or they may re-quire that products whose creation relied upon the licensed resource be shared under the same terms (also known as the Share Alike or Viral Copyleft constraint). They may require that users provide at-tribution of LR creators and/or cite the resource or a specific reference paper. Finally, any license may include other terms that have not been described here because they constitute the long tail of uncom-mon constraints. To give just one example, we are aware of at least one corpus that requires that recip-ients receive certification from their local Institutional Review Board that they have been trained in the treatment of human subjects. An additional complexity in licensing constraints defined by use is that neither copyright law nor the software or data licenses reviewed for this paper provide a bright line to distinguish derivative works (which are typically constrained by such licenses) from transformative uses (which are typically not). Within HLT, we can imagine simple and stereotyped cases. Given one hour of audio recorded from a copyrighted news broadcast, the transcript of the audio and its translation into any language are derivative works subject, at least in the US, to copyright and any licensing constraints imposed on the audio. On the other hand, a unigram frequency list based on the transcript or translation is a highly transformed work generally considered immune to those same limitations. License constraints related to the user, rather than the use, typically prevent commercial organiza-tions from accessing the resource. In at least some cases, the intent of this constraint is to encourage potential commercial users to negotiate directly with the LR provider for access under terms that in-clude a fee structure. More generally, the user types distinguished by LR licenses include academic and not-for-profit organizations, governments and commercial organizations. In some instances, com-panies engaged in pre-commercial technology development may be treated differently. In addition, a model of licensing constraints must distinguish organizations that have executed a specific license re-quired by a LR from those that have not. Organizations may be licensed by enumeration or by fea-tures. As an example of licensing by enumeration, the Linguistic Data Consortium maintains databases of all users, all licenses required by their LRs and a table of which user organizations have executed each license. However, users may also be considered licensed if they possess certain features, for ex-ample, if they are non-profit organizations.   One use that seems to have been overlooked by existing grid licenses is the ability of service pro-viders to derive benefit from the processing they offer. For example, one could imagine a translation service that not only outputs translations for submitted input text but also computes n-grams from that text and uses them to improve its source language models. Were this practice allowed, it would further complicate licensing within web service architectures where it is not always clear that the user who submits data for processing has the authority to permit the service providers to exploit that data. 
96
5 Combining Licensing Constraints Having enumerated the dimensions along which LR use may be constrained, we can easily imagine some use cases in which specific workflows should be prohibited or at least flagged. The obvious case would be one in which some input data required a specific license that the potential user had not exe-cuted. Another example would be the case in which some processing service required a fee that the potential user had not yet paid. Similarly a commercial organization seeking to process data that is only available under a no-commercial-use license should be prevented or at least warned by the ser-vice grid. At least within the United States ? and this probably holds for many other jurisdictions ? the law that governs copyright and the individual licenses commonly associated with LRs are relatively underspecified on a number of questions relevant to web services. For example, within US copyright law the only functional definition of ?fair use? is a description of the four dimensions along which fair use claims are to be evaluated. Given this situation, we should not be surprised that current bodies of law offer no calculus for combining constraints imposed on the multiple LRs that may support any given workflow.  For example if a specific pipeline makes use of two data resources, one of which permits commer-cial use while the other prohibits it, what constraints apply to the final output? To make this concrete, consider a pipeline in which a language identification service detects the language of an input text and routes it to an appropriate machine translation service. If the language identification service relies on a data set available under a no-commercial-use license but neither the input text nor the translation en-gine are similarly constrained, may the user sell access to the translation? Our tendency may be to think this use is acceptable. Would we feel the same if the translation engine relied on data that im-posed the no-commercial-use constraint? What if the input text was available under a no-commercial-use license but no other component in the pipeline constrained use? While we may have intuitions about acceptable use in these cases, there is no body of law, nor much precedent, to support one or another interpretation.  6 The Language Application Grid In order to work through a possible solution, we now consider the specific tool and data resources implemented in the US NSF funded Language Applications Grid. To date, the LAPPS Grid has used 27 unique software packages (programs, toolkits, APIs, libraries) that are available under the 9 unique licenses summarized in Table 1.  Table 1: LAPPS Grid Software by License License	 ? Software	 ?Apache	 ?2.0	 ? Language	 ?Grid	 ?software,	 ?NLTK,	 ?ANC2G0,	 ?UIMA,	 ?OAQA,	 ?Uimafit,	 ?guava-??libraries,	 ?Ac-??tiveMQ,	 ?AnyObject,	 ?Jaxws-??maven-??plug-??in,	 ?Jetty,	 ?OpenNLP	 ?	 ?BSD	 ? Hamcrest,	 ?NERsuite,	 ?CRFsuite	 ?(in	 ?NERsuite)	 ?CDDL	 ?1.1	 ? Jaxws-??rt	 ?CPL	 ?1.0	 ? MALLET,	 ?AGTK,	 ?JUnit	 ?Eclipse	 ?1.0	 ? logback	 ?(v1.0),	 ?Jetty	 ?HTK-??Cambridge	 ? HTK	 ?MIT	 ? Mockito,	 ?libLBFGS	 ?(in	 ?NERsuite),	 ?GIZA	 ?(v3)	 ?Python	 ? NLTK	 ?WordNet	 ? Genia	 ?tagger	 ?library	 ?(in	 ?NERsuite)	 ? Many of the constraints imposed by these licenses fall into recognizable categories summarized in Ta-ble 2   
97
Table 2: LAPPS Grid Licenses and Common Constraints 
License	 ? Redistribution	 ? Use	 ? Derivative	 ?Use	 ? Attribution	 ? Share	 ?Alike	 ? Fee	 ?Apache	 ?2.0	 ? Yes	 ? Commercial	 ? Commercial	 ? Yes	 ? No	 ? No	 ?BSD	 ? Yes	 ? Commercial	 ? Commercial	 ? No	 ? No	 ? No	 ?CDDL	 ?	 ?1.1	 ?	 ? Yes	 ? Commercial	 ? Commercial	 ? Yes	 ? Yes	 ? No	 ?CPL	 ?	 ?1.0	 ? Yes	 ? Commercial	 ? Commercial	 ? No	 ? No	 ? No	 ?Eclipse	 ?1.0	 ? Yes	 ? Commercial	 ? Commercial	 ? Yes	 ? Yes	 ? No	 ?HTK-??Cambridge	 ? No	 ? Commercial	 ? Commercial	 ? No	 ? No	 ? No	 ?MIT	 ? Yes	 ? Commercial	 ? Commercial	 ? No	 ? No	 ? Yes	 ?Python	 ? Yes	 ? Commercial	 ? Commercial	 ? Yes	 ? No	 ? No	 ?WordNet	 ? Yes	 ? Commercial	 ? Commercial	 ? Yes	 ? No	 ? No	 ?LDC	 ?FP	 ?Member	 ? No	 ? Commercial	 ? Commercial	 ? No	 ? No	 ? No	 ?LDC	 ?NFP	 ?Member	 ? No	 ? Research	 ? Research	 ? No	 ? No	 ? No	 ?LDC	 ?Non-??member	 ? No	 ? Research	 ? Research	 ? No	 ? No	 ? Yes	 ?CC-??Zero	 ? Yes	 ? Commercial	 ? Commercial	 ? No	 ? No	 ? No	 ?CC-??BY	 ? Yes	 ? Commercial	 ? Commercial	 ? Yes	 ? No	 ? No	 ?CC-??BY-??SA	 ? Yes	 ? Commercial	 ? Commercial	 ? Yes	 ? Yes	 ? No	 ?CC-??BY-??ND	 ? Yes	 ? Commercial	 ? None	 ? Yes	 ? No	 ? No	 ?CC-??BY-??NC	 ? Yes	 ? Research	 ? Research	 ? Yes	 ? No	 ? No	 ?CC-??BY-??NC-??SA	 ? Yes	 ? Research	 ? None	 ? Yes	 ? Yes	 ? No	 ?CC-??BY-??NC-??ND	 ? Yes	 ? Research	 ? None	 ? Yes	 ? No	 ? No	 ?GPL	 ?(v2,3)	 ? Yes	 ? Commercial	 ? Commercial	 ? Yes	 ? Yes	 ? No	 ? These many license have in common a relatively small number of constraint types and values as sum-marized in Table 3.  Table 3: LAPPS Grid Common Constraints and Values Constraint	 ? Values	 ?Redistribution	 ? Yes/No	 ?Use	 ? Commercial/Research	 ?Only	 ?Derivative	 ?Use	 ? Commercial/Research	 ?Only/None	 ?Transformative	 ?Use	 ? Commercial/Research	 ?Only	 ?/None	 ?Attribution	 ? Yes/No	 ?Share	 ?Alike	 ? Yes/No	 ?Fee	 ? Yes/No	 ?Other	 ?Specific	 ?License,	 ?Constraint	 ? -??-??	 ? However, as one considers the complexity of licensing with the grid, it is important to also consider the limitations on the role of grid operators relative to prior practice. Traditional language resource distribution, before the era of web services, treated licensing constraints variably. For example, where users are required to pay a fee in order to access a LR, that fee is normally required in advance. If a resource requires a specific license to be executed, some data providers may withhold the resource until the agreement is signed either on paper or via a click-through agreement. Others may provide the license with the LR and a statement that by accessing the data, the user is agreeing to the terms of the license. However, beyond these cases, there is little attempt to block access to a resource until licens-ing terms have been satisfied. Indeed many licensing terms constrain future action and thus cannot be required as a condition of access. For example, the constraints on redistribution, use of derivative works, attribution and share alike all affect action that necessarily takes place after the LR has been accessed. Given these limitations, we expect that the ability of any web service policy or procedure to enforce such constraints is similarly limited. Thus, within the LAPPS Grid, we distinguish two types of enforcement of licensing constraints, requirement and notification as summarized in Table 4. In a 98
small number of cases, we block the execution of a service pipeline if required conditions are not met but otherwise accumulate notifications that we present to users before allowing them to execute the pipeline. We must also note here that no summary of licensing terms can legally stand-in for the actual license executed so that any approach we use must also make reference to the actual licenses.  Table 4: Constraint Enforcement Constraint	 ? Action	 ?Redistribution	 ? Notify	 ?Use	 ? Notify	 ?Derivatives	 ?Use	 ? Notify	 ?Attribution	 ? Notify	 ?Share	 ?Alike	 ? Notify	 ?Fee	 ? Require	 ?Other	 ?Specific	 ?License	 ? Require	 ?Other	 ?Specific	 ?Constraint	 ? ?	 ?7 A Grid Licensing Model Putting together the discussion to date, we propose the model in Figure 3 for managing licenses within a service grid framework. Specifically, this model benefits from features already implemented for the LAPPS Grid while imposing some limitations of its own. First, within the LAPPS Grid, users build pipelines using one of two workflow management tools developed by the project. The Composer, de-scribed in greater detail in Ide et al. 2014, displays for the user the set of available tool and data ser-vices allowing the user to select one or more, determine their order of application and even create branches to allow two or more tools of the same types to operate on the data in parallel so that their performance may be evaluated and compared. The Composer takes note of the input and output re-quirements of each tool in the chain and, in complex workflows, correctly routes data to appropriate processing services. The workflow Planner, still under development, allows the user to specify input data and desired output and then uses its knowledge of each tool?s inputs and outputs to construct a pipeline that produces the desired result. The licensing model makes use of these workflow managers. First we require that any service registered in the grid only respond to requests from one of the work-flow managers. This keeps the grid ecosystem closed and prevents a user from directing output of one of the services outside the grid where one cannot monitor use. We also require that service providers register the licenses that govern use of their services. The user initiates a session with the workflow managers by authenticating themselves. As the user builds a workflow, the manager requests from each service the list of constraints imposed. As in Table 2 and Table 4, these may be requirements for a fee or the execution of a specific license or they may be notifications of the future behavior expected of users. The workflow manager also queries a local database or API connected to a service provider or data center to determine whether the user has satisfied the payment and specific license require-ments. If not, the pipeline is blocked. Otherwise the user continues to build the pipeline while the manager accumulates a summary of the click-through licenses required and general licensing con-straints imposed. Before the user can execute the pipeline, the workflow manager presents a summary of the licenses required, with links to the original text, as well as a summary of the general constraints imposed. The user must click to agree to the terms before processing will begin. For each service that provides processing, the workflow manager also displays any attribution requirements or license statements normally displayed at the command line or in a README file since these are generally invisible to a grid user. Of course, this model only works if the grid or other collection of web services constitutes a closed system where a small number of management programs can control the inputs and outputs to each process. Naturally, the grid licensing model is unable to resolve issues that remain unresolved in gen-eral such as the lack of a bright line distinguishing derivative and transformative use of linguistic data and tools. In such cases it takes a legally conservative approach, acting for example as though as uses may be considered derivative and issuing appropriate warnings. 
99
 Figure 3: Service Grid Licensing Model 8 Conclusion We discussed the features of web service architectures that complicate the licensing of LRs, including data and tools, both by introducing ecosystems not contemplated during the drafting of relevant intel-lectual property law and the development of the LR and by creating complex workflows not entirely under the control of any single user. We sketched the dimension along which licenses constrain LR use. Making the discussion more concrete, we then enumerated the license and constraint types that affect the resources used to build the US LAPPS Grid. Finally, we sketched a model for protecting intellectual property via the use of workflow managers while allowing users with appropriate creden-tials to construct complex pipelines. This approach relies on the closed nature of the service grid and would need to be extended in cases where the pipeline could combine web services without bounds. 9 Acknowledgments This work was supported by National Science Foundation grants NSF-ACI 1147944 and NSF-ACI 1147912. References 
Bird, S., Klein, E., Loper, E. (2009) Natural Language Processing with Python. O'Reilly Media. Bosca, A., Dini, L., Kouylekov, M., Trevisan, M. (2012) Linguagrid: A network of Linguistic and Semantic Services for the Italian Language. In Proceedings of the Eighth International Language Resources and Evaluation (LREC12), Istanbul, Turkey. European Language Resources Association (ELRA). Ide, N., Pustejovsky, J., Cieri, C., Nyberg, E., DiPersio, D., Shi, C., Suderman, K., Verhagen, M., Wang, D., Wright, J. (2014) The Language Application Grid. In Proceedings of the Ninth International Language Re-sources and Evaluation (LREC14), Reykjavik, Iceland. European Language Resources Association (ELRA). Ide, N. and Suderman, K. (2014). The Linguistic Annotation Framework: A Standard for Annotation Interchange and Merging. Language Resources and Evaluation. Manning, C., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S., McClosky, D. (2014) The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60.  Piperdis, S. (2012). The META-SHARE Language Resources Sharing Infrastructure: Principles, Challenges, Solutions. In Proceedings of the Eighth International Language Resources and Evaluation (LREC12), Istan-bul, Turkey. European Language Resources Association (ELRA). 
100
