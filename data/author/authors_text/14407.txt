A Maximum Entropy-based Word Sense Disambiguation system

Armando Suarez Manuel Palomar
Departamento de Lenguajes y Sistemas Informaticos
Universidad de Alicante
Apartado de correos, 99
E-03080 Alicante, Spain
farmando, mpalomarg@dlsi.ua.es
http://www.dlsi.ua.es/armando/publicaciones.html
Abstract
In this paper, a supervised learning system of
word sense disambiguation is presented. It is
based on conditional maximum entropy models.
This system acquires the linguistic knowledge
from an annotated corpus and this knowledge
is represented in the form of features. Several
types of features have been analyzed using the
SENSEVAL-2 data for the Spanish lexical sam-
ple task. Such analysis shows that instead of
training with the same kind of information for
all words, each one is more eectively learned
using a dierent set of features. This best-
feature-selection is used to build some systems
based on dierent maximum entropy classiers,
and a voting system helped by a knowledge-
based method.
1 Introduction
Word sense disambiguation (WSD) is an open
research eld in natural language processing
(NLP). The task of WSD consists in assign-
ing the correct sense to words using an elec-
tronic dictionary as the source of word deni-
tions. This is a hard problem that is receiving
a great deal of attention from the research com-
munity.
Currently, there are two main methodologi-
cal approaches in this research area: knowledge-
based methods and corpus-based methods. The
former approach relies on previously acquired
linguistic knowledge, and the latter uses tech-
niques from statistics and machine learning to
induce models of language usage from large
samples of text (Pedersen, 2001). Learning can
be supervised or unsupervised. With supervised

This paper has been partially supported by the
Spanish Government (CICYT) under project number
TIC2000-0664-C02-02.
learning, the actual status (here, sense label)
for each piece of data in the training example is
known, whereas with unsupervised learning the
classication of the data in the training example
is not known (Manning and Schutze, 1999).
At SENSEVAL-2, researchers showed the lat-
est contributions to WSD. Some supervised sys-
tems competed in the Spanish lexical sample
task. The Johns Hopkins University system
(Yarowsky et al, 2001) combined, by means of
a voting-based classier, several WSD subsys-
tems based on dierent methods: decision lists
(Yarowsky, 2000), cosine-based vector models,
and Bayesian classiers. The University of
Maryland system (UMD-SST) (Cabezas et al,
2001) used support vector machines.
Pedersen (2002) proposes a baseline method-
ology for WSD based on decision tree learning
and naive Bayesian classiers, using simple lex-
ical features. Several systems that combine dif-
ferent classiers using distinct sets of features
competed at SENSEVAL-2, both in the English
and Spanish lexical sample tasks.
This paper presents a system that implements
a corpus-based method of WSD. The method
used to perform the learning over a set of sense-
disambiguated examples is that of maximum en-
tropy (ME) probability models. Linguistic in-
formation is represented in the form of feature
vectors, which identify the occurrence of certain
attributes that appear in contexts containing
linguistic ambiguities. The context is the text
surrounding an ambiguity that is relevant to the
disambiguation process. The features used may
be of a distinct nature: word collocations, part-
of-speech (POS) labels, keywords, topic and
domain information, grammatical relationships,
and so on. Instead of training with the same
kind of information for all words, which under-
estimates which information is more relevant to
each word, our research shows that each word is
more eectively learned using a dierent set of
features. Therefore, a more accurate feature se-
lection can be done testing several combinations
of features by means of a n-fold cross-validation
over the training data.
At SENSEVAL-2, Stanford University pre-
sented a metalearner (Ilhan et al, 2001) com-
bining simple classiers (naive-Bayes, vector
space, memory-based and others) that use vot-
ing and conditional ME models. Garca Varea
et al (2001) do machine translation tasks using
ME to perform some kind of semantic classi-
cation, but they also rely on another statistical
training procedure to dene word classes.
In the following discussion, the ME frame-
work will be described. Then, feature imple-
mentation and the complete set of feature de-
nitions used in this work will be detailed. Next,
evaluation results using several combinations of
these features will be shown. Finally, some con-
clusions will be presented, along with a brief
discussion of work in progress and future work
planned.
2 The Maximum Entropy
Framework
ME modeling provides a framework for integrat-
ing information for classication frommany het-
erogeneous information sources (Manning and
Schutze, 1999). ME probability models have
been successfully applied to some NLP tasks,
such as POS tagging or sentence boundary de-
tection (Ratnaparkhi, 1998).
The WSD method used in this work is based
on conditional ME models. It has been im-
plemented using a supervised learning method
that consists of building word-sense classiers
using a semantically tagged corpus. A classi-
er obtained by means of an ME technique con-
sists of a set of parameters or coe?cients which
are estimated using an optimization procedure.
Each coe?cient is associated with one feature
observed in the training data. The main pur-
pose is to obtain the probability distribution
that maximizes the entropy, that is, maximum
ignorance is assumed and nothing apart from
the training data is considered. Some advan-
tages of using the ME framework are that even
knowledge-poor features may be applied accu-
rately; the ME framework thus allows a virtu-
ally unrestricted ability to represent problem-
specic knowledge in the form of features (Rat-
naparkhi, 1998).
Let us assume a set of contexts X and a set
of classes C. The function cl : X ! C chooses
the class c with the highest conditional proba-
bility in the context x: cl(x) = argmax
c
p(cjx).
Each feature is calculated by a function that is
associated to a specic class c
0
, and it takes the
form of equation (1), where cp(x) is some ob-
servable characteristic in the context
1
. The con-
ditional probability p(cjx) is dened by equation
(2), where 
i
is the parameter or weight of the
feature i, K is the number of features dened,
and Z(x) is a value to ensure that the sum of
all conditional probabilities for this context is
equal to 1.
f(x; c) =
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 389?397,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
An Empirical Study on Class-based Word Sense Disambiguation?
Rube?n Izquierdo & Armando Sua?rez
Deparment of Software and Computing Systems
University of Alicante. Spain
{ruben,armando}@dlsi.ua.es
German Rigau
IXA NLP Group.
EHU. Donostia, Spain
german.rigau@ehu.es
Abstract
As empirically demonstrated by the last
SensEval exercises, assigning the appro-
priate meaning to words in context has re-
sisted all attempts to be successfully ad-
dressed. One possible reason could be the
use of inappropriate set of meanings. In
fact, WordNet has been used as a de-facto
standard repository of meanings. How-
ever, to our knowledge, the meanings rep-
resented by WordNet have been only used
for WSD at a very fine-grained sense level
or at a very coarse-grained class level. We
suspect that selecting the appropriate level
of abstraction could be on between both
levels. We use a very simple method for
deriving a small set of appropriate mean-
ings using basic structural properties of
WordNet. We also empirically demon-
strate that this automatically derived set of
meanings groups senses into an adequate
level of abstraction in order to perform
class-based Word Sense Disambiguation,
allowing accuracy figures over 80%.
1 Introduction
Word Sense Disambiguation (WSD) is an inter-
mediate Natural Language Processing (NLP) task
which consists in assigning the correct semantic
interpretation to ambiguous words in context. One
of the most successful approaches in the last years
is the supervised learning from examples, in which
statistical or Machine Learning classification mod-
els are induced from semantically annotated cor-
pora (Ma`rquez et al, 2006). Generally, super-
vised systems have obtained better results than
the unsupervised ones, as shown by experimental
work and international evaluation exercises such
?This paper has been supported by the European Union
under the projects QALL-ME (FP6 IST-033860) and KY-
OTO (FP7 ICT-211423), and the Spanish Government under
the project Text-Mess (TIN2006-15265-C06-01) and KNOW
(TIN2006-15049-C03-01)
as Senseval1. These annotated corpora are usu-
ally manually tagged by lexicographers with word
senses taken from a particular lexical semantic re-
source ?most commonly WordNet2 (WN) (Fell-
baum, 1998).
WN has been widely criticized for being a sense
repository that often provides too fine?grained
sense distinctions for higher level applications
like Machine Translation or Question & Answer-
ing. In fact, WSD at this level of granularity
has resisted all attempts of inferring robust broad-
coverage models. It seems that many word?sense
distinctions are too subtle to be captured by auto-
matic systems with the current small volumes of
word?sense annotated examples. Possibly, build-
ing class-based classifiers would allow to avoid
the data sparseness problem of the word-based ap-
proach. Recently, using WN as a sense reposi-
tory, the organizers of the English all-words task
at SensEval-3 reported an inter-annotation agree-
ment of 72.5% (Snyder and Palmer, 2004). In-
terestingly, this result is difficult to outperform by
state-of-the-art sense-based WSD systems.
Thus, some research has been focused on deriv-
ing different word-sense groupings to overcome
the fine?grained distinctions of WN (Hearst and
Schu?tze, 1993), (Peters et al, 1998), (Mihalcea
and Moldovan, 2001), (Agirre and LopezDeLa-
Calle, 2003), (Navigli, 2006) and (Snow et al,
2007). That is, they provide methods for grouping
senses of the same word, thus producing coarser
word sense groupings for better disambiguation.
Wikipedia3 has been also recently used to over-
come some problems of automatic learning meth-
ods: excessively fine?grained definition of mean-
ings, lack of annotated data and strong domain de-
pendence of existing annotated corpora. In this
way, Wikipedia provides a new very large source
of annotated data, constantly expanded (Mihalcea,
2007).
1http://www.senseval.org
2http://wordnet.princeton.edu
3http://www.wikipedia.org
389
In contrast, some research have been focused on
using predefined sets of sense-groupings for learn-
ing class-based classifiers for WSD (Segond et al,
1997), (Ciaramita and Johnson, 2003), (Villarejo
et al, 2005), (Curran, 2005) and (Ciaramita and
Altun, 2006). That is, grouping senses of different
words into the same explicit and comprehensive
semantic class.
Most of the later approaches used the origi-
nal Lexicographical Files of WN (more recently
called SuperSenses) as very coarse?grained sense
distinctions. However, not so much attention has
been paid on learning class-based classifiers from
other available sense?groupings such as WordNet
Domains (Magnini and Cavaglia`, 2000), SUMO
labels (Niles and Pease, 2001), EuroWordNet
Base Concepts (Vossen et al, 1998), Top Con-
cept Ontology labels (Alvez et al, 2008) or Ba-
sic Level Concepts (Izquierdo et al, 2007). Obvi-
ously, these resources relate senses at some level
of abstraction using different semantic criteria and
properties that could be of interest for WSD. Pos-
sibly, their combination could improve the overall
results since they offer different semantic perspec-
tives of the data. Furthermore, to our knowledge,
to date no comparative evaluation has been per-
formed on SensEval data exploring different levels
of abstraction. In fact, (Villarejo et al, 2005) stud-
ied the performance of class?based WSD com-
paring only SuperSenses and SUMO by 10?fold
cross?validation on SemCor, but they did not pro-
vide results for SensEval2 nor SensEval3.
This paper empirically explores on the super-
vised WSD task the performance of different
levels of abstraction provided by WordNet Do-
mains (Magnini and Cavaglia`, 2000), SUMO la-
bels (Niles and Pease, 2001) and Basic Level Con-
cepts (Izquierdo et al, 2007). We refer to this ap-
proach as class?based WSD since the classifiers
are created at a class level instead of at a sense
level. Class-based WSD clusters senses of differ-
ent words into the same explicit and comprehen-
sive grouping. Only those cases belonging to the
same semantic class are grouped to train the clas-
sifier. For example, the coarser word grouping ob-
tained in (Snow et al, 2007) only has one remain-
ing sense for ?church?. Using a set of Base Level
Concepts (Izquierdo et al, 2007), the three senses
of ?church? are still represented by faith.n#3,
building.n#1 and religious ceremony.n#1.
The contribution of this work is threefold. We
empirically demonstrate that a) Basic Level Con-
cepts group senses into an adequate level of ab-
straction in order to perform supervised class?
based WSD, b) that these semantic classes can
be successfully used as semantic features to boost
the performance of these classifiers and c) that
the class-based approach to WSD reduces dramat-
ically the required amount of training examples to
obtain competitive classifiers.
After this introduction, section 2 presents the
sense-groupings used in this study. In section 3 the
approach followed to build the class?based system
is explained. Experiments and results are shown in
section 4. Finally some conclusions are drawn in
section 5.
2 Semantic Classes
WordNet (Fellbaum, 1998) synsets are organized
in forty five Lexicographer Files, more recetly
called SuperSenses, based on open syntactic cat-
egories (nouns, verbs, adjectives and adverbs) and
logical groupings, such as person, phenomenon,
feeling, location, etc. There are 26 basic cate-
gories for nouns, 15 for verbs, 3 for adjectives and
1 for adverbs.
WordNet Domains4 (Magnini and Cavaglia`,
2000) is a hierarchy of 165 Domain Labels which
have been used to label all WN synsets. Informa-
tion brought by Domain Labels is complementary
to what is already in WN. First of all a Domain La-
bels may include synsets of different syntactic cat-
egories: for instance MEDICINE groups together
senses from nouns, such as doctor and hospital,
and from Verbs such as to operate. Second, a Do-
main Label may also contain senses from differ-
ent WordNet subhierarchies. For example, SPORT
contains senses such as athlete, deriving from life
form, game equipment, from physical object, sport
from act, and playing field, from location.
SUMO5 (Niles and Pease, 2001) was created as
part of the IEEE Standard Upper Ontology Work-
ing Group. The goal of this Working Group is
to develop a standard upper ontology to promote
data interoperability, information search and re-
trieval, automated inference, and natural language
processing. SUMO consists of a set of concepts,
relations, and axioms that formalize an upper on-
tology. For these experiments, we used the com-
plete WN1.6 mapping with 1,019 SUMO labels.
4http://wndomains.itc.it/
5http://www.ontologyportal.org/
390
Basic Level Concepts6 (BLC) (Izquierdo et al,
2007) are small sets of meanings representing the
whole nominal and verbal part of WN. BLC can
be obtained by a very simple method that uses ba-
sic structural WN properties. In fact, the algorithm
only considers the relative number of relations of
each synset alng the hypernymy chain. The pro-
cess follows a bottom-up approach using the chain
of hypernymy relations. For each synset in WN,
the process selects as its BLC the first local maxi-
mum according to the relative number of relations.
The local maximum is the synset in the hypernymy
chain having more relations than its immediate
hyponym and immediate hypernym. For synsets
having multiple hypernyms, the path having the
local maximum with higher number of relations
is selected. Usually, this process finishes having
a number of preliminary BLC. Obviously, while
ascending through this chain, more synsets are
subsumed by each concept. The process finishes
checking if the number of concepts subsumed by
the preliminary list of BLC is higher than a cer-
tain threshold. For those BLC not representing
enough concepts according to the threshold, the
process selects the next local maximum following
the hypernymy hierarchy. Thus, depending on the
type of relations considered to be counted and the
threshold established, different sets of BLC can be
easily obtained for each WN version.
In this paper, we empirically explore the perfor-
mance of the different levels of abstraction pro-
vided by Basic Level Concepts (BLC) (Izquierdo
et al, 2007).
Table 1 presents the total number of BLC and
its average depth for WN1.6, varying the threshold
and the type of relations considered (all relations
or only hyponymy).
Thres. Rel. PoS #BLC Av. depth.
0
all Noun 3,094 7.09Verb 1,256 3.32
hypo Noun 2,490 7.09Verb 1,041 3.31
20
all Noun 558 5.81Verb 673 1.25
hypo Noun 558 5.80Verb 672 1.21
50
all Noun 253 5.21Verb 633 1.13
hypo Noun 248 5.21Verb 633 1.10
Table 1: BLC for WN1.6 using all or hyponym relations
6http://adimen.si.ehu.es/web/BLC
Classifier Examples # of examples
church.n#2 (sense approach) church.n#2 58
church.n#2 58
building.n#1 48
hotel.n#1 39
building, edifice (class approach) hospital.n#1 20
barn.n#1 17
....... ......
TOTAL= 371 examples
Table 2: Examples and number of them in Semcor, for
sense approach and for class approach
3 Class-based WSD
We followed a supervised machine learning ap-
proach to develop a set of class-based WSD tag-
gers. Our systems use an implementation of a Sup-
port Vector Machine algorithm to train the clas-
sifiers (one per class) on semantic annotated cor-
pora for acquiring positive and negative examples
of each class and on the definition of a set of fea-
tures for representing these examples. The system
decides and selects among the possible semantic
classes defined for a word. In the sense approach,
one classifier is generated for each word sense, and
the classifiers choose between the possible senses
for the word. The examples to train a single clas-
sifier for a concrete word are all the examples of
this word sense. In the semantic?class approach,
one classifier is generated for each semantic class.
So, when we want to label a word, our program
obtains the set of possible semantic classes for
this word, and then launch each of the semantic
classifiers related with these semantic categories.
The most likely category is selected for the word.
In this approach, contrary to the word sense ap-
proach, to train a classifier we can use all examples
of all words belonging to the class represented by
the classifier. In table 2 an example for a sense
of ?church? is shown. We think that this approach
has several advantages. First, semantic classes re-
duce the average polysemy degree of words (some
word senses are grouped together within the same
class). Moreover, the well known problem of ac-
quisition bottleneck in supervised machine learn-
ing algorithms is attenuated, because the number
of examples for each classifier is increased.
3.1 The learning algorithm: SVM
Support Vector Machines (SVM) have been
proven to be robust and very competitive in many
NLP tasks, and in WSD in particular (Ma`rquez et
al., 2006). For these experiments, we used SVM-
Light (Joachims, 1998). SVM are used to learn
an hyperplane that separates the positive from the
391
negative examples with the maximum margin. It
means that the hyperplane is located in an interme-
diate position between positive and negative ex-
amples, trying to keep the maximum distance to
the closest positive example, and to the closest
negative example. In some cases, it is not possi-
ble to get a hyperplane that divides the space lin-
early, or it is better to allow some errors to obtain a
more efficient hyperplane. This is known as ?soft-
margin SVM?, and requires the estimation of a pa-
rameter (C), that represent the trade-off allowed
between training errors and the margin. We have
set this value to 0.01, which has been proved as a
good value for SVM in WSD tasks.
When classifying an example, we obtain the
value of the output function for each SVM clas-
sifier corresponding to each semantic class for the
word example. Our system simply selects the class
with the greater value.
3.2 Corpora
Three semantic annotated corpora have been used
for training and testing. SemCor has been used
for training while the corpora from the English
all-words tasks of SensEval-2 and SensEval-3
has been used for testing. We also consid-
ered SemEval-2007 coarse?grained task corpus
for testing, but this dataset was discarded because
this corpus is also annotated with clusters of word
senses.
SemCor (Miller et al, 1993) is a subset of the
Brown Corpus plus the novel The Red Badge of
Courage, and it has been developed by the same
group that created WordNet. It contains 253 texts
and around 700,000 running words, and more than
200,000 are also lemmatized and sense-tagged ac-
cording to Princeton WordNet 1.6.
SensEval-27 English all-words corpus (here-
inafter SE2) (Palmer et al, 2001) consists on 5,000
words of text from three WSJ articles represent-
ing different domains from the Penn TreeBank II.
The sense inventory used for tagging is WordNet
1.7. Finally, SensEval-38 English all-words cor-
pus (hereinafter SE3) (Snyder and Palmer, 2004),
is made up of 5,000 words, extracted from two
WSJ articles and one excerpt from the Brown Cor-
pus. Sense repository of WordNet 1.7.1 was used
to tag 2,041 words with their proper senses.
7http://www.sle.sharp.co.uk/senseval2
8http://www.senseval.org/senseval3
3.3 Feature types
We have defined a set of features to represent the
examples according to previous works in WSD
and the nature of class-based WSD. Features
widely used in the literature as in (Yarowsky,
1994) have been selected. These features are
pieces of information that occur in the context of
the target word, and can be organized as:
Local features: bigrams and trigrams that
contain the target word, including part-of-speech
(PoS), lemmas or word-forms.
Topical features: word?forms or lemmas ap-
pearing in windows around the target word.
In particular, our systems use the following ba-
sic features:
Word?forms and lemmas in a window of 10
words around the target word
PoS: the concatenation of the preced-
ing/following three/five PoS
Bigrams and trigrams formed by lemmas and
word-forms and obtained in a window of 5 words.
We use of all tokens regardless their PoS to build
bi/trigrams. The target word is replaced by X
in these features to increase the generalization of
them for the semantic classifiers
Moreover, we also defined a set of Semantic
Features to explode different semantic resources
in order to enrich the set of basic features:
Most frequent semantic class calculated over
SemCor, the most frequent semantic class for the
target word.
Monosemous semantic classes semantic
classes of the monosemous words arround the
target word in a window of size 5. Several types
of semantic classes have been considered to create
these features. In particular, two different sets
of BLC (BLC20 and BLC509), SuperSenses,
WordNet Domains (WND) and SUMO.
In order to increase the generalization capabil-
ities of the classifiers we filter out irrelevant fea-
tures. We measure the relevance of a feature10. f
for a class c in terms of the frequency of f. For each
class c, and for each feature f of that class, we cal-
culate the frequency of the feature within the class
(the number of times that it occurs in examples
9We have selected these set since they represent different
levels of abstraction. Remember that 20 and 50 refer to the
threshold of minimum number of synsets that a possible BLC
must subsume to be considered as a proper BLC. These BLC
sets were built using all kind of relations.
10That is, the value of the feature, for example a feature
type can be word-form, and a feature of that type can be
?houses?
392
of the class), and also obtain the total frequency
of the feature, for all the classes. We divide both
values (classFreq / totalFreq) and if the result is
not greater than a certain threshold t, the feature
is removed from the feature list of the class c11.
In this way, we ensure that the features selected
for a class are more frequently related with that
class than with others. We set this threshold t to
0.25, obtained empirically with very preliminary
versions of the classifiers on SensEval3 test.
4 Experiments and Results
To analyze the influence of each feature type in the
class-based WSD, we designed a large set of ex-
periments. An experiment is defined by two sets of
semantic classes. First, the semantic class type for
selecting the examples used to build the classifiers
(determining the abstraction level of the system).
In this case, we tested: sense12, BLC20, BLC50,
WordNet Domains (WND), SUMO and Super-
Sense (SS). Second, the semantic class type used
for building the semantic features. In this case, we
tested: BLC20, BLC50, SuperSense, WND and
SUMO. Combining them, we generated the set of
experiments described later.
Test pos Sense BLC20 BLC50 WND SUMO SS
SE2 N 4.02 3.45 3.34 2.66 3.33 2.73V 9.82 7.11 6.94 2.69 5.94 4.06
SE3 N 4.93 4.08 3.92 3.05 3.94 3.06V 10.95 8.64 8.46 2.49 7.60 4.08
Table 3: Average polysemy on SE2 and SE3
Table 3 presents the average polysemy on SE2
and SE3 of the different semantic classes.
4.1 Baselines
The most frequent classes (MFC) of each word
calculated over SemCor are considered to be the
baselines of our systems. Ties between classes on
a specific word are solved obtaining the global fre-
quency in SemCor of each of these tied classes,
and selecting the more frequent class over the
whole training corpus. When there are no occur-
rences of a word of the test corpus in SemCor (we
are not able to calculate the most frequent class of
the word), we obtain again the global frequency
for each of its possible semantic classes (obtained
11Depending on the experiment, around 30% of the origi-
nal features are removed by this filter.
12We included this evaluation for comparison purposes
since the current system have been designed for class-based
evaluation only.
from WN) over SemCor, and we select the most
frequent.
4.2 Results
Tables 4 and 5 present the F1 measures (harmonic
mean of recall and precision) for nouns and verbs
respectively when training our systems on Sem-
Cor and testing on SE2 and SE3. Those results
showing a statistically significant13 positive dif-
ference when compared with the baseline are in
marked bold. Column labeled as ?Class? refers to
the target set of semantic classes for the classifiers,
that is, the desired semantic level for each exam-
ple. Column labeled as ?Sem. Feat.? indicates
the class of the semantic features used to train the
classifiers. For example, class BLC20 combined
with Semantic Feature BLC20 means that this set
of classes were used both to label the test exam-
ples and to define the semantic features. In order
to compare their contribution we also performed
a ?basicFeat? test without including semantic fea-
tures.
As expected according to most literature in
WSD, the performances of the MFC baselines are
very high. In particular, those corresponding to
nouns (ranging from 70% to 80%). While nom-
inal baselines seem to perform similarly in both
SE2 and SE3, verbal baselines appear to be con-
sistently much lower for SE2 than for SE3. In
SE2, verbal baselines range from 44% to 68%
while in SE3 verbal baselines range from 52% to
79%. An exception is the results for verbs con-
sidering WND: the results are very high due to
the low polysemy for verbs according to WND.
As expected, when increasing the level of abstrac-
tion (from senses to SuperSenses) the results also
increase. Finally, it also seems that SE2 task is
more difficult than SE3 since the MFC baselines
are lower.
As expected, the results of the systems increase
while augmenting the level of abstraction (from
senses to SuperSenses), and almost in every case,
the baseline results are reached or outperformed.
This is very relevant since the baseline results are
very high.
Regarding nouns, a very different behaviour is
observed for SE2 and SE3. While for SE3 none
of the system presents a significant improvement
over the baselines, for SE2 a significant improve-
ment is obtained by using several types of seman-
13Using the McNemar?s test.
393
tic features. In particular, when using WordNet
Domains but also BLC20. In general, BLC20 se-
mantic features seem to be better than BLC50 and
SuperSenses.
Regarding verbs, the system obtains significant
improvements over the baselines using different
types of semantic features both in SE2 and SE3.
In particular, when using again WordNet Domains
as semantic features.
In general, the results obtained by BLC20 are
not so much different to the results of BLC50
(in a few cases, this difference is greater than
2 points). For instance, for nouns, if we con-
sider the number of classes within BLC20 (558
classes), BLC50 (253 classes) and SuperSense (24
classes), BLC classifiers obtain high performance
rates while maintaining much higher expressive
power than SuperSenses. In fact, using Super-
Senses (40 classes for nouns and verbs) we can
obtain a very accurate semantic tagger with per-
formances close to 80%. Even better, we can use
BLC20 for tagging nouns (558 semantic classes
and F1 over 75%) and SuperSenses for verbs (14
semantic classes and F1 around 75%).
Obviously, the classifiers using WordNet Do-
mains as target grouping obtain very high per-
formances due to its reduced average polysemy.
However, when used as semantic features it seems
to improve the results in most of the cases.
In addition, we obtain very competitive classi-
fiers at a sense level.
4.3 Learning curves
We also performed a set of experiments for mea-
suring the behaviour of the class-based WSD sys-
tem when gradually increasing the number of
training examples. These experiments have been
carried for nouns and verbs, but only noun results
are shown since in both cases, the trend is very
similar but more clear for nouns.
The training corpus has been divided in portions
of 5% of the total number of files. That is, com-
plete files are added to the training corpus of each
incremental test. The files were randomly selected
to generate portions of 5%, 10%, 15%, etc. of the
SemCor corpus14. Then, we train the system on
each of the training portions and we test the sys-
tem on SE2 and SE3. Finally, we also compare the
14Each portion contains also the same files than the previ-
ous portion. For example, all files in the 25% portion are also
contained in the 30% portion.
Class Sem. Feat. SensEval2 SensEval3Poly All Poly All
Sense
baseline 59.66 70.02 64.45 72.30
basicFeat 61.13 71.20 65.45 73.15
BLC20 61.93 71.79 65.45 73.15
BLC50 61.79 71.69 65.30 73.04
SS 61.00 71.10 64.86 72.70
WND 61.13 71.20 65.45 73.15
SUMO 61.66 71.59 65.45 73.15
BLC20
baseline 65.92 75.71 67.98 76.29
basicFeat 65.65 75.52 64.64 73.82
BLC20 68.70 77.69 68.29 76.52
BLC50 68.83 77.79 67.22 75.73
SS 65.12 75.14 64.64 73.82
WND 68.97 77.88 65.25 74.24
SUMO 68.57 77.60 64.49 73.71
BLC50
baseline 67.20 76.65 68.01 76.74
basicFeat 64.28 74.57 66.77 75.84
BLC20 69.72 78.45 68.16 76.85
BLC50 67.20 76.65 68.01 76.74
SS 65.60 75.52 65.07 74.61
WND 70.39 78.92 65.38 74.83
SUMO 71.31 79.58 66.31 75.51
WND
baseline 78.97 86.11 76.74 83.8
basicFeat 70.96 80.81 67.85 77.64
BLC20 72.53 81.85 72.37 80.79
BLC50 73.25 82.33 71.41 80.11
SS 74.39 83.08 68.82 78.31
WND 78.83 86.01 76.58 83.71
SUMO 75.11 83.55 73.02 81.24
SUMO
baseline 66.40 76.09 71.96 79.55
basicFeat 68.53 77.60 68.10 76.74
BLC20 65.60 75.52 68.10 76.74
BLC50 65.60 75.52 68.72 77.19
SS 68.39 77.50 68.41 76.97
WND 68.92 77.88 69.03 77.42
SUMO 68.92 77.88 70.88 78.76
SS
baseline 70.48 80.41 72.59 81.50
basicFeat 69.77 79.94 69.60 79.48
BLC20 71.47 81.07 72.43 81.39
BLC50 70.20 80.22 72.92 81.73
SS 70.34 80.32 65.12 76.46
WND 73.59 82.47 70.10 79.82
SUMO 70.62 80.51 71.93 81.05
Table 4: Results for nouns
resulting system with the baseline computed over
the same training portion.
Figures 1 and 2 present the learning curves over
SE2 and SE3, respectively, of a class-based WSD
system based on BLC20 using the basic features
and the semantic features built with WordNet Do-
mains.
Surprisingly, in SE2 the system only improves
the F1 measure around 2% while increasing the
training corpus from 25% to 100% of SemCor.
In SE3, the system again only improves the F1
measure around 3% while increasing the training
corpus from 30% to 100% of SemCor. That is,
most of the knowledge required for the class-based
WSD system seems to be already present on a
small part of SemCor.
Figures 3 and 4 present the learning curves over
SE2 and SE3, respectively, of a class-based WSD
system based on SuperSenses using the basic fea-
tures and the semantic features built with WordNet
Domains.
Again, in SE2 the system only improves the F1
394
Class Sem. Feat. SensEval2 SensEval3Poly All Poly All
Sense
baseline 41.20 44.75 49.78 52.88
basicFeat 42.01 45.53 54.19 57.02
BLC20 41.59 45.14 53.74 56.61
BLC50 42.01 45.53 53.6 56.47
SS 41.80 45.34 53.89 56.75
WND 42.01 45.53 53.89 56.75
SUMO 42.22 45.73 54.19 57.02
BLC20
baseline 50.21 55.13 54.87 58.82
basicFeat 52.36 57.06 57.27 61.10
BLC20 52.15 56.87 56.07 59.92
BLC50 51.07 55.90 56.82 60.60
SS 51.50 56.29 57.57 61.29
WND 54.08 58.61 57.12 60.88
SUMO 52.36 57.06 57.42 61.15
BLC50
baseline 49.78 54.93 55.96 60.06
basicFeat 53.23 58.03 58.07 61.97
BLC20 52.59 57.45 57.32 61.29
BLC50 51.72 56.67 57.01 61.01
SS 52.59 57.45 57.92 61.83
WND 55.17 59.77 58.52 62.38
SUMO 52.16 57.06 57.92 61.83
WND
baseline 84.80 90.33 84.96 92.20
basicFeat 84.50 90.14 78.63 88.92
BLC20 84.50 90.14 81.53 90.42
BLC50 84.50 90.14 81.00 90.15
SS 83.89 89.75 78.36 88.78
WND 85.11 90.52 84.96 92.20
SUMO 85.11 90.52 80.47 89.88
SUMO
baseline 54.24 60.35 59.69 64.71
basicFeat 56.25 62.09 61.41 66.21
BLC20 55.13 61.12 61.25 66.07
BLC50 56.25 62.09 61.72 66.48
SS 53.79 59.96 59.69 64.71
WND 55.58 61.51 61.56 66.35
SUMO 54.69 60.74 60.00 64.98
SS
baseline 62.79 68.47 76.24 79.07
basicFeat 66.89 71.95 75.47 78.39
BLC20 63.70 69.25 74.69 77.70
BLC50 63.70 69.25 74.69 77.70
SS 63.70 69.25 74.84 77.84
WND 66.67 71.76 77.02 79.75
SUMO 64.84 70.21 74.69 77.70
Table 5: Results for verbs
measure around 2% while increasing the training
corpus from 25% to 100% of SemCor. In SE3,
the system again only improves the F1 measure
around 2% while increasing the training corpus
from 30% to 100% of SemCor. That is, with only
25% of the whole corpus, the class-based WSD
system reaches a F1 close to the performance us-
ing all corpus. This evaluation seems to indicate
that the class-based approach to WSD reduces dra-
matically the required amount of training exam-
ples.
In both cases, when using BLC20 or Super-
Senses as semantic classes for tagging, the be-
haviour of the system is similar to MFC baseline.
This is very interesting since the MFC obtains high
results due to the way it is defined, since the MFC
over the total corpus is assigned if there are no oc-
currences of the word in the training corpus. With-
out this definition, there would be a large number
of words in the test set with no occurrences when
using small training portions. In these cases, the
recall of the baselines (and in turn F1) would be
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100
F1
% corpus
System SV2MFC SV2
Figure 1: Learning curve of BLC20 on SE2
 62
 64
 66
 68
 70
 72
 74
 76
 78
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100
F1
% corpus
System SV3MFC SV3
Figure 2: Learning curve of BLC20 on SE3
much lower.
5 Conclusions and discussion
We explored on the WSD task the performance
of different levels of abstraction and sense group-
ings. We empirically demonstrated that Base
Level Concepts are able to group word senses into
an adequate medium level of abstraction to per-
form supervised class?based disambiguation. We
also demonstrated that the semantic classes pro-
vide a rich information about polysemous words
and can be successfully used as semantic fea-
tures. Finally we confirm the fact that the class?
based approach reduces dramatically the required
amount of training examples, opening the way to
solve the well known acquisition bottleneck prob-
lem for supervised machine learning algorithms.
In general, the results obtained by BLC20 are
not very different to the results of BLC50. Thus,
we can select a medium level of abstraction, with-
out having a significant decrease of the perfor-
395
 68
 70
 72
 74
 76
 78
 80
 82
 84
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100
F1
% corpus
System SV2MFC SV2
Figure 3: Learning curve of SuperSense on SE2
 70
 72
 74
 76
 78
 80
 82
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100
F1
% corpus
System SV3MFC SV3
Figure 4: Learning curve of SuperSense on SE3
mance. Considering the number of classes, BLC
classifiers obtain high performance rates while
maintaining much higher expressive power than
SuperSenses. However, using SuperSenses (46
classes) we can obtain a very accurate semantic
tagger with performances around 80%. Even bet-
ter, we can use BLC20 for tagging nouns (558 se-
mantic classes and F1 over 75%) and SuperSenses
for verbs (14 semantic classes and F1 around
75%).
As BLC are defined by a simple and fully au-
tomatic method, they can provide a user?defined
level of abstraction that can be more suitable for
certain NLP tasks.
Moreover, the traditional set of features used for
sense-based classifiers do not seem to be the most
adequate or representative for the class-based ap-
proach. We have enriched the usual set of fea-
tures, by adding semantic information from the
monosemous words of the context and the MFC
of the target word. With this new enriched set of
features, we can generate robust and competitive
class-based classifiers.
To our knowledge, the best results for class?
based WSD are those reported by (Ciaramita and
Altun, 2006). This system performs a sequence
tagging using a perceptron?trained HMM, using
SuperSenses, training on SemCor and testing on
SensEval3. The system achieves an F1?score of
70.54, obtaining a significant improvement from
a baseline system which scores only 64.09. In
this case, the first sense baseline is the SuperSense
of the most frequent synset for a word, according
to the WN sense ranking. Although this result is
achieved for the all words SensEval3 task, includ-
ing adjectives, we can compare both results since
in SE2 and SE3 adjectives obtain very high per-
formance figures. Using SuperSenses, adjectives
only have three classes (WN Lexicographic Files
00, 01 and 44) and more than 80% of them belong
to class 00. This yields to really very high perfor-
mances for adjectives which usually are over 90%.
As we have seen, supervised WSD systems are
very dependent of the corpora used to train and
test the system. We plan to extend our system by
selecting new corpora to train or test. For instance,
by using the sense annotated glosses from Word-
Net.
References
E. Agirre and O. LopezDeLaCalle. 2003. Clustering
wordnet word senses. In Proceedings of RANLP?03,
Borovets, Bulgaria.
J. Alvez, J. Atserias, J. Carrera, S. Climent, E. Laparra,
A. Oliver, and G. Rigau G. 2008. Complete and
consistent annotation of wordnet using the top con-
cept ontology. In 6th International Conference on
Language Resources and Evaluation LREC, Mar-
rakesh, Morroco.
M. Ciaramita and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP?06), pages 594?602,
Sydney, Australia. ACL.
M. Ciaramita and M. Johnson. 2003. Supersense tag-
ging of unknown nouns in wordnet. In Proceedings
of the Conference on Empirical methods in natural
language processing (EMNLP?03), pages 168?175.
ACL.
J. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL?05), pages 26?33. ACL.
396
C. Fellbaum, editor. 1998. WordNet. An Electronic
Lexical Database. The MIT Press.
M. Hearst and H. Schu?tze. 1993. Customizing a lexi-
con to better suit a computational task. In Proceed-
ingns of the ACL SIGLEX Workshop on Lexical Ac-
quisition, Stuttgart, Germany.
R. Izquierdo, A. Suarez, and G. Rigau. 2007. Explor-
ing the automatic selection of basic level concepts.
In Galia Angelova et al, editor, International Con-
ference Recent Advances in Natural Language Pro-
cessing, pages 298?302, Borovets, Bulgaria.
T. Joachims. 1998. Text categorization with support
vector machines: learning with many relevant fea-
tures. In Claire Ne?dellec and Ce?line Rouveirol, edi-
tors, Proceedings of ECML-98, 10th European Con-
ference on Machine Learning, number 1398, pages
137?142, Chemnitz, DE. Springer Verlag, Heidel-
berg, DE.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into wordnet. In Proceedings of LREC,
Athens. Greece.
Ll. Ma`rquez, G. Escudero, D. Mart??nez, and G. Rigau.
2006. Supervised corpus-based methods for wsd. In
E. Agirre and P. Edmonds (Eds.) Word Sense Disam-
biguation: Algorithms and applications., volume 33
of Text, Speech and Language Technology. Springer.
R. Mihalcea and D. Moldovan. 2001. Automatic gen-
eration of coarse grained wordnet. In Proceding of
the NAACL workshop on WordNet and Other Lex-
ical Resources: Applications, Extensions and Cus-
tomizations, Pittsburg, USA.
R. Mihalcea. 2007. Using wikipedia for automatic
word sense disambiguation. In Proceedings of
NAACL HLT 2007.
G. Miller, C. Leacock, R. Tengi, and R. Bunker. 1993.
A Semantic Concordance. In Proceedings of the
ARPA Workshop on Human Language Technology.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In ACL-44: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 105?112, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
I. Niles and A. Pease. 2001. Towards a standard upper
ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001), pages 17?19. Chris Welty and
Barry Smith, eds.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and
H. Trang Dang. 2001. English tasks: All-
words and verb lexical sample. In Proceedings
of the SENSEVAL-2 Workshop. In conjunction with
ACL?2001/EACL?2001, Toulouse, France.
W. Peters, I. Peters, and P. Vossen. 1998. Automatic
sense clustering in eurowordnet. In First Interna-
tional Conference on Language Resources and Eval-
uation (LREC?98), Granada, Spain.
F. Segond, A. Schiller, G. Greffenstette, and J. Chanod.
1997. An experiment in semantic tagging using hid-
den markov model tagging. In ACL Workshop on
Automatic Information Extraction and Building of
Lexical Semantic Resources for NLP Applications,
pages 78?81. ACL, New Brunswick, New Jersey.
R. Snow, Prakash S., Jurafsky D., and Ng A. 2007.
Learning to merge word senses. In Proceedings of
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 1005?
1014.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Rada Mihalcea and Phil Edmonds, edi-
tors, Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis
of Text, pages 41?43, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics.
L. Villarejo, L. Ma`rquez, and G. Rigau. 2005. Ex-
ploring the construction of semantic class classi-
fiers for wsd. In Proceedings of the 21th Annual
Meeting of Sociedad Espaola para el Procesamiento
del Lenguaje Natural SEPLN?05, pages 195?202,
Granada, Spain, September. ISSN 1136-5948.
P. Vossen, L. Bloksma, H. Rodriguez, S. Climent,
N. Calzolari, A. Roventini, F. Bertagna, A. Alonge,
and W. Peters. 1998. The eurowordnet base con-
cepts and top ontology. Technical report, Paris,
France, France.
D. Yarowsky. 1994. Decision lists for lexical ambigu-
ity resolution: Application to accent restoration in
spanish and french. In Proceedings of the 32nd An-
nual Meeting of the Association for Computational
Linguistics (ACL?94).
397
Semantic Pattern Learning Through Maximum Entropy-based WSD
technique  
Maximiliano Saiz-Noeda
Depto. de Lenguajes y
Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
max@dlsi.ua.es
Armando Sua?rez
Depto. de Lenguajes y
Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
armando@dlsi.ua.es
Manuel Palomar
Depto. de Lenguajes y
Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
mpalomar@dlsi.ua.es
Abstract
This paper describes a Natural Lan-
guage Learning method that extracts
knowledge in the form of semantic pat-
terns with ontology elements associated
to syntactic components in the text. The
method combines the use of EuroWord-
Net?s ontological concepts and the cor-
rect sense of each word assigned by
a Word Sense Disambiguation(WSD)
module to extract three sets of pat-
terns: subject-verb, verb-direct object
and verb-indirect object. These sets de-
fine the semantic behaviour of the main
textual elements based on their syntac-
tic role. On the one hand, it is shown
that Maximum Entropy models applied
to WSD tasks provide good results. The
evaluation of the WSD module has re-
vealed a accuracy rate of 64% in a pre-
liminary test. On the other hand, we ex-
plain how an adequate set of semantic
or ontological patterns can improve the
success rate of NLP tasks such us pro-
noun resolution. We have implemented
both modules in C++ and although the
evaluation has been performed for En-
glish, their general features allow the
treatment of other languages like Span-
ish.

This paper has been partially supported by the Spanish
Government (CICYT) project number TIC2000-0664-C02-
02.
1 Introduction
Semantic patterns, as defined in this method, con-
figure a system to add a new information source
to Natural Language Processing (NLP) tasks. To
obtain these semantic patterns, it is necessary to
count on different tools. On the one hand, a full
parser must make a syntactic analysis of the text.
This parsing will allow the selection of the differ-
ent syntactic functional elements such as subject,
direct object (DObj) and indirect object (IObj).
On the other hand, a WSD tool must provide the
correct sense in order to ensure the appropriate
selection of the ontological concept associated to
each word. Finally, with the parsing and the cor-
rect sense of each word, the pattern extraction
method will form and store ontological pairs that
define the semantic behaviour of each sentence.
2 Full parsing
The analyzer used for this work is the Conexor?s
FDG Parser (Pasi Tapanainen and Timo Ja?rvinen,
1997). This parser tries to provide a build depen-
dency tree from the sentence. When this is not
possible, the parser tries to build partial trees that
often result from unresolved ambiguity. One vi-
sual example of this dependency trees is shown in
Figure 1 where the parsing tree of sentence (1) is
illustrated.
(1) The minister gave explanations to
the Government.
As seen in Figure 2, the analyzer assigns
to each word a text token (second column), a
base form (third column) and functional link
0
1 The the det:>2 @DN> DET SG/PL
2 minister minister subj:>3 @SUBJ N NOM SG
3 gave give main:>0 @+FMAINV V PAST
4 explanations explanation obj:>3 @OBJ N NOM PL
5 to to dat:>3 @ADVL PREP
6 the the det:>7 @DN> DET SG/PL
7 Government government pcomp:>5 @<P N NOM SG/PL
. .
Figure 2: FDG Analyser?s output example
Figure 1: Parsing tree
names, lexico-syntactic function labels and parts
of speech (fourth column). Figure 1 shows the
parsing tree related to this output. These elements
are enough for the pattern extraction method to be
applied to NLP tasks.
Regarding to the evaluation of the parser, the
authors report an average precision and recall of
95% and 88% respectively in the detection of the
correct head. Furthermore, they report a precision
rate between 89% and 95% and a recall rate be-
tween 83% and 96% in the selection of the func-
tional dependencies.
3 WSD based on Maximum Entropy
A WSD module is applied to this parser?s output,
in order to select the correct sense of each entry.
Maximum Entropy(ME) modeling is a frame-
work for integrating information from many het-
erogeneous information sources for classification
(Manning and Schu?tze, 1999). This WSD sys-
tem is based on conditional ME probability mod-
els. The system implements a supervised learn-
ing method consisting of the building of word
sense classifiers through training on a semanti-
cally tagged corpus. A classifier obtained by
means of a ME technique consist of a set of
parameters or coefficients estimated by an opti-
mization procedure. Each coefficient associates
a weight to one feature observed in the training
data. A feature is a function that gives infor-
mation about some characteristic in a context as-
sociated to a class. The basic idea is to obtain
the probability distribution that maximizes the en-
tropy, that is, maximum ignorance is assumed and
nothing apart of training data is considered. As
advantages of ME framework, knowledge-poor
features applying and accuracy can be mentioned;
ME framework allows a virtually unrestricted
ability to represent problem-specific knowledge
in the form of features (Ratnaparkhi, 1998).
Let us assume a set of contexts

and a
set of classes  . The function 
 	
 that performs the classification in a condi-
tional probability model 
 chooses the class with
the highest conditional probability: 
The University of Alicante systems at SENSEVAL-3?
Sonia Va?zquez, Rafael Romero
Armando Sua?rez and Andre?s Montoyo
Dpt. of Software and Computing Systems
Universidad de Alicante, Spain
{svazquez,romero}@dlsi.ua.es
{armando,montoyo}@dlsi.ua.es
Iulia Nica and Antonia Mart?? ?
Dpt. of General Linguistics
Universidad de Barcelona, Spain
iulia@clic.fil.ub.es
amarti@ub.edu
Abstract
The DLSI-UA team is currently working on sev-
eral word sense disambiguation approaches, both
supervised and unsupervised. These approaches are
based on different ways to use both annotated and
unannotated data, and several resources generated
from or exploiting WordNet (Miller et al, 1993),
WordNet Domains, EuroWordNet (EWN) and addi-
tional corpora. This paper presents a view of differ-
ent system results for Word Sense Disambiguation
in different tasks of SENSEVAL-3.
1 Introduction
Word Sense Disambiguation (WSD) is an open re-
search field in Natural Language Processing (NLP).
The task of WSD consists in assigning the correct
sense to words in a particular context using an elec-
tronic dictionary as the source of words definitions.
This is a difficult problem that is receiving a great
deal of attention from the research community.
At the Second International Workshop on
Evaluating Word Sense Disambiguation Systems,
SENSEVAL-2, several supervised and unsupervised
systems took part. The more successful systems re-
lay on corpus-based and supervised learning meth-
ods. At SENSEVAL-2 the average level of accu-
racy achieved rounded 70%, which is insufficient
for such other NLP tasks as information retrieval,
machine translation, or question answering.
The DLSI-UA systems were applied to three
SENSEVAL-3 tasks: English all-words, English lex-
ical sample and Spanish Lexical Sample. Our sys-
tems use both corpus-based and knowledge-based
approaches: Maximum Entropy(ME) (Lau et al,
1993; Berger et al, 1996; Ratnaparkhi, 1998) is
a corpus-based and supervised method based on
linguistic features; ME is the core of a bootstrap-
ping algorithm that we call re-training inspired
? This paper has been partially supported by the Spanish
Government (CICyT) under project number TIC-2003-7180
and the Valencia Government (OCyT) under project number
CTIDIB-2002-151
by co-training (Blum and Mitchell, 1998); Rele-
vant Domains (RD) (Montoyo et al, 2003) is a
resource built from WordNet Domains (Magnini
and Cavaglia, 2000) that is used in an unsuper-
vised method that assigns domain and sense la-
bels; Specification Marks(SP) (Montoyo and Palo-
mar, 2000) exploits the relations between synsets
stored in WordNet (Miller et al, 1993) and does not
need any training corpora; Commutative Test (CT)
(Nica et al, 2003), based on the Sense Discrimi-
nators device derived from EWN (Vossen, 1998),
disambiguates nouns inside their syntactic patterns,
with the help of information extracted from raw cor-
pus.
A resume of which methods and how were used
in which SENSEVAL-3 tasks is shown in Table 1.
DLSI-UA Method Combined
Systems Results
ALL-NOSU RD No
LS-ENG-SU Re-t No
LS-ENG-
NOSU
RD No
LS-SPA-SU ME+Re-t No
LS-SPA-NOSU SM + ME Nouns: SM
Verbs and adj.: ME
LS-SPA- Pattern-Nica Nouns: SM
PATTERN + ME Verbs and adj.: ME
Table 1: DLSI-UA Systems at SENSEVAL-3
Most of these methods are relatively new and our
goal when participating at SENSEVAL-3 is to evalu-
ate for the first time such approaches. At the mo-
ment of writing this paper we can conclude that
these are promising contributions in order to im-
prove current WSD systems.
In the following section each method is described
briefly. Then, details of how the SENSEVAL-3 train
and testing data were processed are shown. Next,
the scores obtained by each system are explained.
Finally, some conclusions and future work are pre-
sented.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2 Methods and Algorithms
In this section we describe the set of methods and
techniques that we used to build the four systems
that had participated in SENSEVAL-3.
2.1 Re-training and Maximum Entropy
In this section, we describe our bootstrapping
method, which we call re-training. Our method
is derived from the co-training method. Our re-
training system is based on two different views of
the data (as is also the case for co-training), de-
fined using several groups of features from those de-
scribed in Figure 1, with several filters that ensure a
high confidence sense labelling.
? the target word itself
? lemmas of content-words at positions ?1, ?2, ?3
? words at positions ?1, ?2,
? words at positions ?1, ?2, ?3
? content-words at positions ?1, ?2, ?3
? POS-tags of words at positions ?1, ?2, ?3
? lemmas of collocations at positions (?2,?1),
(?1,+1), (+1,+2)
? collocations at positions (?2,?1), (?1,+1),
(+1,+2)
? lemmas of nouns at any position in context, occur-
ring at least m% times with a sense
? grammatical relation of the target word
? the word that the target word depends on
? the verb that the target word depends on
? the target word belongs to a multi-word, as identi-
fied by the parser
? ANPA codes (Spanish only)
? IPTC codes (Spanish only)
Figure 1: Features Used for the Supervised Learn-
ing
These two views consist of two weak ME learn-
ers, based on different sets of linguistic features,
for every possible sense of a target word. We de-
cided to use ME as the core of our bootstrapping
method because it has shown to be competitive in
WSD when compared to other machine learning ap-
proaches (Sua?rez and Palomar, 2002; Ma`rquez et
al., 2003).
The main difference with respect co-training is
that the two views are used in parallel in order to
get a consensus of what label to assign to a particu-
lar context. Additional filters will ultimately deter-
mine which contexts will then be added to the next
training cycle.
Re-training performs several binary partial train-
ings with positive and negative examples for each
sense. These classifications must be merged in a
unique label for such contexts with enough evidence
of being successfully classified. This ?evidence? re-
lies on values of probability assigned by the ME
module to positive and negative labels, and the fact
that the unlabeled example is classified as positive
for a unique sense only. The set of new labeled ex-
amples feeds the training corpora of the next itera-
tion with positive and negative examples. The stop-
ping criteria is a certain number of iterations or the
failure to obtain new examples from the unlabeled
corpus.
2.2 Specification Marks
Specification Marks is an unsupervised WSD
method over nouns. Its context is the group of words
that co-occur with the word to be disambiguated in
the sentence and their relationship to the noun to
be disambiguated. The disambiguation is resolved
with the use of the WordNet lexical knowledge base.
The underlying hypothesis of the method we
present here is that the higher the similarity between
two words, the larger the amount of information
shared by two concepts. In this case, the informa-
tion commonly shared by two concepts is indicated
by the most specific concept that subsumes them
both in the taxonomy.
The input for the WSD module is a group of
nouns W = {w1, w2, ..., wn} in a context. Each
word wi is sought in WordNet, each having an asso-
ciated set of possible senses Si ={Si1, Si2, ..., Sin},
and each sense having a set of concepts in the IS-A
taxonomy (hypernymy/hyponymy relations). First,
the common concept to all the senses of the words
that form the context is gathered. This concept is
marked by the initial specification mark (ISM). If
this initial specification mark does not resolve the
ambiguity of the word, we then descend through
the WordNet hierarchy, from one level to another,
assigning new specification marks. The number of
concepts contained within the subhierarchy is then
counted for each specification mark. The sense that
corresponds to the specification mark with the high-
est number of words is the one chosen as the sense
disambiguated within the given context
We define six heuristics for our system: Heuris-
tic of Hypernym, Heuristic of Definition, Heuristic
of Common Specification Mark, Heuristic of Gloss
Hypernym, Heuristic of Hyponym and Heuristic of
Gloss Hyponym.
2.3 Relevant Domains
This is an unsupervised WSD method based on the
WordNet Domains lexical resource (Magnini and
Cavaglia, 2000). The underlying working hypoth-
esis is that domain labels, such as ARCHITEC-
TURE, SPORT and MEDICINE provide a natural
way to establish semantic relations between word
senses, that can be used during the disambiguation
process. This resource has already been used on
Word Sense Disambiguation (Magnini and Strappa-
rava, 2000), but it has not made use of glosses infor-
mation. So our approach make use of a new lexical
resource obtained from glosses information named
Relevant Domains.
First step is to obtain the Relevant Domains re-
source from WordNet glosses. For this task is nec-
essary a previous part-of-speech tagging of Word-
Net glosses (each gloss has associated a domain la-
bel). So we extract all nouns, verbs, adjectives and
adverbs from glosses and assign them their associ-
ated domain label. With this information and using
the Association Ratio formula (w=word,D=domain
label), in (1), we obtain the Relevant Domains re-
source.
AR(w,D) = Pr(w|D)log2Pr(w|D)Pr(w) (1)
The final result is for each word, a set of domain
labels sorted by Association Ratio, for example,
for word plant? its Relevant Domains are: genetics
0.177515, ecology 0.050065, botany 0.038544 . . . .
Once obtained Relevant Domains the disam-
biguation process is carried out. We obtain from
the text source the context words that co-occur with
the word to be disambiguated (context could be
a sentence or a window of words). We obtain a
context vector from Relevant Domains and context
words (in case of repeated domain labels, they are
weighted). Furthermore we need a sense vector ob-
tained in the same way as context vector from words
of glosses of each word sense. We select the cor-
rect sense using the cosine measure between con-
text vector and sense vectors. So the selected sense
is that for which the cosine with the context vector
is closer to one.
2.4 Pattern-Nica
This is an unsupervised method only for Spanish
nouns exploiting both EuroWordNet and corpus.
In this method we adopt a different approach to
WSD: the occurrence to be disambiguated is con-
sidered not separately, but integrated into a syn-
tactic pattern, and its disambiguation is carried
out in relation to this pattern. A syntactic pat-
tern is a triplet X-R-Y, formed by two lexical con-
tent units X and Y and an eventual relational el-
ement R, which corresponds to a syntactic rela-
tion between X and Y. Examples: [X=canal-noun
R=de-preposition Y=televisio?n-noun], [X=pasaje-
noun R=? Y=ae?reo-adjective]. The strategy is
based on the hypothesis that syntactic patterns in
which an ambiguous occurrence participates have
decisive influence on its meaning. We also assume
that inside a syntactic pattern a word will tend to
have the same sense: the ?quasi one sense per syn-
tactic pattern? hypothesis. The method works as fol-
lows:
Step 1, the identification of the syntactic patterns
of the ambiguous occurrence;
Step 2, the extraction of information related to it:
from corpus and from the sentential context;
Step 3, the application of the WSD algorithm on
the different information previously obtained;
Step 4, the final sense assignment by combining
the partial sense proposals from step 3.
For step 1, we POS-tag the test sentence and ex-
tract the sequences that correspond to previously de-
fined combinations of POS tags. We only kept the
patterns with frequency 5 or superior.
In step 2, we use a search corpus previously POS-
tagged. For every syntactic pattern of the ambigu-
ous occurrence X, we obtain from corpus two sets of
words: the substitutes of X into the pattern (S1) and
the nouns that co-occur with the pattern in any sen-
tence from the corpus (S2), In both cases, we keep
only the element with frequency 5 or superior.
We perform step 3 by means of the heuristics de-
fined by the Commutative Test (CT) algorithm ap-
plied on each set from 2. The algorithm is related
to the Sense Discriminators (SD) lexical device, an
adaptation of the Spanish WordNet, consisting in a
set of sense discriminators for every sense of a given
noun in WordNet. The Commutative Test algorithm
lays on the hypothesis that if an ambiguous occur-
rence can be substituted in a syntactic pattern by a
sense discriminator, then it can have the sense cor-
responding to that sense discriminator.
For step 4, we first obtain a sense assignment in
relation with each syntactic pattern, by intersecting
the sense proposals from the two heuristics corre-
sponding to a pattern; then we choose the most fre-
quent sense between those proposed by the differ-
ent syntactic patterns; finally, if there are more final
proposed senses, we choose the most frequent sense
on the base of sense numbers in WordNet.
The method we propose for nouns requires only a
large corpus, a minimal preprocessing phase (POS-
tagging) and very little grammatical knowledge, so
it can easily be adapted to other languages. Sense
assignment is performed exploiting information ex-
tracted from corpus, thus we make an intensive use
of sense untagged corpora for the disambiguation
process.
3 Tasks Processing
At this point we explain for each task the systems
processing. The results of each system are shown in
Table2:
DLSI-UA Systems Precision Recall
LS-SPA-SU 84% 84%
LS-ENG-SU 82% 32%
ALL-NOSU 34% 28%
LS-ENG-NOSU 32% 20%
LS-SPA-NOSU 62% 62%
LS-SPA-PATTERN 84% 47%
Table 2: Results at SENSEVAL-3
3.1 DLSI-UA-LS-SPA-SU
Our system, based on re-training and maximum en-
tropy methods, processed both sense labelled and
unlabelled Spanish Lexical Sample data in three
consecutive steps:
Step 1, analyzing the train corpus: words which
most frequent sense is under 70% were selected.
For each one of these words, each feature was used
in a 3-fold cross-validation in order to determine the
best set of features for re-training.
Step 2, feeding training corpora: for these se-
lected words, based on the results of the previous
step, each training corpus was enriched with new
examples from the unlabelled data using re-training.
Step 3, classifying the test data: for the selected
words, re-training was used again to obtain a first set
of answers with, a priori, a label with a high level of
confidence; the remaining contexts that re-training
could not classify were processed with the ME sys-
tem using a unique set of features for all words.
The lemmatization and POS information supplied
into the SENSEVAL-3 Spanish data were the infor-
mation used for defining the features of the system.
0ur system obtained an accuracy of 0.84 for the
Spanish lexical sample task. Unfortunately, a shal-
low analysis of the answers revealed that the UA.5
system performed slightly worse than if only the ba-
sic ME system were used1. This fact means that the
new examples extracted from the unlabelled data in-
troduced too much noise into the classifiers. Be-
cause this anomalous behavior was present only on
some words, a complete study of such new exam-
ples must be done. Probably, the number of itera-
tions done by re-training over unlabelled data were
too low and the enrichment of the training corpora
not large enough.
1The ME system, without using re-training, has not com-
peted at SENSEVAL-3: our own scoring of these set of answers
reported an accuracy of 0.856
3.2 DLSI-UA-LS-ENG-SU
In the English Lexical Sample task our system goal
was to prove that the re-training method ensures a
high level of precision.
By means of a 3-fold cross-validation of the train
data, the features were ordered from higher to lower
precision. Based on this information, four execu-
tions of re-training over the test data were done with
different selections of features for the two views of
the method. Each execution feed the learning cor-
pora of the next one with new examples, those that
re-training considered as the most probably correct.
For this system Minipar parser (Lin, 1998)was
used to properly add syntactic information to the
training and testing data.
Almost 40% of the test contexts were la-
belled by our system, obtaining these scores (for
?fine-grained? and ?coarse-grained?, respectively):
0.782/0.828 precision and 0.310/0.329 recall. In our
opinion, such results must be interpreted as very
positive because the re-training method is able to
satisfy a high level of precision if the parameters of
the system are correctly set.
3.3 DLSI-UA-ALL-NOSU and
DLSI-UA-LS-ENG-NOSU
In the English All Words and English Lexical Sam-
ple tasks RD system was performed with informa-
tion obtained from Relevant Domains resource us-
ing for the disambiguation process all the 165 do-
main labels.
For All Words task we used as input information
all nouns, verbs, adjectives and adverbs present in
a 100 words window around the word to be disam-
biguated. So our system obtained a 34% of preci-
sion and a reduced recall around 28%.
For Lexical Sample task we used all nouns, verbs,
adjectives and adverbs present in the context of each
instance obtaining around 32% precision.
We obtained a reduced precision due to we use all
the domains label hierarchy. In some experiments
realized on SENSEVAL-2 data, our system obtained
a more high precision when grouping domains into
the first three levels. Therefore we expect with re-
ducing the number of domains labels, an improve-
ment on precision.
3.4 DLSI-UA-LS-SPA-NOSU
We used a combined system for Spanish Lexical
Sample task, using the SM method for disambiguat-
ing nouns and the ME method for disambiguating
verbs and adjectives. We obtained around 62% pre-
cision and a 62% recall.
3.5 DLSI-UA-LS-SPA-PATTERN
Our goal when participating in this task was to
demonstrate that the applying of syntactic patterns
to WSD maintains high levels of precision.
In this task we used also a combined system for
Spanish Lexical Sample task, using Pattern-Nica
method for disambiguating nouns and ME method
for disambiguating verbs and adjectives. We ob-
tained around 84% precision and a 47% recall.
4 Conclusions
The supervised systems for the English and Span-
ish lexical sample tasks are very competitive. Al-
though the processing of the train and test data was
different for each task, both systems rely on re-
training, a bootstrapping method, that uses a max-
imum entropy-based WSD module.
The results for the English task prove that re-
training is capable of maintaining a high level of
precision. Nevertheless, for the Spanish task, al-
though the scores achieved were excellent, the sys-
tem must be redesigned in order to improve the clas-
sifiers.
The re-training method is a proposal that we are
trying to incorporate into text retrieval and ques-
tion answering systems that could take advantage of
sense disambiguation of a subset of words.
The unsupervised systems presented here does
not appear to be sufficient for a stand-alone WSD
solution. Wether these methods can be combined
with other supervised methods to improve their re-
sults requires further investigation.
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the 11th Annual Conference on
Computational Learning Theory, pages 92?100,
Madison, Wisconsin, July. ACM Press.
R. Lau, R. Rosenfeld, and S. Roukos. 1993.
Adaptative statistical language modeling using
the maximum entropy principle. In Proceedings
of the Human Language Technology Workshop,
ARPA.
Dekang Lin. 1998. Dependency-based evaluation
of minipar. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, First Inter-
national Conference on Language Resources and
Evaluation, Granada, Spain.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating Subject Field Codes into WordNet. In
M. Gavrilidou, G. Crayannis, S. Markantonatu,
S. Piperidis, and G. Stainhaouer, editors, Pro-
ceedings of LREC-2000, Second International
Conference on Language Resources and Evalu-
ation, pages 1413?1418, Athens, Greece.
Bernardo Magnini and C. Strapparava. 2000. Ex-
periments in Word Domain Disambiguation for
Parallel Texts. In Proceedings of the ACL Work-
shop on Word Senses and Multilinguality, Hong
Kong, China.
Llu??s Ma`rquez, Fco. Javier Raya, John Car-
roll, Diana McCarthy, Eneko Agirre, David
Mart??nez, Carlo Strapparava, and Alfio
Gliozzo. 2003. Experiment A: several all-words
WSD systems for English. Technical Report
WP6.2, MEANING project (IST-2001-34460),
http://www.lsi.upc.es/?nlp/meaning/meaning.html.
George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J. Miller.
1993. Five Papers on WordNet. Special Issue of
the International journal of lexicography, 3(4).
Andre?s Montoyo and Manuel Palomar. 2000. Word
Sense Disambiguation with Specification Marks
in Unrestricted Texts. In Proceedings of 11th In-
ternational Workshop on Database and Expert
Systems Applications (DEXA 2000), pages 103?
107, Greenwich, London, UK, September. IEEE
Computer Society.
Andre?s Montoyo, Sonia Va?zquez, and German
Rigau. 2003. Me?todo de desambiguacio?n le?xica
basada en el recurso le?xico Dominios Rele-
vantes. Procesamiento del Lenguaje Natural, 30,
september.
Iulia Nica, Antonia Mart??, and Andre?s Mon-
toyo. 2003. Colaboracio?n entre informacio?n
paradigma?tica y sintagma?tica en la desam-
biguacio?n sema?ntica automa?tica. XIX Congreso
de la SEPLN 2003.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
Armando Sua?rez and Manuel Palomar. 2002.
A maximum entropy-based word sense disam-
biguation system. In Hsin-Hsi Chen and Chin-
Yew Lin, editors, Proceedings of the 19th In-
ternational Conference on Computational Lin-
guistics, pages 960?966, Taipei, Taiwan, August.
COLING 2002.
Piek Vossen. 1998. EuroWordNet: Building a Mul-
tilingual Database with WordNets for European
Languages. The ELRA Newsletter, 3(1).
The R2D2 Team at SENSEVAL-3?
Sonia Va?zquez, Rafael Romero
Armando Sua?rez and Andre?s Montoyo
Dpto. de Lenguajes y Sistemas. Informa?ticos
Universidad de Alicante, Spain
{svazquez,romero}@dlsi.ua.es
{armando,montoyo}@dlsi.ua.es
Manuel Garc??a, M. Teresa Mart??n ?
M. ?Angel Garc??a and L. Alfonso Uren?a
Dpto. de Informa?tica
Universidad de Jae?n, Spain
{mgarcia,maite}@ujaen.es
{magc,laurena}@ujaen.es
Davide Buscaldi, Paolo Rosso ?
Antonio Molina, Ferra?n Pla? and Encarna Segarra
Dpto. de Sistemas Informa?ticos y Computacio?n
Univ. Polit. de Valencia, Spain
{dbuscaldi,prosso}@dsic.upv.es
{amolina,fpla,esegarra}@dsic.upv.es
Abstract
The R2D2 systems for the English All-Words and
Lexical Sample tasks at SENSEVAL-3 are based on
several supervised and unsupervised methods com-
bined by means of a voting procedure. Main goal
was to take advantage of training data when avail-
able, and getting maximum coverage with the help
of methods that not need such learning examples.
The results reported in this paper show that super-
vised and unsupervised methods working in par-
allel, and a simple sequence of preferences when
comparing the answers of such methods, is a feasi-
ble method. . .
The whole system is, in fact, a cascade of deci-
sions of what label to assign to a concrete instance
based on the agreement of pairs of systems, when
it is possible, or selecting the available answer from
one of them. In this way, supervised are preferred to
unsupervised methods, but these last ones are able
to tag such words that not have available training
data.
1 Introduction
Designing a system for Natural Language Process-
ing (NLP) requires a large knowledge on language
structure, morphology, syntax, semantics and prag-
matic nuances. All of these different linguistic
knowledge forms, however, have a common asso-
ciated problem, their many ambiguities, which are
difficult to resolve.
In this paper we concentrate on the resolution
of the lexical ambiguity that appears when a given
word in a context has several different meanings.
? This paper has been partially supported by the Spanish
Government (CICyT) under project number TIC-2003-7180
and the Valencia Government (OCyT) under project number
CTIDIB-2002-151
This specific task is commonly referred as Word
Sense Disambiguation (WSD). This is a difficult
problem that is receiving a great deal of attention
from the research community because its resolu-
tion can help other NLP applications as Machine
Translation (MT), Information Retrieval (IR), Text
Processing, Grammatical Analysis, Information Ex-
traction (IE), hypertext navigation and so on.
The R2D2 Team has participated in two tasks:
English all-words and lexical sample. We use sev-
eral different systems both supervised and unsuper-
vised. The supervised methods are based on Max-
imum Entropy (ME) (Lau et al, 1993; Berger et
al., 1996; Ratnaparkhi, 1998), neural network using
the Learning Vector Quantization algorithm (Koho-
nen, 1995) and Specialized Hidden Markov Mod-
els (Pla, 2000). The unsupervised methods are Rel-
evant Domains (RD) (Montoyo et al, 2003) and
the CIAOSENSO WSD system which is based on
Conceptual Density (Agirre and Rigau, 1995), fre-
quency of WordNet (Miller et al, 1993a) senses and
WordNet Domains (Magnini and Cavaglia, 2000).
In the following section we will show a more
complete description of the systems. Next, how
such methods were combined in two voting sys-
tems, and the results obtained in SENSEVAL-3. Fi-
nally, some conclusions will be presented.
2 Systems description
In this section the systems that have participated at
SENSEVAL-3 will be described.
2.1 Maximum Entropy
ME modeling provides a framework for integrating
information for classification from many heteroge-
neous information sources (Manning and Schu?tze,
1999). ME probability models have been success-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
fully applied to some NLP tasks, such as POS tag-
ging or sentence boundary detection (Ratnaparkhi,
1998). ME have been also applied to WSD (van
Halteren et al, 2001; Montoyo and Sua?rez, 2001;
Sua?rez and Palomar, 2002), and as meta-learner in
(Ilhan et al, 2001).
Our ME-based system has been shown competi-
tive (Ma`rquez et al, 2003) when compared to other
supervised systems such as Decision Lists, Support
Vector Machines, and AdaBoost. The features that
were defined to train the system are those described
in Figure 1.
? the target word itself
? lemmas of content-words at positions ?1, ?2, ?3
? words at positions ?1, ?2,
? words at positions ?1, ?2, ?3
? content-words at positions ?1, ?2, ?3
? POS-tags of words at positions ?1, ?2, ?3
? lemmas of collocations at positions (?2,?1),
(?1,+1), (+1,+2)
? collocations at positions (?2,?1), (?1,+1),
(+1,+2)
? lemmas of nouns at any position in context, occur-
ring at least m% times with a sense
? grammatical relation of the target word
? the word that the target word depends on
? the verb that the target word depends on
? the target word belongs to a multi-word, as identi-
fied by the parser
Figure 1: Features Used for the Supervised Learn-
ing of the ME system
Because the ME system needs annotated data
for the training, Semcor (Miller et al, 1993b) was
used for the English All-Words task, the system
was trained using Semcor (Miller et al, 1993b), and
parsed by Minipar (Lin, 1998). Only those words
that have 10 examples or more in Semcor were pro-
cessed in order to obtain a ME classifier.
For the Spanish Lexical Sample task, the train-
ing data from SENSEVAL-3 was the source of la-
beled examples. We did not use any parser, just the
lemmatization and POS-tagging information sup-
plied into the training data itself.
2.2 UPV-SHMM-AW
The upv-shmm-aw WSD system is a supervised ap-
proach based on Specialized Hidden Markov Mod-
els (SHMM).
Basically, a SHMM consists of changing the
topology of a Hidden Markov Model in order to get
a more accurate model which includes more infor-
mation. This is done by means of an initial step
previous to the learning process. It consists of the
redefinition of the input vocabulary and the output
tags. This redefinition is done by means of two pro-
cesses which transform the training set: the selec-
tion process chooses which input features (words,
lemmas, part-of-speech tags, ...) are relevant to the
task, and the specialization process redefines the
output tags by adding information from the input.
This specialization produces some changes in the
model topology, in order to allow the model to bet-
ter capture some contextual restrictions and to get a
more accurate model.
We used as training data the part of the Sem-
Cor corpus that is semantically annotated and su-
pervised for nouns, verbs, adjectives and adverbs,
and the test data set provided by SENSEVAL-2.
We used 10% of the training corpus as a develop-
ment data set in order to determine the best selection
and specialization criteria.
In the experiments, we used WordNet1.6 (Miller
et al, 1993a) as a dictionary that supplies all the
possible semantic senses for a given word. Our sys-
tem disambiguated all the polysemic lemmas, that
is, the coverage of our system was 100%. For un-
known words (words that did not appear in the train-
ing data set), we assigned the first sense in WordNet.
2.3 Relevant Domains
This is an unsupervised WSD method based on the
WordNet Domains lexical resource (Magnini and
Cavaglia, 2000). The underlaying working hypoth-
esis is that domain labels, such as ARCHITEC-
TURE, SPORT and MEDICINE provide a natural
way to establish semantic relations between word
senses, that can be used during the disambiguation
process. This resource has already been used on
Word Sense Disambiguation (Magnini and Strappa-
rava, 2000), but it has not made use of glosses infor-
mation. So our approach make use of a new lexical
resource obtained from glosses information named
Relevant Domains.
First step is to obtain the Relevant Domains re-
source from WordNet glosses. For this task is nec-
essary a previous part-of-speech tagging of Word-
Net glosses (each gloss has associated a domain la-
bel). So we extract all nouns, verbs, adjectives and
adverbs from glosses and assign them their associ-
ated domain label. With this information and using
the Association Ratio formula(w=word,D=domain
label), in (1), we obtain the Relevant Domains re-
source.
AR(w,D) = Pr(w|D)log2Pr(w|D)Pr(w) (1)
The final result is for each word, a set of domain
labels sorted by Association Ratio, for example,
for word plant? its Relevant Domains are: genetics
0.177515, ecology 0.050065, botany 0.038544 . . . .
Once obtained Relevant Domains the disam-
biguation process is carried out. We obtain from
the text source the context words that co-occur with
the word to be disambiguated (context could be
a sentence or a window of words). We obtain a
context vector from Relevant Domains and context
words (in case of repeated domain labels, they are
weighted). Furthermore we need a sense vector ob-
tained in the same way as context vector from words
of glosses of each word sense. We select the cor-
rect sense using the cosine measure between con-
text vector and sense vectors. So the selected sense
is that for which the cosine with the context vector
is closer to one.
2.4 LVQ-JA ?EN-ELS
The LVQ-JA ?EN-ELS system (Garc??a-Vega et al,
2003) is based on a supervised learning algorithm
for WSD. The method trains a neural network using
the Learning Vector Quantization (LVQ) algorithm
(Kohonen, 1995), integrating Semcor and several
semantic relations of WordNet.
The Vector Space Model (VSM) is used as an in-
formation representation model. Each sense of a
word is represented as a vector in an n-dimensional
space where n is the number of words in all its con-
texts.
We use the LVQ algorithm to adjust the word
weights. The input vector weights are calculated
as shown by (Salton and McGill, 1983) with the
standard (tf ? idf). They are presented to the LVQ
network and, after training, the output vectors are
obtained, containing the adjusted weights for all
senses of each word.
Any word to disambiguate is represented with a
vector in the same way. This representation must be
compared with all the trained sense vectors of the
word by applying the cosine similarity rule:
sim(wk, xi) = wk ? xi| wk | ? | xi | (2)
The sense corresponding to the vector of highest
similarity is selected as the disambiguated sense.
To train the neural network we have inte-
grated semantic information from two linguistic re-
sources: SemCor1.6 corpus and WordNet1.7.1 lex-
ical database. From Semcor1.6 we used the para-
graph as a contextual semantic unit and each con-
text was included in the training vector set. From
WordNet1.7.1 some semantic relations were consid-
ered, specifically, synonymy, antonymy, hyponymy,
homonymy, hyperonymy, meronymy, and coordi-
nate terms. This information was introduced to the
training set through the creation of artificial para-
graphs with the words of each relation. So, for a
word with 7 senses, 7 artificial paragraphs with the
synonyms of the 7 senses were added, 7 more with
all its hyponyms, and so on.
The learning algorithm is very simple. First, the
learning rate and the codebook vectors are initial-
ized. Then, the following procedure is repeated for
all the training input vectors until a stopping crite-
rion is satisfied:
- Select a training input pattern, x, with class d,
and present it to the network
- Calculate the Euclidean distance between the in-
put vector and each codebook vector || x? wk ||
- Select the codebook vector, wc, that is closest to
the input vector, x, like the winner sense.
- The winner neuron updates its weights accord-
ing the learning equation:
wc(t+ 1) = wc(t) + s ? ?(t) ? [x(t)? wc(t)] (3)
where s = 0, if k 6= c; s = 1, if x(t) and wc(t)
belong to the same class (c = d); and s = ?1, if
they do not (c 6= d). ?(t) is the learning rate, and
0 < ?(t) < 1 is a monotically decreasing func-
tion of time. It is recommended that ?(t) should
already initially be rather small, say, smaller than
0.1 (Kohonen, 1995) and ?(t) continues decreasing
to a given threshold, u, very close to 0.
2.5 CIAOSENSO
The CIAOSENSO WSD system is an unsupervised
system based on Conceptual Density, the frequency
of WordNet sense, and WordNet Domains. Concep-
tual Density is a measure of the correlation among
the sense of a given word and its context. The
noun sense disambiguation is performed by means
of a formula combining the Conceptual Density
with WordNet sense frequency (Rosso et al, 2003).
The context window used in both the English all-
words and lexical sample tasks is of 4 nouns. Ad-
ditional weights are assigned to those senses hav-
ing the same domain as the context nouns? senses.
Each weight is proportional to the frequency of such
senses, and is calculated as MDW (f, i) = 1/f ?1/i
where f is an integer representing the frequency
of the sense of the word to be disambiguated and
i gives the same information for the context word.
Example: If the word to be disambiguated is doc-
tor, the domains for senses 1 and 4 are, respec-
tively, Medicine and School. Therefore, if one of
the context words is university, the resulting weight
for doctor(4) and university(3) is 1/4 ? 1/3.
The sense disambiguation of an adjective is per-
formed only on the basis of the above weights.
Given one of its senses, we extract the synsets ob-
tained by the similar to, pertainym and attribute
relationships. For each of them, we calculate the
MDW with respect to the senses of the context
noun. The weight assigned to the adjective sense
is the average between these MDWs. The se-
lected sense is the one having the maximum average
weight.
The sense disambiguation of a verb is done nearly
in the same way, but taking into consideration only
the MDWs with the context words. In the all-words
task the context words are the noun before and af-
ter the verb, whereas in the lexical sample task the
context words are four (two before and two after the
verb), without regard to their morphological cate-
gory. This has been done in order to improve the
recall in the latter task, for which the test corpus is
made up mostly by verbs.
The sense disambiguation of adverbs (in both
tasks) is carried out in the same way of the disam-
biguation of verbs for the lexical sample task.
3 Tasks Processing
We have selected several combinations of such sys-
tems described before for two voting systems, one
for the Lexical-Sample task and the other for the
All-Words task.
3.1 English Lexical Sample Task
At the English Lexical Sample task we combined
the answers of four systems: Relevant Domains,
CIAOSENSO, LVQ-JA ?EN-ELS and Maximum En-
tropy.
The four methods worked in parallel and their
sets of answers were the input of a majority voting
procedure. This procedure selected those answers
with more systems agreements. In case of tie we
gave priority to supervised systems.
With this voting system we obtained around a
63% precision and a 52% recall.
3.2 English All Words Task
For this task we used a voting system combining
the results of Relevant Domains, Maximum En-
tropy, CIAOSENSO and UPV-SHMM-AW. So we
obtained the final results after 10 steps.
Step 1, we selected those answers with agree-
ment between ME and UPV-SHMM-AW (super-
vised systems).
Step 2, from no agreement in step 1 we selected
those answers with agreement between ME and Rel-
evant Domains.
Step 3, from no agreement in step 2 we selected
those answers with agreement between ME and
CIAOSENSO.
Step 4, from no agreement in step 3 we se-
lected those answers with agreement between
CIAOSENSO and UPV-SHMM-AW.
Step 5, from no agreement in step 4 we se-
lected those answers with agreement between UPV-
SHMM-AW and Relevant Domains.
Step 6, from no agreement in step 5 we selected
those answers with agreement between Relevant
Domains and CIAOSENSO.
Step 7, from no agreement in step 6 we selected
Maximum Entropy answers.
Step 8, from the remaining unlabeled instances
we selected UPV-SHMM-AW answers.
Step 9, from the remaining unlabeled instances
we selected Relevant Domains answers.
Step 10, from the remaining unlabeled instances
we selected CIAOSENSO answers.
Last step was labeling with the most frequent
sense in WordNet those instances that had been not
tagged by any system, but in view of the final results
only two instances had not answer and we didn?t
find them in WordNet.
With this voting system preference was given to
supervised systems over unsupervised systems.
We obtained around a 63% precision and a 63%
recall.
4 Conclusions
This paper presents the main characteristics of
the Maximum Entropy, LVQ-JAEN-ELS, UPV-
SHMM-AW, Relevant Domains and CIAOSENSO
systems within the framework of SENSEVAL-3 En-
glish Lexical Sample and All Words tasks. These
systems are combined with a voting technique ob-
taining a promising results for English All Words
and English Lexical Sample tasks.
References
Eneko Agirre and German Rigau. 1995. A pro-
posal for word sense disambiguation using Con-
ceptual Distance. In Proceedings of the Interna-
tional Conference ?Recent Advances in Natural
Language Processing? (RANLP95).
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Manuel Garc??a-Vega, Mar??a Teresa Mart??n-
Valdivia, and Luis Alfonso Uren?a. 2003.
Aprendizaje competitivo lvq para la desam-
biguacio?n le?xica. Revista de la Sociedad
Espaola para el Procesamiento del Lenguaje
Natural, 31:125?132.
H. Tolga Ilhan, Sepandar D. Kamvar, Dan Klein,
Christopher D. Manning, and Kristina Toutanova.
2001. Combining Heterogeneous Classifiers for
Word-Sense Disambiguation. In Judita Preiss
and David Yarowsky, editors, Proceedings of the
2nd International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-2),
pages 87?90, Toulouse, France, July. ACL-
SIGLEX.
T. Kohonen. 1995. Self-organization and associa-
tive memory. 2nd Ed. Springer Verlag, Berlin.
R. Lau, R. Rosenfeld, and S. Roukos. 1993.
Adaptative statistical language modeling using
the maximum entropy principle. In Proceedings
of the Human Language Technology Workshop,
ARPA.
Dekang Lin. 1998. Dependency-based evaluation
of minipar. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, First Inter-
national Conference on Language Resources and
Evaluation, Granada, Spain.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating Subject Field Codes into WordNet. In
M. Gavrilidou, G. Crayannis, S. Markantonatu,
S. Piperidis, and G. Stainhaouer, editors, Pro-
ceedings of LREC-2000, Second International
Conference on Language Resources and Evalu-
ation, pages 1413?1418, Athens, Greece.
Bernardo Magnini and C. Strapparava. 2000. Ex-
periments in Word Domain Disambiguation for
Parallel Texts. In Proceedings of the ACL Work-
shop on Word Senses and Multilinguality, Hong
Kong, China.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cambridge,
Massachusetts.
Llu??s Ma`rquez, Fco. Javier Raya, John Car-
roll, Diana McCarthy, Eneko Agirre, David
Mart??nez, Carlo Strapparava, and Alfio
Gliozzo. 2003. Experiment A: several all-words
WSD systems for English. Technical Report
WP6.2, MEANING project (IST-2001-34460),
http://www.lsi.upc.es/?nlp/meaning/meaning.html.
George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J. Miller.
1993a. Five Papers on WordNet. Special Issue of
the International journal of lexicography, 3(4).
George A. Miller, C. Leacock, R. Tengi, and
T. Bunker. 1993b. A Semantic Concordance. In
Proceedings of ARPA Workshop on Human Lan-
guage Technology, pages 303?308, Plainsboro,
New Jersey.
Andre?s Montoyo and Armando Sua?rez. 2001.
The University of Alicante word sense disam-
biguation system. In Judita Preiss and David
Yarowsky, editors, Proceedings of the 2nd In-
ternational Workshop on Evaluating Word Sense
Disambiguation Systems (SENSEVAL-2), pages
131?134, Toulouse, France, July. ACL-SIGLEX.
Andre?s Montoyo, Sonia Va?zquez, and German
Rigau. 2003. Me?todo de desambiguacio?n le?xica
basada en el recurso le?xico Dominios Rele-
vantes. Procesamiento del Lenguaje Natural, 30,
september.
F. Pla. 2000. Etiquetado Le?xico y Ana?lisis
Sinta?ctico Superficial basado en Modelos Es-
tad??sticos. Tesis doctoral, Departamento de Sis-
temas Informa?ticos y Computacio?n. Universidad
de Polite?cnica de Valencia, Septiembre.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
P. Rosso, F. Masulli, D. Buscaldi, F. Pla, and
A. Molina. 2003. Automatic noun disambigua-
tion. LNCS, Springer Verlag, 2588:273?276.
G. Salton and M.J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill,
New York.
Armando Sua?rez and Manuel Palomar. 2002.
A maximum entropy-based word sense disam-
biguation system. In Hsin-Hsi Chen and Chin-
Yew Lin, editors, Proceedings of the 19th In-
ternational Conference on Computational Lin-
guistics, pages 960?966, Taipei, Taiwan, August.
COLING 2002.
H. van Halteren, J. Zavrel, and W. Daelemans.
2001. Improving accuracy in wordclass tag-
ging through combination of machine learning
systems. Computational Linguistics, 27(2):199?
230.
The ?Meaning? System on the English Allwords Task
L. Villarejo   , L. Ma`rquez   , E. Agirre  , D. Mart??nez  , , B. Magnini  ,
C. Strapparava  , D. McCarthy  , A. Montoyo  , and A. Sua?rez 
 
TALP Research Center, Universitat Polite`cnica de Catalunya,  luisv,lluism  @lsi.upc.es
 IXA Group, University of the Basque Country,  eneko,davidm  @si.ehu.es
 ITC-irst (Istituto per la Ricerca Scientifica e Tecnologica),  magnini,strappa  @itc.it
 University of Sussex, dianam@sussex.ac.uk
 LSI, University of Alicante, montoyo@dlsi.ua.es,armando.suarez@ua.es
1 Introduction
The ?Meaning? system has been developed within
the framework of the Meaning European research
project1 . It is a combined system, which integrates
several supervised machine learning word sense
disambiguation modules, and several knowledge?
based (unsupervised) modules. See section 2 for de-
tails. The supervised modules have been trained ex-
clusively on the SemCor corpus, while the unsuper-
vised modules use WordNet-based lexico?semantic
resources integrated in the Multilingual Central
Repository (MCR) of the Meaning project (Atserias
et al, 2004).
The architecture of the system is quite simple.
Raw text is passed through a pipeline of linguis-
tic processors (tokenizers, POS tagging, named en-
tity extraction, and parsing) and then a Feature Ex-
traction module codifies examples with features ex-
tracted from the linguistic annotation and MCR.
The supervised modules have priority over the un-
supervised and they are combined using a weighted
voting scheme. For the words lacking training ex-
amples, the unsupervised modules are applied in a
cascade sorted by decreasing precision. The tuning
of the combination setting has been performed on
the Senseval-2 allwords corpus.
Several research groups have been providers of
resources and tools, namely: IXA group from the
University of the Basque Country, ITC-irst (?Is-
tituto per la Ricerca Scientifica e Tecnologica?),
University of Sussex (UoS), University of Alicante
(UoA), and TALP research center at the Technical
University of Catalonia. The integration was carried
out by the TALP group.
2 The WSD Modules
We have used up to seven supervised learning sys-
tems and five unsupervised WSD modules. Some
of them have also been applied individually to the
1Meaning, Developing Multilingual Web-scale Lan-
guage Technologies (European Project IST-2001-34460):
http://www.lsi.upc.es/  nlp/meaning/meaning.html.
Senseval-3 lexical sample and allwords tasks.
 Naive Bayes (NB) is the well?known Bayesian
algorithm that classifies an example by choos-
ing the class that maximizes the product, over
all features, of the conditional probability of
the class given the feature. The provider of this
module is IXA. Conditional probabilities were
smoothed by Laplace correction.
 Decision List (DL) are lists of weighted clas-
sification rules involving the evaluation of one
single feature. At classification time, the algo-
rithm applies the rule with the highest weight
that matches the test example (Yarowsky,
1994). The provider is IXA and they also ap-
plied smoothing to generate more robust deci-
sion lists.
 In the Vector Space Model method (cosVSM),
each example is treated as a binary-valued fea-
ture vector. For each sense, one centroid vec-
tor is obtained from training. Centroids are
compared with the vectors representing test ex-
amples, using the cosine similarity function,
and the closest centroid is used to classify the
example. No smoothing is required for this
method provided by IXA.
 Support Vector Machines (SVM) find the hy-
perplane (in a high dimensional feature space)
that separates with maximal distance the pos-
itive examples from the negatives, i.e., the
maximal margin hyperplane. Providers are
TALP (SVM  ) and IXA (SVM 	 ) groups. Both
used the freely available implementation by
(Joachims, 1999), linear kernels, and one?vs?
all binarization, but with different parameter
tuning and feature filtering.
 Maximum Entropy (ME) are exponential
conditional models parametrized by a flexible
set of features. When training, an iterative opti-
mization procedure finds the probability distri-
bution over feature coefficients that maximizes
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the entropy on the training data. This system is
provided by UoA.
 AdaBoost (AB) is a method for learning an en-
semble of weak classifiers and combine them
into a strong global classification rule. We
have used the implementation described in
(Schapire and Singer, 1999) with decision trees
of depth fixed to 3. The provider of this system
is TALP.
 Domain Driven Disambiguation (DDD) is an
unsupervised method that makes use of do-
main information in order to solve lexical am-
biguity. The disambiguation of a word in
its context is mainly a process of compari-
son between the domain of the context and
the domains of the word?s senses (Magnini
et al, 2002). ITC-irst provided two variants
of the system DDD   and DDD  , aiming at
maximizing precision and F  score, respec-
tively. The UoA group also provided another
domain?based unsupervised classifier (DOM).
Their approach exploits information contained
in glosses of WordNet Domains and introduces
a new lexical resource ?Relevant Domains? ob-
tained from Association Ratio over glosses of
WordNet Domains.
 Automatic Predominant Sense (autoPS) pro-
vide an unsupervised first sense heuristic for
the polysemous words in WordNet. This
is produced by UoS automatically from the
BNC (McCarthy et al, 2004). The method
uses automatically acquired thesauruses for the
main PoS categories. The nearest neighbors
for each word are related to its WordNet senses
using a WordNet similarity measure.
 We also used a Most Frequent Sense tagger,
according to the WordNet ranking of senses
(MFS).
3 Evaluation of Individual Modules
For simplicity, and also due to time constraints, the
supervised modules were trained exclusively on the
SemCor-1.6 corpus, intentionally avoiding the use
of other sources of potential training examples, e.g,
other corpora, WordNet examples and glosses, sim-
ilar/substitutable examples extracted from the same
Semcor-1.6, etc. An independent training set was
generated for each polysemous word (of a certain
part?of?speech) with 10 or more examples in the
SemCor-1.6 corpus. This makes a total of 2,440 in-
dependent learning problems, on which all super-
vised WSD systems were trained.
The feature representation of the training exam-
ples was shared between all learning modules. It
consists of a rich feature representation obtained
using the Feature Extraction module of the TALP
team in the Senseval-3 English lexical sample task.
The feature set includes the classic window?based
pattern features extracted from a local context and
the ?bag?of?words? type of features taken from a
broader context. It also contains a set of features
representing the syntactic relations involving the
target word, and semantic features of the surround-
ing words extracted from the MCR of the Meaning
project. See (Escudero et al, 2004) for more details
on the set of features used.
The validation corpus for these classifiers was the
Senseval-2 allwords dataset, which contains 2,473
target word occurrences. From those, 2,239 occur-
rences correspond to polysemous words. We will
refer to this subcorpus as S2-pol. Only 1,254 words
from S2-pol were actually covered by the classifiers
trained on the SemCor-1.6 corpus. We will refer to
this subset of words as the S2-pol-sup corpus. The
conversion between WordNet-1.6 synsets (SemCor-
1.6) and WordNet-1.7 (Senseval-2) was performed
on the output of the classifiers by applying an auto-
matically derived mapping provided by TALP2.
Table 1 shows the results (precision and cover-
age) obtained by the individual supervised modules
on the S2-pol-sup subcorpus, and by the unsuper-
vised modules on the S2-pol subcorpus (i.e., we
exclude from evaluation the monosemous words).
Support Vector Machines and AdaBoost are the best
performing methods, though all of them perform in
a small accuracy range from 53.4% to 59.5%.
Regarding the unsupervised methods, DDD is
clearly the best performing method, achieving a re-
markable precision of 61.9% with the DDD   vari-
ant, at a cost of a lower coverage. The DDD  ap-
pears to be the best system for augmenting the cov-
erage of the former. Note that the autoPS heuristic
for ranking senses is a more precise estimator than
the WordNet most?frequent?sense (MFS).
4 Integration of WSD modules
All the individual modules have to be integrated in
order to construct a complete allwords WSD sys-
tem. Following the architecture described in section
1, we decided to apply the unsupervised modules
only to the subset of the corpus not covered by the
training examples. Some efforts on applying the
unsupervised modules jointly with the supervised
failed at improving accuracy. See an example in ta-
ble 3.
2http://www.lsi.upc.es/  nlp/tools/mapping.html
supervised, S2-pol-sup corpus unsupervised, S2-pol corpus
SVM   AB cosVSM SVM  ME NB DL DDD  DDD  autoPS MFS DOM
prec. 59.5 59.1 57.8 57.1 56.3 54.6 53.4 61.9 50.2 45.2 32.5 23.8
cov. 100.0 100.0 100.0 100.0 100.0 100.0 100.0 48.8 99.6 89.6 98.0 49.1
Table 1: Results of individual supervised and unsupervised WSD modules
As a first approach, we devised three baseline
systems (Base-1, Base-2, and Base-3), which use
the best modules available in both subsets. Base-1
applies the SVM  supervised method and the MFS
for the non supervised part. Base-2 applies also the
SVM  supervised method and the cascade DDD   ?
MFS for the non supervised part (MFS is used in the
cases in which DDD   abstains). Base-3 shares the
same approach but uses a third unsupervised mod-
ule: DDD   ?DDD  ?MFS.
The precision results of the baselines systems can
be found in the right hand side of table 3. As it can
be observed, the positive contribution of the DDD  
module is very significant since Base-2 performs
2.2 points higher than Base-1. The addition of the
third unsupervised module (DDD   ) makes Base-3
to gain 0.4 extra precision points.
As simple combination schemes we considered
majority voting and weighted voting. More sophis-
ticated combination schemes are difficult to tune
due to the extreme data sparseness on the valida-
tion set. In the case of unsupervised systems, these
combination schemes degraded accuracy because
the least accurate systems perform much worse that
the best ones. Thus, we simply decided to apply a
cascade of unsupervised modules sorted by preci-
sion on the Senseval-2 corpus.
In the case of the supervised classifiers there is a
chance of improving the global performance, since
there are several modules performing almost as well
as the best. Previous to the experiments, we cal-
culated the agreement rates on the outputs of each
pair of systems (low agreements increase the prob-
ability of uncorrelatedness between errors of differ-
ent systems). We obtained an average agreement of
83.17%, with values between 64.7% (AB vs SVM 	 )
and 88.4% (SVM 	 vs cosVSM).
The ensembles were obtained by incrementally
aggregating, to the best performing classifier, the
classifiers from a list sorted by decreasing accu-
racy. The ranking of classifiers can be performed
by evaluating them at different levels of granular-
ity: from particular words to the overall accuracy
on the whole validation set. The level of granularity
defines a tradeoff between classifier specialization
and risk of overfitting to the tuning corpus. We de-
cided to take an intermediate level of granularity,
and sorted the classifiers according to their perfor-
mance on word sets based on the number of training
examples available3 .
Table 2 contains the results of the ranking exper-
iment, by considering five word-sets of increasing
number of training examples: between 10 and 20,
between 21 and 40, between 41 and 80, etc. At each
cell, the accuracy value is accompanied by the rel-
ative position the system achieves in that particu-
lar subset. Note that the resulting orderings, though
highly correlated, are quite different from the one
derived from the overall results.
(10,20) (21,40) (41,80) (81,160)  160
SVM  60.9-1 59.1-1 64.2-2 61.1-2 56.4-1
AB 60.9-1 56.6-2 60.0-7 64.7-1 56.1-2
c-VSM 59.9-2 56.6-2 62.6-3 57.0-4 55.8-3
SVM  50.8-5 55.1-4 61.6-4 57.4-3 53.1-5
ME 56.7-3 55.3-3 65.3-1 53.3-5 53.8-4
NB 59.9-2 54.6-5 61.1-5 49.2-6 51.5-7
DL 56.4-4 49.9-6 60.5-6 47.2-7 52.5-6
Table 2: Results on frequency?based word sets
Table 3 shows the precision results4 of the Mean-
ing system obtained on the whole Senseval-2 corpus
by combining from 1 to 7 supervised classifiers ac-
cording to the classifier orderings of table 2 for each
subset of words. The unsupervised classifiers are
all applied in a cascade sorted by precision. M-Vot
stands for a majority voting scheme, while W-Vot
refers to the weighted voting scheme. The weights
for the classifiers are simply the accuracy values on
the validation corpus. As an additional example,
the column M-Vot+ shows the results of the vot-
ing scheme when the unsupervised DDD   module
is also included in the ensemble. The table also in-
cludes the baseline results.
Unfortunately, the ensembles of classifiers did
not provide significant improvements on the final
precision. Only in the case of weighted voting a
slight improvement is observed when adding up to
3 classifiers. From the fourth classifier performance
also degrades. The addition of unsupervised sys-
tems to the supervised ensemble systematically de-
graded performance.
As a reference, the best result (67.5% precision
3One of the factors that differentiates between learning al-
gorithms is the amount of training examples needed to learn.
4Coverage of the combined systems is 98% in all cases.
M-Vot W-Vot M-Vot+ Base-1 Base-2 Base-3
1 67.3 67.3 66.4 ? ? ?
2 ? 67.4 66.3 ? ? ?
3 67.2 67.5 67.1 ? ? ?
4 ? 67.1 66.9 ? ? ?
5 66.5 66.5 66.7 ? ? ?
6 ? 66.3 66.3 ? ? ?
7 65.7 65.9 66.0 ? ? ?
best 67.3 67.5 67.1 64.8 67.0 67.4
Table 3: Results of the combination of systems
System prec. recall F 
Meaning-c 61.1% 61.0% 61.05
Meaning-wv 62.5% 62.3% 62.40
Table 4: Results on the Senseval-3 test corpus
and 98.0% coverage) would have put our combined
system in second place in the Senseval-2 allwords
task.
5 Evaluation on the Senseval-3 Corpus
The Senseval-3 test set contains 2,081 target words,
1,851 of them polysemous. The subset covered by
the SemCor-1.6 training contains 1,211 target words
(65.42%, compared to the 56.0% of the Senseval-2
corpus). We submitted the outputs of two different
configurations of the Meaning system: Meaning-
c and Meaning-wv. These systems correspond to
Base-3 and W-Vot (in the best configuration) from
table 3, respectively. The results from the official
evaluation are given in table 4. Again, we applied an
automatic mapping from WordNet-1.6 to WordNet-
1.7.1 synset labels. However, there are senses in
1.7.1 that do not exist in 1.6, thus our system sim-
ply cannot assign them.
It can be observed that, even though on the tun-
ing corpus both variants obtained very similar pre-
cision (67.4 and 67.5), on the test set the weighted
voting scheme is clearly better than the baseline sys-
tem, probably due to the robustness achieved by the
ensemble. The performance decrease observed on
the test set with respect to the Senseval-2 corpus is
very significant (   5 points). Given that the baseline
system performs worse than the voted approach, it
seems unlikely that there is overfitting during the
ensemble tuning. However, we plan to repeat the
tuning experiments directly on the Senseval-3 cor-
pus to see if the same behavior and conclusions
are observed. Probably, the decrease in perfor-
mance is due to the differences between the train-
ing and test corpora. We intend to investigate the
differences between SemCor-1.6, Senseval-2, and
Senseval-3 corpora at different levels of linguistic
information in order to check the appropriateness of
using SemCor-1.6 as the main information source.
6 Acknowledgements
This research has been possible thanks to the sup-
port of European and Spanish research projects:
IST-2001-34460 (Meaning), TIC2000-0335-C03-
02 (Hermes). The authors would like to thank also
Gerard Escudero for letting us use the Feature Ex-
traction module and German Rigau for helpful sug-
gestions and comments.
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and P. Vossen. 2004. The
Meaning multilingual central repository. In Pro-
ceedings of the Second International WordNet
Conference.
G. Escudero, L. Ma`rquez, and G. Rigau. 2004.
TALP system for the english lexical sample task.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
T. Joachims. 1999. Making large?scale SVM learn-
ing practical. In B. Scho?lkopf, C. J. C. Burges,
and A. J. Smola, editors, Advances in Kernel
Methods ? Support Vector Learning, pages 169?
184. MIT Press, Cambridge, MA.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
D. McCarthy, R. Koeling, J. Weeds, and J. Car-
roll. 2004. Using automatically acquired pre-
dominant senses for word sense disambiguation.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
R. Schapire and Y. Singer. 1999. Improved boost-
ing algorithms using confidence?rated predic-
tions. Machine Learning, 37(3):297?336.
David Yarowsky. 1994. Decision lists for lexi-
cal ambiguity resolution: Application to accent
restoration in Spanish and French. In Proceed-
ings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 88?95,
Las Cruces, NM.
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157?160,
Prague, June 2007. c?2007 Association for Computational Linguistics
GPLSI: Word Coarse-grained Disambiguation aided by Basic Level
Concepts?
Rube?n Izquierdo Armando Sua?rez
GPLSI Group, DLSI
University of Alicante
Spain
{ruben, armando}@dlsi.ua.es
German Rigau
IXA NLP Group
EHU/UPV
Donostia, Basque Country
german.rigau@ehu.es
Abstract
We present a corpus-based supervised lear-
ning system for coarse-grained sense disam-
biguation. In addition to usual features for
training in word sense disambiguation, our
system also uses Base Level Concepts au-
tomatically obtained from WordNet. Base
Level Concepts are some synsets that gene-
ralize a hyponymy sub?hierarchy, and pro-
vides an extra level of abstraction as well as
relevant information about the context of a
word to be disambiguated. Our experiments
proved that using this type of features re-
sults on a significant improvement of preci-
sion. Our system has achieved almost 0.8 F1
(fifth place) in the coarse?grained English
all-words task using a very simple set of fea-
tures plus Base Level Concepts annotation.
1 Introduction
The GPLSI system in SemEval?s task 7, coarse?
grained English all-words, consists of a corpus-
based supervised-learning method which uses lo-
cal context information. The system uses Base Le-
vel Concepts (BLC) (Rosch, 1977) as features. In
short, BLC are synsets of WordNet (WN) (Fell-
baum, 1998) that are representative of a certain hy-
ponymy sub?hierarchy. The synsets that are se-
lected to be BLC must accomplish certain condi-
tions that will be explained in next section. BLC
?This paper has been supported by the European Union un-
der the project QALL-ME (FP6 IST-033860) and the Spanish
Government under the project Text-Mess (TIN2006-15265-
C06-01) and KNOW (TIN2006-15049-C03-01)
are slightly different from Base Concepts of Eu-
roWordNet1 (EWN) (Vossen et al, 1998), Balkanet2
or Meaning Project3 because of the selection crite-
ria but also because our method is capable to define
them automatically. This type of features helps our
system to achieve 0.79550 F1 (over the First?Sense
baseline, 0.78889) while only four systems outper-
formed ours being the F1 of the best one 0.83208.
WordNet has been widely criticised for being a
sense repository that often offers too fine?grained
sense distinctions for higher level applications like
Machine Translation or Question & Answering. In
fact, WSD at this level of granularity, has resisted
all attempts of inferring robust broad-coverage mo-
dels. It seems that many word?sense distinctions are
too subtle to be captured by automatic systems with
the current small volumes of word?sense annotated
examples. Possibly, building class-based classifiers
would allow to avoid the data sparseness problem of
the word-based approach.
Thus, some research has been focused on deri-
ving different sense groupings to overcome the fine?
grained distinctions of WN (Hearst and Schu?tze,
1993) (Peters et al, 1998) (Mihalcea and Moldo-
van, 2001) (Agirre et al, 2003) and on using predefi-
ned sets of sense-groupings for learning class-based
classifiers for WSD (Segond et al, 1997) (Ciaramita
and Johnson, 2003) (Villarejo et al, 2005) (Curran,
2005) (Ciaramita and Altun, 2006). However, most
of the later approaches used the original Lexico-
graphical Files of WN (more recently called Super-
1http://www.illc.uva.nl/EuroWordNet/
2http://www.ceid.upatras.gr/Balkanet
3http://www.lsi.upc.es/ nlp/meaning
157
senses) as very coarse?grained sense distinctions.
However, not so much attention has been paid on
learning class-based classifiers from other available
sense?groupings such as WordNet Domains (Mag-
nini and Cavaglia, 2000), SUMO labels (Niles and
Pease, 2001), EuroWordNet Base Concepts or Top
Concept Ontology labels (Atserias et al, 2004). Ob-
viously, these resources relate senses at some level
of abstraction using different semantic criteria and
properties that could be of interest for WSD. Pos-
sibly, their combination could improve the overall
results since they offer different semantic perspecti-
ves of the data. Furthermore, to our knowledge, to
date no comparative evaluation have been performed
exploring different sense?groupings.
This paper is organized as follows. In section 2,
we present a method for deriving fully automatica-
lly a number of Base Level Concepts from any WN
version. Section 3 shows the details of the whole
system and finally, in section 4 some concluding re-
marks are provided.
2 Automatic Selection of Base Level
Concepts
The notion of Base Concepts (hereinafter BC) was
introduced in EWN. The BC are supposed to be the
concepts that play the most important role in the va-
rious wordnets4 (Fellbaum, 1998) of different lan-
guages. This role was measured in terms of two
main criteria:
? A high position in the semantic hierarchy;
? Having many relations to other concepts;
Thus, the BC are the fundamental building blocks
for establishing the relations in a wordnet and give
information about the dominant lexicalization pat-
terns in languages. BC are generalizations of featu-
res or semantic components and thus apply to a ma-
ximum number of concepts. Thus, the Lexicografic
Files (or Supersenses) of WN could be considered
the most basic set of BC.
Basic Level Concepts (Rosch, 1977) should not
be confused with Base Concepts. BLC are the result
of a compromise between two conflicting principles
of characterization:
4http://wordnet.princeton.edu
#rel. synset
18 group 1,grouping 1
19 social group 1
37 organisation 2,organization 1
10 establishment 2,institution 1
12 faith 3,religion 2
5 Christianity 2,church 1,Christian church 1
#rel. synset
14 entity 1,something 1
29 object 1,physical object 1
39 artifact 1,artefact 1
63 construction 3,structure 1
79 building 1,edifice 1
11 place of worship 1, ...
19 church 2,church building 1
#rel. synset
20 act 2,human action 1,human activity 1
69 activity 1
5 ceremony 3
11 religious ceremony 1,religious ritual 1
7 service 3,religious service 1,divine service 1
1 church 3,church service 1
Table 1: Possible Base Level Concepts for the noun
Church
? Represent as many concepts as possible;
? Represent as many features as possible;
As a result of this, Basic Level Concepts typically
occur in the middle of hierarchies and less than the
maximum number of relations. BC mostly involve
the first principle of the Basic Level Concepts only.
Our work focuses on devising simple methods for
selecting automatically an accurate set of Basic Le-
vel Concepts from WN. In particular, our method se-
lects the appropriate BLC of a particular synset con-
sidering the relative number of relations encoded in
WN of their hypernyms.
The process follows a bottom-up approach using
the chain of hypernym relations. For each synset
in WN, the process selects as its Base Level Con-
cept the first local maximum according to the rela-
tive number of relations. For synsets having multi-
ple hypernyms, the path having the local maximum
with higher number of relations is selected. Usually,
this process finishes having a number of ?fake? Base
Level Concepts. That is, synsets having no descen-
dants (or with a very small number) but being the
first local maximum according to the number of re-
lations considered. Thus, the process finishes che-
cking if the number of concepts subsumed by the
158
Senses BLC SuperSenses
Nouns 4.92 4.10 3.01
Verbs 11.00 8.67 1.03
Nouns + Verbs 7.66 6.16 3.47
Table 2: Polysemy degree over SensEval?3
preliminary list of BLC is higher than a certain th-
reshold. For those BLC not representing enough
concepts according to a certain threshold, the pro-
cess selects the next local maximum following the
hypernym hierarchy.
An example is provided in table 1. This table
shows the possible BLC for the noun ?church? using
WN1.6. The table presents the hypernym chain for
each synset together with the number of relations en-
coded in WN for the synset. The local maxima along
the hypernym chain of each synset appears in bold.
Table 2 presents the polysemy degree for nouns
and verbs of the different words when grouping its
senses with respect the different semantic classes on
SensEval?3. Senses stand for the WN senses, BLC
for the Automatic BLC derived using a threshold of
20 and SuperSenses for the Lexicographic Files of
WN.
3 The GPLSI system
The GPLSI system uses a publicly available imple-
mentation of Support Vector Machines, SVMLight5
(Joachims, 2002), and Semcor as learning corpus.
Semcor has been properly mapped and labelled with
both BLC6 and sense-clusters.
Actually, the process of training-classification has
two phases: first, one classifier is trained for each
possible BLC class and then the SemEval test data
is classified and enriched with them, and second, a
classifier for each target word is built using as addi-
tional features the BLC tags in Semcor and SemE-
val?s test.
Then, the features used for training the classifiers
are: lemmas, word forms, PoS tags7, BLC tags, and
first sense class of target word (S1TW). All features
5http://svmlight.joachims.org/
6Because BLC are automatically defined from WN, some tu-
ning must be performed due to the nature of the task 7. We have
not enough room to present the complete study but threshold 20
has been chosen, using SENSEVAL-3 English all-words as test
data. Moreover, our tests showed roughly 5% of improvement
against not using these features.
7TreeTagger (Schmid, 1994) was used
were extracted from a window [?3.. + 3] except for
the last type (S1TW). The reason of using S1TW
features is to assure the learning of the baseline. It is
well known that Semcor presents a higher frequency
on first senses (and it is also the baseline of the task
finally provided by the organizers).
Besides, these are the same features for both first
and second phases (obviously except for S1TW be-
cause of the different target set of classes). Nevert-
heless, the training in both cases are quite different:
the first phase is class-based while the second is
word-based. By word-based we mean that the lear-
ning is performed using just the examples in Semcor
that contains the target word. We obtain one classi-
fier per polysemous word are in the SemEval test
corpus. The output of these classifiers is a sense-
cluster. In class-based learning all the examples in
Semcor are used, tagging those ones belonging to a
specific class (BLC in our case) as positive exam-
ples while the rest are tagged as negatives. We ob-
tain so many binary classifiers as BLC are in Se-
mEval test corpus. The output of these classifiers
is true or false, ?the example belongs to a class?
or not. When dealing with a concrete target word,
only those BLC classifiers that are related to it are
?activated? (i.e, ?animal? classifier will be not used
to classify ?church?), ensuring that the word will be
tagged with coherent labels. In order to avoid statis-
tical bias because of very large set of negative exam-
ples, the features are defined from positive examples
only (although they are obviously used to characte-
rize all the examples).
4 Conclusions and further work
The WSD task seems to have reached its maxi-
mum accuracy figures with the usual framework.
Some of its limitations could come from the sense?
granularity of WN. In particular, SemEval?s coarse-
grained English all-words task represents a solution
in this direction.
Nevertheless, the task still remains oriented to
words rather than classes. Then, other problems
arise like data sparseness just because the lack of
adequate and enough examples. Changing the set of
classes could be a solution to enrich training corpora
with many more examples Another option seems to
be incorporating more semantic information.
159
Base Level Concepts (BLC) are concepts that are
representative for a set of other concepts. A simple
method for automatically selecting BLC from WN
based on the hypernym hierarchy and the number of
stored relationships between synsets have been used
to define features for training a supervised system.
Although in our system BLC play a simple role
aiding to the disambiguation just as additional fea-
tures, the good results achieved with such simple
features confirm us that an appropriate set of BLC
will be a better semantic discriminator than senses
or even sense-clusters.
References
E. Agirre, I. Aldezabal, and E. Pociello. 2003. A pi-
lot study of english selectional preferences and their
cross-lingual compatibility with basque. In Procee-
dings of the International Conference on Text Speech
and Dialogue (TSD?2003), CeskBudojovice, Czech
Republic.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The meaning mul-
tilingual central repository. In Proceedings of Global
WordNet Conference (GWC?04), Brno, Czech Repu-
blic.
M. Ciaramita and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction with
a supersense sequence tagger. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?06), pages 594?602, Syd-
ney, Australia. ACL.
M. Ciaramita and M. Johnson. 2003. Supersense tagging
of unknown nouns in wordnet. In Proceedings of the
Conference on Empirical methods in natural language
processing (EMNLP?03), pages 168?175. ACL.
J. Curran. 2005. Supersense tagging of unknown nouns
using semantic similarity. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics (ACL?05), pages 26?33. ACL.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
M. Hearst and H. Schu?tze. 1993. Customizing a lexicon
to better suit a computational task. In Proceedingns
of the ACL SIGLEX Workshop on Lexical Acquisition,
Stuttgart, Germany.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer Academic
Publishers.
B. Magnini and G. Cavaglia. 2000. Integrating subject
fields codes into wordnet. In Proceedings of the Se-
cond International Conference on Language Resour-
ces and Evaluation (LREC?00).
R. Mihalcea and D. Moldovan. 2001. Automatic ge-
neration of coarse grained wordnet. In Proceding of
the NAACL workshop on WordNet and Other Lexical
Resources: Applications, Extensions and Customiza-
tions, Pittsburg, USA.
I. Niles and A. Pease. 2001. Towards a standard up-
per ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001), pages 17?19. Chris Welty and Ba-
rry Smith, eds.
W. Peters, I. Peters, and P. Vossen. 1998. Automatic
sense clustering in eurowordnet. In First Internatio-
nal Conference on Language Resources and Evalua-
tion (LREC?98), Granada, Spain.
E. Rosch. 1977. Human categorisation. Studies in
Cross-Cultural Psychology, I(1):1?49.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of NemLap-
94, pages 44?49, Manchester, England.
F. Segond, A. Schiller, G. Greffenstette, and J. Chanod.
1997. An experiment in semantic tagging using hid-
den markov model tagging. In ACL Workshop on Au-
tomatic Information Extraction and Building of Lexi-
cal Semantic Resources for NLP Applications, pages
78?81. ACL, New Brunswick, New Jersey.
L. Villarejo, L. Ma`rquez, and G. Rigau. 2005. Explo-
ring the construction of semantic class classifiers for
wsd. In Proceedings of the 21th Annual Meeting of
Sociedad Espaola para el Procesamiento del Lenguaje
Natural SEPLN?05, pages 195?202, Granada, Spain,
September. ISSN 1136-5948.
P. Vossen, L. Bloksma, H. Rodriguez, S. Climent, N. Cal-
zolari, A. Roventini, F. Bertagna, A. Alonge, and
W. Peters. 1998. The eurowordnet base concepts and
top ontology. Technical report, Paris, France, France.
160
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 402?406,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
GPLSI-IXA: Using Semantic Classes to Acquire Monosemous Training
Examples from Domain Texts
Rub
?
en Izquierdo & Armando Su
?
arez
GPLSI Group
University of Alicante. Spain
{ruben,armando}@dlsi.ua.es
German Rigau
IXA NLP Group.
EHU. Donostia, Spain
german.rigau@ehu.es
Abstract
This paper summarizes our participation
in task #17 of SemEval?2 (All?words
WSD on a specific domain) using a su-
pervised class-based Word Sense Disam-
biguation system. Basically, we use Sup-
port Vector Machines (SVM) as learning
algorithm and a set of simple features to
build three different models. Each model
considers a different training corpus: Sem-
Cor (SC), examples from monosemous
words extracted automatically from back-
ground data (BG), and both SC and
BG (SCBG). Our system explodes the
monosemous words appearing as mem-
bers of a particular WordNet semantic
class to automatically acquire class-based
annotated examples from the domain text.
We use the class-based examples gathered
from the domain corpus to adapt our tra-
ditional system trained on SemCor. The
evaluation reveal that the best results are
achieved training with SemCor and the
background examples from monosemous
words, obtaining results above the first
sense baseline and the fifth best position
in the competition rank.
1 Introduction
As empirically demonstrated by the last SensEval
and SemEval exercises, assigning the appropriate
meaning to words in context has resisted all at-
tempts to be successfully addressed. In fact, super-
vised word-based WSD systems are very depen-
dent of the corpora used for training and testing
the system (Escudero et al, 2000). One possible
reason could be the use of inappropriate level of
abstraction.
Most supervised systems simply model each
polysemous word as a classification problem
where each class corresponds to a particular synset
of the word. But, WordNet (WN) has been widely
criticized for being a sense repository that often
provides too fine?grained sense distinctions for
higher level applications like Machine Translation
or Question & Answering. In fact, WSD at this
level of granularity has resisted all attempts of in-
ferring robust broad-coverage models. It seems
that many word?sense distinctions are too subtle
to be captured by automatic systems with the cur-
rent small volumes of word?sense annotated ex-
amples.
Thus, some research has been focused on deriv-
ing different word-sense groupings to overcome
the fine?grained distinctions of WN (Hearst and
Sch?utze, 1993), (Peters et al, 1998), (Mihalcea
and Moldovan, 2001), (Agirre and LopezDeLa-
Calle, 2003), (Navigli, 2006) and (Snow et al,
2007). That is, they provide methods for grouping
senses of the same word, thus producing coarser
word sense groupings for better disambiguation.
In contrast, some research have been focused on
using predefined sets of sense-groupings for learn-
ing class-based classifiers for WSD (Segond et al,
1997), (Ciaramita and Johnson, 2003), (Villarejo
et al, 2005), (Curran, 2005), (Kohomban and Lee,
2005) and (Ciaramita and Altun, 2006). That is,
grouping senses of different words into the same
explicit and comprehensive semantic class. Most
of the later approaches used the original Lexico-
graphical Files of WN (more recently called Su-
perSenses) as very coarse?grained sense distinc-
tions.
We suspect that selecting the appropriate level
of abstraction could be on between both levels.
Thus, we use the semantic classes modeled by the
Basic Level Concepts
1
(BLC) (Izquierdo et al,
2007). Our previous research using BLC empiri-
cally demonstrated that this automatically derived
1
http://adimen.si.ehu.es/web/BLC
402
set of meanings groups senses into an adequate
level of abstraction in order to perform class-based
Word Sense Disambiguation (WSD) (Izquierdo et
al., 2009). Now, we also show that class-based
WSD allows to successfully incorporate monose-
mous examples from the domain text. In fact,
the robustness of our class-based WSD approach
is shown by our system that just uses the Sem-
Cor examples (SC). It performs without any kind
of domain adaptation as the Most Frequent Sense
(MFS) baseline.
This paper describes our participation in
SemEval-2010 Task 17 (Agirre et al, 2010). In
section 2 semantic classes used and selection al-
gorithm used to obtain them automatically from
WordNet are described. In section 3 the technique
employed to extract monosemous examples from
background data is described. Section 4 explains
the general approach of our system, and the ex-
periments designed, and finally, in section 5, the
results and some analysis are shown.
2 Semantic Classes
The set of semantic classes used in this work are
the Basic Level Concepts
2
(BLC) (Izquierdo et
al., 2007). These concepts are small sets of mean-
ings representing the whole nominal and verbal
part of WN. BLC can be obtained by a very simple
method that uses basic structural WordNet proper-
ties. In fact, the algorithm only considers the rel-
ative number of relations of each synset alng the
hypernymy chain. The process follows a bottom-
up approach using the chain of hypernymy rela-
tions. For each synset in WN, the process selects
as its BLC the first local maximum according to
the relative number of relations. The local maxi-
mum is the synset in the hypernymy chain having
more relations than its immediate hyponym and
immediate hypernym. For synsets having multi-
ple hypernyms, the path having the local maxi-
mum with higher number of relations is selected.
Usually, this process finishes having a number of
preliminary BLC. Figure 1 shows an example of
selection of a BLC. The figure represents the hy-
pernymy hierarchy of WordNet, with circles rep-
resenting synsets, and links between them repre-
senting hypernym relations. The algorithm selects
the D synset as BLC for J, due to D is the first
maximum in the hypernymy chain, according to
the number of relations (F has 2 hyponyms, D has
2
http://adimen.si.ehu.es/web/BLC
3, and A has 2, so D is the first maximum).
I
D
G H
C
A
J
F
2
3
2
B
E
2
B L C
S y n s e t
Figure 1: Example of BLC selection
Obviously, while ascending through this chain,
more synsets are subsumed by each concept. The
process finishes checking if the number of con-
cepts subsumed by the preliminary list of BLC is
higher than a certain threshold. For those BLC
not representing enough concepts according to the
threshold, the process selects the next local max-
imum following the hypernymy hierarchy. Thus,
depending on the type of relations considered to
be counted and the threshold established, different
sets of BLC can be easily obtained for each WN
version.
We have selected the set which considers WN
version 3.0, the total number of relations per
synset, and a minimum threshold of 20 concepts to
filter out not representative BLC (BLC?20). This
set has shown to reach good performance on previ-
ous SensEval and SemEval exercices (Izquierdo et
al., 2009). There are 649 different BLC for nouns
on WordNet 3.0, and 616 for verbs. Table 2 shows
the three most frequent BLC per POS, with the
number of synsets subsumed by each concept, and
its WordNet gloss.
3 Using Monosemous Examples from the
Domain
We did not applied any kind of specific domain
adaptation technique to our class-based supervised
system. In order to adapt our supervised system to
the environmental domain we only increased the
training data with new examples of the domain. To
acquire these examples, we used the environmen-
tal domain background documents provided by the
organizers. Specifically, we used the 122 back-
403
PoS Num. BLC Gloss
Nouns
4.792 person.n.01 a human being
1.935 activity.n.01 any specific behavior
1.846 act.n.02 something that people do or cause to happen
Verbs
1.541 change.v.01 cause to change; make different; cause a transformation
1.085 change.v.02 undergo a change; become different in essence; losing one?s or its original na-
ture
519 move.v.02 cause to move or shift into a new position or place, both in a concrete and in an
abstract sense
Table 1: Most frequent BLC?20 semantic classes on WordNet 3.0
ground documents
3
. TreeTagger has been used
to preprocess the documents, performing PoS tag-
ging and lemmatization. Since the background
documents are not semantically annotated, and our
supervised system needs labeled data, we have se-
lected only the monosemous words occurring in
the documents. In this way, we have obtained au-
tomatically a large set of examples annotated with
BLC. Table 3 presents the total number of training
examples extracted from SemCor (SC) and from
the background documents (BG). As expected, by
this method a large number of monosemous ex-
amples can be obtained for nouns and verbs. Also
as expected, verbs are much less productive than
nouns. However, all these background examples
correspond to a reduced set of 7,646 monosemous
words.
Nouns Verbs N+V
SC 87.978 48.267 136.245
BG 193.536 10.821 204.357
Total 281.514 59.088 340.602
Table 2: Number of training examples
Table 3 lists the ten most frequent monosemous
nouns and verbs occurring in the background doc-
uments. Note that all these examples are monose-
mous according to BLC?20 semantic classes.
Nouns Verbs
Lemma # ex. Lemma # ex.
1 biodiversity 7.476 monitor 788
2 habitat 7.206 achieve 784
3 specie 7.067 target 484
4 climate 3.539 select 345
5 european 2.818 enable 334
6 ecosystem 2.669 seem 287
7 river 2.420 pine 281
8 grassland 2.303 evaluate 246
9 datum 2.276 explore 200
10 directive 2.197 believe 172
Table 3: Most frequent monosemic words in BG
3
We used the documents contained on the trial data and
the background.
4 System Overview
Our system applies a supervised machine learn-
ing approach. We apply a feature extractor to
represent the training examples of the examples
acquired from SemCor and the background doc-
uments. Then, a machine learning engine uses
the annotated examples to train a set of classi-
fiers. Support Vector Machines (SVM) have been
proven to be robust and very competitive in many
NLP tasks, and in WSD in particular (M`arquez et
al., 2006). We used the SVM-Light implementa-
tion
4
(Joachims, 1998).
We create a classifier for each semantic class.
This approach has several advantages compared to
word?based approach. The training data per clas-
sifier is increased (we can use examples of dif-
ferent target words for a single classifier, when-
ever all examples belong to the same semantic
class), the polysemy is reduced (some different
word senses can be collapsed into the same se-
mantic class), and, finally, semantic classes pro-
vide higher levels of abstraction.
For each polysemous word occurring in the test
corpus, we obtain its potential BLC?20 classes.
Then, we only apply the classifiers corresponding
to the BLC-20 classes of the polysemous word. Fi-
nally, our system simply selects the BLC?20 class
with the greater prediction.
In order to obtain the correct WordNet 3.0
synset required by the task, we apply a simple
heuristic that has shown to be robust and accurate
(Kohomban and Lee, 2005). Our classifiers ob-
tain first the semantic class, and then, the synset of
the first WordNet sense that fits with the semantic
class is assigned to the word.
We selected a simple feature set widely used in
many WSD systems. In particular, we use a win-
dow of five tokens around the target word to ex-
tract word forms, lemmas; bigrams and trigrams
of word forms and lemmas; trigrams of PoS tags,
4
http://svmlight.joachims.org
404
and also the most frequent BLC?20 semantic class
of the target word in the training corpus.
Our system is fully described in (Izquierdo et
al., 2009). The novelty introduced here is the use
of semantic classes to obtain monosemous exam-
ples from the domain corpus.
Following the same framework (BLC?20 se-
mantic architecture and basic set of features) we
designed three runs, each one using a different
training corpus.
? SC: only training examples extracted from
SemCor
? BG: only monosemous examples extracted
from the background data
? SCBG: training examples extracted from
SemCor and monosemous background data
The first run shows the behavior of a supervised
system trained on a general corpus, and tested in a
specific domain. The second one analyzes the con-
tribution of the monosemous examples extracted
from the background data. Finally, the third run
studies the robustness of the approach when com-
bining the training examples from SemCor and
from the background.
5 Results and Discussion
A total of 29 runs has been submitted for the En-
glish All?words WSD on a Specific Domain. Ta-
ble 5 shows the ranking results of our three runs
with respect to the other participants. The figures
for the first sense (1sense) and random sense (Ran-
dom) baselines are included.
In general, the results obtained are not very
high. The best system only achieves a precision of
0.570, and the first sense baseline reaches a preci-
sion of 0.505. This shows that the task is hard to
solve, and the domain adaptation of WSD systems
is not an easy task.
Interestingly, our worst result is obtained by the
system using only the monosemous background
examples (BG). This system ranks 23th with a Pre-
cision and Recall of 0.380 (0.385 for nouns and
0.366 for verbs). The system using only SemCor
(SC) ranks 6th with Precision and Recall of 0.505
(0.527 for nouns and 0.443 for verbs). This is also
the performance of the first sense baseline. As ex-
pected, the best result of our three runs is obtained
when combining the examples from SemCor and
the background (SCBG). This supervised system
obtains the 5th position with a Precision and Re-
call of 0.513 (0.534 for nouns, 0.454 for verbs)
which is slightly above the baseline.
Rank Precision Recall
1 0.570 0.555
2 0.554 0.540
3 0.534 0.528
4 0.522 0.516
(SCBG) 5 0.513 0.513
1sense 0.505 0.505
(SC) 6 0.505 0.505
7 0.512 0.495
8 0.506 0.493
9 0.504 0.491
10 0.481 0.481
11 0.492 0.479
12 0.461 0.460
13 0.447 0.441
14 0.436 0.435
15 0.440 0.434
16 0.496 0.433
17 0.498 0.432
18 0.433 0.431
19 0.426 0.425
20 0.424 0.422
21 0.437 0.392
22 0.384 0.384
(BG) 23 0.380 0.380
24 0.381 0.356
25 0.351 0.350
26 0.370 0.345
27 0.328 0.322
28 0.321 0.315
29 0.312 0.303
Random 0.230 0.230
Table 4: Results of task#17
Possibly, the reason of low performance of the
BG system is the high correlation between the fea-
tures of the target word and its semantic class. In
this case, these features correspond to the monose-
mous word while when testing corresponds to the
target word. However, it also seems that class-
based systems are robust enough to incorporate
large sets of monosemous examples from the do-
main text. In fact, to our knowledge, this is the first
time that a supervised WSD algorithm have been
successfully adapted to an specific domain. Fur-
thermore, our system trained only on SemCor also
achieves a good performance, reaching the first
sense baseline, showing that class-based WSD ap-
proaches seem to be robust to domain variations.
Acknowledgments
This paper has been supported by the Euro-
pean Union under the project KYOTO (FP7 ICT-
211423), the Valencian Region Government un-
der PROMETEO project for excellence groups
and the Spanish Government under the projects
405
KNOW2 (TIN2009-14715-C04-04) and TEXT-
MESS-2 (TIN2009-13391-C04-04).
References
E. Agirre and O. LopezDeLaCalle. 2003. Clustering
wordnet word senses. In Proceedings of RANLP?03,
Borovets, Bulgaria.
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluations (SemEval-2010), Association for Com-
putational Linguistics.
M. Ciaramita and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP?06), pages 594?602,
Sydney, Australia. ACL.
M. Ciaramita and M. Johnson. 2003. Supersense tag-
ging of unknown nouns in wordnet. In Proceedings
of the Conference on Empirical methods in natural
language processing (EMNLP?03), pages 168?175.
ACL.
J. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL?05), pages 26?33. ACL.
G. Escudero, L. M`arquez, and G. Rigau. 2000. An
Empirical Study of the Domain Dependence of Su-
pervised Word Sense Disambiguation Systems. In
Proceedings of the joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing
and Very Large Corpora, EMNLP/VLC, Hong Kong,
China.
M. Hearst and H. Sch?utze. 1993. Customizing a lexi-
con to better suit a computational task. In Proceed-
ingns of the ACL SIGLEX Workshop on Lexical Ac-
quisition, Stuttgart, Germany.
R. Izquierdo, A. Suarez, and G. Rigau. 2007. Explor-
ing the automatic selection of basic level concepts.
In Galia Angelova et al, editor, International Con-
ference Recent Advances in Natural Language Pro-
cessing, pages 298?302, Borovets, Bulgaria.
Rub?en Izquierdo, Armando Su?arez, and German Rigau.
2009. An empirical study on class-based word sense
disambiguation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 389?397, Athens, Greece, March. As-
sociation for Computational Linguistics.
T. Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant
features. In Claire N?edellec and C?eline Rouveirol,
editors, Proceedings of ECML-98, 10th European
Conference on Machine Learning, pages 137?142,
Chemnitz, DE. Springer Verlag, Heidelberg, DE.
Upali S. Kohomban and Wee Sun Lee. 2005. Learning
semantic classes for word sense disambiguation. In
ACL ?05: Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
34?41, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Ll. M`arquez, G. Escudero, D. Mart??nez, and G. Rigau.
2006. Supervised corpus-based methods for wsd. In
E. Agirre and P. Edmonds (Eds.) Word Sense Disam-
biguation: Algorithms and applications., volume 33
of Text, Speech and Language Technology. Springer.
R. Mihalcea and D. Moldovan. 2001. Automatic gen-
eration of coarse grained wordnet. In Proceding of
the NAACL workshop on WordNet and Other Lex-
ical Resources: Applications, Extensions and Cus-
tomizations, Pittsburg, USA.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In ACL-44: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 105?112, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
W. Peters, I. Peters, and P. Vossen. 1998. Automatic
sense clustering in eurowordnet. In First Interna-
tional Conference on Language Resources and Eval-
uation (LREC?98), Granada, Spain.
F. Segond, A. Schiller, G. Greffenstette, and J. Chanod.
1997. An experiment in semantic tagging using hid-
den markov model tagging. In ACL Workshop on
Automatic Information Extraction and Building of
Lexical Semantic Resources for NLP Applications,
pages 78?81. ACL, New Brunswick, New Jersey.
R. Snow, Prakash S., Jurafsky D., and Ng A. 2007.
Learning to merge word senses. In Proceedings of
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 1005?
1014.
L. Villarejo, L. M`arquez, and G. Rigau. 2005. Ex-
ploring the construction of semantic class classi-
fiers for wsd. In Proceedings of the 21th Annual
Meeting of Sociedad Espaola para el Procesamiento
del Lenguaje Natural SEPLN?05, pages 195?202,
Granada, Spain, September. ISSN 1136-5948.
406
