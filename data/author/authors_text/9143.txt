Evaluation of Restricted Domain Question-Answering Systems 
Anne R. Diekema, Ozgur Yilmazel, and Elizabeth D. Liddy 
Center for Natural Language Processing 
School of Information Studies 
Syracuse University 
4-206 Center for Science and Technology 
Syracuse, NY 13244 
{diekemar,liddy,oyilmaz}@syr.edu 
 
 
Abstract 
Question-Answering (QA) evaluation efforts 
have largely been tailored to open-domain 
systems. The TREC QA test collections contain 
newswire articles and the accompanying queries 
cover a wide variety of topics. While some 
apprehension about the limitations of restricted-
domain systems is no doubt justified, the strict 
promotion of unlimited domain QA evaluations 
may have some unintended consequences. 
Simply applying the open domain QA evaluation 
paradigm to a restricted-domain system poses 
problems in the areas of test question 
development, answer key creation, and test 
collection construction. This paper examines the 
evaluation requirements of restricted domain 
systems. It incorporates evaluation criteria 
identified by users of an operational QA system 
in the aerospace engineering domain. While the 
paper demonstrates that user-centered task-based 
evaluations are required for restricted domain 
systems, these evaluations are found to be 
equally applicable to open domain systems. 
1 Introduction 
The Text REtrieval Conference (TREC) 
organized the first QA evaluation (QA track) in 
1999 (Voorhees, 2000) and annual evaluations of 
this nature are ongoing (Voorhees, to appear). 
While the tasks and answer requirements have 
varied slightly from year to year, the purpose 
behind QA evaluations remains the same: to 
move from the traditional document retrieval to 
actual information retrieval by providing an 
answer to a question rather than a ranked list of 
relevant documents. The track was originally 
intended to bring together the fields of 
Information Extraction (IE) and Information 
Retrieval (IR). This legacy still continues in the 
factoid questions that require an IE type answer 
snippet in response, e.g.: ?What country is the 
Aswan High Dam located in?? This style of QA 
evaluation is spreading with very similar 
evaluations in Asia (Fukumoto, Kato, Masui, 
2003) and Europe (Magnini et al, 2003). 
Although these evaluations have a multilingual 
slant, they are strongly modeled after the TREC 
QA track. 
Typical QA systems that participate in these 
evaluations classify the questions into types 
which determine what kind of answer is required. 
After an initial retrieval of documents pertaining 
to the question, some form of text processing is 
then applied to identify possible answer 
sentences in the documents. Sentences that are 
near or contain keywords from the original 
question and contain the desired answer pattern 
are selected for answer extraction. Since it is 
difficult for systems to determine which part of 
the sentence is the correct answer, especially if it 
contains multiple extractions of the desired type, 
many systems have resorted to redundancy 
tactics (Banko et al, 2002; Buchholz, 2002). 
These systems use the Web as an answer 
verification tool by choosing the answer that 
appears most often together with the question 
keywords. While this technique is very 
successful in open domain evaluations, 
restricted-domain systems do not have the luxury 
of using redundancy, making these evaluations 
inappropriate for systems such as these. 
Our QA system participated in the three 
earlier TREC evaluations, e.g. (Diekema et al, 
2002). However, after starting work in the 
restricted-domain of re-usable launch vehicles, 
we found that the TREC evaluation no longer 
suited our system development needs and 
maintaining two different QA systems was too 
costly. 
 
 
2 Restricted-domain system 
characteristics  
The restricted-domain systems of today are 
different from the toy systems from the early 
years of QA (Voorhees and Tice, 2000), which 
might be what first comes to mind when reading 
the term ?restricted-domain?. Early systems like 
LUNAR (with a domain somewhat tangentially 
related to ours, namely lunar archeology) were 
developed by researchers in the field of natural 
language understanding. These early systems 
encoded large amounts of domain knowledge in 
databases. The restricted-domain systems of 
today are far less dependent on large knowledge 
bases and do not aim for language understanding 
per se. Rather, they use specialized extraction 
rules on a domain specific collection. The one 
thing that both types of restricted-domain 
systems have in common is that they are often 
developed with a certain goal or task in mind.  
As we will see later, this task orientation 
becomes equally important in the evaluation of 
these QA systems.  
An example of a modern-day restricted-
domain system is our Knowledge Acquisition 
and Access System (KAAS) QA system. The 
KAAS was developed for use in a collaborative 
learning environment (Advanced Interactive 
Discovery Environment for Engineering 
Education or AIDE) for undergraduate students 
from two universities majoring in aeronautical 
engineering. While students are working within 
the AIDE they can ask questions and quickly get 
answers. The collection against which the 
questions are searched consists of textbooks, 
technical papers, and websites that have been 
pre-selected for relevance and pedagogical value. 
The KAAS system uses a two-stage retrieval 
model to find answers in relevant passages. 
Relevant passages are processed by the Center 
for Natural Language Processing?s eQuery 
information extraction system using additional 
rules in the domain of reusable launch vehicles. 
Users are aided in their question formulations 
through domain specific query expansions. 
 
3 Initiating a restricted domain 
evaluation 
When it came time to evaluate the KAAS 
system, we initially defaulted to the TREC style 
QA evaluation with short, fact-based questions, 
adjudicated answers to these questions, and a test 
collection in which to find those answers. This 
choice of evaluation was not surprising since 
early versions of our system grew out of that 
environment. However, it quickly became 
apparent that this evaluation style posed 
problems for our restricted-domain, specific 
purpose system. 
Developing a set of test questions was easier 
said than done. Unlike the open domain 
evaluations, where test questions can be mined 
from question logs (Encarta, Excite, AskJeeves), 
no question sets are at the disposal of restricted-
domain evaluators. To build a set of test 
questions, we hired two sophomore aerospace 
engineering students. Based on class project 
papers of the previous semester and examples of 
TREC questions, the students were asked to 
create as many short factoid questions as they 
could, i.e ?What is APAS?? However, the real 
user questions that we collected later did not look 
anything like the short test questions in this 
initial evaluation set. The user questions were 
much more complex, e.g. ?How difficult is it to 
mold and shape graphite-epoxies compared with 
alloys or ceramics that may be used for thermal 
protective applications?? A more in depth 
analysis of KAAS question types can be found in 
Diekema et al (to appear). 
Establishing answers for the initial test 
questions proved difficult as well. The students 
did fine at collecting the questions that they had 
while reading the papers, but lacked sufficient 
domain expertise to establish answer correctness. 
Another issue was determining recall because it 
wasn?t always clear whether the (small) corpus 
simply did not contain the answer or whether the 
system was not able to find it. A third student, a 
doctoral student in aerospace engineering, was 
hired to help with these issues. To facilitate 
automatic evaluation we wanted to represent the 
answers in simple patterns but found that 
complex answers are not necessarily suitable for 
such a representation, even though patterns have 
proven feasible for TREC systems.  
While a newswire document collection for 
general domain evaluation is easy to find, a 
collection in our specialized domain needed to be 
created from scratch. Not only did the collection 
of documents take time, the conversion of most 
of these documents to text proved to be quite an 
unexpected hurdle as well. 
As is evident, the TREC style QA evaluation 
did not suit our restricted domain system. It also 
leaves out the user entirely. While information-
based evaluations are necessary to establish the 
ability of the system to answer questions 
correctly, we felt that they were not sufficient for 
evaluating a system with real users. 
4 User-based evaluation dimensions 
Restricted domain systems tend to be situated 
not only within a specific domain, but also within 
a certain user community and within a specific 
task domain. A generic evaluation is neither 
sufficient nor suitable for a restricted domain 
system. The environment in which KAAS is 
situated should drive the evaluation. Unlike 
many of the systems that participate in a TREC 
QA evaluation, the KAAS system has to function 
in real time with real users, not in batch mode 
with surrogate relevance assessors. This brings 
with it additional evaluation criteria such as 
utility and system speed (Nyberg and Mitamura, 
2003). 
KAAS users were asked in two separate 
surveys about their use and experiences with the 
system. The surveys were part of larger scale, 
cross-university course evaluations which looked 
at the students? perceptions of distance learning, 
collaboration at a distance, the collaborative 
software package, the KAAS, and each 
participating faculty member. While there was 
some structure and guidance in the user survey of 
the QA system, it was minimal and the survey is 
mainly characterized by the open nature of the 
responses. There were 25 to 30 students 
participating in each full course survey, but since 
we do not have the actual surveys that were 
turned in, we are not certain as to exactly how 
many students completed the survey section on 
the KAAS. However, it appears that most, if not 
all of the students provided feedback. 
Given the free text nature of the responses, it 
was decided that the three researchers would do a 
content analysis of the responses and 
independently derive a set of evaluation 
dimensions that they detected in the students? 
responses. Through content analysis of the user 
responses and follow-up discussion, we 
identified 5 main areas of importance to KAAS 
users when using the system: system 
performance, answers, database content, display, 
and expectations (see Figure 1). Each of the 
categories will be described in more detail 
below. 
4.1 System Performance 
System Performance is the category that deals 
with system speed and system availability. Users 
indicated that the speed with which answers were 
returned to them mattered. While they did not 
necessarily expect an immediate answer, they 
also did not want to wait, e.g. ?took so long, so I 
gave up?. Whenever users have a question, they 
want to find an answer immediately. If the 
system is down or not available to them at that 
moment, they will not come back later and try 
again. 
Possible system performance metrics are the 
?answer return rate?, and ?up time?. The answer 
return rate measures how long it takes (on 
average) to return an answer after the user has 
submitted a question.  ?Up-time? measures for a 
certain time period how often the system is 
available (system available time divided by the 
length of up-time time period). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 1: User-based evaluation dimensions. 
 
 
1 System Performance 
1.1 Speed 
1.2 Availability / reliability / upness 
2 Answers  
2.1 Completeness 
2.2 Accuracy 
2.3 Relevance 
2.4 Applicability to task / utility / usefulness 
3 Database Content 
3.1 Authority / provenance / Source quality 
3.2 Scope /extensiveness / coverage 
3.3 Size 
3.4 Updatedness 
4 Display (UI) 
4.1 Input 
4.1.1 Question understanding / info need 
understanding 
4.1.2 Querying style 
4.1.2.1 Question 
4.1.2.1.1 NL query 
4.1.2.2 Keywords 
4.1.2.3 Browsing 
4.1.3 Question formulation assistance 
4.1.3.1 Spell Checker 
4.1.3.2 Abbreviation recognition 
4.2 Output 
4.2.1 Organization 
4.2.2 Feedback Solicitation 
5 Expectations 
5.1 Googleness 
 
4.2 Answers  
What users find important in an answer is 
captured in the Answers category. The users not 
only wanted answers to be accurate, they also 
wanted them to be complete and, something that 
is not tested at all in a regular evaluation, 
applicable to their task. e.g. ?in general what I 
received was helpful and accurate?, ?it [the 
system] was useful for the Columbia incident 
exercise??. 
Possible metrics concerning answers are 
?accuracy or correctness?, ?completeness?, 
?relevance?, or ?task suitability?. While the first 
three metrics are used in some shape or form in 
the TREC evaluations, ?task suitability? is not.  
Perhaps this measure requires a certain task 
description with a question to test whether the 
answer provided by the system allowed the user 
to complete the task. 
4.3 Database Content  
Users also shared thoughts about the Database 
Content or source documents that are searched 
for answers. They find it important that these 
documents are reputable. They also shared 
concerns about the size of the database, fearing 
that a limit in size would restrict the number of 
answerable questions, e.g. ?it needs more 
documents?. The same is true for the scope of the 
collection. Users desired extended coverage to 
ensure that a wide range of questions could be 
fielded by the collection, e.g. ?I found the data 
too limited in scope?. 
Possible database content metrics are 
?authority?, ?coverage?, ?size?, and ?up-to-
dateness?. To measure ?authority? one would 
first have to identify the core authors for a 
domain through citation analysis. Once that is 
established, one could measure the percentage of 
database content created by these core 
researchers. ?Coverage? could be measured in a 
similar way after the main research areas within 
a domain are identified.  ?Size? could simply be 
measured in megabytes or gigabytes. ?Up-to-
dateness? could be measured by calculating the 
number of articles per year or simply noting the 
date of the most recent article. 
 
4.4 User Interface 
The User Interface of a system was also found 
of importance. Users were critical about the way 
they were asked to input their questions. They 
did not always want to phrase their question as a 
question but sometimes preferred to use 
keywords, e.g. ?a keyword search would be more 
useful?. They also expected the system to prompt 
them with assistance in case they misspelled 
terms, or when the system did not understand the 
question, e.g ?sometimes very good at correcting 
you to what you need, other times not very 
good?. Users also care about the way in which 
the results are presented to them and whether the 
system desires any additional responses from 
them. They did not like being prompted for 
feedback on a document?s relevance for example, 
e.g. ??the ?was this useful? window was 
disruptive?. 
Measuring UI related aspects can be done 
through observation, questionnaires and 
interviews and does not typically result in actual 
metrics but rather a set of recommendations that 
can be implemented in the next version of the 
system. 
4.5 Expectations 
Another interesting aspect of user criteria is 
Expectations , e.g. ?the documents in the e-Query 
database were useful, but Google is much 
faster?. All users are familiar with Google and 
tend to have little patience with systems that 
have a different look and feel. 
Expectations can be captured by survey so 
that it can be established whether these 
expectations are reasonable and whether they can 
be met.   
5 Restricted domain QA Evaluation  
If we consider a restricted domain QA system 
as a system developed for a certain application, it 
is clear that these systems require a situated 
evaluation. The evaluation has to be situated in 
the task, domain, and user community for which 
the system is developed.  
How then can a restricted domain system best 
be evaluated? We believe that the evaluation 
should be driven by the dimensions identified by 
the users as important: system performance, 
answers, database content, display, and 
expectations. 
The system should be evaluated on its 
performance. How many seconds does it take to 
answer a question? Once the speed is known, one 
can determine how long users are willing to wait 
for an answer. It may very well be that the 
answer-finding capability of a system will need 
to be simplified in order to speed up the system 
and satisfy its users. Similarly, tests to determine 
robustness need to be part of the system 
performance evaluation. Users tend to shy away 
from systems that are periodically unavailable or 
slow to a crawl during peak usage hours.  
Systems should also be evaluated on their 
answer providing ability. This evaluation should 
include measures for answer completeness, 
accuracy, and relevancy. Test questions should 
be within the domain of the QA system in order 
to test the answer quality for that domain. 
Answers to certain questions require a more fine-
grained scoring procedure: answers that are 
explanations or summaries or biographies or 
comparative evaluations cannot be meaningfully 
rated as simply right or wrong. The answer 
providing capability should be evaluated in light 
of the task or purpose of the system. For 
example, users of the KAAS are learners in the 
field and are not well served with exact answer 
snippets. For their task, they need answer context 
information to be able to learn from the answer 
text.  
The evaluation should also include measures 
of the Database Content. Rather than assuming 
relevancy of a collection, it should be evaluated 
whether the content is regularly updated, whether 
the contents are of acceptable quality to the 
users, and whether the coverage of the restricted 
domain is extensive enough. 
Another system component that should be 
evaluated is the User Interface. Is the system 
easy to use? Does the interface provide clear 
guidance and/or assistance to the user? Does it 
allow users to search in multiple ways? 
Finally, it may be pertinent to evaluate how 
far the system goes in living up to user 
expectations. Although it is impossible to satisfy 
everybody, the system developers need to know 
whether there is a large discrepancy between user 
expectations and the actual system, since this 
may influence the use of the system. 
6 Cross-fertilization between evaluations 
How different are restricted-domain 
evaluations from open-domain evaluations? Are 
they so diametrically opposed that restricted-
domain systems require separate evaluations 
from open-domain systems and vice versa? As 
pointed out in Section 1, we stopped 
participating in the TREC QA evaluations 
because that evaluation was not well suited to 
our restricted-domain system. However, we 
regretted this as we believe we could, 
nevertheless, have gained valuable insights. 
Clearly, open-domain systems would benefit 
from the evaluation dimensions discussed in 
Section 4. The difference would be that the test 
questions used for evaluation would be general 
rather than tailored to a specific domain. 
Additionally, it may be harder to evaluate the 
database content (i.e. the collection) for a general 
domain system than would be the case for 
restricted-domain systems.  
To make open-domain evaluations more 
applicable to restricted-domain systems, they 
could be extended to include metrics about 
answer speed, and the ability of answering within 
a certain task. For example, the evaluation could 
include system performance to get an indication 
as to how much processing time, given certain 
hardware, is required in getting the answers. As 
for answer correctness itself, it may be 
interesting to require extensive use of task 
scenarios that would determine aspects such as 
answer length and level of detail. It may also be 
desirable to evaluate runs without redundancy 
techniques separately. Ideally, users would be 
incorporated into the evaluation to assess the user 
interface and the ability of the system to assist 
them in completion of a certain task. 
 
7 Summary 
Restricted-domain systems require a more 
situated evaluation than is generally provided in 
open-domain evaluations. A restricted-domain 
evaluation extends beyond domain specific test 
questions and collections to include the user and 
their task. Users of the restricted-domain KAAS 
system identified five areas that should be 
included in an evaluation: System Performance, 
Answers, Database Content, Display, and 
Expectations. Most of these evaluation 
dimensions could be applied to open-domain 
evaluations as well. Adding system performance 
metrics (such as answer speed) and specific task 
requirements may allow a convergence between 
open domain and restricted domain QA 
evaluations. 
 
Acknowledgements 
Funding for this research has been jointly provided by 
NASA,  NY State, and AT&T. 
 
References  
 
Banko, M., Brill, E., Dumais, S. and Lin, J. 2002.  
AskMSR: Question answering using the worldwide 
Web.  In Proceedings of the 2002 AAAI Spring 
Symposium on Mining Answers from Texts and 
Knowledge Bases, March 2002, Palo Alto, 
California.  
Buchholz, S. 2002. Using Grammatical Relations, 
Answer Frequencies and the World Wide Web for 
TREC Question Answering. In: E. M. Voorhees 
and D. K. Harman (Eds.), The Tenth Text REtrieval 
Conference (TREC 2001), volume 500-250 of 
NIST Special Publication, Gaithersburg, MD. 
National Institute of Standards and Technology, 
2002, pp. 502-509. 
Diekema, A.R., Chen, J., McCracken, N, Ozgencil, 
N.E., Taffet, M.D., Yilmazel, O. and Liddy, E.D. 
2002. Question Answering: CNLP at the TREC-
2002 Question Answering Track. In: Proceedings 
of the Eleventh Text Retrieval Conference (TREC-
2002). E.M. Voorhees and D.K. Harman (Eds.). 
Gaithersburg, MD: Department of Commerce, 
National Institute of Standards and Technology, 
2002. 
Fukumoto, J., Kato, T., and Masui, F. 2003. Question 
Answering Challenge (QAC-1): An Evaluation of 
Question Answering Tasks at the NTCIR 
Workshop 3. In Proceedings of the AAAI Spring 
Symposium: New Directions in Question 
Answering, p.122-133, 2003. 
Diekema, A.R., Yilmazel, O., Chen, J., Harwell, S., 
He, L., and Liddy, E.D. Finding Answers to 
Complex Questions. To appaer. In Maybury, M. 
(Ed.) New Directions in Question Answering. 
AAAI-MIT Press. 
Magnini, B., Romagnoli, S., Vallin, A., Herrera, J. 
Pe?as, A., Peinado, V., Verdejo, F., M. de Rijke, 
The Multiple Language Question Answering Track 
at CLEF 2003. In Carol Peters (Ed.), Working 
Notes for the CLEF 2003 Workshop, 21-22 August, 
Trondheim, Norway, 2003. 
Nyberg E. and T. Mitamura. 2002. Evaluating QA 
Systems on Multiple Dimensions. In Proceedings 
of LREC 2002 Workshop on QA Strategy and 
Resources, May 28th, Las Palmas, Gran Canaria. 
Voorhees, E.M. 2003. DRAFT Overview of the 
TREC 2003 Question Answering Track. To appear 
in Proceedings of TREC 2003. Gaithersburg, MD, 
NIST, to appear. 
Voorhees, E.M. Overview of the TREC-8 Question 
Answering Track Report. In Proceedings of TREC-
8, 77-82. Gaithersburg, MD, NIST, 2000. 
Voorhees, E.M. & Tice, D.M.  Implementing a 
Question Answering Evaluation. In Proceedings of 
LREC?2000 Workshop on Using Evaluation within 
HLT Programs: Results and Trends. 2000. 
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 17?24,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
Modeling Reference Interviews as a Basis for Improving Automatic QA 
Systems 
 
Nancy J. McCracken, Anne R. Diekema, Grant Ingersoll, Sarah C. 
Harwell, Eileen E. Allen, Ozgur Yilmazel, Elizabeth D. Liddy 
Center for Natural Language Processing 
Syracuse University 
Syracuse, NY 13244 
{ njmccrac, diekemar, gsingers, scharwel, eeallen, oyilmaz, liddy}@syr.edu 
 
 
 
 
 
Abstract 
 
The automatic QA system described in 
this paper uses a reference interview 
model to allow the user to guide and 
contribute to the QA process.  A set of 
system capabilities was designed and 
implemented that defines how the user?s 
contributions can help improve the 
system.  These include tools, called the 
Query Template Builder and the 
Knowledge Base Builder, that tailor the 
document processing and QA system to 
a particular domain by allowing a 
Subject Matter Expert to contribute to 
the query representation and to the 
domain knowledge.  During the QA 
process, the system can interact with the 
user to improve query terminology by 
using Spell Checking, Answer Type 
verification, Expansions and Acronym 
Clarifications.  The system also has 
capabilities that depend upon, and 
expand the user?s history of interaction 
with the system, including a User 
Profile, Reference Resolution, and 
Question Similarity modules 
 
 
1  Introduction 
 
Reference librarians have successfully fielded 
questions of all types for years using the Reference 
Interview to clarify an unfocused question, narrow 
a broad question, and suggest further information 
that the user might not have thought to ask for.  
The reference interview tries to elicit sufficient 
information about the user?s real need to enable a 
librarian to understand the question enough to 
begin searching.  The question is clarified, made 
more specific, and contextualized with relevant 
detail.  Real questions from real users are often 
?ill-formed? with respect to the information 
system; that is, they do not match the structure of 
?expectations? of the system (Ross et al, 2002). A 
reference interview translates the user?s question 
into a representation that the librarian and the 
library systems can interpret correctly. The human 
reference interview process provides an ideal, 
well-tested model of how questioner and answerer 
work together co-operatively and, we believe, can 
be successfully applied to the digital environment.  
The findings of researchers applying this model in 
online situations (Bates, 1989, Straw, 2004) have 
enabled us to understand how a system might work 
with the user to provide accurate and relevant 
answers to complex questions. 
 Our long term goal in developing Question-
Answering (QA) systems for various user groups is 
to permit, and encourage users to positively 
contribute to the QA process, to more nearly 
mirror what occurs in the reference interview, and 
to develop an automatic QA system that provides 
fuller, more appropriate, individually tailored 
responses than has been available to date. 
 Building on our Natural Language 
Processing (NLP) experience in a range of 
information access applications, we have focused 
our QA work in two areas:  1) modeling the subject 
domain of the collections of interest to a set of 
 
 
 
17
users for whom we are developing the QA system, 
and; 2) modeling the query clarification and 
negotiation interaction between the information 
seeker and the information provider. Examples of 
these implementation environments are: 
 
1. Undergraduate aerospace engineering students 
working in collaborative teams on course 
projects designing reusable launch vehicles, 
who use a QA system in their course-related 
research. 
2. Customers of online business sites who use a 
QA system to learn more about the products or 
services provided by the company, or who 
wish to resolve issues concerning products or 
service delivery. 
 
In this paper, we describe the capabilities we 
have developed for these specific projects in order 
to explicate a more general picture of how we 
model and utilize both the domains of inquiry and 
typical interaction processes observed in these 
diverse user groups. 
 
2 Background and related research 
 
Our work in this paper is based on two premises: 
1) user questions and responsive answers need to 
be understood within a larger model of the user?s 
information needs and requirements, and, 2) a 
good interactive QA system facilitates a dialogue 
with its users to ensure it understands and satisfies 
these information needs. The first premise is based 
on the long-tested and successful model of the 
reference interview (Bates, 1997, Straw, 2004), 
which was again validated by the findings of an 
ARDA-sponsored workshop to increase the 
research community?s understanding of the 
information seeking needs and cognitive processes 
of intelligence analysts (Liddy, 2003). The second 
premise instantiates this model within the digital 
and distributed information environment. 
 Interactive QA assumes an interaction 
between the human and the computer, typically 
through a combination of a clarification dialogue 
and user modeling to capture previous interactions 
of users with the system. De Boni et al (2005) 
view the clarification dialogue mainly as the 
presence or absence of a relationship between the 
question from the user and the answer provided by 
the system. For example, a user may ask a 
question, receive an answer and ask another 
question in order to clarify the meaning, or, the 
user may ask an additional question which expands 
on the previous answer. In their research De Boni 
et al (2005) try to determine automatically 
whether or not there exists a relationship between a 
current question and preceding questions, and if 
there is a relationship, they use this additional 
information in order to determine the correct 
answer.  
 We prefer to view the clarification dialogue 
as more two-sided, where the system and the user 
actually enter a dialogue, similar to the reference 
interview as carried out by reference librarians 
(Diekema et al, 2004). The traditional reference 
interview is a cyclical process in which the 
questioner poses their question, the librarian (or the 
system) questions the questioner, then locates the 
answer based on information provided by the 
questioner, and returns an answer to the user who 
then determines whether this has satisfied their 
information need or whether further clarification or 
further questions are needed.  The HITIQA 
system?s (Small et al, 2004) view of a clarification 
system is closely related to ours?their dialogue 
aligns the understanding of the question between 
system and user. Their research describes three 
types of dialogue strategies: 1) narrowing the 
dialogue, 2) broadening the dialogue, and 3) a fact 
seeking dialogue. 
 Similar research was carried out by Hori et 
al. (2003), although their system automatically 
determines whether there is a need for a dialogue, 
not the user. The system identifies ambiguous 
questions (i.e. questions to which the system could 
not find an answer). By gathering additional 
information, the researchers believe that the system 
can find answers to these questions. Clarifying 
questions are automatically generated based on the 
ambiguous question to solicit additional 
information from the user. This process is 
completely automated and based on templates that 
generate the questions. Still, removing the 
cognitive burden from the user through automation 
is not easy to implement and can be the cause of 
error or misunderstanding. Increasing user 
involvement may help to reduce this error. 
 As described above, it can be seen that 
interactive QA systems have various levels of 
dialogue automation ranging from fully automatic 
(De Boni et al, 2004, Hori et al, 2004) to a strong 
18
user involvement (Small et al, 2004, Diekema et 
al., 2004). Some research suggests that 
clarification dialogues in open-domain systems are 
more unpredictable than those in restricted domain 
systems, the latter lending itself better to 
automation (Hori et al, 2003, J?nsson et al, 2004).  
Incorporating the user?s inherent knowledge of the 
intention of their query is quite feasible in 
restricted domain systems and should improve the 
quality of answers returned, and make the 
experience of the user a less frustrating one. While 
many of the systems described above are 
promising in terms of IQA, we believe that 
incorporating knowledge of the user in the 
question negotiation dialogue is key to developing 
a more accurate and satisfying QA system.   
 
3 System Capabilities 
 
In order to increase the contribution of users to our 
question answering system, we expanded our 
traditional domain independent QA system by 
adding new capabilities that support system-user 
interaction. 
 
3.1  Domain Independent QA 
 
Our traditional domain-independent QA capability 
functions in two stages, the first information 
retrieval stage selecting a set of candidate 
documents, the second stage doing the answer 
finding within the filtered set.  The answer finding 
process draws on models of question types and 
document-based knowledge to seek answers 
without additional feedback from the user.  Again, 
drawing on the modeling of questions as they 
interact with the domain representation, the system 
returns answers of variable lengths on the fly in 
response to the nature of the question since factoid 
questions may be answered with a short answer, 
but complex questions often require longer 
answers.  In addition, since our QA projects were 
based on closed collections, and since closed 
collections may not provide enough redundancy to 
allow for short answers to be returned, the variable 
length answer capability assists in finding answers 
to factoid questions.  The QA system provides 
answers in the form of short answers, sentences, 
and answer-providing passages, as well as links to 
the full answer-providing documents. The user can 
provide relevance feedback by selecting the full 
documents that offer the best information.  Using 
this feedback, the system can reformulate the 
question and look for a better set of documents 
from which to find an answer to the question. 
Multiple answers can be returned, giving the user a 
more complete picture of the information held 
within the collection.   
 One of our first tactics to assist in both 
question and domain modeling for specific user 
needs was to develop tools for Subject Matter 
Experts (SMEs) to tailor our QA systems to a 
particular domain.  Of particular interest to the 
interactive QA community is the Query Template 
Builder (QTB) and the Knowledge Base Builder 
(KBB).  
 Both tools allow a priori alterations to 
question and domain modeling for a community, 
but are not sensitive to particular users.  Then the 
interactive QA system permits question- and user-
specific tailoring of system behavior simply 
because it allows subject matter experts to change 
the way the system understands their need at the 
time of the search. 
 Question Template Builder (QTB) allows 
a subject matter expert to fine tune a question 
representation by adding or removing stopwords 
on a question-by-question basis, adding or masking 
expansions, or changing the answer focus.  The 
QTB displays a list of Question-Answer types, 
allows the addition of new Answer Types, and 
allows users to select the expected answer type for 
specific questions.  For example, the subject matter 
expert may want to adjust particular ?who? 
questions as to whether the expected answer type is 
?person? or ?organization?.  The QTB enables 
organizations to identify questions for which they 
want human intervention and to build specialized 
term expansion sets for terms in the collection.  
They can also adjust the stop word list, and refine 
and build the Frequently or Previously Asked 
Question (FAQ/PAQ) collection. 
 Knowledge Base Builder (KBB) is a suite 
of tools developed for both commercial and 
government customers.  It allows the users to view 
and extract terminology that resides in their 
document collections.  It provides useful statistics 
about the corpus that may indicate portions that 
require attention in customization.  It collects 
frequent / important terms with categorizations to 
enable ontology building (semi-automatic, 
permitting human review), term collocation for use 
19
in identifying which sense of a word is used in the 
collection for use in term expansion and 
categorization review.  KBB allows companies to 
tailor the QA system to the domain vocabulary and 
important concept types for their market.  Users 
are able to customize their QA applications 
through human-assisted automatic procedures.  
The Knowledge Bases built with the tools are  
 
 
IR Answer Providers
Question 
Processing
Session 
Tracking
Reference 
Resolution
User Profile
Question 
Similarity
User
Answer
Spell 
checking
Answer 
Type 
Verification
Expansion 
Clarification
Domain Modeling
QTB KBB
 
Figure 1. System overview 
 
 
primarily lexical semantic taxonomic resources.  
These are used by the system in creating frame 
representations of the text.  Using automatically 
harvested data, customers can review and alter 
categorization of names and entities and expand 
the underlying category taxonomy to the domain of 
interest.  For example, in the NASA QA system, 
experts added categories like ?material?, ?fuel?, 
?spacecraft? and ?RLV?, (Reusable Launch 
Vehicles).  They also could specify that ?RLV? is a 
subcategory of ?spacecraft? and that space shuttles 
like ?Atlantis? have category ?RLV?.  The KBB 
works in tandem with the QTB, where the user can 
find terms in either documents or example queries 
 
3.2 Interactive QA Development 
 
In our current NASA phase, developed for 
undergraduate aerospace engineering students to 
quickly find information in the course of their 
studies on reusable launch vehicles, the user can 
view immediate results, thus bypassing the 
Reference Interviewer, or they may take the 
opportunity to utilize its increased functionality 
and interact with the QA system. The capabilities 
we have developed, represented by modules added 
to the system, fall into two groups. Group One 
includes capabilities that draw on direct interaction 
with the user to clarify what is being asked and that 
address terminological issues.  It includes Spell 
Checking, Expansion Clarification, and Answer 
Type Verification. Answers change dynamically as 
the user provides more input about what was 
meant. Group Two capabilities are dependent 
upon, and expand upon the user?s history of 
interaction with the system and include User 
Profile, Session Tracking, Reference Resolution, 
Question Similarity and User Frustration 
Recognition modules.  These gather knowledge 
about the user, help provide co-reference 
resolution within an extended dialogue, and 
monitor the level of frustration a user is 
experiencing.   
20
 The capabilities are explained in greater 
detail below.  Figure 1 captures the NASA system 
process and flow.  
 
Group One: 
  
In this group of interactive capabilities, after the 
user asks a query, answers are returned as in a 
typical system.  If the answers presented aren?t 
satisfactory, the system will embark on a series of 
interactive steps (described below) in which 
alternative spelling, answertypes, clarifications and 
expansions will be suggested.   The user can 
choose from the system?s suggestions or type in 
their own.  The system will then revise the query 
and return a new set of answers.  If those answers 
aren?t satisfactory, the user can continue 
interacting with the system until appropriate 
answers are found. 
Spell checking: Terms not found in the 
index of the document collection are displayed as 
potentially misspelled words.  In this preliminary 
phase, spelling is checked and users have the 
opportunity to select correct and/or alternative 
spellings.  
 AnswerType verification: The interactive 
QA system displays the type of answer that the 
system is looking for in order to answer the 
question.  For example for the question, Who 
piloted the first space shuttle?, the answer type is 
?person?, and the system will limit the search for 
candidate short answers in the collection to those 
that are a person?s name.  The user can either 
accept the system?s understanding of the question 
or reject the type it suggests.  This is particularly 
useful in semantically ambiguous questions such as 
?Who makes Mountain Dew?? where the system 
might interpret the question as needing a person, 
but the questioner actually wants the name of a 
company.  
Expansion:  This capability allows users to 
review the possible relevant terms (synonyms and 
group members) that could enhance the question-
answering process.  The user can either select or 
deselect terms of interest which do or do not 
express the intent of the question.  For example, if 
the user asks: How will aerobraking change the 
orbit size? then the system can bring back the 
following expansions for ?aerobraking?:  By 
aerobraking do you mean the following: 1) 
aeroassist, 2) aerocapture, 3) aeromaneuvering, 4) 
interplanetary transfer orbits, or 5) transfer orbits. 
Acronym Clarification: For abbreviations 
or acronyms within a query, the full explications 
known by the system for the term can be displayed 
back to the user.  The clarifications implemented 
are a priori limited to those that are relevant to the 
domain.  In the aerospace domain for example, if 
the question was What is used for the TPS of the 
RLV?, the clarifications of TPS would be thermal 
protection system, thermal protection subsystem, 
test preparation sheet, or twisted pair shielded, and 
the clarification of RLV would be reusable launch 
vehicle.  The appropriate clarifications can be 
selected to assist in improving the search.  For a 
more generic domain, the system would offer 
broader choices.  For example, if the user types in 
the question: What educational programs does the 
AIAA offer?, then the system might return: By 
AIAA, do you mean (a) American Institute of 
Aeronautics and Astronautics (b) Australia 
Indonesia Arts Alliance or (c) Americans for 
International Aid & Adoption?   
 
Group Two: 
 
User Profile: The User Profile keeps track of more 
permanent information about the user.  The profile 
includes a small standard set of user attributes, 
such as the user?s name and / or research interests.  
In our commercially funded work, selected 
information gleaned from the question about the 
user was also captured in the profile.  For example, 
if a user asks ?How much protein should my 
husband be getting every day??, the fact that the 
user is married can be added to their profile for 
future marketing, or for a new line of dialogue to 
ask his name or age.  This information is then 
made available as context information for the QA 
system to resolve references that the user makes to 
themselves and their own attributes.  
 For the NASA question-answering 
capability, to assist students in organizing their 
questions and results, there is an area for users to 
save their searches as standing queries, along with 
the results of searching (Davidson, 2006).  This 
information, representing topics and areas of 
interest, can help to focus answer finding for new 
questions the user asks. 
Not yet implemented, but of interest, is the 
ability to save information such as a user?s 
21
preferences (format, reliability, sources), that could 
be used as filters in the answer finding process. 
 Reference Resolution:  A basic feature of 
an interactive QA system is the requirement to 
understand the user?s questions and responsive 
answers as one session. The sequence of questions 
and answers forms a natural language dialogue 
between the user and the system.  This necessitates 
NLP processing at the discourse level, a primary 
task of which is to resolve references across the 
session.  Building on previous work in this area 
done for the Context Track of TREC 2001 
(Harabagiu et al 2001) and additional work (Chai 
and Jin, 2004) suggesting discourse structures are 
needed to understand the question/answer 
sequence, we have developed session-based 
reference resolution capability. In a dialogue, the 
user naturally includes referring phrases that 
require several types of resolution. 
 The simplest case is that of referring 
pronouns, where the user is asking a follow-up 
question, for example: 
 
Q1:  When did Madonna enter the music business? 
A1:  Madonna's first album, Madonna, came out in 
1983 and since then she's had a string of hits, been 
a major influence in the music industry and 
become an international icon. 
Q2:  When did she first move to NYC? 
 
In this question sequence, the second 
question contains a pronoun, ?she?, that refers to 
the person ?Madonna? mentioned both in the 
previous question and its answer.    Reference 
resolution would transform the question into 
?When did Madonna first move to NYC?? 
Another type of referring phrase is the 
definite common noun phrase, as seen in the next 
example: 
 
Q1: If my doctor wants me to take Acyclovir, is it 
expensive?  
A1:  Glaxo-Wellcome, Inc., the company that 
makes Acyclovir, has a program to assist 
individuals that have HIV and Herpes.  
Q2:  Does this company have other assistance 
programs? 
 
The second question has a definite noun 
phrase ?this company? that refers to ?Glaxo-
Wellcome, Inc.? in the previous answer, thus 
transforming the question to ?Does Glaxo-
Wellcome, Inc. have other assistance programs?? 
Currently, we capture a log of the 
question/answer interaction, and the reference 
resolution capability will resolve any references in 
the current question that it can by using linguistic 
techniques on the discourse of the current session.  
This is almost the same as the narrative 
coreference resolution used in documents, with the 
addition of the need to understand first and second 
person pronouns from the dialogue context.  The 
coreference resolution algorithm is based on 
standard linguistic discourse processing techniques 
where referring phrases and candidate resolvents 
are analyzed along a set of features that typically 
includes gender, animacy, number, person and the 
distance between the referring phrase and the 
candidate resolvent. 
Question Similarity: Question Similarity is 
the task of identifying when two or more questions 
are related.  Previous studies (Boydell et al, 2005, 
Balfe and Smyth, 2005) on information retrieval 
have shown that using previously asked questions 
to enhance the current question is often useful for 
improving results among like-minded users.  
Identifying related questions is useful for finding 
matches to Frequently Asked Questions (FAQs) 
and Previously Asked Questions (PAQs) as well as 
detecting when a user is failing to find adequate 
answers and may be getting frustrated.  
Furthermore, similar questions can be used during 
the reference interview process to present 
questions that other users with similar information 
needs have used and any answers that they 
considered useful.   
CNLP?s question similarity capability 
comprises a suite of algorithms designed to 
identify when two or more questions are related.  
The system works by analyzing each query using 
our Language-to-Logic (L2L) module to identify 
and weight keywords in the query, provide 
expansions and clarifications, as well as determine 
the focus of the question and the type of answer the 
user is expecting (Liddy et al, 2003).  We then 
compute a series of similarity measures on two or 
more L2L queries.  Our measures adopt a variety 
of approaches, including those that are based on 
keywords in the query: cosine similarity, keyword 
string matching, expansion analysis, and spelling 
variations.  In addition, two measures are based on 
the representation of the whole query:answer type 
22
and answer frame analysis. An answer frame is our 
representation of the meaningful extractions 
contained in the query, along with metadata about 
where they occur and any other extractions that 
relate to in the query. 
Our system will then combine the weighted 
scores of two or more of these measures to 
determine a composite score for the two queries, 
giving more weight to a measure that testing has 
determined to be more useful for a particular task. 
We have utilized our question similarity 
module for two main tasks.  For FAQ/PAQ (call it 
XAQ) matching, we use question similarity to 
compare the incoming question with our database 
of XAQs.  Through empirical testing, we 
determined a threshold above which we consider 
two questions to be similar. 
Our other use of question similarity is in the 
area of frustration detection.  The goal of 
frustration detection is to identify the signs a user 
may be giving that they are not finding relevant 
answers so that the system can intervene and offer 
alternatives before the user leaves the system, such 
as similar questions from other users that have 
been successful.    
 
4 Implementations:  
 
The refinements to our Question Answering 
system and the addition of interactive elements 
have been implemented in three different, but 
related working systems, one of which is strictly an 
enhanced IR system.  None of the three 
incorporates all of these capabilities.  In our work 
for MySentient, Ltd, we developed the session-
based reference resolution capability, implemented 
the variable length and multiple answer capability, 
modified our processing to facilitate the building 
of a user profile, added FAQ/PAQ capability, and 
our Question Similarity capability for both 
FAQ/PAQ matching and frustration detection.  A 
related project, funded by Syracuse Research 
Corporation, extended the user tools capability to 
include a User Interface for the KBB and basic 
processing technology.  Our NASA project has 
seen several phases.  As the project progressed, we 
added the relevant developed capabilities for 
improved performance.  In the current phase, we 
are implementing the capabilities which draw on 
user choice.  
 
5 Conclusions and Future Work 
 
 The reference interview has been 
implemented as an interactive dialogue between 
the system and the user, and the full system is near 
completion. We are currently working on two 
types of evaluation of our interactive QA 
capabilities. One is a system-based evaluation in 
the form of unit tests, the other is a user-based 
evaluation. The unit tests are designed to verify 
whether each module is working correctly and 
whether any changes to the system adversely affect 
results or performance. Crafting unit tests for 
complex questions has proved challenging, as no 
gold standard for this type of question has yet been 
created.  As the data becomes available, this type 
of evaluation will be ongoing and part of regular 
system development. 
 As appropriate for this evolutionary work 
within specific domains for which there are not 
gold standard test sets, our evaluation of the QA 
systems has focused on qualitative assessments. 
What has been a particularly interesting outcome is 
what we have learned in elicitation from graduate 
students using the NASA QA system, namely that 
they have multiple dimensions on which they 
evaluate a QA system, not just traditional recall 
and precision (Liddy et al 2004). The high level 
dimensions identified include system performance, 
answers, database content, display, and 
expectations. Therefore the evaluation criteria we 
believe appropriate for IQA systems are centered 
around the display (UI) category as described in 
Liddy et al (2004).  We will evaluate aspects of 
the UI input subcategory, including question 
understanding, information need understanding, 
querying style, and question formulation 
assistance. Based on this user evaluation the 
system will be improved and retested.   
 
 
References 
 
Evelyn Balfe and Barry Smyth. 2005. An Analysis 
of Query Similarity in Collaborative Web 
Search. In Proceedings of the 27th European 
Conference on Information Retrieval. Santiago 
de Compostela, Spain. 
 
23
Marcia J. Bates. 1989. The Design of Browsing 
and Berrypicking Techniques for the Online 
Search Interface.  Online Review, 13: 407-424. 
 
Mary Ellen Bates. 1997. The Art of the Reference 
Interview.  Online World. September 15. 
 
Ois?n Boydell, Barry Smyth, Cathal Gurrin, and 
Alan F. Smeaton. 2005. A Study of Selection 
Noise in Collaborative Web Search. In 
Proceedings of the 19th International Joint 
Conference on Artificial Intelligence. Edinburgh, 
Scotland. 
http://www.ijcai.org/papers/post-0214.pdf 
 
Joyce Y. Chai, and Rong Jin. 2004.  Discourse 
Structure for Context Question Answering.  In 
Proceedings of the Workshp on the Pragmatics 
of Quesiton Answering, HST-NAACL, Boston. 
http://www.cse.msu.edu/~rongjin/publications/H
LTQAWorkshop04.pdf   
 
Barry D. Davidson. 2006. An Advanced Interactive 
Discovery Learning Environment for 
Engineering Education: Final Report.  
Submitted to R. E. Gillian, National Aeronautics 
and Space Administration. 
 
Marco De Boni and Suresh Manandhar. 2005. 
Implementing Clarification Dialogues in Open 
Domain Question Answering. Natural Language 
Engineering 11(4): 343-361. 
 
Anne R. Diekema, Ozgur Yilmazel, Jiangping 
Chen, Sarah Harwell, Lan He, and Elizabeth D. 
Liddy. 2004. Finding Answers to Complex 
Questions. In New Directions in Question 
Answering. (Ed.) Mark T. Maybury. The MIT 
Press, 141-152. 
 
Sanda Harabagiu, Dan Moldovan, Marius Pa?ca, 
Mihai Surdeanu, Rada Mihalcea, Roxana G?rju, 
Vasile Rus, Finley L?c?tu?u, Paul Mor?rescu, 
R?zvan Bunescu. 2001. Answering Complex, 
List and Context Questions with LCC?s 
Question-Answering Server, TREC 2001. 
 
Chiori Hori, Takaaki Hori., Hideki Isozaki, Eisaku 
Maeda, Shigeru Katagiri, and Sadaoki Furui. 
2003. Deriving Disambiguous Queries in a 
Spoken Interactive ODQA System. In ICASSP. 
Hongkong, I: 624-627. 
 
Arne J?nsson, Frida And?n, Lars Degerstedt, 
Annika Flycht-Eriksson, Magnus Merkel, and 
Sara Norberg. 2004. Experiences from 
Combining Dialogue System Development With 
Information Extraction Techniques. In New 
Directions in Question Answering. (Ed.) Mark T. 
Maybury. The MIT Press, 153-164. 
 
Elizabeth D. Liddy. 2003. Question Answering in 
Contexts. Invited Keynote Speaker. ARDA 
AQUAINT Annual Meeting. Washington, DC. 
Dec 2-5, 2003. 
 
Elizabeth D. Liddy, Anne R. Diekema, Jiangping 
Chen, Sarah Harwell, Ozgur Yilmazel, and Lan 
He. 2003. What do You Mean? Finding Answers 
to Complex Questions. Proceedings of New 
Directions in Question Answering. AAAI Spring 
Symposium, March 24-26. 
 
Elizabeth D. Liddy, Anne R. Diekema, and Ozgur 
Yilmazel. 2004. Context-Based Question-
Answering Evaluation. In Proceedings of the 27th 
Annual ACM-SIGIR Conference. Sheffield, 
England 
 
Catherine S. Ross, Kirsti Nilsen, and Patricia 
Dewdney. 2002. Conducting the Reference 
Interview.  Neal-Schuman, New York, NY. 
 
Sharon Small, Tomek Strzalkowski, Ting Liu, 
Nobuyuki Shimizu, and Boris Yamrom. 2004. A 
Data Driven Approach to Interactive QA. In New 
Directions in Question Answering. (Ed.) Mark T. 
Maybury. The MIT Press, 129-140. 
 
Joseph E. Straw. 2004. Expecting the Stars but 
Getting the Moon: Negotiating around Patron 
Expectations in the Digital Reference 
Environment. In The Virtual Reference 
Experience: Integrating Theory into Practice. 
Eds. R. David Lankes, Joseph Janes, Linda C. 
Smith, and Christina M.  Finneran. Neal-
Schuman, New York, NY. 
 
24
Proceedings of the 8th International Conference on Computational Semantics, pages 326?332,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Towards a Cognitive Approach for the Automated
Detection of Connotative Meaning
Jaime Snyder*, Michael A. D?Eredita, Ozgur Yilmazel, and
Elizabeth D. Liddy
School of Information Studies, Syracuse University, Syracuse, NY, USA
* Corresponding author, jasnyd01@syr.edu
1 Introduction
The goal of the research described here is to automate the recognition of con-
notative meaning in text using a range of linguistic and non-linguistic fea-
tures. Pilot results are used to illustrate the potential of an integrated multi-
disciplinary approach to semantic text analysis that combines cognitive-
oriented human subject experimentation with Machine Learning (ML) based
Natural Language Processing (NLP). The research presented here was funded
through the Advanced Question and Answering for Intelligence (AQUAINT)
Project of the U.S. federal government?s Intelligence Advanced Research
Projects Activity (IARPA) Office. Funded as an exploratory ?Blue Sky?
project, this award enabled us to develop an extensible experimental setup
and to make progress towards training a machine learning system.
Automated understanding of connotative meaning of text requires an
understanding of both the mechanics of text and the human behaviors in-
volved in the disambiguation of text. We glean more from text than what
can be explicitly parsed from parts of speech or named entities. There are
other aspects of meaning that humans take away from text, such as a sincere
apology, an urgent request for help, a serious warning, or a perception of
personal threat. Merging cognitive and social cognitive psychology research
with sophisticated machine learning could extend current NLP systems to
account for these aspects. Building on current natural language processing
research [?], this pilot project encapsulates an end-to-end research method-
ology that begins by 1) establishing a human-understanding baseline for
the distinction between connotative and denotative meaning, 2) then ex-
tends the analysis of the mechanics of literal versus non-literal meaning by
326
applying NLP tools to the human-annotated text, and 3) uses these cu-
mulative results to feed a machine learning system that will be taught to
recognize the potential for connotative meaning at the sentence level, across
a much broader corpus. This paper describes the preliminary iteration of
this methodology and suggests ways that this approach could be improved
for future applications.
2 Analytic framework: A cognitive approach
We view an excerpt of text to be a stimulus, albeit much more complex than
most stimuli used in typical psychological experiments. The meaning of any
excerpt of text is tieds to a constructive cognitive process that is heavily
influenced by previous experience and cues, or features, embedded within
the text. Our goal is to gain a better understanding of (1) what features are
attended to when the text is being interpreted, (2) which of these features
are most salient and (3) how these features affect connotative meaning.
One?s ability to derive connotative meaning from text is behavior that
is learned, becoming intuitive in much the same way an individual learns
any skill or behavior. When this process of attending and learning is re-
peated across instances, specific skills become more automatic, or reliable
[?, ?]. This process is considered to be constructive and episodic in nature,
yet heavily dependent upon ?cues? that work to draw or focus one?s atten-
tion [?]. Further, research on communities suggests that the meaning of an
artifact (e.g., a specific excerpt of text) is heavily influenced by how it is
used in practice [?] The meaning of text is constructed in a similar manner.
Members of a speech community tend to make similar assumptions, or in-
ferences. The mechanics of making such inferences are scaled to the amount
of contextual information provided. Our preliminary research suggests that
when presented with a sentence that is out of context an individual seem-
ingly makes assumptions about one or all of the following: who created the
text, the context from which it was pulled and the intended meaning given
the features of the text.
3 Methods
3.1 Data
Blog text was used as the corpus for this research. Sentences were deemed
the most practical and fruitful unit of analysis because words were consid-
327
ered too restrictive and pieces of text spanning more than one sentence too
unwieldy. A single sentence presented enough context while still allowing
for a wide range of interpretation. Sentences were randomly selected from
a pool of texts automatically extracted from blogs, using a crawler set with
keywords such as ?oil?, ?Middle East? or ?Iraq.? Topics were selected with
the intention of narrowing the range of vocabulary used in order to aid the
machine learning experiments.
3.2 Preliminary phase
To start, we conducted a series of eight semi-structured, face-to-face inter-
views. Individuals were presented with 20 sentences selected to include some
texts that were expected to be perceived as highly connotative as well as
some expected to be perceived as highly denotative. Each interviewee was
asked to exhaustively share all possible meanings they could derive from
the stimulus text, while also pinpointing what it was about the text that
led them to make their conclusions. Based on these interviews, we modified
our probes slightly and moved the human evaluation process to an open-
ended, on-line instrument in order to increase the number of responses. We
presented a series of 20 sentences to participants (N=193) and, for each stim-
ulus text, asked: 1) ?What does this sentence suggest?? & ?What makes
you think this??; and 2) ?What else does this sentence suggest?? & ?What
makes you think this?? Upon analysis of the responses, we found that while
interpretations of the text were relatively idiosyncratic, how people allocated
their attention was more consistent. Most people tended to be making as-
sumptions about the (1) author (addressing who created the artifact), (2)
context (addressing from where the sentence was taken) and/or (3) intended
meaning of the words. We interpreted this to mean that these three areas
were potentially important for identifying inferred meanings of texts.
3.3 Design of pilot experiment
Next, our efforts focused on designing a reusable and scalable online evalu-
ation tool that would allow us to systematically gather multiple judgments
for each sentence using a much larger pool of stimulus text. Scaling up the
human evaluations also allowed us to decipher between responses that were
either systematically patterned or more idiosyncratic (or random). Accord-
ing to our forced-choice design, each online participant was presented with
a series of 32 pairs of sentences, one pair at a time, and asked to identify the
sentence that provided more of an opportunity to read between the lines.
328
Half the participants were presented with a positive prompt (which sentence
provides the most opportunity) and half were presented with a negative
prompt (which sentence provides the least opportunity). Positive/negative
assignment was determined randomly. The 16 sentences selected during
the first round were re-paired in a second round. This continued until 4
sentences remained, representing sentences that were more strongly conno-
tative or denotative, depending on the prompt. Final sentence scores were
averaged across all evaluations received.
The forced choice scenario requires a sample of only 13 participants to
evaluate 832 sentences. This was a significant improvement over previous
methods, increasing the number of sentences and the number of evaluations
per sentence and therefore increasing the reliability of our findings. For ex-
ample, using this scalable setup on a set of 832 sentences we need only 26
participants to generate two evaluations per sentence in the set, 39 partici-
pants to yield three evaluations per sentence, etc. We ran the system with
a randomly selected sample of both sentences and participants with the in-
tent to eventually make direct comparison among more controlled samples
of sentences and participants. This has direct implication for the evalua-
tion phase of our pilot. Because sentences were selected at random, without
guarantee of a certain number of each type of sentence, our goal was to
achieve results on a par with chance. Anything else would reveal systematic
bias in the experiment design or implementation. This also provides us with
a baseline for future investigations where the stimulus text would be more
wilfully controlled.
4 Results
4.1 Evaluation of text by human subjects
In the first iteration of the pilot setup, each of 832 sentences were viewed
by six different participants, three assigned to a positive group and three to
a negative group, as described above. The denotative condition ranged in
ratings from 0 to -3 while the connotative condition ranged in rating from 0
to 3. These were then averaged to achieve an overall score for each sentence.
Because they were randomly selected, each sentence had predictable chance
of ultimately being identified as connotative or denotative. In other words,
each sentence had an equal chance of being identified as connotative.
Having established a baseline based on chance, we can next control for
various features and evaluate the relative impact as systematic differences
from the baseline. We will be able to say with a relatively high degree of
329
certainty that ?x,? ?y? or ?z? feature, sentence structure, behavior, etc.
was responsible for skewing the odds in a reliable manner because we will
be able to control for these variables across various experimental scenarios.
This, combined with improved validity resulting from an increased number
of human judgments and an increased number of sentences viewed, marks
the strength of this methodology.
Additionally, we will be able to compare sentences within each scenario
even when an overall chance outcome occurs. For example, in the initial run
of our sentences, we achieved an overall chance outcome. However, ?anoma-
lies? emerged, sentences that were strongly skewed towards being assigned a
neutral evaluation score or towards an extreme score (either distinctly con-
notative or distinctly denotative). This allowed us to gather a reliable and
valid subset of data that can be utilized in ML experiments. See below for
a very short list of sample sentences grouped according to the overall scores
they received determine by the six human reviewers:
Denotative examples-
? The equipment was a radar system.
? Kosovo has been part of modern day Serbia since 1912.
? The projected figure for 2007 is about $ 3100.
Connotative examples-
? In fact, do what you bloody well like .
? But it?s pretty interesting , in a depressing sort of way .
? It?s no more a language than American English or Quebecois French
4.2 Experimental Machine Learning system
Our preliminary analysis suggests that humans are consistent in recogniz-
ing the extremes of connotative and denotative sentences and an automatic
recognition system could be built to identify when a text is likely to convey
connotative meaning. Machine Learning (ML) techniques could be used to
enable a system to first classify a text according to whether it conveys a
connotative or denotative level of meaning, and eventually, identify specific
connotations. ML techniques usually assume a feature space within which
the system learns the relative importance of features to use in classification.
Since humans process language at various levels (morphological, lexical, syn-
tactic, semantic, discourse and pragmatic), some multi-level combination of
features is helping them reach consistent conclusions. Hence, the initial ma-
chine learning classification decision will be made based on a class of critical
330
features, as cognitive and social-cognitive theory suggests happens in human
interpretation of text.
TextTagger, an Information Extraction System developed at Syracuse
University?s Center for Natural Language Processing, currently can identify
sentence boundaries, part-of-speech tag words, stem and lemmatize words,
identify various types of phrases, categorize named entities and common
nouns, recognize relations, and resolve co-references in text. We are in the
process of designing a ML framework that utilizes these tags and can learn
from a few examples provided by the human subject experiments described
above, then train on other sets of similar data marked by analysts as pos-
sessing the features illustrated by the sentences consistently identified as
conveying connotative meaning.
For preliminary ML-based analysis, the data collection included 266 sen-
tences (from the original 832 used in human subject experiments), 145
tagged as strongly connotative and 121 tagged as strongly denotative by
subjects. Fifty sentences from each set became a test collection and the
remaining 95 connotative and 71 denotative sentences were used for train-
ing. Our baseline results (without TextTagger annotations) were: Precision:
44.77 ; Recall: 60; F: 51.28. After tagging, when we only use proper names
and common nouns the results improved: Precision: 51.61 Recall: 92; F:
67.13. Although these results are not as high as some categorization results
reported in the literature for simpler categorization tasks such as document
labeling or spam identification, we believe that using higher level linguistic
features extracted by our NLP technology will significantly improve them.
More sophisticated analysis will be conducted during future applications of
this methodology.
5 Discussion and Future Work
By allowing the ML system to do time- and labor-intensive analysis, and
exploiting a natural human ability to ?know it when they see it? (in this case
?it? referring to connotative meaning), we feel that this pilot methodology
has great potential to deliver robust results. In addition to the significant
contribution this research will make in the area of natural language process-
ing, it will also provide a model for future work that seeks to create similar
bridges between psychological investigation and system building. Prelimi-
nary results suggest that our approach is viable and that a system composed
of multiple layers of analysis-with each level geared towards reducing the
variability of the next-holds promise.
331
Future work will concentrate efforts in two areas. First, the notion of
speech communities will be addressed. The pilot study looked at a very gen-
eralized speech community, expecting to achieve equally generalized results.
While this has merit, there is much to be learned by implementing this ap-
proach using a more targeted community. Second, the protocol used in this
pilot study was run using a relatively modest number of human evaluators
and a relatively small set of data. With the experience gained during the
pilot, the reliability of the data used to train the ML system can be easily
improved by increasing the size of both human subject samples and data
sets. With a more robust set of initial data, ML experiments can progress
beyond the basic proof-of-concept results reported here and produce action-
able feature sets tuned to specific speech communities.
References
[1] M. A. D?Eredita and C. Barreto. How does tacit knowledge proliferate? Orga-
nization Studies, 27(12):1821, 2006.
[2] E.D. Liddy, E. Hovy, J. Lin, J. Prager, D. Radev, L. Vanderwende, and
R. Weischedel. Natural Language Processing. Encyclopedia of Library and
Information Science, pages 2126?2136, 2003.
[3] G. D. Logan. Toward an instance theory of automatization. Psychological
Review, 95(4):492?527, 1988.
[4] E. Wenger. Communities of Practice: Learning, Meaning, and Identity. Cam-
bridge University Press, 1999.
[5] R.S. Wyer and J.A. Bargh. The Automaticity of Everyday Life. Lawrence
Erlbaum Associates, 1997.
332
