Proceedings of NAACL-HLT 2013, pages 802?807,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improving the Quality of Minority Class Identification in Dialog Act Tagging
Adinoyi Omuya
10gen
New York, NY, USA
wisdom@10gen.com
Vinodkumar Prabhakaran
CS, Columbia University
New York, NY, USA
vinod@cs.columbia.edu
Owen Rambow
CCLS, Columbia University
New York, NY, USA
rambow@ccls.columbia.edu
Abstract
We present a method of improving the perfor-
mance of dialog act tagging in identifying mi-
nority classes by using per-class feature opti-
mization and a method of choosing the class
based not on confidence, but on a cascade of
classifiers. We show that it gives a minor-
ity class F-measure error reduction of 22.8%,
while also reducing the error for other classes
and the overall error by about 10%.
1 Introduction
In this paper, we discuss dialog act tagging, the
task of assigning a dialog act to an utterance, where
a dialog act (DA) is a high-level categorization of
the pragmatic meaning of the utterance. Our data is
email. Our starting point is the tagger described in
(Hu et al, 2009), which uses a standard multi-class
classifier based on support vector machines (SVMs).
While the performance of this system is pretty good
as measured by accuracy, it performs badly on the
DA REQUEST-ACTION, which is a rare class. Multi-
class SVMs are typically implemented as a set of
SVMs, one per class, with the overall choice of class
being determined by the SVM with the highest con-
fidence (?one-against-all?). Multi-class SVMs are
typically packaged as a single system, whose inner
workings are ignored by the NLP researcher. In this
paper we show that, for our problem of DA classi-
fication, we can boost the performance of the rare
classes (while maintaining the overall performance)
by performing feature optimization separately for
each individual classifier. But we also show that we
can achieve an all-around error reduction by alter-
ing the method by which the multi-class classifier
combines the individual SVMs. This new method
of combination is a simple cascade: we run the in-
dividual classifiers in ascending order of frequency
of the classes in the training corpus; the first classi-
fier to classify the data point positively determines
the choice of the overall classifier. If no classifier
classifies the data point positively, we use the usual
confidence-based method. This new method obtains
a 22.8% error reduction for the minority class, and
around 10% error reduction for the other classes and
for the overall classifier.
This paper is structured as follows. We start out
by discussing related work (Section 2). We then
present our data in Section 3, and in Section 4 we
present the experiments with our systems and the re-
sults. We report the results of an extrinsic evaluation
in Section 5, and conclude.
2 Related Work
Dialog act (DA) annotations and tagging, inspired
by the speech act theory of Austin (1975) and Searle
(1976), have been used in the NLP community to un-
derstand and model dialog. Initial work was done on
spoken interactions (see for example (Stolcke et al,
2000)). Recently, studies have explored dialog act
tagging in written interactions such as emails (Co-
hen et al, 2004), forums (Kim et al, 2006; Kim et
al., 2010b), instant messaging (Kim et al, 2010a)
and Twitter (Zhang et al, 2012). Most DA tagging
systems for written interactions use a message/post
level tagging scheme, and allow multiple tags for
each message/post. In such a tagging scheme, indi-
802
vidual binary classifiers for each tag are independent
of one another. However, recent studies have found
merit in segmenting each message into functional
units and assigning a single DA to each segment (Hu
et al, 2009). Our work falls in this paradigm (we
choose a single DA for smaller textual units). We
build on the work by (Hu et al, 2009); we improve
their dialog act predicting performance on minority
classes using per-class feature optimization.
3 Data
In this study, we use the email corpus presented in
(Hu et al, 2009), which is manually annotated for
DA tags. The corpus contains 122 email threads
with a total of 360 messages and 20,740 word to-
kens. This set of email threads is chosen from a ver-
sion of the Enron email corpus with some missing
messages restored from other emails in which they
were quoted (Yeh and Harnly, 2006; Agarwal et al,
2012). Most emails are concerned with exchanging
information, scheduling meetings, or solving prob-
lems, but there are also purely social emails.
Dialog Act Tag Count (%)
REQUEST-ACTION (R-A) 35 (2.5%)
REQUEST-INFORMATION (R-I) 151 (10.7%)
CONVENTIONAL (CONV) 357 (25.4%)
INFORM (INF) 853 (60.7%)
Total # of DFUs 1406
Table 1: Annotation statistics
Each message in the thread is segmented into Di-
alog Functional Units (DFUs). A DFU is a con-
tiguous span within an email message which has
a coherent communicative intention. Each DFU
is assigned a single DA label which is one of the
following: REQUEST-ACTION (R-A), REQUEST-
INFORMATION (R-I), CONVENTIONAL (CONV)
and INFORM (INF). There are three other DA labels
? INFORM-OFFLINE, COMMIT, and NODA for no
dialog act ? which occurred 5 or fewer times in the
corpus. We ignore these DA labels in this paper. The
corpus also contains links between the DFUs, but we
do not use those annotations in this study. Table 1
presents the distribution of DA labels in our corpus.
We now describe each of the DAs we consider in our
experiments.
In a REQUEST-ACTION, the writer signals
her desire that the reader perform some non-
communicative act, i.e., an act that cannot in itself
be part of the dialogue. For example, a writer can
ask the reader to write a report or make coffee.
In a REQUEST-INFORMATION, the writer signals
her desire that the reader perform a specific com-
municative act, namely that he provide information
(either facts or opinion).
In an INFORM, the writer conveys information, or
more precisely, the writer signals that her desire that
the reader adopt a certain belief. It covers many dif-
ferent types of information that can be conveyed in-
cluding answers to questions, beliefs (committed or
not), attitudes, and elaborations on prior DAs.
A CONVENTIONAL dialog act does not signal any
specific communicative intention on the part of the
writer, but rather it helps structure and thus facilitate
the communication. Examples include greetings, in-
troductions, expressions of gratitude, etc.
4 System
We developed four systems for our experiments: a
baseline (BAS) system which is close to the system
described in (Hu et al, 2009), and three variants of
our novel divide and conquer (DAC) system. Fea-
tures used in both systems are extracted as explained
in Section 4.2. Section 4.3 describes the baseline
system, the basic DAC system, and two variations
of the DAC system.
4.1 Experimental Framework
In all our experiments, we use linear kernel Sup-
port Vector Machines (SVM). However, across the
systems, there are differences in how we use them.
Our framework was built with the ClearTK toolkit
(Ogren et al, 2008) with its wrapper for SVMLight
(Joachims, 1999). The ClearTK wrapper internally
shifts the prediction threshold based on posterior
probabilistic scores calculated using the algorithm
of Lin et al (2007). We report results from 5-fold
cross validation performed on the entire corpus.
4.2 Feature Engineering
In developing our system, we classified our features
into three categories: lexical, verbal and message-
803
level. Lexical features consists of n-grams of words,
n-grams of POS tags, mixed n-grams of closed class
words and POS tags (Prabhakaran et al, 2012), as
well as a small set of specialized features ? Start-
POS/Lemma (POS tag and lemma of the first word),
LastPOS/Lemma (POS tag and lemma of the last
word), MDCount (number of modal verbs in the
DFU) and QuestionMark (is there a question mark
in the DFU). We used the POS tags produced by the
OpenNLP POS tagger. Verbal features capture the
position and identity of the first verb in the DFU. Fi-
nally, message-level features capture aspects of the
location of the DFU in the message and of the mes-
sage in the thread (relative position and size). In
optimizing each system, we first performed an ex-
haustive search across all combinations of features
within each category. For the lexical n-gram fea-
tures we varied the n-gram window from 1 to 5. This
step gave us the best performing feature combination
within each category. In a second step, we found the
best combination of categories, using the previously
determined features for each category. In this pa-
per, we do not report best performing feature sets
for each configuration, due to lack of space.
4.3 Experiments
Baseline (BAS) System This system uses the
ClearTK built-in one-versus-all multiclass SVM in
prediction. Internally, the multi-class SVM builds
a set of binary classifiers, one for each dialog act.
For a given test instance, the classifier that obtains
the highest probability score determines the overall
prediction. We performed feature optimization on
the whole multiclass classifier (as described in Sec-
tion 4.2), i.e., the same set of features was available
to all component classifiers. We optimized for sys-
tem accuracy. Table 2 shows results using this sys-
tem. In this and all tables, we give the performance
of the system on the four DAs, using precision, re-
call, and F-measure. The DAs are listed in ascend-
ing order of frequency in the corpus (least frequent
DA first). We also give an overall accuracy evalua-
tion. As we can see, detecting REQUEST-ACTION is
much harder than detecting the other DAs.
Basic Divide and Conquer (DAC) System Like
the BAS system, the DAC system also builds a bi-
nary classifier for each dialog act separately, and the
Prec. Rec. F-meas.
R-A 57.9 31.4 40.7
R-I 91.5 78.2 84.3
CONV 92.0 95.8 93.8
INF 91.6 95.1 93.3
Accuracy 91.3
Table 2: Results for baseline (BAS) system (standard
multiclass SVM)
component classifier with highest probability score
determines the overall prediction. The crucial dif-
ference in the DAC system is that the feature opti-
mization is performed for each component classifier
separately. Each component classifier is optimized
for F-measure. Table 3 shows results using this sys-
tem.
Prec. Recall F-meas. ER
R-A 66.7 40.0 50.0 15.6
R-I 91.5 78.2 84.3 0.0
CONV 93.9 94.1 94.0 2.6
INF 91.4 96.1 93.7 5.7
Accuracy 91.7 4.9
Table 3: Results for basic DAC system (per-class feature
optimization followed by maximum confidence based
choice); ?ER? refers to error reduction in percent over
standard multiclass SVM (Table 2)
Minority Preference (DACMP) System This sys-
tem is exactly the same as the basic DAC system
except for one crucial difference: overall classifica-
tion is biased towards a specified minority class. If
the minority class binary classifier predicts true, this
system chooses the minority class as the predicted
class. In cases where the minority class classifier
predicts false, it backs off to the basic DAC system
after removing the minority class classifier from the
confidence tally. Table 4 shows our results using
REQUEST-ACTION as the minority class.
Cascading Minority Preference (DACCMP) System
This system is similar to the Minority Preference
System; however, instead of a single supplied mi-
nority class, the system accepts an ordered list of
classes. The classifier then works, in order, through
this list; whenever any classifier in the list predicts
804
Prec. Recall F-meas. ER
R-A 66.7 45.7 54.2 22.8
R-I 91.5 78.2 84.3 0.0
CONV 93.9 94.1 94.0 2.6
INF 91.6 96.0 93.8 6.5
Accuracy 91.8 5.7
Table 4: Results for minority-preference DAC system ?
DACMP (first consult REQUEST-ACTION tagger, then de-
fault to choice by maximum confidence); ?ER? refers to
error reduction in percent over standard multiclass SVM
(Table 2)
true, for a given instance, it then assigns this class
as the predicted class. The subsequent classifiers in
the list are not run. If all classifiers predict false, we
back off to the basic DAC system, i.e., the compo-
nent classifier with highest probability score deter-
mines the overall prediction. We ordered the list of
classes in the ascending order of their frequencies in
the training data. This ordering is driven by the ob-
servation that the less frequent classes are also hard
to predict correctly. Table 5 shows our results using
the ordered list: (REQUEST-ACTION, REQUEST-
INFORMATION, CONVENTIONAL, INFORM).
Prec. Recall F-meas. ER
R-A 66.7 45.7 54.2 22.8
R-I 91.0 80.8 85.6 8.4
CONV 93.7 95.3 94.5 10.1
INF 92.4 95.8 94.0 10.0
Accuracy 92.2 10.6
Table 5: Results for cascading minority-preference DAC
system ? DACCMP (consult classifiers in reverse order
of frequency of class); ?ER? refers to error reduction in
percent over standard multiclass SVM (Table 2)
4.4 Discussion
As shown in Table 3, the basic DAC system obtained
a 15.6% F-measure error reduction for the minor-
ity class REQUEST-ACTION over the BAS system.
It also improves performance of two other classes
? CONVENTIONAL and INFORM, and obtaines a
4.9% error reduction on overall accuracy. Recall
here that the only difference between the DAC sys-
tem and the BAS system is the per-class feature op-
timization and therefore this must be the reason for
this boost in performance. When we turn to DACMP,
we see that the performance on the minority class
REQUEST-ACTION is further enhanced, with an F-
measure error reduction of 22.8%; the overall ac-
curacy improves slightly with an error reduction of
5.7%. Finally, DACCMP further improves the perfor-
mance. Since the method of choosing the minor-
ity class REQUEST-ACTION does not change over
DACMP, the F-measure error reduction remains the
same. However, now all three other classes also im-
prove their performance, and we obtain a 10.6% er-
ror reduction on overall accuracy over the baseline
system.
Following (Guyon et al, 2002), we performed a
post-hoc analysis by inspecting the feature weights
of the best performing models created for each in-
dividual classifier in the DAC system. Table 6 lists
some interesting features chosen during feature opti-
mization for the individual SVMs. We selected them
from the top 25 features in terms of absolute value
of feature weights.
Some features help distinguish different DA cat-
egories. For example, the feature QuestionMark
is the feature with the highest negative weight for
INFORM, but has the highest positive weight for
REQUEST-INFORMATION. Features like fyi and pe-
riod (.) have high positive weights for INFORM
and high negative weights for CONVENTIONAL.
Some other features are important only for certain
classes. For e.g., please and VB NN are important
for REQUEST-ACTION, but not so for other classes.
Overall, the most discriminating features for both
INFORM and CONVENTIONAL are mostly word
ngrams, while those for REQUEST-ACTION and
REQUEST-INFORMATION are mostly POS ngrams.
This shows why our approach of per-class feature
optimization is important to boost the classification
performance.
Another interesting observation is that the least
frequent category, REQUEST-ACTION, has the least
strong indicators (as measured by feature weights).
Presumably this is because there is much less train-
ing data for this class. This explains why our cascad-
ing classifiers approach giving priority to the least
frequent categories worked better than a simple con-
fidence based approach, since the simple approach
drowns out the less confident classifiers.
805
REQUEST-ACTION REQUEST-INFORMATION CONVENTIONAL INFORM
please (0.9) QuestionMark (6.6) StartPOS NNP (2.7) QuestionMark (-3.0)
VB NN (0.7) BOS PRP (-1.2) thanks (2.3) thanks (-2.2)
you VB (0.3) WRB (1.0) . (-2.0) . (2.2)
PRP (-0.3) PRP VBP (-0.9) fyi (-2.0) fyi (1.9)
MD PRP VB (0.3) BOS MD (0.8) , (0.9) you (-1.0)
will (-0.2) BOS DT (-0.7) QuestionMark (-0.8) can you (-0.9)
Table 6: Post-hoc analysis on the models built by the DAC system: some of the top features with corresponding
feature weights in parentheses, for each individual tagger. (POS tags are capitalized; BOS stands for Beginning Of
Sentence)
5 Extrinsic Evaluation
In this section, we perform an extrinsic evaluation
for the dialog act tagger presented in Section 4 by
applying it to the task of identifying Overt Displays
of Power (ODP) in emails, proposed by Prabhakaran
et al (2012). The task is to identify utterances where
the linguistic form introduces additional constraints
on its responses, beyond those introduced by the
general dialog act. The dialog act features were
found to be useful and the best performing system
obtained an F-measure of 65.8 using gold dialog
act tags. For our extrinsic evaluation, we retrained
the ODP tagger using dialog act tags predicted by
our BAS and DACCMP systems instead of gold dia-
log acts. ODP tagger uses the same dataset as ours
for training. In the cross validation step, we made
sure that the test folds for ODP were excluded from
training the taggers to obtain DA tags. At each ODP
cross validation step, we trained a BAS or DACCMP
tagger using ODP?s training folds for that step and
used tags produced by that tagger for both training
and testing the ODP tagger for that step. Table 7 lists
the results obtained.
Prec. Rec. F-meas.
No-DA 55.7 45.4 50.0
Gold-DA 75.8 58.1 65.8
BAS-DA 60.6 46.5 52.6
DACCMP-DA 67.2 45.4 54.2
Table 7: Results for ODP system using various sources
of DA tags
Using BAS tagged DA, the F-measure of ODP
system reduced by 13.2 points to 52.6 from using
gold dialog acts (F=65.8). Using DACCMP, the F-
measure improved over BAS by 1.6 points to 54.2.
This constitutes an error reduction of 12.1%, tak-
ing the system using gold DA tags as the reference.
This improvement is noteworthy, given the fact that
the overall error reduction obtained by DACCMP over
BAS in the DA tagging was around 10.6%. Also, the
DACCMP-based ODP system obtained an error reduc-
tion of about 26.6% over a system that does not use
the DA features at all (F=50.0).
6 Conclusion
We presented a method of improving the perfor-
mance of dialog act tagging in identifying minority
classes by using per-class feature optimization and
choosing the class based on a cascade of classifiers.
We showed that it gives a minority class F-measure
error reduction of 22.8% while also reducing the er-
ror on other classes and the overall error by around
10%. We also presented an extrinsic evaluation of
this technique on detecting Overt Displays of Power
in dialog, where we achieve an error reduction of
12.1% over using the standard multiclass SVM to
generate dialog act tags.
Acknowledgements
This work is supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
views of the sponsor. While working on this project,
the first author Adinoyi Omuya was affiliated with
the Center for Computational Learning Systems at
Columbia University. We thank several anonymous
reviewers for their constructive feedback.
806
References
Apoorv Agarwal, Adinoyi Omuya, Aaron Harnly, and
Owen Rambow. 2012. A Comprehensive Gold Stan-
dard for the Enron Organizational Hierarchy. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 161?165, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
J. L. Austin. 1975. How to Do Things with Words. Har-
vard University Press, Cambridge, Mass.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to Classify Email into
?Speech Acts? . In Dekang Lin and Dekai Wu, ed-
itors, Proceedings of EMNLP 2004, pages 309?316,
Barcelona, Spain, July. Association for Computational
Linguistics.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene Selection for Cancer
Classification using Support Vector Machines. Mach.
Learn., 46:389?422, March.
Jun Hu, Rebecca Passonneau, and Owen Rambow. 2009.
Contrasting the Interaction Structure of an Email and
a Telephone Corpus: A Machine Learning Approach
to Annotation of Dialogue Function Units. In Pro-
ceedings of the SIGDIAL 2009 Conference, London,
UK, September. Association for Computational Lin-
guistics.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Bernhard Scho?lkopf, Christo-
pher J.C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning, Cam-
bridge, MA, USA. MIT Press.
J. Kim, G. Chern, D. Feng, E. Shaw, and E. Hovy.
2006. Mining and Assessing Discussions on the Web
Through Speech Act Analysis. In Proceedings of the
Workshop on Web Content Mining with Human Lan-
guage Technologies at the 5th International Semantic
Web Conference.
S.N. Kim, L. Cavedon, and T. Baldwin. 2010a. Classify-
ing Dialogue Acts in One-on-one Live Chats. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 862?871.
Association for Computational Linguistics.
S.N. Kim, L. Wang, and T. Baldwin. 2010b. Tagging
and Linking Web Forum Posts. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 192?202. Association for
Computational Linguistics.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A Note on Platt?s Probabilistic Outputs for Support
Vector Machines. Mach. Learn., 68:267?276, Octo-
ber.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA toolkit for statistical natural
language processing. In Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP work-
shop at Language Resources and Evaluation Confer-
ence (LREC).
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting Overt Display of Power in
Written Dialogs. In Human Language Technologies:
The 2012 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Montreal, Canada, June. Association for Compu-
tational Linguistics.
J.R. Searle. 1976. A Classification of Illocutionary Acts.
Language in society, 5(01):1?23.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,
and M. Meteer. 2000. Dialogue Act Modeling for
Automatic Tagging and Recognition of Conversational
Speech. Computational linguistics, 26(3):339?373.
J.Y. Yeh and A. Harnly. 2006. Email Thread Reassembly
Using Similarity Matching. In Third Conference on
Email and Anti-Spam (CEAS), pages 27?28.
R. Zhang, D. Gao, and W. Li. 2012. Towards Scalable
Speech Act Recognition in Twitter: Tackling Insuffi-
cient Training Data. EACL 2012, page 18.
807
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 161?165,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comprehensive Gold Standard for the Enron Organizational Hierarchy
Apoorv Agarwal1* Adinoyi Omuya1** Aaron Harnly2? Owen Rambow3?
1 Department of Computer Science, Columbia University, New York, NY, USA
2 Wireless Generation Inc., Brooklyn, NY, USA
3 Center for Computational Learning Systems, Columbia University, New York, NY, USA
* apoorv@cs.columbia.edu ** awo2108@columbia.edu
?aaron@cs.columbia.edu ?rambow@ccls.columbia.edu
Abstract
Many researchers have attempted to predict
the Enron corporate hierarchy from the data.
This work, however, has been hampered by
a lack of data. We present a new, large, and
freely available gold-standard hierarchy. Us-
ing our new gold standard, we show that a
simple lower bound for social network-based
systems outperforms an upper bound on the
approach taken by current NLP systems.
1 Introduction
Since the release of the Enron email corpus, many
researchers have attempted to predict the Enron cor-
porate hierarchy from the email data. This work,
however, has been hampered by a lack of data about
the organizational hierarchy. Most researchers have
used the job titles assembled by (Shetty and Adibi,
2004), and then have attempted to predict the rela-
tive ranking of two people?s job titles (Rowe et al,
2007; Palus et al, 2011). A major limitation of the
list compiled by Shetty and Adibi (2004) is that it
only covers those ?core? employees for whom the
complete email inboxes are available in the Enron
dataset. However, it is also interesting to determine
whether we can predict the hierarchy of other em-
ployees, for whom we only have an incomplete set
of emails (those that they sent to or received from
the core employees). This is difficult in particular
because there are dominance relations between two
employees such that no email between them is avail-
able in the Enron data set. The difficulties with the
existing data have meant that researchers have ei-
ther not performed quantitative analyses (Rowe et
al., 2007), or have performed them on very small
sets: for example, (Bramsen et al, 2011a) use 142
dominance pairs for training and testing.
We present a new resource (Section 3). It is a large
gold-standard hierarchy, which we extracted manu-
ally from pdf files. Our gold standard contains 1,518
employees, and 13,724 dominance pairs (pairs of
employees such that the first dominates the second
in the hierarchy, not necessarily immediately). All
of the employees in the hierarchy are email corre-
spondents on the Enron email database, though ob-
viously many are not from the core group of about
158 Enron employees for which we have the com-
plete inbox. The hierarchy is linked to a threaded
representation of the Enron corpus using shared IDs
for the employees who are participants in the email
conversation. The resource is available as a Mon-
goDB database.
We show the usefulness of this resource by inves-
tigating a simple predictor for hierarchy based on
social network analysis (SNA), namely degree cen-
trality of the social network induced by the email
correspondence (Section 4). We call this a lower
bound for SNA-based systems because we are only
using a single simple metric (degree centrality) to
establish dominance. Degree centrality is one of
the features used by Rowe et al (2007), but they
did not perform a quantitative evaluation, and to our
knowledge there are no published experiments us-
ing only degree centrality. Current systems using
natural language processing (NLP) are restricted to
making informed predictions on dominance pairs for
which email exchange is available. We show (Sec-
tion 5) that the upper bound performance of such
161
NLP-based systems is much lower than our SNA-
based system on the entire gold standard. We also
contrast the simple SN-based system with a specific
NLP system based on (Gilbert, 2012), and show that
even if we restrict ourselves to pairs for which email
exchange is available, our simple SNA-based sys-
tems outperforms the NLP-based system.
2 Work on Enron Hierarchy Prediction
The Enron email corpus was introduced by Klimt
and Yang (2004). Since then numerous researchers
have analyzed the network formed by connecting
people with email exchange links (Diesner et al,
2005; Shetty and Adibi, 2004; Namata et al, 2007;
Rowe et al, 2007; Diehl et al, 2007; Creamer et al,
2009). Rowe et al (2007) use the email exchange
network (and other features) to predict the domi-
nance relations between people in the Enron email
corpus. They however do not present a quantitative
evaluation.
Bramsen et al (2011b) and Gilbert (2012) present
NLP based models to predict dominance relations
between Enron employees. Neither the test-set nor
the system of Bramsen et al (2011b) is publicly
available. Therefore, we compare our baseline SNA
based system with that of Gilbert (2012). Gilbert
(2012) produce training and test data as follows: an
email message is labeled upward only when every
recipient outranks the sender. An email message is
labeled not-upward only when every recipient does
not outrank the sender. They use an n-gram based
model with Support Vector Machines (SVM) to pre-
dict if an email is of class upward or not-upward.
They make the phrases (n-grams) used by their best
performing system publicly available. We use their
n-grams with SVM to predict dominance relations
of employees in our gold standard and show that a
simple SNA based approach outperforms this base-
line. Moreover, Gilbert (2012) exploit dominance
relations of only 132 people in the Enron corpus for
creating their training and test data. Our gold stan-
dard has dominance relations for 1518 Enron em-
ployees.
3 The Enron Hierarchy Gold Standard
Klimt and Yang (2004) introduced the Enron email
corpus. They reported a total of 619,446 emails
taken from folders of 158 employees of the Enron
corporation. We created a database of organizational
hierarchy relations by studying the original Enron
organizational charts. We discovered these charts
by performing a manual, random survey of a few
hundred emails, looking for explicit indications of
hierarchy. We found a few documents with organi-
zational charts, which were always either Excel or
Visio files. We then searched all remaining emails
for attachments of the same filetype, and exhaus-
tively examined those with additional org charts. We
then manually transcribed the information contained
in all org charts we found.
Our resulting gold standard has a total of 1518
nodes (employees) which are described as be-
ing in immediate dominance relations (manager-
subordinate). There are 2155 immediate dominance
relations spread over 65 levels of dominance (CEO,
manager, trader etc.) From these relations, we
formed the transitive closure and obtained 13,724
hierarchal relations. For example, if A immediately
dominates B and B immediately dominates C, then
the set of valid organizational dominance relations
are A dominates B, B dominates C and A domi-
nates C. This data set is much larger than any other
data set used in the literature for the sake of predict-
ing organizational hierarchy.
We link this representation of the hierarchy to the
threaded Enron corpus created by Yeh and Harnley
(2006). They pre-processed the dataset by combin-
ing emails into threads and restoring some missing
emails from their quoted form in other emails. They
also co-referenced multiple email addresses belong-
ing to one person, and assigned unique identifiers
and names to persons. Therefore, each person is a-
priori associated with a set of email addresses and
names (or name variants), but has only one unique
identifier. Our corpus contains 279,844 email mes-
sages. These messages belong to 93,421 unique per-
sons. We use these unique identifiers to express our
gold hierarchy. This means that we can easily re-
trieve all emails associated with people in our gold
hierarchy, and we can easily determine the hierar-
chical relation between the sender and receivers of
any email.
The whole set of person nodes is divided into two
parts: core and non-core. The set of core people are
those whose inboxes were taken to create the Enron
162
email network (a set of 158 people). The set of non-
core people are the remaining people in the network
who either send an email to and/or receive an email
from a member of the core group. As expected, the
email exchange network (the network induced from
the emails) is densest among core people (density of
20.997% in the email exchange network), and much
less dense among the non-core people (density of
0.008%).
Our data base is freely available as a MongoDB
database, which can easily be interfaced with using
APIs in various programming languages. For infor-
mation about how to obtain the database, please con-
tact the authors.
4 A Hierarchy Predictor Based on the
Social Network
We construct the email exchange network as fol-
lows. This network is represented as an undirected
weighted graph. The nodes are all the unique em-
ployees. We add a link between two employees if
one sends at least one email to the other (who can
be a TO, CC, or BCC recipient). The weight is
the number of emails exchanged between the two.
Our email exchange network consists of 407,095
weighted links and 93,421 nodes.
Our algorithm for predicting the dominance rela-
tion using social network analysis metric is simple.
We calculate the degree centrality of every node in
the email exchange network, and then rank the nodes
by their degree centrality. Recall that the degree cen-
trality is the proportion of nodes in the network with
which a node is connected. (We also tried eigenvalue
centrality, but this performed worse. For a discus-
sion of the use of degree centrality as a valid indica-
tion of importance of nodes in a network, see (Chuah
and Coman, 2009).) Let CD(n) be the degree cen-
trality of node n, and let DOM be the dominance re-
lation (transitive, not symmetric) induced by the or-
ganizational hierarchy. We then simply assume that
for two people p1 and p2, if CD(p1) > CD(p2),
then DOM(p1,p2). For every pair of people who
are related with an organizational dominance rela-
tion in the gold standard, we then predict which per-
son dominates the other. Note that we do not pre-
dict if two people are in a dominance relation to be-
gin with. The task of predicting if two people are
Type # pairs %Acc
All 13,724 83.88
Core 440 79.31
Inter 6436 93.75
Non-Core 6847 74.57
Table 1: Prediction accuracy by type of predicted organi-
zational dominance pair; ?Inter? means that one element
of the pair is from the core and the other is not; a negative
error reduction indicates an increase in error
in a dominance relation is different and we do not
address that task in this paper. Therefore, we re-
strict our evaluation to pairs of people (p1, p2) who
are related hierarchically (i.e., either DOM(p1,p2) or
DOM(p2,p1) in the gold standard). Since we only
predict the directionality of the dominance relation
of people given they are in a hierarchical relation,1
the random baseline for our task performs at 50%.
We have 13,724 such pairs of people in the gold
standard. When we use the network induced simply
by the email exchanges, we get a remarkably high
accuracy of 83.88% (Table 1). We denote this sys-
tem by SNAG.
In this paper, we also make an observation crucial
for the task of hierarchy prediction, based on the dis-
tinction between the core and the non-core groups
(see Section 3). This distinction is crucial for this
task since by definition the degree centrality mea-
sure (which depends on how accurately the underly-
ing network expresses the communication network)
suffers from missing email messages (for the non-
core group). Our results in table 1 confirm this in-
tuition. Since we have a richer network for the core
group, degree centrality is a better predictor for this
group than for the non-core group.
We also note that the prediction accuracy is by far
the highest for the inter hierarchal pairs. The in-
ter hierarchal pairs are those in which one node is
from the core group of people and the other node
is from the non-core group of people. This is ex-
plained by the fact that the core group was chosen
by law enforcement because they were most likely
to contain information relevant to the legal proceed-
ings against Enron; i.e., the owners of the mailboxes
1This style of evaluation is common (Diehl et al, 2007;
Bramsen et al, 2011b).
163
were more likely more highly placed in the hierar-
chy. Furthermore, because of the network character-
istics described above (a relatively dense network),
the core people are also more likely to have a high
centrality degree, as compared to the non-core peo-
ple. Therefore, the correlation between centrality
degree and hierarchal dominance will be high.
5 Using NLP and SNA
In this section we compare and contrast the per-
formance of NLP-based systems with that of SNA-
based systems on the Enron hierarchy gold standard
we introduce in this paper. This gold standard al-
lows us to notice an important limitation of the NLP-
based systems (for this task) in comparison to SNA-
based systems in that the NLP-based systems require
communication links between people to make a pre-
diction about their dominance relation, whereas an
SNA-based system may predict dominance relations
without this requirement.
Table 2 presents the results for four experiments.
We first determine an upper bound for current NLP-
based systems. Current NLP-based systems pre-
dict dominance relations between a pair of people
by using the language used in email exchanges be-
tween these people; if there is no email exchange,
such methods cannot make a prediction. Let G be
the set of all dominance relations in the gold stan-
dard (|G| = 13, 723). We define T ? G to be
the set of pairs in the gold standard such that the
people involved in the pair in T communicate with
each other. These are precisely the dominance rela-
tions in the gold standard which can be established
using a current NLP-based approach. The number
of such pairs is |T | = 2, 640. Therefore, if we
consider a perfect NLP system that correctly pre-
dicts the dominance of 2, 640 tuples and randomly
guesses the dominance relation of the remaining
11, 084 tuples, the system would achieve an accu-
racy of (2640 + 11084/2)/13724 = 59.61%. We
refer to this number as the upper bound on the best
performing NLP system for the gold standard. This
upper bound of 59.61% for an NLP-based system is
lower (24.27% absolute) than a simple SNA-based
system (SNAG, explained in section 4) that predicts
the dominance relation for all the tuples in the gold
standard G.
As explained in section 2, we use the phrases
provided by Gilbert (2012) to build an NLP-based
model for predicting dominance relations of tuples
in set T ? G. Note that we only use the tu-
ples from the gold standard where the NLP-based
system may hope to make a prediction (i.e. peo-
ple in the tuple communicate via email). This sys-
tem, NLPGilbert achieves an accuracy of 82.37%
compared to the social network-based approach
(SNAT ) which achieves a higher accuracy of
87.58% on the same test set T . This comparison
shows that SNA-based approach out-performs the
NLP-based approach even if we evaluate on a much
smaller part of the gold standard, namely the part
where an NLP-based approach does not suffer from
having to make a random prediction for nodes that
do not comunicate via email.
System Test set # test points %Acc
UBNLP G 13,724 59.61
NLPGilbert T 2604 82.37
SNAT T 2604 87.58
SNAG G 13,724 83.88
Table 2: Results of four systems, essentially comparing
performance of purely NLP-based systems with simple
SNA-based systems.
6 Future Work
One key challenge of the problem of predicting
domination relations of Enron employees based on
their emails is that the underlying network is incom-
plete. We hypothesize that SNA-based approaches
are sensitive to the goodness with which the underly-
ing network represents the true social network. Part
of the missing network may be recoverable by an-
alyzing the content of emails. Using sophisticated
NLP techniques, we may be able to enrich the net-
work and use standard SNA metrics to predict the
dominance relations in the gold standard.
Acknowledgments
We would like to thank three anonymous reviewers
for useful comments. This work is supported by
NSF grant IIS-0713548. Harnly was at Columbia
University while he contributed to the work.
164
References
Philip Bramsen, Martha Escobar-Molano, Ami Patel, and
Rafael Alonso. 2011a. Extracting social power rela-
tionships from natural language. In ACL, pages 773?
782. The Association for Computer Linguistics.
Philip Bramsen, Martha Escobar-Molano, Ami Patel, and
Rafael Alonso. 2011b. Extracting social power rela-
tionships from natural language. ACL.
Mooi-Choo Chuah and Alexandra Coman. 2009. Iden-
tifying connectors and communities: Understand-
ing their impacts on the performance of a dtn pub-
lish/subscribe system. International Conference on
Computational Science and Engineering (CSE ?09).
Germa?n Creamer, Ryan Rowe, Shlomo Hershkop,
and Salvatore J. Stolfo. 2009. Segmentation
and automated social hierarchy detection through
email network analysis. In Haizheng Zhang, Myra
Spiliopoulou, Bamshad Mobasher, C. Lee Giles, An-
drew Mccallum, Olfa Nasraoui, Jaideep Srivastava,
and John Yen, editors, Advances in Web Mining and
Web Usage Analysis, pages 40?58. Springer-Verlag,
Berlin, Heidelberg.
Christopher Diehl, Galileo Mark Namata, and Lise
Getoor. 2007. Relationship identification for social
network discovery. AAAI ?07: Proceedings of the
22nd National Conference on Artificial Intelligence.
Jana Diesner, Terrill L Frantz, and Kathleen M Carley.
2005. Communication networks from the enron email
corpus it?s always about the people. enron is no dif-
ferent. Computational & Mathematical Organization
Theory, 11(3):201?228.
Eric Gilbert. 2012. Phrases that signal workplace hierar-
chy. In Proceedings of the ACM 2012 conference on
Computer Supported Cooperative Work (CSCW).
Bryan Klimt and Yiming Yang. 2004. Introducing the
enron corpus. In First Conference on Email and Anti-
Spam (CEAS).
Galileo Mark S. Namata, Jr., Lise Getoor, and Christo-
pher P. Diehl. 2007. Inferring organizational titles
in online communication. In Proceedings of the 2006
conference on Statistical network analysis, ICML?06,
pages 179?181, Berlin, Heidelberg. Springer-Verlag.
Sebastian Palus, Piotr Brodka, and Przemys?aw
Kazienko. 2011. Evaluation of organization structure
based on email interactions. International Journal of
Knowledge Society Research.
Ryan Rowe, German Creamer, Shlomo Hershkop, and
Salvatore J Stolfo. 2007. Automated social hierar-
chy detection through email network analysis. Pro-
ceedings of the 9th WebKDD and 1st SNA-KDD 2007
workshop on Web mining and social network analysis,
pages 109?117.
Jitesh Shetty and Jaffar Adibi. 2004. Ex employee
status report. http://www.isi.edu/?adibi/
Enron/Enron_Employee_Status.xls.
Jen Yuan Yeh and Aaron Harnley. 2006. Email thread
reassembly using similarity matching. In Proceedings
of CEAS.
165
