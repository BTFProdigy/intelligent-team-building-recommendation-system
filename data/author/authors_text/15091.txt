2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 172?181,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Apples to Oranges: Evaluating Image Annotations from Natural Language
Processing Systems
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
{rebecca,ec}@cs.brown.edu
Abstract
We examine evaluation methods for systems
that automatically annotate images using co-
occurring text. We compare previous datasets
for this task using a series of baseline mea-
sures inspired by those used in information re-
trieval, computer vision, and extractive sum-
marization. Some of our baselines match or
exceed the best published scores for those
datasets. These results illuminate incorrect as-
sumptions and improper practices regarding
preprocessing, evaluation metrics, and the col-
lection of gold image annotations. We con-
clude with a list of recommended practices for
future research combining language and vi-
sion processing techniques.
1 Introduction
Automatic image annotation is an important area
with many applications such as tagging, generat-
ing captions, and indexing and retrieval on the web.
Given an input image, the goal is to generate rel-
evant descriptive keywords that describe the visual
content of the image. The Computer Vision (CV)
literature contains countless approaches to this task,
using a wide range of learning techniques and visual
features to identify aspects such as objects, people,
scenes, and events.
Text processing is computationally less expensive
than image processing and easily provides informa-
tion that is difficult to learn visually. For this reason,
most commerical image search websites identify the
semantic content of images using co-occurring text
exclusively. But co-occurring text is also a noisy
source for candidate annotations, since not all of the
text is visually relevant. Techniques from Natural
Language Processing help align descriptive words
and images. Some examples of previous research
use named-entity recognition to identify people in
images (Deschacht and Moens, 2007); term associa-
tion to estimate the ?visualness? of candidate anno-
tations (Boiy et al, 2008; Leong et al, 2010); and
topic models to annotate images given both visual
and textual features (Feng and Lapata, 2010b).
Image annotation using NLP is still an emerging
area with many different tasks, datasets, and eval-
uation methods, making it impossible to compare
many recent systems to each other. Although there is
some effort being made towards establishing shared
tasks1, it is not yet clear which kinds of tasks and
datasets will provide interesting research questions
and practical applications in the long term. Until
then, establishing general ?best practices? for NLP
image annotation will help advance and legitimitize
this work. In this paper, we propose some good prac-
tices and demonstrate why they are important.
2 Image Annotation Evaluation in CV and
NLP
In this section, we first review related work in im-
age annotation evaluation in computer vision, spe-
cific challenges, and proposed solutions. We then
relate these challenges to the NLP image annotation
task and some of the specific problems we propose
to address.
1http://imageclef.org/
172
2.1 Related Work in Computer Vision
The work of Mu?ller et al (2002) is one of the first
to address the issue of evaluation for image annota-
tion systems. While using the exact same annotation
system, dataset, and evaluation metric, they dramati-
cally improve the apparent performance of their sys-
tem by using dataset pruning heuristics.
Others have criticized commonly-used CV
datasets for being too ?easy? ? images with the
same keywords are extremely similar in low-level
features such as orientation, lighting, and color;
while differences between images with different
keywords are very clear (Westerveld and de Vries,
2003; Ponce et al, 2006; Herve? and Boujemaa,
2007; Tang and Lewis, 2007). These features are
unwittingly exploited by certain algorithms and
obscure the benefits of using more complex tech-
niques (Ponce et al, 2006). The problem is further
exacerbated by evaluation metrics which essentially
prefer precision over recall and are biased towards
certain keywords. Annotations in test data might
not include all of the ?correct? keywords, and
evaluation metrics need to account for the fact that
frequent keywords in the corpus are safer guesses
than keywords that appear less frequently (Monay
and Gatica-Perez, 2003).
New baseline techniques, evaluation metrics, and
datasets for image annotation have been developed
in response to these problems. Makadia et al (2008;
2010) define a basic set of low-level features, and
propose new baselines for more complex systems to
evaluate against. Barnard et al (2003) present a nor-
malized loss function to address the preference to-
wards precision in evaluation metrics. New datasets
are larger and provide more diverse images, and it is
now easy to obtain multiple human-annotations per
image thanks to distributed services such as Ama-
zon?s Mechanical Turk, and the ESP game (von
Ahn and Dabbish, 2004). Hanbury (2008) provides
an overview of popular CV annotation datasets and
methods used for building them.
2.2 Image Annotation using Natural Language
Processing
Many of the problems from CV image annotation
are also applicable to NLP image annotation, and
bringing NLP to the task brings new challenges as
well. One of these challenges is whether to allow
infrequent words to be pruned. In CV annotation it
is typical to remove infrequent terms from both the
keyword vocabulary and the evaluation data because
CV algorithms typically need a large number of ex-
amples to train on. However, using NLP systems
and baselines one can correctly annotate using key-
words that did not appear in the training set. Remov-
ing ?unlearnable? keywords from evaluation data, as
done in (Boiy et al, 2008; Feng and Lapata, 2010b),
artificially inflates performance against simple base-
lines such as term frequency.
Nearly all NLP annotation datasets use naturally-
occurring sources of images and text. A particu-
larly popular source is news images alongside cap-
tions or articles, which are collected online from
sources such as Yahoo! News (Berg et al, 2004; De-
schacht and Moens, 2007). There are also domain-
specific databases with images and descriptions such
as the art, antiques, and flowers corpora used in Boiy
et al (2008). Wikipedia has also been used as a
source of images and associated text (Tsikrika et al,
2011). These sources typically offer well-written
and cleanly formatted text but introduce the problem
of converting text into annotations, and the annota-
tions may not meet the requirements of the new task
(as shown in Section 3.1). Obtaining data via image
search engines is a common practice in CV (Fei-Fei
et al, 2004; Berg and Forsyth, 2006) and can also
be used to provide more challenging and diverse in-
stances of images and co-occurring text. The addi-
tional challenge for NLP is that text content on many
websites is written to improve their rank in search
engines, using techniques such as listing dozens of
popular keywords. Co-occurring text for retrieved
images on popular queries may not be representative
of the task to be performed.
3 Datasets
In this paper, we examine two established image an-
notation datasets: the BBC News Dataset of Feng
and Lapata (2008) (henceforth referred to as BBC),
and the general web dataset of Leong et al (2010)
(henceforth referred to as UNT). These datasets
were both built to evaluate image annotation systems
that use longer co-occurring text such as a news ar-
ticle or a webpage, but they use data from differ-
173
Dataset BBC UNT
data instances article, image, and caption from a
news story
image and text from a webpage
source of data scraped from BBC News website Google Image Search results
candidate keywords or
collocations for anno-
tation
descriptive unigram words from
training data
n ? 7-grams extracted from co-
occurring text; collocations must ap-
pear as article names on Wikipedia
gold annotations descriptive words from held-out im-
age captions
multiple human-authored annota-
tions for each image
evaluation metric precision and recall against gold an-
notations
metrics adapted from evaluation of
lexical substitutions (SemEval)
number of train in-
stances
3121 instances of related news arti-
cle, image, and caption
none (train using cross-validation)
number of test in-
stances
240 instances of news article and re-
lated image
300 instances of webpage with text
and image
preprocessing proce-
dure
lemmatize tokens, remove from
dataset al words that are not descrip-
tive or that appear fewer than five
times in training articles
stem all tokens
average number of
text tokens after
preprocessing
169 word tokens per article, 4.5 per
caption
278 word tokens per webpage
average document title
length
4 word tokens 6 word tokens
total vocabulary after
preprocessing
10479 word types 8409 word types
Table 1: Comparison of the BBC and UNT image annotation datasets.
ent domains, different sources of gold image anno-
tations, different preprocessing procedures, and dif-
ferent evaluation measures.
Table 1 provides an overview of the datasets;
while this section covers the source of the datsets
and their gold annotations in more detail.
3.1 BBC
The BBC Dataset (Feng and Lapata, 2008)2 contains
news articles, image captions, and images taken
from the BBC News website. Training instances
consist of a news article, image, and image caption
from the same news story. Test instances are just the
image and the article, and hold-out the caption as a
source of gold image annotations.
Using news image captions as annotations has
2Downloaded from http://homepages.inuf.ed.
ac.uk/s0677528/data.html
the disadvantage that captions often describe back-
ground information or relate the photo to the story,
rather than listing important entities in the image.
It also fails to capture variation in how humans de-
scribe images, since it is limited to one caption per
image.3 However, captions are a cheap source of
data; BBC has ten times as many images as UNT.
To address the problem of converting natural lan-
guage into annotations, a large amount of prepro-
cessing is performed. The established preprocessing
procedure for this dataset is to lemmatize and POS-
tag using TreeTagger (Schmid, 1994) then remove
all but the ?descriptive? words (defined as nouns, ad-
jectives, and certain classes of verbs). This leaves
a total text vocabulary of about 32K words, which
3The Pascal Sentences dataset (vision.cs.uiuc.edu/
pascal-sentences) provides multiple captions per image,
but they are not naturally-occurring.
174
is further reduced by removing words that appear
fewer than five times in the training set articles. Ta-
ble 1 shows the number of word tokens and types
after performing these steps.4
3.2 UNT
The UNT Dataset (Leong et al, 2010)5 consists
of images and co-occurring text from webpages.
The webpages are found by querying Google Image
Search with frequent English words, and randomly
selecting from the results.
Each image in UNT is annotated by five people
via Mechanical Turk. In order to make human and
system results comparable, human annotators are re-
quired to only select words and collocations that are
directly extracted from the text, and the gold anno-
tations are the count of how many times each key-
word or collocation is selected. The human annota-
tors write keywords into a text box; while the col-
locations are presented as a list of candidates and
annotators mark which ones are relevant. Human
annotators tend to select subsets of collocations in
addition to the entire collocation. For example, the
gold annotation for one image has ?university of
texas?, ?university of texas at dallas?, ?the univer-
sity of texas?, and ?the university of texas at dal-
las?, each selected by at least four of the five an-
notators. Additionally, annotators can select mul-
tiple forms of the same word (such as ?tank? and
?tanks?). Gold annotations are stemmed after they
are collected, and keywords with the same stem have
their counts merged. For this reason, many key-
words have a higher count than the number of an-
notators.
4 We are unable to reproduce work from Feng & Lapata
(2008; 2010a; 2010b) and Feng (2011). Specifically, our vocab-
ulary counts after preprocessing (as in Table 1) are much higher
than reported counts, although the number of tokens per arti-
cle/caption they report is higher than ours. We have contacted
the authors, who confirmed that they took additional steps to re-
duce the size of the vocabulary, but were unable to tell us exactly
what those steps are. Therefore, all system and baseline scores
presented on their dataset are of our own implementation, and
do not match those reported in previous publications.
5Downloaded from http://lit.csci.unt.edu/
index.php?P=research/downloads
4 Baselines
We run several baselines on the datasets. Term fre-
quency, tf*idf, and corpus frequency are features
that are often used in annotation systems, so it is im-
portant to test them on their own. Document Title
and tf*idf are both baselines that were used in the
original papers where these datasets came from.
Sentence extraction is a new baseline that we pro-
pose specifically for the BBC dataset, in order see if
we can exploit certain properties of the gold annota-
tions, which are also derived from sentences.
4.1 Term Frequency
Term frequency has been shown to be a power-
ful feature in summarization (Nenkova and Vander-
wende, 2005). Words that appear frequently are
considered more meaningful than infrequent words.
Term frequency is the number of times a term (ex-
cluding function words) appears in a document, di-
vided by the total number of terms in that document.
On the UNT dataset we use the stopword list in-
cluded with the MALLET6 toolkit, while the BBC
dataset doesn?t matter because the function words
have already been removed.
4.2 tf*idf
While term frequency baseline requires the use of an
ad hoc function word list, tf*idf adjusts the weights
of different words depending on how important they
are in the corpus. It is a standard baseline used for
information retrieval tasks, based on the intuition
that a word that appears in a smaller number of doc-
uments is more likely to be meaningful than a word
that appears in many documents.
tf*idf is the product of term frequency and inverse
document frequency ? idf(ti) = log Nni where N is
the number of documents in the corpus, and ni is
the number of documents that contain the term ti.
For the BBC Dataset, we base the idf weights on
the document frequency of the training articles. For
UNT, we use the reported tf*idf score which uses the
British National Corpus to calculate the idf scores.7
6mallet.cs.umass.edu
7We also ran tf*idf where for each document we recalcu-
late idf using the other 299, but it didn?t make any meaningful
difference.
175
4.3 Corpus Frequency
Image annotations in both NLP and CV tend to be
distributed with a relatively small number of fre-
quently occuring keywords, and a long tail of key-
words that only appear a few times. For UNT, we
use the total keyword frequency of all the gold an-
notations, except for the one document that we are
currently scoring. For BBC, we only measure the
frequency of keywords in the training set captions,
since we are specifically interested in the frequency
of terms in captions.
4.4 Document Title
For BBC, the news article headline, and for UNT,
the title of the webpage.
4.5 Sentence Extraction
Our baseline extracts the most central sentence from
the co-occurring text and uses descriptive words
from that sentence as the image annotation. Un-
like sentence extraction techniques from Feng and
Lapata (2010a), we determine which sentence to ex-
tract using the term frequency distribution directly.
We extract the sentence with the minimum KL-
divergence to the entire document.8
5 BBC Dataset Experiments
5.1 System Comparison
In addition to the baselines, we compare against the
Mix LDA system from Feng and Lapata (2010b). In
Mix LDA, each instance is represented as a bag of
textual features (unigrams) and visual features (SIFT
features quantized to discrete ?image words? using
k-means). A Latent Dirichlet Allocation topic model
is trained on articles, images, and captions from the
training set. Keywords are generated for an unseen
image and article pair by estimating the distribution
of topics that generates the test instance, then multi-
plying them with the word distributions in each topic
to find the probability of textual keywords for the
image. Text LDA is is the same model but only us-
ing words and not image features.
8One could also think of this as a version of the KLSum
summarization system (Haghighi and Vanderwende, 2009) that
stops after one sentence.
5.2 Evaluation
The evaluation metric and the source of gold anno-
tations is described in Table 1. For the baselines 4.1,
4.2, 4.3 and the Mix LDA system, the generated an-
notation for each test image is its ten most likely
keywords. We also run all baselines and the Mix
LDA system on an unpruned version of the dataset,
where infrequent terms are not removed from train-
ing data, test data, or the gold annotations. The pur-
pose of this evaluation is to see if candidate key-
words deemd ?unlearnable? by the Mix LDA system
can be learned by the baselines.
5.3 Results
The evaluation results for the BBC Dataset are
shown in Table 2. Clearly, term frequency is a
stronger baseline than tf*idf by a large margin. The
reason for this is simple: since nearly all of BBC?s
function words are removed during preprocessing,
the only words downweighted by the idf score are
common ? but meaningful ? words such as police or
government. This is worth pointing out because, in
many cases, the choice of using a term frequency or
tf*idf baseline is made based on what was used in
previous work. As we show here and in Section 6.3,
the choice of frequency baseline should be based on
the data and processing techniques being used.
We use the corpus frequency baseline to illus-
trate the difference between standard and include-
infrequent evaluations. Since including infrequent
words doesn?t change which are most frequent in
the dataset, precision for corpus frequency doesn?t
change. But since infrequent words are now in-
cluded in the evaluation data, we see a 0.5% drop in
recall (since corpus frequency won?t capture infre-
quent words). Compared to the other baselines, this
is not a large difference. Other baselines see a larger
drop in recall because they have both more gold key-
words to estimate and more candidate keywords to
consider. tf*idf is the most affected by this, because
idf overly favors very infrequent keywords, despite
their low term frequency. In comparison, the term
frequency baseline is not as negatively affected and
even improves in precision because there are some
cases where a word is very important to an article
in the test set but just didn?t appear very often in the
training set (see Table 3 for examples). But the base-
176
Standard Include-infrequent
Precision Recall F1 Precision Recall F1
Term Frequency 13.13 27.84 17.84 13.62 25.71 17.81
tf * idf 9.21 19.97 12.61 7.25 13.52 9.44
Doc Title 17.23 13.70 15.26 15.91 11.86 13.59
Corpus Frequency 3.17 6.52 4.26 3.17 6.02 4.15
Sentence Extraction 16.67 15.61 16.13 18.62 16.83 17.68
Mix LDA 7.30 16.16 10.06 7.50 13.98 9.76
Text LDA 8.38 17.46 11.32 7.79 14.52 10.14
Table 2: Image annotation results for previous systems and our proposed baselines on the BBC Dataset.
Cadbury increase
contamination
testing level
malaria parasite
spread mosquito
Table 3: Examples of gold annotations from the test sec-
tion of the BBC Dataset. The bolded words are the ones
that appear five or more times in the training set; the un-
bolded words appear fewer than five times and would be
removed from both the candidate and gold keywords in
the standard BBC evaluation.
lines with the best precision are the Doc Title and
Sentence Extraction baselines, which do not need to
generate ten keywords for every image.
While sentence extraction has a lower recall than
term frequency, it is the only baseline or system
that has improved recall when including infrequent
words. This is unexpected because our baseline se-
lects a sentence based on the term frequency of the
document, and the recall for term frequency fell.
One possible explanation is that extraction implic-
itly uses correlations between keywords. Probabili-
ties of objects appearing together in an image are not
independent; and the accuracy of annotations can be
improved by generating annotation keywords as a
set (Moran and Lavrenko, 2011). Recent works in
image captioning also use these correlations: explic-
itly, using graphical models (Kulkarni et al, 2011;
Yang et al, 2011); and implicitly, using language
models (Feng and Lapata, 2010a). In comparison,
sentence extraction is very implicit.
Unsurprisingly, the Text LDA and Mix LDA sys-
tems do worse on the include-infrequent evaluation
than they do on the standard, because words that
do not appear in the training set will not have high
probability in the trained topic models. We were un-
able to reproduce the reported scores for Mix LDA
from Feng and Lapata (2010b) where Mix LDA?s
scores were double the scores of Text LDA (see
Footnote 4). We were also unable to reproduce re-
ported scores for tf*idf and Doc Title (Feng and Lap-
ata, 2008). However, we have three reasons why we
believe our results are correct. First, BBC has more
keywords, and fewer images, than typically seen in
CV datasets. The BBC dataset is simply not suited
for learning from visual data. Second, a single SIFT
descriptor describes which way edges are oriented
at a certain point in an image (Lowe, 1999). While
certain types of edges may correlate to visual objects
also described in the text, we do not expect SIFT fea-
tures to be as informative as textual features for this
task. Third, we refer to the best system scores re-
ported by Leong et al (2010), who evaluate their text
mining system (see section 6.1) on the standard BBC
dataset.9 While their f1 score is slightly worse than
our term frequency baseline, they do 4.86% better
than tf*idf. But, using the baselines reported in Feng
and Lapata (2008), their improvement over tf*idf is
12.06%. Next, we compare their system against fre-
quency baselines using the 10 keyword generation
task on the UNT dataset (the oot normal scores in
table 5). Their best system performs 4.45% better
9Combined model; precision: 13.38, recall: 25.17, f1:
17.47. Crucially, they do not reimplement previous systems or
baselines, but use scores reported from Feng and Lapata (2008).
177
than term frequency, and 0.55% worse than tf*idf.10
Although it is difficult to compare different datasets
and evaluation metrics, our baselines for BBC seem
more reasonable than the reported baselines, given
their relative performance to Leong et als system.
6 UNT Dataset Experiments
6.1 System Comparison
We evaluate against the text mining system from
(Leong et al, 2010). Their system generates image
keywords by extracting text from the co-occurring
text of an image. It uses three features for select-
ing keywords. Flickr Picturability queries the Flickr
API with words from the text in order to find re-
lated image tags. Retrieved tags that appear as sur-
face forms in the text are rewarded proportional to
their frequency in the text. Wikipedia Salience as-
signs scores to words based on a graph-based mea-
sure of importance that considers each term?s docu-
ment frequency in Wikipedia. Pachinko Allocation
Model is a topic model that captures correlations be-
tween topics (Li and McCallum, 2006). PAM infers
subtopics and supertopics for the text, then retrieves
top words from the top topics as annotations. There
is also a combined model of these features using an
SVM with 10-fold cross-validation.
6.2 Evaluation
Evaluation on UNT uses a framework originally de-
veloped for the SemEval lexical substitution task
(McCarthy and Navigli, 2007). This framework
accounts for disagreement between annotators by
weighting each generated keyword by the number of
human annotators who also selected that keyword.
The scoring framework consists of four evaluation
measures: best normal, best mode, oot (out-of-ten)
normal, and oot mode.11
The two best evaluations find the accuracy of a
single ?best? keyword generated by the system12.
10And as we stated earlier, the relative performance of term
frequency vs tf*idf is different from dataset to dataset.
11Both the original framework and its adaptation by Leong
et al (2010) give precision and recall for each of the evaluation
measures. However, precision and recall are identical for all
baselines and systems, and only slightly different on the upper
bound (human) scores. To preserve space, we only present the
metric and scores for precision.
12In contrast to the original SemEval task, where systems can
Best normal measures the accuracy for each system
annotation aj as the number of times aj appears in
the Rj , the multi-set union of human tags, and aver-
ages over all the test images.
Bestnormal =
?
ij?I
|aj?Rj |
|Rj |
|I|
In oot normal, up to ten unordered guesses can be
made without penalty.
ootnormal =
?
ij?I
?
aj?Aj
|aj?Rj |
|Rj |
|I|
where Aj is the set of ten system annotations for
image ij .
The best mode and oot mode metrics are the same
as the normal metrics except they only evaluate sys-
tem annotations for images where Rj contains a sin-
gle most frequent tag. We use the scoring software
provided by SemEval13 with the gold annotation file
provided in the UNT Dataset.
6.3 Results
The results of the lexical substitution evaluation on
the UNT Dataset are shown in Table 5. The results
from the normal show support for our earlier idea
that the relative performance of term frequency vs
tf*idf depends on the dataset. Although the term fre-
quency baseline uses a stopword list, there are other
words that appear frequently enough to suggest they
are not meaningful to the document ? such as copy-
right disclaimers.
Recall that the mode evaluation is only measured
on data instances where the gold annotations have
a single most frequent keyword. While running
the evaluation script on the gold annotation file that
came with the UNT dataset, we discover that Se-
mEval only identifies 28 of the 300 instances as hav-
ing a single mode annotation, and that for 21 of
those 28 instances, the mode keyword is ?cartoon?.
Those 21/28 images correspond to the 75% best
mode score obtained by Corpus Frequency baseline.
Given the small number of instances that actually
make from zero to many ?best? guesses, penalized by the total
number of guesses made.
13http://nlp.cs.swarthmore.edu/semeval/
tasks/task10/data.shtml
178
cartoon(6), market(5), market share(5),
declin(3), imag(3), share(3), pictur(1),
illustr(1), cartoonstock(1), origin(1),
artist(1), meet(1), jfa0417(1), meeting-
copyright(1)
cartoon(6), bill gate(5), gate(4), monop-
oli(4), pearli gate(4), bill(3), imag(3),
caricatur(2), pictur(2), illustr(1), copy-
right(1), artist(1), own(1), pearli(1)
lift index(5), gener(3), index(3), con-
dit(2), comput(2), comput gener(2),
unstabl(2), zone(2), area(1), field(1),
between(1), stabl(1), encyclopedia(1),
thunderstorm(1), lift(1), free encyclope-
dia(1), wikipedia(1)
Table 4: Examples of gold annotations from the UNT Dataset.
Best Out-of-ten (oot)
Normal Mode Normal Mode
Term Frequency 5.67 14.29 33.40 89.29
tf * idf 5.94 14.29 38.40 78.57
Doc Title 6.40 7.14 35.19 92.86
Corpus Frequency 2.54 75.00 8.22 82.14
Flickr Picturability 6.32 78.57 35.61 92.86
Wikipedia Salience 6.40 7.14 35.19 92.86
Topic Model (PAM) 5.99 42.86 37.13 85.71
Combined (SVM) 6.87 67.49 37.85 100.00
Table 5: Image annotation results for our proposed baselines, the text mining systems from (Leong et al, 2010)
count towards these metrics, we conclude that mode
evaluation is not a meaningful way to compare im-
age annotation systems on the UNT dataset.
That said, the number of cartoons in the dataset
does seem to be strikingly high. Looking at the
source of the images, we find that 45 of the 300
images were collected from a single online cartoon
library. Predictably, we find that the co-occurring
text to these images contains a long list of keywords,
and little other text that is relevant to the image. We
looked at a small sample of the rest of the dataset
and found that many of the other text documents in
UNT also have keyword lists.
Including this types of text in a general web cor-
pus is not necessarily a problem, but it?s difficult to
measure the benefits of using complex techniques
like topic modeling and graph similarily to find and
extract annotations when in so many cases the anno-
tations have already been found and extracted. This
is shown in the normal evaluation results, where the
combined system is only slightly better at selecting
the single best keyword, and no better than tf*idf for
the out-of-ten measure.
7 Conclusion
The intent of this paper is not to teach researchers
how to inflate their own results, but to encourage bet-
ter practices. With that purpose in mind, we make
the following suggestions regarding future work in
this area:
179
Get to know your data. The ability to quickly
and cheaply collect very large ? but very noisy ? col-
lections of data from the internet is a great advance
for both NLP and CV research. However, there still
needs to be an appopriate match betwen the task be-
ing performed, the system being proposed, and the
dataset being used; and large noisy datasets can hide
unintended features or incorrect assumptions about
the data.
Use relevant gold annotations. Do not convert
other sources of data into annotations. When collect-
ing human annotations, avoid postprocessing steps
such as merging or deleting keywords that change
the annotators? original intent. Keep an open di-
alogue with annotators about issues that they find
confusing, since that is a sign of an ill-formed task.
Preprocessing should be simple and reprodu-
cable. The use of different preprocessing proce-
dures affects the apparent performance of systems
and sometimes has unintended consequences.
Use strong baselines and compare to other work
only when appropriate. Systems developed for dif-
ferent tasks or datasets can make for misleading
comparisons if they don?t use all features available.
Strong baselines explicitly exploit low-level features
that are implicitly exploited by proposed systems, as
well as low-level features of the dataset.
Don?t remove keywords from gold annotations.
Just because keywords are impossible for one sys-
tem to learn, does not mean they are impossible for
all systems to learn. Removing evaluation data arti-
ficially inflates system scores and limits comparison
to related work.
If a proposed system is to learn associations be-
tween visual and textual features, then it is neces-
sary to use larger datasets. In general, global an-
notations, such as scenes, is easiest; identifying spe-
cific objects is more difficult; and identification of
events, activities, and other abstract qualities has a
very low success rate (Fluhr et al, 2006). Alter-
nately, use simpler image features that are known
to have a high sucess rate. For example, Deschacht
and Moens (2007) used a face detector to determine
the number of faces in an image, and then used NLP
to determine the names of those people from associ-
ated text.
References
K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D.M.
Blei, and M.I. Jordan. 2003. Matching words and
pictures. The Journal of Machine Learning Research,
3:1107?1135.
Tamara L. Berg and David A. Forsyth. 2006. Animals on
the web. In Proceedings of the 2006 IEEE Computer
Society Conference on Computer Vision and Pattern
Recognition - Volume 2, CVPR ?06, pages 1463?1470,
Washington, DC, USA. IEEE Computer Society.
T.L. Berg, A.C. Berg, J. Edwards, M. Maire, R. White,
Yee-Whye Teh, E. Learned-Miller, and D.A. Forsyth.
2004. Names and faces in the news. In Computer Vi-
sion and Pattern Recognition, 2004. CVPR 2004. Pro-
ceedings of the 2004 IEEE Computer Society Confer-
ence on, volume 2, pages II?848 ? II?854 Vol.2, june-2
july.
E. Boiy, K. Deschacht, and M.-F. Moens. 2008. Learn-
ing visual entities and their visual attributes from text
corpora. In Database and Expert Systems Applica-
tion, 2008. DEXA ?08. 19th International Workshop
on, pages 48 ?53, sept.
Koen Deschacht and Marie-Francine Moens. 2007. Text
analysis for automatic image annotation. In ACL, vol-
ume 45, page 1000.
Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learn-
ing generative visual models from few training ex-
amples: An incremental bayesian approach tested on
101 object categories. In Proceedings of the 2004
Conference on Computer Vision and Pattern Recog-
nition Workshop (CVPRW?04) Volume 12 - Volume
12, CVPRW ?04, pages 178?, Washington, DC, USA.
IEEE Computer Society.
Yansong Feng and Mirella Lapata. 2008. Automatic im-
age annotation using auxiliary text information. Pro-
ceedings of ACL-08: HLT, pages 272?280.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In HLT-
NAACL, pages 831?839.
Yansong Feng. 2011. Automatic caption generation for
news images. Ph.D. thesis, University of Edinburgh.
Christian Fluhr, Pierre-Alain Mollic, and Patrick Hde.
2006. Usage-oriented multimedia information re-
trieval technological evaluation. In James Ze Wang,
Nozha Boujemaa, and Yixin Chen, editors, Multime-
dia Information Retrieval, pages 301?306. ACM.
180
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370, Boulder, Colorado, June. Association
for Computational Linguistics.
Allan Hanbury. 2008. A survey of methods for image
annotation. J. Vis. Lang. Comput., 19:617?627, Octo-
ber.
Nicolas Herve? and Nozha Boujemaa. 2007. Image an-
notation: which approach for realistic databases? In
Proceedings of the 6th ACM international conference
on Image and video retrieval, CIVR ?07, pages 170?
177, New York, NY, USA. ACM.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In CVPR, pages 1601?
1608.
Chee Wee Leong, Rada Mihalcea, and Samer Hassan.
2010. Text mining for automatic image tagging. In
COLING, pages 647?655.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: Dag-structured mixture models of topic correla-
tions. In Proceedings of the 23rd international confer-
ence on Machine learning, ICML ?06, pages 577?584,
New York, NY, USA. ACM.
D.G. Lowe. 1999. Object recognition from local scale-
invariant features. In Computer Vision, 1999. The Pro-
ceedings of the Seventh IEEE International Confer-
ence on, volume 2, pages 1150 ?1157 vol.2.
Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Kumar.
2008. A new baseline for image annotation. In Pro-
ceedings of the 10th European Conference on Com-
puter Vision: Part III, ECCV ?08, pages 316?329,
Berlin, Heidelberg. Springer-Verlag.
A. Makadia, V. Pavlovic, and S. Kumar. 2010. Baselines
for image annotation. International Journal of Com-
puter Vision, 90(1):88?105.
D. McCarthy and R. Navigli. 2007. Semeval-2007 task
10: English lexical substitution task. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations (SemEval-2007), pages 48?53.
Florent Monay and Daniel Gatica-Perez. 2003. On im-
age auto-annotation with latent space models. In Pro-
ceedings of the eleventh ACM international conference
on Multimedia, Multimedia ?03, pages 275?278, New
York, NY, USA. ACM.
S. Moran and V. Lavrenko. 2011. Optimal tag sets for
automatic image.
Henning Mu?ller, Ste?phane Marchand-Maillet, and
Thierry Pun. 2002. The truth about corel - evaluation
in image retrieval. In Proceedings of the International
Conference on Image and Video Retrieval, CIVR ?02,
pages 38?49, London, UK, UK. Springer-Verlag.
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization. Technical report,
Microsoft Research.
J. Ponce, T. Berg, M. Everingham, D. Forsyth, M. Hebert,
S. Lazebnik, M. Marszalek, C. Schmid, B. Russell,
A. Torralba, C. Williams, J. Zhang, and A. Zisser-
man. 2006. Dataset issues in object recognition.
In Jean Ponce, Martial Hebert, Cordelia Schmid, and
Andrew Zisserman, editors, Toward Category-Level
Object Recognition, volume 4170 of Lecture Notes
in Computer Science, pages 29?48. Springer Berlin /
Heidelberg. 10.1007/11957959 2.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of international
conference on new methods in language processing,
volume 12, pages 44?49. Manchester, UK.
Jiayu Tang and Paul Lewis. 2007. A study of quality
issues for image auto-annotation with the corel data-
set. IEEE Transactions on Circuits and Systems for
Video Technology, Vol. 1(NO. 3):384?389, March.
T. Tsikrika, A. Popescu, and J. Kludas. 2011. Overview
of the wikipedia image retrieval task at imageclef
2011. In CLEF (Notebook Papers/LABs/Workshops):
CLEF.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, CHI ?04, pages 319?326, New York, NY,
USA. ACM.
Thijs Westerveld and Arjen P. de Vries. 2003. Ex-
perimental evaluation of a generative probabilistic im-
age retrieval model on ?easy? data. In In Proceedings
of the SIGIR Multimedia Information Retrieval Work-
shop 2003, Aug.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing (EMNLP), Edinburgh,
Scotland.
181
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 69?76,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Domain-Independent Captioning of Domain-Specific Images
Rebecca Mason
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
rebecca@cs.brown.edu
Abstract
Automatically describing visual content is an
extremely difficult task, with hard AI prob-
lems in Computer Vision (CV) and Natural
Language Processing (NLP) at its core. Pre-
vious work relies on supervised visual recog-
nition systems to determine the content of im-
ages. These systems require massive amounts
of hand-labeled data for training, so the num-
ber of visual classes that can be recognized is
typically very small. We argue that these ap-
proaches place unrealistic limits on the kinds
of images that can be captioned, and are un-
likely to produce captions which reflect hu-
man interpretations.
We present a framework for image caption
generation that does not rely on visual recog-
nition systems, which we have implemented
on a dataset of online shopping images and
product descriptions. We propose future work
to improve this method, and extensions for
other domains of images and natural text.
1 Introduction
As the number of images on the web continues to in-
crease, the task of automatically describing images
becomes especially important. Image captions can
provide background information about what is seen
in the image, can improve accessibility of websites
for visually-impaired users, and can improve im-
age retrieval by providing text to search user queries
against. Typically, online search engines rely on col-
located textual information to resolve queries, rather
than analyzing visual content directly. Likewise,
earlier image captioning research from the Natural
Language Processing (NLP) community use collo-
cated information such as news articles or GPS co-
ordinates, to decide what information to include in
the generated caption (Deschacht and Moens, 2007;
Aker and Gaizauskas, 2010; Fan et al, 2010; Feng
and Lapata, 2010a).
However, in some instances visual recognition is
necessary because collocated information is miss-
ing, irrelevant, or unreliable. Recognition is a clas-
sic Computer Vision (CV) problem including tasks
such as recognizing instances of object classes in
images (such as car, cat, or sofa); classifying
images by scene (such as beach or forest); or
detecting attributes in an image (such as wooden
or feathered). Recent works in image caption
generation represent visual content via the output
of trained recognition systems for a pre-defined set
of visual classes. They then use linguistic models
to correct noisy initial detections (Kulkarni et al,
2011; Yang et al, 2011), and generate more natural-
sounding text (Li et al, 2011; Mitchell et al, 2012;
Kuznetsova et al, 2012).
A key problem with this approach is that it as-
sumes that image captioning is a grounding prob-
lem, with language acting only as labels for visual
meaning. One good reason to challenge this assump-
tion is that it imposes unrealistic constraints on the
kinds of images that can be automatically described.
Previous work only recognizes a limited number of
visual classes ? typically no more than a few dozen
in total ? because training CV systems requires a
huge amount of hand-annotated data. For example,
the PASCAL VOC dataset1 has 11,530 training im-
1http://pascallin.ecs.soton.ac.uk/
69
ages with 27,450 labeled objects, in order to learn
only 20 object classes. Since training visual recog-
nition systems is such a burden, ?general-domain?
image captioning datasets are limited by the current
technology. For example, the SBU-Flickr dataset
(Ordonez et al, 2011), which contains 1 million im-
ages and captions, is built by first querying Flickr
using a pre-defined set of queries, then further filter-
ing to remove instances where the caption does not
contain at least two words belonging to their term
list. Furthermore, detections are too noisy to gener-
ate a good caption for the majority of images. For
example, Kuznetsova et al (2012) select their test
set according to which images receive the most con-
fident visual object detection scores.
We instead direct our attention to the domain-
specific image captioning task, assuming that we
know a general object or scene category for the
query image, and that we have access to a dataset of
images and captions from the same domain. While
some techniques may be unrealistic in assuming that
high-quality collocated text is always available, as-
suming that there is no collocated information at
all is equally unrealistic. Data sources such as
file names, website text, Facebook likes, and web
searches all provide clues to the content of an im-
age. Even an image file by itself carries metadata
on where and when it was taken, and the camera
settings used to take it. Since visual recognition is
much easier for domain-specific tasks, there is more
potential for natural language researchers to do re-
search that will impact the greater community.
Finally, labeling visual content is often not
enough to provide an adequate caption. The mean-
ing of an image to a user is more than just listing the
objects in the image, and can even change for dif-
ferent users. This problem is commonly known as
?bridging the semantic gap?:
?The semantic gap is the lack of coinci-
dence between the information that one
can extract from the visual data and the
interpretation that the same data have for
a user in a given situation. A linguis-
tic description is almost always contex-
tual, whereas an image may live by itself.?
(Smeulders et al, 2000)
challenges/VOC/
General-domain models of caption generation fail to
capture context because they assume that all the rel-
evant information has been provided in the image.
However, training models on data from the same do-
main gives implicit context about what information
should be provided in the generated text.
This thesis proposes a framework for image cap-
tioning that does not require supervision in the form
of hand-labeled examples. We train a topic model on
a corpus of images and captions in the same domain,
in order to jointly learn image features and natural
language descriptions. The trained topic model is
used to estimate the likelihood of words appearing
in a caption, given an unseen query image. We then
use these likelihoods to rewrite an extracted human-
written caption to accurately describe the query im-
age. We have implemented our framework using a
dataset of online shopping images and captions, and
propose to extend this model to other domains, in-
cluding natural images.
2 Framework
In this section, we provide an overview of our im-
age captioning framework, as it is currently imple-
mented. As shown in Figure 1, the data that we use
are a set of images and captions in a specific do-
main, and a query image that is from the same do-
main, but is not included in the training data. The
training data is used in two ways: for sentence ex-
traction from the captions of training images that
are visually similar to the query image overall; and
for training a topic model of individual words and
local image features, in order to capture fine-grained
details. Finally, a sentence compression algorithm
is used to remove details from the extracted captions
that do not fit the query image.
The work that we have done so far has been imple-
mented using the Attribute Discovery Dataset (Berg
et al, 2010), a publicly available dataset of shop-
ping images and product descriptions.2 Here, we
run our framework on the women?s shoes section,
which has over 14000 images and captions, rep-
resenting a wide variety of attributes for texture,
shapes, materials, colors, and other visual quali-
ties. The women?s shoes section is formally split
2http://tamaraberg.com/
attributesDataset/index.html
70
Figure 1: Overview of our framework for image caption generation.
into ten subcategories, such as wedding shoes,
sneakers, and rainboots. However, many of
the subcategories contain multiple visually distinct
kinds of shoes. We do not make use of the sub-
categories, instead we group all of the categories of
shoe images together. The shoes in the images are
mostly posed against solid color backgrounds, while
the captions have much more variability in length
and linguistic quality.
For our thesis work, we intend to extend our cur-
rent framework to different domains of data, includ-
ing natural images. However, it is important to point
out that no part of the framework as it is currently
implemented is specific to describing shoes or shop-
ping images. This will be described in Section 4.
2.1 Sentence Extraction
GIST (Oliva and Torralba, 2001) is a global image
descriptor which describes how gradients are ori-
ented in different regions of an image. It is com-
monly used for classifying background scenes in
images, however images in the Attribute Discovery
Dataset do not have ?backgrounds? per se. Instead,
we treat the overall shape of the object as the ?scene?
and extract a caption sentence using GIST nearest
neighbors between the query image and the images
in the training set. Because similar objects and at-
tributes tend to appear in similar scenes, we expect
that at least some of the extracted caption will de-
scribe local attributes that are also in the query im-
age. The rest of our framework finds and removes
the parts of the extracted caption that are not accu-
rate to the query image.
2.2 Topic Model
Image captions often act as more than labels of vi-
sual content. Some visual ideas can be described
using several different words, while others are typ-
ically not described at all. Likewise, some words
describe background information that is not shown
visually, or contextual information that is interpreted
by the user. Rather than modeling images and text
such that one generates the other, we a topic model
based on LDA (Blei et al, 2003) where both an im-
age and its caption are generated by a shared latent
distribution of topics.
Previous work by (Feng and Lapata, 2010b)
shows that topic models where image features or re-
gions generate text features (such as Blei and Jor-
dan (2003)) are not appropriate for modeling images
with captions or other collocated text. We use a topic
model designed for multi-lingual data, specifically
the Polylingual Topic Model (Mimno et al, 2009).
This model was developed for correlated documents
in different languages that are topically similar, but
are not direct translations, such as Wikipedia or
news articles in different languages. We train the
topic model with images and text as two languages.
For query images, we estimate the topic distribu-
tion that generated just the image, and then In the
model, images and their captions are represented us-
ing bag-of-words, a commonly-used technique for
document representation in both CV and NLP re-
search. The textual features are non-function words
in the model, including words that describe specific
objects or attributes (such as boot, snake-skin,
buckle, and metallic) in addition to words that
describe more abstract attributes and affordances
(such as professional, flirty, support,
71
Original: Go all-out glam in the shimmer-
ing Dyeables Roxie sandals. Metallic faux
leather upper in a dress thong sandal style
with a round open toe. ...
Original: Find the softness of shearling combined with sup-
port in this clog slipper. The cork footbed mimics the foot?s
natural shape, offering arch support, while a flexible outsole
flexes with your steps and resists slips. ...
Original: Perforated leather with cap toe and
bow detail.
Extracted: Shimmering snake-
embossed leather upper in a slingback
evening dress sandal style with a round
open toe .
Extracted: This sporty sneaker clog
keeps foot cool and comfortable and
fully supported.
Extracted: Italian patent leather peep-
toe ballet flat with a signature tailored
grosgrain bow .
System: Shimmering upper in a sling-
back evening dress sandal style with a
round open toe .
System: This clog keeps foot comfort-
able and supported.
System: leather ballet flat with a signa-
ture tailored grosgrain bow .
Table 1: Some examples of shoes images from the Attribute Discovery Dataset and performance with our image
captioning model. Left: Correctly removes explicitly visual feature ?snake-embossed leather? from extraction; leaves
in correct visual attributes ?shimmering?, ?slingback?, and ?round open toe?. Center: Extracted sentence with some
contextually visual attributes; the model correctly infers that ?sporty? and ?cool? are not likely given an image of a
wool bedroom slipper, but ?comfortable? and ?supported? are likely because of the visible cork soles. Right: Extracted
sentence with some non-visual attributes; model removes ?Italian? but keeps ?signature tailored?.
and waterproof). For ?image words?, we com-
pute features at several points in the image such as
the color values of pixels, the angles of edges or
corners, and response to various filters, and cluster
them into discrete image words. However, the in-
formation that an image word conveys is very dif-
ferent than the information conveyed in a text word,
so models which require direct correspondence be-
tween features in the two modalities would not be
appropriate here.
We train the topic model with images and text as
two languages. We estimate the probabilities of tex-
tual words given a query image by first estimating
the topic distribution that generated the image, and
then using the same distribution to find the probabil-
ities of textual words given the query image. How-
ever, we also perform an annotation task similarly
to Feng and Lapata (2010b), in order to evaluate
the topic model on its own. Our method has a 30-
35% improvement in finding words from the held-
out image caption, compared to previous methods
and baselines.
2.3 Sentence Compression via Caption
Generation
We describe an ILP for caption generation, draw-
ing inspiration from sentence compression work by
Clarke and Lapata (2008). The ILP has three in-
puts: the extracted caption; the prior probabilities
words appearing in captions, p(w); and their pos-
terior probabilities of words appearing in captions
given the query image, p(w|query). The latter is
estimated using the topic model we have just de-
scribed. The output of the ILP is a compressed im-
age caption where the inaccurate words have been
deleted.
Objective: The formal ILP objective3 is to max-
imize a weighted linear combination of two mea-
sures. The first we define as?ni=1 ?i ? I(wi), where
wi, ..., wn are words in the extracted caption, ?i is a
binary decision variable which is true if we include
wi in the compressed output, and I(wi) is a score for
the accuracy of each word. For non-function words,
3To formulate this problem as a linear program, the proba-
bilities are actually log probabilities, but we omit the logs in this
paper to save space.
72
I(wi) = p(w|query)?p(w), which can have a pos-
itive or negative value. We do not use p(wi|query)
directly in order to distinguish between cases where
p(wi|query) is low because wi is inaccurate, and
cases where p(wi|query) is low because p(wi) is
low generally. Function words do not affect the ac-
curacy of the generated caption, so I(wi) = 0.
The second measure in the objective is a tri-
gram language model, described in detail in Clarke
(2008). In the original sentence compression task,
the language model is a component as it naturally
prefers shorter output sentences. However, our ob-
jective is not to generate a shorter caption, but to
generate a more accurate caption. However, we still
include the language model in the objective, with a
weighting factor , as it helps remove unnecessary
function words and help reduce the search space of
possible sentence compressions.
Constraints: The ILP constraints include sequen-
tial constraints to ensure the mathematical validity
of the model, and syntactic constraints that ensure
the grammatical correctness of the compressed sen-
tence. We do not have space here to describe all
of the constraints, but basically, using the ?semantic
head? version of the headfinder from Collins (1999),
we constrain that the head word of the sentence and
the head word of the sentence?s object cannot be
deleted, and for any word that we include in the out-
put sentence, we must include its head word as well.
We also have constraints that define valid use of co-
ordinating conjunctions and punctuation.
We evaluate generated captions using automatic
metrics such as BLEU (Papineni et al, 2002) and
ROUGE (Lin, 2004). These metrics are commonly
used in summarization and translation research and
have been previously used in image captioning re-
search to compare automatically generated captions
to human-written captions for each image (Ordonez
et al, 2011; Yang et al, 2011; Kuznetsova et al,
2012). Although human-written captions may use
synonyms to describe a visual object or attribute, or
even describe entirely different attributes than what
is described in the generated captions, computing
the automatic metrics over a large test set finds sta-
tistically significant improvements in the accuracy
of the extracted and compressed captions over ex-
traction alone.
For our proposed work (Section 4), we also plan
to perform manual evaluations of our captions based
on their content and language quality. However,
cross-system comparisons would be more difficult
because our method uses an entirely different kind
of data. In order to compare our work to related
methods (Section 3), we would have to train for vi-
sual recognition systems for hundreds of visual at-
tributes, which would mean having to hand-label the
entire dataset.
3 Related Work in Image Captioning
In addition to visual recognition, caption genera-
tion is a very challenging problem. In some ap-
proaches, sentences are constructed using templates
or grammar rules, where content words are selected
according to the output of visual recognition systems
(Kulkarni et al, 2011; Yang et al, 2011; Mitchell et
al., 2012). Function words, as well as words like
verbs and prepositions which are difficult to rec-
ognize visually, may be selected using a language
model trained on non-visual text. There is also simi-
lar work that uses large-scale ngram models to make
the generated output sound more natural (Li et al,
2011).
In other approaches, captions are extracted in
whole or in part from similar images in a database.
For example, Farhadi et al (2010) and Ordonez et
al. (2011) build semantic representations for visual
content of query images, and extract captions from
database images with similar content. Kuznetsova et
al. (2012) extract phrases corresponding to classes of
objects and scenes detected in the query image, and
combine extracted phrases into a single sentence.
Our work is different than these approaches, because
we directly measure how visually relevant individual
words are, rather than only using visual similarity to
extract sentences or phrases.
Our method is most similar to that of Feng and
Lapata (2010a), who generate captions for news im-
ages. Like them, we train an LDA-like model on
both images and text to find latent topics that gener-
ate both. However, their model requires both an im-
age and collocated text (a news article) to estimate
the topic distribution for an unseen image, while our
topic model only needs related text for the training
data. They also use the news article to help gen-
erate captions, which means that optimizing their
73
generated output for content and grammaticality is
a much easier problem. Although their model com-
bines phrases and n-grams from different sentences
to form an image caption, they only consider the text
from a single news article for extraction, and they
can assume that the text is mostly accurate and rele-
vant to the content of the image.
In this sense, our method is more like Kuznetsova
et al (2012), which also uses an Integer Linear Pro-
gram (ILP) to rapidly optimize how well their gen-
erated caption fits the content of the image model.
However, it is easier to get coherent image captions
from our model since we are not combining parts
of sentences from multiple sources. Since we build
our output from extracted sentences, not phrases, our
ILP requires fewer grammaticality and coherence
constraints than it would for building new sentences
from scratch. We also model how relevant each in-
dividual word is to the query image, while they ex-
tract phrases based on visual similarity of detected
objects in the images.
4 Proposed Work
One clear direction for future work is to extend our
image captioning framework to natural images. By
?natural images? we refer to images of everyday
scenes seen by people, unlike the shopping images,
where objects tend to be posed in similar positions
against plain backgrounds. Instead of domains such
as handbags and shoes, we propose to cluster the
training data based on visual scene domains such as
mountains, beaches, and living rooms. We are par-
ticularly interested in the scene attributes and clas-
sifiers by Patterson and Hays (2012) which builds
an attribute-based taxonomy of scene types using
crowd-sourcing, rather than categorical scene types
which are typically used.
Visual recognition is generally much more diffi-
cult in natural scenes than in posed images, since
lighting and viewpoints are not consistent, and ob-
jects may be occluded by other objects or truncated
by the edge of the image. However, we are opti-
mistic because we do not need to solve the general
visual recognition task, since our model only learns
how visual objects and attributes appear in specific
domains of scenes, a much easier problem. Addi-
tionally, the space of likely objects and attributes to
detect is limited by what typically appears in that
type of scene. Finally, we can use the fact that our
image captioning method is not grounded in our fa-
vor, and assume that if an object is partially occluded
or truncated in an image, than it is less likely that
the photographer considered that object to be inter-
esting, so it is not as important whether that object
is described in the caption or not.
Finally, there is also much that could be done to
improve the text generation component on its own.
Our framework currently extracts only a single cap-
tion sentence to compress, while recent work in
summarization has focused on the problem of learn-
ing how to jointly extract and compress (Martins and
Smith, 2009; Berg-Kirkpatrick et al, 2011). Since
a poor extraction choice can make finding an accu-
rate compression impossible, we should also study
different methods of extraction to learn about what
kinds of features are most likely to help us find good
sentences. As mentioned in Section 2.1, we have
already found that global feature descriptors are bet-
ter than bag of image word descriptors for extract-
ing sentences to use in image caption compressions
in the shopping dataset. As we extend our frame-
work to other domains of images, we are interested
in finding whether scene-based descriptors and clas-
sifiers in general are better at finding good sentences
than local descriptors, and whether there is a con-
nection between region and phrase-based detectors
correlating better with sentence and phrase-length
text, while local image descriptors are more related
to single words. Finding patterns like this in visual
text in general would be helpful for many other tasks
besides image captioning.
References
Ahmet Aker and Robert Gaizauskas. 2010. Generating
image descriptions using dependency relational pat-
terns. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, ACL
?10, pages 1250?1258, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and character-
ization from noisy web data. In Proceedings of
the 11th European conference on Computer vision:
Part I, ECCV?10, pages 663?676, Berlin, Heidelberg.
Springer-Verlag.
74
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages 481?
490, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David M. Blei and Michael I. Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th annual in-
ternational ACM SIGIR conference on Research and
development in informaion retrieval, SIGIR ?03, pages
127?134, New York, NY, USA. ACM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. J. Artif. Int. Res., 31(1):399?429,
March.
James Clarke. 2008. Global Inference for Sentence Com-
pression: An Integer Linear Programming Approach.
Dissertation, University of Edinburgh.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
Philadelphia, PA, USA. AAI9926110.
Koen Deschacht and Marie-Francine Moens. 2007. Text
analysis for automatic image annotation. In ACL, vol-
ume 45, page 1000.
Xin Fan, Ahmet Aker, Martin Tomko, Philip Smart, Mark
Sanderson, and Robert Gaizauskas. 2010. Automatic
image captioning from the web for gps photographs.
In Proceedings of the international conference on Mul-
timedia information retrieval, MIR ?10, pages 445?
448, New York, NY, USA. ACM.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences from images. In Proceedings of
the 11th European conference on Computer vision:
Part IV, ECCV?10, pages 15?29, Berlin, Heidelberg.
Springer-Verlag.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In HLT-
NAACL, pages 831?839.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In CVPR, pages 1601?
1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg, and Yejin Choi. 2012. Collective
generation of natural image descriptions. In ACL.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, CoNLL ?11,
pages 220?228, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Andre? F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Pro-
cessing, ILP ?09, pages 1?9, Stroudsburg, PA, USA.
Association for Computational Linguistics.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 880?889, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alexander C. Berg, Tamara L. Berg, and Hal Daume?
III. 2012. Midge: Generating image descriptions
from computer vision detections. In European Chap-
ter of the Association for Computational Linguistics
(EACL).
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42:145?175.
V. Ordonez, G. Kulkarni, and T.L. Berg. 2011. Im2text:
Describing images using 1 million captioned pho-
tographs. In NIPS.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
75
G. Patterson and J. Hays. 2012. Sun attribute database:
Discovering, annotating, and recognizing scene at-
tributes. 2012 IEEE Conference on Computer Vision
and Pattern Recognition, 0:2751?2758.
Arnold W. M. Smeulders, Marcel Worring, Simone San-
tini, Amarnath Gupta, and Ramesh Jain. 2000.
Content-based image retrieval at the end of the early
years. IEEE Trans. Pattern Anal. Mach. Intell.,
22(12):1349?1380, December.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing (EMNLP), Edinburgh,
Scotland.
76
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592?598,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Nonparametric Method for Data-driven Image Captioning
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
{rebecca,ec}@cs.brown.edu
Abstract
We present a nonparametric density esti-
mation technique for image caption gener-
ation. Data-driven matching methods have
shown to be effective for a variety of com-
plex problems in Computer Vision. These
methods reduce an inference problem for
an unknown image to finding an exist-
ing labeled image which is semantically
similar. However, related approaches for
image caption generation (Ordonez et al,
2011; Kuznetsova et al, 2012) are ham-
pered by noisy estimations of visual con-
tent and poor alignment between images
and human-written captions. Our work
addresses this challenge by estimating a
word frequency representation of the vi-
sual content of a query image. This al-
lows us to cast caption generation as an
extractive summarization problem. Our
model strongly outperforms two state-of-
the-art caption extraction systems accord-
ing to human judgments of caption rele-
vance.
1 Introduction
Automatic image captioning is a much studied
topic in both the Natural Language Processing
(NLP) and Computer Vision (CV) areas of re-
search. The task is to identify the visual content
of the input image, and to output a relevant natural
language caption.
Much prior work treats image captioning as
a retrieval problem (see Section 2). These ap-
proaches use CV algorithms to retrieve similar im-
ages from a large database of captioned images,
and then transfer text from the captions of those
images to the query image. This is a challenging
problem for two main reasons. First, visual simi-
larity measures do not perform reliably and do not
Query Image: Captioned Images:
1. 2. 3.
4. 5. 6.
1.) 3 month old baby girl with blue eyes in her crib
2.) A photo from the Ismail?s portrait shoot
3.) A portrait of a man, in black and white
4.) Portrait in black and white with the red rose
5.) I apparently had this saved in black and white as well
6.) Portrait in black and white
Table 1: Example of a query image from the SBU-
Flickr dataset (Ordonez et al, 2011), along with
scene-based estimates of visually similar images.
Our system models visual content using words that
are frequent in these captions (highlighted) and ex-
tracts a single output caption.
capture all of the relevant details which humans
might describe. Second, image captions collected
from the web often contain contextual or back-
ground information which is not visually relevant
to the image being described.
In this paper, we propose a system for transfer-
based image captioning which is designed to ad-
dress these challenges. Instead of selecting an out-
put caption according to a single noisy estimate
of visual similarity, our system uses a word fre-
quency model to find a smoothed estimate of vi-
sual content across multiple captions, as Table 1
illustrates. It then generates a description of the
query image by extracting the caption which best
represents the mutually shared content.
The contributions of this paper are as follows:
592
1. Our caption generation system effectively lever-
ages information from the massive amounts of
human-written image captions on the internet. In
particular, it exhibits strong performance on the
SBU-Flickr dataset (Ordonez et al, 2011), a noisy
corpus of one million captioned images collected
from the web. We achieve a remarkable 34%
improvement in human relevance scores over a
recent state-of-the-art image captioning system
(Kuznetsova et al, 2012), and 48% improvement
over a scene-based retrieval system (Patterson et
al., 2014) using the same computed image fea-
tures.
2. Our approach uses simple models which can
be easily reproduced by both CV and NLP re-
searchers. We provide resources to enable com-
parison against future systems.
1
2 Image Captioning by Transfer
The IM2TEXT model by Ordonez et al (2011)
presents the first web-scale approach to image cap-
tion generation. IM2TEXT retrieves the image
which is the closest visual match to the query im-
age, and transfers its description to the query im-
age. The COLLECTIVE model by Kuznetsova et
al. (2012) is a related approach which uses trained
CV recognition systems to detect a variety of vi-
sual entities in the query image. A separate de-
scription is retrieved for each visual entity, which
are then fused into a single output caption. Like
IM2TEXT, their approach uses visual similarity as
a proxy for textual relevance.
Other related work models the text more di-
rectly, but is more restrictive about the source
and quality of the human-written training data.
Farhadi et al (2010) and Hodosh et al (2013)
learn joint representations for images and cap-
tions, but can only be trained on data with very
strong alignment between images and descriptions
(i.e. captions written by Mechanical Turkers). An-
other line of related work (Fan et al, 2010; Aker
and Gaizauskas, 2010; Feng and Lapata, 2010)
generates captions by extracting sentences from
documents which are related to the query image.
These approaches are tailored toward specific do-
mains, such as travel and news, where images tend
to appear with corresponding text.
1
See http://bllip.cs.brown.edu/
download/captioning_resources.zip or ACL
Anthology.
3 Dataset
In this paper, we use the SBU-Flickr dataset
2
. Or-
donez et al (2011) query Flickr.com using a
huge number of words which describe visual en-
tities, in order to build a corpus of one million
images with captions which refer to image con-
tent. However, further analysis by Hodosh et al
(2013) shows that many captions in SBU-Flickr
(?67%) describe information that cannot be ob-
tained from the image itself, while a substantial
fraction (?23%) contain almost no visually rel-
evant information. Nevertheless, this dataset is
the only web-scale collection of captioned images,
and has enabled notable research in both CV and
NLP.
3
4 Our Approach
4.1 Overview
For a query image I
q
, our task is to generate a rele-
vant description by selecting a single caption from
C, a large dataset of images with human-written
captions. In this section, we first define the feature
space for visual similarity, then formulate a den-
sity estimation problem with the aim of modeling
the words which are used to describe visually sim-
ilar images to I
q
. We also explore methods for
extractive caption generation.
4.2 Measuring Visual Similarity
Data-driven matching methods have shown to be
very effective for a variety of challenging prob-
lems (Hays and Efros, 2008; Makadia et al,
2008; Tighe and Lazebnik, 2010). Typically these
methods compute global (scene-based) descriptors
rather than object and entity detections. Scene-
based techniques in CV are generally more robust,
and can be computed more efficiently on large
datasets.
The basic IM2TEXT model uses an equally
weighted average of GIST (Oliva and Torralba,
2001) and TinyImage (Torralba et al, 2008) fea-
tures, which coarsely localize low-level features
in scenes. The output is a multi-dimensional
image space where semantically similar scenes
(e.g. streets, beaches, highways) are projected
near each other.
2
http://tamaraberg.com/CLSP11/
3
In particular, papers stemming from the 2011 JHU-CLSP
Summer Workshop (Berg et al, 2012; Dodge et al, 2012;
Mitchell et al, 2012) and more recently, the best paper award
winner at ICCV (Ordonez et al, 2013).
593
Patterson and Hays (2012) present ?scene at-
tribute? representations which are characterized
using low-level perceptual attributes as used by
GIST (e.g. openness, ruggedness, naturalness),
as well as high-level attributes informed by open-
ended crowd-sourced image descriptions (e.g., in-
door lighting, running water, places for learning).
Follow-up work (Patterson et al, 2014) shows
that their attributes provide improved matching for
image captioning over IM2TEXT baseline. We
use their publicly available
4
scene attributes for
our experiments. Training set and query images
are represented using 102-dimensional real-valued
vectors, and similarity between images is mea-
sured using the Euclidean distance.
4.3 Density Estimation
As shown in Bishop (2006), probability density
estimates at a particular point can be obtained by
considering points in the training data within some
local neighborhood. In our case, we define some
region R in the image space which contains I
q
.
The probability mass of that space is
P =
?
R
p(I
q
)dI
q
(1)
and if we assume thatR is small enough such that
p(I
q
) is roughly constant in R, we can approxi-
mate
p(I
q
) ?
k
img
n
img
V
img
(2)
where k
img
is the number of images within R in
the training data, n
img
is the total number of im-
ages in the training data, and V
img
is the volume
ofR. In this paper, we fix k
img
to a constant value,
so that V
img
is determined by the training data
around the query image.
5
At this point, we extend the density estima-
tion technique in order to estimate a smoothed
model of descriptive text. Let us begin by consid-
ering p(w|I
q
), the conditional probability of the
word
6
w given I
q
. This can be described using a
4
https://github.com/genp/sun_
attributes
5
As an alternate approach, one could fix the value of
V
img
and determine k
img
from the number of points in R,
giving rise to the kernel density approach (a.k.a. Parzen
windows). However we believe the KNN approach is more
appropriate here, because the number of samples is nearly
10000 times greater than the number of dimensions in the
image representation.
6
Here, we use word to refer to non-function words, and
assume all function words have been removed from the cap-
tions.
Bayesian model:
p(w|I
q
) =
p(I
q
|w)p(w)
p(I
q
)
(3)
The prior for w is simply its unigram frequency in
C, where n
txt
w
and n
txt
are word token counts:
p(w) =
n
txt
w
n
txt
(4)
Note that n
txt
is not the same as n
img
because a
single captioned image can have multiple words
in its caption. Likewise, the conditional density
p(I
q
|w) ?
k
txt
w
n
txt
w
V
img
(5)
considers instances of observed words within R,
although the volume of R is still defined by the
image space. k
txt
w
is the number of times w is used
withinR while n
txt
w
is the total number of times w
is observed in C.
Combining Equations 2, 4, and 5 and canceling
out terms gives us the posterior probability:
p(w|I
q
) =
k
txt
w
k
img
?
n
img
n
txt
(6)
If the number of words in each caption is inde-
pendent of its image?s location in the image space,
then p(w|I
q
) is approximately the observed uni-
gram frequency for the captions insideR.
4.4 Extractive Caption Generation
We compare two selection methods for extractive
caption generation:
1. SumBasic SumBasic (Nenkova and Vander-
wende, 2005) is a sentence selection algorithm for
extractive multi-document summarization which
exclusively maximizes the appearance of words
which have high frequency in the original docu-
ments. Here, we adapt SumBasic to maximize the
average value of p(w|I
q
) in a single extracted cap-
tion:
output = argmax
c
txt
?R
?
w?c
txt
1
|c
txt
|
p(w|I
q
) (7)
The candidate captions c
txt
do not necessarily
have to be observed in R, but in practice we did
not find increasing the number of candidate cap-
tions to be more effective than increasing the size
ofR directly.
594
Figure 1: BLEU scores vs k for SumBasic extrac-
tion.
2. KL Divergence We also consider a KL
Divergence selection method. This method out-
performs the SumBasic selection method for ex-
tractive multi-document summarization (Haghighi
and Vanderwende, 2009). It also generates the best
extractive captions for Feng and Lapata (2010),
who caption images by extracting text from a re-
lated news article. The KL Divergence method is
output = argmin
c
txt
?R
?
w
p(w|I
q
) log
p(w|I
q
)
p(w|c
txt
)
(8)
5 Evaluation
5.1 Automatic Evaluation
Although BLEU (Papineni et al, 2002) scores
are widely used for image caption evaluation, we
find them to be poor indicators of the quality of
our model. As shown in Figure 1, our system?s
BLEU scores increase rapidly until about k = 25.
Past this point we observe the density estimation
seems to get washed out by oversmoothing, but the
BLEU scores continue to improve until k = 500
but only because the generated captions become
increasingly shorter. Furthermore, although we
observe that our SumBasic extracted captions ob-
tain consistently higher BLEU scores, our per-
sonal observations find KL Divergence captions to
be better at balancing recall and precision. Never-
theless, BLEU scores are the accepted metric for
recent work, and our KL Divergence captions with
k = 25 still outperform all other previously pub-
lished systems and baselines. We omit full results
here due to space, but make our BLEU setup with
captions for all systems and baselines available for
documentary purposes.
System Relevance
COLLECTIVE 2.38 (? = 1.45)
SCENE ATTRIBUTES 2.15 (? = 1.45)
SYSTEM 3.19 (? = 1.50)
HUMAN 4.09 (? = 1.14)
Table 2: Human evaluations of relevance: mean
ratings and standard deviations. See Section 5.2.
5.2 Human Evaluation
We perform our human evaluation of caption rele-
vance using a similar setup to that of Kuznetsova
et al (2012), who have humans rate the image cap-
tions on a 1-5 scale (5: perfect, 4: almost per-
fect, 3: 70-80% good, 2: 50-70% good, 1: to-
tally bad). Evaluation is performed using Amazon
Mechanical Turk. Evaluators are shown both the
caption and the query image, and are specifically
instructed to ignore errors in grammaticality and
coherence.
We generate captions using our system with KL
Divergence sentence selection and k = 25. We
also evaluate the original HUMAN captions for
the query image, as well as generated captions
from two recently published caption transfer sys-
tems. First, we consider the SCENE ATTRIBUTES
system (Patterson et al, 2014), which represents
both the best scene-based transfer model and a
k = 1 nearest-neighbor baseline for our system.
We also compare against the COLLECTIVE system
(Kuznetsova et al, 2012), which is the best object-
based transfer model.
In order to facilitate comparison, we use the
same test/train split that is used in the publicly
available system output for the COLLECTIVE sys-
tem
7
. However, we remove some query images
which have contamination between the train and
test set (this occurs when a photographer takes
multiple shots of the same scene and gives all the
images the exact same caption). We also note that
their test set is selected based on images where
their object detection systems had good perfor-
mance, and may not be indicative of their perfor-
mance on other query images.
Table 2 shows the results of our human study.
Captions generated by our system have 48%
improvement in relevance over the SCENE AT-
TRIBUTES system captions, and 34% improve-
7
http://www.cs.sunysb.edu/
?
pkuznetsova/generation/cogn/captions.
html
595
COLLECTIVE: One of the birds seen in
company of female and
juvenile.
View of this woman sit-
ting on the sidewalk in
Mumbai by the stained
glass. The boy walk-
ing by next to match-
ing color walls in gov t
building.
Found this mother bird
feeding her babies in
our maple tree on the
phone.
Found in floating grass
spotted alongside the
scenic North Cascades
Hwy near Ruby arm a
black bear.
SCENE
ATTRIBUTES:
This small bird is pretty
much only found in the
ancient Caledonian pine
forests of the Scottish
Highlands.
me and allison in front
of the white house
The sand in this beach
was black...I repeat
BLACK SAND
Not the green one, but
the almost ghost-like
white one in front of it.
SYSTEM: White bird found in
park standing on brick
wall
by the white house pine tree covered in ice
:)
Pink flower in garden w/
moth
HUMAN: Some black head bird
taken in bray head.
Us girls in front of the
white house
Male cardinal in snowy
tree knots
Black bear by the road
between Ucluelet and
Port Alberni, B.C.,
Canada
Table 3: Example query images and generated captions.
ment over the COLLECTIVE system captions. Al-
though our system captions score lower than the
human captions on average, there are some in-
stances of our system captions being judged as
more relevant than the human-written captions.
6 Discussion and Examples
Example captions are shown in Table 3. In many
instances, scene-based image descriptors provide
enough information to generate a complete de-
scription of the image, or at least a sufficiently
good one. However, there are some kinds of
images for which scene-based features alone are
insufficient. For example, the last example de-
scribes the small pink flowers in the background,
but misses the bear.
Image captioning is a relatively novel task for
which the most compelling applications are prob-
ably not yet known. Much previous work in im-
age captioning focuses on generating captions that
concretely describe detected objects and entities
(Kulkarni et al, 2011; Yang et al, 2011; Mitchell
et al, 2012; Yu and Siskind, 2013). However,
human-generated captions and annotations also
describe perceptual features, contextual informa-
tion, and other types of content. Additionally, our
system is robust to instances where entity detec-
tion systems fail to perform. However, one could
consider combined approaches which incorporate
more regional content structures. For example,
previous work in nonparametric hierarchical topic
modeling (Blei et al, 2010) and scene labeling
(Liu et al, 2011) may provide avenues for further
improvement of this model. Compression meth-
ods for removing visually irrelevant information
(Kuznetsova et al, 2013) may also help increase
the relevance of extracted captions. We leave these
ideas for future work.
References
Ahmet Aker and Robert Gaizauskas. 2010. Generating
image descriptions using dependency relational pat-
terns. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, ACL
?10, pages 1250?1258, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Alexander C Berg, Tamara L Berg, Hal Daume, Jesse
Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch,
Margaret Mitchell, Aneesh Sood, Karl Stratos, et al
2012. Understanding and predicting importance in
images. In Computer Vision and Pattern Recog-
nition (CVPR), 2012 IEEE Conference on, pages
3562?3569. IEEE.
Christopher M Bishop. 2006. Pattern recognition and
machine learning, volume 1. Springer New York.
David M. Blei, Thomas L. Griffiths, and Michael I. Jor-
dan. 2010. The nested chinese restaurant process
596
and bayesian nonparametric inference of topic hier-
archies. J. ACM, 57(2):7:1?7:30, February.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yam-
aguchi, Yejin Choi, Hal Daum?e III, Alexander C.
Berg, and Tamara L. Berg. 2012. Detecting visual
text. In North American Chapter of the Association
for Computational Linguistics (NAACL).
Xin Fan, Ahmet Aker, Martin Tomko, Philip Smart,
Mark Sanderson, and Robert Gaizauskas. 2010.
Automatic image captioning from the web for gps
photographs. In Proceedings of the international
conference on Multimedia information retrieval,
MIR ?10, pages 445?448, New York, NY, USA.
ACM.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences from images.
In Proceedings of the 11th European conference on
Computer vision: Part IV, ECCV?10, pages 15?29,
Berlin, Heidelberg. Springer-Verlag.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362?370. Association for
Computational Linguistics.
James Hays and Alexei A Efros. 2008. Im2gps: esti-
mating geographic information from a single image.
In Computer Vision and Pattern Recognition, 2008.
CVPR 2008. IEEE Conference on, pages 1?8. IEEE.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tificial Intelligence Research, 47:853?899.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In CVPR, pages
1601?1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2013. Generalizing
image captions for image-text parallel corpus. In
ACL.
Ce Liu, Jenny Yuen, and Antonio Torralba. 2011.
Nonparametric scene parsing via label transfer.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 33(12):2368?2382.
Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Ku-
mar. 2008. A new baseline for image annotation.
In Computer Vision?ECCV 2008, pages 316?329.
Springer.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Men-
sch, Alexander C. Berg, Tamara L. Berg, and Hal
Daum?e III. 2012. Midge: Generating image de-
scriptions from computer vision detections. In Euro-
pean Chapter of the Association for Computational
Linguistics (EACL).
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42:145?175.
V. Ordonez, G. Kulkarni, and T.L. Berg. 2011.
Im2text: Describing images using 1 million cap-
tioned photographs. In NIPS.
Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C
Berg, and Tamara L Berg. 2013. From large scale
image categorization to entry-level categories. In In-
ternational Conference on Computer Vision.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Genevieve Patterson and James Hays. 2012. Sun at-
tribute database: Discovering, annotating, and rec-
ognizing scene attributes. In Computer Vision and
Pattern Recognition (CVPR), 2012 IEEE Confer-
ence on, pages 2751?2758. IEEE.
Genevieve Patterson, Chen Xu, Hang Su, and James
Hays. 2014. The sun attribute database: Beyond
categories for deeper scene understanding. Interna-
tional Journal of Computer Vision.
Joseph Tighe and Svetlana Lazebnik. 2010. Su-
perparsing: scalable nonparametric image parsing
with superpixels. In Computer Vision?ECCV 2010,
pages 352?365. Springer.
Antonio Torralba, Robert Fergus, and William T Free-
man. 2008. 80 million tiny images: A large data
set for nonparametric object and scene recognition.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 30(11):1958?1970.
597
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods
in Natural Language Processing (EMNLP), Edin-
burgh, Scotland.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 53?63,
Sofia, Bulgaria. Association for Computational Lin-
guistics.
598
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 49?54,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Extractive Multi-Document Summaries Should Explicitly Not Contain
Document-Specific Content
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{rebecca,ec}@cs.brown.edu
Abstract
Unsupervised approaches to multi-document
summarization consist of two steps: find-
ing a content model of the documents to be
summarized, and then generating a summary
that best represents the most salient informa-
tion of the documents. In this paper, we
present a sentence selection objective for ex-
tractive summarization in which sentences are
penalized for containing content that is spe-
cific to the documents they were extracted
from. We modify an existing system, HIER-
SUM (Haghighi & Vanderwende, 2009), to use
our objective, which significantly outperforms
the original HIERSUM in pairwise user eval-
uation. Additionally, our ROUGE scores ad-
vance the current state-of-the-art for both su-
pervised and unsupervised systems with sta-
tistical significance.
1 Introduction
Multi-document summarization is the task of gener-
ating a single summary from a set of documents that
are related to a single topic. Summaries should con-
tain information that is relevant to the main ideas of
the entire document set, and should not contain in-
formation that is too specific to any one document.
For example, a summary of multiple news articles
about the Star Wars movies could contain the words
?Lucas ?and ?Jedi?, but should not contain the name
of a fan who was interviewed in one article. Most
approaches to this problem generate summaries ex-
tractively, selecting whole or partial sentences from
the original text, then attempting to piece them to-
gether in a coherent manner. Extracted text is se-
lected based on its relevance to the main ideas of the
document set. Summaries can be evaluated manu-
ally, or with automatic metrics such as ROUGE (Lin,
2004).
The use of structured probabilistic topic models
has made it possible to represent document set con-
tent with increasing complexity (Daume? & Marcu,
2006; Tang et al, 2009; Celikyilmaz & Hakkani-
Tur, 2010). Haghighi and Vanderwende (2009)
demonstrated that these models can improve the
quality of generic multi-document summaries over
simpler surface models. Their most complex hier-
archial model improves summary content by teasing
out the words that are not general enough to repre-
sent the document set as a whole. Once those words
are no longer included in the content word distri-
bution, they are implicitly less likely to appear in
the extracted summary as well. But this objective
does not sufficiently keep document-specific content
from appearing in multi-document summaries.
In this paper, we present a selection objective that
explicitly excludes document-specific content. We
re-implement the HIERSUM system from Haghighi
and Vanderwende (2009), and show that using our
objective dramatically improves the content of ex-
tracted summaries.
2 Modeling Content
The easiest way to model document content is to find
a probability distribution of all unigrams that appear
in the original documents. The highest frequency
words (after removing stop words) have a high like-
lihood of appearing in human-authored summaries
(Nenkova & Vanderwende, 2005). However, the raw
49
Figure 1: The graphical model for HIERSUM (Haghighi
& Vanderwende, 2009).
unigram distribution may contain words that appear
frequently in one document, but do not reflect the
content of the document set as a whole.
Probabilistic topic models provide a more prin-
cipled approach to finding a distribution of content
words. This idea was first presented by Daume?
and Marcu (2006) for their BAYESUM system for
query-focused summarization, and later adapted for
non-query summarization in the TOPICSUM system
by Haghighi and Vanderwende (2009). 1 In these
systems, each word from the original documents is
drawn from one of three vocabulary distributions.
The first, ?b, is the background distribution of gen-
eral English words. The second, ?d, contains vo-
cabulary that is specific to that one document. And
the third, ?c, is the distribution of content words for
that document set, and contains relevant words that
should appear in the generated summary.
HIERSUM (Haghighi & Vanderwende, 2009)
adds more structure to TOPICSUM by further split-
ting the content distribution into multiple sub-topics.
The content words in each sentence can be gener-
ated by either the general content topic or the con-
tent sub-topic for that sentence, and the words from
the general content distribution are considered when
building the summary.
1The original BAYESUM can also be used without a query,
in which case, BAYESUM and TOPICSUM are the exact same
model.
3 KL Selection
The KL-divergence between two unigram word dis-
tributions P and Q is given by KL(P ||Q) =
?
w P (w) log
P (w)
Q(w) . This quantity is used for sum-
mary sentence selection in several systems includ-
ing Lerman and McDonald (2009) and Haghighi
and Vanderwende (2009), and was used as a feature
in the discrimitive sentence ranking of Daume? and
Marcu (2006).
TOPICSUM and HIERSUM use the following KL
objective, which finds S?, the summary that min-
imizes the KL-divergence between the estimated
content distribution ?c and the summary word dis-
tribution PS:
S? = min
S:|S|?L
KL(?c||PS)
A greedy approximation is used to find S?. Start-
ing with an empty summary, sentences are greedily
added to the summary one at a time until the sum-
mary has reached the maximum word limit, L. The
values of PS are smoothed uniformly in order to en-
sure finite values of KL(?c||PS).
4 Why Document-Specific Words are a
Problem
The KL selection objective effectively ensures the
presence of highly weighted content words in the
generated summary. But it is asymmetric in that it
allows a high proportion of words in the summary
to be words that appear infrequently, or not at all,
in the content word distribution. This asymmetry
is the reason why the KL selection metric does not
sufficiently keep document-specific words out of the
generated summary.
Consider what happens when a document-specific
word is included in summary S. Assume that the
word wi does not appear (has zero probability) in
the content word distribution ?c, but does appear in
the document-specific distribution ?d for document
d. Then wi appearing in S has very little impact
on KL(?c||PS) =
?
j ?c(wj) log
?c(wj)
PS(wj)
because
?c(wi) = 0. There will be a slight impact because
the presence of the wordwi in S will cause the prob-
ability of other words in the summary to be sligntly
smaller. But in a summary of length 250 words (the
50
length used for the DUC summarization task) the
difference is negligible.
The reason why we do not simply substitute
a symmetrical metric for comparing distributions
(e.g., Information Radius) is because we want the se-
lection objective to disprefer only document-specific
words. Specifically, the selection objective should
not disprefer background English vocabulary.
5 KL(c)-KL(d) Selection
In contrast to the KL selection objective, our ob-
jective measures the similarity of both content and
document-specific word distributions to the ex-
tracted summary sentences. We combine these mea-
sures linearly:
S? = min
S:|S|?L
KL(?c||PS)?KL(?d||PS)
Our objective can be understood in comparison
to the MMR criterion by (Carbonell & Goldstein,
1998), which also utilizes a linear metric in order to
maximize informativeness of summaries while min-
imizing some unwanted quality of the extracted sen-
tences (in their case, redundancy). In contrast, our
criterion utilizes information about what kind of in-
formation should not be included in the summary,
which to our knowledge has not been done in previ-
ous summarization systems.2
For comparison to the previous KL objective, we
also use a greedy approximation for S?. However,
because we are extracting sentences from many doc-
uments, the distribution ?d is actually several distri-
butions, a separate distribution for each document
in the document set. The implementation we used
in our experiments is that, as we consider a sen-
tence s to be added to the previously selected sen-
tences S, we set ?d to be the document-specific
distribution of the document that s has been ex-
tracted from. So each time we add a sentence to
the summary, we find the sentence that minimizes
KL(?c||PS?s)?KL(?d(s)||PS?s). Another imple-
mentation we tried was combining all of the ?d dis-
tributions into one distribution, but we did not notice
any difference in the extracted summaries.
2A few anonymous reviewers asked if we tried to optimize
the value of ? for KL(?c||PS) ? ?KL(?d||PS). The answer
is yes, but optimizing ? to maximize ROUGE results in sum-
maries that are perceptibly worse, and manually tuning ? did
not seem to produce any benefit.
6 Evaluation
6.1 Data
We developed our sentence selection objective us-
ing data from the Document Understanding Con-
ference3 (DUC) 2006 summarization task, and used
data from DUC 2007 task for evaluations. In these
tasks, the system is given a set of 25 news arti-
cles related to an event or topic, and needs to gen-
erate a summary of under 250 words from those
documents.4 For each document set, four human-
authored summaries are provided for use with eval-
uations. The DUC 2006 data has 50 document sets,
and the DUC 2007 data has 45 document sets.
6.2 Automatic Evaluation
Systems are automatically evalatued using ROUGE
(Lin, 2004), which has good correlation with hu-
man judgments of summary content. ROUGE com-
pares n-gram recall between system-generated sum-
maries, and human-authored reference summaries.
The first two metrics we compare are unigram and
bigram recall, R-1 and R-2, respectively. The last
metric, R-SU4, measures recall of skip-4 bigrams,
which may skip one or two words in between the
two words to be measured. We set ROUGE to stem
both the system and reference summaries, scale our
results by 102 and present scores with and without
stopwords removed.
The ROUGE scores of the original HIERSUM sys-
tem are given in the first row of table 1, followed
by the scores of HIERSUM using our KL(c-d) se-
lection. The KL(c-d) selection outperforms the KL
selection in each of the ROUGE metrics shown. In
fact, these results are statistically significant over
the baseline KL selection for all but the unigram
metrics (R-1 with and without stopwords). These
results show that our KL(c-d) selection yields sig-
nificant improvements in terms of ROUGE perfor-
mance, since having fewer irrelevant words in the
summaries leaves room for words that are more rel-
evant to the content topic, and therefore more likely
to appear in the reference summaries.
The last two rows of table 1 show the scores
of two recent state-of-the-art multi-document sum-
3http://duc.nist.gov/
4Some DUC summarization tasks also provide a query or
focus for the summary, but we ignore these in this work.
51
System ROUGE w/o stopwords ROUGE w/ stopwords
R-1 R-2 R-SU4 R-1 R-2 R-SU4
HIERSUM w/ KL 34.6 7.3 10.4 43.1 9.7 15.3
HIERSUM w/ KL(c)-KL(d) 35.6 9.9 12.8 43.2 11.6 16.6
PYTHY 35.7 8.9 12.1 42.6 11.9 16.8
HYBHSUM 35.1 8.3 11.8 45.6 11.4 17.2
Table 1: ROUGE scores on the DUC 2007 document sets. The first two rows compare the results of the unigram
HIERSUM system with its original and our improved selection metrics. Bolded scores represent where our system has
a significant improvement over the orignal HIERSUM. For further comparison, the last two rows show the ROUGE
scores of two other state-of-the-art multi-document summarization systems (Toutanova et al, 2007; Celikyilmaz &
Hakkani-Tur, 2010). See section 6.2 for more details.
marization systems. Both of these systems se-
lect sentences discriminatively on many features
in order to maximize ROUGE scores. The first,
PYTHY (Toutanova et al, 2007), trains on dozens
of sentence-level features, such as n-gram and skip-
gram frequency, named entities, sentence length and
position, and also utilizes sentence compression.
The second, HYBHSUM (Celikyilmaz & Hakkani-
Tur, 2010), uses a nested Chinese restaurant process
(Blei et al, 2004) to model a hierarchical content
distribution with more complexity than HIERSUM,
and uses a regression model to predict scores for new
sentences.
For both of these systems, our summaries are sig-
nificantly better for R-2 and R-SU4 without stop-
words, and comparable in all other metrics.5 These
results show that our selection objective can make
a simple unsupervised model competitive with more
complicated supervised models.
6.3 Manual Evaluation
For manual evaluation, we performed a pairwise
comparison of summaries generated by HIERSUM
with both the original and our modified sentence se-
lection objective. Users were given the two sum-
maries to compare, plus a human-generated refer-
ence summary. The order that the summaries ap-
peared in was random. We asked users to select
which summary was better for the following ques-
5Haghighi and Vanderwende (2009) presented a version of
HIERSUM that models documents as a bag of bigrams, and pro-
vides results comparable to PYTHY. However, the bigram HI-
ERSUM model does not find consistent bags of bigrams.
System Q1 Q2 Q3 Q4
HIERSUM w/ KL 29 36 31 36
. . . w/ KL(c)-KL(d) 58 51 56 51
Table 2: Results of manual evaluation. Our criterion out-
performs the original HIERSUM for all attributes, and is
significantly better for Q1 and Q3. See section 6.3 for
details.
tions:6
Q1 Which was better in terms of overall content?
Q2 Which summary had less repetition?
Q3 Which summary was more coherent?
Q4 Which summary had better focus?
We took 87 pairwise preferences from participants
over Mechanical Turk.7 The results of our evalu-
ation are shown in table 2. For all attributes, our
criterion performs better than the original HIERSUM
selection criterion, and our results for Q1 and Q3 are
significantly better as determined by Fisher sign test
(two-tailed P value < 0.01).
These results confirm that our objective noticably
improves the content of extractive summaries by se-
lecting sentences that contain less document-specific
6These are based on the manual evaluation questions from
DUC 2007, and are the same questions asked in Haghighi and
Vanderwende (2009).
7In order to ensure quality results, we asked participants to
write a sentence on why they selected their preference for each
question. We also monitored the time taken to complete each
comparison. Overall, we rejected about 25% of responses we
received, which is similar to the percentage of responses re-
jected by Gillick and Liu (2010).
52
information. This leaves more room in the summary
for content that is relevant to the main idea of the
document set (Q1) and keeps out content that is not
relevant (Q4). Additionally, although neither crite-
rion explicitly addresses coherence, we found that a
significant proportion of users found our summaries
to be more coherent (Q3). We believe this may be
the case because the presence of document-specific
information can distract from the main ideas of the
summary, and make it less likely that the extracted
sentences will flow together.
There is no immediate explanation for why users
found our our summaries less repetitive (Q2), since
if anything the narrowing of topics due to the neg-
ative KL(?d||PS) term should make for more rep-
etition. We currently hypothesize that the improved
score is simply a spillover from the general improve-
ment in document quality.
7 Conclusion
We have described a new objective for sentence se-
lection in extractive multi-document summarization,
which is different in that it explicitly gives negative
weight to sentences that contain document-specific
words. Our objective significantly improves the per-
formance of an existing summarization system, and
improves on current best ROUGE scores with sig-
nificance.
We have observed that while the content in our
extracted summaries is often comparable to the con-
tent in human-written summaries, the extracted sum-
maries are still far weaker in terms of coherence and
repetition. Even though our objective significantly
improves coherence, more sophisticated methods of
decoding are still needed to produce readable sum-
maries. These problems could be addressed through
further refinement of the selection objective, through
simplification or compression of selected sentences,
and through improving the coherence of generated
summaries.
References
Blei, D. M., Griffiths, T. L., Jordan, M. I., & Tenen-
baum, J. B. (2004). Hierarchical topic models and
the nested chinese restaurant process. Advances
in Neural Information Processing Systems.
Carbonell, J., & Goldstein, J. (1998). The use
of mmr, diversity-based reranking for reordering
documents and producing summaries. Proceed-
ings of the 21st annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval (pp. 335?336). New York, NY,
USA: ACM.
Celikyilmaz, A., & Hakkani-Tur, D. (2010). A hy-
brid hierarchical model for multi-document sum-
marization. Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (pp. 815?824). Stroudsburg, PA, USA: Asso-
ciation for Computational Linguistics.
Daume?, III, H., & Marcu, D. (2006). Bayesian
query-focused summarization. ACL-44: Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguis-
tics (pp. 305?312). Morristown, NJ, USA: Asso-
ciation for Computational Linguistics.
Gillick, D., & Liu, Y. (2010). Non-expert evaluation
of summarization systems is risky. Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Me-
chanical Turk (pp. 148?151). Stroudsburg, PA,
USA: Association for Computational Linguistics.
Haghighi, A., & Vanderwende, L. (2009). Exploring
content models for multi-document summariza-
tion. Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (pp. 362?370). Boulder, Col-
orado: Association for Computational Linguis-
tics.
Lerman, K., & McDonald, R. (2009). Contrastive
summarization: an experiment with consumer re-
views. Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, Companion Volume:
Short Papers (pp. 113?116). Stroudsburg, PA,
USA: Association for Computational Linguistics.
Lin, C.-Y. (2004). Rouge: a package for auto-
matic evaluation of summaries. Proceedings of
53
the Workshop on Text Summarization Branches
Out (WAS 2004). Barcelona, Spain.
Nenkova, A., & Vanderwende, L. (2005). The im-
pact of frequency on summarization (Technical
Report). Microsoft Research.
Tang, J., Yao, L., & Chen, D. (2009). Multi-topic
based query-oriented summarization. SDM?09
(pp. 1147?1158).
Toutanova, K., Brockett, C., Gamon, M., Jagarla-
mudi, J., Suzuki, H., & Vanderwende, L. (2007).
The PYTHY Summarization System: Microsoft
Research at DUC 2007. Proc. of DUC.
54
Proceedings of the NAACL HLT Workshop on Vision and Language (WVL ?13), pages 1?9,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Annotation of Online Shopping Images without Labeled Training Examples
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
{rebecca,ec}@cs.brown.edu
Abstract
We are interested in the task of image an-
notation using noisy natural text as training
data. An image and its caption convey dif-
ferent information, but are generated by the
same underlying concepts. In this paper, we
learn latent mixtures of topics that generate
image and product descriptions on shopping
websites by adapting a topic model for multi-
lingual data (Mimno et al, 2009). We use the
trained model to annotate test images without
corresponding text. We capture visual prop-
erties such as color, texture, shape, and ori-
entation by computing low-level image fea-
tures, and measure the contribution of each
type of visual feature towards the accuracy of
the model. Our model significantly outper-
forms both a competitive baseline and a pre-
vious topic model-based system.
1 Introduction
Image annotation is a classic problem in Computer
Vision. Given a query image, the task is to gen-
erate a set of textual labels that describe the visual
content. The typical approach to these problems is
to use supervised models, which require large num-
bers of hand-annotated examples for each of the la-
bels. However, the amount of information available
on the web continues to grow, the task of organiz-
ing and describing visual data becomes increasingly
complex. For example, a shopping website might ar-
range products into broad categories such as ?shoes?
and ?handbags? with each category containing tens
of thousands of products that are difficult for users
to search and navigate. It is often infeasible to dis-
cover all of the attributes within those categories that
are relevant to users and create labeled training ex-
amples for each of them.
Instead, we approach this problem by discovering
visual attributes from noisy natural language cap-
tions. That is, given a collection of images and cap-
tions found on the web, we learn a model of visual
and textual features. Then given a query image with
no text, we can generate likely descriptive words.
This is a difficult task because image captions on the
web are often noisy and incomplete: some captions
might not describe a particular visual feature, might
use a synonym for that feature, or might describe in-
formation that is not visual in the image at all.
A secondary motivation for this work is to use the
image annotations as a component in language gen-
eration systems such as for automatic image caption-
ing. We point to examples of previous work such
as Feng and Lapata (2010a) where image annota-
tions generated from a topic model are used to help
generate full sentences to describe images. Much of
the current research in image captioning is limited
by the current technology for object recognition in
Computer Vision. For example, SBU-Flickr dataset
(Ordonez et al, 2011) with 1 million images and
captions, is considered to be general-domain but is
actually built by querying Flickr using a pre-defined
term list related to visual attributes that there are
trained recognition systems for. While these sys-
tems can accurately generate descriptions for com-
mon visual objects and attributes, they are not as
well-suited for describing the ?long-tail? of visual
attributes which appear in many domain-specific
1
Two adjustable buckle straps top a
classic rubber rain boot grounded by
a thick lug sole for excellent wet-
weather traction.
Size(s) Available: 6, 11.5. Brand &
Style - VANS Kvd Width - Medium
(B, M) Heel Height - Shoe Size is
Womens Size 11.5 = Mens Size 10
1 Inch Heel Material - Canvas Upper
and Man Made Sole
Carlo Fellini - Evening clutch
beaded on a wave pattern
Table 1: Examples of data from the Attribute Discovery Dataset (Berg et al, 2010). The images are fairly clean and
uniform, while captions have more noise and variation.
datasets.
In this paper, we model image and text features
from the training data using a generative model. We
adapt the Polylingual topic model from Mimno et
al. (2009) to train on multi-modal data, and then use
the trained model to generate annotations for test im-
ages. We evaluate our model on two categories of
shopping images using a variety of types of com-
puted image features. For image annotation we out-
perform both a difficult baseline and previous work.
2 Related Work
We use the polylingual topic model from Mimno et
al. (2009), which was developed to model multi-
lingual corpora that are topically comparable be-
tween languages ? the documents are not direct
translations, but they cover the same ideas. For ex-
ample, English and Finnish Wikipedia pages about
skiing are roughly similar, but the subject is covered
more thoroughly in Finnish. Therefore, the number
of tokens assigned to the Finnish topic for skiing is
much higher than it is in the English. While Mimno
et al (2009) show that the model is effective in tasks
such as modeling topically comparable documents
across languages, our work is the first to show that
this model can be used to model data of different
modalities. Another quality of the polylingual topic
model is that words in different languages do not di-
rectly correspond with each other. This is a feature
of other multi-lingual topic models but would not
work for multi-modal data because a textual word
can carry more meaning by itself than an image fea-
ture can.
Countless approaches have been proposed for the
use of topic models in image annotation, but the
vast majority of these approaches consider the text
modality merely as labels for the image modality.
The most highly cited of these is the Correspon-
dence LDA (corr-LDA) model of Blei et al (2003),
where topics are learned using the image modality
alone, and each textual word must be generated by a
specific region in the image. However, more recent
work has started to recognize the textual modality
as a source of information in its own right. Jia et
al. (2011) present a model that allows different in-
formation to be emphasized in each modality, but it
requires very clean text; they do not use documents
with captions that cannot be easily parsed or pro-
cessed. Then, they stem all words, and disregard
sparse word tokens. This works when working with
sources such as Wikipedia, where text captions are
highly edited and consistently formatted. In compar-
ision, our work can be trained on corpra where the
text has poor or inconsistant quality. Additionally,
their work was for the task of image retrieval from a
text query, while we are generating text annotations
for a query image.
Our work is most similar to the MixLDA model
of Feng and Lapata (2010b), except MixLDA mod-
2
els images and their related text as a single bag-
of-features, with visual and textual features coming
from the same vocabulary. This means that some
topics should have a greater proportion of features
from one of modalities, if there is an idea that is bet-
ter expressed in one over the other. Their model was
developed for finding descriptive words given both
an image and a news article, and can also be used
on large and noisy amounts of data, so we compare
MixLDA against our model in the experiments.
Although we use the Attribute Discovery Dataset
of Berg et al (2010), their work is different from
ours in both problem formulation and the types of
attributes discovered. Their primary interest is to
characterize attributes according to how they are vi-
sually represented: global or local; color, texture, or
shape. Their work does not address the task of pre-
dicting attributes for unseen images. Additionally,
they do not work with individual descriptive words,
but cluster them using mutual information of vi-
sual attributes, creating a smaller number of ?visual
synsets?. For example, one of their visual synsets
for images and descriptions of womens handbags is
{mesh, interior, metal} and another is {silver, metal-
lic}. In comparison, in the topic model the same
word can be generated by more than one topic.
Liu et al (2010) examine the use of a variety of
image features in a Bayesian model in order to mea-
sure which are the best for classifying diverse ma-
terials such as stone, glass, and plastic. They found
that the image features they used for shape and color
were better indicators of the material of an object
than texture features, and their best combined model
did not include texture as a feature at all. We are also
interested in finding out whether our performance on
generating descriptive words is affected by different
types of image features.
3 Dataset
We use the Attribute Discovery Dataset from Berg
et al (2010).1 The dataset consists of pairs of im-
ages and captions taken from the shopping website
like.com. The data has four categories: women?s
shoes, handbags, earrings, and neckties. We run our
model on two categories, shoes and handbags, due
1http://tamaraberg.com/
attributesDataset/index.html
to their larger sizes ? 14764 and 9145 image-caption
pairs respectively ? and diversity of features. This is
a reasonable amount of data in the shopping images
domain; more than half of the number of compa-
rable products sold on large retail websites such as
Zappos.com or Amazon.com.
Compared to general datasets such as Pascal
Sentences, the images in the Attribute Discovery
Dataset are more uniform. All image files are
280x280 pixel JPEGs, and images of products are
typically taken from similar angles against a white
or a light-colored solid background. Only rarely do
the images have noisy backgrounds, such as a per-
son wearing the item, or the same item displayed in
multiple colors in one image. However, this does not
necessarily make our task much easier, since the vi-
sual attributes we wish to learn are not pre-defined as
they are in a general-domain dataset. And the lack of
hand-annotated data means no negative examples of
when an attribute is not present, which are typically
used to train visual classifiers.
Furthermore, the captions are extremely noisy in
this dataset. Compared to the 20 object types in the
Pascal Sentences dataset, or about one hundred in
COREL, here there are thousands of words that can
be used to describe features in the images, includ-
ing synonyms, multiple stems of words, and mis-
spellings. In addition to explicit visual descriptions
of the products, the captions describe ?less visual?
features such as details about the construction of the
item, during which season or activity it would be ap-
propriate to wear, or feelings that could be evoked
by looking at the item. These features are difficult
to represent as specific visual attributes, but can be
identified visually by domain-experts. Captions can
also include information that is non-visual such as
sizing and shipping information, or whether the item
is on sale.
The captions can be either full English sentences,
a list of features, or sometimes just a few words.
Longer captions in the dataset are truncated to 250
characters in length.
From our own obervations, we estimate about
10% of the captions in the shoes dataset contain
few or no descriptive words. At least 3.7% of the
shoes captions are entirely Javascript code, have sig-
nificant portions of code, or very long URLs. An-
other 5-6% either contain no information besides
3
sizing or shipping information, only the brand name
or model number of the shoe, or the caption is so
short that there are only one or two descriptive words
that could be used in our model. In the womens?
shoes category, we take some simple steps to remove
URLs and code to avoid learning accidental correla-
tion with legitimate features.2 However, we still use
all image and caption pairs in the training set, in-
cluding those which end up having empty captions,
since they are still useful for learning topics for vi-
sual features. For the handbags captions, we did not
try to remove code or long URLs since it seemed to
be less of a problem in that category.
4 Feature Representation
4.1 Text Features
The bag-of-words model is used for text. We use
Mxterminator (Reynar and Ratnaparkhi, 1997) to
split sentences in the captions (in many instances,
nothing is done in this step becuase there are no
full sentences in the caption), Stanford POS Tag-
ger(Toutanova et al, 2003) to tag words, then in-
clude adjectives, adverbs, verbs, and nouns in the
topic model (except for proper nouns and common
background English words from a stoplist). How-
ever, these tags are really more a rough estimate of
parts of speech due to the number of incomplete sen-
tences and phrases, and the fact that many of the
words used to describe styles or attributes of cloth-
ing have different meanings in colloqueal English.3
All tokens are converted to lower case, but there is
no stemming or lemmatization. After preprocessing,
the size of the shoes text vocabulary is 9578 words,
with an average of 16.33 descriptive words per im-
age, while the bags have a text vocabulary of 6309
word types with 15.41 descriptive words per image
on average.
4.2 Visual Features
The bag-of-features model is used for visual features
as well. Most of these features are standard in com-
puter vision research, and are also used in work we
cited in Section 2.
2Tokens removed: URLs, all tokens that end in ?.sh?, and a
few tokens obviously related to Javascript eg script, src, typeof,
var.
3Some examples of domain-specific words used in shopping
image descriptions: www.zappos.com/glossary
Shape: A SIFT descriptor describes which way
edges are oriented at a certain point in an image
(Lowe, 1999). It was develped to recognize the same
object under different scales and rotations. How-
ever, it is also commonly used for recognizing more
generalized types or features of objects. We use the
VLFeat open source library (Vedaldi and Fulkerson,
2008) to compute SIFT features at points of interest
and to cluster the SIFT features into discrete ?visual
terms? using the k-means algorithm. There are 750
visual terms for SIFT features.
Color: We use two representations for color,
RGB (red, green, blue) and HSV (hue, satura-
tion, value). 25 pixels are sampled from the cen-
ter 100x100 pixels of the image (to avoid sampling
from the background of the image). Those pixel val-
ues are also clustered to visual terms using k-means,
with 100 visual terms each.
Texture: Images are convolved with Gabor filters
at multiple orientations and scales, sampled at ran-
dom locations, then clustered to form texton features
for texture (Leung and Malik, 2001). We convert all
images to grayscale, then sample 25 locations from
the center of the image, and cluster to 100 visual
terms. We also have a color texton feature, where
we sample and cluster textons separately for the red,
green, and blue color channels.
Reflectance/Curvature:4 We use three types of
related features for gradients and curvature. The
first is a bag-of-HOG (histogram of gradients) fea-
ture set (Dalal and Triggs, 2005) computed over a
regular grid on the image to measure changes in in-
tensity.5 The most significant of those features (as
determined by L2 norm) are selected for each im-
age, and like previous features are clustered into vi-
sual terms using k-means. The second two types are
derivatives of HOG which include information about
the amount of curvature at each orientation of the
HOG descriptor.6
4
Figure 1: Polylingual topic model (Mimno et al, 2009)
5 Model
We model textual and visual features using the
polylingual topic model by Mimno et al (2009). In
this section, we describe how the generative process
and inference of this model is adapted to topically
comparable multi-modal data.
Figure 1 shows the original polylingual topic
model. We model multi-modal data using two ?lan-
guages?: txt for the bag-of-words captions, and img
for the combined visual terms. The generative pro-
cess is defined for an image and caption pair, w =<
wimg, wtxt >:
? ? Dir(?, ?m)
zimg ? P (zimg|?) =
?
n
?zimgn
ztxt ? P (ztxt|?) =
?
n
?ztxtn
wimg ? P (wimg|zimg,?img) = ?img
wimgn |z
img
n
wtxt ? P (wtxt|ztxt,?txt) = ?txtwtxtn |ztxtn
First, a topic distribution for w is drawn from an
asymmetric Dirichlet prior with concentration pa-
rameter ? and base measure m. Then a latent topic
assignment is drawn for each word token in wtxt,
and each discrete image feature in wimg. Once the
topic assignments are sampled, the observed tokens
are sampled according to their probability in the
modality-specific topics ?img = {?img1 , ..., ?
img
T }
and ?txt = {?txt1 , ..., ?
txt
T }.
4Note: These features are implemented using code from
(Felzenszwalb et al, ).
5There is significant overlap between these features, al-
though the benefits of overlap are lost due to the bag-of-features
model.
6Personal correspondance, work in progress.
To find the most probable descriptive words for an
unseen image, the first step is to estimate the topic
distribution that generated the image. Gibbs sam-
pling is used to sample topic assignments for visual
terms in the test image dimg:
P (zn = t|d
img, z\n,?
img, ?m)
? ?img
dimgn |t
(Nt)\n + ?mt
?
tNt ? 1 + ?
Assuming that the descriptive words are indepen-
dent, the probability of text word wi given dimg is:
P (wi|d
img) =
?
t
P (wi|z
txt
t )P (zt|d
img)
summing over all topics t ? T .
For training the model, we used the Polylingual
topic model implementation from the Mallet toolkit
(McCallum, 2002) (with some small modifcations
to use it for generation). We use 1000 iterations for
inference, with hyperparameter optimization every
10 iterations. In both shoes and bags categories, the
number of topics is 200, which was minimally tuned
by hand on the shoes data.
6 Experimental Setup and Evaluation
We first run our model on the larger category, shoes.
For both systems and baselines, we find the 10, 15,
and 20 most likely words for the test images. We
evaluate by computing precision and recall against
descriptive words from the held-out captions for
those images.7 We compute macro-averages of these
scores because there is a lot of variation between the
sizes of the captions in the dataset. The split between
training and test instances is 80/20%.
We also evaluate the contributions of different
types of image features. We evaluate the model for
each image feature individually (along with the text
features), as well as combinations of image features.
We compare against the MixLDA system and a
strong baseline. We choose MixLDA because it
is relatively easy to re-implement and because it
7We find descriptive words for test instances in the exact
same way we did for training instances in Section 4.1. Instances
where we did not find any useable descriptive words did not
count towards the evaluation.
5
10 words 15 words 20 words
P R F1 P R F1 P R F1
Baselines
MixLDA 21.02 13.80 16.66 17.41 17.15 17.13 14.88 19.53 16.89
Corpus frequency 21.03 13.73 16.61 17.51 17.14 17.32 15.41 20.12 17.45
Single Attribute
SIFT 27.00 16.30 20.34 22.84 20.65 21.69 20.09 24.22 21.96
Grayscale Texture 21.26 13.88 16.80 18.25 17.87 18.06 15.71 20.52 17.80
RGB Texture 24.77 14.93 18.63 21.01 18.99 19.95 18.49 22.29 20.21
HSV Color 22.17 13.35 16.67 18.59 16.79 17.65 16.48 19.85 18.01
RGB Color 23.21 13.98 17.45 19.78 17.88 18.78 17.53 21.12 19.15
HOG 26.33 15.87 19.80 22.36 20.21 21.23 19.60 23.62 21.42
TriHOG 24.60 14.82 18.50 20.64 18.66 19.60 18.14 21.87 19.83
TriHOG-Polar 26.03 15.69 19.58 22.06 19.94 20.95 19.32 23.29 21.12
Combined Models
All-Color 24.22 14.60 18.22 20.62 18.65 19.59 18.11 21.83 19.80
All-Texture 25.50 15.41 19.24 21.63 19.55 20.53 18.88 22.75 20.64
All-HOG 27.36 16.50 20.58 23.31 21.07 22.14 20.40 24.58 22.30
Combine All 29.31 17.70 22.04 24.88 22.49 23.63 21.71 26.16 23.73
SIFT+RGB Texture+HOG 28.62 17.25 21.52 24.35 22.01 23.12 21.20 25.55 23.17
Table 2: Results of evaluation in the women?s shoes cateogory (top 10-20 words).
has previously outperformed other image annota-
tion systems when trained on natural language cap-
tions. Because the MixLDA model originally only
used SIFT features, we compare it against the SIFT-
only version of our model, with each system using
the same computed image and text features. We
re-implement the MixLDA system mostly as it is
described in Feng and Lapata (2010b), with a few
changes to make it more comparable to our model:
Obviously in our version of MixLDA the test in-
stances are only the unseen image as there is no other
surrounding text. The number of topics is 200 (the
original MixLDA had more but that did not seem to
help here), and the ? and ? hyperparameters are op-
timized every 10 iterations.8
We also compare our model against corpus fre-
quency of words in the training set. Although this
may seem like a trivial baseline, previous work
8We used the Mallet toolkit?s Parallel LDA sampler for in-
ference, while a variational approach is used in the original.
However, we do not believe this would change the outcome of
this experiment. We also tried MixLDA without hyperparam-
eter optimization but we do not show those results as they are
significantly worse.
on image annotation from both computer vision
(Mu?ller et al, 2002; Monay and Gatica-Perez, 2003;
Barnard et al, 2003) and natural language process-
ing (Mason and Charniak, 2012) has shown that a
large portion of the keyword probability mass can
often be accounted for by a very small number of
words, allowing systems to game better-looking re-
sults by simply guessing the frequency distribution
of the text vocabulary. We find this to be espe-
cially true in the domain-specific case, where com-
mon terms (eg shoe, sole, heel, upper) are used in
almost every caption, and in some captions account
for most words used (such as the second example
in Table 1). While domain-frequent words are also
needed for generating new captions, we don?t want
them to account for all of the words our system gen-
erates. Of course, a human evaluation would be
another possible way of addressing this issue, but
it would be difficult and expensive to find enough
people who have sufficient knowledge of womens?
clothing and would be able to accurately say whether
the generated words are appropriate or not (words
such as hobo, PU, stacked, upper, and vamp). Also,
although the gold image captions are noisy, the num-
6
sole upper detail heel print fun fab-
ric patent uppers soles high shoe
rounded leather rubber lining elastic
animal toe feet
style upper heel leather strap sandal
lining toe dress satin shoe comfort
ankle sole adjustable outsole plat-
form stiletto rhinestone sandals
bag, leather, zip, pocket, hardware,
features, shoulder, flap, main, cell,
perfect, length, drop, zipper, clo-
sure, bold, phone, evening, holds,
hobo
This high heel platform shoe has a
patent leather upper with an orna-
menting bow at the toe, a leather lin-
ing, a rounded toe, and a rubber bot-
tom.Available Colors: Black Patent,
Cheetah Print PU.
Create a timeless look with these
Andie dress sandals from Col-
oriffics. Dyeable white satin matte
satin or metallic satin upper in a
two-piece dress-sandal style with
an open round toe crossing pleated
vamp straps with a dazzling rhine-
stone clasp and a wraparound heel
strap with an adjustable buckle clo-
sure.
Treesje Dakota Shoulder Bag Black
Shine - Designer Handbags
Table 3: Example results for unseen images. Both the top words generated by our model and the original held-out
captions for the images are shown. (Note: In the third example, ?hobo? is actually the term that is used to describe
that shape of handbag.)
ber of test documents is very large so we can find
significance on precision and recall using bootstrap
resampling.
We also ran the baseline system and our system
on the handbags category of the dataset. We did not
modify the system in any way when using the bags
dataset, just gave it different file for input.
7 Results and Discussion
The results of our evaluations are in Table 2. As
we expected, the corpus frequency baseline does
very well. It is comparable to MixLDA for 10
and 15 words, and significantly better than MixLDA
for over 20 words. However, the Polylingual topic
model using only SIFT features and text is much bet-
ter than both. The trained MixLDA model has topics
with both image and text features, so when estimat-
ing topics given only an image, it estimates that it
was generated by topics that have a high proportion
of image features. Though it also estimates some
topics that have a mix of visual and text features,
being able to generate good text descriptions from
those topics, the topics that have less text features
will be mainly determined by the smoothing param-
eter ? the uniform distribution, worse than guessing
the corpus distribution.
Out of the single attribute models, all except three
of the single feaure models were significantly higher
than corpus frequency on both precision and recall
at 10, 15, and 20 words. The exceptions are the two
color features and grayscale texture. For grayscale
texture, we had expected it would correlate well with
the material of the shoe; but either the low resolu-
tion of the images makes it difficult to distinguish
materials by their texture, or materials don?t corre-
late with the ?less visual? features as much as we
expected. Interestingly, since the material of an item
tends to correlate strongly with other attributes such
as shape and color, so our model still generates cor-
rect descriptive words for material in many cases.
While neither color nor texture were useful fea-
7
10 words 15 words 20 words
P R F1 P R F1 P R F1
Corpus frequency 17.58 13.19 11.76 13.19 12.70 12.94 11.76 15.10 13.22
Combined Model 24.41 15.67 19.09 21.01 20.22 20.61 18.76 24.09 21.10
Table 4: Results of evaluation in the handbags cateogory (top 10-20 words).
tures on their own, RGB Texture did very well as a
single attribute, and was within signficance of both
the combined color and combined texture models.
This may be related to the fact that RGB Textons
have a larger number of visual terms than those other
features, 100 for each of the three color channels.
Unlike material, we observed that the color of an
object is often not mentioned in the human-written
caption (as seen in the examples in Table 1), or sev-
eral colors are described in the caption where only
one is seen in the image (seen in some of the exam-
ples in Table 3). We also observed that our system
generates very few color words.
The gradient and shape-based features have the
best single-attribute performance by far. Both SIFT
and HOG capture shape at local points, but while
SIFT features are invariant to differences in position
or scale, HOG features are more sensitive to the way
the item is oriented in the image. Although the cur-
vature features TriHOG and TriHog-Polar are nearly
as good as HOG on their own, combining the three
HOG features does not significantly improve perfor-
mance of the model over HOG alone.
Not all of the single-attribute models performed
as well as others, but there was no case where re-
moving one of the features improved the perfor-
mance of the combined model. The fewest num-
ber of image attributes that our model could use
and still get within significance to the full combined
model is three ? SIFT, RGB Texture, and HOG.
However, we found that each image attribute does
slightly improve, the model, even if not by a signifi-
cant amount.
Our results on the handbags category of the
dataset are shown in Table 4. Although our scores
are not as high as they were in the shoes category,
the scores of the corpus frequency baseline are not
as high either, and our model does about as well over
the baseline in each category. But is worth reiterat-
ing that we were able to run our system on both the
bags and shoes shopping categories with absolutely
no modifications or tuning of parameters.
8 Conclusion and Future Work
In conclusion, we have shown that the polylingual
topic model works well for modeling topically com-
parable images and related text, and obtain competi-
tive results for the image annotation task. Our model
is trained on noisy image captions from the web,
rather than hand-labeled data.
For future work, we would like to further adapt
the polylingual topic model for multi-modal data by
allowing some topics to be generated only by one
modality or the other. We are also interested in char-
acterizing the image annotations in order to generate
a single most likely annotation for different types of
features such as texture or color. Finally, we are in-
terested in extending this model to use with other do-
mains of data. For natural images, we could use im-
age segmentation algorithms to separate the object
of interest from the background of the image, or we
could use scene classifcation to cluster the training
images by their background scene and train seprate
models for each.
References
K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D.M.
Blei, and M.I. Jordan. 2003. Matching words and
pictures. The Journal of Machine Learning Research,
3:1107?1135.
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and character-
ization from noisy web data. In Proceedings of
the 11th European conference on Computer vision:
Part I, ECCV?10, pages 663?676, Berlin, Heidelberg.
Springer-Verlag.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In Computer Vision
8
and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages 886
?893 vol. 1, june.
P. F. Felzenszwalb, R. B. Girshick, and D. McAllester.
Discriminatively trained deformable part models, re-
lease 4. http://people.cs.uchicago.edu/ pff/latent-
release4/.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In HLT-
NAACL, pages 831?839.
Yangqing Jia, M. Salzmann, and T. Darrell. 2011. Learn-
ing cross-modality similarity for multinomial data. In
Computer Vision (ICCV), 2011 IEEE International
Conference on, pages 2407 ?2414, nov.
T. Leung and J. Malik. 2001. Representing and recog-
nizing the visual appearance of materials using three-
dimensional textons. International Journal of Com-
puter Vision, 43(1):29?44.
C. Liu, L. Sharan, E.H. Adelson, and R. Rosenholtz.
2010. Exploring features in a bayesian framework for
material recognition. In Computer Vision and Pattern
Recognition (CVPR), 2010 IEEE Conference on, pages
239?246. IEEE.
D.G. Lowe. 1999. Object recognition from local scale-
invariant features. In Computer Vision, 1999. The Pro-
ceedings of the Seventh IEEE International Confer-
ence on, volume 2, pages 1150 ?1157 vol.2.
R. Mason and E. Charniak. 2012. Apples to oranges:
Evaluating image annotations from natural language
processing systems. NAACL.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 880?889, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Florent Monay and Daniel Gatica-Perez. 2003. On im-
age auto-annotation with latent space models. In Pro-
ceedings of the eleventh ACM international conference
on Multimedia, Multimedia ?03, pages 275?278, New
York, NY, USA. ACM.
Henning Mu?ller, Ste?phane Marchand-Maillet, and
Thierry Pun. 2002. The truth about corel - evaluation
in image retrieval. In Proceedings of the International
Conference on Image and Video Retrieval, CIVR ?02,
pages 38?49, London, UK, UK. Springer-Verlag.
V. Ordonez, G. Kulkarni, and T.L. Berg. 2011. Im2text:
Describing images using 1 million captioned pho-
tographs. NIPS.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In In Proceedings of the Fifth Conference
on Applied Natural Language Processing, pages 16?
19.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open
and portable library of computer vision algorithms.
http://www.vlfeat.org/.
9
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11?20,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Domain-Specific Image Captioning
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
{rebecca,ec}@cs.brown.edu
Abstract
We present a data-driven framework for
image caption generation which incorpo-
rates visual and textual features with vary-
ing degrees of spatial structure. We pro-
pose the task of domain-specific image
captioning, where many relevant visual
details cannot be captured by off-the-shelf
general-domain entity detectors. We ex-
tract previously-written descriptions from
a database and adapt them to new query
images, using a joint visual and textual
bag-of-words model to determine the cor-
rectness of individual words. We imple-
ment our model using a large, unlabeled
dataset of women?s shoes images and nat-
ural language descriptions (Berg et al.,
2010). Using both automatic and human
evaluations, we show that our caption-
ing method effectively deletes inaccurate
words from extracted captions while main-
taining a high level of detail in the gener-
ated output.
1 Introduction
Broadly, the task of image captioning is: given a
query image, generate a natural language descrip-
tion of the image?s visual content. Both the im-
age understanding and language generation com-
ponents of this task are challenging open problems
in their respective fields. A wide variety of ap-
proaches have been proposed in the literature, for
both the specific task of caption generation as well
as related problems in understanding images and
text.
Typically, image understanding systems use su-
pervised algorithms to detect visual entities and
concepts in images. However, these typically re-
quire accurate hand-labeled training data, which
is not available in most specific domains. Ideally,
1. Extract existing human-authored caption according to
similarity of coarse visual features.
Query Image Nearest-Neighbor
Nearest-neighbor caption: This sporty sneaker clog keeps
foot cool and comfortable and fully supported.
2. Estimate correctness of extracted words using domain-
specific joint model of text and visual bag-of-word features.
This sporty sneaker clog keeps foot cool and comfortable and
fully supported.
3. Compress extracted caption to adapt its content while
maintaining grammatical correctness.
Output: This clog keeps foot comfortable and supported.
a domain-specific image captioning system would
learn in a less supervised fashion, using captioned
images found on the web.
This paper focuses on image caption genera-
tion for a specific domain ? images of women?s
shoes, collected from online shopping websites.
Our framework has three main components. We
extract an existing description from a database
of human-captions, by projecting query images
into a multi-dimensional space where structurally
similar images are near each other. We also
train a joint topic model to discover the latent
topics which generate both captions and images.
We combine these two approaches using sentence
compression to delete modifying details in the ex-
tracted caption which are not relevant to the query
image.
Our captioning framework is inspired by sev-
eral recent approaches at the intersection of Nat-
ural Language Processing and Computer Vision.
Previous work such as Farhadi et al. (2010) and
Ordonez et al. (2011) explore extractive methods
for image captioning, but these rely on general-
domain visual detection systems, and only gener-
11
ate extractive captions. Other models learn corre-
spondences between domain-specific images and
natural language captions (Berg et al., 2010; Feng
and Lapata, 2010b) but cannot generate descrip-
tions for new images without the use of auxil-
iary text. Kuznetsova et al. (2013) propose a
sentence compression model for editing image
captions, but their compression objective is not
conditioned on a query image, and their system
also requires general-domain visual detections.
This paper proposes an image captioning frame-
work which extends these ideas and culminates in
the first domain-specific image caption generation
system.
More broadly, our goal for image caption gener-
ation is to work toward less supervised captioning
methods which could be used to generate detailed
and accurate descriptions for a variety of long-tail
domains of captioned image data, such as in nature
and medicine.
2 Related Work
Our framework for domain-specific image cap-
tioning consists of three main components: ex-
tractive caption generation, image understanding
through topic modeling, and sentence compres-
sion.
1
These methods have previously been ap-
plied individually to related tasks such as gen-
eral domain image captioning and annotation. We
briefly describe some of the related work:
2.1 Extractive Caption Generation
In previous work on image caption extraction, cap-
tions are generated by retrieving human-authored
descriptions from visually similar images. Farhadi
et al. (2010) and Ordonez et al. (2011) retrieve
whole captions to apply to a query image, while
Kuznetsova et al. (2012) generate captions using
text retrieved from multiple sources. The descrip-
tions are related to visual concepts in the query
image, but these models use visual similarity to
approximate textual relevance; they do not model
image and textual features jointly.
2.2 Image Understanding
Recent improvements in state-of-the-art visual ob-
ject class detections (Felzenszwalb et al., 2010)
1
A research proposal for this framework and other image
captioning ideas was previously presented at NAACL Stu-
dent Research Workshop in 2013 (Mason, 2013). This paper
presents a completed project including implementation de-
tails and experimental results.
have enabled much recent work in image caption
generation (Farhadi et al., 2010; Ordonez et al.,
2011; Kulkarni et al., 2011; Yang et al., 2011;
Mitchell et al., 2012; Yu and Siskind, 2013). How-
ever, these systems typically rely on a small num-
ber of detection types, e.g. the twenty object cate-
gories from the PASCAL VOC challenge.
2
These
object categories include entities which are com-
monly described in general domain images (peo-
ple, cars, cats, etc) but these require labeled train-
ing data which is not typically available for the vi-
sually relevant entities in specific domains.
Our caption generation system employs a multi-
modal topic model from our previous work (Ma-
son and Charniak, 2013) which generates descrip-
tive words, but lacks the spatial structure needed
to generate a full sentence caption. Other previ-
ous work uses topic models to learn the semantic
correspondence between images and labels (e.g.
Blei and Jordan (2003)), but learning from natural
language descriptions is considerably more diffi-
cult because of polysemy, hypernymy, and mis-
alginment between the visual content of an im-
age and the content humans choose to describe.
The MixLDA model (Feng and Lapata, 2010b;
Feng and Lapata, 2010a) learns from news images
and natural language descriptions, but to generate
words for a new image it requires both a query
image and query text in the form of a news arti-
cle. Berg et al. (2010) use discriminative models
to discover visual attributes from online shopping
images and captions, but their models do not gen-
erate descriptive words for unseen images.
2.3 Sentence Compression
Typical models for sentence compression (Knight
and Marcu, 2002; Furui et al., 2004; Turner and
Charniak, 2005; Clarke and Lapata, 2008) have a
summarization objective: reduce the length of a
source sentence without changing its meaning. In
contrast, our objective is to change the meaning of
the source sentence, letting its overall correctness
relative to the query image determine the length
of the output. Our objective differs from that of
Kuznetsova et al. (2013), who compress image
caption sentences with the objective of creating a
corpus of generally transferrable image captions.
Their compression objective is to maximize the
probability of a caption conditioned on the source
2
http://pascallin.ecs.soton.ac.uk/
challenges/VOC/
12
Two adjustable buckle
straps top a classic rubber
rain boot grounded by a
thick lug sole for excellent
wet-weather traction.
Available in Plus Size. Faux
snake skin flats with a large
crossover buckle at the toe.
Padded insole for a comfort-
able all day fit.
Glitter-covered elastic up-
per in a two-piece dress san-
dal style with round open
toe. Single vamp strap with
contrasting trim matching
elasticized heel strap criss-
crosses at instep.
Explosive! These white
leather joggers are sure to
make a big impression. De-
tails count, including a toe
overlay, millennium trim
and lightweight raised sole.
Table 1: Example data from the Attribute Discovery Dataset (Berg et al., 2010). See Section 3.
image, while our objective is conditioned on the
query image that we are generating a caption for.
Additionally, their model also relies on general-
domain trained visual detections.
3 Dataset and Preprocessing
The dataset we use is the women?s shoes sec-
tion of the publicly available Attribute Discov-
ery Dataset
3
from Berg et al. (2010), which con-
sists of product images and captions scraped from
the shopping website Like.com. We use the
women?s shoes section of the dataset which has
14764 captioned images. Product descriptions de-
scribe many different attributes such as styles, col-
ors, fabrics, patterns, decorations, and affordances
(activities that can be performed while wearing the
shoe). Some examples are shown in Table 1.
For preprocessing in our framework, we first de-
termine an 80/20% train test split. We define a tex-
tual vocabulary of ?descriptive words?, which are
non-function words ? adjectives, adverbs, nouns
(except proper nouns), and verbs. This gives us
a total of 9578 descriptive words in the training
set, with an average of 16.33 descriptive words per
caption.
4 Image Captioning Framework
4.1 Extraction
To repeat, our overall process is to first find a cap-
tion sentence from our database to use as a tem-
plate, and then correct the template sentences us-
ing sentence compresion. We compress by remov-
3
http://tamaraberg.com/
attributesDataset/index.html
ing details that are probably not correct for the test
image. For example, if the sentence describes ?a
red slipper? but the shoe in the query image is yel-
low, we want to remove ?red? and keep the rest.
As in this simple example, the basic paradigm
for compression is to keep the head words of
phrases (?slipper?) and remove modifiers. Thus
we want to extraction stage of our scheme to be
more likely to find a candidate sentence with cor-
rect head words, figuring that the compression
stage can edit the mistakes. Our hypothesis is that
headwords tend to describe more spatially struc-
tured visual concepts, while modifier words de-
scribe those that are more easily represented using
local or unstructured features.
4
Table 2 contains
additional example captions with parses.
GIST (Oliva and Torralba, 2001) is a com-
monly used feature in Computer Vision which
coarsely localizes perceptual attributes (e.g. rough
vs smooth, natural vs manmade). By computing
the GIST of the images, we project them into a
multi-dimensional Euclidean space where images
with semantically similar structures are located
near each other. Thus the extraction stage of our
caption generation process selects a sentence from
the GIST nearest-neighbor to the query image.
5
4.2 Joint Topic Model
The second component of our framework incorpo-
rates visual and textual features using a less struc-
tured model. We use a multi-modal topic model
4
For example, the color ?red? can be described using a
bag of random pixels, while a ?slipper? is a spatial configura-
tion of parts in relationship to each other.
5
See Section 5.1 for additional implementation details.
13
Table 2: Example parses of women?s shoes descriptions. Our hypothesis is that the headwords in phrases
are more likely to describe visual concepts which rely on spatial locations or relationships, while modi-
fiers words can be represented using less-structured visual bag-of-words features.
to learn the latent topics which generate bag-of-
words features for an image and its caption.
The bag-of-words model for Computer Vision
represents images as a mixture of topics. Mea-
sures of shape, color, texture, and intensity are
computed at various points on the image and clus-
tered into discrete ?codewords? using the k-means
algorithm.
6
Unlike text words, an individual code-
word has little meaning on its own, but distri-
butions of codewords can provide a meaningful,
though unstructured, representation of an image.
An image and its caption do not express exactly
the same information, but they are topically re-
lated. We employ the Polylingual Topic Model
(Mimno et al., 2009), which is originally used to
model corresponding documents in different lan-
guages that are topically comparable, but not par-
allel translations. In particular, we employ our
previous work (Mason and Charniak, 2013) which
extends this model to topically similar images and
natural language captions. The generative process
for a captioned image starts with a single topic
distribution drawn from concentration parameter
? and base measure m:
? ? Dir(?, ?m) (1)
Modality-specific latent topic assignments z
img
and z
txt
are drawn for each of the text words and
codewords:
z
img
? P (z
img
|?) =
?
n
?
z
img
n
(2)
6
While space limits a more detailed explanation of visual
bag-of-word features, Section 5.2 provides a brief overview
of the specific visual attributes used in this model.
z
txt
? P (z
txt
|?) =
?
n
?
z
txt
n
(3)
Observed words are generated according to their
probabilities in the modality-specific topics:
w
img
? P (w
img
|z
img
,?
img
) = ?
img
w
img
n
|z
img
n
(4)
w
txt
? P (w
txt
|z
txt
,?
txt
) = ?
txt
w
txt
n
|z
txt
n
(5)
Given the uncaptioned query image q
img
and
the trained multi-modal topic model, it is now pos-
sible to infer the shared topic proportion for q
img
using Gibbs sampling:
P (z
n
= t|q
img
, z
\n
,?
img
, ?m)
? ?
img
q
img
n
|t
(N
t
)
\n
+ ?m
t
?
t
N
t
? 1 + ?
(6)
4.3 Sentence Compression
Let w = w
1
, w
2
, ..., w
n
be the words in the ex-
tracted caption for q
img
. For each word, we de-
fine a binary decision variable ?, such that ?
i
= 1
if w
i
is included in the output compression, and
?
i
= 0 otherwise. Our objective is to find values
of ? which generate a caption for q
img
which is
both semantically and grammatically correct.
We cast this problem as an Integer Linear Pro-
gram (ILP), which has previously been used for
the standard sentence compression task (Clarke
and Lapata, 2008; Martins and Smith, 2009). ILP
is a mathematical optimization method for deter-
mining the optimal values of integer variables in
order to maximize an objective given a set of con-
straints.
14
4.3.1 Objective
The ILP objective is a weighted linear combina-
tion of two measures which represent the correct-
ness and fluency of the output compression:
Correctness: Recall in Section 3 we defined
words as either descriptive words or function
words. For each descriptive word, we estimate
P (w
i
|q
img
), using topic proportions estimated us-
ing Equation 6:
P (w
i
|q
img
) =
?
t
P (w
i
|z
txt
t
)P (z
t
|q
img
) (7)
This is used to find I(w
i
), a function of the likeli-
hood of each word in the extracted caption:
I(w
i
) =
{
P (w
i
|q
img
)? P (w
i
), if descriptive
0, function word
(8)
This function considers the prior probability of w
i
because frequent words often have a high posterior
probability even when they are inaccurate. Thus
the sum
?
n
i=1
?
i
? I(w
i
) is the overall measure of
the correctness of a proposed caption conditioned
on q
img
.
Fluency: We formulate a trigram language
model as an ILP, which requires additional binary
decision variables: ?
i
= 1 if w
i
begins the out-
put compression, ?
ij
= 1 if the bigram sequence
w
i
, w
j
ends the compression, ?
ijk
= 1 if the tri-
gram sequence w
i
, w
j
, w
k
is in the compression,
and a special ?start token? ?
0
= 1. This language
model favors shorter sentences, which is not nec-
essarily the objective for image captioning, so we
introduce a weighting factor, ?, to lessen the ef-
fect.
Here is the combined objective, using P to rep-
resent logP :
max z =
(
n
?
i=1
?
i
? P (w
i
|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k=j+1
?
ijk
? P (w
k
|w
i
, w
j
)
+
n?1
?
i=0
n
?
j=i+1
?
ij
? P (end|w
i
, w
j
)
)
? ?
+
n
?
i=1
?
i
? I(w
i
) (9)
Sequential
1.)
?
i
?
i
= 1
2.) ?
k
? ?
k
?
?
k?2
i=0
?
k?1
j=1
?
ijk
= 0
?k : k ? 1...n
3.) ?
j
?
?
j?1
i=0
?
n
k=j+1
?
ijk
?
?
j?1
i=0
?
ij
= 0
?j : j ? 1...n
4.)
?
n?1
j=i+1
?
n
k=j+1
?
ijk
?
?
n
j=i+1
?
ij
?
?
i?1
h=0
?
hi
? ?
i
= 0
?i : i ? 1...n
5.)
?
n?1
i=0
?
n
j=i+1
?
ij
= 1
Modifier
1. If head of the extracted sentence= w
i
, then
?
i
= 1
2. If w
i
is head of a noun phrase, then ?
i
= 1
3. Punctuation and coordinating conjunctions
follow special rules (below). Otherwise, if
headof(w
i
) = w
j
, then ?
i
? ?
j
Other
1.
?
i
?
i
? 3
2. Define valid use of puncutation and coordi-
nating conjunctions.
Table 3: Summary of ILP constraints.
4.3.2 ILP Constraints
The ILP constraints ensure both the mathematical
validity of the model, and the grammatical correct-
ness of its output. Table 3 summarizes the list of
constraints. Sequential constraints are defined as
in Clarke (2008) ensure that the ordering of the tri-
grams is valid, and that the mathematical validity
of the model holds.
5 Implementation Details
5.1 Extraction
GIST features are computed using code by Oliva
and Torralba (2001)
7
. GIST is computed with im-
ages converted to grayscale; since color features
tend to act as modifiers in this domain. Nearest-
neighbors are selected according to minimum dis-
tance from q
img
to both a regularly-oriented and a
horizontally-flipped training image.
Only one sentence from the first nearest-
neighbor caption is extracted. In the case of multi-
sentence captions, we select the first suitable sen-
tence according to the following criteria 1.) has
at least five tokens, 2.) does not contain NNP or
NNPS (brand names), 3.) does not fail to parse
using Stanford Parser (Klein and Manning, 2003).
If the nearest-neighbor caption does not have any
sentences meeting these criteria, caption sentences
from the next nearest-neighbor(s) are considered.
7
http://people.csail.mit.edu/torralba/
code/spatialenvelope/
15
5.2 Joint Topic Model
We use the Joint Topic Model that we imple-
mented in our previous work; please see Mason
and Charniak (2013) for the full model and imple-
mentation details. The topic model is trained with
200 topics using the polylingual topic model im-
plementation from MALLET
8
. Briefly, the code-
words represent the following attributes:
SHAPE: SIFT (Lowe, 1999) describes the
shapes of detected edges in the image, using de-
scriptors which are invariant to changes in rotation
and scale.
COLOR: RGB (red, green, blue) and HSV (hue,
saturation, value) pixel values are sampled from a
central area of the image to represent colors.
TEXTURE: Textons (Leung and Malik, 2001)
are computed by convolving images with Gabor
filters at multiple orientations and scales, then
sampling the outputs at random locations.
INTENSITY: HOG (histogram of gradients)
(Dalal and Triggs, 2005) describes the direction
and intensity of changes in light. These features
are computed on the image over a densely sam-
pled grid.
5.3 Compression
The sentence compression ILP is implemented us-
ing the CPLEX optimization toolkit
9
. The lan-
guage model weighting factor in the objective is
? = 10
?3
, which was hand-tuned according to
observed output. The trigram language model
is trained on training set captions using Berke-
leyLM (Pauls and Klein, 2011) with Kneser-Ney
smoothing. For the constraints, we use parses
from Stanford Parser (Klein and Manning, 2003)
and the ?semantic head? variation of the Collins
headfinder Collins (1999).
6 Evaluation
6.1 Setup
We compare the following systems and baselines:
KL (EXTRACTION): The top performing ex-
tractive model from Feng and Lapata (2010a), and
the second-best captioning model overall. Using
estimated topic distributions from our joint model,
we extract the source with minimum KL Diver-
gence from q
img
.
8
http://mallet.cs.umass.edu/
9
http://www-01.ibm.com/
software/integration/optimization/
cplex-optimization-studio/
ROUGE-2 Average 95% Confidence int.
KL (EXTRACTION)
P .06114 ( .05690 - .06554 )
R .02499 ( .02325 - .02686)
F .03360 ( .03133 - .03600 )
GIST (EXTRACTION)
P .10894 ( .09934 - .11921 )
R .05474 ( .04926 - .06045)
F .06863 ( .06207 - .07534)
LM-ONLY (COMPRESSION)
P .13782 ( .12602 - .14864 )
R .02437 ( .02193 - .02700 )
F .03864 ( .03512 - .04229)
SYSTEM (COMPRESSION)
P .16752 (.15679 -.17882 )
R .05060 ( .04675 - .05524 )
F .07204 ( .06685 - .07802 )
Table 4: ROUGE-2 (bigram) scores. The pre-
cision of our system compression (bolded) sig-
nificantly improves over the caption that it com-
presses (GIST), without a significant decrease in
recall.
GIST (EXTRACTION): The sentence extracted
using GIST nearest-neighbors, and the uncom-
pressed source for the compression systems.
LM-ONLY (COMPRESSION): We include this
baseline to demonstrate that our model is effec-
tively conditioning output compressions on q
img
,
as opposed to simply generalizing captions as in
Kuznetsova et al. (2013)
10
. We modify the com-
pression ILP to ignore the content objective and
only maximize the trigram language model (still
subject to the constraints).
SYSTEM (COMPRESSION): Our full system.
Unfortunately, we cannot compare our system
against prior work in general-domain image cap-
tioning, because those models use visual detec-
tion systems which train on labeled data that is not
available in our domain.
6.2 Automatic Evaluation
We perform automatic evaluation using similar-
ity measures between automatically generated and
human-authored captions. Note that currently
our system and baselines only generate single-
sentence captions, but we compare against entire
10
Technically their model is conditioned on the source im-
age, in order to address alignment issues which are not appli-
cable in our setup.
16
BLEU@1
KL (EXTRACTION) .2098
GIST (EXTRACTION) .4259
LM-ONLY (COMPRESSION) .4780
SYSTEM (COMPRESSION) .4841
Table 5: BLEU@1 scores of generated captions
against human authored captions. Our model
(bolded) has the highest BLEU@1 score with sig-
nificance.
held-out captions in order to increase the amount
of text we have to compare against.
ROUGE (Lin, 2004) is a summarization eval-
uation metric which has also been used to eval-
uate image captions (Yang et al., 2011). It is
usually a recall-oriented measure, but we also re-
port precision and f-measure because our sen-
tence compressions do not improve recall. Table 4
shows ROUGE-2 (bigram) scores computed with-
out stopwords.
We observe that our system very significantly
improves ROUGE-2 precision of the GIST ex-
tracted caption, without significantly reducing re-
call. While LM-Only also improves precision
against GIST extraction, it indiscriminately re-
moves some words which are relevant to the
query image. We also observe that GIST extrac-
tion strongly outperforms the KL model, which
demonstrates the importance of visual structure.
We also report BLEU (Papineni et al., 2002)
scores, which are the most popularly accepted au-
tomatic metric for captioning evaluation (Farhadi
et al., 2010; Kulkarni et al., 2011; Ordonez et
al., 2011; Kuznetsova et al., 2012; Kuznetsova
et al., 2013). Results are very similar to the
ROUGE-2 precision scores, except the difference
between our system and LM-Only is less pro-
nounced because BLEU counts function words,
while ROUGE does not.
6.3 Human Evaluation
We perform human evaluation of compressions
generated by our system and LM-Only. Users are
shown the query image, the original uncompressed
caption, and a compressed caption, and are asked
two questions: does the compression improve the
accuracy of the caption, and is the compression
grammatical.
We collect 553 judgments from six women who
are native English-speakers and knowledgeable
Query Image GIST Nearest-Neighbor
Extraction: Shimmering snake-embossed leather upper in a
slingback evening dress sandal style with a round open toe.
Compression: Shimmering upper in a slingback evening
dress sandal style with a round open toe.
Query Image GIST Nearest-Neighbor
Extraction: This sporty sneaker clog keeps foot cool and
comfortable and fully supported.
Compression: This clog keeps foot comfortable and sup-
ported.
Query Image GIST Nearest-Neighbor
Extraction: Italian patent leather peep-toe ballet flat with a
signature tailored grosgrain bow.
Compression: leather ballet flat with a signature tailored
grosgrain bow.
Query Image GIST Nearest-Neighbor
Extraction: Platform high heel open toe pump with horsebit
available in silver guccissima leather with nickel hardware
with leather sole.
Compression: Platform high heel open toe pump with
horsebit available in leather with nickel hardware with
leather sole.
Table 6: Example output from our full system.
Red underlined words indicate the words which
are deleted by our compression model.
17
SYSTEM LM-ONLY
Yes No Yes No
Compression
improves
accuracy
63.2% 36.8% 42.6% 57.4%
Compression is
grammatical
73.1% 26.9% 82.2% 17.8%
Table 7: Human evaluation results.
about fashion.
11
Users were recruited via email
and did the study over the internet.
Table 7 reports the results of the human evalu-
ation. Users report 63.2% of SYSTEM compres-
sions improve accuracy over the original, while
the other 36.8% did not improve accuracy. (Keep
in mind that a bad compression does not make the
caption less accurate, just less descriptive.) LM-
ONLY improves accuracy for less than half of the
captions, which is significantly worse than SYS-
TEM captions (Fisher exact test, two-tailed p less
than 0.01).
Users find LM-Only compressions to be slightly
more grammatical than System compressions, but
the difference is not significant. (p > 0.05)
7 Conclusion
We introduce the task of domain-specific image
captioning and propose a captioning system which
is trained on online shopping images and natu-
ral language descriptions. We learn a joint topic
model of vision and text to estimate the correct-
ness of extracted captions, and use a sentence
compression model to propose a more accurate
output caption. Our model exploits the connection
between image and sentence structure, and can be
used to improve the accuracy of extracted image
captions.
The task of domain-specific image caption
generation has been overlooked in favor of the
general-domain case, but we believe the domain-
specific case deserves more attention. While
image captioning can be viewed as a complex
grounding problem, a good image caption should
do more than label the objects in the image. When
an expert looks at images in a specific domain, he
or she makes inferences that would not be made by
a non-expert. Providing this information to non-
11
About 15% of output compressions are the same for both
systems, and about 10% have no deleted words in the output
compression. We include the former in the human evaluation,
but not the latter.
Query Image GIST Nearest-Neighbor
Extraction: Classic ballet flats with decorative canvas
strap and patent leather covered buckle.
Compression: Classic ballet flats covered.
Query Image GIST Nearest-Neighbor
Extraction: This shoe is the perfect shoe for you , fea-
turing an open toe and a lace up upper with a high heel
, and a two tone color .
Compression: This shoe is the shoe , featuring an open toe
and upper with a high heel .
Table 8: Examples of bad performance. The top
example is a parse error, while the bottom exam-
ple deletes modifiers that are not part of the image
description.
expert users in the form of an image caption will
greatly expand the utility for automatic image cap-
tioning.
References
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and charac-
terization from noisy web data. In Proceedings of
the 11th European conference on Computer vision:
Part I, ECCV?10, pages 663?676, Berlin, Heidel-
berg. Springer-Verlag.
David M. Blei and Michael I. Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th annual
international ACM SIGIR conference on Research
and development in informaion retrieval, SIGIR ?03,
pages 127?134, New York, NY, USA. ACM.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. J. Artif. Int. Res., 31(1):399?
429, March.
James Clarke. 2008. Global Inference for Sentence
Compression: An Integer Linear Programming Ap-
proach. Dissertation, University of Edinburgh.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
Philadelphia, PA, USA. AAI9926110.
18
N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In Computer Vision
and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages
886 ?893 vol. 1, june.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences from images.
In Proceedings of the 11th European conference on
Computer vision: Part IV, ECCV?10, pages 15?29,
Berlin, Heidelberg. Springer-Verlag.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2010. Object detection with discrim-
inatively trained part based models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(9):1627?1645.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In
HLT-NAACL, pages 831?839.
Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka,
and Chiori Hori. 2004. Speech-to-text and speech-
to-speech summarization of spontaneous speech.
IEEE TRANS. ON SPEECH AND AUDIO PRO-
CESSING, 12(4):401?408.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107, July.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In CVPR, pages
1601?1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2013. Generalizing
image captions for image-text parallel corpus. In
ACL.
T. Leung and J. Malik. 2001. Representing and rec-
ognizing the visual appearance of materials using
three-dimensional textons. International Journal of
Computer Vision, 43(1):29?44.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
D.G. Lowe. 1999. Object recognition from local scale-
invariant features. In Computer Vision, 1999. The
Proceedings of the Seventh IEEE International Con-
ference on, volume 2, pages 1150 ?1157 vol.2.
Andr?e F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, ILP ?09, pages 1?9, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
R. Mason and E. Charniak. 2013. Annotation of online
shopping images without labeled training examples.
Workshop on Vision and Language (WVL).
Rebecca Mason. 2013. Domain-independent caption-
ing of domain-specific images. NAACL Student Re-
search Workshop.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 880?889, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Men-
sch, Alexander C. Berg, Tamara L. Berg, and Hal
Daum?e III. 2012. Midge: Generating image de-
scriptions from computer vision detections. In Euro-
pean Chapter of the Association for Computational
Linguistics (EACL).
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42:145?175.
V. Ordonez, G. Kulkarni, and T.L. Berg. 2011.
Im2text: Describing images using 1 million cap-
tioned photographs. In NIPS.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
19
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of ACL,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 290?297, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods
in Natural Language Processing (EMNLP), Edin-
burgh, Scotland.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 53?63,
Sofia, Bulgaria. Association for Computational Lin-
guistics.
20
