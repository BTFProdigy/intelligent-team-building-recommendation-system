Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 862?870,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Bilingual dictionary generation for low-resourced language pairs 
 
 
Varga Istv?n 
Yamagata University,  
Graduate School of Science and Engineering 
dyn36150@dip.yz.yamagata-u.ac.jp 
Yokoyama Shoichi 
Yamagata University,  
Graduate School of Science and Engineering 
yokoyama@yz.yamagata-u.ac.jp 
 
  
 
Abstract 
Bilingual dictionaries are vital resources in 
many areas of natural language processing. 
Numerous methods of machine translation re-
quire bilingual dictionaries with large cover-
age, but less-frequent language pairs rarely 
have any digitalized resources. Since the need 
for these resources is increasing, but the hu-
man resources are scarce for less represented 
languages, efficient automatized methods are 
needed. This paper introduces a fully auto-
mated, robust pivot language based bilingual 
dictionary generation method that uses the 
WordNet of the pivot language to build a new 
bilingual dictionary. We propose the usage of 
WordNet in order to increase accuracy; we 
also introduce a bidirectional selection method 
with a flexible threshold to maximize recall. 
Our evaluations showed 79% accuracy and 
51% weighted recall, outperforming represen-
tative pivot language based methods. A dic-
tionary generated with this method will still 
need manual post-editing, but the improved 
recall and precision decrease the work of hu-
man correctors. 
1 Introduction 
In recent decades automatic and semi-automatic 
machine translation systems gradually managed 
to take over costly human tasks. This much wel-
comed change can be attributed not only to major 
developments in techniques regarding translation 
methods, but also to important translation re-
sources, such as monolingual or bilingual dic-
tionaries and corpora, thesauri, and so on. How-
ever, while widely used language pairs can fully 
take advantage of state-of-the-art developments 
in machine translation, certain low-frequency, or 
less common language pairs lack some or even 
most of the above mentioned translation re-
sources. In that case, the key to a highly accurate 
machine translation system switches from the 
choice and adaptation of the translation method 
to the problem of available translation resources 
between the chosen languages. 
One possible solution is bilingual corpus ac-
quisition for statistical machine translation 
(SMT). However, for highly accurate SMT sys-
tems large bilingual corpora are required, which 
are rarely available for less represented lan-
guages. Rule or sentence pattern based systems 
are an attractive alternative, for these systems the 
need for a bilingual dictionary is essential. 
Our paper targets bilingual dictionary genera-
tion, a resource which can be used within the 
frameworks of a rule or pattern based machine 
translation system. Our goal is to provide a low-
cost, robust and accurate dictionary generation 
method. Low cost and robustness are essential in 
order to be re-implementable with any arbitrary 
language pair. We also believe that besides high 
precision, high recall is also crucial in order to 
facilitate post-editing which has to be performed 
by human correctors. For improved precision, we 
propose the usage of WordNet, while for good 
recall we introduce a bidirectional selection 
method with local thresholds. 
Our paper is structured as follows: first we 
overview the most significant related works, af-
ter which we analyze the problems of current 
dictionary generation methods. We present the 
details of our proposal, exemplified with the 
Japanese-Hungarian language pair. We evaluate 
the generated dictionary, performing also a com-
parative evaluation with two other pivot-
language based methods. Finally we present our 
conclusions. 
2 Related works 
2.1 Bilingual dictionary generation 
Various corpus based, statistical methods with 
very good recall and precision were developed 
starting from the 1980?s, most notably using the 
862
Dice-coefficient (Kay & R?scheisen, 1993), cor-
respondence-tables (Brown, 1997), or mutual 
information (Brown et al, 1998).  
As an answer to the corpus-based method?s 
biggest disadvantage, namely the need for a large 
bilingual corpus, in the 1990?s Tanaka and 
Umemura (1994) presented a new approach. As a 
resource, they only use dictionaries to and from a 
pivot language to generate a new dictionary. 
These so-called pivot language based methods 
rely on the idea that the lookup of a word in an 
uncommon language through a third, intermedi-
ated language can be automated. Tanaka and 
Umemura?s method uses bidirectional source-
pivot and pivot-target dictionaries (harmonized 
dictionaries). Correct translation pairs are se-
lected by means of inverse consultation, a 
method that relies on counting the number of 
pivot language definitions of the source word, 
through which the target language definitions can 
be identified (Tanaka and Umemura, 1994).  
Sj?bergh (2005) also presented an approach to 
pivot language based dictionary generation. 
When generating his English pivoted Swedish-
Japanese dictionary, each Japanese-to-English 
description is compared with each Swedish-to-
English description. Scoring is based on word 
overlap, weighted with inverse document fre-
quency; the best matches being selected as trans-
lation pairs.  
These two approaches described above are the 
best performing ones that are general enough to 
be applicable with other language pairs as well. 
In our research we used these two methods as 
baselines for comparative evaluation.  
There are numerous refinements of the above 
methods, but for various reasons they cannot be 
implemented with any arbitrary language pair. 
Shirai and Yamamoto (2001) used English to 
design a Korean-Japanese dictionary, but be-
cause the usage of language-specific information, 
they conclude that their method ?can be consid-
ered to be applicable to cases of generating 
among languages similar to Japanese or Korean 
through English?. In other cases, only a small 
portion of the lexical inventory of the language is 
chosen to be translated: Paik et al (2001) pro-
posed a method with multiple pivots (English 
and Kanji/Hanzi characters) to translate Sino-
Korean entries. Bond and Ogura describe a Japa-
nese-Malay dictionary that uses a novel tech-
nique in its improved matching through normali-
zation of the pivot language, by means of seman-
tic classes, but only for nouns (2007). Besides 
English, they also use Chinese as a second pivot.  
2.2 Lexical database in lexical acquisition 
Large lexical databases are vital for many areas 
in natural language processing (NLP), where 
large amount of structured linguistic data is 
needed. The appearance of WordNet (Miller et 
al., 1990) had a big impact in NLP, since not 
only did it provide one of the first wide-range 
collections of linguistic data in electronic format, 
but it also offered a relatively simple structure 
that can be implemented with other languages as 
well. In the last decades since the first, English 
WordNet, numerous languages adopted the 
WordNet structure, thus creating a potential large 
multilingual network. The Japanese language is 
one of the most recent ones added to the Word-
Net family (Isahara et al 2008), but the Hungar-
ian WordNet is still under development 
(Pr?sz?ky et al 2001; Mih?ltz and Pr?sz?ky 
2004). 
Multilingual projects, such as EuroWordNet 
(Vossen 1998; Peters et al 1998), Balkanet 
(Stamou et al 2002) or Multilingual Central Re-
pository (Agirre et al 2007) aim to solve numer-
ous problems in natural language processing. 
EuroWordNet was specifically designed for 
word disambiguation purposes in cross-language 
information retrieval (Vossen 1998). The internal 
structure of the multilingual WordNets itself can 
be a good starting point for bilingual dictionary 
generation. In case of EuroWordNet, besides the 
internal design of the initial WordNet for each 
language, an Inter-Lingual-Index interlinks word 
meaning across languages is implemented (Pe-
ters et al 1998). However, there are two limita-
tions: first of all, the size of each individual lan-
guage database is relatively small (Vossen 1998), 
covering only the most frequent words in each 
language, thus not being sufficient for creating a 
dictionary with a large coverage. Secondly, these 
multilingual databases cover only a handful of 
languages, with Hungarian or Japanese not being 
part of them. Adding a new language would re-
quire the existence of a WordNet of that lan-
guage.  
3 Problems of current pivot language 
based methods 
3.1 Selection method shortcomings 
Previous pivot language based methods generate 
and score a number of translation candidates, and 
the candidate?s scores that exceed a certain pre-
defined global threshold are selected as viable 
translation pairs. However, the scores highly de-
863
pend on the entry itself or the number of transla-
tions in the pivot language, therefore there is a 
variance in what that score represents. For this 
reason, a large number of good entries are en-
tirely left out from the dictionary, because all of 
their translation candidates scored low, while 
faulty translation candidates are selected, be-
cause they exceed the global threshold. Due to 
this effect the recall value drops significantly. 
3.2 Dictionaries not enough as resource 
Regardless of the language pair, in most cases 
the meanings of the corresponding words are not 
identical; they only overlap to a certain extent. 
Therefore, the pivot language based dictionary 
generation problem can be defined as the identi-
fication of the common elements or the extent of 
the relevant overlapping in the source-to-pivot 
and target-to-pivot definitions.  
Current methods perform a strictly lexical 
overlap of the source-pivot and target-pivot en-
tries. Even if the meanings of the source and tar-
get head words are transferred to the pivot lan-
guage, this is rarely done with the same set of 
words or definitions. Thus, due to the different 
word-usage or paraphrases, even semantically 
identical or very similar head words can have 
different definitions in different dictionaries. As 
a result, performing only lexical overlap, current 
methods cannot identify the differences between 
totally different definitions resulted by unrelated 
concepts, and differences in only nuances re-
sulted by lexicographers describing the same 
concept, but with different words.  
4 Proposed method 
4.1 Specifics of our proposal 
For higher precision, instead of the familiar lexi-
cal overlap of the current methods we calculate 
the semantically expanded lexical overlap of the 
source-to-pivot and target-to-pivot translations. 
In order to do that, we use semantic information 
extracted from the WordNet of the pivot lan-
guage. 
To improve recall, we introduce bidirectional 
selection. As we stated above, the global thresh-
old eliminates a large number of good translation 
pairs, resulting in a low recall. As a solution, we 
can group the translations that share the same 
source or target entry, and set local thresholds 
for each head word. For example, for a source 
language head word entry_source there could be 
multiple target language candidates:  en-
try_target1, ? ,entry_targetn. If the top scoring 
entry_targetk candidates are selected, we ensure 
that at least one translation will be available for 
entry_source, maintaining a high recall. Since we 
can group the entries in the source language and 
target language as well, we perform this selection 
twice, once in each direction. Local thresholds 
depend on the top scoring entry_target, being set 
to maxscore?c. Constant c varies between 0 and 1, 
allowing a small window not only for the maxi-
mum, but high scoring candidates as well. It is 
language and selection method dependent (see 
?5.1 for details). 
4.2 Translation resources 
As an example of a less-common language pair, 
we have chosen Japanese and Hungarian. For 
translation candidate generation, we have chosen 
two freely available dictionaries with English as 
the pivot language. The Japanese-English dic-
tionary had 197282, while the Hungarian-English 
contained 189331 1-to-1 entry pairs. The Japa-
nese-English dictionary had part-of-speech 
(POS) information as well, but to ensure robust-
ness, our method does not use this information.  
To select from the translation candidates, we 
mainly use WordNet (Miller et. al., 1990). From 
WordNet we consider four types of information: 
sense categorization, synonymy, antonymy and 
semantic categories provided by the tree struc-
ture of nouns and verbs.  
4.3 Dictionary generation method 
Our proposed method consists of two steps. In 
step 1 we generate a number of translation pair 
candidates, while in step 2 we score and select 
from them based on semantic information ex-
tracted from WordNet.  
Step 1: translation candidate generation 
Using the source-pivot and pivot-target diction-
aries, we connect the source and target entries 
that share at least one common translation in the 
pivot language. We consider each source-target 
pair a translation candidate. With our Japanese-
English and English-Hungarian dictionaries we 
accumulated 436966 Japanese-Hungarian trans-
lation candidates. 
Step 2: translation pair selection 
We examine the translation candidates one by 
one, looking up the source-pivot and target-pivot 
dictionaries, comparing the translations in the 
pivot language. There are six types of transla-
tions that we label A-F and explain below. First, 
864
we perform a strictly lexical match based only on 
the dictionaries. Next, using information ex-
tracted from WordNet we attempt to identify the 
correct translation pairs.     
(a) Lexically unambiguous translation pairs 
Some of the translation candidates have exactly 
the same translations into in the pivot language; 
we consider these pairs as being correct by de-
fault. Also among the translation candidates we 
identified a number of source entries that had 
only one target translation; and a number of tar-
get entries that had only one source translation. 
Being the sole candidates for the given entries, 
we consider these pairs too as being correct. 
37391 Japanese-Hungarian translation pairs were 
retrieved with this method (type A pairs). 
(b) Using sense description 
For most polysemous words WordNet has de-
tailed descriptions with synonyms for each sense. 
We use these synonyms of WordNet?s sense de-
scriptions to disambiguate the meanings of the 
common translations. For a given source-target 
translation candidate (s,t) we look up the source-
pivot and target-pivot translations 
(s?I={s?i1,?,s?in} and 
t?I={t?i1,?,t?im}). We select the elements 
that are common in the two definitions 
(I?=(s?I)?(t?I)) and we look up their respec-
tive senses from WordNet (sns(I?)). We identify 
the words? senses comparing each synonym in 
the WordNet?s synonym description with each 
word from the dictionary definition. As a result, 
for each common word we arrive at a certain set 
of senses from the source-pivot definitions 
(sns((s?I?)) and a certain set of senses from the 
target-pivot definitions (sns((t?I?)). We mark 
scoreB(s,t) the maximum ratio of the identical 
and total number of identified senses (Jaccard 
coefficient). The higher the scoreB(s,t) is, the 
more probable is candidate (s,t) a valid transla-
tion. 
( ) ( ) ( )
( ) ( )
( ) ( )''
''
max,
' itsnsissns
itsnsissns
tsscore
ItIsi
B
???
???
=
????
 (1) 
For example, ?? (seikai: correct, right, cor-
rect interpretation) and helyes (correct, proper, 
right, appropriate) have two common transla-
tions (I?={right, correct}), thus scoreB(s,t) can be 
performed with these two words. The adjective 
right has 13 senses according to WordNet, 
among them 4 were identified from the Japanese 
to English definition (sns(right)={#1, #3, #5, 
#10}, all identified through correct) and 5 from 
the Hungarian to English definition 
(sns(right)={#1, #3, #5, #6, #10}, through cor-
rect or proper). As a result, 4 senses are com-
mon, and 1 is different. Thus the adjective right?s 
score is 0.8 (scoreB(s,t)[right](??,helyes)). The 
adjective correct has 4 senses, all of them are 
recognized by both definitions through right, 
therefore the score through correct is 1 
(scoreB(s,t)[correct](?? ,helyes)). The maxi-
mum of the above scores is the final score: 
scoreB(s,t)(??,helyes)=1. 
All translation candidates are verified based 
on all four POS available from WordNet. Since 
synonymy information is available for nouns (N), 
verbs (V), adjectives (A) and adverbs (R), four 
separate scores are calculated for each POS. 
Scores that pass a global threshold are consid-
ered correct. 33971 Japanese-Hungarian candi-
dates (type B translations) were selected, with 
these two languages the global threshold was set 
to 0.1. Even this low value ensures that at least 
one of ten meanings is shared by the two entries 
of the pair, thus being suitable as translation pair. 
(c) Using synonymy, antonymy and semantic 
categories 
We expand the source-to-pivot and target-to-
pivot definitions with information from WordNet 
(synonymy, antonymy and semantic category, 
respectively). Thus the similarity of the two ex-
panded pivot language descriptions gives a better 
indication on the suitability of the translation 
candidate. Using the three relations, the common 
versus total number of translations (Jaccard coef-
ficient) will define the appropriateness of the 
translation candidate. 
( ) ( ) ( )( ) ( )itextisext
itextisext
tsscore EDC
???
???
=,
,,
 (2) 
Since the same word or concept?s translations 
into the pivot language also share the same se-
mantic value, the extension with synonyms 
(ext(l?i)=(l?i)?syn(l?i), where l={s,t}) the 
extended translation should share more common 
elements.  
In case of antonymy, we expand the initial 
definitions with the antonyms of the antonyms 
(ext(l?i)=(l?i)?ant(ant(l?i)), where l={s,t}). 
This extension is different from the synonymy 
extension, in most cases the resulting set of 
words being considerably larger. 
Along with synonymy, antonymy is also avail-
able for nouns, verbs, adjectives and adverbs, 
four separate scores are calculated for each POS. 
865
Semantic categories are provided by the tree 
structure (hypernymy/hyponymy) of nouns and 
verbs of WordNet. We transpose each entry from 
the pivot translations to its semantic categories 
(ext(l?i)=?semcat(l?i), where l={s,t}). We as-
sume that the correct translation pairs share a 
high percentage of semantic categories. Accord-
ingly, the translations of semantically similar or 
identical entries should share a high number of 
common semantic categories. 
The scores based on these relations highly de-
pend on the number of pivot language transla-
tions; therefore we use the bidirectional selection 
method with local thresholds for each source and 
target head word. Local thresholds are set based 
on the best scoring candidate for a given entry. 
The thresholds were maxscore?0.9 for synonymy 
and antonymy; and maxscore?0.8 for the seman-
tic categories (see ?5.1 for details). 
Using synonymy, 196775 candidate pairs 
(type C), with antonymy 99614 pairs (type D); 
while with semantic categories 195480 pairs 
(type E) were selected. 
(d) Combined semantic information 
The three separate lists of type C, D and E selec-
tion methods resulted in slightly different results, 
proving that they cannot be used as standalone 
selection methods (see ?5.2 for details). 
Because of the multiple POS labelling of nu-
merous words in WordNet, many translation 
pairs can be selected up to four times based on 
separate POS information (noun, verb, adjective, 
adverb), all within one single semantic informa-
tion based methods. Since we use a bidirectional 
selection method, experiments showed that trans-
lation pairs that were selected during both direc-
tions, in most cases were the correct translations. 
Similarly, translation pairs selected during only 
one direction were less accurate. In other words, 
translation pairs whose target language transla-
tion was selected as a good translation for the 
source language entry; and whose source lan-
guage translation was also selected as a good 
translation for the target language entry, should 
be awarded with a higher score. In the same way, 
entries selected only during one direction should 
receive a penalty. For every translation candidate 
we select the maximum score from the several 
POS (noun, verb, adjective and adverb for syn-
onymy and antonymy relations; noun and verb 
for semantic category) based scores, multiplied 
by a multiplication factor (mfactor). The multi-
plication factor varies between 0 and 1, awarding 
the candidates that were selected both times dur-
ing the double directional selection; and punish-
ing when selection was made only in a single 
direction. The product gives the combined score 
(scoreF), c1, c2 and c3 are constants. In case of 
Japanese and Hungarian, these method scored 
best with the constants set to 1, 0.5 and 0.8, re-
spectively. The combined score also highly de-
pends on the word entry, therefore local thresh-
olds are used in this selection method as well, 
which were empirically set to maxscore?0.85 (see 
?5.1 for details). 
( ) ( )( )( )( )( )? ???
?
???
?
?+
?+
=
rel rel
rel
F tsmfactorcc
tsscorec
tsscore
,
,max
,
32
1
 (3) 
As an example, for the Japanese entry ?? 
(k?ny?: buy, purchase) there are 10 possible 
Hungarian translations; using the above methods 
5 of them (#1, #7, #8, #9, #10) are selected as 
correct ones. Among these, only 1 of them (#1) 
is a correct translation, the rest have similar or 
totally different meanings. However, with the 
combined scores the faulty translations were 
eliminated and a new, correct, but previously 
average scoring translation (#2) was selected 
(Table 1). 
 
scoreC scoreD scoreE # translation candidate scoreF N V A R N V A R N V 
1 v?tel (purchase) 2.012 0.193 0.096 0 0 0 0.500 0 0 0.154 0.500 
2 ?zlet (business transaction) 1.387 0.026 0.030 0 0 0 0.250 0 0 0.020 0.077 
3 hozam (output, yield) 1.348 0.095 0.071 0 0 0 0 0 0 0.231 0.062 
4 emel?r?d (lever, purchase) 1.200 0.052 0.079 0 0 0 0 0 0 0.111 0.067 
5 el?ny (advantage, virtue) 1.078 0.021 0.020 0 0 0 0 0 0 0.054 0.056 
6 t?masz (purchase, support) 1.053 0.014 0.015 0 0 0 0 0 0 0.037 0.031 
7 v?s?rl?s (shopping) 0.818 0.153 0.285 0 0 0 0 0 0 0.273 0.200 
8 szerzem?ny (attainment) 0.771 0.071 0.285 0 0 0 0 0 0 0.136 0.200 
9 k?nny?t?s (facilitation) 0.771 0.064 0.285 0 0 0 0 0 0 0.136 0.200 
10 emel?szerkezet (lever) 0.459 0.285 0.285 0 0 0 0 0 0 0.429 0.200 
Table 1: Translation candidate scoring for ??: buy, purchase (above thresholds in bold) 
866
161202 translation pairs were retrieved with 
this method (type F).  
During pre-evaluation type A and type B trans-
lations received a score of above 75%, while type 
C, type D and type E scored low (see ?5.2 for 
details). However, type F translations scored 
close to 80%, therefore from the six translation 
methods presented above we chose only three 
(type A, B and F) to construct the dictionary, 
while the remaining three methods (type C, D 
and E) are used only indirectly for type F selec-
tion. 
With the described selection methods 187761 
translation pairs, with 48973 Japanese and 44664 
Hungarian unique entries was generated. 
5 Threshold settings and pre-evaluation 
5.1 Local threshold settings 
As development set we considered all translation 
candidates whose Hungarian entry starts with 
?zs? (IPA: ?). We assume that the behaviour of 
this subset of words reflects the behaviour of the 
entire vocabulary. 133 unique entries totalling 
515 translation candidates comprise this devel-
opment set. After this, we manually scored the 
515 translation candidates as correct (the transla-
tion conveys the same meaning, or the meanings 
are slightly different, but in a certain context the 
translation is possible) or wrong (the translation 
pair?s two entries convey a different meaning). 
The scoring was performed by one of the authors 
who is a native Hungarian and fluent in Japanese. 
273 entries were marked as correct. Next, we 
experimented with a number of thresholds to de-
termine which ones provide with the best F-
scores (Table 2). The F-scores were determined 
as follows: for example using synonymy infor-
mation (type C) in case of threshold=0.85%, 343 
of the 515 translation pairs were above the 
threshold. Among these, 221 were marked as 
correct by our manual evaluator, thus the preci-
sion being 221/343?100=64.43 and the recall be-
ing 221/273?100=80.95. F-score is the harmonic 
mean of precision and recall (71.75 in this case). 
 
threshold value (%) selection 
type 0.75 0.80 0.85 0.90 0.95 
C 70.27 70.86 71.75 72.81 66.95 
D 69.92 70.30 70.32 70.69 66.66 
E 73.71 74.90 72.52 71.62 65.09 
F 78.78 79.07 79.34 78.50 76.94 
Table 2: Selection type F-scores with varying thresh-
olds (best threshold values in bold) 
5.2 Selection method evaluation 
As a pre-evaluation of the above selection meth-
ods, we randomly selected 200 1-to-1 source-
target entries resulted by each method. The same 
evaluator scored the translation pairs as correct 
(the translation conveys the same meaning, or the 
meanings are slightly different, but in a certain 
context the translation is possible), undecided 
(the translation pair?s semantic value is similar, 
but a translation based on them would be faulty) 
or wrong (the translation pair?s two entries con-
vey a different meaning). 
 
evaluation score (%) selection 
type correct undecided wrong 
A 75.5 6.5 18 
B 83 7 10 
C 68 5.5 26.5 
D 60 9 31 
E 71 5.5 23.5 
F 79 5 16 
Table 3: Selection type evaluation 
The results showed that type A and type B selec-
tions scored higher than all order-based selec-
tions, with type C, type D and type E selections 
failing to deliver the desired accuracy (Table 3). 
6 Evaluation 
We performed three types of evaluation: 
(1) frequency-weighted recall evaluation 
(2) 1-to-1 entry precision evaluation 
(3) 1-to-multiple entry evaluation 
For comparative purposes we also performed 
each type of evaluation for two other pivot lan-
guage based methods whose characteristics per-
mit to be implementable with virtually any lan-
guage pair. In order to do so, we constructed two 
other Hungarian-Japanese dictionaries using the 
methods proposed by Tanaka & Umemura and 
Sj?bergh, using the same source dictionaries.  
6.1 Recall evaluation 
It is well known that one of the most challenging 
aspects of dictionary generation is word ambigu-
ity. It is relatively easy to automatically generate 
the translations of low-frequency keywords, be-
cause they tend to be less ambiguous. On the 
contrary, the ambiguity of the high frequency 
words is much higher than their low-frequency 
counterparts, and as a result conventional meth-
ods fail to translate a considerable number of 
them. However, this discrepancy is not reflected 
in the traditional recall evaluation, since each 
867
word has an equal weight, regardless of its fre-
quency of use. As a result, we performed a fre-
quency weighted recall evaluation. We used a 
Japanese frequency dictionary (FD) generated 
from the Japanese EDR corpus (Isahara, 2007) to 
weight each Japanese entry. Setting the standard 
to the frequency dictionary (its recall value being 
100), we automatically search for each entry (w) 
from the frequency dictionary, looking whether 
or not it is included in the bilingual dictionary 
(WD). If it is recalled, we weight it with its fre-
quency from the frequency dictionary. 
( )
( ) 100?= ?
?
?
?
D
D
Fw
Ww
w
wfrequency
wfrequency
recall  (4) 
method recall 
our method 51.68 
Sj?bergh method 37.03 
Tanaka method 30.76 
initial candidates 51.68 
Japanese-English(*) 73.23 
Table 4: Recall evaluation results (* marks a manu-
ally created dictionary) 
The frequency weighted recall value results 
show that our method?s dictionary (51.68) out-
scores every other automatically generated 
method?s dictionary (37.03, 30.76) with a sig-
nificant advantage. Moreover, it maintains the 
score of the initial translation candidates, there-
fore managing to maximize the recall value, ow-
ing to the bidirectional selection method with 
local thresholds. However, the recall value of a 
manually created Japanese-English dictionary is 
higher than any automatically generated diction-
ary?s value (Table 4). 
6.2 1-to-1 precision evaluation 
With 1-to-1 precision evaluation we determine 
the translation accuracy of our method, com-
pared with the two baseline methods. 200 ran-
dom pairs were selected from each of the three 
Hungarian-Japanese dictionaries, scoring them 
manually the same way as with selection type 
evaluation (correct, undecided, wrong) (Table 5). 
The manual scoring was performed by one of the 
authors, who is a native Hungarian and fluent in 
Japanese. Since no independent evaluator was 
available for these two languages, after a random 
identification code being assigned to each of the 
600 selected translation pairs (200 from each 
dictionary), they were mixed. Therefore the 
evaluator did not know the origin of the transla-
tion pairs, only after manual scoring the total 
score for each dictionary was available, after re-
grouping based on the initial identification codes. 
The process was repeated 10 times, 2000 pairs 
were manually checked from each dictionary. 
 
code Japanese 
entry 
Hungarian 
entry classification 
k9g6
n5d8 
?? (h?koku: 
information, re-
port) 
h?r (report, infor-
mation, news) correct 
j8h0
k1x5 
? (ubu: innocent, 
naive) 
z?ld (green, ver-
dant) undecided  
a5b6
n8i3 
???? (entori: 
entry <a contest>) 
bej?rat (entry, 
entrance) wrong 
Table 5: 1-to-1 precision evaluation examples 
evaluation score (%) 
method 
correct undecided wrong 
our method 79.15% 6.15% 14.70% 
Sj?bergh method 54.05% 9.80% 36.15% 
Tanaka method 62.50% 7.95% 29.55% 
Table 6: 1-to-1 precision evaluation results 
To rank the methods we only consider the cor-
rect translations. Our method performed best 
with an average of 79.15%, outscoring Tanaka 
method?s 62.50% and Sj?bergh method?s 
54.05% (Table 6). The maximum deviance of the 
correct translations during the 10 repetitions was 
less than 3% from the average. 
6.3 1-to-multiple evaluation 
While with 1-to-1 precision evaluation we esti-
mated the accuracy of the translation pairs, with 
1-to-multiple we calculate the true reliability of 
the dictionary, with the initial translation candi-
dates set as recall benchmark. When looking up 
the meanings or translations of a certain head 
word, the user, whether he?s a human or a ma-
chine, expects all translations to be accurate. 
Therefore we evaluated 200 randomly selected 
Japanese entries from the initial translation can-
didates, together with all of their Hungarian 
translations, scoring them as correct (all transla-
tions are correct), acceptable (the good transla-
tions are predominant, but there are up to 2 erro-
neous translations), wrong (the number or wrong 
translations exceeds 2) or missing (the translation 
is missing) (Table 7).  
The same type of mixed, manual evaluation 
was performed by the same author on samples of 
200 entries from each Japanese-Hungarian dic-
tionary. This evaluation was also repeated 10 
times. 
To rank the methods, we only consider the 
correct translations. Our method scored best with 
868
71.45%, outperforming Sj?bergh method?s 
61.65% and Tanaka method?s 46.95% (Table 8). 
 
code Japanese 
entry 
Hungarian 
translations classification 
j4h8
m9x
5 
??  
(asshuku: 
compres-
sion, 
squeeze) 
?sszenyom?s (com-
pression, crush, 
squeeze: correct) 
?sszeszor?t?s (com-
pression, confinement: 
correct) 
zsugor?t?s (shrinkage: 
correct) 
correct 
h9j9l
3v1 
??  
(teimen: 
base) 
alap (base, bottom, 
foundation: correct) 
alapzat (base, bed, 
bottom: correct) 
l?g (alkali, base: unde-
cided) 
t?mpont (base: correct) 
acceptable 
l0k6
m3n
7 
???  
(narasu: to 
sound, to 
ring, to beat) 
beker?t (to encircle, to 
enclose, to ring: 
wrong) 
cseng (to clang, to 
clank, to ring, to tinkle: 
correct) 
hangzik (to ring, to 
sound: correct) 
horkan (to snort: 
wrong) 
?t (to bang, to knock, 
to ring: wrong) 
wrong 
Table 7: 1-to-multiple entry evaluation examples 
evaluation score (%) 
method 
correct 
accept-
able wrong missing 
our method 71.45 13.85 14.70 0 
Sj?bergh method 61.65 11.30 15.00 12.05 
Tanaka method 46.95 3.35 9.10 40.60 
Table 8: 1-to-many evaluation results 
7 Discussion 
Based on the recall evaluations, the traditional 
methods showed their major weakness by losing 
substantially from the initial recall values, scored 
by the initial translation candidates. Our method 
maintains the same value with the translation 
candidates, but we cannot say that the recall is 
perfect. When compared with a manually created 
dictionary, our method also lost significantly.  
Precision evaluation also showed an im-
provement compared with the traditional meth-
ods, our method outscoring the other two meth-
ods with the 1-to-1 precision evaluation. 1-to-
multiple evaluation was also the highest, proving 
that WordNet based methods outperform dic-
tionary based methods. Discussing the weak-
nesses of our system, we have to divide the prob-
lems into two categories: recall problems deal 
with the difficulty in connecting the target and 
source entries through the pivot language, while 
precision problems discuss the reasons why erro-
neous pairs are produced. 
7.1 Recall problems 
We managed to maximize the recall of our initial 
translation candidates, but in many cases certain 
translation pairs still could not be generated be-
cause the link from the source language to the 
target language through the pivot language sim-
ply doesn?t exist. The main reasons are: the entry 
is missing from at least one of the dictionaries; 
translations in the pivot language are expressions 
or explanations; or there is no direct translation 
or link between the source and target entries. The 
entries that could not be recalled are mostly ex-
pressions, rare entries, words specific to a lan-
guage (ex: tatami: floor-mat, or guly?s: goulash). 
Moreover, a number of head words don?t have 
any synonym, antonym and/or hy-
pernymy/hyponymy information in WordNet, 
and as a result these words could not participate 
in the type B, C, D, E and F scoring. 
7.2 Precision problems 
We identified two types of precision problems. 
The most obvious reasons for erroneous transla-
tions are the polysemous nature of words and the 
meaning-range differences across languages. 
With words whose senses are clear and mostly 
preserved even through the pivot language, most 
of the correct senses were identified and cor-
rectly translated. Nouns, adjectives and adverbs 
had a relatively high degree of accuracy. How-
ever, verbs proved to be the most difficult POS 
to handle. Because semantically they are more 
flexible than other POS categories, and the 
meaning range is also highly flexible across lan-
guages, the identification of the correct transla-
tion is increasingly difficult. For this reason, the 
number of faulty translations and the number of 
meanings that are not translated was relatively 
high. 
One other source of erroneous translations is 
the quality of the initial dictionaries. Even the 
unambiguous type A translations fail to produce 
the desired accuracy, although they are the 
unique candidate for a given word entry. The 
main reason for this is the deficiency of the ini-
tial dictionaries, which contain a great number of 
irrelevant or low usage translations, shadowing 
the main, important senses of some words. In 
other cases the resource dictionaries don?t con-
tain translations of all meanings; homonyms are 
869
present as pivot entries with different meanings, 
sometimes creating unique, but faulty links. 
8 Conclusions 
We proposed a new pivot language based 
method to create bilingual dictionaries that can 
be used as translation resource for machine trans-
lation. In contrast to conventional methods that 
use dictionaries only, our method uses WordNet 
as a main resource of the pivot language to select 
the suitable translation pairs. As a result, we 
eliminate most of the weaknesses caused by the 
structural differences of dictionaries, while prof-
iting from the semantic relations provided by 
WordNet. We believe that because of the nature 
of our method it can be re-implemented with 
most language pairs.  
In addition, owing to features such as the bidi-
rectional selection method with local thresholds 
we managed to maximize recall, while maintain-
ing a precision which is better than any other 
compared method?s score. During exemplifica-
tion, we generated a mid-large sized Japanese-
Hungarian dictionary with relatively good recall 
and promising precision. 
The dictionary is freely available online 
(http://mj-nlp.homeip.net/mjszotar), being also 
downloadable at request. 
References  
Agirre, E., Alegria, I., Rigau, G, Vossen, P. 2007. 
MCR for CLIR, Procesamiento del lenguaje natu-
ral 38, pp 3-15. 
Bond, F., Ogura, K. 2007. Combining linguistic re-
sources to create a machine-tractable Japanese-
Malay dictionary, Language Resources and 
Evaluation, 42(2), pp. 127-136. 
Breen, J.W. 1995. Building an Electric Japanese-
English Dictionary, Japanese Studies Association 
of Australia Conference, Brisbane, Queensland, 
Australia. 
Brown, P., Cocke, J., Della Pietra, S., Della Pietra, V., 
Jelinek, F., Mercer, R., Roossin, P. 1998. A Statis-
tical Approach to Language Translation, Proceed-
ings of COLING-88, pp. 71-76. 
Brown, R.D. 1997. Automated Dictionary Extraction 
for Knowledge-Free Example-Based Translation, 
Proceedings of the 7th International Conference on 
Theoretical and Methodological Issues in Machine 
Translation, pp. 111-118. 
Isahara, H., Bond, F., Uchimoto, K., Uchiyama, M., 
Kanzaki, K. 2008. Development of Japanese 
WordNet, Proceedings of LREC-2008. 
Isahara, H. 2007. EDR Electronic Dictionary ? pre-
sent status (EDR ????????), NICT-EDR 
symposium, pp. 1-14. (in Japanese) 
Kay, M., R?scheisen, M. 1993. Text-Translation 
Alignment, Computational Linguistics, 19(1), pp. 
121-142. 
Mih?ltz, M., Pr?sz?ky, G. 2004. Results and Evalua-
tion of Hungarian Nominal WordNet v1.0, Pro-
ceedings of the Second Global WordNet Confer-
ence, pp. 175-180. 
Miller G.A., Beckwith R., Fellbaum C., Gross D., 
Miller K.J. (1990). Introduction to WordNet: An 
Online Lexical Database, Int J Lexicography 3(4), 
pp. 235-244. 
Paik, K., Bond, F., Shirai, S. 2001. Using Multiple 
Pivots to align Korean and Japanese Lexical Re-
sources, NLPRS-2001, pp. 63-70, Tokyo, Japan. 
Peters, W., Vossen, P., D?ez-Orzas, P., Adriaens, G. 
1998. Cross-linguistic Alignment of Wordnets with 
an Inter-Lingual-Index, Computers and the Hu-
manities 32, pp. 221?251. 
Pr?sz?ky, G., Mih?ltz, M., Nagy, D. 2001. Toward a 
Hungarian WordNet, Proceedings of the NAACL 
2001 Workshop on WordNet and Other Lexical Re-
sources, Pittsburgh, June 2001. 
Sj?bergh, J. 2005. Creating a free Japanese-English 
lexicon, Proceedings of PACLING, pp. 296-300. 
Shirai, S., Yamamoto, K. 2001. Linking English 
words in two bilingual dictionaries to generate an-
other pair dictionary, ICCPOL-2001, pp. 174-179. 
Stamou, S., Oflazer, K., Pala, K., Christoudoulakis, 
D., Cristea, D., Tufi?, D., Koeva, S.,  Totkov, G., 
Dutoit, D., Grigoriadou, M. 1997. BalkaNet: A 
Multilingual Semantic Network for the Balkan 
Languages, In Proceedings of the International 
Wordnet Conference, Mysore, India. 
Tanaka, K., Umemura, K. 1994. Construction of a 
bilingual dictionary intermediated by a third lan-
guage, Proceedings of COLING-94, pp. 297-303. 
Vossen, P. 1998. Introduction to EuroWordNet. Com-
puters and the Humanities 32: 73-89 Special Issue 
on EuroWordNet. 
870
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 217?220,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
iChi: a bilingual dictionary generating tool 
 
 
Varga Istv?n 
Yamagata University,  
Graduate School of Science and Engineering 
dyn36150@dip.yz.yamagata-u.ac.jp 
Yokoyama Shoichi 
Yamagata University,  
Graduate School of Science and Engineering 
yokoyama@yz.yamagata-u.ac.jp 
 
  
 
Abstract 
In this paper we introduce a bilingual diction-
ary generating tool that does not use any large 
bilingual corpora. With this tool we implement 
our novel pivot based bilingual dictionary 
generation method that uses mainly the 
WordNet of the pivot language to build a new 
bilingual dictionary. We propose the usage of 
WordNet for good accuracy, introducing also a 
double directional selection method with local 
thresholds to maximize recall.  
1 Introduction 
Bilingual dictionaries are an essential, perhaps even 
indispensable tool not only as resources for ma-
chine translation, but also in every day activities or 
language education. While such dictionaries are 
available to and from numerous widely used lan-
guages, less represented language pairs have rarely 
a reliable dictionary with good coverage. The need 
for bilingual dictionaries for these less common 
language pairs is increasing, but qualified human 
resources are scarce. Considering that in these con-
ditions manual compilation is highly costly, alter-
native methods are imperative.  
Pivot language based bilingual dictionary gen-
eration is one plausible such alternative (Tanaka 
and Umemura, 1994; Sj?bergh, 2005; Shirai and 
Yamamoto, 2001; Bond and Ogura, 2007). These 
methods do not use large bilingual corpora, thus 
being suitable for low-resourced languages. 
Our paper presents iChi, the implementation 
of our own method, an easy-to-use, customizable 
tool that generates a bilingual dictionary. 
The paper is structured as follows: first we 
briefly describe the methodological background 
of our tool, after which we describe its basic 
functions, concluding with discussions. Thor-
ough description and evaluation, including com-
parative analysis, are available in Varga and Yo-
koyama (2009).  
2 Methodological background 
2.1 Pivot based dictionary generation 
Pivot language based bilingual dictionary gen-
eration methods rely on the idea that the lookup 
of a word in an uncommon language through a 
third, intermediated language can be automated. 
Bilingual dictionaries to a third, intermediate 
language are used to link the source and target 
words. The pivot language translations of the 
source and target head words are compared, the 
suitability of the source-target word pair being 
estimated based on the extent of the common 
elements. 
There are two known problems of conven-
tional pivot methods. First, a global threshold is 
used to determine correct translation pairs. How-
ever, the scores highly depend on the entry itself 
or the number of translations in the intermediate 
language, therefore there is a variance in what 
that score represents. Second, current methods 
perform a strictly lexical overlap of the source-
intermediate and target-intermediate entries. 
Even if the translations from the source and tar-
get languages are semantically transferred to the 
intermediate language, lexically it is rarely the 
case. However, due to the different word-usage 
or paraphrases, even semantically identical or 
very similar words can have different definitions 
in different dictionaries. As a result, because of 
the lexical characteristic of their overlap, current 
methods cannot identify the differences between 
totally different definitions resulted by unrelated 
concepts, and differences in only nuances re-
sulted by lexicographers describing the same 
concept, but with different words. 
2.2 Specifics of our method 
To overcome the limitations, namely low preci-
sion of previous pivot methods, we expand the 
translations in the intermediate language using 
217
information extracted from WordNet (Miller et. 
al., 1990). We use the following information: 
sense description, synonymy, antonymy and se-
mantic categories, provided by the tree structure 
of nouns and verbs. 
To improve recall, we introduce bidirectional 
selection. As we stated above, the global thresh-
old eliminates a large number of good translation 
pairs, resulting in a low recall. As a solution, we 
can group the translations that share the same 
source or target entry, and set local thresholds 
for each head word. For example, for a source 
language head word entry_source there could be 
multiple target language candidates:  en-
try_target1, ? ,entry_targetn. If the top scoring 
entry_targetk candidates are selected, we ensure 
that at least one translation will be available for 
entry_source, maintaining a high recall. Since we 
can group the entries in the source language and 
target language as well, we perform this selection 
twice, once in each direction. Local thresholds 
depend on the top scoring entry_target, being set 
to maxscore?c. Constant c varies between 0 and 1, 
allowing a small window for not maximum, but 
high scoring candidates. It is language and selec-
tion method dependent (See 3.2 for details). 
2.3 Brief method description 
First, using the source-pivot and pivot-target dic-
tionaries, we connect the source (s) and target (t) 
entries that share at least one common translation 
in the intermediate (i) language. We consider 
each such source-target pair a translation candi-
date. Next we eliminate erroneous candidates. 
We examine the translation candidates one by 
one, looking up the source-pivot and target-pivot 
dictionaries, comparing pivot language transla-
tions. There are six types of translations that we 
label A-F and explain below as follows. 
First, we select translation candidates whose 
translations into the intermediate language match 
perfectly (type A translations). 
For most words WordNet offers sense descrip-
tion in form of synonyms for most of its senses. 
For a given translation candidate (s,t) we look up 
the source-pivot and target-pivot translations 
(s?I={s?i1,?,s?in}, t?I={t?i1,?,t?im}). 
We select the elements that are common in the 
two definitions (I?=(s?I)?(t?I)) and we at-
tempt to identify their respective senses from 
WordNet (sns(I?)), comparing each synonym in 
the WordNet?s synonym description with each 
word from the pivot translations. As a result, we 
arrive at a certain set of senses from the source-
pivot definitions (sns((s?I?)) and target-pivot 
definitions (sns((t?I?)). We mark scoreB(s,t) the 
Jaccard coefficient of these two sets. Scores that 
pass a global threshold (0.1) are selected as 
translation pairs. Since synonymy information is 
available for nouns (N), verbs (V), adjectives (A) 
and adverbs (R), four separate scores are calcu-
lated for each POS (type B). 
( ) ( ) ( )( ) ( )''
''
max,
' itsnsissns
itsnsissns
tsscore
ItIsi
B
???
???
=
??? I
 (1) 
We expand the source-to-pivot and target-to-
pivot definitions with information from WordNet 
(synonymy, antonymy and semantic category). 
The similarity of the two expanded pivot lan-
guage descriptions gives a better indication on 
the suitability of the translation candidate. Since 
the same word or concept?s translations into the 
pivot language also share the same semantic 
value, the extension with synonyms 
(ext(l?i)=(l?i)?syn(l?i), where l={s,t}) the 
extended translation should share more common 
elements (type C). 
In case of antonymy, we expand the initial 
definitions with the antonyms of the antonyms 
(ext(l?i)=(l?i)?ant(ant(l?i)), where l={s,t}). 
This extension is different from the synonymy 
extension, in most cases the resulting set of 
words being considerably larger (type D). 
Synonymy and antonymy information are 
available for nouns, verbs, adjectives and ad-
verbs, thus four separate scores are calculated for 
each POS. 
Semantic categories are provided by the tree 
structure (hypernymy/hyponymy) of nouns and 
verbs of WordNet. We transpose each entry from 
the pivot translations to its semantic category 
(ext(l?i)=(l?i)?semcat(l?i), where l={s,t}). 
We assume that the correct translation pairs 
share a high percentage of semantic categories. 
Local thresholds are set based on the best 
scoring candidate for a given entry. The thresh-
olds were maxscore?0.9 for synonymy and an-
tonymy; and maxscore?0.8 for the semantic cate-
gories (see ?3.2 for details). 
( ) ( ) ( )( ) ( )itextisext
itextisext
tsscore EDC
???
???
=,
,,
 (2) 
For a given entry, the three separate candidate 
lists of type C, D and E selection methods re-
sulted in slightly different results. The good 
translations were among the top scoring ones, but 
not always scoring best. To correct this fault, a 
combined selection method is performed com-
bining these lists. For every translation candidate 
we select the maximum score (scorerel(s,t)) from 
218
the several POS (noun, verb, adjective and ad-
verb for synonymy and antonymy relations; noun 
and verb for semantic category) based scores, 
multiplied by a multiplication factor (mfactor). 
This factor varies between 0 and 1, awarding the 
candidates that were selected both times during 
the double directional selection; and punishing 
when selection was made only in a single direc-
tion. c1, c2 and c3 are adjustable language de-
pendent constants, the defaults being 1, 0.5 and 
0.8, respectively (type F). 
( ) ( )( )( )( )( )? ???
?
???
?
?+
?+
=
rel rel
rel
F tsmfactorcc
tsscorec
tsscore
,
,max
,
32
1
 (3) 
2.4 Evaluation 
We generated a Japanese-Hungarian dictionary 
using selection methods A, B and F; with C, D 
and E contributing indirectly through F. 
(a) Recall evaluation 
We used a Japanese frequency dictionary that we 
generated from the Japanese EDR corpus (Isa-
hara, 2007) to weight each Japanese entry. Set-
ting the standard to the frequency dictionary (its 
recall value being 100), we automatically search 
each entry from the frequency dictionary, verify-
ing whether or not it is included in the bilingual 
dictionary. If it is recalled, we weight it with its 
frequency from the frequency dictionary. 
Our method maintains the recall value of the 
initial translation candidates, owing to the bidi-
rectional selection method with local thresholds. 
However, the recall value of a manually created 
Japanese-English dictionary is higher than any 
automatically generated dictionary?s value (Ta-
ble 1). 
 
method recall 
our method 51.68 
initial candidates 51.68 
Japanese-English(*) 73.23 
Table 1: Recall evaluation results (* marks a manu-
ally created dictionary) 
 (b) 1-to-1 precision evaluation 
We evaluated 2000 randomly selected translation 
pairs, manually scoring them as correct (the 
translation conveys the same meaning, or the 
meanings are slightly different, but in a certain 
context the translation is possible: 79.15%), un-
decided (the translation pair?s semantic value is 
similar, but a translation based on them would be 
faulty: 6.15%) or wrong (the translation pair?s 
two entries convey a different meaning: 14.70%). 
 (c) 1-to-multiple evaluation 
With 1-to-multiple evaluation we quantify the 
true reliability of the dictionary: when looking up 
the meanings or translations of a certain key-
word, the user, whether he?s a human or a ma-
chine, expects all translations to be accurate. We 
evaluated 2000 randomly selected Japanese en-
tries from the initial translation candidates, scor-
ing all Hungarian translations as correct (all 
translations are correct: 71.45%), acceptable (the 
good translations are predominant, but there are 
up to 2 erroneous translations: 13.85%), wrong 
(the number or wrong translations exceeds 2: 
14.70%).  
3 iChi 
iChi is an implementation of our method. Pro-
grammed in Java, it is a platform-independent 
tool with a user friendly graphical interface (Im-
age 1). Besides the MySql database it consists of: 
iChi.jar (java executable), iChi.cfg (configura-
tion file), iChi.log (log file) and iChip.jar (pa-
rameter estimation tool). The major functions of 
iChi are briefly explained below. 
 
 
Image 1: User interface of iChi 
3.1 Resources 
The two bilingual dictionaries used as resources 
are text files, with a translation pair in each line: 
source entry 1@pivot entry 1 
source entry 2@pivot entry 2 
The location of the pivot language?s WordNet 
also needs to be specified. All paths are stored in 
the configuration file. 
3.2 Parameter settings 
iChip.jar estimates language dependent parame-
ters needed for the selection methods. Its single 
argument is a text file that contains marked (cor-
rect: $+ or incorrect: $-) translation pairs: 
219
$+source entry 1@correct target entry 1 
$-source entry 2@incorrect target entry 2 
The parameter estimation tool experiments 
with various threshold settings on the same (cor-
rect or incorrect) source entries. For example, 
with Hungarian-Japanese we considered all 
translation candidates whose Hungarian entry 
starts with ?zs? (IPA: ?). 133 head words total-
ling 515 translation candidates comprise this set, 
273 entries being marked as correct. iChip ex-
perimented with a number of thresholds to de-
termine which ones provide with the best F-
scores, e.g. retain most marked correct transla-
tions (Table 2). The F-scores were determined as 
follows: for example using synonymy informa-
tion (type C) in case of threshold=0.85%, 343 of 
the 515 translation pairs were above the thresh-
old. Among these, 221 were marked as correct, 
thus the precision being 221/343?100=64.43 and 
the recall being 221/273?100=80.95. F-score is 
the harmonic mean of precision and recall (71.75 
in this case). 
 
threshold value (%) selection 
type 0.75 0.80 0.85 0.90 0.95 
C 70.27 70.86 71.75 72.81 66.95 
D 69.92 70.30 70.32 70.69 66.66 
E 73.71 74.90 72.52 71.62 65.09 
F 78.78 79.07 79.34 78.50 76.94 
Table 2: Selection type F-scores with varying thresh-
olds (best scores in bold) 
The output is saved into the configuration file. 
If no parameter estimation data is available, the 
parameters estimated using Hungarian-Japanese 
are used as default. 
3.3 Save settings 
The generated source-target dictionary is saved 
into a text file that uses the same format de-
scribed in ?3.1. The output can be customized by 
choosing the desired selection methods. The de-
fault value is a dictionary with selection types A, 
B and F; selection types C, D and E are used 
only indirectly with type F. 
3.4 Tasks 
The tasks are run sequentially, every step being 
saved in the internal database, along with being 
logged into the log file. 
4 Discussion 
If heavily unbalanced resources dictionaries are 
used, due to the bidirectional selection method 
many erroneous entries will be generated. If one 
polysemous pivot entry has multiple translations 
into the source, but only some of them are trans-
lated into the target languages, unique, but incor-
rect source-target pairs will be generated. For 
example, with an English pivoted dictionary that 
has multiple translation of ?bank? onto the source 
(?financial institution?, ?river bank?), but only 
one into the target language (?river bank?), the 
incorrect source(?financial institution?)-
target(?river bank?) pair will be generated, since 
target(?river bank?) has no other alternative. 
Thorough discussion on recall and precision 
problems concerning the methodology of iChi, 
are available in Varga and Yokoyama (2009). 
5 Conclusions 
In this paper we presented iChi, a user friendly 
tool that uses two dictionaries into a third, inter-
mediate language together with the WordNet of 
that third language to generate a new dictionary. 
We briefly described the methodology, together 
with the basic functions. The tool is freely avail-
able online (http://mj-nlp.homeip.net/ichi). 
References  
Bond, F., Ogura, K. 2007. Combining linguistic re-
sources to create a machine-tractable Japanese-
Malay dictionary, Language Resources and 
Evaluation, 42(2), pp. 127-136. 
Breen, J.W. 1995. Building an Electric Japanese-
English Dictionary, Japanese Studies Association 
of Australia Conference, Brisbane, Queensland, 
Australia. 
Isahara, H. (2007). EDR Electronic Dictionary ? pre-
sent status (EDR ????????), NICT-EDR 
symposium, pp. 1-14. (in Japanese) 
Miller G.A., Beckwith R., Fellbaum C., Gross D., 
Miller K.J. (1990). Introduction to WordNet: An 
Online Lexical Database, Int J Lexicography 3(4), 
pp. 235-244. 
Sj?bergh, J. 2005. Creating a free Japanese-English 
lexicon, Proceedings of PACLING, pp. 296-300. 
Shirai, S., Yamamoto, K. 2001. Linking English 
words in two bilingual dictionaries to generate an-
other pair dictionary, ICCPOL-2001, pp. 174-179. 
Tanaka, K., Umemura, K. 1994. Construction of a 
bilingual dictionary intermediated by a third lan-
guage, Proceedings of COLING-94, pp. 297-303. 
Varga, I., Yokoyama, S. 2009. Bilingual dictionary 
generation for low-resourced language pairs, Pro-
ceedings of EMNLP 2009. 
220
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 825?835,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Relation Acquisition using Word Classes and Partial Patterns
Stijn De Saeger?? Kentaro Torisawa? Masaaki Tsuchida? Jun?ichi Kazama?
Chikara Hashimoto? Ichiro Yamada? Jong Hoon Oh? Istva?n Varga? Yulan Yan?
? Information Analysis Laboratory, National Institute of
Information and Communications Technology, 619-0289 Kyoto, Japan
{stijn,torisawa,kazama,ch,rovellia,istvan,yulan}@nict.go.jp
? Information and Media Processing Laboratories, NEC Corporation, 630-0101 Nara, Japan
m-tsuchida@cq.jp.nec.com
? Human & Information Science Research Division,
NHK Science & Technology Research Laboratories, 157-8510 Tokyo, Japan
yamada.i-hy@nhk.or.jp
Abstract
This paper proposes a semi-supervised rela-
tion acquisition method that does not rely on
extraction patterns (e.g. ?X causes Y? for
causal relations) but instead learns a combi-
nation of indirect evidence for the target re-
lation ? semantic word classes and partial
patterns. This method can extract long tail
instances of semantic relations like causality
from rare and complex expressions in a large
JapaneseWeb corpus? in extreme cases, pat-
terns that occur only once in the entire cor-
pus. Such patterns are beyond the reach of cur-
rent pattern based methods. We show that our
method performs on par with state-of-the-art
pattern based methods, and maintains a rea-
sonable level of accuracy even for instances
acquired from infrequent patterns. This abil-
ity to acquire long tail instances is crucial for
risk management and innovation, where an ex-
haustive database of high-level semantic rela-
tions like causation is of vital importance.
1 Introduction
Pattern based relation acquisition methods rely on
lexico-syntactic patterns (Hearst, 1992) for extract-
ing relation instances. These are templates of natu-
ral language expressions such as ?X causes Y ? that
signal an instance of some semantic relation (i.e.,
causality). Pattern based methods (Agichtein and
Gravano, 2000; Pantel and Pennacchiotti, 2006b;
Pas?ca et al, 2006; De Saeger et al, 2009) learn many
? This work was done when all authors were at the National
Institute of Information and Communications Technology.
such patterns to extract new instances (word pairs)
from the corpus.
However, since extraction patterns are learned us-
ing statistical methods that require a certain fre-
quency of observations, pattern based methods fail
to capture relations from complex expressions in
which the pattern connecting the two words is rarely
observed. Consider the following sentence:
?Curing hypertension alleviates the deteriora-
tion speed of the renal function, thereby lower-
ing the risk of causing intracranial bleeding?
Humans can infer that this sentence expresses a
causal relation between the underlined noun phrases.
But the actual pattern connecting them, i.e., ?Cur-
ing X alleviates the deterioration speed of the re-
nal function, thereby lowering the risk of causing
Y ?, is rarely observed more than once even in a 108
page Web corpus. In the sense that the term pat-
tern implies a recurring event, this expression con-
tains no pattern for detecting the causal relation be-
tween hypertension and intracranial bleeding. This
is what we mean by ?long tail instances? ? words
that co-occur infrequently, and only in sparse extrac-
tion contexts.
Yet an important application of relation extraction
is mining the Web for so-called unknown unknowns
? in the words of D. Rumsfeld, ?things we don?t
know we don?t know? (Torisawa et al, 2010). In
knowledge discovery applications like risk manage-
ment and innovation, the usefulness of relation ex-
traction lies in its ability to find many unexpected
remedies for diseases, causes of social problems,
and so on. To give an example, our relation extrac-
825
tion system found a blog post mentioning Japanese
automaker Toyota as a hidden cause of Japan?s de-
flation. Several months later the same connection
was made in an article published in an authoritative
economic magazine.
We propose a semi-supervised relation extraction
method that does not rely on direct pattern evidence
connecting the two words in a sentence. We argue
that the role of binary patterns can be replaced by a
combination of two types of indirect evidence: se-
mantic class information about the target relation
and partial patterns, which are fragments or sub-
patterns of binary patterns. The intuition is this: if
a sentence like the example sentence above contains
some wordX belonging to the class of medical con-
ditions and another word Y from the class of trau-
mas, and X matches the partial pattern ?. . . causing
X?, there is a decent chance that this sentence ex-
presses a causal relation between X and Y . We
show that just using this combination of indirect
evidence we can pick up semantic relations with
roughly 50% precision, regardless of the complexity
or frequency of the expression in which the words
co-occur. Furthermore, by combining this idea with
a straightforward machine learning approach, the
overall performance of our method is on par with
state-of-the-art pattern based methods. However,
our method manages to extract a large number of
instances from sentences that contain no pattern that
can be learned by pattern induction methods.
Our method is a two-stage system. Figure 1
presents an overview. In Stage 1 we apply a state-
of-the-art pattern based relation extractor to a Web
corpus to obtain an initial batch of relation instances.
In Stage 2 a supervised classifier is built from vari-
ous components obtained from the output of Stage
1. Given the output of Stage 1 and access to a
Web corpus, the Stage 2 extractor is completely
self-sufficient, and the whole method requires no
supervision other than a handful of seed patterns
to start the first stage extractor. The whole proce-
dure is therefore minimally supervised. Semantic
word classes and partial patterns play a crucial role
throughout all steps of the process.
We evaluate our method on three relation acqui-
sition tasks (causation, prevention and material re-
lations) using a 600 million Japanese Web page cor-
Figure 1: Proposed method: data flow.
pus (Shinzato et al, 2008) and show that our sys-
tem can successfully acquire relations from both
frequent and infrequent patterns. Our system ex-
tracted 100,000 causal relations with 84.6% preci-
sion, 50,000 prevention relations with 58.4% preci-
sion and 25,000 material relations with 76.1% preci-
sion. In the extreme case, we acquired several thou-
sand word pairs co-occurring only in patterns that
appear once in the entire corpus. We call such pat-
terns single occurrence (SO) patterns. Word pairs
that co-occur only with SO patterns represent the
theoretical limiting case of relations that cannot be
acquired using existing pattern based methods. In
this sense our method can be seen as complemen-
tary with pattern based approaches, and merging our
method?s output with that of a pattern based method
may be beneficial.
2 Stage 1 Extractor
This section introduces our Stage 1 extractor: the
pattern based method from (De Saeger et al, 2009),
which we call CDP for ?class dependent patterns?.
We give a brief overview below, and refer the reader
to the original paper for a more comprehensive ex-
planation.
CDP takes a set of seed patterns as input, and au-
tomatically learns new class dependent patterns as
paraphrases of the seed patterns. Class dependent
patterns are semantic class restricted versions of or-
dinary lexico-syntactic patterns. Existing methods
use class independent patterns such as ?X causes
Y ? to learn causal relations betweenX and Y . Class
dependent patterns however place semantic class re-
826
strictions on the noun pairs they may extract, like
?Yaccidents causes Xincidents?. The accidents and
incidents subscripts specify the semantic class of the
X and Y slot fillers.
These class restrictions make it possible to distin-
guish between multiple senses of highly ambiguous
patterns (so-called ?generic? patterns). For instance,
given the generic pattern ?Y by X?, if we restrict
Y and X in to the semantic classes of injuries and
accidents (as in ?death by drowning?), the class de-
pendent pattern ?Yinjuries by Xaccidents? becomes a
valid paraphrase of ?X causes Y ? and can safely be
used to extract causal relations, whereas other class
dependent versions of the same generic pattern (e.g.,
?Yproducts byXcompanies?, as in ?iPhone by Apple?)
may not.
CDP ranks each noun pair in the corpus accord-
ing to a score that reflects its likelihood of being
a proper instance of the target relation, by calcu-
lating the semantic similarity of a set of seed pat-
terns to the class dependent patterns this noun pair
co-occurs with. The output of CDP is a list of noun
pairs ranked by score, together with the highest scor-
ing class dependent pattern each noun pair co-occurs
with. This list becomes the input to Stage 2 of our
method, as shown in Figure 1. We adopted CDP as
Stage 1 extractor because, besides having generally
good performance, the class dependent patterns pro-
vide the two fundamental ingredients for Stage 2 of
our method ? the target semantic word classes for a
given relation (in the form of the semantic class re-
strictions attached to patterns), and partial patterns.
To obtain fine-grained semantic word classes we
used the large scale word clustering algorithm from
(Kazama and Torisawa, 2008), which uses the EM
algorithm to compute the probability that a word w
belongs to class c, i.e., P (c|w). Probabilistic cluster-
ing defines no discrete boundary between members
and non-members of a semantic class, so we simply
assume w belongs to c whenever P (c|w) ? 0.2. For
this work we clustered 106 nouns into 500 classes.
Finally, we adopt the structural representation of
patterns introduced in (Lin and Pantel, 2001). All
sentences in our corpus are dependency parsed, and
patterns consist of words on the path of dependency
relations connecting two nouns.
3 Stage 2 Extractor
We use CDP as our Stage 1 extractor, and the top
N noun pairs along with the class dependent pat-
terns that extract them are given as input to Stage 2,
which represents the main contribution of this work.
As shown in Figure 1, Stage 2 consists of three mod-
ules: a candidate generator, a training data gener-
ator and a supervised classifier. The training data
generator builds training data for the classifier from
the top N output of CDP and sentences retrieved
from the Web corpus. This classifier then scores and
ranks the candidate relations generated by the can-
didate relation generator. We introduce each module
below.
Candidate Generator This module generates
sentences containing candidate word pairs for the
target relation from the corpus. It does so using the
semantic class restrictions and partial patterns ob-
tained from the output of CDP. The set of all seman-
tic class pairs obtained from the class dependent pat-
terns that extracted the topN results become the tar-
get semantic class pairs from which new candidate
instances are generated. We extract all sentences
containing a word pair belonging to one of the target
class pairs from the corpus.
From these sentences we keep only those that con-
tain a trace of evidence for the target semantic re-
lation. For this we decompose the class dependent
patterns from the Stage 1 extractor into partial pat-
terns. As mentioned previously, patterns consist of
words on the path of dependency relations connect-
ing the two target words in a syntactic tree. To obtain
partial patterns we split this dependency path into its
two constituent branches, each one leading from the
leaf word (i.e. variable) to the syntactic head of the
pattern. For example, ?X subj?? causes obj?? Y ? is
split into two partial patterns ?X subj?? causes? and
?causes obj?? Y ?. These partial patterns capture the
predicate structures in binary patterns.1 We discard
partial patterns with syntactic heads other than verbs
or adjectives.
The candidate genarator retrieves all sentences
from the corpus in which two nouns belonging to
one of the target semantic classes co-occur and
1 In Japanese, case information is encoded in post-positions
attached to the noun.
827
where at least one of the nouns matches a partial pat-
tern. As shown in Figure 1, these sentences and the
candidate noun pairs they contain (called (noun pair,
sentence) triples hereafter) are submitted to the clas-
sifier for scoring. Restricting candidate noun pairs
by this combination of semantic word classes and
partial pattern matching proved to be quite powerful.
For instance, in the case of causal relations we found
that close to 60% of the (noun pair, sentence) triples
produced by the candidate generator were correct
(Figure 6).
Training Data Generator As shown in Figure 1,
the (noun pair, sentence) triples used as training data
for the SVM classifier were generated from the top
results of the Stage 1 extractor and the corpus. We
consider the noun pairs in the top N output of the
Stage 1 extractor as true instances of the target re-
lation (even though they may contain erroneous ex-
tractions), and retrieve from the corpus all sentences
in which these noun pairs co-occur and that match
one of the partial patterns mentioned above. In our
experiments we set N to 25, 000. We randomly se-
lect positive training samples from this set of (noun
pair, sentence) triples.
Negative training samples are also selected ran-
domly, as follows. If one member of the target noun
pair in the positive samples above matches a partial
pattern but the other does not, we randomly replace
the latter by another noun found in the same sen-
tence, and generate this new (noun pair, sentence)
triple as a negative training sample. In the causal
relation experiments this approach had about 5%
chance of generating false negatives ? noun pairs
contained in the top N results of the Stage 1 extrac-
tor. Such samples were discarded. Our experimen-
tal results show that this scheme works quite well in
practice. We randomly sample M positive and neg-
ative samples from the autogenerated training data
to train the SVM. M was empirically set to 50,000
in our experiments.
SVM Classifier We used a straightforward fea-
ture set for training the SVM classifier. Because
our classifier will be faced with sentences contain-
ing long and infrequent patterns where the distance
between the two target nouns may be quite large,
we did not try to represent lexico-syntactic patterns
as features but deliberately restricted the feature set
to local context features of the candidate noun pair
in the target sentence. Concretely, we looked at bi-
grams and unigrams surrounding both nouns of the
candidate relation, as the local context around the
target words may contain many telling expressions
like ?increase in X? or ?X deficiency? which are use-
ful clues for causal relations. Also, in Japanese case
information is encoded in post-positions attached to
the noun, which is captured by the unigram features.
In addition to these base features, we include the
semantic classes to which the candidate noun pair
belongs, the partial patterns they match in this sen-
tence, and the infix words inbetween the target noun
pair. Note that this feature set is not intended to
be optimal beyond the actual claims of this paper,
and we have deliberately avoided exhaustive fea-
ture engineering so as not to obscure the contribu-
tion of semantic classes and partial pattern to our
approach. Clearly an optimal classifier will incorpo-
rate many more advanced features (see GuoDong et
al. (2005) for a comprehensive overview), but even
without sophisticated feature engineering our clas-
sifier achieved sufficient performance levels to sup-
port our claims. An overview of the feature set is
given in Table 1. The relative contribution of each
type of features is discussed in section 4. In prelim-
inary experiments we found a polynomial kernel of
degree 3 gave the best results, which suggests the ef-
fectiveness of combining different types of indirect
evidence.
The SVM classifier outputs (noun pair, sentence)
triples, ranked by SVM score. To obtain the final
output of our method we assign each unique noun
pair the maximum score from all (noun pair, sen-
tence) triples it occurs in, and discard all other sen-
tences for this noun pair. In section 4 below we eval-
uate the acquired noun pairs in the context of the
sentence that maximizes their score.
4 Evaluation
We demonstrate the effectiveness of semantic word
classes and partial pattern matching for relation ex-
traction by showing that the method proposed in this
paper performs at the level of other state-of-the-art
relation acquisition methods. In addition we demon-
strate that our method can successfully extract re-
lation instances from infrequent patterns, and we
828
Feature type Description Number of features
Morpheme features Unigram and bigram morphemes surrounding both target words. 554,395
POS features Coarse- and fine-grained POS tags of the noun pair and morpheme features. 2,411
Semantic features Semantic word classes of the target noun pair. 1000 (500 classes ?2)
Infix word features Morphemes found inbetween the target noun pair. 94,448
Partial patterns Partial patterns matching the target noun pair. 86
Table 1: Feature set used in the Stage 2 classifier, and their number for the causal relation experiments.
explore several criteria for what constitutes an in-
frequent pattern ? including the theoretical limit-
ing case of patterns observed only once in the en-
tire corpus. These instances are impossible to ac-
quire by pattern based methods. The ability to ac-
quire relations from extremely infrequent expres-
sions with decent accuracy demonstrates the utility
of combining semantic word classes with partial pat-
tern matching.
4.1 Experimental Setting
We evaluate our method on three semantic relation
acquisition tasks: causality, prevention and mate-
rial. Two concepts stand in a causal relation when
the source concept (the ?cause?) is directly or indi-
rectly responsible for the subsequent occurrence of
the target concept (its ?effect?). In a prevention rela-
tion the source concept directly or indirectly acts to
avoid the occurrence of the target concept, and in a
material relation the source concept is a material or
ingredient of the target concept.
For our experiments we used the latest version
of the TSUBAKI corpus (Shinzato et al, 2008),
a collection of 600 million Japanese Web pages
dependency parsed by the Japanese dependency
parser KNP2. In our implementation of CDP, lexico-
syntactic patterns consist of words on the path con-
necting two nouns in a dependency parse tree. We
discard patterns from dependency paths longer than
8 constituent nodes. Furthermore, we estimated pat-
tern frequencies in a subset of the corpus (50 million
pages, or 1/12th of the entire corpus) and discarded
patterns that co-occur with less than 10 unique noun
pairs in this smaller corpus. These restrictions do
not apply to the proposed method, which can extract
noun pairs connected by patterns of arbitrary length,
even if found only once in the corpus. For our pur-
2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
pose we treat dependency paths whose observed fre-
quency is below this threshold as insufficiently fre-
quent to be considered as ?patterns?. This threshold
is of course arbitrary, but in section 4 we show that
our results are not affected by these implementation
details.
We asked three human judges to evaluate ran-
dom (noun pair, sentence) triples, i.e. candidate
noun pairs in the context of some corpus sentence
in which they co-occur. If the judges find the sen-
tence contains sufficient evidence that the target re-
lation holds between the candidate nouns, they mark
the noun pair correct. To evaluate the performance
of each method we use two evaluation criteria: strict
(all judges must agree the candidate relation is cor-
rect) and lenient (decided by the judges? majority
vote). Over all experiments the interrater agreement
(Kappa) ranged between 0.57 and 0.82 with an aver-
age of 0.72, indicating substantial agreement (Lan-
dis and Koch, 1977).
4.1.1 Methods Compared
We compare our results to two pattern based
methods: CDP (the Stage 1 extractor) and Espresso
(Pantel and Pennacchiotti, 2006a).
Espresso is a popular bootstrapping based method
that uses a set of seed instances to induce extraction
patterns for the target relation and then acquire new
instances in an iterative bootstrapping process. In
each iteration Espresso performs pattern induction,
pattern ranking and selection using previously ac-
quired instances, and uses the newly acquired pat-
terns to extraction new instances. Espresso com-
putes a reliability score for both instances and pat-
terns based on their pointwise mutual information
(PMI) with the top-scoring patterns and instances
from the previous iteration.3 We refer to (Pantel and
3 In our implementation of Espresso we found that, despite
the many parameters for controlling the bootstrapping process,
829
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 2: Precision of acquired relations (causality). L
and S denote lenient and strict evaluation.
Pennacchiotti, 2006a) for a more detailed descrip-
tion.
For all methods compared we rank the acquired
noun pairs by their score and evaluated 500 random
samples from the top 100,000 results. For noun pairs
acquired by CDP and Espresso we select the pattern
that extracted this noun pair (in the case of Espresso,
the pattern with the highest PMI for this noun pair),
and randomly select a sentence in which the noun
pair co-occurs with that pattern from our corpus. For
the proposed method we evaluate noun pairs in the
context of the (noun pair, sentence) triple with the
highest SVM score.
4.2 Results and Discussion
The performance of each method on the causality,
prevention and material relations are shown in Fig-
ures 2, 3 and 4 respectively. In the causality exper-
iments (Figure 2) the proposed method performs on
par with CDP for the top 25,000 results, both achiev-
ing close to 90% precision. But whereas CDP?s per-
it remains very difficult to prevent semantic drift (Komachi et
al., 2008) from occurring. One small adjustment to the al-
gorithm stabilized the bootstrapping process considerably and
gave overall better results. In the pattern induction step (sec-
tion 3.2 in (Pantel and Pennacchiotti, 2006a)), Espresso com-
putes a reliability score for each candidate pattern based on the
weighted PMI of the pattern with all instances extracted so far.
As the number of extracted instances increases this dispropor-
tionally favours high frequency (i.e. generic) patterns, so in-
stead of using all instances for computing pattern reliability we
only use the m most reliable instances from the previous iter-
ation, which were used to extract the candidate patterns of the
current iteration (m = 200, like the original).
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 3: Precision of acquired relations (prevention). L
and S denote lenient and strict evaluation.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 4: Precision of acquired relations (material). L
and S denote lenient and strict evaluation.
formance drops from there our method maintains
the same high precision throughout (84.6%, lenient).
Both our method and CDP outperform Espresso by
a large margin.
For the prevention relation (Figure 3), precision
is considerably lower for all methods except the top
10,000 of CDP (82% precision, lenient). The pro-
posed method peaks at around 20,000 results (67%
precision, lenient) and performance remains more or
less constant from there on. The proposed method
overtakes CDP?s performance around the top 45,000
mark, which suggests that combining the results of
both methods may be beneficial.
In the material relations the proposed method
slightly outperforms both pattern based methods
in the top 10,000 results (92% precision, lenient).
830
However for this relation our method produced only
35,409 instances. The reason is that the top 25,000
results of CDP were all extracted by just 12 patterns,
and these contained many patterns whose syntactic
head is not a verb or adjective (like ?Y rich in X? or
?Y containing X?). Only 12 partial patterns were ob-
tained, which greatly reduced the output of the pro-
posed method. Taking into account the high perfor-
mance of CDP for material relations, this suggests
that for some relations our method?s N and M pa-
rameters could use some tuning. In conclusion, in
all three relations our method performs at a level
comparable to state-of-the-art pattern based meth-
ods, which is remarkable given that it only uses in-
direct evidence.
Dealing with Difficult Extractions How does our
method handle noun pairs that are difficult to ac-
quire by pattern based methods? The graphs marked
?Prop. w/o CDP? (Proposed without CDP) in Fig-
ures 2 , 3 and 4 show the number and precision of
evaluated samples from the proposed method that do
not co-occur in our corpus with any of the patterns
that extracted the top N results of the first stage ex-
tractor. These graphs show that our method is not
simply regenerating CDP?s top results but actually
extracts many noun pairs that do not co-occur in pat-
terns that are easily learned. Figure 2 shows that
roughly two thirds of the evaluated samples are in
this category, and that their performance is not sig-
nificantly worse than the overall result. The same
conclusion holds for the prevention results (Figure
3), where over 80% of the proposed method?s sam-
ples are noun pairs that do not co-occur with eas-
ily learnable patterns. Their precision is about 5%
worse than all samples from the proposed method.
For material relations (Figure 4) about half of all
evaluated samples are in this category, but their pre-
cision is markedly worse compared to all results.
For genuinely infrequent patterns, the graphs
marked ?Prop. w/o pattern? (Proposed without pat-
tern) in Figures 2 , 3 and 4 show the number and
precision of noun pairs evaluated for the proposed
method that were acquired from sentences without
any discernible pattern. As explained in section 4
above, these constitute noun pairs co-occurring in a
sentence in which the path of dependency relations
connecting them is either longer than 8 nodes or can
 0
 5
 10
 15
 20
1 2 32 1024 32768 1.05x106 3.36x107
% o
f al
l sa
mp
les
# of noun pairs co-occurring with patterns
Pattern frequency, CDPPattern frequency, ProposedPattern frequency, Espresso
Figure 5: Frequencies of patterns in the evaluation data
(causation).
extract fewer than 10 noun pairs in 50 million Web
pages. Note that in theory it is possible that these
noun pairs could not be acquired by pattern based
methods due to this threshold ? patterns must be
able to extract more than 10 different noun pairs in
a subset of our corpus, while the proposed method
does not have this constraint. So at least in the-
ory, pattern based methods might be able to acquire
all noun pairs obtained by our method by lowering
this threshold. To see that this is unlikely to be the
case, consider Figure 5, which shows the pattern fre-
quency of the patterns induced by CDP and Espresso
for the causality experiment. The x-axis represents
pattern frequency in terms of the number of unique
noun pairs a pattern co-occurs with in our corpus (on
a log scale), and the y-axis shows the percentage of
samples that was extracted by patterns of a given fre-
quency.4 Figure 5 shows that for the pattern based
methods, the large majority of noun pairs was ex-
tracted by patterns that co-occur with several thou-
sand different noun pairs. Extrapolating the original
frequency threshold of 10 nounpairs to the size of
our entire corpus roughly corresponds to about 120
distinct noun pairs (10 times in 1/12th of the entire
corpus). In Figure 5, the histograms for the pattern
based methods CDP and Espresso start around 1000
noun pairs, which is far above this new lowerbound.
4 In the case of CDP we ignore semantic class restrictions
on the patterns when comparing frequencies. For Espresso, the
most frequent pattern (?Y by X? at the 24,889,329 data point
on the x-axis) extracted up to 53.8% of the results, but the graph
was cut at 20% for readability.
831
Cau
sali
ty
??????? ??????????????????????????????????????????[????]??????
Because ?catecholamine? causes a rapid increase of heart rate, the change of circulation inside the blood vessels leads to blood vessel
disorders and promotes [thrombus generation].
????? ??????????????????????????????????? [????]?????????????
When we injected Xylocaine during a ?tachycardia seizure?, the patient suddenly lost consciousness and fell into a fit of [convulsions].
???????? ????????? ????????? [???]???????????????
(. . . ) The reason is that by taking a lot of ?animal proteins? the causative agents of [tragomaschalia] increase.
*???????????? ?????? ?????????????????????? [???]?
* [Radon] heightens the (body?s) antioxidative function and is effective for eliminating activated oxygen, which is a cause of aging and
?lifestyle-related? diseases.
Pre
ven
tion
???????????????????? ???? ??????????????[???]?????????????
Because the fatty meat of tuna contains DHA and ?EPA? in abundance, it is effective for preventing [neuralgia].
??????? ????? ??????? [????]?????????
If you use ?nitrogen gas? instead of air you may prevent [dust explosions].
??????????? ??????? ???????????????????????? [???]???????????
In ancient Europe ?orthosiphon aristatus? tea was called a ?diet tea?, and supposedly it helps preventing triglycerides and [adult diseases].
* ?? ???????????????????????? [????]????????
* (It) is something that prevents [scratches] on the screen if the ?calash? gets stuck between the screens during storage.
Table 2: Causality and Prevention relations acquired from Single Occurrence (SO) patterns. ?X? and [Y] indicate the
relation instance?s source and target words, and ?*? indicates erroneous extractions.
Thus, pattern based methods naturally tend to induce
patterns that are much more frequent than the range
of patterns our method can capture, and it is unlikely
that this is a result of implementation details like pat-
tern frequency threshold.
The precision of noun pairs in the category ?Prop.
w/o pattern? is clearly lower than the overall re-
sults, but the graphs demonstrate that our method
still handles these difficult cases reasonably well.
The 500 samples evaluated contained 155 such in-
stances for causality, 403 for prevention and 276 for
material. For prevention, the high ratio of these noun
pairs helps explain why the overall performance was
lower than for the other relations.
Finally, the theoretical limiting case for pattern
based algorithms consists of patterns that only co-
occur with a single noun pair in the entire corpus
(single occurrence or SO patterns). Pattern based
methods learn new patterns that share many noun
pairs with a set of reliable patterns in order to extract
new relation instances. If a noun pair that co-occurs
with a SO pattern also co-occurs with more reliable
patterns there is no need to learn the SO pattern. If
that same noun pair does not co-occur with any other
reliable pattern, the SO pattern is beyond the reach
of any pattern induction method. Thus, SO patterns
are effectively useless for pattern based methods.
For the 500 samples evaluated from the causality
and prevention relations acquired by our method we
found 7 causal noun pairs that co-occur only in SO
patterns and 29 such noun pairs for prevention. The
precision of these instances was 42.9% and 51.7%
respectively. In total we found 8,716 causal noun
pairs and 7,369 prevention noun pairs that co-occur
only with SO patterns. Table 2 shows some example
relations from our causality and prevention experi-
ments that were extracted from SO patterns. To con-
clude, our method is able to acquire correct relations
even from the most extreme infrequent expressions.
Semantic Classes, Partial Patterns or Both? In
the remainder of this section we look at how the
combination of semantic word classes and partial
patterns benefits our method. For each relation we
evaluated 1000 random (noun pair, sentence) triples
satisfying the two conditions from section 3 ?
matching semantic class pairs and partial patterns.
Surprisingly, the precision of these samples was
59% for causality, 40% for prevention and 50.4%
for material, showing just how compelling these two
types of indirect evidence are in combination.
To estimate the relative contribution of each
heuristic we compared our candidate generation
method against two baselines. The first baseline
evaluates the precision of random noun pairs from
832
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 6: Contribution of feature sets (causality).
 30
 40
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 7: Contribution of feature sets (prevention).
the target semantic classes that co-occur in a sen-
tence. The second baseline does the same for the
second heuristic, selecting random sentences con-
taining a noun pair that matches some partial pat-
tern. Evaluating 100 samples for causality and pre-
vention, we found the precision of the semantic class
baseline was 16% for causality and 5% for preven-
tion. The pattern fragment baseline gave 9% for
causality and 22% for prevention. This is consid-
erably lower than the precision of random samples
that satisfy both the semantic class and partial pat-
tern conditions, showing that the combination of se-
mantic classes and partial patterns is more effective
than either one individually.
Finally, we investigated the effect of the various
feature sets used in the classifier. Figures 6, 7 and
8 show the results for the respective semantic re-
lations. The ?Base features? graph shows the per-
 30
 40
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 8: Contribution of feature sets (material).
formance the unigram, bigram and part-of-speech
features. ?All features? uses all features in Table
1. The other graphs show the effect of removing
one type of features. These graphs suggest that the
contribution of the individual feature types (seman-
tic class information, partial patterns or infix words)
to the classification performance is relatively minor,
but in combination they do give a marked improve-
ment over the base features, at least for some rela-
tions like causation and material. In other words,
the main contribution of semantic word classes and
partial patterns to our method?s performance lies not
in the final classification step but seems to occur at
earlier stages of the process, in the candidate and
training data generation steps.
5 Related Work
Using lexico-syntactic patterns to extract semantic
relations was first explored by Hearst (Hearst, 1992),
and has inspired a large body of work on semi-
supervised relation acquisition methods (Berland
and Charniak, 1999; Agichtein and Gravano, 2000;
Etzioni et al, 2004; Pantel and Pennacchiotti,
2006b; Pas?ca et al, 2006; De Saeger et al, 2009),
two of which were used in this work.
Some researchers have addressed the sparse-
ness problems inherent in pattern based methods.
Downey et al (2007) starts from the output of
the unsupervised information extraction system Tex-
tRunner (Banko and Etzioni, 2008), and uses lan-
guage modeling techniques to estimate the reliabil-
ity of sparse extractions. Pas?ca et al (2006) alle-
833
viates pattern sparseness by using infix patterns that
are generalized using classes of distributionally sim-
ilar words. In addition, their method employs clus-
tering based semantic similarities to filter newly ex-
tracted instances in each iteration of the bootstrap-
ping process. A comparison with our method would
have been instructive, but we were unable to imple-
ment their method because the original paper con-
tains insufficient detail to allow replication.
There is a large body of research in the super-
vised tradition that does not use explicit pattern rep-
resentations ? kernel based methods (Zelenko et
al., 2003; Culotta, 2004; Bunescu and Mooney,
2005) and CRF based methods (Culotta et al, 2006).
These approaches are all fully supervised, whereas
in our work the automatic generation of candi-
dates and training data is an integral part of the
method. An interesting alternative is distant super-
vision (Mintz et al, 2009), which trains a classi-
fier using an existing database (Freebase) containing
thousands of semantic relations, with millions of in-
stances. We believe our method is more general, as
depending on external resources like a database of
semantic relations limits both the range of seman-
tic relations (i.e., Freebase contains only relations
between named entities, and none of the relations
in this work) and languages (i.e., no resource com-
parable to Freebase exists for Japanese) to which
the technology can be applied. Furthermore, it is
unclear whether distant supervision can deal with
noisy input such as automatically acquired relation
instances.
Finally, inference based methods (Carlson et al,
2010; Schoenmackers et al, 2010; Tsuchida et al,
2010) are another attempt at relation acquisition that
goes beyond pattern matching. Carlson et al (2010)
proposed a method based on inductive logic pro-
gramming (Quinlan, 1990). Schoenmackers et al
(2010) takes relation instances produced by Tex-
tRunner (Banko and Etzioni, 2008) as input and in-
duces first-order Horn clauses, and new instances are
infered using a Markov Logic Network (Richardson
and Domingo, 2006; Huynh and Mooney, 2008).
Tsuchida et al (2010) generated new relation hy-
potheses by substituting words in seed instances
with distributionally similar words. The difference
between these works and ours lies in the treatment
of evidence. While the above methods learn infer-
ence rules to acquire new relation instances from in-
dependent information sources scattered across dif-
ferent Web pages, our method takes the other option
of working with all the clues and indirect evidence a
single sentence can provide. In the future, a combi-
nation of both approaches may prove beneficial.
6 Conclusion
We have proposed a relation acquisition method that
is able to acquire semantic relations from infrequent
expressions by focusing on the evidence provided by
semantic word classes and partial pattern matching
instead of direct extraction patterns. We experimen-
tally demonstrated the effectiveness of this approach
on three relation acquisition tasks, causality, preven-
tion and material relations. In addition we showed
our method could acquire a significant number of
relation instances that are found in extremely infre-
quent expressions, the most extreme case of which
are single occurrence patterns, which are beyond
the reach of existing pattern based methods. We be-
lieve this ability is of crucial importance for acquir-
ing valuable long tail instances. In future work we
will investigate whether the current framework can
be extended to acquire inter-sentential relations.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proc. of the fifth ACM conference on Digital li-
braries, pages 85?94.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proc. of the 46th ACL-08:HLT, pages 28?36.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 57?64, College Park, Mary-
land, USA, June.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT ?05), pages 724?731.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for neverend-
ing language learning. In Proc of the 24th AAAI, pages
1306?1313.
834
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics (HLT/NAACL), pages 296?303.
Aron Culotta. 2004. Dependency tree kernels for rela-
tion extraction. In In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL-04, pages 423?429.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large Scale
Relation Acquisition Using Class Dependent Patterns.
In Proc. of the 9th International Conference on Data
Mining (ICDM), pages 764?769.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL2007).
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel Weld, and Alexander Yates. 2004. Web-
scale information extraction in KnowItAll. In Proc. of
the 13th international conference on World Wide Web
(WWW04), pages 100?110.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation extrac-
tion. In Proc. of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?05, pages
419?444.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th In-
ternational Conference on Computational Linguistics
(COLING?92), pages 539?545.
Tuyen N. Huynh and Raymond J. Mooney. 2008.
Discriminative structure and parameter learning for
markov logic networks. In Proc. of the 25th ICML,
pages 416?423.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08: HLT), pages 407?415.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in espresso-like bootstrapping algorithms.
In Proc. of EMNLP?08. Honolulu, USA, pages 1011?
1020.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In Proc. of the ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 323?328.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003?1011.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and Similarities on
the Web: Fact Extraction in the Fast Lane. In Proc. of
the COLING-ACL06, pages 809?816.
Patrick Pantel and Marco Pennacchiotti. 2006a.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proc. of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING-ACL-06,
pages 113?120.
Patrick Pantel and Pennacchiotti Pennacchiotti, Marco.
2006b. Espresso: Leveraging generic patterns for au-
tomatically harvesting semantic relations. In Proc. of
the COLING-ACL06, pages 113?120.
J. R. Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5(3):239?266.
Matthew Richardson and Pedro Domingo. 2006.
Markov logic networks. Machine Learning, 26:107?
136.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proc. of EMNLP2010, pages
1088?1098.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: An open search engine infrastructure for
developing new information access. In Proc. of IJC-
NLP, pages 189?196.
Kentaro Torisawa, Stijn De Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa,
Masaaki Murata, Kow Kuroda, and Ichiro Yamada.
2010. Organizing the web?s information explosion to
discover unknown unknowns. New Generation Com-
puting, 28(3):217?236.
Masaaki Tsuchida, Stijn De Saeger, Kentaro Torisawa,
Masaki Murata, Jun?ichi Kazama, Kow Kuroda, and
Hayato Ohwada. 2010. Large scale similarity-based
relation expansion. In Proc of the 4th IUCS, pages
140?147.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
pages 1083?1106.
835
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1619?1629,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Aid is Out There:
Looking for Help from Tweets during a Large Scale Disaster
Istva?n Varga? Motoki Sano? Kentaro Torisawa? Chikara Hashimoto?
Kiyonori Ohtake? Takao Kawai? Jong-Hoon Oh? Stijn De Saeger?
?Information Analysis Laboratory,
National Institute of Information and Communications Technology (NICT), Japan
{istvan, msano, torisawa, ch, kiyonori.ohtake, rovellia, stijn}@nict.go.jp
?Knowledge Discovery Research Laboratories, NEC Corporation, Japan
t-kawai@bx.jp.nec.com
Abstract
The 2011 Great East Japan Earthquake
caused a wide range of problems, and as
countermeasures, many aid activities were
carried out. Many of these problems and
aid activities were reported via Twitter.
However, most problem reports and corre-
sponding aid messages were not success-
fully exchanged between victims and lo-
cal governments or humanitarian organi-
zations, overwhelmed by the vast amount
of information. As a result, victims could
not receive necessary aid and humanitar-
ian organizations wasted resources on re-
dundant efforts. In this paper, we propose
a method for discovering matches between
problem reports and aid messages. Our
system contributes to problem-solving in
a large scale disaster situation by facilitat-
ing communication between victims and
humanitarian organizations.
1 Introduction
The 2011 Great East Japan Earthquake in March
11, 2011 killed 15,883 people and destroyed over
260,000 households (National Police Agency of
Japan, 2013). Accustomed way of living suddenly
became unmanageable and people found them-
selves in extreme conditions for months.
Just after the disaster, many people used Twitter
for posting problem reports and aid messages as
it functioned while most communication channels
suffered disruptions (Winn, 2011; Acar and Mu-
raki, 2011; Sano et al, 2012). Examples of such
problem reports and aid messages, translated from
Japanese tweets, are given below (P1, A1).
P1 My friend said infant formula is sold out. If
somebody knows shops in Sendai-city where
they still have it in stock, please let us know.
A1 At Jusco supermarket in Sendai, you can still
buy water and infant formula.
If A1 would have been forwarded to the sender
of P1, it could have helped since it would help
the ?friend? to obtain infant formula. But in re-
ality, the majority of such reports/messages, es-
pecially unforeseen ones went unnoticed amongst
the mass of information (Ohtake et al, 2013). In
addition, there were cases where many humani-
tarian organizations responded to the same prob-
lems and wasted precious resources. For instance,
many volunteers responded to problems which
were heavily reported by public media, leading
to oversupply (Saijo, 2012). Such waste of re-
sources could have been avoided if the organiza-
tions would have successfully shared the aid mes-
sages for the same problems.
Such observations motivated this work. We de-
veloped methods for recognizing problem reports
and aid messages in tweets and finding proper
matches between them. By browsing the discov-
ered matches, victims can be assisted to over-
come their problems, and humanitarian organiza-
tions can avoid redundant relief efforts. We define
problem reports, aid messages and their successful
matches as follows.
Problem report: A tweet that informs about the
possibility or emergence of a problem that re-
quires a treatment or countermeasure.
Aid message: A tweet that (1) informs about sit-
uations or actions that can be a remedy or so-
lution for a problem, or (2) informs that the
problem is solved or is about to be solved.
Problem-aid tweet match: A tweet pair is a
problem-aid tweet match (1) if the aid mes-
sage informs how to overcome the problem,
(2) if the aid message informs about the set-
1619
tlement of the problem, or (3) if the aid mes-
sage provides information which contributes
to the settlement of the problem.
In this work we excluded direct requests, such as
?Send us food!?, from problem reports. This is be-
cause it is relatively easy to recognize such direct
requests by checking mood types (i.e., imperative)
and their behavior is quite different from prob-
lem reports like ?People in Sendai are starving?.
Problem reports in this work do not directly state
which actions are required, only implying the ne-
cessity of a countermeasure through claiming the
existence of problems.
An underlying assumption of our method is that
we can find a noun-predicate dependency relation
that works as an indicator of problems and aids in
problem reports and aid messages, which we refer
to as problem nucleus and aid nucleus.1 An exam-
ple of problem nucleus is ?infant formula is sold
out? in P1, and that of aid nucleus is ?(can) buy
infant formula? in A1. Many problem-aid tweet
matches can be recognized through problem and
aid nuclei pairs.
We also assume that if the problem and aid nu-
clei match, they share the same noun. Then, the
semantics of predicates in the nuclei is the main
factor that decides whether the nuclei constitute
a match. We introduce a semantic classification
of predicates according to the framework of ex-
citation polarities proposed in Hashimoto et al
(2012). Our hypothesis is that excitation polarities
along with trouble expressions can characterize
problem reports, aid messages and their matches.
We developed a supervised method encoding such
information into its features.
An evident alternative to this approach is to use
sentiment analysis (Mandel et al, 2012; Tsagkali-
dou et al, 2011) assuming that problem reports
should include something ?bad? while aid mes-
sages describe something ?good?. However, we
will show that this does not work well in our exper-
iments. We think this is due to mismatch between
the concepts of problem/aid and sentiment polar-
ity. Note that previous work on ?demand? recogni-
tion also found similar tendencies (Kanayama and
Nasukawa, 2008).
Another issue in this task is, of course, the
context surrounding problem/aid nuclei. The fol-
1We found that out of 500 random tweets only 4.5% of
problem reports and 9.1% of aid messages did not contain
any problem report/aid message nuclei.
lowing (imaginary) tweets exemplify the problems
caused by contexts.
FP1 I do not believe infant formula is sold out
in Sendai.
FA1 At Jusco supermarket in Iwaki, you can still
buy infant formula.
The problem nuclei of FP1 and P1 are the same
but FP1 is not a problem report because of the ex-
pression ?I do not believe?. The aid nuclei of FA1
and A1 are the same but FA1 does not constitute
a proper match with P1 because FA1 and P1 re-
fer to different cities, ?Iwaki? and ?Sendai?. In
this work, the problems concerning the modality
and other semantic modifications to problem/aid
nuclei by context are dealt with by the introduc-
tion of features representing the text surrounding
the nuclei in machine learning. As for the loca-
tion problem, we apply a location recognizer to all
tweets and restrict the matching candidates to the
tweet pairs referring to the same location.
2 Approach
!"#$%"&"'()*&+*,*&-.(.+*,/+/0$0,/0,)-+$*#.(,'+
!"#$%&'("&!#")("&*#+,-.&"(
12001.+ 12001+/0$0,/0,)-+#0&*3",+
$#"4&0!+#0$"#1+$#"4&0!+,5)&05.+
/-0('&11/+&("&*#+,-.&"(
*(/+!0..*'0++*(/+,5)&05.+
$#"4&0!+#0$"#1+$#"4&0!+,5)&05.+ *(/+!0..*'0+*(/+,5)&05.+
!"#$%&'2/-0()3&&)('/)*4(
$#"4&0!+*,/+*(/+,"5,+*#0+1%0+.*!06+.*!0+'0"'#*$%()*&+&")*3",+
&")*3",+#0)"',(70#+
!"#$%&'2/-0('/)*4("&*#+,-.&"(
Figure 1: Problem-aid matching system overview.
We developed machine learning based systems
to recognize problem reports, aid messages and
problem-aid tweet matches. Figure 1 illustrates
the whole system. First, location names in tweets
are identified by matching tweets against our loca-
tion dictionary, described in Section 3. Then, each
tweet is paired with each dependency relation in
the tweet, which is a candidate of problem/aid nu-
clei and given to the problem report and aid mes-
sage recognizers. A tweet-nucleus-candidate pair
judged as problem report is combined with another
tweet-nucleus-candidate pair recognized as an aid
message if the two nuclei share the same noun and
the tweets share the same location name, and given
to the problem-aid match recognizer.
1620
In the following, problem and aid nuclei are
denoted by a noun-template pair. A template is
composed of a predicate and its argument posi-
tion. For instance, ?water supply stopped? in P2
is a problem nucleus, ?water supply recovered? in
A2 is an aid nucleus and they are denoted by the
noun-template pairs ?water supply, X stopped? and
?water supply, X recovered?.
P2 In Sendai city, water supply stopped.
A2 In Sendai city, water supply recovered.
Roughly speaking, we regard the tasks of prob-
lem report recognition and aid message recogni-
tion as the tasks of finding proper problem/aid
nuclei in tweets and our method performs these
tasks based on the semantic properties of nouns
and templates in problem/aid nucleus candidates
and their surrounding contexts.
The basic intuition behind this approach can
be explained using excitation polarity proposed in
Hashimoto et al (2012). Excitation polarity differ-
entiates templates into ?excitatory? or ?inhibitory?
with regard to the main function or effect of en-
tities referred to by their argument noun. While
excitatory templates (e.g., cause X, buy X, suf-
fer from X) entail that the main function or ef-
fect is activated or enhanced, inhibitory templates
(e.g., ruin X, prevent X, X runs out) entail that
the main function or effect is deactivated or sup-
pressed. The templates that do not fit into the
above categorization are classified as ?neutral?.
We observed that problem reports in general
included either of (A) a dependency relation be-
tween a noun referring to some trouble and an
excitatory template or (B) a dependency rela-
tion between a noun not referring to any trouble
and an inhibitory template. Examples of (A) in-
clude ?carbon monoxide poisoning, suffer from
X?, ?false rumor, spread X?. They refer to events
that activate troubles. On the other hand, (B) is
exemplified by ?school, X is collapsed?, ?battery,
X runs out?, which imply that some non-trouble
objects such as resources, appliances and facilities
are dysfunctional. We assume that if we can find
such dependency relations in tweets, the tweets are
likely to be problem reports.
Contrary, a tweet is more likely to be an aid
message when it includes either (C) a dependency
relation between a noun referring to some trouble
and an inhibitory template or (D) a dependency re-
lation between a noun not referring to any trou-
trouble non-trouble
excitatory (A) problem nucleus (D) aid nucleus
inhibitory (C) aid nucleus (B) problem nucleus
Table 1: Problem/aid-excitation matrix.
ble and an excitatory template. Examples of (C)
are ?flu, X was eradicated (in some shelter)? and
?debris, remove X?. They represent the dysfunc-
tion of troubles and can mean the solution or the
settlement of troubles. On the other hand, exam-
ples of (D) include ?school, X re-build? and ?baby
formula, buy X?. They entail that some resources
function properly or become available. These for-
mulations are summarized in Table 1.
As an interesting consequence of such a view
on problem/aid nucleus, we can say the following
regarding problem-aid tweet matchings: when a
problem nucleus and an aid nucleus are an ade-
quate match, the excitation polarities of their tem-
plates are opposite. Consider the following tweets.
P3 Some people were going back to Iwaki, but the
water system has not come back yet. It?s ter-
rible that bath is unusable.
A3 We open the bath for the public, located on
the 2F of Iwaki Kuhon temple. If you?re stay-
ing at a relief shelter and would like to take a
bath, you can use it.
?Bath is unusable? in P3 is a problem nucleus
while ?open the bath? in A3 is an aid nucleus.
Since the problem reported in P3 can be solved
with A3, they are a successful match. The in-
hibitory template ?X is unusable? indicates that
the function of ?bath?, a non-trouble expression,
is suppressed. The excitatory template ?open X?
indicates that the function of ?bath? is activated.
The same holds when we consider the noun re-
ferring to troubles like ?flu?. The polarity of the
template in a problem nucleus should be excita-
tory like ?flu is raging? while that of an aid nucleus
should be inhibitory like ?flu, X was eradicated?.
These examples keep the constraint that the prob-
lem and aid nucleus should have opposite polari-
ties when they constitute a match.
Note that the formulations of problem report,
aid message and their matches or the excitation
matrix (Table 1) were not presented to our anno-
tators and our test/training data may contain data
that contradict with the formulations. These for-
mulations constitute the hypothesis to be validated
in this work.
1621
An important point to be stressed here is that
there are problem-aid tweet matches that do not
fit into our formulations. For instance, we as-
sume that the problem nucleus and aid nucleus in
a proper match share the same noun. However,
tweet pairs such as ?There are many injured people
in Sendai city? and ?We are sending ambulances
to Sendai? can constitute a proper match, but there
is no proper problem-aid nuclei pair that share the
same noun in these tweets. (We can find the de-
pendency relations sharing ?Sendai? but they do
not express anything about the contents of prob-
lem and aid.) The point is that the tweet pairs can
be judged because people know ambulances can
be a countermeasure to injured people as world
knowledge. Introducing such world knowledge is
beyond the scope of this current study.
Also, we exclude direct requests from problem
reports. As mentioned in the introduction, identi-
fying direct requests is relatively easy, hence we
excluded them from our target.
3 Problem Report and Aid Message
Recognizers
We recognize problem reports and aid messages in
given tweets using a supervised classifier, SVMs
with linear kernel, which worked best in our pre-
liminary experiments. The feature set given to
the SVMs are summarized in the top part of Ta-
ble 2. Note that we used a common feature
set for both the problem report recognizer and
aid message recognizer and that it is categorized
into several types: features concerning trouble
expressions (TR), excitation polarity (EX), their
combination (TREX1) and word sentiment polar-
ity (WSP), features expressing morphological and
syntactic structures of nuclei and their context sur-
rounding problem/aid nuclei (MSA), features con-
cerning semantic word classes (SWC) appearing
in nuclei and their context, request phrases, such
as ?Please help us?, appearing in tweets (REQ),
and geographical locations in tweets recognized
by our location recognizer (GL). MSA is used to
express the modality of nuclei and other contex-
tual information surrounding nuclei. REQ was in-
troduced based on our observation that if there are
some requests in tweets, problem nuclei tend to
appear as justification for the requests.
We also attempted to represent nucleus template
IDs, noun IDs and their combinations directly in
our feature set to capture typical templates fre-
TR Whether the nucleus noun is a trouble/non-trouble expression.
EX1 The excitation polarity and the value of the excitation score of the
nucleus template.
TREX1 All possible combinations of trouble/non-trouble of TR and exci-
tation polarities of EX1.
WSP1 Whether the nucleus noun is positive/negative/not in theWord Sen-
timent Polarity (WSP) dictionary.
WSP2 Whether the nucleus template is positive/negative/not in the WSP
dictionary.
WSP3 Whether the nucleus template is followed by a positive/negative
word within the tweet.
MSA1 Morpheme n-grams, syntactic dependency n-grams in the tweet
and morpheme n-grams before and after the nucleus template.
(1 ? n ? 3)
MSA2 Character n-grams of the nucleus template to capture conjugation
and modality variations. (1 ? n ? 3)
MSA3 Morpheme and part-of-speech n-grams within the bunsetsu con-
taining the nucleus template to capture conjugation and modality
variations. (1 ? n ? 3) (A bunsetsu is a syntactic constituent
composed of a content word and several function words, the small-
est unit of syntactic analysis in Japanese.)
MSA4 The part-of-speech of the nucleus template?s head to capture
modality variations outside the nucleus template?s bunsetsu.
MSA5 The number of bunsetsu between the nucleus noun and the nucleus
template. We found that a long distance between the noun and the
template suggests parsing errors.
MSA6 Re-occurrence of the nucleus noun?s postpositional particle be-
tween the nucleus noun and the nucleus template. We found
that the re-occurrence of the same postpositional particle within
a clause suggests parsing errors.
SWC1 The semantic class n-grams in the tweet.
SWC2 The semantic class(es) of the nucleus noun.
REQ Presence of a request phrase in the tweet, identified from within
426 manually collected request phrases.
GL Geographical locations in the tweet identified using our location
recognizer. Existence/non-existence of locations in tweets are also
encoded.
EX2 Whether the problem and aid nucleus templates have the same or
opposite excitation polarities.
EX3 Product of the values of the excitation scores for the problem and
the aid nucleus template.
TREX2 All possible combinations of trouble/non-trouble of TR, excitation
polarity EX1 of the problem nucleus template and excitation po-
larity EX1 of the aid nucleus template.
SIM1 Common semantic word classes of the problem report and aid mes-
sage.
SIM2 Whether there are common nouns modifying the common nucleus
noun or not in the problem report and aid message.
SIM3 Whether the words in the same word class modify the common
nucleus noun or not in the problem report and aid message.
SIM4 The semantic similarity score between the problem nucleus tem-
plate and the aid nucleus template.
CTP Whether the problem nucleus template and the aid nucleus tem-
plate are in contradiction relation dictionary or not.
SSR1 Problem report recognizer?s SVM score of problem nucleus tem-
plate.
SSR2 Problem report recognizer?s SVM score of aid nucleus template.
SSR3 Aid message recognizer?s SVM score of the problem nucleus tem-
plate.
SSR4 Aid message recognizer?s SVM score of the aid nucleus template.
Table 2: Features used with the problem re-
port recognizer and the aid message recognizer
(above); additional features used in training the
problem-aid match recognizer (below).
quently appearing in problem and aid nuclei, but
since there was no improvement we omit them.
The other feature types need some non-trivial
dictionaries. In the following, we explain how we
created the dictionaries for each feature type along
with the motivation behind their introduction.
Trouble Expressions (TR) As mentioned previ-
ously, trouble expressions work as good evidence
for recognizing problem reports and aid messages.
The TR feature indicates whether the noun in the
problem/aid nucleus candidate is a trouble ex-
1622
pression or not. For this purpose, we created
a list of trouble expressions following the semi-
supervised procedure presented in De Saeger et al
(2008). After manual validation of the list, we ob-
tained 20,249 expressions referring to some trou-
bles, such as ?tsunami? and ?flu?. The value of the
TR feature is determined by checking whether the
nucleus noun is contained in the list.
Excitation Polarities (EX) The excitation po-
larities are also important in recognizing problem
reports and aid messages as mentioned before. For
constructing the dictionary for excitation polarities
of templates, we applied the bootstrapping proce-
dure in Hashimoto et al (2012) to 600 millionWeb
pages. Hashimoto?s method provides the value of
the excitation score in [?1, 1] for each template
indicating the polarities and their strength. Posi-
tive value indicates excitatory, negative value in-
hibitory and small absolute value neutral. After
manual checking of the results by the majority
vote of three human annotators (other than the au-
thors), we limited the templates to the ones that
have score values consistent with the majority vote
of the annotators, obtaining a dictionary consisting
of 7,848 excitatory, 836 inhibitory and 7,230 neu-
tral templates. The Fleiss? (1971) kappa-score was
0.48 (moderate agreement). We used the excita-
tion score values as feature values. Excitation has
already been used in many works, such as causal-
ity and contradiction extraction (Hashimoto et al,
2012) or Why-QA (Oh et al, 2013).
Word Sentiment Polarity (WSP) As we sug-
gested before, full-fledged sentiment analysis to
recognize the expressions, including clauses and
phrases, that refer to something good or bad was
not effective in our task. However, the sentiment
polarity, assigned to single words turned out to
be effective. To identify the sentiment polarity
of words, we employed the word sentiment polar-
ity dictionary used with a sentiment analysis tool
for Japanese, the Opinion Extraction Tool soft-
ware2, which is an implementation of Nakagawa
et al (2010). The dictionary includes 9,030 posi-
tive and 27,951 negative words. Note that we used
the Opinion Extraction Tool in the experiments to
check the effectiveness of the full-fledged senti-
ment analysis in this task.
Semantic Word Class (SWC) We assume that
nouns in the same semantic class behave simi-
2Provided at the ALAGIN Forum (http://www.alagin.jp/).
larly in crisis situations. For example, if ?infec-
tion? appears in a problem report, the tweets in-
cluding ?pulmonary embolism? are also likely to
be problem reports. Semantic word class features
are used to capture such tendencies. We applied
an EM-style word clustering algorithm in Kazama
and Torisawa (2008) to 600 millionWeb pages and
clustered 1 million nouns into 500 classes. This
algorithm has been used in many works, such as
relation extraction (De Saeger et al, 2011) and
Why-QA (Oh et al, 2012), and can generate vari-
ous kinds of semantically clean word classes, such
as foods, disease names, and natural disasters. We
used the word classes in tweets as features.3
Geographical Locations (GL) Our location
recognizer matches tweets against our loca-
tion dictionary. Location names and their
existence/non-existence in tweets constitute evi-
dence, thus we encoded such information into our
features. The location dictionary was created from
the Japan Post code data4 and Wikipedia, contain-
ing 2.7 million location names including cities,
schools and other facilities (Kazama et al, 2013).
4 Problem-Aid Match Recognizer
After problem report and aid message recogni-
tion, the positive outputs of the respective classi-
fiers are used as input in this step. The problem-
aid match recognizer classifies an aid message-
nucleus pair together with the problem report-
nucleus pair employing SVMs with linear ker-
nel, which performed best in this task again. The
problem-aid match recognizer uses all the features
used in the problem report recognizer and the aid
message recognizer along with additional features
regarding: excitation polarity (EX) and trouble
expressions (TR), distributional similarity (SIM),
contradiction (CTP) and SVM-scores of the prob-
lem report and aid message recognizers (SSR).
Here also we attempted to capture typical or fre-
quent matches of nuclei using template and noun
IDs and their combinations, but we did not observe
any improvement so we omit them from the fea-
ture set. The bottom part of Table 2 summarizes
the additional feature set, some of which are de-
scribed below in more detail.
3There is a slight complication here. For each noun n, EM
clustering estimates a probability distribution P (n|c?) for n
and semantic class c?. From this distribution we obtained
discrete semantic word classes by assigning each noun n to
semantic class c = argmaxc? p(c?|n).
4http://www.post.japanpost.jp/zipcode/download.html
1623
As for TR and EX, our intuition is that if a prob-
lem nucleus and an aid nucleus are an adequate
match, their excitation polarities are opposite, as
described in Section 2. We encode whether the ex-
citation polarities of nuclei templates are the same
or not in our features. Also, the excitation polar-
ities of problem and aid nuclei and TR are com-
bined (TREX1, TREX2) so that the classifier can
know whether the nuclei follow the constraint for
adequate matches described in Section 2.
As for SIM, if an aid message matches a prob-
lem report, besides the common nucleus noun, it is
reasonable to assume that certain contexts are se-
mantically similar. We capture this characteristic
in three ways. SIM1 looks for common semantic
word classes in the problem report and aid mes-
sage. SIM2 and SIM3 target the modifiers of the
common nucleus noun if they exist.
We also observed that if an aid message matches
a problem report, the problem nucleus template
and aid nucleus template are often distributionally
similar. A typical example is ?X is sold out? and
?buy X?. SIM4 captures this tendency. As the dis-
tributional similarity between templates, we used
a Bayesian distributional similarity measure pro-
posed by Kazama et al (2010).5
CTP indicates whether the problem and aid nu-
clei are in contradiction relation or not. This fea-
ture was implemented based on the observation
that when problem and aid nuclei are in contradic-
tion relation, they are often proper matches (e.g.,
?blackout, ?X starts?? and ?blackout, ?X ends??).
CTP indicates whether nucleus pairs are in the
one million contradiction phrase pairs6 automat-
ically obtained by applying a method proposed by
Hashimoto et al (2012) to 600 million Web pages.
5 Experiments
We evaluated our problem report recognizer and
problem-aid match recognizer. For the sake of
space, we give only the performance figures of the
aid message recognizer at the end of Section 5.1.
We collected tweets posted during and after
the 2011 Great East Japan Earthquake, between
March 10 and April 4, 2011. After applying
keyword-based filtering with a list of over 300
5The original similarity was defined over noun pairs and it
was estimated from dependency relations. Obtaining similar-
ity between template pairs, not noun pairs, is straightforward
given the same dependency relations. We used 600 million
Web pages for this similarity estimation.
6The precision of the pairs was reported as around 70%.
disaster related keywords, we obtained 55 million
tweets. After dependency parsing7, we used them
in our evaluation.
5.1 Problem Report Recognition
Firstly, we evaluated our problem report recog-
nizer. Particularly, we assessed the effect of ex-
citation polarities and trouble expressions in two
settings. The first is against a naturally distributed
gold standard data. The second targets problem
reports with problem nuclei unseen in the training
data.
In both experiments we observed that the per-
formance drops when excitation polarities and
trouble expressions are removed from the feature
set. The performance drop was larger in the sec-
ond experiment which suggests that the excitation
polarities and trouble expressions are more effec-
tive against unseen problem reports.
Training and test data for problem report recog-
nition consist of tweet-nucleus candidate pairs
randomly sampled from our 55 million tweet data.
The training data (R) and test data (T ) consist of
13,000 and 1,000 pairs, respectively, manually la-
beled by three annotators (other than the authors)
as problem or other. Final judgment was made by
majority vote. The Fleiss? kappa score for train-
ing and test data for annotation judgement is 0.74
(substantial agreement).
Our problem report recognizer and its variants
are listed in Table 3. Table 4 shows the evalua-
tion results. The proposed method achieved about
44% recall and nearly 80% precision, outperform-
ing all other systems in terms of precision, F-score
and average precision8. The improvement in pre-
cision when using TR&EX is statistically signif-
icant (p < 0.05).9 Note that F-measure dropped
PROPOSED: Our proposed method with all features used.
PROPOSED-*: The proposed method without the feature set de-
noted by ?*?. Here EX and TR denote all excitation po-
larity and trouble expression related features, respectively,
including their combinations (TREX1).
PROPOSED+OET: The proposed method incorporating the
classification results of problem nucleus candidates by the
Opinion Extraction Tool as additional binary features.
RULE-BASED: The method that regards only nuclei satisfying
the constraint in Table 1 as problem nuclei.
Table 3: Evaluated problem report recognizers.
7http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
8We calculate average precision using the formula: aP =?n
k=1
(Prec(k)?rel(k))
n , where Prec(k) is the precision atcut-off k and rel(k) is an indicator function equaling 1 if
the item at rank k is relevant, zero otherwise.
9Throughout this paper we performed two-tailed test of
1624
Recognition system R (%) P (%) F (%) aP (%)
PROPOSED 44.26 79.41 56.83 71.82
PROPOSED-TR&EX 45.08 74.83 56.26 69.67
PROPOSED-EX 44.67 74.66 55.89 69.90
PROPOSED-TR 43.85 74.31 55.15 69.44
PROPOSED-MSA 28.69 70.71 40.81 57.74
PROPOSED-SWC 43.42 75.97 55.25 70.61
PROPOSED-WSP 43.14 77.83 55.50 70.45
PROPOSED-REQ 42.64 76.16 55.50 54.67
PROPOSED-GL 44.14 78.34 55.50 56.46
PROPOSED+OET 44.24 79.41 56.82 71.81
RULE-BASED 30.32 67.96 41.93 n/a
Table 4: Recall (R), precision (P), F-score (F) and
average precision (aP) of the problem report rec-
ognizers.
whenever each type of feature was removed, im-
plying that each type of feature is effective in this
task. Especially note the performance drop if we
remove excitation polarities (EX), trouble expres-
sion (TR) and both excitation and trouble expres-
sion features (TR&EX), confirming that they are
crucial in recognizing problem reports with high
accuracy. Also note that the performance of PRO-
POSED+OET was actually slightly worse than that
of the proposed method. This suggests that full-
fledged sentiment analysis is not effective at least
in this setting. The rule-based method achieved
relatively high precision despite of the low recall,
demonstrating the importance of problem and aid
nuclei formulations described in Section 1.
The second experiment assessed the efficiency
of our problem report recognizer against unseen
problem nuclei under the condition that every tem-
plate in nuclei has excitation polarity. We sampled
the training and test data so that the problem nu-
cleus nouns and templates in the training and test
data are disjoint. First we created a subset of the
test data by selecting the samples which had nu-
clei with excitation templates. We call this sub-
set T ?. Next, we removed samples from training
data R if either of their problem nouns or tem-
plates appeared in the nuclei of T ?. The result-
ing new training data (called R?) and test data (T ?)
consist of 6,484 and 407 tweet-nucleus candidate
pairs, respectively. We trained our problem report
recognizer using R? and tested its performance us-
ing T ?. Figure 2 shows the precision-recall curves
obtained by changing the threshold on the SVM
scores. The effectiveness of excitation polarities
and trouble expressions was more evident in this
setting. The PROPOSED?s performance was ac-
tually better in this setting (almost 50% recall at
population proportion (Ott and Longnecker, 2010) using
SVM-threshold=0.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pre
cisi
on
Recall
PROPOSED-TRPROPOSED-EX PROPOSED-TR&EXPROPOSED
Figure 2: Precision-recall curves of problem re-
port recognizers against unseen problem nuclei.
more that 80% precision), than the previous set-
ting, showing that excitation templates and trouble
expressions are crucial in achieving high perfor-
mance especially for unseen problem nuclei. The
same was confirmed when we removed excitation
polarity and trouble expression related features,
with performance dropping by 7.43 points in terms
of average precision. The improvement in pre-
cision when using TR&EX is statistically signif-
icant (p < 0.01). This implies, assuming that we
have a wide-coverage dictionary of templates with
excitation polarities, that excitation polarities are
important in dealing with unexpected problems in
disaster situations.
We also evaluated the aid-message recognizer,
using tweet-nucleus pairs in R and T as train-
ing and test data and the annotation scheme was
also the same. The average Fleiss? kappa score
was 0.55 (moderate agreement). Our recognizer
achieved 53.82% recall and 65.67% precision and
showed similar tendencies with the problem report
recognizer, with the excitation polarities and trou-
ble expressions contributing to higher accuracy.
We can conclude that excitation polarities and
trouble expressions are important in identifying
problem reports and aid messages during disaster
situations.
5.2 Problem-Aid Matching
Next, we evaluated the performance of the
problem-aid match recognizer. We applied our
problem report recognizer and aid message recog-
nizer to all 55 million tweets and combined the
tweet-nucleus pairs judged as problem reports and
aid messages, respectively, to create the training
and test data.
The training data consists of two parts (M1 and
M2). M1 includes many variations of the aid
messages for each problem report, while M2 en-
1625
sures diversity in nouns and templates in problem
nuclei. For M1, we randomly picked up problem
reports from the output of the problem report rec-
ognizer and to each we attached up to 30 randomly
picked, distinct aid messages that have the same
nucleus noun. Building M2 follows the construc-
tion method of M1, except that: (1) we used up
to 30 distinct problem nuclei for each noun; (2)
for each problem report we attached only one ran-
domly picked aid message.
In creating the test data T2, we followed the
construction method used for M2 to assess the
performance of our proposal with a large variety
of problems. M1, M2 and T2 consist of 3,000,
6,000 and 1,000 samples, respectively. The an-
notation was done by majority vote of three hu-
man annotators (other than the authors), the aver-
age Fleiss? kappa-score for training and test data
was 0.63 (substantial agreement).
We trained the problem-aid match recognizers
of Table 5 with M1 and M2. The evaluation
results performed on T2 are shown in Table 6.
We can observe that, among the nuclei related
features, the trouble expression (TR) and excita-
tion polarity (EX) features and their combination
(TR&EX) contribute most to the performance, al-
though the contribution of nuclei related features
is less in comparison to the problem report and aid
message recognition. The improvement in preci-
sion when using TR&EX is marginally significant
(p = 0.056). Instead, morphological and syntactic
analysis (MSA) and semantic word class (SWC)
features greatly improved performance.
As the final experiments, we evaluated top-
ranking matches of our problem-aid match recog-
nizer, where the recognizer classified all the possi-
ble combinations of tweet-nuclei pairs taken from
55 million tweets. In addition, we assessed the ef-
fectiveness of excitation polarities and trouble ex-
pressions by comparing all positive matches pro-
duced by our full problem-aid match recognizer
(PROPOSED) and those produced by the problem-
aid match recognizer (PROPOSED-TR&EX) that
PROPOSED: Our proposed method with all features used.
PROPOSED-*: The proposed method without the feature set de-
noted by ?*?. Here also EX and TR denote all excitation
polarity and trouble expression related features, respec-
tively, including their combinations (TREX1 and TREX2).
RULE-BASED: The method that judges only problem-aid nuclei
combinations with opposite excitation polarities as proper
matches.
Table 5: Evaluated problem-aid match recogniz-
ers.
Matching system R (%) P (%) F (%) aP (%)
PROPOSED 30.67 70.42 42.92 55.16
PROPOSED-TR&EX 28.83 67.14 40.33 53.99
PROPOSED-EX 31.29 67.11 42.68 54.19
PROPOSED-TR 30.56 69.33 42.42 54.85
PROPOSED-MSA 13.50 53.66 21.57 44.52
PROPOSED-SWC 26.99 67.69 38.59 52.23
PROPOSED-WSP 30.61 69.51 42.50 54.81
PROPOSED-CTP 30.06 70.00 42.05 54.94
PROPOSED-SIM 29.95 70.11 41.97 54.98
PROPOSED-REQ 30.58 70.25 42.61 54.67
PROPOSED-GL 30.61 70.31 42.65 55.02
PROPOSED-SSR 30.67 69.44 42.72 54.91
RULE-BASED 15.33 17.36 16.28 n/a
Table 6: Recall (R), precision (P), F-score (F) and
average precision (aP) of the problem-aid match
recognizers.
did not use excitation polarities and trouble ex-
pressions in its feature set. Note that PROPOSED-
TR&EX was fed by the problem report and aid
message recognizers that didn?t use excitation po-
larities and trouble expressions. For both systems?
training data we used R for the problem report
and aid message recognizers; M1 and M2 for the
problem-aid matching recognizers.
PROPOSED and PROPOSED-TR&EX output 15.2
million and 13.4 million positive matches, cover-
ing 1,691 and 1,442 nucleus nouns, respectively.
Table 7 shows match samples identified with PRO-
POSED. We observed that the output of each sys-
tem was dominated by just a handful of frequent
nucleus nouns, such as ?water? or ?gasoline?. We
preferred to assess the performance of our system
against a large variation of problem-aid nuclei,
thus we restricted the number of matches to 10
for each noun10. After this restriction the number
of matches found by PROPOSED and PROPOSED-
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000  6000  7000  8000  9000
Pre
cisi
on
Rank
PROPOSED (unseen)PROPOSED-TR&EX (unseen)PROPOSED (all)PROPOSED-TR&EX (all)
Figure 3: Problem-aid match recognition perfor-
mance for ?all? and ?unseen? problem reports.
10Note that this setting is a pessimistic estimation of our
system?s overall performance, since according to our obser-
vations problem reports with very frequent nucleus nouns had
proper matches with a higher accuracy than problem reports
with less frequent nucleus nouns.
1626
Problem report: ???????????????????
??????????????????????????
??????????????????????????
(Starting from the 17th, the Iwaki Joban Hospital, the Iwaki
Urology Clinique, the Takebayashi Sadakichi Memorial Clin-
ique and the Izumi Central Clinique have all suspended dial-
ysis sessions. Patients are advised to urgently make contact.)
Aid message: ???????????????????
??????????????????????????
(Restart of dialysis sessions: short dialysis sessions are
available at the Iwaki Urology Clinique between 9 AM and
4 PM.)
Problem report: ??????????????????
?????????????????????????
?????????????????????????
?????
(Please spread this message. According to my father in
Sendai, there are more and more people whose phones ran
out of battery. We need phone chargers!)
Aid message: ???????????????????
???????????
([Please spread] At the City Hall of Wakabayashi-ku, Sendai,
you can recharge your phone battery.)
Table 7: Examples from the output of the proposed
method in the ?all? setting. Problem report and aid
message nuclei are boldfaced in the English trans-
lations.
TR&EX was 8,484 and 7,363, respectively.
The performance of PROPOSED and
PROPOSED-TR&EX were assessed in two
settings: ?all? and ?unseen?. For ?all?, we selected
400 problem-aid matches from the outputs of the
respective systems after applying the 10-match
restriction. For ?unseen?, first we removed the
samples from the systems? outputs if either the
nucleus noun or template pair appear in the nuclei
of the problem-aid match recognizers? training
data. Next we applied the same sampling process
as with ?all?. Three annotators (other than the
authors) manually labeled the sample sets, final
judgment being made by majority vote. The
Fleiss? kappa score for all test data was 0.73
(substantial agreement).
Figure 3 shows the systems? precision curves,
drawn from the samples whose X-axis positions
represent the ranks according to SVM scores. In
both scenarios we can confirm that excitation po-
larity and trouble expression related features con-
tribute to this task. In the ?all? setting in terms
of average precision calculated over the top 7,200
matches, PROPOSED?s 62.36% is 10.48 points
higher than that of PROPOSED-TR&EX. For un-
seen problem/aid nuclei PROPOSED method?s av-
erage precision of 58.57% calculated at the top
3,800 matches is 5.47 points higher than that of
PROPOSED-TR&EX at the same data point. The
improvement in precision when using TR&EX is
statistically significant in both settings (p < 0.01).
6 Related Work
Twitter has been observed as a platform for situ-
ational awareness during various crisis situations
(Starbird et al, 2010; Vieweg et al, 2010), as sen-
sors for an earthquake reporting system (Sakaki et
al., 2010; Okazaki and Matsuo, 2010) or to de-
tect epidemics (Aramaki et al, 2011). Besides
Twitter, blogs or forums have also been the tar-
get of community response analysis (Qu et al,
2009; Torrey et al, 2007). Similar to our work
are the ones of Neubig et al (2011) and Ishino et
al. (2012), who tackle specific problems that occur
during disasters (i.e., safety information and trans-
portation information, respectively); and Munro
(2011), who extracted ?actionable messages? (re-
quests and aids, indiscriminately), matching being
performed manually. Our work differs from (Neu-
big et al, 2011) and (Ishino et al, 2012) in that we
do not restrict the range of problem reports, and as
opposed to (Munro, 2011), matching is automatic.
Systems such as that of Seki (2011)11 or Munro
(2013)12 are successful examples of crisis crowd-
sourcing, but these require extensive human inter-
vention to coordinate useful information.
Another category of related work relevant to our
task is troubleshooting. Baldwin et al (2007) and
Raghavan et al (2010) use discussion forums to
solve technical problems using supervised learn-
ing methods, but these approaches presume that
the solution of a specific problem is within the
same thread. In our work we do not employ struc-
tural characteristics of tweets as restrictions (e.g.,
a problem report and its aid message need to be in
the same tweet chain).
7 Conclusions
In this paper, we proposed a method to dis-
cover matches between problem reports and aid
messages from tweets in large-scale disasters.
Through a series of experiments, we demonstrated
that the performance of the problem-aid match-
ing can be improved with the usage of seman-
tic orientation of excitation polarities, proposed in
(Hashimoto et al, 2012), and trouble expressions.
We are planning to deploy our system and re-
lease model files of the classifiers to assist relief
efforts in future crisis scenarios.
11http://www.sinsai.info/
12http://www.mission4636.org/
1627
References
Adam Acar and Yuya Muraki. 2011. Twitter for cri-
sis communication: Lessons learned from Japan?s
tsunami disaster. International Journal of Web
Based Communities, 7(3):392?402.
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza
epidemics using Twitter. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011), pages 1568?1576.
Timothy Baldwin, David Martinez, and Richard B.
Penman. 2007. Automatic thread classification for
Linux user forum information access. In Proceed-
ings of the 12th Australasian Document Computing
Symposium (ADCS 2007), pages 72?79.
Stijn De Saeger, Kentaro Torisawa, and Jun?ichi
Kazama. 2008. Looking for trouble. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (COLING 2008), pages 185?
192.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong-Hoon Oh, Istva?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 825?835.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 5:378?382.
Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Jong-Hoon Oh, and Jun?ichi Kazama.
2012. Excitatory or inhibitory: A new semantic
orientation extracts contradiction and causality from
the web. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2012), pages 619?630.
Aya Ishino, Shuhei Odawara, Hidetsugu Nanba, and
Toshiyuki Takezawa. 2012. Extracting transporta-
tion information and traffic problems from tweets
during a disaster: Where do you evacuate to? In
Proceedings of the Second International Conference
on Advances in Information Mining and Manage-
ment (IMMM 2012), pages 91?96.
Hiroshi Kanayama and Tetsuya Nasukawa. 2008. Tex-
tual demand analysis: Detection of users? wants and
needs from opinions. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (COLING 2008), pages 409?416.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 407?
415.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A
Bayesian method for robust estimation of distribu-
tional similarities. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 247?256.
Jun?ichi Kazama, Stijn De Saeger, Kentaro Torisawa,
Jun Goto, and Istva?n Varga. 2013. Saigaiji jouhou
e no shitsumon outo shisutemu no tekiyou no koko-
romi. (An attempt for applying question-answering
system on disaster related information). In Pro-
ceeding of the Nineteenth Annual Meeting of The
Association for Natural Language Processing. (in
Japanese).
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during Hurricane Irene. In Proceedings of the Sec-
ond Workshop on Language Analysis in Social Me-
dia (LASM 2012), pages 27?36.
Robert Munro. 2011. Subword and spatiotempo-
ral models for identifying actionable information in
Haitian Kreyol. In Proceedings of the Fifteenth
Conference on Computational Natural Language
Learning (CoNLL-2011), pages 68?77.
Robert Munro. 2013. Crowdsourcing and the
crisis-affected community. Information Retrieval,
16(2):210?266.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using CRFs with hidden variables. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL HLT
2010), pages 786?794.
National Police Agency of Japan. 2013. Damage sit-
uation and public countermeasures associated with
2011 Tohoku district ? off the Pacific Ocean Earth-
quake. http://www.npa.go.jp/archive/
keibi/biki/higaijokyo_e.pdf. (accessed
on 30 April, 2013).
Graham Neubig, Yuichiroh Matsubayashi, Masato
Hagiwara, and Koji Murakami. 2011. Safety infor-
mation mining? what can NLP do in a disaster?.
In Proceedings of the 5th International Joint Con-
ference on Natural Language Processing (IJCNLP
2011), pages 965?973.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Jun?ichi Kazama,
and Yiou Wang. 2012. Why question answering
using sentiment analysis and word classes. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL 2012), pages 368?378.
1628
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013).
Kiyonori Ohtake, Kentaro Torisawa, Jun Goto, and
Stijn De Saeger. 2013. Saigaiji ni okeru hi-
saisha to kyuuen kyuujosha kan no souhoko komyu-
nikeeshon. (Bi-directional communication between
victims and rescures during a crisis). In Proceeding
of the Nineteenth Annual Meeting of The Association
for Natural Language Processing. (in Japanese).
Makoto Okazaki and Yutaka Matsuo. 2010. Seman-
tic Twitter: Analyzing tweets for real-time event
notification. In Proceedings of the 2008/2009 in-
ternational conference on Social software: Re-
cent trends and developments in social software
(BlogTalk 2008), pages 63?74.
R. Lyman Ott and Michael T. Longnecker, 2010. An
Introduction to Statistical Methods and Data Analy-
sis, chapter 10.2. Brooks Cole, 6th edition.
Yan Qu, Philip Fei Wu, and Xiaoqing Wang. 2009.
Online community response to major disaster: A
study of Tianya forum in the 2008 Sichuan Earth-
quake. In 42st Hawaii International International
Conference on Systems Science (HICSS-42), pages
1?11.
Preethi Raghavan, Rose Catherine, Shajith Ikbal,
Nanda Kambhatla, and Debapriyo Majumdar. 2010.
Extracting problem and resolution information from
online discussion forums. In Proceedings of the
16th International Conference on Management of
Data (COMAD 2010).
Takeo Saijo. 2012. Hito-o tasukeru sungoi shikumi. (A
stunning system that saves people). Diamond Inc.
(in Japanese).
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: Real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web (WWW 2010), pages 851?860.
Motoki Sano, Istva?n Varga, Jun?ichi Kazama, and Ken-
taro Torisawa. 2012. Requests in tweets dur-
ing a crisis: A systemic functional analysis of
tweets on the Great East Japan Earthquake and
the Fukushima Daiichi nuclear disaster. In Pa-
pers from the 39th International Systemic Func-
tional Congress (ISFC39), pages 135?140.
Haruyuki Seki. 2011. Higashi-nihon daishinsai fukkou
shien platform sinsai.info no naritachi to kongo no
kadai. (The organizational structure of sinsai.info
restoration support platform for the 2011 Great East
Japan Earthquake and future challenges). Journal of
digital practices, 2(4):237?241. (in Japanese).
Kate Starbird, Leysia Palen, Amanda L. Hughes, and
Sarah Vieweg. 2010. Chatter on the red: What
hazards threat reveals about the social life of mi-
croblogged information. In Proceedings of The
2010 ACM Conference on Computer Supported Co-
operative Work (CSCW 2010), pages 241?250.
Cristen Torrey, Moira Burke, Matthew L. Lee,
Anind K. Dey, Susan R. Fussell, and Sara B. Kiesler.
2007. Connected giving: Ordinary people coordi-
nating disaster relief on the Internet. In Proceedings
of the 40th Annual Hawaii International Conference
on System Sciences (HICSS-40), pages 179?188.
Katerina Tsagkalidou, Vassiliki Koutsonikola, Athena
Vakali, and Konstantinos Kafetsios. 2011. Emo-
tional aware clustering on micro-blogging sources.
In Proceedings of the 4th international conference
on Affective computing and intelligent interaction
(ACII 2011), pages 387?396.
Sarah Vieweg, Amanda L. Hughes, Kate Starbird, and
Leysia Palen. 2010. Microblogging during two nat-
ural hazards events: What Twitter may contribute
to situational awareness. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems (CHI 2010), pages 1079?1088.
Patrick Winn. 2011. Japan tsunami disaster: As Japan
scrambles, Twitter reigns. GlobalPost, 18 March.
1629
