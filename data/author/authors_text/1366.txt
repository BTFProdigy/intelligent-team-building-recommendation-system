  
An Agent-based Approach to Chinese Named Entity Recognition 
 
Shiren Ye Tat-Seng Chua Liu Jimin 
School of Computing, National University of Singapore, 
Singapore, 117543 
yesr@comp.nus.edu.sg chuats@comp.nus.edu.sg Liujm@comp.nus.edu.sg 
   
Abstract 
Chinese NE (Named Entity) recognition is 
a difficult problem because of the 
uncertainty in word segmentation and 
flexibility in language structure. This paper 
proposes the use of a rationality model in a 
multi-agent framework to tackle this 
problem. We employ a greedy strategy and 
use the NE rationality model to evaluate 
and detect all possible NEs in the text. We 
then treat the process of selecting the best 
possible NEs as a multi-agent negotiation 
problem. The resulting system is robust 
and is able to handle different types of NE 
effectively. Our test on the MET-2 test 
corpus indicates that our system is able to 
achieve high F1 values of above 92% on all 
NE types. 
1. Introduction 
Named entity (NE) recognition is a fundamental 
step to many language processing tasks. It was a 
basic task of the Message Understanding 
Conference (MUC) and has been studied 
intensively. Palma & Day (97) reported that 
person (PER), location (LOC) and organization 
(ORG) names are the most difficult sub-tasks as 
compared to other entities as defined in MUC. 
This paper thus focuses only on the recognition 
of PER, LOC and ORG entities. 
Recent research on NE recognition has been 
focused on the machine learning approach, such 
as the transformation-based learning (Aberdeen 
95), hidden Markov model (Bikel et al 97), 
decision tree (Sekin et al 98), collocation 
statistics (Lin 98), maximum entropy model 
(Borthwick 99), and EM bootstrapping 
(Cucerzan & Yarowsky 99). Other than English, 
several recent works examined the extraction of 
information from Spanish, Chinese, and 
Japanese (Isozaki 01). Most approaches for 
Chinese NE recognition used handcrafted rules, 
supplemented by word or character frequency 
statistics. These methods require a lot of 
resources to model the NEs. Chen et al (98) 
used 1-billion person name dictionary and 
employed mainly internal word statistics with no 
generalization. Yu et al (98) employed a 
common framework to model both the context 
and information residing within the entities, and 
performed rule generalization using POS 
(part-of-speech) and some semantic tags. A 
similar system is also reported in Luo & Song 
(01). 
Chinese NE recognition is much more difficult 
than that in English due to two major problems. 
The first is the word segmentation problem 
(Sproat et al 96, Palmer 97). In Chinese, there is 
no white space to delimit the words, where a 
word is defined as consisting of one or more 
characters representing a linguistic token. Word 
is a vague concept in Chinese, and Palmer (97) 
showed that even native speakers could only 
achieve about 75% agreement on ?correct? 
segmentation. As word segmentation is the basic 
initial step to almost all linguistic analysis tasks, 
many techniques developed in English NLP 
cannot be applied to Chinese. 
Second, there is no exterior feature (such as the 
capitalization) to help identify the NEs, which 
share many common characters with non-NE (or 
common words). For example, while ?  is 
normally associated with the country China, it 
could also mean the concepts in, at or hit; and? 
normally refers to the surname Zhang, but it also 
means the concepts open, sheet or spread. 
Moreover, proper names in Chinese may contain 
common words and vice versa. 
Because of the above problems, the use of 
statistical and heuristic rules commonly adopted 
in most existing systems is inadequate to tackle 
the Chinese NE recognition problem. In this 
paper, we consider a new approach of employing 
a rationality model in a multi-agent framework. 
 The main ideas of our approach are as follows. 
First, we use an NE rationality measure to 
evaluate the probability of a sequence of tokens 
being a specific NE type, and adopt a greedy 
approach to detect all possible NEs. Second, we 
treat the process of selecting the best NEs among 
a large set of possibilities as a multi-agent 
negotiation problem. We test our overall 
approach on the MET-2 test set and the system is 
able to achieve high F1 values of over 92% on all 
NE types. The results are significantly better 
than most reported systems on MET-2 test set. 
The rest of the paper describes the details of our 
rationality-based and multi-agent negotiation 
approach to detect and refine NEs. 
2. Rationality Model for NE Detection 
2.1 Named Entity and Its tokens Feature 
For clarity and without lost of generality, we 
focus our discussion mainly on PER entity. The 
problems and techniques discussed are 
applicable to LOC and ORG entities. We 
consider a simple PER name model comprising 
the surname followed by the first-name. Given 
the presence of a surname (as cue-word) in a 
token sequence, we compute the likelihood of 
this token playing the role of surname and the 
next token as the first-name. The pair could be 
recognized as PER only if both tokens are 
labeled as positive (or of the right types) as 
shown in Table 1. If either one of both of the 
tokens are evaluated negatively, then the pair 
will not be recognized as PER based on the 
model defined above.  
Sentence PER? Label Remarks 
?????? Y ?(+) ?(+) 
... invite Zhang Fei 
to speak ... 
?????? N ?(-) ?(-) 
? a piece of airline 
ticket ? 
?????? ? ?(+) ?(-) //Illegal PER 
????* ? ?(-) ?(+) //Illegal PER 
* Strictly, ??? and Mr. Zhang are not really person names. 
They are references to person names and should be detected via 
co-reference.  
Table 1: An example of NE and non-NE 
Although the example depicted in Table 1 is very 
simple, the same idea can be extended to the 
more complex NE Types for ORG and LOCs. 
The number of tokens in a NE may vary from 2 
in PER to about 20 for ORG. One constraint is 
that the sequencing of tokens and their labels 
must be consistent with the respective NE type. 
Also, there are grammatical rules governing the 
composition of different NE type. For example, 
LOC may consist of a sequence of LOCs; and 
ORG may include PER and/or LOC on its left. 
Thus by considering one pair of tokens at a time, 
and by extending the token sequence to the 
adjacent token one at a time, we can draw 
similar conclusion as that depicted in Table 1 for 
complex NE types. 
2.2 The Rationality Computation 
If we know the probability distribution of each 
type of token in a window, NE recognition is 
then the procedure of evaluating the rationality 
or certainty of a sequence of tokens with respect 
to a NE type. Motivated by the results in Table 1 
we view NE recognition as a special coloring 
problem. Initially, all the tokens in the corpus are 
considered as a sequence of White balls. Given a 
chain of tokens appears in a NE window, we 
want to use the probability distribution of these 
tokens to re-paint some of the white balls to 
different colors. A sequence of appropriately 
colored balls would induce an appropriate NE. 
For simplicity, we again focus on PER NE type 
with 2 tokens. The surname token will be 
colored red and first-name blue. We assume that 
the number of PER names in the corpus is N, 
and the rest of tokens is M. Because there are N 
surname and N first-name tokens in the corpus, 
the total number of tokens is M+2N. Hence the 
marginal probability of PER name is 
Pr(PER)=N/(2N+M) . 
 Red  Blue White 
 Format Pr. Format Pr. Format Pr. 
Red aRbR 0 aRbB 1 aRbW 0 
Blue aBbR 
N/(N
+M) aBbB 0 aBbW 
M/(N
+M)
White aWbR 
N/(N
+M) aWbB 0 aWbW 
M/(N
+M)
Note: Red ? Surname; Blue ? First-name; White - Others 
Table 2: Possibility combination of neighboring 
tokens within the corpus for PER 
Table 2 shows the possible relationships 
between the red and blue balls for the PER NE 
type by  considering the grammer that the 
surname must be followed by a first-name in a 
 formal PER. As we only permit the token pair 
for PER to be labeled as a red ball followed by a 
blue ball, the following sequences are not 
possible under our model: (a) a red (or blue) ball 
follows by itself; (b) a red ball follows by white 
ball; and (c) a white ball follows by the blue ball. 
Thus aRbR (a red follows by a red), aRbW, aBbB, 
and aWbB are illegal combinations. 
Given a pair of tokens a and b in the corpus, 
they are labeled as surname |aR| and |bR| times, as 
first-name |aB| and |bB| times, and as non-PER 
|aW| and |bW| times respectively. The expected 
value of a token sequence ab representing a PER 
when a is red and b is blue is: 
 | | | | | || | | | B R BR B R
b a b
a b a
N N
?= ? =  (1) 
The expected value of the cases when the token 
pair ab is not a PER name is the sum of expected 
values of four cases: aBbR, aBbW, aWbR, aWbW (see 
Table 2), which after simplification, is given by: 
 
| | | | | | | | | |
| | | | | | | | | | | || | | |
(| | | |) (| | | |)
(2)
B R B W W R W WR B
B W W R W RB R
B W R W
a b a b a b a b a b
a b a b a ba b
N M N M N M N M
a a b b
N M
= + + +
? ? ??= + + ++ + + +
+ ? += +
 
The ratio between the cases when ab is a PER 
versus when ab is not a PER is: 
| |
| |
C R BR B
ab a b
R B
a b
a b
?? = = ?? ??     (3) 
where | | | |;
(| | | |) (| | | |)
R BR B
a b
B W R W
a b
a a b b
? = ? =+ + ; 
and N M
N
? += . We call RbRacab and ??? ,,  
the rationality values of tokens ab, a and b of 
being a PER, red ball or blue ball respectively. 
On the other hand, the probabilities of a as a 
surname (red ball) and b as a first-name (blue 
ball) are: 
| | | |
,
| | (| | | |) | | (| | | |)
R BR B
a b
R B W B R W
a b
P P
a a a b b b
= =+ + + +  
Thus, ;
1 1
R B
R Ba b
a bR B
a b
P P
P P
? = ? =? ?  (4) 
The form of Equation (4) is similar to the 
concept of odds likelihood O(h), first introduced 
in Duda et al (79) as a generic term to denote 
the ratio of the probability and converse 
probability in the Prospector system, namely: 
 ( ) ( )( )
(- ) 1- ( )
P h P hO h
P h P h
= =  (5) 
Eq. (5) is used in a modified version of the 
Bayes theorem to solve the uncertainty 
reasoning problems. Surprisingly, our approach 
of rationality ? for NE with two tokens can be 
deduced as the product of their odds-likelihood. 
By linking the concept of odds-likelihood and 
rationality, we can compute the probability of a 
sequence of tokens being a specific NE type. 
Since the rationality values of tokens could 
vary from 0 to ?, it may incur overflow or 
underflow during the rationality evaluation. This 
is especially so for unknown tokens where their 
rationality values will be zero. To resolve this 
problem, we construct a piecewise function to 
map the rationality values from the range [0, ?] 
to [?min,?max]. Here we set the parameters 
?min=0.05 and ?max=50, and ensure that most 
rationality values will retain their original values 
after transformation. 
2.3 The Context of NEs 
In addition to identifying the structural 
information within the NEs, it is equally 
important to model the context around the NEs. 
Context is especially pivotal to language such as 
the Chinese or Korean where there is no white 
space and capital characters among the tokens. 
For PER type, the context tokens are likely to be 
person titles and action words. 
 
Figure 1: A NE detection window 
Thus after we have computed the rationality 
values of possible NEs, we enlarge the analysis 
window to cover both the NE candidate and its 
context. As shown in Figure 1, the window 
consists of three components: prefix, suffix and 
the NE candidate. If the NE is at the beginning 
or end of a paragraph, then the corresponding 
Prefix
Boundary of a possible 
Window
Suffix
?? ?   ??   ?? ?? 
NE 
 prefix or suffix is set to void. We can extend the 
rationality computation for an NE to the context 
window by incorporating both the prefix and 
suffix tokens separately. 
2.4 The Overall Procedure 
The overall procedure for estimating the 
likelihood of an NE among a sequence of tokens 
is as follows. 
a) Convert prior probability Pr(e) of each token 
e to rationality ?(e). A token e may have 
multiple Pr(e) values, each is dependent on 
the role token e plays in a possible NE, such 
as the probability of being a surname, 
first-name, prefix, suffix, general token or 
cue-word. 
b) At each cue-word position, compute the 
rationality of a possible NE by considering 
one pair of tokens at a time, and extending to 
the next token on the left or right depending 
on the NE type. The boundaries of PERs are 
extended forward; while that of ORGs and 
LOCs are extended backward. Each extension 
will produce a new NE candidate. The scope 
of the extension is also determined by the 
type of NE. The process terminates when the 
rationality value of the next token falls below 
a minimum threshold. 
c) For all possible NEs, construct the context 
window and compute its final rationality 
value within the context window. 
The process will result in multiple possible NEs, 
with most NEs overlapping with one another. 
3. Multi-Agent Framework for NE 
Confirmation 
3.1 Relationships between possible NEs 
Our greedy approach of identifying all possible 
NEs using the rationality model results in over 
segmentation of NEs. Figure 2 shows a list of 80 
possible NEs detected from a test article in the 
MET-2 test corpus. The number of correct NEs 
in this case is only 13. These possible NEs relate 
to each other in a complex way. The possible 
relationships between them are: 
a. Overlapping: This is the most common case 
when the tokens of multiple NEs overlap each 
other. Examples include ????????? 
and ???????????. They are both 
reasonable ORGs if considered separately. 
However, only one of them can be true. 
b. Repetition: Some possible NEs may repeat 
themselves with same or similar tokens. For 
example, the NE ???????????? 
is similar to ??????????????
???? in different part of the text. It means 
that these NEs have same beliefs and could 
cooperate to enhance each other?s belief. 
Figure 2: All possible NEs identified in a test article 
c. Unification: When the tokens of two NEs are 
adjacent to each other in a sentence, they may 
be unified to become a new NE by combining 
their tokens. For instance, the NEs ???? 
and ???? may be combined to form a new 
NE ????? ?. By the way, not all 
neighboring NEs can be unified because the 
unification must satisfy the syntactic and 
semantic specifications of the language. For 
example, two adjoining PERs cannot be 
unified, while it is possible for LOCs. 
d. Enumerated name list: This is a common 
language construct to present a list of names. 
An example of such construct is: ?????
(??)???, ?????????, and 
???????????????. 
If we knew the relationships between possible 
NEs, we can use this knowledge to modify the 
rationality values of possible NEs. The first 
relationship (overlapping) is of type competition 
while the other three are of type supporting. In a 
competition relationship, the rationality values of 
losing NEs are decremented, whereas in a 
supporting relationship, the rationality of the 
winning NE can be used to reinforce other NEs. 
??????????????????????????
???????? ??  
???????????????? 
??????????????????????? 
????????????????????? 
???????????????????????????
???????????? 
a team
 3.2 Agent-based Reasoning & Negotiation 
There is a need to modify the rationality values 
of possible NEs in order to identify the best 
possible NEs. One way to achieve this is to 
employ a decision tree (Sekine 98) to select the 
best possible candidates. However, it is difficult 
to use the decision tree to handle multiple 
relationships between conflicting NEs, and to 
perform incremental updates of rationality 
values in situations where the number, 
distribution and relationships in possible NEs are 
uncertain. In this work, we adopt a multi-agent 
approach to refine the rationality of possible NEs 
and vote the best potential NEs. 
Agents are software entities that perform some 
operations on behalf of their users or another 
programs with some degree of autonomy, and in 
so doing, employ some knowledge or 
representation of the user?s goals or desires (Don 
et al 96). In our system, we map every possible 
NE detected to an agent, which acts as the 
deputy of the NE and depicts all its attributes. 
Following the approach taken in the DBI system, 
we use the rationality of the NE as the belief, 
denoted by Br(A), of agent A. Agents are 
divided into Teams (Decker & Lesser 95) 
according to their contents and positions in the 
corpus. The division of agents into teams 
facilitates the negotiation of agents? beliefs.  
The negotiation between agents aims to 
eliminate underlying conflicts and uncertainty 
among them. The process of multi-agent 
negotiation is carried out as follows. 
a. We identify agents involved in an unification 
relationship. These agents will be unified if 
the constraints of unification are fulfilled. The 
new agents would inherit the evidences, 
including the rationality values, of its child 
agents. 
b. We divide the resulting agents into teams. 
Agents with overlapping tokens will be 
grouped into same teams, while independent 
agents will be assigned to different teams. 
c. We perform negotiation between agents based 
on the type of their relationship. For agents 
that are in competition relationship (i.e. those 
overlapping agents within the same team), we 
select the agent with the maximal belief (said 
ai) as the winner, and decrement the beliefs of 
the rest of Nt agents in the same team by ?(ai), 
i.e. 
  Br(aj) = Br(aj) - ?(ai), for j=1,.. Nt, and j?i 
 For agents involved in the supporting 
relations, we again select the agent with the 
maximal belief (of say ak) as the winner, but 
increment the rest of agents in the same set Sk 
by ?(ak), i.e. 
  Br(aj) = Br(aj) + ?(ak), for all j in Sk & j?k 
d. Repeat step c until the pre-defined rounds of 
negotiations have been reached. 
In order to ensure fairness in the negotiation 
process, we limit the amount of belief 
adjustment, ?(ai), during each round of 
negotiation. If the desired rounds of negotiation 
is NR, then the amount of adjustment in each 
round should be limited to ?(ai)/NR. NR should 
be set to allow all agents to have a fair chance to 
participate in the negotiation process. Here we 
set NR to 10. 
At the end of negotiation, only agents whose 
beliefs are greater than the threshold are selected. 
Figure 3 shows the resulting set of NEs derived 
from the list given in Figure 2. 
 
 
 
 
 
 
 
Fig. 3: NEs after agents-based modification 
4. The Overall Process of NE Recognition 
Since there is no white space between words in 
Chinese, the first essential step is to perform 
preliminary segmentation. Here, we adopt a 
greedy approach of generating all possible 
segmentation from the input text by performing 
the dictionary-based look-up using a common 
word dictionary. The common word dictionary is 
generated from the PKU corpus (Yu 99) (see 
Section 5.1). 
Second, we compute the rationality value of 
each token in the context of being a keyword, 
general word, or as boundary (prefix or suffix) 
of a specific NE type. 
????????????????????????
?????????? ?? ??????????
???????????????????????
???????????????????????
?????????????? ????????
?????????????????????
 Third, we identify all possible NE cue-words 
and use them as seeds of NE candidates. We 
construct all possible NEs from the cue-word 
positions through boundary extension and 
context inclusion. 
Forth, we modify the rationality values of all 
possible NEs using the agent-based negotiation 
methodology. The conflicts between possible 
NEs will disappear. 
Fifth, we select NEs with the labels of its 
corresponding seed if their rationality values are 
above a predefined limit ?. The value ? affects 
the balance between recall and precision. 
5. Experimental Results and Discussions 
5.1 The Datasets Used in Our Experiments 
We use a number of openly available datasets 
for our training and testing, including the 
PKU-corpus (Yu 99), Hownet (Dong & Dong 
00), MET2 Chinese resources (Chinchor 02), 
and two name lists (for foreign and ORG names) 
collected from the web by using a bootstrapping 
approach. The PKU is a manually tagged corpus 
containing one-month of news report from 
China?s People Daily. It uses over 30 POS tags 
including separate tags for surname and 
first-name. It contains about 37,000 sentences 
with over 106 tokens. From these resources, we 
generate the following dictionaries and statistics.  
a. We use the PKU corpus to build a common 
word dictionary by removing all words that 
are tagged as NE. The resulting dictionary 
contains 37,025 common words. 
b. From the PKU corpus, we compute each 
token?s distribution information based on its 
POS tags, and if it is an NE, its NE type and 
its role with respect to the NE. Altogether, we 
obtain the distribution information of about 
37,000 different tokens. 
c. We maintain a list of LOCs found in the 
MET-2 test corpus. We do not maintain the 
PER and ORG lists, because their 
re-occurrence probabilities are low. 
d. We supplement the distribution information 
derived in step (b) by incorporating tokens 
obtained from other resources stated above. 
The resources we derived are available for down 
loading at http://www.pris.nus.edu.sg/ie.html 
5.2 The Experiment and Results 
We test our resulting model on the MET-2 test 
corpus. Table 3 tabulates the results of our 
system in terms of recall (Rc), precision (Pr) and 
F1 measures. In order to demonstrate the 
effectiveness of our approach, we perform the 
tests under 3 different test configurations. 
a. We perform the baseline test by simply 
performing name-dictionary look-up. Notice 
that we do not use PER dictionary, and hence 
the performance under PER is left blank (*). 
b. We extract all possible NEs by using only the 
rationality-based approach where the 
threshold ? is set to 1.1. If there are conflicts 
between possible NEs, we simply select the 
NE with the maximal rationality. 
c. We employ the agent-based modification in 
conjunction with the rationality-based 
approach to select the best possible NEs. 
For comparison purpose, we also list in Table 3 
the corresponding results reported in Yu et al 
(98) and Chen et al (98) for the MUC-7 tests. 
Type NC NP NW NM NS Rc Pr F1
Base- 
line test 
(a) 
ORG
PER 
LOC 
79 3 0 295 0 
*   * * * * 
363 84 0 303 26 
21 98 35.0
*  * * 
54 86 66.0
Config 
(b) 
ORG
PER 
LOC 
309 5 28 35 47 
154 2 7 11 87 
618 0 29 103 112 
83 79 81.0
89 62 73.4
82 81 81.7
Config 
(c) 
ORG
PER 
LOC 
356 2 5 14 21 
167 1 2 4 9 
703 0 18 29 52 
95 93 93.7
96 93 94.7
94 91 92.3
Results 
of Chen 
et (98) 
ORG
PER 
LOC 
393 0 7 77 44 
159 0 0 25 56 
583 0 65 102 194 
78 83 81.3
91 74 81.6
78 69 73.2
Results 
of Yu et 
al. (98) 
ORG
PER 
LOC 
331 0 14 32 25 
160 0 7 7 74 
682 0 1 67 83 
88 89 88.5
92 66 76.7
91 89 0.0
where Pr = (NC + 0.5*NP)/(NC + NW + NP + NS); 
 Rc = (NC + 0.5*NP)/(NC + NW + NP + NM); 
 F1 = 2*Pr*Rc/(Pr+Rc). 
 and NC gives the number of NEs correctly recognized; 
   NP denotes the number of NEs partially recognized; 
   NW gives the number of NEs incorrectly recognized; 
   NM denotes the number of NEs missed; and finally 
   NS gives the number of NEs found by the system but not 
in the tagged list. 
Table 3: Results of MET2 under different configurations 
Table 3 shows that as we apply the rationality 
model (Config. b) followed by multi-agent 
framework (Config. c), the performance of the 
system improves steadily until it reaches a high 
performance of over 92% in F1 value. In fact 
 Config c results in significant improvements 
over Conig b in both precision and recall forall 
NE types. This shows that the agent-based 
modification could significantly reduce spurious 
and missing NEs. The performance of our 
overall system is significantly better than both 
reported systems as listed in Table 3. 
To demonstrate the effectiveness of our 
approach on general web-based documents, we 
perform another informal test to recognize NEs 
on the 100 randomly collected headline news 
articles from the well-known Chinese web sites 
(www.sina.com.cn, www.sohu.com, www. 
zaobao.com, www.Chinese times.com). The 
topics covered in these articles ranging from 
politic, economic, society to sports. The 
informal test shows that our approach could 
perform well on general web-based articles with 
F1 measures of over 90%. 
6. Conclusion 
Chinese NE recognition is a difficult problem 
because of the uncertainty in word segmentation. 
Many existing techniques that require 
knowledge of word segmentation, and syntactic 
and semantic tagging of text cannot be applied. 
In this paper, we propose a new approach of 
employing a rationality model in a multi-agent 
framework. We employ a greedy strategy and 
use the NE rationality measures to detect all 
possible NEs in the text. We then treat the 
process of selecting the best possible NEs as the 
multi-agent negotiation problem. The resulting 
system is robust and is able to handle different 
NE models. Our test on the MET-2 test corpus 
indicates that we could achieve high F1 values of 
above 92% on all NE types. 
We plan to further test our system on a 
large-scale test corpus. We will refine our 
techniques on a wide variety of text corpuses, 
and apply the bootstrapping technique to tackle 
the data sparseness problem. Finally, we will 
extend our research to perform relation and 
information extraction in multilingual text. 
References 
Bikel D.M., Schwartz R. & Weischedel R.M. (1999) 
An Algorithm that Learns What?s in a Name. 
Machine Learning, 34(1-3), 211-231 
Borthwick A. (1999) A Maximum Entropy Approach 
to Named Entity Recognition. Ph.D. Thesis, New 
York Univ.  
Chen H. H., Ding Y. W. Tsai S.C. & Bian, G.W. (1998) 
Description of the NTU System used for MET-2. In 
MUC-7 Proc. 
Chinchor N. A. (2002), http://www.itl.nist.gov/iaui/ 
894.02/related_projects/muc/. 
Cucerzan S. & Yarowsky D. D. (1999) Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. In Proc 
of 1999 Joint SIGDAT Conference on Empirical 
Methods in NLP & Very Large Corpora, 90-99. 
Decker K., & Lesser V. (1995) Designing a Family of 
Coordination Algorithm, In Proc Of 1st Int?l Conf. 
on Multiagent Sys, 73-80, Menlo Park, CA, AAAI 
Press. 
Don Gilbert, Manny Aparicio, et al(1996) White 
paper on intelligent agents (IBM), http://activist. 
gpl.ibm.com:81/WhitePaper/ptc2.htm. 
Dong Z.D. & Dong Q. (2000) HowNet, available at 
http://www.keenage.com/zhiwang/e_zhiwang.html. 
Duda R., Gaschnig J., & Hart P. (1979) Model design 
in the prospector consultant system for mineral 
exploration. In Expert systems in the micro 
-electronic age, Michie D. Ed., Edinburgh Univ. 
Press, Edinburgh, England. 
Isozaki H. (2001) Japanese Named Entity 
Recognition Based on a Simple Rule Generator and 
Decision Tree Learning, In ACL?01, 306-313. 
Lin D. (1998) Using collocation statistics in 
information extraction. In MUC-7 Proc. 
Luo Z.Y. & Song R. (2001) An Integrated and Fast 
Approach to Chinese Proper Name Recognition in 
Chinese Word Segmentation, In Proc. of Int?l 
Chinese Computing Conf., Singapore 323-328. 
Palmer D. D. (1997) A Trainable Rule-Based 
Algorithm for Word Segmentation, In Proc of 35th 
of ACL & 8th conf. of EACL, 321-328. 
Sproat R., Shih C., et al(1996) A Stochastic 
Finite-state Word Segmentation Algorithm for 
Chinese. Computational Linguistics, 22(3), 
377-404. 
Yu S.H., Bai S.H. & Wu P. (1998) Description of the 
Kent Ridge Digital Labs System Used For MUC-7, 
1998, In MUC-7 Proc. 
Yu S.W. (1999) The Specification and Manual of 
Chinese Word Segmentation and Part of Speech 
Tagging. http:// www.icl.pku.edu.cn/ 
Sekine S. (1998) NYU: Description of The Japanese 
NE System Used for MET-2, in MUC-7 Proc. 
Cascading Use of Soft and Hard Matching Pattern Rules for Weakly 
Supervised Information Extraction 
Jing Xiao 
School of Computing, 
National University of 
Singapore, 117543 
xiaojing@comp.nus.edu.sg 
Tat-Seng Chua 
School of Computing, 
National University of 
Singapore, 117543 
chuats@comp.nus.edu.sg 
Hang Cui 
School of Computing, 
National University of 
Singapore, 117543 
cuihang@comp.nus.edu.sg 
 
Abstract 
Current rule induction techniques based on hard 
matching (i.e., strict slot-by-slot matching) tend to 
fare poorly in extracting information from natural 
language texts, which often exhibit great 
variations. The reason is that hard matching 
techniques result in relatively high precision but 
low recall. To tackle this problem, we take 
advantage of the newly proposed soft pattern rules 
which offer high recall through the use of 
probabilistic matching. We propose a 
bootstrapping framework in which soft and hard 
matching pattern rules are combined in a cascading 
manner to realize a weakly supervised rule 
induction scheme. The system starts with a small 
set of hand-tagged instances. At each iteration, we 
first generate soft pattern rules and utilize them to 
tag new training instances automatically. We then 
apply hard pattern rule induction on the overall 
tagged data to generate more precise rules, which 
are used to tag the data again. The process can be 
repeated until satisfactory results are obtained. Our 
experimental results show that our bootstrapping 
scheme with two cascaded learners approaches the 
performance of a fully supervised information 
extraction system while using much fewer hand-
tagged instances. 
1 Introduction 
Information Extraction (IE) aims to extract specific 
information items of interest from free or semi-
structured texts, and pattern rule induction is one 
of the most common techniques for IE tasks 
(Muslea, 1999). There has been much work in 
learning extraction pattern rules from tagged data, 
e.g., AutoSlog-TS (Riloff, 1996), WHISK 
(Soderland, 1999) and LP2 (Ciravegna, 2001). In a 
typical IE system, generalized pattern rules are 
usually represented as regular expressions and 
matched against test instances through exact 
matching for each slot, which we call hard 
matching. Utilizing hard matching pattern rules 
could obtain precise results from test instances. 
However, the approach is problematic in dealing 
with natural language text, such as news articles, 
which often exhibits great variations in both lexical 
and syntactic constructions. For instance, in the 
terrorism domain, given a common rule ?<victim> 
be kidnapped by ??, hard matching pattern rules 
cannot pick up the instance ?<victim> , kidnapped 
by ?? due to the mismatch in only one token. 
Such hard matching techniques often result in low 
recall. To achieve flexibility in pattern matching 
for natural language texts, soft matching pattern 
rules have been proposed for question answering 
(Cui, et al, 2004). Soft pattern rules match test 
instances using a probabilistic model to better 
accommodate variations in expressions. However, 
differing from the question answering problem, the 
IE task needs to precisely locate the boundaries of 
the extracted slots. As such, soft pattern rules may 
not meet the precision requirement of the task. 
   In this paper, we aim to minimize the number of 
hand-tagged training instances needed to start the 
learning process by adopting a bootstrapping 
strategy such as that proposed in Riloff and Jones 
(1999). In contrast to the existing work, we 
propose a weakly supervised IE framework which 
takes advantages of both soft and hard matching 
pattern rules in both the training and test phases. 
Starting with only a small set of hand-tagged 
training instances, we first generate a set of soft 
pattern rules and utilize them to tag more training 
instances. Next, we apply a hard matching pattern 
rule induction algorithm, GRID (Xiao, et al, 
2003), over both manually and automatically 
tagged instances to generalize precise hard-
matching rules. These hard pattern rules are 
utilized to tag training instances for soft pattern 
rule generation in the next iteration. The process 
runs iteratively till the termination criteria are met. 
At the end of the training process, we obtain two 
sets of pattern rules, namely the hard and soft 
pattern rules. During the test phase, both sets of 
pattern rules are used in a cascaded way, with hard 
pattern rules followed by soft pattern rules, to 
extract target slots from new documents. We have 
conducted two experiments on both semi-
structured and free texts to demonstrate the 
effectiveness of our method. The experimental 
results show that the bootstrapping scheme with 
two cascaded pattern rule learners could achieve a  
performance close to that obtained by fully 
supervised learning while using only 5~10% of the 
hand-tagged data.  
   The main contribution of our work is in 
incorporating soft matching pattern rules in the 
bootstrapping framework. Rooted in instance-
based learning, soft pattern rules are more 
appropriate in dealing with sparse data (Cui, et al, 
2004), and thus can be learned from a relatively 
small number of training instances to start the 
bootstrapping process. Moreover, in test phase, 
soft pattern rules are expected to cover more 
unseen instances, which are likely to be missed by 
hard-matching rules, with its flexible matching 
mechanism. 
   The rest of the paper is organized as follows. 
Section 2 presents the design of our system. 
Section 3 describes the details of data preparation, 
soft pattern matching, hard pattern rule induction 
and the application of the two pattern rules on new 
test instances. Section 4 presents the experimental 
evaluation. We review other work in Section 5 and 
conclude the paper in Section 6. 
2 System Design 
Figure 1 shows the overall system architecture of 
our IE system. The training phase of the system is 
carried out as follows: 
(a) We take a small set of hand-tagged instances 
(seed instances) provided by the user. 
(b) We generate soft pattern rules using the seed 
instances, and denote the soft pattern rules as SPi. 
(c) We apply the learned soft pattern rules (SPi) to 
automatically tag unannotated data. We employ a 
simple cut-off strategy that keeps only the highly-
ranked instances by the soft pattern rules. 
(d) We generate hard pattern rules using GRID 
over the automatically tagged instances and seed 
instances. The resulting hard pattern rules are 
denoted as HPi.  
(e) If the termination condition is satisfied, the 
process ends with a set of learned soft and hard 
pattern rules. Otherwise, the hard pattern rules HPi 
are used to tag the training data again.  We start a 
new round of training from Step (b) using the 
newly tagged training instances and seed instances. 
   In the test phase, we apply both the hard and soft 
pattern rules to match against test instances. 
Specifically, soft matching pattern rules would 
assign a probabilistic score to an instance that is 
not matched by any of the hard matching pattern 
rules. Only those fields that are matched by the 
hard pattern rules or have high scores in soft 
pattern matching will be extracted. 
 
 
 
  Figure 1: Architecture of our IE system 
 
3 Soft and Hard Pattern Rule Learning 
3.1 Data Preparation 
Before pattern rule learning commences, we pre-
process the training and test instances by using a 
natural language chunker 1  to perform part-of-
speech (PoS) tagging and chunking. We also use a 
rule-based named entity tagger (Chua and Liu, 
2002) to capture semantic entities. Given a tagged 
instance, we consider the left and right k chunks 
around the tagged slot as the context:  
<c-k>?<c-2><c-1>tagged_slot<c+1><c+2>?<c+k>       
Here <ci> {i=-k to +k} represents the contextual 
chunks (or slots) of the tagged slot, where k is the 
number of contextual slots considered. <ci> can be 
of various feature types, namely words, 
punctuations, chunking tags like verb and noun 
phrases, or semantic classes. We perform selective 
substitution to generalize the specific terms in each 
slot so as to make the learned pattern rules general 
enough to be applied to other instances. Table 1 
shows the substitution heuristics employed in our 
system with examples.  
(1) 
   Figure 2 gives five examples of original training 
instances for ?starting time? in the seminar 
announcement domain. We substitute the more 
                                                     
1  We use NLProcessor, a commercial parser from 
Infogistics Ltd. http://www.infogistics.com/. 
general syntactic or semantic classes for the lexical 
tokens according to the heuristics in Table 1. 
  
Tokens Substitution Examples 
9 types of 
named 
entities 
NP_Person, 
NP_Location, 
NP_Organization, 
NP_Date,  
NP_Day, 
NP_Time, 
NP_Percentage, 
NP_Money, 
NP_Number. 
?Friday??NP_Day 
?Feb.27??NP_Date 
Noun 
Phrase NP_HeadNoun 
?the seminar? 
?NP_seminar 
Verb Phrase 
(passive or 
active) 
VPpas_RootVerb, 
VPact_RootVerb 
?will speak? 
?VPact_speak, 
?will be held? 
?VPpas_hold 
Preposition 
Phrase 
PP 
?in civilian clothes? 
? PP 
Adjectival 
and 
adverbial 
modifiers 
To be deleted  
All other 
words and 
punctuations 
No substitution ?Time?, ?at?, ?by?, etc. are unchanged. 
Table 1: Substitution heuristics 
 
 
 
 
 
 
 
3.2 Soft Matching Pattern Rules 
Soft pattern rules have been successfully applied to 
text mining (Nahm and Mooney, 2001) and 
question answering (Cui, et al, 2004). We employ 
a variation of the soft pattern rules generation and 
matching method presented in Cui, et al (2004). 
We expect soft pattern rules to offer higher 
coverage in matching against a variety of instances 
in both the training and test phases. 
   For each type of tagged slot (Slot0) such as stime 
in Figure 2, we accumulate all the tagged instances 
and align them according to the positions of Slot0. 
As a result, we obtain a virtual vector Pa 
representing the contextual soft pattern rule as: 
<Slot-k, ? , Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotk: 
Pa>                                                                      (2) 
where Sloti is a vector of tokens occurring in that 
slot with their probabilities of occurrence: 
<(tokeni1, weighti1), (tokeni2, weighti2) ?.(tokenim, 
weightim): Sloti>                                                   (3) 
   Here, tokenij denotes any word, punctuation, 
syntactic or semantic tag contained in Sloti, and 
weightij gives the proportion of occurrences of the 
jth token to the ith slot. Figure 3 shows the 
generated soft pattern rules for the examples given 
in Figure 2. 
 
 
 
 
 
 
 
 
 
 
 
 
 
(1) Training instances: 
Time : <stime> NP_Time </stime> 
VPact_be at <stime> NP_Time </stime>  
NP_Day , NP_Date <stime> NP_Time </stime> - NP_Time 
VPact_be at <stime> NP_Time </stime> , NP_Day , NP_Date 
Time : <stime> NP_Time </stime> - NP_Time 
 
(2) Soft pattern rules based on the instances: 
?? <Slot-2>          <Slot-1>            <Slot0>           <Slot1> ?...
 Time 0.4 
VPact_be 0.4 
, 0.2 
 : 0.4 
at 0.4 
NP_Date  0.2 
 
NP_Time  1 - 0.67 
, 0.33 
 
Figure 3: An excerpt of soft pattern rules 
What results from the generalization process is a 
virtual vector Pa representing the soft pattern rule. 
The soft pattern vector Pa is then used to compute 
the degree of match for the unseen instances. The 
unseen instances are first pre-processed with the 
identical procedures as outlined in Section 3.1. 
Using the same window size k, the token fragment 
S surrounding the potential slot is derived: 
(1) Original instances for slot <stime>: 
Time : <stime> 2:30 PM </stime> 
? will be at <stime> 3 pm </stime> ? 
?Friday, February 17 <stime> 12:00pm </stime> - 1:00pm
    ? will be at <stime> 4pm </stime> , Monday, Feb. 27 ? 
Time: <stime> 12:00 PM </stime> - 1:30 PM 
(2) Substituted instances: 
    Time : <stime> NP_Time </stime> 
    VPact_be at <stime> NP_Time </stime>  
    NP_Day , NP_Date <stime> NP_Time </stime> - NP_Time 
    VPact_be at <stime> NP_Time </stime> , NP_Day , NP_Date 
    Time : <stime> NP_Time </stime> - NP_Time 
<token-k,?, token-2, token-1, Potential_Slot, token1, 
token2, ?, tokenk: S>                                           (4) 
   The degree of match for the unseen instance 
against the soft pattern rules is measured by the 
similarity between the vector S and the virtual soft 
pattern vector Pa. In particular, the match degree is 
the combination of the individual slot content 
similarities and the fidelity degree of slot 
sequences measured by a bi-gram model (Cui, et 
al., 2004).  
Figure 2: Illustration of generalizing instances 
   When applying the soft pattern rules to 
automatically tag training instances, for each 
potential slot, we assign a target tag whose soft 
pattern rule gives the highest score beyond a pre-
defined threshold. 
3.3 Hard Pattern Rule Induction 
We employ a pattern rule induction algorithm 
called GRID (Xiao, et al, 2003) to generalize the 
hard pattern rules over all instances hand-tagged 
by users and automatically annotated by soft 
pattern rules. GRID is a supervised covering 
algorithm. It uses chunks as contextual slots and 
considers a context size of k slots around the 
tagged item as definition in Equation (1).   
   Given the cluster of training instances for a 
specific slot type, GRID aligns all the instances 
according to the central slot (Slot0) as is done in 
soft pattern rules. For each context slot, we store 
all possible representations of slot units as listed in 
Table 1 at the levels of lexical, syntactic and 
semantic simultaneously. Thus, we obtain a global 
context feature representation for the whole 
training corpus as shown in Figure 4. GRID 
records the occurrences of the common slot 
features at a specific position as eij (i = -k, ? ,  -1, 
0, 1, ?, k; jth feature for Sloti).  
 
inst.1: Slot-k, ?, Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotk 
inst.2: Slot-k, ?, Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotk 
.         .      ?      .         .          .         .        .      ?    . 
.         .      ?      .         .          .         .        .      ?    . 
.         .      ?      .         .          .         .        .      ?    . 
inst.h: Slot-k, ?, Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotk 
 
 
 
 
   GRID generates a pattern rule rk(f) by adding slot 
features into the feature set f. The quality of rk(f) is 
determined not only by its coverage in the positive 
training set but also by the number of instances in 
the negative set that it covers which would be 
regarded as errors. We define the remaining 
instances which are not annotated by human and 
soft pattern rules as negative instances.  
   We use a modified Laplacian expected error 
(Soderland, 1999) to define the quality of the rule 
as follows: 
17.0
1
))((
21 +?++
+=
kkk
k
k ppn
n
frLaplacian  
where pk1 denotes the number of instances covered 
by rule rk(f) in the manually annotated set, and pk2 
denotes the number of instances covered by the 
rule rk(f) in the automatically annotated set. nk is 
the number of negative examples or errors covered 
by the rule. We consider all the manually 
annotated instances as correctly tagged and thus 
we put more weight on them than on the 
automatically annotated data set. 
   Instead of generalizing a rule from a specific 
instance as is done in most existing pattern rule 
induction algorithms, GRID examines the global 
feature distribution on the whole set of training 
examples in order to make better decision on rule 
induction. Each time, GRID selects top w features 
(in terms of the eij values) and selects slot feature fij 
with the minimum Laplacian value of the rule 
(rk(f?fij)) according to Equation (5) to induce 
pattern rules (Xiao, et  al., 2003). 
We use GRID to generate rules that cover all 
seed instances and discard some rules generated 
from the automatically tagged instances whose 
Laplacian value is greater than a preset threshold.  
3.4 Cascading Matching of Hard and Soft 
Pattern Rules 
After we have obtained the set of hard pattern rules 
and the set of soft pattern rules through the 
bootstrapping rule induction process, we apply 
both sets of rules in a cascaded way to assign 
appropriate tag to potential slots in new instances. 
The tag assigned to the given test instance t is 
selected by: 
1) tagg   matched by GRID ruleg; 
2) If not matched by any GRID rule, 
tagi  ?>
?
)|Pr(maxarg i
PaPa
Pat
i
   We apply the high-precision hard pattern rules 
generated by GRID first. In this case, we assign 
tagg to the instance if it matches ruleg. In order to 
increase the coverage of the hard pattern rules, we 
allow up to one shift in the context vectors of new 
test instances when matching the instances against 
the hard pattern rules.  
   For the remaining test instances that are not 
matched by any of the hard pattern rules, we score 
them using the soft pattern rules. A test instance is 
assigned tagi if it has the highest conditional 
probability of having t given the soft pattern rule i 
(represented by vector Pai) which is greater than a 
pre-defined threshold ? among all the soft pattern 
rules.  
4 Evaluation 
To verify the generality and effectiveness of our 
bootstrapping framework, we have conducted two 
experiments on free and semi-structured texts. In 
our supervised IE system using GRID (Xiao, et al, 
2003), we had done some trial experiments to 
examine the effect of varying the different context 
length k, and found the IE performance became 
stable when the context length reached 4. As such, 
we set the context length k to 4 for all subsequent 
experiments.  
4.1 Results on free text corpus 
The first evaluation was conducted on the 
terrorism domain using the MUC-4 free text 
corpus (MUC-4, 1992). We employed the same 
evaluation measures as that in (Riloff, 1996; Xiao, 
et al, 2003). The target extracted slots were 
?perpetrator? (Perp.), ?victim? (Vic.) and ?target? 
(Tar.). We varied the number of the human-
annotated instances from the 772 relevant 
Pos. 
e-kj ? e-2j e-1j e0j e1j e2j ? ekj? ? ? ? 
Figure 4: Global distribution of positive instances
(5) 
documents set (the standard training documents for 
MUC-4 plus TST1 and TST2) used in supervised 
IE learning. The manual annotation was guided by 
the associated answer keys given in the MUC-4 
corpus. During testing, we used the 100 texts 
comprising 25 relevant and 25 irrelevant texts from 
the TST3 test set, and 25 relevant and 25 irrelevant 
texts from the TST4 test set.  
   Following the procedure discussed in Section 2, 
we repeated the automated annotation process 
several times (i ?1 in Figure 1). To examine the 
variation of performance along with the changing 
of the number of iterations, we plotted the average 
F1 measures of the three target slots against the 
iteration number (see Figure 5). We also varied the 
number of manually tagged instances that were 
utilized as seed instances for starting the 
bootstrapping process. As can be seen in Figure 5, 
the results improved as the number of iterations 
increased. The system achieved a steady 
performance when the number of iterations 
reached four. Accordingly in the next experiments, 
we considered the system?s performance based on 
four bootstrapping iterations. 
40
45
50
55
60
1 2 3 4 5 6 7 8 9 10
Iteration
A
ve
ra
g
e 
F
1 
m
ea
su
re
5% manually annotated instances
10% manually annotated instances
20% manually annotated instances
 
        Figure 5: Effect of the number of iterations 
   Table 2 shows the performance of the system on 
the test data in terms of F1-measure (with 
recall/precision value in the brackets) using various 
amounts of manually tagged data after four 
iterations. To demonstrate the effectiveness of the 
combination of hard and soft pattern rules, we also 
ran four iterations using only soft pattern rules (SP) 
and another four with only GRID rules.    
   From Table 2, we can draw the following 
conclusions: 
(a) The cascaded learner by combining SP and 
GRID outperforms the learner SP or GRID alone. 
The soft pattern learner (SP) alone cannot achieve 
good precision while the hard pattern learner 
(GRID) alone cannot achieve high recall with a 
small set of hand-annotated instances. 
   
 Perp. Vic. Tar. 
5%(SP) 36 (42/32) 
45 
(49/42) 
42 
(47/38) 
5%(GRID) 34 (35/33) 
44 
(40/49) 
39 
(36/43) 
5%(SP+GRID) 47 (49/45) 
58 
(59/57) 
50 
(50/50) 
10%(SP) 38 (45/33) 
46 
(51/42) 
45 
(49/42) 
10%(GRID) 37 (39/35) 
46 
(41/52) 
44 
(41/47) 
10%(SP+GRID) 50 (53/47) 
61 
(63/59) 
53 
(52/54) 
20%(SP) 40 (46/35) 
48 
(54/43) 
47 
(50/44) 
20%(GRID) 40 (41/39) 
47 
(44/50) 
47 
(45/49) 
20%(SP+GRID) 51 (52/50) 
62 
(63/61) 
54 
(55/53) 
AutoSlog-TS 38 (53/30) 
48 
(62/39) 
47 
(58/39) 
supervised(GRID) 52 
(48/57) 
62 
(58/67) 
56 
(51/62) 
           
 
Results presented in terms of F1(recall/precision). 
               Table 2: Results on free text domain 
(b) Compared with another weakly supervised IE 
system in the same domain, AutoSlog-TS (Riloff, 
1996), our cascaded learner outperforms it with the 
use of only 5% of the manually tagged instances. 
(c) As the percentage of the hand-annotated 
instances increases from 5% to 20%, the 
performance of the cascaded learner (SP+GRID) 
increases steadily, indicating that the bootstrapping 
process is stable and consistent. 
(d) With 20% of hand-tagged training instances, 
the performance of the cascaded learner 
approaches that of the fully supervised IE tagger. 
When more manually tagged instances (>20%) are 
used, the performance of the cascaded learner 
becomes steady. 
(e) Looking at the instances automatically tagged 
by the soft pattern rules, we found that about 75% 
instances are correctly annotated in the first and 
second iteration. The percentage of correctly 
tagged instances by soft pattern rules increases to 
90% when the bootstrapping process runs for four 
times. The percentage increase verifies that our 
automated annotation can provide relatively 
accurate training instances for later rule induction. 
   Nevertheless, our system missed some cases 
which needed deeper NLP analysis. For example, 
given a test sentence ?THEY ARE THE TOP 
MILITARY AND POLITICAL FIGURES IN 
ALFREDO CRISTIANI'S ADMINISTRATION.?, the 
system could not identify ?ALFREDO CRISTIAN?S 
ADMINISTRATION? as the ?perpetrator?. If we 
could associate the previously found ?perpetrator? 
(maybe located far away) to ?they?, then we might 
be able to infer that the ?ALFREDO CRISTIAN?S 
ADMINISTRATION? is the ?perpetrator? too. 
4.2 Results on semi-structured corpus 
The second experiment was conducted on semi-
structured text documents. We used the CMU 
seminar announcements2 for the evaluation. The IE 
task for this domain is to extract the entities of 
?speaker? (SP), ?location? (LOC), ?starting time? 
(ST), and ?ending time? (ET) from a seminar 
announcement. There were 485 seminar 
announcements. In the supervised IE experiments, 
we made five runs and in each run we used one 
half for training and the other half for testing. 
Similarly, to evaluate our weakly supervised 
learning framework, we did five trials as well. In 
each run, we varied the percentage of manually 
annotated instances for training in the supervised 
experiments. Table 3 shows the performance (the 
average F1 measure and recall/precision for five 
runs) of the system with different percentage of 
manually tagged instances used to start the 
training. We also compare the performances 
between the single learners and the cascaded 
learner. All results are based on four bootstrapping 
iterations. 
 SP LOC ST ET 
5%(SP) 70 (74/66) 
65 
(70/61) 
94 
(95/93) 
90 
(93/88) 
5%(GRID) 68 (65/72) 
61 
(59/64) 
93 
(91/94) 
89 
(86/92) 
5%(SP+GRID) 82 (83/81) 
73 
(74/72) 
98 
(98/98) 
94 
(96/92) 
10%(SP) 72 (75/70) 
68 
(72/64) 
96 
(96/95) 
93 
(94/92) 
10%(GRID) 72 (67/77) 
67 
(63/72) 
95 
(94/96) 
93 
(91/96) 
10%(SP+GRID) 84 (84/83) 
75 
(75/74) 
99 
(99/99) 
95 
(97/94) 
20%(SP) 75  (77/74) 
71 
(75/67) 
97 
(97/97) 
95 
(96/95) 
20%(GRID) 75 (69/82) 
71 
(66/77) 
97 
(95/99) 
95 
(94/96) 
20%(SP+GRID) 85 (85/85) 
76 
(76/75) 
99 
(99/99) 
96 
(97/95) 
supervised 
(GRID) 
86 
(84/88) 
76 
(73/80) 
99 
(99/100) 
96 
(95/97)
Results presented in terms of F1(recall/precision).         
         Table 3: Results on semi-structured data 
     
    From Table 3, we make the following 
observations: 
(a) The cascaded learner with two pattern learners 
significantly outperforms the learner SP or GRID 
alone as in the case of free text corpus. With 10% 
of hand-tagged instances, the cascaded learner 
(SP+GRID) approaches the performance of the 
fully supervised IE tagger. Also the performance of 
the cascaded learner increases steadily when the 
number of hand-tagged instances increases from 
5% to 20%. 
(b) With more hand-annotated instances (>20%), 
the performance of the bootstrapping system with 
the cascading use of SP and GRID becomes stable 
and consistent. 
                                                     
2 http://www.isi.edu/info-agents/RISE/repository.html 
(c) Soft pattern rules tag 90% of the instances 
correctly, as we found out in our random checks. 
   The lower performance of our system on the 
?location? slot is mainly due to the use of a general 
named entity recognizer which is good at 
identifying common locations such as cities, 
mountains etc. In seminar announcements, many 
locations are room numbers such as ?WeH 8220?; 
thus, we missed out some seminar venues. 
5 Related Work 
Many hard pattern rule inductive learning systems 
have been developed for information extraction 
from free texts or semi-structured texts. 
Specifically, AutoSlog-TS (Riloff, 1996) generates 
extraction patterns using annotated text and a set of 
heuristic rules and it eliminates the dependency on 
tagged text and only requires the pre-classified 
texts as input. WHISK (Soderland, 1999) induces 
multi-slot rules from a training corpus top-down. It 
is designed to handle text styles ranging from 
highly structured text to free text. WHISK 
performs rule induction starting from a randomly 
selected seed instance. (LP)2 (Ciravegna, 2001) is a 
covering algorithm for adaptive IE systems that 
induces symbolic rules. In (LP)2, training is 
performed in two steps: first, a set of tagging rules 
is learned to identify the boundaries of slots; next, 
additional rules are induced to correct mistakes in 
the first step of tagging. In contrast to their work, 
GRID utilizes global feature distribution to induce 
pattern rules and uses chunk as the context unit. 
Nahm and Mooney (2001) proposed the learning 
of soft matching rules from texts by combining 
rule-based and instance-based learning. Words in 
each slot are generalized by traditional rule 
induction techniques and test instances are 
matched to the rules by their cosine similarities. 
The learning of soft pattern rules in this paper 
augments the soft matching method advocated by 
Nahm and Mooney (2001) by combining lexical 
tokens alongside syntactic and semantic features 
and adopting a probabilistic framework that 
combines slot content and sequential fidelity in 
computing the degree of pattern match. 
   The bootstrapping scheme using the co-training 
(Blum and Mitchell, 1998) technique has been 
widely explored for IE tasks in recent years. 
Collins and Singer (1999) presented several 
techniques using co-training schemes for Named 
Entity (NE) extraction seeded by a small set of 
manually crafted NE rules. Riloff and Jones (1999) 
presented a multi-level bootstrapping algorithm 
that generates both the semantic lexicon and 
extraction patterns simultaneously. Yangarber 
(2003) proposed a counter-training approach to 
provide natural stopping criteria for unsupervised 
learning. 
   Our framework of combining two pattern 
learners is close to Niu, et al (2003) in which two 
successive learners are used to learn named entities 
classifiers starting from a small set of concept-
based seed words. The bootstrapping procedure is 
implemented as training a decision list and an 
HMM classifier sequentially. The HMM classifier 
uses the training corpus automatically, tagged by 
the first learner, i.e., the decision list learner. Our 
work differs from Niu, et al (2003) in two ways. 
First, we repeat the automatic annotation process 
until it satisfies the stopping criteria. Second, we 
apply different patterns (hard and soft pattern 
rules) in both the training and test phases. 
6 Conclusion 
We have presented a novel bootstrapping 
approach for information extraction by the 
cascading use of soft and hard pattern rules. Our 
framework takes advantages of the high-recall of 
soft pattern rules and the high-precision of hard 
pattern rules. We use soft pattern rules to 
automatically annotate more training instances so 
as to provide a more comprehensive basis for hard 
pattern rule induction. The integration of soft 
pattern matching in the extraction phase also 
provides more target entities from test instances 
that would otherwise be missed by hard pattern 
matching. With much less manual input, the 
proposed bootstrapping system approaches the 
performance obtained by fully supervised learning 
on both semi-structured and free texts corpora. 
7 Acknowledgement 
The authors would like to thank Alexia Leong 
for proofreading this paper. The third author is 
supported by Singapore Millennium Foundation 
Scholarship (ref no. 2003-SMS-0230). 
References  
A. Blum and T. Mitchell. 1998. Combining 
Labeled and Unlabeled Data with Co-training. 
Proceedings of the 11th Annual Conference on 
Computational Learning Theory (COLT-98), 
pages 92-100. 
T.-S. Chua and J. Liu. 2002. Learning Pattern 
Rules for Chinese Named Entity Extraction. 
Proceedings of the 18th National Conference on 
Artificial Intelligence. (AAAI-02), pages 411-
418. 
F. Ciravegna. 2001. Adaptive Information 
Extraction from Text by Rule Induction and 
Generalisation. Proceedings of the 17th 
International Joint Conference on Artificial 
Intelligence (IJCAI-2001),  pages 1251-1256. 
M. Collins and Y. Singer. 1999. Unsupervised 
Models for Named Entity Classification. 
Proceedings of the 1999 Joint SIGDAT 
Conference on EMNLP and VLC. 
H. Cui, M.-Y. Kan and T.-S. Chua. 2004. 
Unsupervised Learning of Soft Patterns for 
Definitional Question Answering. Proceedings 
of 13th World Wide Web Conference. (WWW-04), 
pages 90-99. 
MUC-4, 1992. Proceedings of the Fourth Message 
Understanding Conference. San Mateo, CA: 
Morgan Kaufmann. 1992. 
I. Muslea. 1999. Extraction Patterns for 
Information Extraction Tasks: A Survey. The 
AAAI-99 Workshop on Machine Learning for 
Information Extraction. 
U. Y. Nahm and R. J. Mooney. 2001. Mining Soft 
Matching Rules from Textual Data. Proceedings 
of the 17th International Joint Conference on 
Artificial Intelligence. (IJCAI-01), pages 979-
986. 
C. Niu, W. Li, J. Ding and R. K. Srihari. 2003. A 
Bootstrapping Approach to Named Entity 
Classification Using Successive Learners. 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics. 
(ACL-03), pages 335-342. 
E. Riloff. 1996. Automatically Generating 
Extraction  Patterns from Untagged Text. 
Proceedings of the 13th National Conference on 
Artificial Intelligence (AAAI-96), pages 1044-
1049. 
E. Riloff and R. Jones, 1999,  Learning 
Dictionaries for Information Extraction by 
Multi-Level Bootstrapping, Proceedings of the 
Sixteenth National Conference on Artificial 
Intelligence (AAAI-99), pages 474-479. 
S. Soderland. 1999. Learning Information 
Extraction Rules for Semi-structured and Free 
Text. Machine Learning, vol.34, pages 233-272. 
J. Xiao, T.-S. Chua and J. Liu. 2003. A Global 
Rule Induction Approach to Information 
Extraction. Proceedings of the 15th IEEE 
International Conference on Tools with Artificial 
Intelligence. (ICTAI-03), pages 530-536. 
R. Yangarber. 2003. Counter-Training in 
Discovery of Semantic Patterns. Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-03), pages 343-
350. 
Web-Based List Question Answering  
Hui Yang, Tat-Seng Chua 
School of Computing 
National University of Singapore 
3 Science Drive 2, 117543, Singapore  
yangh@lycos.co.uk,chuats@comp.nus.edu.sg  
 
Abstract 
While research on question answering has be-
come popular in recent years, the problem of ef-
ficiently locating a complete set of distinct 
answers to list questions in huge corpora or the 
Web is still far from being solved. This paper ex-
ploits the wealth of freely available text and link 
structures on the Web to seek complete answers 
to list questions. We introduce our system, 
FADA, which relies on question parsing, web 
page classification/clustering, and content extrac-
tion to find reliable distinct answers with high re-
call.  
1 Introduction 
The Text REtrieval Conference Series (TREC) has 
greatly encouraged Question Answering (QA) re-
search in the recent years. The QA main task in the 
recent TREC-12 involved retrieving short concise 
answers to factoid and list questions, and answer 
nuggets for definition questions (Voorhees, 2003). 
The list task in TREC-12 required systems to as-
semble a set of distinct and complete exact answers 
as responses to questions like ?What are the brand 
names of Belgian chocolates??. Unlike the questions 
in previous TREC conferences, TREC-12 list ques-
tions did not specify a target number of instances to 
return but expected all answers contained in the cor-
pus. Current QA systems (Harabagiu et al, 2003; 
Katz et al, 2003) usually extract a ranked list of fac-
toid answers from the top returned documents by 
retrieval engines. This is actually the traditional way 
to find factoid answers. The only difference between 
answering list questions and factoid questions here is 
that list QA systems allow for multiple answers, 
whose scores are above a cut-off threshold. 
An analysis of the results of TREC-12 list QA 
systems (Voorhees, 2003) reveals that many of them 
severely suffer from two general problems: low re-
call and non-distinctive answers. The median aver-
age F1 performance of list runs was only 21.3% 
while the best performer could only achieve 39.6% 
(Table 1). This unsatisfactory performance exposes 
the limitation of using only traditional Information 
Retrieval and Natural Language Processing tech-
niques to find an exhaustive set of factoid answers as 
compared to only one. 
TREC-12 Run Tag Avg F1  
LCCmainS03 0.396 
nusmml03r2 0.319 
MITCSAIL03c 0.134 
isi03a 0.118 
BBN2003B 0.097 
Average 0.213 
Table 1: TREC-12 Top 5 Performers (Voorhees, 2003) 
In contrast to the traditional techniques, the Web 
is used extensively in systems to rally round factoid 
questions. QA researchers have explored a variety of 
uses of the Web, ranging from surface pattern min-
ing (Ravichandran et al, 2002), query formulation 
(Yang et al, 2003), answer validation (Magnini et 
al., 2002), to directly finding answers on the Web by 
data redundancy analysis (Brill et al, 2001). These 
systems demonstrated that with the help of the Web 
they could generally boost baseline performance by 
25%-30% (Lin 2002). 
The well-known redundancy-based approach iden-
tifies the factoid answer as an N-gram appearing 
most frequently on the Web (Brill et al 2001). This 
idea works well on factoid questions because factoid 
questions require only one instance and web docu-
ments contains a large number of repeated informa-
tion about possible answers. However, when dealing 
with list questions, we need to find all distinct in-
stances and hence we cannot ignore the less frequent 
answer candidates. The redundancy-based approach 
fails to spot novel or unexpectedly valuable informa-
tion in lower ranked web pages with few occur-
rences.  
In this paper, we propose a novel framework to 
employ the Web to support list question answering.  
Based on the observations that multiple answer in-
stances often appear in the list or table of a single 
web page while multiple web pages may also con-
tain information about the same instance, we differ-
entiate these two types of web pages. For the first 
category, which we call Collection Page (CP), we 
need to extract table/list content from the web page. 
For the second category, which we call Topic Page 
(TP), we need to find distinct web pages relating to 
different answer instances. We will demonstrate that 
the resulting system, FADA (Find All Distinct An-
swers), could achieve effective list question answer-
ing in the TREC corpus.  
Figure 1: Examples of Collection Page (top) 
and Topic Page (bottom)  
The remainder of this paper is organized as fol-
lowing. Section 2 gives the design considerations of 
our approach. Section 3 details our question analysis 
and web query formulation. Section 4 describes the 
web page classification and web document features 
used in FADA. Section 5 shows the algorithm of 
topic page clustering while Section 6 details the an-
swer extraction process. Section 7 discusses experi-
mental results. Section 8 concludes the paper. 
2 Design Considerations  
Our goal is to find as many distinct exact answers on 
the Web as possible. This requires us to:  
? perform effective and exhaustive search; and  
? extract distinct answers. 
In order to perform effective search, we employ 
question transformation to get effectual web queries. 
However, this is not a trivial task. If the query is too 
general, too many documents may be retrieved and 
the system would not have sufficient resources to 
scan through all of them. If the query is too specific, 
no pages may be retrieved. 
Given millions of web pages returned by search 
engines, our strategy is to divide-and-conquer by 
first identify Collection Pages (CP) that contain a list 
of answer instances. For example, for the question 
?What breeds of dog have won the "Best in Show" 
award at the Westminster Dog Show??, we can find 
a Collection Page as shown in Figure 1 (top). Such a 
web page is a very good resource of answers. In 
general, we observe that there is a large number of 
named entities of the type desired appearing in a 
Collection Page, typically in a list or table. Our in-
tuition is that if we can find a Collection Page that 
contains almost all the answers, then the rest of the 
work is simply to extract answers from it or related 
web pages by wrapper rule induction.  
Another kind of ?good? web page is a Topic Page, 
that contains just one answer instance (Figure 1, bot-
tom). It typically contains many named entities, 
which correspond to our original query terms and 
some other named entities of the answer target type. 
Given the huge amount of web data, there will be 
many Topic Pages that refer to the same answer in-
stance.  There is hence a need to group the pages and 
to identify a pertinent and distinctive page in order 
to represent a distinct answer.  
 
Table 2: Web Page Classes 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The rest of the top returned web pages could be 
either relevant or irrelevant to the question. In sum-
mary, we need to classify web pages into four 
classes: Collection Page, Topic Page, Relevant Page, 
and Irrelevant Page (Table 2), based on their func-
tionality and contribution in finding list answers. 
Based on the above considerations, we propose a 
general framework to find list answers on the Web 
using the following steps: 
a) Retrieve a good set of web documents. 
b) Identify Collection Pages and distinct Topic 
Pages as main resources of answers. 
c) Perform clustering on other web pages based on 
their similarities to distinct Topic Pages to form 
clusters that correspond to distinct answer in-
stances. 
d) Extract answers from Collection Pages and Topic 
Page clusters. 
3 Question Transformation and Web Page 
Retrieval  
Agichtein et al (2001) presented a technique on 
learning search engine specific query transforma-
tions for question answering. A set of transformation 
rules are learned from a training corpus and applied 
to the questions at the search time. Related work 
could also be found in Kwok et al (2001) where the 
user?s question is processed by a parser to learn its 
syntactic structure and various query modulation 
techniques are applied to the initial questions to get 
high quality results for later answer extraction. 
FADA performs question parsing to identify key 
question words and the expected answer type. It ex-
tracts several sets of words from the original ques-
tion and identifies the detailed question classes. It 
Web page class Description 
Collection Page Containing a list of  answers  
Topic Page The best page to represent an answer 
instance 
Relevant Page Relevant to an  answer instance by 
providing either support or objection to 
the Topic Page 
Irrelevant Page Not related to any answer instance 
then formulates a number of queries by combining 
the known facets together with heuristic patterns for 
list questions.  
We perform both shallow and full parsing on a 
question followed by Named Entity Recognition 
(NER) to get the known query facets and their types. 
The shallow parser we used is the free online mem-
ory-based chunker1 and the full parser is MINIPAR2. 
Both parsers are very efficient and usually parse 300 
words within a second. The procedure of query pars-
ing is as follows:  
a) Remove head words. The head words in a ques-
tion could be wh-question words and leading 
verbs. The list of head words includes ?who, what, 
when, where, which, how, how much, how many, 
list, name, give, provide, tell?, etc. Removing 
them enables us to get the correct subject/object 
relation and verb in the question. For example, for 
question ?What breeds of dog have won the ?Best 
in Show? award at the Westminster Dog Show??, 
after removing the head word, the question be-
comes ?breeds of dog have won the ?Best in 
Show? award at the Westminster Dog Show?.  
b) Detect subject and object for the remaining ques-
tion segments by shallow parsing. For example, 
after parsing the above question, we get: 
[NP1Subject breeds//NNS NP1Subject] {PNP [P 
of/IN P] [NP dog/NN NP] PNP} [VP1 
have/VBP won/VBN VP1] [NP1Object the/DT 
``/`` Best/JJS NP1Object] {PNP [P in/IN 
P] [NP Show/NNP ''/'' award/NN NP] 
PNP} {PNP [P at/IN P] [NP the/DT 
Westminster/NNP Dog//NNP Show/NNP NP] 
PNP} 
From the parsed sentence, we want to get the logi-
cal subject as the sentence subject or its immedi-
ate modifiers. Here we have the logical subject-
?breeds of dog?, verb-?won?, and logical object-
?the best in show award?. If the resulting logical 
subject/object is the term ?that? as in the follow-
ing parsed query for ?U.S. entertainers that later 
became politicians?: 
[NP U.S./NNP entertainers//NNS NP] 
[NP1Subject that/WDT NP1Subject] [ADVP 
later/RB ADVP] [VP1 became/VBD VP1] 
[NP1Object politicians//NNS NP1Object] 
we get the noun or noun phrase before the clause 
as the logical subject/object. Hence, we have the 
logical subject-?entertainers?, action-?became?, 
and logical object-?politician?. 
c) Extract all the noun phrases as potential descrip-
tions from the remaining question segments, 
which are usually prepositional phrases or clauses. 
For the ?dog breeds? example, we get the descrip-
tions-?Westminster Dog Show?.  
                                                          
1 http://ilk.kub.nl/cgi-bin/tstchunk/demo.pl 
2 http://www.cs.ualberta.ca/~lindek/minipar.htm 
d) Apply named entity recognition to the resulting 
description phrases by using NEParser, a fine-
grained named entity recognizer used in our 
TREC-12 system (Yang et al, 2003). It assigns 
tags like ?person?, ?location?, ?time?, ?date?, 
?number?. For the ?dog breed? example, ?West-
minster? gets the location tag. 
After the above analysis, we obtain all the known 
facets provided in the original question. We then 
make use of this knowledge to form web queries to 
get the right set of pages. This is a crucial task in 
dealing with the Web. One of the query transforma-
tion rules is given as follows:  
(list|directoty|category|top|favorite
)? (:|of)? <subj> <action>? <object>? 
<description1>? <description2>? ? 
<descriptionN>? 
The rule starts the query optionally with leading 
words (list, directory, category), optionally followed 
by a colon or ?of?, followed by subject phrase 
(<subj>), optionally followed by action (<action>), 
optionally followed by object (<object>) and de-
scription phrases (<description1>?<descriptionN>). 
In the above pattern, ??? denotes optional, ??? omit, 
and ?|? alternative. For example, for the ?dog breed? 
question, we form queries ?breed of dog won best in 
show Westminster Dog Show?, ?directory breed of 
dog best in show Westminster Dog Show?, and ?list 
breed of dog won best in show? etc. 
Transforming the initial natural language ques-
tions into a good query can dramatically improve the 
chances of finding good answers. FADA submits 
these queries to well-known search engines (Google, 
AltaVista, Yahoo) to get the top 1,000 Web pages 
per search engine per query. Here we attempt to re-
trieve a large number of web pages to serve our goal 
- find All Distinct answers. Usually, there are a large 
number of web pages which are redundant as they 
come from the same URL addresses. We remove the 
redundant web pages using the URL addresses as the 
guide. We also filter out files whose formats are nei-
ther HTML nor plain text and those whose lengths 
are too short or too long. Hence the size of the re-
sulting document set for each question varies from a 
few thousands to ten of thousands. 
4 Web Page Classification  
In order to group the web pages returned by the 
search engines into the four categories discussed 
earlier, it is crucial to find a good set of features to 
represent the web pages. Many techniques such as 
td.idf (Salton and Buckley, 1988) and a stop word 
list have been proposed to extract lexical features to 
help document clustering. However, they do not 
work well for question answering. As pointed out by 
Ye et al (2003) in their discussion on the per-
son/organization finding task, given two resume 
pages about different persons, it is highly possible 
that they are grouped into one cluster because they 
share many similar words and phrases. On the other 
hand, it is difficult to group together a news page 
and a resume page about the same target entity, due 
to the diversity in subject matter, word choice, liter-
ary styles and document format.  To overcome this 
problem, they used mostly named entity and link 
information as the basis for clustering. Compared to 
their task, our task of finding good web documents 
containing answers is much more complex. The fea-
tures are more heterogeneous, and it is more difficult 
to choose those that reflect the essential characteris-
tics of list answers. 
In our approach, we obtain the query words 
through subject/object detection and named entity 
recognition.  We found that there are a large number 
of named entities of the same type appearing in a 
Collection Page, typically within a list or table.  And 
in a Topic Page, there is also typically a group of 
named entities, which could correspond to our origi-
nal query terms or answer target type. Therefore, 
named entities play important roles in semantic ex-
pression and should be used to reflect the content of 
the pages.  
The Web track in past TREC conferences shows 
that URL, HTML structure, anchor text, hyperlinks, 
and document length tend to contain important heu-
ristic clues for web clustering and information re-
trieval (Craswell and Hawking, 2002). We have 
found that a Topic Page is highly likely to repeat the 
subject in its URL, title, or at the beginning of its 
page. In general, if the subject appears in important 
locations, such as in HTML tags <title>, <H1> and 
<H2>, or appears frequently, then the corresponding 
pages should be Topic Pages and their topic is about 
the answer target.  
Followed the above discussion, we design a set of 29 
features based on Known Named Entity Type, An-
swer Named Entity Type, ordinary Named Entities, 
list, table, URL, HTML structure, Anchor, Hyper-
links, and document length to rep resent the web 
pages. Table 3 lists the features used in our system. 
In the table and subsequent sections, NE refers to 
Named Entity. 
We trained two classifiers: the Collection Page 
classifier and the Topic Page classifier. The former 
classifies web pages into Collection Pages and non-
collection pages while the later further classifies the 
non-collection pages into Topic Pages and Others. 
Both Classifiers are implemented using Decision 
Tree C4.5 (Quinlan 1993). We used 50 list questions 
from TREC-10 and TREC-11 for training and 
TREC-12 list questions for testing. We parse the 
questions, formulate web queries and collect web 
pages by using the algorithm described in Section 2.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 3: Web Page Features 
Each sample is represented using the features 
listed in Table 3. Some of the decision rules are as 
follows: 
a) OUT_Link >= 25 & NE > 78 &  
b) Answer_NE >= 30 -> Class CP OUT_Link 
<= 25 & Answer_NE <= 5 & NE > 46 -> 
Class TP  
c) OUT_Link >= 25 & URL_Depth > 3 -> 
Others 
d) NE <= 4  -> Others 
Rule a) implies that good Collection Pages should 
have many outlinks, NEs and especially answer NEs. 
Rule b) implies that good Topic Pages should have 
many NEs but relatively few links and answer NEs. 
Rule c) show that Others have deeper URL depth; 
while Rule d) shows that they have fewer NEs. 
 Feature Meaning 
1 |PER| # of Person NEs 
2 |ORG| # of Organization NEs 
3 |LOC| # of Location NEs 
4 |TME| # of Time NEs, including date, year, month 
5 |NUM| # of Numer NEs 
6 |COD| # of Code NEs, including phone number, 
zip code, etc 
7 |OBJ| # of Object NEs, including animal, planet, 
book,  etc 
8 |NE| Total number of the above NEs 
9 |Known_NE| Total # of NEs within the same NE type as 
in the question. In the ?dog breed? example, 
it is the number of Location NEs since 
?Westminster? is identified as Location by 
NER. 
10 |Unknown_N
E| 
# of NEs belonging to other NE type. In the 
?dog breed? example, it is the total number 
of Time and Breed NEs 
11 |Answer_NE| # of NEs belonging to expected answer 
type. In the ?dog breed? example, it is the 
number of Breed NEs 
12 |Known_NE| 
/ |NE| 
Ratio of  | Known _NE| to |NE| 
13 |Unknown_N
E| / |NE| 
Ratio of  | Unknown _NE| to |NE| 
14 |Answer_NE| 
/ |NE| 
Ratio of  |Answer_NE| to |NE| 
15 Length # of tokens in a page 
16 Content_Len
gth 
# of words in a page excluding HTML 
tags 
17 |NE|/Length Ratio of |NE| to |Token| 
18 |NE|/Content
_Length 
Ratio of |NE| to |Word| 
19 |In_Link| # of in-links 
20 |Out_Link| # of out-links  
21 |All_Link| The sum of in-links and out-links 
22 Keyword_in
_Title 
Boolean indicating presence of keywords in 
page title 
23 Keyword_ 
in_URL 
Boolean indicating presence of keywords in 
URL 
24 Keyword_ 
in_Page 
Boolean indicating presence of keywords in 
the page 
25 |Answer_NE
_in_Title| 
# of NEs belonging to expected answer type 
presenting in page title 
26 |Answer_NE
_in_URL| 
# of NEs belonging to expected answer type 
presenting in URL 
27 |<li>| # of HTML tags representing a list or table, 
including <li>, <ol>, <ul>, <br>,<td> 
28 |<li><a 
href=| 
# of HTML tags, including <li>, <ol>, 
<ul>, <br>,<td> to represent a list/table of 
anchors, 
29 URL_Depth The depth of URL 
 
Web page classification enables us to get Collec-
tion Pages, Topic Pages and the rest of the pages. 
Our experiments on TREC-12 list questions showed 
that we can achieve a classification precision of 
91.1% and 92% for Collection Pages and Topic 
Pages respectively. 
5 Finding Answer Sources  
Based on Web page classification, we form the ini-
tial sets of Collection Pages CPSet, Topic Pages 
TPSet and OtherSet. In order to boost the recall, we 
first use the outgoing links of Collection Pages to 
find more Topic Pages.  These outgoing pages are 
potential Topic Pages but not necessarily appearing 
among the top returned web documents. Our subse-
quent tests reveal that the new Topic Pages intro-
duced by links from Collection Pages greatly 
increase the overall answer recall by 23%. The new 
Topic Page set becomes: 
TPSet? = TPSet + {outgoing pages of CPs} 
Second, we select distinct Topic Pages. We com-
pare the page similarity between each pair of Topic 
Pages using the algorithm below.  
for each pair {tpi, tpj} in TPSet?  
  if (sim(tpi,tpj)> ?) 
    if ?ANE_in_tpi > ?ANE_in_tpj 
  move tpj into OtherSet; 
Here the page similarity function sim() is a linear 
combination of overlaps between Known_NE, An-
swer_NE, URL similarity and link similarity. ? is 
preset at 0.75 and may be overridden by the user. 
?ANE_in_tpi is the number of named entities of 
answer type in Topic Page tpi. For those pairs with 
high similarity, we keep the page that contains more 
named entities of answer type in TPSet? and move 
the other into OtherSet. The resulting Topic Pages 
in TPSet? are distinct and will be used as cluster 
seeds for the next step. 
Third, we identify and dispatch Relevant Pages 
from OtherSet into appropriate clusters based on 
their similarities with the cluster seeds.  
for each rpi in OtherSet { 
  k = argmax {sim(rpi , tpk) } 
 if (sim(rpi , tpk ) >  ? )  
   insert rpi into clusterk; 
  else  
    insert rpi into IrrelevantSet; } 
Here ?  is preset at 0.55, and sim() is defined as 
above. Each cluster corresponds to a distinct answer 
instance. The Topic Page provides the main facts 
about that answer instance while Relevant Pages 
provide supporting materials for the unique answer 
instance. The average ratio of correct clustering is 
54.1% in our experiments. 
Through web page clustering, we avoid early an-
swer redundancy, and have a higher chance to find-
ing distinct answers on the noisy Web. 
6 Answer Extraction 
6.1 HTML Source Page Cleaning  
Many HTML web pages contain common HTML 
mistakes, including missing or unmatched tags, end 
tags in the wrong order, missing quotes round attrib-
utes, missed / in end tags, and missing > closing tags, 
etc. We use HtmlTidy3 to clean up the web pages 
before classification and clustering. FADA also uses 
an efficient technique to remove advertisements. We 
periodically update the list from Accs-Net4, a site 
that specializes in creating such blacklists of adver-
tisers. If a link address matches an entry in a black-
list, the HTML portion that contained the link is 
removed.  
6.2 Answer Extraction from CP 
Collection Pages are very good answer resources for 
list QA. However, to extract the ?exact? answers 
from the resource page, we need to perform wrapper 
rule induction to extract the useful content. There is 
a large body of related work in content extraction, 
which enables us to process only extracted content 
rather than cluttered data coming directly from the 
web. Gupta et al (2003) parsed HTML documents 
to a Document Object Model tree and to extract the 
main content of a web page by removing the link 
lists and empty tables. In contrast, our link list ex-
tractor finds all link lists, which are table cells or 
lists for which the ratio of the number of links to the 
number of non-linked words is greater than a spe-
cific ratio. We have written separate extractors for 
each answer target type. The answers obtained in 
Collection Pages are then ?projected? onto the 
TREC AQUAINT corpus to get the TREC answers 
(Brill et al, 2001). 
6.3 Answer Extraction from TP Cluster 
Having web pages clustered for a certain question, 
especially when the clusters nicely match distinct 
answer, facilitates the task of extracting the possible 
answers based on the answer target type. We per-
form this by first analyzing the main Topic Pages in 
each cluster.  In case we find multiple passages con-
taining different answer candidates in the same 
Topic Page, we select the answer candidate from the 
passage that has the most variety of NE types since 
it is likely to be a comprehensive description about 
different facets of a question topic. The answer 
found in the Topic Page is then ?projected? onto the 
QA corpus to get the TREC answers as with the Col-
lection Page. In case no TREC answers can be found 
                                                          
3 http://htmltrim.sourceforge.net/tidy.html 
4 http://www.accs-net.com/hosts/get_hosts.html 
based on the Topic Page, we go to the next most 
relevant page in the same cluster to search for the 
answer. The process is repeated until either an an-
swer from the cluster is found in the TREC corpus 
or when all Relevant Pages in the cluster have been 
exhausted.   
For the question ?Which countries did the first 
lady Hillary Clinton visit??, we extracted the Loca-
tions after performing Named Entity analysis on 
each cluster and get 38 country names as answers. 
The recall is much higher than the best performing 
system (Harabagiu et al, 2003) in TREC-12 which 
found 26 out of 44 answers. 
7 Evaluation on TREC-12 Question Set 
We used the 37 TREC-12 list questions to test the 
overall performance of our system and compare the 
answers we found in the TREC AQUAINT corpus 
(after answer projection (Brill et al 2001)) with the 
answers provided by NIST.  
7.1 Tests of Web Page Classification 
In Section 3, the web pages are classified into three 
classes: Collection Pages, Topic Pages, and Others. 
Table 4 shows the system performance of the classi-
fication. We then perform a redistribution of classi-
fied pages, where the outgoing pages from CPs go to 
TP collection, and the Relevant Pages are grouped as 
supportive materials into clusters, which are based 
on distinct Topic Page. Nevertheless, the perform-
ance of web page classification will influence the 
later clustering and answer finding task. Table 4 
shows that we could achieve an overall classification 
average precision of 0.897 and average recall of 
0.851. This performance is adequate to support the 
subsequent steps of finding complete answers. 
 
 
 
 
 
 
Table 4: Performance of Web Page Classification 
7.2 Performance and Effects of Web Page 
Clustering 
Relevant Pages are put into clusters to provide sup-
portive material for a certain answer instance. The 
performance of Relevant Page dispatch/clustering is 
54.1%. We also test different clustering thresholds 
for our web page clustering as defined in Section 5. 
We use the F1 measure of the TREC-12 list QA re-
sults as the basis to compare the performance of dif-
ferent clustering threshold combinations as shown in 
xx. We obtain the best performance of F1 = 0.464 
when ?=0.55 and ?=0.75. 
 
 ?(0.55) ?(0.65) ?(0.75) ?=0.85
?=0.25 0.130 0.234 0. 324 0.236 
?=0.35 0.136 0.244 0. 338 0.232 
?=0.45 0.148 0.332 0. 428 0.146 
?=0.55 0.166 0.408 0. 464 0.244 
?=0.65 0.200 0.322 0. 432 0.236 
Table 5: Clustering Threshold Effects 
7.3 Overall Performance 
Table 6 compares a baseline list question answering 
system with FADA. The baseline is based on a sys-
tem which we used in participation in the TREC-12 
QA task (Yang et al, 2003).  It extends the tradi-
tional IR/NLP approach for factoid QA to perform 
list QA, as is done in most other TREC-12 systems. 
It achieves an average F1 of 0.319, and is ranked 2nd 
in the list QA task.   
We test two variants of FADA ? one without in-
troducing the outgoing pages from CPs as potential 
TPs (FADA1), and one with (FADA2). The two 
variants are used to evaluate the effects of CPs in the 
list QA task. The results of these two variants of 
FADA on the TREC-12 list task are presented in 
Table 6.  
Table 6: Performance on TREC-12 Test Set 
Without the benefit of the outgoing pages from 
CPs to find potential answers, FADA1 could boost 
the average recall by 30% and average F1 by 16.6% 
as compared to the baseline. The great improvement 
in recall is rather encouraging because it is crucial 
for a list QA system to find a complete set of an-
swers, which is how list QA differ from factoid QA. 
By taking advantage of the outgoing pages from 
CPs, FADA2 further improves performance to an 
average recall of 0.422 and average F1 of 0.464. It 
outperforms the best TREC-12 QA system (Voorhees, 
2003) by 19.6% in average F1 score. 
From Table 6, we found that the outgoing pages 
from the Collection Pages (or resource pages) con-
tribute much to answer finding task. It gives rise to 
an improvement in recall of 22.7% as compared to 
the variant of FADA1 that does not take advantage 
of outgoing pages. We think this is mainly due to the 
characteristics of the TREC-12 questions. Most 
questions ask for well-known things, and famous 
events, people, and organization. For this kind of 
questions, we can easily find a Collection Page that 
contains tabulated answers since there are web sites 
that host and maintain such information. For in-
stance, ?Westminster Dog Show? has an official 
 Avg P Avg R Avg F1
Baseline 0.568 0.264 0.319 
FADA1 (w/o outgoing pages) 0.406 0.344 0.372 
FADA2 (w/ outgoing pages) 0.516 0.422 0.464 
TREC-12 best run - - 0.396 
Page Class Avg Prec. Avg Rec. 
Collection 91.1% 89.5% 
Topic 92.0% 88.4% 
Relevant 86.5% 83.4% 
Overall 89.7% 85.1% 
 
web site5. However, for those questions that lack 
Collection Pages, such as ?Which countries did the 
first lady Hillary Clinton visit??, we still need to rely 
more on Topic Pages and Relevant Pages. 
With the emphasis on answer completeness and 
uniqueness, FADA uses a large set of documents 
obtained from the Web to find answers. As com-
pared to the baseline system, this results in a drop in 
average answer precision although both recall and F1 
are significantly improved. This is due to the fact 
that we seek most answers from the noisy Web di-
rectly, whereas in the baseline system, the Web is 
merely used to form new queries and the answers are 
found from the TREC AQUAINT corpus.  We are 
still working to find a good balance between preci-
sion and recall. 
The idea behind FADA system is simple: Since 
Web knowledge helps in answering factoid ques-
tions, why not list questions? Our approach in 
FADA demonstrates that this is possible. We believe 
that list QA should benefit even more than factoid 
QA from using Web knowledge. 
8 Conclusion 
We have presented the techniques used in FADA, 
a system which aims to find complete and distinct 
answers on the Web using question parsing, web 
page classification/clustering and content extraction. 
By using the novel approach, we can achieve a recall 
of 0.422 and F1 of 0.464, which is significantly bet-
ter than the top performing systems in the TREC-12 
List QA task. The method has been found to be ef-
fective. Our future work includes discovering an-
swers on non-text web information, such as images. 
Much text information is stored as images on the 
web, and hence, cannot be accessed by our approach, 
and some do contain valuable information. 
References  
E. Agichtein, S. Lawrence, and L. Gravano. 2001. 
"Learning search engine specific query transforma-
tions for question answering.? In the Proceedings of 
the 10th ACM World Wide Web Conference (WWW 
2001).  
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001. 
?Data-intensive question answering?. In the Pro-
ceedings of the 10th  Text REtrieval Conference 
(TREC 2001). 
N. Craswell, D Hawking. 2002. ?Overview of the 
TREC-2002 Web Track?, In the Proceedings of the 
11th Text REtrieval Conference. (TREC 2002). 
S. Gupta, G. Kaiser, D. Neistadt, P. Grimm, 2003. 
?DOM-based Content Extraction of HTML Docu-
ments?, In the Proceedings of the 12th ACM World 
Wide Web conference. (WWW 2003). 
                                                          
5 http://www.westminsterkennelclub.org/ 
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, J. 
Williams, J. Bensley, 2003 ?Answer Mining by 
Combining Extraction Techniques with Abductive 
Reasoning,? In the notebook of the 12th Text RE-
trieval Conference (TREC 2003), 46-53.  
B. Katz, J. Lin, D. Loreto, W. Hildebrandt, M. Bilotti, 
S. Felshin, A. Fernandes, G. Marton, F. Mora, 2003, 
?Integrating Web-Based and Corpus-Based Tech-
niques for Question Answering?, In the notebook of 
the 12th Text REtrieval Conference (TREC 2003), 
472-480. 
C. Kwok, O. Etzioni, and D. S. Weld, 2001, ?Scaling 
Question Answering to the Web?, In the Proceed-
ings of the 10th ACM World Wide Web conference. 
(WWW 2001). 
C. Y. Lin, ?The Effectiveness of Dictionary and Web-
Based Answer Reranking.? In the Proceedings of the 
19th International Conference on Computational 
Linguistics (COLING 2002). 
B. Magnini, M. Negri, R. Prevete and H. Tanev. 2002. 
?Is it the Right Answer? Exploiting Web Redun-
dancy for Answer Validation?. In the Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics. (ACL 2002), 425-432. 
J. R. Quinlan, 1993. C4.5: Programs for Machine 
Learning. Morgan-Kaufmann, San Francisco. 
D. Ravichandran, and E. H. Hovy. 2002. ?Learning 
Surface Text Patterns for a Question Answering 
System.? In the Proceedings of the 40th  ACL con-
ference. (ACL 2002). 
G. Salton and C. Buckley, "Term-weighting ap-
proaches in automatic text retrieval", Information 
Processing and Management: an International Jour-
nal, v.24 n.5, 1988 
E.M.Voorhees. 2003. ?Overview of the TREC 2003 
Question Answering Track.? In the notebook of the 
12th Text REtrieval Conference (TREC 2003), 14-27. 
H. Yang, T. S. Chua, S Wang, C. K. Koh. 2003. 
?Structured Use of External Knowledge for Event-
based Open Domain Question Answering?, In the 
Proceedings of the 26th Annual International ACM 
SIGIR Conference on Research and Development in 
Information Retrieval (SIGIR 2003).  
H. Yang, H. Cui, M. Maslennikov, L. Qiu, M. Y. Kan, 
T. S. Chua. 2003. ?QUALIFIER in the TREC12 QA 
Main Task?, In the notebook of the 12th Text RE-
trieval Conference (TREC 2003). 
S. Ye, T. S. Chua, J. R. Kei. 2003. ?Querying and 
Clustering Web Pages about Persons and Organiza-
tions?. Web Intelligence 2003, 344-350. 
363
364
365
366
367
368
369
370
Modeling Context in Scenario Template Creation
Long Qiu, Min-Yen Kan, Tat-Seng Chua
Department of Computer Science
National University of Singapore
Singapore, 117590
{qiul,kanmy,chuats}@comp.nus.edu.sg
Abstract
We describe a graph-based approach to Sce-
nario Template Creation, which is the task
of creating a representation of multiple re-
lated events, such as reports of different hur-
ricane incidents. We argue that context is
valuable to identify important, semantically
similar text spans from which template slots
could be generalized. To leverage context,
we represent the input as a set of graphs
where predicate-argument tuples are ver-
tices and their contextual relations are edges.
A context-sensitive clustering framework is
then applied to obtain meaningful tuple clus-
ters by examining their intrinsic and extrin-
sic similarities. The clustering framework
uses Expectation Maximization to guide the
clustering process. Experiments show that:
1) our approach generates high quality clus-
ters, and 2) information extracted from the
clusters is adequate to build high coverage
templates.
1 Introduction
Scenario template creation (STC) is the problem of
generating a common semantic representation from
a set of input articles. For example, given multiple
newswire articles on different hurricane incidents,
an STC algorithm creates a template that may in-
clude slots for the storm?s name, current location, di-
rection of travel and magnitude. Slots in such a sce-
nario template are often to be filled by salient entities
in the scenario instance (e.g., ?Hurricane Charley?
or ?the coast area?) but some can also be filled by
prominent clauses, verbs or adjectives that describe
these salient entities. Here, we use the term salient
aspect (SA) to refer to any of such slot fillers that
people would regard as important to describe a par-
ticular scenario. Figure 1 shows such a manually-
built scenario template in which details about im-
portant actions, actors, time and locations are coded
as slots.
STC is an important task that has tangible bene-
fits for many downstream applications. In the Mes-
sage Understanding Conference (MUC), manually-
generated STs were provided to guide Information
Extraction (IE). An ST can also be viewed as reg-
ularizing a set of similar articles as a set of at-
tribute/value tuples, enabling multi-document sum-
marization from filled templates.
Despite these benefits, STC has not received
much attention by the community. We believe this
is because it is considered a difficult task that re-
quires deep NL understanding of the source articles.
A problem in applications requiring semantic simi-
larity is that the same word in different contexts may
have different senses and play different roles. Con-
versely, different words in similar contexts may play
similar roles. This problem makes approaches that
rely on word similarity alone inadequate.
We propose a new approach to STC that incor-
porates the use of contextual information to address
this challenge. Unlike previous approaches that con-
centrate on the intrinsic similarity of candidate slot
fillers, our approach explicitly models contextual ev-
idence. And unlike approaches to word sense disam-
biguation (WSD) and other semantic analyses that
157
use neighboring or syntactically related words as
contextual evidence, we define contexts by semantic
relatedness which extends beyond sentence bound-
aries. Figure 2 illustrates a case in point with two
excerpts from severe storm reports. Here, although
the intrinsic similarity of the main verbs ?hit? and
?land? is low, their contextual similarity is high as
both are followed by clauses sharing similar subjects
(hurricanes) and the same verbs. Our approach en-
codes such contextual information as graphs, map-
ping the STC problem into a general graph overlay
problem that is solvable by a variant of Expectation
Maximization (EM).
Our work also contributes resources for STC re-
search. Until now, few scenario templates have been
publicly available (as part of MUC), rendering any
potential evaluation of automated STC statistically
insignificant. As part of our study, we have com-
piled a set of input articles with annotations that we
are making available to the research community.
Scenario Template: Storm
Storm Name Charley
Storm Action landed
Location Florida?s Gulf coast
Time Friday at 1950GMT
Speed 145 mph
Victim Category 1 13 people
Action died
Victim Category 2 over one million
Action affected
Figure 1: An example scenario template (filled).
2 Related Work
A natural way to automate the process of STC is to
cluster similar text spans in the input article set. SAs
then emerge through clustering; if a cluster of text
spans is large enough, the aspects contained in it will
be considered as SAs. Subsequently, these SAs will
be generalized into one or more slots in the template,
depending on the definition of the text span. As-
suming scenarios are mainly defined by actions, the
focus should be on finding appropriate clusters for
text spans each of which represents an action. Most
of the related work (although they may not directly
address STC) shares this assumption and performs
Charley landed further south on Florida?s
Gulf coast than predicted, ... The hurricane
... has weakened and is moving over South
Carolina.
At least 21 others are missing after the storm
hit on Wednesday. .... But Tokage had
weakened by the time it passed over Japan?s
capital, Tokyo, where it left little damage be-
fore moving out to sea.
Figure 2: Contextual evidence of similarity. Curved
lines indicate similar contexts, providing evidence
that ?land? and ?hit? from two articles are semanti-
cally similar.
action clustering accordingly. While the target ap-
plication varies, most systems that need to group text
spans by similarity measures are verb-centric.
In addition to the verb, many systems expand
their representation by including named entity tags
(Collier, 1998; Yangarber et al, 2000; Sudo et al,
2003; Filatova et al, 2006), as well as restrict-
ing matches (using constraints on subtrees (Sudo et
al., 2003; Filatova et al, 2006), predicate argument
structures (Collier, 1998; Riloff and Schmelzen-
bach, 1998; Yangarber et al, 2000; Harabagiu and
Maiorano, 2002) or semantic roles).
Given these representations, systems then cluster
similar text spans. To our knowledge, all current
systems use a binary notion of similarity, in which
pairs of spans are either similar or not. How they de-
termine similarity is tightly coupled with their text
span representation. One criterion used is pattern
overlap: for example, (Collier, 1998; Harabagiu and
Lacatusu, 2005) judge text spans to be similar if they
have similar verbs and share the same verb argu-
ments. Working with tree structures, Sudo et al and
Filatova et al instead require shared subtrees.
Calculating text span similarity ultimately boils
down to calculating word phrase similarity. Ap-
proaches such as Yangarber?s or Riloff and
Schmelzenbach?s do not employ a thesaurus and
thus are easier to implement, but can suffer from
over- or under-generalization. In certain cases, ei-
ther the same actor is involved in different actions or
different verbs realize the same action. Other sys-
tems (Collier, 1998; Sudo et al, 2003) do employ
158
lexical similarity but threshold it to obtain binary
judgments. Systems then rank clusters by cluster
size and correlation with the relevant article set and
equate top clusters as output scenario slots.
3 Context-Sensitive Clustering (CSC)
Automating STC requires handling a larger degree
of variations than most previous work we have sur-
veyed. Note that the actors involved in actions in a
scenario generally differ from event to event, which
makes most related work on text span similarity cal-
culation unsuitable. Also, action participants are not
limited to named entities, so our approach needs to
process all NPs. As both actions and actors may be
realized using different words, a similarity thesaurus
is necessary. Our approach to STC uses a thesaurus
based on corpus statistics (Lin, 1998) for real-valued
similarity calculation. In contrast to previous ap-
proaches, we do not threshold word similarity re-
sults; we retain their fractional values and incorpo-
rate these values holistically. Finally, as the same
action can be realized in different constructions, the
semantic (not just syntactic) roles of verb arguments
must be considered, lest agent and patient roles be
confused. For these reasons, we use a semantic role
labeler (Pradhan et al, 2004) to provide and delimit
the text spans that contain the semantic arguments
of a predicate. We term the obtained text spans as
predicate argument tuples (tuples) throughout the
paper. The semantic role labeler reportedly achieves
an F 1 measure equal to 68.7% on identification-classification of predicates and core arguments on a
newswire text corpus (LDC, 2002). Within the con-
fines of our study, we find it is able to capture most
of the tuples of interest.
Our approach explicitly captures contextual ev-
idence. We define a tuple?s contexts as other tu-
ples in the same article segment where no topic shift
occurs. This definition refines the n-surrounding
word constraint commonly used in spelling correc-
tion (for example, (Hirst and Budanitsky, 2005)),
Word Sense Disambiguation ((Preiss, 2001), (Lee
and Ng, 2002), for instance), etc. while still en-
sures the relatedness between a tuple and its con-
texts. Specifically, a tuple is contextually related to
other tuples by two quantifiable contextual relations:
argument-similarity and position-similarity. For our
experiments, we use the leads of newswire articles
as they normally summarize the news. We also as-
sume a lead qualifies as a single article segment, thus
making all of its tuples as potential contexts to each
other.
from A2
from A1
weakened(storm)
v21
hit(storm)
v22
moving(storm)
v23
weakened(hurricane)
v11
landed(hurricane)
v12
moving(hurricane)
v13
e21,2
e22,1 e21,3
e23,1
e22,3
e23,2
e11,2
e12,1 e11,3
e13,1
e12,3
e13,2
Figure 3: Being similar contexts, ?weakened? and
?moving? provide contextual evidence that ?land?
and ?hit? are similar.
First, we split the input article leads into sentences
and perform semantic role labeling immediately af-
terwards. Our system could potentially benefit from
additional pre-processing such as co-reference reso-
lution. Currently these pre-processing steps have not
been properly integrated with the rest of the system,
and thus we have not yet measured their impact.
We then transform each lead Ai into a graph Gi =
{V i, Ei}. As shown in Figure 3, vertices V i =
{vij}(j = 1, ..., N) are the N predicate argumenttuples extracted from the ith article, and directed
edges Ei = {eim,n = (vim, vin)} reflect contextualrelations between tuple vim and vin. Edges only con-nect tuples from the same article, i.e., within each
graph Gi. We differentiate between two types of
edges. One is argument-similarity, where the two
tuples have semantically similar arguments. This
models tuple cohesiveness, where the edge weight is
determined by the similarity score of the most sim-
ilar inter-tuple argument pair. The other is position-
similarity, represented as the offset of the ending tu-
ple with respect to the other, measured in sentences.
This edge type is directional to account for simple
causality.
Given this set of graphs, the clustering task is to
find an optimal alignment of all graphs (i.e., super-
imposing the set of article graphs to maximize vertex
overlap, constrained by the edges). We adapt Expec-
tation Maximization (Dempster et al, 1977) to find
159
an optimal clustering. This process assigns tuples to
suitable clusters where they are semantically similar
and share similar contexts with other tuples. Algo-
rithm 1 outlines this alignment process.
Algorithm 1 Graph Alignment(G)
/*G is a set of graph {Gi}*/
T ? all tuples in G
C ? highly cohesive tuples clusters
other? remaining tuples semantically connected with C
C[C.length]? otherrepeat
/*E step*/for each i such that i < C.length dofor each j such that j < C.length doif i == j then
continue;
re-estimate parameters[C[i],C[j]] /*distribution
parameters of edges between two clusters*/
tupleReassigned = false /*reset*/
/*M step*/for each i such that i < T.length do
aBestLikelihood = T [i].likelihood; /*likelihood of
being in its current cluster*/for each tuple tcontxt that contextually related with
T [i] dofor each cluster ccand, any candidate cluster thatcontextually related with tcontxt.cluster do
P (T [i] ? ccand) = comb(Ps, Pc)
likelihood = log(P (T [i] ? ccand))if likelihood > aBestLikelihood then
aBestLikelihood = likelihood
T [i].cluster = ccand
tupleReassigned = trueuntil tupleReassigned == false /*alignment stable*/return
During initialization, tuples whose pairwise simi-
larity higher than a threshold ? are merged to form
highly cohesive seed clusters. To compute a con-
tinuous similarity Sim(ta, tb) of tuples ta and tb,we use the similarity measure described in (Qiu et
al., 2006), which linearly combines similarities be-
tween the semantic roles shared by the two tuples.
Some other tuples are related to these seed clus-
ters by argument-similarity. These related tuples are
temporarily put into a special ?other? cluster. The
cluster membership of these related tuples, together
with those currently in the seed clusters, are to be
further adjusted. The ?other? cluster is so called be-
cause a tuple will end up being assigned to it if it
is not found to be similar to any other tuple. Tuples
that are neither similar to nor contextually related by
argument-similarity to another tuple are termed sin-
gletons and excluded from being clustered.
We then iteratively (re-)estimate clusters of tuples
across the set of article graphs G. In the E-step of the
EM algorithm, all contextual relations between each
pair of clusters are collected as two set of edges.
Here we assume argument-similarity and position-
similarity are independent and thus we differenti-
ate them in the computation. Accordingly, there
are two sets: edgesas and edgesps. For simplicity,we assume independent normal distributions for the
strength of each set (inter-tuple argument similarity
for edgesas and sentence distance for edgesps). Theedge strength distribution parameters for both sets
between each pair of clusters are re-estimated based
on current edges in edgesas and edgesps.In the M-step, we examine each tuple?s fitness for
belonging to its cluster and relocate some tuples to
new clusters to maximize the likelihood given the
latest estimated edge strength distributions. In the
following equations, we denote the proposition that
predicate argument tuple ta belongs to cluster cm as
ta?cm; a typical tuple (the centroid) of the cluster
cm as tcm ; and the cluster of ta as cta . The objectivefunction to maximize is:
Obj(G) =
X
ta?G
log(P (ta?cta)), (1)
where P (ta?cm) = 2Ps(ta?cm) Pc(ta?cm)Ps(ta?cm) + Pc(ta?cm) . (2)
Equation 2 takes the harmonic mean of two factors:
a contextual factor Pc and and a semantic factor Ps:
Pc(ta?cm) = max{P (edges(ta, tb)|
tb:edges(ta,tb)6=null
edges(cm, ctb ))}, (3)
Ps(ta?cm) =
(
simdefault, cm = cother,
Sim(ta, tcm), otherwise. (4)
Here the contextual factor Pc models how likely
ta belongs to cm according to the contextual infor-mation, i.e., the conditional probability of the con-
textual relations between cm and ctb given the con-textual relations between ta and one particular con-text tb, which maximizes this probability. Accord-ing to Bayes? theorem, it is computed as shown in
Equation 3. In practice, we multiply two conditional
probabilities: P (edgeas(ta, tb)|edgesas(cm, ctb))and P (edgeps(ta, tb)|edgesps(cm, ctb)), assumingindependence between edgesas and edgesps.We assume there are still singleton tuples that are
not semantically similar to another tuple and should
belong to the special ?other? cluster. Given that they
160
are dissimilar to each other, we set simdefault toa small nonzero value in Equation 4 to prevent the
?other? cluster from expelling them based on their
low semantic similarity. Tuples? cluster member-
ships are recalculated, and the parameters describ-
ing the contextual relations between clusters are re-
estimated. New EM iterations are performed as long
as one or more tuple relocations occur. Once the
EM halts, clusters of equivalent tuples are formed.
Among these clusters, some correspond to salient
actions that, together with their actors, are all SAs
to be generalized into template slots. Cluster size
is a good indicator of salience, and each large clus-
ter (excluding the ?other? cluster) can be viewed as
containing instances of a salient action.
Formulating the clustering process as a variant of
iterative EM is well-motivated as we consider the
similarity scores as noisy and having missing obser-
vations. Calculating semantic similarity is at best
inaccurate. Thus it is difficult to cluster tuples cor-
rectly based only on their semantic similarity. Also
to check whether a tuple shares contexts with a clus-
ter of tuples, the cluster has to be relatively clean.
An iterative EM as we have proposed naturally im-
prove the cleanness of these tuple clusters gradually
as new similarity information comes to light.
4 Evaluation
For STC, we argue that it is crucial to cluster tuples
with high recall so that an SA?s various surface
forms can be captured and the size of clusters can
serve as a salience indicator. Meanwhile, precision
should not be sacrificed, as more noise will hamper
the downstream generalization process which
outputs template slots. We conduct experiments
designed to answer two relevant research questions:
1) Cluster Quality: Whether using contexts (in
CSC) produces better clustering results than ignor-
ing it (in the K-means baseline); and
2) Template Coverage: Whether slots generalized
from CSC clusters cover human-defined templates.
4.1 Data Set and Baseline
A straightforward evaluation of a STC system would
compare its output against manually-prepared gold
standard templates, such as those found in MUC.
Unfortunately, such scenario templates are severely
limited and do not provide enough instances for a
proper evaluation. To overcome this problem, we
have prepared a balanced news corpus, where we
have manually selected articles covering 15 scenar-
ios. Each scenario is represented by a total of 45 to
50 articles which describe 10 different events.
Our baseline is a standard K-means clusterer. Its
input is identical to that of CSC ? the tuples ex-
tracted from relevant news articles and are not ex-
cluded from being clustered by CSC in the initial-
ization stage (refer to Section 3) ? and employs the
same tuple similarity measure (Qiu et al, 2006). The
differentiating factor between CSC and K-means is
the use of contextual evidence. A standard K-means
clusterer requires a k to be specified. For each sce-
nario, we set its k as the number of clusters gener-
ated by CSC for direct comparison.
We fix the test set for each scenario as ten ran-
domly selected news articles, each reporting a dif-
ferent instance of the scenario; the development set
(which also serves as the training set for determin-
ing the EM initialization threshold ? and simdefaultin Equation 4) is a set of ten articles from the ?Air-
linerCrash? scenario, which are excluded from the
test set. Both systems analyze the first 15 sentences
of each article, and sentences generate 2 to 3 predi-
cate argument tuples on average, resulting in a total
of 10 ? 15 ? (2 to 3) = 300 to 450 tuples for each
scenario.
4.2 Cluster Quality
This experiment compares the clustering results of
CSC and K-means. We use the standard cluster-
ing metrics of purity and inverse purity (Hotho et
al., 2003). The first author manually constructed the
gold standard clusters for each scenario using a GUI
before conducting any experiments. A special clus-
ter, corresponding to the ?other? cluster in the CSC
clusters, was created to hold the singleton tuples for
each scenario. Table 1 shows this under the column
?#Gold Standard Clusters?.
Using the manual clusters as the gold standard, we
obtain the purity (P) and inverse purity (IP) scores
of CSC and K-means on each scenario. In Table 1,
we see that CSC outperforms K-means on 10 of 15
scenarios for both P and IP. For the remaining 5 sce-
narios, where CSC and K-means have comparable
161
P scores, the IP scores of CSC are all significantly
higher than that of K-means. This suggests clus-
ters tend to be split apart more in K-means than in
CSC when they have similar purity. One thing worth
mentioning here is that the ?other? cluster normally
is relatively large for each scenario, and thus may
skew the results. To remove this effect, we excluded
tuples belonging to the CSC ?other? cluster from the
K-means input, generating one fewer cluster. Run-
ning the evaluation again, the resulting P-IP scores
again show that CSC outperforms the baseline K-
means. We only report the results for all tuples in
our paper for simplicity.
#Gold Std. CSC K-meansScenario Clusters P IP P IP
AirlinerCrash 23 .61 .42 .52 .28
Earthquake 18 .60 .44 .53 .30
Election 10 .77 .49 .75 .21
Fire 14 .65 .44 .64 .26
LaunchEvent 12 .77 .37 .73 .22
Layoff 10 .71 .28 .70 .19
LegalCase 8 .75 .37 .75 .18
Nobel 6 .77 .28 .77 .19
Obituary 7 .85 .46 .81 .28
RoadAccident 20 .61 .49 .56 .40
SoccerFinal 5 .88 .39 .88 .15
Storm 14 .61 .31 .61 .22
Tennis 6 .87 .19 .87 .12
TerroristAttack 14 .64 .48 .62 .25
Volcano 16 .68 .38 .66 .17Average 12.2 .72 .39 .69 .23
Table 1: CSC outperforms K-means with respect to
the purity (P) and inverse purity (IP) scores.
A close inspection of the results reveals some
problematic cases. One issue worth mentioning is
that for certain actions both CSC and K-means pro-
duce split clusters. In the CSC case, we traced this
problem back to the thesaurus, where predicates for
one action seem to belong to two or more totally dis-
similar semantic categories. The corresponding tu-
ples are thus assigned to different clusters as their
low semantic similarity forces the tuples to remain
separate, despite the shared contexts trying to join
them. One example is ?blast (off)? and ?lift (off)? in
the ?Launch Event? scenario. The thesaurus shows
the two verbs are dissimilar and the corresponding
tuples end up being in two split clusters. This can
not be solved easily without an improved thesaurus.
We are considering adding a prior to model the op-
timal size for clusters, which may help to compact
such cases.
4.3 Template Coverage
We also assess how well the resulting, CSC-
generated tuple clusters serve in creating good sce-
nario template slots. We start from the top largest
clusters from each scenario, and decompose each
of them into six sets: the predicates, agents, pa-
tients, predicate modiers, agent modiers and pa-
tient modiers. For each of the first three sets for
each cluster, we create a generalized term to repre-
sent it using an extended version of a generaliza-
tion algorithm (Tseng et al, 2006). These terms
are deemed output slots, and are put into the tem-
plate with their agent-predicate-patient relations pre-
served. The size of the template may increase when
more clusters are generalized, as new slots may re-
sult.
We manually compare the slots that are output
from the system with those defined in existing sce-
nario templates in MUC. The results here are only
indicative and not conclusive, as there are only two
MUC7 templates available for comparison: Aviation
Disaster and Launch Event.
Template semantic role general term
action crash
cluster 1 agent aircraft
patient ?
action kill
cluster 2 agent heavier-than-
air-craft
patient people
Figure 4: Automated scenario template of ?Avia-
tionDisaster?.
Figure 4 shows an excerpt of the automatically
generated template ?AviationDisaster? (?Airliner-
Crash? in our corpus) where the semantic roles in
the top two biggest clusters have been generalized.
Their modifiers are quite semantically diverse, as
shown in Table 2. Thus, generalization (probably
after a categorization operation) remains as a chal-
lenging problem.
Nonetheless, the information contained in these
semantic roles and their modifiers covers human-
162
semantic role modifier head samples
agent:aircraft A, U.N., The, Swiss, Canadian-
built, AN, China, CRJ-200, mil-
itary, Iranian, Air, refueling, US,
...
action:crash Siberia, mountain, rain, Tues-
day, flight, Sharjah, flames, Sun-
day, board, Saturday, 225, Rock-
away, approach, United, moun-
tain, hillside
patient:people all, 255, 71
Table 2: Sample automatically detected modifier
heads of different semantic roles.
AviationDisaster LaunchEvent
* AIRCRAFT * VEHICLE
* AIRLINE * VEHICLE TYPE
DEPARTURE POINT * VEHICLE OWNER
DEPARTURE DATE * PAYLOAD
* AIRCRAFT TYPE PAYLOAD TYPE
* CRASH DATE PAYLOAD FUNC
* CRASH SITE * PAYLOAD OWNER
CAUSE INFO PAYLOAD ORIGIN
* VICTIMS NUM * LAUNCH DATE
* LAUNCH SITE
MISSION TYPE
MISSION FUNCTION
MISSION STATUS
Figure 5: MUC-7 template coverage: asterisks
marking all the slots that could be automatically
generated.
defined scenario templates quite well. The two
MUC7 templates are shown as a list of slots in Fig-
ure 5, where horizontal lines delimit slots about dif-
ferent semantic roles, and asterisks mark all the slots
that could be automatically generated by our system
once it has an improved generalizer. We can see
substantial amount of overlap, indicating that a STC
system powered by CSC is able to capture scenarios?
important facts.
5 Conclusion
We have introduced a new context-sensitive ap-
proach to the scenario template creation (STC) prob-
lem. Our method leverages deep NL processing, us-
ing semantic role labeler?s structured semantic tu-
ples as input. Despite the use of deeper semantics,
we believe that intrinsic semantic similarity by itself
is not sufficient for clustering. We have shown this
through examples and argue that an approach that
considers contextual similarity is necessary. A key
aspect of our work is the incorporation of such con-
textual information. Our approach uses a notion of
context that combines two aspects: positional simi-
larity (when two tuples are adjacent in the text), and
argument similarity (when they have similar argu-
ments). The set of relevant articles are represented
as graphs where contextual evidence is encoded.
By mapping our problem into a graphical formal-
ism, we cast the STC clustering problem as one of
multiple graph alignment. Such a graph alignment is
solved by an adaptation of EM, which handles con-
texts and real-valued similarity by treating both as
noisy and potentially unreliable observations.
While scenario template creation (STC) is a dif-
ficult problem, its evaluation is arguably more dif-
ficult due to the dearth of suitable resources. We
have compiled and released a corpus of over 700
newswire articles that describe different instances of
15 scenarios, as a suitable input dataset for further
STC research. Using this dataset, we have evaluated
and analyzed our context-sensitive approach. While
our results are indicative, they show that considering
contextual evidence improves performance.
Acknowledgments
The authors are grateful to Kathleen R. McKeown
and Elena Filatova at Columbia University for their
stimulating discussions and comments over different
stages of the preparation of this paper.
References
Robin Collier. 1998. Automatic Template Creation forInformation Extraction. Ph.D. thesis, University ofSheffield, UK.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-gorithm. JRSSB, 39:1?38.
Elena Filatova, Vasileios Hatzivassiloglou, and KathleenMcKeown. 2006. Automatic creation of domain tem-plates. In Proceedings of the COLING/ACL ?06.
Sanda M. Harabagiu and V. Finley Lacatusu. 2005.
Topic themes for multi-document summarization. InProceedings of SIGIR ?05.
163
Sanda M. Harabagiu and S. J. Maiorano. 2002. Multi-
document summarization with GISTEXTER. In Pro-ceedings of LREC ?02.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-recting real-word spelling errors by restoring lexical
cohesion. Natural Language Engineering, 11(1).
Andreas Hotho, Steffen Staab, and Gerd Stumme. 2003.WordNet improves text document clustering. In Pro-ceedings of the SIGIR 2003 Semantic Web Workshop.
LDC. 2002. The aquaint corpus of english news text,
catalog no. LDC2002t31.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empiri-cal evaluation of knowledge sources and learning algo-rithms for word sense disambiguation. In Proceedingsof EMNLP ?02.
Dekang Lin. 1998. Automatic retrieval and clustering ofsimilar words. In Proceedings of COLING/ACL ?98.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-ings of HLT/NAACL ?04.
Judita Preiss. 2001. Local versus global context for wsdof nouns. In Proceedings of CLUK4.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.Paraphrase recognition via dissimilarity significanceclassification. In Proceedings of EMNLP ?06.
Ellen Riloff and M. Schmelzenbach. 1998. An empiri-
cal approach to conceptual case frame acquisition. InProceedings of WVLC ?98.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003. An improved extraction pattern representation
model for automatic IE pattern acquisition. In Pro-ceedings of ACL ?03.
Yuen-Hsien Tseng, Chi-Jen Lin, Hsiu-Han Chen, and Yu-I Lin. 2006. Toward generic title generation for clus-
tered documents. In Proceedings of AIRS ?06.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen. 2000. Unsupervised discovery ofscenario-level patterns for information extraction. InProceedings of ANLP ?00.
164
 
Building Semantic Perceptron Net for Topic Spotting 
 
Jimin Liu  and  Tat-Seng Chua 
School of Computing 
National University of Singapore 
SINGAPORE 117543 
  {liujm, chuats}@comp.nus.edu.sg 
 
 
 
 
Abstract 
This paper presents an approach to 
automatically build a semantic 
perceptron net (SPN) for topic spotting. 
It uses context at the lower layer to 
select the exact meaning of key words, 
and employs a combination of context, 
co-occurrence statistics and thesaurus to 
group the distributed but semantically 
related words within a topic to form 
basic semantic nodes. The semantic 
nodes are then used to infer the topic 
within an input document. Experiments 
on Reuters 21578 data set demonstrate 
that SPN is able to capture the semantics 
of topics, and it performs well on topic 
spotting task. 
 
1. Introduction 
 
Topic spotting is the problem of identifying the 
presence of a predefined topic in a text document. 
More formally, given a set of n topics together with 
a collection of documents, the task is to determine 
for each document the probability that one or more 
topics is present in the document. Topic spotting 
may be used to automatically assign subject codes 
to newswire stories, filter electronic emails and on-
line news, and pre-screen document in information 
retrieval and information extraction applications. 
Topic spotting, and its related problem of text 
categorization, has been a hot area of research for 
over a decade. A large number of techniques have 
been proposed to tackle the problem, including: 
regression model, nearest neighbor classification, 
Bayesian probabilistic model, decision tree, 
inductive rule learning, neural network, on-line 
learning, and, support vector machine (Yang & Liu, 
1999; Tzeras & Hartmann, 1993). Most of these 
methods are word-based and consider only the 
relationships between the features and topics, but 
not the relationships among features. 
It is well known that the performance of the 
word-based methods is greatly affected by the lack 
of linguistic understanding, and, in particular, the 
inability to handle synonymy and polysemy. A 
number of simple linguistic techniques has been 
developed to alleviate such problems, ranging from 
the use of stemming, lexical chain and thesaurus 
(Jing & Tzoukermann, 1999; Green, 1999), to 
word-sense disambiguation (Chen & Chang, 1998; 
Leacock et al 1998; Ide & Veronis, 1998) and 
context (Cohen & Singer, 1999; Jing & 
Tzoukermann, 1999). 
The connectionist approach has been widely 
used to extract knowledge in a wide range of 
information processing tasks including natural 
language processing, information retrieval and 
image understanding (Anderson, 1983; Lee & 
Dubin, 1999; Sarkas & Boyer, 1995; Wang & 
Terman, 1995). Because the connectionist 
approach closely resembling human cognition 
process in text processing, it seems natural to adopt 
this approach, in conjunction with linguistic 
analysis, to perform topic spotting. However, there 
have been few attempts in this direction. This is 
mainly because of difficulties in automatically 
constructing the semantic networks for the topics. 
In this paper, we propose an approach to 
automatically build a semantic perceptron net 
(SPN) for topic spotting. The SPN is a 
connectionist model with hierarchical structure. It 
uses a combination of context, co-occurrence 
statistics and thesaurus to group the distributed but 
semantically related words to form basic semantic 
nodes. The semantic nodes are then used to identify 
the topic. This paper discusses the design, 
implementation and testing of an SPN for topic 
spotting. 
The paper is organized as follows. Section 2 
discusses the topic representation, which is the 
prototype structure for SPN. Sections 3 & 4 
respectively discuss our approach to extract the 
semantic correlations between words, and build 
semantic groups and topic tree. Section 5 describes 
the building and training of SPN, while Section 6 
presents the experiment results. Finally, Section 7 
concludes the paper. 
2. Topic Representation 
 
The frame of Minsky (1975) is a well-known 
knowledge representation technique. A frame 
represents a high-level concept as a collection of 
slots, where each slot describes one aspect of the 
concept. The situation is similar in topic spotting. 
For example, the topic ?water? may have many 
aspects (or sub-topics). One sub-topic may be 
about ?water supply?, while the other is about 
?water and environment protection?, and so on. 
These sub-topics may have some common 
attributes, such as the word ?water?, and each sub-
topic may be further sub-divided into finer sub-
topics, etc. 
The above points to a hierarchical topic 
representation, which corresponds to the hierarchy 
of document classes (Figure 1). In the model, the 
contents of the topics and sub-topics (shown as 
circles) are modeled by a set of attributes, which is 
simply a group of semantically related words 
(shown as solid elliptical shaped bags or 
rectangles). The context (shown as dotted ellipses) 
is used to identify the exact meaning of a word. 
 
topic 
a word 
the context of a word 
Sub-topic  
Aspect attribute 
common attribute 
 
Figure 1. Topic representation 
Hofmann (1998) presented a word occurrence 
based cluster abstraction model that learns a 
hierarchical topic representation. However, the 
method is not suitable when the set of training 
examples is sparse. To avoid the problem of 
automatically constructing the hierarchical model, 
Tong et al(1987) required the users to supply the 
model, which is used as queries in the system. 
Most automated methods, however, avoided this 
problem by modeling the topic as a feature vector, 
rule set, or instantiated example (Yang & Liu, 
1999). These methods typically treat each word 
feature as independent, and seldom consider 
linguistic factors such as the context or lexical 
chain relations among the features. As a result, 
these methods are not good at discriminating a 
large number of documents that typically lie near 
the boundary of two or more topics. 
In order to facilitate the automatic extraction 
and modeling of the semantic aspects of topics, we 
adopt a compromise approach. We model the topic 
as a tree of concepts as shown in Figure 1. 
However, we consider only one level of hierarchy 
built from groups of semantically related words. 
These semantic groups may not correspond strictly 
to sub-topics within the domain. Figure 2 shows an 
example of an automatically constructed topic tree 
on ?water?. 
 
Contexts 
Basic Semantic 
 Nodes 
Topic 
   price 
agreement  
   water 
    ton 
  
   waste 
environment 
    bank 
   provide 
costumer 
corporation 
      plant  
 
  rain 
rainfall 
  dry  water  
water 
 
water 
river 
tourist 
  f e 
d c b a 
 
Figure 2. An example of a topic tree 
In Figure 2, node ?a? contains the common 
feature set of the topic; while nodes ?b?, ?c? and 
?d? are related to sub-topics on ?water supply?, 
?rainfall?, and ?water and environment protection? 
respectively. Node ?e? is the context of the word 
?plant?, and node ?f? is the context of the word 
?bank?. Here we use training to automatically 
resolve the corresponding relationship between a 
node and an attribute, and the context word to be 
used to select the exact meaning of a word. From 
this representation, we observe that: 
a) Nodes ?c? and ?d? are closely related and may 
not be fully separable. In fact, it is sometimes 
difficult even for human experts to decide how 
to divide them into separate topics. 
b) The same word, such as ?water?, may appear in 
both the context node and the basic semantic 
node.  
c) Some words use context to resolve their 
meanings, while many do not need context. 
3. Semantic Correlations  
 
Although there exists many methods to derive the 
semantic correlations between words (Lee, 1999; 
Lin, 1998; Karov & Edelman, 1998; Resnik, 1995; 
Dagan et al 1995), we adopt a relatively simple 
and yet practical and effective approach to derive 
three topic -oriented semantic correlations: 
thesaurus-based, co-occurrence-based and context-
based correlation.  
3.1 Thesaurus based correlation  
WordNet is an electronic thesaurus popularly used 
in many researches on lexical semantic acquisition, 
and word sense disambiguation (Green, 1999; 
Leacock et al 1998). In WordNet, the sense of a 
word is represented by a list of synonyms (synset), 
and the lexical information is represented in the 
form of a semantic network. 
However, it is well known that the granularity 
of semantic meanings of words in WordNet is often 
too fine for practical use. We thus need to enlarge 
the semantic granularity of words in practical 
applications. For example, given a topic on 
?children education?, it is highly likely that the 
word ?child? will be a key term. However, the 
concept ?child? can be expressed in many 
semantically related terms, such as ?boy?, ?girl?, 
?kid?, ?child?, ?youngster?, etc. In this case, it 
might not be necessary to distinguish the different 
meaning among these words, nor the different 
senses within each word. It is, however, important 
to group all these words into a large synset {child, 
boy, girl, kid, youngster}, and use the synset to 
model the dominant but more general meaning of 
these words in the context. 
In general, it is reasonable and often useful to 
group lexically related words together to represent 
a more general concept. Here, two words are 
considered to be lexically related if they are related 
to by the ?is_a?, ?part_of?, ?member_of?, or 
?antonym? relations, or if they belong to the same 
synset. Figure 3 lists the lexical relations that we 
considered, and the examples. 
Since in our experiment, there are many 
antonyms co-occur within the topic, we also group 
antonyms together to identify a topic. Moreover, if 
a word had two senses of, say, sense-1 and sense-2. 
And if there are two separate words that are 
lexically related to this word by sense-1 and sense-
2 respectively, we simply group these words 
together and do not attempt to distinguish the two 
different senses. The reason is because if a word is 
so important to be chosen as the keyword of a 
topic, then it should only have one dominant 
meaning in that topic. The idea that a keyword 
should have only one dominant meaning in a topic 
is also suggested in Church & Yarowsky (1992). 
 
corn 
 
maize 
metal 
zinc  
per
import 
export  
perso
synset         is_a              part_of       member_of      antonym 
tree 
leaf  
family  
son  
per  Figure 3: Examples of lexical relationship 
Based on the above discussion, we compute the 
thesaurus-based correlation between the two terms 
t1 and t2, in topic Ti, as: 
                         1    (t1 and t2 are in the same synset, or t1=t2) 
                         0.8  (t1 and t2 have ?antonym? relation) 
0..5  (t1 and t2 have relations of ?is_a?,  
          ?part_of?, or ?member_of?) 
                        0   (others) 
=),( 21)( ttR iL
 
3.2 Co-occurrence based correlation 
Co-occurrence relationship is like the global 
context of words. Using co-occurrence statistics, 
Veling & van der Weerd (1999) was able to find 
many interesting conceptual groups in the Reuters-
2178 text corpus. Examples of the conceptual 
groups found include: {water, rainfall, dry}, 
{bomb, injured, explosion, injuries}, and {cola, 
PEP, Pepsi, Pespi-cola, Pepsico}. These groups 
are meaningful, and are able to capture the 
important concepts within the corpus. 
Since in general, high co-occurrence words are 
likely to be used together to represent (or describe) 
a certain concept, it is reasonable to group them 
together to form a large semantic node. Thus for 
topic Ti, the co-occurrence-based correlation of two 
terms, t1 and t2, is computed as: 
)(/)(),( 21
)(
21
)(
21
)( ttdfttdfttR iiico ??=  (2) 
where )( 21)( ttdf i ?  ( )( 21)( ttdf i ? ) is the fraction of 
documents in Ti that contains t1 and (or) t2. 
3.3 Context based correlation 
Broadly speaking, there are three kinds of context: 
domain, topic and local contexts (Ide & Vernois, 
1998). Domain context requires extensive 
knowledge of domain and is not considered in this 
paper. Topic context can be modeled 
approximately using the co-occurrence 
(1) 
relationships between the words in the topic. In this 
section, we will define the local context explicitly. 
The local context of a word t is often defined as  
the set of non-trivial words near t. Here a word wd 
is said to be near t if their word distance is less than 
a given threshold, which is set to be 5 in our 
experiment. 
We represent the local context of term tj in topic 
Ti by a context vector cv(i)(tj). To derive cv(i)(tj), we 
first rank all candidate context words of ti by their 
density values: 
)(/)( )()()( jikij
i
jk tnwdm=r  (3) 
where )()( ji tn is the number of occurrence of tj in 
Ti, and )()( kij wdm is the number of occurrences of 
wdk near t j. We then select from the ranking, the top 
ten words as the context of tj in Ti as: 
),(),...,,(),,{()( )(10
)(
10
)(
2
)(
2
)(
1
)(
1
)( i
j
i
j
i
j
i
j
i
j
i
jj
i wdwdwdtcv rrr=  (4) 
When the training sample is sufficiently large, 
the context vector will have good statistic 
meanings. Noting again that an important word to a 
topic should have only one dominant meaning 
within that topic, and this meaning should be 
reflected by its context. We can thus draw the 
conclusion that if two words have a very high 
context similarity within a topic, it will have a high 
possibility that they are semantic related. Therefore 
it is reasonable to group them together to form a 
larger semantic node. We thus compute the 
context-based correlation between two term t1 and 
t2 in topic Ti as: 
2/12)(
2
2/12)(
1
10
1
)(
)(2
)(
1
)(
)(2
)(
1
)(
21)(
])([*])([
**),(
),(
??
?
= =
k
i
kk
i
k
k
i
km
i
k
i
km
i
k
i
co
ic
wdwdR
ttR
rr
rr
 (5) 
where  ),(maxarg)( )(2
)(
1
)( i
s
i
k
i
co
s
wdwdRkm =  
For example, in Reuters 21578 corpus, 
?company? and ?corp? are context-related words 
within the topic ?acq?. This is because they have 
very similar context of ?say, header, acquire, 
contract?. 
4. Semantic Groups & Topic Tree 
 
There are many methods that attempt to construct 
the conceptual representation of a topic from the 
original data set (Veling & van der Weerd, 1999; 
Baker & McCallum, 1998; Pereira et al 1993). In 
this Section, we will describe our semantic -based 
approach to finding basic semantic groups and 
constructing the topic tree. Given a set of training 
documents, the stages involved in finding the 
semantic groups for each topic are given below. 
A) Extract all distinct terms {t1,  t2, ..tn} from the 
training document set for topic Ti. For each term 
tj, compute its df(i)(tj) and cv(i)(t j), where df(i)(tj)  
is defined as the fraction of documents in T i that 
contain tj. In other words, df (i)(tj) gives the 
conditional probability of tj appearing in Ti. 
B)  Derive the semantic group Gj using tj as the 
main keyword. Here we use the semantic 
correlations defined in Section 3 to derive the 
semantic relationship between tj and any other 
term tk. Thus: 
 For each pair (t j,tk), k=1,..n,  set Link(tj,tk)=1 
 if )( iLR (tj,tk)>0,   or,    
  df (i)(tj)>d0  and  )(icoR (tj, tk)>d1  or 
  df (i)(tj)>d2  and  )(icR  (tj, tk)>d 3. 
 where d0, d1, d2, d3  are predefined thresholds. 
 For all tk with Link(tj,tk)=1, we form a semantic 
group centered around tj denoted by: 
 },...,,{},...,,{ 2121 njjjj ttttttG jk ?=  
(6) 
 Here tj is the main keyword of node G j and is 
denoted by  main(Gj)=t j. 
C)  Calculate the information value inf (i)(Gj) of each 
basic semantic group. First we compute the 
information value of each tj: 
  }1,0max{*)()(inf )()( Nptdft ijj
i
j
i -=  (7) 
 where     
?
=
=
N
k
ki
j
i
ij
tdf
tdfp
1
)(
)(
)(
)(  
 and N is the number of topics. Thus 1/N  denotes 
the probability that a term is in any class, and pij 
denotes the normalized conditional probability 
of tj in Ti. Only those terms whose normalized 
conditional probability is higher than 1/N will 
have a positive information value. 
 The information value of the semantic group Gj 
is simply the summation of information value of 
its constituent terms weighted by their 
maximum semantic correlation with t j as: 
 ?=
=
jk
k
ki
i
jkj
i twG
1
)()()( )](inf*[)(inf  (8) 
 where )},(),,(),,(max{ )()()()( kjiLkj
i
ckj
i
co
i
jk ttRttRttRw =  
D) Select the essential semantic groups using the 
following algorithm: 
 a) Initialize: 
  },...,,{ 11 nGGGS ? ,  F?Groups , 
b) Select the semantic group with highest 
information value: 
 ))((infmaxarg )( kiSGk
Gj
k ?
?  
 c) Terminate if inf (i)(Gj) is less than a 
predefined threshold d4. 
 d) Add Gj into the set Groups: 
  jGSS -= ,  and  }{ jGGroupsGroups ??  
 e) Eliminate those groups in S whose key terms 
appear in the selected group Gj. That is: 
  For each  SGk ? , if jk GGmain ?)( , then  
 }{ kGSS -?  
 f) Eliminate those terms in remaining groups in 
S that are found in the selected group G j. 
That is: 
  For each SGk ? ,  jkk GGG -? ,  
  and if F=kG , then  }{ kGSS -?  
 g) If F=S  then stop; else go to step (b). 
In the above grouping algorithm, the predefined 
thresholds d0,d1,d2,d3 are used to control the size of 
each group, and d4 is used to control the number of 
groups. 
The set of basic semantic groups found then 
forms the sub-topics of a 2-layered topic tree as 
illustrated in Figure 2. 
5. Building and Training of SPN 
 
The Combination of local perception and global 
arbitrator has been applied to solve perception 
problems (Wang & Terman, 1995; Liu & Shi, 
2000). Here we adopt the same strategy for topic 
spotting. For each topic, we construct a local 
perceptron net (LPN), which is designed for a 
particular topic. We use a global expert (GE) to 
arbitrate all decisions of LPNs and to model the 
relationships between topics. Here we discuss the 
design of both LPN and GE, and their training 
processes. 
5.1 Local Perceptron Net (LPN) 
We derive the LPN directly from the topic tree as 
discussed in Sectio n 2 (see Figure 2). Each LPN is 
a multi- layer feed-forward neural network with a 
typical structure as shown in Figure 4. 
In Figure 4, x ij represents the feature value of 
keyword wdi j in the ith semantic group; xijk?s (where 
k=1,?10) represent the feature values of the context 
words wdijk?s of keyword wd ij; and aij denotes the 
meaning of keyword wd i j as determined by its 
context. Ai corresponds to the ith basic semantic 
node. The weights wi, wi j, and wijk and biases ?i and 
? ij are learned from training, and y(i)(x) is the output 
of the network. 
 y(i) 
  iA  
  iw  
  ijw  
  ijkw  
  ija  
  ijkx  
Context  key term  
Semantic 
group 
Class 
  ijx  
Basic 
meaning 
q (i) 
  ijq  
 
Figure 4: The architecture of LPN for topic i 
Given a document: 
 x = {(xi j,cv i j) | i=1,2,?m, j=1,?ij} 
where m is the number of basic semantic nodes, ij 
is the number of key terms contained in the i th 
semantic node, and cv ij={xi j1,xi j2? ijijkx } is the 
context of term x ij. The output y(i) =y(i)(x) is 
calculated as follows: 
 ?==
=
m
i
ii
ii Awxyy
1
)()( )(  (9) 
where  ])*(exp[1
1*
? --+
=
? ijijk cvx
ijijkijk
ijij xw
xa
q  (10) 
and     
)exp(1
)exp(1
1
1
?-+
?--
=
=
=
j
j
i
j
iji
i
j
iji
i
aw
aw
A
 (11) 
Equation (10) expresses the fact that only if a 
key term is present in the document (i.e. xij > 0), its 
context needs to be checked. 
For each topic Ti, there is a corresponding net 
y(i) =y(i)(x) and a threshold q(i). The pair of (y(i)(x), 
q(i)) is a local binary classifier for Ti such that: 
If y(i)(x)-q(i) > 0, then Ti is present; otherwise 
  Ti is not present in document x. 
From the procedures employed to building the 
topic tree, we know that each feature is in fact an 
evidence to support the occurrence of the topic. 
This gives us the suggestion that the activation 
function for each node in the LPN should be a non-
decreasing function of the inputs. Thus we impose 
a weight constraint on the LPN as: 
 wi>0,  wi j>0,  wijk>0 (12) 
5.2 Global expert (GE) 
Since there are relations among topics, and LPNs 
do not have global information, it is inevitable that 
LPNs will make wrong decisions. In order to 
overcome this problem, we use a global expert 
(GE) to arbitrate al local decisions. Figure 5 
illustrates the use of global expert to combine the 
outputs of LPNs. 
 
)()( iiy q-
Y(i)  
)()( jjy q-
ijW  
)1()1( q-y
 )(iQ  
 
Figure 5: The architecture of global expert 
Given a document x, we first use each LPN to 
make a local decision. We then combine the 
outputs of LPNs as follows: 
])([)( )()()()()( )(
0)()(
ij
ij
iii j
jjy
ij
yWyY Q--?+-=
>-
?
qq
q
 (13) 
where Wij?s are the weights between the global 
arbitrator i and the j th LPN; and )(iQ ?s are the 
global bias. From the result of Equation (13), we 
have: 
If Y(i) > 0; then topic Ti is present; otherwise 
  Ti is not present in document x  
The use of Equation (13) implies that: 
a) If a LPN is not activated, i.e., y(i)  ? q(i), then its 
output is not used in the GE. Thus it will not 
affect the output of other LPN.  
b) The weight Wi j models the relationship or 
correlation between topic i and j. If Wi j > 0, it 
means that if document x is related to Tj, it may 
also have some contribution ( Wij) to topic T j. On 
the other hand, if Wi j < 0, it means the two 
topics are negatively correlated, and a document 
x will not be related to both Tj and Ti. 
The overall structure of SPN is as follows: 
 
Input document 
Local Perception 
Global Expert 
x 
y(i) 
Y(i) 
 
Figure 6: Overall structure of SPN 
5.3 The Training of SPN 
In order to adopt SPN for topic spotting, we 
employ the well-known BP algorithm to derive the 
optimal weights and biases in SPN. The training 
phase is divided to two stages. The first stage 
learns a LPN for each topic, while the second stage 
trains the GE. As the BP algorithm is rather 
standard, we will discuss only the error functions 
that we employ to guide the training process. 
In topic spotting, the goal is to achieve both 
high recall and precision. In particular, we want to 
allow y(x) to be as large (or as small) as possible in 
cases when there is no error, or when +W?x  and 
q>)(xy  (or -W?x  and q<)( xy ). Here +W  and -W  
denote the positive and negative training document 
sets respectively. To achieve this, we adopt a new 
error function as follows to train the LPN: 
?
W+W
W+
?
W+W
W=
-
+
W?
-
+-
+
W?
+
+-
-
x
x
iijijijk
xy
xywwwE
)),((
||||
||
)),((
||||
||),,,,(
qe
qeqq
 (14) 
where 
??
??
?
?
<-=+
)(0
)()(
2
1
),(
2
q
qqqe
x
xxx ,  and 
 ),(),( qeqe --= +- xx  
Equation (14) defines a piecewise differentiable 
error function. The coefficients 
||||
||
+-
-
W+W
W  and 
||||
||
+-
+
W+W
W  are used to ensure that the contributions 
of positive and negative examples are equal. 
After the training, we choose the node with the 
biggest wi value as the common attribute node. 
Also, we trim the topic representation by removing 
those words or context words with very small wij or 
wijk values. 
We adopt the following error function to train 
GE: 
? ? Q+? Q=Q
= W?
-
W?
+
-+
n
i x
iii
x
iiiiij
ii
xYxYWE
1
])),(()),(([),( ee  (15) 
where +W i  is the set of positive examples of Ti. 
6. Experiment and Discussion 
 
We employ the ModApte Split version of Reuters-
21578 corpus to test our method. In order to ensure 
that the training is meaningful, we select only those 
classes that have at least one document in each of 
the training and test sets. This results in 90 classes 
in both the training and test sets. After eliminating 
documents that do not belong to any of these 90 
classes, we obtain a training set of 7,770 
documents and a test set of 3,019 documents.  
From the set of training documents, we derive the 
set of semantic nodes for each topic using the 
procedures outlined in Section 4. From the training 
set, we found that the average number of semantic 
nodes for each topic is 132, and the average 
number of terms in each node is 2.4. For 
illustration, Table 1 lists some examples of the 
semantic nodes that we found. From table 1, we 
can draw the following general observations. 
Node 
ID 
Semantic Node 
(SN) 
Method used 
to find SNs 
Topic 
1 wheat  1 
2 import, export, 
output  
1,2,3 
3 farmer, production,  
mln, ton 
2 
4 disease, insect, pest 2 
 
 
Wheat 
5 fall, fell, rise, rose 3 Wpi 
Method 1 ? by looking up WordNet 
Method 2 ? by analyzing co-occurrence correlation 
Method 3 ? by analyzing context correlation  
Table 1: Examples of semantic nodes 
a) Under the topic ?wheat?, we list four semantic 
nodes. Node 1 contains the common attribute 
set of the topic. Node 2 is related to the ?buying 
and selling of wheat?. Node 3 is related to 
?wheat production?; and node 4 is related to 
?the effects of insect on wheat production?. The 
results show that the automatically extracted 
basic semantic nodes are meaningful and are 
able to capture most semantics of a topic. 
b) Node 1 originally contains two terms ?wheat? 
and ?corn? that belong to the same synset found 
by looking up WordNet. However, in the 
training stage, the weight of the word ?corn? 
was found to be very small in topic ?wheat?, 
and hence it was removed from the semantic 
group. This is similar to the discourse based 
word sense disambiguation.  
c) The granularity of information expressed by the 
semantic nodes may not be the same as what 
human expert produces. For example, it is 
possible that a human expert may divide node 2 
into two nodes {import} and {export, output}. 
d) Node 5 contains four words and is formed by 
analyzing context. Each context vector of the 
four words has the same two components: 
?price? and ?digital number?. Meanwhile, 
?rise? and ?fall? can also be grouped together 
by ?antonym? relation. ?fell? is actually the past 
tense of ?fall?. This means that by comparing 
context, it is possible to group together those 
words with grammatical variations without 
performing grammatical analysis. 
Table 2 summarizes the results of SPN in terms 
of macro and micro F1 values (see Yang & Liu 
(1999) for definitions of the macro and micro F1 
values). For comparison purpose, the Table also 
lists the results of other TC methods as reported in 
Yang & Liu (1999). From the table, it can be seen 
that the SPN method achieves the best macF1 
value. This indicates that the method performs well 
on classes with a small number of training samples. 
In terms of the micro F1 measures, SPN out-
performs NB, NNet, LSF and KNN, while posting 
a slightly lower performance than that of SVM. 
The results are encouraging as they are rather 
preliminary. We expect the results to improve 
further by tuning the system ranging from the 
initial values of various parameters, to the choice 
of error functions, context, grouping algorithm, and 
the structures of topic tree and SPN. 
Method MicR MicP micF1 macF1   
SVM 0.8120 0.9137 0.8599 0.5251 
KNN 0.8339 0.8807 0.8567 0.5242 
LSF 0.8507 0.8489 0.8498 0.5008 
NNet 0.7842 0.8785 0.8287 0.3763 
NB 0.7688 0.8245 0.7956 0.3886 
SPN 0.8402 0.8743 0.8569 0.6275 
Table 2. The performance comparison 
7. Conclusion 
 
In this paper, we proposed an approach to 
automatically build semantic perceptron net (SPN) 
for topic spotting. The SPN is a connectionist 
model in which context is used to select the exact 
meaning of a word. By analyzing the context and 
co-occurrence statistics, and by looking up 
thesaurus, it is able to group the distributed but 
semantic related words together to form basic 
semantic nodes. Experiments on Reuters 21578 
show that, to some extent, SPN is able to capture 
the semantics of topics and it performs well on 
topic spotting task. 
It is well known that human expert, whose most 
prominent characteristic is the ability to understand 
text documents, have a strong natural ability to spot 
topics in documents. We are, however, unclear 
about the nature of human cognition, and with the 
present state-of-art natural language processing 
technology, it is still difficult to get an in-depth 
understanding of a text passage. We believe that 
our proposed approach provides a promising 
compromise between full understanding and no 
understanding. 
Acknowledgment 
 
The authors would like to acknowledge the support 
of the National Science and Technology Board, and 
the Ministry of Education of Singapore for the 
provision of a research grant RP3989903 under 
which this research is carried out. 
References 
 
J.R. Anderson (1983). A Spreading Activation 
Theory of Memory. J. of Verbal Learning & 
Verbal Behavior, 22(3):261-295. 
L.D. Baker & A.K. McCallum (1998). 
Distributional Clustering of Words for Text 
Classification. SIGIR?98. 
J.N. Chen & J.S. Chang (1998). Topic Clustering 
of MRD Senses based on Information Retrieval 
Technique.  Comp Linguistic, 24(1), 62-95. 
G.W.K. Church & D. Yarowsky (1992). One Sense 
per Discourse. Proc. of 4th DARPA Speech and 
Natural Language Workshop. 233-237. 
W.W. Cohen & Y. Singer (1999). Context-
Sensitive Learning Method for Text 
Categorization. ACM Trans. on Information 
Systems, 17(2), 141-173, Apr. 
I. Dagan, S. Marcus & S. Markovitch (1995). 
Contextual Word Similarity and Estimation 
from Sparse Data. Computer speech and 
Language, 9:123-152. 
S.J. Green (1999). Building Hypertext Links by 
Computing Semantic Similarity. IEEE Trans on 
Knowledge & Data Engr, 11(5). 
T. Hofmann (1998). Learning and Representing 
Topic, a Hierarchical Mixture Model for Word 
Occurrences in Document Databases. 
Workshop on Learning from Text and the 
Web, CMU.  
N. Ide & J. Veronis (1998). Introduction to the 
Special Issue on Word Sense Disambiguation: 
the State of Art. Comp Linguistics, 24(1), 1-39. 
H. Jing & E. Tzoukermann (1999). Information 
Retrieval based on Context Distance and 
Morphology. SIGIR?99, 90-96. 
Y. Karov & S. Edelman (1998). Similarity-based 
Word Sense Disambiguation, Computational 
Linguistics, 24(1), 41-59. 
C. Leacock & M. Chodorow & G. Miller (1998). 
Using Corpus Statistics and WordNet for Sense 
Identification. Comp. Linguistic, 24(1), 147-
165. 
L. Lee (1999). Measure of Distributional 
Similarity.  Proc of 37 th Annual Meeting of 
ACL. 
J. Lee & D. Dubin (1999). Context-Sensitive 
Vocabulary Mapping with a Spreading 
Activation Network. SIGIR?99, 198-205. 
D. Lin (1998). Automatic Retrieval and Clust ering 
of Similar Words.  In COLING-ACL?98, 768-
773. 
J. Liu & Z. Shi (2000). Extracting Prominent 
Shape by Local Interactions and Global 
Optimizations. CVPRIP?2000, USA.  
M.A. Minsky (1975). A Framework for 
Representing Knowledge. In: Winston P (eds). 
?The psychology of computer vision?, 
McGraw-Hill, New York, 211-277. 
F.C.N. Pereira, N.Z. Tishby & L. Lee (1993). 
Distributional Clustering of English Words. 
ACL?93, 183-190. 
P. Resnik (1995). Using Information Content to 
Evaluate Semantic Similarity in a Taxonomy. 
Proc of IJCAI-95, 448-453. 
S. Sarkas & K.L. Boyer (1995). Using Perceptual 
Inference Network to Manage Vision 
Processes.  Computer Vision & Image 
Understanding, 62(1), 27-46. 
R. Tong, L. Appelbaum, V. Askman & J. 
Cunningham (1987). Conceptual Information  
Retrieval using RUBRIC.  SIGIR?87, 247? 253. 
K. Tzeras & S. Hartmann (1993). Automatic 
Indexing based on Bayesian Inference 
Networks. SIGIR?93, 22-34. 
A. Veling & P. van der Weerd (1999). Conceptual 
Grouping in Word Co-occurrence Networks. 
IJCAI 99: 694-701. 
D. Wang & D. Terman (1995). Locally Excitatory 
Globally Inhibitory Oscillator Networks. IEEE 
Trans. Neural Network. 6(1). 
Y. Yang & X. Liu (1999). Re-examination of Text 
Categorization. SIGIR?99, 43-49. 
 
 
Extracting Key Semantic Terms from Chinese Speech Query for Web
Searches

Gang WANG

National University of
Singapore
wanggang_sh@hotmail.com

Tat-Seng CHUA 

National University of Singa-
pore
chuats@comp.nus.edu.sg

Yong-Cheng WANG

Shanghai Jiao Tong Univer-
sity, China, 200030
ycwang@mail.sjtu.edu.cn
Abstract
This paper discusses the challenges and pro-
poses a solution to performing information re-
trieval on the Web using Chinese natural language
speech query. The main contribution of this re-
search is in devising a divide-and-conquer strategy
to alleviate the speech recognition errors. It uses
the query model to facilitate the extraction of main
core semantic string (CSS) from the Chinese natu-
ral language speech query. It then breaks the CSS
into basic components corresponding to phrases,
and uses a multi-tier strategy to map the basic
components to known phrases in order to further
eliminate the errors. The resulting system has been
found to be effective.
1 Introduction
We are entering an information era, where infor-
mation has become one of the major resources in
our daily activities. With its wide spread adoption,
Internet has become the largest information wealth
for all to share. Currently, most (Chinese) search
engines can only support term-based information
retrieval, where the users are required to enter the
queries directly through keyboards in front of the
computer. However, there is a large segment of
population in China and the rest of the world who
are illiterate and do not have the skills to use the
computer. They are thus unable to take advantage
of the vast amount of freely available information.
Since almost every person can speak and under-
stand spoken language, the research on ?(Chinese)
natural language speech query retrieval? would
enable average persons to access information using
the current search engines without the need to learn
special computer skills or training. They can sim-
ply access the search engine using common de-
vices that they are familiar with such as the
telephone, PDA and so on.
In order to implement a speech-based informa-
tion retrieval system, one of the most important
challenges is how to obtain the correct query terms
from the spoken natural language query that con-
vey the main semantics of the query. This requires
the integration of natural language query process-
ing and speech recognition research.
Natural language query processing has been an
active area of research for many years and many
techniques have been developed (Jacobs and
Rau1993; Kupie, 1993;  Strzalkowski, 1999; Yu et
al, 1999). Most of these techniques, however, focus
only on written language, with few devoted to the
study of spoken language query processing.
Speech recognition involves the conversion of
acoustic speech signals to a stream of text. Because
of the complexity of human vocal tract, the speech
signals being observed are different, even for mul-
tiple utterances of the same sequence of words by
the same person (Lee et al1996). Furthermore, the
speech signals can be influenced by the differences
across different speakers, dialects, transmission
distortions, and speaking environments. These
have contributed to the noise and variability of
speech signals. As one of the main sources of er-
rors in Chinese speech recognition come from sub-
stitution (Wang 2002; Zhou 1997), in which a
wrong but similar sounding term is used in place of
the correct term, confusion matrix has been used to
record confused sound pairs in an attempt to elimi-
nate this error. Confusion matrix has been em-
ployed effectively in spoken document retrieval
(Singhal et al 1999 and Srinivasan et al2000) and
to minimize speech recognition errors (Shen et al
1998). However, when such method is used di-
rectly to correct speech recognition errors, it tends
to bring in too many irrelevant terms (Ng 2000).
Because important terms in a long document are
often repeated several times, there is a good chance
that such terms will be correctly recognized at least
once by a speech recognition engine with a reason-
able level of word recognition rate. Many spoken
document retrieval (SDR) systems took advantage
of this fact in reducing the speech recognition and
matching errors (Meng et al2001; Wang et al2001;
Chen et al2001). In contrast to SDR, very little
work has been done on Chinese spoken query
processing (SQP), which is the use of spoken que-
ries to retrieval textual documents. Moreover, spo-
ken queries in SQP tend to be very short with few
repeated terms.
In this paper, we aim to integrate the spoken
language and natural language research to process
spoken queries with speech recognition errors. The
main contribution of this research is in devising a
divide-and-conquer strategy to alleviate the speech
recognition errors. It first employs the Chinese
query model to isolate the Core Semantic String
(CSS) that conveys the semantics of the spoken
query. It then breaks the CSS into basic compo-
nents corresponding to phrases, and uses a multi-
tier strategy to map the basic components to known
phrases in a dictionary in order to further eliminate
the errors.
In the rest of this paper, an overview of the pro-
posed approach is introduced in Section 2. Section
3 describes the query model, while Section 4 out-
lines the use of multi-tier approach to eliminate
errors in CSS. Section 5 discusses the experimental
setup and results. Finally, Section 6 contains our
concluding remarks.
2 Overview of the proposed approach
There are many challenges in supporting surfing of
Web by speech queries. One of the main challenges
is that the current speech recognition technology is
not very good, especially for average users that do
not have any speech trainings. For such unlimited
user group, the speech recognition engine could
achieve an accuracy of less than 50%. Because of
this, the key phrases we derived from the speech
query could be in error or missing the main seman-
tic of the query altogether. This would affect the
effectiveness of the resulting system tremendously.
Given the speech-to-text output with errors, the
key issue is on how to analyze the query in order to
grasp the Core Semantic String (CSS) as accurately
as possible. CSS is defined as the key term se-
quence in the query that conveys the main seman-
tics of the query. For example, given the query:
?
 	
	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 571?578,
Sydney, July 2006. c?2006 Association for Computational Linguistics
  ARE: Instance Splitting Strategies for Dependency Relation-based 
Information Extraction 
Mstislav Maslennikov Hai-Kiat Goh Tat-Seng Chua 
Department of Computer Science 
School of Computing 
National University of Singapore 
{maslenni, gohhaiki, chuats}@ comp.nus.edu.sg 
 
Abstract 
Information Extraction (IE) is a fundamen-
tal technology for NLP. Previous methods 
for IE were relying on co-occurrence rela-
tions, soft patterns and properties of the 
target (for example, syntactic role), which 
result in problems of handling paraphrasing 
and alignment of instances. Our system 
ARE (Anchor and Relation) is based on the 
dependency relation model and tackles 
these problems by unifying entities accord-
ing to their dependency relations, which we 
found to provide more invariant relations 
between entities in many cases. In order to 
exploit the complexity and characteristics 
of relation paths, we further classify the re-
lation paths into the categories of ?easy?, 
?average? and ?hard?, and utilize different 
extraction strategies based on the character-
istics of those categories. Our extraction 
method leads to improvement in perform-
ance by 3% and 6% for MUC4 and MUC6 
respectively as compared to the state-of-art 
IE systems. 
1 Introduction 
Information Extraction (IE) is one of the funda-
mental problems of natural language processing. 
Progress in IE is important to enhance results in 
such tasks as Question Answering, Information 
Retrieval and Text Summarization. Multiple efforts 
in MUC series allowed IE systems to achieve near-
human performance in such domains as biological 
(Humphreys et al, 2000), terrorism (Kaufmann, 
1992; Kaufmann, 1993) and management succes-
sion (Kaufmann, 1995). 
The IE task is formulated for MUC series as 
filling of several predefined slots in a template. The 
terrorism template consists of slots Perpetrator, 
Victim and Target; the slots in the management 
succession template are Org, PersonIn, PersonOut 
and Post. We decided to choose both terrorism and 
management succession domains, from MUC4 and 
MUC6 respectively, in order to demonstrate that 
our idea is applicable to multiple domains. 
Paraphrasing of instances is one of the crucial 
problems in IE. This problem leads to data sparse-
ness in situations when information is expressed in 
different ways. As an example, consider the ex-
cerpts ?Terrorists attacked victims? and ?Victims 
were attacked by unidentified terrorists?. These 
instances have very similar semantic meaning. 
However, context-based approaches such as 
Autoslog-TS by Riloff (1996) and Yangarber et al 
(2002) may face difficulties in handling these in-
stances effectively because the context of entity 
?victims? is located on the left context in the first 
instance and on the right context in the second. For 
these cases, we found that we are able to verify the 
context by performing dependency relation parsing 
(Lin, 1997), which outputs the word ?victims? as an 
object in both instances, with ?attacked? as a verb 
and ?terrorists? as a subject. After grouping of same 
syntactic roles in the above examples, we are able 
to unify these instances.  
Another problem in IE systems is word align-
ment. Insertion or deletion of tokens prevents in-
stances from being generalized effectively during 
learning. Therefore, the instances ?Victims were 
attacked by terrorists? and ?Victims were recently 
attacked by terrorists? are difficult to unify. The 
common approach adopted in GRID by Xiao et al 
(2003) is to apply more stable chunks such as noun 
phrases and verb phrases. Another recent approach 
by Cui et al (2005) utilizes soft patterns for prob-
abilistic matching of tokens. However, a longer 
insertion leads to a more complicated structure, as 
in the instance ?Victims, living near the shop, went 
out for a walk and were attacked by terrorists?. 
Since there may be many inserted words, both ap-
proaches may also be inefficient for this case. Simi-
lar to the paraphrasing problem, the word align-
ment problem may be handled with dependency 
relations in many cases. We found that the relation 
subject-verb-object for words ?victims?, ?attacked? 
and ?terrorists? remains invariant for the above two 
instances. 
Before IE can be performed, we need to iden-
tify sentences containing possible slots. This is 
571
done through the identification of cue phrases 
which we call anchors or anchor cues. However, 
natural texts tend to have diverse terminologies, 
which require semantic features for generalization. 
These features include semantic classes, Named 
Entities (NE) and support from ontology (for ex-
ample, synsets in Wordnet). If such features are 
predefined, then changes in terminology (for in-
stance, addition of new terrorism organization) will 
lead to a loss in recall. To avoid this, we exploit 
automatic mining techniques for anchor cues. Ex-
amples of anchors are the words ?terrorists? or 
?guerrilla? that signify a possible candidate for the 
Perpetrator slot. 
From the reviewed works, we observe that the 
inefficient use of relations causes problems of 
paraphrasing and alignment and the related data 
sparseness problem in current IE systems. As a re-
sult, training and testing instances in the systems 
often lack generality. This paper aims to tackle 
these problems with the help of dependency rela-
tion-based model for IE. Although dependency re-
lations provide invariant structures for many in-
stances as illustrated above, they tend to be effi-
cient only for short sentences and make errors on 
long distance relations. To tackle this problem, we 
classify relations into ?simple?, ?average? and 
?hard? categories, depending on the complexity of 
the dependency relation paths. We then employ 
different strategies to perform IE in each category. 
The main contributions of our work are as fol-
lows. First, we propose a dependency relation 
based model for IE. Second, we perform classifica-
tion of instances into several categories based on 
the complexity of dependency relation structures, 
and employ the action promotion strategy to tackle 
the problem of long distance relations. 
The remaining parts of the paper are organized 
as follows. Section 2 discusses related work and 
Section 3 introduces our approach for constructing 
ARE. Section 4 introduces our method for splitting 
instances into categories. Section 5 describes our 
experimental setups and results and, finally, Sec-
tion 6 concludes the paper. 
2 Related work 
There are several research directions in Information 
Extraction. We highlight a few directions in IE 
such as case frame based modeling in PALKA by 
Kim and Moldovan (1995) and CRYSTAL by So-
derland et al (1995); rule-based learning in 
Autoslog-TS by Riloff et al (1996); and classifica-
tion-based learning by Chieu et al (2002). Al-
though systems representing these directions have 
very different learning models, paraphrasing and 
alignment problems still have no reliable solution.  
Case frame based IE systems incorporate do-
main-dependent knowledge in the processing and 
learning of semantic constraints. However, concept 
hierarchy used in case frames is typically encoded 
manually and requires additional human labor for 
porting across domains. Moreover, the systems 
tend to rely on heuristics in order to match case 
frames. PALKA by Kim and Moldovan (1995) per-
forms keyword-based matching of concepts, while 
CRYSTAL by Soderland et al (1995) relied on 
additional domain-specific annotation and associ-
ated lexicon for matching. 
Rule-based IE models allow differentiation of 
rules according to their performance. Autoslog-TS 
by Riloff (1996) learns the context rules for extrac-
tion and ranks them according to their performance 
on the training corpus. Although this approach is 
suitable for automatic training, Xiao et al (2004) 
stated that hard matching techniques tend to have 
low recall due to data sparseness problem. To over-
come this problem, (LP)2 by Ciravegna (2002) util-
izes rules with high precision in order to improve 
the precision of rules with average recall. However, 
(LP)2 is developed for semi-structured textual do-
main, where we can find consistent lexical patterns 
at surface text level. This is not the same for free-
text, in which different order of words or an extra 
clause in a sentence may cause paraphrasing and 
alignment problems respectively, such as the ex-
ample excerpts ?terrorists attacked peasants? and 
?peasants were attacked 2 months ago by terrorists?.  
The classification-based approaches such as by 
Chieu and Ng (2002) tend to outperform rule-based 
approaches. However, Ciravegna (2001) argued 
that it is difficult to examine the result obtained by 
classifiers. Thus, interpretability of the learned 
knowledge is a serious bottleneck of the classifica-
tion approach. Additionally, Zhou and Su (2002) 
trained classifiers for Named Entity extraction and 
reported that performance degrades rapidly if the 
training corpus size is below 100KB. It implies that 
human experts have to spend long hours to annotate  
a sufficiently large amount of training corpus. 
Several recent researches focused on the ex-
traction of relationships using classifiers. Roth and 
Yih (2002) learned the entities and relations to-
gether. The joint learning improves the perform-
ance of NE recognition in cases such as ?X killed 
Y?. It also prevents the propagation of mistakes in 
NE extraction to the extraction of relations. How-
ever, long distance relations between entities are 
likely to cause mistakes in relation extraction. A 
possible approach for modeling relations of differ-
ent complexity is the use of dependency-based ker-
nel trees in support vector machines by Culotta and 
Sorensen (2004). The authors reported that non-
relation instances are very heterogeneous, and 
572
hence they suggested the additional step of extract-
ing candidate relations before classification. 
3 Our approach 
Differing from previous systems, the language 
model in ARE is based on dependency relations 
obtained from Minipar by Lin (1997). In the first 
stage, ARE tries to identify possible candidates for 
filling slots in a sentence. For example, words such 
as ?terrorist? or ?guerrilla? can fill the slot for Per-
petrator in the terrorism domain. We refer to these 
candidates as anchors or anchor cues. In the sec-
ond stage, ARE defines the dependency relations 
that connect anchor cues. We exploit dependency 
relations to provide more invariant structures for 
similar sentences with different syntactic structures. 
After extracting the possible relations between an-
chor cues, we form several possible parsing paths 
and rank them.  Based on the ranking, we choose 
the optimal filling of slots.  
Ranking strategy may be unnecessary in cases 
when entities are represented in the SVO form. 
Ranking strategy may also fail in situations of long 
distance relations. To handle such problems, we 
categorize the sentences into 3 categories of: sim-
ple, average and hard, depending on the complexity 
of the dependency relations. We then apply differ-
ent strategies to tackle sentences in each category 
effectively. The following subsections discuss de-
tails of our approach. 
 
Features Perpetrator_Cue 
(A) 
Action_Cue 
(D) 
Victim_Cue 
(A) 
Target_Cue 
(A) 
Lexical  
(Head 
noun) 
terrorists,  
individuals,  
soldiers 
attacked, 
murder,  
massacre 
mayor, 
general, 
priests 
bridge,  
house,  
ministry 
Part-of-
Speech 
Noun Verb Noun Noun 
Named 
Entities 
Soldiers  
(PERSON) 
- Jesuit priests 
(PERSON) 
WTC  
(OBJECT) 
Synonyms Synset 130, 166 Synset 22 Synset 68 Synset 71 
Concept 
Class 
ID 2, 3 ID 9  ID 22, 43 ID 61, 48 
Co-
referenced 
entity 
He -> terrorist, 
soldier 
- They -> 
peasants 
- 
Table 1. Linguistic features for anchor extraction 
Every token in ARE may be represented at a 
different level of representations, including: Lexi-
cal, Part-of-Speech, Named Entities, Synonyms and 
Concept classes. The synonym set and concept 
classes are mainly obtained from Wordnet. We use 
NLProcessor from Infogistics Ltd for the extraction 
of part-of-speech, noun phrases and verb phrases 
(we refer to them as phrases). Named Entities are 
extracted with the program used in Yang et al 
(2003). Additionally, we employed the co-
reference module for the extraction of meaningful 
pronouns. It is used for linking entities across 
clauses or sentences, for example in ?John works in 
XYZ Corp. He was appointed as a vice-president a 
month ago? and could achieve an accuracy of 62%. 
After preprocessing and feature extraction, we ob-
tain the linguistic features in Table 1. 
3.1 Mining of anchor cues 
In order to extract possible anchors and relations 
from every sentence, we need to select features to 
support the generalization of words. This generali-
zation may be different for different classes of 
words. For example, person names may be general-
ized as a Named Entity PERSON, whereas for 
?murder? and ?assassinate?, the optimal generaliza-
tion would be the concept class ?kill? in the Word-
Net hypernym tree. To support several generaliza-
tions, we need to store multiple representations of 
every word or token. 
Mining of anchor cues or anchors is crucial in 
order to unify meaningful entities in a sentence, for 
example words ?terrorists?, ?individuals? and ?sol-
diers? from Table 1. In the terrorism domain, we 
consider 4 types of anchor cues: Perpetrator, Action, 
Victim, and Target of destruction. For management 
succession domain, we have 6 types: Post, Person 
In, Person Out, Action and Organization. Each set 
of anchor cues may be seen as a pre-defined se-
mantic type where the tokens are mined automati-
cally. The anchor cues are further classified into 
two categories: general type A and action type D. 
Action type anchor cues are those with verbs or 
verb phrases describing a particular action or 
movement. General type encompasses any prede-
fined type that does not fall under the action type 
cues.  
In the first stage, we need to extract anchor 
cues for every type. Let P be an input phrase, and 
Aj be the anchor of type j that we want to match. 
The similarity score of P for Aj in sentence S is 
given by: 
 
Phrase_Scores(P,Aj)=?1* S_lexicalS(P,Aj+?2* S_POSS(P,Aj) 
                         +?3* S_NES(P,Aj) +?4 * S_SynS(P,Aj)  
                         +?5* S_Concept-ClassS(P,Aj)   (1) 
 
where S_XXXS(P,Aj) is a score function for the type 
Aj and ?i is the importance weight for Aj. In order to 
extract the score function, we use entities from 
slots in the training instances. Each S_XXXS(P,Aj) is 
calculated as a ratio of occurrence in positive slots 
versus all the slots: 
 
  )2(
)(#
)(#
),(_
j
j
jS Atypetheofslotsall
AtypetheofslotspositiveinP
APXXXS =  
 
We classify the phrase P as belonging to an anchor 
cue A of type j if Phrase_ScoreS(P,Aj) ? ?, where 
? is an empirically determined threshold. The 
weights ( )51 ,..., ??? = are learned automatically 
using Expectation Maximization by Dempster et al 
(1977). Using anchors from training instances as 
ground truth, we iteratively input different sets of 
weights into EM to maximize the overall score. 
573
Consider the excerpts ?Terrorists attacked 
victims?, ?Peasants were murdered by unidentified 
individuals? and ?Soldiers participated in massacre 
of Jesuit priests?. Let Wi denotes the position of 
token i in the instances. After mining of anchors, 
we are able to extract meaningful anchor cues in 
these sentences as shown in Table 2: 
 
W-3 W-2 W-1 W0 W1 W2 W3
 Perp_Cue Action_Cue Victim_Cue    
   Victim_Cue were Action_Cue by
In Action_Cue Of Victim_Cue    
Table 2. Instances with anchor cues 
3.2 Relationship extraction and ranking 
In the next stage, we need to 
find meaningful relations to 
unify instances using the anchor 
cues. This unification is done 
using dependency trees of sen-
tences. The dependency 
relations for the first sentence 
are given in Figure 1.  
 
 From the dependency tree, we need to identify 
the SVO relations between anchor cues. In cases 
when there are multiple relations linking many po-
tential subjects, verbs or objects, we need to select 
the best relations under the circumstances. Our 
scheme for relation ranking is as follows.  
First, we rank each single relation individually 
based on the probability that it appears in the re-
spective context template slot in the training data. 
We use the following formula to capture the quality 
of a relation Rel which gives higher weight to more 
frequently occurring relations:  
 
)3(
||}|{||
||},|{||
),,( 21 ?
?
?
=?=
S iii
S iii
SRR
elRRRRR
AAleRQuality
where S is a set of sentences containing relation 
Rel, anchors A1 and A2; R denotes relation path con-
necting A1 and A2 in a sentence Si; ||X|| denotes size 
of the set X. 
 Second, we need to take into account the entity 
height in the dependency tree. We calculate height 
as a distance to the root node. Our intuition is that 
the nodes on the higher level of dependency tree 
are more important, because they may be linked to 
more nodes or entities. The following example in 
Figure 2 illustrates it.  
 
 
Figure 2. Example of entity in a dependency tree 
Here, the node ?terrorists? is the most representative 
in the whole tree, and thus relations nearer to ?ter-
rorists? should have higher weight. Therefore, we 
give a slightly higher weight to the links that are 
closer to the root node as follows: 
 
   Heights(Rel) = log2(Const ? Distance(Root, Rel))         (4) 
 
where Const is set to be larger than the depth of 
nodes in the tree.  
Third, we need to calculate the score of rela-
tion path Ri->j between each pair of anchors Ai and 
Aj, where Ai and Aj belong to different anchor cue 
types. The path score of Ri->j depends on both qual-
ity and height of participating relations:  
 
Scores(Ai, Aj)=?Ri?R {Heights(Ri)*Quality(Ri)}/Lengthij   (5) 
 
where Lengthij is the length of path Ri->j. Division 
on Lengthij allows normalizing Score against the 
length of Ri->j. The formula (5) tends to give higher 
scores to shorter paths. Therefore, the path ending 
with ?terrorist? will be preferred in the previous 
example to the equivalent path ending with 
?MRTA?. 
 Finally, we need to find optimal filling of a 
template T. Let C = {C1, .. , CK} be the set of slot 
types in T and A = {A1, .., AL} be the set of ex-
tracted anchors. First, we regroup anchors A ac-
cording to their respective types. Let 
},...,{ )()(1
)( k
L
kk
k
AAA =  be the projection of A onto 
the type Ck, ?k?N, k ? K. Let F = A(1) ? A(2) ?..? 
A(K) be the set of possible template fillings. The 
elements of F are denoted as F1, ..,FM, where every 
Fi ? F is represented as Fi = {Ai(1),..,Ai(K)}. Our aim 
is to evaluate F and find the optimal filling F0 ? F. 
For this purpose, we use the previously calculated 
scores of relation paths between every two anchors 
Ai and Aj.  
 Based on the previously defined ScoreS(Ai, Aj), 
it is possible to rank all the fillings in F. For each 
filling Fi?F we calculate the aggregate score for all 
the involved anchor pairs: 
)7(
),(
)(_ ,1
M
AAcoreS
FScoreelationR
jiSKji
iS
? ??=
where K is number of slot types and M denotes the 
number of relation paths between anchors in Fi.  
 After calculating Relation_ScoreS(Fi), it is used 
for ranking all possible template fillings. The next 
step is to join entity and relation scores. We defined 
the entity score of Fi as an average of the scores of 
participating anchors:   
 
)8(/)(_)(_
1
)(? ??= Kk kiSiS KAScorePhraseFScoreEntity
We combine entity and relation scores of Fi into the 
overall formula for ranking. 
 
 RankS(Fi)=?*Entity_ScoreS(Fi)+(1-?)*Relation_ScoreS(Fi )      (9) 
 
The application of Subject-Verb-Object (SVO) 
relations facilitates the grouping of subjects, 
Figure 1.  
Dependency tree 
574
verbs and objects together. For the 3 instances in 
Table 2 containing the anchor cues, the unified 
SVO relations are given in Table 3. 
 
W-2 W-1 W0 Instance is 
Perp_Cue attacked Victim_Cue + 
Perp_Cue murdered Victim_Cue + 
Perp_Cue participated ? - 
Table 3.  Unification based on SVO relations 
The first 2 instances are unified correctly. The 
only exception is the slot in the third case, which 
is missing because the target is not an object of 
?participated?. 
4 Category Splitting 
Through our experiments, we found that the com-
bination of relations and anchors are essential for 
improving IE performance. However, relations 
alone are not applicable across all situations be-
cause of long distance relations and possible de-
pendency relation parsing errors, especially for 
long sentences. Since the relations in long sen-
tences are often complicated, parsing errors are 
very difficult to avoid. Furthermore, application of 
dependency relations on long sentences may lead to 
incorrect extractions and decrease the performance.  
Through the analysis of instances, we noticed 
that dependency trees have different complexity for 
different sentences. Therefore, we decided to clas-
sify sentences into 3 categories based on the com-
plexity of dependency relations between the action 
cues (V) and the likely subject (S) and object cues 
(O). Category 1 is when the potential SVO?s are 
connected directly to each other (simple category); 
Category 2 is when S or O is one link away from V 
in terms of nouns or verbs (average category); and 
Category 3 is when the path distances between po-
tential S, V, and Os are more than 2 links away 
(hard category).  
 
 
 
 
  
Figure 3. Simple category   Figure 4. Average category  
Figure 3 and Figure 4 illustrate the dependency 
parse trees for the simple and average categories 
respectively derived from the sentences: ?50 peas-
ants of have been kidnapped by terrorists? and ?a 
colonel was involved in the massacre of the Jesu-
its?. These trees represent 2 common structures in 
the MUC4 domain. By taking advantage of this 
commonality, we can further improve the perform-
ance of extraction. We notice that in the simple 
category, the perpetrator cue (?terrorists?) is always 
a subject, action cue (?kidnapped?) a verb, and vic-
tim cue (?peasants?) an object. For the average 
category, perpetrator and victim commonly appear 
under 3 relations: subject, object and pcomp-n. The 
most difficult category is the hard category, since 
in this category relations can be distant. We thus 
primarily rely on anchors for extraction and have to 
give less importance to dependency parsing.   
 In order to process the different categories, we 
utilize the specific strategies for each category. As 
an example, the instance ?X murdered Y? requires 
only the analysis of the context verb ?murdered? in 
the simple category. It is different from the in-
stances ?X investigated murder of Y? and ?X con-
ducted murder of Y? in the average category, in 
which transition of word ?investigated? into ?con-
ducted? makes X a perpetrator. We refer to the an-
chor ?murder? in the first and second instances as 
promotable and non-promotable respectively. Ad-
ditionally, we denote that the token ?conducted? is 
the optimal node for promotion of ?murder?, 
whereas the anchor ?investigate? is not. This exam-
ple illustrates the importance of support verb analy-
sis specifically for the average category.  
 
 
 
 
Figure 5. Category processing 
The main steps of our algorithm for performing IE 
in different categories are given in Figure 5. Al-
though some steps are common for every category, 
the processing strategies are different. 
 
Simple category 
For simple category, we reorder tokens according 
to their slot types. Based on this reordering, we fill 
the template. 
 
Algorithm 
 
1) Analyze category  
     If(simple)  
       - Perform token reordering based on SVO relations 
     Else if (average) ProcessAverage 
     Else ProcessHard 
2) Fill template slots 
 
Function ProcessAverage 
1) Find the nearest missing anchor in the previous sentences  
2) Find the optimal linking node for action anchor in every Fi 
3) Find the filling Fi(0) = argmaxi Rank(Fi) 
4) Use Fi for filling the template if Rank0 > ?2, where ?2 is an 
empirical threshold 
 
Function ProcessHard 
1) Perform token reordering based on anchors 
2) Use linguistic+ syntactic + semantic feature of the head 
noun. Eg. Caps, ?subj?, etc 
3) Find the optimal linking node for action anchor in every Fi 
4) Find the filling Fi(0) = argmaxi Rank(Fi) 
5) Use Fi for filling the template if Rank0 > ?3, where ?3 is an 
empirical threshold 
575
Average category 
For average category, our strategy consists of 4 
steps. First, in the case of missing anchor type we 
try to find it in the nearest previous sentence. Con-
sider an example from MUC-6: ?Look at what hap-
pened to John Sculley, Apple Computer's former 
chairman. Earlier this month he abruptly resigned 
as chairman of troubled Spectrum Information 
Technologies.? In this example, a noisy cue ?he? 
needs to be substituted with ?John Sculley?, which 
is a strong anchor cue. Second, we need to find an 
optimal promotion of a support verb. For example, 
in ?X conducted murder of Y?, the verb ?murder? 
should be linked with X and in the excerpt ?X in-
vestigated murder of Y?, it should not be promoted. 
Thus, we need to make 2 steps for promotion: (a) 
calculate importance of every word connecting the 
action cue such as ?murder? and ?distributed? and (b) 
find the optimal promotion for the word ?murder?. 
Third, using the predefined threshold ? we cutoff 
the instances with irrelevant support verbs (e.g., 
?investigated?). Fourth, we reorder the tokens in 
order to group them according to the anchor types. 
The following algorithm in Figure 6 estimates 
the importance of a token W for type D in the sup-
port verb structure. The input of the algorithm con-
sists of sentences S1?SN and two sets of tokens 
Vneg, Vpos co-occurring with anchor cue of type D. 
Vneg and Vpos are automatically tagged as irrelevant 
and relevant respectively based on preliminary 
marked keys in the training instances. The algo-
rithm output represents the importance value be-
tween 0 to 1.  
 
 
Figure 6. Evaluation of word importance 
We use the linguistic features for W and D as given 
in Table 1 to form the instances.  
 
Hard category 
In the hard category, we have to deal with long-
distance relations: at least 2 anchors are more than 
2 links away in the dependency tree. Consequently, 
dependency tree alone is not reliable for connecting 
nodes. To find an optimal connection, we primarily 
rely on comparison between several possible fill-
ings of slots based on previously extracted anchor 
cues. Depending on the results of such comparison, 
we chose the filling that has the highest score. As 
an example, consider the hard category in the ex-
cerpt ?MRTA today distributed leaflets claiming 
responsibility for the murder of former defense 
minister Enrique Lopez Albujar?. The dependency 
tree for this instance is given in Figure 7.  
 
Although words ?MRTA?, ?murder? and ?min-
ister? might be correctly extracted as anchors, the 
challenging problem is to decide whether ?MRTA? 
is a perpetrator. Anchors ?MRTA? and ?minister? 
are connected via the verb ?distributed?. However, 
the word ?murder? belongs to another branch of this 
verb. 
 
 
Figure 7. Hard case 
Processing of such categories is challenging. 
Since relations are not reliable, we first need to rely 
on the anchor extraction stage. Nevertheless, the 
promotion strategy for the anchor cue ?murder? is 
still possible, although the corresponding branch in 
the dependency tree is long. Henceforth, we try to 
replace the verb ?distributed? by promoting the an-
chor ?murder?. To do so, we need to evaluate 
whether the nodes in between may be eliminated. 
For example, such elimination is possible in the 
pairs ?conducted? -> ?murder? and not possible in 
the pair ?investigated? -> ?murder?, since in the ex-
cerpt ?X investigated murder? X is not a perpetra-
tor. If the elimination is possible, we apply the 
promotion algorithm given on Figure 8: 
 
 
Figure 8. Token promotion algorithm 
The algorithm checks path Pj1->j2 that connect an-
chors Ai(j1) and Ai(j2) in the filling Fi; the nodes from 
Pj1->j2 are added to the set Z. Finally, the top node 
of the set Z is chosen as an optimal node for the 
promotion. The example optimal node for promo-
tion of the word ?murder? on Figure 7 is the node 
?distributed?. 
Another important difference between the hard 
and average cases is in the calculation of RankS (Fi) 
in Equation (9). We set ?hard > ?average because long 
distance relations are less reliable in the hard case 
than in the average case. 
CalculateImportance (W, D) 
 
1) Select sentences that contain anchor cue D 
2) Extract linguistic features of Vpos, Vneg and D 
3) Train using SVM on instances (Vpos,D) and  
instances (Vneg,D) 
4) Return Importance(W) using SVM 
FindOptimalPromotion (Fi) 
1) Z = ? 
2) For each Ai(j1), Ai(j2) ? Fi 
   Z = Z ? Pj1->j2 
     End_for 
3) Output Top(Z) 
576
5 Evaluation 
In order to evaluate the efficiency of our method, 
we conduct our experiments in 2 domains: MUC4 
(Kaufmann, 1992) and MUC6 (Kaufmann, 1995). 
The official corpus of MUC4 is released with 
MUC3; it covers terrorism in the Latin America 
region and consists of 1,700 texts. Among them, 
1,300 documents belong to the training corpus. 
Testing was done on 25 relevant and 25 irrelevant 
texts from TST3, plus 25 relevant and 25 irrelevant 
texts from TST4, as is done in Xiao et al (2004). 
MUC6 covers news articles in Management Suc-
cession domain. Its training corpus consists of 1201 
instances, whereas the testing corpus consists of 76 
person-ins, 82 person-outs, 123 positions, and 79 
organizations. These slots we extracted in order to 
fill templates on a sentence-by-sentence basis, as is 
done by Chieu et al (2002) and Soderland (1999). 
Our experiments were designed to test the 
effectiveness of both case splitting and action verb 
promotion. The performance of ARE is compared 
to both the state-of-art systems and our baseline 
approach. We use 2 state-of-art systems for MUC4 
and 1 system for MUC6. Our baseline system, 
Anc+rel, utilizes only anchors and relations 
without category splitting as described in Section 3. 
For our ARE system with case splitting, we present 
the results on Overall corpus, as well as separate 
results on Simple, Average and Hard categories. 
The Overall performance of ARE represents the 
result for all the categories combined together. 
Additionally, we test the impact of the action 
promotion (in the right column) for the average and 
hard categories. 
 
 Without promotion With promotion 
Case (%) P R F1 P R F1 
GRID 58% 56% 57% - - - 
Riloff?05 46% 52% 48% - - - 
Anc+rel (100%) 58% 59% 58% 58% 59% 58% 
Overall (100%) 57% 60% 59% 58% 61% 60% 
Simple (13%) 79% 86% 82% 79% 86% 82% 
Average (22%) 64% 70% 67% 67% 71% 69% 
Hard (65%) 50% 52% 51% 51% 53% 52% 
Table 4. Results on MUC4 with case splitting 
 
The comparative results are presented in Table 
4 and Table 5 for MUC4 and MUC6, respectively. 
First, we review our experimental results on MUC4 
corpus without promotion (left column) before pro-
ceeding to the right column. 
a) From the results on Table 4 we observe that our 
baseline approach Anc+rel outperforms all the 
state-of-art systems. It demonstrates that both an-
chors and relations are useful. Anchors allow us to 
group entities according to their semantic meanings 
and thus to select of the most prominent candidates. 
Relations allow us to capture more invariant repre-
sentation of instances. However, a sentence may 
contain very few high-quality relations. It implies 
that the relations ranking step is fuzzy in nature. In 
addition, we noticed that some anchor cues may be 
missing, whereas the other anchor types may be 
represented by several anchor cues. All these fac-
tors lead only to moderate improvement in per-
formance, especially in comparison with GRID 
system. 
b) Overall, the splitting of instances into categories 
turned out to be useful. Due to the application of 
specific strategies the performance increased by 1% 
over the baseline. However, the large dominance of 
the hard cases (65%) made this improvement mod-
est. 
c) We notice that the amount of variations for con-
necting anchor cues in the Simple category is rela-
tively small. Therefore, the overall performance for 
this case reaches F1=82%. The main errors here 
come from missing anchors resulting partly from 
mistakes in such component as NE detection. 
d) The performance in the Average category is 
F1=67%. It is lower than that for the simple cate-
gory because of higher variability in relations and 
negative influence of support verbs. For example, 
for excerpt such as ?X investigated murder of Y?, 
the processing tends to make mistake without the 
analysis of semantic value of support verb ?investi-
gated?. 
e) Hard category achieves the lowest performance 
of F1=51% among all the categories. Since for this 
category we have to rely mostly on anchors, the 
problem arises if these anchors provide the wrong 
clues. It happens if some of them are missing or are 
wrongly extracted. The other cause of mistakes is 
when ARE finds several anchor cues which belong 
to the same type. 
Additional usage of promotion strategies al-
lowed us to improve the performance further. 
f) Overall, the addition of promotion strategy en-
ables the system to further boost the performance to 
F1=60%. It means that the promotion strategy is 
useful, especially for the average case. The im-
provement in comparison to the state-of-art system 
GRID is about 3%. 
g) It achieved an F1=69%, which is an improve-
ment of 2%, for the Average category. It implies 
that the analysis of support verbs helps in revealing 
the differences between the instances such as ?X 
was involved in kidnapping of Y? and ?X reported 
kidnapping of Y?.  
h) The results in the Hard category improved mod-
erately to F1=52%. The reason for the improvement 
is that more anchor cues are captured after the 
promotion. Still, there are 2 types of common mis-
577
takes: 1) multiple or missing anchor cues of the 
same type and 2) anchors can be spread across sev-
eral sentences or several clauses in the same sen-
tence.  
 
 Without promotion With promotion 
Case (%) P R F1 P R F1 
Chieu et al?02 74% 49% 59% - - - 
Anc+rel (100%) 78% 52% 62% 78% 52% 62% 
Overall (100%) 72% 58% 64% 73% 58% 65% 
Simple (45%) 85% 67% 75% 87% 68% 76% 
Average (27%) 61% 55% 58% 64% 56% 60% 
Hard (28%) 59% 44% 50% 59% 44% 50% 
Table 5. Results on MUC6 with case splitting 
For the MUC6 results given in Table 5, we ob-
serve that the overall improvement in performance 
of ARE system over Chieu et al?02 is 6%. The 
trends of results for MUC6 are similar to that in 
MUC4. However, there are few important differ-
ences. First, 45% of instances in MUC6 fall into 
the Simple category, therefore this category domi-
nates. The reason for this is that the terminologies 
used in Management Succession domain are more 
stable in comparison to the Terrorism domain. Sec-
ond, there are more anchor types for this case and 
therefore the promotion strategy is applicable also 
to the simple case. Third, there is no improvement 
in performance for the Hard category. We believe 
the primary reason for it is that more stable lan-
guage patterns are used in MUC6. Therefore, de-
pendency relations are also more stable in MUC6 
and the promotion strategy is not very important. 
Similar to MUC4, there are problems of missing 
anchors and mistakes in dependency parsing. 
6 Conclusion 
The current state-of-art IE methods tend to use co-
occurrence relations for extraction of entities. Al-
though context may provide a meaningful clue, the 
use of co-occurrence relations alone has serious 
limitations because of alignment and paraphrasing 
problems. In our work, we proposed to utilize de-
pendency relations to tackle these problems. Based 
on the extracted anchor cues and relations between 
them, we split instances into ?simple?, ?average? 
and ?hard? categories. For each category, we ap-
plied specific strategy. This approach allowed us to 
outperform the existing state-of-art approaches by 
3% on Terrorism domain and 6% on Management 
Succession domain. In our future work we plan to 
investigate the role of semantic relations and inte-
grate ontology in the rule generation process. An-
other direction is to explore the use of bootstrap-
ping and transduction approaches that may require 
less training instances. 
 
References 
H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy Ap-
proach to Information Extraction from Semi-Structured 
and Free Text. In Proc of AAAI-2002, 786-791. 
H. Cui, M.Y. Kan, and Chua T.S. 2005. Generic Soft Pat-
tern Models for Definitional Question Answering. In 
Proc of ACM SIGIR-2005. 
A. Culotta and J. Sorensen J. 2004. Dependency tree kernels 
for relation extraction. In Proc of ACL-2004. 
F. Ciravegna. 2001. Adaptive Information Extraction from 
Text by Rule Induction and Generalization. In Proc of 
IJCAI-2001. 
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum like-
lihood from incomplete data via the EM algorithm. Jour-
nal of the Royal Statistical Society B, 39(1):1?38 
K. Humphreys, G. Demetriou and R. Gaizuskas. 2000. Two 
applications of Information Extraction to Biological Sci-
ence: Enzyme interactions and Protein structures. In 
Proc of the Pacific Symposium on Biocomputing, 502-
513 
M. Kaufmann. 1992. MUC-4. In Proc of  MUC-4. 
M. Kaufmann. 1995. MUC-6. In Proc of MUC-6. 
J. Kim and D. Moldovan. 1995. Acquisition of linguistic 
patterns for knowledge-based information extraction. 
IEEE Transactions on KDE, 7(5): 713-724 
D. Lin. 1997. Using Syntactic Dependency as Local Context 
to Resolve Word Sense Ambiguity. In Proc of ACL-97. 
E. Riloff. 1996. Automatically Generating Extraction Pat-
terns from Untagged Text. In Proc of AAAI-96, 1044-
1049. 
D. Roth and W. Yih. 2002. Probabilistic Reasoning for En-
tity & Relation Recognition. In Proc of COLING-2002. 
S. Soderland, D. Fisher, J. Aseltine and W. Lehnert. 1995. 
Crystal: Inducing a Conceptual Dictionary. In Proc of 
IJCAI-95, 1314-1319. 
S. Soderland. 1999. Learning Information Extraction Rules 
for Semi-Structured and Free Text. Machine Learning 
34:233-272. 
J. Xiao, T.S. Chua and H. Cui. 2004. Cascading Use of Soft 
and Hard Matching Pattern Rules for Weakly Supervised 
Information Extraction. In Proc of COLING-2004. 
H. Yang, H. Cui, M.-Y. Kan, M. Maslennikov, L. Qiu and 
T.-S. Chua. 2003. QUALIFIER in TREC 12 QA Main 
Task. In Proc of TREC-12, 54-65. 
R. Yangarber, W. Lin, R. Grishman. 2002. Unsupervised 
Learning of Generalized Names. In Proc of COLING-
2002. 
G.D. Zhou and J. Su. 2002. Named entity recognition using 
an HMM-based chunk tagger. In Proc of ACL-2002, 
473-480 
578
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 592?599,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Multi-resolution Framework for Information Extraction from Free Text
Mstislav Maslennikov and Tat-Seng Chua 
Department of Computer Science 
National University of Singapore 
{maslenni,chuats}@comp.nus.edu.sg 
Abstract 
Extraction of relations between entities is 
an important part of Information Extraction 
on free text. Previous methods are mostly 
based on statistical correlation and depend-
ency relations between entities. This paper 
re-examines the problem at the multi-
resolution layers of phrase, clause and sen-
tence using dependency and discourse rela-
tions. Our multi-resolution framework 
ARE (Anchor and Relation) uses clausal 
relations in 2 ways: 1) to filter noisy de-
pendency paths; and 2) to increase reliabil-
ity of dependency path extraction. The re-
sulting system outperforms the previous 
approaches by 3%, 7%, 4% on MUC4, 
MUC6 and ACE RDC domains respec-
tively. 
1 Introduction 
Information Extraction (IE) is the task of identify-
ing information in texts and converting it into a 
predefined format. The possible types of informa-
tion include entities, relations or events. In this 
paper, we follow the IE tasks as defined by the 
conferences MUC4, MUC6 and ACE RDC: slot-
based extraction, template filling and relation ex-
traction, respectively. 
Previous approaches to IE relied on co-
occurrence (Xiao et al, 2004) and dependency 
(Zhang et al, 2006) relations between entities. 
These relations enable us to make reliable extrac-
tion of correct entities/relations at the level of a 
single clause. However, Maslennikov et al (2006) 
reported that the increase of relation path length 
will lead to considerable decrease in performance. 
In most cases, this decrease in performance occurs 
because entities may belong to different clauses.  
Since clauses in a sentence are connected by 
clausal relations (Halliday and Hasan, 1976), it is 
thus important to perform discourse analysis of a 
sentence.  
Discourse analysis may contribute to IE in sev-
eral ways. First, Taboada and Mann (2005) re-
ported that discourse analysis helps to decompose 
long sentences into clauses. Therefore, it helps to 
distinguish relevant clauses from non-relevant 
ones. Second, Miltsakaki (2003) stated that entities 
in subordinate clauses are less salient. Third, the 
knowledge of textual structure helps to interpret 
the meaning of entities in a text (Grosz and Sidner 
1986). As an example, consider the sentences 
?ABC Co. appointed a new chairman. Addition-
ally, the current CEO was retired?. The word ?ad-
ditionally? connects the event in the second sen-
tence to the entity ?ABC Co.? in the first sentence. 
Fourth, Moens and De Busser (2002) reported that 
discourse segments tend to be in a fixed order for 
structured texts such as court decisions or news. 
Hence, analysis of discourse order may reduce the 
variability of possible relations between entities. 
To model these factors, we propose a multi-
resolution framework ARE that integrates both 
discourse and dependency relations at 2 levels. 
ARE aims to filter noisy dependency relations 
from training and support their evaluation with 
discourse relations between entities. Additionally, 
we encode semantic roles of entities in order to 
utilize semantic relations. Evaluations on MUC4, 
MUC6 and ACE RDC 2003 corpora demonstrates 
that our approach outperforms the state-of-art sys-
tems mainly due to modeling of discourse rela-
tions. 
The contribution of this paper is in applying dis-
course relations to supplement dependency rela-
tions in a multi-resolution framework for IE. The 
592
framework enables us to connect entities in differ-
ent clauses and thus improve the performance on 
long-distance dependency paths.  
Section 2 describes related work, while Section 
3 presents our proposed framework, including the 
extraction of anchor cues and various types of rela-
tions, integration of extracted relations, and com-
plexity classification. Section 4 describes our ex-
perimental results, with the analysis of results in 
Section 5. Section 6 concludes the paper. 
2 Related work 
Recent work in IE focuses on relation-based, se-
mantic parsing-based and discourse-based ap-
proaches. Several recent research efforts were 
based on modeling relations between entities. Cu-
lotta and Sorensen (2004) extracted relationships 
using dependency-based kernel trees in Support 
Vector Machines (SVM). They achieved an F1-
measure of 63% in relation detection. The authors 
reported that the primary source of mistakes comes 
from the heterogeneous nature of non-relation in-
stances. One possible direction to tackle this prob-
lem is to carry out further relationship classifica-
tion. Maslennikov et al (2006) classified relation 
path between candidate entities into simple, aver-
age and hard cases. This classification is based on 
the length of connecting path in dependency parse 
tree. They reported that dependency relations are 
not reliable for the hard cases, which, in our opin-
ion, need the extraction of discourse relations to 
supplement dependency relation paths. 
Surdeanu et al (2003) applied semantic parsing 
to capture the predicate-argument sentence struc-
ture. They suggested that semantic parsing is use-
ful to capture verb arguments, which may be con-
nected by long-distance dependency paths. How-
ever, current semantic parsers such as the ASSERT 
are not able to recognize support verb construc-
tions such as ?X conducted an attack on Y? under 
the verb frame ?attack? (Pradhan et al 2004). 
Hence, many useful predicate-argument structures 
will be missed. Moreover, semantic parsing be-
longs to the intra-clausal level of sentence analysis, 
which, as in the dependency case, will need the 
support of discourse analysis to bridge inter-clausal 
relations. 
Webber et al (2002) reported that discourse 
structure helps to extract anaphoric relations. How-
ever, their set of grammatical rules is heuristic. Our 
task needs construction of an automated approach 
to be portable across several domains. Cimiano et 
al. (2005) employed a discourse-based analysis for 
IE. However, their approach requires a predefined 
domain-dependent ontology in the format of ex-
tended logical description grammar as described by 
Cimiano and Reely (2003). Moreover, they used 
discourse relations between events, whereas in our 
approach, discourse relations connect entities. 
3 Motivation for using discourse relations  
Our method is based on Rhetorical Structure The-
ory (RST) by Taboada and Mann (2005). RST 
splits the texts into 2 parts: a) nuclei, the most im-
portant parts of texts; and b) satellites, the secon-
dary parts. We can often remove satellites without 
losing the meaning of text. Both nuclei and satel-
lites are connected with discourse relations in a 
hierarchical structure. In our work, we use 16 
classes of discourse relations between clauses: At-
tribution, Background, Cause, Comparison, Condi-
tion, Contrast, Elaboration, Enablement, Evalua-
tion, Explanation, Joint, Manner-Means, Topic-
Comment, Summary, Temporal, Topic-Change. 
The additional 3 relations impose a tree structure: 
textual-organization, span and same-unit. All the 
discourse relation classes are potentially useful, 
since they encode some knowledge about textual 
structure. Therefore, we decide to include all of 
them in the learning process to learn patterns with 
best possible performance. 
We consider two main rationales for utilizing 
discourse relations to IE. First, discourse relations 
help to narrow down the search space to the level 
of a single clause. For example, the sentence 
?[<Soc-A1>Trudeau</>'s <Soc-A2>son</> told 
everyone], [their prime minister was his father], 
[who took him to a secret base in the arctic] [and 
let him peek through a window].? contains 4 
clauses and 7 anchor cues (key phrases) for the 
type Social, which leads to 21 possible variants. 
Splitting this sentence into clauses reduces the 
combinations to 4 possible variants. Additionally, 
this reduction eliminates the long and noisy de-
pendency paths.  
Second, discourse analysis enables us to connect 
entities in different clauses with clausal relations. 
As an example, we consider a sentence ?It?s a dark 
comedy about a boy named <AT-A1>Marshal</> 
played by Amourie Kats who discovers all kinds of 
593
on and scary things going on in <AT-A2>a seem-
ingly quiet little town</>?. In this example, we 
need to extract the relation ?At? between the enti-
ties ?Marshal? and ?a seemingly quiet little town?. 
The discourse structure of this sentence is given in 
.Figure 1   
 
 
Figure 1. Example of discourse parsing 
The discourse path ?Marshal <-elaboration- _ 
<-span- _ -elaboration-> _ -elaboration-> town? 
is relatively short and captures the necessary rela-
tions. At the same time, prediction based on de-
pendency path ?Marshal <?obj- _ <-i- _ <-fc- _ 
<-pnmod- _ <-pred- _ <-i- _ <-null- _ -null-> _ -
rel-> _ -i-> _ -mod-> _ -pcomp-n-> town? is un-
reliable, since the relation path is long. Thus, it is 
important to rely on discourse analysis in this ex-
ample. In addition, we need to evaluate both the 
score and reliability of prediction by relation path 
of each type. 
4 Anchors and Relations 
In this section, we define the key components that 
we use in ARE: anchors, relation types and general 
architecture of our system. Some of these compo-
nents are also presented in detail in our previous 
work (Maslennikov et al, 2006). 
4.1 Anchors 
The first task in IE is to identify candidate phrases 
(which we call anchor or anchor cue) of a pre-
defined type  (anchor  type) to fill a desired slot in 
an  IE  template.  The  example  anchor  for  the  phrase 
 
 ?Marshal? is shown in Figure 2. 
Given a training set of sentences, 
we extract the anchor cues ACj = 
[A1, ?, ANanch] of type Cj using 
the procedures described in 
Maslennikov et al (2006). The 
linguistic features of these an-
chors for the anchor types of Per-  
petrator, Action, Victim and Target for the MUC4 
domain are given in Table 1. 
 
Anchor  
types
Feature 
Perpetrator_Cue 
(A) 
Action_Cue 
(D) 
Victim_Cue 
(A) 
Target_Cue 
(A) 
Lexical  
(Head noun) 
terrorists,  
individuals,  
Soldiers 
attacked, 
murder,  
Massacre 
Mayor, 
general, 
priests 
bridge,  
house,  
Ministry 
Part-of-Speech Noun Verb Noun Noun 
Named Enti-
ties 
Soldiers  
(PERSON) 
- Jesuit priests 
(PERSON) 
WTC  
(OBJECT) 
Synonyms Synset 130, 166 Synset 22 Synset 68 Synset 71 
Concept Class ID 2, 3 ID 9  ID 22, 43 ID 61, 48 
Co-referenced 
entity 
He -> terrorist, 
soldier 
- They -> 
peasants 
- 
Clausal type Nucleus 
Satellite 
Nucleus, 
Satellite 
Nucleus, 
Satellite 
Nucleus, 
Satellite 
Argument type Arg0 , Arg1
Root 
 Target, -, 
ArgM-MNR 
Arg0 ,  Arg1 Arg1 , ArgM-
MNR 
Table 1. Linguistic features for anchor extraction 
 
Given an input phrase P from a test sentence, we 
need to classify if the phrase belongs to anchor cue 
type Cj. We calculate the entity score as: 
 
 Entity_Score(P) =?  ? i * Feature_Scorei(P,Cj) (1) 
 
where Feature_Score(P,Cj) is a score function for 
a particular linguistic feature representation of type 
Cj, and ? i is the corresponding weight for that rep-
resentation in the overall entity score.  The weights 
are learned automatically using Expectation Maxi-
mization (Dempster et al, 1977). The Fea-
ture_Scorei(P,Cj) is estimated from the training set 
as the number of slots containing the correct fea-
ture representation type versus all the slots: 
 
 Feature_Scorei(P,Cj) = #(positive slots) / #(all slots) (2) 
 
We classify the phrase P as belonging to an anchor 
type Cj when its Entity_score(P) is above an em-
pirically determined threshold ?. We refer to this 
anchor as Aj. We allow a phrase to belong to mul-
tiple anchor types and hence the anchors alone are 
not enough for filling templates. 
4.2 Relations 
To resolve the correct filling of phrase P of type Ci 
in a desired slot in the template, we need to con-
sider the relations between multiple candidate 
phrases of related slots. To do so, we consider sev-
eral types of relations between anchors: discourse, 
dependency and semantic relations. These relations 
capture the interactions between anchors and are 
therefore useful for tackling the paraphrasing and 
alignment problems (Maslennikov et al, 2006). 
Given 2 anchors Ai and Aj of anchor types Ci and 
Cj, we consider a relation Pathl = [Ai, Rel1,?, 
Reln, Aj] between them, such that there are no an-
chors between Ai and Aj. Additionally, we assume 
that the relations between anchors are represented 
in the form of a tree Tl, where l = {s, c, d} refers to 
Satellite 
who discovers all kinds of on and 
scary things going on in a seem-
ingly quiet little town. 
Nucleus 
It's a dark 
comedy 
about a boy 
Satellite 
named Mar-
shal 
Nucleus 
played by 
Amourie Kats 
Nucleus Satellite
span elaboration 
span elaboration elaboration span 
Figure 2. Exam-
ple of anchor 
Anchor Ai 
 
Marshal 
pos_NNP 
list_personWord 
Cand_AtArg1 
Minipar_obj 
Arg2 
Spade_Satellite 
594
discourse, dependency and semantic relation types 
respectively. We describe the nodes and edges of 
Tl separately for each type, because their represen-
tations are different: 
1) The nodes of discourse tree Tc consist of clauses 
[Clause1, ?, ClauseNcl]; and their relation edges 
are obtained from the Spade system described in 
Soricut and Marcu (2003). This system performs 
RST-based parsing at the sentence level. The re-
ported accuracy of Spade is 49% on the RST-DT 
corpus. To obtain a clausal path, we map each 
anchor Ai to its clause in Spade. If anchors Ai 
and Aj belong to the same clause, we assign 
them the relation same-clause. 
es.  
2) The nodes of dependency tree Td consist of 
words in sentences; and their relation edges are 
obtained from Minipar by Lin (1997). Lin 
(1997) reported a parsing performance of Preci-
sion = 88.5% and Recall = 78.6% on the SU-
SANNE corpus. 
3) The nodes of semantic tree Ts consist of argu-
ments [Arg0, ?, ArgNarg] and targets [Target1, 
?, TargetNtarg]. Both arguments and targets are 
obtained from the ASSERT parser developed by 
Pradhan (2004). The reported performance of 
ASSERT is F1=83.8% on the identification and 
classification task for all arguments, evaluated 
using PropBank and AQUAINT as the training 
and testing corpora, respectively. Since the rela-
tion edges have a form Targetk -> Argl, the rela-
tion path in semantic frame contains only a sin-
gle relation. Therefore, we encode semantic rela-
tions as part of the anchor features.  
In later parts of this paper, we consider only dis-
course and dependency relation paths Pathl, where 
l={c, d}. 
 
 
Figure 3. Architecture of the system 
4.3 Architecture of ARE system 
In order to perform IE, it is important to extract 
candidate entities (anchors) of appropriate anchor 
types, evaluate the relationships between them, 
further evaluate all possible candidate templates, 
and output the final template. For the case of rela-
tion extraction task, the final templates are the 
same as an extracted binary relation. The overall 
architecture of ARE is given in Figure 3. 
The focus of this paper is in applying discourse 
relations for binary relationship evaluation. 
5 Overall approach 
In this section, we describe our relation-based ap-
proach to IE. We start with the evaluation of rela-
tion paths (single relation ranking, relation path 
ranking) to assess the suitability of their anchors as 
entities to template slots. Here we want to evaluate 
given a single relation or relation path, whether the 
two anchors are correct in filling the appropriate 
slots in a template. This is followed by the integra-
tion of relation paths and evaluation of templates. 
5.1 Evaluation of relation path 
In the first stage, we evaluate from training data 
the relevance of relation path Pathl = [Ai, Rel1,?, 
Reln, Aj] between candidate anchors Ai and Aj of 
types Ci and Cj. We divide this task into 2 steps. 
The first step ranks each single relation Relk ? 
Pathl; while the second step combines the evalua-
tions of Relk to rank the whole relation path Pathl.  
Single relation ranking 
Let Seti and Setj be the set of linguistic features of 
anchors Ai and Aj respectively. To evaluate Relk, 
we consider 2 characteristics: (1) the direction of 
relation Relk as encoded in the tree structure; and 
(2) the linguistic features, Seti and Setj, of anchors 
Ai and Aj. We need to construct multiple single 
relation classifiers, one for each anchor pair of 
types Ci and Cj, to evaluate the relevance of Relk 
with respect to these 2 anchor typ
Preprocessing Corpus 
 
(a) Construction of classifiers. The training data 
to each classifier consists of anchor pairs of types 
Ci and Cj extracted from the training corpus. We 
use these anchor pairs to construct each classifier 
in four stages. First, we compose the set of possi-
ble patterns in the form P+ = { Pm = <Si ?Rel-> 
Sj> | Si ? Seti , Sj ? Setj }. The construction of Pm 
Anchor 
evaluation 
Templates 
Anchor NEs 
Template 
evaluation 
Sentences 
Binary relationship 
evaluation 
Candidate 
templates 
595
conforms to the 2 characteristics given above. 
Figure 4 illustrates several discourse and depend-
ency patterns of P+ constructed from a sample sen-
tence.  
 
 
Figure 4.  Examples of discourse and dependency patterns 
Second, we identify the candidate anchor A, 
whose type matches slot C in a template. Third, we 
find the correct patterns for the following 2 cases: 
1) Ai, Aj are of correct anchor types; and 2) Ai is an 
action anchor, while Aj is a correct anchor. Any 
other patterns are considered as incorrect. We note 
that the discourse and dependency paths between 
anchors Ai and Aj are either correct or wrong si-
multaneously. 
  
Fourth, we evaluate the relevance of each pat-
tern Pm ? P+. Given the training set, let PairSetm 
be the set of anchor pairs extracted by Pm; and 
PairSet+(Ci, Cj) be the set of correct anchor pairs 
of types Ci, Cj. We evaluate both precision and 
recall of Pm as
 
   
 ||||
|),(||
)(
m
jim
m PairSet
CCPairsSetPairSet
PrecisionP
|=
+?  
 (3) 
 
   
 ||),(||
|),(||
)(
ji
jim
m CCPairsSet
CCPairsSetPairSet
PecallR +
+ |= ?   (4) 
 
These values are stored and used in the training 
model for use during testing. 
 
(b) Evaluation of relation. Here we want to 
evaluate whether relation InputRel belongs to a 
path between anchors InputAi and InputAj. We 
employ the constructed classifier for the anchor 
types InputCi and InputCj in 2 stages. First, we 
find a subset P(0) = { Pm = <Si ?InputRel-> Sj> ? 
P+  | Si ? InputSeti, Sj ? InputSetj } of applicable 
patterns. Second, we utilize P(0) to find the pattern 
Pm(0) with maximal precision: 
 
 
 
 Precision(Pm
(0)) = argmaxPm?P(0) Precision (Pm) (5) 
 
A problem arises if Pm(0) is evaluated only on a 
small amount of training instances. For example, 
we noticed that patterns that cover 1 or 2 instances 
may lead to Precision=1, whereas on the testing 
corpus their accuracy becomes less than 50%. 
Therefore, it is important to additionally consider 
the recall parameter of Pm(0). 
Relation path ranking 
In this section, we want to evaluate relation path 
connecting template slots Ci and Cj. We do this 
independently for each relation of type discourse 
and dependency. Let Recallk and Precisionk be the 
recall  and precision values of Relk in Path = [Ai, 
Rel1,?, Reln, Aj], both obtained from the previous 
step. First, we calculate the average recall of the 
involved relations: 
 
 W = (1/LengthPath) * ?Relk?Path Recallk (6) 
 
W gives the average recall of the involved rela-
tions and can be used as a measure of reliability of 
the relation Path. Next, we compute a combined 
score of average Precisionk weighted by Recallk:  
 
 Score = 1/(W*LengthPath)*?Relk?Path Recallk*Precisionk (7) 
 
We use all Precisionk values in the path here, be-
cause omitting a single relation may turn a correct 
path into the wrong one, or vice versa. The com-
bined score value is used as a ranking of the rela-
tion path. Experiments show that we need to give 
priority to scores with higher reliability W. Hence 
we use (W, Score) to evaluate each Path.  
5.2 Integration of different relation path 
types 
The purpose of this stage is to integrate the evalua-
tions for different types of relation paths. The input 
to this stage consists of evaluated relation paths 
PathC and PathD for discourse and dependency 
relations respectively. Let (Wl, Scorel) be an 
evaluation for Pathl, l ? [c, d]. We first define an 
integral path PathI between Ai and Aj as: 1) PathI 
is enabled if at least one of Pathl, l ? [c, d], is en-
abled; and 2) PathI is correct if at least one of 
Pathl is correct. To evaluate PathI, we consider the 
average recall Wl of each Pathl, because Wl esti-
elaboration 
obj 
Anchor Aj 
 
town 
pos_NN 
Cand_AtArg2 
Minipar_pcompn
ArgM-Loc 
Spade_Satellite
Anchor Ai 
 
Marshal 
pos_NNP 
list_personWord 
Cand_AtArg1 
Minipar_obj 
Arg2 
Spade Satellite 
pcomp-n
fc 
span 
Discourse path 
Dependency path 
i 
elaboration 
Input sentence 
Marshal? named <At-A1> </> played by Amourie Kats who discovers all kinds 
of on and scary things going on in <At-A2>
Dependency patterns 
 
Minipar_obj <?i- ArgM-Loc 
Minipar_obj <?obj- ArgM-Loc 
Minipar_obj ?pcompn-> Minipar_pcompn
Minipar_obj ?mod-> Minipar_pcompn 
?
a seemingly quiet little town</> ... 
elaboration 
pnmod 
pred i 
null 
null 
rel 
i mod 
Discourse patterns 
 
list_personWord <?elaboration- pos_NN 
list_personWord ?elaboration-> town 
list_personWord <?span- town 
list_personWord <?elaboration- town 
? 
596
mates the reliability of Scorel. We define a 
weighted average for Pathl as: 
 
 WI = WC + WD (8) 
 
 ScoreI = 1/WI * ? l  Wl*Scorel (9) 
 
Next, we want to determine the threshold score 
ScoreIO above which ScoreI is acceptable. This 
score may be found by analyzing the integral paths 
on the training corpus. Let SI = { PathI } be the set 
of integral paths between anchors Ai and Aj on the 
training set. Among the paths in SI, we need to de-
fine a set function SI(X) = { PathI | ScoreI(PathI) 
? X } and find the optimal threshold for X. We find 
the optimal threshold based on F1-measure, be-
cause precision and recall are equally important in 
IE. Let SI(X)+ ? SI(X) and S(X)+ ? S(X) be sets of 
correct path extractions. Let FI(X) be F1-measure 
of SI(X): 
 
    
 ||)(||
||)(||
)(
XS
XS
XP
I
I
I
+
=  (10)  
||)(||
||)(||
)( +
+
=
XS
XS
XR II
 (11) 
 
  
 )()(
)(*)(*2
)(
XRXP
XRXP
XF
II
II
I +=
 (12) 
 
Based on the computed values FI(X) for each X on 
the training data, we determine the optimal thresh-
old as Score  = argmax  F  (X)IO X I , which corre-
sponds to the maximal expected F1-measure of 
anchor pair Ai and Aj.  
5.3 Evaluation of templates 
At this stage, we have a set of accepted integral 
relation paths between any anchor pair Ai and Aj. 
The next task is to merge appropriate set of an-
chors into candidate templates. Here we follow the 
methodology of Maslennikov et al (2006). For 
each sentence, we compose a set of candidate tem-
plates T using the extracted relation paths between 
each Ai and Aj. To evaluate each template Ti?T, 
we combine the integral scores from relation paths 
between its anchors Ai and Aj into the overall Rela-
tion_ScoreT: 
 
  
 
M
AAScore
TScoreelationR Kji
jiI
iT
? ??= ,1 ),()(_  (13) 
 
where K is the number of extracted slots, M is the 
number of extracted relation paths between an-
chors Ai and Aj, and ScoreI(Ai, Aj) is obtained 
from Equation (9). 
Next, we calculate the extracted entity score 
based on the scores of all the anchors in Ti: 
 
 ? ??= Kk kiT KAScoreEntityTScoreEntity 1 /)(_)(_  (14) 
 
where Entity_Score(Ai) is taken from Equation (1).  
Finally, we obtain the combined evaluation for a 
template:  
 
  
 
ScoreT(Ti) = (1- ?) * Entity_ScoreT (Ti) + 
                           ?  * Relation_ScoreT (Ti) (15) 
 
where ? is a predefined constant. 
In order to decide whether the template Ti 
should be accepted or rejected, we need to deter-
mine a threshold ScoreTO from the training data. If 
anchors of a candidate template match slots in a 
correct template, we consider the candidate tem-
plate as correct. Let TrainT = { Ti }  be the set of 
candidate templates extracted from the training 
data, TrainT+ ? TrainT be the subset of correct 
candidate templates, and TotalT+ be the total set of 
correct templates in the training data. Also, let 
TrainT(X) = { Ti | ScoreT(Ti) ? X, Ti ? TrainT } be 
the set of candidate templates with score above X 
and TrainT+(X) ? TrainT(X) be the subset of cor-
rect candidate templates. We define the measures 
of precision, recall and F1 as follows: 
 
   
 ||)(||
||)(||
)(
XTrainT
XTrainT
XPT
+
=  (16) ||||
||)(||
)( +
+
=
TotalT
XTrainT
XRT (17) 
 
   
 )()(
)()(*2
)(
XRXP
XRXP
XF
TT
TT
T +=
 
 
 
(18) 
 
Since the performance in IE is measured in F1-
measure, an appropriate threshold to be used for 
the most prominent candidate templates is: 
 
 ScoreT
O = argmaxX FT (X) (19) 
 
The value ScoreTO is used as a training model. 
During testing, we accept a candidate template In-
putTi if ScoreT(InputTi) > Sco Ore . T
As an additional remark, we note that domains 
MUC4, MUC6 and ACE RDC 2003 are signifi-
cantly different in the evaluation methodology for 
the candidate templates. While the performance of 
the MUC4 domain is measured for each slot indi-
vidually; the MUC6 task measures the perform-
ance on the extracted templates; and the ACE RDC 
2003 task evaluates performance on the matching 
relations. To overcome these differences, we con-
struct candidate templates for all the domains and 
measure the required type of performance for each 
domain. Our candidate templates for the ACE 
RDC 2003 task consist of only 2 slots, which cor-
respond to entities of the correct relations. 
597
6 Experimental results 
We carry out our experiments on 3 domains: 
MUC4 (Terrorism), MUC6 (Management Succes-
sion), and ACE-Relation Detection and Characteri-
zation (2003). The MUC4 corpus contains 1,300 
documents as training set and 200 documents 
(TST3 and TST4) as official testing set. We used a 
modified version of the MUC6 corpus described 
by Soderland (1999). This version includes 599 
documents as training set and 100 documents as 
testing set. Following the methodology of Zhang et 
al. (2006), we use only the English portion of ACE 
RDC 2003 training data. We used 97 documents 
for testing and the remaining 155 documents for 
training. Our task is to extract 5 major relation 
types and 24 subtypes. 
 
Case (%) P R F1
GRID 52% 62% 57% 
Riloff?05 46% 51% 48% 
ARE (2006) 58% 61% 60% 
ARE 65% 61% 63% 
Table 2. Results on MUC4 
To compare the results on the terrorism domain 
in MUC4, we choose the recent state-of-art sys-
tems GRID by Xiao et al (2004), Riloff et al 
(2005) and ARE (2006) by Maslennikov et al 
(2006) which does not utilize discourse and seman-
tic relations. The comparative results are given in 
Table 2. It shows that our enhanced ARE results in 
3% improvement in F1 measure over ARE (2006) 
that does not use clausal relations. The improve-
ment was due to the use of discourse relations on 
long paths, such as ?X distributed leaflets claiming 
responsibility for murder of Y?. At the same time, 
for many instances, it would be useful to store the 
extracted anchors for another round of learning. 
For example, the extracted features of discourse 
pattern ?murder ?same_clause-> HUM_PERSON? 
may boost the score for patterns that correspond to 
relation path ?X <-span- _ -Elaboration->  mur-
der?. In this way, high-precision patterns will sup-
port the refinement of patterns with average recall 
and low precision. This observation is similar to 
that described in Ciravegna?s work on (LP)2 
(Ciravegna 2001). 
 
Case (%) P R F1  
Chieu et al?02 75% 49% 59% 
ARE (2006) 73% 58% 65% 
ARE 73% 70% 72% 
Table 3. Results on MUC6 
Next, we present the performance of our system 
on MUC6 corpus (Management Succession) as 
shown in Table 3. The improvement of 7% in F1 is 
mainly due to the filtering of irrelevant depend-
ency relations. Additionally, we noticed that 22% 
of testing sentences contain 2 answer templates, 
and entities in many of such templates are inter-
twined. One example is the sentence ?Mr. Bronc-
zek who is 39 years old succeeds Kenneth Newell 
55 who was named to the new post of senior vice 
president?, which refers to 2 positions. We there-
fore we need to extract 2 templates ?PersonIn: 
Bronczek, PersonOut: Newell? and ?PersonIn: 
Newell, Post: senior vice president?. The discourse 
analysis is useful to extract the second template, 
while rejecting another long-distance template 
?PersonIn: Bronczek, PersonOut: Newell, Post: 
seniour vice president?. Another remark is that it 
is important to assign 2 anchors of 
?Cand_PersonIn? and ?Cand_PersonOut? for the 
phrase ?Kenneth Newell?.  
The characteristic of the ACE corpus is that it 
contains a large amount of variations, while only 
2% of possible dependency paths are correct. Since 
many of the relations occur only at the level of sin-
gle clause (for example, most instances of relation 
At), the discourse analysis is used to eliminate 
long-distance dependency paths. It allows us to 
significantly decrease the dimensionality of the 
problem. We noticed that 38% of relation paths in 
ACE contain a single relation, 28% contain 2 rela-
tions and 34% contain ? 3 relations. For the case 
of  ? 3 relations, the analysis of dependency paths 
alone is not sufficient to eliminate the unreliable 
paths. Our results for general types and specific 
subtypes are presented in Tables 6 and 7, respec-
tively. 
 
 
Case (%) P R F1  
Zhang et al?06 77% 65% 70% 
ARE 79% 66% 73% 
Table 4. Results on ACE RDC?03, general types 
Based on our results in Table 4, discourse and 
dependency relations support each other in differ-
ent situations. We also notice that multiple in-
stances require modeling of entities in the path. 
Thus, in our future work we need to enrich the 
search space for relation patterns. This observation 
corresponds to that reported in Zhang et al (2006). 
Discourse parsing is very important to reduce 
the amount of variations for specific types on ACE  
598
RDC?03, as there are 48 possible anchor types.  
 
Case (%) P R F1
Zhang et al?06 64% 51% 57% 
ARE 67% 54% 61% 
Table 5. Results on ACE RDC?03, specific types 
The relatively small improvement of results in 
Table 5 may be attributed to the following reasons: 
1) it is important to model the commonality rela-
tions, as was done by Zhou et al (2006); and 2) 
our relation paths do not encode entities. This is 
different from Zhang et al (2006), who were using 
entities in their subtrees. 
Overall, the results indicate that the use of dis-
course relations leads to improvement over the 
state-of-art systems.  
7 Conclusion 
We presented a framework that permits the inte-
gration of discourse relations with dependency re-
lations. Different from previous works, we tried to 
use the information about sentence structure based 
on discourse analysis. Consequently, our system 
improves the performance in comparison with the 
state-of-art IE systems. Another advantage of our 
approach is in using domain-independent parsers 
and features. Therefore, ARE may be easily port-
able into new domains.  
Currently, we explored only 2 types of relation 
paths: dependency and discourse. For future re-
search, we plan to integrate more relations in our 
multi-resolution framework. 
References 
P. Cimiano and U. Reyle. 2003. Ontology-based semantic 
construction, underspecification and disambiguation. In 
Proc of the Prospects and Advances in the Syntax-
Semantic Interface Workshop. 
P. Cimiano, U. Reyle and J. Saric. 2005. Ontology-driven 
discourse analysis for information extraction. Data & 
Knowledge Engineering, 55(1):59-83. 
H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy Ap-
proach to Information Extraction from Semi-Structured 
and Free Text. In Proc of AAAI-2002. 
F. Ciravegna. 2001. Adaptive Information Extraction from 
Text by Rule Induction and Generalization. In Proc of 
IJCAI-2001. 
A. Culotta and J. Sorensen J. 2004. Dependency tree ker-
nels for relation extraction. In Proc of ACL-2004. 
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum 
likelihood from incomplete data via the EM algorithm. 
Journal of the Royal Statistical Society B, 39(1):1?38. 
B. Grosz and C. Sidner. 1986. Attention, Intentions and  
the Structure of Discourse. Computational Linguistics, 
12(3):175-204. 
M. Halliday and R. Hasan. 1976. Cohesion in English. 
Longman, London. 
D. Lin. 1997. Dependency-based Evaluation of Minipar. In 
Workshop on the Evaluation of Parsing systems. 
M. Maslennikov, H.K. Goh and T.S. Chua. 2006. ARE: 
Instance Splitting Strategies for Dependency Relation-
based Information Extraction. In Proc of ACL-2006. 
E. Miltsakaki. 2003. The Syntax-Discourse Interface: Ef-
fects of the Main-Subordinate Distinction on Attention 
Structure. PhD thesis. 
M.F. Moens and R. De Busser. 2002. First steps in building 
a model for the retrieval of court decisions. International 
Journal of Human-Computer Studies, 57(5):429-446. 
S. Pradhan, W. Ward, K. Hacioglu, J. Martin and D. Juraf-
sky. 2004. Shallow Semantic Parsing using Support 
Vector Machines. In Proc of HLT/NAACL-2004. 
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting Sub-
jectivity Classification to Improve Information Extrac-
tion. In Proc of AAAI-2005. 
S. Soderland. 1999. Learning Information Extraction Rules 
for Semi-Structured and Free Text. Machine Learning, 
34:233-272. 
R. Soricut and D. Marcu. 2003. Sentence Level Discourse 
Parsing using Syntactic and Lexical Information. In 
Proc of HLT/NAACL. 
M. Surdeanu, S. Harabagiu, J. Williams, P. Aarseth. 2003. 
Using Predicate Arguments Structures for Information 
Extraction. In Proc of ACL-2003. 
M. Taboada and W. Mann. 2005. Applications of Rhetori-
cal Structure Theory. Discourse studies, 8(4). 
B. Webber, M. Stone, A. Joshi and A. Knott. 2002. 
Anaphora and Discourse Structure. Computational Lin-
guistics, 29(4). 
J. Xiao, T.S. Chua and H. Cui. 2004. Cascading Use of 
Soft and Hard Matching Pattern Rules for Weakly Su-
pervised Information Extraction. In Proc of COLING-
2004. 
M. Zhang, J. Zhang, J. Su and G. Zhou. 2006. A Compos-
ite Kernel to Extract Relations between Entities with 
both Flat and Structured Features. In Proc of ACL-2006. 
G. Zhou, J. Su and M. Zhang. 2006. Modeling Commonal-
ity among Related Classes in Relation Extraction. In 
Proc of ACL-2006. 
599
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 199?207,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Summarizing Definition from Wikipedia
Shiren Ye and Tat-Seng Chua and Jie Lu
Lab of Media Search
National University of Singapore
{yesr|chuats|luj}@comp.nus.edu.sg
Abstract
Wikipedia provides a wealth of knowl-
edge, where the first sentence, infobox
(and relevant sentences), and even the en-
tire document of a wiki article could be
considered as diverse versions of sum-
maries (definitions) of the target topic.
We explore how to generate a series of
summaries with various lengths based on
them. To obtain more reliable associations
between sentences, we introduce wiki con-
cepts according to the internal links in
Wikipedia. In addition, we develop an
extended document concept lattice model
to combine wiki concepts and non-textual
features such as the outline and infobox.
The model can concatenate representative
sentences from non-overlapping salient lo-
cal topics for summary generation. We test
our model based on our annotated wiki ar-
ticles which topics come from TREC-QA
2004-2006 evaluations. The results show
that the model is effective in summariza-
tion and definition QA.
1 Introduction
Nowadays, ?ask Wikipedia? has become as pop-
ular as ?Google it? during Internet surfing, as
Wikipedia is able to provide reliable information
about the concept (entity) that the users want. As
the largest online encyclopedia, Wikipedia assem-
bles immense human knowledge from thousands of
volunteer editors, and exhibits significant contribu-
tions to NLP problems such as semantic related-
ness, word sense disambiguation and question an-
swering (QA).
For a given definition query, many search en-
gines (e.g., specified by ?define:? in Google) often
place the first sentence of the corresponding wiki1
article at the top of the returned list. The use of
1 For readability, we follow the upper/lower case rule
on web (say, ?web pages? and ?on the Web?), and utilize
one-sentence snippets provides a brief and concise
description of the query. However, users often need
more information beyond such a one-sentence de-
finition, while feeling that the corresponding wiki
article is too long. Thus, there is a strong demand
to summarize wiki articles as definitions with vari-
ous lengths to suite different user needs.
The initial motivation of this investigation is to
find better definition answer for TREC-QA task
using Wikipedia (Kor and Chua, 2007). Accord-
ing to past results on TREC-QA (Voorhees, 2004;
Voorhees and Dang, 2005), definition queries are
usually recognized as being more difficult than fac-
toid and list queries. Wikipedia could help to
improve the quality of answer finding and even
provide the answers directly. Its results are bet-
ter than other external resources such as WordNet,
Gazetteers and Google?s define operator, especially
for definition QA (Lita et al, 2004).
Different from the free text used in QA and sum-
marization, a wiki article usually contains valuable
information like infobox and wiki link. Infobox
tabulates the key properties about the target, such
as birth place/date and spouse for a person as well
as type, founder and products for a company. In-
fobox, as a form of thumbnail biography, can be
considered as a mini version of a wiki article?s sum-
mary. In addition, the relevant concepts existing in
a wiki article usually refer to other wiki pages by
wiki internal links, which will form a close set of
reference relations. The current Wikipedia recur-
sively defines over 2 million concepts (in English)
via wiki links. Most of these concepts are multi-
word terms, whereas WordNet has only 50,000 plus
multi-word terms. Any term could appear in the
definition of a concept if necessary, while the total
vocabulary existing in WordNet?s glossary defini-
tion is less than 2000. Wikipedia addresses explicit
semantics for numerous concepts. These special
knowledge representations will provide additional
information for analysis and summarization. We
thus need to extend existing summarization tech-
nologies to take advantage of the knowledge repre-
sentations in Wikipedia.
?wiki(pedia) articles? and ?on (the) Wikipedia?, the latter re-
ferring to the entire Wikipedia.
199
The goal of this investigation is to explore sum-
maries with different lengths in Wikipedia. Our
main contribution lies in developing a summariza-
tion method that can (i) explore more reliable asso-
ciations between passages (sentences) in huge fea-
ture space represented by wiki concepts; and (ii) ef-
fectively combine textual and non-textual features
such as infobox and outline in Wikipedia to gener-
ate summaries as definition.
The rest of this paper is organized as follows: In
the next section, we discuss the background of sum-
marization using both textual and structural fea-
tures. Section 3 presents the extended document
concept lattice model for summarizing wiki arti-
cles. Section 4 describes corpus construction and
experiments are described; while Section 5 con-
cludes the paper.
2 Background
Besides some heuristic rules such as sentence po-
sition and cue words, typical summarization sys-
tems measure the associations (links) between sen-
tences by term repetitions (e.g., LexRank (Erkan
and Radev, 2004)). However, sophisticated authors
usually utilize synonyms and paraphrases in vari-
ous forms rather than simple term repetitions. Fur-
nas et al (1987) reported that two people choose
the same main key word for a single well-known
object less than 20% of the time. A case study by
Ye et al (2007) showed that 61 different words ex-
isting in 8 relevant sentences could be mapped into
16 distinctive concepts by means of grouping terms
with close semantic (such as [British, Britain, UK]
and [war, fought, conflict, military]). However,
most existing summarization systems only consider
the repeated words between sentences, where latent
associations in terms of inter-word synonyms and
paraphrases are ignored. The incomplete data likely
lead to unreliable sentence ranking and selection for
summary generation.
To recover the hidden associations between sen-
tences, Ye et al (2007) compute the semantic simi-
larity using WordNet. The term pairs with semantic
similarity higher than a predefined threshold will be
grouped together. They demonstrated that collect-
ing more links between sentences will lead to bet-
ter summarization as measured by ROUGE scores,
and such systems were rated among the top systems
in DUC (document understanding conference) in
2005 and 2006. This WordNet-based approach has
several shortcomings due to the problems of data
deficiency and word sense ambiguity, etc.
Wikipedia already defined millions of multi-
word concepts in separate articles. Its definition is
much larger than that of WordNet. For instance,
more than 20 kinds of songs and movies called But-
terfly , such as Butterfly (Kumi Koda song), Butter-
fly (1999 film) and Butterfly (2004 film), are listed
in Wikipedia. When people say something about
butterfly in Wikipedia, usually, a link is assigned
to refer to a particular butterfly. Following this
link, we can acquire its explicit and exact seman-
tic (Gabrilovich and Markovitch, 2007), especially
for multi-word concepts. Phrases are more im-
portant than individual words for document re-
trieval (Liu et al, 2004). We hope that the wiki con-
cepts are appropriate text representation for sum-
marization.
Generally, wiki articles have little redundancy
in their contents as they utilize encyclopedia style.
Their authors tend to use wiki links and ?See Also?
links to refer to the involved concepts rather than
expand these concepts. In general, the guideline
for composing wiki articles is to avoid overlong
and over-complicated styles. Thus, the strategy of
?split it? into a series of articles is recommended;
so wiki articles are usually not too long and contain
limited number of sentences. These factors lead to
fewer links between sentences within a wiki article,
as compared to normal documents. However, the
principle of typical extractive summarization ap-
proaches is that the sentences whose contents are
repeatedly emphasized by the authors are most im-
portant and should be included (Silber and McCoy,
2002). Therefore, it is challenging to summarize
wiki articles due to low redundancy (and links)
between sentences. To overcome this problem,
we seek (i) more reliable links between passages,
(ii) appropriate weighting metric to emphasize the
salient concepts about the topic, and (iii) additional
guideline on utilizing non-textual features such as
outline and infobox. Thus, we develop wiki con-
cepts to replace ?bag-of-words? approach for better
link measurements between sentences, and extend
an existing summarization model on free text to in-
tegrate structural information.
By analyzing rhetorical discourse structure of
aim, background, solution, etc. or citation context,
we can obtain appropriate abstracts and the most
influential contents from scientific articles (Teufel
and Moens, 2002; Mei and Zhai, 2008). Similarly,
we believe that the structural information such as
infobox and outline is able to improve summariza-
tion as well. The outline of a wiki article using in-
ner links will render the structure of its definition.
In addition, infobox could be considered as topic
signature (Lin and Hovy, 2000) or keywords about
the topic. Since keywords and summary of a doc-
ument can be mutually boosted (Wan et al, 2007),
infobox is capable of summarization instruction.
When Ahn (2004) and Kor (2007) utilize
Wikipedia for TREC-QA definition, they treat the
Wikipedia as the Web and perform normal search
on it. High-frequency terms in the query snippets
returned from wiki index are used to extend query
and rank (re-rank) passages. These snippets usually
200
come from multiple wiki articles. Here the use-
ful information may be beyond these snippets but
existing terms are possibly irrelevant to the topic.
On the contrary, our approach concentrates on the
wiki article having the exact topic only. We as-
sume that every sentence in the article is used to de-
fine the query topic, no matter whether it contains
the term(s) of the topic or not. In order to extract
some salient sentences from the article as definition
summaries, we will build a summarization model
that describes the relations between the sentences,
where both textual and structural features are con-
sidered.
3 Our Approach
3.1 Wiki Concepts
In this subsection, we address how to find rea-
sonable and reliable links between sentences using
wiki concepts.
Consider a sentence: ?After graduating from
Boston University in 1988, she went to work at a
Calvin Klein store in Boston.? from a wiki article
?Carolyn Bessette Kennedy?2, we can find 11 dis-
tinctive terms, such as after, graduate, Boston, Uni-
versity,1988, go, work, Calvin, Klein, store, Boston,
if stop words are ignored.
However, multi-word terms such as Boston
University and Calvin Klein are linked to the
corresponding wiki articles, where their definitions
are given. Clearly, considering the anchor texts as
two wiki concepts rather than four words is more
reasonable. Their granularity are closer to semantic
content units in a summarization evaluation method
Pyramid (Nenkova et al, 2007) and nuggets in
TREC-QA . When the text is represented by
wiki concepts, whose granularity is similar to the
evaluation units, it is possibly easy to detect the
matching output using a model. Here,
? Two separate words, Calvin and Klein, are
meaningless and should be discarded; oth-
erwise, spurious links between sentences are
likely to occur.
? Boston University and Boston are processed
separately, as they are different named entities.
No link between them is appropriate3.
? Terms such as ?John F. Kennedy, Jr.? and
?John F. Kennedy? will be considered as two
diverse wiki concepts, but we do not account
on how many repeated words there are.
? Different anchor texts, such as U.S.A. and
United States of America, are recognized as
2All sample sentences in this paper come from this article
if not specified.
3Consider new pseudo sentence: ?After graduating from
Stanford in 1988, she went to work ... in Boston.? We do not
need assign link between Stanford and Boston as well.
an identical concept since they refer to the
same wiki article.
? Two concepts, such as money and cash, will
be merged into an identical concept when their
semantics are similar.
In wiki articles, the first occurrence of a wiki
concept is tagged by a wiki link, but there is no
such a link to its subsequent occurrences in the re-
maining parts of the text in most cases. To allevi-
ate this problem, a set of heuristic rules is proposed
to unify the subsequent occurrences of concepts in
normal text with previous wiki concepts in the an-
chor text. These heuristic rules include: (i) edit dis-
tance between linked wiki concept and candidates
in normal text is larger than a predefined threshold;
and (ii) partially overlapping words beginning with
capital letter, etc.
After filtering out wiki concepts, the words re-
maining in wiki articles could be grouped into two
sets: close-class terms like pronouns and preposi-
tions as well as open-class terms like nouns and
verbs. For example, in the sentence ?She died at age
33, along with her husband and sister?, the open-
class terms include die, age, 33, husband and sister.
Even though most open-class terms are defined in
Wikipedia as well, the authors of the article do not
consider it necessary to present their references us-
ing wiki links. Hence, we need to extend wiki con-
cepts by concatenating them with these open-class
terms to form an extended vector. In addition, we
ignore all close-class terms, since we cannot find
efficient method to infer reliable links across them.
As a result, texts are represented as a vector of wiki
concepts.
Once we introduce wiki concepts to replace typ-
ical ?bag-of-words? approach, the dimensions of
concept space will reach six order of magnitudes.
We cannot ignore the data spareness issue and com-
putation cost when the concept space is so huge.
Actually, for a wiki article and a set of relevant arti-
cles, the involved concepts are limited, and we need
to explore them in a small sub-space. For instance,
59 articles about Kennedy family in Wikipedia have
10,399 distinctive wiki concepts only, where 5,157
wiki concepts exist twice and more. Computing the
overlapping among them is feasible.
Furthermore, we need to merge the wiki concepts
with identical or close semantic (namely, building
links between these synonyms and paraphrases).
We measure the semantic similarity between two
concepts by using cosine distance between their
wiki articles, which are represented as the vectors
of wiki concepts as well. For computation effi-
ciency, we calculate semantic similarities between
all promising concept pairs beforehand, and then
retrieve the value in a Hash table directly. We spent
CPU time of about 12.5 days preprocessing the se-
201
mantic calculation. Details are available at our tech-
nical report (Lu et al, 2008).
Following the principle of TFIDF, we define
the weighing metric for the vector represented
by wiki concepts using the entire Wikipedia as
the observation collection. We define the CFIDF
weight of wiki concept i in article j as:
wi,j = cfi,j? idfi = ni,j?
k nk,j
? log |D||dj : ti ? dj| ,
(1)
where cfi,j is the frequency of concept i in arti-
cle j; idfi is the inverse frequency of concept i
in Wikipedia; and D is the number of articles in
Wikipedia. Here, sparse wiki concepts will have
more contribution.
In brief, we represent articles in terms of wiki
concepts using the steps below.
1. Extract the wiki concepts marked by wiki
links in context.
2. Detect the remaining open-class terms as wiki
concepts as well.
3. Merge concepts whose semantic similarity is
larger than predefined threshold (0.35 in our
experiments) into the one with largest idf .
4. Weight all concepts according to Eqn (1).
3.2 Document Concept Lattice Model
Next, we build the document concept lattice (DCL)
for articles represented by wiki concepts. For il-
lustration on how DCL is built, we consider 8 sen-
tences from DUC 2005 Cluster d324e (Ye et al,
2007) as case study. 8 sentences, represented by 16
distinctive concepts A-P, are considered as the base
nodes 1-8 as shown in Figure 1. Once we group
nodes by means of the maximal common concepts
among base nodes hierarchically, we can obtain the
derived nodes 11-41, which form a DCL. A derived
node will annotate a local topic through a set of
shared concepts, and define a sub concept space that
contains the covered base nodes under proper pro-
jection. The derived node, accompanied with its
base nodes, is apt to interpret a particular argument
(or statement) about the involved concepts. Further-
more, one base node among them, coupled with the
corresponding sentence, is capable of this interpre-
tation and could represent the other base nodes to
some degree.
In order to Extract a set of sentences to cover
key distinctive local topics (arguments) as much as
possible, we need to select a set of important non-
overlapping derived nodes. We measure the impor-
tance of node N in DCL of article j in term of rep-
resentative power (RP) as:
RP (N) =
?
ci?N
(|ci|?wi,j)/ log(|N |), (2)
         
	 
     
  
     
	
 
  



      
	
 



 
      
	
 



     
	
 
     
         Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 185?188,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Query Segmentation Based on Eigenspace Similarity
Chao Zhang
? ?
Nan Sun
?
Xia Hu
?
Tingzhu Huang
?
Tat-Seng Chua
?
?
School of Applied Math
?
School of Computing
University of Electronic Science National University of Singapore,
and Technology of China,
Chengdu, 610054, P.R. China Computing 1, Singapore 117590
zhangcha@comp.nus.edu.sg {sunn,huxia,chuats}@comp.nus.edu.sg
tzhuang@uestc.edu.cn
Abstract
Query segmentation is essential to query
processing. It aims to tokenize query
words into several semantic segments and
help the search engine to improve the
precision of retrieval. In this paper, we
present a novel unsupervised learning ap-
proach to query segmentation based on
principal eigenspace similarity of query-
word-frequency matrix derived from web
statistics. Experimental results show that
our approach could achieve superior per-
formance of 35.8% and 17.7% in F-
measure over the two baselines respec-
tively, i.e. MI (Mutual Information) ap-
proach and EM optimization approach.
1 Introduction
People submit concise word-sequences to search
engines in order to obtain satisfying feedback.
However, the word sequences are generally am-
biguous and often fail to convey the exact informa-
tion to search engine, thus severely, affecting the
performance of the system. For example, given
the query ?free software testing tools download?.
A simple bag-of-words query model cannot ana-
lyze ?software testing tools? accurately. Instead, it
returns ?free software? or ?free download? which
are high frequency web phrases. Therefore, how
to segment a query into meaningful semantic com-
ponents for implicit description of user?s intention
is an important issue both in natural language pro-
cessing and information retrieval fields.
There are few related studies on query segmen-
tation in spite of its importance and applicability
in many query analysis tasks such as query sug-
gestion, query substitution, etc. To our knowl-
edge, three approaches have been studied in pre-
vious works: MI (Mutual Information) approach
(Jones et al, 2006; Risvik et al, 2003), supervised
learning approach (Bergsma and Wang, 2007) and
EM optimization approach (Tan and Peng, 2008).
However, MI approach calculates MI value just
between two adjacent words that cannot handle
long entities. Supervised learning approach re-
quires a sufficiently large number of labeled train-
ing data, which is not conducive in real applica-
tions. EM algorithm often converges to a local
maximum that depends on the initial conditions.
There are also many relevant research on Chinese
word segmentation (Teahan et al, 2000; Peng and
Schuurmans, 2001; Xu et al, 2008). However,
they cannot be applied directly to query segmenta-
tion (Tan and Peng, 2008).
Under this scenario, we propose a novel unsu-
pervised approach for query segmentation. Dif-
fering from previous work, we first adopt the n-
gram model to estimate the query term?s frequency
matrix based on word occurrence statistics on the
web. We then devise a new strategy to select prin-
cipal eigenvectors of the matrix. Finally we cal-
culate the similarity of query words for segmen-
tation. Experimental results demonstrate the ef-
fectiveness of our approach as compared to two
baselines.
2 Methodology
In this Section, we introduce our proposed query
segmentation approach, which is based on query
word frequency matrix principal eigenspace simi-
larity. To facilitate understanding, we first present
a general overview of our approach in Section 2.1
and then describe the details in Section 2.2-2.5.
2.1 Overview
Figure 1 briefly shows the main procedure of
our proposed query segmentation approach. It
starts with a query which consists of a vector of
words{w
1
w
2
? ? ?w
n
}. Our approach first build a
query-word frequency matrix M based on web
statistics to describe the relationship between any
185
two query words (Step 1). After decomposing M
(step 2), the parameter k which defines the num-
ber of segments in the query is estimate in Step 3.
Besides, a principal eigenspace of M is built and
the projection vectors({?
i
}, i ? [1, n]) associated
with each query-word are obtained (Step 4). Simi-
larities between projection vectors are then calcu-
lated, which determine whether the corresponding
two words should be segmented together (Step5).
If the number of segmented components is not
equal to k, our approach modifies the threshold ?
and repeats steps 5 and 6 until the correct k num-
ber of segmentations are obtained(Step 7).
Input: one n words query: w
1
w
2
? ? ?w
n
;
Output: k segmented components of query;
Step 1: Build a frequency matrix M (Section
2.2);
Step 2: Decompose M into sorted eigenvalues
and eigenvectors;
Step 3: Estimate parameter k (Section 2.4);
Step 4: Build principal eigenspace with first
k eigenvectors and get the projection
({?
i
}) of M in principal eigenspace
(Section 2.3);
Step 5: Segment the query: if (?
i
??
T
j
)/(??
i
??
??
j
?) ? ?, segment w
i
and w
j
to-
gether (Section 2.5)
Step 6: If the number of segmented parts does
not equal to k, modify ?, go to step 5;
Step 7: output the right segmentations
Figure 1: Query Segmentation based on query-
word-frequency matrix eigenspace similarity
2.2 Frequency Matrix
Let W = w
1
, w
2
, ? ? ? , w
n
be a query of n words.
We can build the relationships of any two words
using a symmetric matrix: M = {m
i,j
}
n?n
m
i,j
=
?
?
?
?
?
F (w
i
) if i = j
F (w
i
w
i+1
? ? ?w
j
) if i < j
m
j,i
if i > j
(1)
F (w
i
w
i+1
? ? ?w
j
) =
count(w
i
w
i+1
? ? ?w
j
)
?
n
i=1
w
i
(2)
Here m
i,j
denotes the correlation between
(w
i
? ? ?w
j?1
) and w
j
, where (w
i
? ? ?w
j?1
) means
a sequence and w
j
is a word. Considering the dif-
ference of each matrix element m
i,j
, we normalize
m
i,j
with:
m
i,j
= 2 ?m
i,j
/(m
i,i
+m
j,j
) (3)
F (?) is a function measuring the frequency of
query words or sequences. To improve the preci-
sion of measurement and reduce the computation
cost, we adopt the approach proposed by (Wang
et al, 2007) here. First, we extract the relevant
documents associated with the query via Google
Soap Search API. Second, we count the number
of all possible n-gram sequences which are high-
lighted in the titles and snippets of the returned
documents. Finally, we use Eqn.(2) to estimate
the value of m
i,j
.
2.3 Principal Eigenspace
Although matrix M depicts the correlation of
query words, it is rough and noisy. Under
this scenario, we transform M into its princi-
pal eigenspace which is spanned by k largest
eigenvectors, and each query word is denoted
by the corresponding eigenvector in the principal
eigenspace.
Since M is a symmetric positive definite ma-
trix, its eigenvalues are real numbers and the
corresponding eigenvectors are non-zero and or-
thotropic to each other. Here, we denote the eigen-
values of M as : ?(M) = {?
1
, ?
2
, ? ? ? , ?
n
}
and ?
1
? ?
2
? ? ? ? ? ?
n
. All eigenvalues
of M have corresponding eigenvectors:V (M) =
{x
1
, x
2
, ? ? ? , x
n
}.
Suppose that principal eigenspace M(M ?
R
n?k
) is spanned by the first k eigenvectors, i.e.
M = Span{x
1
, x
2
, ? ? ?x
k
}, then row i of M can
be represented by vector ?
i
which denotes the i-th
word for similarity calculation in Section 2.5, and
?
i
is derived from:
{?
T
1
, ?
T
2
, ? ? ? , ?
T
n
}
T
= {x
1
, x
2
, ? ? ? , x
k
} (4)
Section 2.4 discusses the details of how to select
the parameter k.
2.4 Parameter k Selection
PCA (principal component analysis) (Jolliffe,
2002) often selects k principal components by the
following criterion:
k is the smallest integer which satisfies:
?
k
i=1
?
i
?
n
i=1
?
i
? Threshold (5)
186
where n is the number of eigenvalues. When ?
k
?
?
k+1
, Eqn.(5) is very effective. However, accord-
ing to the Gerschgorin circle theorem, the non-
diagonal values of M are so small that the eigen-
values cannot be distinguished easily. Under this
circumstance, a prefixed threshold is too restric-
tive to be applied in complex situations. Therefore
a function of n is introduced into the threshold as
follows:
?
k
i=1
?
i
?
n
i=1
?
i
? (
n? 1
n
)
2
(6)
If k eigenvalues are qualified to be the princi-
pal components, then the threshold in Eqn.(5) can-
not be lower than 0.5, and need not be higher than
n?1
n
. If the length of the shortest query we seg-
mented is 4, we choose (
n?1
n
)
2
because it will be
smaller than
n?1
n
and larger than 0.5 with n no
smaller than 4.
The k eigenvectors will be used to segment the
query into k meaningful segments (Weiss, 1999;
Ng et al, 2001). In the k-dimensional principal
eigenspace, each dimension of the space describes
a semantic concept of the query. When one eigen-
value is bigger, the corresponding dimension con-
tains more query words.
2.5 Similarity Computation
If the word i and word j are co-occurrence, ?
i
and ?
j
are approximately parallel in the principal
eigenspace; otherwise, they are approximately or-
thogonal to each other. Hence, we measure the
similarity of ?
i
and ?
j
with inner-product to per-
form the segmentation (Weiss, 1999; Ng et al,
2001). Selecting a proper threshold ?, we segment
the query using Eqn.(7):
S(w
i
, w
j
) =
{
1, (?
i
? ?
T
j
)/(??
i
? ? ??
j
?) ? ?
0, (?
i
? ?
T
j
)/(??
i
? ? ??
j
?) < ?
(7)
If S(w
i
, w
j
) = 1, w
i
and w
j
should be segmented
together, otherwise, w
i
and w
j
belong to different
semantic concepts respectively. Here, we denote
the total number of segments of the query as inte-
ger m.
As mentioned in Section 2.4, m should be equal
to k, therefore, the threshold ? is modified by k
and m. We set the initial value ? = 0.5 and modify
it with binary search method until m = k. If k is
larger than m, it means ? is too small to be a proper
threshold, i.e. some segments should be further
segmented. Otherwise, ? is too large that it should
be reduced.
3 Experiments
3.1 Data set
We experiment on the data set published by
(Bergsma and Wang, 2007). This data set com-
prises 500 queries which were randomly taken
from the AOL search query database and each
query. These queries are all segmented manually
by three annotators (the results are referred as A,
B and C).
We evaluate our results on the five test data sets
(Tan and Peng, 2008), i.e. we use A, B, C, the
intersection of three annotator?s results (referred
to as D) and the conjunction of three annotator?s
results (referred to as E). Besides, three evaluation
metrics are used in our experiments (Tan and Peng,
2008; Peng and Schuurmans, 2001), i.e. Precision
(referred to as Prec), Recall and F-Measure (re-
ferred to as F-mea).
3.2 Experimental results
Two baselines are used in our experiments: one is
MI based method (referred to as MI), and the other
is EM optimization (referred to as EM). Since the
EM proposed in (Tan and Peng, 2008) is imple-
mented with Yahoo! web corpus and only Google
Soap Search API is available in our study, we
adopt t-test to evaluate the performance of MI
with Google data (referred to as MI(G)) and Ya-
hoo! web corpus (referred to as MI(Y)). With the
values of MI(Y) and MI(G) in Table 1 we get the
p-value (p = 0.316 ? 0.05), which indicates that
the performance of MI with different corpuses has
no significant difference. Therefore, we can de-
duce that, the two corpuses have little influence on
the performance of the approaches. Here, we de-
note our approach as ?ES?, i.e. Eigenspace Simi-
larity approach.
Table 1 presents the performance of the three
approaches, i.e. MI (MI(Y) and MI(G)), EM and
our proposed ES on the five test data sets using the
three mentioned metrics. From Table 1 we find
that ES achieves significant improvements as com-
pared to the other two methods in any metric and
data set we used.
For further analysis, we compute statistical per-
formance on mathematical expectation and stan-
dard deviation as shown in Figure 2. We observe
a consistent trend of the three metrics increasing
from left to right as shown in Figure 2, i.e. EM
performs better than MI and ES is the best among
the three approaches.
187
MI(Y) MI(G) EM ES
Prec 0.469 0.548 0.562 0.652
A Recall 0.534 0.489 0.555 0.699
F-mea 0.499 0.517 0.558 0.675
Prec 0.408 0.449 0.568 0.632
B Recall 0.472 0.391 0.578 0.659
F-mea 0.438 0.418 0.573 0.645
Prec 0.451 0.503 0.558 0.614
C Recall 0.519 0.440 0.561 0.649
F-mea 0.483 0.469 0.559 0.631
Prec 0.510 0.574 0.640 0.772
D Recall 0.550 0.510 0.650 0.826
F-mea 0.530 0.540 0.645 0.798
Prec 0.582 0.672 0.715 0.834
E Recall 0.654 0.734 0.721 0.852
F-mea 0.616 0.702 0.718 0.843
Table 1: Performance of different approaches.
Figure 2: Statistical performance of approaches
First, we observe that, EM (Prec: 0.609, Recall:
0.613, F-mea: 0.611) performs much better than
MI (Prec: 0.549, Recall: 0.513, F-mea: 0.529).
This is because EM optimizes the frequencies of
query words with EM algorithms. In addition, it
should be noted that, the recall of MI is especially
unsatisfactory, which is caused by its shortcoming
on handling long entities.
Second, when compared with EM, ES also has
more than 15% increase in the three reference met-
rics (15.1% on Prec, 20.2% on Recall and 17.7%
on F-mea). Here all increases are statistically sig-
nificant with p-value closed to 0. In depth anal-
ysis indicates that this is because ES makes good
use of the frequencies of query words in its princi-
pal eigenspace, while EM algorithm trains the ob-
served data (frequencies of query words) by sim-
ply maximizing them using maximum likelihood.
4 Conclusion and Future work
We proposed an unsupervised approach for query
segmentation. After using n-gram model to es-
timate term frequency matrix using term occur-
rence statistics from the web, we explored a new
method to select principal eigenvectors and calcu-
late the similarities of query words for segmenta-
tion. Experiments demonstrated the effectiveness
of our approach, with significant improvement in
segmentation accuracy as compared to the previ-
ous works.
Our approach will be capable of extracting se-
mantic concepts from queries. Besides, it can ex-
tended to Chinese word segmentation. In future,
we will further explore a new method of parame-
ter k selection to achieve higher performance.
References
S. Bergsma and Q. I. Wang. 2007. Learning Noun
Phrase Query Segmentation. In Proc of EMNLP-
CoNLL
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006.
Generating query substitutions. In Proc of WWW.
I.T. Jolliffe. 2002. Principal Component Analysis.
Springer, NY, USA.
Andrew Y. Ng, Michael I. Jordan, Yair Weiss. 2001.
On spectral clustering: Analysis and an algorithm
In Proc of NIPS.
F. Peng and D. Schuurmans. 2001. Self-Supervised
Chinese Word Segmentation. Proc of the 4th Int?l
Conf. on Advances in Intelligent Data Analysis.
K. M. Risvik, T. Mikolajewski, and P. Boros. 2003.
Query Segmentation for Web Search. In Proc of
WWW.
Bin Tan, Fuchun Peng. 2008. Unsupervised Query
Segmentation Using Generative Language Models
and Wikipedia. In Proc of WWW.
W. J. Teahan Rodger Mcnab Yingying Wen Ian H. Wit-
ten . 2000. A compression-based algorithm for Chi-
nese word segmentation Computational Linguistics.
Xin-Jing Wang, Wen Liu, Yong Qin. 2007. A Search-
based Chinese Word Segmentation Method. In Proc
of WWW.
Yair Weiss. 1999. Segmentation using eigenvectors: a
unifying view. Proc. IEEE Int?l Conf. Computer Vi-
sion, vol. 2, pp. 975-982.
Jia Xu, Jianfeng Gao, Kristina Toutanova, Hermann.
2008. Bayesian Semi-Supervised Chinese Word Seg-
mentation for Statistical Machine Translation. In
Proc of COLING.
188
Extracting Pronunciation-translated Names from Chinese  
Texts using Bootstrapping Approach 
Jing Xiao 
School of Computing, 
National University of Singapore 
xiaojing@comp.nus.edu.sg 
Jimin Liu 
School of Computing, 
National University of Singapore 
liujm@comp.nus.edu.sg 
Tat-Seng Chua 
School of Computing, 
National University of Singapore 
chuats@comp.nus.edu.sg 
 
Abstract  
Pronunciation-translated names (P-Names) 
bring more ambiguities to Chinese word 
segmentation and generic named entity 
recognition. As there are few annotated 
resources that can be used to develop a good 
P-Name extraction system, this paper 
presents a bootstrapping algorithm, called 
PN-Finder, to tackle this problem. Starting 
from a small set of P-Name characters and 
context cue-words, the algorithm iteratively 
locates more P-Names from the Internet. 
The algorithm uses a combination of 
P-Name and context word probabilities to 
identify new P-Names. Experiments show 
that our PN-Finder is able to locate a large 
number of P-Names (over 100,000) from the 
Internet with a high recognition accuracy of 
over 85%. Further tests on the MET-2 test 
set show that our PN-Finder can achieve a 
performance of over 90% in F1 value in 
locating P-Names. The results demonstrate 
that our PN-Finder is effective. 
1 Introduction 
Pronunciation-translated names (P-Names) are 
those foreign names that are translated to Chinese 
characters according to their pronunciations. A 
P-Name sometimes forms part of but not a 
complete named entity. For instance, in the place 
name ?? (Berkeley University), only 
the term ?? (Berkeley) is a P-Name, while 
?? (University) is not since it is translated 
semantically.  
The ability to recognize P-Names helps to reduce 
ambiguities in word segmentation and improve the 
performance of Chinese information retrieval 
since many unknown words are P-Names, 
especially for international Chinese news. Unlike 
English, there is no blank between words in 
Chinese, in which a word is a linguistic token 
consisting of one or more characters. In addition, the 
same characters may appear in multiple context with 
different meanings (Chua and Liu, 2002). The 
presence of P-Names brings more ambiguities to 
Chinese word segmentation since every character in 
a P-Name can be used as a common character. 
Intuitively, we can extract the P-Names based on the 
distinctive sequence of characters that they are  used 
as compared to common words. In addition, we can 
use local context around the P-Names to confirm 
and classify them into person or part of location and 
organization names. One way to perform these tasks 
effectively is to rely on statistics derived from a 
large corpus in which the P-Names are annotated. 
While some annotated corpuses with general named 
entities are available such as the PKUC (Yu, 1999) 
and MET-2 (Chinchor, 2001), there is no such 
annotated corpus for P-Names. While annotated data 
is difficult to obtain, un-annotated data is readily 
available and plentiful, especially on the Internet. To 
take advantage of that, we need to tackle two major 
problems. The first is how to gather sufficient 
distinct P-Names from the Internet, and the second is 
how to use the available resources to derive reliable 
statistical information to characterize the P-Names. 
The problem of gathering sufficient reliable 
information from a small initial set of seed resources 
has been tackled in bootstrapping research for 
information extraction (Agichtein and Gravano, 
2000; Brin, 1998; Collins and Singer, 1999; 
Mihalcea and Moldovan, 2001; Riloff and Jones, 
1999). Bootstrapping approach aims to perform 
unsupervised text processing to extract information 
from open resources such as the Internet using 
minimum manual labor. Given the lack of annotated 
training samples for P-Name extraction, this paper 
introduces a bootstrapping algorithm, called 
PN-Finder. It starts from a small set of seed samples, 
and iteratively locates, extracts and classifies the 
new and more P-Names. It works in conjunction 
with a general Chinese named entity recognizer 
(Chua and Liu, 2002) to extract general named 
entities. 
In the remaining parts of this paper, we describe 
the details of PN-Finder in Section 2 and its 
application in locating P-Names from new 
documents in Section 3. Section 4 presents the 
experimental results using the MET-2 test corpus. 
Section 5 contains our conclusion and outline for 
future work. 
2 Bootstrapping Algorithm for Locating 
P-Names 
Currently, there is no standard corpus that 
annotates all P-Names. Since annotating thousands 
of P-Names is more difficult than collecting 
thousands of P-Names from the Internet, we recur 
to using the Internet search engine to collect a 
large set of P-Names. Figure 1 illustrates our main 
components in bootstrapping process. 
 
 
 
 
Figure 1: Main components of the bootstrapping 
The inputs to the PN-Finder are:  
a) A seed P-Name character set Cs(0) consisting of 
5 characters {??, ??, ??, ?	?, ?
?}. 
b) A set of seed context cue words CW(0) 
consisting of 60 context words, such as 
{?NULL?, ??, ??, ??, ??, ??}. 
These are typical context words found around 
person, location and organization names in 
PKUC1 (the PoS Corpus of Peking University), 
which contains one month of news from the 
People Daily.  
c) A set of P-Name candidates P(0), which is null at 
the beginning. 
d) A common word dictionary extracted from 
PKUC by removing proper nouns, numbers and 
non-Chinese symbols. It contains about 37,000 
words. 
                                                      
1
 http://icl.pku.edu.cn/Introduction/corpustagging.htm 
From the initial seeds, we perform the followings: 
a) We use every two characters in Cs(i-1) as query to 
retrieve relevant web pages from the Internet 
using a commercial search engine. We then 
extract possible P-Names from the returned web 
pages to update P(i). 
b) We find a most probable new P-Name character. 
Update Cs(i-1) to Cs(i) by adding the new character. 
c) We bootstrap new context words around the new 
P-Names found to derive CW(i). We then perform 
the lexical chaining to generalize these context 
words to generate semantic classes. 
d) We repeat the process from step (a) until any of 
the following conditions is satisfied: (i) when no 
new P-Name is found; (ii) when the desired 
number of iterations is reached; or (iii) when the 
number of P-Names found exceeds a desired 
number. 
The following subsections discuss the details of  
the bootstrapping process. 
2.1 Querying and Extracting the P-Names 
from the Web 
The first step of the algorithm is to derive good 
queries from the character set Cs(m-1) to search the 
Internet to obtain new web pages. If we use all single 
characters from Cs(m-1) to perform the search, we are 
likely to get too many pages containing irrelevant 
information. As a compromise, we use every two 
characters cicj in Cs(m-1) (except those combinations 
that have been used in the previous iterations) to 
search the Internet using Google (by using its 
language tool2). We consider only up to 300 entries 
returned by Google. We divide the content of the 
web pages into text segments by using the 
non-alphanumeric characters as delimiters. We 
extract those text segments that contain the search 
characters ci, cj or both and store them in R(m). For 
example, from the web page given in Figure 2, the 
text segments extracted include: strings ?
		

? and ?
		
? from the first 
entry; and strings ?Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 18?26,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Paraphrase Recognition via Dissimilarity Signicance Classication
Long Qiu, Min-Yen Kan and Tat-Seng Chua
Department of Computer Science
National University of Singapore
Singapore, 117543
{qiul,kanmy,chuats}@comp.nus.edu.sg
Abstract
We propose a supervised, two-phase
framework to address the problem of para-
phrase recognition (PR). Unlike most PR
systems that focus on sentence similarity,
our framework detects dissimilarities be-
tween sentences and makes its paraphrase
judgment based on the significance of such
dissimilarities. The ability to differenti-
ate significant dissimilarities not only re-
veals what makes two sentences a non-
paraphrase, but also helps to recall addi-
tional paraphrases that contain extra but
insignificant information. Experimental
results show that while being accurate
at discerning non-paraphrasing dissimilar-
ities, our implemented system is able to
achieve higher paraphrase recall (93%), at
an overall performance comparable to the
alternatives.
1 Introduction
The task of sentence-level paraphrase recognition
(PR) is to identify whether a set of sentences (typ-
ically, a pair) are semantically equivalent. In such
a task, ?equivalence? takes on a relaxed meaning,
allowing sentence pairs with minor semantic dif-
ferences to still be considered as paraphrases.
PR can be thought of as synonym detection ex-
tended for sentences, and it can play an equally
important role in natural language applications.
As with synonym detection, applications such as
summarization can benefit from the recognition
and canonicalization of concepts and actions that
are shared across multiple documents. Automatic
construction of large paraphrase corpora could
mine alternative ways to express the same con-
cept, aiding machine translation and natural lan-
guage generation applications.
In our work on sentence-level PR, we have iden-
tified two main issues through observation of sam-
ple sentences. The first is to identify all discrete in-
formation nuggets, or individual semantic content
units, shared by the sentences. For a pair of sen-
tences to be deemed a paraphrase, they must share
a substantial amount of these nuggets. A trivial
case is when both sentences are identical, word
for word. However, paraphrases often employ dif-
ferent words or syntactic structures to express the
same concept. Figure 1 shows two sentence pairs,
in which the first pair is a paraphrase while the
second is not. The paraphrasing pair (also denoted
Paraphrase (+pp):
Authorities said a young man injured Richard Miller.
Richard Miller was hurt by a young man.Non-Paraphrase (-pp):
The technology-laced Nasdaq Composite Index
.IXIC added 1.92 points, or 0.12 percent, at 1,647.94.
The technology-laced Nasdaq Composite Index
.IXIC dipped 0.08 of a point to 1,646.
Figure 1: Examples: Paraphrasing & Non-
paraphrasing
as the +pp class) use different words. Focusing
just on the matrix verbs, we note differences be-
tween ?injured? and ?hurt?. A paraphrase recogni-
tion system should be able to detect such semantic
similarities (despite the different syntactic struc-
tures). Otherwise, the two sentences could look
even less similar than two non-paraphrasing sen-
tences, such as the two in the second pair. Also in
the paraphrasing pair, the first sentence includes an
extra phrase ?Authorities said?. Human annotators
tend to regard the pair as a paraphrase despite the
presence of this extra information nugget.
18
This leads to the second issue: how to recognize
when such extra information is extraneous with
respect to the paraphrase judgment. Such para-
phrases are common in daily life. In news articles
describing the same event, paraphrases are widely
used, possibly with extraneous information.
We equate PR with solving these two issues,
presenting a natural two-phase architecture. In the
first phase, the nuggets shared by the sentences
are identified by a pairing process. In the second
phase, any unpaired nuggets are classified as sig-
nificant or not (leading to ?pp and +pp classifica-
tions, respectively). If the sentences do not contain
unpaired nuggets, or if all unpaired nuggets are in-
significant, then the sentences are considered para-
phrases. Experiments on the widely-used MSR
corpus (Dolan et al, 2004) show favorable results.
We first review related work in Section 2. We
then present the overall methodology and describe
the implemented system in Section 3. Sections 4
and 5 detail the algorithms for the two phases re-
spectively. This is followed with our evaluation
and discussion of the results.
2 Related Work
Possibly the simplest approach to PR is an infor-
mation retrieval (IR) based ?bag-of-words? strat-
egy. This strategy calculates a cosine similar-
ity score for the given sentence set, and if the
similarity exceeds a threshold (either empirically
determined or learned from supervised training
data), the sentences are paraphrases. PR systems
that can be broadly categorized as IR-based in-
clude (Corley and Mihalcea, 2005; Brockett and
Dolan, 2005). In the former work, the authors
defined a directional similarity formula reflect-
ing the semantic similarity of one text ?with re-
spect to? another. A word contributes to the di-
rectional similarity only when its counterpart has
been identified in the opposing sentence. The as-
sociated word similarity scores, weighted by the
word?s specificity (represented as inverted docu-
ment frequency, idf ), sum to make up the direc-
tional similarity. The mean of both directions
is the overall similarity of the pair. Brockett
and Dolan (2005) represented sentence pairs as
a feature vector, including features (among oth-
ers) for sentence length, edit distance, number of
shared words, morphologically similar word pairs,
synonym pairs (as suggested by WordNet and a
semi-automatically constructed thesaurus). A sup-
port vector machine is then trained to learn the
{+pp,?pp} classifier.
Strategies based on bags of words largely ig-
nore the semantic interactions between words.
Weeds et al (2005) addressed this problem by
utilizing parses for PR. Their system for phrasal
paraphrases equates paraphrasing as distributional
similarity of the partial sub-parses of a candidate
text. Wu (2005)?s approach relies on the genera-
tive framework of Inversion Transduction Gram-
mar (ITG) to measure how similar two sentences
arrange their words based on edit distance.
Barzilay and Lee (2003) proposed to apply
multiple-sequence alignment (MSA) for tradi-
tional, sentence-level PR. Given multiple articles
on a certain type of event, sentence clusters are
first generated. Sentences within the same clus-
ter, presumably similar in structure and content,
are then used to construct a lattice with ?back-
bone? nodes corresponding to words shared by the
majority and ?slots? corresponding to different re-
alization of arguments. If sentences from differ-
ent clusters have shared arguments, the associated
lattices are claimed to be paraphrase. Likewise,
Shinyama et al (2002) extracted paraphrases from
similar news articles, but use shared named enti-
ties as an indication of paraphrasing. It should be
noted that the latter two approaches are geared to-
wards acquiring paraphrases rather than detecting
them, and as such have the disadvantage of requir-
ing a certain level of repetition among candidates
for paraphrases to be recognized.
All past approaches invariably aim at a proper
similarity measure that accounts for all of the
words in the sentences in order to make a judg-
ment for PR. This is suitable for PR where in-
put sentences are precisely equivalent semanti-
cally. However, for many people the notion of
paraphrases also covers cases in which minor or
irrelevant information is added or omitted in can-
didate sentences, as observed in the earlier ex-
ample. Such extraneous content should not be a
barrier to PR if the main concepts are shared by
the sentences. Approaches that focus only on the
similarity of shared contents may fail when the
(human) criteria for PR include whether the un-
matched content is significant or not. Correctly
addressing this problem should increase accuracy.
In addition, if extraneous portions of sentences
can be identified, their confounding influence on
the sentence similarity judgment can be removed,
19
leading to more accurate modeling of semantic
similarity for both recognition and acquisition.
3 Methodology
As noted earlier, for a pair of sentences to be a
paraphrase, they must possess two attributes:
1. similarity: they share a substantial amount of
information nuggets;
2. dissimilarities are extraneous: if extra infor-
mation in the sentences exists, the effect of
its removal is not significant.
A key decision for our two-phase PR framework
is to choose the representation of an information
nugget. A simple approach is to use representative
words as information nuggets, as is done in the
SimFinder system (Hatzivassiloglou et al, 2001).
Instead of using words, we choose to equate in-
formation nuggets with predicate argument tuples.
A predicate argument tuple is a structured repre-
sentation of a verb predicate together with its argu-
ments. Given a sentence from the example in Fig-
ure 1, its predicate argument tuple form in Prop-
Bank (Kingsbury et al, 2002) format is:
target(predicate): hurtarg0: a young manarg1: Richard Miller
We feel that this is a better choice for the repre-
sentation of a nugget as it accounts for the action,
concepts and their relationships as a single unit.
In comparison, using fine-grained units such as
words, including nouns and verbs may result in in-
accuracy (sentences that share vocabulary may not
be paraphrases), while using coarser-grained units
may cause key differences to be missed. In the rest
of this paper, we use the term tuple for conciseness
when no ambiguity is introduced.
An overview of our paraphrase recognition sys-
tem is shown in Figure 2. A pair of sentences is
first fed to a syntactic parser (Charniak, 2000) and
then passed to a semantic role labeler (ASSERT;
(Pradhan et al, 2004)), to label predicate argu-
ment tuples. We then calculate normalized tuple
similarity scores over the tuple pairs using a met-
ric that accounts for similarities in both syntactic
structure and content of each tuple. A thesaurus
constructed from corpus statistics (Lin, 1998) is
utilized for the content similarity.
We utilize this metric to greedily pair together
the most similar predicate argument tuples across
Figure 2: System architecture
sentences. Any remaining unpaired tuples repre-
sent extra information and are passed to a dissim-
ilarity classifier to decide whether such informa-
tion is significant. The dissimilarity classifier uses
supervised machine learning to make such a deci-
sion.
4 Similarity Detection and Pairing
We illustrate this advantage of using predicate ar-
gument tuples from our running example. In Ta-
ble 1, one of the model sentences is shown in the
middle column. Two edited versions are shown on
the left and right columns. While it is clear that
the left modification is an example of a paraphrase
and the right is not, the version on the left in-
volves more changes in its syntactic structure and
vocabulary. Standard word or syntactic similar-
ity measures would assign the right modification a
higher similarity score, likely mislabeling one or
both modifications.
In contrast, semantic role labeling identifies the
dependencies between predicates and their argu-
ments, allowing a more precise measurement of
sentence similarity. Assuming that the arguments
in predicate argument tuples are assigned the same
role when their roles are comparable1 , we define
the similarity score of two tuples Ta and Tb asthe weighted sum of the pairwise similarities of
all their shared constituents C={(ca, cb)} (c beingeither the target or one of the arguments that both
1ASSERT, which is trained on the Propbank, only guaran-
tees consistency of arg0 and arg1 slots, but we have found in
practice that aligning arg2 and above arguments do not cause
problems.
20
Modification 1: paraphrase Model Sentence Modification 2: non-paraphrase
Sentence Richard Miller was hurt by a
young man.
Authorities said a young man in-
jured Richard Miller.
Authorities said Richard Miller injured
a young man.
(Paired)
Tuples
target: saidarg0: Authoritiesarg1: a young man injured
Richard Miller
target: saidarg0: Authoritiesarg1: Richard Miller injured a
young mantarget: hurtarg0: a young manarg1: Richard Miller
target: injuredarg0: a young manarg1: Richard Miller
target: injuredarg0: Richard Millerarg1: a young man
Table 1: Similarity Detection: pairing of predicate argument tuples
tuples have):
Sim(Ta, Tb) =
1
?
X
Sim(ca, cb)?
c={target,argshared}
wc==targettarget (1)
where normalization factor ? is the sum of the
weights of constituents in C , i.e.:
? = ?{argshared}? + wtarget (2)
In our current implementation we reduce tar-
gets and their arguments to their syntactic head-
words. These headwords are then directly com-
pared using a corpus-based similarity thesaurus.
As we hypothesized that targets are more impor-
tant for predicate argument tuple similarity, we
multiply the target?s similarity by a weighting fac-
tor wtarget , whose value we have empirically de-termined as 1.7, based on a 300-pair development
set from the MSR training set.
We then proceed to pair tuples in the two sen-
tences using a greedy iterative algorithm. The al-
gorithm locates the two most similar tuples from
each sentence, pairs them together and removes
them from futher consideration. The process stops
when subsequent best pairings are below the simi-
larity threshold or when all possible tuples are ex-
hausted. If unpaired tuples still exist in a given
sentence pair, we further examine the copular con-
structions and noun phrases in the opposing sen-
tence for possible pairings2 . This results in a one-
2Copular constructions are not handled by ASSERT. Such
constructions account for a large proportion of the semantic
meaning in sentences. Consider the pair ?Microsoft rose 50
cents? and ?Microsoft was up 50 cents?, in which the second
is in copular form. Similarly, NPs can often be equivalent
to predicate argument tuples when actions are nominalized.
Consider an NP that reads ?(be blamed for) frequent attacks
on soldiers? and a predicate argument tuple: ?(be blamed for)
attacking soldiers?. Again, identical information is conveyed
but not captured by semantic role labeling. In such cases,
they can be paired if we allow a candidate tuple to pair with
the predicative argument (e.g., 50 cents) of a copula, or (the
head of) an NP in the opposing sentence. As these heuristic
matches may introduce errors, we resort to these methods of
matching tuple only in the contingency when there are un-
paired tuples.
to-one mapping with possibly some tuples left un-
paired. The curved arrows in Table 1 denote the
correct results of similarity pairing: two tuples are
paired up if their target and shared arguments are
identical or similar respectively, otherwise they re-
main unpaired even if the ?bag of words? they con-
tain are the same.
5 Dissimilarity Significance
Classification
If some tuples remain unpaired, they are dissimilar
parts of the sentence that need to be labeled by the
dissimilarity classifier. Such unpaired informa-
tion could be extraneous or they could be semanti-
cally important, creating a barrier for paraphrase.
We frame this as a supervised machine learning
problem in which a set of features are used to
inform the classifier. A support vector machine,
SVMLight, was chosen as the learning model as it
has shown to yield good performance over a wide
application range. We experimented with a wide
set of features of unpaired tuples, including inter-
nal counts of numeric expressions, named entities,
words, semantic roles, whether they are similar
to other tuples in the same sentence, and contex-
tual features like source/target sentence length and
paired tuple count. Currently, only two features
are correlated in improved classification, which
we detail now.
Syntactic Parse Tree Path: This is a series of
features that reflect how the unpaired tuple con-
nects with the context: the rest of the sentence.
It models the syntactic connection between the
constituents on both ends of the path (Gildea and
Palmer, 2002; Pradhan et al, 2004). Here, we
model the ends of the path as the unpaired tuple
and the paired tuple with the closest shared ances-
tor, and model the path itself as a sequence of con-
stituent category tags and directions to reach the
destination (the paired target) from the source (the
21
unpaired target) via the the shared ancestor. When
no tuples have been paired in the sentence pair,
the destination defaults to the root of the syntactic
parse tree. For example, the tuples with target ?in-
jured? are unpaired when the model sentence and
the non-paraphrasing modification in Table 1 are
being compared. A path ??V BD, ?V P , ?S , ?SBAR
, ?V P , ?V BD? links a target ?injured? to the pairedtarget ?said?, as shown in Figure 3.
VP`````     VBD
said
SBAR
SXXXXNPaa!!NNP
Richard
NNP
Miller
VP
bb""VBD
injured
NP
Figure 3: Syntactic parse tree path
The syntactic path can act as partial evidence
in significance classification. In the above exam-
ple, the category tag ?V BD? assigned to ?injured?indicates that the verb is in its past tense. Such
a predicate argument tuple bears the main con-
tent of the sentence and generally can not be ig-
nored if its meaning is missing in the opposing
sentence. Another example is shown in Figure
4. The second sentence has one unpaired target
?proposed? while the rest all find their counter-
part. The path we get from the syntactic parse tree
reads ??V BN , ?NP , ?S , ...?, showing that the un-paired tuple (consisting of a single predicate) is a
modifier contained in an NP. It can be ignored if
there is no contradiction in the opposing sentence.
We represent a syntactic path by a set of n-gram
(n ? 4) features of subsequences of category tags
found in the path, along with the respective direc-
tion. We require these n-gram features to be no
more than four category tags away from the un-
paired target, as our primary concern is to model
what role the target plays in its sentence.
Sheena Young of Child, the national infertility sup-
port network, hoped the guidelines would lead to a more
?fair and equitable? service for infertility sufferers.
Sheena Young, a spokesman for Child, the national
infertility support network, said the proposed guide-
lines should lead to a more ?fair and equitable? service
for infertility sufferers.
Figure 4: Unpaired predicate argument tuple as
modifier in a paraphrase
Predicate: This is the lexical token of predi-
cate argument tuple?s target, as a text feature. As
this feature is liable to run into sparse data prob-
lems, the semantic category of the target would be
a more suitable feature. However, verb similar-
ity is generally regarded as difficult to measure,
both in terms of semantic relatedness as well as
in finding a consistent granularity for verb cate-
gories. We investigated using WordNet as well as
Levin?s classification (Levin, 1993) as additional
features on our validation data, but currently find
that using the lexical form of the target works best.
5.1 Classifier Training Set Acquisition
Currently, no training corpus for predicate argu-
ment tuple significance exists. Such a corpus is in-
dispensable for training the classifier. Rather than
manually annotating training instances, we use
an automatic method to construct instances from
paraphrase corpora. This is possible as the para-
phrase judgments in the corpora can imply which
portion of the sentence(s) are significant barriers
to paraphrasing or not. Here, we exploit the simi-
larity detector implemented for the first phase for
this purpose. If unpaired tuples exist after greedy
pairing, we classify them along two dimensions:
whether the sentence pair is a (non-)paraphrase,
and the source of the unpaired tuples:
1. [PS] paraphrasing pairs and unpaired predicate argu-
ment tuples are only from a single sentence;
2. [NS] non-paraphrasing pairs and only one single un-
paired predicate argument tuple exists;
3. [PM] paraphrasing pairs and unpaired predicate argu-
ment tuples are from multiple (both) sentences;
4. [NM] non-paraphrasing pairs and multiple unpaired
predicate argument tuples (from either one or both sen-
tences) exist.
Assuming that similarity detector pairs tuples
correctly, for the first two categories, the para-
phrasing judgment is directly linked to the un-
paired tuples. PS tuple instances are therefore
used as insignificant class instances, and NS as
significant ones. The last two categories can-
not be used for training data, as it is unclear which
of the unpaired tuples is responsible for the (non-)
paraphrasing as the similarity measure may mis-
takenly leave some similar predicate argument tu-
ples unpaired.
6 Evaluation
The goal of our evaluation is to show that our sys-
tem can reliably determine the cause(s) of non-
22
MSR Corpus Label +pp -pp
system prediction correct? T F T F total
# sentence pairs (s-ps) 85 23 55 37 200
# labelings (H&C agree) 80 19 53 35 187
# tuple pairs (t-ps) (S) 80 6 36 35 157
# correct t-ps (H&S agree) 74 6 34 30 144
# missed t-ps (H) 11 10 5 5 31
# sig. unpaired tuples(H) 6 4 69 51 130
# sig. unpaired tuples(S) 0 32 70 0 102
# sig. unpaired tuples(H&S) 0 4 43 0 47
# -pp for other reasons 0 0 5 2 7
Table 2: (H)uman annotations vs. (C)orpus anno-
tations and (S)ystem output
paraphrase examples, while maintaining the per-
formance level of the state-of-the-art PR systems.
For evaluation, we conduct both component
evaluations as well as a holistic one, resulting in
three separate experiments. In evaluating the first
tuple pairing component, we aim for high preci-
sion, so that sentences that have all tuples paired
can be safely assumed to be paraphrases. In evalu-
ating the dissimilarity classifier, we simply aim for
high accuracy. In our overall system evaluation,
we compare our system versus other PR systems
on standard corpora.
Experimental Data Set. For these experi-
ments, we utilized two widely-used corpora for
paraphrasing evaluation: the MSR and PASCAL
RTE corpora. The Microsoft Research Paraphrase
coupus (Dolan et al, 2004) consists of 5801
newswire sentence pairs, 3900 of which are an-
notated as semantically equivalent by human an-
notators. It reflects ordinary paraphrases that peo-
ple often encounter in news articles, and may be
viewed as a typical domain-general paraphrase
recognition task that downstream NLP systems
will need to deal with. The corpus comes divided
into standard training (70%) and testing (30%) di-
visions, a partition we follow in our experiments.
ASSERT (the semantic role labeler) shows for this
corpus a sentence contains 2.24 predicate argu-
ment tuples on average. The second corpus is
the paraphrase acquisition subset of the PASCAL
Recognizing Textual Entailment (RTE) Challenge
corpus (Dagan et al, 2005). This is much smaller,
consisting of 50 pairs, which we employ for test-
ing only to show portability.
To assess the component performance, we need
additional ground truth beyond the {+pp, ?pp}
labels provided by the corpora. For the first eval-
uation, we need to ascertain whether a sentence
pair?s tuples are correctly paired, misidentified or
mispaired. For the second, which tuple(s) (if any)
are responsible for a ?pp instance. However, cre-
ating ground truth by manual annotation is expen-
sive, and thus we only sampled the data set to get
an indicative assessment of performance. We sam-
pled 200 random instances from the total MSR
testing set, and first processed them through our
framework. Then, five human annotators (two au-
thors and three volunteers) annotated the ground
truth for tuple pairing and the semantic signifi-
cance of the unpaired tuples, while checking sys-
tem output. They also independently came up with
their own {+pp,-pp} judgment so we could assess
the reliability of the provided annotations.
The results of this annotation is shown in Ta-
ble 2. Examining this data, we can see that the
similarity detector performs well, despite its sim-
plicity and assumption of a one-to-one mapping.
Out of the 157 predicate argument tuple pairs
identified through similarity detection, annotators
agreed that 144 (92%) are semantically similar or
equivalent. However, 31 similar pairs were missed
by the system, resulting in 82% recall. We defer
discussion on the other details of this table to Sec-
tion 7.
To assess the dissimilarity classifier, we focus
on the ?pp subset of 55 instances recognized by
the system. For 43 unpaired tuples from 40 sen-
tence pairs (73% of 55), the annotators? judgments
agree with the classifier?s claim that they are sig-
nificant. For these cases, the system is able to both
recognize that the sentence pair is not a paraphrase
and further correctly establish a cause of the non-
paraphrase.
In addition to this ground truth sampled evalu-
ation, we also show the effectiveness of the clas-
sifier by examining its performance on PS and NS
tuples in the MSR corpus as described in Section
5. The test set consists of 413 randomly selected
PS and NS instances among which 145 are signif-
icant (leading to non-paraphrases). The classifier
predicts predicate argument tuple significance at
an accuracy of 71%, outperforms a majority clas-
sifier (65%), a gain which is marginally statisti-
cally significant (p < .09).
significant insignificant
112 263 insignificant by classifier
33 5 significant by classifier
We can see the classifier classifies the majority
of insignificant tuples correctly (by outputting a
23
709 Sentence Pairs Without 1016 Sentence Pairs With
Unpaired Tuples Unpaired Tuples Overall
Algorithm (41.1% of Test set) (58.9% of Test set) (100% of Test set)
Acc R P Acc R P Acc R P F1
Majority Classifier 79.5% 100% 79.5% 57.4% 100% 57.4% 66.5% 100% 66.5% 79.9%
SimFinder 82.2% 92.2% 86.4% 66.3% 84.9% 66.1% 72.9% 88.5% 75.1% 81.3%
CM05 - - - - - - 71.5% 92.5% 72.3% 81.2%
Our System 79.5% 100% 79.5% 66.7% 87.0% 66.0% 72.0% 93.4% 72.5% 81.6%
Table 3: Results on MSR test set
17 Sentence Pairs Without 33 Sentence Pairs With
Algorithm Unpaired Tuples Unpaired Tuples Overall
(34% of Test set) (66% of Test set) (100% of Test set)
Acc R P Acc R P Acc R P F1
Majority Classifier 65% 100% 65% 42% 100% 42% 50% 100% 50% 67%
SimFinder 71% 91% 71% 42% 21% 27% 52% 52% 52% 52%
Our System 65% 100% 65% 48% 64% 43% 54% 80% 53% 64%
Table 4: Results on PASCAL PP test set
score greater than zero), which is effectively a
98% recall of insignificant tuples. However, the
precision is less satisfatory. We suspect this is par-
tially due the tuples that fail to be paired up with
their counterpart. Such noise is found among the
automatically collected PS instances used in train-
ing.
0
20
40
60
80
100
120
140
160
180
> 
-
.
5
-
0.5
 
-
 
-
0.2
5
-
.
25
 
-
 
0
0 -
 
.
25
.
25
 
-
 
.
5
.
5 -
 
.
75
.
75
 
-
 
1
< 
1
SVM Prediction
Fr
eq
u
en
cy
Insignificant
Significant
Figure 5: Dissimilarity classifier performance
For the final system-wide evaluation, we imple-
mented two baseline systems: a majority classifier
and SimFinder (Hatzivassiloglou et al, 2001), a
bag-of-words sentence similarity module incorpo-
rating lexical, syntactic and semantic features. In
Table 3, precision and recall are measured with re-
spect to the paraphrasing class. The table shows
sentence pairs falling under the column ?pairs
without unpaired tuples? are more likely to be
paraphrasing than an arbitrary pair (79.5% ver-
sus 66.5%), providing further validation for using
predicate argument tuples as information nuggets.
The results for the experiment benchmarking the
overall system performance are shown under the
?Overall? column: our approach performs compa-
rably to the baselines at both accuracy and para-
phrase recall. The system performance reported in
(CM05; (Corley and Mihalcea, 2005)), which is
among the best we are aware of, is also included
for comparison.
We also ran our system (trained on the MSR
corpus) on the 50 instances in the PASCAL para-
phrase acquisition subset. Again, the system per-
formance (as shown in Table 4) is comparable to
the baseline systems.
7 Discussion
We have just shown that when two sentences have
perfectly matched predicate argument tuples, they
are more likely to be a paraphrase than a random
sentence pair drawn from the corpus. Further-
more, in the sampled human evaluation in Table
2, among the 88 non-paraphrasing instances with
whose MSR corpus labels our annotators agreed
(53 correctly and 35 incorrectly judged by our sys-
tem), the cause of the ?pp is correctly attributed
in 81 cases to one or more predicate argument tu-
ples. The remaining 7 cases (as shown in the last
row) are caused by phenomenon that are not cap-
tured by our tuple representation. We feel this jus-
tifies using predicate argument tuples as informa-
tion nuggets, but we are currently considering ex-
panding our representation to account for some of
these cases.
The evaluation also confirms the difficulty of
making paraphrase judgements. Although the
24
MSR corpus used strict means of resolving inter-
rater disagreements during its construction, the an-
notators agreed with the MSR corpus labels only
93.5% (187/200) of the time.
One weakness of our system is that we rely on a
thesaurus (Lin, 1998) for word similarity informa-
tion for predicate argument tuple pairing. How-
ever, it is designed to provide similarity scores
between pairs of individual words (rather than
phrases). If a predicate argument tuple?s target or
one argument is realized as a phrase (borrow ?
check out, for instance), the thesaurus is unable to
provide an accurate similarity score. For similarity
between predicate argument tuples, negation and
modality have yet to be addressed, although they
account for only a tiny fraction of the corpus.
We further estimated how the similarity detec-
tor can be affected when the semantic role labeler
makes mistakes (by failing to identify arguments
or assigning incorrect role names). Checking 94
pairs ground-truth similar tuples, we found that the
system mislabels 43 of them. The similarity detec-
tor failed to pair around 30% of them. In compar-
sion, all the tuple pairs free of labeling errors are
correctly paired. A saving grace is that labeling
errors rarely lead to incorrect pairing (one pairing
in all the examined sentences). The labeling er-
rors impact the whole system in two ways: 1) they
caused similar tuples that should have been paired
up to be added as noise in that dissimilarity clas-
sifier?s training set and 2) paired tuples with label-
ing errors erroneously increase the target weight
in Equation (1).
Some example paraphrasing cases that are prob-
lematic for our current system are:
1. Non-literal language issues such as implica-
ture, idiom, metaphor, etc. are not addressed in
our current system. When predicate argument tu-
ples imply each other, they are less similar than
what our system currently is trained for, causing
the pairing to fail:
+pp, Later in the day, a standoff developed between French
soldiers and a Hema battlewagon that attempted to pass the
UN compound.
French soldiers later threatened to open fire on a Hema bat-
tlewagon that tried to pass near the UN compound.
2. A paraphrasing pair may exceed the systems?
threshold for syntactic difference:
+pp, With the exception of dancing, physical activity did not
decrease the risk.
Dancing was the only physical activity associated with a
lower risk of dementia.
3. One or more unpaired tuples exist, but their
significance is not inferred correctly:
+pp, Inhibited children tend to be timid with new people,
objects, and situations, while uninhibited children sponta-
neously approach them.
Simply put, shy individuals tend to be more timid with new
people and situations.
In the MSR corpus, the first error case is more
frequent than the other two. We identify these as
challenging cases where idiomatic processing is
needed.
Below we show some unpaired predicate ar-
gument tuples (underlined) that are significant
enough to be paraphrase barriers. These examples
give an indicative categorization of what signifi-
cant tuples are and their corpus frequency (when
predicate argument tuples are the reasons; we ex-
amined 40 such cases for this estimation). There
is one thing in common: every case involves sub-
stantial information that is difficult to infer from
context. Such tuples appear as:
1. (40%) The nucleus of the sentence (often the
matrix tuple):
Michael Hill, a Sun reporter who is a member of the
Washington-Baltimore Newspaper Guild?s bargaining com-
mittee, estimated meetings to last late Sunday.
2. (30%) A part of a coordination:
Security lights have also been installed and police have
swept the grounds for booby traps.
3. (13%) A predicate of a modifying clause:
Westermayer was 26 then, and a friend and former manager
who knew she was unhappy in her job tipped her to another
position.
4. (7%) An adjunct:
While waiting for a bomb squad to arrive, the bomb exploded,
killing Wells.
5. (7%) An embedded sentence:
Dean told reporters traveling on his 10-city ?Sleepless
Summer? tour that he considered campaigning in Texas a
challenge.
6. (3%) Or factual content that conflicts with
the opposing sentence:
Total sales for the period declined 8.0 percent to USD1.99
billion from a year earlier.
Wal-Mart said sales at stores open at least a year rose 4.6
percent from a year earlier.
8 Conclusions
We have proposed a new approach to the para-
phrase recognition (PR) problem: a supervised,
25
two-phase framework emphasizing dissimilarity
classification. To emulate human PR judgment
in which insignificant, extraneous information
nuggets are generally allowed for a paraphrase,
we estimate whether such additional information
nuggets affect the final paraphrasing status of a
sentence pair. This approach, unlike previous PR
approaches, has the key benefit of explaining the
cause of a non-paraphrase sentence pair.
In the first, similarity detection module, using
predicate argument tuples as the unit for compar-
ison, we pair them up in a greedy manner. Un-
paired tuples thus represent additional information
unrepresented in the opposing sentence. A second,
dissimilarity classification module uses the lexical
head of the predicates and the tuples? path of at-
tachment as features to decide whether such tuples
are barriers to paraphrase.
Our evaluations show that the system obtains 1)
high accuracy for the similarity detector in pairing
predicate argument tuples, 2) robust dissimilar-
ity classification despite noisy training instances
and 3) comparable overall performance to current
state-of-the-art PR systems. To our knowledge this
is the first work that tackles the problem of identi-
fying what factors stop a sentence pair from being
a paraphrase.
We also presented corpus examples that illus-
trate the categories of errors that our framework
makes, suggesting future work in PR. While we
continue to explore more suitable representation
of unpaired predicate argument tuples, we plan to
augment the similarity measure for phrasal units
to reduce the error rate in the first component. An-
other direction is to detect semantic redundancy in
a sentence. Unpaired tuples that are semantically
redundant should also be regarded as insignificant.
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL 2003.
Chris Brockett and Bill Dolan. 2005. Support vector ma-
chines for paraphrase identification and corpus construc-
tion. In Proceedings of the 3rd International Workshop onParaphrasing.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Annual Meeting of theNorth American Chapter of the Association for Computa-tional Linguistics (NAACL?2000).
Courtney Corley and Rada Mihalcea. 2005. Measuring the
semantic similarity of texts. In Proceedings of the ACLWorkshop on Empirical Modeling of Semantic Equiva-lence and Entailment, pages 13?18, Ann Arbor, USA.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. InPASCAL Proceedings of the First Challenge Workshop?Recognizing Textual Entailment, Southampton,UK.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Exploiting
massively parallel news sources. In Proceedings of the20th International Conference on Computational Linguis-tics, Geneva, Switzerland.
Daniel Gildea and Martha Palmer. 2002. The necessity of
parsing for predicate argument recognition. In Proceed-ings of the 40th Annual Meeting of the Association forComputational Linguistics, Philadelphia, USA.
Vassileios Hatzivassiloglou, Judith Klavans, Melissa Hol-
combe, Regina Barzilay, Min-Yen Kan, and Kathleen
McKeown. 2001. Simfinder: A flexible clustering tool
for summarization. In Proceedings of the NAACL Work-shop on Automatic Summarization, pages 41?49.
Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the penn treebank. In Pro-ceedings of the Human Language Technology Conference,
San Diego, USA.
Beth Levin. 1993. English verb classes and alternations: A
preliminary investigation. University of Chicago Press.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL ?98, pages
768?774, Montreal, Canada.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Dan Jurafsky. 2004. Shallow semantic pars-
ing using support vector machines. In Proceedings ofHLT/NAACL, Boston, USA.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and Ralph
Grishman. 2002. Automatic paraphrase acquisition from
news articles. In Proceedings of the Human LanguageTechnology Conference, pages 40?46, San Diego, USA.
Julie Weeds, David Weir, and Bill Keller. 2005. The dis-
tributional similarity of sub-parses. In Proceedings of theACL Workshop on Empirical Modeling of Semantic Equiv-alence and Entailment, pages 7?12, Ann Arbor, USA.
Dekai Wu. 2005. Recognizing paraphrases and textual en-
tailment using inversion transduction grammars. In Pro-ceedings of the ACL Workshop on Empirical Modeling ofSemantic Equivalence and Entailment, Ann Arbor, USA.
26
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1155?1163,
Beijing, August 2010
Exploiting Salient Patterns for Question Detection and Question
Retrieval in Community-based Question Answering
Kai Wang
Department of Computer Science
School of Computing
National University of Singapore
kwang@comp.nus.edu.sg
Tat-Seng Chua
Department of Computer Science
School of Computing
National University of Singapore
chuats@comp.nus.edu.sg
Abstract
Question detection serves great purposes
in the cQA question retrieval task. While
detecting questions in standard language
data corpus is relatively easy, it becomes
a great challenge for online content. On-
line questions are usually long and infor-
mal, and standard features such as ques-
tion mark or 5W1H words are likely to be
absent. In this paper, we explore ques-
tion characteristics in cQA services, and
propose an automated approach to detect
question sentences based on lexical and
syntactic features. Our model is capable
of handling informal online languages.
The empirical evaluation results further
demonstrate that our model significantly
outperforms traditional methods in de-
tecting online question sentences, and it
considerably boosts the question retrieval
performance in cQA.
1 Introduction
Community-based Question Answering services
(cQA) such as Yahoo! Answers have emerged
as popular means of information exchange on the
web. They not only connect a network of people
to freely ask and answer questions, but also allow
information seekers to search for relevant histori-
cal questions in the cQA archive (Agichtein et al,
2008; Xue et al, 2008; Wang et al, 2009).
Many research works have been proposed to
find similar questions in cQA. The state-of-the-art
retrieval models include the vector space model
(Duan et al, 2008), language model (Duan et al,
2008; Jeon et al, 2005), Okapi model (Jeon et al,
2005), translation model (Jeon et al, 2005; Rie-
zler et al, 2007; Xue et al, 2008), and syntac-
tic tree matching model(Wang et al, 2009). Al-
though experimental studies in these works show
that the proposed models are capable of improv-
ing question retrieval, they did not give clear ex-
planation on which portion of the question that
the user query is actually matched against. A
question thread from cQA usually comprises sev-
eral sub-questions conveying different informa-
tion needs, and it is highly desirable to identify
individual sub-questions and match each of them
to the user query. Getting sub-questions clearly
identified not only helps the retrieval system to
match user query to the most desirable content but
also improves the retrieval efficiency.
However, the detection of sub-question is non-
trivial. Question sentences in cQA are usually
mixed with various description sentences, and
they usually employ informal languages, where
standard features such as question mark or ut-
terance are likely to be absent. As such, simple
heuristics using question mark or 5W1H words
(who, what, where, why, how) may become in-
adequate. The demand of special techniques in
detecting question sentences online arises due to
three particular reasons. First, the question mark
could be missing at the end of a question1, or
might be used in cases other than questions such
as ?Really bad toothache??. Second, some ques-
tions such as ?I?d like to know the expense of re-
moving wisdom teeth? are expressed in a declar-
ative form, which neither contains 5W1H words
nor is neccessarily ended with ???. Third, some
question-like sentences do not carry any actual in-
formation need, such as ?Please help me??. Fig-
ure 1 illustrates an example of a question thread
1It is reported (Cong et al, 2008) that 30% of online
questions do not end with question marks.
1155
S1: What do you guys do when you find that the 'plastic 
protection seal' is missing or disturbed. 
S2: Throw it out, buy a new one.. or just use it anyways?
S3: Is it really possible or likely that the item you purchased was 
tampered with??
S4: The box was in a plastic wrap but the item itself inside did 
not having the protection seal (box says it should) so I 
couldn't have inspected it before I bought it.
S5: Please suggest?? thanks!
Figure 1: An example of a question thread ex-
tracted from Yahoo! Answers
from Yahoo! Answers, where sub-questions S1
and S2 are posted in non-standard forms, and S5
is merely a question-like simple sentence. To the
best of our knowledge, none of the existing ques-
tion retrieval systems are equipped with a com-
prehensive question detector module to handle
various question forms online, and limited effort
has been devoted to this direction.
In this paper, we extensively explore character-
istics of questions in cQA, and propose a fully
automated approach to detecting question sen-
tences. In particular, we complement lexical pat-
terns with syntactic patterns, and use them as fea-
tures to train a classification model that is capable
of handling informal online languages. To save
human annotations, we further propose to employ
one-class SVM algorithm for model learning, in
which only positive examples are used as opposed
to requiring both positive and negative examples.
The rest of the paper is organized as follows:
Section 2 presents the lexical and syntactic pat-
terns as used for question detection. Section 3
describes the learning algorithm for the classifi-
cation model. Section 4 shows our experimental
results. Section 5 reviews some related work and
Section 6 concludes this paper.
2 Pattern Mining for Question Detection
As has been discussed, human generated content
on the Web are usually not well formatted, and
naive methods such as the use of question mark
and 5W1H words are not adequate to correctly
detect or capture all online questions. Methods
based on hand-crafted rules also fail to cope with
various question forms as randomly appeared on
the Web. To overcome the shortcomings of these
traditional methods, we propose to extract a set
of salient patterns from online questions and use
them as features to detect question sentences.
In this study, we mainly focus on two kinds
of patterns ? sequential pattern at the lexical
level and syntactic shallow pattern at the syntac-
tic level. Sequential patterns have been well dis-
cussed in many literature, including the identifi-
cation of comparative sentences (Jindal and Liu,
2006), the detection of erroneous sentences (Sun
et al, 2007) and question sentences (Cong et al,
2008) etc. However, works on syntactic patterns
have only been partially explored (Zaki and Ag-
garwal, 2003; Sun et al, 2007; Wang et al, 2009).
Grounded on these previous works, we next ex-
plain our mining approach of the sequential and
syntactic shallow patterns.
2.1 Sequential Pattern Mining
Sequential Pattern is also referred to as Labeled
Sequential Pattern (LSP) in the literature. It is
in the form of S?C , where S is a sequence
{t1, . . . , tn}, and C is the class label that the se-
quence S is classified to. In the problem of ques-
tion detection, a sequence is defined to be a se-
ries of tokens from questions, and the class labels
are {Q,NQ}, which stand for question and non-
question respectively.
The purpose of sequential pattern mining is to
extract a set of frequent subsequence of words
that are indicative of questions. For example,
the word subsequence ?anyone know what . . .
to? could be a good indication to characterize the
question sentence ?anyone know what I can do to
make me less tired.?. Note that the mined sequen-
tial tokens need not to be contiguous as appeared
in the original text.
There is a handful of algorithms available for
frequent subsequence extraction. Pei et al (2001)
observed that all occurrences of a frequent pattern
can be classified into groups (approximated pat-
tern) and proposed a Prefixspan algorithm. The
Prefixspan algorithm quickly finds out all rela-
tive frequent subsequences by a pattern growth
method, and determines the approximated pat-
terns from those subsequences. We adopt this al-
gorithm in our work due to its high reported effi-
ciency. We impose the following additional con-
straints for better control over the significance of
the mined patterns:
1156
1. Maximum Pattern Length: It limits the maxi-
mum number of tokens in a mined sequence.
2. Maximum Token Distance: The two adjacent
tokens tn and tn+1 in the pattern need to be
within a threshold window in the original text.
3. Minimum Support: The minimum percentage
of sentences in Q containing the pattern p.
4. Minimum Confidence: The probability of a
pattern p?Q being true in the whole database.
To overcome the word sparseness problem, we
generalize each sentence by applying the Part-of-
Speech (POS) tags to all tokens except some in-
dicative keywords such as 5W1H words, modal
words, stopwords etc. For instance, the question
sentence ?How can I quickly tell if my wisdom
teeth are coming? is converted to ?How can I RB
VBP if my NN NNS VBP VBG?, on top of which
the pattern mining is conducted. To further cap-
ture online language patterns, we mine a set of
frequent tokens that are unique to cQA such as
?any1?, ?im? and ?whats?, and keep them from
being generalized. The reason to hold back this
set of tokens is twofold. First, conventional POS
taggers are trained from standard English corpus,
and they could mis-tag these non-standard words.
Second, the special online tokens are analogue to
standard stopwords, and having them properly ex-
cluded could help reflect the online users? textual
questioning patterns.
It is expected that the converted patterns pre-
serve the most representative features of online
questions. Each discovered pattern makes up a
binary feature for the classification model that we
will introduce in Section 3.
2.2 Syntactic Shallow Pattern Mining
The sequential patterns represent features at the
lexical level, but we found that lexical patterns
might not always be adequate to categorize ques-
tions. For example, the pattern {when, do} could
presume the non-question ?Levator scapulae is
used when you do the traps workout? to be a ques-
tion, whereas the question ?know someone with
an eating disorder?? could be overlooked due to
the lack of indicative lexical patterns.
These limitations, however, could be allevi-
ated by syntactic features. The syntactic pattern
(SBAR(WHADVP(WRB))(S(NP)(VP))) extracted
S
NP VP
NN VBP NP
Anyone try
NP
weight
NNS
watchers
S
NP VP
NN VBP NP
Someone need
DT
a
NNP
diet
NN
motivator?
.
?
.
Anyone try weight watchers? Someone need a diet motivator?
Figure 2: An example of common syntactic pat-
terns observed in two different question sentences
from the former example has the order of NP
and VP being switched, which could indicate
the sentence to be a non-question, whereas the
pattern (VP(VB)(NP(NP)(PP))) may be evidence
that the latter example is indeed a question,
because this pattern is commonly witnessed in
the archived questions. Figure 2 shows an ex-
ample that two questions bear very different
wordings but share the same questioning pat-
tern (S(NP(NN))(VP(VPB)(NP))) at the syntactic
level. In view of the above, we argue that pat-
terns at the syntactic level could complement lex-
ical patterns in identifying question sentences.
To our knowledge, the mining of salient pat-
terns at the syntactic level was limited to a few
tasks. Zaki and Aggarwal (2003) employed tree
patterns to classify XML data, Sun et al (2007)
extracted all frequent sub-tree structures for erro-
neous sentences detection, and Wang et al (2009)
decomposed the parsing tree into fragments and
used them to match similar questions. Our work
differs from these previous works in that: (1) we
also utilize syntactic patterns for the question de-
tection; and (2) we do not blindly extract all pos-
sible sub-tree structures, but focus only on certain
portions of the parsing tree for better pattern rep-
resentation and extraction efficiency.
Given a syntactic tree T , we define syntac-
tic pattern as a part of sub-structures of T such
that the production rule for each non-leaf node in
the patterns is intact. For example, the pattern
(S(NP(NN))(VP(VPB)(NP))) in Figure 2 is con-
sidered to be a valid syntactic pattern, whereas
(S(NP(NN))(VP(VPB))) is not, since the produc-
tion rule VP?VPB?NP is not strictly complied.
We take the following measures to mine salient
syntactic patterns: First, we limit the depth of
each syntactic pattern to be within a certain range.
1157
SBARQ
WHADVP SQ
WRB MD NP ADVP VP
Q: How can I quickly tell if my wisdom teeth are coming?
SQ
MD NP ADVP VP
RBPRP SBARVB
Generalization
Decomposition
?
?
?
VP
VB SBAR
S
SBAR
IN S
VPNP
SBARQ
WHADVP SQ
WRB MD NP VP
SQ
MD NP VP
PRP SBARVB
?
?
?
VP
VB SBAR
S
SBAR
IN S
VPNP
(a) (b) (c) (d)
(a?) (b?) (c?) (d?)
?
?
Figure 3: Illustration of syntactic pattern extrac-
tion and generalization process
It is believed that the syntax structure will become
too specific if it is extended to a deeper level or
too general if the depth is too shallow, neither of
which produces good representative patterns. We
therefore set the depth D of each syntactic pattern
to be within a reasonable range (2?D?4). Sec-
ond, we prune away all leaf nodes as well as the
production rules at the POS tag level. We believe
that nodes at the bottom levels do not carry much
useful structural information favored by question
detector. For example, the simple grammar rule
NP?DT?NN does not give any insight to use-
ful question structures. Third, we relax the def-
inition of syntactic pattern by allowing the re-
moval of some nodes denoting modifiers, prepo-
sition phrases, conjunctions etc. The reason is
that these nodes are not essential in representing
the syntactic patterns and are better excluded for
generalization purpose. Figure 3 gives an illus-
tration of the process for pattern extraction and
generalization. In this example, several syntac-
tic patterns are generated from the question sen-
tence ?How can I quickly tell if my wisdom teeth
are coming??, and the tree patterns (a) and (b) are
generalized into (a?) and (b?), in which the redun-
dant branch (ADVP(RB)) that represents the ad-
verb ?quickly? is detached.
Contents on the Web are prone to noise, and
most off-the-shelf parsers are not well-trained to
parse online questions. For example, the parsing
tree of the question ?whats the matter with it??
will be very different from that of the question
?what is the matter with it??. It would certainly
be nice to know that ?whats? is a widely used
short form of the phrase ?what is? on the Web,
but we are lack of this kind of thesaurus. Nev-
ertheless, we argue that the parsing errors would
not hurt the question detector performance much
as long as the mining database is large enough.
The reason is that if certain irregular forms fre-
quently occur on the Web, there will be statisti-
cal evidences that the syntactic patterns derived
from it, though not desired, will commonly occur
as well. In other words, we take the wrong pat-
terns and utilize them to detect questions in the
irregular forms. Our approach differs from other
systems in that we do not intentionally try to rec-
tify the grammatical errors, but leave the errors as
they are and use the statistical based approach to
capture those informal patterns.
The pattern extraction process is outlined in Al-
gorithm 1. The overall mining strategy is analo-
gous to the mining of sequential patterns, where
support and confidence measures are taken into
account to control the significance of the mined
patterns. All mined syntactic patterns together
with the lexical patterns will be used as features
for learning the classification model.
Algorithm 1 ExtractPattern(S, D)
Input: A set of syntactic trees for sentences (S); the depth
range (D)
Output: A set of sub-tree patterns extracted from S
1: Patterns = {}
2: for all Syntactic tree T ? S do
3: Nodes ? Top-down level order traversal of T
4: for all node n ? Nodes do
5: Extract subtree p rooted under node n, with depth
within the range D
6: p ? generalize(p)
7: Patterns.add(p)
8: end for
9: end for
10: return Patterns
3 Learning the Classification Model
Although Conditional Random Fields (CRF) is
good sequential learning algorithm and has been
used in other related work (Cong et al, 2008),
here we select Support Vector Machines (SVM)
as an alternative learner. The reason is that our
task not only deals with sequential patterns but
also involves syntactic patterns that possess no
sequential criteria. Additionally, SVM has been
widely shown to provide superior results com-
pared to other classifiers.
1158
The input to a SVM binary classifier normally
consists of both positive and negative examples.
While it is easy to discover certain patterns from
questions, it is unnatural to identify character-
istics for non-questions, as they usually do not
share such common lexical and syntactic patterns.
The lack of good negative examples leads tra-
ditional SVM to perform poorly. To adapt the
imbalanced input data, we proposed to employ
a one-class SVM method (Manevitz and Yousef,
2002) for learning. The basic idea of one-class
SVM is to transform features from only positive
examples via a kernel to a hyper-plane and treats
the origin as the only member of the second class.
It uses relaxation parameters to separate the posi-
tive examples from the origin, and finally applies
the standard two-class SVM techniques to learn
a decision boundary. As a result, anything out-
side the boundary are considered to be outliers
(i.e. non-questions in this problem).
More formally, given n training samples
x1, . . . , xn of one class, the hyperplane separating
them from the origin is constructed by solving
min 12?w?
2 + 1?n
n?
i=1
?i ? ? (1)
subject to: w ? ?(xi) ? ? ? ?i, where ? is a ker-
nel function, ?i is the slack variable, and ? is the
parameter controlling the upper bound percentage
of outliers. If w and ? solve this problem, the de-
cision function f(x) = sign(w ??(x)??) will be
positive for most examples xi in the training set.
Supervised learning methods usually require
training data to be manually annotated. To save
labeling efforts, we take a shortcut by treating all
sentences ending with question marks as an initial
positive examples. This assumption is acceptable,
as Cong et al (2008) reported that the rule-based
method using only question mark achieves a very
high precision of over 97% in detecting questions.
It in turn indicates that questions ending with ???
are highly reliable to be real questions.
However, the initial training data still contain
many sentences ending with ??? but are not true
questions. These possible outliers will shift the
decision boundary away from the optimal one,
and we need to remove them from the training
dataset for better classification. Many prepro-
cessing strategies are available for training data
Good positive examples
(true questions)
Bad positive examples 
(non-questions)
Origin
(i) (ii) (iii)
Iterations for training 
data refinement
(i)
Decision
Boundary
Iterations
Figure 4: Illustration of one-class SVM classifi-
cation with training data refinement (conceptual
only). Three iterations (i) (ii) (iii) are presented.
refinement, including bootstrapping, condensing,
and editing etc. In this work, we employ a SVM-
based data editing and classification method pro-
posed by Song et al (2008), which iteratively sets
a small value to the parameter ? of the one-class
SVM so as to continuously refine the decision
boundary. The algorithm could be better visual-
ized with Figure 4. In each iteration, a new de-
cision boundary will be determined based on the
existing set of data points, and a portion of pos-
sible outliers will be removed from the training
set. It is expected that the learned hyperplane will
eventually be very close to the optimal one.
We use the freely available software LIBSVM2
to conduct the one-class SVM training and test-
ing. A linear kernel is used, as it is shown to be
superior in our experiments. In each refinement
iteration, the parameter ? is conservatively set to
0.02. The number of iteration is dynamically de-
termined according to the algorithm depicted in
(Song et al, 2008). Other parameters are all set to
default. The refined decision boundary from the
training dataset will be applied to classify ques-
tions from non-questions. The question detector
model learned will serve as a component for the
cQA question retrieval system in our experiments.
4 Experiments
In this section, we present empirical evaluation
results to assess the effectiveness of our ques-
tion detection model. In particular, we first ex-
amine the effects of the number of patterns on
question detection performance. We further con-
duct experiments to show that our question de-
2Available at: http://www.csie.ntu.edu.tw/?cjlin/libsvm
1159
# of Lexical Confidence # of Syntactic Confidence
Patterns 60% 65% 70% 75% 80% Patterns 60% 65% 70% 75% 80%
Su
pp
or
t 0.40% 1685 1639 1609 1585 1545
Su
pp
or
t 0.03% 916 758 638 530 453
0.45% 1375 1338 1314 1294 1277 0.04% 707 580 488 402 341
0.50% 1184 1151 1130 1113 1110 0.05% 546 450 375 308 261
0.55% 1037 1007 989 975 964 0.06% 468 379 314 260 218
Table 1: Number of lexical and syntactic patterns mined over different support and confidence values
Lexical
Patterns
Confidence Syntactic
Patterns
Confidence
65% 70% 75% 60% 65% 70%
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
Su
pp
or
t 0.40% 85.7 90.7 88.1 86.9 88.6 87.7 87.8 86.6 87.2
Su
pp
or
t 0.03% 80.4 83.3 81.9 85.1 77.5 81.1 90.7 70.2 79.1
0.45% 86.6 90.2 88.4 88.9 88.5 88.7 89.6 86.7 88.2 0.04% 79.0 86.1 82.4 90.1 78.2 83.7 90.8 70.8 79.6
0.50% 88.5 91.6 88.4 86.4 89.0 87.7 86.2 87.9 87.0 0.05% 80.3 82.5 81.4 88.8 78.4 83.3 89.9 69.0 78.1
0.55% 86.5 89.9 88.1 88.1 87.5 87.8 88.0 89.2 88.6 0.06% 83.0 83.2 83.1 88.5 77.2 82.4 86.7 75.8 80.9
Table 2: Question detection performance over different sets of lexical patterns and syntactic patterns
tection model combining both lexical and syntac-
tic features outperforms traditional rule-based or
lexical-based methods. We finally demonstrate
that our question detection model gives additional
performance boosting to question matching.
4.1 Performance Variation over Different
Pattern Sets
The performance of the question detection model
can be sensitive to the number of features used for
learning. To find the optimal number of features
used for model training, we examine the perfor-
mance variation over different amount of lexical
and syntactic patterns undertaken for training.
Dataset: We collected a total of around 800k
question threads from Yahoo! Answers Health-
care domain. From the collected data, we gener-
ated the following three datasets:
- Pattern Mining Set: Comprising around 350k
sentences from 60k question threads, where
those ending with ??? are treated as questions
and others as non-questions.
- Training Set: Positive examples comprising
around 130k sentences ending with ??? from
another 60k question threads for the one-class
SVM learning algorithm.
- Testing Set: Two annotators are asked to tag
randomly picked sentences from the remaining
set. A total of 2,004 question sentences and
2,039 non-question sentences are annotated.
Methods & Results: We use different combi-
nations of support and confidence values to gen-
erate different set of patterns. The support value
ranges from 0.40% to 0.55% for lexical patterns
with a step size of 0.05%, and ranges from 0.03%
to 0.06% for syntactic patterns with a step size
of 0.01%. The confidence value for both patterns
ranges from 60% to 80% with a step size of 5%.
These value ranges are empirically determined.
Table 1 presents the number of lexical and syn-
tactic patterns mined against different support and
confidence value combinations.
For each set of lexical or syntactic patterns
mined, we use them as features for model train-
ing. We convert the training sentences into a set
of feature vectors and employ the one-class SVM
algorithm to train a classifier. The classifier will
then be applied to predict the question sentences
in the testing set. To evaluate each question de-
tection model, we employ Precision (P ), Recall
(R), and F1 as performance metrics, and Table 2
presents the results3.
We observe from Table 2 that given a fixed sup-
port level, the precision generally increases with
the confidence level for both lexical and syntactic
patterns, but the recall drops. The lexical feature
set comprising 1,314 sequential patterns as gen-
erated with {sup=0.45%, conf=70%} gives the
best F1 score of 88.7%, and the syntactic feature
set comprising 580 syntactic patterns generated
from {sup=0.04%, conf=65%} gives the best F1
score of 83.7%. It is noted that the sequential
patterns give relatively high recall while the syn-
tactic patterns give relatively high precision. Our
reading is that the sequential patterns are capable
of capturing most questions, but it may also give
wrong predictions to non-questions such as ?Lev-
3The results for certain confidence levels are not very
promising and are not shown in the table due to lack of space.
1160
ator scapulae is used when you do the traps work-
out? that bears the sequential pattern {when, do}.
On the other hand, the syntactic patterns could
give reliable predictions, but its coverage could
suffer due to the limited number of syntactic pat-
terns. We conjecture that a combination of both
features could further improve the performance.
4.2 Performance Comparison with
Traditional Question Detection Methods
We next conduct experiments to compare the per-
formance of our question detection model to tra-
ditional rule-based or lexical-based methods.
Methods & Results: We set up five different
systems for meaningful comparisons:
1. 5W1H (baseline1): a rule-based method using
5W1H to determine a question sentence.
2. Question Mark (baseline2): a method using the
question mark ??? to judge a question.
3. SeqPattern: Using only the set of 1,314 se-
quential patterns as features.
4. SynPattern: Using only the set of 580 syntactic
patterns as features.
5. SeqPattern+SynPattern: Merging both lexical
and syntactic patterns and use them as a set of
features for question detection.
We again employ Precision (P ), Recall (R),
and F1 as performance metrics to evaluate each
question detection system, and tabulate the com-
parison results in Table 3. From the Table, we
observe that 5W1H performs poorly in both preci-
sion and recall, and question mark based method
gives relatively low recall although the precision
is the highest amongst all the methods evaluated.
This is in line with the results as observed in
(Cong et al, 2008). SeqPattern outperforms the
two baseline systems in both R and F1 scores,
and its combination with SynPattern augments
the performance in both precision and recall by
a lot. It also achieves statistically significant im-
proved results (t-test, p-value<0.05) as compared
to other four systems. These results are consistent
with our intuition that syntactic patterns can lever-
age sequential patterns in improving the question
detection performance.
It is noted that SeqPattern+SynPattern exhibits
the highest recall (R) amongst all the systems.
The significance test further suggests that many
System Combination P (%) R(%) F1(%)
(1) 5W1H 75.37 49.50 59.76
(2) Question Mark 94.12 77.50 85.00
(3) SeqPattern 88.92 88.47 88.69
(4) SynPattern 90.06 78.19 83.71
(5) SeqPattern+SynPattern 92.11 89.67 90.87
Table 3: Performance comparisons for question
detection on different system combinations
question sentences miss-detected by 5W1H or
Question Mark method could be properly cap-
tured by our model. This improvement is mean-
ingful, as the question coverage is also an im-
portant factor in the cQA question retrieval task,
where high recall implies that more similar ques-
tions could be matched and returned, hence im-
proving the question retrieval performance.
4.3 Performance Evaluation on Question
Retrieval with Question Detection Model
To further demonstrate that our question detection
model can improve question retrieval, we incor-
porate it into different question retrieval systems.
Methods: We select a simple bag-of-word
(BoW) system retrieving questions at the lexical
level, and a syntactic tree matching (STM) model
matching questions at the syntactic level (Wang et
al., 2009) as two baselines. For each baseline, we
further set up two different combinations:
- Baseline+QM: Using question mark to detect
question sentences, and perform question re-
trieval on top of the detected questions.
- Baseline+QD: Using our proposed model to
detect question sentences, and perform ques-
tion retrieval on top of the detected questions.
This gives rise to additional 4 different system
combinations for comparison.
Dataset: We divide the dataset from Yahoo!
Answers into a question repository set (750k) and
a test set (50k). For the baseline systems, all the
repository sentences containing both questions
and non-questions are indexed, whereas for sys-
tems equipped with QM or QD, only the detected
question sentences are indexed for retrieval. We
randomly select 250 single-sentence questions
from the test set as queries, and for each query, the
retrieval system will return a list of top 10 ques-
tion matches. We combine the retrieved results
from different systems and ask two annotators to
label each result to be either ?relevant? or ?irrel-
1161
System BoW BoW BoW STM STM STM
Combination +QM +QD +QM +QD
MAP (%) 58.07 59.89 60.68 66.53 68.41 69.85
% improvement
of MAP over:
Baseline N.A. +3.13 +4.49 N.A. +2.83 +4.99
Baseline+QM N.A. N.A. +1.32 N.A. N.A. +2.10
P@1 (%) 59.81 61.21 63.55 63.08 64.02 65.42
Table 4: Question retrieval performance on differ-
ent system combinations measured by MAP and
P@1 (Baseline is either BoW or STM)
evant? without telling them which system the re-
sult is generated from. By eliminating some query
questions that have no relevant matches, the final
testing set contains 214 query questions.
Metrics & Results: We evaluate the question
retrieval performance using two metrics: Mean
Average Precision (MAP) and Top One Precision
(P@1). The results are presented in Table 4.
We can see from Table 4 that STM outper-
forms BoW. Applying QM or QD over BoW and
STM boosts the system performance in terms of
both MAP and P@1. They also achieve statis-
tical significance as judged by paired t-test (p-
value<0.05). More specifically, the MAP on
QM coupled systems improves by 3.13% and
2.83% respectively over BoW and STM. This is
evidence that having question sentences clearly
identified could help to retrieve relevant ques-
tions more precisely, as without question detec-
tion, the user query is likely to be matched to ir-
relevant description sentences. Our question de-
tection model (QD) further improves the MAP
by 1.32% and 2.1% respectively over BoW+QM
and STM+QM, and it also yields better top one
precision by correctly retrieving questions at the
first position on 136 and 140 questions respec-
tively, out of a total of 214 questions. These im-
provements are in line with our expectation that
our model incorporating salient features at both
the lexical and syntactic levels is comprehensive
enough to capture various forms of questions on-
line, and hence improve the performance of ques-
tion matching.
5 Related Work
Research on detecting question sentences can
generally be classified into two categories. The
first category simply employs rule-based methods
such as question mark, 5W1H words, or hand-
crafted regular expressions to detect questions.
As discussed, these conventional methods are not
adequate to cope with online questions.
The second category uses machine learning ap-
proaches to detect question sentences. Shrestha
and McKeown (2004) proposed a supervised rule
induction method to detect interrogative questions
in email conversations based on part-of-speech
features. Yeh and Yuan (2003) used a statistical
approach to extract a set of question-related words
and derived some syntax and semantic rules to
detect mandarin question sentences. Cong et al
(2008) extracted labeled sequential patterns and
used them as features to learn a classifier for ques-
tion detection in online forums.
Question pattern mining is also closely related
to the learning of answer patterns. Work on an-
swer patterns includes the web based pattern min-
ing (Zhang and Lee, 2002; Du et al, 2005) and a
combination of syntactic and semantic elements
(Soubbotin and Soubbotin, 2002) etc.
In contrast to previous work, we do not only fo-
cus on standard language corpus, but extensively
explore characteristics of online questions. Our
approach exploits salient question patterns at both
the lexical and syntactic levels for question detec-
tion. In particular, we employ the one-class SVM
algorithm such that the learning process is weakly
supervised and no human annotation is involved.
6 Conclusion
This paper proposed a new approach to detecting
question sentences in cQA.Wemined both lexical
and syntactic question patterns, and used them as
features to build classification models. The min-
ing and leaning process is fully automated and re-
quires no human intervention. Empirical evalua-
tion on the cQA archive demonstrated the effec-
tiveness of our model as well as its usefulness in
improving question retrieval performance.
We are still investigating other features that are
helpful to detect questions. One promising direc-
tion for future work is to also employ lexical and
syntactic patterns to other related areas such as
question type classification etc. It is also interest-
ing to employ a hybrid of CRF and SVM learning
methods to boost the accuracy and scalability of
the classifier.
1162
References
Agichtein, Eugene, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Find-
ing high-quality content in social media. In WSDM,
pages 183?194.
Cong, Gao, Long Wang, Chin-Yew Lin, Young-In
Song, and Yueheng Sun. 2008. Finding question-
answer pairs from online forums. In SIGIR, pages
467?474.
Du, Yongping, Helen Meng, Xuanjing Huang, and
Lide Wu. 2005. The use of metadata, web-
derived answer patterns and passage context to
improve reading comprehension performance. In
HLT, pages 604?611.
Duan, Huizhong, Yunbo Cao, Chin-Yew Lin, and
Yong Yu. 2008. Searching questions by identify-
ing question topic and question focus. In HLT-ACL,
pages 156?164.
Jeon, Jiwoon, W. Bruce Croft, and JoonHo Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM, pages 84?90.
Jindal, Nitin and Bing Liu. 2006. Identifying compar-
ative sentences in text documents. In SIGIR, pages
244?251.
Manevitz, Larry M. and Malik Yousef. 2002. One-
class svms for document classification. J. Mach.
Learn. Res., 2:139?154.
Pei, Jian, Jiawei Han, Behzad Mortazavi-asl, Helen
Pinto, Qiming Chen, Umeshwar Dayal, and Mei
chun Hsu. 2001. Prefixspan: Mining sequen-
tial patterns efficiently by prefix-projected pattern
growth. In ICDE, pages 215?224.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In ACL, pages 464?471.
Shrestha, Lokesh and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. In COLING, page 889.
Song, Xiaomu, Guoliang Fan, and M. Rao. 2008.
Svm-based data editing for enhanced one-class
classification of remotely sensed imagery. Geo-
science and Remote Sensing Letters, IEEE,
5(2):189?193.
Soubbotin,Martin M. and Sergei M. Soubbotin. 2002.
Use of patterns for detection of likely answer
strings: A systematic approach. In TREC.
Sun, Guihua, Gao Cong, Xiaohua Liu, Chin-Yew Lin,
and Ming Zhou. 2007. Mining sequential patterns
and tree patterns to detect erroneous sentences. In
AAAI, pages 925?930.
Wang, Kai, Zhaoyan Ming, and Tat-Seng Chua. 2009.
A syntactic tree matching approach to finding sim-
ilar questions in community-based qa services. In
SIGIR, pages 187?194.
Xue, Xiaobing, Jiwoon Jeon, and W. Bruce Croft.
2008. Retrieval models for question and answer
archives. In SIGIR, pages 475?482.
Yeh, Ping-Jer and Shyan-MingYuan. 2003. Mandarin
question sentence detection: A preliminary study.
In EPIA, pages 466?478.
Zaki, Mohammed J. and Charu C. Aggarwal. 2003.
Xrules: an effective structural classifier for xml
data. In KDD, pages 316?325.
Zhang, Dell and Wee Sun Lee. 2002. Web based pat-
tern mining and matching approach to question an-
swering. In TREC.
1163
Coling 2010: Poster Volume, pages 1301?1309,
Beijing, August 2010
Automatic Generation of Semantic Fields for Annotating Web Im-
ages
Gang Wang??, Tat Seng Chua?, Chong-Wah Ngo?, Yong Cheng Wang?
?Shang Hai Jiao Tong University
?School of Computing, National University of Singapore
?Dept of Computer Science, City University of HongKong
?Na Xun Hi-Tech Application Institute
wanggang_sh@hotmail.com,chuats@comp.nus.edu.sg,
cwngo@cs.cityu.edu.hk,ycwang@mail.sjtu.edu.cn
Abstract
The overwhelming amounts of multi-
media contents have triggered the need 
for automatically detecting the semantic 
concepts within the media contents. 
With the development of photo sharing 
websites such as Flickr, we are able to
obtain millions of images with user-
supplied tags. However, user tags tend 
to be noisy, ambiguous and incomplete.
In order to improve the quality of tags
to annotate web images, we propose an 
approach to build Semantic Fields for
annotating the web images. The main 
idea is that the images are more likely to 
be relevant to a given concept, if several 
tags to the image belong to the same 
Semantic Field as the target concept. 
Semantic Fields are determined by a set 
of highly semantically associated terms 
with high tag co-occurrences in the im-
age corpus and in different corpora and 
lexica such as WordNet and Wikipedia.
We conduct experiments on the NUS-
WIDE web image corpus and demon-
strate superior performance on image 
annotation as compared to the state-of-
the-art approaches.
1 Introduction
The advancement in computer processor, sto-
rage and the growing availability of low-cost 
multimedia recording devices has led to an ex-
plosive growth of multimedia data. In order to 
effectively utilize such a huge amount of mul-
timedia contents, we need provide tools to faci-
litate their management and retrieval. One of 
the most important tools is the automatic media
concept detectors, which aim to assign high-
level semantic concepts such as ?bear? to the 
multimedia data. More formally, the concept 
detection for an web image is defined as: given 
a set of predefined concepts C : [C1, C2 ...Cn], 
we assign a semantic concept Ci to the image if 
it appears visually in the image. Traditionally, 
such concept detectors are built by the classifier 
approaches. The performance of such detectors 
depends highly on the quality of training data. 
However, preparing a set of high quality train-
ing data usually needs a large amount of human 
labors. On the other hand, the social web is 
changing the way people create and use infor-
mation. For example, users started to develop 
novel strategies to annotate the massive amount 
of multimedia information from the web. In 
image annotation, Kennedy et al (2006) ex-
plored the trade-offs in acquiring training data 
by automated web image search as opposed to 
manual human labeling. Although the perfor-
mance of systems with training data obtained 
by manual human labeling is still better than 
those whose training data is acquired by auto-
mated web search, the latter approaches have
attracted many researchers? interest due to their 
potential in reducing human label efforts. How-
ever, the tags in the web images are known to 
be ambiguous and overly personalized (Matu-
siak 2006). 
Figure 1 gives four examples to illustrate the
relationships between the visual concept ?bear?
and the annotation tag ?bear?. Generally speak-
ing, there are four types of relationships: 
1301
? The relevant tag: The user-tag ?bear?
properly reflects the content of an image, 
as shown in Figure 1(a). While ?bear? has 
multiple senses, the visual concept corres-
ponds directly to the most common sense 
of ?bear?.
? The ambiguous tag: The user-tag ?bear? is 
ambiguously related to the visual content, 
as shown in Figure 1(b). In this example, 
the visual content is related to another 
sense of ?bear?: ?a surly, uncouth, burly, 
or shambling person? (Merriam-Webster 
dictionary, 2010).
? The noisy tag: The user-tag ?bear? is a 
noisy tag, as shown in Figure 1(c). In this 
example, the visual content is irrelevant to 
the concept ?bear?.
? The incomplete tag: The user-tag ?bear?
doesn?t occur in the tag list of Figure 1(d). 
However, many human annotators believe 
that the visual concept ?bear? exist in the 
Figure 1(d). Also, in Wikipeida, a panda is 
defined as a kind of a bear. 
(a) relevant                                (b)  ambiguous
(c)  noisy                                      (d)  Incomplete 
Figure 1: The relationship between the tags and 
the visual concept ?bear? in NUS-Wide corpus.
In this paper, we aim to assign relevant tags
to images in order to reduce the effects of am-
biguous, noisy and incomplete tags. To distin-
guish relevant tags from other sense of tags, a 
common practice is to perform word sense dis-
ambiguation (WSD) to predict the right sense of 
a tag. Nevertheless, performing a WSD on a 
noisy and sparse set of tags, where the order 
and position of tags do not matter, is by no 
means easy. Most existing works on WSD, such 
as Navigli (2009) are based on clean data and 
word neighborhood statistics. They cannot be 
directly applied to address this problem. Al-
though there are some works such as Wang et al 
(2003) on capturing the semantics of noisy data, 
the problem of ambiguous words has not been 
considered. In addition, some semantic models 
such as PLSA (Hofmann 1999), LDA (David et 
al. 2003) have been proposed to capture the se-
mantics. However, one challenge of employing 
such models is that there are many noisy tags in 
the web image domain. The reason for noisy 
tags is that the purpose of tagging is not only 
for content description, but also for other fac-
tors such as getting attention and so on (Ame 
and Naaman, 2007, Bischoff et al 2008).
Given a web image with a tag list, we pro-
pose an approach to predict the ?Semantic 
Field? of the image. Semantic Field (Jurafsky 
and Martin 2000) is designed to capture a more 
integrated relationship among the entire sets of 
tags. In our work, we consider four different 
cases of examples, as shown in Figure 1. In 1(a), 
the concept ?bear? will be assigned to the image 
with relatively high probability, because ?zoo?,
?bear?, and ?polar? provide clues that ?bear? is 
the major focus of the image. In 1(b), the con-
cept ?bear? could possibly be disambiguated as
not related to ?animal?, the most common sense 
of ?bear?, by investigating other tags such as 
?men?,?guys?. In 1(c), the image will not be 
labeled as ?bear?, since the surrounding tags 
such as ?dogs?, ?pups? do not support the exis-
tence of ?bear? in the image. In 1(d), although
the concept ?bear? is missing, the image will be 
still labeled as ?bear? since the surrounding tags 
such as ?pandas?, ?animals?, and ?zoos? jointly 
suggest that ?bear? appears in the image. The 
significance of user tags towards a target con-
cept can be modeled from three different 
sources: the statistics from the web image cor-
pus, Wordnet and Wikipedia. In summary, in-
stead of directly matching the keywords and 
tags, we consider tags of an image collectively 
to predict the underlying semantic field. Ideally, 
the semantic field can highlight the major visual 
concepts in images so that we can assign the 
correct semantic labels to the images.  
In the rest of this paper, we discuss related 
work in Section 2, while Section 3 reports the 
building of Semantic Fields and its application
to web image ranking. Section 4 discusses the 
experimental setup and results. Finally, Section 
5 contains our concluding remarks.
zoo, bear,
polar,
December,
Vienna
men, bear,
hot, cubs,
bears, fur,
cub, hairy,
guys, fuzzy,
bare
bear, dogs,
pups, pup-
pies, cud-
dle, daisy
animals,
pandas,
zoos
1302
2 Related Work
In this section, we report the works on Semantic 
Field theory, text analysis in multimedia and 
the existing systems for a web image corpus.
2.1 Semantic Fields
Semantic Fields have been hotly debated in lin-
guistics community (Grandy 1992, Garret 
1992). Compared to lexical analysis, it consid-
ers the entire sets of words instead of a single 
word. The FrameNet project (Baker et al 1998) 
is an attempt to realize the Semantic Field.
However, the problem with FrameNet project is 
that it needs extensive human efforts to define 
the thematic roles for each domain and each 
frame, and hence it is domain specific.
2.2 Text Analysis in Multimedia 
In multimedia, one of the important tasks is 
concept detection, which attempts to find the 
visual appearance of a concept such as ?bear? in 
an image. However, due to the large variations 
in the low level visual feature space such as 
color, texture etc, in many cases, researchers are 
hardly able to capture the concept by visual in-
formation alone. Some researchers attempted to 
employ natural language analysis to detect the 
visual concept. Rowe (1994) explored the syn-
tax of images? captions to infer the visual con-
cepts present in images. For example, he found 
that the primary subject noun phrase usually 
denotes the most significant information in the 
media datum or its ?focus?. He assumed that 
both visual and text features will describe the 
same focus of the content. Wang et al (2008) 
employed the similar idea to infer visual con-
cepts in news video. They first aligned text in-
formation with visual information, and then 
captured the text focus to infer the visual con-
cept. These works suggest that we can transfer 
the problem of visual concept detection to that 
of finding a text focus. 
In addition, researchers proposed statistical 
models to combine text and visual features, 
such as the translation model (Duygulu et al 
2002, Jin et al 2005), cross media relevance 
model (Jeon et al 2003) and continuous relev-
ance model (Lavrenko and Jeon, 2003). How-
ever, no matter what models we used, the anno-
tation accuracy is still quite low, partially be-
cause of the existence of noise in tags. Jin et al 
(2005) provided a solution to tackle such a noi-
sy tag problem. They first investigated various 
semantic similarity measures between each
keyword pairs in the tag list based on Wordnet. 
They then regarded non-correlated keywords as 
noises and discarded them. In this paper, there 
are three major differences between our work 
and the above work. First, because tags from 
Internet are not always included in Wordnet, we 
employ multi-resources of information to ana-
lyze the semantics. Second, we extend the anal-
ysis of the word pair relationship to the Seman-
tic Field analysis. Third, since it is not easy to 
identify the noise in the tag list directly, we on-
ly analyze the tags which are highly relevant to 
the concept with a specific sense.  
2.3 The State of the Art Systems
NUS-WIDE (Chua et al 2009) is a large scale 
Web image corpus. It provides not only social 
tags from the web, but also the ?gold? labels (or 
ground truth) for 81 concepts from large human 
labeling efforts. As far as we know, there are 
two reported systems that used the whole NUS-
WIDE corpus to test their proposed methods. In 
Chua et al (2009), the 81 concepts are detected 
by k nearest neighbor using the visual features 
of: color moments, color auto-correlogram, col-
or histogram, edge direction histogram, wavelet 
texture, and a bag of visual words. The mean 
average precision (MAP) for the 81 concepts 
reaches 0.1569. Gao et al (2009) extended the 
k-NN approach to use both text tags and visual 
information. For the tag information, they made 
use of the co-occurrence information to com-
pute the probability of an image belonging to a 
contain concept. They used the same visual fea-
tures as in (Chua et al 2009). In their work, the 
taxonomy in WordNet is exploited to identify 
whether a target concept is generic or specific. 
The co-occurrence tag analysis is employed for 
generic concepts, while visual analysis is used 
for specific concepts. The MAP for this ap-
proach reaches 0.2887. 
3 Building Semantic Fields for Annotat-
ing Web Images
In this paper, we attempt to capture text seman-
tics collectively from the tag list of images to 
annotate their visual contents. Semantic Fields 
consist of a selected subset of the tag list and 
1303
the choice of these tags is based on their relev-
ance to the contents of the targeted image with 
a specific sense. There are three characteristics
in our Semantic Field model. First, the Seman-
tic Field is built by only a subset of tag list. For 
example, the Semantic Field in Figure 1(a) is 
{zoo, bear, polar}. It could partially reduce the 
effect of the noise. Second, because inferring 
the visual concept of an image is more reliable 
by joint analysis of tags in the Semantic Field,
rather than investigating one tag at a time in the 
whole tag list, we analyze the whole Semantic 
Field as a unit. By utilizing the context informa-
tion in Semantic Field, the problems of ambi-
guous, noisy and incomplete tags are partially
tackled. Third, we perform normalization to 
estimate the importance of Semantic Field,
which is discussed in Section 3.1. If the value is 
large, it suggests that most of the tags in the 
image support the Semantic Field; that is, the 
probability that the target concept is the focus 
of the image is high, and vice versa. Such a de-
sign aims to minimize the effects of noisy and 
ambiguous tags.
3.1 A Probabilistic Model 
We denote
xC as a target concept that appears 
in the content of an image. We want to deter-
mine the set of tags that are related to 
xC from 
the user-supplied tags by building a Semantic 
Field 
iSF for each image. The probability of 
the appearance of concept )|( ix SFCP can be 
computed as:
)(
)()|(
)|(
i
xxi
ix SFP
CPCSFP
SFCP


(1)
For the purpose of collecting and annotating
images and simplifying the model, we did not
consider the prior knowledge for each image. 
Thus, the prior probability P(Cx) can be viewed 
as a constant with respect to a concept Cx. In 
addition, the range of the normalization factor 
P(SFi) is expected to be small, which will not 
affect the annotation of web images. This as-
sumption is reasonable due to the fact that there 
are a large number of different tags, and these 
tags can be combined to form any Semantic 
Field in an arbitrary manner. The number of 
combinations is exponential to the number of 
possible tags available. This is also evident by 
the observation that most tag lists associated 
with the images are unique. In other words, two 
images with the same Semantic Field are sel-
dom found in reality. With these in mind, Equa-
tion (1) can be approximated and simplified to:
)|()|( xiix CSFPSFCP  (2)
Given a Semantic Field
iSF , it may include n 
related tags
nTTTT ,...,,, 321 . Thus Equation (2) is 
expanded to:
)|,...,()|( 21 xnxi CTTTPCSFP  (3)
Two obvious approaches to compute Equa-
tion (3) are using the product of the individual 
terms or chain rule decomposition. However, 
we consider the individual terms to be inter-
dependent and the chain rule decomposition is
not easy to compute. To simplify the model, we
employ the normalized linear fusion to expand 
Equation (3) as follows:
TN
)|(
)|,...,P(T 121



n
i
xi
xn
CTP
CTT (4)
The normalization factor is the total number 
(TN) of tags in the image tag list.
3.2 Using Multiple External Sources
To estimate the probability of a tag Ti given a 
target concept Cx, i.e., P(Ti|Cx), we consider 
both the domain knowledge and general know-
ledge acquired from Internet. For the former,
we utilize the co-occurrence statistics of tags in
images which can be computed offline from 
any web image corpus. For the latter, we em-
ploy WordNet and Wikipedia for inferring the 
relatedness between tags and a target concept. 
Combining different knowledge sources, the 
probability is estimated as:
)|()|()|()|( ___ xcoixwikiixwdixi CTPCTPCTPCTP  (5)
where Ti_wd, Ti_wiki, Ti_co represent the tag occur-
rences in WordNet, Wikipedia and co-
occurrence statistics, respectively.
To compute Equation (5), we query different 
information sources using the target concept Cx.
In WordNet, because the sense of the concept 
usually refers to the most common sense in our 
corpus, we choose the most common sense 
(noun) as the target. Using Figure 2 as an ex-
ample, the concept "bear" is defined in Word-
Net as ?massive plantigrade carnivorous or om-
nivorous mammals with long shaggy coats and 
strong claws?. In Wikipedia, with Figure 3 as 
an example, the related page is downloaded to 
1304
describe the concept "bear". For the co-
occurrence statistics of the tag lists, we estimate 
their values from co-occurrence information 
from the image corpus. With the above know-
ledge, we compute the conditional probability 
of a tag being related to Cx as:
)(#
),(#
)|(
x
xj
xj C
CT
CTP  (6)
where j = {wd, wiki, co}, #(Tj, Cx) indicates the
number of times the tag and the concept co-
occur in an information source, and #(Cx) de-
notes the number of times the concept Cx appear 
in the information source. In addition, we em-
ploy an add-one smoothing approach [Jurafsky 
and Martin 2000] to further process the results. 
Figure 2: The information in WordNet
Figure 3: The information in Wikipedia.
Given a concept with a special sense, for all 
the tags in the corpus, we can obtain the condi-
tional probabilities of each tag Ti based on Equ-
ation (5). We rank the tags according to 
)C|P(T xi . To reduce computations, we select
the top N (N=200) tags as the highly related
tags to a given concept and place them in a dic-
tionary.
3.3 Building Semantic Field for Image An-
notation
We now build the Semantic Fields to rank the 
images with respect to concept Cx. The detailed
algorithm is shown in Figure 4. 
Input: 
1) Given a target concept, we rank all the tags 
in the corpus based on Equation (5).
2) Given a web image, we have a list of anno-
tation tags ( 121 ,..., nlll ).
Step 1: Generate a dictionary (D) based on top 
N tags
Step 2: For (i=1; i<n1; i++)
If ( Dli  ) then put il into the Semantic 
Field for the image.
Step 3: Annotate the images and compute the 
probability of the occurrence of the 
concept via Equation (4)
Figure 4: The algorithm for building the Se-
mantic Fields and annotating the im-
ages. 
The algorithm comprises three steps:
1. bear 2. bears 3. polar 4. species
5. panda 6. cubs 7. giant 8. grizzly
9.teddy 10. pandas ? ?
Table 1: The top 10 tags for concept ?bear? in
most common sense.
First, given a target concept with a specific 
sense, we generate a dictionary based on the top 
N candidate tags as discussed in Section 3.2.
Table 1 shows the top 10 tags in the dictionary 
for the concept ?bear? with the most common 
sense. As we want to distinguish single and 
plural noun for different visual concepts, we do
not employ the stemming algorithm. Although 
the results are not ideal, we find that many 
highly related words are included in the dictio-
nary.
Second, we infer the annotation tags of the 
image from the dictionary and use that to build 
the Semantic Fields. Figure 1 demonstrates the 
resulting of Semantic Fields for images in Ta-
ble 2. 
Third, we assign the tags to images based on 
their Semantic Fields. Because most of the tags 
in Figure 1(a) and 1(d) are highly relevant to 
?bear? with the most common sense, we assign 
the semantics to these two images with high 
probabilities. Thus, the problem of incomplete
tags is tackled in this case. On the other hand, 
since most of the tags in Figure 1(b) and 1(c) 
fail to support the concept ?bear? with the most 
1305
common sense (the Semantic Field obtains less 
than 20% of tags? support), we only assign the 
semantics with very low probabilities. Thus, 
the ambiguous and noisy problem can be par-
tially tackled. 
Semantic Field for 
Figure 1 (a)
{zoo, bear, polar}
Semantic Field for 
Figure 1 (b)
{bear, bears}
Semantic Field for 
Figure 1 (c)
{bear}
Semantic Field for 
Figure 1 (d)
{animals, pandas, zoos}
Table 2: Semantic Fields of images in Figure 1.
4 Experiments
In this section, we first introduce the test-bed 
and measurement of the experiments. We then 
report the results and compare them with the 
state-of-the-art systems tested on NUS-WIDE 
corpus. 
The NUS-Wide corpus (Chua et al 2009) in-
cludes 269,648 images with 5,018 user-
provided tags, and the ground-truth for 81 con-
cepts for the entire database. These concepts are 
grouped into six different categories: graph, 
program, scene /location, event/activities, 
people and object. The choice of concepts is 
based on the generality and popularity in Flickr, 
the distributions in different categories and the 
common interests of the multimedia community. 
This corpus includes two parts. The first part 
contains 161,789 images to be used for training 
and the second part contains 107,859 images is 
used for testing.
The performance of the system is measured 
using the mean average precision (MAP) based 
on all the test images for all the 81 concepts. 
This is the same as the evaluation used in 
TRECVID. The MAP combines precision and 
recall into one performance value. Let 
},...,,{ 21 k
k iiip  be a ranked version of the 
resulting set A. At any given rank k, let kpR 
be the number of relevant images in the top k of 
p, where |R| is the total number of relevant im-
ages. Then MAP for the 81 concepts Ci is de-
fined as: 
)](
||
1
[
81
1
1
81
1
k
A
k
k
C
i
k
pR
R
MAP
i




 (6)
where the indictor function 1)( ki if Rik  and 
0 otherwise.
4.1 Comparison with the State-of-the-Art 
Systems 
We compare our approach against the re-
ported systems on NUS-WIDE corpus. 
MAP










A visual based k-
NN system
A visual and tag
information based
k-NN system
Our approach
Figure 5: The comparison with the state-of-
the-art system
In our approach, we employ the Semantic 
Field to annotate the images, which requires 
neither training data nor visual analysis, and is 
running directly on the test data. In contrast to 
the two previous approaches in Section 2.3, the 
input to Semantic Field is simply the tag list of 
an image. Figure 5 shows the performance 
comparisons among the three tested approaches. 
As compared to (Chua et al 2009) and (Gao et 
al. 2009), which exhibit the best performance 
on NUS-WIDE so far, Semantic Field achieves 
a MAP of 0.4198 which shows a 45.4% im-
provement.
The reason for the superior performance of 
our approach is that there is insufficient training
data, which means that most learning-based 
systems could not perform well. As seen in 
Figure 6(a), 44% of concepts have less than 
1,000 positive training data. This is insufficient 
for training the classifiers for the visual con-
cepts. Take the visual concept ?flag? as the ex-
ample. Considering that there are at least 200 
national flags from different countries and re-
gions, not to mention other types of flags such 
as holiday flag, there are large variations in
concept "flag" as shown in Figure 6(b). Hence 
it is difficult to train a classifier with visual 
analysis by having only 214 positive training 
samples. This suggests that there may be a large 
1306
gap between the training and test data. On the 
other hand, because web images include not 
only visual features but also text information,
we could employ text analysis to infer the visu-
al concept. The advantages of our Semantic 
Field approach are that we could analyze mul-
tiple information sources to reduce the text var-
iations and the performance of our approach is 
independent of the training data and visual fea-
tures. With the increasing size of the corpus, the 
problems of few positive training data and large 
visual diversity between training and test data 
will be exacerbated. This is the reason why our 
approach is more robust than those based on 
visual analysis and traditional learning-based 
approaches.
(a) The distribution of positive training data in  
NUS-Wide corpus.
(b) Different color and different shapes for the con-
cept ?flag? in NUS-Wide corpus.
Figure 6: Various visual patterns need a lot of 
training data
4.2 The Noisy, Ambiguous and Incomplete
Tag Problems
We design the second experiment to evaluate
the ability of our algorithm to tackle the noisy
and ambiguous and incomplete tag problem in 
user-supplied tags. The baseline system is a
keyword (tag) matching algorithm. That is, if 
the image contains the keyword in the tag list, 
the algorithm will regard it as relevant to the 
concept; otherwise, it is irrelevant. The results 
are shown in Figure 7.
We found that our approach achieves a rela-
tive improvement of 38% as compared to the 
keyword matching approach. This is because 
the Semantic Field approach selects and analyz-
es a group of tags as a whole, which provides 
essential context information and reduces the 
effects of noisy, ambiguous and incomplete tags. 
Figure 7: Comparison with keyword matching
approach
For completeness, we also evaluate the sys-
tem using the Equations (7) and (8) according 
to the top k images (k=1000, 2000, 3000, 4000, 
5000). 
N
A
p
tagP
N
i i
i



1 )(#
)(#
)( (7)
1
#( )
#( )
( )
N
i
i i
p
T
R tag
N



                     (8)
We use pi to represent the number of images 
with the target concept and Ai to represent the 
number of retrieved images for tag i. N denotes
the number of different detected concepts (tags) 
in the ground truth set. In this corpus, the value 
of the N is 81. iT is the number of the ground 
truth for a certain target concept.
Figure 8: Comparison in precision on top-k im-
age ranking. The x-axis indicates the value of k,
while the y-axis shows the P(tag).
1307
Figure 9: Comparison in recall on top-k image 
ranking. The x-axis indicates the value of k,
while the y-axis shows the R(tag).
Figures 8 and 9 report the performance in 
precision and recall respectively. From the re-
sults, we find that our approach is better than 
that of the baseline system in both precision and 
recall. This is because on one hand the Seman-
tic Field tackles the ambiguous and noisy tag 
problems so that we could improve the preci-
sion. On the other hand, the Semantic Field 
analysis includes many highly related tags, 
which tackle the incomplete tags problem so 
that it could improve the performance in recall. 
4.3 Importance of Multi-source Informa-
tion
Semantic Fields combine three information 
sources: WordNet, Wikipedia and the tag?s co-
occurrence information in the NUS-Wide cor-
pus. We design the third experiment to evaluate
the contribution of each information source.
The results are shown in Figure 10.
Figure 10: The comparison between using 
single information source and fusion of 
multiple information sources.
From Figure 10, we find that the 
performance of using WordNet alne obtains
the worst result. This is because the number of 
tags carries the most common sense is limited 
and there are some noisy words in the 
description. For example, in Figure 2, the 
occurrence of the word ?long? does not imply 
the occurrence of the concept ?bear?. Due to the 
lack of further information, using WordNet 
alone can hardly remove the noisy tag "long".
The test result shows that such noisy 
information significantly degrade the 
performance of the system. This suggests the 
importance of incorperating other sources of 
informaiton to provide more complete 
information for the analysis.
We can also observe that using Wikipedia or 
tag co-occurrence shows comparatively better 
performance. This is because both information 
sources include abundance information for 
analysis. Thus, compared to the keyword-based
approach, the performance of the systems 
shows around 17% improvement. Finally, 
fusing the three information sources results in 
the best MAP performance. This is because 
information from different sources 
complements each other and helps in reducing 
the effects of the noisy, ambiguous and 
incomplete tags. 
5 Conclusion
In this paper, we proposed the use of Semantic 
Field to annotate web images. It could reduce 
the influences of noisy, ambiguous and incom-
plete tags so that the quality of the tags assigned 
to the web image can be improved. Our expe-
riments showed that our approach is more ro-
bust and could achieve 38% improvement in 
MAP as compared to the learning-based and 
visual analysis approaches when there is suffi-
cient text information. Also the fusion of mul-
tiple information sources could further boost 
the performance of the system.
The work is only the beginning. Future 
works include the followings. First, as multi-
media data includes multiple modality features, 
how to fuse them to improve the performance 
of the system is an important problem. Second, 
current version of our algorithm only could 
identify one sense of the concept. How to dis-
tinguish among different senses of the concept 
is also an urgent task. Third, we will explore 
more semantic relations from Wordnet, Wiki-
pedia and so on. 
References 
M. Ames and M. Naaman (2007), ?Why We Tag: 
Motivations for Annotation in Mobile and online 
Media?. In Proceedings of the SIGCHI confe-
1308
rence on Human factors in computing systems, pp.
971 ? 980.
C. F. Baker and C. J. Fillmore and J. B. Lowe (1998) 
?The Berkeley FrameNet Project?, Proceedings of 
the 36th Annual Meeting of the Association for 
Computational Linguistics pp. 86-90.
K. Bischoff, C. S. Firan, W. Nejdl, R. Paiu (2008), 
?Can All Tags be Used for Search?, In Proceed-
ings of the 17th ACM conference on Information 
and knowledge management, pp. 193-202.
T. S. Chua, J. H. Tang, R. C. Hong, H. J. Li, Z. P.
Luo, and Y. T. Zheng (2009), "NUS-WIDE: A 
Real-World Web Image Database from National 
University of Singapore", ACM International 
Conference on Image and Video Retrieval.
B. M. David, A. Y. Ng and M. I. Jordan (2003), ?La-
tent Dirichlet Allocation?, Journal of Machine 
Learning Research 3: 993-1022.
P. Duygulu and K. Barnard (2002), ?Object recogni-
tion as machine translation: learning a lexicon for 
a fixed image vocabulary?, In Proceedings of the 
7th European Conference on Computer Vision, 4: 
97-112. 
W. A. Gale and K. Church and D. Yarowsky (1992), 
?A method for disambiguating word sense in a 
corpus?. Computers and the Humanities. 26 pp. 
415-439.
S. H. Gao, L. T. Chia and X. G. Cheng, (2009) ?Un-
derstanding Tag-Cloud and Visual Features for 
Better Annotation of Concepts in NUS-Wide Da-
taBase? , In Proceedings of WSMC 2009. 
M. F. Garrett (1992), ?Lexical Retrieval Processes: 
Semantic Filed Effects?, in Lehrer and Kittay Eds. 
Frames, Fields and Contrasts: New Essays in Se-
mantic and Lexical Organization. pp. 377-396 
Hillsdale: Lawrence Erlbaum. 
R. E. Grandy (1992), ?Semantic Fields, Prototypes, 
and the Lexicon?, in Lehrer and Kittay Eds. 
Frames, Fields and Contrasts: New Essays in Se-
mantic and Lexical Organization. pp. 103-122 
Hillsdale: Lawrence Erlbaum. 
T. Hofmann (1999), ?Probabilitic Latent Semantic 
Indexing?, In Proceedings of the 22rd Annual In-
ternational SIGIR Conference on Research and 
Development in Information Retrieval. 
J. Jeon, V. Lavrenko, and R. Manmatha (2003), 
?Automatic Image annotation and retrieval using 
cross-media relevance modes?, In Proceedings of 
the 26th Annual International ACM SIGIR Confe-
rence on Research and Development in Informa-
tion Retrieval, pp. 119-126.  
Y. Jin, L. Khan, L. Wang and M. Awad (2005), 
? Image Annnotations by Combining multiple 
Evidence & WordNet?, In Proceedings of the 
ACM Multimedia Conference, pp. 706-715.
D. Jurafsky and J. H. Martin (2000), ?Speech and 
language processing?, published by Prentice-Hall 
Inc.
L. S. Kennedy, S. F. Chang and I. V. Kozintsev
(2006), ?To search or To Label?, In Proceedings 
of MIR 2006, pp. 249-258.
R. M. V. Lavrenko and J. Jeon (2003), ? A model 
for learning the semantic of pictures?, In Proceed-
ings of the 17th Annual Conference on Neural In-
formation Processing Systems. 
C. Manning and H. Schutze (1999). ?Foundations of 
Statistical Natural Language Processing?. MIT 
Press, Cambridge, MA.
K. Matusiak (2006), ?Towards user-centered index-
ing in digitial image collections?, OCLC systems
and Services, 22(4): pp. 283-298.
R. Navigli (2009), ?Word Sense Disambiguation: A 
Survey?, ACM Computing Surveys, Vol. 41, No. 
2. Article 10.
N. C. Rowe (1994) ?Inferring depictions in natural 
language captions for efficient access to picture
data?, Information Process & Management Vol.
30 No 3. pp. 379-388.
G. Wang, T. S. Chua and Y. C. Wang (2003), ?Ex-
tracting Key Semantic Terms from Chinese 
Speech Query for Web Searches?. In proceeding 
of 41st Annual Meeting of the Association for 
Computational Linguistics pp. 248-255.
G. Wang, T. S. Chua, M. Zhao (2008), "Exploring 
Knowledge of Sub-domain in a Multi-resolution 
Bootstrapping Framework for Concept Detection 
in News Video", In Proceeding of the 16th ACM 
international Conference on Multimedia. pp. 249-
258.
Merriam Webster Online dictionary (2010), Availa-
ble at http://www.merriam-webster.com/
1309
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 140?150,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Domain-Assisted Product Aspect Hierarchy Generation: Towards
Hierarchical Organization of Unstructured Consumer Reviews
Jianxing Yu1, Zheng-Jun Zha1, Meng Wang1, Kai Wang2, Tat-Seng Chua1
1School of Computing, National University of Singapore
2Institute for Infocomm Research, Singapore
{jianxing, zhazj, wangm, chuats}@comp.nus.edu.sg kwang@i2r.a-star.edu.sg
Abstract
This paper presents a domain-assisted ap-
proach to organize various aspects of a prod-
uct into a hierarchy by integrating domain
knowledge (e.g., the product specifications),
as well as consumer reviews. Based on the
derived hierarchy, we generate a hierarchical
organization of consumer reviews on various
product aspects and aggregate consumer opin-
ions on these aspects. With such organiza-
tion, user can easily grasp the overview of
consumer reviews. Furthermore, we apply the
hierarchy to the task of implicit aspect identi-
fication which aims to infer implicit aspects of
the reviews that do not explicitly express those
aspects but actually comment on them. The
experimental results on 11 popular products in
four domains demonstrate the effectiveness of
our approach.
1 Introduction
With the rapidly expanding e-commerce, most retail
Web sites encourage consumers to write reviews to
express their opinions on various aspects of prod-
ucts. Huge collections of consumer reviews are
now available on the Web. These reviews have be-
come an important resource for both consumers and
firms. Consumers commonly seek quality informa-
tion from online consumer reviews prior to purchas-
ing a product, while many firms use online reviews
as an important resource in their product develop-
ment, marketing, and consumer relationship man-
agement. However, the reviews are disorganized,
leading to the difficulty in information navigation
and knowledge acquisition. It is impractical for user
to grasp the overview of consumer reviews and opin-
ions on various aspects of a product from such enor-
mous reviews. Among hundreds of product aspects,
it is also inefficient for user to browse consumer re-
views and opinions on a specific aspect. Thus, there
is a compelling need to organize consumer reviews,
so as to transform the reviews into a useful knowl-
edge structure. Since the hierarchy can improve in-
formation representation and accessibility (Cimiano,
2006), we propose to organize the aspects of a prod-
uct into a hierarchy and generate a hierarchical or-
ganization of consumer reviews accordingly.
Towards automatically deriving an aspect hierar-
chy from the reviews, we could refer to traditional
hierarchy generation methods in ontology learning,
which first identify concepts from the text, then
determine the parent-child relations between these
concepts using either pattern-based or clustering-
based methods (Murthy et al, 2010). However,
pattern-based methods usually suffer from inconsis-
tency of parent-child relationships among the con-
cepts, while clustering-based methods often result
in low accuracy. Thus, by directly utilizing these
methods to generate an aspect hierarchy from con-
sumer reviews, the resulting hierarchy is usually in-
accurate, leading to unsatisfactory review organiza-
tion. On the other hand, domain knowledge of prod-
ucts is now available on the Web. For example,
there are more than 248,474 product specifications
in the product sellingWeb site CNet.com (Beckham,
2005). These product specifications cover some
product aspects and provide coarse-grained parent-
child relations among these aspects. Such domain
knowledge is useful to help organize the product as-
140
Figure 1: Sample hierarchical organization for iPhone 3G
pects into a hierarchy. However, the initial hierarchy
obtained from domain knowledge usually cannot fit
the review data well. For example, the initial hierar-
chy is usually too coarse and may not cover the spe-
cific aspects commented in the reviews, while some
aspects in the hierarchy may not be of interests to
users in the reviews.
Motivated by the above observations, we propose
in this paper to organize the product aspects into a
hierarchy by simultaneously exploiting the domain
knowledge (e.g., the product specification) and con-
sumer reviews. With derived aspect hierarchy, we
generate a hierarchical organization of consumer re-
views on various aspects and aggregate consumer
opinions on these aspects. Figure 1 illustrates a sam-
ple of hierarchical review organization for the prod-
uct ?iPhone 3G?. With such organization, users can
easily grasp the overview of product aspects as well
as conveniently navigate the consumer reviews and
opinions on any aspect. For example, users can find
that 623 reviews, out of 9,245 reviews, are about the
aspect ?price?, with 241 positive and 382 negative
reviews.
Given a collection of consumer reviews on a spe-
cific product, we first automatically acquire an ini-
tial aspect hierarchy from domain knowledge and
identify the aspects from the reviews. Based on the
initial hierarchy, we develop a multi-criteria opti-
mization approach to construct an aspect hierarchy
to contain all the identified aspects. Our approach
incrementally inserts the aspects into the initial hi-
erarchy based on inter-aspect semantic distance, a
metric used to measure the semantic relation among
aspects. In order to derive reliable semantic dis-
tance, we propose to leverage external hierarchies,
sampled from WordNet and Open Directory Project,
to assist semantic distance learning. With resultant
aspect hierarchy, the consumer reviews are then or-
ganized to their corresponding aspect nodes in the
hierarchy. We then perform sentiment classification
to determine consumer opinions on these aspects.
Furthermore, we apply the hierarchy to the task of
implicit aspect identification. This task aims to infer
implicit aspects of the reviews that do not explic-
itly express those aspects but actually comment on
them. For example, the implicit aspect of the review
?It is so expensive? is ?price.? Most existing aspect
identification approaches rely on the appearance of
aspect terms, and thus are not able to handle implicit
aspect problem. Based on our aspect hierarchy, we
can infer the implicit aspects by clustering the re-
views into their corresponding aspect nodes in the
hierarchy. We conduct experiments on 11 popular
products in four domains. More details of the corpus
are discussed in Section 4. The experimental results
demonstrate the effectiveness of our approach.
The main contributions of this work can be sum-
marized as follows:
1) We propose to hierarchically organize con-
sumer reviews according to an aspect hierarchy, so
as to transfer the reviews into a useful knowledge
structure.
2) We develop a domain-assisted approach to
generate an aspect hierarchy by integrating domain
knowledge and consumer reviews. In order to de-
rive reliable semantic distance between aspects, we
propose to leverage external hierarchies to assist se-
mantic distance learning.
3) We apply the aspect hierarchy to the task of im-
plicit aspect identification, and achieve satisfactory
performance.
The rest of this paper is organized as follows. Our
approach is elaborated in Section 2 and applied to
implicit aspect identification in Section 3. Section
4 presents the evaluations, while Section 5 reviews
141
related work. Finally, Section 6 concludes this paper
with future works.
2 Approach
Our approach consists of four components, includ-
ing initial hierarchy acquisition, aspect identifica-
tion, semantic distance learning, and aspect hierar-
chy generation. Next, we first define some prelimi-
nary and notations and then elaborate these compo-
nents.
2.1 Preliminary and Notations
Preliminary 1. An aspect hierarchy is defined as a
tree that consists of a set of unique aspects A and
a set of parent-child relations R between these as-
pects.
Given the consumer reviews of a product, let
A = {a1, ? ? ? , ak} denotes the product aspects com-
mented in the reviews. H0(A0,R0) denotes the ini-
tial hierarchy derived from domain knowledge. It
contains a set of aspects A0 and relations R0. Our
task is to construct an aspect hierarchy H(A,R), to
cover all the aspects in A and their parent-child re-
lations R, so that the consumer reviews are hierar-
chically organized. Note that H0 can be empty.
2.2 Initial Hierarchy Acquisition
As aforementioned, product specifications on prod-
uct selling websites cover some product aspects and
coarse-grained parent-child relations among these
aspects. Such domain knowledge is useful to help
organize aspects into a hierarchy. We here employ
the approach proposed by Ye and Chua (2006) to au-
tomatically acquire an initial aspect hierarchy from
the product specifications. The method first identi-
fies the Web page region covering product descrip-
tions and removes the irrelevant contents from the
Web page. It then parses the region containing the
product information to identify the aspects as well as
their structure. Based on the aspects and their struc-
ture, it generates an aspect hierarchy.
2.3 Aspect Identification
To identify aspects in consumer reviews, we first
parse each review using the Stanford parser 1. Since
the aspects in consumer reviews are usually noun
1http://nlp.stanford.edu/software/lex-parser.shtml
Figure 2: Sample Pros and Cons reviews
or noun phrases (Liu, 2009), we extract the noun
phrases (NP) from the parse tree as aspect candi-
dates. While these candidates may contain much
noise, we leverage Pros and Cons reviews (see Fig-
ure 2), which are prevalent in forum Web sites,
to assist identify aspects from the candidates. It
has been shown that simply extracting the frequent
noun terms from the Pros and Cons reviews can get
high accurate aspect terms (Liu el al., 2005). Thus,
we extract the frequent noun terms from Pros and
Cons reviews as features, then train a one-class SVM
(Manevitz et al, 2002) to identify aspects from the
candidates. While the obtained aspects may con-
tain some synonym terms, such as ?earphone? and
?headphone?, we further perform synonym cluster-
ing to get unique aspects. Specifically, we first ex-
pand each aspect term with its synonym terms ob-
tained from the synonym termsWeb site 2, then clus-
ter them to obtain unique aspects based on unigram
feature.
2.4 Semantic Distance Learning
Our aspect hierarchy generation approach is essen-
tially based on the semantic relations among as-
pects. We here define a metric, Semantic Distance,
d(ax, ay), to quantitatively measure the semantic re-
lation between aspects ax and ay. We formulate
d(ax, ay) as the weighted sum of some underlying
features,
d(ax, ay) =
?
j
wjfj(ax, ay), (1)
where wj is the weight for j-th feature function
fj(?).
Next, we first introduce the linguistic features
used in our work and then present the semantic dis-
tance learning algorithm that aims to find the opti-
mal weights in Eq.(1).
2http://thesaurus.com
142
2.4.1 Linguistic Features
Given two aspects ax and ay, a feature is defined
as a function generating a numeric score f(ax, ay)
or a vector of scores. The features include Contex-
tual, Co-occurrence, Syntactic, Pattern and Lexical
features (Yang and Callan, 2009). These features are
generated based on auxiliary documents collected
from Web.
Specifically, we issue each aspect term and aspect
term pair as queries to Google and Wikipedia, re-
spectively, and collect the top 100 returned docu-
ments of each query. We then split the documents
into sentences. Based on these documents and sen-
tences, the features are generated as follows.
Contextual features. For each aspect, we collect
the documents containing the aspect as context to
build a unigram language model without smoothing.
Given two aspects, the KL-divergence between their
language models is computed as the Global-Context
feature between them. Similarly, we collect the left
two and right two words surrounding each aspect as
context and build a unigram language model. The
KL-divergence between the language models of two
aspects is defined as the Local-Context feature.
Co-occurrence features. We measure the co-
occurrence of two aspects by Pointwise Mutual
Information (PMI): PMI(ax,ay)=log(Count(ax,ay)/
Count(ax) Count(ay)), where Count(?) stands for the
number of documents or sentences containing the
aspect(s), or the number of Google document hits
for the aspect(s). Based on different definitions of
Count(?), we define the features of Document PMI,
Sentence PMI, and Google PMI, respectively.
Syntactic features. We parse the sentences that
contain each aspect pair into syntactic trees via the
Stanford Parser. The Syntactic-path feature is de-
fined as the average length of the shortest syntactic
path between the aspect pair in the tree. In addi-
tion, for each aspect, we collect a set of sentences
containing it, and label the semantic role of the sen-
tences via ASSERT parser 3. Given two aspects,
the number of the Subject terms overlaps between
their sentence sets is computed as the Subject Over-
lap feature. Similarly, for other semantic roles, such
as objects, modifiers, and verbs, we define the fea-
tures of Object Overlap, Modifier Overlap, and Verb
3http://cemantix.org/assert.html
Overlap, respectively.
Pattern features. 46 patterns are used in our
work, including 6 patterns indicating the hypernym
relations of two aspects (Hearst, 1992), and 40 pat-
terns measuring the part-of relations of two aspects
(Girju et al, 2006). These pattern features are
asymmetric, and they take the parent-child relations
among the aspects into consideration. All the pat-
terns are listed in Appendix A (submitted as supple-
mentary material). Based on these patterns, a 46-
dimensional score vector is obtained for each aspect
pair. A score is 1 if two aspects match a pattern, and
0 otherwise.
Lexical features. We take the word length differ-
ence between two aspects, as Length Difference fea-
ture. In addition, we issue the query ?define:aspect?
to Google, and collect the definition of each aspect.
We then count the word overlaps between the defini-
tions of two aspects, as Definition Overlap feature.
2.4.2 Semantic Distance Learning
This section elaborates the learning algorithm
that optimizes the semantic distance metric, i.e.,
the weighting parameters in Eq.(1). Typically, we
can utilize the initial hierarchy as training data.
The ground-truth distance between two aspects
dG(ax, ay) is generated by summing up all the edge
distances along the shortest path between ax and ay,
where every edge weight is assumed as 1. The dis-
tance metric is then obtained by solving the follow-
ing optimization problem,
argmin
wj |mj=1
?
ax,ay?A0
x<y
(dG(ax, ay) ?
m?
j=1
wjfj(ax, ay))2+??
m?
j=1
w2j ,
(2)
where m is the dimension of linguistic feature, ? is
a tradeoff parameter. Eq.(2) can be rewrote to its
matrix form as,
argmin
w
??d? fTw??2 + ? ? ?w?2 , (3)
where vector d contains the ground-truth distance of
all the aspect pairs. Each element corresponds to
the distance of certain aspect pair, and f is the corre-
sponding feature vector. The optimal solution of w
is given as
w? = (fT f + ? ? I)?1(fTd) (4)
143
where I is the identity metric.
The above learning algorithm can perform well
when sufficient training data (i.e., aspect (term)
pairs) is available. However, the initial hierarchy is
usually too coarse and thus cannot provide sufficient
information. On the other hand, abundant hand-
crafted hierarchies are available on the Web, such
as WordNet and Open Directory Project (ODP). We
here propose to leverage these external hierarchies
to assist semantic distance learning. A distance met-
ric w0 is learned from the external hierarchies us-
ing the above algorithm. Since w0 might be biased
to the characteristics of the external hierarchies, di-
rectly using w0 in our task may not perform well.
Alternatively, we use w0 as prior knowledge to as-
sist learning the optimal distance metric w from the
initial hierarchy. The learning problem is formulated
as follows,
argmin
w
??d? fTw??2 + ? ? ?w?2 + ? ? ?w? w0?2 ,
(5)
where ? and ? are tradeoff parameters.
The optimal solution of w can be obtained as
w? = (fT f + (? + ?) ? I)?1(fTd + ? ? w0). (6)
As a result, we can compute the semantic distance
between each two aspects according to Eq.(1).
2.5 Aspect Hierarchy Generation
Given the aspectsA = {a1, ? ? ? , ak} identified from
reviews and the initial hierarchy H0(A0,R0) ob-
tained from domain knowledge, our task is to con-
struct an aspect hierarchy to contain all the aspects
in A. Inspired by Yang and Callan (2009), we adopt
a multi-criteria optimization approach to incremen-
tally insert the aspects into appropriate positions in
the hierarchy based on multiple criteria.
Before going to the details, we first introduce an
information function to measure the amount of in-
formation carried in a hierarchy. An information
function Info(H) is defined as the sum of the se-
mantic distances of all the aspect pairs in the hierar-
chy (Yang and Callan, 2009).
Info(H(A,R)) =
?
x<y;ax,ay?A
d(ax, ay). (7)
Based on this information function, we then intro-
duce the following three criteria for aspect insertion:
minimum Hierarchy Evolution, minimum Hierarchy
Discrepancy and minimum Semantic Inconsistency.
Hierarchy Evolution is designed to monitor the
structure evolution of a hierarchy. The hierarchy is
incrementally hosting more aspects until all the as-
pects are allocated. The insertion of a new aspect a
into different positions in the current hierarchy H(i)
leads to different new hierarchies. Among these new
hierarchies, we here assume that the optimal one
H(i+1) should introduce the least changes of infor-
mation to H(i).
H?(i+1) = argmin
H(i+1)
?Info(H(i+1) ?H(i)). (8)
By plugging in Eq.(7) and using least square to
measure the information changes, we have,
obj1 = argmin
H(i+1)
(?x<y;ax,ay?Ai?{a} d(ax, ay)
??x<y;ax,ay?Ai d(ax, ay))2, (9)
Hierarchy Discrepancy is used to measure the
global changes of the structure. We assume a good
hierarchy should bring the least changes to the initial
hierarchy,
H?(i+1) = argmin
H(i+1)
?Info(H(i+1) ?H(0))
i + 1 . (10)
We then get,
obj2 = argmin
H(i+1)
1
i+1(
?
x<y;ax,ay?Ai?{a} d(ax, ay)
??x<y;ax,ay?A0 d(ax, ay))2. (11)
Semantic Inconsistency is introduced to quantify
the inconsistency between the semantic distance es-
timated via the hierarchy and that computed from
the feature functions. We assume that a good hier-
archy should precisely reflect the semantic distance
between aspects. For two aspects, their semantic
distance reflected by the hierarchy is computed as
the sum of adjacent distances along the shortest path
between them,
dH(ax, ay) =
?
p<q;(ap,aq)?SP (ax,ay)
d(ap, aq),
(12)
where SP (ax, ay) is the shortest path between the
aspects (ax, ay), (ap, aq) are the adjacent nodes
along the path.
144
We then define the following criteria to find the
hierarchy with minimum semantic inconsistency,
obj3 = argmin
H(i+1)
?
x<y;ax,ay?Ai?{a};
(dH(ax, ay)?d(ax, ay))2,
(13)
where d(ax, ay) is the distance computed based on
the feature functions in Section 2.4.
Through integrating the above criteria, the multi-
criteria optimization framework is formulated as,
obj = argmin
H(i+1)
(?1 ? obj1 + ?2 ? obj2 + ?3 ? obj3)
?1 + ?2 + ?3 = 1; 0 ? ?1, ?2, ?3 ? 1.
(14)
where ?1, ?2, ?3 are the tradeoff parameters.
To summarize, our aspect hierarchy generation
process starts from an initial hierarchy and inserts
the aspects into it one-by-one until all the aspects
are allocated. Each aspect is inserted to the op-
timal position found by Eq.(14). It is worth not-
ing that the insertion order may influence the result.
To avoid such influence, we select the aspect with
the least objective function value in Eq.(14) to in-
sert. Based on resultant hierarchy, the consumer re-
views are then organized to their corresponding as-
pect nodes in the hierarchy. We further prune out the
nodes without reviews from the hierarchy.
Moreover, we perform sentiment classification to
determine consumer opinions on various aspects. In
particular, we train a SVM sentiment classifier based
on the Pros and Cons reviews described in Section
2.3. We collect sentiment terms in the reviews as
features and represent reviews as feature vectors us-
ing Boolean weighting. Note that we define senti-
ment terms as those appear in the sentiment lexicon
provided by MPQA project (Wilson et al, 2005).
3 Implicit Aspect Identification
In this section, we apply the aspect hierarchy to the
task of implicit aspect identification. This task aims
to infer the aspects of reviews that do not explic-
itly express those aspects but actually comment on
them (Liu et al 2005). Take the review ?The phone
is too large? as an example, the task is to infer its
implicit aspect ?size.? It has been observed that the
reviews commenting on a same aspect usually use
some same sentiment terms (Su et al, 2008). There-
fore, sentiment term is an effective feature for identi-
fying implicit aspects. We here collect the sentiment
terms as features to represent each review into a fea-
ture vector. For each aspect node in the hierarchy,
we define its centroid as the average of its feature
vectors, i.e., the feature vectors of all the reviews
that are allocated at this node. We then calculate
the cosine similarity of each implicit-aspect review
to the centriods of all the aspect nodes, and allo-
cate the review into the node with maximum sim-
ilarity. As a result, the implicit aspect reviews are
grouped to their related aspect nodes. In other word,
their aspects are identified as the corresponding as-
pect nodes.
4 Evaluations
In this section, we evaluate the effectiveness of our
approach on aspect identification, aspect hierarchy
generation, and implicit aspect identification.
4.1 Data and Experimental Setting
The details of our product review corpus are given
in Table 1. This corpus contains consumer reviews
on 11 popular products in four domains. These
reviews were crawled from several prevalent fo-
rum Web sites, including cnet.com, viewpoints.com,
reevoo.com and gsmarena.com. All of the reviews
were posted between June, 2009 and Sep 2010. The
aspects of the reviews, as well as the opinions on
the aspects were manually annotated. We also in-
vited five annotators to construct the gold-standard
hierarchies for the products by providing them the
initial hierarchies and the aspects in reviews. The
conflicts between annotators were resolved through
their discussions. For semantic distance learning, we
collected 50 hierarchies from WordNet and ODP, re-
spectively. The details are shown in Table 2. We
listed the topics of the hierarchies in Appendix B
(submitted as supplementary material).
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
Table 1: Statistics of the reviews corpus, # denotes the
size of the reviews/sentences
145
Statistic WordNet ODP
Total # hierarchies 50 50
Total # terms 1,964 2,210
Average # depth 5.5 5.9
Total # related topics 12 16
Table 2: Statistics of the External Hierarchies
Figure 3: Evaluations on Aspect Identification. t-test, p-
values<0.05
We employed F1-measure, which is the combina-
tion of precision and recall, as the evaluation metric
for all the evaluations. For the evaluation on aspect
hierarchy, we defined precision as the percentage of
correctly returned parent-child pairs out of the to-
tal returned pairs, and recall as the percentage of
correctly returned parent-child pairs out of the to-
tal pairs in the gold standard. Throughout the ex-
periments, we empirically set ?1 = 0.4, ?2 = 0.3,
?3 = 0.3, ? = 0.4 and ? = 0.6.
4.2 Evaluations on Aspect Identification
We compared our approach against two state-of-the-
art methods: a) the method proposed by Hu and Liu
(2004), which is based on the association rule min-
ing, and b) the method proposed byWu et al (2009),
which is based on the dependency parser. The re-
sults are presented in Figure 3. On average, our
approach significantly outperforms Hu?s and Wu?s
method in terms of F1-measure by over 5.87% and
3.27%, respectively.
4.3 Evaluations on Aspect Hierarchy
4.3.1 Comparisons with the State-of-the-Arts
We compared our approach against four tra-
ditional hierarchy generation methods in the re-
searches on ontology learning, including a) pattern-
based method (Hearst, 1992) and b) clustering-based
method by Shi et al (2008), c) the method proposed
Figure 4: Evaluations on Aspect Hierarchy Generation. t-
test, p-values<0.05. w/ H denotes the methods with ini-
tial hierarchy, accordingly, w/o H refers to the methods
without initial hierarchy.
by Snow et al (2006) which was based on a proba-
bilistic model, and d) the method proposed by Yang
and Callan (2009). Since our approach and Yang?s
method can utilize initial hierarchy to assist hier-
archy generation, we evaluated their performance
with or without initial hierarchy, respectively. For
the sake of fair comparison, Snow?s, Yang?s and our
methods used the same linguistic features in Section
2.4.1.
Figure 4 shows the performance comparisons
of these five methods. We can see that our ap-
proach without using initial hierarchy outperforms
the pattern-based, clustering-based, Snow?s, and
Yang?s methods by over 17.9%, 19.8%, 2.9% and
6.1% respectively in terms of average F1-measure.
By exploiting initial hierarchy, our approach im-
proves the performance significantly. As compared
to the pattern-based, clustering-based and Snow?s
methods, it improves the average performance by
over 49.4%, 51.2% and 34.3% respectively. Com-
pared to Yang?s method with initial hierarchy, it
achieves 4.7% improvements on the average perfor-
mance.
The results show that pattern-based and
clustering-based methods perform poor. Pattern-
based method may suffer from the problem of low
coverage of patterns, especially when the patterns
are manually pre-defined, while the clustering-
based method (Shi et al, 2008) may sustain to the
bisection clustering mechanism which can only
generate a binary-tree. The results also illustrate
that our approach outperforms Snow?s and Yang?s
methods. By exploiting external hierarchies, our
146
Figure 5: Evaluations on the Impact of Initial Hierarchy.
t-test, p-values<0.05.
approach is able to derive reliable semantic distance
between aspects and thus improve the performance.
4.3.2 Evaluations on Effectiveness of Initial
Hierarchy
In this section, we show that even based on a small
part of the initial hierarchy, our approach can still
generate a satisfactory hierarchy. We explored dif-
ferent proportion of initial hierarchy, including 0%,
20%, 40%, 60% and 80% of the aspect pairs which
are collected top-down from the initial hierarchy. As
shown in Figure 5, the performance increases when
larger proportion of the initial hierarchy is used.
Thus, we can speculate that domain knowledge is
valuable in aspect hierarchy generation.
4.3.3 Evaluations on Effectiveness of
Optimization Criteria
We conducted a leave-one-out study to evaluate
the effectiveness of each optimization criterion. In
particular, we set one of the tradeoff parameters (?1,
?2, ?3) in Eq.(14) to zero, and distributed its weight
to the rest parameters averagely. From Figure 6, we
find that removing any optimization criterion would
degrade the performance on most products. It is in-
teresting to note that removing the third optimiza-
tion criterion, i.e., minimum semantic inconsistency,
slightly increases the performance on two products
(ipad touch and sony MP3). The reason might be
that the values of the three tradeoff parameters (em-
pirically set in Section 4.1) are not suitable for these
two products.
Figure 6: Evaluations of the Optimization Criteria. % of
change in F1-measure when a single criterion is removed.
t-test, p-values<0.05.
Figure 7: Evaluations on the Impact of Linguistic Fea-
tures. t-test, p-values<0.05.
4.3.4 Evaluations on Semantic Distance
Learning
In this section, we evaluated the impact of the fea-
tures and external hierarchies in semantic distance
learning. We investigated five sets of features as de-
scribed in Section 2.4.1, including contextual, co-
occurrence, syntactic, pattern and lexical features.
From Figure 7, we observe that the co-occurrence
and pattern features perform much better than con-
textual and syntactic features. A possible reason
is that co-occurrence and pattern features are more
likely to indicate parent-child aspect relationships,
while contextual and syntactic features are proba-
ble to measure sibling aspect relationships. Among
these features, the lexical features perform the worst.
The combination of all the features achieves the best
performance.
Next, we evaluated the effectiveness of external
hierarchies in semantic distance learning. We com-
pared the performance of our approach with or with-
out the external hierarchies. From Figure 8, we find
that by exploiting the external hierarchies, our ap-
147
Figure 8: Evaluations on the Impact of External Hierar-
chy. t-test, p-values<0.05.
proach improves the performance significantly. The
improvement is over 2.81% in terms of average F1-
measure. This implies that by using external hier-
archies, our approach can obtain effective semantic
distance, and thus improve the performance of as-
pect hierarchy generation.
Additionally, for sentiment classification, our
SVM classifier achieves an average F1-measure of
0.787 in the 11 products.
4.4 Evaluations on Implicit Aspect
Identification
To evaluate the performance of our approach on im-
plicit aspect identification, we collected 29,657 im-
plicit aspect review sentences on the 11 products
from the four forum Web sites introduced in Section
4.1. While most existing approaches for implicit as-
pect identification rely on hand-crafted rules (Liu,
2009), the method proposed in Su et al (2008) can
identify implicit aspects without hand-crafted rules
based on mutual clustering. Therefore, we adopt
Su?s method as the baseline here. Figure 9 illustrates
the performance comparison between Su?s and our
approach. We can see that our approach outperforms
Su?s method by over 9.18% in terms of average F1-
measure. This shows that our approach can iden-
tify the implicit aspects accurately by exploiting the
underlying associations among the sentiment terms
and each aspect in the hierarchy.
5 Related Work
Some researches treated review organization as a
multi-document summarization problem, and gen-
erated a summary by selecting and ordering sen-
tences taken from multiple reviews (Nishikawa et
Figure 9: Evaluations on Implicit Aspects Identification.
t-test, p-values<0.05
al., 2010). These works did not drill down to the
fine-grained level to explore the opinions on the
product aspects. Other researchers proposed to pro-
duce a summary covering consumer opinions on
each aspect. For example, Hu and Liu (2004) fo-
cused on extracting the aspects and determining
opinions on the aspects. However, their gener-
ated summary was unstructured, where the possible
relationships between aspects were not recognized
(Cadilhac et al, 2010). Subsequently, Carenini et
al. (2006) proposed to map the aspect to a user-
defined taxonomy, but the taxonomy was hand-
crafted which was not scalable.
Different from the previous works, we focus on
automatically generating an aspect hierarchy to hi-
erarchically organize consumer reviews. There are
some related works on ontology learning, which
first identify concepts from text, and then determine
parent-child relations between these concepts us-
ing either pattern-based or clustering-based methods
(Murthy et al, 2010). Pattern-based methods usu-
ally defined some lexical syntactic patterns to extract
the relations, while clustering-based methods mostly
utilized the hierarchical clustering methods to build
a hierarchy (Roy et al, 2006). Some works proposed
to integrate the pattern-based and clustering-based
methods in a general model, such as the probabilistic
model (Snow et al, 2006) and metric-based model
(Yang and Callan, 2009).
The researches on aspect identification are also
related to our work. Various aspect identification
methods have been proposed (Popescu et al, 2005),
including supervised methods (Liu el al., 2005), and
unsupervised methods (Mei et al, 2007). Different
148
features have been investigated for this task. For
example, Wu et al (2009) identified aspects based
on the features explored by dependency parser.
For implicit aspect identification, some works pro-
posed to define rules for identification (Liu el al.,
2005), while others suggested to automatically gen-
erate rules via mutual clustering (Su et al, 2008).
On the other hand, there are some related works
on sentiment classification (Pang and Lee, 2008).
These works can be categorized into four granu-
larities: document-level, sentence-level, aspect-level
and word-level sentiment classification (Liu, 2009).
Existing researches have been studied unsupervised
(Kim et al, 2004), supervised (Pang et al, 2002;
Pang et al, 2005) and semi-supervised classification
approaches (Goldberg et al, 2006; Li et al, 2009)
on these four levels.
6 Conclusions and Future Works
In this paper, we have developed a domain-assisted
approach to generate product aspect hierarchy by in-
tegrating domain knowledge and consumer reviews.
Based on the derived hierarchy, we can generate
a hierarchical organization of consumer reviews as
well as consumer opinions on the aspects. With such
organization, user can easily grasp the overview of
consumer reviews, as well as seek consumer reviews
and opinions on any specific aspect by navigating
through the hierarchy. We have further applied the
hierarchy to the task of implicit aspect identification.
We have conducted evaluations on 11 different prod-
ucts in four domains. The experimental results have
demonstrated the effectiveness of our approach. In
the future, we will explore other linguistic features
to learn the semantic distance between aspects, as
well as apply our approach to other applications.
Acknowledgments
This work is supported by NUS-Tsinghua Extreme
Search (NExT) project under the grant number: R-
252-300-001-490. We give warm thanks to the
project and anonymous reviewers for their valuable
comments.
References
P. Beineke, T. Hastie, C. Manning, and S. Vaithyanathan.
An Exploration of Sentiment Summarization. AAAI,
2003.
J. Beckham. The Cnet E-commerce Data set. Technical
Reports, 2005.
G. Carenini, R. Ng, and E. Zwart. Multi-document Sum-
marization of Evaluative Text. ACL, 2006.
A. Cadilhac, F. Benamara, and N. Aussenac-Gilles. On-
tolexical Resources for Feature based OpinionMining:
a Case-study. Ontolex, 2010.
P. Cimiano, A. Madche, S. Staab, and J. Volker. Ontology
Learning. Handbook on Ontologies, Springer, 2004.
P. Cimiano, A. Hotho, and S. Staab. Learning Concept
Hierarchies from Text Corpora using Formal Concept
Analysis. Artificial Intelligence, 2005.
P. Cimiano. Ontology Learning and Population from
Text: Algorithms, Evaluation and Applications.
Springer-Verlag New York, Inc. Secaucus, NJ, USA,
2006.
S. Dasgupta and V. Ng. Mine the Easy, Classify the Hard:
A Semi-supervised Approach to Automatic Sentiment
Classification. ACL, 2009.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. Un-
supervised Named-entity Extraction from theWeb: An
Experimental Study. Artificial Intelligence, 2005.
A. Esuli and F. Sebastiani. A Publicly Available Lexical
Resource for Opinion Mining. LREC, 2006.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
Pulse: Mining Customer Opinions from Free Text.
IDA, 2005.
R. Girju and A. Badulescu. Automatic Discovery of Part-
whole Relations Computational Linguistics, 2006.
A. Goldberg and X. Zhu. Seeing Stars When There
Aren?t Many Stars: Graph-based Semi-supervised
Learning for Sentiment Categorization. ACL, 2006.
M.A. Hearst. Automatic Acquisition of Hyponyms from
Large Text Corpora. Coling, 1992.
M. Hu and B. Liu. Mining and Summarizing Customer
Reviews. SIGKDD, 2004.
X. Hu, N. Sun, C. Zhang, and T.-S. Chua Exploiting
Internal and External Semantics for the Clustering of
Short Texts Using World Knowledge. CIKM, 2009.
S. Kim and E. Hovy. Determining the Sentiment of Opin-
ions. COLING, 2004.
A. C. Konig and E. Brill. Reducing the Human Overhead
in Text Categorization. KDD, 2006.
Z. Kozareva, E. Riloff, and E. Hovy. Semantic Class
Learning from the Web with Hyponym Pattern Link-
age Graphs. ACL, 2008.
T. Li, Y. Zhang, and V. Sindhwani. A Non-negative Ma-
trix Tri-factorization Approach to Sentiment Classifi-
cation with Lexical Prior Knowledge. ACL, 2009.
B. Liu, M. Hu, and J. Cheng. Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web. WWW,
2005.
149
B. Liu. Handbook Chapter: Sentiment Analysis and Sub-
jectivity. Handbook of Natural Language Processing.
Marcel Dekker, Inc. New York, NY, USA, 2009.
L.M.Manevitz andM. Yousef. One-class SVMs for Doc-
ument Classification. Machine Learning, 2002.
Q.Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. Topic
Sentiment Mixture: Modeling Facets and Opinions in
Weblogs. WWW, 2007.
X. Meng and H. Wang. Mining User Reviews: from
Specification to Summarization. ACL-IJCNLP, 2009.
K. Murthy, T.A. Faruquie, L.V. Subramaniam,
K.H. Prasad, and M. Mohania. Automatically
Generating Term-frequency-induced Taxonomies.
ACL, 2010.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Optimizing Informativeness and Readability for Senti-
ment Summarization. ACL, 2010.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Tech-
niques. EMNLP, 2002.
B. Pang and L. Lee. Seeing Stars: Exploiting Class Rela-
tionships for Sentiment Categorization with respect to
Rating Scales. ACL, 2005.
B. Pang and L. Lee. Opinion mining and sentiment anal-
ysis. Foundations and Trends in Information Retrieval,
2008.
HH. Pang, J. Shen, and R. Krishnan Privacy-Preserving,
Similarity-Based Text Retrieval. ACM Transactions
on Internet Technology, 2010.
A.M. Popescu and O. Etzioni. Extracting Product Fea-
tures and Opinions from Reviews. HLT/EMNLP,
2005.
H. Poon and P. Domingos. Unsupervised Ontology In-
duction from Text. ACL, 2010.
G. Qiu, B. Liu, J. Bu, and C. Chen. Expanding Domain
Sentiment Lexicon through Double Propagation. IJ-
CAI, 2009.
S. Roy and L.V. Subramaniam. Automatic Generation
of Domain Models for Call Centers from Noisy Tran-
scriptions. ACL, 2009.
B. Shi and K. Chang. Generating a Concept Hierarchy
for Sentiment Analysis. SMC, 2008.
R. Snow and D. Jurafsky. Semantic Taxonomy Induction
from Heterogenous Evidence. ACL, 2006.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
I. Titov and R. McDonald. A Joint Model of Text and
Aspect Ratings for Sentiment Summarization. ACL,
2008.
P. Turney. Thumbs up or thumbs down? Semantic Orien-
tation Applied to Unsupervised Classification of Re-
views. ACL, 2002.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. Phrase Depen-
dency Parsing for Opinion Mining. ACL, 2009.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
H. Yang and J. Callan A Metric-based Framework for
Automatic Taxonomy Induction. ACL, 2009.
S. Ye and T.-S. Chua. Learning Object Models from
Semi-structured Web Documents. IEEE Transactions
on Knowledge and Data Engineering, 2006.
J. Yi, T. Nasukawa, W. Niblack, and R. Bunescu. Senti-
ment Analyzer: Extracting Sentiments about a Given
Topic using Natural Language Processing Techniques.
ICDM, 2003.
L. Zhuang,F. Jing, and X.Y. Zhu Movie Review Mining
and Summarization CIKM, 2006.
150
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 391?401, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Answering Opinion Questions on Products by Exploiting Hierarchical
Organization of Consumer Reviews
Jianxing Yu, Zheng-Jun Zha, Tat-Seng Chua
School of Computing
National University of Singapore
{jianxing, zhazj, chuats}@comp.nus.edu.sg
Abstract
This paper proposes to generate appropriate
answers for opinion questions about prod-
ucts by exploiting the hierarchical organiza-
tion of consumer reviews. The hierarchy orga-
nizes product aspects as nodes following their
parent-child relations. For each aspect, the re-
views and corresponding opinions on this as-
pect are stored. We develop a new framework
for opinion Questions Answering, which en-
ables accurate question analysis and effective
answer generation by making use the hierar-
chy. In particular, we first identify the (ex-
plicit/implicit) product aspects asked in the
questions and their sub-aspects by referring
to the hierarchy. We then retrieve the corre-
sponding review fragments relevant to the as-
pects from the hierarchy. In order to gener-
ate appropriate answers from the review frag-
ments, we develop a multi-criteria optimiza-
tion approach for answer generation by simul-
taneously taking into account review salience,
coherence, diversity, and parent-child rela-
tions among the aspects. We conduct eval-
uations on 11 popular products in four do-
mains. The evaluated corpus contains 70,359
consumer reviews and 220 questions on these
products. Experimental results demonstrate
the effectiveness of our approach.
1 Introduction
With the rapid development of E-commerce, most
retail websites encourage consumers to post reviews
to express their opinions on the products. For exam-
ple, the review ?The battery of Nokia N95 is amaz-
ing.? reveals positive opinion on the aspect ?bat-
Figure 1: Overview of product opinion-QA framework
tery? of product Nokia N95. An aspect here refers
to a component or an attribute of a certain prod-
uct. Numerous consumer reviews are now available
online, and these reviews contain rich opinionated
information on various aspects of products. They
are naturally a valuable resource for answering opin-
ion questions about products, such as ?How do peo-
ple think about the battery of Nokia N95?? Opin-
ion Question Answering (opinion-QA) on products
seeks to uncover consumers? thinking and feeling
about the products or aspects of products. It is dif-
ferent from traditional factual QA, where the ques-
tions ask for the fact, such as ?Where is the capital
of United States?? and the answer is ?Washington,
D.C.?
For a product opinionated question, the answer
should not be just a best answer. It should reflect the
opinions of various segments of users, and incorpo-
391
rate both positive and negative viewpoints. Hence
the answer should be a summarization of public
opinions and comments on the product or specific
aspect asked in the question (Jiang et al2010).
In addition, it should also include public opinions
and comments on the sub-aspects. Such answers
would help users to understand the inherent reasons
of the opinions on the asked aspect. For exam-
ple, the question ?What do people think the cam-
era of Nokia 5800?? asks for public positive and
negative opinions on the aspect ?camera? of prod-
uct ?Nokia 5800.? The summarization of opinions
on the sub-aspects such as ?lens? and ?resolution?
would help users better understand that the public
complaints on the aspect ?camera? are due to the
poor ?lens? and/or low ?resolution.? Moreover, the
answer should be presented following the general-
to-specific logic, i.e., from general aspects to spe-
cific sub-aspects. This makes the answer easier to
understand by the users (Ouyang et al2009).
Current Opinion-QA methods mainly include
three components, including question analysis that
identifies aspects and opinions asked in the ques-
tions, answer fragment retrieval, and answer gen-
eration which summarizes the retrieved fragments
(Lloret et al2011). Although existing methods
show encouraging performance, they are usually not
able to generate satisfactory answers due to the fol-
lowing drawbacks. First, current methods often
identify aspects as the noun phrases in the questions.
However, noun phrases contain noises that are not
aspects. This gives rise to imprecise aspect identifi-
cation. For example, in the question ?What reasons
can I persuade my wife that people prefer the battery
of Nokia N95?? noun phrases ?wife? and ?people?
are not aspects. Moreover, current methods relied
on noun phrases are not able to reveal the implicit
aspects, which are not explicitly asked in the ques-
tions. For example, the question ?Is iPhone 4 expen-
sive?? asks about the aspect ?price?, but the term
?price? does not appear in the question. Second,
current methods cannot discover sub-aspects of the
asked aspect due to its ignorance of parent-child re-
lations among aspects. Third, the answers generated
by the existing methods do not follow the general-to-
specific logic, leading to difficulty in understanding
the answers.
To overcome these problems, we can resort to
the hierarchical organization of consumer reviews
on products. As illustrated in Figure 2, the hier-
archy organizes product aspects as nodes, follow-
ing their parent-child relations. For each aspect, the
reviews and corresponding opinions on this aspect
are stored. Such hierarchy can naturally facilitate to
identify aspects asked in questions. While explicit
aspects can be recognized by referring to the hier-
archy, implicit aspects can be inferred based on the
associations between sentiment terms and aspects in
the hierarchy (Yu et al2011). The sentiment terms
are discovered from the reviews on corresponding
aspects. Moreover, by following the parent-child re-
lations in the hierarchy, sub-aspects of the asked as-
pect can be directly acquired, and the answers can
present aspects from general to specific.
Motivated by the above observations, we propose
to exploit the hierarchical organization of consumer
reviews for product opinion-QA. As illustrated in
Figure 1, our framework first organizes consumer
reviews of a certain product into a hierarchical or-
ganization. The resulting hierarchy is in turn used
to help question analysis and relevant review frag-
ments retrieval. In order to generate appropriate
answers from the retrieved fragments, we develop
a multi-criteria optimization approach by simulta-
neously taking into account review salience, co-
herence, and diversity. The parent-child relations
among aspects are also incorporated into the ap-
proach to ensure the answers be general-to-specific.
We conduct evaluations on 11 popular products in
four domains. The evaluated corpus contains 70,359
consumer reviews and 220 questions on these prod-
ucts. More details of the dataset are discussed in
Section 4. Experimental results to demonstrate the
effectiveness of our approach.
The main contributions of this paper include,
? We propose to exploit the hierarchical organi-
zation of consumer reviews for answering opin-
ion questions on products.
? With the help of the hierarchy, our pro-
posed framework can accurately identify (ex-
plicit/implicit) aspects asked in the questions,
and the corresponding sub-aspects.
? We develop a multi-criteria optimization ap-
proach to generate informative, coherent, di-
verse and general-to-specific answers.
392
Figure 2: Hierarchical organization for Nokia N95
The rest of this paper is organized as follows. Sec-
tion 2 introduces the components of hierarchical or-
ganization of reviews, question analysis, and answer
fragment retrieval. Section 3 elaborates the multi-
criteria optimization approach for answer generation
. Section 4 presents experimental details, while Sec-
tion 5 reviews related works. Finally, Section 6 con-
cludes this paper with future works.
2 Hierarchical Organization, Question
Analysis, and Answer Fragment
Retrieval
Let R = {r1, ? ? ? , r|R|} denote a collection of con-
sumer reviews of a certain product. Each review re-
flects consumer opinions on the product and/or prod-
uct aspects. Let q denote an opinion question, which
asks for public opinions on a product or some as-
pects of the product. The task is to retrieve the opin-
ionated review fragments relevant to the asked prod-
uct/product aspects, and summarize these fragments
to form an appropriate answer to question q.
Next, we introduce the components of hierarchi-
cal organization that organizes consumer reviews
into a hierarchy, question analysis which identifies
the products/aspects and opinions asked in the ques-
tions, and answer fragment retrieval that retrieves re-
view fragments relevant to the questions.
2.1 Hierarchical Organization of Reviews
We employ the method proposed by Yu et al2011)
to organize consumer reviews of a product into a hi-
erarchical organization. As shown in Figure 2, the
hierarchy organizes product aspects as nodes, fol-
lowing their parent-child relations. In particular, this
method first automatically acquires an initial aspect
hierarchy from the domain knowledge and identifies
aspects commented in the reviews. It then incremen-
tally inserts the identified aspects into appropriate
positions in the initial hierarchy, and finally obtains
an aspect hierarchy that allocates all the newly iden-
tified aspects. The consumer reviews are then orga-
nized to their corresponding aspect nodes in the hi-
erarchy. Sentiment classification is then performed
to determine consumer opinions on the reviews.
The reported performance of Yu et al2011)
on aspect identification, aspect hierarchy generation
and sentiment classification are 0.731, 0.705, 0.787
in terms of average F1-measure, respectively.
2.2 Question Analysis and Answer Fragment
Retrieval
Question analysis consists of five sub-tasks: recog-
nizing product asked in the question; identifying as-
pects in the question; classifying opinions that the
question asks for (the asked opinion could be posi-
tive, negative or both); identifying the question type
(e.g. asking for public opinions, or the reason of
the opinions, etc.); and identifying the question form
(i.e. comparative question or single form question).
Recognizing the product: A name entity recog-
nizer 1 is trained to recognize the product name. In
particular, we collect 420 auxiliary questions from
Yahoo!Answer 2, and manually annotate the prod-
uct names (submitted as supplementary material in
Appendix A). A name entity recognizer for product
is learned on these data, with unigrams and POS tags
as features. Given a testing question, the recognizer
predicts each word as B, I, E or O, where B, I, E de-
note the begin, internal, and end of a product name
respectively, and O corresponds to other words.
Identifying aspects: As aforementioned, simply
extracting the noun phrases as aspects would import
noises. Also, some ?implicit? aspects do not ex-
1http://nlp.stanford.edu/software/CRF-NER.shtml
2http://answers.yahoo.com
393
plicitly appear in the reviews. One simple solution
for these problems can resort to the review hierar-
chy. The hierarchy has organized product aspects,
which can be used to filter the noise noun phrases
for accurately identifying the explicit aspects. For
the implicit aspects, we observe they are usually
modified by some peculiar sentiment terms (Su et
al., 2008). For example, the aspect ?size? is often
modified by the sentiment terms such as ?large?,
but seldom by the terms such as ?expensive.? Thus,
there are some associations between the aspects and
sentiment terms. Such associations can be learned
from the hierarchy and leveraged to infer the im-
plicit aspects (Yu et al2011). In order to simul-
taneously identify the (explicit/implicit) aspects, we
adopt a hierarchical classification technique. The
technique simultaneously learns to identify explicit
aspects, and discovers the associations between as-
pects and sentiment terms by multiple classifiers. In
particular, given a testing question, we identify its
aspect by hierarchically classify (Silla et al2011) it
into the appropriate aspect node of a particular prod-
uct hierarchy. The classification greedily searches a
path in the hierarchy from top to down. The search
begins at the root node, and stops at the leaf node
or a specific node where the relevance score is lower
than a pre-defined threshold. The relevance score on
each node is determined by a SVM classifier. Mul-
tiple SVM classifiers are learned on the hierarchy,
one distinct classifier for a node. The reviews that
are stored in the node and its child-nodes are used
as training samples. We employ the features of noun
terms, and sentiment terms in the sentiment lexicon
provided by MPQA project (Wilson et al2005).
Classifying the opinions: Given a set of testing
questions, we first distinguish the opinion questions
from the factual ones (Yu et al2003). Since the
opinion questions often contain one or more senti-
ment terms, we classify them by employing the sen-
timent terms in the sentiment lexicon provided from
MPQA project (Wilson et al2005). Subsequently,
we learn a SVM sentiment classifier to determine the
opinion polarity of the opinion questions. In partic-
ular, the reviews and corresponding opinions stored
in the hierarchy are used as training samples, which
are represented by the unigram features.
Identifying the question type: Opinion questions
are often categorized into four types (Ku et al
2007),
? Attitude question, asking for public opinion on
a product or product aspect, such as ?What do
people think iPhone 3gs??
? Reason question, asking for the reason of pub-
lic opinion on a product or product aspect, such
as ?Why do people like iPhone 3gs??
? Target question, asking for the object in the
public opinion, such as ?Which phone is better
than Nokia N95??
? Yes/No question, asking for whether a state-
ment is correct, such as ?Is Nokia N95 bad??
We formulate the question type identification as a
multi-class classification problem. A multi-class
SVM classifier 3 is learned for the classification. We
collect 420 auxiliary questions from Yahoo!Answer
and manually annotate their types (submitted as sup-
plementary material in Appendix B). These ques-
tions are used for training, with POS tags and ques-
tion words (i.e. why, what, how, do, is) as features.
Identifying the question form: Question form in-
cludes single and comparative. A question is viewed
as comparative if it contains comparative adjectives
and adverbs (e.g. cheaper, etc.), otherwise as the sin-
gle form (Moghaddam et al2011). The POS tags
are exploited to detect comparative adjectives (i.e.
tag ?JJR?) and adverbs (i.e. tag ?RBR?).
After analyzing the question, we retrieve all re-
view sentences on the asked aspect and all its sub-
aspects from a certain product hierarchy, and choose
the ones relevant to the opinion asked in the ques-
tion. For the single form question, we view the
retrieved sentences as the answer fragments. For
the comparative questions, we select comparative
sentences on the compared products from the re-
trieved sentences, and treat them as the answer frag-
ments. Subsequently, question type is used to define
the template for the answers. In particular, for the
questions asking for reason and attitude, we gener-
ate the answers by summarizing corresponding an-
swer fragments. For questions seeking for a target
as the answer, we output the product names based
on the majority voting of the opinions in the re-
trieved answer fragments. For the yes/no questions,
we first generate the ?yes/no? answer based on the
3http://svmlight.joachims.org/svm multiclass.html
394
consistency between the asked opinions and the ma-
jor opinions in the answer fragments, and then sum-
marize these fragments to form the answers.
3 Answer Generation
Answer generation aims to generate an appropriate
answer for a given opinion question based on the
retrieved answer fragments, i.e., review sentences.
An answer is essentially a sequence of sentences.
Hence, the task of answer generation is to select sen-
tences from the retrieved answer fragments and or-
der them appropriately. We formulate this task into
a multi-criteria optimization problem. We incorpo-
rate multiple criteria in the answer generation pro-
cess, including answer salience, coherence, and di-
versity. The parent-child relations between aspects
is also incorporated to ensure the answer follow the
general-to-specific logic. In the next subsections, we
will introduce details of the proposed multi-criteria
optimization approach.
3.1 Formulation
We first introduce the multiple criteria and then
present the optimization problem.
Salience is used to measure the representative-
ness of the answer. A good answer should consist
of salient review sentences. Let S denote the set
of retrieved sentences. We define a binary variable
si ? {0, 1} to indicate the selection of sentence i
for the answer, i.e. si = 1 (or 0) indicates that si is
selected (or not). Let ?i denote the salience of sen-
tence i. The estimation of ?i will be described in
Section 3.2. The salience score of the answer (i.e.,
a set of sentences) is computed by summing up the
scores of all its constituent sentences, as
?
i?S ?isi.
Coherence is used to quantify the readability of
an answer. To make the answer readable, the con-
stituent sentences in the answer should be ordered
properly. That is, the adjacent sentences should
be coherent. We define ei,j ? {0, 1} to indicate
whether the sentences i and j are adjacent in the an-
swer; where ei,j = 1 (or 0) means they are (or not)
adjacent. The coherence between two adjacent sen-
tences is measured by cij . The estimation of cij will
be described in Section 3.3. As aforementioned, the
answer is expected to be presented in a general-to-
specific manner, i.e. from general aspects to specific
sub-aspects. We define hi,j in Eq.1 to measure the
general-to-specific coherence of sentences i and j.
hi,j =
{
e?
1
leveli?levelj ; if leveli ?= levelj ;
1; otherwise,
(1)
where leveli denotes level position of the aspect
commented in sentence i by referring to the hi-
erarchy, with the root level being 0. The coher-
ence score of the answer is computed by sum-
ming up the scores of all its adjacent sentences as,
?
j?S
?
i?S hi,jci,jei,j .
Diversity. A good answer should diversely cover
all the important information. We introduce a ma-
trix M in Eq.2 to measure the pairwise diversities
among sentences. Mij corresponds to the diversity
between sentences i and j. When sentences i and
j comment on the same aspects, Mij will favor to
select the pair of sentences that discusses on diverse
content (i.e. low similarity). Otherwise, the pair of
sentences commented on different aspects is viewed
to be diverse, and Mij is set as a constant bigger
than one.
Mij =
{
1? similar(i, j) if i, j commented on same aspect
? otherwise,
(2)
where ? is a constant 4.
Multi-Criteria Optimization We integrate the
above criteria into the multi-criteria optimization
formulation,
max{?1 ?
?
i?S ?isi + ?2 ?
?
j?S
?
i?S hi,jci,jei,j
+ ?3 ?
?
j?S
?
i?S siMij ;
{
si, ei,j ? {0, 1},?i, j;
?1 + ?2 + ?3 = 1, 0 ? ?1, ?2, ?3 ? 1,
(3)
where ?1, ?2, ?3 are the trade-off parameters.
We further incorporate the following constrains
into the optimization framework, so as to derive ap-
propriate answers.
? The length of the answer is up to K,
?
i?S
lisi ? K, (4)
where li is the length of sentence i.
? When sentence i is not selected (i.e. si = 0),
the adjacency between any sentence to i is set
4Empirically set to 10 in the experiment.
395
to zero (i.e.
?
i?S ei,j =
?
i?S ej,i = 0).
When sentence i is selected, there are two sen-
tences adjacent to sentence i in the answer, one
before i and another after i. (i.e.
?
i?S ei,j =
?
i?S ej,i = 1).
?
i?S
ei,j =
?
i?S
ej,i = sj , ?j. (5)
? In order to avoid falling into a cycle in sentence
selection, we employ the following constraints
(Deshpande et al2009).
?
i?S f0,i = n + 1;
?
i?S fi,n+1 ? 1;
?
i?S fi,j ?
?
i?S fj,i = sj , ?j;
0 ? fi,j ? (n + 1) ? ei,j , ?i, j,
(6)
where the variable fi,j is an integer to number
the selected adjacent sentences from 1 to n+1,
and the first selected sentence is numbered f0,i
= n + 1. If the last selected sentence obtains a
number fi,n+1 which is bigger then 1, then the
selection has no cycle.
Solution
Given the salience weights ?i|Si=1, and coherence
weights ci,j |Si,j=1, the above multi-criteria optimiza-
tion problem can be solved by Integer Linear Pro-
gramming (Schrijver et al1998). The optimal so-
lutions si|Si=1 and ei,j |Si,j=1 indicate the selected sen-
tences and the order of them. In the next subsec-
tions, we will introduce the estimations of ?i|Si=1
and ci,j |Si,j=1.
3.2 Salience Weight Estimation
The salience weight of sentence i is formulated as
?i =
?G
g=1 ?g(i)/G, where ?(i) denotes the mea-
surement for the importance of sentence i. We de-
fine seven measurements (i.e. G = 7) below.
Helpfulness: Many forum websites provide a
helpfulness score, which is used to rate the quality
of a review. The sentences that come from helpful
reviews are often representative (Mizil et al2009).
We compute ?(i) of sentence i by using helpfulness
score from its host review.
Timeliness: The new coming sentence often con-
tains more updated and useful information (Liu et
al., 2008). ?(i) is the post time of sentence i. We
normalize it to [0, 1].
Grammaticality: The grammatical sentence is
often more readable. We employ the method in
Agichtein et al2008) to calculate the grammar
score. In particular, ?(i) is calculated by the KL-
divergence between language models of sentence i
to Wikipedia articles.
Position: The first sentence in a review is usu-
ally informative (He et al2011). ?(i) is computed
based on the position of the sentence in the review,
i.e. ?(i) = 1/positioni.
Aspect Frequency: The sentence that contains the
frequent aspects is often salient (Nishikawa et al
2010). Hence, ?(i) is computed as the sum of the
frequency for aspects in sentence i.
Centroid Distance: As aforementioned, review
sentences are stored in the corresponding aspect
nodes in the hierarchy. The sentence that is close to
the centroid of the reviews stored in an aspect node
is more likely to be salient (Erkan et al2004). ?(i)
is computed as the Cosine similarity between sen-
tence i to the corresponding review cluster centroid
based on the unigram features.
Local Density: The sentence would be informa-
tive when it is in the dense part of the aspect node
in the feature space (Scott et al1992). We em-
ploy Multivariate Kernel Density Estimation to es-
timate the density. We first represent all the sen-
tences stored in each node into feature vectors, with
unigram as features. The density of a sentence is
then calculated as ?(x) =
?n
i=1 KH(x? xi)/n,
where x denotes the feature vector of sentence i,
n is the size of sentences stored in the node, and
KH(x) = (2?)?1/2 exp(?1/2(xTx)) represents
the Gaussian kernel.
3.3 Coherence Weight Estimation
The coherence ci,j between sentences i and j is
formulated as ci,j = ? ? ?(i, j), where ? is a
weight vector, and ?(i, j) denotes the feature func-
tion. ?(i, j) takes two sentences i and j as input,
and outputs a vector with each dimension indicating
the present/absent of a feature. In order to capture
the sequential relations among sentences, we utilize
features as the Cartesian product over the terms of
N-gram (N=1,2) and POS tags generated from sen-
tences i and j (Lapata et al2003).
To learn the weight vector ?, we employ
the Passive-Aggressive algorithm (Crammer et al
396
2006). It is an online learning algorithm, so that we
can update the weight when more consumer reviews
are available. The algorithm takes up one training
sample and outputs the solution that has the highest
score under the current weight. If the output differs
from training samples, the weight vector is updated
according to Eq.7. Since the consumer reviews often
include multiple sentences, we can directly use the
adjacency of these sentences as training samples. In
particular, we treat the adjacent sentence pairs in the
reviews as training samples (i.e. ci,j = 1).
min
?
??i+1 ? ?i
?
?
{
?i+1 ??(p,q?)? ?i+1 ??(p, q?) ? ?(q?,q?);
?(q?,q?) = 2?T (q?,q
?)
m(m?1)/2 ,
(7)
where ?i is the current weight vector and ?i+1 is
the updated vector, q? and q? are the gold standard
and predicted sequence of sentences, respectively, p
denotes a set of sentences, ?(?) is the feature func-
tion on the whole feature space (i.e.
?
?(?)), ?(?, ?)
is a Kendall?s tau lost function (Lapata et al2006),
T (?, ?) represents the number of inversion operations
that needs to bring q? to q?, and m denotes the num-
ber of sentences.
4 Evaluations
In this section, we evaluate the effectiveness of the
proposed approach, in terms of question analysis
and answer generation.
4.1 Data Set and Experimental Settings
We employed the product review dataset used in Yu
et al2011) as corpus. As illustrated in Table 1, the
dataset contained 70,359 reviews about 11 popular
products in four domains. In addition, we created
220 questions for these products by referring to real
questions in Yahoo!Anwser service. We corrected
the typos and grammar errors for these real ques-
tions. Each product contains 15 opinion questions
and 5 factual questions, respectively. All questions
were shown in Appendix C in supplementary mate-
rial. Three annotators were invited to generate the
gold standard. Each question was labeled by two
annotators. The labels include product name, prod-
uct aspect, opinion, question type and question form.
The average inter-rater agreement in terms of Kappa
statistics is 89%. These annotators were then invited
to read the reviews, and create the ground truth an-
swers by selecting and ordering some review sen-
tences. Such process is time consuming and labor-
intensive. We speed up the annotation process as fol-
lows. We first collected all the review sentences in
the answers generated by three evaluated methods to
be discussed in Section 4.3.1. In addition, we sam-
pled the top-N (N=20) sentences on each asked as-
pect and its sub-aspects respectively, where the sen-
tences were ranked based on their salient weights in
Section 3.2. We then provided such subset of review
sentences to the three annotators, and let them indi-
vidually create an answer of up to 100 words (i.e.
K=100) for each question.
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
Table 1: Statistics of the product review dataset, # denotes
the number of the reviews/sentences.
We employed precision (P), recall (R) and F1-
measure (F1) as the evaluation metric for question
analysis, and utilized ROUGE (Lin et al2003) as
the metric to evaluate the quality of answer gener-
ation. ROUGE is a widely accepted standard for
summarization, which measures the quality of the
summarized answers by counting the overlapping N-
grams between the answers generated by machine
and human, respectively. In the experiment, we
reported the F1-measure of ROUGE-1, ROUGE-2
and ROUGE-SU4, which count the overlapping un-
igrams, bigrams and skip-4 bigrams respectively.
ROUGE-1 can measure informativeness of the an-
swers, while higher order ROUGE-N (N=2,4) cap-
tures the matching of subsequences, which can mea-
sure the fluency and readability of the answers. For
the trade-off parameters, we empirically set ?1 =
0.4, ?2 = 0.3 and ?3 = 0.3.
4.2 Evaluations on Question Analysis
We first evaluated the performance of product recog-
nition, opinion/factual question classification, opin-
ion classification, question type and question form
identification. The experimental results are shown
397
in Table 2. The results show that traditional methods
achieve encouraging performance on the aforemen-
tioned tasks.
Evaluated Topics P R F1
Product recognition 0.755 0.618 0.680
Opinion/factual 0.897 0.895 0.893
Opinion classification 0.755 0.745 0.748
Question type 0.800 0.775 0.783
Question form 0.910 0.903 0.905
Table 2: Performance of question analysis.
Methods P R F1
Our method 0.851* 0.763* 0.805*
Balahur?s method 0.825 0.400 0.538
Table 3: Performance of aspect identification for question
analysis. * denotes the results (i.e. P , R, F1) are tested
for statistical significance using T-Test, p-values<0.05.
Methods P R F1
Our method 0.726* 0.643* 0.682*
Su?s method 0.689 0.571 0.625
Table 4: Performance of implicit aspect identification for
question analysis. T-Test, p-values<0.05
We next examined the performance of our ap-
proach on aspect identification. The method pro-
posed by Balahur et al2008) was reimplemented
as the baseline, which identifies aspects based on
noun phrase extraction. This method achieved good
performance on the opinion QA task in TAC 2008
and was employed in subsequent works. As demon-
strated in Table 3, our approach significantly outper-
forms Balahur?s method by over 49.4% in terms of
average F1-measure. A probable reason is that Bal-
ahur?s method relies on noun phrases, which may
mis-identify some noise noun phrases as aspects,
while our approach performs hierarchical classifica-
tion based on the hierarchy, which can leverage the
prior knowledge encoded in the hierarchy to filter
out the noise and obtain accurate aspects.
Moreover, we evaluated the effectiveness of our
approach on implicit aspect identification. The 70
implicit aspect questions in our question corpus
were used here. The method proposed by Su et al
(2008) was reimplemented as the baseline. It identi-
fies implicit aspects by mutual clustering, and it was
Figure 3: Evaluations on multiple optimization criteria
in terms of ROUGE-1, ROUGE-2, and ROUGE-SU4, re-
spectively.
evaluated in Yu et al2011). As shown in Table 4,
our approach significantly outperforms Su?s method
by over 9.1% in terms of average F1-measure. The
results show that the hierarchy can help to identify
implicit aspects by exploiting the underlying associ-
ations among sentiment terms and aspects.
Methods ROUGE1 ROUGE2 ROUGE-SU4
Our method 0.364* 0.137* 0.138*
Li?s method 0.127 0.043 0.049
Lloret?s method 0.149 0.058 0.065
Table 5: Performance of answer generation. T-Test, p-
values<0.05.
4.3 Evaluations on Answer Generation
4.3.1 Comparisons to the State-of-the-Arts
We compared our multi-criteria optimization ap-
proach against two state-of-the-arts methods: a) the
398
method presented in Li et al2009), which selects
some retrieved sentences to generate the answers
based on a graph-based algorithm; b) the method
proposed by Lloret et al2011) that forms the an-
swers by re-ranking the retrieved sentences.
As shown in Table 5, our approach outperforms
Li?s method and Lloret?s method by the significant
absolute gains of over 23.7%, and 21.5% respec-
tively, in terms of average ROUGE-1. It improves
the performance over these two methods in terms
of average ROUGE-2 by the absolute gains of over
9.41% and 7.87%, respectively; and in terms of
ROUGE-SU4 by the absolute gains of over 8.86%
and 7.31%, respectively. By analyzing the results,
we find that the improvements come from the use
of the hierarchical organization and the answer gen-
eration algorithm which exploits multiple criteria,
especially the parent-child relation among aspects.
In addition, our approach can generate the answers
by following the general-to-specific logic, while Li?s
and Lloret?s methods fail to do so due to their igno-
rance of parent-child relations among aspects.
4.3.2 Evaluations on the Effectiveness of
Multiple Criteria
We further evaluated the effectiveness of each op-
timization criterion by tuning the trade-off parame-
ters (i.e. ?1, ?2, and ?3). We fixed ?1 as a con-
stant in [0, 1] with 0.1 as an interval, and updated ?2
from 0 to 1 ? ?1, ?3 = 1 ? ?1 ? ?2, correspond-
ingly. The performance change is shown in Figure
3 in terms of ROUGE-1, ROUGE-2, and ROUGE-
SU4, respectively. The best performance is achieved
at ?1 = 0.4, ?2 = 0.3, ?3 = 0.3. We observe the
performance drops dramatically when any parame-
ter (i.e. ?1, ?2, ?3) is close to 0 (i.e. remove any of
the corresponding criterion). Thus, we can conclude
that all the criteria are useful in answer generation.
We also find that the performance change is sharp
when ?1 changes. This indicates that the salience
criterion is crucial for answer generation.
Table 6 shows the exemplar answers generated by
our approach. Each answer first gives the statis-
tic of positive and negative reviews. This helps
user to quickly get an overview of public opin-
ions. The summary of relevant review sentences
is then presented in the answer. The answer di-
versely comments the asked aspect and all its avail-
able sub-aspects following the general-to-specific
logic. Moreover, we feel that the answers are in-
formative and readable.
5 Related Works
In this section, we review existing works related
to the four components of our approach, including
organization of reviews, question analysis, answer
fragment retrieval, and answer generation.
For organization of reviews, Carenini et al2006)
proposed to organize the reviews by a hand-crafted
taxonomy, which was not scalable. Yu et al2011)
exploited the domain knowledge and consumer re-
views to automatically generate a hierarchy for or-
ganizing consumer reviews.
Question analysis often has to distinguish the
opinion question from the factual one, and find the
key points asked in the questions, such as the prod-
uct aspect and product name. For example, Yu et
al. (2003) proposed to separate opinions from facts
at both document and sentence level, and determine
the polarity on the opinionated sentences in the an-
swer documents. Similarly, Somasundaran et al
(2007) utilized a SVM classifier to recognize opin-
ionated sentences. The paper argued that the sub-
jective types (i.e. sentiment and arguing) can im-
prove the performance of opinion-QA. Later, Ku et
al. (2007) proposed a two-layered classifier for ques-
tion analysis, and retrieved the answer-fragments by
keyword matching. In particular, they first identified
the opinion questions, and classified them into six
predefined question types, including holder, target,
attitude, reason, majority, and yes/no. These ques-
tion types and corresponding polarity on the ques-
tions were used to filter non-relevant sentences in
the answer fragments. F1-measure was employed as
the evaluation metric.
For the topic of answer generation in opinion-QA,
Li et al2009) formulated it as a sentence ranking
task. They argued that the answers should be simul-
taneously relevant to topics and opinions asked in
the questions. They thus designed the graph-based
methods (i.e. PageRank and HITS) to select some
high-ranked sentences to form answers. They first
built a graph on the retrieved sentences, with each
sentence as the node, and the similarity (i.e. Co-
sine similarity) between each sentences pair as the
399
Question 1: What reasons do people give for preferring iPhone 3gs?
There are 9,928 opinionated reviews about product ?iphone 3gs?, with 5,717 positive and 4,221 negative reviews.
This phone is amazing and I would recommend it to anyone. It looks funky and cool. It is worth the money. It?s great
organiser, simple easy to use software. It is super fast, excellent connection via wifi or 3G. It is able to instantly access email.
It?s amazing and has so many free apps. The design is so simple and global. The hardware is good and reliable. The camera is
a good and colors are vibrant. The touch screen is user friendly and the aesthetics are top notch. Battery is charged quickly,
and power save right after stop using.
Question 2: Does anyone think it is expensive to get a iPhone 3GS?
Yes.
There are 2,645 opinionated reviews on aspect ?price? about product ?iphone 3gs?, with 889 positive and 1,756 negative
reviews.
Throw the costly phone, apple only knows to sell stupid stuff expensively. Don?t fool yourself with iPhone 3gs, believing that it
costs much by Apple luxurious advertising. Apple is so greedy and it just wants to earn easy & fast money by selling its
techless product expensively. The phone will charge once you insert any sim card. iPhone 3gs is high-priced due to the
capacitive and Apple license. You need to pay every application at the end it costs too much. The network provider will make
up some of the cost of the phone on your call charges.
Table 6: Sample answers of our approach.
weight of the corresponding edge. Given a question,
its similarity to each sentence in the graph was com-
puted. Such similarity was viewed as the relevant
score to the corresponding sentence. The sentences
then were ranked based on three metric, i.e. relevant
score to the query, similarity score obtained from the
graph algorithm over sentences, and degree of opin-
ion matching to the query. Respectively, Lloret et
al. (2011) proposed to form answers by re-ranking
the retrieved sentences based on the metric of word
frequency, non-redundancy and the number of noun
phrases. Their method includes three components,
including information retrieval, opinion mining and
text summarization. Evaluations were conducted on
the TAC 2008 Opinion Summarization track. After-
wards, Moghaddam et al2011) developed a system
called AQA to generate answers for questions about
products (i.e. opinion QA on products). It classi-
fies the questions into five types, including target,
attitude, reason, majority and yes/no. As compared
to Ku et al2007), the question types of holder
and majority are not included. They argued that
product questions were seldom asked for the hold-
ers, since the holders (i.e. reviewers) were com-
monly shown in the reviews. Also, product ques-
tions mainly asked for majority opinions, and ma-
jority type was thus not considered. The AQA sys-
tem includes five components, including question
analysis, question expansion, high quality review re-
trieval, subjective sentence extraction, and answer
grouping. The answers are generated by aggregat-
ing opinions in the retrieved fragments.
6 Conclusions and Future Works
In this paper, we have developed a new product
opinion-QA framework, which exploits the hierar-
chical organization of consumer reviews on prod-
ucts. With the help of the hierarchical organization,
our framework can accurately identify the aspects
asked in the questions and also discover their sub-
aspects. We have further formulated the answer gen-
eration from retrieved review sentences as a multi-
criteria optimization problem. The multiple criteria
used include answer salience, diversity, and coher-
ence. The parent-child relations between the aspects
are incorporated into the approach to ensure that the
answers follow the general-to-specific logic. The
proposed framework has been evaluated on 11 pop-
ular products in four domains using 220 questions
on the products. Significant performance improve-
ments were obtained. In the future, we will explore
the more sophisticated NLP features to improve the
proposed framework. This will be done by incorpo-
rating more NLP features in salience and coherence
weights estimation.
Acknowledgments
This work is supported in part by NUS-Tsinghua Ex-
treme Search (NExT) project under the grant num-
ber: R-252-300-001-490. We give warm thanks to
the project and anonymous reviewers for their com-
ments.
400
References
E. Agichtein, C. Castillo, and D. Donato. Finding High-
Quality Content in Social Media. WSDM, 2008.
A. Balahur, E. Boldrini, O. Ferrandez, A. Montoyo,
M. Palomar, and R. Munoz. The DLSIUAES Team?s
Participation in the TAC 2008 Tracks. TAC, 2008.
C. Cardie, J. Wiebe, T. Wilson, and D. Litman. Com-
bining Low-level and Summary Representations of
Opinions for Multi-Perspective Question Answering.
AAAI, 2003.
G. Carenini, R. Ng, and E. Zwart. Multi-document Sum-
marization of Evaluative Text. ACL, 2006.
P. Cimiano. Ontology Learning and Population from
Text: Algorithms, Evaluation and Applications.
Springer-Verlag New York, Inc. Secaucus, NJ, USA,
2006.
K. Crammer, O. Dekel, J. Keshet, S.S. Shwartz, and
Y. Singer. Online Passive Aggressive Algorithms.
Journal of Machine Learning Research, 2006.
P. Deshpande, R. Barzilay, and D.R. Karger. Random-
ized Decoding for Selection-and-Ordering Problems.
NAACL, 2007.
G. Erkan and D.R. Radev. LexRank: Graph-based lexi-
cal centrality as salience in text summarization. AAAI,
2004.
T. Givon. Syntax: A functional-typological Introduction.
Benjamins Pub, 1990.
J. He and D. Dai. Summarization of Yes/No Questions
Using a Feature Function Model. JMLR, 2011.
P. Jiang, H. Fu, C. Zhang, and Z. Niu. A Framework for
Opinion Question Answering. IMS, 2010.
H.D. Kim, D.H. Park, V.G.V. Vydiswaran, and
C.X. Zhai. Opinion Summarization using Entity Fea-
tures and Probabilistic Sentence Coherence Optimiza-
tion: UIUC at TAC 2008 Opinion Summarization Pi-
lot. TAC, 2008.
D. Koller and M. Sahami. Hierarchically Classifying
Documents Using Very Few Words. ICML, 1997.
L.W. Ku, Y.T. Liang, and H.H. Chen. Question Analysis
and Answer Passage Retrieval for Opinion Question
Answering Systems. International Journal of Compu-
tational Linguistics & Chinese Language Processing,
2007.
M. Lapata. Probabilistic Text Structuring: Experiments
with Sentence Ordering. ACL, 2003.
M. Lapata. Automatic Evaluation of Information Order-
ing: Kendalls? Tau. Computational Linguistics, 2006.
F. Li, Y. Tang, M. Huang, and X. Zhu. Answering
Opinion Questions with Random Walks on Graphs.
ACL/AFNLP, 2009.
C.Y. Lin and E.Hovy. Automatic Evaluation of Sum-
maries Using N-gram Co-Occurrence Statistics. HLT-
NAACL, 2003.
Y. Liu, X. Huang, A. An, and X. Yu. Modeling and Pre-
dicting the Helpfulness of Online Reviews. ICDM,
2008.
E. Lloret, A. Balahur, M. Palomar, and A. Montoyo. To-
wards a Unified Approach for Opinion Question An-
swering and Summarization. ACL-HLT, 2011.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Opinion Summarization with Integer Linear Program-
ming Formulation for Sentence Extraction and Order-
ing. COLING, 2010.
C.D. Mizil and G. Kossinets and J. Kleinberg and L. Lee.
How Opinions are Received by Online Communities:
A Case Study on Amazon.com Helpfulness Votes.
WWW, 2009.
S. Moghaddam and M. Ester. AQA: Aspect-based Opin-
ion Question Answering. IEEE-ICDMW, 2011.
Y. Ouyang, W. Li, and Q. Lu. An Integrated Multi-
document Summarization Approach based on Word
Hierarchical Representation. ACL-IJCNLP, 2009.
A. Schrijver. Theory of Linear and Integer Programming.
John Wiley & Sons, 1998.
D.W. Scott. Multivariate Density Estimation: Theory,
Practice, and Visualization. John Wiley & Sons, Inc.,
1992.
C. Silla and A. Freitas. A Survey of Hierarchical Classi-
fication Across Different Application Domains. Data
Mining and Knowledge Discovery, 2011.
S. Somasundaran, T. Wilson, J. Wiebe and V. Stoyanov.
QA with Attitude: Exploiting Opinion Type Analysis
for Improving Question Answering in Online Discus-
sions and the News. ICWSM, 2007.
V. Stoyanov, C. Cardie and J. Wiebe. Multi-
Perspective Question Answering Using the OpQA
Corpus. EMNLP, 2005.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
J. Yu, Z.J. Zha, M. Wang, K. Wang and T.S. Chua.
Domain-Assisted Product Aspect Hierarchy Genera-
tion: Towards Hierarchical Organization of Unstruc-
tured Consumer Reviews. EMNLP, 2011.
J. Yu, Z.J. Zha, M. Wang, and T.S. Chua. Hierarchi-
cal Organization of Unstructured Consumer Reviews.
WWW, 2011.
J. Yu, Z.J. Zha, M. Wang and T.S. Chua. Aspect Rank-
ing: Identifying Important Product Aspects from On-
line Consumer Reviews. ACL, 2011.
H. Yu and V. Hatzivassiloglou. Towards Answering
Opinion Questions: Separating Facts from Opinions
and Identifying the Polarity of Opinion Sentences.
EMNLP, 2003.
401
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 800?809, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
SSHLDA: A Semi-Supervised Hierarchical Topic Model
Xian-Ling Mao??, Zhao-Yan Ming?, Tat-Seng Chua?, Si Li?, Hongfei Yan??, Xiaoming Li?
?Department of Computer Science and Technology, Peking University, China
?School of Computing, National University of Singapore, Singapore
?School of ICE, Beijing University of Posts and Telecommunications, China
{xianlingmao,lxm}@pku.edu.cn, yhf@net.pku.edu.cn
{chuats,mingzhaoyan}@nus.edu.sg, lisi@bupt.edu.cn
Abstract
Supervised hierarchical topic modeling and
unsupervised hierarchical topic modeling are
usually used to obtain hierarchical topics, such
as hLLDA and hLDA. Supervised hierarchi-
cal topic modeling makes heavy use of the in-
formation from observed hierarchical labels,
but cannot explore new topics; while unsu-
pervised hierarchical topic modeling is able
to detect automatically new topics in the data
space, but does not make use of any informa-
tion from hierarchical labels. In this paper, we
propose a semi-supervised hierarchical topic
model which aims to explore new topics auto-
matically in the data space while incorporating
the information from observed hierarchical la-
bels into the modeling process, called Semi-
Supervised Hierarchical Latent Dirichlet Al-
location (SSHLDA). We also prove that hLDA
and hLLDA are special cases of SSHLDA.We
conduct experiments on Yahoo! Answers and
ODP datasets, and assess the performance in
terms of perplexity and clustering. The ex-
perimental results show that predictive ability
of SSHLDA is better than that of baselines,
and SSHLDA can also achieve significant im-
provement over baselines for clustering on the
FScore measure.
1 Introduction
Topic models, such as latent Dirichlet alation
(LDA), are useful NLP tools for the statistical anal-
ysis of document collections and other discrete data.
?This work was done in National University of Singapore.
?Corresponding author.
Furthermore, hierarchical topic modeling is able to
obtain the relations between topics ? parent-child
and sibling relations. Unsupervised hierarchical
topic modeling is able to detect automatically new
topics in the data space, such as hierarchical La-
tent Dirichlet Allocation (hLDA) (Blei et al2004).
hLDAmakes use of nested Dirichlet Process to auto-
matically obtain a L-level hierarchy of topics. Mod-
ern Web documents, however, are not merely col-
lections of words. They are usually documents with
hierarchical labels ? such as Web pages and their
placement in hierarchical directories (Ming et al
2010). Unsupervised hierarchical topic modeling
cannot make use of any information from hierarchi-
cal labels, thus supervised hierarchical topic models,
such as hierarchical Labeled Latent Dirichlet Allo-
cation (hLLDA) (Petinot et al2011), are proposed
to tackle this problem. hLLDA uses hierarchical la-
bels to automatically build corresponding topic for
each label, but it cannot find new latent topics in the
data space, only depending on hierarchy of labels.
As we know that only about 10% of an iceberg?s
mass is seen outside while about 90% of it is unseen,
deep down in water. We think that a corpus with hi-
erarchical labels should include not only observed
topics of labels, but also there are more latent top-
ics, just like icebergs. hLLDA can make use of the
information from labels; while hLDA can explore
latent topics. How can we combine the merits of the
two types of models into one model?
An intuitive and simple combinational method is
like this: first, we use hierarchy of labels as basic hi-
erarchy, called Base Tree (BT); then we use hLDA
to build automatically topic hierarchy for each leaf
800
node in BT, called Leaf Topic Hierarchy (LTH); fi-
nally, we add each LTH to corresponding leaf in the
BT and obtain a hierarchy for the entire dataset. We
refer the method as Simp-hLDA. The performance
of the Simp-hLDA is not so good, as can be seen
from the example in Figure 3 (b). The drawbacks
are: (i) the leaves in BT do not obtain reasonable
and right words distribution, such as ?Computers &
Internet? node in Figure 3 (b), its topical words, ?the
to you and a?, is not about ?Computers & Internet?;
(ii) the non-leaf nodes in BT cannot obtain words
distribution, such as ?Health? node in Figure 3 (b);
(iii) it is a heuristic method, and thus Simp-hLDA
has no solid theoretical basis.
To tackle the above drawbacks, we explore the
use of probabilistic models for such a task where
the hierarchical labels are merely viewed as a part
of a hierarchy of topics, and the topics of a path in
the whole hierarchy generate a corresponding doc-
ument. Our proposed generative model learns both
the latent topics of the underlying data and the la-
beling strategies in a joint model, by leveraging on
the hierarchical structure of labels and Hierarchical
Dirichlet Process.
We demonstrate the effectiveness of the proposed
model on large, real-world datasets in the question
answering and website category domains on two
tasks: the topic modeling of documents, and the use
of the generated topics for document clustering. Our
results show that our joint, semi-hierarchical model
outperforms the state-of-the-art supervised and un-
supervised hierarchical algorithms. The contribu-
tions of this paper are threefold: (1) We propose a
joint, generative semi-supervised hierarchical topic
model, i.e. Semi-Supervised Hierarchical Latent
Dirichlet Allocation (SSHLDA), to overcome the
defects of hLDA and hLLDA while combining the
their merits. SSHLDA is able to not only explore
new latent topics in the data space, but also makes
use of the information from the hierarchy of ob-
served labels; (2) We prove that hLDA and hLLDA
are special cases of SSHLDA; (3) We develop a
gibbs sampling inference algorithm for the proposed
model.
The remainder of this paper is organized as fol-
lows. We review related work in Section 2. In Sec-
tion 3, we introduce some preliminaries; while we
introduce SSHLDA in Section 4. Section 5 details
a gibbs sampling inference algorithm for SSHLDA;
while Section 6 presents the experimental results.
Finally, we conclude the paper and suggest direc-
tions for future research in Section 7.
2 Related Work
There have been many variations of topic mod-
els. The existing topic models can be divided
into four categories: Unsupervised non-hierarchical
topic models, Unsupervised hierarchical topic mod-
els, and their corresponding supervised counter-
parts.
Unsupervised non-hierarchical topic models are
widely studied, such as LSA (Deerwester et al
1990), pLSA (Hofmann, 1999), LDA (Blei et al
2003), Hierarchical-concept TM (Chemudugunta et
al., 2008c; Chemudugunta et al2008b), Corre-
lated TM (Blei and Lafferty, 2006) and Concept TM
(Chemudugunta et al2008a; Chemudugunta et al
2008b) etc. The most famous one is Latent Dirichlet
Allocation (LDA). LDA is similar to pLSA, except
that in LDA the topic distribution is assumed to have
a Dirichlet prior. LDA is a completely unsupervised
algorithm that models each document as a mixture
of topics. Another famous model that not only rep-
resents topic correlations, but also learns them, is
the Correlated Topic Model (CTM). Topics in CTM
are not independent; however it is noted that only
pairwise correlations are modeled, and the number
of parameters in the covariance matrix grows as the
square of the number of topics.
However, the above models cannot capture the
relation between super and sub topics. To address
this problem, many models have been proposed
to model the relations, such as Hierarchical LDA
(HLDA) (Blei et al2004), Hierarchical Dirichlet
processes (HDP) (Teh et al2006), Pachinko Allo-
cation Model (PAM) (Li and McCallum, 2006) and
Hierarchical PAM (HPAM) (Mimno et al2007)
etc. The relations are usually in the form of a hi-
erarchy, such as the tree or Directed Acyclic Graph
(DAG). Blei et al2004) proposed the hLDA model
that simultaneously learns the structure of a topic
hierarchy and the topics that are contained within
that hierarchy. This algorithm can be used to extract
topic hierarchies from large document collections.
Although unsupervised topic models are suffi-
801
ciently expressive to model multiple topics per doc-
ument, they are inappropriate for labeled corpora be-
cause they are unable to incorporate the observed la-
bels into their learning procedure. Several modifica-
tions of LDA to incorporate supervision have been
proposed in the literature. Two such models, Su-
pervised LDA (Blei and McAuliffe, 2007; Blei and
McAuliffe, 2010) and DiscLDA (Lacoste-Julien et
al., 2008) are first proposed to model documents as-
sociated only with a single label. Another category
of models, such as the MM-LDA (Ramage et al
2009b), Author TM (Rosen-Zvi et al2004), Flat-
LDA (Rubin et al2011), Prior-LDA (Rubin et al
2011), Dependency-LDA (Rubin et al2011) and
Partially LDA (PLDA) (Ramage et al2011) etc.,
are not constrained to one label per document be-
cause they model each document as a bag of words
with a bag of labels. However, these models obtain
topics that do not correspond directly with the la-
bels. Labeled LDA (LLDA) (Ramage et al2009a)
can be used to solve this problem.
None of these non-hierarchical supervised mod-
els, however, leverage on dependency structure,
such as parent-child relation, in the label space. For
hierarchical labeled data, there are also few models
that are able to handle the label relations in data.
To the best of our knowledge, only hLLDA (Petinot
et al2011) and HSLDA (Perotte et al2011) are
proposed for this kind of data. HSLDA cannot ob-
tain a probability distribution for a label. Although
hLLDA can obtain a distribution over words for each
label, hLLDA is unable to capture the relations be-
tween parent and child node using parameters, and it
also cannot detect automatically latent topics in the
data space. In this paper, we will propose a genera-
tive topic model to tackle these problems of hLLDA.
3 Preliminaries
The nested Chinese restaurant process (nCRP) is a
distribution over hierarchical partitions (Blei et al
2004). It generalizes the Chinese restaurant process
(CRP), which is a distribution over partitions. The
CRP can be described by the following metaphor.
Imagine a restaurant with an infinite number of ta-
bles, and imagine customers entering the restaurant
in sequence. The dth customer sits at a table accord-
Table 1: Notations used in the paper.
Sym Description
V Vocabulary (word set), w is a word in V
D Document collection
Tj
The set of paths in the sub-tree whose root is the
jth leaf node in the hierarchy of observed topics
m A document m that consists of words and labels
wm The text of document m, wi is ith words in w
cm The topic set of document m
com The set of topics with observed labels for document m
cem The set of topics without labels for document m
ce?m The set of latent topics for all documents other than m
zem
The assignment of the words in the mth document
to one of the latent topics
wem
The set of the words belonging to one of the latent
topics in the the mth document
zm,n
The assignment of the nth word in the mth document
to one of the L available topics
z The set of zm,n for all words in all documents
ci A topic in the ith level in the hierarchy
? The word distribution set for Z, i.e., {?}z?c
? Dirichlet prior of ?
?ci The multinomial distribution over the sub-topics of ci?1
?ci Dirichlet prior of ?ci
? Dirichlet prior of ?
? The multinomial distribution of words
?m The distributions over topics for document m
? The set for ?m, m ? {1, ..., D}
ing to the following distribution,
p(cd = k|c1:(d?1)) ?
{ mk if k is previous occupied
? if k is a new tabel, (1)
where mk is the number of previous customers sit-
ting at table k and ? is a positive scalar. AfterD cus-
tomers have sat down, their seating plan describes a
partition of D items.
In the nested CRP, imagine now that tables are or-
ganized in a hierarchy: there is one table at the first
level; it is associated with an infinite number of ta-
bles at the second level; each second-level table is
associated with an infinite number of tables at the
third level; and so on until the Lth level. Each cus-
tomer enters at the first level and comes out at the
Lth level, generating a path with L tables as she sits
in each restaurant. Moving from a table at level l to
one of its subtables at level l+1, the customer draws
following the CRP using Formula (1). In this paper,
we will make use of nested CRP to explore latent
topics in data space.
To elaborate our model, we first define two con-
cepts. If a model can learn a distribution over words
for a label, we refer the topic with a corresponding
label as a labeled topic. If a model can learn an un-
seen and latent topic without a label, we refer the
802
Figure 1: The graphical model of SSHLDA.
topic as a latent topic.
4 The Semi-Supervised Hierarchical Topic
Model
In this section, we will introduce a semi-
supervised hierarchical topic model, i.e., the Semi-
Supervised Hierarchical Latent Dirichlet Allocation
(SSHLDA). SSHLDA is a probabilistic graphical
model that describes a process for generating a hi-
erarchical labeled document collection. Like hi-
erarchical Labeled LDA (hLLDA) (Petinot et al
2011), SSHLDA can incorporate labeled topics into
the generative process of documents. On the other
hand, like hierarchical Latent Dirichlet Allocation
(hLDA) (Blei et al2004), SSHLDA can automat-
ically explore latent topic in data space, and extend
the existing hierarchy of observed topics. SSHLDA
makes use of not only observed topics, but also la-
tent topics.
The graphical model of SSHLDA is illustrated in
Figure 1. In the model, N is the number of words in
a document, D is the total number of documents in
a collection, M is the number of leaf nodes in hier-
archical observed nodes, ci is a node in the ith level
in the hierarchical tree, ?, ? and ?ci are dirichlet
prior parameters, ?k is a distribution over words, ?
is a document-specific distribution over topics, ?ci is
a multinomial distribution over observed sub-topics
of topic ci, w is an observed word, z is the topic
assigned to w, Dirk(.) is a k-dimensional Dirichlet
distribution, Tj is a set of paths in the hierarchy of
latent topics for jth leaf node in the hierarchy of ob-
Figure 2: One illustration of SSHLDA. The tree has 5
levels. The shaded nodes are observed topics, and circled
nodes are latent topics. The latent topics are generated
automatically by SSHLDA model. After learning, each
node in this tree will obtain a corresponding probability
distribution over words, i.e. a topic.
served topics, ? is a Multi-nomial distribution over
paths in the tree. All notations used in this paper are
listed in Table 1.
SSHLDA, as shown in Figure 1, assumes the fol-
lowing generative process:
(1) For each table k ? T in the infinite tree,
(a) Draw a topic ?k ? Dir(?).
(2) For each document, m ? {1, 2, ..., D}
(a) Let c1 be the root node.
(b) For each level l ? {2, ..., L}:
(i) If nodes in this level have been observed,
draw a node cl from Mult(?cl?1 |?cl?1).
(ii) Otherwise, draw a table cl from restaurant
cl?1 using Formula (1).
(c) Draw an L-dimensional topic proportion vec-
tor ?m from Dir(?).
(d) For each word n ? {1, ..., N}:
(i) Draw z ? {1, ..., L} from Mult(?).
(ii) Draw wn from the topic associated with
restaurant cz .
As the example showed in Figure 2, we assume
that we have known a hierarchy of observed top-
ics: {A1,A2,A17,A3,A4}, and assume the height
of the desired topical tree is L = 5. All circled
nodes are latent topics, and shaded nodes are ob-
served topics. A possible generative process for a
document m can be: It starts from A1, and chooses
node A17 at level 2, and then chooses A18, A20 and
A25 in the following levels. Thus we obtain a path:
cm = {A1, A17, A18, A20, A25}. After getting the
path for m, SSHLDA generates each word from one
of topics in this set of topics cm.
803
5 Probabilistic Inference
In this section, we describe a Gibbs sampling al-
gorithm for sampling from the posterior and corre-
sponding topics in the SSHLDA model. The Gibbs
sampler provides a method for simultaneously ex-
ploring the model parameter space (the latent topics
of the whole corpus) and the model structure space
(L-level trees).
In SSHLDA, we sample the paths cm for docu-
ment m and the per-word level allocations to topics
in those paths zm,n. Thus, we approximate the pos-
terior p(cm, zm|?, ?,w,?). The hyper-parameter ?
reflects the tendency of the customers in each restau-
rant to share tables, ? denotes the expected variance
of the underlying topics (e.g., ?  1 will tend to
choose topics with fewer high-probability words),
?ci is the dirichlet prior of ?ci , and ? is the set of
?ci . wm,n denotes the nth word in the mth docu-
ment; and cm,l represents the restaurant correspond-
ing to the lth-level topic in document m; and zm,n,
the assignment of the nth word in the mth document
to one of the L available topics. All other variables
in the model, ? and ?, are integrated out. The Gibbs
sampler thus assesses the values of zm,n and cm,l.
The Gibbs sampler can be divided into two main
steps: the sampling of level allocations and the sam-
pling of path assignments.
First, given the values of the SSHLDA hidden
variables, we sample the cm,l variables which are as-
sociated with the CRP prior. Noting that cm is com-
posed of com and cem , com is the set of observed
topics for document m, and cem is the set of latent
topics for document m. The conditional distribution
for cm, the L topics associated with documentm, is:
p(cm|z,w, c?m,?)
=p(com |?)p(cem |zem ,wem , ce?m)
?p(com |?)p(wem |cem ,we?m , zem)
p(cem |ce?m) (2)
where
p(com |?) =
|com |?1
?
i=0
p(ci,m|?ci) (3)
and
p(wem |cem ,we?m , zem)
=
|cem |
?
l=1
(
?(n.cem,l,?m + |V |?)
?
w ?(nwcem,l,?m + ?)
?
?
w ?(nwcem,l,?m + n
w
cem,l,m + ?)
?(n.cem,l,?m + n
?
cem,l,m + |V |?)
)
(4)
ce?m is the set of latent topics for all documents
other than m, zem is the assignment of the words
in the mth document to one of the latent topics, and
wem is the set of the words belonging to one of the
latent topics in the the mth document. nwcem,l,?m is
the number of instances of word w that have been
assigned to the topic indexed by cem,l, not including
those in the document m.
Second, given the current state of the SSHLDA,
we sample the zm,n variables of the underlying
SSHLDA model as follows:
p(zm,n = j|z?(m,n),w, cm,?)
?
nm?n,j + ?
nm?n,. + |cm|
?
nwm,n?n,j + ?wm,n
n.?(m,n) + |V |
(5)
Having obtained the full conditional distribution,
the Gibbs sampling algorithm is then straightfor-
ward. The zm,n variables are initialized to determine
the initial state of the Markov chain. The chain is
then run for a number of iterations, each time find-
ing a new state by sampling each zm,n from the dis-
tribution specified by Equation (5). After obtain-
ing individual word assignments z, we can estimate
the topic multinomials and the per-document mixing
proportions. Specifically, the topic multinomials are
estimated as:
?cm,j,i = p(wi|zcm,j) =
? + nzwicm,j
|V |? +
?
n.zcm,j
(6)
while the per-document mixing proportions fixed
can be estimated as:
?m,j =
?+ nm.,j
|cm|?+ nm.,.
, j ? 1, ..., |cm| (7)
5.1 Relation to Existing Models
In this section, we draw comparisons with the cur-
rent state-of-the-art models for hierarchical topic
804
modeling (Blei et al2004; Petinot et al2011) and
show that at certain choices of the parameters of our
model, these methods fall out as special cases.
Our method generalises not only hierarchi-
cal Latent Dirichlet Allocation (hLDA), but also
Hierarchical Labeled Latent Dirichlet Allocation
(hLLDA). Our proposed model provides a unified
framework allowing us to model hierarchical labels
while to explore new latent topics.
Equivalence to hLDA As introduced in Section 2,
hLDA is a unsupervised hierarchical topic model. In
this case, there are no observed nodes, that is, the
corpus has no hierarchical labels. This means cm is
equal to cem,m; meanwhile the factor p(com,m|?) is
always equal to one because each document has root
node, and this allows us to rewrite Formula (2) as:
p(cm|z,w, c?m,?)
?p(wcm |c,w?m, z)p(cm|c?m) (8)
which is exactly the same as the conditional distribu-
tion for cm, the L topics associated with document
m in hLDA model. In this case, our model becomes
equivalent to the hLDA model.
Equivalence to hLLDA hLLDA is a supervised hi-
erarchical topic model, which means all nodes in hi-
erarchy are observed. In this case, cm is equal to
com,m, and this allows us to rewrite Formula (2) as:
p(cm|z,w, c?m,?) = p(cm|?) ? p(com |?) (9)
which is exactly the same as the step ? Draw a
random path assignment cm? in the generative pro-
cess for hLLDA. Consequentially, in this sense our
model is equivalent to hLLDA.
6 Experiments
We demonstrate the effectiveness of the proposed
model on large, real-world datasets in the question
answering and website category domains on two
tasks: the topic modeling of documents, and the use
of the generated topics for document clustering.
6.1 Datasets
To construct comprehensive datasets for our ex-
periments, we crawled data from two websites.
First, we crawled nearly all the questions and as-
sociated answer pairs (QA pairs) of two top cat-
Table 2: The statistics of the datasets.
Datasets #labels #paths Max level #docs
Y Ans 46 35 4 6,345,786
O Hlth 6695 6505 10 54939
O Home 2432 2364 9 24254
egories of Yahoo! Answers: Computers & Inter-
net and Health. This produced forty-three sub-
categories from 2005.11 to 2008.11, and an archive
of 6,345,786 QA documents. We refer the Yahoo!
Answer data as Y Ans.
In addition, we first crawled two categories of
Open Directory Project (ODP)?: Home and Health.
Then, we removed all categories whose number of
Web sites is less than 3. Finally, for each of Web
sites in categories, we submited the url of each Web
site to Google and used the words in the snippet and
title of the first returned result to extend the sum-
mary of the Web site. We denote the data from the
category Home as O Home, and the data from the
category Health as O Hlth.
The statistics of all datasets are summarized in Ta-
ble 2. From this table, we can see that these datasets
are very diverse: Y Ans has much fewer labels than
O Hlth and O Home, but have much more docu-
ments for each label; meanwhile the depth of hierar-
chical tree for O Hlth and O Home can reach level
9 or above.
All experiments are based on the results of models
with a burn-in of 10000 Gibbs sampling iterations,
symmetric priors ? = 0.1 and free parameter ? = 1.0;
and for ?, we can obtain the estimation of ?ci by
fixed-point iteration (Minka, 2003).
6.2 Case Study
With topic modeling, the top associated words of
topics can be used as good descriptors for topics in
a hierarchy (Blei et al2003; Blei and McAuliffe,
2010). We show in Figure 3 a pair of compara-
tive example of the proposed model and a baseline
model over Y Ans dataset. The tree-based topic vi-
sualizations of Figure 3 (a) and (b) are the results of
SSHLDA and Simp-hLDA.
We have three major observations from the exam-
ple: (i) SSHLDA is a unified and generative model,
after learning, it can obtain a hierarchy of topics;
?http://dmoz.org/
805
Figure 3: (a) A sub network discovered on Y Ans dataset using SSHLDA, and the whole tree has 74 nodes; (b) A sub
network discovered on Y Ans dataset using Simp-hLDA algorithm, and the whole tree has 89 nodes. In both figures,
the shaded and squared nodes are observed labels, not topics; the shaded and round nodes are topics with observed
labels; blue nodes are topics but without labels and the yellow node is one of leaves in hierarchy of labels. Each topic
represented by top 5 terms.
while Simp-hLDA is a heuristic method, and its re-
sult is a mixture of label nodes and topical nodes.
For example, Figure 3 (b) shows that the hierarchy
includes label nodes and topic nodes, and each of la-
beled nodes just has a label, but label nodes in Fig-
ure 3 (a) have their corresponding topics. (ii) Dur-
ing obtaining a hierarchy, SSHLDAmakes use of the
information from observed labels, thus it can gener-
ate a logical, structual hierarchy with parent-child
relations; while Simp-hLDA does not incorporate
prior information of labels into its generation pro-
cess, thus although it can obtain a hierarchy, many
parent-child pairs have not parent-child relation. For
example, in Figure 3 (b), although label ?root? is
a parent of label ?Computers & Internet?, the topi-
cal words of label ?Computers & Internet? show the
topical node is not a child of label ?root?. How-
ever, in Figure 3 (a), label ?root? and ?Computers
& Internet? has corresponding parent-child relation
between their topical words. (iii) In a hierarchy of
topics, if a topical node has correspending label, the
label can help people understand descendant topi-
cal nodes. For example, when we know node ?er-
ror files click screen virus? in Figure 3 (a) has its
label ?Computers & Internet?, we can understand
the child topic ?hard screen usb power dell? is about
?computer hardware?. However, in Figure 3 (b), the
labels in parent nodes cannot provide much informa-
tion to understand descendant topical nodes because
many label nodes have not corresponding right topi-
cal words, such as label ?Computers & Internet?, its
topical words, ?the to you and a?, do not reflect the
connotation of the label.
These observations further confirm that SSHLDA
is better than the baseline model.
6.3 Perplexity Comparison
A good topic model should be able to generalize to
unseen data. To measure the prediction ability of
our model and baselines, we compute the perplex-
ity for each document d in the test sets. Perplex-
ity, which is widely used in the language modeling
and topic modeling community, is equivalent alge-
braically to the inverse of the geometric mean per-
word likelihood (Blei et al2003). Lower perplexity
scores mean better. Our model, SSHLDA, will com-
pare with three state-of-the-art models, i.e. Simp-
hLDA, hLDA and hLLDA. Simp-hLDA has been
introduced in Section 1, and hLDA and hLLDA has
been reviewed in Section 2. We keep 80% of the data
collection as the training set and use the remaining
collection as the held-out test set. We build the mod-
806
els based on the train set and compute the preplexity
of the test set to evaluate the models. Thus, our goal
is to achieve lower perplexity score on a held-out test
set. The perplexity of M test documents is calculated
as:
perplexity(Dtest) = exp
{
?
?M
d=1
?Nd
m=1 log p(wdm)
?M
d=1 Nd
}
(10)
where Dtest is the test collection of M documents,
Nd is document length of document d and wdm is
mth word in document d.
We present the results over the O Hlth dataset in
Figure 4. We choose top 3-level labels as observed,
and assume other labels are not observed, i.e. l = 3.
From the figure, we can see that the perplexities of
SSHLDA, are lower than that of Simp-hLDA, hLDA
and hLLDA at different value of the tree height pa-
rameter, i.e. L ? {5, 6, 7, 8}. It shows that the
performance of SSHLDA is always better than the
state-of-the-art baselines, and means that our pro-
posed model can model the hierarchical labeled data
better than the state-of-the-art models. We can also
obtain similar experimental results over Y Ans and
O Home datasets, and their detailed description is
not included in this paper due to the limitation of
space.
6.4 Clustering performance
To evaluate indirectly the performance of the pro-
posed model, we compare the clustering perfor-
mance of following systems: 1) the proposed model;
2) Simp-hLDA; 3) hLDA; 4) agglomerative cluster-
ing algorithm. There are many agglomerative clus-
tering algorithms, and in this paper, we make use
of the single-linkage method in a software package
called CLUTO (Karypis, 2005) to obtain hierarchies
of clusters over our datasets, with words as features.
We refer the method as h-clustering.
Given a document collectionDSwith aH-level hi-
erarchy of labels, each label in the hierarchy and cor-
responding documents will be taken as the ground
truth of clustering algorithms. The hierarchy of la-
bels denoted as GT-tree. The process of evaluation
is as follows. First, we choose top l-level labels
in GT-tree as an observed hierarchy, i.e. Base Tree
(BT), and we need to construct a L-level hierarchy
(l < L <= H) over the documents DS using a
Figure 4: Perplexities of hLLDA, hLDA, Simp-hLDA
and SSHLDA. The results are run over the O Hlth
dataset, with the height of the hierarchy of observed la-
bels l = 3. The X-axis is the height of the whole topical
tree (L), and Y-axis is the perplexity.
model. The remaining labels in GT-tree and cor-
responding documents are the ground truth classes,
each class denoted as Ci. Then, (i) for h-clustering,
we run single-linkage method over the documents
DS. (ii) for Simp-hLDA, hLDA runs on the doc-
uments in each leaf-node in BT, and the height pa-
rameter is (L ? l) for each hLDA. After training,
each document is assigned to top-1 topic accord-
ing to the distribution over topics for the document.
Each topic and corresponding documents forms a
new cluster. (iii) for hLDA, hLDA runs on all docu-
ments in DS, and the height parameter is L. Similar
to Simp-hLDA, each document is assigned to top-
1 topic. Each topic and corresponding documents
forms a new cluster. (iv) for SSHLDA, we set height
parameter as L. After training, each document is
also assigned to top-1 topic. Topics and their cor-
responding documents form a hierarchy of clusters.
6.4.1 Evaluation Metrics
For each dataset we obtain corresponding clusters
using the various models described in previous sec-
tions. Thus we can use clustering metrics to measure
the quality of various algorithms by using a measure
that takes into account the overall set of clusters that
are represented in the new generated part of a hier-
archical tree.
One such measure is the FScore measure, intro-
807
duced by (Manning et al2008). Given a particular
class Cr of size nr and a particular cluster Si of size
ni, suppose nri documents in the cluster Si belong
to Cr, then the FScore of this class and cluster is
defined to be
F (Cr, Si) =
2?R(Cr, Si)? P (Cr, Si)
R(Cr, Si) + P (Cr, Si)
(11)
where R(Cr, Si) is the recall value defined as
nri/nr, and P (Cr, Si) is the precision value defined
as nri/ni for the classCr and the cluster Si. The FS-
core of the class Cr, is the maximum FScore value
attained at any node in the hierarchical clustering
tree T . That is,
F (Cr) = max
Si?T
F (Cr, Si). (12)
The FScore of the entire clustering solution is then
defined to be the sum of the individual class FScore
weighted according to the class size.
FScore =
c
?
r=1
nr
n
F (Cr), (13)
where c is the total number of classes. In general, the
higher the FScore values, the better the clustering
solution is.
6.4.2 Experimental Results
Each of hLDA, Simp-hLDA and SSHLDA needs
a parameter?the height of the topical tree, i.e. L;
and for Simp-hLDA and SSHLDA, they need an-
other parameter?the height of the hierarchical ob-
served labels, i.e l. The h-clustering does not have
any height parameters, thus its FScore will keep the
same values at different height of the topical tree.
With choosing the height of hierarchical labels for
O Home as 4, i.e. l = 4, the results of our model
and baselines with respect to the height of a hierar-
chy are shown in Figure 5.
From the figure, we can see that our proposed
model can achieve consistent improvement over
the baseline models at different height, i.e. L ?
{5, 6, 7, 8}. For example, the performance of
SSHLDA can reach 0.396 at height 5 while the h-
clustering, hLDA and hLLDA only achieve 0.295,
0.328 and 0.349 at the same height. The result shows
that our model can achieve about 34.2%, 20.7% and
13.5% improvements over h-clustering, hLDA and
Figure 5: FScore measures of h-clustering, hLDA,
Simp-hLDA and SSHLDA. The results are run over the
O Home dataset, with the height of the hierarchy of ob-
served labels l = 3. The X-axis is the height of the whole
topical tree (L), and Y-axis is the FScore measure.
hLLDA at height 5. The improvements are signifi-
cant by t-test at the 95% significance level. We can
also obtain similar experimental results over Y Ans
and O Hlth. However, for the same reason of limita-
tion of space, their detailed descriptions are skipped
in this paper.
7 Conclusion and Future work
In this paper, we have proposed a semi-supervised
hierarchical topic models, i.e. SSHLDA, which aims
to solve the drawbacks of hLDA and hLLDA while
combine their merits. Specially, SSHLDA incorpo-
rates the information of labels into generative pro-
cess of topic modeling while exploring latent topics
in data space. In addition, we have also proved that
hLDA and hLLDA are special cases of SSHLDA.
We have conducted experiments on the Yahoo! An-
swers and ODP datasets, and assessed the perfor-
mance in terms of Perplexity and FScore measure.
The experimental results show that the prediction
ability of SSHLDA is the best, and SSHLDA can
also achieve significant improvement over the base-
lines on Fscore measure.
In the future, we will continue to explore novel
topic models for hierarchical labeled data to further
improve the effectiveness; meanwhile we will also
apply SSHLDA to other media forms, such as im-
age, to solve related problems in these areas.
808
Acknowledgments
This work was partially supported by NSFC with Grant
No.61073082, 60933004, 70903008 and NExT Search
Centre, which is supported by the Singapore National Re-
search Foundation & Interactive Digital Media R&D Pro-
gram Office, MDA under research grant (WBS:R-252-
300-001-490).
References
D. Blei and J. Lafferty. 2006. Correlated topic mod-
els. Advances in neural information processing sys-
tems, 18:147.
D.M. Blei and J.D. McAuliffe. 2007. Supervised topic
models. In Proceeding of the Neural Information Pro-
cessing Systems(nips).
D.M. Blei and J.D. McAuliffe. 2010. Supervised topic
models. Arxiv preprint arXiv:1003.0783.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alation. The Journal of Machine Learning
Research, 3:993?1022.
D. Blei, T.L. Griffiths, M.I. Jordan, and J.B. Tenenbaum.
2004. Hierarchical topic models and the nested chi-
nese restaurant process. Advances in neural informa-
tion processing systems, 16:106.
C. Chemudugunta, A. Holloway, P. Smyth, and
M. Steyvers. 2008a. Modeling documents by com-
bining semantic concepts with unsupervised statistical
learning. The Semantic Web-ISWC 2008, pages 229?
244.
C. Chemudugunta, P. Smyth, and M. Steyvers. 2008b.
Combining concept hierarchies and statistical topic
models. In Proceeding of the 17th ACM conference on
Information and knowledge management, pages 1469?
1470. ACM.
C. Chemudugunta, P. Smyth, and M. Steyvers. 2008c.
Text modeling using unsupervised topic models and
concept hierarchies. Arxiv preprint arXiv:0808.0973.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American society for informa-
tion science, 41(6):391?407.
T. Hofmann. 1999. Probabilistic latent semantic analy-
sis. In Proc. of Uncertainty in Artificial Intelligence,
UAI?99, page 21. Citeseer.
G. Karypis. 2005. Cluto: Software for
clustering high dimensional datasets. In-
ternet Website (last accessed, June 2008),
http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview.
S. Lacoste-Julien, F. Sha, and M.I. Jordan. 2008. ndis-
clda: Discriminative learning for dimensionality re-
duction and classification. Advances in Neural Infor-
mation Processing Systems, 21.
W. Li and A. McCallum. 2006. Pachinko allocation:
Dag-structured mixture models of topic correlations.
In Proceedings of the 23rd international conference on
Machine learning, pages 577?584. ACM.
C.D. Manning, P. Raghavan, and H. Schutze. 2008. In-
troduction to information retrieval, volume 1. Cam-
bridge University Press Cambridge.
D. Mimno, W. Li, and A. McCallum. 2007. Mixtures of
hierarchical topics with pachinko allocation. In Pro-
ceedings of the 24th international conference on Ma-
chine learning, pages 633?640. ACM.
Z.Y. Ming, K. Wang, and T.S. Chua. 2010. Prototype
hierarchy based clustering for the categorization and
navigation of web collections. In Proceeding of the
33rd international ACM SIGIR, pages 2?9. ACM.
T.P. Minka. 2003. Estimating a dirichlet distribution.
Annals of Physics, 2000(8):1?13.
A. Perotte, N. Bartlett, N. Elhadad, and F. Wood. 2011.
Hierarchically supervised latent dirichlet alation.
Neural Information Processing Systems (to appear).
Y. Petinot, K. McKeown, and K. Thadani. 2011. A
hierarchical model of web summaries. In Proceed-
ings of the 49th Annual Meeting of the ACL: Human
Language Technologies: short papers-Volume 2, pages
670?675. ACL.
D. Ramage, D. Hall, R. Nallapati, and C.D. Manning.
2009a. Labeled lda: A supervised topic model for
credit attribution in multi-labeled corpora. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 1-Volume 1,
pages 248?256. Association for Computational Lin-
guistics.
D. Ramage, P. Heymann, C.D. Manning, and H. Garcia-
Molina. 2009b. Clustering the tagged web. In Pro-
ceedings of the Second ACM International Conference
on Web Search and Data Mining, pages 54?63. ACM.
D. Ramage, C.D. Manning, and S. Dumais. 2011. Par-
tially labeled topic models for interpretable text min-
ing. In Proceedings of the 17th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 457?465. ACM.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. The author-topic model for authors and doc-
uments. In Proceedings of the 20th conference on
Uncertainty in artificial intelligence, pages 487?494.
AUAI Press.
T.N. Rubin, A. Chambers, P. Smyth, and M. Steyvers.
2011. Statistical topic models for multi-label docu-
ment classification. Arxiv preprint arXiv:1107.2462.
Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. 2006.
Hierarchical dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
809
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1496?1505,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Aspect Ranking: Identifying Important Product Aspects from Online
Consumer Reviews
Jianxing Yu, Zheng-Jun Zha, Meng Wang, Tat-Seng Chua
School of Computing
National University of Singapore
{jianxing, zhazj, wangm, chuats}@comp.nus.edu.sg
Abstract
In this paper, we dedicate to the topic of aspect
ranking, which aims to automatically identify
important product aspects from online con-
sumer reviews. The important aspects are
identified according to two observations: (a)
the important aspects of a product are usually
commented by a large number of consumers;
and (b) consumers? opinions on the important
aspects greatly influence their overall opin-
ions on the product. In particular, given con-
sumer reviews of a product, we first identify
the product aspects by a shallow dependency
parser and determine consumers? opinions on
these aspects via a sentiment classifier. We
then develop an aspect ranking algorithm to
identify the important aspects by simultane-
ously considering the aspect frequency and
the influence of consumers? opinions given to
each aspect on their overall opinions. The ex-
perimental results on 11 popular products in
four domains demonstrate the effectiveness of
our approach. We further apply the aspect
ranking results to the application of document-
level sentiment classification, and improve the
performance significantly.
1 Introduction
The rapidly expanding e-commerce has facilitated
consumers to purchase products online. More than
$156 million online product retail sales have been
done in the US market during 2009 (Forrester Re-
search, 2009). Most retail Web sites encourage con-
sumers to write reviews to express their opinions on
various aspects of the products. This gives rise to
Figure 1: Sample reviews on iPhone 3GS product
huge collections of consumer reviews on the Web.
These reviews have become an important resource
for both consumers and firms. Consumers com-
monly seek quality information from online con-
sumer reviews prior to purchasing a product, while
many firms use online consumer reviews as an im-
portant resource in their product development, mar-
keting, and consumer relationship management. As
illustrated in Figure 1, most online reviews express
consumers? overall opinion ratings on the product,
and their opinions on multiple aspects of the prod-
uct. While a product may have hundreds of aspects,
we argue that some aspects are more important than
the others and have greater influence on consumers?
purchase decisions as well as firms? product devel-
opment strategies. Take iPhone 3GS as an exam-
ple, some aspects like ?battery? and ?speed,? are
more important than the others like ?moisture sen-
sor.? Generally, identifying the important product
aspects will benefit both consumers and firms. Con-
sumers can conveniently make wise purchase deci-
sion by paying attentions on the important aspects,
while firms can focus on improving the quality of
1496
these aspects and thus enhance the product reputa-
tion effectively. However, it is impractical for people
to identify the important aspects from the numerous
reviews manually. Thus, it becomes a compelling
need to automatically identify the important aspects
from consumer reviews.
A straightforward solution for important aspect
identification is to select the aspects that are fre-
quently commented in consumer reviews as the im-
portant ones. However, consumers? opinions on
the frequent aspects may not influence their over-
all opinions on the product, and thus not influence
consumers? purchase decisions. For example, most
consumers frequently criticize the bad ?signal con-
nection? of iPhone 4, but they may still give high
overall ratings to iPhone 4. On the other hand,
some aspects, such as ?design? and ?speed,? may not
be frequently commented, but usually more impor-
tant than ?signal connection.? Hence, the frequency-
based solution is not able to identify the truly impor-
tant aspects.
Motivated by the above observations, in this pa-
per, we propose an effective approach to automat-
ically identify the important product aspects from
consumer reviews. Our assumption is that the
important aspects of a product should be the as-
pects that are frequently commented by consumers,
and consumers? opinions on the important aspects
greatly influence their overall opinions on the prod-
uct. Given the online consumer reviews of a spe-
cific product, we first identify the aspects in the re-
views using a shallow dependency parser (Wu et al,
2009), and determine consumers? opinions on these
aspects via a sentiment classifier. We then design an
aspect ranking algorithm to identify the important
aspects by simultaneously taking into account the
aspect frequency and the influence of consumers?
opinions given to each aspect on their overall opin-
ions. Specifically, we assume that consumer?s over-
all opinion rating on a product is generated based
on a weighted sum of his/her specific opinions on
multiple aspects of the product, where the weights
essentially measure the degree of importance of the
aspects. A probabilistic regression algorithm is then
developed to derive these importance weights by
leveraging the aspect frequency and the consistency
between the overall opinions and the weighted sum
of opinions on various aspects. We conduct ex-
periments on 11 popular products in four domains.
The consumer reviews on these products are crawled
from the prevalent forum Web sites (e.g., cnet.com
and viewpoint.com etc.) More details of our review
corpus are discussed in Section 3. The experimen-
tal results demonstrate the effectiveness of our ap-
proach on important aspects identification. Further-
more, we apply the aspect ranking results to the ap-
plication of document-level sentiment classification
by carrying out the term-weighting based on the as-
pect importance. The results show that our approach
can improve the performance significantly.
The main contributions of this paper include,
1) We dedicate to the topic of aspect ranking,
which aims to automatically identify important as-
pects of a product from consumer reviews.
2) We develop an aspect ranking algorithm to
identify the important aspects by simultaneously
considering the aspect frequency and the influence
of consumers? opinions given to each aspect on their
overall opinions.
3) We apply aspect ranking results to the applica-
tion of document-level sentiment classification, and
improve the performance significantly.
There is another work named aspect ranking
(Snyder et al, 2007). The task in this work is differ-
ent from ours. This work mainly focuses on predict-
ing opinionated ratings on aspects rather than iden-
tifying important aspects.
The rest of this paper is organized as follows. Sec-
tion 2 elaborates our aspect ranking approach. Sec-
tion 3 presents the experimental results, while Sec-
tion 4 introduces the application of document-level
sentiment classification. Section 5 reviews related
work and Section 6 concludes this paper with future
works.
2 Aspect Ranking Framework
In this section, we first present some notations and
then elaborate the key components of our approach,
including the aspect identification, sentiment classi-
fication, and aspect ranking algorithm.
2.1 Notations and Problem Formulation
Let R = {r1, ? ? ? , r|R|} denotes a set of online con-
sumer reviews of a specific product. Each review
r ? R is associated with an overall opinion rating
1497
Or, and covers several aspects with consumer com-
ments on these aspects. Suppose there arem aspects
A = {a1, ? ? ? , am} involved in the review corpus
R, where ak is the k-th aspect. We define ork as the
opinion on aspect ak in review r. We assume that
the overall opinion rating Or is generated based on
a weighted sum of the opinions on specific aspects
ork (Wang et al, 2010). The weights are denoted as
{?rk}mk=1, each of which essentially measures the
degree of importance of the aspect ak in review r.
Our task is to derive the important weights of as-
pects, and identify the important aspects.
Next, we will introduce the key components of
our approach, including aspect identification that
identifies the aspects ak in each review r, aspect sen-
timent classification which determines consumers?
opinions ork on various aspects, and aspect ranking
algorithm that identifies the important aspects.
2.2 Aspect Identification
As illustrated in Figure 1, there are usually two types
of reviews, Pros and Cons review and free text re-
views on the Web. For Pros and Cons reviews, the
aspects are identified as the frequent noun terms in
the reviews, since the aspects are usually noun or
noun phrases (Liu, 2009), and it has been shown
that simply extracting the frequent noun terms from
the Pros and Cons reviews can get high accurate
aspect terms (Liu el al., 2005). To identify the as-
pects in free text reviews, we first parse each review
using the Stanford parser 1, and extract the noun
phrases (NP) from the parsing tree as aspect can-
didates. While these candidates may contain much
noise, we leverage the Pros and Cons reviews to
assist identify aspects from the candidates. In par-
ticular, we explore the frequent noun terms in Pros
and Cons reviews as features, and train a one-class
SVM (Manevitz et al, 2002) to identify aspects in
the candidates. While the obtained aspects may con-
tain some synonym terms, such as ?earphone? and
?headphone,? we further perform synonym cluster-
ing to get unique aspects. Specifically, we first ex-
pand each aspect term with its synonym terms ob-
tained from the synonym terms Web site 2, and then
cluster the terms to obtain unique aspects based on
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://thesaurus.com
unigram feature.
2.3 Aspect Sentiment Classification
Since the Pros and Cons reviews explicitly express
positive and negative opinions on the aspects, re-
spectively, our task is to determine the opinions in
free text reviews. To this end, we here utilize Pros
andCons reviews to train a SVM sentiment classifier.
Specifically, we collect sentiment terms in the Pros
and Cons reviews as features and represent each re-
view into feature vector using Boolean weighting.
Note that we select sentiment terms as those appear
in the sentiment lexicon provided by MPQA project
(Wilson et al, 2005). With these features, we then
train a SVM classifier based on Pros and Cons re-
views. Given a free text review, since it may cover
various opinions on multiple aspects, we first locate
the opinionated expression modifying each aspect,
and determine the opinion on the aspect using the
learned SVM classifier. In particular, since the opin-
ionated expression on each aspect tends to contain
sentiment terms and appear closely to the aspect (Hu
and Liu, 2004), we select the expressions which con-
tain sentiment terms and are at the distance of less
than 5 from the aspect NP in the parsing tree.
2.4 Aspect Ranking
Generally, consumer?s opinion on each specific as-
pect in the review influences his/her overall opin-
ion on the product. Thus, we assume that the con-
sumer gives the overall opinion rating Or based on
the weighted sum of his/her opinion ork on each as-
pect ak:
?m
k=1 ?rkork, which can be rewritten as
?rTor, where?r and or are the weight and opinion
vectors. Inspired by the work of Wang et al (2010),
we viewOr as a sample drawn from aGaussian Dis-
tribution, with mean ?rTor and variance ?2,
p(Or) =
1?
2??2
exp[?(Or ? ?r
Tor)2
2?2
]. (1)
To model the uncertainty of the importance
weights ?r in each review, we assume ?r as a sam-
ple drawn from a Multivariate Gaussian Distribu-
tion, with ? as the mean vector and? as the covari-
ance matrix,
p(?r) =
1
(2pi)n/2|?|1/2 exp[?
1
2(?r ? ?)
T??1(?r ? ?)].
(2)
1498
We further incorporate aspect frequency as a prior
knowledge to define the distribution of ? and ?.
Specifically, the distribution of ? and ? is defined
based on its Kullback-Leibler (KL) divergence to a
prior distribution with a mean vector?0 and an iden-
tity covariance matrix I in Eq.3. Each element in?0
is defined as the frequency of the corresponding as-
pect: frequency(ak)/
?m
i=1 frequency(ai).
p(?,?) = exp[?? ?KL(Q(?,?)||Q(?0, I))],
(3)
where KL(?, ?) is the KL divergence, Q(?,?) de-
notes a Multivariate Gaussian Distribution, and ? is
a tradeoff parameter.
Base on the above definition, the probability of
generating the overall opinion rating Or on review r
is given as,
p(Or|?, r) =
?
p(Or|?rTor, ?2)
? p(?r|?,?) ? p(?,?)d?r,
(4)
where? = {?,?,?, ?2} are the model parameters.
Next, we utilize Maximum Log-likelihood (ML)
to estimate the model parameters given the con-
sumer reviews corpus. In particular, we aim to find
an optimal ?? to maximize the probability of observ-
ing the overall opinion ratings in the reviews corpus.
?? = argmax
?
?
r?R
log(p(Or|?, r))
= argmin
?
(|R| ? 1) log det(?) +
?
r?R
[log ?2+
(Or??rTor)2
?2 + (?r ? ?)
T??1(?r ? ?)]+
(tr(?) + (?0 ? ?)TI(?0 ? ?)).
(5)
For the sake of simplicity, we denote the objective
function
?
r?R log(p(Or|?, r)) as ?(?).
The derivative of the objective function with re-
spect to each model parameter vanishes at the mini-
mizer:
??(?)
??r = ?
(?rTor?Or)or
?2 ??
?1(?r ? ?)
= 0;
(6)
??(?)
?? =
?
r?R
[???1(?r ? ?)]? ? ? I(?0 ? ?)
= 0;
(7)
??(?)
?? =
?
r?R
{?(??1)T ? [?(??1)T (?r ? ?)
(?r ? ?)T (??1)T ]}+ ? ?
[
(??1)T ? I
]
= 0;
(8)
??(?)
??2 =
?
r?R
(? 1?2 +
(Or??rTor)2
?4 ) = 0,
(9)
which lead to the following solutions:
??r = (oror
T
?2 +?
?1)?1(Oror?2 +?
?1?);
(10)
?? = (|R|??1 + ? ? I)?1(??1
?
r?R
?r + ? ? I?0);
(11)
?? = {[ 1?
?
r?R
[
(?r ? ?)(?r ? ?)T
]
+
( |R|??2? )
2I]1/2 ? (|R|??)2? I}
T ;
(12)
??2 = 1|R|
?
r?R
(Or ? ?rTor)2.
(13)
We can see that the above parameters are involved
in each other?s solution. We here utilize Alternating
Optimization technique to derive the optimal param-
eters in an iterative manner. We first hold the param-
eters ?, ? and ?2 fixed and update the parameters
?r for each review r ? R. Then, we update the
parameters ?, ? and ?2 with fixed ?r (r ? R).
These two steps are alternatively iterated until the
Eq.5 converges. As a result, we obtain the optimal
importance weights ?r which measure the impor-
tance of aspects in review r ? R. We then compute
the final importance score ?k for each aspect ak by
integrating its importance score in all the reviews as,
?k =
1
|R|
?
r?R
?rk, k = 1, ? ? ? ,m (14)
It is worth noting that the aspect frequency is con-
sidered again in this integration process. According
to the importance score ?k, we can identify impor-
tant aspects.
3 Evaluations
In this section, we evaluate the effectiveness of our
approach on aspect identification, sentiment classi-
fication, and aspect ranking.
3.1 Data and Experimental Setting
The details of our product review data set is given
in Table 1. This data set contains consumer reviews
on 11 popular products in 4 domains. These reviews
were crawled from the prevalent forum Web sites,
including cnet.com, viewpoints.com, reevoo.com
and gsmarena.com. All of the reviews were posted
1499
between June, 2009 and Sep 2010. The aspects of
the reviews, as well as the opinions on the aspects
were manually annotated as the gold standard for
evaluations.
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
Table 1: Statistics of the Data Sets, # denotes the size of
the reviews/sentences.
To examine the performance on aspect identifi-
cation and sentiment classification, we employed
F1-measure, which was the combination of preci-
sion and recall, as the evaluation metric. To evalu-
ate the performance on aspect ranking, we adopted
Normalized Discounted Cumulative Gain at top k
(NDCG@k) (Jarvelin and Kekalainen, 2002) as the
performance metric. Given an aspect ranking list
a1, ? ? ? , ak, NDCG@k is calculated by
NDCG@k = 1
Z
k
?
i=1
2t(i) ? 1
log(1 + i)
, (15)
where t(i) is the function that represents the reward
given to the aspect at position i, Z is a normaliza-
tion term derived from the top k aspects of a perfect
ranking, so as to normalize NDCG@k to be within
[0, 1]. This evaluation metric will favor the ranking
which ranks the most important aspects at the top.
For the reward t(i), we labeled each aspect as one of
the three scores: Un-important (score 1), Ordinary
(score 2) and Important (score 3). Three volunteers
were invited in the annotation process as follows.
We first collected the top k aspects in all the rank-
ings produced by various evaluated methods (maxi-
mum k is 15 in our experiment). We then sampled
some reviews covering these aspects, and provided
the reviews to each annotator to read. Each review
contains the overall opinion rating, the highlighted
aspects, and opinion terms. Afterward, the annota-
tors were required to assign an importance score to
each aspect. Finally, we took the average of their
scorings as the corresponding importance scores of
the aspects. In addition, there is only one parameter
? that needs to be tuned in our approach. Through-
out the experiments, we empirically set ? as 0.001.
3.2 Evaluations on Aspect Identification
We compared our aspect identification approach
against two baselines: a) the method proposed by
Hu and Liu (2004), which was based on the asso-
ciation rule mining, and b) the method proposed by
Wu et al (2009), which was based on a dependency
parser.
The results are presented in Table 2. On average,
our approach significantly outperforms Hu?s method
and Wu? method in terms of F1-measure by over
5.87% and 3.27%, respectively. In particular, our
approach obtains high precision. Such results imply
that our approach can accurately identify the aspects
from consumer reviews by leveraging the Pros and
Cons reviews.
Data set Hu?s Method Wu?s Method Our Method
Canon EOS 0.681 0.686 0.728
Fujifilm 0.685 0.666 0.710
Panasonic 0.636 0.661 0.706
MacBook 0.680 0.733 0.747
Samsung 0.594 0.631 0.712
iPod Touch 0.650 0.660 0.718
Sony NWZ 0.631 0.692 0.760
BlackBerry 0.721 0.730 0.734
iPhone 3GS 0.697 0.736 0.740
Nokia 5800 0.715 0.745 0.747
Nokia N95 0.700 0.737 0.741
Table 2: Evaluations on Aspect Identification. * signifi-
cant t-test, p-values<0.05.
3.3 Evaluations on Sentiment Classification
In this experiment, we implemented the follow-
ing sentiment classification methods (Pang and Lee,
2008):
1) Unsupervised method. We employed one un-
supervised method which was based on opinion-
ated term counting via SentiWordNet (Ohana et al,
2009).
2) Supervised method. We employed three su-
pervised methods proposed in Pang et al (2002),
including Na??ve Bayes (NB), Maximum Entropy
(ME), SVM. These classifiers were trained based on
the Pros and Cons reviews as described in Section
2.3.
1500
The comparison results are showed in Table 3. We
can see that supervised methods significantly outper-
form unsupervised method. For example, the SVM
classifier outperforms the unsupervised method in
terms of average F1-measure by over 10.37%. Thus,
we can deduce from such results that the Pros and
Cons reviews are useful for sentiment classification.
In addition, among the supervised classifiers, SVM
classifier performs the best in most products, which
is consistent with the previous research (Pang et al,
2002).
Data set Senti NB SVM ME
Canon EOS 0.628 0.720 0.739 0.726
Fujifilm 0.690 0.781 0.791 0.778
Panasonic 0.625 0.694 0.719 0.697
MacBook 0.708 0.820 0.828 0.797
Samsung 0.675 0.723 0.717 0.714
iPod Touch 0.711 0.792 0.805 0.791
Sony NWZ 0.621 0.722 0.737 0.725
BlackBerry 0.699 0.819 0.794 0.788
iPhone 3GS 0.717 0.811 0.829 0.822
Nokia 5800 0.736 0.840 0.851 0.817
Nokia N95 0.706 0.829 0.849 0.826
Table 3: Evaluations on Sentiment Classification. Senti
denotes the method based on SentiWordNet. * significant
t-test, p-values<0.05.
3.4 Evaluations on Aspect Ranking
In this section, we compared our aspect ranking al-
gorithm against the following three methods.
1) Frequency-based method. The method ranks
the aspects based on aspect frequency.
2) Correlation-based method. This method mea-
sures the correlation between the opinions on spe-
cific aspects and the overall opinion. It counts the
number of the cases when such two kinds of opin-
ions are consistent, and ranks the aspects based on
the number of the consistent cases.
3) Hybrid method. This method captures both the
aspect frequency and correlation by a linear combi-
nation, as ?? Frequency-based Ranking + (1 ? ?)?
Correlation-based Ranking, where ? is set to 0.5.
The comparison results are showed in Table 4. On
average, our approach outperforms the frequency-
based method, correlation-based method, and hy-
brid method in terms of NDCG@5 by over 6.24%,
5.79% and 5.56%, respectively. It improves the
performance over such three methods in terms of
NDCG@10 by over 3.47%, 2.94% and 2.58%, re-
spectively, while in terms of NDCG@15 by over
4.08%, 3.04% and 3.49%, respectively. We can de-
duce from the results that our aspect ranking algo-
rithm can effectively identify the important aspects
from consumer reviews by leveraging the aspect fre-
quency and the influence of consumers? opinions
given to each aspect on their overall opinions. Ta-
ble 5 shows the aspect ranking results of these four
methods. Due to the space limitation, we here only
show top 10 aspects of the product iphone 3GS. We
can see that our approach performs better than the
others. For example, the aspect ?phone? is ranked at
the top by the other methods. However, ?phone? is
a general but not important aspect.
# Frequency Correlated Hybrid Our Method
1 Phone Phone Phone Usability
2 Usability Usability Usability Apps
3 3G Apps Apps 3G
4 Apps 3G 3G Battery
5 Camera Camera Camera Looking
6 Feature Looking Looking Storage
7 Looking Feature Feature Price
8 Battery Screen Battery Software
9 Screen Battery Screen Camera
10 Flash Bluetooth Flash Call quality
Table 5: iPhone 3GS Aspect Ranking Results.
To further investigate the reasonability of our
ranking results, we refer to one of the public user
feedback reports, the ?china unicom 100 customers
iPhone user feedback report? (Chinaunicom Report,
2009). The report demonstrates that the top four as-
pects of iPhone product, which users most concern
with, are ?3G Network? (30%), ?usability? (30%),
?out-looking design? (26%), ?application? (15%).
All of these aspects are in the top 10 of our rank-
ing results.
Therefore, we can conclude that our approach is
able to automatically identify the important aspects
from numerous consumer reviews.
4 Applications
The identification of important aspects can support
a wide range of applications. For example, we can
1501
Frequency Correlation Hybrid Our Method
Data set @5 @10 @15 @5 @10 @15 @5 @10 @15 @5 @10 @15
Canon EOS 0.735 0.771 0.740 0.735 0.762 0.779 0.735 0.798 0.742 0.862 0.824 0.794
Fujifilm 0.816 0.705 0.693 0.760 0.756 0.680 0.816 0.759 0.682 0.863 0.801 0.760
Panasonic 0.744 0.807 0.783 0.763 0.815 0.792 0.744 0.804 0.786 0.796 0.834 0.815
MacBook 0.744 0.771 0.762 0.763 0.746 0.769 0.763 0.785 0.772 0.874 0.776 0.760
Samsung 0.964 0.765 0.794 0.964 0.820 0.840 0.964 0.820 0.838 0.968 0.826 0.854
iPod Touch 0.836 0.830 0.727 0.959 0.851 0.744 0.948 0.785 0.733 0.959 0.817 0.801
Sony NWZ 0.937 0.743 0.742 0.937 0.781 0.797 0.937 0.740 0.794 0.944 0.775 0.815
BlackBerry 0.837 0.824 0.766 0.847 0.825 0.771 0.847 0.829 0.768 0.874 0.797 0.779
iPhone 3GS 0.897 0.836 0.832 0.886 0.814 0.825 0.886 0.829 0.826 0.948 0.902 0.860
Nokia 5800 0.834 0.779 0.796 0.834 0.781 0.779 0.834 0.781 0.779 0.903 0.811 0.814
Nokia N95 0.675 0.680 0.717 0.619 0.619 0.691 0.619 0.678 0.696 0.716 0.731 0.748
Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,
and NDCG@15, respectively. * significant t-test, p-values<0.05.
provide product comparison on the important as-
pects to users, so that users can make wise purchase
decisions conveniently.
In the following, we apply the aspect ranking re-
sults to assist document-level review sentiment clas-
sification. Generally, a review document contains
consumer?s positive/negative opinions on various as-
pects of the product. It is difficult to get the ac-
curate overall opinion of the whole review without
knowing the importance of these aspects. In ad-
dition, when we learn a document-level sentiment
classifier, the features generated from unimportant
aspects lack of discriminability and thus may dete-
riorate the performance of the classifier (Fang et al,
2010). While the important aspects and the senti-
ment terms on these aspects can greatly influence the
overall opinions of the review, they are highly likely
to be discriminative features for sentiment classifica-
tion. These observations motivate us to utilize aspect
ranking results to assist classifying the sentiment of
review documents.
Specifically, we randomly sampled 100 reviews of
each product as the testing data and used the remain-
ing reviews as the training data. We first utilized our
approach to identify the importance aspects from the
training data. We then explored the aspect terms and
sentiment terms as features, based on which each re-
view is represented as a feature vector. Here, we
give more emphasis on the important aspects and
the sentiment terms that modify these aspects. In
particular, we set the term-weighting as 1 + ? ??k,
where ?k is the importance score of the aspect ak,
? is set to 100. Based on the weighted features, we
then trained a SVM classifier using the training re-
views to determine the overall opinions on the test-
ing reviews. For the performance comparison, we
compared our approach against two baselines, in-
cluding Boolean weighting method and frequency
weighting (tf ) method (Paltoglou et al, 2010) that
do not utilize the importance of aspects. The com-
parison results are shown in Table 6. We can see
that our approach (IA) significantly outperforms the
other methods in terms of average F1-measure by
over 2.79% and 4.07%, respectively. The results
also show that the Boolean weighting method out-
performs the frequency weighting method in terms
of average F1-measure by over 1.25%, which are
consistent with the previous research by Pang et al
(2002). On the other hand, from the IA weight-
ing formula, we observe that without using the im-
portant aspects, our term-weighting function will be
equal to Boolean weighting. Thus, we can speculate
that the identification of important aspects is ben-
eficial to improving the performance of document-
level sentiment classification.
5 Related Work
Existing researches mainly focused on determining
opinions on the reviews, or identifying aspects from
these reviews. They viewed each aspect equally
without distinguishing the important ones. In this
section, we review existing researches related to our
work.
Analysis of the opinion on whole review text had
1502
SVM + Boolean SVM + tf SVM + IA
Data set P R F1 P R F1 P R F1
Canon EOS 0.689 0.663 0.676 0.679 0.654 0.666 0.704 0.721 0.713
Fujifilm 0.700 0.687 0.693 0.690 0.670 0.680 0.731 0.724 0.727
Panasonic 0.659 0.717 0.687 0.650 0.693 0.671 0.696 0.713 0.705
MacBook 0.744 0.700 0.721 0.768 0.675 0.718 0.790 0.717 0.752
Samsung 0.755 0.690 0.721 0.716 0.725 0.720 0.732 0.765 0.748
iPod Touch 0.686 0.746 0.714 0.718 0.667 0.691 0.749 0.726 0.737
Sony NWZ 0.719 0.652 0.684 0.665 0.646 0.655 0.732 0.684 0.707
BlackBerry 0.763 0.719 0.740 0.752 0.709 0.730 0.782 0.758 0.770
iPhone 3GS 0.777 0.775 0.776 0.772 0.762 0.767 0.820 0.788 0.804
Nokia 5800 0.755 0.836 0.793 0.744 0.815 0.778 0.805 0.821 0.813
Nokia N95 0.722 0.699 0.710 0.695 0.708 0.701 0.768 0.732 0.750
Table 6: Evaluations on Term Weighting methods for Document-level Review Sentiment Classification. IA denotes
the term weighing based on the important aspects. * significant t-test, p-values<0.05.
been extensively studied (Pang and Lee, 2008). Ear-
lier research had been studied unsupervised (Kim et
al., 2004), supervised (Pang et al, 2002; Pang et al,
2005) and semi-supervised approaches (Goldberg et
al., 2006) for the classification. For example, Mullen
et al (2004) proposed an unsupervised classifica-
tion method which exploited pointwise mutual in-
formation (PMI) with syntactic relations and other
attributes. Pang et al (2002) explored several ma-
chine learning classifiers, including Na??ve Bayes,
Maximum Entropy, SVM, for sentiment classifica-
tion. Goldberg et al (2006) classified the sentiment
of the review using the graph-based semi-supervised
learning techniques, while Li el al. (2009) tackled
the problem using matrix factorization techniques
with lexical prior knowledge.
Since the consumer reviews usually expressed
opinions on multiple aspects, some works had
drilled down to the aspect-level sentiment analysis,
which aimed to identify the aspects from the reviews
and to determine the opinions on the specific aspects
instead of the overall opinion. For the topic of aspect
identification, Hu and Liu (2004) presented the asso-
ciation mining method to extract the frequent terms
as the aspects. Subsequently, Popescu et al (2005)
proposed their system OPINE, which extracted the
aspects based on the KnowItAll Web information
extraction system (Etzioni et al, 2005). Liu el al.
(2005) proposed a supervised method based on lan-
guage pattern mining to identify the aspects in the
reviews. Later, Mei et al (2007) proposed a prob-
abilistic topic model to capture the mixture of as-
pects and sentiments simultaneously. Afterwards,
Wu et al (2009) utilized the dependency parser to
extract the noun phrases and verb phrases from the
reviews as the aspect candidates. They then trained
a language model to refine the candidate set, and
to obtain the aspects. On the other hand, for the
topic of sentiment classification on the specific as-
pect, Snyder et al (2007) considered the situation
when the consumers? opinions on one aspect could
influence their opinions on others. They thus built
a graph to analyze the meta-relations between opin-
ions, such as agreement and contrast. And they pro-
posed a Good Grief algorithm to leveraging such
meta-relations to improve the prediction accuracy
of aspect opinion ratings. In addition, Wang et al
(2010) proposed the topic of latent aspect rating
which aimed to infer the opinion rating on the as-
pect. They first employed a bootstrapping-based al-
gorithm to identify the major aspects via a few seed
word aspects. They then proposed a generative La-
tent Rating Regression model (LRR) to infer aspect
opinion ratings based on the review content and the
associated overall rating.
While there were usually huge collection of re-
views, some works had concerned the topic of
aspect-based sentiment summarization to combat
the information overload. They aimed to summa-
rize all the reviews and integrate major opinions on
various aspects for a given product. For example,
Titov et al (2008) explored a topic modeling method
to generate a summary based on multiple aspects.
They utilized topics to describe aspects and incor-
1503
porated a regression model fed by the ground-truth
opinion ratings. Additionally, Lu el al. (2009) pro-
posed a structured PLSA method, which modeled
the dependency structure of terms, to extract the as-
pects in the reviews. They then aggregated opinions
on each specific aspects and selected representative
text segment to generate a summary.
In addition, some works proposed the topic of
product ranking which aimed to identify the best
products for each specific aspect (Zhang et al,
2010). They used a PageRank style algorithm to
mine the aspect-opinion graph, and to rank the prod-
ucts for each aspect.
Different from previous researches, we dedicate
our work to identifying the important aspects from
the consumer reviews of a specific product.
6 Conclusions and Future Works
In this paper, we have proposed to identify the im-
portant aspects of a product from online consumer
reviews. Our assumption is that the important as-
pects of a product should be the aspects that are fre-
quently commented by consumers and consumers?
opinions on the important aspects greatly influence
their overall opinions on the product. Based on this
assumption, we have developed an aspect ranking al-
gorithm to identify the important aspects by simulta-
neously considering the aspect frequency and the in-
fluence of consumers? opinions given to each aspect
on their overall opinions. We have conducted exper-
iments on 11 popular products in four domains. Ex-
perimental results have demonstrated the effective-
ness of our approach on important aspects identifi-
cation. We have further applied the aspect ranking
results to the application of document-level senti-
ment classification, and have significantly improved
the classification performance. In the future, we will
apply our approach to support other applications.
Acknowledgments
This work is supported in part by NUS-Tsinghua Ex-
treme Search (NExT) project under the grant num-
ber: R-252-300-001-490. We give warm thanks to
the project and anonymous reviewers for their com-
ments.
References
P. Beineke, T. Hastie, C. Manning, and S. Vaithyanathan.
An Exploration of Sentiment Summarization. AAAI,
2003.
G. Carenini, R.T. Ng, and E. Zwart. Extracting Knowl-
edge from Evaluative Text. K-CAP, 2005.
G. Carenini, R.T. Ng, and E. Zwart. Multi-document
Summarization of Evaluative Text. ACL, 2006.
China Unicom 100 Customers iPhone User Feedback
Report, 2009.
Y. Choi and C. Cardie. Hierarchical Sequential Learning
for Extracting Opinions and Their Attributes. ACL,
2010.
H. Cui, V. Mittal, and M. Datar. Comparative Experi-
ments on Sentiment Classification for Online Product
Reviews. AAAI, 2006.
S. Dasgupta and V. Ng. Mine the Easy, Classify the Hard:
A Semi-supervised Approach to Automatic Sentiment
Classification. ACL, 2009.
K. Dave, S. Lawrence, and D.M. Pennock. Opinion Ex-
traction and Semantic Classification of Product Re-
views. WWW, 2003.
A. Esuli and F. Sebastiani. A Publicly Available Lexical
Resource for Opinion Mining. LREC, 2006.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. Un-
supervised Named-entity Extraction from theWeb: An
Experimental Study. Artificial Intelligence, 2005.
J. Fang, B. Price, and L. Price. Pruning Non-Informative
Text Through Non-Expert Annotations to Improve
Aspect-Level Sentiment Classification. COLING,
2010.
O. Feiguina and G. Lapalme. Query-based Summariza-
tion of Customer Reviews. AI, 2007.
Forrester Research. State of Retailing Online 2009: Mar-
keting Report. http://www.shop.org/soro, 2009.
A. Goldberg and X. Zhu. Seeing Stars when There aren?t
Many Stars: Graph-based Semi-supervised Learning
for Sentiment Categorization. ACL, 2006.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
Pulse: Mining Customer Opinions from Free Text.
IDA, 2005.
M. Hu and B. Liu. Mining and Summarizing Customer
Reviews. SIGKDD, 2004.
K. Jarvelin and J. Kekalainen. Cumulated Gain-based
Evaluation of IR Techniques. TOIS, 2002.
S. Kim and E. Hovy. Determining the Sentiment of Opin-
ions. COLING, 2004.
J. Kim, J.J. Li, and J.H. Lee. Discovering the Discrimi-
native Views: Measuring Term Weights for Sentiment
Analysis. ACL, 2009.
1504
Kelsey Research and comscore. Online Consumer-
Generated Reviews Have Significant Impact on Offline
Purchase Behavior.
K. Lerman, S. Blair-Goldensohn, and R. McDonald.
Sentiment Summarization: Evaluating and Learning
User Preferences. EACL, 2009.
B. Li, L. Zhou, S. Feng, and K.F. Wong. A Unified Graph
Model for Sentence-Based Opinion Retrieval. ACL,
2010.
T. Li and Y. Zhang, and V. Sindhwani. A Non-negative
Matrix Tri-factorization Approach to Sentiment Clas-
sification with Lexical Prior Knowledge. ACL, 2009.
B. Liu, M. Hu, and J. Cheng. Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web. WWW,
2005.
B. Liu. Handbook Chapter: Sentiment Analysis and Sub-
jectivity. Handbook of Natural Language Processing.
Marcel Dekker, Inc. New York, NY, USA, 2009.
Y. Lu, C. Zhai, and N. Sundaresan. Rated Aspect Sum-
marization of Short Comments. WWW, 2009.
L.M. Manevitz and M. Yousef. One-class svms for Doc-
ument Classification. The Journal of Machine Learn-
ing, 2002.
R. McDonal, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. Structured Models for Fine-to-coarse Sen-
timent Analysis. ACL, 2007.
Q.Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. Topic
Sentiment Mixture: Modeling Facets and Opinions in
Weblogs. WWW, 2007.
H.J. Min and J.C. Park. Toward Finer-grained Sentiment
Identification in Product Reviews Through Linguistic
and Ontological Analyses. ACL, 2009.
T. Mullen and N. Collier. Sentiment Analysis using
Support Vector Machines with Diverse Information
Sources. EMNLP, 2004.
N. Nanas, V. Uren, and A.D. Roeck. Building and Ap-
plying a Concept Hierarchy Representation of a User
Profile. SIGIR, 2003.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Optimizing Informativeness and Readability for Senti-
ment Summarization. ACL, 2010.
B. Ohana and B. Tierney. Sentiment Classification of Re-
views Using SentiWordNet. IT&T Conference, 2009.
G. Paltoglou and M. Thelwall. A study of Information
Retrieval Weighting Schemes for Sentiment Analysis.
ACL, 2010.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Tech-
niques. EMNLP, 2002.
B. Pang, L. Lee, and S. Vaithyanathan. A Sentimen-
tal Education: Sentiment Analysis using Subjectivity
Summarization based on Minimum cuts Techniques.
ACL, 2004.
B. Pang and L. Lee. Seeing stars: Exploiting Class Re-
lationships for Sentiment Categorization with Respect
to Rating Scales. ACL, 2005.
B. Pang and L. Lee. Opinion mining and sentiment
analysis. Foundations and Trends in Information Re-
trieval, 2008.
A.-M. Popescu and O. Etzioni. Extracting Product Fea-
tures and Opinions from Reviews. HLT/EMNLP,
2005.
R. Prabowo and M. Thelwall. Sentiment analysis: A
Combined Approach. Journal of Informetrics, 2009.
G. Qiu, B. Liu, J. Bu, and C. Chen.. Expanding Domain
Sentiment Lexicon through Double Propagation. IJ-
CAI, 2009.
M. Sanderson and B. Croft. Document-word Co-
regularization for Semi-supervised Sentiment Analy-
sis. ICDM, 2008.
B. Snyder and R. Barzilay. Multiple Aspect Ranking us-
ing the Good Grief Algorithm. NAACL HLT, 2007.
S. Somasundaran, G. Namata, L. Getoor, and J. Wiebe.
Opinion Graphs for Polarity and Discourse Classifica-
tion. ACL, 2009.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
C. Toprak, N. Jakob, and I. Gurevych. Sentence and
Expression Level Annotation of Opinions in User-
Generated Discourse. ACL, 2010.
P. Turney. Thumbs up or Thumbs down? Semantic Ori-
entation Applied to Unsupervised Classification of Re-
views. ACL, 2002.
I. Titov and R. McDonald. A Joint Model of Text and
Aspect Ratings for Sentiment Summarization. ACL,
2008.
H. Wang, Y. Lu, and C.X. Zhai. Latent Aspect Rating
Analysis on Review Text Data: A Rating Regression
Approach. KDD, 2010.
B. Wei and C. Pal. Cross Lingual Adaptation: An Exper-
iment on Sentiment Classifications. ACL, 2010.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
T. Wilson and J. Wiebe. Annotating Attributions and Pri-
vate States. ACL, 2005.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. Phrase Depen-
dency Parsing for Opinion Mining. ACL, 2009.
K. Zhang, R. Narayanan, and A. Choudhary. Voice of
the Customers: Mining Online Customer Reviews for
Product Feature-based Ranking. WOSN, 2010.
J. Zhu, H. Wang, and B.K. Tsou. Aspect-based Sentence
Segmentation for Sentiment Summarization. TSA,
2009.
L. Zhuang, F. Jing, and X.Y. Zhu. Movie Review Mining
and Summarization. CIKM, 2006.
1505
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 582?591,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Community Answer Summarization for Multi-Sentence Question with
Group L1 Regularization
Wen Chan?, Xiangdong Zhou?, Wei Wang?, Tat-Seng Chua?
?School of Computer Science, Fudan University, Shanghai, 200433, China
{11110240007,xdzhou,weiwang1}@fudan.edu.cn
?School of Computing, National University of Singapore
chuats@nus.edu.sg
Abstract
We present a novel answer summarization
method for community Question Answering
services (cQAs) to address the problem of ?in-
complete answer?, i.e., the ?best answer? of a
complex multi-sentence question misses valu-
able information that is contained in other an-
swers. In order to automatically generate a
novel and non-redundant community answer
summary, we segment the complex original
multi-sentence question into several sub ques-
tions and then propose a general Conditional
Random Field (CRF) based answer summary
method with group L1 regularization. Vari-
ous textual and non-textual QA features are
explored. Specifically, we explore four differ-
ent types of contextual factors, namely, the in-
formation novelty and non-redundancy mod-
eling for local and non-local sentence inter-
actions under question segmentation. To fur-
ther unleash the potential of the abundant cQA
features, we introduce the group L1 regu-
larization for feature learning. Experimental
results on a Yahoo! Answers dataset show
that our proposed method significantly outper-
forms state-of-the-art methods on cQA sum-
marization task.
1 Introduction
Community Question and Answering services
(cQAs) have become valuable resources for users
to pose questions of their interests and share their
knowledge by providing answers to questions. They
perform much better than the traditional frequently
asked questions (FAQ) systems (Jijkoun and Rijke
, 2005; Riezler et al, 2007) which are just based
on natural language processing and information re-
trieving technologies due to the need for human in-
telligence in user generated contents(Gyongyi et al,
2007). In cQAs such as Yahoo! Answers, a resolved
question often gets more than one answers and a
?best answer? will be chosen by the asker or voted
by other community participants. This {question,
best answer} pair is then stored and indexed for fur-
ther uses such as question retrieval. It performs very
well in simple factoid QA settings, where the an-
swers to factoid questions often relate to a single
named entity like a person, time or location. How-
ever, when it comes to the more sophisticated multi-
sentence questions, it would suffer from the problem
of ?incomplete answer?. That is, such question often
comprises several sub questions in specific contexts
and the asker wishes to get elaborated answers for as
many aspects of the question as possible. In which
case, the single best answer that covers just one or
few aspects may not be a good choice (Liu et al,
2008; Takechi et al, 2007). Since ?everyone knows
something? (Adamic et al, 2008), the use of a single
best answer often misses valuable human generated
information contained in other answers.
In an early literature, Liu et al(2008) reported that
no more than 48% of the 400 best answers were in-
deed the unique best answers in 4 most popular Ya-
hoo! Answers categories. Table 1 shows an example
of the ?incomplete answer? problem from Yahoo!
Answers1. The asker wishes to know why his teeth
bloods and how to prevent it. However, the best an-
swer only gives information on the reason of teeth
1http://answers.yahoo.com/question/index?qid=
20100610161858AAmAGrV
582
blooding. It is clear that some valuable information
about the reasons of gums blooding and some solu-
tions are presented in other answers.
Question
Why do teeth bleed at night and how do you prevent/stop it? This
morning I woke up with blood caked between my two front teeth.
This is the third morning in a row that it has happened. I brush and
floss regularly, and I also eat a balanced, healthy diet. Why is this
happening and how do I stop it?
Best Answer - Chosen by Asker
Periodontal disease is a possibility, gingivitis, or some gum infec-
tion. Teeth don?t bleed; gums bleed.
Other Answers
Vitamin C deficiency!
Ever heard of a dentist? Not all the problems in life are solved on
the Internet.
You could be brushing or flossing too hard. Try a brush with softer
bristles or brushing/flossing lighter and slower. If this doesn?t solve
your problem, try seeing a dentist or doctor. Gums that bleed could
be a sign of a more serious issue like leukemia, an infection, gum
disease, a blood disorder, or a vitamin deficiency.
wash your mouth with warm water and salt, it will help to strengthen
your gum and teeth, also salt avoid infection. You probably have
weak gums, so just try to follow the advice, it works in many cases
of oral problems.
Table 1: An example of question with incomplete answer
problem from Yahoo! Answers. The ?best answer? seems to
miss valuable information and will not be ideal for re-use when
similar question is asked again.
In general, as noted in (Jurafsky and Martin ,
2009), most interesting questions are not factoid
questions. User?s needs require longer, more infor-
mative answers than a single phrase. In fact, it is
often the case, that a complex multi-sentence ques-
tion could be answered from multiple aspects by dif-
ferent people focusing on different sub questions.
Therefore we address the incomplete answer prob-
lem by developing a novel summarization technique
taking different sub questions and contexts into con-
sideration. Specifically we want to learn a concise
summary from a set of corresponding answers as
supplement or replacement to the ?best answer?.
We tackle the answer summary task as a sequen-
tial labeling process under the general Conditional
Random Fields (CRF) framework: every answer
sentence in the question thread is labeled as a sum-
mary sentence or non-summary sentence, and we
concatenate the sentences with summary label to
form the final summarized answer. The contribution
of this paper is two-fold:
First, we present a general CRF based framework
and incorporate four different contextual factors
based on question segmentation to model the local
and non-local semantic sentence interactions to ad-
dress the problem of redundancy and information
novelty. Various textual and non-textual question
answering features are exploited in the work.
Second, we propose a group L1-regularization ap-
proach in the CRF model for automatic optimal fea-
ture learning to unleash the potential of the features
and enhance the performance of answer summariza-
tion.
We conduct experiments on a Yahoo! Answers
dataset. The experimental results show that the
proposed model improve the performance signifi-
cantly(in terms of precision, recall and F1 measures)
as well as the ROUGE-1, ROUGE-2 and ROUGE-L
measures as compared to the state-of-the-art meth-
ods, such as Support Vector Machines (SVM), Lo-
gistic Regression (LR) and Linear CRF (LCRF)
(Shen et al, 2007).
The rest of the paper is arranged as follows: Sec-
tion 2 presents some definitions and a brief review
of related research. In Section 3, we propose the
summarization framework and then in Section 4 and
5 we detail the experimental setups and results re-
spectively. We conclude the paper in Section 6.
2 Definitions and Related Work
2.1 Definitions
In this subsection we define some concepts that
would be helpful to clarify our problems. First we
define a complex multi-sentence question as a ques-
tion with the following properties:
Definition: A complex multi-sentence question
is one that contains multiple sub-questions.
In the cQAs scenario a question often consists of
one or more main question sentences accompany by
some context sentences described by askers. We
treat the original question and context as a whole
single complex multi-sentence question and obtain
the sub questions by question segmentation. We
then define the incomplete answer problem as:
Definition: The incomplete answer problem is
one where the best answer of a complex multi-
sentence question is voted to be below certain star
ratings or the average similarity between the best an-
swer and all the sub questions is below some thresh-
583
olds.
We study the issues of similarity threshold and the
minimal number of stars empirically in the experi-
mental section and show that they are useful in iden-
tifying questions with the incomplete answer prob-
lem.
2.2 Related Work
There exist several attempts to alleviate the answer
completeness problem in cQA. One of them is to
segment the multi-sentence question into a set of
sub-questions along with their contexts, then se-
quentially retrieve the sub questions one by one,
and return similar questions and their best answers
(Wang et al, 2010). This strategy works well in gen-
eral, however, as the automatic question segmenta-
tion is imperfect and the matched similar questions
are likely to be generated in different contextual sit-
uations, this strategy often could not combine multi-
ple independent best answers of sub questions seam-
lessly and may introduce redundancy in final answer.
On general problem of cQA answer summariza-
tion, Liu et al(2008) manually classified both ques-
tions and answers into different taxonomies and ap-
plied clustering algorithms for answer summariza-
tion.They utilized textual features for open and opin-
ion type questions. Through exploiting metadata,
Tomasoni and Huang(2010) introduced four char-
acteristics (constraints) of summarized answer and
combined them in an additional model as well as
a multiplicative model. In order to leverage con-
text, Yang et al(2011) employed a dual wing fac-
tor graph to mutually enhance the performance of
social document summarization with user generated
content like tweets. Wang et al (2011) learned on-
line discussion structures such as the replying rela-
tionship by using the general CRFs and presented a
detailed description of their feature designs for sites
and edges embedded in discussion thread structures.
However there is no previous work that explores the
complex multi-sentence question segmentation and
its contextual modeling for community answer sum-
marization.
Some other works examined the evaluation of the
quality of features for answers extracted from cQA
services (Jeon et al, 2006; Hong and Davison ,
2009; Shah et al, 2010). In the work of Shah et
al.(2010), a large number of features extracted for
predicting asker-rated quality of answers was evalu-
ated by using a logistic regression model. However,
to the best of our knowledge, there is no work in
evaluating the quality of features for community an-
swer summarization. In our work we model the fea-
ture learning and evaluation problem as a group L1
regularization problem (Schmidt , 2010) on different
feature groups.
3 The Summarization Framework
3.1 Conditional Random Fields
We utilize the probabilistic graphical model to solve
the answer summarization task, Figure 1 gives some
illustrations, in which the sites correspond to the
sentences and the edges are utilized to model the
interactions between sentences. Specifically, let x
be the sentence sequence to all answers within a
question thread, and y be the corresponding label se-
quence. Every component yi of y has a binary value,
with +1 for the summary sentence and -1 otherwise.
Then under CRF (Lafferty et al, 2001), the condi-
tional probability of y given x obeys the following
distribution:
p(y|x) = 1Z(x)exp(
?
v?V,l
?lgl(v, y|v, x)
+
?
e?E,k
?kfk(e, y|e, x)),
(1)
where Z(x) is the normalization constant called
partition function, gl denotes the cQA feature func-
tion of site l, fk denotes the function of edge k( mod-
eling the interactions between sentences), ? and ?
are respectively the weights of function of sites and
edges, and y|t denotes the components of y related
to site (edge) t.
3.2 cQA Features and Contextual Modeling
In this section, we give a detailed description of
the different sentence-level cQA features and the
contextual modeling between sentences used in our
model for answer summarization.
Sentence-level Features
Different from the conventional multi-document
summarization in which only the textual features are
utilized, we also explore a number of non-textual
author related features (Shah et al, 2010) in cQAs.
584
The textual features used are:
1. Sentence Length: The length of the sentence in the
answers with the stop words removed. It seems that a
long sentence may contain more information.
2. Position: The sentence?s position within the answer. If
a sentence is at the beginning or at the end of one answer,
it might be a generation or viewpoint sentence and will
be given higher weight in the summarization task.
3. Answer Length: The length of the answer to which the
sentence belonged, again with the stop words removed.
4. Stopwords Rate: The rate of stop words in the
sentence. If a sentence contains too many stop words, it
is more likely a spam or chitchat sentence rather than an
informative one.
5. Uppercase Rate: The rate of uppercase words.
Uppercase words are often people?s name, address or
other name entities interested by askers.
6. Has Link Whether the sentence contains a hyperlink
or not. The link often points to a more detailed informa-
tion source.
7. Similarity to Question: Semantic similarity to the
question and question context. It imports the semantic
information relevance to the question and question
context.
The non-textual features used include:
8. Best Answer Star: The stars of the best answer
received by the askers or voters.
9. Thumbs Up: The number of thumbs-ups the answer
which contains the sentence receives. Users are often
used to support one answer by giving a thumbs up after
reading some relevant or interesting information for their
intentions.
10. Author Level: The level of stars the author who gives
the answer sentence acquires. The higher the star level,
the more authoritative the asker is.
11. Best Answer Rate: Rate of answers annotated as the
best answer the author who gives the answer sentence
receives.
12. Total Answer Number: The number of total answers
by the author who gives the answer sentence. The
more answers one gives, the more experience he or she
acquires.
13. Total Points: The total points that the author who
gives the answer sentence receives.
The previous literature (Shah et al, 2010) hinted
that some cQA features, such as Sentence Length,
Has Link and Best Answer Star, may be more im-
portant than others. We also expect that some fea-
ture may be redundant when their most related fea-
tures are given, e.g., the Author Level feature is pos-
itively related with the Total Points received by an-
swerers, and Stopwords Rate is of little help when
both Sentence Length (not including stop words) and
Uppercase Rate are given. Therefore, to explore the
optimal combination of these features, we propose
a group L1 regularization term in the general CRF
model (Section 3.3) for feature learning.
All features presented here can be extracted au-
tomatically from the Yahoo! Answers website. We
normalize all these feature values to real numbers
between 0 and 1 by dividing them by the corre-
sponding maximal value of these features. These
sentence-level features can be easily utilized in the
CRF framework. For instance, if the rate of upper-
case words is prominent or the position is close to
the beginning or end of the answer, then the proba-
bility of the label +1 (summary sentence) should be
boosted by assigning it with a large value.
Contextual Modeling Under Question
Segmentation
For cQAs summarization, the semantic interac-
tions between different sentence sites are crucial,
that is, some context co-occurrences should be en-
couraged and others should be penalized for require-
ments of information novelty and non-redundancy
in the generated summary. Here we consider both
local (sentences from the same answer) and global
(sentences from different answers) settings. This
give rise to four contextual factors that we will ex-
plore for modeling the pairwise semantic interac-
tions based on question segmentation. In this paper,
we utilize a simple but effective lightweight ques-
tion segmentation method (Ding et al, 2008; Wang
et al, 2010). It mainly involves the following two
steps:
Step 1. Question sentence detection: every sen-
tence in the original multi-sentence question is clas-
sified into question sentence and non-question (con-
text) sentence. The question mark and 5W1H fea-
tures are applied.
Step 2. Context assignment: every context sen-
tence is assigned to the most relevant question sen-
tence. We compute the semantic similarity(Simpson
and Crowe, 2005) between sentences or sub ques-
585
Figure 1: Four kinds of the contextual factors are considered for answer summarization in our general CRF based
models.
tions as:
sim(x, y) = 2?
?
(w1,w2)?M(x,y)
sim(w1, w2)
|x|+ |y| (2)
where M(x, y) denotes synset pairs matched in sen-
tences x and y; and the similarity between the two
synsets w1 and w2 is computed to be inversely pro-
portional to the length of the path in Wordnet.
One answer sentence may related to more than
one sub questions to some extent. Thus, we de-
fine the replied question Qri as the sub question
with the maximal similarity to sentence xi: Qri =
argmaxQjsim(xi, Qj). It is intuitive that different
summary sentences aim at answering different sub
questions. Therefore, we design the following two
contextual factors based on the similarity of replied
questions.
Dissimilar Replied Question Factor: Given two
answer sentences xi , xj and their corresponding
replied questions Qri, Qrj . If the similarity2 of Qri
and Qrj is below some threshold ?lq, it means that
xi and xj will present different viewpoints to answer
different sub questions. In this case, it is likely that
xi and xj are both summary sentences; we ensure
this by setting the contextual factor cf1 with a large
value of exp ?, where ? is a positive real constant
often assigned to value 1; otherwise we set cf1 to
exp? ? for penalization.
cf1 =
{
exp ?, yi = yj = 1
exp? ?, otherwise
Similar Replied Question Factor: Given two an-
2We use the semantic similarity of Equation 2 for all our
similarity measurement in this paper.
swer sentences xi , xj and their corresponding
replied questions Qri, Qrj . If the similarity of Qri
and Qrj is above some upper threshold ?uq, this
means that xi and xj are very similar and likely to
provide similar viewpoint to answer similar ques-
tions. In this case, we want to select either xi or
xj as answer. This is done by setting the contextual
factor cf2 such that xi and xj have opposite labels,
cf2 =
{
exp ?, yi ? yj = ?1
exp ? ?, otherwise
Assuming that sentence xi is selected as a sum-
mary sentence, and its next local neighborhood sen-
tence xi+1 by the same author is dissimilar to it but
it is relevant to the original multi-sentence question,
then it is reasonable to also pick xi+1 as a summary
sentence because it may offer new viewpoints by
the author. Meanwhile, other local and non-local
sentences which are similar to it at above the up-
per threshold will probably not be selected as sum-
mary sentences as they offer similar viewpoint as
discussed above. Therefore, we propose the follow-
ing two kinds of contextual factors for selecting the
answer sentences in the CRF model.
Local Novelty Factor: If the similarity of answer
sentence xi and xi+1 given by the same author is
below a lower threshold ?ls, but their respective sim-
ilarities to the sub questions both exceed an upper
threshold ?us, then we will boost the probability of
selecting both as summary sentences by setting:
cf3 =
{
exp ?, yi = yi+1 = 1
exp? ?, otherwise
Redundance Factor: If the similarity of answer
586
sentence xi and xj is greater than the upper thresh-
old ?us, then they are likely to be redundant and
hence should be given opposite labels. This is done
by setting:
cf4 =
{
exp ?, yi ? yj = ?1
exp? ?, otherwise
Figure 1 gives an illustration of these four con-
textual factors in our proposed general CRF based
model. The parameter estimation and model infer-
ence are discussed in the following subsection.
3.3 Group L1 Regularization for Feature
Learning
In the context of cQA summarization task, some fea-
tures are intuitively to be more important than oth-
ers. As a result, we group the parameters in our CRF
model with their related features3 and introduce a
group L1-regularization term for selecting the most
useful features from the least important ones, where
the regularization term becomes,
R(?) = C
G
?
g=1
?
??
?g?2, (3)
where C controls the penalty magnitude of the pa-
rameters, G is the number of feature groups and
??
?g
denotes the parameters corresponding to the partic-
ular group g. Notice that this penalty term is indeed
a L(1, 2) regularization because in every particu-
lar group we normalize the parameters in L2 norm
while the weight of a whole group is summed in L1
form.
Given a set of training data D = (x(i), y(i)), i =
1, ..., N , the parameters ? = (?l, ?k) of the general
CRF with the group L1-regularization are estimated
in using a maximum log likelihood function L as:
L =
N
?
i=1
log(p?(y(i)|x(i)))? C
G
?
g=1
?
??
?g?2, (4)
3We note that every sentence-level feature discussed in Sec-
tion 3.2 presents a variety of instances (e.g., the sentence with
longer or shorter length is the different instance), and we may
call it sub-feature of the original sentence-level feature in the
micro view. Every sub-feature has its corresponding weight in
our CRF model. Whereas in a macro view, those related sub-
features can be considered as a group.
where N denotes the total number of training sam-
ples. we compute the log-likelihood gradient com-
ponent of ? in the first term of Equation 4 as in
usual CRFs. However, the second term of Equation
4 is non-differentiable when some special ?
??
?g?2 be-
comes exactly zero. To tackle this problem, an ad-
ditional variable is added for each group (Schmidt ,
2010); that is, by replacing each norm ?
??
?g?2 with
the variable ?g, subject to the constraint ?g ?
?
??
?g?2, i.e.,
L =
N
?
i=1
log(p?(y(i)|x(i)))? C
G
?
g=1
?g,
subject to ?g ? ?
??
?g?2,?g.
(5)
This formulation transforms the non-differentiable
regularizer to a simple linear function and maximiz-
ing Equation 5 will lead to a solution to Equation 4
because it is a lower bound of the latter. Then, we
add a sufficient small positive constant ? when com-
puting the L2 norm (Lee et al, 2006), i.e., |
??
?g?2 =
?
?|g|
j=1 ?2gj + ?, where |g| denotes the number of
features in group g. To obtain the optimal value of
parameter ? from the training data, we use an effi-
cient L-BFGS solver to solve the problem, and the
first derivative of every feature j in group g is,
?L
??gj
=
N
?
i=1
Cgj(y(i), x(i))?
N
?
i=1
?
y
p(y|x(i))Cgj(y, x(i))? 2C
?gj
?
?|g|
l=1 ?2gl + ?
(6)
where Cgj(y, x) denotes the count of feature j in
group g of observation-label pair (x, y). The first
two terms of Equation 6 measure the difference be-
tween the empirical and the model expected values
of feature j in group g, while the third term is the
derivative of group L1 priors.
For inference, the labeling sequence can be ob-
tained by maximizing the probability of y condi-
tioned on x,
y? = argmaxyp?(y|x). (7)
We use a modification of the Viterbi algorithm to
perform inference of the CRF with non-local edges
587
previously used in (Galley , 2006). That is , we
replace the edge connection zt = (yt?2, yt?1, yt)
of order-2 Markov model by zt = (yNt , yt?1, yt),
where yNt represents the label at the source of the
non-local edge. Although it is an approximation of
the exact inference, we will see that it works well for
our answer summarization task in the experiments.
4 Experimental Setting
4.1 Dataset
To evaluate the performance of our CRF based an-
swer summarization model, we conduct experiments
on the Yahoo! Answers archives dataset. The Ya-
hoo! WebscopeTM Program4 opens up a number of
Yahoo! Answers datasets for interested academics
in different categories. Our original dataset con-
tains 1,300,559 questions and 2,770,896 answers in
ten taxonomies from Yahoo! Answers. After fil-
tering the questions which have less than 5 answers
and some trivial factoid questions using the features
by (Tomasoni and Huang, 2010) , we reduce the
dataset to 55,132 questions. From this sub-set, we
next select the questions with incomplete answers
as defined in Section 2.1. Specifically, we select the
questions where the average similarity between the
best answer and all sub questions is less than 0.6 or
when the star rating of the best answer is less than 4.
We obtain 7,784 questions after this step. To eval-
uate the effectiveness of this method, we randomly
choose 400 questions in the filtered dataset and in-
vite 10 graduate candidate students (not in NLP re-
search field) to verify whether a question suffers
from the incomplete answer problem. We divide the
students into five groups of two each. We consider
the questions as the ?incomplete answer questions?
only when they are judged by both members in a
group to be the case. As a result, we find that 360
(90%) of these questions indeed suffer from the in-
complete answer problem, which indicates that our
automatic detection method is efficient. This ran-
domly selected 400 questions along with their 2559
answers are then further manually summarized for
evaluation of automatically generated answer sum-
maries by our model in experiments.
4http://sandbox.yahoo.com/
4.2 Evaluation Measures
When taking the summarization as a sequential bi-
classification problem, we can make use of the usual
precision, recall and F1 measures (Shen et al, 2007)
for classification accuracy evaluation.
In our experiments, we also compare the preci-
sion, recall and F1 score in the ROUGE-1, ROUGE-
2 and ROUGE-L measures (Lin , 2004) for answer
summarization performance.
5 Experimental Results
5.1 Summarization Results
We adapt the Support Vector Machine (SVM) and
Logistic Regression (LR) which have been reported
to be effective for classification and the Linear CRF
(LCRF) which is used to summarize ordinary text
documents in (Shen et al, 2007) as baselines for
comparison. To better illustrate the effectiveness of
question segmentation based contextual factors and
the group L1 regularization term, we carry the tests
in the following sequence: (a) we use only the con-
textual factors cf3 and cf4 with default L2 regular-
ization (gCRF); (b) we add the reply question based
factors cf1 and cf2 to the model (gCRF-QS); and (c)
we replace default L2 regularization with our pro-
posed group L1 regularization term (gCRF-QS-l1).
For linear CRF system, we use all our textual and
non-textual features as well as the local (exact pre-
vious and next) neighborhood contextual factors in-
stead of the features of (Shen et al, 2007) for fair-
ness. For the thresholds used in the contextual fac-
tors, we enforce ?lq to be equal to ?ls and ?uq equal
to ?us for the purpose of simplifying the parameters
setting (?lq = ?ls = 0.4, ?uq = ?us = 0.8 in our ex-
periments). We randomly divide the dataset into ten
subsets (every subset with 40 questions and the as-
sociated answers), and conduct a ten-fold cross val-
idation and for each round where the nine subsets
are used to train the model and the remaining one
for testing. The precision, recall and F1 measures of
these models are presented in Table 2.
Table 2 shows that our general CRF model based
on question segmentation with group L1 regulariza-
tion out-performs the baselines significantly in all
three measures (gCRF-QS-l1 is 13.99% better than
SVM in precision, 9.77% better in recall and 11.72%
better in F1 score). We note that both SVM and LR,
588
Model R1 P R1 R R1 F1 R2 P R2 R R2 F1 RL P RL R RL F1
SVM 79.2% 52.5% 63.1% 71.9% 41.3% 52.4% 67.1% 36.7% 47.4%
LR 75.2%? 57.4%? 65.1%? 66.1%? 48.5%? 56.0%? 61.6%? 43.2%? 50.8%?
LCRF 78.7%- 61.8%? 69.3%- 71.4%- 54.1%? 61.6%? 67.1%- 49.6%? 57.0%?
gCRF 81.9%? 65.2%? 72.6%? 76.8%? 57.3%? 65.7%? 73.9%? 53.5%? 62.1%?
gCRF-QS 81.4%- 70.0%? 75.3%? 76.2%- 62.4%? 68.6%? 73.3%- 58.6%? 65.1%?
gCRF-QS-l1 86.6%? 68.3%- 76.4%? 82.6%? 61.5%- 70.5%? 80.4%? 58.2%- 67.5%?
Table 3: The Precision, Recall and F1 of ROUGE-1, ROUGE-2, ROUGE-L in the baselines SVM,LR, LCRF and our
general CRF based models (gCRF, gCRF-QS, gCRF-QS-l1). The down-arrow means performance degradation with
statistical significance.
Model Precision Recall F1
SVM 65.93% 61.96% 63.88%
LR 66.92%- 61.31%- 63.99%-
LCRF 69.80% ? 63.91%- 66.73%?
gCRF 73.77%? 69.43%? 71.53%?
gCRF-QS 74.78%? 72.51%? 73.63%?
gCRF-QS-l1 79.92%? 71.73%- 75.60%?
Table 2: The Precision, Recall and F1 measures of the
baselines SVM,LR, LCRF and our general CRF based
models (gCRF, gCRF-QS, gCRF-QS-l1). The up-arrow
denotes the performance improvement compared to the
precious method (above) with statistical significance un-
der p value of 0.05, the short line ?-? denotes there is no
difference in statistical significance.
which just utilize the independent sentence-level
features, behave not vary well here, and there is no
statistically significant performance difference be-
tween them. We also find that LCRF which utilizes
the local context information between sentences per-
form better than the LR method in precision and F1
with statistical significance. While we consider the
general local and non-local contextual factor cf3 and
cf4 for novelty and non-redundancy constraints, the
gCRF performs much better than LCRF in all three
measures; and we obtain further performance im-
provement by adding the contextual factors based
on QS, especially in the recall measurement. This
is mainly because we have divided the question into
several sub questions, and the system is able to se-
lect more novel sentences than just treating the origi-
nal multi-sentence as a whole. In addition, when we
replace the default L2 regularization by the group
L1 regularization for more efficient feature weight
learning, we obtain a much better performance in
precision while not sacrificing the recall measure-
ment statistically.
We also compute the Precision, Recall and F1
in ROUGE-1, ROUGE-2 and ROUGE-L measure-
ments, which are widely used to measure the quality
of automatic text summarization. The experimental
results are listed in Table 3. All results in the Ta-
ble are the average of the ten-fold cross validation
experiments on our dataset.
It is observed that our gCRF-QS-l1 model im-
proves the performance in terms of precision, recall
and F1 score on all three measurements of ROUGE-
1, ROUGE-2 and ROUGE-L by a significant mar-
gin compared to other baselines due to the use of
local and non-local contextual factors and factors
based on QS with group L1 regularization. Since
the ROUGEmeasures care more about the recall and
precision of N-grams as well as common substrings
to the reference summary rather than the whole sen-
tence, they offer a better measurement in modeling
the user?s information needs. Therefore, the im-
provements in these measures are more encouraging
than those of the average classification accuracy for
answer summarization.
From the viewpoint of ROUGE measures we ob-
serve that our question segmentation method can en-
hance the recall of the summaries significantly due
to the more fine-grained modeling of sub questions.
We also find that the precision of the group L1 reg-
ularization is much better than that of the default
L2 regularization while not hurting the recall signifi-
cantly. In general, the experimental results show that
our proposed method is more effective than other
baselines in answer summarization for addressing
the incomplete answer problem in cQAs.
589
Figure 2: The accumulated weight of each site feature
group in the group L1-regularization to our Yahoo! An-
swer dataset. The horizonal axis corresponds to the name
of each feature group.
5.2 Evaluation of Feature Learning
For group L1 regularization term, we set the ? =
10?4 in Equation 6. To see how much the dif-
ferent textual and non-textual features contribute to
community answer summarization, the accumulated
weight of each group of sentence-level features5 is
presented in Figure 2. It shows that the textual fea-
tures such as 1 (Sentence Length), 2 (Position) 3 (An-
swer Length), 6 (Has Link) and non-textual features
such as 8 (Best Answer Star) , 12 (Total Answer
Number) as well as 13 (Total Points) have larger
weights, which play a significant role in the sum-
marization task as we intuitively considered; fea-
tures 4 (Stopwords Rate), 5 (Uppercase Rate) and 9
(Thumbs Up) have medium weights relatively; and
the other features like 7 (Similarity to Question), 10
(Author Level) and 11 (Best Answer Rate) have the
smallest accumulated weights. The main reasons
that the feature 7 (Similarity to Question) has low
contribution is that we have utilized the similarity
to question in the contextual factors, and this simi-
larity feature in the single site becomes redundant.
Similarly, the features Author Level and Best An-
swer Number are likely to be redundant when other
non-textual features(Total Answer Number and To-
tal Points) are presented together. The experimental
results demonstrate that with the use of group L1-
regularization we have learnt better combination of
these features.
5Note that we have already evaluated the contribution of the
contextual factors in Section 5.1.
5.3 An Example of Summarized Answer
To demonstrate the effectiveness of our proposed
method, Table 4 shows the generated summary of
the example question which is previously illustrated
in Table 1 in the introduction section. The best an-
swer available in the system and the summarized an-
swer generated by our model are compared in Table
4. It is found that the summarized answer contains
more valuable information about the original multi-
sentence question, as it better answers the reason of
teeth blooding and offers some solution for it. Stor-
ing and indexing this summarized answer in ques-
tion archives should provide a better choice for an-
swer reuse in question retrieval of cQAs.
Question
Why do teeth bleed at night and how do you prevent/stop it? This
morning I woke up with blood caked between my two front teeth.[...]
Best Answer - Chosen by Asker
Periodontal disease is a possibility, gingivitis, or some gum infec-
tion. Teeth don?t bleed; gums bleed.
Summarized Answer Generated by Our Method
Periodontal disease is a possibility, gingivitis, or some gum infec-
tion. Teeth don?t bleed; gums bleed. Gums that bleed could be a
sign of a more serious issue like leukemia, an infection, gum dis-
ease, a blood disorder, or a vitamin deficiency. wash your mouth
with warm water and salt, it will help to strengthen your gum and
teeth, also salt avoid infection.
Table 4: Summarized answer by our general CRF based model
for the question in Table 1.
6 Conclusions
We proposed a general CRF based community an-
swer summarization method to deal with the in-
complete answer problem for deep understanding of
complex multi-sentence questions. Our main con-
tributions are that we proposed a systematic way
for modeling semantic contextual interactions be-
tween the answer sentences based on question seg-
mentation and we explored both the textual and non-
textual answer features learned via a group L1 reg-
ularization. We showed that our method is able to
achieve significant improvements in performance of
answer summarization compared to other baselines
and previous methods on Yahoo! Answers dataset.
We planed to extend our proposed model with more
advanced feature learning as well as enriching our
summarized answer with more available Web re-
590
sources.
Acknowledgements
This work was supported by the NSFC under Grant
No.61073002 and No.60773077.
References
L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman.
2008. Knowledge sharing and yahoo answers: every-
one knows something. Proceedings of WWW 2008.
Shilin Ding, Gao Cong, Chin-Yew Lin and Xiaoyan Zhu.
2008. Rouge: Using Conditional Random Fields to
Extract Contexts and Answers of Questions from On-
line Forums. Proceedings of ACL-08: HLT, pages
710?718.
Michel Galley. 2006. A Skip-Chain Conditional Ran-
dom Field for Ranking Meeting Utterances by Impor-
tance. Proceedings of EMNLP 2006.
Z. Gyongyi, G. Koutrika, J. Pedersen, and H. Garcia-
Molina. 2007. Questioning yahoo! answers. Tech-
nical report. Stanford InfoLab.
F. Maxwell Harper, Daphne Raban, Sheizaf Rafaeli, and
Joseph A. Konstan. 2008. Predictors of Answer Qual-
ity in Online Q&A Sites. Proceedings of CHI 2008.
Liangjie Hong and Brian D. Davison. 2009. A
Classification-based Approach to Question Answering
in Discussion Boards. Proceedings of the 32th ACM
SIGIR Conference, pages 171?178.
Eduard Hovy, Chin Y. Lin, and Liang Zhou. 2005. A
BE-based Multi-document Summarization with Sen-
tence Compression. Proceedings of Multilingual Sum-
marization Evaluation (ACL 2005 workshop).
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee and Soyeon
Park 2006. A Framework to Predict the Quality of
Answers with NonTextual Features. Proceedings of
the 29th ACM SIGIR Conference, pages 228?235.
V. Jijkoun and M. de Rijke. 2005. Retrieving answers
from frequently asked questions pages on the web. In
CIKM.
Daniel Jurafsky and James H. Martin. 2009. Speech
and Language Processing: An introduction to natural
language processing, computational linguistics, and
speech recognition. Published by Pearson Education.
John D. Lafferty, Andrew McCallum, and Fernando C.
N. Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. Proceedings of the 18th ICML, pages 282?289.
S. Lee, H. Lee, P. Abbeel, and A. Ng. 2006. Efficient L1
Regularized Logistic Regression. In AAAI.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. Proceedings of ACL Work-
shop, pages 74?81.
Yandong Liu, Jiang Bian, and Eugene Agichtein. 2008.
Predicting Information Seeker Satisfaction in Commu-
nity Question Answering. Proceedings of the 31th
ACM SIGIR Conference.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,
Dingyi Han, and Yong Yu. 2008. Understanding and
summarizing answers in community-based question
answering services. Proceedings of the 22nd ICCL,
pages 497?504.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. Proceedings of the 45th
Annual Meeting of ACL.
Mark Schmidt. 2010. Graphical Model Structure Learn-
ing with L1-Regularization. Doctoral Thesis.
Chirag Shah and Jefferey Pomerantz. 2010. Evaluat-
ing and Predicting Answer Quality in Community QA.
Proceedings of the 33th ACM SIGIR Conference.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang and Zheng
Chen. 2007. Document Summarization using Condi-
tional Random Fields. Proceedings of the 20th IJCAI.
Troy Simpson and Malcolm Crowe 2005. WordNet.Net
http://opensource.ebswift.com/WordNet.Net
Mineki Takechi, Takenobu Tokunaga, and Yuji Mat-
sumoto. 2007. Chunking-based Question Type Iden-
tification for Multi-Sentence Queries. Proceedings of
SIGIR 2007 Workshop.
Mattia Tomasoni and Minlie Huang. 2010. Metadata-
Aware Measures for Answer Summarization in Com-
munity Question Answering. Proceedings of the 48th
Annual Meeting of ACL, pages 760?769.
Hongning Wang, Chi Wang, ChengXiang Zhai, Jiawei
Han 2011. Learning Online Discussion Structures by
Conditional Random Fields. Proceedings of the 34th
ACM SIGIR Conference.
Kai Wang, Zhao-Yan Ming and Tat-Seng Chua. 2009.
A Syntactic Tree Matching Approach to Finding Simi-
lar Questions in Community-based QA Services. Pro-
ceedings of the 32th ACM SIGIR Conference.
Kai Wang, Zhao-Yan Ming, Xia Hu and Tat-Seng Chua.
2010. Segmentation of Multi-Sentence Questions:
Towards Effective Question Retrieval in cQA Ser-
vices. Proceedings of the 33th ACM SIGIR Confer-
ence, pages 387?394.
X. Xue, J.Jeon, and W.B.Croft. 2008. Retrieval models
for question and answers archives. Proceedings of the
31th ACM SIGIR Conference.
Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhou Su, and
Juanzi Li. 2011. Social Context Summarization. Pro-
ceedings of the 34th ACM SIGIR Conference.
Liang Zhou, Chin Y. Lin, and Eduard Hovy. 2006. Sum-
marizing answers for complicated questions. Proceed-
ings of the 5th International Conference on LREC,
Genoa, Italy.
591
