Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 543?550,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Investigation of Question Classifier in Question Answering
Zhiheng Huang
EECS Department
University of California
at Berkeley
CA 94720-1776, USA
zhiheng@cs.berkeley.edu
Marcus Thint
Intelligent Systems Research Center
British Telecom Group
Chief Technology Office
marcus.2.thint@bt.com
Asli Celikyilmaz
EECS Department
University of California
at Berkeley
CA 94720-1776, USA
asli@cs.berkeley.edu
Abstract
In this paper, we investigate how an ac-
curate question classifier contributes to
a question answering system. We first
present a Maximum Entropy (ME) based
question classifier which makes use of
head word features and their WordNet hy-
pernyms. We show that our question clas-
sifier can achieve the state of the art per-
formance in the standard UIUC question
dataset. We then investigate quantitatively
the contribution of this question classifier
to a feature driven question answering sys-
tem. With our accurate question classifier
and some standard question answer fea-
tures, our question answering system per-
forms close to the state of the art using
TREC corpus.
1 Introduction
Question answering has drawn significant atten-
tion from the last decade (Prager, 2006). It at-
tempts to answer the question posed in natural
language by providing the answer phrase rather
than the whole documents. An important step in
question answering (QA) is to classify the ques-
tion to the anticipated type of the answer. For
example, the question of Who discovered x-rays
should be classified into the type of human (indi-
vidual). This information would narrow down the
search space to identify the correct answer string.
In addition, this information can suggest different
strategies to search and verify a candidate answer.
In fact, the combination of question classification
and the named entity recognition is a key approach
in modern question answering systems (Voorhees
and Dang, 2005).
The question classification is by no means triv-
ial: Simply using question wh-words can not
achieve satisfactory results. The difficulty lies
in classifying the what and which type questions.
Considering the example What is the capital of Yu-
goslavia, it is of location (city) type, while What
is the pH scale is of definition type. As with
the previous work of (Li and Roth, 2002; Li and
Roth, 2006; Krishnan et al, 2005; Moschitti et
al., 2007), we propose a feature driven statistical
question classifier (Huang et al, 2008). In partic-
ular, we propose head word feature and augment
semantic features of such head words using Word-
Net. In addition, Lesk?s word sense disambigua-
tion (WSD) algorithm is adapted and the depth of
hypernym feature is optimized. With further aug-
ment of other standard features such as unigrams,
we can obtain accuracy of 89.0% using ME model
for 50 fine classes over UIUC dataset.
In addition to building an accurate question
classifier, we investigate the contribution of this
question classifier to a feature driven question an-
swering rank model. It is worth noting that, most
of the features we used in question answering rank
model, depend on the question type information.
For instance, if a question is classified as a type of
sport, we then only care about whether there are
sport entities existing in the candidate sentences.
It is expected that a fine grained named entity rec-
ognizer (NER) should make good use of the accu-
rate question type information. However, due to
the lack of a fine grained NER tool at hand, we
employ the Stanford NER package (Finkel et al,
2005) which identifies only four types of named
entities. Even with such a coarse named entity
recognizer, the experiments show that the question
classifier plays an important role in determining
the performance of a question answering system.
The rest of the paper is organized as follow-
ing. Section 2 reviews the maximum entropy
model which are used in both question classifica-
tion and question answering ranking. Section 3
presents the features used in question classifica-
tion. Section 4 presents the question classification
543
accuracy over UIUC question dataset. Section 5
presents the question answer features. Section 6
illustrates the results based on TREC question an-
swer dataset. And Section 7 draws the conclusion.
2 Maximum Entropy Models
Maximum entropy (ME) models (Berger et al,
1996; Manning and Klein, 2003), also known as
log-linear and exponential learning models, pro-
vide a general purpose machine learning technique
for classification and prediction which has been
successfully applied to natural language process-
ing including part of speech tagging, named entity
recognition etc. Maximum entropy models can in-
tegrate features from many heterogeneous infor-
mation sources for classification. Each feature
corresponds to a constraint on the model. Given
a training set of (C,D), where C is a set of class
labels and D is a set of feature represented data
points, the maximal entropy model attempts to
maximize the log likelihood
logP (C|D,?) =
?
(c,d)?(C,D)
log
exp
?
i
?
i
f
i
(c, d)
?
c
?
exp
?
j
?
j
f
i
(c, d)
,
(1)
where f
i
(c, d) are feature indicator functions. We
use ME models for both question classification
and question answer ranking. In question answer
context, such function, for instance, could be the
presence or absence of dictionary entities (as pre-
sented in Section 5.2) associated with a particular
class type (either true or false, indicating a sen-
tence can or cannot answer the question). ?
i
are
the parameters need to be estimated which reflects
the importance of f
i
(c, d) in prediction.
3 Question Classification Features
Li and Roth (2002) have developed a machine
learning approach which uses the SNoW learning
architecture. They have compiled the UIUC ques-
tion classification dataset1 which consists of 5500
training and 500 test questions.2 All questions in
the dataset have been manually labeled according
to the coarse and fine grained categories as shown
in Table 1, with coarse classes (in bold) followed
by their fine classes.
The UIUC dataset has laid a platform for the
follow-up research including (Hacioglu and Ward,
2003; Zhang and Lee, 2003; Li and Roth, 2006;
1Available at http://12r.cs.uiuc.edu/?cogcomp/Data/QA/QC.
2Test questions are from TREC 10.
Table 1: 6 coarse and 50 fine Question types de-
fined in UIUC question dataset.
ABBR letter desc NUM
abb other manner code
exp plant reason count
ENTITY product HUMAN date
animal religion group distance
body sport individual money
color substance title order
creative symbol desc other
currency technique LOC period
dis.med. term city percent
event vehicle country speed
food word mountain temp
instrument DESC other size
lang definition state weight
Krishnan et al, 2005; Moschitti et al, 2007). In
contrast to Li and Roth (2006)?s approach which
makes use of a very rich feature set, we propose
to use a compact yet effective feature set. The fea-
tures are briefly described as following. More de-
tailed information can be found at (Huang et al,
2008).
Question wh-word The wh-word feature is the
question wh-word in given questions. For ex-
ample, the wh-word of question What is the
population of China is what.
Head Word head word is defined as one single
word specifying the object that the question
seeks. For example the head word of What
is a group of turkeys called, is turkeys. This
is different to previous work including (Li
and Roth, 2002; Krishnan et al, 2005) which
has suggested a contiguous span of words
(a group of turkeys in this example). The
single word definition effectively avoids the
noisy information brought by non-head word
of the span (group in this case). A syntac-
tic parser (Petrov and Klein, 2007) and the
Collins rules (Collins, 1999) are modified to
extract such head words.
WordNet Hypernym WordNet hypernyms are
extracted for the head word of a given ques-
tion. The classic Lesk algorithm (Lesk, 1986)
is used to compute the most probable sense
for a head word in the question context, and
then the hypernyms are extracted based on
that sense. The depth of hypernyms is set to
544
six with trial and error.3 Hypernyms features
capture the general terms of extracted head
word. For instance, the head word of ques-
tion What is the proper name for a female
walrus is extracted as walrus and its direct
hypernyms such as mammal and animal are
extracted as informative features to predict
the correct question type of ENTY:animal.
Unigram words Bag of words features. Such
features provide useful question context in-
formation.
Word shape Five word shape features, namely all
upper case, all lower case, mixed case, all
digits, and other are used to serve as a coarse
named entity recognizer.
4 Question Classification Experiments
We train a Maximum Entropy model using the
UIUC 5500 training questions and test over the
500 test questions. Tables 2 shows the accuracy of
6 coarse class and 50 fine grained class, with fea-
tures being fed incrementally. The question classi-
fication performance is measured by accuracy, i.e.,
the proportion of the correctly classified questions
among all test questions. The baseline using the
Table 2: Question classification accuracy using in-
cremental feature sets for 6 and 50 classes over
UIUC split.
6 class 50 class
wh-word 46.0 46.8
+ head word 92.2 82.0
+ hypernym 91.8 85.6
+ unigram 93.0 88.4
+ word shape 93.6 89.0
wh-head word results in 46.0% and 46.8% respec-
tively for 6 coarse and 50 fine class classification.
The incremental use of head word boosts the accu-
racy significantly to 92.2% and 82.0% for 6 and 50
classes. This reflects the informativeness of such
feature. The inclusion of hypernym feature within
6 depths boosts 3.6% for 50 classes, while result-
ing in slight loss for 6 coarse classes. The further
use of unigram feature leads to 2.8% gain in 50
classes. Finally, the use of word shape leads to
0.6% accuracy increase for 50 classes. The best
3We performed 10 cross validation experiment over train-
ing data and tried various depths of 1, 3, 6, 9 and ?, with ?
signifies that no depth constraint is imposed.
accuracies achieved are 93.6% and 89.0% for 6
and 50 classes respectively.
The individual feature contributions were dis-
cussed in greater detail in (Huang et al, 2008).
Also, The SVM (rathern than ME model) was em-
ployed using the same feature set and the results
were very close (93.4% for 6 class and 89.2% for
50 class). Table 3 shows the feature ablation ex-
periment4 which is missing in that paper. The
experiment shows that the proposed head word
and its hypernym features play an essential role
in building an accurate question classifier.
Table 3: Question classification accuracy by re-
moving one feature at a time for 6 and 50 classes
over UIUC split.
6 class 50 class
overall 93.6 89.0
- wh-word 93.6 89.0
- head word 92.8 88.2
- hypernym 90.8 84.2
- unigram 93.6 86.8
- word shape 93.0 88.4
Our best result feature space only consists of
13?697 binary features and each question has 10
to 30 active features. Compared to the over feature
size of 200?000 in Li and Roth (2002), our feature
space is much more compact, yet turned out to be
more informative as suggested by the experiments.
Table 4 shows the summary of the classification
accuracy of all question classifiers which were ap-
plied to UIUC dataset.5 Our results are summa-
rized in the last row.
In addition, we have performed the 10 cross
validation experiment over the 5500 UIUC train-
ing corpus using our best model. The result is
89.05?1.25 and 83.73?1.61 for 6 and 50 classes,6
which outperforms the best result of 86.1?1.1 for
6 classes as reported in (Moschitti et al, 2007).
5 Question Answer Features
For a pair of a question and a candidate sentence,
we extract binary features which include CoNLL
named entities presence feature (NE), dictionary
4Remove one feature at a time from the entire feature set.
5Note (1) that SNoW accuracy without the related word
dictionary was not reported. With the semantically related
word dictionary, it achieved 91%. Note (2) that SNoW with a
semantically related word dictionary achieved 84.2% but the
other algorithms did not use it.
6These results are worse than the result over UIUC split;
as the UIUC test data includes a larger percentage of easily
classified question types.
545
Table 4: Accuracy of all question classifiers which
were applied to UIUC dataset.
Algorithm 6 class 50 class
Li and Roth, SNoW ?(1) 78.8(2)
Hacioglu et al, SVM+ECOC ? 80.2-82
Zhang & Lee, Linear SVM 87.4 79.2
Zhang & Lee, Tree SVM 90.0 ?
Krishnan et al, SVM+CRF 93.4 86.2
Moschitti et al, Kernel 91.8 ?
Maximum Entropy Model 93.6 89.0
entities presence feature (DIC), numerical entities
presence feature (NUM), question specific feature
(SPE), and dependency validity feature (DEP).
5.1 CoNLL named entities presence feature
We use Stanford named entity recognizer (NER)
(Finkel et al, 2005) to identify CoNLL style NEs7
as possible answer strings in a candidate sentence
for a given type of question. In particular, if the
question is ABBR type, we tag CoNLL LOC,
ORG and MISC entities as candidate answers; If
the question is HUMAN type, we tag CoNLL PER
and ORG entities; And if the question is LOC
type, we tag CoNLL LOC and MISC entities. For
other types of questions, we assume there is no
candidate CoNLL NEs to tag. We create a binary
feature NE to indicate the presence or absence of
tagged CoNLL entities. Further more, we cre-
ate four binary features NE-PER, NE-LOC, NE-
ORG, and NE-MISC to indicate the presence of
tagged CoNLL PER, LOC, ORG and MISC enti-
ties.
5.2 Dictionary entities presence feature
As four types of CoNLL named entities are not
enough to cover 50 question types, we include the
101 dictionary files compiled in the Ephyra project
(Schlaefer et al, 2007). These dictionary files con-
tain names for specific semantic types. For exam-
ple, the actor dictionary comprises a list of actor
names such as Tom Hanks and Kevin Spacey. For
each question, if the head word of such question
(see Section 3) matches the name of a dictionary
file, then each noun phrase in a candidate sentence
is looked up to check its presence in the dictio-
nary. If so, a binary DIC feature is created. For
example, for the question What rank did Chester
7Person (PER), location (LOC), organization (ORG), and
miscellaneous (MISC).
Nimitz reach, as there is a military rank dictionary
matches the head word rank, then all the noun
phrases in a candidate sentence are looked up in
the military rank dictionary. As a result, a sen-
tence contains word Admiral will result in the DIC
feature being activated, as such word is present in
the military rank dictionary.
Note that an implementation tip is to allow the
proximity match in the dictionary look up. Con-
sider the question What film introduced Jar Jar
Binks. As there is a match between the ques-
tion head word film and the dictionary named
film, each noun phrase in the candidate sentence
is checked. However, no dictionary entities have
been found from the candidate sentence Best plays
Jar Jar Binks, a floppy-eared, two-legged creature
in ?Star Wars: Episode I ? The Phantom Men-
ace?, although there is movie entitled Star Wars
Episode I: The Phantom Menace in the dictionary.
Notice that Star Wars: Episode I ? The Phantom
Menace in the sentence and the dictionary entity
Star Wars Episode I: The Phantom Menace do not
have exactly identical spelling. The use of prox-
imity look up which allows edit distance being less
than 10% error can resolve this.
5.3 Numerical entities presence feature
There are so far no match for question types of
NUM (as shown in Table 1) including NUM:count
and NUM:date etc. These types of questions
seek the numerical answers such as the amount of
money and the duration of period. It is natural to
compile regular expression patterns to match such
entities. For example, for a NUM:money typed
question What is Rohm and Haas?s annual rev-
enue, we compile NUM:money regular expression
pattern which matches the strings of number fol-
lowed by a currency sign ($ and dollars etc). Such
pattern is able to identify 4 billion $ as a candidate
answer in the candidate sentence Rohm and Haas,
with 4 billion $ in annual sales... There are 13 pat-
terns compiled to cover all numerical types. We
create a binary feature NUM to indicate the pres-
ence of possible numerical answers in a sentence.
5.4 Specific features
Specific features are question dependent. For ex-
ample, for question When was James Dean born,
any candidate sentence matches the pattern James
Dean (number - number) is likely to answer such
question. We create a binary feature SPE to indi-
cate the presence of such match between a ques-
546
tion and a candidate sentence. We list all question
and sentence match patterns which are used in our
experiments as following:
when born feature 1 The question begins with when is/was
and follows by a person name and then follows by key
word born; The candidate sentence contains such per-
son name which follows by the pattern of (number -
number).
when born feature 2 The question begins with when is/was
and follows by a person name and then follows by key
word born; The candidate sentence contains such per-
son name, a NUM:date entity, and a key word born.
where born feature 1 The question begins with where
is/was and follows by a person name and then follows
by key word born; The candidate sentence contains
such person name, a NER LOC entity, and a key word
born.
when die feature 1 The question begins with when did and
follows by a person name and then follows by key word
die; The candidate sentence contains such person name
which follows by the pattern of (number - number).
when die feature 2 The question begins with when did and
follows by a person name and then follows by key
word die; The candidate sentence contains such person
name, a NUM:date entity, and a key word died.
how many feature The question begins with how many and
follows by a noun; The candidate sentence contains a
number and then follows by such noun.
cooccurrent Feature This feature takes two phrase argu-
ments, if the question contains the first phrase and the
candidate sentence contains the second, such feature
would be activated.
Note that the construction of specific features
require the access to aforementioned extracted
named entities. For example, the when born fea-
ture 2 pattern needs the information whether a
candidate sentence contains a NUM:date entity
and where born feature 1 pattern needs the in-
formation whether a candidate sentence contains
a NER LOC entity. Note also that the patterns of
when born feature and when die feature have
similar structure and thus can be simplified in im-
plementation. How many feature can be used
to identify the sentence Amtrak annually serves
about 21 million passengers for question How
many passengers does Amtrak serve annually. The
cooccurrent feature is the most general one. An
example of cooccurrent feature would take the
arguments of marry and husband, or marry and
wife. Such feature would be activated for ques-
tion Whom did Eileen Marie Collins marry and
candidate sentence ... were Collins? husband,
Pat Youngs, an airline pilot... It is worth noting
that the two arguments are not necessarily differ-
ent. For example, they could be both established,
which makes such feature activated for question
When was the IFC established and candidate sen-
tence IFC was established in 1956 as a member of
the World Bank Group. The reason why we use the
cooccurrence of the word established is due to its
main verb role, which may carry more information
than other words.
5.5 Dependency validity features
Like (Cui et al, 2004), we extract the dependency
path from the question word to the common word
(existing in both question and sentence), and the
path from candidate answer (such as CoNLL NE
and numerical entity) to the common word for
each pair of question and candidate sentence using
Stanford dependency parser (Klein and Manning,
2003; Marneffe et al, 2006). For example, for
question When did James Dean die and candidate
sentence In 1955, actor James Dean was killed in
a two-car collision near Cholame, Calif., we ex-
tract the pathes of When:advmod:nsubj:Dean and
1955:prep-in:nsubjpass:Dean for question and
sentence respectively, where advmod and nsubj
etc. are grammatical relations. We propose the
dependency validity feature (DEP) as following.
For all paired paths between a question and a can-
didate sentence, if at least one pair of path in which
all pairs of grammatical relations have been seen
in the training, then the DEP feature is set to be
true, false otherwise. That is, the true validity fea-
ture indicates that at least one pair of path between
the question and candidate sentence is possible to
be a true pair (ie, the candidate noun phrase in the
sentence path is the true answer).
6 Question Answer Experiments
Recall that most of the question answer features
depend on the question classifier. For instance,
the NE feature checks the presence or absence of
CoNLL style named entities subject to the clas-
sified question type. In this section, we evaluate
how the quality of question classifiers affects the
question answering performance.
6.1 Experiment setup
We use TREC99-03 factoid questions for training
and TREC04 factoid questions for testing. To fa-
cilitate the comparison to others work (Cui et al,
2004; Shen and Klakow, 2006), we first retrieve
all relevant documents which are compiled by Ken
Litkowski8 to create training and test datasets. We
8Available at http://trec.nist.gov/data/qa.html.
547
then apply key word search for each question and
retrieve the top 20 relevant sentences. We create
a feature represented data point using each pair of
question and candidate sentence and label it either
true or false depending on whether the sentence
can answer the given question or not. The labeling
is conducted by matching the gold factoid answer
pattern against the candidate sentence.
There are two extra steps performed for train-
ing set but not for test data. In order to construct
a high quality training set, we manually check the
correctness of the training data points and remove
the false positive ones which cannot support the
question although there is a match to gold answer.
In addition, in order to keep the training data well
balanced, we keep maximum four false data points
(question answer pair) for each question but no
limit over the true label data points. In doing so,
we use 1458 questions to compile 8712 training
data points and among them 1752 have true labels.
Similarly, we use 202 questions to compile 4008
test data points and among them 617 have true la-
bels.
We use the training data to train a maximum
entropy model and use such model to rank test
data set. Compared with a classification task (such
as the question classifier), the ranking process re-
quires one extra step: For data points which share
the same question, the probabilities of being pre-
dicted as true label are used to rank the data points.
In align with the previous work, performance is
evaluated using mean reciprocal rank (MRR), top
1 prediction accuracy (top1) and top 5 prediction
accuracy (top5). For the test data set, 157 among
the 202 questions have correct answers found in
retrieved sentences. This leads to the upper bound
of MRR score being 77.8%.
To evaluate how the quality of question clas-
sifiers affects the question answering, we have
created three question classifiers: QC1, QC2
and QC3. The features which are used to train
these question classifiers and their performance
are shown in Table 5. Note that QC3 is the best
question classifier we obtained in Section 4.
Table 5: Features used to train and the perfor-
mance of three question classifiers.
Name features 6 class 50 class
QC1 wh-word 46.0 46.8
QC2 wh-word+ head 92.2 82.0
QC3 All 93.6 89.0
6.2 Experiment results
The first experiment is to evaluate the individ-
ual contribution of various features derived using
three question classifiers. Table 6 shows the base-
line result and results using DIC, NE, NE-4, REG,
SPE, and DEP features. The baseline is the key
word search without the use of maximum entropy
model. As can be seen, the question classifiers
do not affect the DIC feature at all, as DIC fea-
ture does not depend on question classifiers. Bet-
ter question classifier boosts considerable gain for
NE, NE-4 and REG in their contribution to ques-
tion answering. For example, the best question
classifier QC3 outperforms the worst one (QC1)
by 1.5%, 2.0%, and 2.0% MRR scores for NE,
NE-4 and REG respectively. However, it is sur-
prising that the MRR and top5 contribution of NE
and NE-4 decreases if QC1 is replaced by QC2, al-
though the top1 score results in performance gain
slightly. This unexpected results can be partially
explained as follows. For some questions, even
QC2 produces correct predictions, the errors of
NE and NE-4 features may cause over-confident
scores for certain candidate sentences. As SPE and
DEP are not directly dependent on question clas-
sifier, their individual contribution only changes
slightly or remains the same for different ques-
tion classifiers. If the best question classifier is
used, the most important features are SPE and
REG, which can individually boost the MRR score
over 54%, while the others result in less significant
gains.
We now incrementally use various features and
the results are show in Table 6 as well. As can
be seen, the more features and the better question
classifier are used, the higher performance the ME
model has. The inclusion of REG and SPE results
in significant boost for the performance. For ex-
ample, if the best question classifier QC3 is used,
the REG results in 6.9% and 8% gain for MRR
and top1 scores respectively. This is due to a large
portion of NUM type questions in test dataset. The
SPE feature contributes significantly to the per-
formance due to its high precision in answering
birth/death time/location questions. NE and NE-4
result in reasonable gains while DEP feature con-
tributes little. However, this does not mean that
DEP is not important, as once the model reaches a
high MRR score, it becomes hard to improve.
Table 6 clearly shows that the question type
classifier plays an essential role in a high perfor-
548
Table 6: Performance of individual and incremental feature sets for three question classifiers.
Individual
Feature MRR Top1 Top5
QC1 QC2 QC3 QC1 QC2 QC3 QC1 QC2 QC3
Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4
DIC 49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4
NE 48.5 47.5 50.0 40.6 40.6 42.6 61.9 60.9 63.4
NE-4 49.5 48.5 51.5 41.6 42.1 44.6 62.4 61.9 64.4
REG 52.0 54.0 54.0 44.1 47.0 47.5 64.4 65.3 65.3
SPE 55.0 55.0 55.0 48.5 48.5 48.5 64.4 64.4 64.4
DEP 51.0 51.5 52.0 43.6 44.1 44.6 65.3 65.8 65.8
Incremental
Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4
+DIC 49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4
+NE 50.0 48.5 51.0 43.1 42.1 44.6 62.9 61.4 64.4
+NE-4 51.5 50.0 53.0 44.1 43.6 46.0 63.4 62.9 65.8
+REG 55.0 56.9 59.9 48.0 51.0 54.0 68.3 68.8 71.8
+SPE 60.4 62.4 65.3 55.4 58.4 61.4 70.8 70.8 73.8
+DEP 61.4 62.9 66.3 55.9 58.4 62.4 71.8 71.8 73.8
mance question answer system. Assume all the
features are used, the better question classifier sig-
nificantly boosts the overall performance. For ex-
ample, the best question classifier QC3 outper-
forms the worst QC1 by 4.9%, 6.5%, and 2.0%
for MRR, top1 and top5 scores respectively. Even
compared to a good question classifier QC2, the
gain of using QC3 is still 3.4%, 4.0% and 2.0%
for MRR, top1 and top5 scores respectively. One
can imagine that if a fine grained NER is available
(rather than the current four type coarse NER), the
potential gain is much significant.
The reason that the question classifier affects
the question answering performance is straightfor-
ward. As a upstream source, the incorrect classi-
fication of question type would confuse the down-
stream answer search process. For example, for
question What is Rohm and Haas?s annual rev-
enue, our best question classifier is able to clas-
sify it into the correct type of NUM:money and
thus would put $ 4 billion as a candidate answer.
However, the inferior question classifiers misclas-
sify it into HUM:ind type and thereby could not
return a correct answer. Figure 1 shows the indi-
vidual MRR scores for the 42 questions (among
the 202 test questions) which have different pre-
dicted question types using QC3 and QC2. For al-
most all test questions, the accurate question clas-
sifier QC3 achieves higher MRR scores compared
to QC2.
Table 7 shows performance of various question
answer systems including (Tanev et al, 2004; Wu
et al, 2005; Cui et al, 2004; Shen and Klakow,
0 5 10 15 20 25 30 35 40 45
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
question id
M
R
R
 
 
QC3
QC2
Figure 1: Individual MRR scores for questions
which have different predicted question types us-
ing QC3 and QC2.
2006) and this work which were applied to the
same training and test datasets. Among all the sys-
tems, our model can achieve the best MRR score
of 66.3%, which is close to the state of the art of
67.0%. Considering the question answer features
used in this paper are quite standard, the boost is
mainly due to our accurate question classifier.
Table 7: Various system performance comparison.
System MRR Top1 Top5
Tanev et al 2004 57.0 49.0 67.0
Cui et al 2004 60.0 53.0 70.0
Shen and Klakow, 2006 67.0 62.0 74.0
This work 66.3 62.4 73.8
549
7 Conclusion
In this paper, we have presented a question clas-
sifier which makes use of a compact yet effi-
cient feature set. The question classifier outper-
forms previous question classifiers over the stan-
dard UIUC question dataset. We further investi-
gated quantitatively how the quality of question
classifier impacts the performance of question an-
swer system. The experiments showed that an ac-
curate question classifier plays an essential role
in question answering system. With our accurate
question classifier and some standard question an-
swer features, our question answering system per-
forms close to the state of the art.
Acknowledgments
We wish to thank the three anonymous review-
ers for their invaluable comments. This re-
search was supported by British Telecom grant
CT1080028046 and BISC Program of UC Berke-
ley.
References
A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra. 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. PhD thesis, University of
Pennsylvania.
H. Cui, K Li, R. Sun, T. Chua, and M. Kan. 2004. Na-
tional university of singapore at the trec-13 question
answering. In Proc. of TREC 2004, NIST.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of
ACL, pages 363-370.
D. Hacioglu and W. Ward. 2003. Question classifica-
tion with support vector machines and error correct-
ing codes. In Proc. of the ACL/HLT, vol. 2, pages
28?30.
Z. Huang, M. Thint, and Z. Qin. 2008. Question clas-
sification using head words and their hypernyms. In
Proc. of the EMNLP.
D. Klein and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In Proc. of ACL 2003, vol. 1, pages
423?430.
V. Krishnan, S. Das, and S. Chakrabarti. 2005. En-
hanced answer type inference from questions using
sequential models. In Proc. of the HLT/EMNLP.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In ACM Special Inter-
est Group for Design of Communication Proceed-
ings of the 5th annual international conference on
Systems documentation, pages 24?26.
X. Li and D. Roth. 2002. Learning question classi-
fiers. In the 19th international conference on Com-
putational linguistics, vol. 1, pages 1-7.
X. Li and D. Roth. 2006. Learning question classifiers:
the role of semantic information. Natural Language
Engineering, 12(3):229?249.
C. D. Manning and D. Klein. 2003. Optimization,
maxent models, and conditional estimation with-
out magic. Tutorial at HLT-NAACL 2003 and ACL
2003.
M. D. Marneffe, B. MacCartney and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC 2006.
A. Moschitti, S. Quarteroni, R. Basili and S. Manand-
har 2007. Exploiting syntactic and shallow seman-
tic kernels for question answer classification. In
Proc. of ACL 2007, pages 776-783.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of the HLT-NAACL.
J. Prager. 2006. Open-domain question-answering.
In Foundations and Trends in Information Retrieval,
vol. 1, pages 91-231, 2006.
N. Schlaefer, J. Ko, J. Betteridge, G. Sautter, M. Pathak
and E. Nyberg. 2007. Semantic extensions of the
Ephyra QA system for TREC 2007. In Proc. of the
TREC 2007.
D. Shen and D. Klakow. 2006. Exploring correlation
of dependency relation paths for answer extraction.
In Proc. of the ACL 2006.
H. Tanev, M. Kouylekov, and B. Magnini. 2004.
Combining linguistic processing and web mining for
question answering: Itc-irst at TREC-2004. In Proc.
of the TREC 2004, NIST.
E. M. Voorhees and H. T. Dang. 2005. Overview of
the TREC 2005 question answering track. In Proc.
of the TREC 2005, NIST.
M. Wu, M. Duan, S. Shaikh, S. Small, and T. Strza-
lkowski. 2005. University at Albanys ILQUA in
TREC 2005. In Proc. of the TREC 2005, NIST.
D. Zhang and W. S. Lee. 2003. Question classification
using support vector machines. In The ACM SIGIR
conference in information retrieval, pages 26?32.
550
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1232?1240,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Accurate Semantic Class Classifier for Coreference Resolution
Zhiheng Huang1, Guangping Zeng1,2, Weiqun Xu3, and Asli Celikyilmaz1
1EECS Department, University of California at Berkeley,
CA 94720, USA
{zhiheng,gpzeng,asli}@eecs.berkeley.edu
2Computer Science Department, School of Information Engineering,
University of Science and Technology Beijing, China
3ThinkIT, Institute of Acoustics, Chinese Academy of Sciences,
Beijing, 100190, China
xuweiqun@hccl.ioa.ac.cn
Abstract
There have been considerable attempts
to incorporate semantic knowledge into
coreference resolution systems: different
knowledge sources such as WordNet and
Wikipedia have been used to boost the per-
formance. In this paper, we propose new
ways to extract WordNet feature. This
feature, along with other features such as
named entity feature, can be used to build
an accurate semantic class (SC) classifier.
In addition, we analyze the SC classifica-
tion errors and propose to use relaxed SC
agreement features. The proposed accu-
rate SC classifier and the relaxation of SC
agreement features on ACE2 coreference
evaluation can boost our baseline system
by 10.4% and 9.7% using MUC score and
anaphor accuracy respectively.
1 Introduction
Coreference resolution is used to determine which
noun phrases (including pronouns, proper names,
and common nouns) refer to the same entities in
documents. Much work on coreference resolution
is based on (Soon et al, 2001), which built a de-
cision tree classifier to label pairs of mentions as
coreferent or not. Recent work aims to improve
the performance from two aspects: new models
and new features. The former cast the pair wise
mention classifications into various forms such as
the best path in a Bell tree (Luo et al, 2004), the
best graph cut (Nicolae and Nicolae, 2006), in-
teger linear programming (Denis and Baldridge,
2007) and graph partition based conditional model
(McCallum and Wellner, 2004). The latter de-
velop and investigate new linguistic features for
the problem. For instance, WordNet (Poesio et al,
2004), Wikipedia (Ponzetto and Strube, 2006), se-
mantic neighbor words (Ng, 2007a), and pattern
based features (Yang and Su, 2007) have been ex-
tensively studied.
Deeper linguistic knowledge is required to en-
able the coreference resolution to reach a higher
level of performance (Kehler et al, 2004). An im-
portant type of semantic knowledge that has been
employed in coreference resolution system is the
semantic class (SC) of an NP, which can be used
to filter out the coreference between semantically
incompatible NPs. However, the difficulty is to
accurately compute the semantic class features. In
this paper, we show that the WordNet may not be
efficiently employed in the traditional way such
as (Soon et al, 2001; Ng, 2007a; Ponzetto and
Strube, 2006) to compute the semantic class fea-
tures. We introduce new ways to use the WordNet
and the experiments show its effectiveness in de-
termining the semantic classes for noun phrases.
In addition, we analyze the classification errors of
the SC classifier and propose to use relaxed SC
agreement features. With these proposed features
and other standard syntactic features (which are
commonly employed in existing coreference sys-
tems), our coreference resolution system can ob-
tain an increase of 10.4% for MUC score and 9.7%
for anaphor accuracy from the baseline in ACE2
evaluation.
2 Related Work
WordNet (Fellbaum, 1998) as an important knowl-
edge source has been widely employed in previ-
ous coreference resolution work. For example,
Harabagiu et al (2001) have used WordNet rela-
tions such as synonym and is-a to mine the pat-
terns of WordNet paths for pairs of antecedents
and anaphors. Due to the nature of the rule based
coreference system (in contrast to machine learn-
ing based), the weights of relations may not be
accurately estimated. Vieira and Poesio (2000)
and Markert and Nissim (2005) have used Word-
Net synonym and hyponym etc. to determine if
an anaphor semantically relates to one previous
NP. Ponzetto and Strube (2006) have used Word-
Net semantic similarity and relatedness scores be-
tween antecedents and candidate anaphors. Their
1232
work is different to this work in the following: 1)
Their work involves various relations such as hy-
ponyms and meronyms while ours only makes use
of hypernyms; and 2) Their work focuses on in-
vestigating if two NPs have particular WordNet re-
lations or not, while ours focuses on using Word-
Net hypernyms for their SC classification and then
testing their SC compatibility. In doing so, we can
directly model the accuracy of semantic class clas-
sification and test its impact on coreference reso-
lution.
While the SC of a proper name is computed
fairly accurately using a named entity (NE) recog-
nizer, many coreference resolution systems sim-
ply assign to a common noun the first (i.e., most
frequent) WordNet synset as its SC (Soon et al,
2001; Markert and Nissim, 2005). This heuris-
tics, apparently, did not lead to good performance.
The best reported ACE2 coreference resolution
system (Ng, 2007a; Ng, 2007b) has proposed an
accurate SC classifier which used heterogeneous
semantic knowledge sources. WordNet is just
one of the several knowledge sources which have
been utilized. However, the WordNet based fea-
tures is not informative compared to other features
such as the semantic neighbor feature. Similarly,
Ponzetto and Strube (2006) have discovered that
the WordNet feature is no more informative than
the community-generated Wikipedia feature. In
this paper, we focus on the investigation of vari-
ous usages of WordNet for the SC classification
task. The work which is directly comparable to
ours would be (Ng, 2007a; Ng, 2007b).
Other similar work includes the mention detec-
tion (MD) task (Florian et al, 2006) and joint
probabilistic model of coreference (Daume? III and
Marcu, 2005). The MD task identifies the bound-
ary of a mention, its mention type (e.g., pronoun,
name), and its semantic type (e.g., person, orga-
nization). Unlike them, we do not perform the
boundary detection, as we make use of the noun
phrases directly from the noun phrase chunker and
NE recognizer. The joint probabilistic model mod-
els the MD and coreference simultaneously, while
our work focuses on them separately.
3 Semantic Class Classification
In this section, we describe how we compile the
training corpus and extract features using Word-
Net. We report our results on the ACE coreference
corpus due to that it has been commonly used and
it was annotated SCs of six types.1 As in (Ng,
1Person, organization, gpe, location and facility are ex-
plicitly annotated. The rest noun phrases are other type.
2007a), we first train a classifier to predict the SC
of an NP. This SC information is used later in the
coreference resolution stage. For example, the au-
dience is classified as SC of person, and it thus
should not be coreferent with the security industry,
which is usually classified as organization. This
task is by no means trivial. First, while the classi-
fication of Tom Hanks being SC of person can be
accurately achieved by an NE recognizer, the as-
sociation of audience and person requires seman-
tic language source such as WordNet. Second, the
same noun phrase can be annotated with different
SCs under different context. For example, the au-
thorities is usually annotated as person, but it is
sometimes as organization. Even worse, the same
noun phrases are sometimes annotated with one of
the five explicitly annotated classes while some-
times are not annotated at all (thus falling into the
other SC). For example, people is annotated as
person SC explicitly 20 times and is not annotated
at all 21 times in the ACE2 testset. This inconsis-
tent annotation adversely affects the performance
of an SC classifier. And this in turn would cause
errors during coreference stage. In section 4.3, we
show how to relax the strict SC agreement feature
to address this.
3.1 Training instance creation
We use ACE Phase 2 Coreference corpus to train
the SC classifier. Each noun phrase which is iden-
tified by the noun phrase chunker or NE recognizer
is used to create a training instance. Each instance
is represented by a set of lexical, syntactic and se-
mantic features, as described below. If the NP un-
der consideration is annotated as one of the five
ACE SCs in the corpus, then the classification of
the associated training instance is the ACE SC of
the NP. Otherwise, the instance is labeled as other.
ACE 2 corpus has a training set and a test set
which comprise of 422 and 97 texts respectively.
We divide the training set into a new training and a
development set: the former consists of 90% ran-
domly generated and stratified original training in-
stances and the latter consists of the rest 10% in-
stances. The test set remains the same as in ACE2
corpus. The size of each dataset and its SC dis-
tributions are shown in Table 1. Note that the
training and development datasets have exactly the
same distributions of SCs due to the stratification
procedure. That is, each class has the same pro-
portion in training and development datasets. We
tune the feature parameters against development
set and report performance on both development
set and test set.
1233
Table 1: Distributions of SCs in ACE2 corpus.
Size PER ORG GPE FAC LOC OTH
Train 55629 20.29 7.30 8.42 0.61 0.55 62.80
Dev 6181 20.29 7.30 8.42 0.61 0.55 62.80
Test 15360 20.48 7.57 6.90 0.85 0.41 63.79
3.2 Lexical features
Each instance is represented as a bag of features
and is fed into a classifier in training stage. We
present four binary lexical feature sets as follows.
Word unigrams and bigrams: An N-gram is
a sub-sequence of N words from a given noun
phrase. Unigram forms the bag of words feature,
and bigram forms the pairs of words feature, and
so forth. We have considered word unigram and
bigram features in our experiments.
First and last words: This feature extracts the
first and last words of an NP. For example, the first
word the and the last word store are extracted from
the NP the main store. This feature does not only
coarsely models the influence of the first word, for
example, a or the, but also models the head word,
since the head word usually is the last word in the
NP.
Head word: We use Collins style rules
(Collins, 1999) to extract the head words for given
NPs. These features should be most informative
if the training corpus is large enough.2 For exam-
ple, the head word company of the NP the com-
pany immediately determines its SC being organi-
zation. However, due to the sparseness of training
data, its potential importance is adversely affected.
3.3 Semantic features
NE feature is extracted from Stanford named en-
tity recognizer (NER) (Finkel et al, 2005). Three
types of named entities: person, location and or-
ganization can be recognized for a given NP. This
feature is primarily useful for SC classification of
proper nouns.
WordNet is a large English lexicon in which se-
mantically related words are connected via cogni-
tive synonyms (synsets). The WordNet is a use-
ful tool for word semantics analysis and has been
widely used in natural language processing appli-
cations. In WordNet, synsets are organized into hi-
erarchies with hypernym/hyponym relationships:
Y is a hypernym of X if every X is a (kind of) Y
(X is called a hyponym of Y in this case).
The WordNet is employed in (Ng, 2007a) as
following to create the WN CLASS feature. For
each keyword w as shown in the right column of
2It, however, is mostly useful for nominal noun phrase and
not for the pronoun and proper noun phrases.
Table 2, if the head noun of a given NP is a hy-
ponym of w in WordNet,3 then the word w be-
comes a feature for such NP. It is explained that
these keywords are correlated with the ACE SCs
and they are obtained via experimentation with
WordNet and the ACE SCs of the NPs in the ACE
training data. However, it is likely that these hand-
crafted keywords have poor coverage for general
cases. As a result, it may not make full use of
WordNet semantic knowledge. This will be shown
in our individual feature contribution experiment
in Section 3.5.
Table 2: List of keywords used in WordNet seman-
tic feature in (Ng, 2007a).
ACE SC Keywords
PER person
ORG social group
FAC establishment, construction, building,
facility, workplace
GPE country, province, government, town, city,
administration, society, island, community
LOC dry land, region, landmass, body of water
geographical area, geological formation
There are other ways of using WordNet for se-
mantic feature extraction. For example, Ponzetto
and Strube (2006) have employed WordNet sim-
ilarity measure for coreference resolution. The
difference is that they created the feature di-
rectly at the coreference resolution stage, ie, us-
ing the WordNet similarity between the antecedent
and anaphor to determine if they are coreferent,
while we focus on using this feature to classify
an NP into a particular SC. For comparison, we
implemented a WordNet similarity based feature
(WN SIM) as follows: for a given NP head word
and a key word as listed in Table 2, the WordNet
similarity package (Seco et al, 2004) models the
length of path traveling from the head word to the
key word over the WordNet network. It then com-
putes the semantic similarity based on the path.
For example, the similarity between company and
social group is 0.77, while the similarity between
company and person is 0.59. The key word which
receives the highest similarity to the head word is
marked as a feature.
The WN CLASS feature may suffer from the
coverage problem and the WN SIM feature is
heavily dependent on the definition of similarity
metric which may turn out to be inappropriate for
coreference resolution task. To make better use of
WordNet knowledge, we attempt to directly intro-
duce hypernyms for the NP head words (we denote
3Only the first synset of the NP is used.
1234
it as WN HYP feature). The most similar work
to ours is (Daume? III and Marcu, 2005), in which
two most common synsets from WordNet for all
words in an NP and their hypernyms are extracted
as features. We avoid augmenting the hypernyms
for non-head words in the NP to prevent introduc-
ing noisy information, which may potentially cor-
rupt the hypernym feature space.
Considering a WordNet hypernym structure as
shown in Fig. 1 for the word company, its first
synset (an institution created to conduct business)
has a unique id of 08058098 and can also be rep-
resented by a set of description words (company
in this case). Its third synset (the state of being
with someone) has an id of 13929588 and descrip-
tion words of company, companionship, fellow-
ship, society. Each synset can be extended by its
hypernym synsets. For example, the direct hyper-
nym of the first synset is the synset of 08053576
which can be described as institution, establish-
ment. The augmentation of hypernyms for NP
head words can introduce useful information, but
can also bring noise if the head word or the synset
of head word are not correctly identified. For an
optimal use of WordNet hypernyms, four ques-
tions shall be addressed: 1) how many depths are
required to tradeoff the generality (thus more in-
formative) and the specificity (thus less noisy)? 2)
which synset of the given word is needed to be
augmented? 3) which representation (synset id or
synset word) is better? and 4) is it helpful to en-
code the hypernym depth into the hypernym fea-
ture?4 These four questions provide the guideline
to search the optimal use of WordNet. We will de-
sign experiments in Section 3.5 to determine the
optimal configuration of WN HYP feature.
state
(08008335)
(13931145)
(13928668)
(08053576)
(07950920)
company
depth 2
depth 3
depth 4
depth 1
social group
institution,establishment
organization,organisation
(00024720)
friendship,friendlyrelationship
fellowship,society
(13929588)(08058098)
company, companionship,
company
relationship
Figure 1: WordNet hypernym hierarchy for the
word company.
4For example, we encode the synset 08053576 as
08053576-1, with the last digit 1 indicating the depth of hy-
pernym with regard to the entry word company.
3.4 Learning algorithm
Maximum entropy (ME) models (Berger et al,
1996; Manning and Klein, 2003), also known as
log-linear and exponential learning models, has
been adopted in the SC classification task. Max-
imum entropy models can integrate features from
many heterogeneous information sources for clas-
sification. Each feature corresponds to a constraint
on the model. Given a training set of (C,D),
where C is a set of class labels and D is a set
of feature represented data points, the maximum
entropy model attempts to maximize the log like-
lihood
logP (C|D,?) =
?
(c,d)?(C,D)
log
exp
?
i
?
i
f
i
(c, d)
?
c
?
exp
?
j
?
j
f
i
(c, d)
(1)
where f
i
(c, d) are feature indicator functions and
?
i
are the parameters to be estimated. We use ME
models for both SC classification and mention pair
classification.
3.5 SC classification evaluation
We design three experiments to test the accuracy
of our classifiers. The first experiment evalu-
ates the individual contribution of different fea-
ture sets to SC classification accuracy. In par-
ticular, a ME model is trained on the 55,629
training instances using the following feature sets
separately: 1) unigram, 2) bigram, 3) first-last
word, 4) head word (HW), 5) named entities
(NE), 6) HW+WN CLASS, 7) HW+WN SIM,
and 8) variants of HW+WN HYP. Note that
HW+WN CLASS is the semantic feature used in
(Ng, 2007a), HW+WN SIM is the semantic fea-
ture using WordNet similarity measure (Seco et
al., 2004), and variants of HW+WN HYP are the
work proposed in this paper. We combine head
word and the semantic features due to the fact that
WordNet features are dependent on head words
and they could be treated as units. In the second
experiment, features are fed into the ME model
incrementally until all features have been used.5
Finally, we perform the feature ablation experi-
ments. That is, we remove one feature at a time
from the entire feature set and test the accuracy
loss. The SC classification performance is mea-
sured by accuracy, i.e., the proportion of the cor-
rectly classified instances among all test instances.
Individual feature contribution Table 3 shows
the SC classification accuracy of all NPs (all)
and non-pronoun NPs (non-PN) on the develop-
ment and test datasets using individual feature
5The optimal of HW+WN HYP configuration is used.
1235
sets. Among all the lexical features, unigram fea-
Table 3: SC classification accuracy of ME using
individual feature sets for development and test
ACE2 datasets.
Feature type dev test
all non-PN all non-PN
unigram 81.3 81.6 72.4 71.9
bigram 32.5 36.4 26.3 28.4
first-last word 80.1 80.2 71.6 71.0
HW 78.2 78.0 68.3 67.1
NE 74.0 82.8 73.1 81.9
HW+WN CLASS 79.5 79.4 70.3 69.5
HW+WN SIM 81.2 81.4 73.8 73.6
HW+WN HYP (1) 82.6 83.1 74.8 74.7
HW+WN HYP (3) 82.8 83.4 75.2 75.2
HW+WN HYP (6) 83.1 83.7 75.6 75.7
HW+WN HYP (9) 83.0 83.6 75.7 75.7
HW+WN HYP (?) 83.1 83.7 75.8 75.9
HW+WN HYP (6) 82.8 83.3 75.6 75.7
word form
HW+WN HYP (6) 82.9 83.5 75.4 75.4
depth encoded
HW+WN HYP (6) 83.0 83.6 76.4 76.6
first synset
ture performs the best (81.3%) for all NPs over the
development dataset. The bigram feature performs
poorly due to the sparsity problem: NPs usually
consist of one to three words. The first-last word
feature effectively models the prefix words (such
as a and the) and the head words and thus obtains a
reasonably high accuracy of 80.1%. As mentioned
before, the head word feature may suffer from the
sparsity and it results in the accuracy of 78.2%.
We also list the accuracies for non-pronoun NP
SC classification, which are slightly different com-
pared to all NP SC classification except for bi-
gram, in which the accuracy has increased 3.9%.
Although Stanford NER performs well on
named entity recognition task, it results in ac-
curacy of 74.0% for all NP SC classification,
due to its inability to deal with pronouns such
as he and common nouns such as the govern-
ment. The removal of pronouns significantly
boosts its accuracy to 82.8%. The introduc-
tion of semantic feature HW+WN CLASS boosts
the performance to 79.5% compared to the head
word alone of 78.2%. This conforms to (Ng,
2007a) that only small gain can be achieved us-
ing WN CLASS feature. The HW+WN SIM
feature outperforms HW+WN CLASS and the
accuracy reaches 81.2%. For the variants of
HW+WN HYP, we first search the optimal depth.
This is performed by using all synsets for NP head
word, encoding the feature using synset id (rather
than synset word), and no hypernym depth is en-
coded in the features. We try various depths of
1, 3, 6, 9 and ?, with ? signifies that no depth
constraint is imposed. The optimal depth of 6 is
obtained with the accuracy of 83.1% over the de-
velopment dataset. We then fix the depth of 6 to try
using synset word as features, using synset id with
depth encoded as features, and using first synset
only. The results show that the optimum is to en-
code the features using hypernym synset id with-
out hypernym depth information and all synsets
are considered for hypernym extraction. This is
slightly different from the previous finding (Soon
et al, 2001; Lin, 1998b) that a coreference res-
olution system employing only the first WordNet
synset performs slightly better than that employ-
ing more than one synset.6 The best result reaches
the accuracy of 83.1%. Although the best seman-
tic feature only outperforms the best lexical fea-
ture by 1.8% on the development dataset, its gain
in the test dataset is more significant (3.2%, from
72.4% to 75.6%).
Incremental feature contribution Once we
use the training and development datasets to find
the optimal configuration of HW+WN HYP se-
mantic feature, we use all lexical features and the
optimal HW+WN HYP feature incrementally to
train an ME model over the combination of train-
ing and development datasets. Table 4 shows
the SC classification accuracy of all NPs (all)
and non-pronoun NPs (non-PN) on the train-
ing+development (we refer it as training hereafter)
and test datasets.
Table 4: SC classification accuracy of ME using
incremental feature sets for training and test ACE2
datasets.
Feature type train test
all non-PN all non-PN
HW 87.8 89.0 68.6 67.6
+WN HYP 87.8 89.0 75.7 75.8
+unigram 91.5 93.3 77.7 78.1
+bigram 93.1 95.2 78.7 79.2
+first-last word 93.2 95.3 78.8 79.3
+NE 93.4 95.6 83.1 84.4
Ng 2007a - 85.0 - 83.3
Note that the significant higher accuracies in
training compared to test are due to the overfit-
ting problem. The interesting evaluation thus re-
mains on the test data. As can be seen, the in-
clusion of more features results in higher perfor-
mance. This is more obvious in the test dataset
than in the training dataset. The inclusion of the
6In fact, the accuracy of the test data supports their claims.
The accuracy using the first synset compared to using all
synsets results in the accuracy increase from 75.6% to 76.4%
for all NPs over the test dataset.
1236
optimized WN HYP feature (ie, using all synsets?
hypernyms up to 6 depth and with synset id encod-
ing) results in 7.1% increase for all NP SC classifi-
cation over test data. This shows the effectiveness
of the WN HYP features to overcome the sparsity
of head word feature. The unigram, bigram and
first-last word features offer reasonable accuracy
gain, and the final inclusion of NE boosts the over-
all performance to 83.1% for all NP and 84.4% for
non pronoun NPs over test data. This result can
be directly compared to the SC classification ac-
curacy as reported in (Ng, 2007a), in which the
highest accuracy is 83.3% for non pronoun NPs.7
The large difference between the highest training
accuracies is due to that our classifier is trained di-
rectly on the ACE2 training dataset, while their SC
classifier was trained on BBN Entity Type Corpus
(Weischedel and Brunstein, 2005), which is five
times larger than the ACE2 corpus used by us. In
addition to WordNet, they have adopted multiple
knowledge sources which include BBN?s Identi-
Finder (this is equivalent to the Stanford NER in
our work), BLLIP corpus and Reuters Corpus,8
and dependency based thesaurus (Lin, 1998a). It
is remarkable that our SC classifier can achieve
even higher accuracy only using WordNet hyper-
nym and NE features. It is worth noting that the
small accuracy gain is indeed hard to achieve con-
sidering that the test data size is large (15360).
Feature ablation experiment We now perform
the feature ablation experiments to further deter-
mine the importance of individual features. We re-
move one feature at a time from the entire feature
set. Table 5 shows the SC classification accuracy
of all NPs (all) and non-pronoun NPs (non-PN) on
the training and test datasets respectively.
Table 5: SC classification accuracy of ME by re-
moving one feature at a time for training and test
ACE2 datasets.
Feature type train test
all non-PN all non-PN
overall 93.4 95.6 83.1 84.4
-HW 93.4 95.5 82.9 84.2
-WN HYP 93.4 95.5 82.6 83.8
-HW+WN HYP 93.4 95.5 82.3 83.5
-unigram 93.4 95.5 82.9 84.2
-bigram 92.5 94.5 82.7 84.0
-first-last word 93.4 95.5 82.9 84.1
-NE 93.2 95.3 78.8 79.3
Again, the significant higher accuracies in train-
ing compared to test are due to overfitting. The re-
7All NP accuracy was not reported as they excluded the
pronouns in creating their training and test data.
8They use these corpus to extract patterns to induce SC of
common nouns.
moval of NE feature results in the largest accuracy
loss of 4.3% (from 83.1% to 78.8%) for all nouns
on test data. It follows WN HYP (0.5% loss) and
the bigram (0.4%). If we treat HW+WN HYP as
one feature, the removal of it results in accuracy
loss of 0.8% for all nouns on test data. The un-
igram, first-last word and head word each results
in the loss of 0.2%. The reason that the removal
of NE results in a much significant loss is due to
the fact that the NE feature is quite different from
other features. Its strength is to distinguish SCs for
proper names, while other features are more sim-
ilar (their targets are common nouns). The pro-
posed use of HW+WN HYP can bring 0.8% gain
on top of other features, higher than other informa-
tive lexical features including unigram and first-
last word.
3.6 Error analysis
A closer look at the errors produced by our SC
classifier reveals that the second probable label is
very likely to be the actual labels if the first proba-
ble one is wrong. In fact, if we allow the classifier
to predict two most probable labels and the clas-
sification is judged to be true if the actual label is
one of the two predictions, then the classification
accuracy increases from 83.1% to 96.4%. This
is because that the same noun phrases are some-
times annotated with one of the five explicitly an-
notated classes while sometimes are not annotated
at all (thus falling into the other SC). Again for
the example of people. It is annotated as person
SC 20 times and is not annotated at all 21 times.
Given the same feature set for this instance, the
best the classifier can do is to classify it to other
semantic class. To address this annotation incon-
sistency issue, we relax the SC agreement feature
from the strict match in designing coreference res-
olution features. For example, if the first probable
SC of an NP matches the second probable SC of
another NP, we still give some partial match credit.
4 Application to Coreference Resolution
We can now incorporate the NP SC classifier into
our ME based coreference resolution system. This
section examines how our WordNet hypernym fea-
tures help improve the coreference resolution per-
formance.
4.1 Experimental setup
We use the ACE-2 (version 1.0) coreference cor-
pus. Each raw text in this corpus was prepro-
cessed automatically by a pipeline of NLP com-
ponents, including sentence boundary detection,
1237
POS-tagging and text chunking. The statistics of
corpus and mention extraction are shown in Table
6, where g-mention is the automatically extracted
mentions which contain the annotated (gold) men-
tions. The recalls of gold mentions are 95.88%
and 95.93% for training and test data respectively.
Table 6: Statistics for corpus and extracted men-
tions.
text# mention# g-mention# gold# recall(%)
train 422 61810 22990 23977 95.88
test 97 15360 5561 5797 95.93
Our coreference system uses Maximum En-
tropy model to determine whether two NPs are
coreferent. As in (Soon et al, 2001; Ponzetto and
Strube, 2006), we generate training instances as
follows: a positive instance is created for each
anaphoric NP, NP
j
, and its closest antecedent,
NP
i
; and a negative instance is created for NP
j
paired with each of the intervening NPs, NP
i+1
,
NP
i+2
, ..., NP
j?1
. Each instance is represented
by syntactic or semantic features described as fol-
lows. All training data are used to train a maxi-
mum entropy model. In the test stage,we select the
closest preceding NP that is classified as corefer-
ent with NP
j
as the antecedent of NP
j
. If no such
NP exists, no antecedent is selected for NP
j
.
Unlike other natural language processing tasks
such as information extraction which have de facto
evaluation metrics, it is an open question which
evaluation is the most suitable one. The evalu-
ation becomes more complicated when automat-
ically extracted mentions (in contrast to the gold
mentions) are used. To facilitate the comparison
with previous work, we report performance us-
ing two different scoring metrics: the commonly-
used MUC scorer (Vilain et al, 1995) and the ac-
curacy of the anaphoric references (Ponzetto and
Strube, 2006). An anaphoric reference is correctly
resolved if it and its closest antecedent are in the
same coreference chain in the resulting partition.
4.2 Baseline features
We briefly review the baseline features used in
this paper as follows. More detailed information
and implementations can be found at (Soon et al,
2001; Versley et al, 2008). For example, the
ALIAS feature takes values of true or false. The
value of true means that the antecedent and the
anaphor refer to the same entity (date, person, or-
ganization or location). The ALIAS feature de-
tection works differently depending on the named
entity type. For date, the day, month, and year
values are extracted and compared. For person,
the last words of the noun phrases are compared.
For organization names, the alias detection checks
for acronym match such as IBM and International
Business Machines Corp.
Lexical features STRING MATCH: true if
NP
i
and NP
j
have the same spelling after remov-
ing article and demonstrative pronouns, false oth-
erwise. ALIAS: true if NP
j
is the alias of NP
i
.
Grammatical features I PRONOUN: true if
NP
i
is a pronoun; J PRONOUN: true if NP
j
is pronoun; J REFL PRONOUN: true if NP
j
is
reflexive pronoun; J PERS PRONOUN: true if
NP
j
is personal pronoun; J POSS PRONOUN:
true if NP
j
is possessive pronoun; J PN: true
if NP
j
is proper noun; J DEF: true if NP
j
starts with the; J DEM: true if NP
j
starts with
this, that, these or those; J DEM NOMINAL:
true if NP
j
is a demonstrative nominal noun;
J DEM PRONOUN: true if NP
j
is a demonstra-
tive pronoun; PROPER NAME: true if both NP
i
and NP
j
are proper names; NUMBER: true if
NP
i
and NP
j
agree in number; GENDER: true
if NP
i
and NP
j
agree in gender; APPOSITIVE:
true if NP
i
and NP
j
are appositions.
Distance feature DISTANCE: how many sen-
tences NP
i
and NP
j
are apart.
Semantic feature SEMCLASS: This feature is
implemented from (Soon et al, 2001). Its possible
values are true, false, or unknown. First the fol-
lowing semantic classes are defined: female, male,
person, organization, location, date, time, money,
percent, and object. Each of these defined seman-
tic classes is then mapped to a WordNet synset.
Then the semantic class determination module de-
termines the semantic class for every NP as the
first synset of the head noun of the NP. If such
synset is a hyponym of defined semantic class,
then such semantic class is assigned to the NP.
Otherwise, unknown class is assigned. Finally, the
agreement of semantic classes of NP
i
and NP
j
is
unknown if either assigned class is unknown; true
if their assigned class are the same, false other-
wise. Notice that the WordNet use in (Ng, 2007a)
and this feature apply in the same principle except
that 1) the former is used in SC classification while
the latter is used directly for coreference resolu-
tion, and 2) they have different semantic class cat-
egories.
4.3 Proposed WordNet agreement features
For each instance which consists of NP
i
and NP
j
,
we apply our SC classifier to label them, say l
i
and
l
j
respectively. We then use these two induced la-
1238
bels to propose the SC agreement feature for NP
i
and NP
j
. In particular, SC STRICT is true if l
i
and l
j
are the same and they are not of other type,
false otherwise; SC COARSE is true if both l
i
and
l
j
are not of other type; In addition, we propose
two other SC agreement features to cope with the
SC classification errors. SC RELAX1 is true if the
first probable of NP
i
, l
i1
, is not other type and is
the same as the second probable of N
j
, l
j2
, or vice
visa. SC RELAX2 is true if the second probable
of NP
i
, l
i2
, is not other type and is the same as the
second probable label of NP
j
, l
j2
. The purpose in
using SC RELAX1 and SC RELAX2 features is
to relax the strict SC agreement feature in the hope
that partial SC match is useful for coreference res-
olution.
4.4 Coreference results
Table 7 shows the MUC score for ACE2 corpus
and its three partitions: bnews, npaper, and nwire
using baseline and the proposed semantic features.
It also shows the accuracy of resolving anaphors
for all nouns in ACE2 corpus. SC STRICT is
the configuration that uses the baseline features
with the SEMCLASS (Soon et al, 2001) replaced
by SC STRICT, and SC COARSE, SC RELAX1,
and SC RELAX2 are incrementally included into
the SC STRICT feature set.
As can be seen, the SC STRICT significantly
boosts the performance: it improves the MUC
F score and anaphor accuracy of baseline from
57.7% to 65.7% and 37.7% to 46.3% respectively.
It is remarkable that the new use of WordNet can
obtain such significant gain in both MUC score
and anaphor accuracy. The large improvement
of the precision from 58.1% to 73.3% for all
NPs shows that the SC STRICT feature can ef-
fectively filter out the semantic incompatible pairs
of antecedents and anaphors. In accordance with
our hypothesis, the relaxation of strict SC agree-
ment by including SC COARSE, SC RELAX1
and SC RELAX2 help improve the performance
further, which is reflected by both MUC score and
anaphor accuracy. For example, compared to the
baseline, the use of all proposed four SC agree-
ment features results in the maximal accuracy gain
of 9.7% (from 37.7% to 47.4%) and the use of
SC STRICT, SC COARSE, and SC RELAX1 re-
sults in the maximal MUC score gain of 10.4%
(from 57.7% to 68.1%).
Our best MUC score is 68.1% which outper-
forms the MUC score of 64.6% as reported in
(Ng, 2007a) by 3.5%, while our best accuracy
of anaphor is 47.4%, which is 4.1% less than
the accuracy of 51.5% in (Ng, 2007a). Note
that, unlike (Ng, 2007a) which performed exten-
sive experiments using different machine learn-
ing algorithms, alternative use of features (either
constraint or normal features), and heterogeneous
knowledge sources, this paper simply uses one
learning classifier (ME model) and only employs
WordNet and Stanford NER semantic sources.
The different MUC and accuracy scores reflect
the non-trivial cases of evaluating coreference sys-
tems. While we leave out the discussion of which
evaluation is more appropriate, we focus on show-
ing that the proposed SC classifier can bring sig-
nificant boost from the baseline using both MUC
and accuracy metrics.
5 Conclusion
We have showed that the traditional use of Word-
Net in coreference resolution may not effectively
exploit the WordNet semantic knowledge. We pro-
posed new ways to extract WordNet feature. This
feature, along with other features such as named
entity feature, can be used to build an accurate se-
mantic class (SC) classifier. In addition, we ana-
lyzed the classification errors of the SC classifier
and relaxed SC agreement features to cope with
part of the classification errors. The proposed ac-
curate SC classifier and the relaxation of SC agree-
ment features can boost our baseline coreference
resolution system by 10.4% and 9.7% using MUC
score and anaphor accuracy respectively.
Acknowledgments
We wish to thank Yannick Versley for his sup-
port with BART coreference resolution system and
the three anonymous reviewers for their invaluable
comments. This research was supported by British
Telecom grant CT1080028046 and BISC Program
of UC Berkeley.
References
A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
M. Collins 1999. Head-driven statistical models for
natural language parsing. PhD thesis, University of
Pennsylvania.
H. Daume? III and D. Marcu. 2005. A large-scale
exploration of effective global features for a joint
entity detection and tracking model. In Proc. of
HLT/EMNLP, pages 97-104.
1239
Table 7: MUC score and accuracy of baseline and proposed SC agreement features for ACE2 dataset.
MUC score Accuracy
All bnews npaper nwire All
System R P F R P F R P F R P F
baseline 57.4 58.1 57.7 56.6 55.4 56.0 59.3 60.4 59.9 56.2 58.6 57.3 37.7
Ng 2007a 59.5 70.6 64.6 - - - - - - - - - 51.5
SC STRICT 59.6 73.3 65.7 61.6 72.8 66.7 60.3 74.9 66.8 56.8 72.1 63.5 46.3
+ SC COARSE 59.2 76.7 66.8 61.0 76.7 67.9 59.8 77.2 67.4 56.6 76.2 64.9 45.9
+ SC RELAX1 59.8 79.0 68.1 61.3 79.8 69.3 60.9 80.3 69.3 57.2 76.7 65.5 47.2
+ SC RELAX2 60.2 77.7 67.8 61.5 78.2 68.9 61.4 78.9 69.1 57.5 75.7 65.4 47.4
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In HLT-NAACL.
C. Fellbaum. 1998. An electronic lexical database.
The MIT press.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of
ACL, pages 363-370.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni.
2006. Factorizing complex models: a case study in
mention detection. In Proc. of COLING/ACL, pages
473-480.
S. M. Harabagiu, R. C. Bunescu, and S. J. Maiorano.
2001. Text and knowledge mining for coreference
resolution. In Proc. of NAACL, pages 55-62.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proc. of HLT/NAACL,
pages 289-296.
D. Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proc. of COLING/ACL, pages
768-774.
D. Lin. 1998b. Using collocation statistics in informa-
tion extraction. In Proc. of MUC-7.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S.
Roukos. 2004. A mention synchronous coreference
resolution algorithm based on the Bell tree. In Proc.
of the ACL.
C. Manning and D. Klein. 2003. Optimization,
Maxent Models, and Conditional Estimation with-
out Magic. Tutorial at HLT-NAACL 2003 and ACL
2003.
K. Markert and M. Nissim. 2005. Comparing knowl-
edge sources for nominal anaphora resolution. Com-
putational Linguistics, 31(3):367-401.
A. McCallum and B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Proc. of the NIPS.
V. Ng. 2007a. Semantic Class Induction and Corefer-
ence Resolution. In Proc. of the ACL.
V. Ng. 2007b. Shallow Semantics for Coreference
Resolution. In Proc. of the IJCAI.
C. Nicolae and G. Nicolae 2006. BESTCUT: A Graph
Algorithm for Coreference Resolution. In Proc. of
the EMNLP.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.
2004. Learning to resolve bridging references. In
Proc. of the ACL.
S. P. Ponzetto and M. Strube. 2006. Exploiting se-
mantic role labeling, WordNet and Wikipedia for
coreference resolution. In Proc. of the HLT/NAACL,
pages 192-199.
N. Seco, T. Veale, and J. Hayes. 2004. An Intrinsic
Information Content Metric for Semantic Similarity
in WordNet. Proc. of the European Conference of
Artificial Intelligence.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. A machine
learning approach to coreference resolution of noun
phrases. Computation Linguistics, 27(4):521-544.
Y. Versley, S. P. Ponzetto, M. Poesio, V. Eidelman, A.
Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
BART: a modular toolkit for coreference resolution.
ACL 2008 System demo.
R. Vieira and M. Poesio. 2000. An empirically-based
system for processing definite descriptions. Compu-
tational Linguistics, 26(4):539-593.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.
Hirschman. 1995. A model-theoretic coreference
scoreing scheme. In Proc. of MUC-6, pages 45-52.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Linguistic Data
Consortium.
X. Yang and J. Su. 2007. Coreference resolution us-
ing semantic relatedness information from automat-
ically discovered pattens. In Proc. of the ACL.
1240
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 719?727,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Graph-based Semi-Supervised Learning for Question-Answering
Asli Celikyilmaz
EECS Department
University of California
at Berkeley
Berkeley, CA, 94720
asli@berkeley.edu
Marcus Thint
Intelligent Systems Research Centre
British Telecom (BT Americas)
Jacksonville, FL 32256, USA
marcus.2.thint@bt.com
Zhiheng Huang
EECS Department
University of California
at Berkeley
Berkeley, CA, 94720
zhiheng@eecs.berkeley.edu
Abstract
We present a graph-based semi-supervised
learning for the question-answering (QA)
task for ranking candidate sentences. Us-
ing textual entailment analysis, we obtain
entailment scores between a natural lan-
guage question posed by the user and the
candidate sentences returned from search
engine. The textual entailment between
two sentences is assessed via features rep-
resenting high-level attributes of the en-
tailment problem such as sentence struc-
ture matching, question-type named-entity
matching based on a question-classifier,
etc. We implement a semi-supervised
learning (SSL) approach to demonstrate
that utilization of more unlabeled data
points can improve the answer-ranking
task of QA. We create a graph for labeled
and unlabeled data using match-scores of
textual entailment features as similarity
weights between data points. We apply
a summarization method on the graph to
make the computations feasible on large
datasets. With a new representation of
graph-based SSL on QA datasets using
only a handful of features, and under lim-
ited amounts of labeled data, we show im-
provement in generalization performance
over state-of-the-art QA models.
1 Introduction
Open domain natural language question answer-
ing (QA) is a process of automatically finding an-
swers to questions searching collections of text
files. There are intensive research in this area
fostered by evaluation-based conferences, such as
the Text REtrieval Conference (TREC) (Voorhees,
2004), etc. One of the focus of these research, as
well as our work, is on factoid questions in En-
glish, whereby the answer is a short string that in-
dicates a fact, usually a named entity.
A typical QA system has a pipeline structure
starting from extraction of candidate sentences
to ranking true answers. In order to improve
QA systems? performance many research focus
on different structures such as question process-
ing (Huang et al, 2008), information retrieval
(Clarke et al, 2006), information extraction (Sag-
gion and Gaizauskas, 2006), textual entailment
(TE) (Harabagiu and Hickl, 2006) for ranking, an-
swer extraction, etc. Our QA system has a sim-
ilar pipeline structure and implements a new TE
module for information extraction phase of the QA
task. TE is a task of determining if the truth of a
text entails the truth of another text (hypothesis).
Harabagui and Hickl (2006) has shown that using
TE for filtering or ranking answers can enhance
the accuracy of current QA systems, where the an-
swer of a question must be entailed by the text that
supports the correctness of this answer.
We derive information from pair of texts, i.e.,
question as hypothesis and candidate sentence
as the text, potentially indicating containment of
true answer, and cast the inference recognition
as classification problem to determine if a ques-
tion text follows candidate text. One of the chal-
lenges we face with is that we have very lim-
ited amount of labeled data, i.e., correctly labeled
(true/false entailment) sentences. Recent research
indicates that using labeled and unlabeled data in
semi-supervised learning (SSL) environment, with
an emphasis on graph-based methods, can im-
prove the performance of information extraction
from data for tasks such as question classifica-
tion (Tri et al, 2006), web classification (Liu et
al., 2006), relation extraction (Chen et al, 2006),
passage-retrieval (Otterbacher et al, 2009), vari-
ous natural language processing tasks such as part-
of-speech tagging, and named-entity recognition
(Suzuki and Isozaki, 2008), word-sense disam-
719
biguation (Niu et al, 2005), etc.
We consider situations where there are much
more unlabeled data, XU , than labeled data, XL,
i.e., nL  nU . We construct a textual entail-
ment (TE) module by extracting features from
each paired question and answer sentence and de-
signing a classifier with a novel yet feasible graph-
based SSL method. The main contributions are:
? construction of a TE module to extract match-
ing structures between question and answer sen-
tences, i.e., q/a pairs. Our focus is on identifying
good matching features from q/a pairs, concerning
different sentence structures in section 2,
? representation of our linguistic system by a
form of a special graph that uses TE scores in de-
signing a novel affinity matrix in section 3,
? application of a graph-summarization method
to enable learning from a very large unlabeled and
rather small labeled data, which would not have
been feasible for most sophisticated learning tools
in section 4. Finally we demonstrate the results of
experiments with real datasets in section 5.
2 Feature Extraction for Entailment
Implementation of different TE models has pre-
viously shown to improve the QA task using su-
pervised learning methods (Harabagiu and Hickl,
2006). We present our recent work on the task of
QA, wherein systems aim at determining if a text
returned by a search engine contains the correct
answer to the question posed by the user. The ma-
jor categories of information extraction produced
by our QA system characterizes features for our
TE model based on analysis of q/a pairs. Here we
give brief descriptions of only the major modules
of our QA due to space limitations.
2.1 Pre-Processing for Feature Extraction
We build the following pre-processing modules
for feature extraction to be applied prior to our tex-
tual entailment analysis.
Question-Type Classifier (QC): QC is the task
of identifying the type of a given question among
a predefined set of question types. The type of
a question is used as a clue to narrow down the
search space to extract the answer. We used our
QC system presented in (Huang et al, 2008),
which classifies each question into 6-coarse cat-
egories (i.e., abbr., entity, human, location, num-
ber, description) as well as 50-fine categories (i.e.,
color, food, sport, manner, etc.) with almost
90% accuracy. For instance, for question ?How
many states are there in US??, the question-type
would be ?NUMBER? as course category, and
?Count? for the finer category, represented jointly
as NUM:Count. The QC model is trained via sup-
port vector machines (SVM) (Vapnik, 1995) con-
sidering different features such as semantic head-
word feature based on variation of Collins rules,
hypernym extraction via Lesk word disambigua-
tion (Lesk, 1988), regular expressions for wh-
word indicators, n-grams, word-shapes(capitals),
etc. Extracted question-type is used in connection
with our Named-Entity-Recognizer, to formulate
question-type matching feature, explained next.
Named-Entity Recognizer (NER): This com-
ponent identifies and classifies basic entities such
as proper names of person, organization, prod-
uct, location; time and numerical expressions such
as year, day, month; various measurements such
as weight, money, percentage; contact information
like address, web-page, phone-number, etc. This
is one of the fundamental layers of information
extraction of our QA system. The NER module
is based on a combination of user defined rules
based on Lesk word disambiguation (Lesk, 1988),
WordNet (Miller, 1995) lookups, and many user-
defined dictionary lookups, e.g. renown places,
people, job types, organization names, etc. During
the NER extraction, we also employ phrase analy-
sis based on our phrase utility extraction method
using Standford dependency parser ((Klein and
Manning, 2003)). We can categorize entities up
to 6 coarse and 50 fine categories to match them
with the NER types from QC module.
Phrase Identification(PI): Our PI module un-
dertakes basic syntactic analysis (shallow pars-
ing) and establishes simple, un-embedded linguis-
tic structures such as noun-phrases (NN), basic
prepositional phrases (PP) or verb groups (VG).
In particular PI module is based on 56 different
semantic structures identified in Standford depen-
dency parser in order to extract meaningful com-
pound words from sentences, e.g., ?They heard
high pitched cries.?. Each phrase is identified with
a head-word (cries) and modifiers (high pitched).
Questions in Affirmative Form: To derive lin-
guistic information from pair of texts (statements),
we parse the question and turn into affirmative
form by replacing the wh-word with a place-
holder and associating the question word with the
question-type from the QC module. For example:
720
?What is the capital of France?? is written in af-
firmative form as ?[X]LOC:City is the capital of
FranceLOC:Country.?. Here X is the answer text
of LOC:City NER-type, that we seek.
Sentence Semantic Component Analysis: Us-
ing shallow semantics, we decode the underlying
dependency trees that embody linguistic relation-
ships such as head-subject (H-S), head-modifier
(complement) (H-M), head-object (H-O), etc. For
instance, the sentence ?Bank of America acquired
Merrill Lynch in 2008.? is partitioned as:
? Head (H): acquired
? Subject (S): Bank of America[Human:group]
? Object (O): Merrill Lynch[Human:group]
? Modifier (M): 2008[Num:Date]
These are used as features to match components of
questions like ?Who purchased Merrill Lynch??.
Sentence Structure Analysis: In our question
analysis, we observed that 98% of affirmed ques-
tions did not contain any object and they are also
in copula (linking) sentence form that is, they
are only formed by subject and information about
the subject as: {subject + linking-verb + subject-
info.}. Thus, we investigate such affirmed ques-
tions different than the rest and call them copula
sentences and the rest as non-copula sentences. 1
For instance our system recognizes affirmed ques-
tion ? Fred Durst?s group name is [X]DESC:Def?.
as copula-sentence, which consists of subject (un-
derlined) and some information about it.
2.2 Features from Paired Sentence Analysis
We extract the TE features based on the above lex-
ical, syntactic and semantic analysis of q/a pairs
and cast the QA task as a classification problem.
Among many syntactic and semantic features we
considered, here we present only the major ones:
(1) (QTCF) Question-Type-Candidate Sen-
tence NER match feature: Takes on the value
?1? when the candidate sentence contains the fine
NER of the question-type, ?0.5? if it contains the
coarse NER or ?0? if no NER match is found.
(2) (QComp) Question component match fea-
tures: The sentence component analysis is applied
on both the affirmed question and the candidate
sentence pairs to characterize their semantic com-
ponents including subject(S), object(O), head (H)
and modifiers(M). We match each semantic com-
ponent of a question to the best matching com-
1One option would have been to leave out the non-copula
questions and build the model for only copula questions.
ponent of a candidate sentence. For example for
the given question, ?When did Nixon die??, when
the following candidate sentence, i.e., ?Richard
Nixon, 37th President of USA, passed away of
stroke on April 22, 1994.? is considered, we ex-
tract the following component match features:
? Head-Match: die?pass away
? Subject-Match: Nixon?Richard Nixon
? Object-Match: ?
? Modifier-Match: [X]?April 22, 1994
In our experiments we observed that converted
questions have at most one subject, head, object
and a few modifiers. Thus, we used one feature for
each and up to three for M-Match features. The
feature values vary based on matching type, i.e.,
exact match, containment, synonym match, etc.
For example, the S-Match feature will be ?1.0?
due to head-match of the noun-phrase.
(3) (LexSem) Lexico-Syntactic Alignment
Features: They range from the ratio of consecu-
tive word overlap between converted question (Q)
and candidate sentence (S) including
?Unigram/Bigram, selecting individual/pair of ad-
jacent tokens in Q matching with the S
?Noun and verb counts in common, separately.
?When words don?t match we attempt matching
synonyms in WordNet for most common senses.
?Verb match statistics using WordNet?s cause and
entailment relations.
As a result, each q/a pair is represented as a fea-
ture vector xi ? <d characterizing the entailment
information between them.
3 Graph Based Semi-Supervised
Learning for Entailment Ranking
We formulate semi-supervised entailment rank
scores as follows. Let each data point in
X = {x1, ..., xn}, xi ? <d represents infor-
mation about a question and candidate sentence
pair and Y = {y1, ..., yn} be their output la-
bels. The labeled part of X is represented with
XL = {x1, ..., xl} with associated labels YL =
{y1, ..., yl}
T . For ease of presentation we concen-
trate on binary classification, where yi can take
on either of {?1,+1} representing entailment or
non-entailment. X has also unlabeled part, XU =
{x1, ..., xu}, i.e., X = XL ? XU . The aim is to
predict labels for XU . There are also other testing
points, XTe, which has the same properties as X .
Each node V in graph g = (V,E) represents a
feature vector, xi ? <d of a q/a pair, characteriz-
721
ing their entailment relation information. When all
components of a hypothesis (affirmative question)
have high similarity with components of text (can-
didate sentence), then entailment score between
them would be high. Another pair of q/a sentences
with similar structures would also have high en-
tailment scores as well. So similarity between two
q/a pairs xi, xj , is represented with wij ? <n?n,
i.e., edge weights, and is measured as:
wij = 1?
d?
q=1
|xiq?xjq |
d (1)
As total entailment scores get closer, the larger
their edge weights would be. Based on our sen-
tence structure analysis in section 2, given dataset
can be further separated into two, i.e., Xcp con-
taining q/a pairs in which affirmed questions are
copula-type, and Xncp containing q/a pairs with
non-copula-type affirmed questions. Since cop-
ula and non-copula sentences have different struc-
tures, e.g., copula sentences does not usually have
objects, we used different sets of features for each
type. Thus, we modify edge weights in (1) as fol-
lows:
w?ij =
?
??????
??????
0 xi ? Xcp, xj ? Xncp
1?
dcp?
q=1
|xiq?xjq |
dcp
xi, xj ? Xcp
1?
dncp?
q=1
|xiq?xjq |
dncp
xi, xj ? Xncp
(2)
The diagonal degree matrix D is defined for graph
g by D=
?
j w?ij . In general graph-based SSL, a
function over the graph is estimated such that it
satisfies two conditions: 1) close to the observed
labels , and 2) be smooth on the whole graph by:
argminf
?
i?L
(fi ? yi)
2+?
?
i,j?L?U
w?ij(fi ? fj)
2
(3)
The second term is a regularizer to represent the
label smoothness, fTLf , where L = D?W is the
graph Laplacian. To satisfy the local and global
consistency (Zhou et al, 2004), normalized com-
binatorial Laplacian is used such that the second
term in (3) is replaced with normalized Laplacian,
L = D?1/2LD?1/2, as follows:
?
i,j?L?U
wij(
fi?
di
? fj?
dj
)2 = fTLf (4)
Setting gradient of loss function to zero, optimum
f?, where Y = {YL ? YU} , YU =
{
ynl+1 = 0
}
;
f? = (1+ ? (1? L))?1 Y (5)
Most graph-based SSLs are transductive, i.e., not
easily expendable to new test points outside L?U .
In (Delalleau et al, 2005) an induction scheme is
proposed to classify a new point xTe by
f?(xTe) =
?
i?L?U wxifi?
i?L?U wxi
(6)
Thus, we use induction, where we can, to avoid
re-construction of the graph for new test points.
4 Graph Summarization
Research on graph-based SSL algorithms point
out their effectiveness on real applications, e.g.,
(Zhu et al, 2003), (Zhou and Scho?lkopf, 2004),
(Sindhwani et al, 2007). However, there is still
a need for fast and efficient SSL methods to deal
with vast amount of data to extract useful informa-
tion. It was shown in (Delalleau et al, 2006) that
the convergence rate of the propagation algorithms
of SSL methods isO(kn2), which mainly depends
on the form of eigenvectors of the graph Laplacian
(k is the number of nearest neighbors). As the
weight matrix gets denser, meaning there will be
more data points with connected weighted edges,
the more it takes to learn the classifier function via
graph. Thus, the question is, how can one reduce
the data points so that weight matrix is sparse, and
it takes less time to learn?
Our idea of summarization is to create repre-
sentative vertices of data points that are very close
to each other in terms of edge weights. Suffice to
say that similar data points are likely to represent
denser regions in the hyper-space and are likely to
have same labels. If these points are close enough,
we can characterize the boundaries of these group
of similar data points with respect to graph and
then capture their summary information by new
representative vertices. We replace each data point
within the boundary with their representative ver-
tex, to form a summary graph.
4.1 Graph Summarization Algorithm
Let each selected dataset be denoted as Xs =
{xsi} , i = 1...m, s = 1, ..., q, where m is the
number of data points in the sample dataset and
q is the number of sample datasets drawn from
X . The labeled data points, i.e., XL, are ap-
pended to each of these selected Xs datasets,
Xs =
{
xs1, ...x
s
m?l
}
? XL. Using a separate
learner, e.g., SVM (Vapnik, 1995), we obtain pre-
dicted outputs, Y? s =
(
y?s1, ..., y?
s
m?l
)
ofXs and ap-
pend observed labels Y? s = Y? s ? YL.
722
Figure 1: Graph Summarization. (a) Actual data point with predicted class labels, (b) magnified view of
a single node (black) and its boundaries (c) calculated representative vertex, (d) summary dataset.
We define the weight W s and degree Ds ma-
trices of Xs using (1). Diagonal elements of Ds
is converted into a column vector and is sorted to
find the high degree vertices that are surrounded
with large number of close neighbors.
The algorithm starts from the highest degree
node xsi ? X
s, where initial neighbor nodes have
assumably the same labels. This is shown in Fig-
ure 1-(b) with the inner square around the mid-
dle black node, corresponding high degree node.
If its immediate k neighbors, dark blue colored
nodes, have the same label, the algorithm contin-
ues to search for the secondary k neighbors, the
light blue colored nodes, i.e., the neighbors of the
neighbors, to find out if there are any opposite la-
beled nodes around. For instance, for the corre-
sponding node (black) in Figure 1-(b) we can only
go up to two neighbors, because in the third level,
there are a few opposite labeled nodes, in red. This
indicates boundary Bsi for a corresponding node
and unique nearest neighbors of same labels.
Bsi =
{
xsi ?
{
xsj
}nm
j=1
}
(7)
In (7), nm denotes the maximum number of nodes
of aBsi and ?x
s
j , x
s
j? ? B
s
i , y
s
j = y
s
j? = yBsi , where
yBsi is the label of the selected boundary B
s
i .
We identify the edge weights wsij between each
node in the boundary Bsi via (1), thus the bound-
ary is connected. We calculate the weighted av-
erage of the vertices to obtain the representative
summary node of Bsi as shown in Figure 1-(c);
X
s
Bi =
?nm
i 6=j=1
1
2w
s
ij(x
s
i + x
s
j)
?nm
i 6=j=1w
s
ij
(8)
The boundaries of some nodes may only con-
tain themselves because their immediate neigh-
bors may have opposite class labels. Similarly
some may have only k + 1 nodes, meaning only
immediate neighbor nodes have the same labels.
For instance in Fig. 1 the boundary is drawn af-
ter the secondary neighbors are identified (dashed
outer boundary). This is an important indication
that some representative data points are better indi-
cators of class labels than the others due to the fact
that they represent a denser region of same labeled
points. We represent this information with the lo-
cal density constraints. Each new vertex is asso-
ciated with a local density constraint, 0 ? ?j ? 1,
which is equal to the total number of neighbor-
ing nodes used to construct it. We use the nor-
malized density constraints for ease of calcula-
tions. Thus, for a each sample summary dataset,
a local density constraint vector is identified as
?s = {?s1, ..., ?
s
nb}
T . The local density constraints
become crucial for inference where summarized
labeled data are used instead of overall dataset.
Algorithm 1 Graph Summary of Large Dataset
1: Given X = {x1, ..., xn} , X = XL ?XU
2: Set q ? max number of subsets
3: for s? 1, ..., q do
4: Choose a random subset with repetitions
5: Xs = {xs1, ..., x
s
m?l, xm?l+1, ..., xm}
6: Summarize Xs to obtain X
s
in (9)
7: end for
8: Obtain summary datasetX =
{
X
s}q
s=1
=
{
Xi
}p
i=1
and
local density constrains, ? = {?i}
p
i=1.
After all data points are evaluated, the sample
dataset Xs can now be represented with the sum-
mary representative vertices as
X
s
=
{
X
s
B1 , ..., X
s
Bnb
}
. (9)
and corresponding local density constraints as,
?s = {?s1, ..., ?
s
nb}
T , 0 < ?si ? 1 (10)
723
The summarization algorithm is repeated for each
random subset Xs, s = 1, ..., q of very large
dataset X = XL ? XU , see Algorithm 1. As
a result q number of summary datasets X
s
each
of which with nb labeled data points are com-
bined to form a representative sample of X , X =
{
X
s}q
s=1 reducing the number of data from n to
a much smaller number of data, p = q ? nb  n.
So the new summary of the X can be represented
with X =
{
Xi
}p
i=1. For example, an origi-
nal dataset with 1M data points can be divided
up to q = 50 random samples of m = 5000
data points each. Then using graph summariza-
tion each summarized dataset may be represented
with nb ?= 500 data points. After merging sum-
marized data, final summarized samples compile
to 500 ? 50 ?= 25K  1M data points, reduced to
1/40 of its original size. Each representative data
point in the summarized dataset X is associated
with a local density constraints, a p = q ? nb
dimensional row vector as ? = {?i}
p
i=1.
We can summarize a graph separately for dif-
ferent sentence structures, i.e., copula and non-
copula sentences. Then representative data points
from each summary dataset are merged to form fi-
nal summary dataset. The Hybrid graph summary
models in the experiments follow such approach.
4.2 Prediction of New Testing Dataset
Instead of using large dataset, we now use sum-
mary dataset with predicted labels, and local den-
sity constraints to learn the class labels of nte
number of unseen data points, i.e., testing data
points, XTe = {x1, ..., xnte}. Using graph-based
SSL method on the new representative dataset,
X ? = X ? XTe, which is comprised of sum-
marized dataset, X =
{
Xi
}p
i=1, as labeled data
points, and the testing dataset, XTe as unlabeled
data points. Since we do not know estimated lo-
cal density constraints of unlabeled data points, we
use constants to construct local density constraint
column vector for X ? dataset as follows:
?? = {1 + ?i}
p
i=1 ? [1 ... 1]
T ? <nte (11)
0 < ?i ? 1. To embed the local density con-
straints, the second term in (3) is replaced with the
constrained normalized Laplacian, Lc = ?TL?,
?
i,j?L?T
wij(
fi
?
??i ? di
?
fj
?
??j ? dj
)2 = fTLcf
(12)
If any testing vector has an edge between a labeled
vector, then with the usage of the local density
constraints, the edge weights will not not only be
affected by that labeled node, but also how dense
that node is within that part of the graph.
5 Experiments
We demonstrate the results from three sets of ex-
periments to explore how our graph representa-
tion, which encodes textual entailment informa-
tion, can be used to improve the performance of
the QA systems. We show that as we increase
the number of unlabeled data, with our graph-
summarization, it is feasible to extract information
that can improve the performance of QA models.
We performed experiments on a set of 1449
questions from TREC-99-03. Using the search en-
gine 2, we retrieved around 5 top-ranked candi-
date sentences from a large newswire corpus for
each question to compile around 7200 q/a pairs.
We manually labeled each candidate sentence as
true or false entailment depending on the contain-
ment of the true answer string and soundness of
the entailment to compile quality training set. We
also used a set of 340 QA-type sentence pairs from
RTE02-03 and 195 pairs from RTE04 by convert-
ing the hypothesis sentences into question form to
create additional set of q/a pairs. In total, we cre-
ated labeled training dataset XL of around 7600
q/a pairs . We evaluated the performance of graph-
based QA system using a set of 202 questions from
the TREC04 as testing dataset (Voorhees, 2003),
(Prager et al, 2000). We retrieved around 20 can-
didate sentences for each of the 202 test questions
and manually labeled each q/a pair as true/false en-
tailment to compile 4037 test data.
To obtain more unlabeled training data XU,
we extracted around 100,000 document headlines
from a large newswire corpus. Instead of match-
ing headline and first sentence of the document as
in (Harabagiu and Hickl, 2006), we followed a dif-
ferent approach. Using each headline as a query,
we retrieved around 20 top-ranked sentences from
search engine. For each headline, we picked the
1st and the 20th retrieved sentences. Our assump-
tion is that the first retrieved sentence may have
higher probability to entail the headline, whereas
the last one may have lower probability. Each of
these headline-candidate sentence pairs is used as
additional unlabeled q/a pair. Since each head-
2http://lucene.apache.org/java/
724
Features Model MRR Top1 Top5
Baseline ? 42.3% 32.7% 54.5%
QTCF SVM 51.9% 44.6% 63.4%
SSL 49.5% 43.1% 60.9%
LexSem SVM 48.2% 40.6% 61.4%
SSL 47.9% 40.1% 58.4%
QComp SVM 54.2% 47.5% 64.3%
SSL 51.9% 45.5% 62.4%
Table 1: MRR for different features and methods.
line represents a converted question, in order to
extract the question-type feature, we use a match-
ing NER-type between the headline and candidate
sentence to set question-type NER match feature.
We applied pre-processing and feature extrac-
tion steps of section 2 to compile labeled and un-
labeled training and labeled testing datasets. We
use the rank scores obtained from the search en-
gine as baseline of our system. We present the
performance of the models using Mean Recipro-
cal Rank (MRR), top 1 (Top1) and top 5 predic-
tion accuracies (Top5) as they are the most com-
monly used performance measures of QA systems
(Voorhees, 2004). We performed manual iterative
parameter optimization during training based on
prediction accuracy to find the best k-nearest pa-
rameter for SSL, i.e., k = {3, 5, 10, 20, 50} , and
best C =
{
10?2, .., 102
}
and ? =
{
2?2, .., 23
}
for RBF kernel SVM. Next we describe three dif-
ferent experiments and present individual results.
Graph summarization makes it feasible to exe-
cute SSL on very large unlabeled datasets, which
was otherwise impossible. This paper has no as-
sumptions on the performance of the method in
comparison to other SSL methods.
Experiment 1. Here we test individual con-
tribution of each set of features on our QA sys-
tem. We applied SVM and our graph based SSL
method with no summarization to learn models
using labeled training and testing datasets. For
SSL we used the training as labeled and testing
as unlabeled dataset in transductive way to pre-
dict the entailment scores. The results are shown
in Table 1. From section 2.2, QTCF represents
question-type NER match feature, LexSem is the
bundle of lexico-semantic features and QComp is
the matching features of subject, head, object, and
three complements. In comparison to the baseline,
QComp have a significant effect on the accuracy
of the QA system. In addition, QTCF has shown
to improve the MRR performance by about 22%.
Although the LexSem features have minimal se-
mantic properties, they can improve MRR perfor-
mance by 14%.
Experiment 2. To evaluate the performance of
graph summarization we performed two separate
experiments. In the first part, we randomly se-
lected subsets of labeled training dataset XiL ?
XL with different sample sizes, niL ={1% ? nL,
5% ? nL, 10% ? nL, 25% ? nL, 50% ? nL,
100% ? nL}, where nL represents the sample size
of XL. At each random selection, the rest of the
labeled dataset is hypothetically used as unlabeled
data to verify the performance of our SSL using
different sizes of labeled data. Table 2 reports
the MRR performance of QA system on testing
dataset using SVM and our graph-summary SSL
(gSum SSL) method using the similarity function
in (1). In the second part of the experiment, we
applied graph summarization on copula and non-
copula questions separately and merged obtained
representative points to create labeled summary
dataset. Then using similarity function in (2) we
applied SSL on labeled summary and unlabeled
testing via transduction. We call these models as
Hybrid gSum SSL. To build SVM models in the
same way, we separated the training dataset into
two based on copula and non-copula questions,
Xcp, Xncp and re-run the SVM method separately.
The testing dataset is divided into two accordingly.
Predicted models from copula sentence datasets
are applied on copula sentences of testing dataset
and vice versa for non- copula sentences. The pre-
dicted scores are combined to measure overall per-
formance of Hybrid SVM models. We repeated
the experiments five times with different random
samples and averaged the results.
Note from Table 2 that, when the number of
labeled data is small (niL < 10% ? nL), graph
based SSL, gSum SSL, has a better performance
compared to SVM. As the percentage of labeled
points in training data increase, the SVM perfor-
mance increases, however graph summary SSL is
still comparable with SVM. On the other hand,
when we build separate models for copula and
non-copula questions with different features, the
performance of the overall model significantly in-
creases in both methods. Especially in Hybrid
graph-Summary SSL, Hybrid gSum SSL, when
the number of labeled data is small (niL < 25% ?
nL) performance improvement is better than rest
725
% SVM gSum SSL Hybrid SVM Hybrid gSum SSL
#Labeled MRR Top1 Top5 MRR Top1 Top5 MRR Top1 Top5 MRR Top1 Top5
1% 45.2 33.2 65.8 56.1 44.6 72.8 51.6 40.1 70.8 59.7 47.0 75.2
5% 56.5 45.1 73.0 57.3 46.0 73.7 54.2 40.6 72.3 60.3 48.5 76.7
10% 59.3 47.5 76.7 57.9 46.5 74.2 57.7 47.0 74.2 60.4 48.5 77.2
25% 59.8 49.0 78.7 58.4 45.0 79.2 61.4 49.5 78.2 60.6 49.0 76.7
50% 60.9 48.0 80.7 58.9 45.5 79.2 62.2 51.0 79.7 61.3 50.0 77.2
100% 63.5 55.4 77.7 59.7 47.5 79.7 67.6 58.0 82.2 61.9 51.5 78.2
Table 2: The MRR (%) results of graph-summary SSL (gSum SSL) and SVM as well as Hybrid gSum
SSL and Hybrid SVM with different sizes of labeled data.
#Unlabeled MRR Top1 Top5
25K 62.1% 52.0% 76.7%
50K 62.5% 52.5% 77.2%
100K 63.3% 54.0% 77.2%
Table 3: The effect of number of unlabeled data
on MRR from Hybrid graph Summarization SSL.
of the models. As more labeled data is introduced,
Hybrid SVM models? performance increase dras-
tically, even outperforming the state-of-the art
MRR performance on TREC04 datasets presented
in (Shen and Klakow, 2006) i.e., MRR=67.0%,
Top1=62.0%, Top5=74.0%. This is due to the fact
that we establish two seperate entailment models
for copula and non-copula q/a sentence pairs that
enables extracting useful information and better
representation of the specific data.
Experiment 3. Although SSL methods are ca-
pable of exploiting information from unlabeled
data, learning becomes infeasible as the number
of data points gets very large. There are vari-
ous research on SLL to overcome the usage of
large number of unlabeled dataset challenge (De-
lalleau et al, 2006). Our graph summarization
method, Hybrid gsum SSL, has a different ap-
proach. which can summarize very large datasets
into representative data points and embed the orig-
inal spatial information of data points, namely lo-
cal density constraints, within the SSL summa-
rization schema. We demonstrate that as more la-
beled data is used, we would have a richer sum-
mary dataset with additional spatial information
that would help to improve the the performance
of the graph summary models. We gradually in-
crease the number of unlabeled data samples as
shown in Table 3 to demonstrate the effects on the
performance of testing dataset. The results show
that the number of unlabeled data has positive ef-
fect on performance of graph summarization SSL.
6 Conclusions and Discussions
In this paper, we applied a graph-based SSL al-
gorithm to improve the performance of QA task
by exploiting unlabeled entailment relations be-
tween affirmed question and candidate sentence
pairs. Our semantic and syntactic features for tex-
tual entailment analysis has individually shown to
improve the performance of the QA compared to
the baseline. We proposed a new graph repre-
sentation for SSL that can represent textual en-
tailment relations while embedding different ques-
tion structures. We demonstrated that summariza-
tion on graph-based SSL can improve the QA task
performance when more unlabeled data is used to
learn the classifier model.
There are several directions to improve our
work: (1) The results of our graph summarization
on very large unlabeled data is slightly less than
best SVM results. This is largely due to using
headlines instead of affirmed questions, wherein
headlines does not contain question-type and some
of them are not in proper sentence form. This ad-
versely effects the named entity match of question-
type and the candidate sentence named entities as
well as semantic match component feature extrac-
tion. We will investigate experiment 3 by using
real questions from different sources and construct
different test datasets. (2) We will use other dis-
tance measures to better explain entailment be-
tween q/a pairs and compare with other semi-
supervised and transductive approaches.
726
References
Jinxiu Chen, Donghong Ji, C. Lim Tan, and Zhengyu
Niu. 2006. Relation extraction using label propaga-
tion based semi-supervised learning. In Proceedings
of the ACL-2006.
Charles L.A. Clarke, Gordon V. Cormack, R. Thomas
Lynam, and Egidio L. Terra. 2006. Question an-
swering by passage selection. In In: Advances in
open domain question answering, Strzalkowski, and
Harabagiu (Eds.), pages 259?283. Springer.
Oliver Delalleau, Yoshua Bengio, and Nicolas Le
Roux. 2005. Efficient non-parametric function in-
duction in semi-supervised learning. In Proceedings
of AISTAT-2005.
Oliver Delalleau, Yoshua Bengio, and Nicolas Le
Roux. 2006. Large-scale algorithms. In In: Semi-
Supervised Learning, pages 333?341. MIT Press.
Sandra Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In In Proc. of ACL-2006, pages
905?912.
Zhiheng Huang, Marcus Thint, and Zengchang Qin.
2008. Question classification using headwords and
their hypernyms. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-08), pages 927?936.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the ACL-2003, pages 423?430.
Michael Lesk. 1988. They said true things, but called
them by wrong names - vocabulary problems in re-
trieval systems. In In Proc. 4th Annual Conference
of the University of Waterloo Centre for the New
OED.
Rong Liu, Jianzhong Zhou, and Ming Liu. 2006. A
graph-based semi-supervised learning algorithm for
web page classification. In Proc. Sixth Int. Conf. on
Intelligent Systems Design and Applications.
George Miller. 1995. Wordnet: A lexical database for
english. In Communications of the ACL-1995.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.
2005. Word sense disambiguation using labeled
propagation based semi-supervised learning. In
Proceedings of the ACL-2005.
Jahna Otterbacher, Gunes Erkan, and R. Radev
Dragomir. 2009. Biased lexrank:passage retrieval
using random walks with question-based priors. In-
formation Processing and Management, 45:42?54.
Eric W. Prager, John M.and Brown, Dragomir Radev,
and Krzysztof Czuba. 2000. One search engine or
two for question-answering. In Proc. 9th Text RE-
trieval conference.
Horacio Saggion and Robert Gaizauskas. 2006. Ex-
periments in passage selection and answer extrac-
tion for question answering. In Advances in natural
language processing, pages 291?302. Springer.
Dan Shen and Dietrich Klakow. 2006. Exploring cor-
relation of dependency relation paths for answer ex-
traction. In Proceedings of ACL-2006.
Vikas Sindhwani, Wei Chu, and S. Sathiya Keerthi.
2007. Semi-supervised gaussian process classifiers.
In Proceedings of the International Joint Conference
on Artificial Intelligence (IJCAI-07), pages 1059?
1064.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of the
ACL-2008.
Nguyen Thanh Tri, Nguyen Minh Le, and Akira Shi-
mazu. 2006. Using semi-supervised learning for
question classification. In ICCPOL, pages 31?41.
LNCS 4285.
Vilademir Vapnik. 1995. The nature of statistical
learning theory. In Springer-Verlag, New York.
Ellen M. Voorhees. 2003. Overview of the trec 2003
question answering track. In Proc. 12th Text RE-
trieval conference.
Ellen M. Voorhees. 2004. Overview of trec2004 ques-
tion answering track.
Dengyong Zhou and Bernhard Scho?lkopf. 2004.
Learning from labeled and unlabeled data using ran-
dom walks. In Proceedings of the 26th DAGM Sym-
posium, (Eds.) Rasmussen, C.E., H.H. Blthoff, M.A.
Giese and B. Schlkopf, pages 237?244, Berlin, Ger-
many. Springer.
Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Ja-
son Weston, and Bernhard Scho?lkopf. 2004. Learn-
ing with local and global consistency. Advances
in Neural Information Processing Systems, 16:321?
328.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani.
2003. Semi-supervised learning: From Gaus-
sian Fields to Gaussian processes. Technical Re-
port CMU-CS-03-175, Carnegie Mellon University,
Pittsburgh.
727
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2094?2104,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Resolving Referring Expressions in Conversational Dialogs for
Natural User Interfaces
Asli Celikyilmaz, Zhaleh Feizollahi, Dilek Hakkani-Tur, Ruhi Sarikaya
Microsoft
asli@ieee.org, zhalehf@microsoft.com
dilek@ieee.org, ruhi.sarikaya@microsoft.com
Abstract
Unlike traditional over-the-phone spoken
dialog systems (SDSs), modern dialog
systems tend to have visual rendering on
the device screen as an additional modal-
ity to communicate the system?s response
to the user. Visual display of the system?s
response not only changes human behav-
ior when interacting with devices, but also
creates new research areas in SDSs. On-
screen item identification and resolution
in utterances is one critical problem to
achieve a natural and accurate human-
machine communication. We pose the
problem as a classification task to cor-
rectly identify intended on-screen item(s)
from user utterances. Using syntactic, se-
mantic as well as context features from the
display screen, our model can resolve dif-
ferent types of referring expressions with
up to 90% accuracy. In the experiments we
also show that the proposed model is ro-
bust to domain and screen layout changes.
1 Introduction
Todays natural user interfaces (NUI) for applica-
tions running on smart devices, e.g, phones (SIRI,
Cortana, GoogleNow), consoles (Amazon FireTV,
XBOX), tablet, etc., can handle not only simple
spoken commands, but also natural conversational
utterances. Unlike traditional over-the-phone spo-
ken dialog systems (SDSs), user hears and sees the
system?s response displayed on the screen as an
additional modality. Having visual access to the
system?s response and results changes human be-
havior when interacting with the machine, creating
new and challenging problems in SDS.
[System]: How can i help you today ?
[User]: Find non-fiction books by Chomsky.
[System]: (Fetches the following books from database)
[User]: ?show details for the oldest production? or
?details for the syntax book? or
?open the last one? or
?i want to see the one on linguistics? or
?bring me Jurafsky?s text book?
Table 1: A sample multi-turn dialog. A list of second turn
utterances referring to the last book (in bold) and a new search
query (highlighted) are shown.
Consider a sample dialog in Table 1 between a
user and a NUI in the books domain. After the sys-
tem displays results on the screen, the user may
choose one or more of the on-screen items with
natural language utterances as shown in Table 1.
Note that, there are multiple ways of referring to
the same item, (e.g. the last book)
1
. To achieve a
natural and accurate human to machine conversa-
tion, it is crucial to accurately identify and resolve
referring expressions in utterances. As important
as interpreting referring expressions (REs) is for
modern NUI designs, relatively few studies have
investigated withing the SDSs. Those that do fo-
cus on the impact of the input from multimodal
interfaces such as gesture for understanding (Bolt,
1980; Heck et al., 2013; Johnston et al., 2002),
touch for ASR error correction (Huggins-Daines
and Rudnicky, 2008), or cues from the screen
(Balchandran et al., 2008; Anastasiou et al., 2012).
Most of these systems are engineered for a specific
1
An item could be anything from a list, e.g. restaurants,
games, contact list, organized in different lay-outs on the
screen.
2094
task, making it harder to generalize for different
domains or SDSs. In this paper, we investigate a
rather generic contextual model for resolving nat-
ural language REs for on-screen item selection to
improve conversational understanding.
Our model, which we call FIS (Flexible Item
Selection), is able to (1) detect if the user is re-
ferring to any item(s) on the screen, and (2) re-
solve REs to identify which items are referred to
and score each item. FIS is a learning based sys-
tem that uses information from pair of user utter-
ance and candidate items on the screen to model
association between them. We cast the task as a
classification problem to determine whether there
is a relation between the utterance and the item,
representing each instance in the training dataset
as relational features.
In a typical SDS, the spoken language under-
standing (SLU) engine maps user utterances into
meaning representation by identifying user?s in-
tent and token level semantic slots via a seman-
tic parser (Mori et al., 2008). The dialog man-
ager uses the SLU components to decide on the
correct system action. For on-screen item selec-
tion SLU alone may not be sufficient. To correctly
associate the user?s utterance with any of the on-
screen items one would need to resolve the rela-
tional information between the utterance and the
items. For instance, consider the dialog in Ta-
ble 1. SLU engine can provide signals to the di-
alog model about the selected item, e.g., that ?lin-
guistics? is a book-genre or content, but may not
be enough to indicate which book the user is refer-
ring. FIS module provides additional information
for the dialog manager by augmenting SLU com-
ponents.
In ?3, we provide details about our data as well
as data collection and annotation steps. In ?4, we
present various syntactic and semantic features to
resolve different REs in utterances. In the exper-
iments (?6), we evaluate the individual impact of
each feature on the FIS model. We analyze the
performance of the FIS model per each type of
REs. Finally, we empirically investigate the ro-
bustness of the FIS model to domain and display
screen changes. When tested on a domain that
is unseen to the training data or on a device that
has a different NUI design, the performance only
slightly degrades proving its robustness to domain
and design changes.
2 Related Work
Although the problems of modern NUIs on smart
devices are fairly new, RE resolution in natural
language has been studied by many in NLP com-
munity.
Multimodal systems provide a natural and ef-
fective way for users to interact with computers
through multiple modalities such as speech, ges-
ture, and gaze. Since the first appearance of the
Put-That-There system (Bolt, 1980), a number of
multimodal systems have been built, among which
there are systems that combine speech, point-
ing (Neal, 1991), and gaze (Koons et al., 1993),
systems that engage users in an intelligent con-
versation (Gustafson et al., 2000). Earlier stud-
ies have shown that multimodal interfaces enable
users to interact with computers naturally and ef-
fectively (Schober and Clark, 1989; Oviatt et al.,
1997). Considered as part of the situated interac-
tive frameworks, many work focus on the prob-
lem of predicting how the user has resolved REs
that is generated by the system, e.g., (Clark and
Wilkes-Gibbs, ; Dale and Viethen, 2009; Giesel-
mann, 2004; Janarthanam and Lemon, 2010; Gol-
land et al., 2010). In this work, focusing on smart
devices, we investigate how the system resolves
the REs in user utterances to take the next correct
action.
In (Pfleger and J.Alexandersson, 2006) a refer-
ence resolution model is presented for a question-
answering system on a mobile, multi-modal inter-
face. Their system has several features to parse
the posed question and keep history of the dia-
log to resolve co-reference issues. Their question-
answering model uses gesture as features to re-
solve queries such as ?what?s the name of that
[pointing gesture] player??, but they do not re-
solve locational referrals such as ?the middle one?
or ?the second harry potter movie?. Others such as
(Funakoshi et al., 2012) resolve anaphoric (?it?)
or exophoric (?this one?) types of expressions in
user utterances to identify geometric objects. In
this paper, we study several types of REs to build
a natural and flexible interaction for the user.
(Heck et al., 2013) present an intent prediction
model enriched with gesture detector to help dis-
ambiguate between different user intents related to
the interface. In (Misu et al., 2014) a situated in-
car dialog model is presented to answer drivers?
spoken queries about their surroundings (no dis-
play screen). They integrate multi-modal inputs of
2095
speech, geo-location and gaze. We investigate a
variety of REs for visual interfaces, and analyze
automatic resolution in a classification task intro-
ducing a wide range of syntactic, semantic and
contextual features. We look at how REs change
with screen layout comparing different devices.
To the best of our knowledge, our work is first to
analyze REs from these aspects.
3 Data
Crowdsourcing services, such as Amazon Me-
chanical Turk or CrowdFlower, have been exten-
sively used for a variety of NLP tasks (Callison-
Burch and Dredze, 2010). Here we explain how
we collected the raw utterances from Crowd-
Flower platform (crowdflower.com).
For each HITApp (Human Intelligence Task
Application), we provide judges with a written ex-
planation about our Media App, a SDS built on a
device with a large screen which displays items in
a grid style layout, and what this particular sys-
tem would do, namely search for books, music,
tv and movies media result
2
Media App returns
results based on the user query using an already
implemented speech recognition, SLU and dialog
engines. For each HIT, the users are shown a dif-
ferent screenshot showing the Media App?s search
results after a first-turn query is issued (e.g., ?find
non-fiction books by Chomsky? in Table 1). Users
are asked to provide five different second turn text
utterances for each screenshot. We launch several
hitapps each with a different prompt to cover dif-
ferent REs.
3.1 HITApp Types and Data Collection
A grid of media items is shown to the user with
a red arrow pointing to the media result we want
them to refer to (see Fig. 1). They can ask to play
(an album or an audio book), select, or ask details
about the particular media item. Each item in each
grid layout becomes a different HIT or screenshot.
3.1.1 Item Layout and Screen Type Variation
The applications we consider have the following
row?column layouts: 1?6, 2?6 and 3?6, as
shown in Fig. 1 (columns vary depending on the
returned item size). By varying the layout, we ex-
pect the referent of the last and the bottom layer
items to change depending on how many rows, or
2
Please e-mail the first author to inquire about the
datasets.
(a) A two row display. (b) A three row display.
(c) Single row display. (d) Display forcing location
based referring expressions.
Figure 1: Sketches of different HITApp Screens.
The red arrows point to the media we want the an-
notators to refer.
columns exist in the grid. In addition, phrases like
?middle?, or ?center? would not appear in the data
when there are only one or two rows. Also, we ex-
pect that the distribution of types of utterances to
vary. For example, in a grid of 1?6, ?the second
one? makes sense, but not so much on a 2?6 grid.
We expect similar change based on the number of
columns.
We use two kinds of screenshots to collect ut-
terances with variations in REs. The first type of
screenshots are aimed to bias the users to refer to
items ?directly? using (full/partial) titles or ?indi-
rectly? using other descriptors, or meta informa-
tion such as year the movie is taken, or the au-
thor of the book. To collect utterances that indi-
rectly referred to items, we need to show screen
shots displaying system results with common ti-
tles, eventually forcing the user to use other de-
scriptors for disambiguation. For example, given
the first turn query ?find harry potter movies?, the
system returns all the Harry Potter series, all of
which contain the words Harry Potter in the title.
The user can either refer in their utterance with the
series number, the subtitle (e.g. The prisoners of
Azkaban) or the location of the movie in the grid
or by date, e.g., ?the new one?,
Because some media items have long titles, or
contain foreign names that are not easy to pro-
nounce, users may chose to refer these items by
their location on the display, such as ?top right?,
?first album?, ?the movie on the bottom left?, etc.
The second type of screen shots contains a tem-
plate for each layout with no actual media item
(Fig. 1(d)) which simply forces user to use loca-
tional references.
2096
3.1.2 Interface Design Variation
In order to test our model?s robustness to a
different screen display on a new device, we
employ an additional collection running another
application named places, designed for hand-
held devices. The places application can assist
users in finding local businesses (restaurants, ho-
tels, schools, etc.). and by nature of the de-
vice size can display fewer media items and ar-
ranges them in a list (one column). The num-
ber of items on the screen at any given time de-
pends on the size of the hand-held device screen.
Figure 2: A HitApp
screen of places app. Items
returned by the system re-
garding the first-turn utter-
ance ?burger places near
me??
The user can scroll
down to see the rest
of the results. Our
collection displays the
items in a 3, 4, and
5-rows per 1 column
layout as shown in
Fig. 2. We use the
same variations in
prompts as in ?3.1. To
generate the HitApp
screens, we search
for nearby places, in
the top search engines
(Google, Bing) and
collect the results to
the first turn natural language search queries
(e.g.,?find me sushi restaurants near me?).
3.2 Data Annotation
We collect text utterances using our media and
places application. Using a similar HitApp we
labeled each utterance with a domain, intent and
segments in utterance with slot tags (see Table 2).
The annotation agreement, Kappa measure (Co-
hen, 1960) is around 85%. Since we are building a
relational model between utterances and each item
on the screen, we ask the annotators to label each
utterance-item as ?0? or ?1? indicating if the utter-
ance is referring to that item or not. ?1? means
the item is the intended one. ?0? indicates the item
is not intended one or the utterance is not refer-
ring to any item on the screen, e.g., new search
query. We also ask the annotators to label each
utterance whether they contain locational (spatial)
references.
Domain Intents (I) & Slots
movie I: find-movie/director/actor,buy-ticket
Slots: name, mpaa-rating (g-rated), date,
books I: find-book, buy-book,
Slots: name, genre(thriller), author, publisher,
music I: find-album, find-song,
Slots: song-name, genre, album-type,...
tv I: find-tvseries/play/add-to-queue..
Slots: name, type(cartoon), show-time....
places I: find-place, select-item(first one)..
Slots: place-type, rating, nearby(closest)....
Table 2: A sample of intents and semantic slot tags
of utterance segments per domain. Examples for
some slots values are presented in parenthesis as
italicized.
3.3 Types of Observed Referring Expressions
We observe four main categories of REs in the ut-
terances that are collected by varying the prompts
and HITApp screens in crowd-sourcing:
Explicit Referential : Explicit mentions of
whole or portions of the title of the item on the
screen, and no other descriptors, e.g.,?show me the
details of star wars six? (referring to the item with
title ?Star wars: Episode VI - Return of the Jedi?).
Implicit Referential : The user refers to the
item using distinguishing features other than the
title, such as the release or publishing date, writ-
ers, actors, image content (describing the item im-
age), genre, etc. ?how about the one with Kevin
Spacey?.
Explicit Locational : The user refers to the
item using the grid design, e.g., ?i want to pur-
chase the e-book on the bottom right corner?.
Implicit Locational : Locational references in
relation to other items on the screen, e.g., ?the sec-
ond of Dan Brown?s book? (showing two of the
Dan Brown?s book on the same row).
4 Feature Extraction for FIS Model
Here, provide descriptions of each set of features
of FIS model used to resolve each expression.
4.1 Similarity Features (SIM)
Similarity features represent the lexical overlap
between the utterance and the item?s title (that
is displayed on the user?s screen) and are mainly
aimed to resolve explicit REs. We represent
each utterance u
i
and item-title t
k
as sequence of
words:
u
i
={w
i
(1), . . . , w
i
(n
i
)}
t
k
={w
k
(1), . . . , w
k
(m
k
)}
2097
item bigrams <bos> call five guys and fries <eos>
<bos> five
five guys
guys burgers
burgers and
and fries
fries <eos>
Table 3: Bigram overlap between the item ?five guys burg-
ers and fries? and utterance?five guys and fries?.
where w
i
(j) and w
k
(j) are the jth word in the se-
quence. Since inflectional morphology may make
a word appear in an utterance in a different form
than what occurs in the official title, we use both
the word form as it appears in the utterance and in
the item title. For example, burger and burgers, or
woman and women are considered as four distinct
words and all included in the bag-of-words. Us-
ing this representation we calculate four different
similarity measures:
Jaccard Similarity: A common feature that
can represent the ratio of the intersection to
the union of unigrams. Consider, for instance,
u
i
=?call five guys and fries? and the item t
k
=?five
guys burgers and fries? in Fig 2. The Jaccard sim-
ilarity S(i,k) is:
S(i,k)=1- ( c(r
i
? r
k
)/c(r
i
? r
k
) )
where the r
i
and r
k
are unigrams of u
i
and t
k
re-
spectively. c(r
i
? r
k
) is the number of common
words of u
i
and t
k
, c(r
i
? r
k
) is the total unigram
vocabulary size between them. In this case, the
S(i,k)=0.66.
Orthographic Distance: Orthographic dis-
tance represent similarity of two text and can be
as simple as an edit distance (Levenshtein dis-
tance) between their graphemes. The Levenshtein
distance (Levenshtein, 1965) counts the insertion,
deletion and substitution operations that are re-
quired to transform an utterance u
i
into item?s title
t
k
.
Word Order: This feature represents how sim-
ilar are the order of words in two text. Sentences
containing the same words but in different orders
may result in different meanings. We extend Jac-
card similarity by defining bigram word vectors r
i
and r
k
and look for overlapping bigrams as in Ta-
ble 3. Among 6 bigrams between them, only 2
are overapping, hence the word-order similarity is
S(i,k)=0.33.
Word Vector: This feature is the cosine sim-
ilarity between the utterance u
i
and the item-
title t
k
that measures the cosine of the an-
gle between them. Here, we use the uni-
gram word counts to represent the word vec-
tors and the word vector similarity is defined as:
S(i, k)=(r
i
? r
k
)/?r
i
?? ?r
k
?.
4.2 Knowledge Graph Features
This binary feature is used to represent overlap be-
tween utterance and the meta information about
the item and is mainly aimed to resolve implicit
REs.
First, we obtain the meta information about
the on-screen items using Freebase (Bollacker et
al., 2008), the knowledge graph that contains
knowledge about classes (books, movies, ...) and
their attributes (title, publisher, year-released, ...).
Knowledge is often represented as the attributes
of the instances, along with values for those prop-
erties. Once we obtain the attribute values of
the item from Freebase, we check if any attribute
overlaps with part of the utterance. For instance,
given an utterance ?how about the one with Kevin
Spacey?, and the item-title ?House of Cards?, the
knowledge graph attributes include year(2013),
cast(Kevin Spacey), director(James Foley),... We
turn the freebase feature ?on? since the actor at-
tribute of that item is contained in the utterance.
We also consider partial matches, e.g., last name
of the actor attribute.
This feature is also used to resolve implicit REs,
with item descriptions, such as ?the messenger boy
with bicycle? referring to the media item Ride Like
Hell, a movie about a bike messenger. The syn-
opsis feature in Freebase fires the freebase meta
feature as the synopsis includes the following pas-
sage: ?... in which real
:::::::::
messenger
:::::
boys are used
as stunts... ?.
4.3 Semantic Location Labeler (SLL)
Feature
This feature set captures spatial cues in utterances
and is mainly aimed to resolve explicit locational
REs. Our goal is to capture the location indicating
tokens in utterances and then resolve the referred
location on the screen by using an indicator fea-
ture. We implement the SLL (Semantic Location
Labeler), a sequence labeling model to tag loca-
tional cues in utterances using Conditional Ran-
dom Fields (CRF) (Lafferty et al., 2001).
We sampled a set of locational utterances from
each domain to be used as training set. We
asked the annotators to label tokens with four
different semantic tags that indicate a location.
2098
The semantic tags include row and column in-
dicator tags, referring to the position or pivotal
reference. For instance, in ?
::::::
second from the
:::
top?, ?second? is the column-position, and
?top? is the row-pivot, indicating the pivotal
reference of the row in a multi-row grid dis-
play. Also in ?
::::
third from the
:::
last?, the ?third?
is the column-position, and the ?last? is the
column-pivot, the pivotal reference of the col-
umn in a multi-column grid display. The fourth
tag, row-position, is used when the specific
row is explicitly referred, such as in ?the Harry
Potter movie in the
:::
first row?.
To train our CRF-based SLL model we use
three types of features: the current word, window
words e.g., previous-word, next-word, etc., using
five-window around the current word, and syntac-
tic features from the part-of-speech (POS) tagger
using the Stanford?s parser (Klein and Manning,
2003).
Row Indicator Feature: This feature sets the
relationship between the n-gram in an utterance in-
dicated by the row-position or row-pivot
tag and the item?s row number on the screen. For
instance, given SSL output row-pivot(?top?)
and item?s location row=1, the value of the feature
is set to ?1?. If no row tag is found by SLL, this
feature is set to ?0?. We use regular expressions to
parse the numerical indicators, e.g., ?top?=?1?.
Column Indicator Feature: Similarly,
this feature indicates if a phrase in utterance
indicated by the column-position or
column-pivot tag matches the item?s col-
umn number on the screen. If SLL model tags
column-pivot(?on the left?), then using the
item?s column number(=1), the value of this
feature is set to ?1?.
4.4 SLU Features
The SLU (Spoken Language Understanding) fea-
tures are used to resolve implicit and explicit REs.
For our dialog system, we build one SLU model
per each domain to extract two sets of semantic at-
tributes from utterances: user?s intent and seman-
tic slots based on a predefined semantic schema
(see examples in Table 2). We use the best in-
tent hypothesis as a categorical feature in our FIS
model. Although FIS is not an intent detection
model, the intent from SLU is an effective seman-
tic feature in resolving REs. Consider second turn
utterance such as ?weather in seattle?, which is
a ?find? intent that is a new search or not related
to any item on the screen. We map SLU intents
such as find-book or find-place, to more specific
ones, so that the intent feature would have values
such as find, filter, check-time, not specific to a
domain or device. The intent feature helps us to
identify if user?s utterance is related to any item
on the screen. We also use the best slot hypothesis
from the SLU slot model and search if there is full
overlap of any recognized slot value with either the
item-title or the item meta-information from free-
base. In addition, we include the longest slot value
n-gram match as an additional feature. We add
a binary feature per domain, indicating whether
there is a slot value match. Because we are us-
ing generic intents as categorical features instead
of specific intents, and a slot value match feature
instead of domain specific slot types as features,
our models are rather domain independent.
5 GBDT Classifier
Among various classifier learning algorithms, we
choose the GBDT (gradient boosted decision tree)
(Friedman, 2001; Hastie et al., 2009), also known
as MART (Multiple Additive Regression Trees).
GBDT
3
is an efficient algorithm which learns an
ensemble of trees. We find the main advantage
of the decision tree classifier as opposed to other
non-linear classifiers such as SVM (support vec-
tor machines) (Vapnik, 1995) or NN (neural net-
works) (Bishop, 1995) is the interpretability. De-
cision trees are ?white boxes? in the sense that per-
feature gain can be expressed by the magnitude
of their weights, while SVM or NN?s are gener-
ally black boxes, i.e. we cannot read the acquired
knowledge in a comprehensible way. Addition-
ally, decision trees can easily accept categorical
and continuous valued features. We also present
the results of the SVM models.
6 Experiments
We investigate several aspects of the SISI model
including its robustness in resolving REs for do-
main or device variability. We start with the details
of the data and model parameters.
We collect around 16K utterances in the me-
dia domains (movies, music, tv, and books) and
around 10K utterances in places (businesses and
3
Treenet: http://www.salford-systems.com/products/
treenet is the implementation of the GBDT which is used in
this paper.
2099
Movies Tv Music Book Overall Media Places
Feature Description GBDT SVM GBDT SVM GBDT SVM GBDT SVM GBDT SVM GBDT SVM
SLL 79.6 77.1 62.0 62.0 77.1 76.5 63.7 63.0 83.6 82.7 67.9 68.9
SIM 86.6 85.7 78.7 74.1 84.9 84.0 81.6 77.3 88.5 88.3 67.1 66.5
Knowledge Graph (KG) 81.0 82.0 64.8 65.6 86.3 85.4 77.8 77.9 84.4 84.1 76.5 76.5
SLU (Gold) 91.7 91.8 89.1 88.5 87.8 87.5 86.3 84.9 83.7 83.2 77.8 71.1
SLU (Pred.) 75.8 72.6 80.3 79.8 84.3 84.1 82.4 82.4 81.4 80.9 71.4 67.8
SIM+SLL 90.9 90.2 87.2 87.1 85.9 86.2 88.5 87.6 91.9 91.9 78.9 73.4
SIM+SLL+KG 91.7 91.3 89.9 89.1 89.1 87.7 91.4 90.3 93.0 92.7 85.9 82.3
SIM+SLL+KG+SLU(Gold) 96.2 95.01 95.2 95.09 90.3 89.9 94.6 94.0 93.7 93.2 86.3 84.3
SIM+SLL+KG+SLU(Pred.) 90.9 90.8 92.3 92.00 86.9 85.7 93.1 93.0 89.3 88.9 85.7 83.9
Table 5: Performance of the FIS models on test data using different features. Acc:Accuracy,. SIM: sim-
ilarity features; SLU:Spoken Language Understanding features (intent and slot features); SLL:Semantic
Locational Labeler features; Gold: using true intent and slot values, Pred.: using predicted intent and
slot values from the SLU models.
Model: Movies TV Music Books Places
Intent Acc. 84.5% 87.4% 87.6% 98.1% 89.5%
Slot F-score 92.1F 89.4F 88.5F 86.6F 88.4F
Table 4: The performance of the SLU Engine?s
intent detection models in accuracy (Acc.) and slot
tagging models in F-Score on the test dataset.
locations) domain. We also construct additional
negative instances from utterance-item pairs us-
ing first turn non-selection queries, which mainly
indicate a new search or starting over. In total
we compile around 250K utterance-item pairs for
media domains and 150K utterance-item pairs for
the places domain.
4
We randomly split each col-
lection into 60%-20%-20% parts to construct the
train/dev/test datasets. We use the dev set to tune
the regularization parameter for the GBDT and
SVM using LIBSVM (Chang and Lin, 2011) with
linear kernel.
We use the training dataset to build the SLU in-
tent and slot models for each domain. For the in-
tent model, we use the GBDT classifier with n-
gram and lexicon features. The lexicon entries are
obtained from Freebase and are used as indicator
variables, e.g., whether the utterance contains an
instance which exists in the lexicon. Similarly,
we train a semantic slot tagging model using CRF
method. We use n-gram features with up to five-
gram window, and lexicon features similar to the
intent models. Table 4 shows the accuracy and F-
score values of SLU models on the test data. The
slot and intent performance is consistent accroess
4
In the final version of the paper, we will provide anno-
tated data sets on a web page, which is reserved due to blind
review.
domains. The books domain has only two intents
and hence we observe much better intent perfor-
mance compared to other domains.
6.1 Impact of Individual FIS Features
In our first experiment, we investigate the impact
of individual feature sets on FIS model?s perfor-
mance. We train a set of FIS models on the entire
media dataset to investigate the per-feature gain on
the test dataset for each domain. We also train an-
other set of FIS models with the same feature sets,
this time on the places dataset and present the re-
sults on the places test set. Table 5 shows the re-
sults. We measure the performance starting with
individual feature sets, and then incrementally add
each feature set. Note that the SLU feature set
includes the categorical intent, binary slot-value
match and the longest slot value n-gram match
with the item?s title or meta information. The SLL
feature set includes two features indicating the row
and column (see ?4.3).
As expected, larger gains in accuracy are ob-
served when features that resolve different REs
are used. Resolving locational cues in utter-
ances with SLL features considerably impacts the
performance when used together with similarity
(SIM) features. We see a positive impact on per-
formance as we add the knowledge graph features,
which are used to resolve implicit REs. Using
only the predicted SLU features in feature gener-
ation without golden values degrades the perfor-
mance. Although the results are not statistically
significant, the GBDT outperforms the SVM for
almost all models, except for a few models, where
the results are similar. However, the models which
2100
0.2 0.4 0.6 0.8 1
Knowledge-Graph
SIM-WordOrder
SLL
SIM-Jaccard
SLU-Slot
SIM-Levenstein
SIM-WordVector
SLU-intent
Feature Weights
 Places Domain N All Media Domains
Figure 3: A sample of normalized feature weights of the
GBDT FIS models across domains.
combine different features as apposed to individ-
ual feature set (the above the line models versusu
below the horizantil line models) are statistically
significant (based on the student t-test p?0.01).
Next, we illustrate the significance of individual
features across domains as well as devices. Fig. 3
compares the normalized feature weights of me-
dia and places domains. Across domains there are
similar features with similar weight values such as
SLU-intent, some similarity features (SIM-) and
even spatial cue features (SLL). It is not surpris-
ing to observe that the places domain knowledge-
graph meta feature weights are noticeably larger
than all media model features. We think that this
is due to the way the REs are used when the de-
vice changes (places app is on a phone with a
smaller screen display). Especially, places appli-
cation users refer items related to restaurants, li-
braries, etc., not so much by their names, but more
so with implicit REs by using: the location (refer-
ring to the address: ?call the one on 31 street?) or
cuisine (?Chinese?), or the star-rating (?with the
most stars?), etc.
6.2 Resolution Across REs
We go on to analyze the performance of differ-
ent RE types. A particularly interesting set of er-
rors we found from the previous experiments are
those that involve implicit referrals. Table 6 shows
the distribution of different REs in the collected
datasets.
Some noticeable instances with false positives
for implicit locational REs include ambiguous
cases or item referrals with one of its facets that
require further resolution including comparison to
other items, e.g., ?the nearest one?. Table 7 shows
further examples. As might be expected, the lo-
cational cues are less common compared to other
All Media Places
Utterance Type % Acc. % Acc.
All utterances 100% 93.7% 100% 86.3%
Direct/Indirect RE 81% 93.9% 73% 86.9%
Locational RE 19% 92.5% 28% 85.2%
Explicit RE 60% 94.3% 45% 88.4%
Implicit RE 21% 83.4% 28% 72.2%
Explicit Locational RE 15% 75.2% 24% 86.2%
Implicit Locational RE 3% 56.6% 2% 56.7%
Table 6: Distribution of referring expressions
(RE) in the media (large screen like tv) and places
(handheld device like phone) corpus and the FIS
accuracies per RE type.
Utterance Displayed on screen
?the most rated restaurant? FFF?s next to each item
?first thomas crown affair? original release (vs. remake)
?second one over? (incomplete row/col. information)
Table 7: Display screen as user utters.
expressions. We also confirm that the handheld
(places domain) users implicitly refer to the items
more commonly compared to media app, and use
the contextual information about the items such as
their location, address, star-rating, etc. The mod-
els are considerably better at resolving explicit re-
ferrals (both non-spatial and spatial) compared to
implicit ones. However, for locational referrals,
the difference between the accuracy of implicit
and explicit REs is significant (75.2% vs. 56.6%
in media and 86.2% vs. 56.7% in places). Al-
though not very common, we observe negative ex-
pressions, e.g., ?the one with no reviews?, which
are harder for the FIS to resolve. They require
quantifying over every other item on the screen,
namely the context features, which we leave as a
future work.
6.3 New Domains and Device Independence
In the series of experiments below, we empirically
investigate the FIS model?s robustness to when a
new domain or device is introduced.
Robustness to New Domains: So far we
trained media domain FIS models on utterances
from all domains. To investigate how FIS models
would behave when tested on a new domain, we
train additional models by leaving out utterances
from one domain and test on the left out domain.
We used GBDT with all the feature sets. To set
up an upper bound, we also train models on each
individual domain and test on the same domain.
Table 8 shows the performance of the FIS mod-
2101
Models tested on:
Model trained on: Movies TV Music Books
All domains 96.2% 95.2% 90.3% 94.6%
All other domains 94.6% 92.4% 89.7% %
Only *this domain 96.4% 96.8% 93.4% %
Table 8: Accuracy of FIS models tested on domains that
are: seen at training time (all domains), unseen at training
time (all other domains) and trained on individual domains.
0 50 100
93
94
95
96
20%
% of Movies instances in training data
0 50 100
94
95
96
10%
% of TV instances in training data
0 50 100
89
90
15%
% of Music instances in training data
Figure 4: The accuracy (y-axis) versus the percentage (%)
of in-domain utterances used in the training dataset. The
dashed vertical line indicates an optimum threshold for the
amount of in-domain data to be added to the training data.
els in accuracy on each media test domain. The
first row shows the results when all domains are
used at training time (same as in Table 5). The
second row represents models where one domain
is unseen at training time. We notice that the ac-
curacy, although degraded for movies and tv do-
mains, is in general not significantly effected by
the domain variations. We setup another experi-
ment, where we incrementally add utterances from
the domain that we are testing the model on. For
instance, we incrementally add random samples
from movies training utterances on the dataset that
does not contain movies utterances and test on all
movies test data. The charts in Fig. 4 show the
% improvement in accuracy as in-domain data is
incrementally added to the training dataset. The
results are interesting in that, using as low as 10-
20% in-domain data is sufficient to build a flexi-
ble item selection model given enough utterances
from other domains with varying REs.
Robustness to a New Device: The difference
between the vocabulary and language usage ob-
served in the data collected from the two devices
Media
?only the new movies? ; ?second one on the left?
?show me the thriller song?; ?by Lewis Milestone?
?the first harry potter book?
Places
?directions to Les Schwab tire center?
?the closest one? ;?show me a map of ...?
?get hours of Peking restaurant?; ?call Mike?s burgers?
Table 9: Sample of utterances collected from media and
places applications illustrating the differences in language us-
age.
Trained on Tested on Media Tested on Places
Media 93.7 % 85.9%
Places 85.9% 86.3%
Media+Places 92.7% 85.8%
Table 10: Accuracy of FIS models tested on two separate
devices (large screen media, and small screen places) that are
unseen at test time.
is mainly due to changes in: (i) the screen design
(places on phone has one column format wheres
the media app has multi-column layout); (ii) the
domain of the data. Table 9 shows some exam-
ples. Here, we add a little bit of complexity, and
train one FIS model using the training data col-
lected on one device and test the model on a dif-
ferent one, which is unseen at training time. Table
10 shows the comparisons for media and phone
interfaces. The results are interesting. The perfor-
mance of the places domain on phone does not get
affected when the models are trained on the media
data and tested on the phone device (86.3% down
to 85.9% which is statistically insignificant). But
when the data is trained on the places and tested
on the media, we see a rather larger degradation
on the performance (93.7% down to 85.9%). This
is due to the fact that the media display screens
are much complicated compared to phone result-
ing in a larger vocabulary with more variation in
REs compared to places domain.
6.4 Conclusion
We presented a framework for identifying and rec-
ognizing referring expressions in user utterances
of human-machine conversations in natural user
interfaces. We use several on-screen cues to in-
terpret whether the user is referring to on-screen
items, and if so, which item is being referred to.
We investigate the effect of different set of fea-
tures on the FIS models performance. We also
show that our model is domain and device inde-
pendent which is very beneficial when new do-
2102
mains are added to the application to cover more
scenarios or when FIS is implemented on new de-
vices. As a future work, we would like to adapt our
model for different languages and include other
features from multi modality including gesture or
geo-location.
References
Dimitr Anastasiou and Cui Jian and Desislava
Zhaekova. 2012. Speech and gesture interaction in
an ambient assited living lab. In. Proc. of the 1st
Workshop on Speech and Multimodal Interaction in
Assitive Environments at ACL?2012.
Rajesh Balchandran, and Mark E. Epstein, and Gerasi-
mos Potamianos, and Lsadislav Seredi. 2008. A
multi-modal spoken dialog system for interactive tv.
In. Proc. of the 10th International Conference on
Multimodal Interfaces.
Christopher M. Bishop. 1995. Neural networks for
Pattern recognition.
Kurt Bollacker and Colin Evans and Praveen Paritosh
and Ttim Sturge and Jamie Taylor. 2008. Free-
base: A collaboratively created graph database for
structuring human knowledge. In. Proc. of the 2008
International Conference on Management of Data
(SIGMOD-08).
Richard A. Bolt. 1980. Put-that-there: Voice and ges-
ture at the graphics interface. Computer Graphics.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with amazons me-
chanical turk. In. Proc. of NAACL.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
Herbert H. Clark and Deanna Wilkes-Gibbs. Referring
as colloborative processes.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20.
Robert Dale and Jette Viethen. 2009. Referring ex-
pression generation through attribute-based heuris-
tics. In Proc. of the 12th European Workshop on
Natural Language Generation (ENLG).
Jerome H. Friedman. 2001. Greedy function approx-
imation: A gradient boosting machine. Annals of
Statistics, 2001.
Kotaro Funakoshi, Mikio Nakano, Takenobu Toku-
naga, and Ryu Iida. 2012. A unified probabilis-
tic approach to referring expressions. In Proc. of
the Special Interest Group on Discourse and Dialog
(SIGDIAL).
Petra Gieselmann. 2004. Reference resolution mecha-
nisms in dialogue management. In Proc. of the 8th
Workshop on the semantics and pragmatics of dia-
logues (CATALOG).
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 410?419, Cambridge, MA, October.
Association for Computational Linguistics.
Joakim Gustafson, Linda Bell, Jonas Beskow, Johan
Boye, Rolf Carlson, Jens Edlund, Bjorn Granstrom,
David House, and Mats Wiren. 2000. Adapt -
a multimodal conversational dialogue system in an
apartment domain. In Proc. of the 6th International
Conference on Spoken Language Processing (IC-
SLP), pages 134?137.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2009. The Elements of Statistical Learning
(2nd ed.) Chapter 10. Boosting and Additive Trees,
2009.
Larry Heck, Dilek Hakkani-Tur, Madhu Chinthakunta,
Gokhan Tur, Rukmini Iyer, Partha Parthasarathy,
Lisa Stifelman, Elizabeth Shriberg, and Ashley Fi-
dler. 2013. Multi-modal conversational search and
browse. In Proc. of the IEEE Workshop on Speech,
Language and Audio in Multimedia.
Dvaid Huggins-Daines and Alexander I. Rudnicky.
2008. Interactive asr error correction for touch-
screen devices. In Proc. of ACL, Demo session.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive referring expression generation in spoken
dialog systems: Evaluation with real users. In Proc.
of SIGDIAL 2010: the 11th Annual Meeting of the
Special Interest Group on Discourse and Dialogue.
Mark Johnston, Srinivas Bangalore, Gunaranjan
Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn
Walker, and Steve Whittaker and Preetam Maloor.
2002. Match: an architecture for multimodal dialog
systems. In. Proc. of the ACL.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing.
David B. Koons, Carlton J. Sparrell, and Kristinn R.
Thorisson. 1993. Integrating simultaneous input
from speech, gaze and hand gestures. In Proc. of
the In Maybury, M. (Ed.), Intelligent Multimedia In-
terfaces, pages 257?276.
John Lafferty, Andrew McCallum, and Fernando C.N.
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. ICML.
Vlademir Levenshtein. 1965. Binary codes capa-
ble of correcting deletions, insertions and rever-
sals. In Proc. of the Doklady Akademii Nauk SSSR,
163:845?848.
2103
Teruhisa Misu, Antoine Raux, Rakesh Gupta, and Ian
Lane. 2014. Situated language understanding at 25
miles per hour. In. Proc. of the SIGDIAL - Annual
Meeting on Discourse and Dialogue.
Renato De Mori, Frederic Bechet, Dilek Hakkani-Tur,
Michael McTear, Giuseppe Riccardi, and Gokhan
Tur. 2008. Spoken language understanding: A sur-
vey. IEEE Signal Processing Magazine, 25:50?58.
Joseph G. Neal. 1991. Intelligent multimedia inter-
face technology. In Proc. of the Intelligent User In-
terfaces: In Sullivan, J., and Tyler, S. (Eds.), pages
45?68.
Sharon Oviatt, Antonella DeAngeli, and Karen Khun.
1997. Integration and synchronization of input
modes during multimodal human-computer interac-
tion. In Proc. of the Human Factors in Computing
Systems: CHI, pages 415?422.
Nobert Pfleger and Jan Alexandersson. 2006. To-
wards resolving referring expressions by implicitly
activated referents in practical dialog systems. In.
Proc. of the 10th Workshop on the Semantics and
Pragmatics of Dialog (SemDial-10).
Michael F. Schober and Herbert H. Clark. 1989. Un-
derstanding by addressees and overhearers. In Proc.
of the Cognitive Psychology, pages 211?232.
Vlademrr Vapnik. 1995. The nature of statistical
learning theory.
2104
Proceedings of NAACL-HLT 2013, pages 416?425,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning to Relate Literal and Sentimental Descriptions of Visual Properties
Mark Yatskar
Computer Science & Engineering
University of Washington
Seattle, WA
my89@cs.washington.edu
Svitlana Volkova
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Asli Celikyilmaz
Conversational Understanding Sciences
Microsoft
Mountain View, CA
asli@ieee.org
Bill Dolan
NLP Group
Microsoft Research
Redmond, WA
billdol@microsoft.edu
Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
Language can describe our visual world at
many levels, including not only what is lit-
erally there but also the sentiment that it in-
vokes. In this paper, we study visual language,
both literal and sentimental, that describes the
overall appearance and style of virtual char-
acters. Sentimental properties, including la-
bels such as ?youthful? or ?country western,?
must be inferred from descriptions of the more
literal properties, such as facial features and
clothing selection. We present a new dataset,
collected to describe Xbox avatars, as well as
models for learning the relationships between
these avatars and their literal and sentimen-
tal descriptions. In a series of experiments,
we demonstrate that such learned models can
be used for a range of tasks, including pre-
dicting sentimental words and using them to
rank and build avatars. Together, these re-
sults demonstrate that sentimental language
provides a concise (though noisy) means of
specifying low-level visual properties.
1 Introduction
Language can describe varied aspects of our visual
world, including not only what is literally there but
also the social, cultural, and emotional sentiment it
invokes. Recently, there has been a growing effort
to study literal language that describes directly ob-
servable properties, such as object color, shape, or
This is a light tan young man
with short and trim haircut. He
has straight eyebrows and large
brown eyes. He has a neat and
trim appearance.
State of mind: angry, upset,
determined. Likes: country
western, rodeo. Occupation:
cowboy, wrangler, horse trainer.
Overall: youthful, cowboy.
Figure 1: (A) Literal avatar descriptions and (B) sen-
timental descriptions of four avatar properties, in-
cluding possible occupations and interests.
category (Farhadi et al, 2009; Mitchell et al, 2010;
Matuszek et al, 2012). Here, we add a focus on
sentimental visual language, which compactly de-
scribes more subjective properties such as if a person
looks determined, if a resume looks professional, or
if a restaurant looks romantic. Such models enable
many new applications, such as text editors that au-
tomatically select properties including font, color, or
text alignment to best match high level descriptions
such as ?professional? or ?artistic.?
416
In this paper, we study visual language, both lit-
eral and sentimental, that describes the overall ap-
pearance and style of virtual characters, like those in
Figure 1. We use literal language as feature norms, a
tool used for studying semantic information in cog-
nitive science (Mcrae et al, 2005). Literal words,
such ?black? or ?hat,? are annotated for objects to in-
dicate how people perceive visual properties. Such
feature norms provide our gold-standard visual de-
tectors, and allow us to focus on learning to model
sentimental language, such as ?youthful? or ?goth.?
We introduce a new corpus of descriptions of
Xbox avatars created by actual gamers. Each avatar
is specified by 19 attributes, including clothing and
body type, allowing for more than 1020 possibil-
ities. Using Amazon Mechanical Turk,1 we col-
lected literal and sentimental descriptions of com-
plete avatars and many of their component parts,
such as the cowboy hat in Figure 1(B). In all, there
are over 100K descriptions. To demonstrate poten-
tial for learning, we also report an A/B test which
shows that native speakers can use sentimental de-
scriptions to distinguish the labeled avatars from
random distractors. This new data will enable study
of the relationships between the co-occurring literal
and sentimental text in a rich visual setting.2
We describe models for three tasks: (i) classify-
ing when words match avatars, (ii) ranking avatars
given a description, and (iii) constructing avatars to
match a description. Each model includes literal part
descriptions as feature norms, enabling us to learn
which literal and sentinel word pairs best predict
complete avatars.
Experiments demonstrate the potential for jointly
modeling literal and sentimental visual descriptions
on our new dataset. The approach outperforms sev-
eral baselines and learns varied relationships be-
tween the sentimental and literal descriptions. For
example, in one experiment ?nerdy student? is pre-
dictive of an avatar with features indicating its shirt
is ?plaid? and glasses are ?large? and faces that are
not ?bearded.? We also show that individual sen-
timental words can be predicted but that multiple
avatars can match a single sentimental description.
Finally, we use our model to build complete avatars
1www.mturk.com
2Data available at http://homes.cs.washington.
edu/?my89/avatar.
and show that we can accurately predict the senti-
mental terms annotators ascribe to them.
2 Related Work
To the best of our knowledge, our focus on learn-
ing to understand visual sentiment descriptions is
novel. However, visual sentiment has been stud-
ied from other perspectives. Jrgensen (1998) pro-
vides examples which show that visual descriptions
communicate social status and story information in
addition to literal object and properties. Tousch et
al. (2012) draw the distinction between ?of-ness?
(objective and concrete) and ?about-ness? (subjec-
tive and abstract) in image retrieval, and observe
that many image queries are abstract (for example,
images about freedom). Finally, in descriptions of
people undergoing emotional distress, Fussell and
Moss (1998) show that literal descriptions co-occur
frequently with sentimental ones.
There has been significant work on more lit-
eral aspects of grounded language understand-
ing, both visual and non-visual. The Words-
Eye project (Coyne and Sproat, 2001) generates
3D scenes from literal paragraph-length descrip-
tions. Generating literal textual descriptions of vi-
sual scenes has also been studied, including both
captions (Kulkarni et al, 2011; Yang et al, 2011;
Feng and Lapata, 2010) and descriptions (Farhadi
et al, 2010). Furthermore, Chen and Dolan (2011)
collected literal descriptions of videos with the
goal of learning paraphrases while Zitnick and
Parikh (2013) describe a corpus of descriptions for
clip art that supports the discovery of semantic ele-
ments of visual scenes.
There has also been significant recent work on au-
tomatically recovering visual attributes, both abso-
lute (Farhadi et al, 2009) and relative (Kovashka et
al., 2012), a challenge that we avoid having to solve
with our use of feature norms (Mcrae et al, 2005).
Grounded language understanding has also re-
ceived significant attention, where the goal is to
learn to understand situated non-visual language
use. For example, there has been work on learning
to execute instructions (Branavan et al, 2009; Chen
and Mooney, 2011; Artzi and Zettlemoyer, 2013),
provide sports commentary (Chen et al, 2010), un-
derstand high level strategy guides to improve game
417
Figure 2: The number of assets per category and ex-
ample images from the hair, shirt and hat categories.
play (Branavan et al, 2011; Eisenstein et al, 2009),
and understand referring expression (Matuszek et
al., 2012).
Finally, our work is similar in spirit to sentiment
analysis (Pang et al, 2002), emotion detection from
images and speech (Zeng et al, 2009), and metaphor
understanding (Shutova, 2010a; Shutova, 2010b).
However, we focus on more general visual context.
3 Data Collection
We gathered a large number of natural language de-
scriptions from Mechanical Turk (MTurk). They in-
clude: (1) literal descriptions of specific facial fea-
tures, clothing or accessories and (2) high level sub-
jective descriptions of human-generated avatars.3
Literal Descriptions We showed annotators a sin-
gle image of clothing, a facial feature or an acces-
sory and asked them to produce short descriptions.
Figure 2 shows the distribution over object types.
We restricted descriptions to be between 3 and 15
words. In all, we collected 33.2K descriptions and
had on average 7 words per descriptions. The ex-
ample annotations with highlighted overlapping pat-
terns are in Table 1.
Sentimental Descriptions We also collected 1913
gamer-created avatars from the web. The avatars
were filtered to contain only items from the set of
665 for which we gathered literal descriptions. The
gender distribution is 95% male.
3(2) also has phrases describing emotional reactions. We
also collected (3) multilingual literal, (4) relative literal and (5)
comprehensive full-body descriptions. We do not use this data,
but it will be included in the public release.
LITERAL DESCRIPTIONS
full-sleeved executive blue shirt
blue , long-sleeved button-up shirt
mens blue button dress shirt with dark blue stripes
multi-blue striped long-sleeve button-up dress
shirt with cuffs and breast pocket
Table 1: Literal descriptions of shirt in Figure 2.
To gather high level sentimental descriptions, an-
notators were presented with an image of an avatar
and asked to list phrases in response to the follow
different aspects:
- State of mind of the avatar.
- Things the avatar might care about.
- What the avatar might do for a living.
- Overall appearance of the avatar.
6144 unique vocabulary items occurred in these
descriptions, but only 1179 occurred more than 10
times. Figure 1 (B) shows an avatar and its corre-
sponding sentimental descriptions.
Quality Control All annotations in our dataset are
produced by non-expert annotators. We relied on
manual spot checks to limit poor annotations. Over
time, we developed a trusted crowd of annotators
who produced only high quality annotations during
the earliest stage of data collection.
4 Feasibility
Our hypothesis is that sentimental language does not
uniquely identify an avatar, but instead summarizes
or otherwise describes its overall look. In general,
there is a trade off between concise and precise de-
scriptions. For example, given a single word you
might be able to generally describe the overall look
of an avatar, but a long, detailed, literal description
would be required to completely specify their ap-
pearance.
To demonstrate that the sentimental descriptions
we collected are precise enough to be predictive
of appearance, we conducted an experiment that
prompts people to judge when avatars match de-
scriptions. We created an A/B test where we show
English speakers two avatars and one sentimental
description. They were asked to select which avatar
is better matched by the description and how dif-
ficult they felt, on a scale from 1 to 4, it was to
judge. For 100 randomly selected descriptions, we
418
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2  
1
 1.5
 2
 2.5
 3
 3.5
data 
diffic
ulty l
ess t
han X
Kapp
a vs 
Cum
ulativ
e Dif
ficult
y
game
r is m
ajorit
y lab
el kapp
a
portio
n of d
ata
Figure 3: Judged task difficulty versus agreement,
gamer avatar preference, and percentage of data cov-
ered. The difficulty axis is cumulative.
asked 5 raters to compare the gamer avatars to ran-
domly generated ones (where each asset is selected
independently according to a uniform distribution).
Figure 3 shows a plot of Kappa and the percent of
the time a majority of the raters selected the gamer
avatar. The easiest 20% of the data pairs had the
strongest agreement, with kappa=.92, and two thirds
of the data has kappa = .70. While agreement falls
off to .52 for the full data set, the gamer avatar re-
mains the majority judgment 81% of the time.
The fact that random avatars are sometimes pre-
ferred indicates that it can be difficult to judge sen-
timental descriptions. Consider the avatars in Fig-
ure 4. Neither conforms to a clear sentimental de-
scription based on the questions we asked. The
right one is described with conflicting words and
the words describing the left one are very general
(like ?dumb?). This corresponds to our intuition that
while many avatars can be succinctly summarized
with our questions, some would be more easily de-
scribed using literal language.
5 Tasks and Evaluation
We formulate three tasks to study the feasibility of
learning the relationship between sentimental and
literal descriptions. In this section, we first define
the space of possible avatars, followed by the tasks.
Avatars Figure 5 summarizes the notation we will
develop to describe the data. An avatar is defined by
a 19 dimensional vector ~a where each position is an
State of mind:
playful, happy;
Likes: sex
Occupation: hobo
Overall: dumb
State of mind: content, humble, satisfied,
peaceful, relaxed, calm. Likes: fashion,
friends, money, cars, music, education.
Occupation: teacher, singer, actor,
performer, dancer, computer engineer.
Overall: nerdy, cool, smart, comfy,
easygoing, reserved
Figure 4: Avatars rated as difficult.
index into a list of possible items~i. Each dimension
represents a position on the avatar, for example, hat
or nose. Each possible item is called an asset and
is associated with a set of positions it can fill. Most
assets take up exactly one position, while there are
a few cases where assets take multiple positions.4
An avatar ~a is valid if all of its mandatory positions
are filled, and no two assets conflict on a position.
Mandatory positions include hair, eyes, ears, eye-
brows, nose, mouth, chin, shirt, pants, and shoes.
All other positions are optional. We refer to this set
of valid ~a as A. Practically speaking, if an avatar is
not valid, it cannot be reliably rendered graphically.
Each item i is associated with the literal descrip-
tions ~di ? D where D is the set of literal descrip-
tions. Furthermore, every avatar~a is associated a list
of sentimental query words ~q, describing subjective
aspects of an avatar.5
Sentimental Word Prediction We first study in-
dividual words. The word prediction task is to de-
cide whether a given avatar can be described with a
4For example, long sleeve shirts cover up watches, so they
take up both shirt and wristwear positions. Costumes tend to
span many more positions, for example there a suit that takes
up shirt, pants, wristwear and shoes positions.
5We do not distinguish which prompt (e.g., ?state of mind?
or ?occupation?) a word in ~q came from, although the vocabu-
laries are relatively disjoint.
419
Figure 5: Avatars, queries, items, literal descriptions.
particular sentimental word q?. We evaluate perfor-
mance with F-score.
Avatar Ranking We also consider an avatar re-
trieval task, where the goal is to rank the set of
avatars in our data, ?j=1...n ~aj , according to which
one best matches a sentimental description, ~qi. As
an automated evaluation, we report the average per-
centile position assigned to the true ~ai for each ex-
ample. However, in general, many different avatars
can match each ~qi, an interesting phenomena we will
further study with human evaluation.
Avatar Generation Finally, we consider the prob-
lem of generating novel, previously unseen avatars,
by selecting a set of items that best embody some
sentimental description. As with ranking, we aim to
construct the avatar ~ai that matches each sentimen-
tal description ~qi. We evaluate by considering the
item overlap between ~ai and the output avatar ~a?,
discounting for empty positions:6
f =
?| ~a?|
j=1 I( ~a
?
j = ~aij)
max(numparts( ~a?), numparts(~ai))
, (1)
where numparts returns the number of non-empty
avatar positions. The score is a conservative measure
because some items are significantly more visually
salient than others. For instance, shirts and pants oc-
cupy a large portion of the physical realization of the
avatar, while rings are small and virtually unnotice-
able. We additionally perform a human evaluation
in Section 8 to better understand these challenges.
6Optional items are infrequently used. Therefore not pre-
dicting them at all offers a strong baseline. Yet doing this
demonstrates nothing about an algorithm?s ability to predict
items which contribute to the sentimental qualities of an avatar.
6 Methods
We present two different models: one that considers
words in isolation and another that jointly models
the query words. This section defines the models
and how we learn them.
6.1 Independent Sentimental Word Model
The independent word model (S-Independent) as-
sumes that each word independently describes the
avatar. We construct a separate linear model for each
word in the vocabulary.
To train these model, we transform the data to
form a binary classification problem for each word,
where the positive data includes all avatars the word
was seen with, (q, ~ai, 1) for all i and q ? ~qi, and the
rest are negative, (q, ~ai, 0) for all i and q /? ~qi.
We use the following features:
? an indicator feature for the cross product of a
sentiment query word q, a literal description
word w ? D, and the avatar position index j
(for example, q = ?angry? with w = ?pointy?
and j = eyebrows):
I(q ? ~qi, w ? ~daij , j)
? a bias feature for keeping a position empty:
I(q ? ~qi, aij = empty, j)
These features will allow the model to capture
correlations between our feature norms which pro-
vide descriptions of visual attributes, like black, and
sentimental words, like gothic.
420
S-Independent is used for both word prediction
and ranking. For prediction, we train a linear model
using averaged binary perceptron. For ranking, we
try to rank all positive instances above negative in-
stances. We use an averaged structured perceptron
to train the ranker (Collins, 2002). To rank with re-
spect to an entire query ~qi, we sum the scores of each
word q ? ~qi.
6.2 Joint Sentimental Model
The second approach (S-Joint) jointly models the
query words to learn the relationships between lit-
eral and sentimental words with score s:
s(~a|~q,D) =
|~a|?
i=1
|~q|?
j=1
?T f(~ai, ~qj , ~dai)
Where every word in the query has a separate factor
and every position is treated independently subject
to the constraint that ~a is valid. The feature function
f uses the same features as the word independent
model above.
This model is used for ranking and generation.
For ranking, we try to rank the avatar ai for query
qi above all other avatars in the candidate set. For
generation, we try to score ai above all other valid
avatars given the query qi. In both cases, we train
with averaged structured perceptron (Collins, 2002)
on the original data, containing query, avatar pairs
(~qi, ~ai).
7 Experimental Setup
Random Baseline For the ranking and avatar gen-
eration tasks, we report random baselines. For rank-
ing, we randomly order the avatars. In the genera-
tion case, we select an item randomly for every posi-
tion. This baseline does not generate optional assets
because they are rare in the real data.
Sentimental-Literal Overlap (SL-Overlap) We
also report a baseline that measures the overlap be-
tween words in the sentiment query ~qi and words in
the literal asset descriptions D. In generation, for
each position in the avatar, ~ai, SL-Overlap selects
the item whose literal description has the most words
in common with ~qi. If no item had overlap with the
query, we backoff to a random choice. In the case of
ranking, it orders avatars by the sum over every po-
sition of the number of words in common between
Word F-Score Precision Recall N
happi 0.84 0.89 0.78 149
student 0.78 0.82 0.74 129
friend 0.76 0.84 0.70 153
music 0.74 0.89 0.63 148
confid 0.74 0.82 0.76 157
sport 0.69 0.62 0.76 76
casual 0.63 0.6 0.67 84
youth 0.6 0.57 0.64 88
waitress 0.59 0.42 1 5
smart 0.57 0.54 0.6 88
fashion 0.54 0.54 0.54 70
monei 0.54 0.52 0.56 76
cool 0.54 0.52 0.56 84
relax 0.53 0.52 0.56 90
game 0.51 0.44 0.62 61
musician 0.51 0.44 0.61 66
parti 0.51 0.43 0.62 58
content 0.5 0.47 0.53 75
friendli 0.49 0.42 0.6 56
smooth 0.49 0.4 0.63 57
Table 2: Top 20 words (stemmed) for classification.
N is the number of occurances in the test set.
the literal description and the query, ~qi. This base-
line tests the degree to which literal and sentimental
descriptions overlap lexically.
Feature Generation For all models that use lexi-
cal features, we limited the number of words. 6144
unique vocabulary items occur in the query set, and
3524 in the literal description set. There are over
400 million entries in the full set of features that in-
clude the cross product of these sets with all possible
avatar positions, as described in Section 6. Since this
would present a challenge for learning, we prune in
two ways. We stem all words with a Porter stemmer.
We also filter out all features which do not occur at
least 10 times in our training set. The final model
has approximately 700k features.
8 Results
We present results for the tasks described in Sec-
tion 5 with the appropriate models from Section 6.
8.1 Word Prediction Results
The goal of our first experiment is to study when
individual sentiment words can be accurately pre-
dicted. We computed sentimental word classifica-
tion accuracy for 1179 word classes with 10 or more
421
Algorithm Percentile Rank
S-joint 77.3
S-independant 73.5
SL-overlap 60.4
Random 48.8
Table 3: Automatic evaluation of ranking. The aver-
age percentile that a test avatar was ranked given its
sentimental description.
mentions. Table 2 shows the top 20 words ordered
by F-score.7 Many common words can be predicted
with relatively high accuracy. Words with strong
individual cues like happy (a smiling mouth), and
confidence (wide eyes) and nerdi (particular glasses)
can be predicted well.
The average F-score among all words was .085.
33.2% of words have an F-score of zero. These zeros
include words like: unusual, bland, sarcastic, trust,
prepared, limber, healthy and poetry. Some of these
words indicate broad classes of avatars (e.g., unusual
avatars) and others indicate subtle modifications to
looks that without other words are not specific (e.g.,
a prepared surfer vs. a prepared business man). Fur-
thermore, evaluation was done assuming that when
a word is not mentioned, it is should be predicted as
negative. This fails to account for the fact that peo-
ple do not mention everything that?s true, but instead
make choices about what to mention based on the
most relevant qualities. Despite these difficulties,
the classification performance shows that we can ac-
curately capture usage patterns for many words.
8.2 Ranking Results
Ranking allows us to test the hypothesis that multi-
ple avatars are valid for a high level description. Fur-
thermore, we consider the differences between S-
Joint and S-Independent, showing that jointly mod-
elings all words improves ranking performance.
Automatic Evaluation The results are shown in
Table 3. Both S-Independent and S-Joint outperform
the SL-overlap baseline. SL-Overlap?s poor perfor-
mance can be attributed to low direct overlap be-
tween sentimental words and literal words. S-Joint
also outperforms the S-Independent.
7Accuracy numbers are inappropriate in this case because
the number of negative instances, in most cases, is far larger
than the number of positive ones.
Inspection of the parameters shows that S-Joint
does better than S-Independent in modeling words
that only relate to a subset of body positions. For
example, in one case we found that for the word
?puzzled? nearly 50% of the weights were on fea-
tures that related to eyebrows and eyes. This type
of specialization was far more pronounced for S-
Joint. The joint nature of the learning allows the fea-
tures for individual words to specialize for specific
positions. In contrast, S-Independent must indepen-
dently predict all parts for every word.
Human Evaluation We report human relevancy
judgments for the top-5 returned results from S-
Joint. On average, 56.2% were marked to be rele-
vant. This shows that S-Joint is performing better
than automatic numbers would indicate, confirming
our intuition that there is a one-to-many relationship
between a sentimental description and avatars. Sen-
timental descriptions, while having significant sig-
nal, are not exact. These results also indicate that
relying on automatic measures of accuracy that as-
sume a single reference avatar underestimates per-
formance. Figure 6 shows the top ranked results
returned by S-Joint for a sentimental description
where the model performs well.
8.3 Generation Results
Finally we evaluate three models for avatar genera-
tion: Random, SL-Overlap and S-Joint using auto-
matic measures and human evaluation.
Automatic Evaluation Table 4 presents results
for automatic evaluation. The Random baseline per-
forms badly, on average assigning items correctly to
less than 1 position in the generated avatar. The SL-
Overlap baseline improves, but still performs quite
poorly. The S-Joint model performs significantly
better, correctly guessing 2-3 items for each output
avatar. However, as we will see in the manual eval-
uation, many of the non-matching parts it produces
are still a good fit for the query.
Human Evaluation As before, there are many
reasonable avatars that could match as well as the
reference avatars. Therefore, we also evaluated gen-
eration with A/B tests, much like in Section 4. An-
notators were asked to judge which of two avatars
better matched a sentimental description. They
422
pensive,confrontational; music,socializing; musician,bar tending,club owner; smart,cool.
Figure 6: A sentimental description paired with the highest ranked avatars found by S-Joint.
Model Overlap
Random 0.041
SL-Overlap 0.049
S-Joint 0.126
Table 4: Automatic generation evaluation results.
The item overlap metric is defined in Section 5.
Kappa Majority Random Sys.
SL-Overlap 0.20 0.25 0.34 0.32
S-Joint 0.52 0.90 0.07 0.81
Gamer 0.52 0.81 0.08 0.77
Table 5: Human evaluation of automatically gener-
ated avatars. Majority represents the percentage of
time the system output is preferred by a majority of
raters. Random and System (Sys.) indicate the per-
centage of time each was preferred.
could rate System A or System B as better, or re-
port that they were equal or that neither matched
the description. We consider two comparisons: SL-
Overlap vs. Random and S-Joint vs Random. Five
annotators performed each condition, rating 100 ex-
amples with randomly ordered avatars.
We report the results for human evaluation includ-
ing kappa, majority judgments, and a distribution
over judgments in Table 5. The SL-Overlap baseline
is indistinguishable from a random avatar. This con-
trasts with the ranking case, where this simple base-
line showed improvement, indicating that generation
is a much harder problem. Furthermore, agreement
is low; people felt the need to make a choice but
were not consistent.
We also see in Table 5 that people prefer the S-
Joint model outputs to random avatars as often as
they prefer gamer to random. While this does not
necessarily imply that S-Joint creates gamer-quality
avatars, it indicates substantial progress by learning
a mapping between literal and sentimental words.
Qualitative Results Table 6 presents the highest
and lowest weighted features for different sentimen-
tal query words. Figure 7 shows four descriptions
that were assigned high quality avatars.
In general, many of the weaker avatars had as-
pects of the descriptions but lacked such distinctive
overall looks. This was especially true when the
descriptions contained seemingly contradictory in-
formation. For example, one avatar was described
as being both nerdy and popular. We generated a
look that had aspects of both of these descriptions,
including a head that contained both conservative el-
ements (like glasses) and less conservative elements
(like crazy hair and earrings). However, the combi-
nation would not be described as nerdy or popular,
because of difficult to predict global interactions be-
tween the co-occurring words and items. This is an
important area for future work.
9 Conclusions
We explored how visual language, both literal and
sentimental, maps to the overall physical appearance
and style of virtual characters. While this paper fo-
cused on avatar design, our approach has implica-
tions for a broad class of natural language-driven
423
Ambition; business,
fashion, success;
salesman; smooth,
professional.
Capable, confident, firm; heavy metal,
extreme sports, motorcycles; engineer,
mechanic, machinist; aggressive,
strong, protective.
Stressed, bored,
discontent; emo music;
works at a record store;
goth, dark, drab.
Happy, content, confident,
home, career, family,
secretary,student,
classy,clean,casual
Figure 7: Avatars automatically generated with the S-Joint model.
Sentiment positive features negative features
happi mouth:thick, mouth:smilei, mouth:make, mouth:open mouth:tight, mouth:emotionless, mouth:brownish, mouth:attract
gothic shoes:brown, shirt:black, pants:hot, shirt:band shirt:half, shirt:tight, pants:sexi, hair:brownish
retro eyebrows:men, eyebrows:large, hair:round, pants:light eyebrows:beauti, pants:side; eyebrows:trim, pants:cut
beach pants:yello, pants:half, nose:narrow, pants:white shirt:brown, shirt:side; shoes:long, pants:jean
Table 6: Most positive and negative features for a word stem. A feature is [position]:[literal word].
dialog scenarios. In many situations, a user may
be perfectly able to formulate a high-level descrip-
tion of their intent (?Make my resume look cleaner?
?Buy me clothes for a summer wedding,? or ?Play
something more danceable?) while having little or
no understanding of the complex parameter space
that the underlying software must manipulate in or-
der to achieve this result.
We demonstrated that these high-level sentimen-
tal specifications can have a strong relationship to
literal aspects of a problem space and showed that
sentimental language is a concise, yet noisy, way
of specifying high level characteristics. Sentimen-
tal language is an unexplored avenue for improving
natural language systems that operate in situated set-
tings. It has the potential to bridge the gap between
lay and expert understandings of a problem domain.
Acknowledgments
This work is partially supported by the DARPA
CSSG (N11AP20020) and the NSF (IIS-1115966).
The authors would like to thank Chris Brockett,
Noelle Sophy, Rico Malvar for helping with collect-
ing and processing the data. We would also like
to thank Tom Kwiatkowski and Nicholas FitzGer-
ald and the anonymous reviewers for their helpful
comments.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1(1):49?62.
SRK Branavan, H. Chen, L.S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 82?90.
SRK Branavan, David Silver, and Regina Barzilay. 2011.
Learning to win by reading manuals in a monte-carlo
framework. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 268?
277.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 190?200.
D.L. Chen and R.J. Mooney. 2011. Learning to interpret
natural language navigation instructions from observa-
424
tions. In Proceedings of the 25th AAAI Conference on
Artificial Intelligence (AAAI-2011), pages 859?865.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal of Artificial
Intelligence Research, 37:397?435.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing, pages 1?8.
B. Coyne and R. Sproat. 2001. Wordseye: an automatic
text-to-scene conversion system. In Proceedings of the
28th annual conference on Computer graphics and in-
teractive techniques, pages 487?496.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 958?967.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
In Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences from images. In Proceedings of
the 11th European conference on Computer Vision,
ECCV?10, pages 15?29.
Yansong Feng and Mirella Lapata. 2010. Topic models
for image annotation and text illustration. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 831?839.
Susan R Fussell and Mallie M Moss. 1998. Figura-
tive language in emotional communication. Social and
cognitive approaches to interpersonal communication,
page 113.
Corinne Jrgensen. 1998. Attributes of images in describ-
ing tasks. Information Processing & Management,
34(23):161 ? 174.
Adriana Kovashka, Devi Parikh, and Kristen Grauman.
2012. Whittlesearch: Image search with relative at-
tribute feedback. In Computer Vision and Pattern
Recognition (CVPR), pages 2973?2980.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understanding
and generating simple image descriptions. In Com-
puter Vision and Pattern Recognition (CVPR), pages
1601?1608.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A Joint
Model of Language and Perception for Grounded At-
tribute Learning. In Proc. of the 2012 International
Conference on Machine Learning.
Ken Mcrae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547?559.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2010. Natural reference to objects in a visual domain.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference, INLG ?10, pages 95?
104.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 79?86.
Ekaterina Shutova. 2010a. Automatic metaphor inter-
pretation as a paraphrasing task. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 1029?1037.
Ekaterina Shutova. 2010b. Models of metaphor in nlp.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL ?10, pages
688?697.
Anne-Marie Tousch, Stphane Herbin, and Jean-Yves Au-
dibert. 2012. Semantic hierarchies for image annota-
tion: A survey. Pattern Recognition, 45(1):333 ? 345.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009.
A survey of affect recognition methods: Audio, vi-
sual, and spontaneous expressions. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
31(1):39?58.
C Lawrence Zitnick and Devi Parikh. 2013. Bringing
semantics into focus using visual abstraction. In Com-
puter Vision and Pattern Recognition (To Appear).
425
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 815?824,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Hybrid Hierarchical Model for Multi-Document Summarization
Asli Celikyilmaz
Computer Science Department
University of California, Berkeley
asli@eecs.berkeley.edu
Dilek Hakkani-Tur
International Computer Science Institute
Berkeley, CA
dilek@icsi.berkeley.edu
Abstract
Scoring sentences in documents given ab-
stract summaries created by humans is im-
portant in extractive multi-document sum-
marization. In this paper, we formulate ex-
tractive summarization as a two step learn-
ing problem building a generative model
for pattern discovery and a regression
model for inference. We calculate scores
for sentences in document clusters based
on their latent characteristics using a hi-
erarchical topic model. Then, using these
scores, we train a regression model based
on the lexical and structural characteris-
tics of the sentences, and use the model to
score sentences of new documents to form
a summary. Our system advances current
state-of-the-art improving ROUGE scores
by ?7%. Generated summaries are less
redundant and more coherent based upon
manual quality evaluations.
1 Introduction
Extractive approach to multi-document summa-
rization (MDS) produces a summary by select-
ing sentences from original documents. Doc-
ument Understanding Conferences (DUC), now
TAC, fosters the effort on building MDS systems,
which take document clusters (documents on a
same topic) and description of the desired sum-
mary focus as input and output a word length lim-
ited summary. Human summaries are provided for
training summarization models and measuring the
performance of machine generated summaries.
Extractive summarization methods can be clas-
sified into two groups: supervised methods that
rely on provided document-summary pairs, and
unsupervised methods based upon properties de-
rived from document clusters. Supervised meth-
ods treat the summarization task as a classifica-
tion/regression problem, e.g., (Shen et al, 2007;
Yeh et al, 2005). Each candidate sentence is
classified as summary or non-summary based on
the features that they pose and those with high-
est scores are selected. Unsupervised methods
aim to score sentences based on semantic group-
ings extracted from documents, e.g., (Daume?III
and Marcu, 2006; Titov and McDonald, 2008;
Tang et al, 2009; Haghighi and Vanderwende,
2009; Radev et al, 2004; Branavan et al, 2009),
etc. Such models can yield comparable or bet-
ter performance on DUC and other evaluations,
since representing documents as topic distribu-
tions rather than bags of words diminishes the ef-
fect of lexical variability. To the best of our knowl-
edge, there is no previous research which utilizes
the best features of both approaches for MDS as
presented in this paper.
In this paper, we present a novel approach that
formulates MDS as a prediction problem based
on a two-step hybrid model: a generative model
for hierarchical topic discovery and a regression
model for inference. We investigate if a hierarchi-
cal model can be adopted to discover salient char-
acteristics of sentences organized into hierarchies
utilizing human generated summary text.
We present a probabilistic topic model on sen-
tence level building on hierarchical Latent Dirich-
let Allocation (hLDA) (Blei et al, 2003a), which
is a generalization of LDA (Blei et al, 2003b). We
construct a hybrid learning algorithm by extract-
ing salient features to characterize summary sen-
tences, and implement a regression model for in-
ference (Fig.3). Contributions of this work are:
? construction of hierarchical probabilistic model
designed to discover the topic structures of all sen-
tences. Our focus is on identifying similarities of
candidate sentences to summary sentences using a
novel tree based sentence scoring algorithm, con-
cerning topic distributions at different levels of the
discovered hierarchy as described in ? 3 and ? 4,
? representation of sentences by meta-features to
815
characterize their candidacy for inclusion in sum-
mary text. Our aim is to find features that can best
represent summary sentences as described in ? 5,
? implementation of a feasible inference method
based on a regression model to enable scoring of
sentences in test document clusters without re-
training, (which has not been investigated in gen-
erative summarization models) described in ? 5.2.
We show in ? 6 that our hybrid summarizer
achieves comparable (if not better) ROUGE score
on the challenging task of extracting the sum-
maries of multiple newswire documents. The hu-
man evaluations confirm that our hybrid model can
produce coherent and non-redundant summaries.
2 Background and Motivation
There are many studies on the principles govern-
ing multi-document summarization to produce co-
herent and semantically relevant summaries. Pre-
vious work (Nenkova and Vanderwende, 2005;
Conroy et al, 2006), focused on the fact that fre-
quency of words plays an important factor. While,
earlier work on summarization depend on a word
score function, which is used to measure sentence
rank scores based on (semi-)supervised learn-
ing methods, recent trend of purely data-driven
methods, (Barzilay and Lee, 2004; Daume?III and
Marcu, 2006; Tang et al, 2009; Haghighi and
Vanderwende, 2009), have shown remarkable im-
provements. Our work builds on both methods by
constructing a hybrid approach to summarization.
Our objective is to discover from document
clusters, the latent topics that are organized into hi-
erarchies following (Haghighi and Vanderwende,
2009). A hierarchical model is particularly ap-
pealing to summarization than a ?flat? model, e.g.
LDA (Blei et al, 2003b), in that one can discover
?abstract? and ?specific? topics. For instance, dis-
covering that ?baseball? and ?football? are both
contained in an abstract class ?sports? can help to
identify summary sentences. It follows that sum-
mary topics are commonly shared by many docu-
ments, while specific topics are more likely to be
mentioned in rather a small subset of documents.
Feature based learning approaches to summa-
rization methods discover salient features by mea-
suring similarity between candidate sentences and
summary sentences (Nenkova and Vanderwende,
2005; Conroy et al, 2006). While such methods
are effective in extractive summarization, the fact
that some of these methods are based on greedy
algorithms can limit the application areas. More-
over, using information on the hidden semantic
structure of document clusters would improve the
performance of these methods.
Recent studies focused on the discovery of la-
tent topics of document sets in extracting sum-
maries. In these models, the challenges of infer-
ring topics of test documents are not addressed
in detail. One of the challenges of using a pre-
viously trained topic model is that the new docu-
ment might have a totally new vocabulary or may
include many other specific topics, which may or
may not exist in the trained model. A common
method is to re-build a topic model for new sets
of documents (Haghighi and Vanderwende, 2009),
which has proven to produce coherent summaries.
An alternative yet feasible solution, presented in
this work, is building a model that can summa-
rize new document clusters using characteristics
of topic distributions of training documents. Our
approach differs from the early work, in that, we
combine a generative hierarchical model and re-
gression model to score sentences in new docu-
ments, eliminating the need for building a genera-
tive model for new document clusters.
3 Summary-Focused Hierarchical Model
Our MDS system, hybrid hierarchical summa-
rizer, HybHSum, is based on an hybrid learn-
ing approach to extract sentences for generating
summary. We discover hidden topic distributions
of sentences in a given document cluster along
with provided summary sentences based on hLDA
described in (Blei et al, 2003a)1. We build a
summary-focused hierarchical probabilistic topic
model, sumHLDA, for each document cluster at
sentence level, because it enables capturing ex-
pected topic distributions in given sentences di-
rectly from the model. Besides, document clusters
contain a relatively small number of documents,
which may limit the variability of topics if they are
evaluated on the document level. As described in ?
4, we present a new method for scoring candidate
sentences from this hierarchical structure.
Let a given document cluster D be represented
with sentences O={om}|O|m=1 and its corresponding
human summary be represented with sentences
S={sn}|S|n=1. All sentences are comprised of words
V =
{
w1, w2, ..w|V |
}
in {O ? S}.
1Please refer to (Blei et al, 2003b) and (Blei et al, 2003a)
for details and demonstrations of topic models.
816
Summary hLDA (sumHLDA): The hLDA
represents distribution of topics in sentences by
organizing topics into a tree of a fixed depth L
(Fig.1.a). Each candidate sentence om is assigned
to a path com in the tree and each word wi in a
given sentence is assigned to a hidden topic zom
at a level l of com . Each node is associated with a
topic distribution over words. The sampler method
alternates between choosing a new path for each
sentence through the tree and assigning each word
in each sentence to a topic along that path. The
structure of tree is learnt along with the topics us-
ing a nested Chinese restaurant process (nCRP)
(Blei et al, 2003a), which is used as a prior.
The nCRP is a stochastic process, which as-
signs probability distributions to infinitely branch-
ing and infinitely deep trees. In our model, nCRP
specifies a distribution of words into paths in an
L-level tree. The assignments of sentences to
paths are sampled sequentially: The first sentence
takes the initial L-level path, starting with a sin-
gle branch tree. Later, mth subsequent sentence is
assigned to a path drawn from the distribution:
p(pathold, c|m,mc) =
mc
?+m?1
p(pathnew, c|m,mc) =
?
?+m?1
(1)
pathold and pathnew represent an existing and
novel (branch) path consecutively, mc is the num-
ber of previous sentences assigned to path c, m is
the total number of sentences seen so far, and ? is
a hyper-parameter which controls the probability
of creating new paths. Based on this probability
each node can branch out a different number of
child nodes proportional to ?. Small values of ?
suppress the number of branches.
Summary sentences generally comprise abstract
concepts of the content. With sumHLDA we want
to capture these abstract concepts in candidate sen-
tences. The idea is to represent each path shared
by similar candidate sentences with representative
summary sentence(s). We let summary sentences
share existing paths generated by similar candi-
date sentences instead of sampling new paths and
influence the tree structure by introducing two sep-
arate hyper-parameters for nCRP prior:
? if a summary sentence is sampled, use ? = ?s,
? if a candidate sentence is sampled, use ? = ?o.
At each node, we let summary sentences sample
a path by choosing only from the existing children
of that node with a probability proportional to the
number of other sentences assigned to that child.
This can be achieved by using a small value for ?s
(0 < ?s ? 1). We only let candidate sentences
to have an option of creating a new child node
with a probability proportional to ?o. By choos-
ing ?s ? ?o we suppress the generation of new
branches for summary sentences and modify the
? of nCRP prior in Eq.(1) using ?s and ?o hyper-
parameters for different sentence types. In the ex-
periments, we discuss the effects of this modifica-
tion on the hierarchical topic tree.
The following is the generative process for
sumHLDA used in our HybHSum :
(1) For each topic k ? T , sample a distribution
?k v Dirichlet(?).
(2) For each sentence d ? {O ? S},
(a) if d ? O, draw a path cd v nCRP(?o),
else if d ? S, draw a path cd v nCRP(?s).
(b) Sample L-vector ?d mixing weights from
Dirichlet distribution ?d ? Dir(?).
(c) For each word n, choose: (i) level zd,n|?d
and (ii) word wd,n| {zd,n, cd, ?}
Given sentence d, ?d is a vector of topic pro-
portions from L dimensional Dirichlet parameter-
ized by ? (distribution over levels in the tree.) The
nth word of d is sampled by first choosing a level
zd,n = l from the discrete distribution ?d with
probability ?d,l. Dirichlet parameter ? and ?o con-
trol the size of tree effecting the number of topics.
(Small values of ?s do not effect the tree.) Large
values of ? favor more topics (Blei et al, 2003a).
Model Learning: Gibbs sampling is a common
method to fit the hLDA models. The aim is to ob-
tain the following samples from the posterior of:
(i) the latent tree T , (ii) the level assignment z for
all words, (iii) the path assignments c for all sen-
tences conditioned on the observed words w.
Given the assignment of words w to levels z and
assignments of sentences to paths c, the expected
posterior probability of a particular word w at a
given topic z=l of a path c=c is proportional to the
number of times w was generated by that topic:
p(w|z, c,w, ?) ? n(z=l,c=c,w=w) + ? (2)
Similarly, posterior probability of a particular
topic z in a given sentence d is proportional to
number of times z was generated by that sentence:
p(z|z, c, ?) ? n(c=cd,z=l) + ? (3)
n(.) is the count of elements of an array satisfy-
ing the condition. Note from Eq.(3) that two sen-
tences d1 and d2 on the same path c would have
817
different words, and hence different posterior topic
probabilities. Posterior probabilities are normal-
ized with total counts and their hyperparameters.
4 Tree-Based Sentence Scoring
The sumHLDA constructs a hierarchical tree
structure of candidate sentences (per document
cluster) by positioning summary sentences on the
tree. Each sentence is represented by a path in the
tree, and each path can be shared by many sen-
tences. The assumption is that sentences sharing
the same path should be more similar to each other
because they share the same topics. Moreover, if
a path includes a summary sentence, then candi-
date sentences on that path are more likely to be
selected for summary text. In particular, the sim-
ilarity of a candidate sentence om to a summary
sentence sn sharing the same path is a measure
of strength, indicating how likely om is to be in-
cluded in the generated summary (Algorithm 1):
Let com be the path for a given om. We find
summary sentences that share the same path with
om via: M = {sn ? S|csn = com}. The score of
each sentence is calculated by similarity to the best
matching summary sentence in M :
score(om) = maxsn?M sim(om, sn) (4)
If M=?, then score(om)=?. The efficiency of our
similarity measure in identifying the best match-
ing summary sentence, is tied to how expressive
the extracted topics of our sumHLDA models are.
Given path com , we calculate the similarity of om
to each sn, n=1..|M | by measuring similarities on:
? sparse unigram distributions (sim1) at each
topic l on com : similarity between p(wom,l|zom =
l, com , vl) and p(wsn,l|zsn = l, com , vl)
?? distributions of topic proportions (sim2);
similarity between p(zom |com) and p(zsn |com).
? sim1: We define two sparse (discrete) un-
igram distributions for candidate om and sum-
mary sn at each node l on a vocabulary iden-
tified with words generated by the topic at that
node, vl ? V . Given wom =
{
w1, ..., w|om|
}
,
let wom,l ? wom be the set of words in om that
are generated from topic zom at level l on path
com . The discrete unigram distribution poml =
p(wom,l|zom = l, com , vl) represents the probabil-
ity over all words vl assigned to topic zom at level
l, by sampling only for words in wom,l. Similarly,
psn,l = p(wsn,l|zsn , com , vl) is the probability of
words wsn in sn of the same topic. The proba-
bility of each word in pom,l and psn,l are obtained
using Eq. (2) and then normalized (see Fig.1.b).
Algorithm 1 Tree-Based Sentence Scoring
1: Given tree T from sumHLDA, candidate and summary
sentences: O = {o1, ..., om} , S = {s1, ..., sn}
2: for sentences m? 1, ..., |O| do
3: - Find path com on tree T and summary sentences
4: on path com : M = {sn ? S|csn = com}
5: for summary sentences n? 1, ..., |M | do
6: - Find score(om)=maxsn sim(om, sn),
7: where sim(om, sn) = sim1 ? sim2
8: using Eq.(7) and Eq.(8)
9: end for
10: end for
11: Obtain scores Y = {score(om)}
|O|
m=1
The similarity between pom,l and psn,l is
obtained by first calculating the divergence
with information radius- IR based on Kullback-
Liebler(KL) divergence, p=pom,l, q=psn,l :
IRcom ,l(pom,l, psn,l)=KL(p||
p+q
2 )+KL(q||
p+q
2 ) (5)
where, KL(p||q)=Pi pi log
pi
qi
. Then the divergence
is transformed into a similarity measure (Manning
and Schuetze, 1999):
Wcom,l(pom,l, psn,l) = 10
?IRcom,l(pom,l,psn,l)
(6)
IR is a measure of total divergence from the av-
erage, representing how much information is lost
when two distributions p and q are described in
terms of average distributions. We opted for IR
instead of the commonly used KL because with
IR there is no problem with infinite values since
pi+qi
2 6=0 if either pi 6=0 or qi 6=0. Moreover, un-
like KL, IR is symmetric, i.e., KL(p,q) 6=KL(q,p).
Finally sim1 is obtained by average similarity of
sentences using Eq.(6) at each level of com by:
sim1(om, sn) = 1L
?L
l=1 Wcom ,l(pom,l, psn,l) ? l
(7)
The similarity between pom,l and psn,l at each level
is weighted proportional to the level l because the
similarity between sentences should be rewarded
if there is a specific word overlap at child nodes.
?sim2: We introduce another measure based
on sentence-topic mixing proportions to calculate
the concept-based similarities between om and sn.
We calculate the topic proportions of om and sn,
represented by pzom = p(zom |com) and pzsn =
p(zsn |com) via Eq.(3). The similarity between the
distributions is then measured with transformed IR
818
(a) Snapshot of Hierarchical Topic Structure of a 
document cluster on ?global warming?. (Duc06)
z
1  
z
2  
z
3
z
z
1  
z
2  
z
3
z
Posterior Topic 
Distributions
v
z1
z
3
.
.
.
.
.
.
.
.
.
.
w
5
z
2
w
8
.
.
.
.
.
.
.
.
w
2
.
z
1
w
5
.
.
.
.
.
.
.
w
7
w
1
Posterior Topic-Word Distributions
candidate o
m            
summary s
n
(b) Magnified view of sample path c [z
1
,z
2
,z
3
] showing  
o
m
={w
1
,w
2
,w
3
,w
4
,w
5
} and s
n
={w
1
,w
2
,w
6,
w
7
,w
8
}
...
...
z
1
z
K-1
z
K
z
4
z
2
z
3
human
warming
incidence
research
global
predict
health
change
disease
forecast
temperature
slow
malaria
sneeze
starving
middle-east
siberia
o
m
: ?Global
1
 warming
2
 may rise
3
 incidence
4
 of malaria
5
.?
s
n
:?Global
1
 warming
2
 effects
6
 human
7
 health
8
.?
level:3
level:1
level:2
v
z1
v
z2
v
z2
v
z3
v
z3
w
1
w
5
w
6 
w
7
.... 
 
 
w
2
 w
8
 ....  
w
5
  ....  
 
 
w
5
  ....  
w
6
w
1
w
5
w
6 
w
7
.... 
.
 
 
w
2
 w
8
 ....  
.
p
o
m
z
p
s
n
z
p(w 
     
|z
1
, c   )
s
n,1
s
n
p(w 
     
|z
1
, c   )
o
m,1
o
m
p(w 
     
|z
2
, c   )
s
n,2
s
n
p(w 
     
|z
2
, c   )
o
m,2
o
m
p(w 
     
|z
3
, c   )
s
n,3
s
n
p(w 
     
|z
3
, c   )
o
m,3
o
m
Figure 1: (a) A sample 3-level tree using sumHLDA. Each sentence is associated with a path c through the hierarchy, where
each node zl,c is associated with a distribution over terms (Most probable terms are illustrated). (b) magnified view of a path
(darker nodes) in (a). Distribution of words in given two sentences, a candidate (om) and a summary (sn) using sub-vocabulary
of words at each topic vzl . Discrete distributions on the left are topic mixtures for each sentence, pzom and pzsn .
as in Eq.(6) by:
sim2 (om, sn) = 10
?IRcom (pzom ,pzsn ) (8)
sim1 provides information about the similarity
between two sentences, om and sn based on topic-
word distributions. Similarly, sim2 provides in-
formation on the similarity between the weights of
the topics in each sentence. They jointly effect the
sentence score and are combined in one measure:
sim(om, sn) = sim1(om, sn) ? sim2 (om, sn) (9)
The final score for a given om is calculated from
Eq.(4). Fig.1.b depicts a sample path illustrating
sparse unigram distributions of om and sm at each
level as well as their topic proportions, pzom , and
pzsn . In experiment 3, we discuss the effect of our
tree-based scoring on summarization performance
in comparison to a classical scoring method pre-
sented as our baseline model.
5 Regression Model
Each candidate sentence om, m = 1..|O| is rep-
resented with a multi-dimensional vector of q fea-
tures fm = {fm1, ..., fmq}. We build a regression
model using sentence scores as output and selected
salient features as input variables described below:
5.1 Feature Extraction
We compile our training dataset using sentences
from different document clusters, which do not
necessarily share vocabularies. Thus, we create n-
gram meta-features to represent sentences instead
of word n-gram frequencies:
(I) nGram Meta-Features (NMF): For each
document cluster D, we identify most fre-
quent (non-stop word) unigrams, i.e., vfreq =
{wi}
r
i=1 ? V , where r is a model param-
eter of number of most frequent unigram fea-
tures. We measure observed unigram proba-
bilities for each wi ? vfreq with pD(wi) =
nD(wi)/
?|V |
j=1 nD(wj), where nD(wi) is the
number of times wi appears in D and |V | is the
total number of unigrams. For any ith feature, the
value is fmi = 0, if given sentence does not con-
tain wi, otherwise fmi = pD(wi). These features
can be extended for any n-grams. We similarly
include bigram features in the experiments.
(II) Document Word Frequency Meta-
Features (DMF): The characteristics of sentences
at the document level can be important in sum-
mary generation. DMF identify whether a word
in a given sentence is specific to the document
in consideration or it is commonly used in the
document cluster. This is important because
summary sentences usually contain abstract terms
rather than specific terms.
To characterize this feature, we re-use the r
most frequent unigrams, i.e., wi ? vfreq. Given
sentence om, let d be the document that om be-
longs to, i.e., om ? d. We measure unigram prob-
abilities for each wi by p(wi ? om) = nd(wi ?
om)/nD(wi), where nd(wi ? om) is the number
of timeswi appears in d and nD(wi) is the number
of times wi appears in D. For any ith feature, the
value is fmi = 0, if given sentence does not con-
tain wi, otherwise fmi = p(wi ? om). We also
include bigram extensions of DMF features.
819
(III) Other Features (OF): Term frequency of
sentences such as SUMBASIC are proven to be
good predictors in sentence scoring (Nenkova and
Vanderwende, 2005). We measure the average
unigram probability of a sentence by: p(om) =
P
w?om
1
|om|
PD(w), where PD(w) is the observed
unigram probability in the document collection D
and |om| is the total number of words in om. We
use sentence bigram frequency, sentence rank in
a document, and sentence size as additional fea-
tures.
5.2 Predicting Scores for New Sentences
Due to the large feature space to explore, we chose
to work with support vector regression (SVR)
(Drucker et al, 1997) as the learning algorithm
to predict sentence scores. Given training sen-
tences {fm, ym}
|O|
m=1, where fm = {fm1, ..., fmq}
is a multi-dimensional vector of features and
ym=score(om)? R are their scores obtained via
Eq.(4), we train a regression model. In experi-
ments we use non-linear Gaussian kernel for SVR.
Once the SVR model is trained, we use it to predict
the scores of ntest number of sentences in test (un-
seen) document clusters, Otest =
{
o1, ...o|Otest|
}
.
Our HybHSum captures the sentence character-
istics with a regression model using sentences in
different document clusters. At test time, this valu-
able information is used to score testing sentences.
Redundancy Elimination: To eliminate redun-
dant sentences in the generated summary, we in-
crementally add onto the summary the highest
ranked sentence om and check if om significantly
repeats the information already included in the
summary until the algorithm reaches word count
limit. We use a word overlap measure between
sentences normalized to sentence length. A om is
discarded if its similarity to any of the previously
selected sentences is greater than a threshold iden-
tified by a greedy search on the training dataset.
6 Experiments and Discussions
In this section we describe a number of experi-
ments using our hybrid model on 100 document
clusters each containing 25 news articles from
DUC2005-2006 tasks. We evaluate the perfor-
mance of HybHSum using 45 document clusters
each containing 25 news articles from DUC2007
task. From these sets, we collected v80K and
v25K sentences to compile training and testing
data respectively. The task is to create max. 250
word long summary for each document cluster.
We use Gibbs sampling for inference in hLDA
and sumHLDA. The hLDA is used to capture ab-
straction and specificity of words in documents
(Blei et al, 2009). Contrary to typical hLDA mod-
els, to efficiently represent sentences in summa-
rization task, we set ascending values for Dirichlet
hyper-parameter ? as the level increases, encour-
aging mid to low level distributions to generate as
many words as in higher levels, e.g., for a tree of
depth=3, ? = {0.125, 0.5, 1}. This causes sen-
tences share paths only when they include similar
concepts, starting higher level topics of the tree.
For SVR, we set  = 0.1 using the default choice,
which is the inverse of the average of ?(f)T?(f)
(Joachims, 1999), dot product of kernelized input
vectors. We use greedy optimization during train-
ing based on ROUGE scores to find best regular-
izer C =
{
10?1..102
}
using the Gaussian kernel.
We applied feature extraction of ? 5.1 to com-
pile the training and testing datasets. ROUGE
is used for performance measure (Lin and Hovy,
2003; Lin, 2004), which evaluates summaries
based on the maxium number of overlapping units
between generated summary text and a set of hu-
man summaries. We use R-1 (recall against uni-
grams), R-2 (recall against bigrams), and R-SU4
(recall against skip-4 bigrams).
Experiment 1: sumHLDA Parameter Analy-
sis: In sumHLDA we introduce a prior different
than the standard nested CRP (nCRP). Here, we
illustrate that this prior is practical in learning hi-
erarchical topics for summarization task.
We use sentences from the human generated
summaries during the discovery of hierarchical
topics of sentences in document clusters. Since
summary sentences generally contain abstract
words, they are indicative of sentences in docu-
ments and should produce minimal amount of new
topics (if not none). To implement this, in nCRP
prior of sumHLDA, we use dual hyper-parameters
and choose a very small value for summary sen-
tences, ?s = 10e?4  ?o. We compare the re-
sults to hLDA (Blei et al, 2003a) with nCRP prior
which uses only one free parameter, ?. To ana-
lyze this prior, we generate a corpus ofv1300 sen-
tences of a document cluster in DUC2005. We re-
peated the experiment for 9 other clusters of sim-
ilar size and averaged the total number of gener-
ated topics. We show results for different values
of ? and ?o hyper-parameters and tree depths.
820
? = ?o 0.1 1 10
depth 3 5 8 3 5 8 3 5 8
hLDA 3 5 8 41 267 1509 1522 4080 8015
sumHLDA 3 5 8 27 162 671 1207 3598 7050
Table 1: Average # of topics per document cluster from
sumHLDA and hLDA for different ? and ?o and tree depths.
?s = 10e?4 is used for sumHLDA for each depth.
Features Baseline HybHSum
R-1 R-2 R-SU4 R-1 R-2 R-SU4
NMF (1) 40.3 7.8 13.7 41.6 8.4 12.3
DMF (2) 41.3 7.5 14.3 41.3 8.0 13.9
OF (3) 40.3 7.4 13.7 42.4 8.0 14.4
(1+2) 41.5 7.9 14.0 41.8 8.5 14.5
(1+3) 40.8 7.5 13.8 41.6 8.2 14.1
(2+3) 40.7 7.4 13.8 42.7 8.7 14.9
(1+2+3) 41.4 8.1 13.7 43.0 9.1 15.1
Table 2: ROUGE results (with stop-words) on DUC2006
for different features and methods. Results in bold show sta-
tistical significance over baseline in corresponding metric.
As shown in Table 1, the nCRP prior for
sumHLDA is more effective than hLDA prior in
the summarization task. Less number of top-
ics(nodes) in sumHLDA suggests that summary
sentences share pre-existing paths and no new
paths or nodes are sampled for them. We also
observe that using ?o = 0.1 causes the model
to generate minimum number of topics (# of top-
ics=depth), while setting ?o = 10 creates exces-
sive amount of topics. ?0 = 1 gives reasonable
number of topics, thus we use this value for the
rest of the experiments. In experiment 3, we use
both nCRP priors in HybHSum to analyze whether
there is any performance gain with the new prior.
Experiment 2: Feature Selection Analysis
Here we test individual contribution of each set
of features on our HybHSum (using sumHLDA).
We use a Baseline by replacing the scoring algo-
rithm of HybHSum with a simple cosine distance
measure. The score of a candidate sentence is the
cosine similarity to the maximum matching sum-
mary sentence. Later, we build a regression model
with the same features as our HybHSum to create
a summary. We train models with DUC2005 and
evaluate performance on DUC2006 documents for
different parameter values as shown in Table 2.
As presented in ? 5, NMF is the bundle of fre-
quency based meta-features on document cluster
level, DMF is a bundle of frequency based meta-
features on individual document level and OF rep-
resents sentence term frequency, location, and size
features. In comparison to the baseline, OF has a
significant effect on the ROUGE scores. In addi-
tion, DMF together with OF has shown to improve
all scores, in comparison to baseline, on average
by 10%. Although the NMF have minimal indi-
vidual improvement, all these features can statis-
tically improve R-2 without stop words by 12%
(significance is measured by t-test statistics).
Experiment 3: ROUGE Evaluations
We use the following multi-document summariza-
tion models along with the Baseline presented in
Experiment 2 to evaluate HybSumm.
? PYTHY : (Toutanova et al, 2007) A state-
of-the-art supervised summarization system that
ranked first in overall ROUGE evaluations in
DUC2007. Similar to HybHSum, human gener-
ated summaries are used to train a sentence rank-
ing system using a classifier model.
? HIERSUM : (Haghighi and Vanderwende,
2009) A generative summarization method based
on topic models, which uses sentences as an addi-
tional level. Using an approximation for inference,
sentences are greedily added to a summary so long
as they decrease KL-divergence.
? HybFSum (Hybrid Flat Summarizer): To
investigate the performance of hierarchical topic
model, we build another hybrid model using flat
LDA (Blei et al, 2003b). In LDA each sentence
is a superposition of all K topics with sentence
specific weights, there is no hierarchical relation
between topics. We keep the parameters and the
features of the regression model of hierarchical
HybHSum intact for consistency. We only change
the sentence scoring method. Instead of the new
tree-based sentence scoring (? 4), we present a
similar method using topics from LDA on sen-
tence level. Note that in LDA the topic-word dis-
tributions ? are over entire vocabulary, and topic
mixing proportions for sentences ? are over all
the topics discovered from sentences in a docu-
ment cluster. Hence, we define sim1 and sim2
measures for LDA using topic-word proportions ?
(in place of discrete topic-word distributions from
each level in Eq.2) and topic mixing weights ? in
sentences (in place of topic proportions in Eq.3)
respectively. Maximum matching score is calcu-
lated as same as in HybHSum.
? HybHSum1 and HybHSum2: To analyze the ef-
fect of the new nCRP prior of sumHLDA on sum-
821
ROUGE w/o stop words w/ stop words
R-1 R-2 R-4 R-1 R-2 R-4
Baseline 32.4 7.4 10.6 41.0 9.3 15.2
PYTHY 35.7 8.9 12.1 42.6 11.9 16.8
HIERSUM 33.8 9.3 11.6 42.4 11.8 16.7
HybFSum 34.5 8.6 10.9 43.6 9.5 15.7
HybHSum1 34.0 7.9 11.5 44.8 11.0 16.7
HybHSum2 35.1 8.3 11.8 45.6 11.4 17.2
Table 3: ROUGE results of the best systems on
DUC2007 dataset (best results are bolded.)
marization model performance, we build two dif-
ferent versions of our hybrid model: HybHSum1
using standard hLDA (Blei et al, 2003a) and
HybHSum2 using our sumHLDA.
The ROUGE results are shown in Table 3. The
HybHSum2 achieves the best performance on R-
1 and R-4 and comparable on R-2. When stop
words are used the HybHSum2 outperforms state-
of-the-art by 2.5-7% except R-2 (with statistical
significance). Note that R-2 is a measure of bi-
gram recall and sumHLDA of HybHSum2 is built
on unigrams rather than bigrams. Compared to
the HybFSum built on LDA, both HybHSum1&2
yield better performance indicating the effective-
ness of using hierarchical topic model in summa-
rization task. HybHSum2 appear to be less re-
dundant than HybFSum capturing not only com-
mon terms but also specific words in Fig. 2, due
to the new hierarchical tree-based sentence scor-
ing which characterizes sentences on deeper level.
Similarly, HybHSum1&2 far exceeds baseline built
on simple classifier. The results justify the per-
formance gain by using our novel tree-based scor-
ing method. Although the ROUGE scores for
HybHSum1 and HybHSum2 are not significantly
different, the sumHLDA is more suitable for sum-
marization tasks than hLDA.
HybHSum2 is comparable to (if not better than)
fully generative HIERSUM. This indicates that
with our regression model built on training data,
summaries can be efficiently generated for test
documents (suitable for online systems).
Experiment 4: Manual Evaluations
Here, we manually evaluate quality of summaries,
a common DUC task. Human annotators are given
two sets of summary text for each document set,
generated from two approaches: best hierarchi-
cal hybrid HybHSum2 and flat hybrid HybFSum
models, and are asked to mark the better summary
New federal  rules for organic 
food will assure consumers that 
the products are grown and 
processed to the same standards 
nationwide. But as  sales grew 
more than 20 percent a year 
through the 1990s, organic food 
came to account for $1 of every 
$100 spent  on food, and in 1997 
t h e a g e n c y t o o k n o t i c e , 
proposing national organic 
standards for all food. 
By the year 2001, organic 
products are projected to 
command 5 percent of total food 
sales in the United  States. The 
sale of organics rose by about 30 
percent  last year, driven by 
concerns over food safety, the 
environment  and a fear of 
genetically engineered food. U.S. 
sales of organic foods have 
grown by 20 percent annually  for 
the last seven years.
(c) HybFSum Output
(b) HybHSum
2
 Output
The Agriculture Department 
began to propose standards for 
all  organic foods in the late 
1990's  because their sale had 
grown more than 20 per cent a 
year in that decade. In January 
1999 the USDA approved a 
"certified organic" label for 
meats and poultry that were 
raised without growth hormones, 
pesticide-treated feed, and 
antibiotics.
(a) Ref. Output
word
organic 6 6 6
genetic 2 4 3
allow 2 2 1
agriculture 1 1 1
standard 5 7 0
sludge 1 1 0
federal 1 1 0
bar 1 1 0
certified 1 1 0
s
p
e
c
i
f
i
c
H
y
b
H
S
u
m
2
H
y
b
F
S
u
m
R
e
f
Figure 2: Example summary text generated by systems
compared in Experiment 3. (Id:D0744 in DUC2007). Ref.
is the human generated summary.
Criteria HybFSum HybHSum2 Tie
Non-redundancy 26 44 22
Coherence 24 56 12
Focus 24 56 12
Responsiveness 30 50 12
Overall 24 66 2
Table 4: Frequency results of manual quality evaluations.
Results are statistically significant based on t-test. T ie indi-
cates evaluations where two summaries are rated equal.
according to five criteria: non-redundancy (which
summary is less redundant), coherence (which
summary is more coherent), focus and readabil-
ity (content and not include unnecessary details),
responsiveness and overall performance.
We asked 4 annotators to rate DUC2007 pre-
dicted summaries (45 summary pairs per anno-
tator). A total of 92 pairs are judged and eval-
uation results in frequencies are shown in Table
4. The participants rated HybHSum2 generated
summaries more coherent and focused compared
to HybFSum. All results in Table 4 are statis-
tically significant (based on t-test on 95% con-
fidence level.) indicating that HybHSum2 sum-
maries are rated significantly better.
822
...
Document Cluster
1
...
Document Cluster
2
...
Document Cluster
n
...
...
f
1
f
2
f
3
f
q
f-input features
...
f
1
f
2
f
3
f
q
f-input features
...
f
1
f
2
f
3
f
q
f-input features
h(f,y) : regression model for sentence ranking
...
...
...
.
.
z
z
K
zz
z
z
sumHLDA 
...
...
.
.
z
z
K
zz
z
z
sumHLDA 
...
...
.
.
z
z
K
zz
z
z
sumHLDA 
...
...
y-output
candidate sentence scores
0.02
0.01
0.0
.
.
y-output
candidate sentence scores
0.35
0.09
0.01
.
.
y-output
candidate sentence scores
0.43
0.20
0.03
.
.
Figure 3: Flow diagram for Hybrid Learning Algorithm for Multi-Document Summarization.
7 Conclusion
In this paper, we presented a hybrid model for
multi-document summarization. We demonstrated
that implementation of a summary focused hierar-
chical topic model to discover sentence structures
as well as construction of a discriminative method
for inference can benefit summarization quality on
manual and automatic evaluation metrics.
Acknowledgement
Research supported in part by ONR N00014-02-1-
0294, BT Grant CT1080028046, Azerbaijan Min-
istry of Communications and Information Tech-
nology Grant, Azerbaijan University of Azerbai-
jan Republic and the BISC Program of UC Berke-
ley.
References
R. Barzilay and L. Lee. Catching the drift: Proba-
bilistic content models with applications to gen-
eration and summarization. In In Proc. HLT-
NAACL?04, 2004.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
Hierarchical topic models and the nested chi-
nese restaurant process. In In Neural Informa-
tion Processing Systems [NIPS], 2003a.
D. Blei, T. Griffiths, and M. Jordan. The nested
chinese restaurant process and bayesian non-
parametric inference of topic hierarchies. In
Journal of ACM, 2009.
D. M. Blei, A. Ng, and M. Jordan. Latent dirichlet
allocation. In Jrnl. Machine Learning Research,
3:993-1022, 2003b.
S.R.K. Branavan, H. Chen, J. Eisenstein, and
R. Barzilay. Learning document-level seman-
tic properties from free-text annotations. In
Journal of Artificial Intelligence Research, vol-
ume 34, 2009.
J.M. Conroy, J.D. Schlesinger, and D.P. O?Leary.
Topic focused multi-cument summarization us-
ing an approximate oracle score. In In Proc.
ACL?06, 2006.
H. Daume?III and D. Marcu. Bayesian query fo-
cused summarization. In Proc. ACL-06, 2006.
H. Drucker, C.J.C. Burger, L. Kaufman, A. Smola,
and V. Vapnik. Support vector regression ma-
chines. In NIPS 9, 1997.
A. Haghighi and L. Vanderwende. Exploring con-
tent models for multi-document summarization.
In NAACL HLT-09, 2009.
T. Joachims. Making large-scale svm learning
practical. In In Advances in Kernel Methods -
Support Vector Learning. MIT Press., 1999.
C.-Y. Lin. Rouge: A package for automatic evalu-
ation of summaries. In In Proc. ACL Workshop
on Text Summarization Branches Out, 2004.
823
C.-Y. Lin and E.H. Hovy. Automatic evaluation
of summaries using n-gram co-occurance statis-
tics. In Proc. HLT-NAACL, Edmonton, Canada,
2003.
C. Manning and H. Schuetze. Foundations of sta-
tistical natural language processing. In MIT
Press. Cambridge, MA, 1999.
A. Nenkova and L. Vanderwende. The impact of
frequency on summarization. In Tech. Report
MSR-TR-2005-101, Microsoft Research, Red-
wood, Washington, 2005.
D.R. Radev, H. Jing, M. Stys, and D. Tam.
Centroid-based summarization for multiple
documents. In In Int. Jrnl. Information Process-
ing and Management, 2004.
D. Shen, J.T. Sun, H. Li, Q. Yang, and Z. Chen.
Document summarization using conditional
random fields. In Proc. IJCAI?07, 2007.
J. Tang, L. Yao, and D. Chens. Multi-topic based
query-oriented summarization. In SIAM Inter-
national Conference Data Mining, 2009.
I. Titov and R. McDonald. A joint model of text
and aspect ratings for sentiment summarization.
In ACL-08:HLT, 2008.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarla-
mudi, H. Suzuki, and L. Vanderwende. The ph-
thy summarization system: Microsoft research
at duc 2007. In Proc. DUC, 2007.
J.Y. Yeh, H.-R. Ke, W.P. Yang, and I-H. Meng.
Text summarization using a trainable summa-
rizer and latent semantic analysis. In Informa-
tion Processing and Management, 2005.
824
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 491?499,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Discovery of Topically Coherent Sentences for Extractive Summarization
Asli Celikyilmaz
Microsoft Speech Labs
Mountain View, CA, 94041
asli@ieee.org
Dilek Hakkani-Tu?r
Microsoft Speech Labs |Microsoft Research
Mountain View, CA, 94041
dilek@ieee.org
Abstract
Extractive methods for multi-document sum-
marization are mainly governed by informa-
tion overlap, coherence, and content con-
straints. We present an unsupervised proba-
bilistic approach to model the hidden abstract
concepts across documents as well as the cor-
relation between these concepts, to generate
topically coherent and non-redundant sum-
maries. Based on human evaluations our mod-
els generate summaries with higher linguistic
quality in terms of coherence, readability, and
redundancy compared to benchmark systems.
Although our system is unsupervised and opti-
mized for topical coherence, we achieve a 44.1
ROUGE on the DUC-07 test set, roughly in the
range of state-of-the-art supervised models.
1 Introduction
A query-focused multi-document summarization
model produces a short-summary text of a set of
documents, which are retrieved based on a user?s
query. An ideal generated summary text should con-
tain the shared relevant content among set of doc-
uments only once, plus other unique information
from individual documents that are directly related
to the user?s query addressing different levels of de-
tail. Recent approaches to the summarization task
has somewhat focused on the redundancy and co-
herence issues. In this paper, we introduce a series
of new generative models for multiple-documents,
based on a discovery of hierarchical topics and their
correlations to extract topically coherent sentences.
Prior research has demonstrated the usefulness
of sentence extraction for generating summary text
taking advantage of surface level features such as
word repetition, position in text, cue phrases, etc,
(Radev, 2004; Nenkova and Vanderwende, 2005a;
Wan and Yang, 2006; Nenkova et al, 2006). Be-
cause documents have pre-defined structures (e.g.,
sections, paragraphs, sentences) for different levels
of concepts in a hierarchy, most recent summariza-
tion work has focused on structured probabilistic
models to represent the corpus concepts (Barzilay
et al, 1999; Daume?-III and Marcu, 2006; Eisenstein
and Barzilay, 2008; Tang et al, 2009; Chen et al,
2000; Wang et al, 2009). In particular (Haghighi
and Vanderwende, 2009; Celikyilmaz and Hakkani-
Tur, 2010) build hierarchical topic models to iden-
tify salient sentences that contain abstract concepts
rather than specific concepts. Nonetheless, all these
systems crucially rely on extracting various levels of
generality from documents, focusing little on redun-
dancy and coherence issues in model building. A
model than can focus on both issues is deemed to be
more beneficial for a summarization task.
Topical coherence in text involves identifying key
concepts, the relationships between these concepts,
and linking these relationships into a hierarchy. In
this paper, we present a novel, fully generative
Bayesian model of document corpus, which can dis-
cover topically coherent sentences that contain key
shared information with as little detail and redun-
dancy as possible. Our model can discover hierar-
chical latent structure of multi-documents, in which
some words are governed by low-level topics (T)
and others by high-level topics (H). The main con-
tributions of this work are:
? construction of a novel bayesian framework to
491
capture higher level topics (concepts) related to sum-
mary text discussed in ?3,
? representation of a linguistic system as a sequence
of increasingly enriched models, which use posterior
topic correlation probabilities in sentences to design
a novel sentence ranking method in ?4 and 5,
? application of the new hierarchical learning
method for generation of less redundant summaries
discussed in ?6. Our models achieve compara-
ble qualitative results on summarization of multiple
newswire documents. Human evaluations of gener-
ated summaries confirm that our model can generate
non-redundant and topically coherent summaries.
2 Multi-Document Summarization Models
Prior research has demonstrated the usefulness of
sentence extraction for summarization based on lex-
ical, semantic, and discourse constraints. Such
models often rely on different approaches includ-
ing: identifying important keywords (Nenkova et al,
2006); topic signatures based on user queries (Lin
and Hovy, 2002; Conroy et al, 2006; Harabagiu
et al, 2007); high frequency content word feature
based learning (Nenkova and Vanderwende, 2005a;
Nenkova and Vanderwende, 2005b), to name a few.
Recent research focusing on the extraction of la-
tent concepts from document clusters are close in
spirit to our work (Barzilay and Lee, 2004; Daume?-
III and Marcu, 2006; Eisenstein and Barzilay, 2008;
Tang et al, 2009; Wang et al, 2009). Some of these
work (Haghighi and Vanderwende, 2009; Celikyil-
maz and Hakkani-Tur, 2010) focus on the discov-
ery of hierarchical concepts from documents (from
abstract to specific) using extensions of hierarchal
topic models (Blei et al, 2004) and reflect this hier-
archy on the sentences. Hierarchical concept learn-
ing models help to discover, for instance, that ?base-
ball? and ?football? are both contained in a general
class ?sports?, so that the summaries reference terms
related to more abstract concepts like ?sports?.
Although successful, the issue with concept learn-
ing methods for summarization is that the extracted
sentences usually contain correlated concepts. We
need a model that can identify salient sentences re-
ferring to general concepts of documents and there
should be minimum correlation between them.
Our approach differs from the early work, in that,
we utilize the advantages of previous topic models
and build an unsupervised generative model that can
associate each word in each document with three
random variables: a sentence S, a higher-level topic
H, and a lower-level topic T, in an analogical way
to PAM models (Li and McCallum, 2006), i.e., a di-
rected acyclic graph (DAG) representing mixtures of
hierarchical structure, where super-topics are multi-
nomials over sub-topics at lower levels in the DAG.
We define a tiered-topic clustering in which the up-
per nodes in the DAG are higher-level topics H, rep-
resenting common co-occurence patterns (correla-
tions) between lower-level topics T in documents.
This has not been the focus in prior work on genera-
tive approaches for summarization task. Mainly, our
model can discover correlated topics to eliminate re-
dundant sentences in summary text.
Rather than representing sentences as a layer in
hierarchical models, e.g., (Haghighi and Vander-
wende, 2009; Celikyilmaz and Hakkani-Tur, 2010),
we model sentences as meta-variables. This is sim-
ilar to author-topic models (Rosen-Zvi et al, 2004),
in which words are generated by first selecting an
author uniformly from an observed author list and
then selecting a topic from a distribution over topics
that is specific to that author. In our model, words
are generated from different topics of documents by
first selecting a sentence containing the word and
then topics that are specific to that sentence. This
way we can directly extract from documents the
summary related sentences that contain high-level
topics. In addition in (Celikyilmaz and Hakkani-Tur,
2010), the sentences can only share topics if the sen-
tences are represented on the same path of captured
topic hierarchy, restricting topic sharing across sen-
tences on different paths. Our DAG identifies tiered
topics distributed over document clusters that can be
shared by each sentence.
3 Topic Coherence for Summarization
In this section we discuss the main contribution,
our two hierarchical mixture models, which improve
summary generation performance through the use of
tiered topic models. Our models can identify lower-
level topics T (concepts) defined as distributions
over words or higher-level topics H, which represent
correlations between these lower level topics given
492
sentences. We present our synthetic experiment for
model development to evaluate extracted summaries
on redundancy measure. In ?6, we demonstrate the
performance of our models on coherence and infor-
mativeness of generated summaries by qualitative
and intrinsic evaluations.
For model development we use the DUC 2005
dataset1, which consists of 45 document clusters,
each of which include 1-4 set of human gener-
ated summaries (10-15 sentences each). Each doc-
ument cluster consists ? 25 documents (25-30 sen-
tences/document) retrieved based on a user query.
We consider each document cluster as a corpus and
build 45 separate models.
For the synthetic experiments, we include the pro-
vided human generated summaries of each corpus
as additional documents. The sentences in human
summaries include general concepts mentioned in
the corpus, the salient sentences of documents. Con-
trary to usual qualitative evaluations of summariza-
tion tasks, our aim during development is to measure
the percentage of sentences in a human summary
that our model can identify as salient among all other
document cluster sentences. Because human pro-
duced summaries generally contain non-redundant
sentences, we use total number of top-ranked hu-
man summary sentences as a qualitative redundancy
measure in our synthetic experiments.
In each model, a document d is a vector of Nd
words wd, where each wid is chosen from a vocabu-
lary of size V , and a vector of sentences S, represent-
ing all sentences in a corpus of size SD. We identify
sentences as meta-variables of document clusters,
which the generative process models both sentences
and documents using tiered topics. A sentence?s re-
latedness to summary text is tied to the document
cluster?s user query. The idea is that a lexical word
present or related to a query should increase its sen-
tence?s probability of relatedness.
4 Two-Tiered Topic Model - TTM
Our base model, the two-tiered topic model (TTM),
is inspired by the hierarchical topic model, PAM,
proposed by Li and McCallum (2006). PAM struc-
tures documents to represent and learn arbitrary,
nested, and possibly sparse topic correlations using
1www-nlpir.nist.gov/projects/duc/data.html
(Background)
Specific
Content 
Parameters
w
S
Sentences
x
T
Lower-
Level 
Topics
?
Summary Related 
Word Indicator
S
D
K
2
?
H
Summary 
Content 
Indicator 
Parameters 
?
?
T
Lower-Level 
Topic 
Parameters 
Higher-Level 
Topic 
Parameters 
K
1?
K
2
K
1
?
Documents in a Document Cluster
N
d
Document
Sentence 
selector
y
Higher-Level 
Topics
H
Figure 1: Graphical model depiction of two-tiered topic model
(TTM) described in section ?4. S are sentences si=1..SD in doc-
ument clusters. The high-level topics (Hk1=1...K1 ), represent-
ing topic correlations, are modeled as distributions over low-
level-topics (Tk2=1...K2 ). Shaded nodes indicate observed vari-
ables. Hyper-parameters for ?, ?H , ?T , ? are omitted.
a directed acyclic graph. Our goals are not so dif-
ferent: we aim to discover concepts from documents
that would attribute for the general topics related to a
user query, however, we want to relate this informa-
tion to sentences. We represent sentences S by dis-
covery of general (more general) to specific topics
(Fig.1). Similarly, we represent summary unrelated
(document specific) sentences as corpus specific dis-
tributions ? over background words wB, (functional
words like prepositions, etc.).
Our two-tiered topic model for salient sentence
discovery can be generated for each word in the doc-
ument (Algorithm 1) as follows: For a word wid in
document d, a random variable xid is drawn, which
determines if wid is query related, i.e., wid either ex-
ists in the query or is related to the query2. Oth-
erwise, wid is unrelated to the user query. Then
sentence si is chosen uniformly at random (ysi?
Uniform(si)) from sentences in the document con-
taining wid (deterministic if there is only one sen-
tence containing wid). We assume that if a word is
related to a query, it is likely to be summary-related
2We measure relatedness to a query if a word exists in the
query or it is synonymous based on information extracted from
WordNet (Miller, 1995).
493
H1
H
2
H
3
T
1
T
2
T
3
T
T
T
T
w
B
...
W
W
W
...
H
4
T
4
T
W
Sentences
Document 
Specific 
Words
?
S
H
T
T
W
K
1
K
2
T
3 :?network?
?retail?
C
4
H
1
starbucks, 
coffee, schultz, 
tazo, pasqua, 
states, subsidiary 
acquire,  bought, 
purchase, 
disclose, joint-
venture, johnson
starbucks, coffee, 
retailer, 
frappaccino
francisco, pepsi, 
area, profit, 
network, internet, 
Francisco-based
H
2
H
3
T
2 :?coffee?
T
4 :?retail?
T
1 :?acquisition?
High-Level Topics
Low-
Level 
Topics
Figure 2: Depiction of TTM given the query ?D0718D: Star-
bucks Coffee : How has Starbucks Coffee attempted to ex-
pand and diversify through joint ventures, acquisitions, or
subsidiaries??. If a word is query/summary related sentence
S, first a sentence then a high-level (H) and a low-level (T )
topic is sampled. (
C
represents that a random variable is a
parent of all C random variables.) The bolded links fromH?T
represent correlated low-level topics.
(so as the sampled sentence si). We keep track of
the frequency of si?s in a vector, DS ? ZSD . Ev-
ery time an si is sampled for a query related wid, we
increment its count, a degree of sentence saliency.
Given that wid is related to a query, it is as-
sociated with two-tiered multinomial distributions:
high-level H topics and low-level T topics. A high-
level topic Hki is chosen first from a distribution
over low-level topics T specific to that si and one
low-level topic Tkj is chosen from a distribution
over words, and wid is generated from the sampled
low-level topic. If wid is not query-related, it is gen-
erated as a background word wB .
The resulting tiered model is shown as a graph
and plate diagrams in Fig.1 & 2. A sentence sampled
from a query related word is associated with a dis-
tribution over K1 number of high-level topics Hki ,
each of which are also associated with K2 number
of low-level topics Tkj , a multinomial over lexical
words of a corpus. In Fig.2 the most confident words
of four low-level topics is shown. The bolded links
between Hki and Tkj represent the strength of cor-
Algorithm 1 Two-Tiered Topic Model Generation
1: Sample: si = 1..SD: ? ? Beta(?),
2: k1 = 1...K1: ?H ? Dirichlet(?H),
3: k2 = 1...K1 ?K2: ?T ? Dirichlet(?T ),
4: and k = 1..K2: ? ? Dirichlet(?).
5: for documents d? 1, ..., D do
6: for words wid, i? 1, ..., Nd do
7: - Draw a discrete x ? Binomial(?wid)
?
8: - If x = 1, wid is summary related;
9: ? conditioned on S draw a sentence
10: ysi ? Uniform(si) containing wi,
11: ? sample a high-level topicHk1 ? ?
H
k1(?
H),
12: and a low-level topic Tk2 ? ?
T
k2(?
T ),
13: ? sample a word wik1k2 ? ?Hk1Tk2 (?),
14: - If x = 0, the word is unrelated ??
15: sample a word wB ? ?(?),
16: corpus specific distribution.
17: end for
18: end for
? if wid exists or related to the the query then x = 1 deterministic,
otherwise it is stochastically assigned x ? Bin(?).
?? wid is a background word.
relation between Tkj ?s, e.g., the topic ?acquisition?
is found to be more correlated with ?retail? than the
?network? topic given H1. This information is used
to rank sentences based on the correlated topics.
4.1 Learning and Inference for TTM
Our learning procedure involves finding parame-
ters, which likely integrates out model?s posterior
distribution P (H,T|Wd,S), d?D. EM algorithms
might face problems with local maxima in topic
models (Blei et al, 2003) suggesting implementa-
tion of approximate methods in which some of the
parameters, e.g., ?H , ?T , ?, and ?, can be integrated
out, resulting in standard Dirichlet-multinomial as
well as binomial distributions. We use Gibbs sam-
pling which allows a combination of estimates from
several local maxima of the posterior distribution.
For each word, xid is sampled from a sentence
specific binomial ? which in turn has a smooth-
ing prior ? to determine if the sampled word wid is
(query) summary-related or document-specific. De-
pending on xid, we either sample a sentence along
with a high/low-level topic pair or just sample back-
ground words wB . The probability distribution over
sentence assignments, P (ysi = s|S) si ? S, is as-
sumed to be uniform over the elements of S, and de-
terministic if there is only one sentence in the docu-
494
ment containing the corresponding word. The opti-
mum hyper-parameters are set based on the training
dataset model performance via cross-validation 3.
For each word we sample a high-level Hki and
a low-level Tkj topic if the word is query related
(xid = 1). The sampling distribution for TTM
for a word given the remaining topics and hyper-
parameters ?H , ?T , ?, ?, ? is:
pTTM(Hk1 , Tk2 , x = 1|w,H?k1 ,T?k2) ?
?H + nk1d?
H? ?
H? + nd
?
?T + nk1k2d?
T ? ?
T ? + ndH
?
? + nk1k2x
2? + nk1k2
?
?w + nwk1k2x?
w? ?w? + nk1k2x
and when x = 0 (a corpus specific word),
pTTM(x = 0|w, zH?k, zt?k) ?
? + nxk1k2
2? + nk1k2
?
?w + nw
?
w? ?w? + n
The nk1d is the number of occurrences of high-level
topic k1 in document d, and n
k1k2
d is the number of
times the low-level topic k2 is sampled together with
high-level topic k1 in d, nwk1k2x is the number of oc-
currences of word w sampled from path H-T given
that the word is query related. Note that the number
of tiered topics in the model is fixed to K1 and K2,
which is optimized with validation experiments. It
is also possible to construct extended models of TTM
using non-parametric priors, e.g., hierarchal Dirich-
let processes (Li et al, 2007) (left for future work).
4.2 Summary Generation with TTM
We can observe the frequency of draws of every sen-
tence in a document cluster S, given it?s words are
related, through DS ? ZSD . We obtain DS during
Gibbs sampling (in ?4.1), which indicates a saliency
score of each sentence sj ? S, j = 1..SD:
scoreTTM(sj) ? # [wid ? sj , xid = 1] /nwj (1)
where wid indicates a word in a document d that ex-
ists in sj and is sampled as summary related based
on random indicator variable xid. nwj is the num-
ber of words in sj and normalizes the score favoring
3An alternative way would be to use Dirichlet priors (Blei et
al., 2003) which we opted for due to computational reasons but
will be investigated as future research.
sentences with many related words. We rank sen-
tences based on (1). We compare TTM results on
synthetic experiments against PAM (Li and McCal-
lum, 2006) a similar topic model that clusters topics
in a hierarchical structure, where super-topics are
distributions over sub-topics. We obtain sentence
scores for PAM models by calculating the sub-topic
significance (TS) based on super-topic correlations,
and discover topic correlations over the entire docu-
ment space (corpus wide). Hence; we calculate the
TS of a given sub-topic, k = 1, ..,K2 by:
TS(zk) =
1
D
?
d?D
1
K1
K1?
k1
p(zksub|z
k1
sup) (2)
where zksub is a sub-topic k = 1..K2 and z
k1
sup is a
super-topic k1. The conditional probability of a sub-
topic k given a super-topic k1, p(zksub|z
k1
sup), explains
the variation of that sub-topic in relation to other
sub-topics. The higher the variation over the entire
corpus, the better it represents the general theme of
the documents. So, sentences including such topics
will have higher saliency scores, which we quantify
by imposing topic?s significance on vocabulary:
scorePAM(si) =
1
K2
K2?
k
?
w?si
p(w|zksub) ? TS(zk)
(3)
Fig. 4 illustrates the average salience sentence se-
lection performance of TTM and PAM models (for
45 models). The x-axis represents the percentage of
sentences selected by the model among all sentences
in the DUC2005 corpus. 100% means all sentences
in the corpus included in the summary text. The
y-axis is the % of selected human sentences over
all sentences. The higher the human summary sen-
tences are ranked, the better the model is in select-
ing the salient sentences. Hence, the system which
peaks sooner indicates a better model.
In Fig.4 TTM is significantly better in identifying
human sentences as salient in comparison to PAM.
The statistical significance is measured based on the
area under the curve averaged over 45 models.
5 Enriched Two-Tiered Topic Model
Our model can discover words that are related to
summary text using posteriors P? (?H) and P? (?T ),
495
?acquisition?
?coffee?
?network?
H
2
T,W
H
?retail?
seattle, 
acquire, sales, 
billion...
coffee, 
starbucks...
purchase, 
disclose, 
joint-venture, 
johnson
schultz, tazo, 
pasqua, 
states, 
subsidiary 
pepsi, area, 
profit,network
francisco
frappaccino, 
retailer, 
mocca, 
organic
T
2
T,W
H
High-Level Topics
H
1
W
L
T
1
T
3
T
4
Low-Level Topics
Low-Level Topics
L=2
L=2
L=2
L=2
L=1
L=1
?
L
Indicator
Word
Level 
(Background)
Specific
Content 
Parameters
w
S
Sentences
x
T
H
Lower-
Level 
Topics
Higher-Level 
Topics
Summary Related 
Word Indicator
S
D
?
H
Summary 
Content 
Indicator 
Parameters 
?
?
T
Lower-Level 
Topic 
Parameters 
Higher-Level 
Topic 
Parameters 
Sentence 
selector
K
1?
K
2
K
1
y
?
Documents in a Document Cluster
N
d
Document
?
K
1 
+K
2
W
L
W
L
W
L
Figure 3: Graphical model depiction of sentence level enriched two-tiered model (ETTM) described in section ?5. Each path
defined byH/T pair k1k2, has a multinomial ? over which level of the path outputs a given word. L indicates which level, i.e, high
or low, the word is sampled from. On the right is the high-level topic-word and low-level topic-word distributions characterized by
ETTM. Each Hk1 also represented as distributions over general words WH as well as indicates the degree of correlation between
low-level topics denoted by boldness of the arrows.
as well as words wB specific to documents (via
P? (?)) (Fig.1). TTM can discover topic correlations,
but cannot differentiate if a word in a sentence is
more general or specific given a query. Sentences
with general words would be more suitable to in-
clude in summary text compared to sentences con-
taining specific words. For instance for a given sen-
tence: ?Starbucks Coffee has attempted to expand
and diversify through joint ventures, and acquisi-
tions.?, ?starbucks? and ?coffee? are more gen-
eral words given the document clusters compared
to ?joint? and ?ventures? (see Fig.2), because they
appear more frequently in document clusters. How-
ever, TTM has no way of knowing that ?starbucks?
and ?coffee? are common terms given the context.
We would like to associate general words with high-
level topics, and context specific words with low-
level topics. Sentence containing words that are
sampled from high-level topics would be a bet-
ter candidate for summary text. Thus; we present
enriched TTM (ETTM) generative process (Fig.3),
which samples words not only from low-level top-
ics but also from high-level topics as well.
ETTM discovers three separate distributions over
words: (i) high-level topics H as distributions over
corpus general words WH, (ii) low-level topics T
as distributions over corpus specific words WL, and
Level Generation for Enriched TTM
Fetch ?k ? Beta(?); k = 1...K1 ?K2.
For wid, i = 1, ..., Nd, d = 1, ..D:
If x = 1, sentence si is summary related;
- sample Hk1 and Tk2
- sample a level L from Bin(?k1k2)
- If L = 1 (general word); wid ? ?Hki
- else if L = 2 (context specific); wid ? ?Hk1Tk2
else if x = 0, do Step 14-16 in Alg. 1.
(iii) background word distributions, i.e,. document
specific WB (less confidence for summary text).
Similar to TTM?s generative process, if wid is re-
lated to a given query, then x = 1 is determin-
istic, otherwise x ? {0, 1} is stochastically deter-
mined if wid should be sampled as a background
word (wB) or through hierarchical path, i.e., H-T
pairs. We first sample a sentence si for wid uni-
formly at random from the sentences containing the
word ysi?Uniform(si)). At this stage we sample a
level Lwid ? {1, 2} for wid to determine if it is a
high-level word, e.g., more general to context like
?starbucks? or ?coffee? or more specific to related
context such as ?subsidiary?, ?frappucino?. Each
path through the DAG, defined by a H-T pair (total
of K1K2 pairs), has a binomial ?K1K2 over which
496
10.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
%
 
o
f
 
h
u
m
a
n
 
g
e
n
e
r
a
t
e
d
 
s
e
n
t
e
n
c
e
s
 
u
s
e
d
 
i
n
 
t
h
e
 
g
e
n
e
r
a
t
e
d
 
s
u
m
m
a
r
y
0          10          20         30         40         50         60          70         80         90        100
% of sentences added to the generated summary text.
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0           2            4           6            8           10
ETIM
TIM
PAM
hPAM
ETIM
TIM
hPAM
PAM
TIM
ETIM
PAM
HPAM
Figure 4: Average saliency performance of four systems over
45 different DUC models. The area under each curve is shown
in legend. Inseam is the magnified view of top-ranked 10% of
sentences in corpus.
level of the path outputs sampled word. If the word
is a specific type, x = 0, then it is sampled from the
background word distribution ?, a document specific
multinomial. Once the level and conditional path is
drawn (see level generation for ETTM above) the rest
of the generative model is same as TTM.
5.1 Learning and Inference for ETTM
For each word, x is sampled from a sentence spe-
cific binomial ?, just like TTM. If the word is related
to the query x = 1, we sample a high and low-level
topic pair H ? T as well as an additional level L is
sampled to determine which level of topics the word
should be sampled from. L is a corpus specific bi-
nomial one for all H ? T pairs. If L = 1, the word
is one of corpus general words and sampled from
the high-level topic, otherwise (L = 2) the word
is corpus specific and sampled from a the low-level
topic. The optimum hyper-parameters are set based
on training performance via cross validation.
The conditional probabilities are similar to TTM,
but with additional random variables, which deter-
mine the level of generality of words as follows:
pETTM(Tk1 , Tk2 , L|w,T?k1 ,T?k2 , L) ?
pTTM(Tk1 , Tk2 , x = 1|.) ?
?+NLk1k2
2?+nk1k2
5.2 Summary Generation with ETTM
For ETTM models, we extend the TTM sentence
score to be able to include the effect of the general
words in sentences (as word sequences in language
models) using probabilities of K1 high-level topic
distributions, ?wHk=1..K1
, as:
scoreETTM(si) ? # [wid ? sj , xid = 1] /nwj ?
1
K1
?
k=1..K1
?
w?si
p(w|Tk)
where p(w|Tk) is the probability of a word in si
being generated from high-level topic Hk. Using
this score, we re-rank the sentences in documents
of the synthetic experiment. We compare the re-
sults of ETTM to a structurally similar probabilis-
tic model, entitled hierarchical PAM (Mimno et al,
2007), which is designed to capture topics on a hi-
erarchy of two layers, i.e., super topics and sub-
topics, where super-topics are distributions over ab-
stract words. In Fig. 4 out of 45 models ETTM has
the best performance in ranking the human gener-
ated sentences at the top, better than the TTM model.
Thus; ETTM is capable of capturing focused sen-
tences with general words related to the main con-
cepts of the documents and much less redundant
sentences containing concepts specific to user query.
6 Final Experiments
In this section, we qualitatively compare our models
against state-of-the art models and later apply an in-
trinsic evaluation of generated summaries on topical
coherence and informativeness.
For a qualitative comparison with the previous
state-of-the models, we use the standard summariza-
tion datasets on this task. We train our models on the
datasets provided by DUC2005 task and validate the
results on DUC 2006 task, which consist of a total
of 100 document clusters. We evaluate the perfor-
mance of our models on DUC2007 datasets, which
comprise of 45 document clusters, each containing
25 news articles. The task is to create max. 250
word long summary for each document cluster.
6.1. ROUGE Evaluations: We train each docu-
ment cluster as a separate corpus to find the optimum
parameters of each model and evaluate on test docu-
ment clusters. ROUGE is a commonly used measure,
a standard DUC evaluation metric, which computes
recall over various n-grams statistics from a model
generated summary against a set of human generated
summaries. We report results in R-1 (recall against
unigrams), R-2 (recall against bigrams), and R-SU4
497
ROUGE w/o stop words w/ stop words
R-1 R-2 R-4 R-1 R-2 R-4
PYTHY 35.7 8.9 12.1 42.6 11.9 16.8
HIERSUM 33.8 9.3 11.6 42.4 11.8 16.7
HybHSum 35.1 8.3 11.8 45.6 11.4 17.2
PAM 32.1 7.1 11.0 41.7 9.1 15.3
hPAM 31.9 7.0 11.1 41.2 8.9 15.2
TTM? 34.0 8.7 11.5 44.7 10.7 16.5
ETTM? 32.4 8.3 11.2 44.1 10.4 16.4
Table 1: ROUGE results of the best systems on DUC2007
dataset (best results are bolded.) ? indicate our models.
(recall against skip-4 bigrams) ROUGE scores w/ and
w/o stop words included.
For our models, we ran Gibbs samplers for 2000
iterations for each configuration throwing out first
500 samples as burn-in. We iterated different values
for hyperparameters and measured the performance
on validation dataset to capture the optimum values.
The following models are used as benchmark:
(i) PYTHY (Toutanova et al, 2007): Utilizes hu-
man generated summaries to train a sentence rank-
ing system using a classifier model; (ii) HIERSUM
(Haghighi and Vanderwende, 2009): Based on hier-
archical topic models. Using an approximation for
inference, sentences are greedily added to a sum-
mary so long as they decrease KL-divergence of the
generated summary concept distributions from doc-
ument word-frequency distributions. (iii) HybHSum
(Celikyilmaz and Hakkani-Tur, 2010): A semi-
supervised model, which builds a hierarchial LDA to
probabilistically score sentences in training dataset
as summary or non-summary sentences. Using these
probabilities as output variables, it learns a discrim-
inative classifier model to infer the scores of new
sentences in testing dataset. (iv) PAM (Li and Mc-
Callum, 2006) and hPAM (Mimno et al, 2007): Two
hierarchical topic models to discover high and low-
level concepts from documents, baselines for syn-
thetic experiments in ?4 & ?5.
Results of our experiments are illustrated in Table
6. Our unsupervised TTM and ETTM systems yield a
44.1 R-1 (w/ stop-words) outperforming the rest of
the models, except HybHSum. Because HybHSum
uses the human generated summaries as supervision
during model development and our systems do not,
our performance is quite promising considering the
generation is completely unsupervised without see-
ing any human generated summaries during train-
ing. However, the R-2 evaluation (as well as R-4) w/
stop-words does not outperform other models. This
is because R-2 is a measure of bi-gram recall and
neither of our models represent bi-grams whereas,
for instance, PHTHY includes several bi-gram and
higher order n-gram statistics. For topic models bi-
grams tend to degenerate due to generating inconsis-
tent bag of bi-grams (Wallach, 2006).
6.2. Manual Evaluations: A common DUC
task is to manually evaluate models on the qual-
ity of generated summaries. We compare our best
model ETTM to the results of PAM, our benchmark
model in synthetic experiments, as well as hybrid
hierarchical summarization model, hLDA (Celiky-
ilmaz and Hakkani-Tur, 2010). Human annotators
are given two sets of summary text for each docu-
ment set, generated from either one of the two ap-
proaches: best ETTM and PAM or best ETTM and
HybHSum models. The annotators are asked to
mark the better summary according to five criteria:
non-redundancy (which summary is less redundant),
coherence (which summary is more coherent), fo-
cus and readability (content and no unnecessary de-
tails), responsiveness and overall performance.
We asked 3 annotators to rate DUC2007 predicted
summaries (45 summary pairs per annotator). A to-
tal of 42 pairs are judged for ETTM vs. PAM mod-
els and 49 pairs for ETTM vs. HybHSum models.
The evaluation results in frequencies are shown in
Table 6. The participants rated ETTM generated
summaries more coherent and focused compared to
PAM, where the results are statistically significant
(based on t-test on 95% confidence level) indicat-
ing that ETTM summaries are rated significantly bet-
ter. The results of ETTM are slightly better than
HybHSum. We consider our results promising be-
cause, being unsupervised, ETTM does not utilize
human summaries for model development.
7 Conclusion
We introduce two new models for extracting topi-
cally coherent sentences from documents, an impor-
tant property in extractive multi-document summa-
rization systems. Our models combine approaches
from the hierarchical topic models. We empha-
498
PAM ETTM Tie
HybHSum
ETTM Tie
Non-Redundancy 13 26 3 12 18 19
Coherence 13 26 3 15 18 16
Focus 14 24 4 14 17 18
Responsiveness 15 24 3 19 12 18
Overall 15 25 2 17 22 10
Table 2: Frequency results of manual evaluations. Tie in-
dicates evaluations where two summaries are rated equal.
size capturing correlated semantic concepts in docu-
ments as well as characterizing general and specific
words, in order to identify topically coherent sen-
tences in documents. We showed empirically that a
fully unsupervised model for extracting general sen-
tences performs well at summarization task using
datasets that were originally used in building auto-
matic summarization system challenges. The suc-
cess of our model can be traced to its capability
of directly capturing coherent topics in documents,
which makes it able to identify salient sentences.
Acknowledgments
The authors would like to thank Dr. Zhaleh Feizol-
lahi for her useful comments and suggestions.
References
R. Barzilay and L. Lee. 2004. Catching the drift: Proba-
bilistic content models with applications to generation
and summarization. In Proc. HLT-NAACL?04.
R. Barzilay, K.R. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-document
summarization. Proc. 37th ACL, pages 550?557.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
2004. Hierarchical topic models and the nested chi-
nese restaurant process. In Neural Information Pro-
cessing Systems [NIPS].
A. Celikyilmaz and D. Hakkani-Tur. 2010. A hybrid hi-
erarchical model for multi-document summarization.
Proc. 48th ACL 2010.
D. Chen, J. Tang, L. Yao, J. Li, and L. Zhou. 2000.
Query-focused summarization by combining topic
model and affinity propagation. LNCS? Advances in
Data and Web Development.
J. Conroy, H. Schlesinger, and D. OLeary. 2006. Topic-
focused multi-document summarization using an ap-
proximate oracle score. Proc. ACL.
H. Daume?-III and D. Marcu. 2006. Bayesian query fo-
cused summarization. Proc. ACL-06.
J. Eisenstein and R. Barzilay. 2008. Bayesian unsuper-
vised topic segmentation. Proc. EMNLP-SIGDAT.
A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
NAACL HLT-09.
S. Harabagiu, A. Hickl, and F. Lacatusu. 2007. Sat-
isfying information needs with multi-document sum-
maries. Information Processing and Management.
W. Li and A. McCallum. 2006. Pachinko allocation:
Dag-structure mixture models of topic correlations.
Proc. ICML.
W. Li, D. Blei, and A. McCallum. 2007. Nonparametric
bayes pachinko allocation. The 23rd Conference on
Uncertainty in Artificial Intelligence.
C.Y. Lin and E. Hovy. 2002. The automated acquisi-
tion of topic signatures fro text summarization. Proc.
CoLing.
G. A. Miller. 1995. Wordnet: A lexical database for
english. ACM, Vol. 38, No. 11: 39-41.
D. Mimno, W. Li, and A. McCallum. 2007. Mixtures
of hierarchical topics with pachinko allocation. Proc.
ICML.
A. Nenkova and L. Vanderwende. 2005a. Document
summarization using conditional random fields. Tech-
nical report, Microsoft Research.
A. Nenkova and L. Vanderwende. 2005b. The impact
of frequency on summarization. Technical report, Mi-
crosoft Research.
A. Nenkova, L. Vanderwende, and K. McKowen. 2006.
A composition context sensitive multi-document sum-
marizer. Prof. SIGIR.
D. R. Radev. 2004. Lexrank: graph-based centrality as
salience in text summarization. Jrnl. Artificial Intelli-
gence Research.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. The author-topic model for authors and docu-
ments. UAI.
J. Tang, L. Yao, and D. Chens. 2009. Multi-topic based
query-oriented summarization. SIAM International
Conference Data Mining.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The ph-
thy summarization system: Microsoft research at duc
2007. In Proc. DUC.
H. Wallach. 2006. Topic modeling: Beyond bag-of-
words. Proc. ICML 2006.
X. Wan and J. Yang. 2006. Improved affinity graph
based multi-document summarization. HLT-NAACL.
D. Wang, S. Zhu, T. Li, and Y. Gong. 2009. Multi-
document summarization using sentence-based topic
models. Proc. ACL 2009.
499
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 330?338,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Joint Model for Discovery of Aspects in Utterances
Asli Celikyilmaz
Microsoft
Mountain View, CA, USA
asli@ieee.org
Dilek Hakkani-Tur
Microsoft
Mountain View, CA, USA
dilek@ieee.org
Abstract
We describe a joint model for understanding
user actions in natural language utterances.
Our multi-layer generative approach uses both
labeled and unlabeled utterances to jointly
learn aspects regarding utterance?s target do-
main (e.g. movies), intention (e.g., finding a
movie) along with other semantic units (e.g.,
movie name). We inject information extracted
from unstructured web search query logs as
prior information to enhance the generative
process of the natural language utterance un-
derstanding model. Using utterances from five
domains, our approach shows up to 4.5% im-
provement on domain and dialog act perfor-
mance over cascaded approach in which each
semantic component is learned sequentially
and a supervised joint learning model (which
requires fully labeled data).
1 Introduction
Virtual personal assistance (VPA) is a human to
machine dialog system, which is designed to per-
form tasks such as making reservations at restau-
rants, checking flight statuses, or planning weekend
activities. A typical spoken language understanding
(SLU) module of a VPA (Bangalore, 2006; Tur and
Mori, 2011) defines a structured representation for
utterances, in which the constituents correspond to
meaning representations in terms of slot/value pairs
(see Table 1). While target domain corresponds to
the context of an utterance in a dialog, the dialog
act represents overall intent of an utterance. The
slots are entities, which are semantic constituents at
the word or phrase level. Learning each component
Sample utterances on ?plan a night out? scenario
(I) Show me theaters in [Austin] playing [iron man 2].
(II)I?m in the mood for [indian] food tonight, show me the
ones [within 5 miles] that have [patios].
Extracted Class and Labels
Domain Dialog Act Slots=Values
(I) Movie find Location=Austin
theater Movie-Name= iron man 2
(II) Restaurant find Rest-Cusine=indian
restaurant Location=within 5 miles
Rest-Amenities= patios
Table 1: Examples of utterances with corresponding se-
mantic components, i.e., domain, dialog act, and slots.
is a challenging task not only because there are no
a priori constraints on what a user might say, but
also systems must generalize from a tractably small
amount of labeled training data. In this paper, we
argue that each of these components are interdepen-
dent and should be modeled simultaneously. We
build a joint understanding framework and introduce
a multi-layer context model for semantic representa-
tion of utterances of multiple domains.
Although different strategies can be applied,
typically a cascaded approach is used where
each semantic component is modeled sepa-
rately/sequentially (Begeja et al, 2004), focusing
less on interrelated aspects, i.e., dialog?s domain,
user?s intentions, and semantic tags that can be
shared across domains. Recent work on SLU
(Jeong and Lee, 2008; Wang, 2010) presents joint
modeling of two components, i.e., the domain and
slot or dialog act and slot components together.
Furthermore, most of these systems rely on labeled
training utterances, focusing little on issues such
as information sharing between the discourse and
word level components across different domains,
or variations in use of language. To deal with de-
330
pendency and language variability issues, a model
that considers dependencies between semantic
components and utilizes information from large
bodies of unlabeled text can be beneficial for SLU.
In this paper, we present a novel generative
Bayesian model that learns domain/dialog-act/slot
semantic components as latent aspects of text ut-
terances. Our approach can identify these semantic
components simultaneously in a hierarchical frame-
work that enables the learning of dependencies. We
incorporate prior knowledge that we observe in web
search query logs as constraints on these latent as-
pects. Our model can discover associations between
words within a multi-layered aspect model, in which
some words are indicative of higher layer (meta) as-
pects (domain or dialog act components), while oth-
ers are indicative of lower layer specific entities.
The contributions of this paper are as follows:
(i) construction of a novel Bayesian framework for
semantic parsing of natural language (NL) utter-
ances in a unifying framework in ?4,
(ii) representation of seed labeled data and informa-
tion from web queries as informative prior to design
a novel utterance understanding model in ?3 & ?4,
(iii) comparison of our results to supervised sequen-
tial and joint learning methods on NL utterances in
?5. We conclude that our generative model achieves
noticeable improvement compared to discriminative
models when labeled data is scarce.
2 Background
Language understanding has been well studied in
the context of question/answering (Harabagiu and
Hickl, 2006; Liang et al, 2011), entailment (Sam-
mons et al, 2010), summarization (Hovy et al,
2005; Daume?-III and Marcu, 2006), spoken lan-
guage understanding (Tur and Mori, 2011; Dinarelli
et al, 2009), query understanding (Popescu et al,
2010; Li, 2010; Reisinger and Pasca, 2011), etc.
However data sources in VPA systems pose new
challenges, such as variability and ambiguities in
natural language, or short utterances that rarely con-
tain contextual information, etc. Thus, SLU plays
an important role in allowing any sophisticated spo-
ken dialog system (e.g., DARPA Calo (Berry et al,
2011), Siri, etc.) to take the correct machine actions.
A common approach to building SLU framework
is to model its semantic components separately, as-
suming that the context (domain) is given a pri-
ori. Earlier work takes dialog act identification as
a classification task to capture the user?s intentions
(Margolis et al, 2010) and slot filling as a sequence
learning task specific to a given domain class (Wang
et al, 2009; Li, 2010). Since these tasks are con-
sidered as a pipeline, the errors of each component
are transfered to the next, causing robustness issues.
Ideally, these components should be modeled si-
multaneously considering the dependencies between
them. For example, in a local domain application,
users may require information about a sub-domain
(movies, hotels, etc.), and for each sub-domain, they
may want to take different actions (find a movie, call
a restaurant or book a hotel) using domain specific
attributes (e.g., cuisine type of a restaurant, titles for
movies or star-rating of a hotel). There?s been little
attention in the literature on modeling the dependen-
cies of SLU?s correlated structures.
Only recent research has focused on the joint
modeling of SLU (Jeong and Lee, 2008; Wang,
2010) taking into account the dependencies at learn-
ing time. In (Jeong and Lee, 2008), a triangular
chain conditional random fields (Tri-CRF) approach
is presented to model two of the SLU?s components
in a single-pass. Their discriminative approach rep-
resents semantic slots and discourse-level utterance
labels (domain or dialog act) in a single structure
to encode dependencies. However, their model re-
quires fully labeled utterances for training, which
can be time consuming and expensive to generate for
dynamic systems. Also, they can only learn depen-
dencies between two components simultaneously.
Our approach differs from the earlier work- in
that- we take the utterance understanding as a multi-
layered learning problem, and build a hierarchical
clustering model. Our joint model can discover
domain D, and user?s act A as higher layer latent
concepts of utterances in relation to lower layer la-
tent semantic topics (slots) S such as named-entities
(?New York?) or context bearing non-named enti-
ties (?vegan?). Our work resembles the earlier work
of PAM models (Mimno et al, 2007), i.e., directed
acyclic graphs representing mixtures of hierarchical
topic structures, where upper level topics are multi-
nomial over lower level topics in a hierarchy. In an
analogical way to earlier work, the D and A in our
331
approach represent common co-occurrence patterns
(dependencies) between semantic tags S (Fig. 2).
Concretely, correlated topics eliminate assignment
of semantic tags to segments in an utterance that
belong to other domains, e.g., we can discover that
?Show me vegan restaurants in San Francisco? has
a low probably of outputting a movie-actor slot. Be-
ing generative, our model can incorporate unlabeled
utterances and encode prior information of concepts.
3 Data and Approach Overview
Here we define several abstractions of our joint
model as depicted in Fig. 1. Our corpus mainly
contains NL utterances (?show me the nearest dim-
sum places?) and some keyword queries (?iron man
2 trailers?). We represent each utterance u as a vec-
tor wu of Nu word n-grams (segments), wuj , each
of which are chosen from a vocabulary W of fixed-
size V. We use entity lists obtained from web sources
(explained next) to identify segments in the corpus.
Our corpus contains utterances from KD=4 main
domains:? {movies, hotels, restaurants, events},
as well as out-of-domain other class. Each utterance
has one dialog act (A) associated with it. We assume
a fixed number of possible dialog acts KA for each
domain. Semantic Tags, slots (S) are lexical units
(segments) of an utterance, which we classify into
two types: domain-independent slots that are shared
across all domains, (e.g., location, time, year, etc.),
and domain-dependent slots, (e.g. movie-name,
actor-name, restaurant-name, etc.). For tractability,
we consider a fixed number of latent slot types KS .
Our algorithm assigns domain/dialog-act/slot labels
to each topic at each layer in the hierarchy using la-
beled data (explained in ?4.)
We represent domain and dialog act components
as meta-variables of utterances. This is similar to
author-topic models (Rosen-Zvi et al, 2004), that
capture author-topic relations across documents. In
that case, words are generated by first selecting an
author uniformly from an observed author list and
then selecting a topic from a distribution over words
that is specific to that author. In our model, each
utterance u is associated with domain and dialog
act topics. A word wuj in u is generated by first
selecting a domain and an act topic and then slot
topic over words of u. The domain-dependent slots
in utterances are usually not dependent on the di-
alog act. For instance, while ?find [hugo] trailer?
and ?show me where [hugo] is playing? have both
a movie-name slot (?hugo?), they have different di-
alog acts, i.e., find-trailer and find-movie, respec-
tively. We predict posterior probabilities for domain
P? (d ? D|u) dialog act P? (a ? A|ud) and slots
P? (sj ? S|wuj , d, sj?1) of words wuj in sequence.
To handle language variability, and hence dis-
cover correlation between hierarchical aspects of ut-
terances1, we extract prior information from two
web resources as follows:
Web n-Grams (G). Large-scale engines such as
Bing or Google log more than 100M search queries
each day. Each query in the search logs has an as-
sociated set of URLs that were clicked after users
entered a given query. The click information can
be used to infer domain class labels, and there-
fore, can provide (noisy) supervision in training do-
main classifiers. For example, two queries (?cheap
hotels Las Vegas? and ?wine resorts in Napa?),
which resulted in clicks on the same base URL (e.g.,
www.hotels.com) probably belong to the same do-
main (?hotels? in this case).
movie rest. hotel event other
?
G   
= P(d=hotel|w
j
=?room?)
d|wj
Given query logs, we
compile sets of in-domain
queries based on their
base URLs2. Then, for
each vocabulary item
wj ? W in our corpus, we calculate frequency of
wj in each set of in-domain queries and represent
each word (e.g., ?room?) as a discrete normalized
probability distribution ?jG over KD domains
{?d|jG }? ?
j
G. We inject them as nonuniform priors
over domain and dialog act parameters in ?4.
Entity Lists (E). We limit our model to a set
of named-entity slots (e.g., movie-name, restaurant-
name) and non-named entity slots (e.g., restaurant-
cuisine, hotel-rating). For each entity slot, we ex-
tract a large collection of entity lists through the url?s
on the web that correspond to our domains, such
as movie-names listed on IMDB, restaurant-names
on OpenTable, or hotel-ratings on tripadvisor.com.
1Two utterances can be intrinsically related but contain no
common terms, e.g., ?has open bar? and ?serves free drinks?.
2We focus on domain specific search engines such as
IMDB.com, RottenTomatoes.com for movies, Hotels.com and
Expedia.com for hotels, etc.
332
slot 
transition 
parameters
slot topics
dialog act 
topics 
!
A
domain specific 
act parameters
n-gram 
prior 
from
web query logs
entity 
prior 
from 
web documents
domain topics
domain 
parameters
Utterance
w w
+1
w
uj
movie restaurant hotel
menu 0.02 0.93 0.01
rooms 0.001 0.001 0.98
(?
G
) Web N-Gram Context Prior
(?
E
) Entity List Prior
V?D
w
uj
movie
name
restaurant
name
hotel 
name
hotel california 0.5 0.0 0.5
zucca 0.0 1.0 0.0
S
w
-1
S
+1
S
-1
D
A
!
D
!
S
K
S
?
G
"
S
K
S
topic-word 
parameters
?
E
M
D
M
A
M
S
Figure 1: Graphical model depiction of the MCM. D,A,S are
domain, dialog act and slot in a hierarchy, each consisting of
KD,KA,KS components. Shaded nodes indicate observed
variables. Hyper-parameters are omitted. Sample informative
priors over latent topics ?E and ?G are shown. Blue arrows
indicate frequency of vocabulary terms sampled for each topic.
We represent each entity list as observed nonuniform
priors ?E and inject them into our joint learning pro-
cess as V sparse multinomial distributions over la-
tent topics D, and S to ?guide? the generation of
utterances (Fig. 1 top-left table), explained in ?4.
4 Multi-Layer Context Model - MCM
The generative process of our multi-layer context
model (MCM) (Fig. 1) is shown in Algorithm 1. Each
utterance u is associated with d = 1..KD multino-
mial domain-topic distributions ?dD. Each domain d,
is represented as a distribution over a = 1, ..,KA
dialog acts ?daA (?
d
D ? ?
da
A ). In our MCM model, we
assume that each utterance is represented as a hidden
Markov model with KS slot states. Each state gen-
erates n-grams according to a multinomial n-gram
distribution. Once domain Du and act Aud topics
are sampled for u, a slot state topic Sujd is drawn
to generate each segment wuj of u by considering
the word-tag sequence frequencies based on a sim-
ple HMM assumption, similar to the content models
of (Sauper et al, 2011). Initial and transition prob-
ability distributions over the HMM states are sam-
pled from Dirichlet distribution over slots ?dsS . Each
slot state s generates words according to multino-
mial word distribution ?sS . We also keep track of the
frequency of vocabulary termswj?s in a V ?KD ma-
trixMD. Every time awj is sampled for a domain d,
we increment its count, a degree of domain bearing
words. Similarly, we keep track of dialog act and
slot bearing words in V ?KA and V ?KS matrices,
MA and MS (shown as red arrows in Fig 1). Being
Bayesian, each distribution ?dD, ?
ad
A , and ?
ds
S is sam-
pled from a Dirichlet prior distribution with different
parameters, described next.
Algorithm 1 Multi-Layer Context Model Generation
1: for each domain d? 1, ...,KD
2: draw domain dist. ?dD ? Dir(?
?
D)
?,
3: for each dialog-act a? 1, ...,KA
4: draw dialog act dist. ?daA ? Dir(?
?
A),
5: for each slot type s? 1, ...,KS
6: draw slot dist. ?dsS ? Dir(?
?
S).
7: endfor
8: draw ?sS ? Dir(?) for each slot type s? 1, ...,KS .
9: for each utterance u? 1, ..., |U | do
10: Sample a domain Du?Multi(?dD) and,
11: and act topic Aud?Multi(?daA ).
12: for words wuj , j ? 1, ..., Nu do
13: - Draw Sujd?Multi(?
Du,Su(j?1)d
S )
?.
14: - Sample wuj?Multi(?Sujd ).
15: end for
16: end for
? Dir(??D), Dir(?
?
A), Dir(?
?
S) are parameterized based on prior
knowledge.
? Here HMM assumption over utterance words is used.
In hierarchical topic models (Blei et al, 2003;
Mimno et al, 2007), etc., topics are represented
as distributions over words, and each document ex-
presses an admixture of these topics, both of which
have symmetric Dirichlet (Dir) prior distributions.
Symmetric Dirichlet distributions are often used,
since there is typically no prior knowledge favoring
one component over another. In the topic model lit-
erature, such constraints are sometimes used to de-
terministically allocate topic assignments to known
labels (Labeled Topic Modeling (Ramage et al,
2009)) or in terms of pre-learnt topics encoded as
prior knowledge on topic distributions in documents
(Reisinger and Pas?ca, 2009). Similar to previous
work, we define a latent topic per each known se-
mantic component label, e.g., five domain topics for
five defined domains. Different from earlier work
though, we also inject knowledge that we extract
from several resources including entity lists from
web search query click logs as well as seed labeled
training utterances as prior information. We con-
strain the generation of the semantic components of
our model by encoding prior knowledge in terms of
333
asymmetric Dirichlet topic priors ?=(?m1,...,?mK)
where each kth topic has a prior weight ?k=?mk,
with varying base measure m=(m1,...,mk) 3.
We update parameter vectors of Dirichlet domain
prior ?u?D ={(?D??
u1
D ),..., ?D??
uKD
D }, where ?D is
the concentration parameter for domain Dirichlet
distribution and ?uD={?
ud
D }
KD
d=1 is the base mea-
sure which we obtain from various resources. Be-
cause base measure updates are dependent on prior
knowledge of corpus words, each utterance u gets
a different base measure. Similarly, we update
the parameter vector of the Dirichlet dialog act
and slot priors ?u?A ={(?A??
u1
A ),...,(?A??
uKA
A )} and
?u?S ={(?S ??
u1
S ),...,(?S ??
uKS
S )} using base measures
?uA={?
ua
A }
KA
a=1 and ?Su={?
us
S }
KS
s=1 respectively.
Before describing base measure update for do-
main, act and slot Dirichlet priors, we explain the
constraining prior knowledge parameters below:
? Entity List Base Measure(?jE): Entity fea-
tures are indicative of domain and slots and MCM
utilizes these features while sampling topics. For
instance, entities hotel-name ?Hilton? and location
?New York? are discriminative features in classi-
fying ?find nice cheap double room in New York
Hilton? into correct domain (hotel) and slot (hotel-
name) clusters. We represent entity lists correspond-
ing to known domains as multinomial distributions
?jE , where each ?
d|j
E is the probability of entity-
word wj used in the domain d. Some entities may
belong to more than one domain, e.g., ?hotel Cali-
fornia? can either be a movie, or song or hotel name.
? Web n-Gram Context Base Measure (?jG):
As explained in ?3, we use the web n-grams as ad-
ditional information for calculating the base mea-
sures of the Dirichlet topic distributions. Normal-
ized word distributions ?jG over domains were used
as weights for domain and dialog act base measure.
? Corpus n-Gram Base Measure (?jC): Sim-
ilar to other measures, MCM also encodes n-gram
constraints as word-frequency features extracted
from labeled utterances. Concretely, we cal-
culate the frequency of vocabulary items given
domain-act label pairs from the training labeled ut-
terances and convert there into probability mea-
sures over domain-acts. We encode conditional
3See (Wallach, 2008) Chapter 3 for analysis of hyper-priors
on topic models.
probabilities {?ad|jC }??
j
C as multinomial distribu-
tions of words over domain-act pairs, e.g., ?ad|jC =
P(d=?restaurant?, a=?make-reservation?|?table?).
Base measure update: The ?-base measures are
used to shape Dirichlet priors ?u?D , ?
u?
A and ?
u?
S . We
update the base measures of each sampled domain
Du = d given each vocabulary wj as:
?djD =
{
?d|jE , ?
d|j
E > 0
?d|jG , otherwise
(1)
In (1) we assume that entities (E) are more indica-
tive of the domain compared to other n-grams (G)
and should be more dominant in sampling decision
for domain topics. Given an utterance u, we calcu-
late its base measure ?udD =(
?Nu
j ?
dj
D )/Nu.
Once the domain is sampled, we update the prior
weight of dialog acts Aud = a:
?ajA = ?
ad|j
C ? ?
d|j
G (2)
and slot components Sujd = s:
?sjS = ?
d|j
E (3)
Then we update their base measures for a given u as:
?uaA =(
?Nu
j ?
aj
A )/Nu and ?
us
S =(
?Nu
j ?
sj
S )/Nu.
4.1 Inference and Learning
The goal of inference is to predict the domain, user?s
act and slot distributions over each segment given
an utterance. The MCM has the following set of pa-
rameters: domain-topic distributions ?dD for each u,
the act-topic distributions ?daA for each domain topic
d of u, local slot-topic distributions for each do-
main ?S , and ?sS for slot-word distributions. Pre-
vious work (Asuncion et al, 2009; Wallach et al,
2009) shows that the choice of inference method has
negligible effect on the probability of testing doc-
uments or inferred topics. Thus, we use Markov
Chain Monte Carlo (MCMC) method,specifically
Gibbs sampling, to model the posterior distribution
PMCM(Du, Aud, Sujd|?u?D , ?
u?
A , ?
u?
S , ?) by obtaining
samples (Du, Aud, Sujd) drawn from this distribu-
tion. For each utterance u, we sample a domain Du
and act Aud and hyper-parameters ?D and ?A and
their base measures ?udD , ?
ua
A (from Eq. 1,2):
?dD =
Ndu + ?D?
ud
D
Nu + ?u?D
; ?daA =
Na|ud + ?
A?udD
Nud + ?u?A
(4)
The Ndu is the number of occurrences of domain
topic d in utterance u, Na|ud is the number of occur-
rences of act a given d in u. During sampling of a
334
slot state Sujd, we assume that utterance is generated
by the HMM model associated with the assigned
domain. For each segment wuj in u, we sample a
slot state Sujd given the remaining slots and hyper-
parameters ?S , ? and base measure ?usS (Eq. 3) by:
p(Sujd = s|w,Du,S?(ujd)?
u?
S , ?) ?
Nkujd + ?
Nk(.) + V ?
? (N
Du,Su(j?1)d
s + ?S?
us
S )?
NDu,sSu(j+1)d + I(Suj?1, s) + I(Suj+1, s) + ?S?
us
S
NDu,s(.) + I(Suj?1, s) +KD?
u?
S
(5)
The Nkujd is the number of times segment wuj is
generated from slot state s in all utterances as-
signed to domain topic d, NDu,s1s2 is the num-
ber of transitions from slot state s1 to s2, where
s1 ?{Su(j?1)d,Su(j+1)d}, I(s1, s2)=1 if slot s1=s2.
4.2 Semantic Structure Extraction with MCM
During Gibbs sampling, we keep track of the fre-
quency of draws of domain, dialog act and slot in-
dicating n-grams wj , in MD, MA and MS matri-
ces, respectively. These n-grams are context bearing
words (examples are shown in Fig.1.). For given u
the predicted domain d?u is determined by:
d?u = arg maxd P? (d|u) = arg maxd[?
d
D ?
?Nu
j=1
MjdD
MD
]
and predicted dialog act by arg maxa P? (a|ud
?):
a?u = arg maxa[?
d?a
A ?
?Nu
j=1
MjaA
MA
] (6)
For each segment wuj in u, its predicted slot are de-
termined by arg maxs P (sj |wuj , d
?, sj?1):
s?uj = arg maxs[p(Sujd? = s|.) ?
?Nu
j=1
ZjsS
ZS
] (7)
5 Experiments
We performed several experiments to evaluate our
proposed approach. Before presenting our results,
we describe our datasets as well as two baselines.
5.1 Datasets, Labels and Tags
Our dataset contains utterances obtained from di-
alogs between human users and our personal assis-
tant system. We use the transcribed text forms of
Domain Sample Dialog Acts (DAs) & Slots
movie DAs: find-movie/director/actor,buy-ticket
Slots: name, mpaa-rating (g-rated), date,
director/actor-name, award(oscar winning)...
hotel DAs: find-hotel, book-hotel,
Slots: name, room-type(double), amenities,
smoking, reward-program(platinum elite)...
restaurant DAs: find-restaurant, make-reservation,
Slots: opening-hour, amenities, meal-type,...
event DAs: find-event/ticket/performers, get-info..
Slots: name, type(concert), performer....
Table 2: List of domains, dialog acts and semantic slot
tags of utterance segments. Examples for some slots val-
ues are presented in parenthesis as italicized.
the utterances obtained from (acoustic modeling en-
gine) to train our models 4. Thus, our dataset con-
tains 18084 NL utterances, 5034 of which are used
for measuring the performance of our models. The
dataset consists of five domain classes, i.e, movie,
restaurant, hotel, event, other, 42 unique dialog acts
and 41 slot tags. Each utterance is labeled with a
domain, dialog act and a sequence of slot tags cor-
responding to segments in utterance (see examples
in Table 1). Table 2 shows sample dialog act and
slot labels. Annotation agreement, Kappa measure
(Cohen, 1960), was around 85%.
We pulled a month of web query logs and ex-
tracted over 2 million search queries from the movie,
hotel, event, and restaurant domains. We also used
generic web queries to compile a set of ?other? do-
main queries. Our vocabulary consists of n-grams
and segments (phrases) in utterances that are ex-
tracted using web n-grams and entity lists of ?3. We
extract distributions of n-grams and entities to inject
as prior weights for entity list base (?jE) and web
n-gram context base measures (?jG) (see ?4).
5.2 Baselines and Experiment Setup
We evaluated two baselines and two variants of our
joint SLU approach as follows:
? Sequence-SLU: A traditional approach to SLU
extracts domain, dialog act and slots as seman-
tic components of utterances using three sequential
models. Typically, domain and dialog act detec-
tion models are taken as query classification, where
a given NL query is assigned domain and act la-
bels. Among supervised query classification meth-
4We submitted sample utterances used in our models as ad-
ditional resource. Due to licensing issues, we will reveal the full
train/test utterances upon acceptance of our paper.
335
movie
restaurant
movie, theater, 
ticket, matinee, 
fandango 
menu, table, 
dinner, togo 
kids-friendly
chinese, coffee
D
1
D
2
find-movie
A
1
find-review
A
2
reservation
A
3
check-menu
A
4
movie-name
S
1
actor-name
S
2
iron man 2, 
hugo, muppets
descendants
rest-name
S
3
cuisine
S
4
S
k
tom hanks, 
angelina jolie, 
cameron
reviews, critics 
ratings, mpaa, 
breath-taking
scary, ticket  
iron-man 2, 
oscar winner
kid-friendly 
reserve, table
wait-time
menu, list, 
vine list, 
check, hotpot
nearest, 
city center, 
Vancouver, 
New York
amici, zucca 
new york 
bagel 
starbucks
chinese, 
vietnamese, 
italian, 
fast food
D
O
M
A
I
N
D
I
A
L
O
G
 
A
C
T
S
location
S
L
O
T
S
domain 
in-
dependent 
slots
Figure 2: Sample topics discovered by Multi-Layer Context
Model (MCM). Given samples of utterances, MCM is able to in-
fer a meaningful set of dialog act (A) and slots (S), falling into
broad categories of domain classes (D).
ods, we used the Adaboost, utterance classifica-
tion method that starts from a set of weak classifiers
and builds a strong classifier by boosting the weak
classifiers. Slot discovery is taken as a sequence la-
beling task in which segments in utterances are la-
beled (Li, 2010). For segment labeling we use Semi-
Markov Conditional Random Fields (Semi-CRF)
(Sarawagi and Cohen, 2004) method as a benchmark
in evaluating semantic tagging performance.
? Tri-CRF: We used Triangular Chain CRF (Jeong
and Lee, 2008) as our supervised joint model base-
line. It is a state-of-the art method that learns the
sequence labels and utterance class (domain or dia-
log act) as meta-sequence in a joint framework. It
encodes the inter-dependence between the slot se-
quence s and meta-sequence label (d or a) using a
triangular chain (dual-layer) structure.
? Base-MCM: Our first version injects an informa-
tive prior for domain, dialog act and slot topic dis-
tributions using information extracted from only la-
beled training utterances and inject as prior con-
straints (corpus n-gram base measure ?jC) during
topic assignments.
? WebPrior-MCM: Our full model encodes distri-
butions extracted from labeled training data as well
as structured web logs as asymmetric Dirichlet pri-
ors. We analyze performance gain by the informa-
tion from web sources (?jG and ?
j
E) when injected
into our approach compared to Base-MCM.
We inject dictionary constraints as features
to train supervised discriminative methods, i.e.,
boosting and Semi-CRF in Sequence-SLU, and
Tri-CRF models. For semantic tagging, dictionary
constraints apply to the features between individual
segments and their labels, and for utterance classifi-
cation (to predict domain and dialog acts) they apply
to the features between utterance and its label. Given
a list of dictionaries, these constraints specify which
label is more likely. For discriminative methods,
we use several named entities, e.g., Movie-Name,
Restaurant-Name, Hotel-Name, etc., non-named en-
tities, e.g., Genre, Cuisine, etc., and domain inde-
pendent dictionaries, e.g., Time, Location, etc.
We train domain and dialog act classifiers via
Icsiboost (Favre et al, 2007) with 10K iterations
using lexical features (up to 3-n-grams) and con-
straining dictionary features (all dictionaries). For
feature templates of sequence learners, i.e., Semi-
CRF and Tri-CRF, we use current word, bi-gram
and dictionary features. For Base-MCM and
WebPrior-MCM, we run Gibbs sampler for 2000
iterations with the first 500 samples as burn-in.
5.3 Evaluations and Discussions
We evaluate the performance of our joint model on
two experiments using two metrics. For domain and
dialog act detection performance we present results
in accuracy, and for slot detection we use the F1 pair-
wise measure.
Experiment 1. Encoding Prior Knowledge: A
common evaluation method in SLU tasks is to mea-
sure the performance of each individual semantic
model, i.e., domain, dialog act and semantic tagging
(slot filling). Here, we not only want to demon-
strate the performance of each component of MCM
but also their performance under limited amount of
labeled data. We randomly select subsets of labeled
training data U iL ? UL with different samples sizes,
niL ={? ?nL}, where nL represents the sample size
of UL and ?={10%,25%,..} is the subset percentage.
At each random selection, the rest of the utterances
are used as unlabeled data to boost the performance
of MCM. The supervised baselines do not leverage the
unlabeled utterances.
The results reported in Figure 3 reveal both
the strengths and some shortcomings of our ap-
proach. When the number of labeled data is
small (niL ?25%*nL), our WebPrior-MCM has
a better performance on domain and act predic-
tions compared to the two baselines. Compared to
Sequence-SLU, we observe 4.5% and 3% perfor-
mance improvement on the domain and dialog act
336
10 25 50 75 100
91
92
93
94
95
96
% Labeled Data
A
cc
u
ra
cy
%
Utterance Domain Performance
20 40 60 80 100
82
83
84
85
86
87
88
% Labeled Data
A
cc
u
ra
cy
%
Dialog Act Performance
20 40 60 80 100
65
70
75
80
85
% Labeled Data
F
-
M
ea
su
re
Semantic Tag (Slot) Performance
Sequence-SLU Tri-CRF Base-MCM WebPrior-MCM
Figure 3: Semantic component extraction performance measures for various baselines as well as our approach with different priors.
models, whereas our gain is 2.6% and 1.7% over
Tri-CRF models. As the percentage of labeled ut-
terances in training data increase, Tri-CRF perfor-
mance increases, however WebPrior-MCM is still
comparable with Sequence-SLU. This is because
we utilize domain priors obtained from the web
sources as supervision during generative process as
well as unlabeled utterances that enable handling
language variability. Adding labeled data improves
the performance of all models however supervised
models benefit more compared to MCM models.
Although WebPrior-MCM?s domain and dialog
act performances are comparable (if not better than)
the other baselines, it falls short on the semantic
tagging model. This is partially due to the HMM
assumption compared to the supervised conditional
model?s used in the other baselines, i.e., Semi-CRF
in Sequence-SLU and Tri-CRF). Our work can
be extended by replacing HMM assumption with
CRF based sequence learner to enhance the capa-
bility of the sequence tagging component of MCM.
Experiment 2. Less is More? Being Bayesian,
our model can incorporate unlabeled data at train-
ing time. Here, we evaluate the performance gain on
domain, act and slot predictions as more unlabeled
data is introduced at learning time. We use only 10%
of the utterances as labeled data in this experiment
and incrementally add unlabeled data (90% of la-
beled data are treated as unlabeled).
The results are shown in Table 3. n% (n=10,25,..)
unlabeled data indicates that the WebPrior-MCM
is trained using n% of unlabeled utterances along
with training utterances. Adding unlabeled data has
a positive impact on the performance of all three se-
Table 3: Performance evaluation results of
WebPrior-MCM using different sizes of unlabeled
utterances at learning time.
Unlabeled Domain Dialog Act Slot
% Accuracy Accuracy F-Measure
10% 94.69 84.17 52.61
25% 94.89 84.29 54.22
50% 95.08 84.39 56.58
75% 95.19 84.44 57.45
100% 95.28 84.52 58.18
mantic components when WebPrior-MCM is used.
The results show that our joint modeling approach
has an advantage over the other joint models (i.e.,
Tri-CRF) in that it can leverage unlabeled NL ut-
terances. Our approach might be usefully extended
into the area of understanding search queries, where
an abundance of unlabeled queries is observed.
6 Conclusions
In this work, we introduced a joint approach to
spoken language understanding that integrates two
properties (i) identifying user actions in multiple
domains in relation to semantic units, (ii) utilizing
large amounts of unlabeled web search queries that
suggest the user?s hidden intentions. We proposed a
semi-supervised generative joint learning approach
tailored for injecting prior knowledge to enhance the
semantic component extraction from utterances as a
unifying framework. Experimental results using the
new Bayesian model indicate that we can effectively
learn and discover meta-aspects in natural language
utterances, outperforming the supervised baselines,
especially when there are fewer labeled and more
unlabeled utterances.
337
References
A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. 2009.
On smoothing and inference for topic models. UAI.
S. Bangalore. 2006. Introduction to special issue of spo-
ken language understanding in conversational systems.
In Speech Conversation, volume 48, pages 233?238.
L. Begeja, B. Renger, Z. Liu D. Gibbon, and
B. Shahraray. 2004. Interactive machine learning
techniques for improving slu models. In Proceedings
of the HLT-NAACL 2004 Workshop on Spoken Lan-
guage Understanding for Conversational Systems and
Higher Level Linguistic Information for Speech Pro-
cessing.
Pauline M. Berry, Melinda Gervasio, Bart Peintner, and
Neil Yorke-Smith. 2011. Ptime: Personalized assis-
tance for calendaring. In ACM Transactions on Intel-
ligent Systems and Technology, volume 2, pages 1?40.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. In Educational and Psychological Measure-
ment, volume 20, pages 37?46.
H. Daume?-III and D. Marcu. 2006. Bayesian query fo-
cused summarization.
M. Dinarelli, A. Moschitti, and G. Riccardi. 2009. Re-
ranking models for spoken language understanding.
Proc. European Chapter of the Annual Meeting of the
Association of Computational Linguistics (EACL).
B. Favre, D. Hakkani-Tu?r, and Sebastien Cuendet.
2007. Icsiboost. http://code.google.come/
p/icsiboost.
S. Harabagiu and A. Hickl. 2006. Methods for using
textual entailment for question answering. pages 905?
912.
E. Hovy, C.Y. Lin, and L. Zhou. 2005. A be-based multi-
document summarizer with query interpretation. Proc.
DUC.
M. Jeong and G. G. Lee. 2008. Triangular-chain con-
ditional random fields. EEE Transactions on Audio,
Speech and Language Processing (IEEE-TASLP).
X. Li. 2010. Understanding semantic structure of noun
phrase queries. Proc. of the Annual Meeting of the
Association of Computational Linguistics (ACL).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency based compositional semantics.
A. Margolis, K. Livescu, and M. Osterdorf. 2010. Do-
main adaptation with unlabeled data for dialog act tag-
ging. In Proc. Workshop on Domain Adaptation for
Natural Language Processing at the the Annual Meet-
ing of the Association of Computational Linguistics
(ACL).
D. Mimno, W. Li, and A. McCallum. 2007. Mixtures
of hierarchical topics with pachinko allocation. Proc.
ICML.
A. Popescu, P. Pantel, and G. Mishne. 2010. Semantic
lexicon adaptation for use in query interpretation. 19th
World Wide Web Conference (WWW-10).
D. Ramage, D. Hall, R. Nallapati, and C. D. Man-
ning. 2009. Labeled lda: A supervised topic model
for credit attribution in multi-labeled corpora. Proc.
EMNLP.
J. Reisinger and M. Pas?ca. 2009. Latent variable models
of concept-attribute attachement. Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL).
J. Reisinger and M. Pasca. 2011. Fine-grained class la-
bel markup of search queries. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL).
M. Sammons, V. Vydiswaran, and D. Roth. 2010. Ask
not what textual entailment can do for you... In Proc.
of the Annual Meeting of the Association of Computa-
tional Linguistics (ACL), Uppsala, Sweden, 7.
S. Sarawagi and W. W. Cohen. 2004. Semimarkov
conditional random fields for information extraction.
Proc. NIPS.
C. Sauper, A. Haghighi, and R. Barzilay. 2011. Content
models with attitude. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL).
G. Tur and R. De Mori. 2011. Spoken language under-
standing: Systems for extracting semantic information
from speech. Wiley.
H. Wallach, D. Mimno, and A. McCallum. 2009. Re-
thinking lda: Why priors matter. NIPS.
H. Wallach. 2008. Structured topic models for language.
Ph.D. Thesis, University of Cambridge.
Y.Y. Wang, R. Hoffman, X. Li, and J. Syzmanski.
2009. Semi-supervised learning of semantic classes
for query understanding from the web and for the
web. In The 18th ACM Conference on Information and
Knowledge Management.
Y-Y. Wang. 2010. Strategies for statistical spoken lan-
guage understanding with small amount of data - an
emprical study. Proc. Interspeech 2010.
338
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 914?923,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semi-Supervised Semantic Tagging of Conversational Understanding
using Markov Topic Regression
Asli Celikyilmaz
Microsoft
Mountain View, CA, USA
asli@ieee.org
Dilek Hakkani-Tur, Gokhan Tur
Microsoft Research
Mountain View, CA, USA
dilek@ieee.org
gokhan.tur@ieee.org
Ruhi Sarikaya
Microsoft
Redmond, WA, USA
rusarika@microsoft.com
Abstract
Finding concepts in natural language ut-
terances is a challenging task, especially
given the scarcity of labeled data for learn-
ing semantic ambiguity. Furthermore,
data mismatch issues, which arise when
the expected test (target) data does not
exactly match the training data, aggra-
vate this scarcity problem. To deal with
these issues, we describe an efficient semi-
supervised learning (SSL) approach which
has two components: (i) Markov Topic
Regression is a new probabilistic model
to cluster words into semantic tags (con-
cepts). It can efficiently handle seman-
tic ambiguity by extending standard topic
models with two new features. First, it en-
codes word n-gram features from labeled
source and unlabeled target data. Sec-
ond, by going beyond a bag-of-words ap-
proach, it takes into account the inherent
sequential nature of utterances to learn se-
mantic classes based on context. (ii) Ret-
rospective Learner is a new learning tech-
nique that adapts to the unlabeled target
data. Our new SSL approach improves
semantic tagging performance by 3% ab-
solute over the baseline models, and also
compares favorably on semi-supervised
syntactic tagging.
1 Introduction
Semantic tagging is used in natural language un-
derstanding (NLU) to recognize words of seman-
tic importance in an utterance, such as entities.
Typically, a semantic tagging model require large
amount of domain specific data to achieve good
performance (Tur and DeMori, 2011). This re-
quires a tedious and time intensive data collection
and labeling process. In the absence of large la-
beled training data, the tagging model can behave
poorly on test data (target domain). This is usually
caused by data mismatch issues and lack of cover-
age that arise when the target data does not match
the training data.
To deal with these issues, we present a new
semi-supervised learning (SSL) approach, which
mainly has two components. It initially starts with
training supervised Conditional Random Fields
(CRF) (Lafferty et al, 2001) on the source train-
ing data which has been semantically tagged. Us-
ing the trained model, it decodes unlabeled dataset
from the target domain. With the data mismatch
issues in mind, to correct errors that the supervised
model make on the target data, the SSL model
leverages the additional information by way of a
new clustering method. Our first contribution is a
new probabilistic topic model, Markov Topic Re-
gression (MTR), which uses rich features to cap-
ture the degree of association between words and
semantic tags. First, it encodes the n-gram context
features from the labeled source data and the unla-
beled target data as prior information to learn se-
mantic classes based on context. Thus, each latent
semantic class corresponds to one of the seman-
tic tags found in labeled data. MTR is not invari-
ant to reshuffling of words due to its Markovian
property; hence, word-topic assignments are also
affected by the topics of the surrounding words.
Because of these properties, MTR is less sensitive
to the errors caused by the semantic ambiguities.
Our SSL uses MTR to smooth the semantic tag pos-
teriors on the unlabeled target data (decoded using
the CRF model) and later obtains the best tag se-
quences. Using the labeled source and automati-
914
cally labeled target data, it re-trains a new CRF-
model.
Although our iterative SSL learning model can
deal with the training and test data mismatch, it
neglects the performance effects caused by adapt-
ing the source domain to the target domain. In
fact, most SSL methods used for adaptation, e.g.,
(Zhu, 2005), (Daume?-III, 2010), (Subramanya et
al., 2010), etc., do not emphasize this issue. With
this in mind, we introduce a new iterative training
algorithm, Retrospective Learning, as our second
contribution. While retrospective learning itera-
tively trains CRF models with the automatically
annotated target data (explained above), it keeps
track of the errors of the previous iterations so as
to carry the properties of both the source and target
domains.
In short, through a series of experiments we
show how MTR clustering provides additional in-
formation to SSL on the target domain utter-
ances, and greatly impacts semantic tagging per-
formance. Specifically, we analyze MTR?s perfor-
mance on two different types of semantic tags:
named-entities and descriptive tags as shown in
Table 1. Our experiments show that it is much
harder to detect descriptive tags compared to
named-entities.
Our SSL approach uses probabilistic clustering
method tailored for tagging natural language utter-
ances. To the best of our knowledge, our work is
the first to explore the unlabeled data to iteratively
adapt the semantic tagging models for target do-
mains, preserving information from the previous
iterations. With the hope of spurring related work
in domains such as entity detection, syntactic tag-
ging, etc., we extend the earlier work on SSL part-
of-speech (POS) tagging and show in the experi-
ments that our approach is not only useful for se-
mantic tagging but also syntactic tagging.
The remainder of this paper is divided as fol-
lows: ?2 gives background on SSL and semantic
clustering methods, ?3 describes our new cluster-
ing approach, ?4 presents the new iterative learn-
ing, ?5 presents our experimental results and ?6
concludes our paper.
2 Related Work and Motivation
(I) Semi-Supervised Tagging. Supervised meth-
ods for semantic tagging in NLU require a large
number of in-domain human-labeled utterances
and gazetteers (movie, actor names, etc.), increas-
? Are there any [comedies] with [Ryan Gosling]?
? How about [oscar winning] movies by
[James Cameron]?
? Find [Woody Allen] movies similar to [Manhattan].
[Named Entities]
director: James Cameron, Woody Allen,...
actor: Ryan Gosling, Woody Allen,...
title: Manhattan, Midnight in Paris,...
[Descriptive Tags]
restriction: similar, suitable, free,rate,...
description: oscar winning, new release, gardening,...
genre: spooky, comedies, feel good, romance,...
Table 1: Samples of semantically tagged utter-
ances from movie domain, named-entities and de-
scriptive tags.
ing the need for significant manual labor (Tur and
DeMori, 2011). Recent work on similar tasks
overcome these challenges using SSL methods as
follows:
? (Wang et al, 2009; Li et al, 2009; Li,
2010; Liu et al, 2011) investigate web query
tagging using semi-supervised sequence models.
They extract semantic lexicons from unlabeled
web queries, to use as features. Our work dif-
fers from these, in that, rather than just detecting
named-entities, our utterances include descriptive
tags (see Table 1).
? Typically the source domain has different dis-
tribution than the target domain, due to topic shifts
in time, newly introduced features (e.g., until re-
cently online articles did not include facebook
?like? feature.), etc. Adapting the source domain
using unlabeled data is the key to achieving good
performance across domains. Recent adaptation
methods for SSL use: expectation minimization
(Daume?-III, 2010) graph-based learning (Chapelle
et al, 2006; Zhu, 2005), etc. In (Subramanya et
al., 2010) an efficient iterative SSL method is de-
scribed for syntactic tagging, using graph-based
learning to smooth POS tag posteriors. However,
(Reisinger and Mooney, 2011) argues that vector
space models, such as graph-learning, may fail to
capture the richness of word meaning, as simi-
larity is not a globally consistent metric. Rather
than graph-learning, we present a new SSL using
a probabilistic model, MTR, to cluster words based
on co-occurrence statistics.
?Most iterative SSL methods, do not keep track
of the errors made, nor consider the divergence
from the original model. (Lavoie et al, 2011) ar-
gues that iterative learning models should mitigate
new errors made by the model at each iteration by
915
keeping the history of the prior predictions. This
ensures that a penalty is paid for diverging from
the previous model?s predictions, which will be
traded off against the benefit of reducing classi-
fication loss. We present a retrospective SSL for
CRF, in that, the iterative learner keeps track of the
errors of the previous iterations so as to carry the
properties of both the source and target domains.
(II) Semantic Clustering. A common prop-
erty of several context-based word clustering tech-
niques, e.g., Brown clustering (Brown et al,
1992), Clustering by Committee (Pantel, 2003),
etc., is that they mainly cluster based on local con-
text such as nearby words. Standard topic models,
such as Latent Dirichlet Allocation (LDA) (Blei
et al, 2003), use a bag-of-words approach, which
disregards word order and clusters words together
that appear in a similar global context. Such mod-
els have been effective in discovering lexicons in
many NLP tasks, e.g., named-entity recognition
(Guo et al, 2009), word-sense disambiguation
(Boyd-Graber et al, 2007; Li et al, 2010), syntac-
tic/semantic parsing (Griffiths et al, 2005; Singh
et al, 2010), speaker identification (Nyugen et al,
2012), etc. Recent topic models consider word
sequence information in documents (Griffiths et
al., 2005; Moon et al, 2010). The Hidden Topic
Markov Model (HTMM) by (Gruber et al, 2005),
for instance, models sentences in documents as
Markov chains, assuming all words in a sentence
have the same topic. While MTR has a similar
Markovian property, we encode features on words
to allow each word in an utterance to sample from
any of the given semantic tags, as in ?what are
[scary]genre movies by [Hitchcock]director??.
In LDA, common words tend to dominate all
topics causing related words to end up in differ-
ent topics. In (Petterson et al, 2010), the vector-
based features of words are used as prior informa-
tion in LDA so that the words that are synonyms
end up in same topic. Thus, we build a seman-
tically rich topic model, MTR, using word context
features as side information. Using a smoothing
prior for each word-topic pair (instead of a con-
stant ? smoother), MTR assures that the words are
distributed over topics based on how similar they
are. (e.g., ?scary? and ?spooky?, which have sim-
ilar context features, go into the same semantic
tag, ?genre?). Thus, to best of our knowledge,
MTR is the first topic model to incorporate word
features while considering the sequence of words.
3 Markov Topic Regression - MTR
3.1 Model and Abstractions
LDA assumes that the latent topics of documents
are sampled independently from one of K topics.
MTR breaks down this independence assumption
by allowing Markov relations between the hidden
tags to capture the relations between consecutive
words (as sketched in Figure 1 and Algorithm 1).
(I) Semantic Tags (si): Each word wi of a
given utterance with Nj words, uj={wi}Nji=1?U ,
j=1,..|U |, from a set of utterances U , is associated
with a latent semantic tag (state) variable si?S,
where S is the set of semantic tags. We assume a
fixed K topics corresponding to semantic tags of
labeled data. In a similar way to HTMM (Gruber
et al, 2005) described for documents, MTR sam-
ples each si from a Markov chain that is specific
to its utterance uj . Each state si generates a word,
wi, based on the word-state co-occurrences. MTR
allows for sampling of consecutive words from
different tag clusters. The initial probabilities of
the latent states are sampled from a Dirichlet dis-
tribution over state variables, ?j , with ? hyper-
parameter for each uj .
(II) Tag Transition Indicator (?v): Given ut-
terance uj , the decision to sample a wi from a
new topic is determined by an indicator variable,
cj,i, that is sampled from a Binomial(?v=wi) dis-
tribution with a Beta conjugate prior. (There are v
binomials for each vocabulary term.) cj,i=1 sug-
gests that a new state be sampled from K possible
tags for the word wi in uj , and cj,i=0 suggests that
the state si of wi should be the same as the previ-
ous word?s latent state si?1. The first position of
the sequence is sampled from a new state, hence
cj,i=1=1.
(III) Tag Transition Base Measure (?): Prior
probability of a word given a tag should increase
the chances of sampling words from the correct se-
mantic tag. MTR constrains the generation of a tag
si given the previous tag si?1 and the current wi
based on cj,i by using a vocabulary specific Beta
prior, ?v?Beta(?v) 1, on each word in vocabulary
wv=1,..V . We inject the prior information on se-
mantic tags to define values of the base measure
?v using external knowledge from two sources:
(a) Entity Priors (?S): Prior probability on
named-entities and descriptive tags denoted as
1For each beta distribution we use symmetric
Beta(?v)=Beta(?=?v ,?=?v).
916
latent 
semantic tag
distribution over 
semantic tags
s
1
...
w
1
...
!
j
"
 c
2
 c
3
#
$
kv
%
kv
x
v
&
k
s
2
s
3
w
2
w
3
w
n
'
V
K
|U|
V
indicator for 
sampling 
semantic tags
vocabulary 
features
as prior
information
semantic tag 
dependent 
smoothing coefficient
semantic tag 
indicator 
parameter
prior on 
per-word 
state 
transitions
$
k
 ! Dir(%
kv
|x;&
k
)
!
k
 = exp(f(x;&
k
))
semantic tag 
distribution 
over tags
smoother for 
tag-word
pair
 c
N
j
s
N
j
Figure 1: The graph representation of the Markov
Topic Regression (MTR). To demonstrate hidden
state Markov Chain, the generation of each word
is explicitly shown (inside of the plate).
?S=p(si|si?1,wi=v,wi?1). We use web sources
(wiki pages on movies and urls such as imdb.com)
and labeled training data to extract entity lists that
correspond to the semantic tags of our domains.
We keep the frequency of each n-gram to convert
into (empirical) prior probability distribution.
(b) Language Model Prior (?W ): Probabilities
on word transitions denoted as ?W=p(wi=v|wi?1).
We built a language model using SRILM (Stol-
cke, 2002) on the domain specific sources such as
top wiki pages and blogs on online movie reviews,
etc., to obtain the probabilities of domain-specific
n-grams, up to 3-grams. The observed priors, ?S
and ?W , are used for calculating the base measure
? for each vocabulary wv as:
?si|si?1v =
{
?si|si?1,wi=vS , if ?si|si?1,wi=vS exists,
?wi=v,wi?1W , otherwise (1)
In Eq.(1), we assume that the prior on the se-
mantic tags, ?S , is more indicative of the deci-
sion for sampling a wi from a new tag compared
to language model posteriors on word sequences,
?W . Here we represent the base-measure (hyper-
parameter) of the semantic tag indicator variable,
which is not to be confused with a probability
measure 2
We update the indicator parameter via mean cri-
teria, ?v=wi=
?K
i,j=1?
si|sj
v=wi /(K2). If no prior on
2The base-measure used in Eq.(1) does not relate to a
back-off model in LM sense. Here, instead of using a
constant value for the hyper-parameters, we use probability
scores that we obtain from LM.
Algorithm 1 Markov Topic Regression
1: for each semantic tag topic sk, k ? 1, ...,K do
2: ? draw a topic mixture ?k ? Dir(?k|?k,x),
3: ? let ?k=exp(f(x;?k)); x={xv}Vlv=1, ?k? RVl4: for each word wv in vocabulary v ? 1, ..., V do
5: ? draw a tag indicator mixture ?v ? Beta(?),
6: for each utterance j ? 1, ..., |U | do
7: ?draw transition distribution ?sj ? Dir(?)
8: over states si and set cj1=1.
9: ?for words wi in uj , i? 1, ..., Nj do
10:  if i >1, toss a coin cj,i ? Binomial(?wi).
11:  If cj,i=1, draw si?Multi(?si,si?1j )?12: otherwise si=si?1.
13:  Sample wi?Multi(?si ).
? Markov assumption over utterance words is used (See Eq.(4)).
a specific word exists, a default value is used for
base measure, ?v=0.01.
(IV) Topic-Word Distribution Priors (?k):
Different from (Mimno et al, 2008), which uses
asymmetric hyper-parameters on document-topic
distributions, in MTR, we learn the asymmetric
hyper-parameters of the semantic tag-word distri-
butions. We use blocked Gibbs sampling, in which
the topic assignments sk and hyper-parameters
{?k}Kk=1 are alternately sampled at each Gibbs
sampling lag period g given all other variables. We
impose the prior knowledge on naturally related
words, such that if two words ?funny? and ?hilar-
ious? indicate the same given ?genre? class, then
their latent tag distributions should also be simi-
lar. We enforce this on smoothing parameter ?k,v,
e.g., ?k,?funny???k,?hilarious? for a given tag k as
follows:
At each g lag period of the Gibbs sampling, K
log-linear models with parameters, ?(g)k ?RM , is
trained to predict ?(g)kv ??k, for each wv of a tag
sk:
?(g)k = exp(f(xl;?
(g)
k )) (2)
where the log-linear function f is:
n(g)kv = f(xlv;?
(g)
k ) =
?
m
?(g)k,mxlv,m (3)
Here x?RV?M is the input matrix x, wherein
rows xv?RM represents M -dimensional scalar
vector of explanatory features on vocabulary
words. We use the word-tag posterior probabili-
ties obtained from a CRF sequence model trained
on labeled utterances as features. The x={xl,xu}
has labeled (l) and unlabeled (u) parts. The labeled
part contains Vl size vocabulary of which we know
the semantic tags, xl={(xl1,s1),...,(xlVl ,sVl)}. Atthe start of the Gibbs sampling, we designate the
917
K latent topics to the K semantic tags of our la-
beled data. Therefore, we assign labeled words to
their designated topics. This way we use observed
scalar counts of each labeled word v associated
with its semantic tag k, n(g)kv , as the output labelof its input vector, xlv; an indication of likelihood
of words getting sampled from the correspond-
ing semantic label sk. Since the impact of the
asymmetric prior is equivalent to adding pseudo-
counts to the sufficient statistics of the semantic
tag to which the word belongs, we predict the
pseudo-counts ?(g)kv using the scalar counts of the
labeled data, n(g)kv , based on the log-linear model
in Eq. (2). At g=0, we use ?(0)kv =28, if xv?X l; oth-
erwise ?(0)kv =2?2, commonly used values for largeand small ?. Note that larger ?-values indicate
correlation between the word and the topic.
3.2 Collapsed Sampler
The goal of MTR is to infer the degree of relation-
ship between a word v and each semantic tag k,
?kv. To perform inference we need two compo-
nents:
? a sampler which can draw from condi-
tional PMTR(sji=k|sji?1, s\ji, ?, ?i, ?ji), when
cj,i=1, where sji and sji?1 are the semantic
tags of the current wi=v of vocabulary v and
previous word wi?1 in utterance uj , and s\ji
are the semantic tag topics of all words except
for wi; and,
? an estimation procedure for (?kv, ?k) (see
?3.1).
We integrate out the multinomial and binomial pa-
rameters of the model: utterance-tag distributions
?j , binomial state transition indicator distribution
per each word ?v, and ?k for tag-word distribu-
tions. We use collapsed Gibbs sampling to re-
duce random components and model the posterior
distribution by obtaining samples (sji, cj,i) drawn
from this distribution. Under the Markov assump-
tion, for each word wi=v in a given utterance uj ,
if cj,i=1, we sample a new tag si=k given the
remaining tags and hyper-parameters ?k, ?, and
?si|si?1wi=v . Using the following parameters; n(si)ji ,
which is the number of words assigned to a seman-
tic class si=k excluding case i, and n(si?1)si is the
number of transitions from class si?1 to si, where
indicator I(si?1, si)=1 if slot si=si?1, the update
equation is formulated as follows:
p(sji = k|w, s?ji, ?, ?si|si?1wi ,?k) ?
n(si)ji + ?kwi
n(k)(.) +
?
v ?kv
? (n(si?1)si + ?)?
(n(si)si+1 + I(si?1, si) + I(si+1, si) + ?)
n(si)(.) + I(si?1, k) +K?
(4)
4 Semi-Supervised Semantic Labeling
4.1 Semi Supervised Learning (SSL) with
CRF
In (Subramanya et al, 2010), a new SSL method
is described for adapting syntactic POS tagging of
sentences in newswire articles along with search
queries to a target domain of natural language
(NL) questions. They decode unlabeled queries
from target domain (t) using a CRF model trained
on the POS-labeled newswire data (source do-
main (o)). The unlabeled POS tag posteriors are
then smoothed using a graph-based learning algo-
rithm. On graph, the similarities are defined over
sequences by constructing the graph over types,
word 3-grams, where types capture the local con-
text of words. Since CRF tagger only uses lo-
cal features of the input to score tag pairs, they
try to capture all the context with the graph with
additional context features on types. Later, using
viterbi decoding, they select the 1-best POS tag
sequence, s?j for each utterance uj . Graph-based
SSL defines a new CRF objective function:
?(t)n+1 =argmin
??RK{
??
j=1:l
log p(sj |uj ; ?(t)n ) + ???(t)n ?2
}
?
{
?
?l+u
j=l log pn(s?j |uj ; ?
(t)
n )
}
(5)
The first bracket in Eq.(5) is the loss on the la-
beled data and L2 regularization on parameters,
?(t)n , from nth iteration, same as standard CRF.
The last term is the loss on unlabeled data from
target domain with a hyper-parameter ? . They use
a small value for ? to enable the new model to be
as close as possible to the initial model trained on
source data.
4.2 Retrospective Semi-Supervised CRF
We describe a Retrospective SSL (R-SSL) train-
ing with CRF (Algorithm 2), using MTR as a
918
smoothing model, instead of a graph-based model,
as follows:
I. DECODING and SMOOTHING. The poste-
rior probability of a tag sji=k given a word wji
in unlabeled utterance uj from target domain (t)
p?n(j, i)=p?n(sji=k|wji; ?(t)n ), is decoded using the
n-th iteration CRF model. MTR uses the decoded
probabilities as semantic tag prior features on vo-
cabulary items. We generate a word-tag matrix of
posteriors, x?(0, 1)V?K , where K is the number
of semantic tags and V is the vocabulary size from
n-th iteration. Each row is aK dimensional vector
of tag posterior probabilities xv={xv1,. . . xvK} on
the vocabulary term, wv. The labeled rows xl of
the vocabulary matrix, x={xl,xu}, contain only
{0,1} values, indicating the word?s observed se-
mantic tags in the labeled data. Since a labeled
term wv can have different tags (e.g., ?clint east-
wood? may be tagged as actor-name and director-
name in the training data), ?Kk xvk?1 holds. The
x is used as the input matrix of the kth log-linear
model (corresponding to kth semantic tag (topic))
to infer the ? hyper-parameter of MTR in Eq. (2).
MTR generates smoothed conditional probabilities
?kv for each vocabulary term v given semantic tag
k.
II. INTERPOLATION. For each word wji=v
in unlabeled utterance uj , we interpolate tag
marginals from CRF and MTR for each semantic
tag sji = k:
q?n(sji|wij ; ?(t)n ) = pi
CRF posterior? ?? ?
p?n(sji|wij ; ?(t)n )
+(1? pi)
MTR????
?kv (6)
III. VITERBI. Using viterbi decoding over
the tag marginals, q?n(sji|wij ; ?(t)n ), and transition
probabilities obtained from the CRF model of n-th
iteration, we get p?n(s?j |uj ; ?(t)n ), the 1-best decode
s?j of each unlabeled utterance uj?Uun .
IV. RETROSPECTIVE SSL (R-SSL). After
we decode the unlabeled data, we re-train a new
CRF model at each iteration. Each iteration makes
predictions on the semantic tags of unlabeled data
with varying posterior probabilities. Motivated by
(Lavoie et al, 2011), we want the loss function to
have a dependency on the prior model predictions.
Thus, R-SSL encodes the history of the prior pre-
Algorithm 2 Retrospective Semi-Supervised CRF
Input: Labeled U l, and unlabeled Uu data.
Process: ?(o)n =crf-train(Ul) at n=0, n=n+1 ?.
While not converged
p?=posterior-decode(Uun ,?(o)n )
?=smooth-posteriors(p?) using MTR,
q?=interpolate-posteriors(p?,?),
Uun=viterbi-decode(q?)
?(t)n+1=crf-retrospective(U l, Uun ,. . . ,Uu1 ,?(t)n )
? (n):iteration, (t):target, (o):source domains.
dictions, as follows:
?(t)n+1 =argmin
??RK{
??
j=1:l
log p(sj |uj ; ?(t)n ) + ???(t)n ?2
}
{
??
j=1:(l+u)
max{0, p???n }
}
(7)
where, p???n =1 ? log hn(uj)p?n(s?j |uj ; ?(t)n ). The
first two terms are same as standard CRF. The
last term ensures that the predictions of the cur-
rent model have the same sign as the predic-
tions of the previous models (using labeled and
unlabeled data), denoted by a maximum margin
hinge weight, hn(uj)= 1n?1
?n?1
1 p?n(s?j |uj ; ?
(t)
n ).
It should also be noted that with MTR, the R-SSL
learns the word-tag relations by using features that
describe the words in context, eliminating the need
for additional type representation of graph-based
model. MTR provides a separate probability dis-
tribution ?j over tags for each utterance j, implic-
itly allowing for the same word v in separate utter-
ances to differ in tag posteriors ?kv.
5 Experiments
5.1 Datasets and Tagsets
5.1.1 Semantic Tagging Datasets
We focus here on audiovisual media in the movie
domain. The user is expected to interact by voice
with a system than can perform a variety of tasks
such as browsing, searching, querying informa-
tion, etc. To build initial NLU models for such
a dialog system, we used crowd-sourcing to col-
lect and annotate utterances, which we consider
our source domain. Given movie domain-specific
tasks, we asked the crowd about how they would
919
interact with the media system as if they were talk-
ing to a person.
Our data from target domain is internally col-
lected from real-use scenarios of our spoken dia-
log system. The transcribed text forms of these ut-
terances are obtained from speech recognition en-
gine. Although the crowd-sourced data is similar
to target domain, in terms of pre-defined user in-
tentions, the target domain contains more descrip-
tive vocabulary, which is almost twice as large as
the source domain. This causes data-mismatch is-
sues and hence provides a perfect test-bed for a
domain adaptation task. In total, our corpus has
a 40K semantically tagged utterances from each
source and target domains. There are around 15
named-entity and 10 descriptive tags. We sep-
arated 5K utterances to test the performance of
the semantic tagging models. The most frequent
entities are: movie-director (?James Cameron?),
movie-title (?Die Hard?), etc.; whereas top de-
scriptive tags are: genre (?feel good?), description
(?black and white?, ?pg 13?), review-rate (?epic?,
?not for me?), theater-location (?near me?,?city
center?), etc.
Unlabeled utterances similar to the movie do-
main are pulled from a month old web query logs
and extracted over 2 million search queries from
well-known sites, e.g., IMDB, Netflix, etc. We
filtered queries that are similar to our target set
that start with wh-phrases (?what?, ?who?, etc.) as
well as imperatives ?show?, ?list?, etc. In addition,
we extracted web n-grams and entity lists (see ?3)
from movie related web sites, and online blogs and
reviews. We collected around 300K movie review
and blog entries on the entities observed in our
data. We extract prior distributions for entities and
n-grams to calculate entity list ? and word-tag ?
priors (see ?3.1).
5.1.2 Syntactic Tagging Datasets
We use the Wall Street Journal (WSJ) section of
the Penn Treebank as our labeled source data. Fol-
lowing previous research, we train on sections 00-
18, comprised of 38,219 POS-tagged sentences.
To evaluate the domain adaptation (DA) approach
and to compare with results reported by (Subra-
manya et al, 2010), we use the first and second
half of QuestionBank (Judge et al, 2006) as our
development and test sets (target). The Question-
Bank contains 4000 POS-tagged questions, how-
ever it is difficult to tag with WSJ-trained tag-
gers because the word order is different than WSJ
and contains a test-set vocabulary that is twice
as large as the one in the development set. As
for unlabeled data we crawled the web and col-
lected around 100,000 questions that are similar
in style and length to the ones in QuestionBank,
e.g. ?wh? questions. There are 36 different tag
sets in the Penn dataset which includes tag la-
bels for verbs, nouns, adjectives, adverbs, modal,
determiners, prepositions, etc. More information
about the Penn Tree-bank tag set can be found here
(Marcus et al, 1993).
5.2 Models
We evaluated several baseline models on two
tasks:
5.2.1 Semantic Clustering
Since MTR provides a mixture of properties
adapted from earlier models, we present perfor-
mance benchmarks on tag clustering using: (i)
LDA; (ii) Hidden Markov Topic Model HMTM
(Gruber et al, 2005); and, (iii) w-LDA (Petterson
et al, 2010) that uses word features as priors in
LDA. When a uniform ? hyper-parameter is used
with no external information on the state transi-
tions in MTR, it reduces to a HMTM model. Sim-
ilarly, if no Markov properties are used (bag-of-
words), MTR reduces to w-LDA. Each topic model
uses Gibbs sampling for inference and parameter
learning. We sample models for 1000 iterations,
with a 500-iteration burn-in and a sampling lag of
10. For testing we iterated the Gibbs sampler us-
ing the trained model for 10 iterations on the test-
ing data.
5.2.2 SSL for Semantic/Syntactic Tagging
We evaluated three different baselines against our
SSL models:
? CRF: a standard supervised sequence tagging.
? Self-CRF: a wrapper method for SSL using
self-training. First a supervised learning algorithm
is used to build a CRF model based on the labeled
data. A CRF model is used to decode the unla-
beled data to generate more labeled examples for
re-training.
? SSL-Graph: A SSL model presented in (Sub-
ramanya et al, 2010) that uses graph-based learn-
ing as posterior tag smoother for CRF model using
Eq.(5).
In addition to the three baseline, we evaluated
three variations of our SSL method:
? SSL-MTR: Our first version of SSL uses MTR to
920
LDA w-LDA HMTM MTR
0.6
0.7
0.8
0.9
82%
77%
84%
82%
79%78%
74% ? Descriptive Tags Named-Entities
 All Tags
F-M
eas
ure
Figure 2: F-measure for semantic clustering per-
formance. Performance differences for three dif-
ferent baseline models and our MTR approach by
different semantic tags.
smooth the semantic tag posteriors of a unlabeled
data decoded by the CRF model using Eq.(5).
? R-SSL-Graph: Our second version uses
graph-learning to smooth the tag posteriors and re-
train a new CRF model using retrospective SSL in
Eq.(7).
? R-SSL-MTR: Our full model uses MTR as a
Bayesian smoothing model, and retrospective SSL
in Eq.(7) for iterative CRF training.
For all the CRF models, we use lexical fea-
tures consisting of unigrams in a five-word win-
dow around the current word. To include contex-
tual information, we add binary features for all
possible tags. We inject dictionary constraints to
all CRF models, such as features indicating label
prior information. For each model we use sev-
eral named entity features, e.g., movie-title, actor-
name, etc., non-named entity (descriptive) fea-
tures, e.g., movie-description, movie-genre, and
domain independent dictionaries, e.g, time, loca-
tion, etc. For graph-based learning, we imple-
mented the algorithm presented in (Subramanya
et al, 2010) and used the same hyper-parameters
and features. For the rest of the hyper-parameters,
we used: ?=0.01 for MTR, pi=0.5 for interpolation
mixing. These parameters were chosen based on
the performance of the development set. All CRF
objective functions were optimized using Stochas-
tic Gradient Descent.
5.3 Results and Discussions
5.3.1 Experiment 1: Clustering Semantic
Tags.
Here, we want to demonstrate the performance
of MTR model for capturing relationships between
words and semantic tags against baseline topic
models: LDA, HMTM, w-LDA. We take the se-
mantically labeled utterances from the movie tar-
get domain and use the first half for training and
the rest for performance testing. We use all the
collected unlabeled web queries from the movie
domain. For fair comparison, each benchmark
topic model is provided with prior information on
word-semantic tag distributions based on the la-
beled training data, hence, each K latent topic is
assigned to one of K semantic tags at the begin-
ning of Gibbs sampling.
We evaluate the performance separately on de-
scriptive tags, named-entities, and all tags to-
gether. The performance of the four topic models
are reported in Figure 2. LDA shows the worst per-
formance, even though some supervision is pro-
vided by way of labeled semantic tags. Although
w-LDA improves semantic clustering performance
over LDA, the fact that it does not have Markov
properties makes it fall short behind MTR. As for
the effect of word features in MTR, we see a 3%
absolute performance gain over the second best
performing HMTM baseline on named-entity tags,
a 1% absolute gain on descriptive tags and a 2%
absolute overall gain. As expected, we see a drop
in F-measure on all models on descriptive tags.
5.3.2 Experiment 2: Domain Adaptation
Task.
We compare the performance of our SSL model
to that of state-of-the-art models on semantic and
syntactic tagging. Each SSL model is built us-
ing labeled training data from the source do-
main and unlabeled training data from target do-
main. In Table 2 we show the results on Movie
and QuestionBank target test datasets. The re-
sults of SSL-Graph on QuestionBank is taken
from (Subramanya et al, 2010). The self-
training model, Self-CRF adds 3% improve-
ment over supervised CRF models on movie do-
main, but does not improve syntactic tagging. Be-
cause it is always inherently biased towards the
source domain, self-training tends to reinforce
the knowledge that the supervised model already
has. SSL-Graph works much better for both
syntactic and semantic tagging compared to CRF
and Self-CRF models. Our Bayesian MTR ef-
ficiently extracts information from the unlabeled
data for the target domain. Combined with retro-
spective training, R-SSL-MTR demonstrates no-
ticeable improvements, ?2% on descriptive tags,
and 1% absolute gains in overall semantic tag-
921
ging performance over SSL-Graph. On syntac-
tic tagging, the two retrospective learning models
is comparable, close to 1% improvement over the
SSL-Graph and SSL-MTR.
Movie Domain QBank
Model Desc. NE All POS
CRF 75.05 75.84 75.84 83.80
Self-CRF 78.96 79.53 79.19 84.00
SSL-Graph 80.27 81.35 81.23 86.80
SSL-MTR 79.87 79.31 79.19 86.30
R-SSL-Graph 80.58 81.95 81.52 87.12
R-SSL-MTR 82.76 82.27 82.24 87.34
Table 2: Domain Adaptation performance
in F-measure on Semantic Tagging on
Movie Target domain and POS tagging on
QBank:QuestionBank. Best performing models
are bolded.
5.3.3 Experiment 3: Analysis of Semantic
Disambiguation.
Here we focus on the accuracy of our models in
tagging semantically ambiguous words. We inves-
tigate words that have more than one observed se-
mantic tag in training data, such as ?are there any
[war]genre movies available.?, ?remove all movies
about [war]description.?). Our corpus contained
30,000 unique vocabulary, 55% of which are con-
tained in one or more semantic categories. Only
6.5% of those are tagged as multiple categories
(polysemous), which are the sources of semantic
ambiguity. Table-3 shows the precision of two best
models for most confused words.
We compare our two best SSL models with dif-
ferent smoothing regularizes: R-SSL-MTR (MTR)
and R-SSL-Graph (GRAPH). We use preci-
sion and recall criterion on semantically confused
words.
In Table 3 we show two most frequent descrip-
tive tags; genre and description, and commonly
misclassified words by the two models. Results
indicate that the R-SSL-MTR, performs better
than the R-SSL-Graph, in activating the correct
meaning of a word. The results indicate that incor-
porating context information with MTR is an effec-
tive option for identifying semantic ambiguity.
6 Conclusions
We have presented a novel semi supervised learn-
ing approach using a probabilistic clustering
genre description
Vocab. GRAPH MTR GRAPH MTR
war 50% 100% 75% 88%
popular 90% 89% 80% 100%
kids 78% 86% ? 100%
crime 49% 80% 86% 67%
zombie 67% 89% 67% 86%
Table 3: Classification performance in F-measure
for semantically ambiguous words on the most fre-
quently confused descriptive tags in the movie do-
main.
method to semantically tag spoken language ut-
terances. Our results show that encoding priors
on words and context information contributes sig-
nificantly to the performance of semantic cluster-
ing. We have also described an efficient iterative
learning model that can handle data inconsisten-
cies that leads to performance increases in seman-
tic and syntactic tagging.
As a future work, we will investigate using ses-
sion data, namely the entire dialog between the
human and the computer. Rather than using sin-
gle turn utterances, we hope to utilize the con-
text information, e.g., information from previous
turns for improving the performance of the seman-
tic tagging of the current turns.
References
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
J. Boyd-Graber, D. Blei, and X. Zhu. 2007. A
topic model for word sense disambiguation. Proc.
EMNLP.
P.F. Brown, V.J.D. Pietra, P.V. deSouza, and J.C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
O. Chapelle, B. Schlkopf, and Alexander Zien. 2006.
Semi-supervised learning. MIT Press.
H. Daume?-III. 2010. Frustratingly easy semi-
supervised domain adaptation. Proc. Workshop on
Domain Adaptation for Natural Language Process-
ing at ACL.
T.L Griffiths, M. Steyvers, D.M. Blei, and J.M. Tenen-
baum. 2005. Integrating topics and syntax. Proc. of
NIPS.
A. Gruber, M. Rosen-Zvi, and Y. Weiss. 2005. Hidden
topic markov models. Proc. of ICML.
H. Guo, H. Zhu, Z. Guo, X. Zhang, X. Wu, and Z. Su.
2009. Domain adaptation with latent semantic asso-
ciation for named entity recognition. Proc. NAACL.
922
J. Judge, A. Cahill, and J.Van Genabith. 2006.
Question-bank: Creating corpus of parse-annotated
questions. Proc. Int. Conf. Computational Linguis-
tics and ACL.
A. Lavoie, M.E. Otey, N. Ratliff, and D. Sculley. 2011.
History dependent domain adaptation. Proc. NIPS
Workshop on Domain Adaptation.
X. Li, Y.-Y. Wang, and A. Acero. 2009. Extracting
structured information from user queries with semi-
supervised conditional random fields. Proc. of SI-
GIR.
L. Li, B. Roth, and C. Sporleder. 2010. Topic mod-
els for word sense disambiguation and token-based
idiom detection. Proc. ACL.
X. Li. 2010. Understanding semantic structure of noun
phrase queries. Proc. ACL.
J Liu, X. Li, A. Acero, and Ye-Yi Wang. 2011. Lex-
icon modeling for query understanding. Proc. of
ICASSP.
M. P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 27:1?30.
D. Mimno, W. Li, and A. McCallum. 2008.
Topic models conditioned on arbitrary features with
dirichlet-multinomial regression. Proc. UAI.
T. Moon, K. Erk, and J. Baldridge. 2010. Crouch-
ing dirichlet, hidden markov model: Unsupervised
pos tagging with context local tag generation. Proc.
ACL.
V.-A. Nyugen, J. Boyd-Graber, and P. Resnik. 2012.
Sits: A hierarchical nonparametric model using
speaker identity for topic segmentation in multiparty
conversations. Proc. ACL.
P. Pantel. 2003. Clustering by committee. Ph.D. The-
sis, University of Alberta, Edmonton, Alta., Canada.
J. Petterson, A. Smola, T. Caetano, W. Buntine, and
S. Narayanamurthy. 2010. Word features for latent
dirichlet alocation. In Proc. NIPS.
J. Reisinger and R. Mooney. 2011. Cross-cutting mod-
els of lexical semantics. In Proc. of EMNLP.
S. Singh, D. Hillard, and C. Leggetter. 2010.
Minimally-supervised extraction of entities from
text advertisements. Proc. NAACL-HLT.
A. Stolcke. 2002. An extensible language modeling
toolkit. Proc. Interspeech.
A. Subramanya, S. Petrov, and F. Pereira. 2010. Effi-
cient graph-based semi-supervised learning of struc-
tured tagging models. In Proc. EMNLP.
G. Tur and R. DeMori. 2011. Spoken language under-
standing: Systems for extracting semantic informa-
tion from speech. Wiley Press.
Y.-Y. Wang, R. Hoffman, X. Li, and J. Syzmanski.
2009. Semi-supervised learning of semantic classes
for query understanding from the web and for the
web. In The 18th ACM Conference on Information
and Knowledge Management.
X. Zhu. 2005. Semi-supervised learning litera-
ture survey. Technical Report 1530, University of
Wisconsin-Madison.
923
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 1?9,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
LDA Based Similarity Modeling for Question Answering
Asli Celikyilmaz
Computer Science Department
University of California, Berkeley
asli@eecs.berkeley.edu
Dilek Hakkani-Tur
International Computer
Science Institute
Berkeley, CA
dilek@icsi.berkeley.edu
Gokhan Tur
Speech Technology and
Research Laboratory
SRI International
Menlo Park, CA, USA
gokhan@speech.sri.com
Abstract
We present an exploration of generative mod-
eling for the question answering (QA) task to
rank candidate passages. We investigate La-
tent Dirichlet Allocation (LDA) models to ob-
tain ranking scores based on a novel similar-
ity measure between a natural language ques-
tion posed by the user and a candidate passage.
We construct two models each one introducing
deeper evaluations on latent characteristics of
passages together with given question. With
the new representation of topical structures on
QA datasets, using a limited amount of world
knowledge, we show improvements on perfor-
mance of a QA ranking system.
1 Introduction
Question Answering (QA) is a task of automatic
retrieval of an answer given a question. Typically
the question is linguistically processed and search
phrases are extracted, which are then used to retrieve
the candidate documents, passages or sentences.
A typical QA system has a pipeline structure start-
ing from extraction of candidate sentences to rank-
ing true answers. Some approaches to QA use
keyword-based techniques to locate candidate pas-
sages/sentences in the retrieved documents and then
filter based on the presence of the desired answer
type in candidate text. Ranking is then done using
syntactic features to characterize similarity to query.
In cases where simple question formulation is not
satisfactory, many advanced QA systems implement
more sophisticated syntactic, semantic and contex-
tual processing such as named-entity recognition
(Molla et al, 2006), coreference resolution (Vicedo
and Ferrandez, 2000), logical inferences (abduction
or entailment) (Harabagiu and Hickl, 2006) trans-
lation (Ma and McKeowon, 2009), etc., to improve
answer ranking. For instance, how questions, or spa-
tially constrained questions, etc., require such types
of deeper understanding of the question and the re-
trieved documents/passages.
Many studies on QA have focused on discrimina-
tive models to predict a function of matching fea-
tures between each question and candidate passage
(set of sentences), namely q/a pairs, e.g., (Ng et al,
2001; Echihabi and Marcu, 2003; Harabagiu and
Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz
et al, 2009). Despite their success, they have some
room for improvement which are not usually raised,
e.g., they require hand engineered features; or cas-
cade features learnt separately from other modules
in a QA pipeline, thus propagating errors. The struc-
tures to be learned can become more complex than
the amount of training data, e.g., alignment, entail-
ment, translation, etc. In such cases, other source
of information, e.g., unlabeled examples, or human
prior knowledge, should be used to improve perfor-
mance. Generative modeling is a way of encoding
this additional information, providing a natural way
to use unlabeled data.
In this work, we present new similarity measures
to discover deeper relationship between q/a pairs
based on a probabilistic model. We investigate two
methods using Latent Dirichlet Allocation (LDA)
(Blei, 2003) in ? 3, and hierarchical LDA (hLDA)
(Blei, 2009) in ? 4 to discover hidden concepts. We
present ways of utilizing this information within a
discriminative classifier in ? 5. With empirical ex-
periments in ? 6, we analyze the effects of gener-
ative model outcome on a QA system. With the
new representation of conceptual structures on QA
1
datasets, using a limited amount of world knowl-
edge, we show performance improvements.
2 Background and Motivation
Previous research have focused on improving mod-
ules of the QA pipeline such as question processing
(Huang et al, 2009), information retrieval (Clarke
et al, 2006), information extraction (Saggion and
Gaizauskas, 2006). Recent work on textual en-
tailment has shown improvements on QA results
(Harabagiu and Hickl, 2006), (Celikyilmaz et al,
2009), when used for filtering and ranking answers.
They discover similarities between q/a pairs, where
the answer to a question should be entailed by the
text that supports the correctness of its answer.
In this paper, we present a ranking schema fo-
cusing on a new similarity modeling approach via
generative and discriminative methods to utilize best
features of both approaches. Combinations of dis-
criminative and generative methodologies have been
explored by several authors, e.g. (Bouchard and
Triggs, 2004; McCallum et al, 2006; Bishop and
Lasserre, 2007; Schmah et al, 2009), in many fields
such as natural language processing, speech recog-
nition, etc. In particular, the recent ?deep learning?
approaches (Weston et al, 2008) rely heavily on a
hybrid generative-discriminative approach: an un-
supervised generative learning phase followed by a
discriminative fine-tuning.
In an analogical way to the deep learning meth-
ods, we discover relations between the q/a pairs
based on the similarities on their latent topics dis-
covered via Bayesian probabilistic approach. We in-
vestigate different ways of discovering topic based
similarities following the fact that it is more likely
that the candidate passage entails given question and
contains true answer if they share similar topics.
Later we combine this information in different ways
into a discriminative classifier-based QA model.
The underlying mechanism of our similarity mod-
eling approach is Latent Dirichlet Allocation (LDA)
(Blei et al, 2003b). We argue that similarities can
be characterized better if we define a semantic simi-
larity measure based on hidden concepts (topics) on
top of lexico-syntactic features. We later extend our
similarity model using a hierarchical LDA (hLDA)
(Blei et al, 2003a) to discover latent topics that are
organized into hierarchies. A hierarchical structure
is particularly appealing to QA task than a flat LDA,
in that one can discover abstract and specific topics.
For example, discovering that baseball and football
are both contained in a more abstract class sports
can help to relate to a general topic of a question.
3 Similarity Modeling with LDA
We assume that for a question posed by a user, the
document sets D are retrieved by a search engine
based on the query expanded from the question. Our
aim is to build a measure to characterize similar-
ities between a given question and each candidate
passage/sentence s ? D in the retrieved documents
based on similarities of their hidden topics. Thus,
we built bayesian probabilistic models on passage
level rather than document level to explicitly extract
their hidden topics. Moreover, the fact that there is
limited amount of retrieved documents D per ques-
tion (?100 documents) makes it appealing to build
probabilistic models on passages in place of docu-
ments and define semantically coherent groups in
passages as latent concepts. Given window size n
sentences, we define a passage as s = (|D| ?n) + 1
based on a n-sliding-window, where |D| is the to-
tal number of sentences in retrieved documents D.
There are 25+ sentences in documents, hence we ex-
tracted around 2500 passages for each question.
3.1 LDA Model for Q/A System
We briefly describe LDA (Blei et al, 2003b) model
as used in our QA system. A passage in retrieved
documents (document collection) is represented as a
mixture of fixed topics, with topic z getting weight
?(s)z in passage s and each topic is a distribution
over a finite vocabulary of words, with word w hav-
ing a probability ?(z)w in topic z. Placing symmet-
ric Dirichlet priors on ?(s) and ?(z), with ?(s) ?
Dirichlet(?) and ?(z) ? Dirichlet(?), where ?
and ? are hyper-parameters to control the sparsity
of distributions, the generative model is given by:
wi|zi, ?
(zi)
wi ? Discrete(?
(zi)), i = 1, ...,W
?(z) ? Dirichlet(?), z = 1, ...,K
zi|?(si) ? Discrete(?(si)), i = 1, ...,W
?(s) ? Dirichlet(?), s = 1, ..., S
(1)
2
where S is the number of passages discovered from
the document collection, K is the total number of
topics, W is the total number of words in the docu-
ment collection, and si and zi are the passage and the
topic of the ith word wi, respectively. Each word in
the vocabulary wi ? V = {w1, ...wW } is assigned
to each latent topic variable zi=1,...,W of words.
After seeing the data, our goal is to calculate the
expected posterior probabilities ??(zi)wi of a word wi
in a candidate passage given a topic zi = k and ex-
pected posterior probability ??(s) of topic mixings of
a given passage s, using the count matrices:
??(zi)wi =
nWKwik
+?
PW
j=1 n
WK
wjk
+W?
??(s) =
nSKsk +?PK
j=1 n
SK
sj +K?
(2)
where nWKwik is the count of wi in topic k, and n
SK
sk
is the count of topic k in passage s. The LDA model
makes no attempt to account for the relation of topic
mixtures, i.e., topics are distributed flat, and each
passage is a distribution over all topics.
3.2 Degree of Similarity Between Q/A via
Topics from LDA:
We build a LDA model on the set of retrieved pas-
sages s along with a given question q and calculate
the degree of similarity DESLDA(q,s) between each
q/a pair based on two measures (Algorithm 1):
(1) simLDA1 : To capture the lexical similarities
on hidden topics, we represent each s and q as
two probability distributions at each topic z =
k. Thus, we sample sparse unigram distributions
from each ??(z) using the words in q and s. Each
sparse word given topic distribution is denoted as
p(z)q = p(wq|z, ??(z)) with the set of words wq =
(w1, ..., w|q|) in q and ps = p(ws|z, ??(z)) with the
set of words ws = (w1, ..., w|s|) in s, and z = 1...K
represent each topic.
The sparse probability distributions per topic are
represented with only the words in q and s, and the
probabilities of the rest of the words in V are set
to zero. The W dimensional word probabilities is
the expected posteriors obtained from LDA model
(Eq.(2)), p(z)s = (??
(z)
w1 , ..., ??
(z)
w|s| , 0, 0, ..) ? (0, 1)
W ,
p(z)q = (??
(z)
w1 , ..., ??
(z)
w|q| , 0, 0, ..) ? (0, 1)
W . Given a
topic z, the similarity between p(z)q and p
(z)
s is mea-
sured via transformed information radius (IR). We
Posterior Topic-Word Distributions
q :
           
 
s :
z
1
.
.
.
.
.
w
5
.
w
4
w
1
w
6
w
2
w
7
w
3
z
2
.
.
.
.
.
w
5
.
w
4
w
1
w
6
w
2
w
7
w
3
z
K
.
.
.
.
.
w
5
.
w
4
w
1
w
6
w
2
w
7
w
3
...
(b) Magnified view of word given topic and topic given passage 
distributions showing  s={w
1
,w
2
,w
3
,w
4
,w
5
} and q={w
1
,w
2
,w
6,
w
7
}
(a) Snapshot of Flat Topic Structure of passages s 
for a question q on ?global warming?.
s: ?Global
1
 warming
2
 may rise
3
 incidence
4
 of malaria
5
.?
q: ?How does global
1
 warming
2
 effect
6
 humans
7
??
?
Posterior Passage- Topic Distributions
z
1
z
2 .........
z
K
z
?
(q)
z
1
z
2 .........
z
K
z
?
(s)
V
w
1
w
2
w
3
..w
5
w
6
w
7
....  
p
q
(z
1
)
V
p
s
(z
1
)
w
1
w
2
w
3
w
4
w
5
w
6
w
7
.... 
V
w
1
w
2
w
3
w
4
w
5
w
6
w
7
....  
p
q
(z
2
)
V
w
1
w
2
w
3
w
4
w
5
w
6
w
7
....  
p
q
(z
K
)
...
...
V
p
s
(z
K
)
w
1
w
2
w
3
w
4
w
5
w
6
w
7
....  
...
z
1warming
predict
healthdisease
forecast
temperature
malaria
sneeze
z
K
z
2
?
cooling
Topic Proportions
Topic-Word Distributions
V
w
1
w
2
w
3
w
4
w
5
w
6
w
7
....  
p
(z
2
)
s
Figure 1: (a) The topic distributions of a passage s and a
question q obtained from LDA. Each topic zk is a distri-
bution over words (Most probable terms are illustrated).
(b) magnified view of (a) demonstrating sparse distribu-
tions over the vocabulary V, where only words in passage
s and question q get values. The passage-topic distribu-
tions are topic mixtures, ?(s) and ?(q), for s and q.
first measure the divergence at each topic using IR
based on Kullback-Liebler (KL) divergence:
IR(p(z)q ,p(z)s )=KL(p(z)q ||
p
(z)
q +p
(z)
s
2 )+KL(p
(z)
s ||
p
(z)
q +p
(z)
s
2 )
(3)
where, KL(p||q) =
?
i pi log
pi
qi
. The divergence is
transformed into similarity measure (Manning and
Schutze, 1999):
W (p(z)q , p
(z)
s ) = 10??IR(p
(z)
q ,p
(z)
s )1 (4)
To measure the similarity between probability distri-
butions we opted for IR instead of commonly used
KL because with IR there is no problem with infinite
values since pq+ps2 6= 0 if either pq 6= 0 or ps 6= 0,
and it is also symmetric, IR(p,q)=IR(q,p). The simi-
larity of q/a pairs on topic-word basis is the average
1In experiments ? = 1 is used.
3
of transformed divergence over the entire K topics:
simLDA1 (q, s) =
1
K
?K
k=1W (p
(z=k)
q , p
(z=k)
s ) (5)
(2) simLDA2 : We introduce another measure based on
passage-topic mixing proportions in q and s to cap-
ture similarities between their topics using the trans-
formed IR in Eq.(4) as follows:
simLDA2 (q, s) = 10
?IR(??(q), ??(s)) (6)
The ??(q) and ??(s) are K-dimensional discrete topic
weights in question q and a passage s from Eq.(2).
In summary, simLDA1 is a measure of lexical simi-
larity on topic-word level and simLDA2 is a measure
of topical similarity on passage level. Together they
form the degree of similarity DESLDA(s, q) and are
combined as follows:
DESLDA(s,q)=simLDA1 (q,s)*sim
LDA
2 (q, s) (7)
Fig.1 shows sparse distributions obtained for sam-
ple q and s. Since the topics are not distributed hi-
erarchially, each topic distribution is over the entire
vocabulary of words in retrieved collection D. Fig.1
only shows the most probable words in a given topic.
Moreover, each s and q are represented as a discrete
probability distribution over all K topics.
Algorithm 1 Flat Topic-Based Similarity Model
1: Given a query q and candidate passages s ? D
2: Build an LDA model for the retrieved passages.
3: for each passages s ? D do
4: - Calculate sim1(q, s) using Eq.(5)
5: - Calculate sim2(q, s) using Eq.(6)
6: - Calculate degree of similarity between q and s:
7: DESLDA(q,s)=sim1(q, s) ? sim2(q, s)
8: end for
4 Similarity Modeling with hLDA
Given a question, we discover hidden topic distribu-
tions using hLDA (Blei et al, 2003a). hLDA orga-
nizes topics into a tree of a fixed depth L (Fig.2.(a)),
as opposed to flat LDA. Each candidate passage s is
assigned to a path cs in the topic tree and each word
wi in s is assigned to a hidden topic zs at a level
l of cs. Each node is associated with a topic dis-
tribution over words. The Gibbs sampler (Griffiths
and Steyvers, 2004) alternates between choosing a
new path for each passage through the tree and as-
signing each word in each passage to a topic along
that path. The structure of tree is learnt along with
the topics using a nested Chinese restaurant process
(nCRP) (Blei et al, 2003a), which is used as a prior.
The nCRP is a stochastic process, which assigns
probability distributions to infinitely branching and
deep trees. nCRP specifies a distribution of words in
passages into paths in an L-level tree. Assignments
of passages to paths are sampled sequentially: The
first passage takes the initial L-level path, starting
with a single branch tree. Next,mth subsequent pas-
sage is assigned to a path drawn from distribution:
p(pathold, c|m,mc) =
mc
?+m?1
p(pathnew, c|m,mc) =
?
?+m?1
(8)
pathold and pathnew represent an existing and novel
(branch) path consecutively, mc is the number of
previous passages assigned to path c, m is the to-
tal number of passages seen so far, and ? is a hyper-
parameter, which controls the probability of creating
new paths. Based on this probability each node can
branch out a different number of child nodes propor-
tional to ?. The generative process for hLDA is:
(1) For each topic k ? T , sample a distribution ?k v
Dirichlet(?).
(2) For each passage s in retrieved documents,
(a) Draw a path cs v nCRP(?),
(b) Sample L-vector ?s mixing weights from
Dirichlet distribution ?s ? Dir(?).
(c) For each word n, choose :
(i) a level zs,n|?s, (ii) a word ws,n| {zs,n, cs, ?}
Given passage s, ?s is a vector of topic propor-
tions from L dimensional Dirichlet parameterized
by ? (distribution over levels in the tree.) The
nth word of s is sampled by first choosing a level
zs,n = l from the discrete distribution ?s with prob-
ability ?s,l. Dirichlet parameter ? and ? control the
size of tree effecting the number of topics. Large
values of ? favor more topics (Blei et al, 2003a).
Model Learning: Gibbs sampling is a common
method to fit the hLDA models. The aim is to ob-
tain the following samples from the posterior of: (i)
the latent tree T , (ii) the level assignment z for all
words, (iii) the path assignments c for all passages
conditioned on the observed words w.
Given the assignment of words w to levels z and
assignments of passages to paths c, the expected
4
(a) Snapshot of Hierarchical Topic Structure of 
passages s for a question q on ?global warming?.
z
1  
z
2  
z
3
z
z
1  
z
2  
z
3
z
Posterior Topic 
Distributions
v
z1
z
3
.
.
.
.
.
.
.
.
.
.
w
5
z
2
.
.
.
.
.
.
.
.
w
2
.
z
1
w
5
.
.
.
.
.
.
.
w
7
w
1
Posterior Topic-Word Distributions
candidate s
            
question q
(b) Magnified view of sample path c [z
1
,z
2
,z
3
] showing  
s={w
1
,w
2
,w
3
,w
4
,w
5
} and q={w
1
,w
2
,w
6,
w
7
}
...
z
1
z
K-1
z
K
z
4
z
2
z
3
human
warming
incidence
research
global
predict
health
change
disease
forecast
temperature
slow
malaria
sneeze
starving
middle-east
siberia
s: ?Global
1
 warming
2
 may rise
3
 incidence
4
 of malaria
5
.?
q: ?How does global
1
 warming
2
 effect
6
 humans
7
??
v
z1
v
z2
v
z2
v
z3
v
z3
w
1
w
5
w
6  
....   
 
 
w
2
 w
7
 ....  
w
5
  ....  
 
 
w
5
  ....  
w
6
w
1
w
5
w
6    
....  
.
 
 
w
2
 w
7
 ....  
.
p
s
z
p(w 
   
|z
1
, c )
s
,1
s
p(w 
     
|z
2
, c  )
q
,2
q
p(w 
     
|z
3
, c  )
q
,3
q
.
p
 q
z
p(w 
   
|z
2
, c )
s
,2
s
p(w 
   
|z
3
, c )
s
,3
s
p(w 
   
|z
1
, c )
q
,1
q
level:3
level:1
level:2
Figure 2: (a) A sample 3-level tree using hLDA. Each passage is associated with a path c through the hierarchy, where
each node zs = l is associated with a distribution over terms (Most probable terms are illustrated). (b) magnified view
of a path (darker nodes) in (a). Distribution of words in given passage s and a question (q) using sub-vocabulary of
words at each level topic vl. Discrete distributions on the left are topic mixtures for each passage, pzq and pzs .
posterior probability of a particular word w at a
given topic z=l of a path c=c is proportional to the
number of times w was generated by that topic:
p(w|z, c,w, ?) ? n(z=l,c=c,w=w) + ? (9)
Similarly, posterior probability of a particular topic
z in a given passage s is proportional to number of
times z was generated by that passage:
p(z|s, z, c, ?) ? n(c=cc,z=l) + ? (10)
n(.) is the count of elements of an array satisfying
the condition. Posterior probabilities are normalized
with total counts and their hyperparameters.
4.1 Tree-Based Similarity Model
The hLDA constructs a hierarchical tree structure
of candidate passages and given question, each of
which are represented by a path in the tree, and each
path can be shared by many passages/question. The
assumption is that passages sharing the same path
should be more similar to each other because they
share the same topics (Fig.2). Moreover, if a path
includes a question, then other passages on that path
are more likely to entail the question than passages
on the other paths. Thus, the similarity of a can-
didate passage s to a question q sharing the same
path is a measure of semantic similarity (Algorithm
2). Given a question, we build an hLDA model on
retrieved passages. Let cq be the path for a given
q. We identify the candidate passages that share the
same path with q, M = {s ? D|cs = cq}. Given
path cq and M , we calculate the degree of similarity
DEShLDA(s, q) between q and s by calculating two
similarity measures:
(1) simhLDA1 : We define two sparse (discrete) uni-
gram distributions for candidate s and question q at
each node l to define lexical similarities on topic
level. The distributions are over a vocabulary of
words generated by the topic at that node, vl ?
V . Note that, in hLDA the topic distributions at
each level of a path is sampled from the vocabu-
lary of passages sharing that path, contrary to LDA,
in which the topics are over entire vocabulary of
words. This enables defining a similarity measure
on specific topics. Given wq =
{
w1, ..., w|q|
}
, let
wq,l ? wq be the set of words in q that are gener-
ated from topic zq at level l on path cq. The discrete
unigram distribution pql = p(wq,l|zq = l, cq, vl) rep-
resents the probability over all words vl assigned to
topic zq at level l, by sampling only for words in
wq,l. The probability of the rest of the words in vl are
set 0. Similarly, ps,l = p(ws,l|zs, cq, vl) is the proba-
bility of words ws in s extracted from the same topic
(see Fig.2.b). The word probabilities in pq,l and ps,l
are obtained using Eq. (9) and then normalized.
The similarity between pq,l and ps,l at each level
is obtained by transformed information radius:
Wcq,l(pq,l, ps,l) = 10
?-IRcq,l(pq,l,ps,l) (11)
5
where the IRcq,l(pq,l, ps,l) is calculated as in Eq.(3)
this time for pq,l and ps,l (? = 1). Finally simhLDA1 is
obtained by averaging Eq.(11) over different levels:
simhLDA1 (q, s) =
1
L
?L
l=1 Wcq ,l(pq,l, ps,l) ? l (12)
The similarity between pq,l and ps,l is weighted by
the level l because the similarity should be rewarded
if there is a specific word overlap at child nodes.
Algorithm 2 Tree-Based Similarity Model
1: Given candidate passages s and question q.
2: Build hLDA on set of s and q to obtain tree T .
3: Find path cq on tree T and candidate passages
4: on path cq , i.e., M = {s ? D|cs = cq}.
5: for candidate passage s ?M do
6: Find DEShDLA(q, s) = simhLDA1 ? sim
hLDA
2
7: using Eq.(12) and Eq.(13)
8: end for
9: if s /?M , then DEShDLA(q, s)=0.
(2) simhLDA2 : We introduce a concept-base mea-
sure based on passage-topic mixing proportions to
calculate the topical similarities between q and s.
We calculate the topic proportions of q and s, rep-
resented by pzq = p(zq|cq) and pzs = p(zs|cq) via
Eq.(10). The similarity between the distributions is
then measured with transformed IR as in Eq.(11) by:
simhLDA2 (q, s) = 10
?IRcq(pzq ,pzs) (13)
In summary, simhLDA1 provides information about
the similarity between q and s based on topic-word
distributions, and simhLDA2 is the similarity between
the weights of their topics. The two measures are
combined to calculate the degree of similarity:
DEShLDA(q,s)=simhLDA1 (q,s)*sim
hLDA
2 (q, s) (14)
Fig.2.b depicts a sample path illustrating sparse uni-
gram distributions of a q and s at each level and their
topic proportions, pzq , and pzs . The candidate pas-
sages that are not on the same path as the question
are assigned DEShLDA(s, q) = 0.
5 Discriminitive Model for QA
In (Celikyilmaz et al, 2009), the QA task is posed
as a textual entailment problem using lexical and se-
mantic features to characterize similarities between
q/a pairs. A discriminative classifier is built to pre-
dict the existence of an answer in candidate sen-
tences. Although they show that semi-supervised
methods improve accuracy of their QA model un-
der limited amount of labeled data, they suggest that
with sufficient number of labeled data, supervised
methods outperform semi-supervised methods. We
argue that there is a lot to discover from unlabeled
text to help improve QA accuracy. Thus, we pro-
pose using Bayesian probabilistic models. First we
briefly present the baseline method:
Baseline: We use the supervised classifier
model presented in (Celikyilmaz et al, 2009) as
our baseline QA model. Their datasets, provided in
http://www.eecs.berkeley.edu/?asli/asliPublish.html,
are q/a pairs from TREC task. They define each
q/a pair as a d dimensional feature vector xi ? <d
characterizing entailment information between
them. They build a support vector machine (SVM)
(Drucker et al, 1997) classifier model to predict the
entailment scores for q/a pairs.
To characterize the similarity between q/a pairs
they use: (i) features represented by similarities
between semantic components, e.g., subject, ob-
ject, verb, or named-entity types discovered in q/a
pairs, and (ii) lexical features represented by lexico-
syntactic alignments such as n-gram word overlaps
or cause and entailment relations discovered from
WordNet (Miller, 1995). For a given question q, they
rank the candidate sentences s based on predicted
entailment scores from the classifier, TE(q, s).
We extend the baseline by using the degree of
similarity between question and candidate passage
obtained from LDA, DESLDA(q, s), as well as hLDA
DEShLDA(q, s), and evaluate different models:
Model M-1: Degree of Similarity as Rank
Scores: In this model, the QA is based on a fully
generative approach in which the similarity mea-
sures of Eq.(7) in ?3 and Eq.(14) in ?4 are used to
obtain ranking scores. We build two separate mod-
els, M-1.1 using DESLDA(q, s), and M-1.2 using
DEShLDA(q, s) as rank scores and measure accu-
racy by re-ranking candidate passages accordingly.
Given a question, this model requires training indi-
vidual LDA and hLDA models.
Model M-2: Interpolation Between
Classifier-Based Entailment Scores and Genera-
tive Model Scores: In this model, the underlying
6
mechanism of QA is the discriminative method
presented in baseline. We linearly combine the
probabilistic similarity scores from generative
models, DES scores in M-1, with the baseline
scores. We build two additional models to calculate
the final rank scores; M-2.1 using:
score(s|q) = a?TE(q, s)+b?DESLDA(q, s) (15)
and M-2.2 using:
score(s|q) = a?TE(q, s)+b?DEShLDA(q, s) (16)
where 0 ? a ? 1 and 0 ? b ? 1 and a + b = 1.
We find the optimum a? and b? based on the valida-
tion experiments on training dataset. The candidate
sentences are re-ranked based on these scores.
Model M-3: Degree of Similarity as Entail-
ment Features: Another way to incorporate the la-
tent information into the discriminitive QA model
is to utilize the latent similarities as explanatory
variables in the classifier model. Particularly we
build M-3.1 by using simLDA1 , sim
LDA
2 as well as
DESLDA(q, s) as additional features for the SVM, on
top of the the existing features used in (Celikyilmaz
et al, 2009). Similarly, we build M-3.2 by using
simhLDA1 , sim
hLDA
2 as well as DES
hLDA(q, s) as addi-
tional features to the SVM classifier model to predict
entailment scores. This model requires building two
new SVM classifier models with the new features.
6 Experiments and Discussions
We demonstrate the results of our experiments on
exploration of the effect of different generative mod-
els presented in ?5 on TREC QA datasets.
We performed experiments on the datasets used in
(Celikyilmaz et al, 2009). Their train dataset com-
poses of a set of 1449 questions from TREC-99-
03. For each question, the 5 top-ranked candidate
sentences are extracted from a large newswire cor-
pora (Acquaint corpus) through a search engine, i.e.,
Lucene 2. The q/a pairs are labeled as true/false de-
pending on the containment of the true answer string
in retrieved passages. Additionally, to calculate the
LDA and hLDA similarity measures for each candi-
date passage, we also extract around 100 documents
in the same fashion using Lucene and identify pas-
sages to build the probabilistic models. We calculate
2http://lucene.apache.org/java/
the probabilistic similarities, i.e., simLDA1 , sim
LDA
2 ,
simhLDA1 , sim
hLDA
2 , and the degree of similarity val-
ues, i.e., DESLDA(q, s) and DEShLDA(q, s) for
each of the 5 top-ranked candidate sentences in
training dataset at inference time. Around 7200 q/a
pairs are compiled accordingly.
The provided testing data contains a set of 202
questions from TREC2004 along with 20 candidate
sentences for each question, which are labeled as
true/false. To calculate the similarities for the 20
candidate sentences, we extract around 100 docu-
ments for each question and build LDA and hLDA
models. 4037 testing q/a pairs are compiled.
We report the retrieval performance of our mod-
els in terms of Mean Reciprocal Rank (MRR), top
1 (Top1) and top 5 prediction accuracies (Top5)
(Voorhees, 2004). We performed parameter opti-
mization during training based on prediction ac-
curacy to find the best C =
{
10?2, .., 102
}
and
? =
{
2?2, .., 23
}
for RBF kernel SVM. For the
LDA models we present the results with 10 top-
ics. In hLDA models, we use four levels for the
tree construction and set the topic Dirichlet hyper-
parameters in decreasing order of levels at ? =
{1.0, 0.75, 0.5, 0.25} to encourage as many terms in
the mid to low levels as the higher levels in the hi-
erarchy, for a better comparison between q/a pairs.
The nested CRP parameter ? is fixed at 1.0. We
evaluated n-sliding-window size of sentences in se-
quence, n = {1, 3, 5}, to compile candidate pas-
sages for probabilistic models (Table 1). The output
scores for SVM models are normalized to [0,1].
? As our baseline (in ?5), we consider supervised
classifier based QA presented in (Celikyilmaz et al,
2009). The baseline MRR on TREC-2004 dataset is
MRR=%67.6, Top1=%58, Top5=%82.2.
? The results of the new models on testing dataset
are reported in Table 1. Incorporating the genera-
tive model output to the classifier model as input
features, i.e., M-3.1 and M-3-2, performs con-
sistently better than the rest of the models and the
baseline, where MRR result is statistically signifi-
cant based on t-test statistics (at p = 0.95 confi-
dence level). When combined with the textual en-
tailment scores, i.e., M-2.1 and M-2.2, they pro-
vide a slightly better ranking, a minor improvement
compared to the baseline. However, using the gen-
erative model outcome as sole ranking scores in
7
Window-size 1-window 3-window 5-window
MRR categories MRR Top1 Top5 MRR Top1 Top5 MRR Top1 Top5
M
od
el
s
M-1.1 (with LDA) 42.7 30.2 64.4 42.1 30.2 64.4 42.1 30.2 64.4
M-1.1 (with hLDA) 55.8 45.5 71.0 55.8 45.5 71.0 54.9 45.5 71.0
M-2.1 (with LDA) 66.2 55.1 82.2 65.2 54.5 80.7 65.2 54.5 80.7
M-2.2 (with hLDA) 68.2 58.4 82.2 67.6 58.0 82.2 67.4 58.0 81.6
M-3.1 (with LDA) 68.0 61.0 82.2 68.0 58.1 82.2 68.2 58.1 82.2
M-3.2 (with hLDA) 68.4 63.4 82.2 68.3 61.0 82.2 68.3 61.0 82.2
Table 1: The MRR results of the models presented in ?5 on testing dataset (TREC 2004) using different window sizes
of candidate passages. The statistically significant model results in each corresponding MRR category are bolded.
Baseline MRR=%67.6, Top1=%58, Top5=%82.2.
M-1.1 and M-1.2 do not reveal as good results as
the other models, suggesting room for improvement.
? In Table 1, Top1 MRR yields better improve-
ment compared to the other two MRRs, especially
for models M-3.1 and M-3.2. This suggests that
the probabilistic model outcome rewards the can-
didate sentences containing the true answer by es-
timating higher scores and moves them up to the
higher levels of the rank.
? The analysis of different passage sizes suggest
that the 1-window size yields best results and no sig-
nificant performance improvement is observed when
window size is increased. Thus, the similarity be-
tween q/a pairs can be better explained if the candi-
date passage contains less redundant sentences.
? The fact that the similarity scores obtained from
the hLDA models are significantly better than LDA
models in Table 1 indicates an important property
of hierarchal topic models. With the hLDA specific
and generic topics can be identified on different lev-
els of the hierarchy. Two candidate passages can
be characterized with different abstract and specific
topics (Fig. 2) enabling representation of better fea-
tures to identify similarity measures between them.
Whereas in LDA, each candidate passage has a pro-
portion in each topic. Rewarding the similarities on
specific topics with the hLDA models help improve
the QA rank performance.
? In M-3.1 and M-3.2 we use probabilistic sim-
ilarities and DES as inputs to the classifier. In Table
2 we show the individual effects of these features on
the MRR testing performance along with other lexi-
cal and semantic features of the baseline. Although
the effect of each feature is comparable, the DESLDA
Features M-3.1 Features M-3.1
sim1LDA 67.7 sim1hLDA 67.8
sim2LDA 67.5 sim2hLDA 68.0
DESLDA 67.9 DEShLDA 68.1
Table 2: The MRR results of the similarity measures on
testing dataset (TREC 2004) when used as input features.
and DEShLDA features reveal slightly better results.
7 Conclusion and Future Work
In this paper we introduced a set of methods based
on Latent Dirichlet Allocation (LDA) to character-
ize the similarity between the question and the can-
didate passages, which are used as ranking scores.
The results of our experiments suggest that extract-
ing information from hidden concepts improves the
results of a classifier-based QA model.
Although unlabeled data exploration through
probabilistic graphical models can help to improve
information extraction, devising a machinery with
suitable generative models for the given natural lan-
guage task is a challenge. This work helps with
such understanding via extensive simulations and
puts forward and confirms a hypothesis explaining
the mechanisms behind the effect of unsupervised
pre-training for the final discriminant learning task.
In the future, we would like to further evaluate
the models presented in this paper for larger datasets
and for different tasks such as question paraphrase
retrieval or query expansion. Moreover, we would
like to enhance the similarities with other semantic
components extracted from questions such as ques-
tion topic and question focus.
8
References
C. M. Bishop and J. Lasserre. Generative or dis-
criminative? getting the best of both worlds. In In
Bayesian Statistics 8, Bernardo, J. M. et al (Eds),
Oxford University Press, 2007.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
Hierarchical topic models and the nested chinese
restaurant process. In In Neural Information Pro-
cessing Systems [NIPS], 2003a.
D. M. Blei, A. Ng, and M. Jordan. Latent dirichlet
allocation. In Jrnl. Machine Learning Research,
3:993-1022, 2003b.
G. Bouchard and B. Triggs. The tradeoff between
generative and discriminative classifiers. In Proc.
of COMPSTAT?04, 2004.
A. Celikyilmaz, M. Thint, and Z. Huang. Graph-
based semi-supervised learning for question an-
swering. In Proc. of the ACL-2009, 2009.
C.L.A. Clarke, G. V. Cormack, R. T. Lynam, and
E. L. Terra. Question answering by passage se-
lection. In In: Advances in open domain question
answering, Strzalkowski, and Harabagiu (Eds.),
pages 259?283. Springer, 2006.
H. Drucker, C.J.C. Burger, L. Kaufman, A. Smola,
and V. Vapnik. Support vector regression ma-
chines. In NIPS 9, 1997.
A. Echihabi and D. Marcu. A noisy-channel ap-
proach to question answering. In ACL-2003,
2003.
T. Griffiths and M. Steyvers. Finding scientific top-
ics. In PNAS, 101(Supp. 1): 5228-5235, 2004.
S. Harabagiu and A. Hickl. Methods for using tex-
tual entailment in open-domain question answer-
ing. In In Proc. of ACL-2006, pages 905?912,
2006.
Z. Huang, M. Thint, and A. Celikyilmaz. Investiga-
tion of question classifier in question answering.
In In EMNLP?09, 2009.
W.-Y. Ma and K. McKeowon. Where?s the verb?
correcting machine translation during question
answering. In In ACL-IJCNLP?09, 2009.
C. Manning and H. Schutze. Foundations of statis-
tical natural language processing. In MIT Press.
Cambridge, MA, 1999.
A. McCallum, C. Pal, G. Druck, and
X. Wang. Multi-conditional learning: Gen-
erative/discriminative training for clustering and
classification. In AAAI 2006, 2006.
G.A. Miller. Wordnet: A lexical database for en-
glish. In ACM, 1995.
D. Molla, M.V. Zaanen, and D. Smith. Named en-
tity recognition for question answering. In In
ALTW2006, 2006.
H.T. Ng, J.L.P. Kwan, and Y. Xia. Question answer-
ing using a large text database: A machine learn-
ing approach. In EMNLP-2001, 2001.
H. Saggion and R. Gaizauskas. Experiments in pas-
sage selection and answer extraction for ques-
tion answering. In In: Advances in open domain
question answering, Strzalkowski, and Harabagiu
(Eds.), pages 291?302. Springer, 2006.
T. Schmah, G. E Hinton, R. Zemel, S. L. Small,
and S. Strother. Generative versus discriminative
training of rbms for classification of fmri images.
In Proc. NIPS 2009, 2009.
Dan Shen and Dietrich Klakow. Exploring correla-
tion of dependency relation paths for answer ex-
traction. In Proc. of ACL-2006, 2006.
J.L. Vicedo and A. Ferrandez. Applying anaphora
resolution to question answering and information
retrieval systems. In In LNCS, volume 1846,
pages 344?355, 2000.
Ellen M. Voorhees. Overview of trec2004 question
answering track. 2004.
J. Weston, F. Rattle, and R. Collobert. Deep learning
via semi-supervised embedding. In ICML, 2008.
9
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 27?35,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Graph-Based Semi-Supervised Learning for Question Semantic Labeling
Asli Celikyilmaz
Computer Science Division
University of California, Berkeley
asli@berkeley.edu
Dilek Hakkani-Tur
International Computer Science Institute
Berkeley, CA
dilek@icsi.berkeley.edu
Abstract
We investigate a graph-based semi-supervised
learning approach for labeling semantic com-
ponents of questions such as topic, focus,
event, etc., for question understanding task.
We focus on graph construction to handle
learning with dense/sparse graphs and present
Relaxed Linear Neighborhoods method, in
which each node is linearly constructed from
varying sizes of its neighbors based on the
density/sparsity of its surrounding. With the
new graph representation, we show perfor-
mance improvements on syntactic and real
datasets, primarily due to the use of unlabeled
data and relaxed graph construction.
1 Introduction
One of the important steps in Question Answering
(QA) is question understanding to identify semantic
components of questions. In this paper, we inves-
tigate question understanding based on a machine
learning approach to discover semantic components
(Table 1).
An important issue in information extraction from
text is that one often deals with insufficient la-
beled data and large number of unlabeled data,
which have led to improvements in semi-supervised
learning (SSL) methods, e.g., (Belkin and Niyogi.,
2002b), (Zhou et al, 2004). Recently, graph based
SSL methods have gained interest (Alexandrescu
and Kirchhoff, 2007), (Goldberg and Zhu, 2009).
These methods create graphs whose vertices corre-
spond to labeled and unlabeled data, while the edge
weights encode the similarity between each pair of
data points. Classification is performed using these
graphs by scoring unlabeled points in such a way
What? ?? ?
other
film
? ?? ?
focus
introduced? ?? ?
event
Jar Jar Binks? ?? ?
topic
?
Semantic Components & Named-Entitiy Types
topic: ?Jar? (Begin-Topic); ?Jar? (In-Topic) ;
?Binks? (In-Topic)(HUMAN:Individual)
focus: ?film? (Begin-Focus) (DESCRIPTION:Definition)
action / event: ?introduced? (Begin-Event)
expected answer-type: ENTITY:creative
Table 1: Question Analysis - Semantic Components of a
sample question from TREC QA task.
that instances connected by large weights are given
similar labels. Such methods can perform well when
no parametric information about distribution of data
is available and when data is characterized by an un-
derlying manifold structure.
In this paper, we present a semantic component
labeling module for our QA system using a new
graph-based SSL to benefit from unlabeled ques-
tions. One of the issues affecting the performance
of graph-based methods (Maier and Luxburg, 2008)
is that there is no reliable approach for model se-
lection when there are too few labeled points (Zhou
et al, 2004). Such issues have only recently came
into focus (Wang and Zhang, 2006). This is some-
what surprising because graph construction is a
fundamental step. Rather than proposing yet an-
other learning algorithm, we focus on graph con-
struction for our labeling task, which suffers from
insufficient graph sparsification methods. Such
problems are caused by fixed neighborhood assign-
ments in k-nearest neighbor approaches, treating
sparse and denser regions of data equally or using
improper threshold assumptions in -neighborhood
27
graphs, yielding disconnected components or sub-
graphs or isolated singleton vertices. We propose
a Relaxed Linear Neighborhood (RLN) method to
overcome fixed k or  assumptions. RLN approx-
imates the entire graph by a series of overlapped
linear neighborhood patches, where neighborhood
N (xi) of any node xi is captured dynamically based
on the density/sparsity of its surrounding. Moreover,
RLN exploits degree of neighborhood during re-
construction method rather than fixed assignments,
which does not get affected by outliers, producing a
more robust graph, demonstrated in Experiment #1.
We present our question semantic component
model in section 3 with the following contributions:
(1) a new graph construction method for SSL,
which relaxes neighborhood assumptions yielding
robust graphs as defined in section 5,
(2) a new inference approach to enable learning
from unlabeled data as defined in section 6.
The experiments in section 7 yield performance im-
provement in comparison to other labeling methods
on different datasets. Finally we draw conclusions.
2 Related Work on Question Analysis
An important step in question analysis is extracting
semantic components like answer type, focus, event,
etc. The ?answer-type? is a quantity that a question
is seeking. A question ?topic? usually represents ma-
jor context/constraint of a question (?Jar Jar Binks?
in Table 1). A question ?focus? (e.g., film) denotes a
certain aspect (or descriptive feature) of a question
?topic?. To extract topic-focus from questions, (Ha-
jicova et al, 1993) used rule-based approaches via
dependency parser structures. (Burger, 2006) im-
plemented parsers and a mixture of rule-based and
learning methods to extract different salient features
such as question type, event, entities, etc. (Chai and
Jin, 2004) explored semantic units based on their
discourse relations via rule-based systems.
In (Duan et al, 2008) a language model is pre-
sented to extract semantic components from ques-
tions. Similarly, (Fan et al, 2008)?s semantic chunk
annotation uses conditional random fields (CRF)
(Lafferty et al, 2001) to annotate semantic chunks
of questions in Chinese. Our work aparts from
these studies in that we use a graph-based SSL
method to extract semantic components from unla-
beled questions. Graph-based methods are suitable
for labeling tasks because when two lexical units
in different questions are close in the intrinsic ge-
ometry of question forms, their semantic compo-
nents (labels) will be similar to each other. Labels
vary smoothly along the geodesics, i.e., manifold
assumption, which plays an essential role in SSL
(Belkin et al, 2006).
This paper presents a new graph construction to
improve performance of an important module of QA
when labeled data is sparse. We compare our re-
sults with other graph construction methods. Next,
we present the dataset construction for our semantic
component labeling model before we introduce the
new graph construction and inference for SSL.
3 Semantic Component Labeling
Each word (token) in a question is associated with
a label among a pre-determined list of semantic
tags. A question i is defined as a sequence of in-
put units (words/tokens) xi = (x1i, ..., xT i) ? X T
which are tagged with a sequence of class labels,
yi = (y1i, ..., yT i) ? YT , semantic components.
The task is to learn classifier F that, given a new
sequence xnew, predicts a sequence of class labels
ynew = F(xnew). Among different semantic com-
ponent types presented in previous studies, we give
each token a MUC style semantic label from a list of
11 labels.
(1) O: other;
(2) BT:begin-topic;
(3) IT:in-topic
(4) BF:begin-focus;
(5) IF:in-focus
(6) BE:begin-event;
(7) IE:in-event
(8) BCL:begin-clause
(9) ICL:in-clause
(10) BC:begin-complement
(11) IC:in-complement
More labels can be appended if necessary. The first
token of a component gets ?begin? prefix and con-
secutive words are given ?in? prefix, e.g., Jar (begin-
topic), Jar (in-topic), Binks (in-topic) in Table 1.
In graph-based SSL methods, a graph is con-
structed G = ?V, E?, where V = X is a vertex set,
E is an edge set, associated with each edge eij rep-
28
resents a relation between xi and xj . The task is to
assign a label (out of 11 possible labels) to each to-
ken of a question i, xti, t = 1, ..., T , T is the max
number of tokens in a given query. We introduce
a set of nodes for each token (xti), each represent-
ing a binary relation between that token and one of
possible tags (yti). A binary relation represents an
agreement between a given token and assigned label,
so our SSL classifier predicts the probability of true
relation between token and assigned label. Thus,
for each token, we introduce 11 different nodes us-
ing yk ? {O,BT,IT,BF,IF,BC,IC,BE,IE,BCL,ICL}.
There will be 11 label probability assignments ob-
tained from each of the 11 corresponding nodes. For
labeled questions, intuitively, only one node per to-
ken is introduced to the graph for known(true) to-
ken/label relations. We find the best question label
sequence via Viterbi algorithm (Forney, 1973).
3.1 Feature Extraction For Labeling Task
The following pre-processing modules are built for
feature extraction prior to graph construction.
3.1.1 Pre-Processing For Feature Extraction
Phrase Analysis(PA): Using basic syntactic anal-
ysis (shallow parsing), the PA module re-builds
phrases from linguistic structures such as noun-
phrases (NN), basic prepositional phrases (PP) or
verb groups (VG). Using Stanford dependency
parser (Klein and Manning, 2003), (Marneffe et al,
2006), which produces 48 different grammatical re-
lations, PA module re-constructs the phrases. For
example for the question in Table 1, dependency
parser generates two relations:
? nn(Binks-3, Jar-1) and nn(Binks-3, Jar-2),
PA reveals ?Jar Jar Binks? as a noun phrase re-
constructing the nn:noun compound modifier. We
also extract part of speech tags of questions via de-
pendency parser to be used for feature extraction.
Question Dependency Relations (QDR): Using
shallow semantics, we decode underlying Stanford
dependency trees (Marneffe et al, 2006) that em-
body linguistic relationships such as head-subject
(H-S), head-modifier (complement) (H-M), head-
object (H-O), etc. For example: ?How did Troops
enter the area last Friday?? is chunked as:
? Head (H): enter ? Object (O): area
? Subject (S): Troops ?Modifier (M): last Friday
Later, the feature functions (FF) are extracted based
on generalized rules such as S and O?s are usually
considered topic/focus, H is usually an event, etc.
3.1.2 Features for Token-Label Pairs
Each node vi in a graph G represents a relation of
any token(word) i, xti to its label yti, denoted as a
feature vector xti ? <d. A list of feature functions
are formed to construct multi-dimensional training
examples. We extract mainly first and second order
features to identify token-label relations, as follows:
Lexicon Features (LF): These features are over
words and their labels along with information about
words such as POS tags, etc. A sample first order
lexicon feature, z(yt, x1:T , t):
z =
{
1 if yt =(BE/IE) and POSxt=VB
0 otherwise
(1)
is set to 1, if its assigned label yt is of event type
(BE/IE) and word?s POS tag is VB(verb) (such
token-label assignment would be correct). A simi-
lar feature is set to 1 if a word has ?VB? as its POS
tag and it is a copula word, so it?s correct label can
only be ?O:other?. Nodes satisfying only this con-
straint and have a relation to ?O? label get the value
of ?1?. Similar binary features are: if the word is a
WH type (query word), if its POS tag is an article, if
its POS tag is NN(P)(noun), IN, etc.
Compound Features (CF): These features ex-
ploit semantic compound information obtained from
our PA and QDR modules, in which noun-phrases
are labeled as focus/topics, or verb-phrases as event.
For instance, if a token is part of a semantic com-
pound, e.g., subject, identified via our QDR mod-
ule, then for any of the 11 nodes generated for this
token, if token-label is other than ?O(Other)?, then
such feature would be 1, and 0 otherwise. Similarly,
if a word is part of a noun-phrase, then a node having
a relation to any of the labels other than ?O/BE/IE?
would be given the value 1, and 0 otherwise We
eliminate inclusion of some nodes with certain la-
bels such as words with ?NN? tags are not usually
considered events.
Probability Feature Functions (PFF): We cal-
culate word unigram and bigram frequencies from
training samples to extract label conditional prob-
abilities given a word, e.g., P(BT??IBM?), P(O-
BE??Who founded?). When no match is found in
29
unigram and bigram label conditional probability ta-
bles for testing cases, we use unigram and bigram
label probabilities given the POS tag of that word,
e.g., P(BT?NNP), P(O-BE??WP-VBD?). We ex-
tract 11 different features for each word correspond-
ing to each possible label to form the probability fea-
tures from unigram frequencies, max. of 11X11 fea-
tures for bigram frequencies, where some bigrams
are never seen in training dataset.
Second-Order Features (SOF): Such features
denote relation between a token, tag and tag?1, e.g.,:
z =
{
1 if yt?1 =BT, yt =IT and POSxt=NN
0 otherwise
(2)
which indicates if previous label is a start of a topic
tag (BT) and current POS tag is NN, then a node
with a relation to label ?In-Topic (IT)? would yield
value ?1?. For any given token, one should introduce
112 different nodes to represent a single property. In
experiments we found that only a limited number of
second order nodes are feasible.
4 Graph Construction for SSL
Let XL = {x1, ..., xl} be labeled question tokens
with associated labels YL = {y1, ..., yl}
T and XU =
{x1, ..., xu} be unlabeled tokens, X = XL ? XU .
A weighted symmetric adjacency matrix W is
formed in two steps with edges E in G whereWij ?
<nxn, and non-zero elements represent the edge
weight between vi and vj . Firstly, similarity be-
tween each pair of nodes is obtained by a measure
to create a full affinity matrix, A ? <nxn, using a
kernel function, Aij = k(xi, xj) as weight measure
(Zhou et al, 2004) wij ? <n?n:
wij = exp
(
??xi ? xj? /2?2
)
(3)
Secondly, based on chosen graph sparsification
method, a sparse affinity matrix is obtained by re-
moving edges that do not convey with neighborhood
assumption. Usually a k-nearest neighbor (kNN) or
 neighbor (N) methods are used for sparsification.
Graph formation is crucial in graph based SSL
since sparsity ensures that the predicted model re-
mains efficient and robust to noise, e.g., especially
in text processing noise is inevitable. N graphs pro-
vide weaker performance than the k-nearest neigh-
borhood graphs (Jebara et al, 2009). In addition,
the issue with kNN sparsification of graph is that the
number of neighbors is fixed at the start, which may
cause fault neighborhood assumptions even when
neighbors are far apart. Additionally, kernel simi-
larity functions may not rate edge weights because
they might be useful locally but not quite efficient
when nodes are far apart. Next, we present Relaxed
Linear Neighborhoods to address these issues.
5 Relaxed Linear Neighborhoods (RLN)
Instead of measuring pairwise relations (3), we use
neighborhood information to construct G. When
building a sparse affinity matrix, we re-construct
each node using a linear combination of its neigh-
bors, similar to Locally Linear Embedding (Roweis
and Saul, 2000) and Linear Neighborhoods (Wang
and Zhang, 2006), and minimize:
min
?
i ||xi ?
?
j:xj?N (xi)wijxj ||
2 (4)
where N (xi) is neighborhood of xi, and wij is the
degree of contribution of xj to xi. In (4) each node
can be optimally reconstructed using a linear combi-
nation of its neighborhood (Roweis and Saul, 2000).
However, having fixed k neighbors at start of the
algorithm can effect generalization of classifier and
can also cause confusion on different manifolds.
We present novel RLN method to reconstruct
each object (node) by using dynamic neighborhood
information, as opposed to fixed k neighbors of
(Wang and Zhang, 2006). RLN approximates entire
graph by a series of overlapped linear neighborhood
patches, where neighborhood N (xi) of a node xi is
captured dynamically via its neighbor?s density.
Boundary Detection: Instead of finding fixed k
neighbors of each node xi (Wang and Zhang, 2006),
RLN captures boundary of each node B(xi) based
on neighborhood information and pins each node
within this boundary as its neighbors. We define
weightW matrix using a measure like (3) as a first
pass sparsification. We identify neighbors for each
node xi ? X and save information in boundary ma-
trix, B. kNN recovers its k neighbors using a simi-
larity function, e.g., a kernel distance function, and
instantiates via:
Nxi;k(xj) =
{
1 d(xi, xj1) < d(xi, xj2)
0 otherwise
}
(5)
30
Figure 1: Neighborhood Boundary. Having same number
of neighbors (n=15), boundaries of x1 and x2 are similar
based on kNN (e.g., k=15), but dissimilar based on N .
Similarly, with the N approach the neighbors are
instantiated when they are at most  far away:
Nxi;(xj) =
{
1 d(xi, xj) < 
0 otherwise
}
(6)
Both methods have limitations when sparsity or den-
sity is to concern. For sparse regions, if we restrict
definition to k neighbors, thenN (xi) would contain
dissimilar points. Similarly, improper threshold val-
ues could result in disconnected components or sub-
graphs or isolated singleton vertices. -radius would
not define a graph because not every neighborhood
radius would have the same density (see Fig. 1).
Neighborhoods of two points (x1, x2) are different,
although they contain same number of nodes.
We can use both kNN and NN approaches to
define the neighborhood between any xi and xj as:
Nxi;k,(xj) =
{
1 |N(xi)| > k
Nxi;k(xj) otherwise
}
(7)
|N(xi)| denotes cardinality of -neighbors of xi,
and Nxi;k(xj) ? {0, 1} according to (5). Thus if
there are enough number of nodes in the  vicinity
(> k), then the boundary is identified. Otherwise
we use kNN . Boundary set of any xi is defined as:
B(xi) =
{
xj=1..n ? X
?
?
?INxi;k,(xj)=1
}
(8)
Relaxed Boundary Detection: Adjusting bound-
aries based on a neighborhood radius and density
might cause some problems. Specifically, if dense
regions (clusters) exist and parameters are set large
for sparse datasets, e.g., k and , then neighborhood
sets would include more (and even noisy) nodes
than necessary. Similarly, for low density regions
if parameters are set for dense neighborhoods, weak
neighborhood bonds will be formed to re-construct
via linear neighborhoods. An algorithm that can
handle a wide range of change interval would be
advantageous. It should also include information
provided by neighboring nodes closest to the corre-
sponding node, which can take neighborhood rela-
tion into consideration more sensitively. Thus we
extend neighborhood definition in (7) and (8) ac-
counting for sensitivity of points with varying dis-
tances to neighbor points based on parameter k > 0:
Nxi(xj) = max {(1? k (d(xi, xj)/dmax)) , 0}
(9)
dmax = maxxi,xj?X d(xi, xj)
d(xi, xj) =
??m
p=1(xip ? xjp)2
(10)
In (10) m is the max. feature vector dimension of
any xi, k plays a role in determining neighborhood
radius, such that it could be adjusted as follows:
1? k (/dmax) = 0? k = dmax/ (11)
The new boundary set of any given xi includes:
B(xi) = {xj=1..n ? X |Nxi(xj) ? [0, 1]} (12)
In the experiments, we tested our RLN approach
(9), 0 < Nxi(xj) < 1 for boundary detection, in
comparison to the static neighborhood assignments
where the number of neighbors, k is fixed.
(3) Graph Formation: Instead of measuring pair-
wise relations as in (3), we use neighborhood in-
formation to represent G. In an analogical man-
ner to (Roweis and Saul, 2000), (Wang and Zhang,
2006), for graph sparcification, for our Relaxed Lin-
ear Neighborhood, we re-construct each node using
a linear combination of its dynamic neighbors:
minw
?
i
?
?
?xi ?
?
j:xj?B(xi)Nxi(xj)wijxj
?
?
?
2
s.t.
?
j wij = 1, wij ? 0
(13)
where 0 < Nxi(xj) < 1 is the degree of neighbor-
hood to boundary set B(xi) andwij is degree of con-
tribution of xj to xi, to be predicted. ANxi(xj) = 0
means no edge link. To prevent negative weights,
and satisfy their normalization to unity, we used a
constraint in (13) for RLN.
Edge weights of G are found using above relaxed
boundary assumption, and relaxed neighborhood
31
method. A sparse relaxed weight matrix (W?)ij =
w?ij is formed representing different number of con-
nected edges for every node, which are weighted ac-
cording to their neighborhood density. Since wij is
constructed via linear combination of varying num-
ber of neighbors of each node, W? is used as the edge
weights of G. Next we form a regularization frame-
work in place of label propagation (LP).
6 Regularization and Inference
Given a set of token-label assignments X =
{x1, ..., xl, xl+1, ..., xn}, and binary labels of first l
points, Y = {y1, ..., yl, 0, .., 0}, the goal is to predict
if the label assignment of any token of a given test
question is true or false. Let F denote set of clas-
sifying functions defined on X , and ?f ? F a real
value fi to every point xi is assigned. At each it-
eration, any given data point exploits a part of label
information from its neighbors, which is determined
by RLN. Thus, predicted label of a node xi at t+1:
f t+1i = ?yi + (1? ?)
?
j Nxi(xj)wijf
t
j (14)
where xj ? Bxi, 0< ? <1 sets a portion of la-
bel information that xi gets from its local neighbors,
ft = (f t1, f
t
2, ..., f
t
n) is the prediction label vector at
iteration t and f0 = y. We can re-state (14) as:
ft+1 = ?yi + (1? ?)W?ft (15)
Each node?s label is updated via (15) until conver-
gence, which might be at t ? ?. In place of LP,
we can develop a regularization framework (Zhou et
al., 2004) to learn f. In graph-based SSL, a function
over a graph is estimated to satisfy two conditions:
(i) close to the observed labels , and (ii) be smooth
on the whole graph via following loss function:
argminQ(f) =
?n
i=1 (fi ? yi)
2+
?
?n
i,j=1
?
j:xj?B(xi) ?xi(xj) ?fi, fj?
(16)
where ?xi(xj) = Nxi(xj)w?ij . Setting gradient of
loss function Q(f) to zero, we obtain:
?fQ(f) = 2(Y? f)+?[(I??)+(I??)T ]f (17)
Relaxed weight matrix W? is normalized according
to constraint in (13), so as degree matrix, D =
?
j W?ij , and graph Laplacian, i.e., L = (D? ?
W?)/D? = I ? W? . Since f is a function on the man-
ifold and the graph is discretized form of a manifold
(Belkin and Niyogi, 2002a), f can also be regarded
as the discrete form of f , which is equivalent at the
nodes of graph. So the second term of (16) yields:
[(I?W?)+(I?W?)T ]f ? 2Lf ? [(I?W?)]f (18)
Hence optimum f? is obtained by new form of
derivative in (17) after replacing (18):
f? = (1? ?)
(
I ? ?W?
)?1
Y (19)
Most graph-based SSLs are transductive, i.e., not
easily expendable to new testing points. In (Delal-
leau et al, 2005) an induction scheme is proposed to
classify a new point xTe by
f?(xTe) =
?
i?L?U W?xifi/
?
i?L?U W?xi (20)
Thus, we use induction, where we can, to avoid re-
construction of the graph for new test points.
7 Experiments and Discussions
In the next, we evaluate the performance of the pro-
posed RLN in comparison to the other methods on
syntactic and real datasets.
Exp. 1. Graph Construction Performance:
Here we use a similar syntactic data in (Jebara et
al., 2009) shown in Fig.2.a, which contains two
clusters of dissimilar densities and shapes. We
investigate three graph construction methods, lin-
ear k-neighborhoods of (Roweis and Saul, 2000) in
Fig.2.b, b-matching(Jebara et al, 2009) in Fig.2.c
and RLN of this work in Fig.2.d using a dataset of
300 points with binary output values. b-matching
permits a given datum to select k neighboring points
but also ensures that exactly k points selects given
datum as their neighbor.
In each graph construction method Gaussian ker-
nel distance is used. Experiments are run 50 times
where at each fold only 2 labeled samples from op-
posite classes are used to predict the rest. The exper-
iments are repeated for different k, b and  values. In
Fig. 2, average of trials is shown when k, b are 10
and  >0.5. We also used the N approach but it did
not show any improvement over kNN approach.
32
Figure 2: Graph Construction Experiments. (a) Syntactic data. (b) linear k-neighborhood (c) b-matching (d) RLN.
In Fig. 2.d, RLN can separate two classes
more efficiently than the rest. Compared to the b-
matching approach, RLN clearly improves the ro-
bustness. There are more links between clusters in
other graph methods than RLN, which shows that
RLN can separate two classes much efficiently. Also
since dynamic number of edges are constructed with
RLN, unnecessary links are avoided, but for the rest
of the graph methods there are edges between far
away nodes (shown with arrows). In the rest of the
experiments, we use b-matching for benchmark as it
is the closest approach to the proposed RLN.
Exp. 2. Semantic Component Recognition:
We demonstrate the performance of the new RLN
with two sets of experiments for sequence labeling
of question recognition task. As a first step in un-
derstanding semantic components of questions, we
asked two annotators to annotate a random subsam-
ple of 4000 TREC factoid and description questions
obtained from tasks of 1999-2006. There are 11
predefined semantic categories (section 3), close to
280K labeled tokens. Annotators are told that each
question must have one topic and zero or one focus
and event, zero or more of the rest of the compo-
nents. Inter-tagger agreement is ? = 0.68, which
denotes a considerable agreement.
We trained models on 3500 random set of ques-
tions and reserved the rest of 500 for testing the per-
formance. We applied pre-processing and feature
selection of section 3 to compile labeled and unla-
beled training and labeled testing datasets. At train-
ing time, we performed manual iterative parameter
optimization based on prediction accuracy to find
the best parameter sets, i.e., k = {3, 5, 10, 20, 50},
 ? {0, 1}, distance = {linear, gaussion}.
We use the average loss (L?) per sequence (query)
other topic focus event rest
# Samples 1997 1142 525 264 217
CRF 0.935 0.903 0.823 0.894 0.198
b-matching 0.871 0.900 0.711 0.847 0.174
RLN 0.911 0.910 0.761 0.834 0.180
Table 2: Chunking accuracy on testing data. ?other?=O,
?topic?=BT+IT, ?focus? = BF+IF, ?event?= ?BE+IE?,
?rest?= rest of the labels, i.e., IE, BC, IC, BCL, ICL.
to evaluate the semantic chunking performance:
L? = 1N
?N
i=1
[
1
Li
?Li
j=1 I ((y?i)j 6= (yi)j)
]
(21)
where y? and y are predicted and actual sequence re-
spectively; N is the number of test examples; Li is
the length of ith sequence; I is the 0-1 loss function.
(1) Chunking Performance: Here, we investigate
the accuracy of our models on individual component
prediction. We use CRF, b-matching and our RLN
to learn models from labeled training data and eval-
uate performance on testing dataset. For RLN and
b-matching we use training as labeled and testing as
unlabeled dataset in transductive way to predict to-
ken labels. The testing results are shown in Table 2
for different group of components. The accuracy for
?topic? and ?focus? components are relatively high
compared to other components. Most of the errors
on the ?rest? labels are due to confusion with ?topic?
or ?focus?. On some components, i.e., topic, other,
RLN performed significantly better than b-matching
based on t-test statistics (at 95% confidence). No
statistical significance between CRF and RLN is ob-
served indicating that RLN?s good performance on
individual label scoring, as it shows that RLN can
be used efficiently for sequence labeling.
(2) Question Labeling Performance. Having
33
Labeled CRF SSL sCRF b-match RLN
1% 0.240 0.235 0.223 0.233 0.220
5% 0.222 0.218 0.215 0.203 0.189
10% 0.170 0.219 0.186 0.194 0.180
25% 0.173 0.196 0.175 0.174 0.170
50% 0.160 0.158 0.147 0.156 0.158
75% 0.140 0.163 0.138 0.160 0.155
100% 0.120 0.170 0.123 0.155 0.149
Table 3: Test Data Average Loss on graph construction
with RLN, b-matching, standard SSL with kNN as well
as CRF, CRF with Self Learning (sCRF).
demonstrated that RLN is an alternative method
to the standard sequence learning methods for
the question labeling task, next we evaluate per
sequence (question) performance, rather than in-
dividual label performance using unlabeled data.
Firstly, we randomly select subset of labeled train-
ing dataset, XiL ? XL with different sample sizes,
niL = 5% ? nL, 10% ? nL, 25% ? nL, 50% ? nL,
75% ? nL, 100% ? nL, where nL is the size of XL.
Thus, instead of fixing the number of labeled records
and varying the number of unlabeled points, we pro-
pose to fix the percentage of unlabeled points in
training dataset. We hypothetically use unselected
part of the labeled dataset as unlabeled data at each
random selection. We compare the result of RLN to
other graph based methods including standard SSL
(Zhu et al, 2003) using kNN, and b-matching. We
also build a CRF model using the same features
as RLN except the output information, which CRF
learns through probabilistic structure. In addition,
we implemente self training for CRF (sCRF), most
commonly known SSL method, by adding most con-
fident (x, f(x)) unlabeled data back to the data and
repeat the process 10 times. Table 3 reports average
loss of question recognition tasks on testing dataset
using these methods.
When the number of labeled data is small (niL <
25%nL), RLN has better performance compared to
the rest (an average of 7% improvement). The SSL
and sCRF performance is slightly better than CRF at
this stage. As expected, as the percentage of labeled
points in training is increased, the CRF outperforms
the rest of the models. However, observing no sta-
tistical significance between CRF, b-matching and
# Unlabeled tokens 25K 50K 75K 100K
Average Loss 0.150 0.146 0.141 0.139
Table 4: Average Loss Results for RLN graph based SSL
as unlabeled tokens is increased.
RLN up to 25-50% labeled points indicates RLNs
performance on unlabeled datasets. Thus, for se-
quence labeling, the RLN can be a better alternative
to known sequence labeling methods, when manual
annotation of the entire dataset is not feasible.
Exp. 3. Unlabeled Data Performance: Here
we evaluate the effect of the size of unlabeled data
on the performance of RLN by gradually increas-
ing the size of unlabeled questions. The assump-
tion is that as more unlabeled data is used, the model
would have additional spatial information about to-
ken neighbors that would help to improve its gener-
alization performance. We used the questions from
the Question and Answer pair dataset distributed by
Linguistic Data Consortium for the DARPA GALE
project (LDC catalog number: LDC2008E16). We
compiled 10K questions, consisting of 100K tokens.
Although the error reduction is small (Table 4),
the empirical results indicate that unlabeled data can
have positive effect on the performance of the RLN
method. As we introduce more unlabeled data, the
RLN performance is increased, which indicates that
there is a lot to discover from unlabeled questions.
8 Conclusions
In this paper, we presented a graph-based semi-
supervised learning method with a new graph con-
struction. Our new graph construction relaxes the
neighborhood assumptions yielding robust graphs
when the labeled data is sparse, in comparison to
previous methods, which set rigid boundaries. The
new algorithm is particularly appealing to question
semantic component recognition task, namely ques-
tion understanding, in that in this task we usually
deal with very few labeled data and considerably
larger unlabeled data. Experiments on question se-
mantic component recognition show that our semi-
supervised graph-based method can improve perfor-
mance by up to 7-10% compared to well-known se-
quence labeling methods, especially when there are
more unlabeled data than the labeled data.
34
References
A. Alexandrescu and K. Kirchhoff. 2007. Data-driven
graph construction for semi-supervised graph-based
learning in nlp. In Proc. of HLT 2007.
M. Belkin and P. Niyogi. 2002a. Laplacian eigenmaps
and spectral techniques for embedding and clustering.
In Advances in Neural Information Processing Sys-
tems.
M. Belkin and P. Niyogi. 2002b. Using manifold struc-
ture for partially labeled classification. In Proc. of
NIPS 2002.
M. Belkin, P. Niyogi, and V. Sindhwani. 2006. A ge-
ometric framework for learning from examples. In
Journal of Machine Learning Research.
J. D. Burger. 2006. Mitre?s qanda at trec-15. In Proc. of
the TREC-2006.
J.Y. Chai and R. Jin. 2004. Discourse structure for
context question answering. In Proc. of HLT-NAACL
2004.
O. Delalleau, Y. Bengio, and N.L. Roux. 2005. Efficient
non-parametric function induction in semi-supervised
learning. In Proc. of AISTAT-2005.
H. Duan, Cao Y, C.Y. Lin, and Y. Yu. 2008. Searching
questions by identifying question topic and question
focus. In Proc. of ACL-08.
S. Fan, Y. Zhang, W.W.Y. Ng, Xuan Wang, and X. Wang.
2008. Semantic chunk annotation for complex ques-
tions using conditional random field. In Coling 2008:
Proc. of Workshop on Knowledge and Reasoning for
Answering Questions.
GD. Forney. 1973. The viterbi algorithm. In Proc. of
IEEE 61(3), pages 269?278.
A. Goldberg and X. Zhu. 2009. Keepin? it real: Semi-
supervised learning with realistic tuning. In Proc.
of NAACL-09 Workshop on Semi-Supervised Learning
for NLP.
E. Hajicova, P. Sgall, and H. Skoumalova. 1993. Iden-
tifying topic and focus by an automatic procedure. In
Proc. of the EACL-1993.
T. Jebara, J. Wang, and S.F. Chang. 2009. Graph con-
struction and b-matching for semi-supervised learning.
In Proc. of ICML-09.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the ACL-2003, pages 423?430.
J.D. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of 18th
International Conf. on Machine Learning (ICML?01).
M. Maier and U.V. Luxburg. 2008. Influence of graph
construction on graph-based clustering measures. In
Proc. of Neural Infor. Proc. Sys. (NIPS 2008).
M.-C.D. Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed-dependency parsers from
phrase structure parsers. In In LREC2006.
S.T. Roweis and L.K. Saul. 2000. Nonlinear dimension-
ality reduction by locally embedding. In Science, vol-
ume 290, pages 2323?2326.
F. Wang and C. Zhang. 2006. Label propagation through
linear neighborhoods. In Proc. of the ICML-2006.
Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Ja-
son Weston, and Bernhard Scho?lkopf. 2004. Learning
with local and global consistency. Advances in Neural
Information Processing Systems, 16:321?328.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani.
2003. Semi-supervised learning: From Gaussian
Fields to Gaussian processes. Technical Report CMU-
CS-03-175, Carnegie Mellon University, Pittsburgh.
35
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 37?40,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Mining Search Query Logs for Spoken Language Understanding
Dilek Hakkani-Tu?r, Gokhan Tu?r, Asli Celikyilmaz
Microsoft, Mountain View, CA 94041, USA
dilek|gokhan.tur|asli@ieee.org
Abstract
In a spoken dialog system that can handle nat-
ural conversation between a human and a ma-
chine, spoken language understanding (SLU)
is a crucial component aiming at capturing
the key semantic components of utterances.
Building a robust SLU system is a challeng-
ing task due to variability in the usage of lan-
guage, need for labeled data, and requirements
to expand to new domains (movies, travel, fi-
nance, etc.). In this paper, we survey recent
research on bootstrapping or improving SLU
systems by using information mined or ex-
tracted from web search query logs, which
include (natural language) queries entered by
users as well as the links (web sites) they click
on. We focus on learning methods that help
unveiling hidden information in search query
logs via implicit crowd-sourcing.
1 Introduction
Building a robust spoken dialog system involves hu-
man language technologies to cooperate to answer
natural language (NL) user requests. First user?s
speech is recognized using an automatic speech
recognition (ASR) engine. Then a spoken language
understanding (SLU) engine extracts their meaning
to be sent to dialog manager for taking the appropri-
ate system action.
Three key tasks of an SLU system are domain
classification, intent determination and slot filling
(Tur and Mori, 2011). While the state-of-the-art
SLU systems rely on data-driven methods, collect-
ing and annotating naturally spoken utterances to
train the required statistical models is often costly
and time-consuming, representing a significant bar-
rier to deployment. However, previous work shows
that it may be possible to alleviate this hurdle by
leveraging the abundance of implicitly labeled web
search queries in search engines. Large-scale en-
gines, e.g., Bing or Google, log more than 100M
queries every day. Each logged query has an associ-
ated set of URLs that were clicked after the users en-
tered the query. This information can be valuable for
building more robust SLU components, therefore,
provide (noisy) supervision in training SLU mod-
els. Take domain detection problem: Two users who
enter different queries but click on the same URL
(www.hotels.com) would probably be searching for
concepts in the same domain (?hotels? in this case).
The use of click information obtained through
massive search query click logs has been the fo-
cus of previous research. Specifically, query logs
have been used for building more robust web search
and better information retrieval (Pantel and Fuxman,
2011; Li et al, 2008), improve personalization expe-
rience and understand social networking behaviors
(Wang et al, 2011), etc. The use of query logs in
spoken dialog research is fairly new. In this paper,
we will survey the recent research on utilizing the
search query logs to obtain more accurate and ro-
bust spoken dialog systems, focusing on the SLU.
Later in the discussion section, we will discuss the
implimications on the dialog models.
The paper is organized as follows: In ? 2, we
briefly describe query click logs. We then summa-
rize recent research papers to give a snapshot of how
user search queries are being used in ? 3, and how
information from click-through graphs (queries and
37
movie theaters 
in san bruno
regal sunset 
square cinemas
the majestic 
crest theater
cinema in rosewell
rave cinemas
in eastfield
Queries Clicked URLs
moviefone.com
movies.eventful.com
yelp.com
amctheaters.com
find cheap tickets
for inception
movie ticket deals
watch movie
deals for inception
Queries Clicked URLs
fandango.com
movietickets.com
movie.yahoo.com
ticketmakers.com
movieworld carslton
ticket prices
1783
20
530
32
24
549
46
121
Figure 1: A sample query click graph. The squared queries
are samples from training data which are natural language ut-
terances. Edges include click frequencies from query to link.
clicked links) are exploited to boost the SLU perfor-
mance. Lastly, we discuss possible future directions.
2 What are Query Click Logs (QCL)?
QCL are logs of unstructured text including both the
users queries sent to a search engine and the links
that the users clicked on from the list of sites re-
turned by that search engine. A common representa-
tion of such data is a bi-partite query-click graph as
shown in (Fig 1), where one set of nodes represents
queries, and the other set of nodes represents URLs,
and an edge is placed between two nodes represent-
ing a query q and a URL u, if at least one user who
typed the q clicked on u.
Traditionally, the edge of the click graph is
weighted based on the raw click frequency (number
of clicks) from a query to a URL. Some of the chal-
lenges in extracting useful information from QCL is
that the feature space is high dimensional (there are
thousands of url clicks linked to many queries), and
there are millions of queries logged daily.
3 Exploiting NL Search Queries for SLU
Previous work on web search has benefited from the
use of query click logs for improving query intent
classification. Li et al use query click logs to de-
termine the domain of a query (typically keyword
search queries), and then infer the class member-
ships of unlabeled queries from those of the labeled
search queries using the URLs the users clicked (Li
et al, 2009; Li et al, 2008). QCL have been used to
extract named-entities to improve web search and ad
publishing experience (Hillard and Leggetter, 2010)
using (un)supervised learning methods on keyword
based search queries. Different from previous re-
search, in this paper we focus on recent research that
utilize NL search queries to boost the performance
of SLU components, i.e., domain detection, intent
determination, and slot filling.
In (Hakkani-Tur et al, 2011a), they use the search
query logs for domain classification by integrat-
ing noisy supervision into the semi-supervised la-
bel propagation algorithm, and sample high-quality
query click data. Specifically, they extract a set of
queries, whose users clicked on the URLs that are
related to their target domain categories. Then they
mine query click logs to get al instances of these
search queries and the set of links that were clicked
on by search engine users who entered the same
query. They compare two semi-supervised learn-
ing methods, self-training and label propagation, to
exploit the domain information obtained form the
URLs user have clicked on. The analysis indicate
that query sampling through semi-supervised learn-
ing enables extracting NL queries for use in domain
detection. They also argue that using raw queries
with and without the noisy labels in semi-supervised
learning reduces domain detection error rate by 20%
relative to supervised learning which uses only the
manually labeled examples.
The search queries found in click logs and the NL
spoken utterances are different in the sense that the
search queries are usually short and keyword based
compared to NL utterances that are longer and are
usually grammatical sentences (see Fig. 1). Hence,
in (Hakkani-Tur et al, 2012), they choose a statis-
tical machine translation (SMT) approach to search
query mining for SLU as sketched in Fig. 2. The
assumption is that, users typically have conceptual
intents underlying their requests when they inter-
act with web search engine or use a virtual assis-
tance system with built in SLU engine, e.g., ?avatar
awards? versus ?which awards did the movie avatar
win??. They translate NL queries into search queries
and mine similar search queries in QCL. They also
exploit QCL for bootstrapping domain detection
models, using only the NL queries hitting to seed
domain indicator URLs (Hakkani-Tur et al, 2011c).
Specifically, if one needs to detect a domain detector
for the hotels domain, the queries hitting hotels.com,
or tripadvisor.com, may be used to mine.
Query click logs have been explored for slot fill-
ing models as well. The slot filling models of SLU
38
Figure 2: Using natural language to query language translation
for mining query click logs.
aim to capture semantic components given the do-
main and a common way is to use gazetteer features
(dictionaries specific to domain such as movie-name
or actors in movie domain). In (Hillard et al, 2011),
they propose to mine and weight gazetteer entries
using query click logs. The gazetteer entries are
scored using a function of posterior probabilities for
that entry hitting a URL (compared to others URLs)
and for that URL being related to the target domain.
In such a schema the movie name ?gone with the
wind? gets higher score than the movie ?up?.
In (Tur et al, 2011), an unsupervised approach is
presented to implicitly annotate the training data us-
ing the QCL. Being unsupervised, this method auto-
matically populates gazetteers as opposed to man-
ually crafted gazetteers. Specifically they use an
abundant set of web search query logs with their
click information (see Fig. 1). They start by de-
tecting target URLs (such as imdb.com/title
for the movie names). Then they obtain a list
of entities and their target URLs (for example,
www.imdb.com/title/tt047723 can be the target URL
for the movie ?the count of monte carlo?. Then they
extract all queries hitting those links if they include
that entity. This method enables automatically ob-
taining annotated queries such as: ?review of the
hand? or ?mad men season one synopsis? (bold
terms are automatically discovered entities.)
4 Mining Click Graph Features for SLU
In the previous section, we presented examples of
recent research that use queries obtained from QCL
to bootstrap and improve SLU models. Note that
each query in QCL is linked to one or many web
sites (links), which indicate a certain feature of the
query (queries that the hotels.com linked are clicked
after they are entered might indicate hotels domain).
Such features extracted from QCL data (called click-
through features) has been demonstrated to signifi-
cantly improve the performance of ranking models
for Web search applications (Gao et al, 2009), es-
timating relations between entities and web search
queries (Pantel and Fuxman, 2011), etc.
In SLU research community, only recently the use
of click-through features has shown to improve the
performance of domain and intent of NL user utter-
ances. In one study (Hakkani-Tur et al, 2011b), in-
stead of mining more data to train a domain clas-
sifier with lexical features, they enrich their fea-
tures using the click-through features with the in-
tuition that the queries with similar click patterns
should be semantically similar. They search all the
NL utterances in the training data set amongst the
search queries. Once they obtain search queries,
they pull the list of clicked URLs and their frequen-
cies for each query which represent the click fea-
tures. To reduce the number of features, they ex-
tract only the base URLs (such as opentable.com or
wikipedia.com), as is commonly done in the web
search literature. T use the list of the 1000 most fre-
quently clicked base URLs for extracting classifica-
tion features (QCL features). For each input user
utterance, xj , they compute P (URLi|xj), where
i = 1..1000. They compute the click probability dis-
tribution distance between a query and the queries in
a target domain, Dk, using the KL divergence:
KLk = KL(P (URLi|xj)||P (URLi|Dk)) (1)
Thus, for a given domain Dk, the KLk and the do-
main with the lowest KL divergence are used as ad-
ditional features.
Although the click-through are demonstrated to
be beneficial for SLU models, such benefits, how-
ever, are severely limited by the data sparseness
problem, i.e., many queries and documents have no
or very few clicks. The SLU models thus cannot rely
strongly on click-through features. In (Celikyilmaz
et al, 2011), the sparsity issue of representing the
queries with click-through features are investigated.
They represent each unlabeled query from QCL as
39
a high dimensional sparse vector of click frequen-
cies. Since the true dimensionality of a query is un-
known (the number of clicks are infinitely many),
they utilize an unbounded factor analysis approach
and build an infinite dimensional latent factor anal-
ysis, namely the Indian Buffet Process (IBP) (Grif-
fiths and Ghahramani, 2005), specifically to model
the latent factor structure of the given set of queries.
They implement a graph summarization algorithm
to capture representative queries from a large set of
unlabeled queries that are similar to a rather smaller
set of labeled queries. They capture the latent factor
structure of the labeled queries via IBP and reduce
the dimensionality of the queries to manageable size
and collect additional queries in this latent factor
space. They use the new set of utterances boost the
intent detection performance of SLU models.
5 Discussions and Future Directions
This paper surveyed previous research on the usage
of the query click logs (the click through data) pro-
vide valuable statistics that can potentially improve
performance of the SLU models. We presented sev-
eral methods that has been used to extract infor-
mation in the form of additional vocabulary, unla-
beled utterances and hidden features to represent ut-
terances. The current research is only the beginning,
and most approaches such as query expansion, sen-
tence compression, etc. can be easily adopted for
dialog state update processes. Thus, the state-of-the
art in NL understanding can be improved by:
? clustering of URLs as well as queries for extract-
ing better features as well as to extend ontologies.
The search community has access to vast amounts
of search data that would benefit natural language
processing research,
? mining multi-lingual data for transferring dialog
systems from one language to others,
? mining information from search sessions, for ex-
ample, users rephrasing of their own search queries
for better results.
One issue that has been the topic of recent discus-
sions is the accessibility of QCL data to researchers.
Note that, QCL is not a crowd-source data that only
large web search organizations like Google or Mi-
crosoft Bing can mine and exploit for NL under-
standing, but various other forms may be imple-
mented by interested researchers by using a simple
web service or a mobile app (such as AT&T SpeakIt
or Dragon Go) or using a targeted search engine.
References
A. Celikyilmaz, D. Hakkani-Tur, and G. Tur. 2011.
Leveraging web query logs to learn user intent via
bayesian latent variable model. In ICML?11 - WS on
Combining Learning Strategies to Reduce Label Cost.
J. Gao, J.-Y. Nie, W. Yuan, X. Li, and K. Deng. 2009.
Smoothing clickthrough data for web search ranking.
In SIGIR?09.
T. Griffiths and Z. Ghahramani. 2005. Infinite latent fea-
ture models and the indian buffet process. In NIPS?05.
D. Hakkani-Tur, G. Tur, and L. Heck. 2011a. Exploiting
web search query click logs for utterance domain de-
tection in spoken language understanding. In ICASSP
2011.
D. Hakkani-Tur, G. Tur, L. Heck, A. Celikyilmaz, A. Fi-
dler, D. Hillard, R. Iyer, and S. Parthasarathy. 2011b.
Employing web search query click logs for multi-
domain spoken language understanding. In ASRU?11.
D. Hakkani-Tur, G. Tur, L. Heck, and E. Shriberg. 2011c.
Bootstrapping domain detection using query click logs
for new domains. In Interspeech?11.
D. Hakkani-Tur, G. Tur, R. Iyer, and L. Heck. 2012.
Translating natural language utterances to search
queries for slu domain detection using query click
logs. In ICASSP?12.
D. Hillard and C. Leggetter. 2010. Clicked phrase doc-
ument expansion for sponsored search ad retrieval. In
SIGIR?10.
D. Hillard, A. Celikyilmaz, D. Hakkani-Tur, and G. Tur.
2011. Learning weighted entity lists from web click
logs for slu. In Interspeech?11.
X. Li, Y.-Y. Wang, and A. Acero. 2008. Learning query
intent from regularized click graphs. In SIGIR08.
X. Li, Y.-Y. Wang, and A. Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields. In ACM
SIGIT?09.
P. Pantel and A. Fuxman. 2011. Jigs and lures: Associ-
ating web queries with structured entities. In ACL?11.
G. Tur and R. De Mori, editors. 2011. Spoken Language
Understanding: Systems for Extracting Semantic In-
formation from Speech. John Wiley and Sons.
G. Tur, D. Hakkani-Tur, D. Hillard, and A. Celikyilmaz.
2011. Towards unsupervised spoken language under-
standing: Exploiting query click logs for slot filling.
In Interspeech?11.
C. Wang, R. Raina, D. Fong, D. Zhou, J. Han, and
G. Badros. 2011. Learning relevance from a hetero-
geneous social network and application in online tar-
geting. In SIGIR?11.
40
