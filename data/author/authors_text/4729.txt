Generated Narratives for Computer-aided Language Teaching
Michael LEVISON
School of Computing
Queen?s University
Kingston, Ontario
Canada K7L 3N6
levison@cs.queensu.ca
Greg LESSARD
French Studies
Queen?s University
Kingston, Ontario
Canada K7L 3N6
lessardg@post.queensu.ca
Abstract
VINCI is a Natural Language Generation envi-
ronment designed for use in computer-aided sec-
ond language instruction. It dynamically gener-
ates multiple parallel trees representing an ini-
tial text, questions on this text, and expected
answers, and either orthographic or phonetic
output. Analyses of a learner?s answers to ques-
tions are used to diagnose comprehension and
language skills and to adaptively control sub-
sequent generation. The paper traces stages
in the generation of short texts in English and
French, and discusses issues of architecture, tex-
tual enrichment, and planning.
1 VINCI: Architecture,
Implementation and Output
Ideally, a language teaching system should both
?encourage the creative use of language in com-
municatively relevant settings? (Menzel and
Schroeder, 1998) and also provide detailed and
adaptive feedback (cf. (Michaud, 2002)). Many
systems resolve the issue by means of complex
parsing. In what follows, we describe VINCI, a
multilingual generation environment which rep-
resents a complementary approach in that it as-
sumes conversational control, dynamically pro-
ducing more or less complex texts and asking
information of users. VINCI is based on a col-
lection of metalanguages which define the se-
mantics, syntax, lexicon and morphology of a
language. Text files defining the language are
read by an interpreter (written in C) and out-
put is presented either orthographically or pho-
netically.
When used in a teaching context, VINCI cre-
ates an utterance and presents it to a learner.
It also creates a series of questions based on
each utterance, together with some hidden an-
swers. The learner is prompted to respond to
each question, and his or her response is com-
pared with the hidden one (or ones) and a de-
tailed report is produced on the relation be-
tween the two. This report provides informa-
tion on a learner?s comprehension and language
skills, as well as guidance for subsequent gener-
ation.
VINCI is capable of generation in many lan-
guages, and has been tested on such diverse lan-
guages as Spanish, Italian, Russian and Chi-
nese. Our work to date has been carried out
in both English and French, predominantly the
latter.
For the generation of simple utterances,
VINCI constructs a syntax tree using context-
free rules and syntactic transformations. The
nodes of the tree may be decorated with at-
tributes, whose role is to maintain grammatical
and perhaps semantic agreement by restricting
the choice of lexical items and controlling mor-
phology. Once a tree is formed, its leaves are
assigned suitable lexical choices which are in-
flected by morphology rules. In the most basic
situation, choices among syntactic alternatives
and among possible lexical entries are made at
random. In such a model, the semantic con-
trol exercised by attributes is minimal. They
can, for example, ensure that the subject of a
verb such as eat is animate and its object edible,
but they cannot influence the overall meaning of
the utterance produced. To achieve this, VINCI
makes use of preselections.
1.1 Preselections
Preselections may be thought of as forming a
metaphoric blackboard on which the person re-
quiring an utterance writes some choices about
its features. Lexical entries corresponding to
these features are selected before the syntax tree
is formed, and syntax rules have access both to
the words themselves and to information about
them obtained from the lexicon. We will illus-
trate this by showing the steps in the generation
of a short fairy tale, although other sorts of texts
are also possible.
In a typical fairy tale we have a collection
of characters, including a pompous twit (a
king or a rich merchant), a victim or heroine
(the twit?s daughter), a villain (a sorcerer or
witch), a hero (a prince or a brave woodcutter),
a goodfairy (the victim?s fairy godmother), a
magicobj (a sword or a silver goblet). These
form the basis for a set of preselections such as:
twit :
PN/"Midas";
victim:
PN/ pre twit/@14: daughter;
hero :
PN[male, brave, handsome];
magicobj :
N[physobj, magic]
These preselections presuppose a database of
characters/objects which are simply entries in a
lexicon:
"Midas"|PN|
male, rich, weak, vain|...
daughter: "Marie";...
type: "king"; ...
home: "castle"; ...
"Marie"|PN|
female, beautiful, kind|...
type: "princess"; ...
Preselections can be specified with more or
less precision. In this example, only Midas can
be chosen for the role of twit, but any mem-
ber of the class PN (proper names) having the
attributes male, brave and handsome can be
selected as hero. We might well have writ-
ten twit: PN[rich]/@14: daughter mak-
ing the twit any PN who is rich and has a
pointer to a daughter in lexical field 14.
These preselections are global, in that they
persist across several utterances. If the hero is a
prince in one sentence, he cannot be a woodcut-
ter in the next. In contrast, the local preselec-
tions described below associate these characters
with a semantic role in a particular sentence,
for example, the agent or the patient. We can
envisage a user typing/editing a global preselec-
tions file to select favorite characters for a story.
Alternatively, there may be an interface which
allows a user to choose characters from a set of
menus. In the following tale, we assume that
Wanda has been preselected as the goodfairy
and magic sword as magicobj.
1.2 Semantic Expressions
A semantic expression is a representation of the
content of an utterance in a form in which the
grammatical constraints of any particular natu-
ral language have been abstracted away, leav-
ing only some expression of meaning behind.
A simple functional notation is used, described
more fully in (Levison et al, 2001b). These
expressions are transformed into VINCI pres-
elections, triggering syntax rules which, in their
turn, yield sentences in some language. The
same sequence of expressions can transformed
into paragraphs in different languages or differ-
ent paragraphs in the same language.
The plot for the fairy tale can be specified as
a sequence of these expressions:
exists(twit)
Once upon a time there was a twit.
describe(twit)
He was rich and vain.
exists(victim)
The twit had a daughter, the victim.
describe(victim)
She was beautiful and kind.
admonish(twit, victim, action)
The twit warned the victim about
walking in the forest.
disobey(victim)
However, the victim disobeyed.
action(victim)
She went for a walk in the forest.
exists(villain)
In the forest there lived a villain.
describe(villain)
He was strong and evil.
kidnap(villain, victim)
The villain kidnapped the victim.
exists(hero)
In the same area, there lived a hero.
seekhelp(twit, hero)
The twit sought his help.
seek(hero, goodfairy)
The hero went to find the goodfairy...
give(goodfairy, hero, magicobj)
who gave him a magicobj.
seek(hero, villain)
The hero sought the villain...
kill(hero, villain, magicobj)
and killed him with the magicobj.
rescue(hero, victim)
The hero rescued the victim, ...
marry(hero, victim)
married her, ...
livehappily(hero, victim)
and lived happily ever after.
Obviously, the plot can be modified simply
by varying the expressions. Indeed there might
be alternative plots or sections, perhaps cho-
sen by a user or produced by a text plan-
ner. Let us repeat that these expressions are
language-independent. The names of the func-
tions and parameters are, in fact, VINCI at-
tributes, and although English words have been
used here, any string of letters or digits could
have been substituted. If a French grammar and
lexicon is built using the same attributes, as
in: "donner"|V|vtdi, give, ...|... then
VINCI can construct French sentences from the
same semantic expressions.
1.3 Local Preselections
Each of the expressions in the previous section is
equivalent to and is transformed by VINCI into
a set of local preselections which apply to the
generation of a single sentence. For example,
give(goodfairy, hero, magicobj) becomes:
vtdi; {this attribute selects a
segment of syntax based
on a verb with direct and
indirect objects}
act : V[give];
{e.g.: "give","offer"}
agent : PN/ pre goodfairy;
beneficiary : PN/ pre hero;
theme : N/ pre magicobj
Some of these local preselections refer back
to the global ones, associating the characters
selected in the earlier preselections with the
semantic roles they will play in the current
sentence: agent, beneficiary and theme. So
goodfairy (and hence Wanda) becomes the
agent of the act of giving, magicobj (the magic
sword) becomes its theme, and hero its benefi-
ciary.
In effect, semantic expressions provide a more
user-friendly form for the set of preselections.
Conversion from the former to the latter is ef-
fected by semantic transformations; for exam-
ple:
give : vtdi;
act : V[give];
agent : PN/ pre #1;
beneficiary : PN/ pre #2;
theme : N/ pre #3
whose left-hand side matches give(goodfairy,
hero, magicobj), #1 being associated with
goodfairy, #2 with hero and #3 with
magicobj. In practice it shorter, if less ele-
gant, to replace this semantic expression by
vtdi(give, goodfairy, hero, magicobj),
since this single expression can be used for any
verb taking both direct and indirect objects.
1.4 Syntax Rules
Syntax rules in Vinci take the abstract semantic
representations produced by semantic expres-
sions and preselections and clothe them in the
syntax of the language chosen. Among other
things, this allows the system to capture con-
straints on word order, argument structure, and
agreement. This is accomplished by means of
inheritance of attributes down the nodes of a
syntax tree, and guarded syntax rules, in which
attributes present on a parent node are used to
determine the nature of child nodes. For ex-
ample, given a parent node such as NP (noun
phrase) containing the attribute ?p1? (first per-
son), a guarded syntax rule (these are headed
by the symbol <) will determine that the only
possible child node is a pronoun. However, in
the default case (these are headed by the sym-
bol >), the child may take either the form of a
pronoun or a full noun phrase.
Let us now return to the example of prese-
lections developed above and resume with the
action of syntax rules on the output of prese-
lections and semantic expressions. The section
of the context-free rules corresponding to vtdi
describe the structure of a sentence with a vtdi
verb in terms of the its agent, theme and ben-
eficiary. Thus, assuming the local preselections
given above:
ROOT =
< _pre_ vtdi:
NPP[sing, agent, def]
V[p3, sing, past]/_pre_ act
NPP[sing, beneficiary, def]
NP[sing, theme, indef] %
NPP = inherit Fn: Function, Nu: Number,
De: Detkind;
DET[De] N[Nu]/_pre_ Fn/@13:type %
NP = inherit Fn: Function, Nu: Number,
De: Detkind;
DET[De] N[Nu]/_pre_ Fn %
The root of the utterance (the top of its syn-
tax tree) has four child nodes, two of them
proper noun phrases (NPP), a third a com-
mon noun phrase (NP). To the first it passes
a Number-value, sing, a Function-value, agent,
and a Detkind-value. When this NPP is devel-
oped into its two children, it assigns these at-
tribute values to Nu, Fn and De, passing De to
DET (hence a or the) and Nu to the child noun,
which will therefore be singular. This noun is
also directed to obtain its lexical entry from the
preselection labelled Fn (i.e. agent, which in
turn refers to goodfairy, and hence to Wanda).
Furthermore, rather than using Wanda itself,
the chosen noun must be replaced the lexical
entry indicated by tag type in field 13 (fairy
godmother).
The other noun phrases obtain their nouns
similarly from beneficiary and theme, though
the last (NP) uses the preselected magicobj di-
rectly. The root?s verb-child, which will be third
person singular past tense, will obtain its lexi-
cal entry from the preselection labelled act. So,
we get: the fairy godmother gave the prince a
magic sword.
1.5 Two Generated Stories
Using a simple English lexicon and the grammar
described above, VINCI generates fairy tales, of
which the following is an intentionally short and
simple example.
Once upon a time there was a king
called Midas who lived in a castle.
He was rich and vain. The king had
a daughter, a princess named Marie,
who was beautiful. The king warned
Marie not to go out of the castle. The
princess disobeyed the king. She left
the castle.
A sorcerer called Merlin lived in the
woods. He was evil. The sorcerer kid-
napped the princess.
Nearby there lived a woodcutter
who was named Axel. The king sought
the help of the woodcutter. The wood-
cutter went to look for the fairy god-
mother. The fairy godmother passed
Axel a magic sword. Axel searched for
the sorcerer. The woodcutter killed the
sorcerer with the magic sword. The
woodcutter rescued the princess. The
woodcutter and the princess got mar-
ried and lived happily ever after.
When linked to a French lexicon, morphology
and syntax, VINCI generates comparable texts
in French, as the following example shows:
Il e?tait une fois un roi qui s?appelait
Midas et qui vivait dans un beau
cha?teau. Il e?tait riche et vain. Le
roi avait une fille: une princesse qui
s?appelait Marie et qui e?tait belle.
Le roi interdit a` Marie de quitter le
cha?teau. La princesse de?sobe?it au roi.
Elle quitta le cha?teau.
Dans la fo?ret il y avait un sorcier
qui s?appelait Merloc. Il e?tait me?chant.
Le sorcier enleva la princesse.
Aux alentours vivait un prince qui
s?appelait Coeur de Lion et qui e?tait
beau. Le roi demanda l?aide du prince.
Le prince chercha la bonne fe?e. La
bonne fe?e donna une e?pe?e magique
au prince. Le prince chercha le sor-
cier. Coeur de Lion utilisa l?e?pe?e mag-
ique pour tuer le sorcier. Le prince
libe?ra la princesse. Le prince e?pousa
la princesse et ils eurent beaucoup
d?enfants.
Along with orthographic output, (Thomas,
2002) describes the generation of good quality
prosodically controlled French oral output by
linking VINCI with the MBROLA speech syn-
thesizer (Dutoit, 2004). At this time, learner
responses must still be entered orthographically.
1.6 Analysis of User Input
As well as constructing the story, VINCI may
produce a series of questions to put before a
learner; for example:
Question: What was the name of
the good fairy?
Expected (hidden) answer: The
good fairy was called Wanda.
Question: Describe Maria.
Expected (hidden) answer:
Maria was beautiful and
kind.
In French, whose complex morphology gives
scope for more varied errors, a typical question
such as:
Question: Ou` vivait le roi?
and a reponse from a particularly incompetent
learner (one of the authors), gives rise to the
following error report:
EXPECTED : le roi vivait dans
un beau cha^teau
RESPONSE : la roi vivrait en
une chapeau belle
C4 DELETE dans
S4 INSERT en
C6 ORDER C7 C6
C1 S1 la/le MORPH fe?m/masc
C2 S2 EXACT
C3 S3 vivrait/vivait MORPH
cond/imparf
C5 S5 une/un MORPH fe?m/masc
C6 S7 belle/beau MORPH fe?m/masc
C7 S6 APPROX chapeau/cha^teau
If the learner had typed habitait, VINCI
would have reported the change as LEX syn,
habiter being tagged in the lexicon as a syn-
onym for vivre. If he had omitted the circumflex
accent on cha?teau, the error would have been
marked as PHON, since the two forms would
have been similar in sound. This is made pos-
sible by the fact that in VINCI, lexical entries
may carry both orthographic and phonological
information.
Output of the error analysis routines as shown
above is not designed to be presented directly
to a learner. However, since it is machine-
generated, it is relatively easy to parse by a
routine which uses it to present error analyses
in a more user-friendly format. At the same
time, results of each analysis may be stored
and then used by a driver program to build
a user model, and to adaptively control sub-
sequent generation (Levison et al, 2001a). In
this way, VINCI?s architecture ?closes the loop?
in the traditional pipeline approach to genera-
tion, in that the output of analysis and diagnosis
drives the input of textual planning.
2 Enhancements and Future Work
VINCI?s use of semantic input by means of
functional expressions is designed to allow it
to function either as an autonomous narrative
generation system (cf. (Callaway and Lester,
2001a), (Bringsjord and Ferrucci, 2000) for ex-
amples) or as a story authoring environment
(cf. (Umaschi and Cassell, 1997)) in which a
language teacher may select or construct high-
level utterance specifications, or alternatively,
a learner may play with the order of a set of
semantic specifications, or even add new char-
acters with their own traits, examining in each
case the texts produced. Two kinds of enhance-
ments can be used to improve output.
2.1 Encyclopedic Enrichment
In examples shown above, descriptions are
based on simple static attributes (beauty,
morality, etc.). In fact, VINCI?s compound at-
tribute mechanism also allows for the expres-
sion of actions by characters. Thus, the at-
tribute kill.monsters in the lexical entry for
Prince Braveheart might cause exists(hero)
to lead to: Nearby there lived a prince called
Braveheart, who was renowned for killing mon-
sters. This mechanism is also applicable to the
expression of a character?s thoughts and atti-
tudes, and past background information, both
narrative desiderata (Bringsjord and Ferrucci,
2000), (Robin, 1993) as well as the generation
more or less complex versions of the same text
(cf. (Chali, 1998)). Work is underway on mech-
anisms for the dynamic temporal tagging of at-
tributes, as a story develops. For example, a
learner given $50 and instructed to purchase
groceries in a textual supermarket would have
his or her remaining money reduced by each
purchase he or she describes.
It should also be noted that the micro-world
defined by means of Vinci may be fictional, as
in the cases above, or based on real people and
events. For example, we have performed exper-
iments based on a database of French authors,
their works, and their biographical details such
as date of birth, death, etc.
2.2 Narrative Enrichment
Appropriate use of anaphoric pronous and ag-
gregation of sentences both have a significant
effect on perceptions of text quality (Callaway
and Lester, 2001b). In a number of systems,
both processes occur after sentences have been
realized, at the level of revision, which often
requires that utterances be reformulated. We
propose to perform comparable operations at
the level of semantic expressions. Suppose two
functions: exists(X), which generates There
was an X, and describe(X) which generates X
was brave and handsome. The fact that both ex-
pressions share a common argument allows for
their replacement by another, say exdesc(X),
which aggregates the two functions to produce
There was a brave and handsome X. Similarly,
in the case of anaphoric relations, shared argu-
ments allow for replacement of full names by
pronouns. We are currently researching this.
Finally, taking account of work by (Karamanis
and Manurung, 2002) which shows that sharing
of at least one argument characterizes a high
percentage of successive sentences in a text, it
is possible to use the sequence of arguments to
order a sequence of semantic expressions. Per-
haps more interestingly, it may be that one of
the criteria of a new paragraph is a break in the
chain of shared arguments from one semantic
expression to the next. The paragraph breaks
in the English and French texts above, while
human-constructed, respect this constraint.
References
S. Bringsjord and D.A. Ferrucci. 2000. Artifi-
cial Intelligence and Literary Creativity: In-
side the Mind of BRUTUS, a Storytelling Ma-
chine. London, Lawrence Erlbaum.
C.B. Callaway and J.C. Lester. 2001a. Eval-
uating the effects of natural language gen-
eration techniques on reader satisfaction. In
Proceedings of the 23rd Annual Conference of
the Cognitive Science Society, pages 164?169.
Edinburgh, UK.
C.B. Callaway and J.C. Lester. 2001b. Narra-
tive prose generation. In Proceedings of the
Seventeenth International Joint Conference
on Artificial Intelligence (IJCAI 2001), pages
1241?1248. Seattle, WA.
Y. Chali. 1998. Text expansion using causal
and temporal relations. In Proceedings of the
Third International Conference on Natural
Language Processing and Industrial Applica-
tions. Moncton, New Brunswick.
T. Dutoit. 2004. The MBROLA Project.
http://tcts.fpms.ac.be/synthesis/mbrola.html.
N. Karamanis and H.M. Manurung. 2002.
Stochastic text structuring using the princi-
ple of continuity. In Proceedings of INLG-02,
pages 81?88. Columbia University.
M. Levison, G. Lessard, A-M. Danielson, and
D. Merven. 2001a. From symptoms to diag-
nosis. In (K. Cameron, editor, CALL ? The
Challenge of Change, pages 53?59. Elm Bank,
Exeter.
M. Levison, G. Lessard, B. Gottesman, and
M. Stringer. 2001b. Semantic expressions:
An experiment. Working paper, found
at: http://www.cs.queensu.ca/CompLing/
semanticexpressions.html.
W. Menzel and I. Schroeder. 1998. Constraint-
based diagnosis for intelligent language tu-
toring systems. In Proc. IT&KNOWS, XV.
IFIP World Computer Congress, pages 484?
497. Vienna/Budapest.
L.N. Michaud. 2002. Modeling User Interlan-
guage in a Second Language Tutoring Sys-
tem for Deaf Users of American Sign Lan-
guage. PhD Dissertation, Department of
Computer and Information Sciences, Univer-
sity of Delaware.
J. Robin. 1993. A revision-based generation ar-
chitecture for reporting facts in their histor-
ical context. In M. Zock H. Horecek, editor,
New Concepts in Natural Language Genera-
tion, pages 238?268. Pinter, London.
C. Thomas. 2002. A Prosodic Transcription
Method for Natural Language Generation.
MSc Thesis, Queen?s University, Kingston.
M. Umaschi and J. Cassell. 1997. Storytelling
systems: Constructing the innerface of the
interface. In Cognitive Technologies Proceed-
ings ?97, pages 98?108. IEEE.
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 52?60,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
Groundhog DAG: Representing Semantic Repetition in Literary Narratives?
Greg Lessard
French Studies
Queen?s University
Canada
greg.lessard@queensu.ca
Michael Levison
School of Computing
Queen?s University
Canada
levison@cs.queensu.ca
Abstract
This paper discusses the concept of semantic
repetition in literary texts, that is, the recur-
rence of elements of meaning, possibly in the
absence of repeated formal elements. A ty-
pology of semantic repetition is presented, as
well as a framework for analysis based on the
use of threaded Directed Acyclic Graphs. This
model is applied to the script for the movie
Groundhog Day. It is shown first that seman-
tic repetition presents a number of traits not
found in the case of the repetition of formal el-
ements (letters, words, etc.). Consideration of
the threaded DAG also brings to light several
classes of semantic repetition, between individ-
ual nodes of a DAG, between subDAGs within
a larger DAG, and between structures of sub-
DAGs, both within and across texts. The model
presented here provides a basis for the detailed
study of additional literary texts at the seman-
tic level and illustrates the tractability of the
formalism used for analysis of texts of some
considerable length and complexity.
1 Background
Repetition, that is, the reuse of a finite number of
elements, is a fundamental characteristic of natural
language. Thus, the words of a language are com-
posed of a small number of phonemes or letters, sen-
tences are constructed from repeated words, as well
as larger collocational or syntactic chunks, and so on.
Most work on repetition has been concerned with the
study of such recurring formal elements, typically
?This research was supported in part by the Social Sciences
and Humanities Research Council of Canada.
from the perspective of their frequency. However,
it is important to recognize that a text can present
not only cases in which some form recurs, as in 1(a)
below, but also instances where meaning recurs, with-
out any formal element being necessarily repeated,
as in 1(b).
(1) (a) Brutus has killed Caesar! He has killed
him!
(b) Brutus has killed Caesar! He plunged his
knife into him and our beloved leader is
dead!
It is such semantic repetition that concerns us here:
that is, the repetition of some semantic content within
a text, without there being necessarily a formal ele-
ment which recurs. In particular, we wish to study
semantic repetition in literary texts. This is important,
since literature often brings with it the expectation
that repetition is significant. To put this another way,
repetition tends to be semanticized: its very existence
?means? something. Consider this first at the formal
level. It is well-known that human language process-
ing tends to extract meaning from sequences of forms
and retain the forms themselves for only a limited
time. Literary texts counteract this fading effect by
several linguistic means, including physical proxim-
ity of repeated items, stress, and syntactic position.
Devices such as these often carry additional informa-
tion on importance or some other factor, as when an
orator repeats the same word or sequence. This has
been much discussed. To mention several examples
among many, Jakobson (1960) refers to this as the
poetic function of language, Genette (1972) provides
a typology of narrative repetition, Tsur (2008) argues
52
that repetition is one of the devices which ?slows
down? processing of text and contributes to poetic
effects, Tannen (1989) gives examples of the usage of
repetition in oral discourse, Okpewho (1992) shows
its importance in folk literature, and Johnstone (1991)
examines the role of repetition in Arabic discourse.
As we will see below, semantic repetition in lit-
erature also lends itself to semanticization. In other
words, the fact that events are repeated in a narrative
can be, and often is, seen not as the product of chance
but rather as part of a larger pattern. The potential
for this is supported by several features of meaning.
First, as Stanhope et al (1993) have shown in their
work on the long-term retention of a novel, unlike
formal elements, at least some semantic elements
are extremely resistant to decay and can be recalled
weeks and even many months later. As a result, the
fading effects observed earlier for formal repetition
cannot be assumed to apply in exactly the same fash-
ion to semantic repetition: items remain accessible
across entire texts and even across different texts.
Second, there is in principle no upper limit on the
size of semantic elements which may be repeated.
At one extreme, a single character from a novel may
remain in memory, along with some of the items as-
sociated with him or her. If one hears the single word
Hamlet, what comes to mind? At the other, entire
plots may be recalled. If asked to summarize the
plot of A Christmas Carol in 100 words, most native
speakers would have no difficulty in doing this. And
third, by their tendency to exploit and semanticize
repetition, literary texts differ from other genres, such
as expository texts, whose goal is typically to present
some set of information in a coherent fashion such
that the same element not be repeated.
In light of this, our goal here is threefold: to give a
sense of the diversity of semantic repetition in literary
texts, including its various granularities; to propose a
formal model capable of dealing with these various
dimensions of semantic repetition; and to test this
model against an actual text of some considerable
length.
2 Events and repetition
Let us assume for the moment that semantic repeti-
tion is limited to repeated events, leaving aside issues
of repeated qualities, entities and so on. A number
of formal and semantic tools suggest themselves for
dealing with this case. Within a single utterance, a
neo-Davidsonian event semantics might be used, as
shown in (2), where e represents the ?glue? which ties
together the action and the agent.
?e[speak(e) ? agent(e) = fred(e)] (2)
This places the event at the centre of focus. The
logical machinery behind this has been extended in
various ways. For example, Hewitt (2012) proposes
the use of serial logic to capture ordered sets of
events. In addition, since events are also repeated
across utterances and related to other events, as in
conversations, Asher and Lascarides (2003) provides
an extended logical formalism to begin to deal with
this and Helbig (2006) proposes several specific
functions for linking propositions, including CAUS
(causality), CONTR (contrast), and CONF (conformity
with an abstract frame). However, both approaches
are applied to short spans of text and neither deals
explicitly with repetition.
At a slightly higher level of granularity, Rhetori-
cal Structure Theory (Mann and Thompson, 1988)
provides a set of frameworks to describe relation-
ships among elements of a paragraph, some of which,
Restatement and Elaboration in particular,
have the potential to deal with elements of repeti-
tion.1 Work in Natural Language Generation, which
has often focused on the production of longer exposi-
tory texts, has also typically paid more attention to
the reduction of repetition than to its production.2
Even work on narrative generation has tended to con-
centrate mostly on reduction of repetition (Callaway
and Lester, 2001; Callaway and Lester, 2002).
Several attempts have been made to deal with
longer spans of texts, typically based on the markup
of elements within a text. Most recently, Mani (2012)
proposes a Narrative Markup Language capable of
dealing with elements of repetition, but this markup
is anchored to the text itself and it is unclear how such
an approach could capture more abstract elements of
semantic repetition. In fact, the fundamental issue
1For details, see http://www.sfu.ca/rst/
01intro/definitions.html.
2See, however, de Rosis and Grasso (2000) who argue for
the role of what they call redundancy.
53
is that semantic repetition exists across a wide range
of spans, from the very smallest (both across differ-
ent events and within elements of some inherently
repeated activity (Tovena and Donazzan, 2008)), to
the very largest, spanning multiple texts. To illustrate
this, consider the following cases.
(a) A single event and the memory of the event in
the mind of the perpetrator. For example, Brutus
stabs Caesar, and then the next day replays the
stabbing in his memory.
(b) A single event seen from the point of view of
two different characters. For example, Livia
sees Brutus stab Caesar, and so does Cassius.
(c) A single, perhaps complex, event, whose dif-
ferent facets are represented, perhaps in an in-
terspersed fashion. Good examples of this are
found in cinema, such as Eisenstein?s famous
bridge scene in October, or the Odessa steps
scene in Battleship Potemkin, where the same
images recur (such as the opening bridge, the
dead horse, or the baby carriage tipping on the
end of a stairway).
Examples such as these illustrate what might be
called representational repetition, in which the same
(perhaps complex) event is shown from different
points of view. However, we also find examples of
what might be called class-based repetition, in which
various simple examples share a common abstract
structure, as the following examples illustrate.
(d) Two sets of events in the same text represent
instantiations of the same topos, or recurring
narrative structure. For example, the Hebrew
Bible contains multiple instances in which a par-
ent favours a younger sibling over an older one.
Thus, the Deity favours Abel over Cain, Abra-
ham favours Isaac over Ishmael, Isaac favours
Jacob over Esau, and so on. In these cases, we
are actually faced with an abstract framework
which is instantiated with different actual pa-
rameters.
(e) Two different texts represent the same abstract
plot. Thus, Pyramus and Thisbe and Romeo
and Juliet may both be represented by the same
abstract formula, which we captures the story of
star-crossed lovers whose feuding families lead
to their demise.
Examples such as (d) and (e) show that at least
some elements of literary repetition may only be cap-
tured by some device which permits a greater degree
of abstraction than is provided by traditional devices
like predicate calculus or instance-based markup.
From the literary perspective, they are sometimes
referred to as topoi, that is, recurring narrative se-
quences.3 However, as formulated in most literary
analyses, the notion of topos has several shortcom-
ings. First, definitions tend to be informal.4 Second,
the granularity of topoi is unclear. One might ex-
press a given topos in very general terms or quite
specifically.
Our goal here is to build on the insights of liter-
ary theory regarding the meaning of literary texts,
while retaining some level of formalism. To do this,
we need first to respect the empirical richness of lit-
erary texts. As the examples above show, simple
two-line examples are not sufficient to show the true
complexity of semantic repetition. Accordingly, we
have chosen as our corpus an entire movie script,
described below. Second, in the case of semantic
repetition, we need a formalism capable of capture-
ing various levels of granularity, from quite fine to
very general, and which shows not just differences
of point of view, but elements of class inclusion. To
do accomplish this, we have adopted the formalism
described in Levison et al (2012), based on a func-
tional representation of meaning elements by means
of semantic expressions.5 When combined with the
use of threaded Directed Acyclic Graphs, discussed
below, this formalism permits the representation of
elements of meaning at various levels of granularity,
3 Groundhog Day
To illustrate the phenomenon of semantic repetition,
we have created a formal analysis of the screenplay
for the popular movie Groundhog Day (henceforth,
3A detailed list of topoi, together with examples, may be
found in http://satorbase.org.
4See Lessard et al (2004) for one attempt at formalization.
Note also that the concept of topos shares features with the
concept of scripts (Schank and Abelson, 1977), which has been
formalized to some degree.
5The formalism is inspired by the Haskell programming lan-
guage (Bird, 1998).
54
GH).6 Because of its plot structure, discussed below,
the script represents arguably an extreme case of se-
mantic repetition and thus a good test of the proposed
model of semantic repetition.
GH recounts the story of Phil Connors, an ego-
centric weatherman, who has been sent with his pro-
ducer, Rita, and cameraman Larry, to cover the an-
nual February 2 event at Punxsutawney, Pennsylva-
nia, where a groundhog (Punxsutawney Phil), by
seeing or not seeing his shadow, provides a predic-
tion on the number of weeks remaining in winter.
Displeased at such a lowly assignment, Connors be-
haves badly to all. However, on waking up the next
day, he discovers that it is still February 2, and the
day unfolds as it had previously. In the many sub-
sequent iterations of the day, Connors discovers the
possibilities inherent in there being no consequences
to his acts, the advantages of being able to perfect
the elements of a seduction by repeated trials, and
finally, the value of altruism and love. At this point,
after many iterations, the cycle is broken, and Phil
and Rita, now in love, greet February 3.7
4 Directed Acyclic Graphs
To capture the various elements of granularity in the
GH script, we make use of the well-known distinc-
tion in literary theory between two perspectives on
a narrative. The fabula or histoire is the information
on which the narrative is based; the sjuzhet or re?cit
is a particular telling (Bal, 1985; Genette, 1983). In
our model, we represent the former, which we shall
term a story, by a Directed Acyclic Graph, hence-
forth DAG. A directed graph is a collection of nodes
linked by unidirectional paths. In an acyclic graph,
no sequence of paths may link back to a node already
visited. In technical terms, the dependency relation
portrayed by the graph is transitive, irreflexive and
antisymmetric. Within the DAG, nodes denote pieces
of the meaning, perhaps at different levels of granu-
larity, and directed paths which indicate the depen-
dence of one node upon another. By dependence,
6It should be noted that this screenplay, which may be found
online at http://www.dailyscript.com/scripts/
groundhogday.pdf, diverges in some respects from the film.
It contains some scenes which do not appear in the film, and it
does not contain some others which do appear in the film.
7A fuller synopsis can be found at http://www.imdb.
com/title/tt0107048/synopsis.
we mean that subsequent nodes in the DAG make
use of information present on previous nodes. In a
finer analysis, the nature of the various dependencies
might be sub-divided into subclasses like logical de-
pendency, temporal dependency, and so on, but we
will not do that here.
As noted earlier, we represent the meanings car-
ried by the nodes of a DAG by means of semantic
expressions. So, for example, given the semantic en-
tities phil and rita, and the action meet, the ex-
pression meet(phil, rita) represents a meet-
ing between the two characters in the film. This ex-
pression represents what is called, in the framework
used, a completion. Although the functional repre-
sentation used permits the representation of semantic
niceties like temporal relations and definiteness, the
model used here does not include them. In the anal-
ysis here, each semantic expression corresponds to
one node of the DAG. Of course, such a model may
vary in granularity. At one extreme, the entire script
could be represented by a single expression (as in
improve(phil). At the other, each small event
could form the basis of a semantic expression. For
the purposes of the present analysis, we have adopted
an intermediate granularity.8
Each element of the functional representation is
drawn from a semantic lexicon composed of a formal
specification and an informal one, which provides a
basic-level textual output, as shown by the following
examples:
meet :: (entity, entity)
-> completion
meet(x,y) =
"[x] meets [y]"
where the first line shows the formal specification
and the second line the informal one. The sequence
of semantic expressions, when used to call the in-
formal representations, thus provides the gist of the
script, or alternatively, can be used to drive a natural
language generation environment. In addition, since
the elements of the DAG are formally specified in the
semantic lexicon, they may be analyzed or further
manipulated by graph manipulation software. To take
a trivial case, the transitive closure of a DAG might
be calculated.
8A fuller discussion of these issues may be found in Levison
and Lessard (2012).
55
5 Threads and threading of a DAG
A particular telling of a story, which we call here
the narrative, may be conceived of as a particular
traversal of the DAG. To designate this, we make
use of the concept of threading. Threads are simply
sequences of nodes and we often display them in the
diagram of a DAG by a dotted line through the nodes.
A thread need not follow the edges of the DAG, nor
need it be acyclic. In other words, the same thread
may traverse the same node more than once. The
ordering of the threads of a narrative is assumed to
correspond to narrative time. The various segments
in our diagrams are numbered. Threads may traverse
some but not necessarily all nodes of the DAG.
It should be noted that a particular DAG may give
rise to numerous possible threadings. So, for exam-
ple, a story may be told in chronological order (?Once
upon a time, there was a beautiful princess ... she was
kidnapped by an evil wizard ... a handsome prince
rescued her ... they lived happily ever after.?), or in
reverse (?The prince and the princess were prepar-
ing for their wedding ... this was the outcome of his
rescue of her ... she had been kidnapped...?). Fur-
thermore, a DAG may be threaded to capture not just
some telling of the narrative, but also in terms of the
point of view of some character, the states of some
object in the narrative, or the representation of space
or description of places or characters.
We will apply this conceptual machinery to the
analysis semantic repetition in the GH script.
6 Analysis
At an abstract level, the relationships behind GH
(that is, the story) may be represented by three nodes
joined by solid edges, which show the semantic de-
pendencies among the nodes, as shown in Figure
1. The first sets the scene by placing Phil in Punx-
sutawney, the second represents Phil?s recursive ac-
tions during his endless series of February 2?s, and
the third represents his escape from recursion.
At the opposite extreme of granularity, it is pos-
sible to show the GH DAG with a thread travers-
ing fine-grained nodes, each represented by a se-
mantic expression. This representation, which con-
tains 172 nodes and 171 edges, is far too large to
fit onto a page. It may be viewed in its entirety
at http://tinyurl.com/awsb4x6. As noted
Figure 1: The most abstract DAG for GH
above, the segments of the thread are numbered and
dotted. Following them in order thus recounts the
semantic representation of the GH narrative at a rel-
atively fine level of granularity. Between these two
extremes of the abstract DAG and the linear thread-
ing, we will now examine several issues of semantic
repetition.
6.1 Repetition as return of threads to a node
The simplest form of semantic repetition takes the
form of a thread passing through some node more
than once. Figure 2 provides a simple case of this.
Figure 2: A thread passing multiple times through the
same node
Thus, Phil meets a beggar at several points in the
narrative (threads 9, 53, 146), with various outcomes,
including ignoring the beggar (threads 10, 26, 54)
and helping him (thread 147). Despite this help, the
beggar dies (thread 148), but Phil is given the oppor-
tunity to replay this sequence (thread 149), choosing
then to feed the beggar (thread 150).
56
6.2 DAGs and subDAGs
Consideration of the entire GH threading shows not
just return of the thread to a single node, but also con-
stellations of nodes which ?hang together?. In some
cases, this is based on common membership of the
nodes in some class of events. One example of this is
provided by Phil?s various attempts at suicide. Since
Phil returns to life after each suicide, each suicide
attempt (a toaster dropped into a bathtub, leaping
from a tall building, walking in front of a bus, and
so on) shares with the others only membership in the
class of suicide events. This state of affairs may be
captured by including each of these nodes within a
local subDAG, which itself represents a subnode of
the larger DAG. So, for example, we could represent
the local subDAG here by means of the semantic
expression attempt(phil, suicide). Such
subDAGs may be further refined or combined, sim-
ilar to the concept of stepwise refinement found in
computer programming.
In the case of the various suicide attempts, it is
important to note that the various attempts show no
dependency among themselves, and no order among
them is required, beyond that imposed by a particular
threading. This may be represented as follows:
kill(phil, phil, with(electricity))
kill(phil, phil, with(jump))
and so on. A similar example is found in Phil?s
attempts to improve himself, which involve learning
Italian, music, sculpture and medicine, among other
things.
However, we also find instances in which several
nodes within a subDAG do show dependency rela-
tions within a common subDAG. So, for example,
when Phil meets Rita at a bar, the same sequence is
followed: he buys her a drink, they make a toast, and
they discuss Rita?s previous education, as can be seen
in Figure 3.
Note that both temporal and logical dependence ex-
ists between two of the nodes (Phil must buy the
drink in order for them to make a toast). There is
no dependence between these two and the discussion
of Rita?s education, but the threading may indicate a
temporal order.
Mixed models are also possible, in which some
elements of a subDAG show dependency while others
do not, as in the case where Phil awakens to the fact
Figure 3: The subDAG for Phil and Rita at the bar
that his acts have no long-term consequences. In
one reaction to this, he robs a bank, buys a car and
tours the town. Each of these steps depends on the
previous one. However, he also gets a tattoo and
throws a party, both of which are independent of
each other and of the others. However, together, all
these elements constitute the subDAG of exploring
the absence of consequences.
6.3 Parametrized subDAGs
In the presentation so far, we have treated the seman-
tic expressions within nodes as constants. However,
examination of the GH DAG brings to light several
instances in which some part of the DAG is repeated
with one or more elements replaced systematically
with different ones. One illustration of this may be
found in Phil?s various reportings of the events at
Gobbler?s Knob, when the groundhog appears. Over
the course of the narrative, he is first glib and sarcas-
tic, then confused, then professional, then learned,
poetic, and finally profound. This might be repre-
sented by five distinct copies of the part.
describe(phil, groundhog, ironic)
describe(phil, groundhog, confused)
and so on. However, given the similarity between
the five nodes, it would be more efficient to create a
single, separated, copy containing parameters, which
could be instantiated in each of the five places with
the parameters replaced by the appropriate variants.
57
A similar series of parameters may be found else-
where in GH, for example, when Phil greets the man
on the stairs of the B&B first ironically, then angrily,
and finally with good humour, in Italian. Or again, at
a more complex level, we find a series of instances
where Phil learns some new skill (French poetry, pi-
ano, Italian, sculpture,...) and subsequently applies it.
This is illustrated by two typical subDAGs in Figure
4.
learn(phil, music)
play(phil, piano)
learn(phil, sculpture)
sculpt(phil, rita)
Figure 4: Learning and implementation
Each of these series forms a sequence such as:
improve(phil, altruism, 0)
improve(phil, altruism, 1)
and so on, where the third parameter indicates Phil?s
progression along the scale of character development.
This particular series provides a means of capturing
each particular state in Phil?s evolution from egotist
to altruist.
Note however that Phil?s moral development does
not progress through different areas of his life, one
series at a time. In other words, he does not first
change from a sarcastic to a poetic reporter, then
grow from an egotist to an altruist in the community,
then make the transformation from a seducer to an
attentive lover, and so on. Rather, his personal im-
provement happens more or less at the same pace
across different facets of his life, reflecting his over-
all personal growth, although evidence of this might
be drawn first from one and then from another of his
activities.
6.4 Parallel DAGs
In the discussion to this point, we have been con-
cerned with repetition within a single subDAG. How-
ever, in GH, we also find instances where one sub-
DAG shows an architectural similarity to another.
This similarity can be construed as a sort of high-level
repetition. For example, while on location in Punx-
sutawney, Phil meets and seduces Nancy, a woman
from Punxsutawney. At the same time, he attempts
to seduce Rita, his producer.
In both cases, Phil makes an initial attempt and
is rebuffed, by both Nancy and Rita. Undaunted, he
then seeks more information about both, determining
Nancy?s name and obtaining enough information to
pass as a former high school classmate, and deter-
mining that Rita drinks tequila with lime, that she
toasts world peace, and that she likes French poetry.
He then uses the information about Nancy to seduce
her, but the same tactic is unsuccessful with Rita.
The two parallel subDAGs may be represented by
a higher-level subDAG where almost all the individ-
ual elements change from case to case, with only
the general framework remaining. This might be
expressed schematically as follows:
experiment(x,y) =
slist(
meet(x,y)
learn(x, of(y, characteristics)))
and so on.
Applied within a single narrative, such an approach
deals with the sort of parallel cases seen here. Ap-
plied across narratives, it gives rise to texts seen as
?telling the same story?, like Romeo and Juliet men-
tioned earlier. At an even more abstract level, it pro-
vides a means of modelling parodies, or works based
on some previous model. Think of Joyce?s Ullysses,
in which Stephen Daedalus? peregrinations around
Dublin represent a parellel to Homer?s Odyssey.
6.5 Connections between subDAGs
We now have a means of representing semantic repe-
tition at both the low level, of individual nodes of a
DAG, as well as within and across DAGs. However,
we have left unspecified an important point, to which
we now return. Earlier, we showed that individual
nodes may contain subDAGs of interior nodes, up
to some indefinite level of complexity. This varying
granularity provides a model for different degrees
of detail in the recounting of a story, between the
highest-level and coarsest summary, to the finest de-
tail. Consider now the following case from GH. Each
day, Phil wakes up, hears the same song on the radio,
followed by inane banter from two disc jockeys. At
58
the level of the DAG, this may be represented as an
overarching node which contains two interior nodes,
as shown formulaically here:
wakeup(phil) = slist(
hear(phil, song)
hear(phil, dj_banter))
and graphically in Figure 5.
Figure 5: Part of the DAG for Phil?s waking up
However, the actual threading of this higher-level
nodes and its interior nodes in the narrative varies
over the course of the narration, as shown in Figure
6.
Figure 6: The threading of Phil?s waking up
Thus, in threads 5, 22, 50, 103, 119, 122 and 135,
Phil?s waking up is followed by his hearing of the
song, but in thread 36, Phil?s waking up is followed
immediately by the DJ banter. Similarly, threads 6,
23, 51 and 104 join the hearing of the song with
the hearing of the banter, but in the case of threads
120 and 123, the recounting of Phil?s hearing of the
song is followed directly by suicide attempts, with
no mention of the banter. In both these cases, we
can presume that the DAG remains constant, but the
threading represents either a complete traversal of
all the interior nodes, or, typically later in the narra-
tive, narrative ?shortcuts? which indicate the entire
wakeup DAG by explicitly mentioning only some
elements. Such shortcuts may be found in most narra-
tives. For example, subsequent references to a known
character or event may be reduced to the minimum,
since a simple mention reactivates the entire refer-
ence. Conversely, the exploration of interior nodes
rather than higher-level ones (in other words, provid-
ing more detail) may produce an effect of slowdown
(Bal, 1985).
In the case of semantic repetition, shortcuts like
those just described demonstrate that not only can
repetition occur in the absence of repeated formal el-
ements, but even in the absence of explicitly repeated
semantic elements. At the extreme, the activation of
a higher-level node by reference to an interior node
provides a model for literary allusions, perhaps the
most subtle type of repetition, where some element
in one text activates a reference to another.
7 Conclusions and next steps
The series of examples presented here provide evi-
dence for the existence of semantic repetition at both
the atomic and structural levels. They also show
that these can be captured by a model which permits
various levels of granularity, from atomic semantic
expressions to higher-level subDAGs and DAGs. It
must be admitted however that, at this stage of the
research, only human intelligence has permitted the
identification of semantic repetition in its various
forms. In an ideal world, a computer program might
be capable of arriving at the same judgments. Work
such as Chambers and Jurafsky (2008) or Hobbs et al
(1993) might provide a good starting point for this.
In the meantime, we believe that there is value in con-
tinuing the meaning-first perspective illustrated here,
as a complement to the more usual text-first analyses.
When combined with a user-friendly formalism, this
approach would go some way to bridging the divide
between computer scientists and literary specialists
in their analysis of literary texts.
59
References
Nicholas Asher and Alex Lascarides. 2003. Logics of
conversation. Cambridge University Press, Cambridge.
Mieke Bal. 1985. Narratology: introduction to the theory
of narrative. University of Toronto Press, Toronto.
Richard Bird. 1998. Introduction to functional program-
ming using Haskell. Prentice-Hall, London, 2nd edi-
tion.
Charles Callaway and James Lester. 2001. Evaluating the
effects of natural language generation techniques on
reader satisfaction. In Proceedings of the Twenty-Third
Annual Conference of the Cognitive Science Society,
pages 164?169.
Charles Callaway and James Lester. 2002. Narrative
prose generation. Artificial Intelligence, 139(2):213?
252.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceedings
of ACL-08: HLT, pages 789?797.
Fiorella de Rosis and Floriana Grasso. 2000. Affective
natural language generation. In A.M. Paiva, editor,
Affective instructions, pages 204?218. Springer, Berlin.
Ge?rard Genette. 1972. Figures III. Editions du Seuil,
Paris.
Ge?rard Genette. 1983. Nouveau discours du re?cit. Edi-
tions du Seuil, Paris.
Hermann Helbig. 2006. Knowledge representation and
the semantics of natural language. Springer, Berlin.
Simon Hewitt. 2012. The logic of finite order. Notre
Dame Journal of Formal Logic, 53(3):297?318.
Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, and
Paul Martin. 1993. Interpretation as abduction. Artifi-
cial Intelligence, 63:69?142.
Roman Jakobson. 1960. Linguistics and poetics. In
Thomas A Sebeok, editor, Style in language, pages
350?377. MIT, Cambridge, Mass.
Barbara Johnstone. 1991. Repetition in Arabic discourse:
paradigms, syntagms, and the ecology of language. J.
Benjamins, Amsterdam.
Greg Lessard, Ste?fan Sinclair, Max Vernet, Franc?ois
Rouget, Elisabeth Zawisza, Louis-E?mile Fromet de
Rosnay, and E?lise Blumet. 2004. Pour une recherche
semi-automatise?e des topo?? narratifs. In P. Enjalbert
and M. Gaio, editors, Approches se?mantiques du docu-
ment e?lectronique, pages 113?130. Europia, Paris.
Michael Levison and Greg Lessard. 2012. Is this a DAG
that I see before me? An onomasiological approach to
narrative analysis and generation. In Mark Finlayson,
editor, The Third Workshop on Computational Mod-
els of Narrative, pages 134?141, LREC Conference,
Istanbul.
Michael Levison, Greg Lessard, Craig Thomas, and
Matthew Donald. 2012. The Semantic Representation
of Natural Language. Bloomsbury, London.
Inderjeet Mani. 2012. Computational Modelling of Nar-
rative. Synthesis Lectures on Human Language Tech-
nologies. Morgan and Claypool.
William C. Mann and Sandra Thompson. 1988. Rhetori-
cal Structure Theory: toward a functional theory of text
organization. Text, 8(3):243?281.
Isidore Okpewho. 1992. African oral literature: back-
grounds, character, and continuity. Indiana University
Press, Bloomington.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding: An Inquiry into Hu-
man Knowledge Structures. Lawerence Erlbaum Asso-
ciates, Hillsdale, NJ.
Nicola Stanhope, Gillian Cohen, and Martin Conway.
1993. Very long-term retention of a novel. Applied
Cognitive Psychology, 7:239?256.
Deborah Tannen. 1989. Talking voices: repetition, dia-
logue, and imagery in conversational discourse. Cam-
bridge University Press, Cambridge.
Lucia M. Tovena and Marta Donazzan. 2008. On ways
of repeating. Recherches linguistiques de Vincennes,
37:85?112.
Reuven Tsur. 2008. Toward a theory of cognitive poetics.
Sussex Academic Press, Brighton, 2nd edition.
60
Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 25?30,
Gothenburg, Sweden, April 27, 2014.
c?2014 Association for Computational Linguistics
Time after Time:
Representing Time in Literary Texts
Michael Levison
School of Computing
Queen?s University, Canada
levison@cs.queensu.ca
Greg Lessard
French Studies
Queen?s University, Canada
greg.lessard@queensu.ca
Abstract
The representation of temporal informa-
tion in text represents a significant com-
putational challenge. The problem is par-
ticularly acute in the case of literary texts,
which tend to be massively underspeci-
fied, often relying on a network of seman-
tic relations to establish times and timings.
This paper shows how a model based on
threaded directed acyclic graphs makes it
possible to capture a range of subtle tem-
poral information in this type of text and
argues for an onomasiological approach
which places meaning before form.
1 Time and Text
This paper deals with the representation of tem-
poral phenomena in literary texts. As a result, it
builds on work from a number of fields.
1
Thus, it
takes as given the longstanding observation from
philosophy that time is not a simple issue of days,
months and years, but reflects issues of perception
and culture (Ricoeur, 1983). At the same time,
it assumes that the study of temporal phenomena
will be enhanced by use of a formal representation
(Allen, 1984). It further assumes the traditional
narratological distinction between the information
which underlies a text, variously known as the fab-
ula or histoire and which we will henceforth call
the story and some particular instantiation of this
in text, often called the sjuzhet or r?ecit, which we
will henceforth call the narrative (Genette, 1972).
Mani (2010), based on earlier work, suggests
that the temporal relations BEFORE, DURING,
IMMEDIATELY BEFORE, BEGINS, ENDS, SI-
MULTANEOUS AND OVERLAPS are adequate
for representing time in human languages. This
raises the interesting empirical question of how
1
In what follows, for lack of space, we restrict ourselves
to citing some typical examples from a vast literature.
well this model applies to literary texts, given their
complex but typically underspecified nature. In
fact, in the case of time, a literary text often gives
no explicit indication of temporal phenomena, but
relies on encyclopedic knowledge available to the
reader. In addition, we might ask how different
temporal relations are distributed across literary
texts, as compared with what is found in exposi-
tory or other types of texts, or simpler narratives.
At the same time, as Lascarides and Asher (1991)
point out, it is important to see temporal relations
as a subset of a broader range of relations includ-
ing Narration, Explanation, Background, and Re-
sult, all of which have temporal implications.
There does exist a growing body of analyses of
narrative texts, but most of these are based on rel-
atively simple third person narratives such as fa-
bles. Such texts tend to be event-driven (one thing
follows another) and they tend to lack more com-
plex literary phenomena such as first person narra-
tive, where all information is not known, multiple,
sometimes competing, perspectives, and signifi-
cant temporal shifts. It will be important to anal-
yse literary texts in their full complexity before
we are capable of pronouncing on the use of time.
This will no doubt be aided by research on nar-
rative generation, such as (Callaway and Lester,
2002), (Riedl and Young, 2012), and (Montfort,
2007), where temporal representations at the ab-
stract level are made use of, but this must be com-
plemented by empirical work on actual texts.
The empirical study of temporal relations in
complex literary texts will be complicated by the
fact that, despite recent progress (for example,
Kolomiyets et al. (2012)), parsers still do not
match the performance of humans in assigning
temporal points and relations. As a result, build-
ing a detailed corpus of literary texts will still take
some time and much human effort. When it is
undertaken, one of the fundamental decisions to
be faced will be what is to be represented. Most
25
work to date takes texts themselves as the object
of tagging, and schemes such as TimeML (Puste-
jovsky et al., 2003) are designed to allow quite
precise temporal information to be added to texts
in the form of markup. As a result, they focus
on phenomena in the narrative rather than in the
story. To put this another way, they adopt a se-
masiological perspective (from form to meaning),
rather than an onomasiological one (from meaning
to form) (Baldinger, 1964). However, it is reason-
able to ask whether the appropriate level of rep-
resentation should in fact be one level up, at the
level of the story. We argue here that this is the
case. Elson (2012) takes a first step in this direc-
tion by mapping the textual layer to a propositional
layer; however, most of the texts he deals with
are relatively simple. We will show below how,
in some complex examples, representing temporal
phenomena at the story level requires at the least
additional ?machinery? based on multiple points of
view, temporal shifts including prolepsis, and en-
cyclopedic knowledge, but that it also offers in-
sights into narrative structure not apparent at the
textual level.
2 DAGs and Threading
The story which underlies a literary text may be
represented by means of a directed acyclic graph,
henceforth DAG, composed of nodes connected
by unidirectional edges. The acyclicity requires
that no sequence of edges may return to an earlier
node. The nodes carry segments of meaning repre-
sented by semantic expressions. These are func-
tional representations, described in (Levison et al.,
2012). Each semantic expression is composed of
elements drawn from a semantic lexicon.
A simple example might be stab(brutus,
caesar), where the two entities brutus and
caesar denote certain human beings, and stab
is a function which takes two entities and returns a
completion
2
having the obvious meaning. On the
basis of functions such as these, it is possible to
construct the DAG shown in Fig. 1.
3
The unidirectional edges between the various
2
A completion may be thought of as the semantic equiva-
lent of an utterance, an entity as the semantic equivalent of a
noun, and an action as the semantic equivalent of a verb.
3
The DAGs shown here were constructed with yEd
(http://www.yworks.com/en/products_yed_
about.html), which generates a GraphML representation
for each graph. For simplicity, we have ignored represen-
tation of tense and aspect in these examples, although the
formalism permits this.
Figure 1: A DAG for the various states of Caesar
nodes represent semantic dependency, that is, the
fact that subsequent nodes depend upon informa-
tion contained in previous nodes, and by transitive
closure, parents of previous nodes. So, in Fig. 1,
the expression stab(brutus, caesar) de-
pends on the fact that Caesar is at the Senate, while
Caesar being dead depends on the stabbing. The
relation carried by edges may be one of order (one
node occurs before another), or of some sort of
causality, whereby a subsequent node is made pos-
sible by a previous node. In addition, nodes which
convey a coarse level of meaning may themselves
be refined into DAGs at a finer level. And so on,
recursively.
Since a DAG exists prior to any text which rep-
resents it, a text may begin at the start of the DAG
and follow the nodes, as in Caesar arrived at the
Senate, then Brutus stabbed him, then he died, or
alternatively at the end, as in Caesar died because
Brutus stabbed him after his arrival at the Senate,
in the middle, as in Brutus stabbed Caesar after he
arrived at the Senate, and then he died, or even in
a zigzag order, as in Caesar arrived at the Senate
and then died because Brutus stabbed him.
4
Each of these narrations may be represented by
a sequence of nodes, in other words, a thread,
showing the order in which the meaning carried
by the nodes is presented to the reader. Note that
the thread passes through some or all of the nodes,
but need not follow the edges of the DAG. Nor is
it constrained to be acyclic: it may visit the same
node more than once. An example of this is pro-
vided by a narration in which the same event is
recounted twice. To take an extreme case, in the
movie Groundhog Day (http://www.imdb.
com/title/tt0107048/), the character Phil
relives the same day and its events many times.
In our DAGs, we represent threads by a dot-
4
For more examples, see (Mani, 2012).
26
ted line to distinguish them from the edges of the
DAG. By threading coarse or refined DAGs, the
narration can be at different levels of detail. In ad-
dition, a single DAG may be traversed by multiple
threads representing, among other things, differ-
ent points of view. So, for example, suppose that a
third party, say Livia, finds Caesar?s dead body,
observes the stab wounds, and concludes that a
previously living Caesar has been killed. From the
point of view of the Livia thread, the ?Caesar is
dead? node is traversed before the stabbing node
(although from Livia?s point of view, it may not be
clear who has done the stabbing). Alternatively,
a fourth character may observe a stabbing in the
distance, then on approach note that the stabbee is
Caesar, assumed alive until that point, and finally
learn that Caesar is dead.
3 Relative and Absolute Timestamps
Within the DAG model, the simple chronological
ordering of events or activities requires no extra
features except perhaps ?colouring? certain edges
to distinguish between those which denote chrono-
logical dependence and those whose dependence
is based on other reasons. Figure 1 above illus-
trates this. However, more complex temporal rela-
tionships such as ?while? can be signified by nodes
indicating relative or absolute times, as in:
reltime(7)
{relative time}
exacttime(0900, 5, Mar, 2013)
{exact time}
Consider, for example, the DAG shown in Fig.
2. Here, both event1 and event2 take place af-
ter reltime(7) and before reltime(8).
5
If
no other activities take place in the same context,
we might conclude that while event1 was taking
place, event2 was happening elsewhere. Both
events conclude before event4. In addition,
event3 occurs after event1, but it may have
started before or after reltime(8); and there
is no information about its relation to event4.
Additional arguments can be added to specify
whether an event is durative or punctual, be-
cause nothing says that event1 actually began at
reltime(7) and ended at reltime(8). The
function exacttime() allows us to anchor parts
of the DAG at, or more precisely after, specific
moments.
5
The parameters to reltime, by the way, are irrelevant;
they are included only for the purposes of this commentary.
Figure 2: A DAG showing relative times and
events
4 Some Empirical Tests of the Formalism
To empirically test the model proposed here, we
will examine several actual texts. Of course, these
represent only a small selection of a vast range
of temporal phenomena. Our object is simply to
show how the proposed model may be applied.
4.1 Prolepsis
As noted earlier, a literary text may bring into play
a variety of perspectives. One of these is pro-
lepsis, or foreknowledge of a future event. Con-
sider the following example from Homer?s Iliad.
6
Achilles asks Zeus for success in a battle and that
Patroclus survive the battle. Zeus grants the first
wish, but not the second.
7
As a result, he (Zeus)
and by extension, we, as readers, know that Pa-
troclus will die. However Patroclus himself is un-
aware of this. We may represent this part of the
story by means of the DAG shown in Fig. 3, which
contains two sets of dependencies, one which links
Zeus to the decision that Patroclus will die, and the
other which links Patroclus to his fighting and dy-
ing. We may then capture the temporality of the
narrative by threading this DAG.
8
An example like this may seem arcane, but
cases of multiple points of view, or multiple
threading, are found in a variety of textual models.
Thus, in a murder mystery, the detective comes
to understand the ordering of particular events, in-
cluding the murder, and may subsequently explain
6
This instance of prolepsis is discussed in Grethlein
(2010).
7
Iliad, ch. 16, v. 249, http://classics.mit.
edu/Homer/iliad.html.
8
Threads are shown here by numbered dotted lines to in-
dicate their order, while dependencies are shown by unnum-
bered solid lines.
27
Figure 3: A DAG for part of the Iliad
this to the reader.
4.2 Parallel Series of Events
Consider the following passage from the Conan
Doyle story entitled The Red-headed League.
9
When I saw him that afternoon so
enwrapped in the music at St. James?s
Hall . . .
10
?You want to go home, no doubt,
Doctor,? he remarked as we emerged.
?Yes, it would be as well.?
?And I have some business to do
which will take some hours. . . . to-day
being Saturday rather complicates mat-
ters. I shall want your help to-night.?
?At what time??
?Ten will be early enough.?
?I shall be at Baker Street at ten.?
. . . It was a quarter-past nine when
I started from home and made my way
. . . to Baker Street. . . . On entering his
room I found Holmes in animated con-
versation with two men, . . .
The text itself provides two absolute times, one
prescriptive, that of the time when Watson is to
meet Holmes, and the other descriptive, the time
reported by Watson for his leaving home. Another
more approximate time is also provided, the fact
that Watson and Holmes are listening to music in
St James?s Hall on a Saturday afternoon. All of
these could be marked up in the text itself. How-
ever, others would provide a greater challenge. On
Watson?s return to meet Holmes, he discovers that
9
First published in the Strand magazine in 1891. See
http://www.gutenberg.org/ebooks/1661.
10
Several non-pertinent elements of the text have been
elided. These are shown by suspension points.
Figure 4: A DAG for part of the Red-headed
League
others are present, presumably at Holmes? invita-
tion, although this is not specified in the text it-
self. The chronology of Watson?s activities is pro-
vided only by its placement in the text, between
the conversation with Holmes and the return to
meet Holmes, while the arrival of the others can-
not be marked up at all at the textual level since it
is not even mentioned in the text. Such a model
provides a serious challenge to a semasiological
markup, for obvious reasons. However, it may be
easily represented by a DAG, as shown in Fig. 4.
Note that the nodes of the DAG are all en-
closed in a higher-level node situated on Satur-
day. This ?envelope? provides the framework for
the detailed events. However, within this enve-
lope, a branching occurs, separating Watson?s ex-
plicitly noted activities from those which we must
suppose Holmes to have accomplished. The two
series are bracketed between a relative temporal
marker (the moment when Watson and Holmes
leave each other) and an absolute temporal marker
(Watson?s arrival at Holmes? lodgings around 10).
4.3 Access to Encyclopaedic Information
Reading a text is not a simple activity. Among
other things, it requires a constant reference to
background ?encyclopaedic? information. The na-
ture of this information will vary from reader
to reader. As an illustration, consider the fol-
lowing paragraph, which opens Flaubert?s novel
Salammb?o.
11
11
We provide here the English translation from
http://www.gutenberg.org/files/1290/
1290-h/1290-h.htm#link2HCH0001.
28
Figure 5: A DAG for the introduction to
Salammb?o
It was at Megara, a suburb of
Carthage, in the gardens of Hamilcar.
The soldiers whom he had commanded
in Sicily were having a great feast to cel-
ebrate the anniversary of the battle of
Eryx, and as the master was away, and
they were numerous, they ate and drank
with perfect freedom.
At the most basic level, any reader may use the
tense (had commanded) and some lexical items
(anniversary) to determine the anteriority of the
battle of Eryx with respect to the feast. However,
more educated readers will probably be able to use
the proper name Carthage to locate the text in the
far past, while even more educated readers will be
able to use the names Hamilcar and Eryx to place
the feast after the period 244-242 BCE.
We may represent the interplay between what
is given by the text and the information available
to the reader (which, importantly, is also repre-
sentable by a DAG) as shown in Fig. 5, where
we see that the node exist(troops...),
represented in the text, depends on the node
command(hamilcar...) also represented in
the text. However, this latter node is a subnode of
the higher-level node describe(hamilcar),
which provides information (including tempo-
ral information) not present in the text. Sim-
ilarly, the node exist(battle...), present
in the text, is part of another higher-level
node (describe(punic-war)), which con-
tains more detailed encyclopaedic information.
This model captures both the temporal elastic-
ity provided by the interplay of logical depen-
dency and the varying levels of temporal assign-
ment noted above. To put this another way, it cap-
tures the set of readings which the same text may
carry for different readers. In particular, different
readings may thread this DAG at different levels
of granularity, some coarse, some finer.
5 Conclusions and Next Steps
Although they are limited to issues of time, the
examples studied above suggest that an onomasi-
ological approach gives access to textual and lit-
erary phenomena which escape tagging of tex-
tual contents alone. While the use of DAGs and
threading currently requires human intervention,
the output of the model, by its formality, provides
a means of studying in detail the instantiation of
stories as narratives, and thereby, a complement to
existing approaches to literary time.
References
James F. Allen. 1984. Towards a general theory of
action and time. Artificial Intelligence, 23:123?154.
Kurt Baldinger. 1964. S?emasiologie et onomasiologie.
Revue de linguistique romane, 28:249?272.
Charles Callaway and James Lester. 2002. Nar-
rative prose generation. Artificial Intelligence,
139(2):213?252.
David K. Elson. 2012. Modeling Narrative Discourse.
PhD thesis, Columbia University.
G?erard Genette. 1972. Figures III.
?
Editions du Seuil,
Paris.
Jonas Grethlein. 2010. The narrative reconfiguration
of time beyond Ricoeur. Poetics Today, 31(2):313?
329.
Oleksandr Kolomiyets, Steven Bethard and Marie-
France Moens. 2012. Extracting narrative timelines
as temporal dependency structures. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL?2012), pp. 88-97.
Alex Lascarides and Nicholas Asher. 1991. Discourse
relations and defeasible knowledge. In Proceedings
of the 29th Annual Meeting of the Association of
Computational Linguistics (ACL91), pp. 55-63.
Michael Levison, Greg Lessard, Craig Thomas, and
Matthew Donald. 2012. The Semantic Representa-
tion of Natural Language. Bloomsbury Publishing,
London.
Inderjeet Mani. 2012. Computational Modeling of
Narrative. Morgan and Claypool, San Rafael, CA.
Inderjeet Mani. 2010. The Imagined Moment: Time,
Narrative and Computation. University of Nebraska
Press, Lincoln, Nebraska.
29
Nick Montfort. 2007. Ordering events in interactive
fiction narratives. In Proceedings of the AAAI Fall
Symposium on Interactive Narrative Technologies.
Technical Report FS-07-05, B.S. Magerki and M.
Riedl, eds., AAAI Press, Menlo Park, CA, pp. 87?
94.
James Pustejovsky, Jose M. Casta?no, Robert Ingria,
Roser Saur??, Robert Gaizauskas, Andrea Setzer, and
Graham Katz. 2003. TimeML: Robust specification
of event and temporal expressions in text. In Fifth
International Workshop on Computational Seman-
tics (IWCS-5).
Paul Ric?ur. 1983. Temps et r?ecit. Volume 1.
?
Editions
du Seuil, Paris.
Mark Riedl and R. Michael Young. 2010. Narrative
planning: balancing plot and character. Journal of
Artificial Intelligence Research, 39:217?268.
30
