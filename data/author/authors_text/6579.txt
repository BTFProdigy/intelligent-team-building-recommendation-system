Towards Robust Semantic Role Labeling
Sameer S. Pradhan?
BBN Technologies
Wayne Ward??
University of Colorado
James H. Martin?
University of Colorado
Most semantic role labeling (SRL) research has been focused on training and evaluating on
the same corpus. This strategy, although appropriate for initiating research, can lead to over-
training to the particular corpus. This article describes the operation of ASSERT, a state-of-the
art SRL system, and analyzes the robustness of the system when trained on one genre of data
and used to label a different genre. As a starting point, results are first presented for training
and testing the system on the PropBank corpus, which is annotated Wall Street Journal (WSJ)
data. Experiments are then presented to evaluate the portability of the system to another source of
data. These experiments are based on comparisons of performance using PropBanked WSJ data
and PropBanked Brown Corpus data. The results indicate that whereas syntactic parses and
argument identification transfer relatively well to a new corpus, argument classification does
not. An analysis of the reasons for this is presented and these generally point to the nature of the
more lexical/semantic features dominating the classification task where more general structural
features are dominant in the argument identification task.
1. Introduction
Automatic, accurate, and wide-coverage techniques that can annotate naturally oc-
curring text with semantic structure can play a key role in NLP applications such as
information extraction (Harabagiu, Bejan, and Morarescu 2005), question answering
(Narayanan and Harabagiu 2004), and summarization. Semantic role labeling (SRL) is
one method for producing such semantic structure. When presented with a sentence,
a semantic role labeler should, for each predicate in the sentence, first identify and
then label its semantic arguments. This process entails identifying groups of words
in a sentence that represent these semantic arguments and assigning specific labels to
them. In the bulk of recent work, this problem has been cast as a problem in supervised
machine learning. Using these techniques with hand-corrected syntactic parses, it has
? Department of Speech and Language Processing, 10 Moulton Street, Room 2/245, Cambridge, MA 02138.
E-mail: sameer@cemantix.org.
?? The Center for Spoken Language Research, Campus Box 594, Boulder, CO 80309.
E-mail: whw@colorado.edu.
? The Center for Spoken Language Research, Campus Box 594, Boulder, CO 80309.
E-mail: martin@colorado.edu.
Submission received: 15 July 2006; revised submission received: 3 May 2007; accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
been possible to achieve accuracies within the range of human inter-annotator agree-
ment. More recent approaches have involved using improved features such as n-best
parses (Koomen et al 2005; Toutanova, Haghighi, and Manning 2005); exploiting argu-
ment interdependence (Jiang, Li, and Ng 2005); using information from fundamentally
different, and complementary syntactic, views (Pradhan, Ward et al 2005); combining
hypotheses from different labeling systems using inference (Ma`rquez et al 2005); as well
as applying novel learning paradigms (Punyakanok et al 2005; Toutanova, Haghighi,
and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual
information. Some have also tried to jointly decode the syntactic and semantic structures
(Yi and Palmer 2005; Musillo and Merlo 2006). This problem has also been the subject
of two CoNLL shared tasks (Carreras and Ma`rquez 2004; Carreras and Ma`rquez 2005).
Although all of these systems perform quite well on the standard test data, they show
significant performance degradation when applied to test data drawn from a genre
different from the data on which the system was trained. The focus of this article is
to present results from an examination into the primary causes of the lack of portability
across genres of data.
To set the stage for these experiments we first describe the operation of ASSERT, our
state-of-the art SRL system. Results are presented for training and testing the system on
the PropBank corpus, which is annotatedWall Street Journal (WSJ) data.
Experiments are then presented to assess the portability of the system to another
genre of data. These experiments are based on comparisons of performance using
PropBanked WSJ data and PropBanked Brown corpus data. The results indicate that
whereas syntactic parses and identification of the argument bearing nodes transfer
relatively well to a new corpus, role classification does not. Analysis of the reasons for
this generally point to the nature of the more lexical/semantic features dominating the
classification task, as opposed to the more structural features that are relied upon for
identifying which constituents are associated with arguments.
2. Semantic Annotation and Corpora
In this article, we report on the task of reproducing the semantic labeling scheme used
by the PropBank corpus (Palmer, Gildea, and Kingsbury 2005). PropBank is a 300k-word
corpus in which predicate argument relations are marked for almost all occurrences
of non-copula verbs in the WSJ part of the Penn Treebank (Marcus, Santorini, and
Marcinkiewicz 1993). PropBank uses predicate independent labels that are sequential
from ARG0 to ARG5, where ARG0 is the PROTO-AGENT (usually the subject of a tran-
sitive verb) and ARG1 is the PROTO-PATIENT (usually its direct object). In addition to
these core arguments, additional adjunctive arguments, referred to as ARGMs, are also
marked. Some examples are ARGM-LOC, for locatives, and ARGM-TMP, for temporals.
Table 1 shows the argument labels associated with the predicate operate in PropBank.
Following is an example structure extracted from the PropBank corpus. The syntax
tree representation along with the argument labels is shown in Figure 1.
[ARG0 It] [predicate operates] [ARG1 stores] [ARGM?LOC mostly in Iowa and Nebraska].
The PropBank annotation scheme assumes that a semantic argument of a predicate
aligns with one or more nodes in the hand-corrected Treebank parses. Although most
frequently the arguments are identified by one node in the tree, there can be cases where
the arguments are discontinuous and more than one node is required to identify parts
of the arguments.
290
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Table 1
Argument labels associated with the predicate operate (sense: work) in the PropBank corpus.
Tag Description
ARG0 Agent, operator
ARG1 Thing operated
ARG2 Explicit patient (thing operated on)
ARG3 Explicit argument
ARG4 Explicit instrument
Treebank trees can also have trace nodes which refer to another node in the tree, but
do not have any words associated with them. These can also be marked as arguments.
As traces are typically not reproduced by current automatic parsers, we decided not
to consider them in our experiments?whether or not they represent arguments of a
predicate. None of the previous work has attempted to recover such trace arguments.
PropBank also contains arguments that are coreferential.
We treat discontinuous and coreferential arguments in accordance to the CoNLL
shared task on semantic role labeling. The first part of a discontinuous argument is
labeled as it is, and the second part of the argument is labeled with a prefix ?C-?
appended to it. All coreferential arguments are labeled with a prefix ?R-? appended.
We follow the standard convention of using Section 02 to Section 21 as the training
set, Section 00 as the development set, and Section 23 as the test set. The training set
comprises about 90,000 predicates instantiating about 250,000 arguments and the test
set comprises about 5,000 predicates instantiating about 12,000 arguments.
3. Task Description
In ASSERT, the task of semantic role labeling is implemented by assigning role labels to
constituents of a syntactic parse. Parts of the overall process can be analyzed as three
different tasks as introduced by Gildea and Jurafsky (2002):
1. Argument Identification?This is the process of identifying parsed
constituents in the sentence that represent semantic arguments of
Figure 1
Syntax tree for a sentence illustrating the PropBank tags.
291
Computational Linguistics Volume 34, Number 2
Figure 2
Syntax tree for a sentence illustrating the PropBank arguments.
a given predicate. Each node in a parse tree can be classified (with
respect to a given predicate) as either one that represents a semantic
argument (i.e., a NON-NULL node) or one that does not represent
any semantic argument (i.e., a NULL node).
2. Argument Classification?Given constituents known to represent
arguments of a predicate, this process assigns the appropriate
argument labels to them.
3. Argument Identification and Classification?A combination of the two tasks.
For example, in the tree shown in Figure 2, the node IN that dominates for is a
NULL node because it does not correspond to a semantic argument. The node NP
that dominates about 20 minutes is a NON-NULL node, because it does correspond to
a semantic argument?ARGM-TMP.
4. ASSERT (Automatic Statistical SEmantic Role Tagger)
4.1 System Architecture
ASSERT
1 produces a separate set of semantic role labels for each candidate predicate in
a sentence. Because PropBank only annotates arguments for non-copula/non-auxiliary
verbs, those are also the predicates considered by ASSERT. ASSERT performs constituent-
based role assignment. The basic inputs are a sentence and a syntactic parse of the
sentence. For each constituent in the parse tree, the system extracts a set of features
and uses a classifier to assign a label to the constituent. The set of labels used are the
PropBank argument labels plus NULL, which means no argument is assigned to that
constituent for the predicate under consideration.
Support vector machines (SVMs) (Burges 1998; Vapnik 1998) have been shown to
perform well on text classification tasks, where data is represented in a high dimen-
sional space using sparse feature vectors (Joachims 1998; Kudo and Matsumoto 2000;
Lodhi et al 2002). We formulate the problem as a multi-class classification problem
using an SVM classifier. We employ a ONE vs ALL (OVA) approach to train n classifiers
for a multi-class problem. The classifiers are trained to discriminate between examples
1 www.cemantix.org/assert.
292
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
of each class, and those belonging to all other classes combined. During testing, the
classifier scores on an example are combined to predict its class label.
ASSERT was developed using TinySVM2 along with YamCha3 (Kudo and
Matsumoto 2000, 2001) as the SVM training and classification software. The system
uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C = 1;
and, tolerance of the termination criterion, e = 0.001. SVMs output distances from the
classification hyperplane, not probabilities. These distances may not be comparable
across classifiers, especially if different features are used to train each binary classifier.
These raw SVM scores are converted to probabilities by fitting to a sigmoid function as
done by Platt (2000).
The architecture just described has the drawback that each argument classification
is made independently, without considering other arguments assigned to the same
predicate. This ignores a potentially important source of information: that a predicate is
likely to instantiate a certain set of arguments. To represent this information, a backed-
off trigram model is trained for the argument sequences. In this model, the predicate is
considered as an argument and is part of the sequence. This model represents not only
what arguments a predicate is likely to take, but also the probability of a given sequence
of arguments. During the classification process the system generates an argument
lattice using the n-best hypotheses for each node in the syntax tree. A Viterbi search
through the lattice uses the probabilities assigned by the sigmoid as the observation
probabilities, along with the argument sequence language model probabilities, to find
the maximum likelihood path such that each node is either assigned a value belonging
to the PropBank arguments, or NULL. The search is also constrained so that no two
nodes that overlap are both assigned NON-NULL labels.
4.2 Features
The feature set used in ASSERT is a combination of features described in Gildea and
Jurafsky (2002) as well as those introduced in Pradhan et al (2004), Surdeanu et al
(2003), and the syntactic-frame feature proposed in (Xue and Palmer 2004). Following is
the list of features used.
4.2.1 Predicate. This is the predicate whose arguments are being identified. The surface
form as well as the lemma are added as features.
4.2.2 Path. The syntactic path through the parse tree from the parse constituent to the
predicate being classified.
For example, in Figure 3, the path from ARG0 (The lawyers) to the predicate went is
represented with the string NP?S?VP?VBD. ? and ? represent upward and downward
movement in the tree, respectively.
4.2.3 Phrase Type. Syntactic category (NP, PP, etc.) of the constituent.
4.2.4 Position.Whether the constituent is before or after the predicate.
2 www.chasen.org/~taku/software/TinySVM/.
3 www.chasen.org/~taku/software/YamCha/.
293
Computational Linguistics Volume 34, Number 2
Figure 3
Illustration of path NP?S?VP?VBD.
4.2.5 Voice. Whether the predicate is realized as an active or passive construction. A
set of hand-written tgrep expressions operating on the syntax tree is used to identify
passives.
4.2.6 SubCategorization. This is the phrase structure rule expanding the predicate?s parent
node in the parse tree. For example, in Figure 3, the subcategorization for the predicate
?went? is VP?VBD-PP-NP.
4.2.7 Predicate Cluster. The distance function used for clustering is based on the intuition
that verbs with similar semantics will tend to have similar direct objects. For example,
verbs such as eat, devour, and savor will tend to all occur with direct objects describing
food. The clustering algorithm uses a database of verb?direct-object relations extracted
by Lin (1998). The verbs were clustered into 64 classes using the probabilistic co-
occurrence model of Hofmann and Puzicha (1998). We then use the verb class of the
current predicate as a feature.
4.2.8 Head Word. Syntactic head of the constituent.
4.2.9 Head Word POS. Part of speech of the head word.
4.2.10 Named Entities in Constituents. Binary features for seven named entities
(PERSON, ORGANIZATION, LOCATION, PERCENT, MONEY, TIME, DATE) tagged by
IdentiFinder (Bikel, Schwartz, and Weischedel 1999).
4.2.11 Path Generalizations.
1. Partial Path?Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
2. Clause-based path variations?Position of the clause node (S, SBAR)
seems to be an important feature in argument identification (Hacioglu
et al 2004). Therefore we experimented with four clause-based path
feature variations.
(a) Replacing all the nodes in a path other than clause nodes with an
asterisk. For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD.
294
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
(b) Retaining only the clause nodes in the path, which for the given
example would produce NP?S?S?VBD.
(c) Adding a binary feature that indicates whether the constituent is in
the same clause as the predicate.
(d) Collapsing the nodes between S nodes, which gives
NP?S?NP?VP?VBD.
3. Path n-grams?This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes: NP?S?VP,
S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, and so on. Shorter paths were
padded with nulls.
4. Single character phrase tags?Each phrase category is clustered to a
category defined by the first character of the phrase label.
4.2.12 Predicate Context. We added the predicate context to capture predicate sense
variations. Two words before and two words after were added as features. The POS
of the words were also added as features.
4.2.13 Punctuation. Punctuation plays an particularly important role for some adjunctive
arguments, so punctuation on the left and right of the constituent are included as
features. The absence of punctuation in either location was indicated with a NULL
feature value.
4.2.14 Head Word of PP. Many adjunctive arguments, such as temporals and locatives,
occur as prepositional phrases in a sentence, and it is often the case that the head words
of those phrases, which are prepositions, are not very discriminative; for example, in
the city and in a few minutes both share the same head word in and neither contain a
named entity, but the former is ARGM-LOC, whereas the latter is ARGM-TMP. The head
word of the first noun phrase inside the prepositional phrase is used for this feature.
Preposition information is represented by appending it to the phrase type, for example,
?PP-in? instead of ?PP.?
4.2.15 First and Last Word/POS in Constituent. The first and last words in a constituent
along with their parts of speech.
4.2.16 Ordinal Constituent Position. In order to avoid false positives where constituents
far away from the predicate are spuriously identified as arguments, we added this
feature which is a concatenation of the constituent type and its ordinal position from
the predicate.
4.2.17 Constituent Tree Distance. This is a more fine-grained way of specifying the already
present position feature. This is the number of constituents that are encountered in the
path from the predicate to the constituent under consideration.
4.2.18 Constituent Relative Features. These are nine features representing the phrase type,
head word, and head word part of speech of the parent, and left and right siblings of
the constituent.
4.2.19 Temporal Cue Words. There are several temporal cue words that are not captured
by the named entity tagger and were added as binary features indicating their presence.
295
Computational Linguistics Volume 34, Number 2
The BOW toolkit was used to identify words and bigrams that had highest average
mutual information with the ARGM-TMP argument class.
4.2.20 Syntactic Frame. Sometimes there are multiple children under a constituent having
the same phrase type, and one or both of them represent arguments of the predicate. In
such situations, the path feature is not very good at discriminating between them, and
the position feature is also not very useful. To overcome this limitation, Xue and Palmer
(2004) proposed a feature which they call the syntactic frame. For example, if the sub-
categorization for the predicate is VP?VBD-NP-NP, then the syntactic frame feature
for the first NP in the sequence would be, ?vbd NP np,? and for the second it would be
?vbd np NP.?
4.3 Performance
Table 2 illustrates the performance of the system using Treebank parses and using parses
produced by a Charniak parser (Automatic). Precision (P), Recall (R), and F-scores are
given for the identification and combined tasks, and Classification Accuracy (A) for the
classification task. Classification performance using Charniak parses is only 1% absolute
worse than when using Treebank parses. On the other hand, argument identification
performance using Charniak parses is 10.9% absolute worse. About half of the ID errors
are due to missing constituents in the Charniak parse. Techniques to address the issue
of constituents missing from the syntactic parse tree are reported in Pradhan, Ward
et al (2005).
4.4 Feature Salience
In Pradhan, Hacioglu et al (2005) we reported on a series of experiments to show the
relative importance of features to the Identification task and the Classification task.
The data show that different features are more salient for each of the two tasks. For
the Identification task, the most salient features are the Path and Partial Path. The
Predicate was not particularly salient. For Classification, the most salient features are
Head Word, First Word, and Last Word of a constituent as well as the Predicate itself.
For Classification, the Path and Phrase Type features were not very salient.
A reasonable conclusion is that structural features dominate the Identification task,
whereas more specific lexical or semantic features are important for Classification. As
Table 2
Performance of ASSERT on WSJ test set (Section 23) using correct Treebank parses as well as
Charniak parses.
Parse Task P (%) R (%) F A (%)
Treebank Id. 97.5 96.1 96.8
Class. ? ? ? 93.0
Id. + Class. 91.8 90.5 91.2
Automatic Id. 87.8 84.1 85.9
Class. ? ? ? 92.0
Id. + Class. 81.7 78.4 80.0
296
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
we?ll see later, this pattern has critical implications for the portability of these features
across genres.
5. Robustness to Genre of Data
Most work on SRL systems has been focused on improving the labeling performance
on a test set belonging to the same genre of text as the training set. Both the Treebank on
which the syntactic parser is trained, and the PropBank on which the SRL systems are
trained represent articles from the year 1989 of theWall Street Journal. Improvements to
the system may reflect tuning to the specific data set rather than real progress. For this
technology to be widely accepted it is critical that it perform reasonably well on text
with styles different from the training data. The availability of PropBank annotation
for another corpus of a very different style than WSJ makes it possible to evaluate
the portability of SRL techniques, and to understand some of the factors affecting
performance.
5.1 The Brown Corpus
The Brown Corpus is a standard corpus of American English that consists of about one
million words of English text printed in the calendar year 1961 (Kuc?era and Francis
1967). The corpus contains about 500 samples of 2,000+ words each. The motivation
for creating this corpus was to create a heterogeneous sample of English text useful for
comparative language studies. Table 3 lists the sections in the Brown corpus.
5.2 Semantic Annotation
Release 3 of the Penn Treebank contains hand-corrected syntactic trees from a subset
of the Brown Corpus (sections F, G, K, L, M, N, P, and R). Sections belonging to the
newswire genre were not included because a considerable amount of similar material
was already available from the WSJ portion of the Treebank. Palmer, Gildea, and
Kingsbury (2005) annotated a significant portion of the Treebanked Brown corpus
Table 3
List of sections in the Brown corpus.
A. Press reportage
B. Press editorial
C. Press reviews (theater, books, music, and dance)
D. Religion
E. Skills and hobbies
F. Popular lore
G. Belles lettres, biography, memoirs, etc.
H. Miscellaneous
J. Learned
K. General fiction
L. Mystery and detective fiction
M. Science fiction
N. Adventure and Western fiction
P. Romance and love story
R. Humor
297
Computational Linguistics Volume 34, Number 2
with PropBank roles. The PropBanking philosophy is the same as described earlier.
In all, about 17,500 predicates are tagged with their semantic arguments. For these
experiments we use the release of the Brown PropBank dated September 2005.
Table 4 shows the number of predicates that have been tagged for each section:
6. Robustness Experiments
In this section, we present a series of experiments comparing the performance of ASSERT
on the WSJ corpus to performance on the Brown corpus. The intent is to understand
how well the algorithms and features transfer to other sources and to understand the
nature of any problems.
6.1 Cross-Genre Testing
The first experiment evaluates the performance of the system when it is trained on
annotated data from one genre of text (WSJ) and is used to label a test set from a different
genre (the Brown corpus). The ASSERT system described earlier, trained on WSJ Sec-
tions 02?21, was used to label arguments for the PropBanked portion of the Brown
corpus. As before, the Charniak parser was used to generate the syntax parse trees.
Table 5 shows the F-score for Identification and combined Identification and Classi-
fication for each of the eight different text genres as well as the overall performance
on Brown. As can be seen, there is a significant degradation across all the various
sections of Brown. In addition, although there is a noticeable drop in performance for
the Identification task, the bulk of the degradation comes in the combined task.
The following are among the likely factors contributing to this performance
degradation:
1. Syntactic parsing errors?The semantic role labeler is completely
dependent on the quality of the syntactic parses; missing, mislabeled,
and misplaced constituents will all lead to errors. Because the syntactic
parser used to generate the parse trees is heavily lexicalized, the genre
difference will have an impact on the accuracy of the parses, and the
features extracted from them.
2. The Brown corpus may in fact be fundamentally more difficult than the
WSJ. There are many potential sources for this kind of difficulty. Among
Table 4
Number of predicates that have been tagged in the PropBanked portion of the Brown corpus.
Section Total Propositions Total Lemmas
F 926 321
G 777 302
K 8,231 1,476
L 5,546 1,118
M 167 107
N 863 269
P 788 252
R 224 140
298
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Table 5
Performance on the entire PropBanked Brown corpus when ASSERT is trained on WSJ.
Train Test Id. F Id. + Class F
WSJ WSJ (Section 23) 85.9 80.0
WSJ Brown (Popular lore) 77.2 64.9
WSJ Brown (Biography, memoirs) 77.1 61.1
WSJ Brown (General fiction) 78.9 64.9
WSJ Brown (Detective fiction) 82.9 67.1
WSJ Brown (Science fiction) 83.8 64.5
WSJ Brown (Adventure) 82.5 65.5
WSJ Brown (Romance and love story) 81.2 63.9
WSJ Brown (Humor) 78.8 62.5
WSJ Brown (All) 81.2 63.9
Table 6
Deleted/missing argument-bearing constituents in Charniak parses of the WSJ test set
(Section 23) and the entire PropBanked Brown corpus.
Total Misses %
WSJ (Section 23) 13,612 851 6.2
Brown (Popular lore) 2,280 219 9.6
Brown (Biography, memoirs) 2,180 209 9.6
Brown (General fiction) 21,611 1,770 8.2
Brown (Detective fiction) 14,740 1,105 7.5
Brown (Science fiction) 405 23 5.7
Brown (Adventure) 2,144 169 7.9
Brown (Romance and love story) 1,928 136 7.1
Brown (Humor) 592 61 10.3
Brown (All) 45,880 3,692 8.1
the most obvious sources are a greater diversity in the range of use of
predicates and headwords in the Brown domain. That is, the lexical
features may be more varied in terms of predicate senses and raw
number of predicates. More consistent usage of predicates and
headwords in the WSJ may allow very specific features to be trained
in WSJ that will not be as well trained or as salient in Brown.
The following discussion explores each of these possibilities in turn.
Table 6 shows the percentage of argument-bearing nodes deleted from the syntactic
parse leading to an Identification error. The syntactic parser deletes 6.2% of the argu-
ment bearing nodes in the tree when it is trained and tested on WSJ. When tested on
Brown, this number increases to 8.1%, a relative increase of 30%. This effect goes some
way toward explaining the decrease in Identification performance, but does not explain
the large degradation in combined task performance.
The effect of errors from the syntactic parse can be removed by using the correct
syntactic trees from the Treebanks for both corpora. This permits an analysis of other
299
Computational Linguistics Volume 34, Number 2
factors affecting the performance difference. For this experiment, we evaluated per-
formance for all combinations of training and testing on WSJ and Brown. A test set
for the Brown corpus was generated by selecting every tenth sentence in the corpus.
The development set used by Bacchiani et al (2006) was withheld for future parameter
tuning. No parameter tuning was done for these experiments. The parameters used
for the data reported in Table 2 were used for all subsequent tests reported in this
article. This procedure results in a training set for Brown that contains approximately
14k predicates. In order to have training sets comparable in size for the two corpora,
stratified sampling was used to create a WSJ training set of the same size as the Brown
training set. Section 23 of WSJ is still used as the test set for that corpus.
Table 7 shows the results of this experiment. Rows 2 and 4 show the conditions
when the system is trained on the 14k predicate WSJ training. Testing on Brown vs. WSJ
results in a modest reduction in F-score from 95.3 to 93.0 for argument identification.
Although there is some reduction in Identification performance in the absence of errors
in the syntactic parse tree, the effect is not large. However, argument classification
shows a large drop in accuracy from 86.1% to 72.9%. These data reiterate the point that
syntactic parse errors are not the major factor accounting for the reduction in performance
for Brown.
The next point to note is the effect of varying the amount of training data for WSJ
for testing results on WSJ and Brown. The first row of Table 7 shows the performance
when ASSERT is trained on the full WSJ training set of Sections 2?21 (90k predicates).
The second row shows performance when it is trained on the reduced set of 14k pred-
icates. Whereas the F1 score for Identification dropped by 1.5 percentage points (from
96.8% to 95.3%) the Classification rate dropped by 6.9% percent absolute. Classification
seemingly requires considerable more data before its performance begins to asymptote.
Table 7
Performance when ASSERT is trained using correct Treebank parses, and is used to classify test
set from either the same genre or another. For each data set, the number of examples used for
training are shown in parentheses.
SRL Train SRL Test Task P (%) R (%) F A (%)
WSJ WSJ Id. 97.5 96.1 96.8
(90k) (5k) Class. 93.0
Id. + Class. 91.8 90.5 91.2
WSJ WSJ Id. 96.3 94.4 95.3
(14k) (5k) Class. 86.1
Id. + Class. 84.4 79.8 82.0
BROWN BROWN Id. 95.7 94.9 95.2
(14k) (1.6k) Class. 80.1
Id. + Class. 79.9 77.0 78.4
WSJ BROWN Id. 94.6 91.5 93.0
(14k) (1.6k) Class. 72.9
Id. + Class. 72.1 67.2 69.6
BROWN WSJ Id. 94.9 93.8 94.3
(14k) (5k) Class. 78.3
Id. + Class. 76.6 73.3 74.9
300
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Finally, row 3 shows the performance for training and testing on Brown. The
performance of argument Identification is essentially the same as when training and
testing on WSJ. However, argument Classification is 6 percentage points worse (80.1%
vs. 86.1%) when training and testing on Brown than when training and testing on WSJ.
This pattern is consistent with our third hypothesis given previously: Brown may be an
intrinsically harder corpus for this task.
Some possible causes for this difficulty are:
1. More unique predicates or head words than are seen in the WSJ set, so
there is less training data for each;
2. More predicate sense ambiguity in Brown;
3. Less consistent relations between predicates and head words;
4. A greater preponderance of difficult semantic roles in Brown;
5. Relatively fewer examples of predictive features such as named entities.
The remainder of this section explores each of these possibilities in turn.
In order to test the importance of predicate sense in this process, we added oracle
predicate sense information as a feature in ASSERT. Because only about 60% of the
PropBanked Brown corpus was tagged with predicate sense information, these results
are not directly comparable to the one reported in the earlier tables. In this case, both the
Brown training and test sets are subsets of the earlier ones, with about 10k predicates
in training and 1k in testing. For comparison, we used the same size WSJ training
data. Table 8 shows the performance when trained on WSJ and Brown, and tested on
Brown, with andwithout predicate sense information, and for both Treebank parses and
Charniak parses. We find that there is a small increase in the combined identification
and classification performance when trained on Brown and tested on Brown.
One reason for this could simply be the raw number of instances that are seen in
the training data. Because we know that Predicate and Head Word are two particularly
salient features for classification, the percentages of a combination of these features in
the Brown test set that are seen in both the training sets should be informative. This
information is shown in Table 9. In order to get a cross-corpus statistic, we also present
the same numbers on the WSJ test set.
Table 8
Performance on Brown test, using Brown and WSJ training sets, with and without oracle
predicate sense information when using Treebank parses.
Id. Id. + Class.
Train Predicate Sense P % R % F P % R % F
Brown
(10k) ? 95.6 95.4 95.5 78.6 76.2 77.4?
95.7 95.7 95.7 81.1 77.1 79.0
WSJ
(10k) ? 93.4 91.7 92.5 71.1 65.8 68.4?
93.3 91.8 92.5 71.3 66.1 68.6
301
Computational Linguistics Volume 34, Number 2
Table 9
Features seen in training for various test sets.
Test? WSJ Brown
Features T seen t seen T seen t seen
Corpora ? (%) (%) (%) (%)
WSJ Predicate Lemma (P) 76 94 65 80
Predicate Sense (S) 79 93 64 78
Head Word (HW) 61 87 49 76
P+HW 19 31 13 17
Brown Predicate Lemma (P) 64 85 86 94
Predicate Sense (S) 29 35 91 96
Head Word (HW) 37 63 68 87
P+HW 10 17 27 33
T = types; t = tokens.
It can be seen that for both theWSJ and Brown corpus test sets, the number of predi-
cate lemmas as well as the particular senses seen in the respective test sets is quite high.
However, a cross comparison shows that there is about a 15% drop in coverage from
WSJ/WSJ to WSJ/Brown. It is also interesting to note that for WSJ, the drop in coverage
for predicate lemmas is almost the same as that for individual predicate senses. This fur-
ther confirms the hypothesis thatWSJ has a more homogeneous collection of predicates.
When we compare the drop in coverage for Brown/Brown vs. WSJ/Brown, we find
about the same drop in coverage for predicate lemmas, but a much more significant
drop for the senses. This variation in senses in Brown is probably the reason that adding
sense information helps more for the Brown test set. In the WSJ case, the addition of
word sense as a feature does not add much information, and so the numbers are not
much different than for the baseline. Similarly, we can see that percentage of headwords
seen across the two genres also drop significantly, and they are much lower to begin
with. Finding the coverage for the predicate lemma and head word combination is still
worse, and this is not even considering the sense. Therefore, data sparseness is another
potential reason that the importance of the predicate sense feature does not reflect in the
performance numbers.
As noted earlier, another possible source of difficulty for Brown may be the distri-
bution of PropBank arguments in this corpus. Table 10 shows the classification perfor-
mance for each argument, for each of the four configurations (train on Brown orWSJ and
test on WSJ or Brown). Among the two most frequent arguments?ARG0 and ARG1?
ARG1 seems to be affected the most. When the training and test sets are from the same
genre, the performance on ARG0 is slightly worse on the Brown test set. ARG1 on the
other hand is about 5% worse on both precision and recall, when trained and tested on
Brown. For core-arguments ARG2?5 which are highly predicate sense dependent, there
is a much larger performance drop.
Finally, another possible reason for the drop in performance is the distribution of
named entities in the corpus. Table 11 shows the frequency of occurrence of name
entities in 10k WSJ and Brown training sets. It can be seen that number of organizations
talked about in Brown is much smaller than in WSJ, and there are more person names.
Also, monetary amounts which frequently fill the ARG3 and ARG4 slots are also much
more infrequent in Brown, and so is the incidence of percentages. This would definitely
have some impact on the usability of these features in the learned models.
302
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
7. Effect of Improved Syntactic Parses
Practical natural language processing systems will always use errorful automatic
parses, and so it would be interesting to find out how much syntactic parser errors hin-
der performance on the task of semantic role labeling. Fortunately, recent improvements
to the Charniak parser provided an opportunity to test this hypothesis. We use the latest
version of the Charniak parser that does n-best re-ranking (Charniak and Johnson 2005)
and the model that is self-trained using the North American News corpus (NANC).
This version adaptsmuch better to the Brown corpus (McClosky, Charniak, and Johnson
Table 10
Classification accuracy for each argument type in the WSJ (W) and Brown (B) test sets.
W?W B?B B?W W?B
Number in Number in P R P R P R P R
Argument WSJ Test Brown Test (%) (%) (%) (%) (%) (%) (%) (%)
ARG0 3,149 1,122 91.1 96.8 90.4 92.8 83.4 92.2 87.4 93.3
ARG1 4,264 1,375 90.2 92.0 85.0 88.5 78.7 79.7 83.4 89.0
ARG2 796 312 73.3 66.6 65.9 60.6 49.7 56.4 59.5 48.1
ARG3 128 25 74.3 40.6 71.4 20.0 30.8 16.0 28.6 4.7
ARG4 72 20 89.1 68.1 57.1 60.0 16.7 5.0 61.1 15.3
C-ARG0 2 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
C-ARG1 165 34 91.5 64.8 80.0 35.3 64.7 32.4 82.1 19.4
R-ARG0 189 45 83.1 93.7 82.7 95.6 62.5 88.9 76.8 77.2
R-ARG1 122 44 77.8 63.1 91.7 75.0 64.5 45.5 54.5 59.8
ARGM-ADV 435 290 78.0 66.0 67.6 64.8 74.7 44.8 49.9 71.0
ARGM-CAU 65 15 82.5 72.3 80.0 53.3 62.5 66.7 86.0 56.9
ARGM-DIR 72 114 57.1 50.0 71.0 62.3 46.6 36.0 39.7 43.1
ARGM-DIS 270 65 87.6 86.7 81.0 72.3 54.1 70.8 89.6 64.1
ARGM-EXT 31 10 83.3 48.4 0.0 0.0 0.0 0.0 33.3 3.2
ARGM-LOC 317 147 73.8 80.8 60.8 70.7 52.6 48.3 60.6 65.6
ARGM-MNR 305 144 56.1 59.0 64.5 63.2 42.6 55.6 51.4 48.9
ARGM-MOD 454 129 99.6 100.0 100.0 100.0 100.0 99.2 99.6 100.0
ARGM-NEG 201 85 100.0 99.5 97.7 98.8 100.0 85.9 94.8 99.5
ARGM-PNC 99 43 60.4 58.6 66.7 55.8 54.8 39.5 52.8 57.6
ARGM-PRD 5 8 0.0 0.0 33.3 12.5 0.0 0.0 0.0 0.0
ARGM-TMP 978 280 85.4 90.4 84.8 85.4 71.3 83.6 82.2 76.0
W?B = ASSERT trained on B and used to classify W test set.
Table 11
Distribution of the named entities in a 10k data fromWSJ and Brown corpora.
Name Entity WSJ Brown
PERSON 1,274 2,037
ORGANIZATION 2,373 455
LOCATION 1,206 555
MONEY 831 32
DATE 710 136
PERCENT 457 5
TIME 9 21
303
Computational Linguistics Volume 34, Number 2
Table 12
Performance for different versions of the Charniak parser used in the experiments.
Train Test F
WSJ WSJ 91.0
WSJ Brown 85.2
Brown Brown 88.4
WSJ+NANC Brown 87.9
2006a, 2006b). We also use another model that is trained on the Brown corpus itself. The
performance of these parsers is shown in Table 12.
We describe the results of the following five experiments:
1. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked WSJ sentences. The syntactic parser (Charniak
parser) is itself trained on the WSJ training sections of the Treebank. This
is used to classify Section 23 of WSJ.
2. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked WSJ sentences. The syntactic parser (Charniak
parser) is itself trained on the WSJ training sections of the Treebank. This
is used to classify the Brown test set.
3. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked Brown corpus sentences. The syntactic parser
is trained using the WSJ portion of the Treebank. This is used to classify
the Brown test set.
4. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked Brown corpus sentences. The syntactic parser
is trained using the Brown training portion of the Treebank. This is used
to classify the Brown test set.
5. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked Brown corpus sentences. The syntactic parser
is the version that is self-trained using 2,500,000 sentences from NANC,
and where the starting version is trained only on WSJ data (McClosky,
Charniak, and Johnson 2006b). This is used to classify the Brown test set.
The same training and test sets used for the systems in Table 7 are used in this
experiment. Table 13 shows the results. For simplicity of discussion we have labeled the
five conditions as A, B, C, D, and E. Comparing conditions B and C shows that when the
features used to train ASSERT are extracted using a syntactic parser that is trained onWSJ
it performs at almost the same level on the task of identification, regardless of whether
it is trained on the PropBanked Brown corpus or the PropBanked WSJ corpus. This,
however, is about 5?6 F-score points lower than when all the three (the syntactic parser
training set, ASSERT training set, and ASSERT test set) are from the same genre?WSJ or
Brown, as seen in A and D. For the combined task, the gap between the performance
for conditions B and C is about 10 F-score points apart (59.1 vs. 69.8). Looking at the
argument classification accuracies, we see that using ASSERT trained on WSJ to test
Brown sentences results in a 12-point drop in F-score. Using ASSERT trained on Brown
304
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Table 13
Performance on WSJ and Brown test sets when ASSERT is trained on features extracted from
automatically generated syntactic parses.
Setup Parser Train SRL Train SRL Test Task P (%) R (%) F A (%)
A. WSJ WSJ WSJ Id. 87.3 84.8 86.0
(40k ? sec:00?21) (14k) (5k) Class. 84.1
Id. + Class. 77.5 69.7 73.4
B. WSJ WSJ Brown Id. 81.7 78.3 79.9
(40k ? sec:00?21) (14k) (1.6k) Class. 72.1
Id. + Class. 63.7 55.1 59.1
C. WSJ Brown Brown Id. 81.7 78.3 80.0
(40k ? sec:00?21) (14k) (1.6k) Class. 79.2
Id. + Class. 78.2 63.2 69.8
D. Brown Brown Brown Id. 87.6 82.3 84.8
(20k) (14k) (1.6k) Class. 78.9
Id. + Class. 77.4 62.1 68.9
E. WSJ+NANC Brown Brown Id. 87.7 82.5 85.0
(2,500k) (14k) (1.6k) Class. 79.9
Id. + Class. 77.2 64.4 70.0
H. WSJ+NANC Brown WSJ Id. 88.2 78.2 82.8
(2,500k) (14k) (5k) Class. 76.9
Id. + Class. 75.4 51.6 61.2
using the WSJ-trained syntactic parser reduces accuracy by about 5 F-score points.
When ASSERT is trained on Brown using a syntactic parser also trained on Brown, we
get a quite similar classification performance, which is again about 5 points lower than
what we get using all WSJ data. Finally, looking at conditions C and D we find that
the difference in performance on the combined task of identification and classification
using the Brown corpus for training ASSERT is very close (69.8 vs. 68.9) even though
the syntactic parser used in C has a performance that is about 3.2 points worse than
that used in D. This indicates that better parse structure is less important than lexical
semantic coverage for obtaining better performance on the Brown corpus.
8. Adapting to a New Genre
One possible way to ameliorate the effects of domain specificity is to incrementally
add small amounts of data from a new domain to the already available out-of-domain
training data. In the following experiments we explore this possibility by slowly adding
data from the Brown corpus to a fixed amount of WSJ data.
One section of the Brown corpus?section K?has about 8,200 predicates anno-
tated. Therefore, we will take six different scenarios?two in which we will use correct
Treebank parses, and the four others in which we will use automatically generated
parses using the variations used before. All training sets start with the same number
of examples as that of the Brown training set. The part of this section used as a test set
for the CoNLL 2005 shared task was used as the test set for these experiments. This test
set contains 804 predicates in 426 sentences of Brown section K.
305
Computational Linguistics Volume 34, Number 2
Table 14 shows the results. In all six settings, the performance on the task of
identification and classification improves gradually until about 5,625 examples of sec-
tion K, which is about 75% of the total added, above which it adds very little. Even
when the syntactic parser is trained on WSJ and the SRL is trained on WSJ, adding
7,500 instances of this new genre achieves almost the same performance as when all
three are from the same genre (67.2 vs. 69.9). For the task of argument identification, the
incremental addition of data from the new genre shows only minimal improvement.
The system that uses a self-trained syntactic parser performs slightly better than other
Table 14
Effect of incrementally adding data from a new genre.
Id. Id. + Class
Parser Train SRL Train P (%) R (%) F P (%) R (%) F
WSJ WSJ (14k) (Treebank parses)
(Treebank +0 examples from K 96.2 91.9 94.0 74.1 66.5 70.1
parses) +1,875 examples from K 96.1 92.9 94.5 77.6 71.3 74.3
+3,750 examples from K 96.3 94.2 95.1 79.1 74.1 76.5
+5,625 examples from K 96.4 94.8 95.6 80.4 76.1 78.1
+7,500 examples from K 96.4 95.2 95.8 80.2 76.1 78.1
Brown Brown (14k) (Treebank parses)
(Treebank +0 examples from K 96.1 94.2 95.1 77.1 73.0 75.0
parses) +1,875 examples from K 96.1 95.4 95.7 78.8 75.1 76.9
+3,750 examples from K 96.3 94.6 95.3 80.4 76.9 78.6
+5,625 examples from K 96.2 94.8 95.5 80.4 77.2 78.7
+7,500 examples from K 96.3 95.1 95.7 81.2 78.1 79.6
WSJ WSJ (14k)
(40k) +0 examples from K 83.1 78.8 80.9 65.2 55.7 60.1
+1,875 examples from K 83.4 79.3 81.3 68.9 57.5 62.7
+3,750 examples from K 83.9 79.1 81.4 71.8 59.3 64.9
+5,625 examples from K 84.5 79.5 81.9 74.3 61.3 67.2
+7,500 examples from K 84.8 79.4 82.0 74.8 61.0 67.2
WSJ Brown (14k)
(40k) +0 examples from K 85.7 77.2 81.2 74.4 57.0 64.5
+1,875 examples from K 85.7 77.6 81.4 75.1 58.7 65.9
+3,750 examples from K 85.6 78.1 81.7 76.1 59.6 66.9
+5,625 examples from K 85.7 78.5 81.9 76.9 60.5 67.7
+7,500 examples from K 85.9 78.1 81.7 76.8 59.8 67.2
Brown Brown (14k)
(20k) +0 examples from K 87.6 80.6 83.9 76.0 59.2 66.5
+1,875 examples from K 87.4 81.2 84.1 76.1 60.0 67.1
+3,750 examples from K 87.5 81.6 84.4 77.7 62.4 69.2
+5,625 examples from K 87.5 82.0 84.6 78.2 63.5 70.1
+7,500 examples from K 87.3 82.1 84.6 78.2 63.2 69.9
WSJ+NANC Brown (14k)
(2,500k) +0 examples from K 89.1 81.7 85.2 74.4 60.1 66.5
+1,875 examples from K 88.6 82.2 85.2 76.2 62.3 68.5
+3,750 examples from K 88.3 82.6 85.3 76.8 63.6 69.6
+5,625 examples from K 88.3 82.4 85.2 77.7 63.8 70.0
+7,500 examples from K 88.9 82.9 85.8 78.2 64.9 70.9
306
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
versions that use automatically generated syntactic parses. The improvement on the
identification performance is almost exclusively due to recall. The precision numbers
are almost unaffected, except when the labeler is trained on WSJ PropBank data.
9. Conclusions
In this article, we have presented results from a state-of-the-art Semantic Role Labeling
system trained on PropBankWSJ data and then used to label test sets from both theWSJ
corpus and the Brown corpus. The system?s performance on the Brown test set exhibited
a large drop compared to the WSJ test set. An analysis of these results revealed that the
subtask of Identification, determining which constituents of a syntax tree are arguments
of a predicate, is responsible for only a relatively small part of the drop in performance.
The Classification task, assigning labels to constituents known to be arguments, is where
the major performance loss occurs.
Several possible factors were examined to determine their effect on this perfor-
mance difference:
 The syntactic parser was trained on WSJ. It was shown that errors in the
syntactic parse are not a large factor in the overall performance difference.
The syntactic parser does not show a large degradation in performance
when run on Brown. Even more telling, there is still a large drop in
performance when training and testing using Treebank parses.
When the system was trained and tested on Brown, the performance was still
significantly worse than training and testing on WSJ, even when the amount of training
data is controlled for. Training and testing on Brown showed performance intermediate
between training and testing on WSJ and training on WSJ and testing on Brown. This
leads to our final hypothesis.
 The Brown corpus is in some sense fundamentally more difficult for this
problem. The most obvious reason for this is that it represents a more
heterogeneous source than the WSJ. Among the likely manifestations of
this is that predicates tend to have a single dominating sense in WSJ and
are more polysemous in Brown. Data was presented using gold-standard
word sense information for the predicates for training and testing Brown.
Adding predicate sense information has a large effect for some predicates,
but over the whole Brown test set has only a small effect. Fewer predicates
and headwords could allow very specific modeling of high frequency
predicates, and predicate?headword relations do have a large effect on
overall performance.
The initial experiment is a case of training on homogeneous data and testing on
different data. The more homogeneous training data allows the system to rely heavily
on specific features and relations in the data. It is usually the case that training on a
more heterogeneous data set does not give quite as high performance on test data from
the same corpus as more homogeneous data, but the heterogeneous data ports better to
other corpora. This is seen when training on Brown compared to WSJ. The observation
that the Identification task ports well while the classification task does not is consistent
with this explanation. For the Identification task, structural features such as path and
307
Computational Linguistics Volume 34, Number 2
partial path tend to be the most salient while the Classification task relies more heavily
on lexical/semantic features such as specific predicate-head word combinations.
The question now is what to do about this. Two possibilities are:
 Less homogeneous corpora?Rather than using many examples drawn
from one source, fewer examples could be drawn from many sources. This
would reduce the likelihood of learning idiosyncratic senses and argument
structures for predicates.
 Less specific features?Features, and the values they take on, should be
designed to reduce the likelihood of learning idiosyncratic aspects of the
training domain. Examples of this might include the use of more general
named entity classes, and the use of abstractions over specific headwords
and predicates rather than the words themselves.
Both of these manipulations would, in all likelihood, reduce performance on both
the training data and on test sets of the same genre as the training data. But they
would be more likely to lead to better generalization across genres. Training on very
homogeneous training sets and testing on similar test sets gives amisleading impression
of the performance of a system.
Acknowledgments
We are extremely grateful to Martha Palmer
for providing us with the PropBanked
Brown corpus, and to David McClosky for
providing us with hypotheses on the Brown
test set as well as a cross-validated version
of the Brown training data for the various
models reported in his work reported at
HLT 2006.
This research was partially supported by
the ARDA AQUAINT program via contract
OCG4423B and by the NSF via grants
IS-9978025 and ITR/HCI 0086132. Computer
time was provided by NSF ARI Grant
CDA-9601817, NSF MRI Grant CNS-0420873,
NASA AIST grant NAG2-1646, DOE SciDAC
grant DE-FG02-04ER63870, NSF sponsorship
of the National Center for Atmospheric
Research, and a grant from the IBM Shared
University Research (SUR) program.
References
Bacchiani, Michiel, Michael Riley, Brian
Roark, and Richard Sproat. 2006. MAP
adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41?68.
Bikel, Daniel M., Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm
that learns what?s in a name.Machine
Learning, 34:211?231.
Burges, Christopher J. C. 1998. A tutorial
on support vector machines for pattern
recognition. Data Mining and Knowledge
Discovery, 2(2):121?167.
Carreras, Xavier and Llu??s Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of the Eighth Conference on Computational
Natural Language Learning (CoNLL),
pages 89?97, Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL),
pages 152?164, Ann Arbor, MI.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and maxent
discriminative reranking. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL),
Ann Arbor, MI.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Hacioglu, Kadri, Sameer Pradhan, Wayne
Ward, James Martin, and Daniel Jurafsky.
2004. Semantic role labeling by tagging
syntactic chunks. In Proceedings of the
Eighth Conference on Computational
Natural Language Learning (CoNLL),
Boston, MA.
Harabagiu, Sanda, Cosmin Adrian Bejan,
and Paul Morarescu. 2005. Shallow
semantics for relation extraction. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1061?1067, Edinburgh,
Scotland.
308
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Hofmann, Thomas and Jan Puzicha. 1998.
Statistical models for co-occurrence
data. Memo, Massachusetts Institute
of Technology Artificial Intelligence
Laboratory, Cambridge, MA.
Jiang, Zheng Ping, Jia Li, and Hwee Tou Ng.
2005. Semantic argument classification
exploiting argument interdependence. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1067?1072, Edinburgh,
Scotland.
Joachims, Thorsten. 1998. Text categorization
with support vector machines: Learning
with many relevant features. In Proceedings
of the European Conference on Machine
Learning (ECML), pages 137?142,
Chemnitz, Germany.
Koomen, Peter, Vasin Punyakanok, Dan
Roth, and Wen-tau Yih. 2005. Generalized
inference with multiple semantic role
labeling systems. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL), pages 181?184,
Ann Arbor, MI.
Kuc?era, Henry and W. Nelson Francis. 1967.
Computational Analysis of Present-day
American English. Brown University Press,
Providence, RI.
Kudo, Taku and Yuji Matsumoto. 2000.
Use of support vector learning for chunk
identification. In Proceedings of the Fourth
Conference on Computational Natural
Language Learning (CoNLL), pages 142?144,
Lisbon, Portugal.
Kudo, Taku and Yuji Matsumoto. 2001.
Chunking with support vector machines.
In Proceedings of the Second Meeting of the
North American Chapter of the Association
for Computational Linguistics (NAACL),
Pittsburgh, PA.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the Seventeenth International
Conference on Computational Linguistics
and Thirty Sixth Annual Meeting of the
Association of Computational Linguistics
(COLING/ACL), pages 768?774, Montreal,
Canada.
Lodhi, Huma, Craig Saunders, John
Shawe-Taylor, Nello Cristianini, and
Chris Watkins. 2002. Text classification
using string kernels. Journal of Machine
Learning Research, 2(Feb):419?444.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn treebank. Computational Linguistics,
19(2):313?330.
Ma`rquez, Llu??s, Mihai Surdeanu, Pere
Comas, and Jordi Turmo. 2005. A robust
combination strategy for semantic role
labeling. In Proceedings of the Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP),
pages 644?651, Vancouver, British
Columbia.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006a. Effective
self-training for parsing. In Proceedings
of the Human Language Technology
Conference/North American Chapter
of the Association of Computational
Linguistics (HLT/NAACL), pages 152?159,
New York, NY.
McClosky, David, Eugene Charniak, and
Mark Johnson. 2006b. Reranking and
self-training for parser adaptation. In
Proceedings of the Twenty First International
Conference on Computational Linguistics
and Forty Fourth Annual Meeting of the
Association for Computational Linguistics
(COLING/ACL), pages 337?344, Sydney,
Australia.
Moschitti, Alessandro. 2006. Syntactic
kernels for natural language learning:
The semantic role labeling case. In
Proceedings of the Human Language
Technology Conference/North American
Chapter of the Association of Computational
Linguistics (HLT/NAACL), pages 97?100,
New York, NY.
Musillo, Gabriele and Paola Merlo. 2006.
Accurate parsing of the proposition bank.
In Proceedings of the Human Language
Technology Conference/North American
Chapter of the Association of Computational
Linguistics (HLT/NAACL), pages 101?104,
New York, NY.
Narayanan, Srini and Sanda Harabagiu.
2004. Question answering based on
semantic structures. In Proceedings of the
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva, Switzerland.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Platt, John. 2000. Probabilities for support
vector machines. In A. Smola, P. Bartlett,
B. Scholkopf, and D. Schuurmans, editors,
Advances in Large Margin Classifiers. MIT
Press, Cambridge, MA, pages 61?74.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James Martin,
and Dan Jurafsky. 2005. Support vector
309
Computational Linguistics Volume 34, Number 2
learning for semantic argument classification.
Machine Learning Journal, 60(1):11?39.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using
support vector machines. In Proceedings
of the Human Language Technology Conference/
North American Chapter of the Association of
Computational Linguistics (HLT/NAACL),
pages 233?240, Boston, MA.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Dan Jurafsky.
2005. Semantic role labeling using different
syntactic views. In Proceedings of the
Forty-Third Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 581?588, Ann Arbor, MI.
Punyakanok, Vasin, Dan Roth, Wen tau Yih,
and Dav Zimak. 2005. Learning and
inference over constrained output. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1117?1123, Edinburgh,
Scotland.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth. 2003.
Using predicate-argument structures for
information extraction. In Proceedings of the
Forty-First Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 8?15, Sapporo, Japan.
Toutanova, Kristina, Aria Haghighi, and
Christopher Manning. 2005. Joint learning
improves semantic role labeling. In
Proceedings of the Forty-Third Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 589?596,
Ann Arbor, MI.
Vapnik, Vladimir. 1998. Statistical Learning
Theory. Wiley, New York.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 88?94,
Barcelona, Spain.
Yi, Szu-ting and Martha Palmer. 2005. The
integration of syntactic parsing and
semantic role labeling. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL), pages 237?240,
Ann Arbor, MI.
310
Proceedings of NAACL HLT 2007, pages 556?563,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Towards Robust Semantic Role Labeling
Sameer Pradhan
BBN Technologies
Cambridge, MA 02138
pradhan@bbn.com
Wayne Ward, James H. Martin
University of Colorado
Boulder, CO 80303
{whw,martin}@colorado.edu
Abstract
Most research on semantic role labeling
(SRL) has been focused on training and
evaluating on the same corpus in order
to develop the technology. This strategy,
while appropriate for initiating research,
can lead to over-training to the particular
corpus. The work presented in this pa-
per focuses on analyzing the robustness
of an SRL system when trained on one
genre of data and used to label a different
genre. Our state-of-the-art semantic role
labeling system, while performing well on
WSJ test data, shows significant perfor-
mance degradation when applied to data
from the Brown corpus. We present a se-
ries of experiments designed to investigate
the source of this lack of portability. These
experiments are based on comparisons of
performance using PropBanked WSJ data
and PropBanked Brown corpus data. Our
results indicate that while syntactic parses
and argument identification port relatively
well to a new genre, argument classifica-
tion does not. Our analysis of the reasons
for this is presented and generally point
to the nature of the more lexical/semantic
features dominating the classification task
and general structural features dominating
the argument identification task.
1 Introduction
Automatic, accurate and wide-coverage techniques
that can annotate naturally occurring text with se-
mantic argument structure play a key role in NLP
applications such as Information Extraction (Sur-
deanu et al, 2003; Harabagiu et al, 2005), Question
Answering (Narayanan and Harabagiu, 2004) and
Machine Translation (Boas, 2002; Chen and Fung,
2004). Semantic Role Labeling (SRL) is the pro-
cess of producing such a markup. When presented
with a sentence, a parser should, for each predicate
in the sentence, identify and label the predicate?s se-
mantic arguments. In recent work, a number of re-
searchers have cast this problem as a tagging prob-
lem and have applied various supervised machine
learning techniques to it. On the Wall Street Jour-
nal (WSJ) data, using correct syntactic parses, it is
possible to achieve accuracies rivaling human inter-
annotator agreement. However, the performance gap
widens when information derived from automatic
syntactic parses is used.
So far, most of the work on SRL systems has been
focused on improving the labeling performance on a
test set belonging to the same genre of text as the
training set. Both the Treebank on which the syntac-
tic parser is trained and the PropBank on which the
SRL systems are trained represent articles from the
year 1989 of the WSJ. While all these systems per-
form quite well on the WSJ test data, they show sig-
nificant performance degradation (approximately 10
point drop in F-score) when applied to label test data
that is different than the genre that WSJ represents
(Pradhan et al, 2004; Carreras and Ma`rquez, 2005).
556
Surprisingly, it does not matter much whether the
data is from another newswire, or a completely dif-
ferent type of text ? as in the Brown corpus. These
results indicate that the systems are being over-fit to
the specific genre of text. Many performance im-
provements on the WSJ PropBank corpus may re-
flect tuning to the corpus. For the technology to
be widely accepted and useful, it must be robust
to change in genre of the data. Until recently, data
tagged with similar semantic argument structure was
not available for multiple genres of text. Recently,
Palmer et al, (2005), have PropBanked a significant
portion of the Treebanked Brown corpus which en-
ables us to perform experiments to analyze the rea-
sons behind the performance degradation, and sug-
gest potential solutions.
2 Semantic Annotation and Corpora
In the PropBank1 corpus (Palmer et al, 2005), pred-
icate argument relations are marked for the verbs
in the text. PropBank was constructed by assign-
ing semantic arguments to constituents of the hand-
corrected Treebank parses. The arguments of a verb
are labeled ARG0 to ARG5, where ARG0 is the
PROTO-AGENT (usually the subject of a transitive
verb) ARG1 is the PROTO-PATIENT (usually its di-
rect object), etc. In addition to these CORE ARGU-
MENTS, 16 additional ADJUNCTIVE ARGUMENTS,
referred to as ARGMs are also marked.
More recently the PropBanking effort has been
extended to encompass multiple corpora. In this
study we use PropBanked versions of the Wall Street
Journal (WSJ) part of the Penn Treebank (Marcus et
al., 1994) and part of the Brown portion of the Penn
Treebank.
The WSJ PropBank data comprise 24 sections
of the WSJ, each section representing about 100
documents. PropBank release 1.0 contains about
114,000 predicates instantiating about 250,000 argu-
ments and covering about 3,200 verb lemmas. Sec-
tion 23, which is a standard test set and a test set
in some of our experiments, comprises 5,400 predi-
cates instantiating about 12,000 arguments.
The Brown corpus is a Standard Corpus of Ameri-
can English that consists of about one million words
of English text printed in the calendar year 1961
1http://www.cis.upenn.edu/?ace/
(Kuc?era and Francis, 1967). The corpus contains
about 500 samples of 2000+ words each. The idea
behind creating this corpus was to create a hetero-
geneous sample of English text so that it would be
useful for comparative language studies.
The Release 3 of the Penn Treebank contains the
hand parsed syntactic trees of a subset of the Brown
Corpus ? sections F, G, K, L, M, N, P and R. Palmer
et al, (2005) have recently PropBanked a signifi-
cant portion of this Treebanked Brown corpus. In
all, about 17,500 predicates are tagged with their se-
mantic arguments. For these experiments we used a
limited release of PropBank dated September 2005.
A small portion of the predicates ? about 8,000 have
also been tagged with frame sense information.
3 SRL System Description
We formulate the labeling task as a classification
problem as initiated by Gildea and Jurafsky (2002)
and use Support Vector Machine (SVM) classi-
fiers (2005). We use TinySVM2 along with Yam-
Cha3 (Kudo and Matsumoto, 2000) (Kudo and Mat-
sumoto, 2001) as the SVM training and classifica-
tion software. The system uses a polynomial kernel
with degree 2; the cost per unit violation of the mar-
gin, C=1; and, tolerance of the termination criterion,
e=0.001. More details of this system can be found
in Pradhan et al, (2005). The performance of this
system on section 23 of the WSJ when trained on
sections 02-21 is shown in Table 1
ALL ARGs Task P R F A
(%) (%) (%)
TREEBANK Id. 97.5 96.1 96.8
Class. - - - 93.0
Id. + Class. 91.8 90.5 91.2
AUTOMATIC Id. 86.9 84.2 85.5
Class. - - - 92.0
Id. + Class. 82.1 77.9 79.9
Table 1: Performance of the SRL system on WSJ
The performance of the SRL system is reported
on three different tasks, all of which are with respect
to a particular predicate: i) argument identification
(ID), is the task of identifying the set of words (here,
parse constituents) that represent a semantic role; ii)
argument classification (Class.), is the task of clas-
sifying parse constituents known to represent some
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
557
semantic role into one of the many semantic role
types; and iii) argument identification and classifi-
cation (ID + Class.), which involves both the iden-
tification of the parse constituents that represent se-
mantic roles of the predicate and their classification
into the respective semantic roles. As usual, argu-
ment classification is measured as percent accuracy
(A), whereas ID and ID + Class. are measured in
terms of precision (P), recall (R) and F-score (F)
? the harmonic mean of P and R. The first three
rows of Table 1 report performance for the system
that uses hand-corrected Treebank parses, and the
next three report performance for the SRL system
that uses automatically generated ? Charniak parser
? parses, both during training and testing.
4 Robustness Experiments
This section describes experiments that we per-
formed using the PropBanked Brown corpus in an
attempt to analyze the factors affecting the portabil-
ity of SRL systems.
4.1 How does the SRL system trained on WSJ
perform on Brown?
In order to test the robustness of the SRL system,
we used a system trained on the PropBanked WSJ
corpus to label data from the Brown corpus. We use
the entire PropBanked Brown corpus (about 17,500
predicates) as a test set for this experiment and use
the SRL system trained on WSJ sections 02-21 to
tag its arguments.
Table 2 shows the performance for training and
testing on WSJ, and for training on WSJ and testing
on Brown. There is a significant reduction in per-
formance when the system trained on WSJ is used
to label data from the Brown corpus. The degrada-
tion in the Identification task is small compared to
that of the combined Identification and Classifica-
tion task. A number of factors could be responsible
for the loss of performance. It is possible that the
SRL models are tuned to the particular vocabulary
and sense structure associated with the training data.
Also, since the syntactic parser that is used for gen-
erating the syntax parse trees (Charniak) is heavily
lexicalized and is trained on WSJ, it could have de-
creased accuracy on the Brown data resulting in re-
duced accuracy for Semantic Role Labeling. Since
the SRL algorithm walks the syntax tree classifying
each node, if no constituent node is present that cor-
responds to the correct argument, the system cannot
produce a correct labeling for the argument.
Train Test Id. Id. + Class
F F
WSJ WSJ 85.5 79.9
WSJ Brown 82.4 65.1
Table 2: Performance of the SRL system on Brown.
In order to check the extent to which constituent
nodes representing semantic arguments were deleted
from the syntax tree due to parser error, we gener-
ated the performance numbers which are shown in
Table 3. These numbers are for top one parse for the
Charniak parser, and represent not all parser errors,
but deletion of argument bearing constituent nodes.
Total Misses %
PropBank 12000 800 6.7
Brown 45880 3692 8.1
Table 3: Constituent deletions in WSJ and Brown.
The parser misses 6.7% of the argument-bearing
nodes in the PropBank test set and about 8.1% in
the Brown corpus. This indicates that the errors in
syntactic parsing account for a fairly small amount
of the argument deletions and probably do not con-
tributing significantly to the increased SRL error
rate. Obviously, just the presence of a argument-
bearing constituent does not necessarily guarantee
the correctness of the structural connections be-
tween itself and the predicate.
4.2 Identification vs Classification Performance
Different features tend to dominate in the identifi-
cation task vs the classification task. For example,
the path feature (representing the path in the syntax
tree from the argument to the predicate) is the sin-
gle most salient feature for the ID task and is not
very important in the classification task. In the next
experiment we look at cross genre performance of
the ID and Classification tasks. We used gold stan-
dard syntactic trees from the Treebank so there are
no errors in generating the syntactic structure. In
addition to training on the WSJ and testing on WSJ
and Brown, we trained the SRL system on a Brown
training set and tested it on a test set alo from the
Brown corpus. In generating the Brown training and
558
SRL SRL Task P R F A
Train Test (%) (%) (%)
WSJ WSJ Id. 97.5 96.1 96.8
(104k) (5k) Class. 93.0
Id. + Class. 91.8 90.5 91.2
WSJ WSJ Id. 96.3 94.4 95.3
(14k) (5k) Class. 86.1
Id. + Class. 84.4 79.8 82.0
BROWN BROWN Id. 95.7 94.9 95.2
(14k) (1.6k) Class. 80.1
Id. + Class. 79.9 77.0 78.4
WSJ BROWN Id. 94.2 91.4 92.7
(14k) (1.6k) Class. 72.0
Id. + Class. 71.8 65.8 68.6
Table 4: Performance of the SRL system using correct Treebank parses.
test sets, we used stratified sampling, which is often
used by the syntactic parsing community (Gildea,
2001). The test set was generated by selecting ev-
ery 10th sentence in the Brown Corpus. We also
held out the development set used by Bacchiani et
al., (2006) to tune system parameters in the future.
This procedure resulted in a training set of approxi-
mately 14,000 predicates and a test set of about 1600
predicates. We did not perform any parameter tun-
ing for any of the following experiments, and used
the parameter settings from the best performing ver-
sion of the SRL system as reported in Table1. We
compare the performance on this test set with that
obtained when the SRL system is trained using WSJ
sections 02-21 and use section 23 for testing. For
a more balanced comparison, we retrained the SRL
system on the same amount of data as used for train-
ing on Brown, and tested it on section 23. As usual,
trace information, and function tag information from
the Treebank is stripped out.
Table 4 shows the results. There is a fairly small
difference in argument Identification performance
when the SRL system is trained on 14,000 predi-
cates vs 104,000 predicates from the WSJ (F-score
95.3 vs 96.8). However, there is a considerable drop
in Classification accuracy (86.1% vs 93.0%). When
the SRL system is trained and tested on Brown data,
the argument Identification performance is not sig-
nificantly different than that for the system trained
and tested on WSJ data (F-score 95.2 vs 95.3). The
drop in argument Classification accuracy is much
more severe (86.1% vs 80.1%).
This same trend between ID and Classification is
even more pronounced when training on WSJ and
testing on Brown. For a system trained on WSJ,
there is a fairly small drop in performance of the
ID task when tested on Brown vs tested on WSJ (F-
score 92.7 vs 95.3). However, in this same condi-
tion, the Classification task has a very large drop in
performance (72.0% vs 86.1%).
So argument ID is not very sensitive to amount
of training data in a corpus, or to the genre of the
corpus, and ports well from WSJ to Brown. This ex-
periment supports the belief that there is no signifi-
cant drop in the task of identifying the right syntactic
constituents that are arguments ? and this is intuitive
since previous experiments have shown that the task
of argument identification is more dependent on the
structural features ? one such feature being the path
in the syntax tree.
Argument Classification seems to be the problem.
It requires more training data within the WSJ corpus,
does not perform as well when trained and tested on
Brown as it does for WSJ and does not port well
from WSJ to Brown. This suggests that the features
it uses are being over-fit to the training data and are
more idiosyncratic to a given dataset. In particular,
the predicate whose arguments are being identified,
and the head word of the syntactic constituent being
classified are both important features in the task of
argument classification.
As a generalization, the features used by the Iden-
tification task reflect structure and port well. The
features used by the Classification task reflect spe-
cific lexical usage and semantics, and tend to require
more training data and are more subject to over-
fitting. Even when training and testing on Brown,
Classification accuracy is considerably worse than
559
training and testing on WSJ (with comparable train-
ing set size). It is probably the case that the predi-
cates and head words in a homogeneous corpus such
as the WSJ are used more consistently, and tend to
have single dominant word senses. The Brown cor-
pus probably has much more variety in its lexical
usage and word senses.
4.3 How sensitive is semantic argument
prediction to the syntactic correctness
across genre?
This experiment examines the same cross-genre ef-
fects as the last experiment, but uses automatically
generated syntactic parses rather than gold standard
ones.
For this experiment, we used the same amount of
training data from WSJ as available in the Brown
training set ? that is about 14,000 predicates. The
examples from WSJ were selected randomly. The
Brown test set is the same as used in the previous
experiment, and the WSJ test set is the entire section
23.
Recently there have been some improvements to
the Charniak parser, use n-best re-ranking as re-
ported in (Charniak and Johnson, 2005) and self-
training and re-ranking using data from the North
American News corpus (NANC) and adapts much
better to the Brown corpus (McClosky et al, 2006a;
McClosky et al, 2006b). The performance of these
parsers as reported in the respective literature are
shown in Table 6 shows the performance (as re-
ported in the literature) of the Charniak parser: when
trained and tested on WSJ, when trained on WSJ and
tested on Brown, When trained and tested on Brown,
and when trained on WSJ and adapted with NANC.
Train Test F
WSJ WSJ 91.0
WSJ Brown 85.2
Brown Brown 88.4
WSJ+NANC Brown 87.9
Table 6: Charniak parser performance.
We describe the results of Semantic Role Label-
ing under the following five conditions:
1. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked WSJ sentences. The syntactic
parser ? Charniak parser ? is itself trained on
the WSJ training sections of the Treebank. This
is used for Semantic Role Labeling of section-
23 of WSJ.
2. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked WSJ sentences. The syntac-
tic parser ? Charniak parser ? is itself trained
on the WSJ training sections of the Treebank.
This is used to classify the Brown test set.
3. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is trained using the WSJ por-
tion of the Treebank. This is used to classify
the Brown test set.
4. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is trained using the Brown
training portion of the Treebank. This is used
to classify the Brown test set.
5. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is the version that is self-
trained using 2,500,000 sentences from NANC,
and where the starting version is trained only
on WSJ data (McClosky et al, 2006b). This is
used to classify the Brown test set.
Table 5 shows the results. For simplicity of dis-
cussion we have tagged the five conditions as 1.,
2., 3., 4., and 5. Comparing conditions 2. and 3.
shows that when the features used to train the SRL
system are extracted using a syntactic parser that is
trained on WSJ it performs at almost the same level
on the task of Identification, regardless of whether
it is trained on the PropBanked Brown corpus or
the PropBanked WSJ corpus. This, however, is sig-
nificantly lower than when all the three ? the syn-
tactic parser training set, the SRL system training
set, and the SRL system test set, are from the same
genre (6 F-score points lower than condition 1, and
5 points lower than conditions 4 and 5). In case of
the combined task, the gap between the performance
for conditions 2 and 3 is about 10 points in F-score
(59.1 vs 69.8). Looking at the argument classifica-
tion accuracies, we see that using the SRL system
560
Setup Parser SRL SRL Task P R F A
Train Train Test (%) (%) (%)
1. WSJ WSJ WSJ Id. 87.3 84.8 86.0
(40k ? sec:00-21) (14k) (5k) Class. 84.1
Id. + Class. 77.5 69.7 73.4
2. WSJ WSJ Brown Id. 81.7 78.3 79.9
(40k ? sec:00-21) (14k) (1.6k) Class. 72.1
Id. + Class. 63.7 55.1 59.1
3. WSJ Brown Brown Id. 81.7 78.3 80.0
(40k ? sec:00-21) (14k) (1.6k) Class. 79.2
Id. + Class. 78.2 63.2 69.8
4. Brown Brown Brown Id. 87.6 82.3 84.8
(20k) (14k) (1.6k) Class. 78.9
Id. + Class. 77.4 62.1 68.9
5. WSJ+NANC Brown Brown Id. 87.7 82.5 85.0
(2,500k) (14k) (1.6k) Class. 79.9
Id. + Class. 77.2 64.4 70.0
Table 5: Performance on WSJ and Brown using automatic syntactic parses
trained on WSJ to test Brown sentences give a 12
point drop in F-score (84.1 vs 72.1). Using the SRL
system trained on Brown using WSJ trained syntac-
tic parser shows a drop in accuracy by about 5 F-
score points (84.1 to 79.2). When the SRL system is
trained on Brown using syntactic parser also trained
on Brown, we get a quite similar classification per-
formance, which is again about 5 points lower than
what we get using all WSJ data. This shows lexical
semantic features might be very important to get a
better argument classification on Brown corpus.
4.4 How much data is required to adapt to a
new genre?
We would like to know how much data from a new
genre we need to annotate and add to the training
data of an existing corpus to adapt the system such
that it gives the same level of performance as when
it is trained on the new genre.
One section of the Brown corpus ? section CK
has about 8,200 predicates annotated. We use six
different conditions ? two in which we use correct
Treebank parses, and the four others in which we
use automatically generated parses using the varia-
tions described before. All training sets start with
the same number of examples as in the Brown train-
ing set. The part of this section used as a test set for
the CoNLL 2005 shared task is used as the test set
here. It contains a total of about 800 predicates.
Table 7 shows a comparison of these conditions.
In all the six conditions, the performance on the task
of Identification and Classification improves gradu-
ally until about 5625 examples of section CK which
is about 75% of the total added, above which they
improve very little. In fact, even 50% of the new
data accounts for 90% of the performance differ-
ence. Even when the syntactic parser is trained on
WSJ and the SRL is trained on WSJ, adding 7,500
instances of the new genres allows it to achieve al-
most the same performance as when all three are
from the same genre (67.2 vs 69.9). Numbers for ar-
gument identification aren?t shown because adding
more data does not have any statistically signifi-
cant impact on its performance. The system that
uses self-trained syntactic parser seems to perform
slightly better than the rest of the versions that use
automatically generated syntactic parses. The preci-
sion numbers are almost unaffected ? except when
the labeler is trained on WSJ PropBank data.
4.5 How much does verb sense information
contribute?
In order to find out how important the verb sense
information is in the process of genre transfer, we
used the subset of PropBanked Brown corpus that
was tagged with verb sense information, ran an ex-
periment similar to that of Experiment 1. We used
the oracle sense information and correct syntactic in-
formation for this experiment.
Table 8 shows the results of this experiment.
There is about 1 point F-score increase on using
oracle sense information on the overall data. We
looked at predicates that had high perplexity in both
the training and test sets, and whose sense distribu-
561
Parser SRL Id. + Class Parser SRL Id. + Class
P R F P R F
Train Train (%) (%) (%) (%)
WSJ WSJ (14k) WSJ Brown (14k)
(Treebank parses) (Treebank parses)
+0 ex. from CK 74.1 66.5 70.1 (40k) +0 ex. from CK 74.4 57.0 64.5
+1875 ex. from CK 77.6 71.3 74.3 +1875 ex. from CK 75.1 58.7 65.9
+3750 ex. from CK 79.1 74.1 76.5 +3750 ex. from CK 76.1 59.6 66.9
+5625 ex. from CK 80.4 76.1 78.1 +5625 ex. from CK 76.9 60.5 67.7
+7500 ex. from CK 80.2 76.1 78.1 +7500 ex. from CK 76.8 59.8 67.2
Brown Brown (14k) Brown Brown (14k)
(Treebank parses) (Treebank parses)
+0 ex. from CK 77.1 73.0 75.0 (20k) +0 ex. from CK 76.0 59.2 66.5
+1875 ex. from CK 78.8 75.1 76.9 +1875 ex. from CK 76.1 60.0 67.1
+3750 ex. from CK 80.4 76.9 78.6 +3750 ex. from CK 77.7 62.4 69.2
+5625 ex. from CK 80.4 77.2 78.7 +5625 ex. from CK 78.2 63.5 70.1
+7500 ex. from CK 81.2 78.1 79.6 +7500 ex. from CK 78.2 63.2 69.9
WSJ WSJ (14k) WSJ+NANC Brown (14k)
(40k) +0 ex. from CK 65.2 55.7 60.1 (2,500k) +0 ex. from CK 74.4 60.1 66.5
+1875 ex. from CK 68.9 57.5 62.7 +1875 ex. from CK 76.2 62.3 68.5
+3750 ex. from CK 71.8 59.3 64.9 +3750 ex. from CK 76.8 63.6 69.6
+5625 ex. from CK 74.3 61.3 67.2 +5625 ex. from CK 77.7 63.8 70.0
+7500 ex. from CK 74.8 61.0 67.2 +7500 ex. from CK 78.2 64.9 70.9
Table 7: Effect of incrementally adding data from a new genre
Train Test Without Sense With Sense
Id. Id.
F F
WSJ Brown (All) 69.1 69.9
WSJ Brown (predicate: go) 46.9 48.9
Table 8: Influence of verb sense feature.
tion was different. One such predicate is ?go?. The
improvement on classifying the arguments of this
predicate was about 2 points (46.9 to 48.9), which
suggests that verb sense is more important when the
sense structure of the test corpus is more ambiguous
and is different from the training. Here we used ora-
cle verb sense information, but one can train a clas-
sifier as done by Girju et al, (2005) which achieves
a disambiguation accuracy in the 80s for within the
WSJ corpus.
5 Conclusions
Our experimental results on robustness to change in
genre can be summarized as follows:
? There is a significant drop in performance when
training and testing on different corpora ? for
both Treebank and Charniak parses
? In this process the classification task is more
disrupted than the identification task.
? There is a performance drop in classification
even when training and testing on Brown (com-
pared to training and testing on WSJ)
? The syntactic parser error is not a large part of
the degradation for the case of automatically
generated parses.
An error analysis leads us to believe that some
reasons for this behavior could be: i) lexical us-
ages that are specific to WSJ, ii) variation in sub-
categorization across corpora, iii) variation in word
sense distribution and iv) changes in topics and enti-
ties. Training and testing on the same corpora tends
to give a high weight to very specific semantic fea-
tures. Two possibilities remedies could be: i) using
less homogeneous corpora and ii) less specific fea-
tures, for eg., proper names are replaced with the
name entities that they represent. This way the sys-
tem could be forced to use the more general features.
Both of these manipulations would most likely re-
duce performance on the training set, and on test
sets of the same genre as the training data. But they
would be likely to generalize better.
6 Acknowledgments
We are extremely grateful to Martha Palmer for pro-
viding us with the PropBanked Brown corpus, and
to David McClosky for providing us with hypothe-
ses on the Brown test set as well as a cross-validated
562
version of the Brown training data for the various
models reported in his work reported at HLT 2006.
This research was partially supported by
the ARDA AQUAINT program via contract
OCG4423B and by the NSF via grants IS-9978025
and ITR/HCI 0086132.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Hans Boas. 2002. Bilingual framenet dictionaries for
machine translation. In Proceedings of LREC-2002.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152?164,
Ann Arbor, MI.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-2005, pages 173?180,
Ann Arbor, MI.
Benfeng Chen and Pascale Fung. 2004. Automatic con-
struction of an english-chinese bilingual framenet. In
Proceedings of the HLT/NAACL-2004, Boston, MA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In In Proceedings of EMNLP-2001.
R. Girju, D. Roth, and M. Sammons. 2005. Token-
level disambiguation of verbnet classes. In Proceed-
ings of the Interdisciplinary Workshop on the Identifi-
cation and Representation of Verb Features and Verb
Classes, K. Erk, A. Melinger, and S. Schulte im Walde
(eds.).
Sanda Harabagiu, Cosmin Adrian Bejan, and
Paul Morarescu. 2005. Shallow semantics for relation
extraction. In IJCAI-2005, pages 1061?1067, Edin-
burgh, Scotland.
Henry Kuc?era and W. Nelson Francis. 1967. Com-
putational analysis of present-day American English.
Brown University Press, Providence, RI.
Taku Kudo and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of CoNLL-2000 and LLL-2000, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of the
NAACL-2001.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure.
David McClosky, Eugene Charniak, and Mark Johnson.
2006a. Effective self-training for parsing. In Proceed-
ings of HLT/NAACL-2006, pages 152?159, New York
City, USA. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2006b. Rerankinng and self-training for parser adapta-
tion. In Proceedings of COLING/ACL-2006, Sydney,
Australia.
Srini Narayanan and Sanda Harabagiu. 2004. Question
answering based on semantic structures. In Proceed-
ings of COLING-2004), Geneva, Switzerland.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of HLT/NAACL-2004, Boston, MA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2005. Semantic role label-
ing using different syntactic views. In Proceedings of
ACL-2005, Ann Arbor, MI.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL-2003, Sapporo, Japan.
563
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 87?92,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 17: English Lexical Sample, SRL and All Words
Sameer S. Pradhan
BBN Technologies,
Cambridge, MA 02138
Edward Loper
University of Pennsylvania,
Philadelphia, PA 19104
Dmitriy Dligach and Martha Palmer
University of Colorado,
Boulder, CO 80303
Abstract
This paper describes our experience in
preparing the data and evaluating the results
for three subtasks of SemEval-2007 Task-17
? Lexical Sample, Semantic Role Labeling
(SRL) and All-Words respectively. We tab-
ulate and analyze the results of participating
systems.
1 Introduction
Correctly disambiguating words (WSD), and cor-
rectly identifying the semantic relationships be-
tween those words (SRL), is an important step for
building successful natural language processing ap-
plications, such as text summarization, question an-
swering, and machine translation. SemEval-2007
Task-17 (English Lexical Sample, SRL and All-
Words) focuses on both of these challenges, WSD
and SRL, using annotated English text taken from
the Wall Street Journal and the Brown Corpus.
It includes three subtasks: i) the traditional All-
Words task comprising fine-grained word sense dis-
ambiguation using a 3,500 word section of the Wall
Street Journal, annotated with WordNet 2.1 sense
tags, ii) a Lexical Sample task for coarse-grained
word sense disambiguation on a selected set of lex-
emes, and iii) Semantic Role Labeling, using two
different types of arguments, on the same subset of
lexemes.
2 Word Sense Disambiguation
2.1 English fine-grained All-Words
In this task we measure the ability of systems to
identify the correct fine-grained WordNet 2.1 word
sense for all the verbs and head words of their argu-
ments.
2.1.1 Data Preparation
We began by selecting three articles
wsj 0105.mrg (on homelessness), wsj 0186.mrg
(about a book on corruption), and wsj 0239.mrg
(about hot-air ballooning) from a section of the WSJ
corpus that has been Treebanked and PropBanked.
All instances of verbs were identified using the
Treebank part-of-speech tags, and also the head-
words of their noun arguments (using the PropBank
and standard headword rules). The locations of the
sentences containing them as well as the locations
of the verbs and the nouns within these sentences
were recorded for subsequent sense-annotation. A
total of 465 lemmas were selected from about 3500
words of text.
We use a tool called STAMP written by Ben-
jamin Snyder for sense-annotation of these in-
stances. STAMP accepts a list of pointers to the in-
stances that need to be annotated. These pointers
consist of the name of the file where the instance
is located, the sentence number of the instance, and
finally, the word number of the ambiguous word
within that sentence. These pointers were obtained
as described in the previous paragraph. STAMP also
requires a sense inventory, which must be stored in
XML format. This sense inventory was obtained by
querying WordNet 2.1 and storing the output as a
87
set of XML files (one for each word to be anno-
tated) prior to tagging. STAMP works by displaying
to the user the sentence to be annotated with the tar-
get word highlighted along with the previous and the
following sentences and the senses from the sense
inventory. The user can select one of the senses and
move on to the next instance.
Two linguistics students annotated the words with
WordNet 2.1 senses. Our annotators examined each
instance upon which they disagreed and resolved
their disagreements. Finally, we converted the re-
sulting data to the Senseval format. For this dataset,
we got an inter-annotator agreement (ITA) of 72%
on verbs and 86% for nouns.
2.1.2 Results
A total of 14 systems were evaluated on the All
Words task. These results are shown in Table 1.
We used the standard Senseval scorer ? scorer21
to score the systems. All the F-scores2 in this table
as well as other tables in this paper are accompanied
by a 95% confidence interval calculated using the
bootstrap resampling procedure.
2.2 OntoNotes English Lexical Sample WSD
It is quite well accepted at this point that it is dif-
ficult to achieve high inter-annotator agreement on
the fine-grained WordNet style senses, and with-
out a corpus with high annotator agreement, auto-
matic learning methods cannot perform at a level
that would be acceptable for a downstream applica-
tion. OntoNotes (Hovy et al, 2006) is a project that
has annotated several layers of semantic information
? including word senses, at a high inter-annotator
agreement of over 90%. Therefore we decided to
use this data for the lexical sample task.
2.2.1 Data
All the data for this task comes from the 1M word
WSJ Treebank. For the convenience of the partici-
pants who wanted to use syntactic parse information
as features using an off-the-shelf syntactic parser,
we decided to compose the training data of Sections
02-21. For the test sets, we use data from Sections
1http://www.cse.unt.edu/?rada/senseval/senseval3/scoring/
2
scorer2 reports Precision and Recall scores for each system. For a sys-
tem that attempts all the words, both Precision and Recall are the same. Since a
few systems had missing answers, they got different Precision and Recall scores.
Therefore, for ranking purposes, we consolidated them into an F-score.
Train Test Total
Verb 8988 2292 11280
Noun 13293 2559 15852
Total 22281 4851
Table 2: The number of instances for Verbs and
Nouns in the Train and Test sets for the Lexical Sam-
ple WSD task.
01, 22, 23 and 24. Fortunately, the distribution of
words was amenable to an acceptable number of in-
stances for each lemma in the test set. We selected
a total of 100 lemmas (65 verbs and 35 nouns) con-
sidering the degree of polysemy and total instances
that were annotated. The average ITA for these is
over 90%.
The training and test set composition is described
in Table 2. The distribution across all the verbs and
nouns is displayed in Table 4
2.2.2 Results
A total of 13 systems were evaluated on the Lexi-
cal Sample task. Table 3 shows the Precision/Recall
for all these systems. The same scoring software was
used to score this task as well.
2.2.3 Discussion
For the all words task, the baseline performance
using the most frequent WordNet sense for the lem-
mas is 51.4. The top-performing system was a su-
pervised system that used a Maximum Entropy clas-
sifier, and got a Precision/Recall of 59.1% ? about 8
points higher than the baseline. Since the coarse and
fine-grained disambiguation tasks have been part of
the two previous Senseval competitions, and we hap-
pen to have access to that data, we can take this op-
portunity to look at the disambiguation performance
trend. Although different test sets were used for ev-
ery evaluation, we can get a rough indication of the
trend. For the fine-grained All Words sense tagging
task, which has always used WordNet, the system
performance has ranged from our 59% to 65.2 (Sen-
seval3, (Decadt et al, 2004)) to 69% (Seneval2,
(Chklovski and Mihalcea, 2002)). Because of time
constraints on the data preparation, this year?s task
has proportionally more verbs and fewer nouns than
previous All-Words English tasks, which may ac-
count for the lower scores.
As expected, the Lexical Sample task using coarse
88
Rank Participant System ID Classifier F
1 Stephen Tratz <stephen.tratz@pnl.gov> PNNL MaxEnt 59.1?4.5
2 Hwee Tou Ng <nght@comp.nus.edu.sg> NUS-PT SVM 58.7?4.5
3 Rada Mihalcea <rada@cs.unt.edu> UNT-Yahoo Memory-based 58.3?4.5
4 Cai Junfu <caijunfu@gmail.com> NUS-ML naive Bayes 57.6?4.5
5 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM kNN 54.4?4.5
6 David Martinez <davidm@csse.unimelb.edu.au> UBC-UMB-2 kNN 54.0?4.5
7 Jonathan Chang <jcone@princeton.edu> PU-BCD Exponential Model 53.9?4.5
8 Radu ION <radu@racai.ro> RACAI Unsupervised 52.7?4.5
9 Most Frequent WordNet Sense Baseline N/A 51.4?4.5
10 Davide Buscaldi <dbuscaldi@dsic.upv.es> UPV-WSD Unsupervised 46.9?4.5
11 Sudip Kumar Naskar <sudip.naskar@gmail.com> JU-SKNSB Unsupervised 40.2?4.5
12 David Martinez <davidm@csse.unimelb.edu.au> UBC-UMB-1 Unsupervised 39.9?4.5
14 Rafael Berlanga <berlanga@uji.es> tkb-uo Unsupervised 32.5?4.5
15 Jordan Boyd-Graber <jbg@princeton.edu> PUTOP Unsupervised 13.2?4.5
Table 1: System Performance for the All-Words task.
Rank Participant System Classifier F
1 Cai Junfu <caijunfu@gmail.com> NUS-ML SVM 88.7?1.2
2 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM SVD+kNN 86.9?1.2
3 Zheng-Yu Niu <niu zy@hotmail.com> I2R Supervised 86.4?1.2
4 Lucia Specia <lspecia@gmail.com> USP-IBM-2 SVM 85.7?1.2
5 Lucia Specia <lspecia@gmail.com> USP-IBM-1 ILP 85.1?1.2
5 Deniz Yuret <dyuret@ku.edu.tr> KU Semi-supervised 85.1?1.2
6 Saarikoski <harri.saarikoski@helsinki.fi> OE naive Bayes, SVM 83.8?1.2
7 University of Technology Brno VUTBR naive Bayes 80.3?1.2
8 Ana Zelaia <ana.zelaia@ehu.es> UBC-ZAS SVD+kNN 79.9?1.2
9 Carlo Strapparava <strappa@itc.it> ITC-irst SVM 79.6?1.2
10 Most frequent sense in training Baseline N/A 78.0?1.2
11 Toby Hawker <toby@it.usyd.edu.au> USYD SVM 74.3?1.2
12 Siddharth Patwardhan <sidd@cs.utah.edu> UMND1 Unsupervised 53.8?1.2
13 Saif Mohammad <smm@cs.toronto.edu> Tor Unsupervised 52.1?1.2
- Toby Hawker <toby@it.usyd.edu.au> USYD? SVM 89.1?1.2
- Carlo Strapparava <strappa@itc.it> ITC? SVM 89.1?1.2
Table 3: System Performance for the OntoNotes Lexical Sample task. Systems marked with an * were
post-competition bug-fix submissions.
grained senses provides consistently higher per-
formance than previous more fine-grained Lexical
Sample Tasks. The high scores here were foreshad-
owed in an evaluation involving a subset of the data
last summer (Chen et al, 2006). Note that the best
system performance is now closely approaching the
ITA for this data of over 90%. Table 4 shows the
performance of the top 8 systems on all the indi-
vidual verbs and nouns in the test set. Owing to
space constraints we have removed some lemmas
that have perfect or almost perfect accuracies. At the
right are mentioned the average, minimum and max-
imum performances of the teams per lemma, and at
the bottom are the average scores per lemma (with-
out considering the lemma frequencies) and broken
down by verbs and nouns. A gap of about 10 points
between the verb and noun performance seems to
indicate that in general the verbs were more difficult
than the nouns. However, this might just be owing
to this particular test sample having more verbs with
higher perplexities, and maybe even ones that are
indeed difficult to disambiguate ? in spite of high
human agreement. The hope is that better knowl-
edge sources can overcome the gap still existing be-
tween the system performance and human agree-
ment. Overall, however, this data indicates that the
approach suggested by (Palmer, 2000) and that is be-
ing adopted in the ongoing OntoNotes project (Hovy
et al, 2006) does result in higher system perfor-
mance. Whether or not the more coarse-grained
senses are effective in improving natural language
processing applications remains to be seen.
89
Lemma S s T t 1 2 3 4 5 6 7 8 Average Min Max
turn.v 13 8 340 62 58 61 40 55 52 53 27 44 49 27 61
go.v 12 6 244 61 64 69 38 66 43 46 31 39 49 31 69
come.v 10 9 186 43 49 46 56 60 37 23 23 49 43 23 60
set.v 9 5 174 42 62 50 52 57 50 57 36 50 52 36 62
hold.v 8 7 129 24 58 46 50 54 54 38 50 67 52 38 67
raise.v 7 6 147 34 50 44 29 26 44 26 24 12 32 12 50
work.v 7 5 230 43 74 65 65 65 72 67 46 65 65 46 74
keep.v 7 6 260 80 56 54 52 64 56 52 48 51 54 48 64
start.v 6 4 214 38 53 50 47 55 45 42 37 45 47 37 55
lead.v 6 6 165 39 69 69 85 69 51 69 36 46 62 36 85
see.v 6 5 158 54 56 54 46 54 57 52 48 48 52 46 57
ask.v 6 3 348 58 84 72 72 78 76 52 67 66 71 52 84
find.v 5 3 174 28 93 93 86 89 82 82 75 86 86 75 93
fix.v 5 3 32 2 50 50 50 50 50 0 0 50 38 0 50
buy.v 5 3 164 46 83 80 80 83 78 76 70 76 78 70 83
begin.v 4 2 114 48 83 65 75 69 79 56 50 56 67 50 83
kill.v 4 1 111 16 88 88 88 88 88 88 88 81 87 81 88
join.v 4 4 68 18 44 50 50 39 56 57 39 44 47 39 57
end.v 4 3 135 21 90 86 86 90 62 87 86 67 82 62 90
do.v 4 2 207 61 92 90 90 93 93 90 85 84 90 84 93
examine.v 3 2 26 3 100 100 67 100 100 67 100 33 83 33 100
report.v 3 2 128 35 89 91 91 91 91 91 91 86 90 86 91
regard.v 3 3 40 14 93 93 86 86 64 86 57 93 82 57 93
recall.v 3 1 49 15 100 100 87 87 93 87 87 87 91 87 100
prove.v 3 2 49 22 90 88 82 80 90 86 70 74 82 70 90
claim.v 3 2 54 15 67 73 80 80 80 80 80 87 78 67 87
build.v 3 3 119 46 74 67 74 61 54 74 61 72 67 54 74
feel.v 3 3 347 51 71 69 69 74 76 69 61 71 70 61 76
care.v 3 3 69 7 43 43 43 43 100 29 57 57 52 29 100
contribute.v 2 2 35 18 67 72 72 67 50 61 50 67 63 50 72
maintain.v 2 2 61 10 80 80 70 100 80 90 90 80 84 70 100
complain.v 2 1 32 14 93 86 86 86 86 86 86 79 86 79 93
propose.v 2 2 34 14 100 86 100 86 100 93 79 79 90 79 100
promise.v 2 2 50 8 88 88 75 88 75 75 62 88 80 62 88
produce.v 2 2 115 44 82 82 77 73 75 75 77 80 78 73 82
prepare.v 2 2 54 18 94 83 89 89 83 86 83 83 86 83 94
explain.v 2 2 85 18 94 89 94 89 94 89 89 94 92 89 94
believe.v 2 2 202 55 87 78 78 86 84 78 74 80 81 74 87
occur.v 2 2 47 22 86 73 91 96 86 96 86 82 87 73 96
grant.v 2 2 19 5 100 80 80 80 40 80 60 80 75 40 100
enjoy.v 2 2 56 14 50 57 57 50 64 57 50 57 55 50 64
need.v 2 2 195 56 89 82 86 89 86 78 70 70 81 70 89
disclose.v 1 1 55 14 93 93 93 93 93 93 93 93 93 93 93
point.n 9 6 469 150 91 91 89 91 92 87 84 79 88 79 92
position.n 7 6 268 45 78 78 78 53 56 65 58 64 66 53 78
defense.n 7 7 120 21 57 48 52 43 48 29 48 48 46 29 57
carrier.n 7 3 111 21 71 71 71 71 67 71 71 62 70 62 71
order.n 7 4 346 57 93 95 93 91 93 92 90 91 92 90 95
exchange.n 5 3 363 61 92 90 92 85 90 88 82 79 87 79 92
system.n 5 3 450 70 79 73 66 67 59 63 63 61 66 59 79
source.n 5 5 152 35 86 80 80 63 83 68 60 29 69 29 86
space.n 5 2 67 14 93 100 93 93 93 86 86 71 89 71 100
base.n 5 4 92 20 75 80 75 50 65 40 50 75 64 40 80
authority.n 4 3 90 21 86 86 81 62 71 33 71 81 71 33 86
people.n 4 4 754 115 96 96 95 96 95 90 91 91 94 90 96
chance.n 4 3 91 15 60 67 60 60 67 73 20 73 60 20 73
part.n 4 3 481 71 90 90 92 97 90 74 66 66 83 66 97
hour.n 4 2 187 48 83 85 92 83 77 90 58 92 83 58 92
development.n 3 3 180 29 100 79 86 79 76 62 79 62 78 62 100
president.n 3 3 879 177 98 97 98 97 93 96 97 85 95 85 98
network.n 3 3 152 55 91 87 98 89 84 88 87 82 88 82 98
future.n 3 3 350 146 97 96 94 97 83 98 89 85 92 83 98
effect.n 3 2 178 30 97 93 80 93 80 90 77 83 87 77 97
state.n 3 3 617 72 85 86 86 83 82 79 83 82 83 79 86
power.n 3 3 251 47 92 87 87 81 77 77 77 74 81 74 92
bill.n 3 3 404 102 98 99 98 96 90 96 96 22 87 22 99
area.n 3 3 326 37 89 73 65 68 84 70 68 65 73 65 89
job.n 3 3 188 39 85 80 77 90 80 82 69 82 80 69 90
management.n 2 2 284 45 89 78 87 73 98 76 67 64 79 64 98
condition.n 2 2 132 34 91 82 82 56 76 78 74 76 77 56 91
policy.n 2 2 331 39 95 97 97 87 95 97 90 64 90 64 97
rate.n 2 2 1009 145 90 88 92 81 92 89 88 91 89 81 92
drug.n 2 2 205 46 94 94 96 78 94 94 87 78 89 78 96
Average Overall 86 83 83 82 82 79 76 77
Verbs 78 75 73 76 73 70 65 70
Nouns 89 87 86 81 83 80 77 76
Table 4: All Supervised system performance per predicate. (Column legend ? S=number of senses in training; s=number senses appearing more than 3 times;
T=instances in training; t=instances in test.; The numbers indicate system ranks.)
90
3 Semantic Role Labeling
Subtask 2 evaluates Semantic Role Labeling (SRL)
systems, where the goal is to locate the constituents
which are arguments of a given verb, and to assign
them appropriate semantic roles that describe how
they relate to the verb. SRL systems are an impor-
tant building block for many larger semantic sys-
tems. For example, in order to determine that ques-
tion (1a) is answered by sentence (1b), but not by
sentence (1c), we must determine the relationships
between the relevant verbs (eat and feed) and their
arguments.
(1) a. What do lobsters like to eat?
b. Recent studies have shown that lobsters pri-
marily feed on live fish, dig for clams, sea
urchins, and feed on algae and eel-grass.
c. In the early 20th century, Mainers would
only eat lobsters because the fish they
caught was too valuable to eat themselves.
Traditionally, SRL systems have been trained on
either the PropBank corpus (Palmer et al, 2005)
? for two years, the CoNLL workshop (Carreras
and Ma`rquez, 2004; Carreras and Ma`rquez, 2005)
has made this their shared task, or the FrameNet
corpus ? Senseval-3 used this for their shared task
(Litkowski, 2004). However, there is still little con-
sensus in the linguistics and NLP communities about
what set of role labels are most appropriate. The
PropBank corpus avoids this issue by using theory-
agnostic labels (ARG0, ARG1, . . . , ARG5), and
by defining those labels to have only verb-specific
meanings. Under this scheme, PropBank can avoid
making any claims about how any one verb?s ar-
guments relate to other verbs? arguments, or about
general distinctions between verb arguments and ad-
juncts.
However, there are several limitations to this ap-
proach. The first is that it can be difficult to make
inferences and generalizations based on role labels
that are only meaningful with respect to a single
verb. Since each role label is verb-specific, we can
not confidently determine when two different verbs?
arguments have the same role; and since no encoded
meaning is associated with each tag, we can not
make generalizations across verb classes. In con-
trast, the use of a shared set of role labels, such
System Type Precision Recall F
UBC-UPC Open 84.51 82.24 83.36?0.5
UBC-UPC Closed 85.04 82.07 83.52?0.5
RTV Closed 81.82 70.37 75.66?0.6
Without ?say?
UBC-UPC Open 78.57 74.70 76.60?0.8
UBC-UPC Closed 78.67 73.94 76.23?0.8
RTV Closed 74.15 57.85 65.00?0.9
Table 5: System performance on PropBank argu-
ments.
as VerbNet roles, would facilitate both inferencing
and generalization. VerbNet has more traditional la-
bels such as Agent, Patient, Theme, Beneficiary, etc.
(Kipper et al, 2006).
Therefore, we chose to annotate the corpus us-
ing two different role label sets: the PropBank role
set and the VerbNet role set. VerbNet roles were
generated using the SemLink mapping (Loper et al,
2007), which provides a mapping between Prop-
Bank and VerbNet role labels. In a small number of
cases, no VerbNet role was available (e.g., because
VerbNet did not contain the appropriate sense of the
verb). In those cases, the PropBank role label was
used instead.
We proposed two levels of participation in this
task: i) Closed ? the systems could use only the an-
notated data provided and nothing else. ii) Open ?
where systems could use PropBank data from Sec-
tions 02-21, as well as any other resource for training
their labelers.
3.1 Data
We selected 50 verbs from the 65 in the lexical sam-
ple task for the SRL task. The partitioning into train
and test set was done in the same fashion as for the
lexical sample task. Since PropBank does not tag
any noun predicates, none of the 35 nouns from the
lexical sample task were part of this data.
3.2 Results
For each system, we calculated the precision, re-
call, and F-measure for both role label sets. Scores
were calculated using the srl-eval.pl script from
the CoNLL-2005 scoring package (Carreras and
Ma`rquez, 2005). Only two teams chose to perform
the SRL subtask. The performance of these two
teams is shown in Table 5 and Table 6.
91
System Type Precision Recall F
UBC-UPC Open 85.31 82.08 83.66?0.5
UBC-UPC Closed 85.31 82.08 83.66?0.5
RTV Closed 81.58 70.16 75.44?0.6
Without ?say?
UBC-UPC Open 79.23 73.88 76.46?0.8
UBC-UPC Closed 79.23 73.88 76.46?0.8
RTV Closed 73.63 57.44 64.53?0.9
Table 6: System performance on VerbNet roles.
3.3 Discussion
Given that only two systems participated in the task,
it is difficult to form any strong conclusions. It
should be noted that since there was no additional
VerbNet role data to be used by the Open system, the
performance of that on PropBank arguments as well
as VerbNet roles is exactly identical. It can be seen
that there is almost no difference between the perfor-
mance of the Open and Closed systems for tagging
PropBank arguments. The reason for this is the fact
that all the instances of the lemma under consider-
ation was selected from the Propbank corpus, and
probably the number of training instances for each
lemma as well as the fact that the predicate is such
an important feature combine to make the difference
negligible. We also realized that more than half of
the test instances were contributed by the predicate
?say? ? the performance over whose arguments is in
the high 90s. To remove the effect of ?say? we also
computed the performances after excluding exam-
ples of ?say? from the test set. These numbers are
shown in the bottom half of the two tables. These
results are not directly comparable to the CoNLL-
2005 shared task since: i) this test set comprises
Sections 01, 22, 23 and 24 as opposed to just Sec-
tion 23, and ii) this test set comprises data for only
50 predicates as opposed to all the verb predicates in
the CoNLL-2005 shared task.
4 Conclusions
The results in the previous discussion seem to con-
firm the hypothesis that there is a predictable corre-
lation between human annotator agreement and sys-
tem performance. Given high enough ITA rates we
can can hope to build sense disambiguation systems
that perform at a level that might be of use to a con-
suming natural language processing application. It
is also encouraging that the more informative Verb-
Net roles which have better/direct applicability in
downstream systems, can also be predicted with al-
most the same degree of accuracy as the PropBank
arguments from which they are mapped.
5 Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022;
National Science Foundation Grant NSF-0415923,
Word Sense Disambiguation; the DTO-AQUAINT
NBCHC040036 grant under the University of
Illinois subcontract to University of Pennsylvania
2003-07911-01; and NSF-ITR-0325646:
Domain-Independent Semantic Interpretation.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL-2004.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. In
Proceedings of CoNLL-2005.
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha Palmer.
2006. An empirical study of the behavior of active learning for
word sense disambiguation. In Proceedings of HLT/NAACL.
Timothy Chklovski and Rada Mihalcea. 2002. Building a
sense tagged corpus with open mind word expert. In
Proceedings of ACL-02 Workshop on WSD.
Bart Decadt, Ve?ronique Hoste, Walter Daelemans, and Antal
Van den Bosch. 2004. GAMBL, genetic algorithm
optimization of memory-based wsd. In Senseval-3.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90%
solution. In Proceedings of HLT/NAACL, June.
Karin Kipper, Anna Korhonen, Neville Ryant, and Martha
Palmer. 2006. Extending VerbNet with novel verb classes. In
LREC-06.
Ken Litkowski. 2004. Senseval-3 task: Automatic labeling of
semantic roles. In Proceedings of Senseval-3.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between propbank and
verbnet. In Proceedings of the IWCS-7.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
proposition bank: A corpus annotated with semantic roles.
Computational Linguistics, 31(1):71?106.
Martha Palmer. 2000. Consistent criteria for sense
distinctions. Computers and the Humanities, 34(1-1):217?222.
92
University of Colorado Dialog Systems for 
Travel and Navigation 
B. Pellom, W. Ward, J. Hansen, R. Cole, K. Hacioglu, J. Zhang, X. Yu, S. Pradhan 
Center for Spoken Language Research, University of Colorado 
Boulder, Colorado 80303, USA 
{pellom, whw, jhlh, cole, hacioglu, zjp, xiu, spradhan}@cslr.colorado.edu 
 
ABSTRACT 
This paper presents recent improvements in the development of 
the University of Colorado ?CU Communicator? and ?CU-
Move? spoken dialog systems. First, we describe the CU 
Communicator system that integrates speech recognition, 
synthesis and natural language understanding technologies using 
the DARPA Hub Architecture. Users are able to converse with an 
automated travel agent over the phone to retrieve up-to-date 
travel information such as flight schedules, pricing, along with 
hotel and rental car availability.  The CU Communicator has 
been under development since April of 1999 and represents our 
test-bed system for developing robust human-computer 
interactions where reusability and dialogue system portability 
serve as two main goals of our work.  Next, we describe our more 
recent work on the CU Move dialog system for in-vehicle route 
planning and guidance.  This work is in joint collaboration with 
HRL and is sponsored as part of the DARPA Communicator 
program.  Specifically, we will provide an overview of the task, 
describe the data collection environment for in-vehicle systems 
development, and describe our initial dialog system constructed 
for route planning. 
1. CU COMMUNICATOR 
1.1 Overview  
The Travel Planning Task 
The CU Communicator system [1,2] is a Hub compliant 
implementation of the DARPA Communicator task [3].  The 
system combines continuous speech recognition, natural 
language understanding and flexible dialogue control to enable 
natural conversational interaction by telephone callers to access 
information from the Internet pertaining to airline flights, hotels 
and rental cars.  Specifically, users can describe a desired airline 
flight itinerary to the Communicator and use natural dialog to 
negotiate a flight plan.  Users can also inquire about hotel 
availability and pricing as well as obtain rental car reservation 
information.   
System Overview 
The dialog system is composed of a Hub and several servers as 
shown in Fig. 1.  The Hub is used as a centralized message router 
through which servers can communicate with one another [4].  
Frames containing keys and values are emitted by each server, 
routed by the hub, and received by a secondary server based on 
rules defined in a ?Hub script?.   
 
 
 
 
 
 
 
 
 
Figure 1.  Block diagram of the functional components that 
comprise the CU Communicator system1. 
1.2 Audio Server 
The audio server is responsible for answering the incoming call, 
playing prompts and recording user input.  Currently, our system 
uses the MIT/MITRE audio server that was provided to DARPA 
Communicator program participants.  The telephony hardware 
consists of an external serial modem device that connects to the 
microphone input and speaker output terminals on the host 
computer.  The record process is pipelined to the speech 
recognition server and the play process is pipelined the text-to-
speech server.  This audio server does not support barge-in. 
Recently we have developed a new audio server that supports 
barge-in using the Dialogic hardware platform.  The new audio 
server implements a Fast Normalized Least-Mean-Square (LMS) 
algorithm for software-based echo cancellation.  During 
operation, the echo from the system speech is actively cancelled 
from the recorded audio to allow the user to cut through while 
                                                          
1
 This work was supported by DARPA through SPAWAR under 
Grant No. N66001-002-8906.  The ?CU Move? system is 
supported in part through a joint collaboration with HRL 
Laboratories. 
 
 
 
Language
Generator
Language
enerator
Hub
Speech 
Recognizer
Speech 
Recognizer
Speech 
Synthesizer
Speech 
Synthesizer
Semantic
Parser
Se antic
Parser
Dialogue
Manager
Dialogue
anager
Data Base / 
Backend
Data Base / 
Backend
Confidence
Server
Confidence
Server
Audio ServerAudio Server
www
the system is speaking.  The new audio server operates in the 
Linux environment and is currently being field-tested at CSLR.  
Because the server implements software-based echo cancellation, 
it can work on virtually any low-cost Dialogic hardware 
platform.  This server will be made available to the research 
community as a resource in the near future. 
1.3 Speech Recognizer 
We are currently using the Carnegie Mellon University Sphinx-II 
system [5] in our speech recognition server. This is a semi-
continuous Hidden Markov Model recognizer with a class 
trigram language model. The recognition server receives the 
input vectors from the audio server. The recognition server 
produces a word lattice from which a single best hypothesis is 
picked and sent to the hub for processing by the dialog manager. 
Acoustic Modeling 
During dialog interaction with the user, the audio server sends 
the acoustic samples to three Sphinx-II speech recognizers.  
While the language model is the same for each decoder, the 
acoustic models consist of (i) speaker independent analog 
telephone, (ii) female adapted analog telephone, and (iii) cellular 
telephone adapted acoustic model sets.   Each decoder outputs a 
word string hypothesis along with a word-sequence probability 
for the best path.  An intermediate server is used to examine each 
hypothesis and pass the most likely word string onto the natural 
language understanding module.   
Language Modeling 
The Communicator system is designed for end users to get up-to-
date worldwide air travel, hotel and rental car information via the 
telephone. In the task there are word lists for countries, cities, 
states, airlines, etc.  To train a robust language model, names are 
clustered into different classes. An utterance with class tagging is 
shown in Fig.2.  In this example, city, hour_number, and am_pm 
are class names. 
Figure 2.  Examples of class-based and grammar-based 
language modeling  
Each commonly used word takes one class. The probability of 
word Wi given class Ci is estimated from training corpora. After 
the corpora are correctly tagged, a back-off class-based trigram 
language model can be computed from the tagged corpora.  We 
use the CMU-Cambridge Statistical Language Modeling Toolkit 
to compute our language models. 
More recently, we have developed a dialog context dependent 
language model (LM) combining stochastic context free 
grammars (SCFGs) and n-grams [6,7].  Based on a spoken 
language production model in which a user picks a set of 
concepts with respective values and constructs word sequences 
using phrase generators associated with each concept in 
accordance with the dialog context, this LM computes the 
probability of a word, P(W), as 
 
         P(W) = P(W/C) P(C/S)          (1) 
 
where W is the sequence of words, C is the sequence of concepts 
and S is the dialog context. Here, the assumptions are (i) S is 
given, (ii) W is independent of S but C, and (iii) W and C 
associations are unambiguous. This formulation can be 
considered as a general extension of the standard class word 
based statistical language model as seen in Fig. 2. 
 
The first term in (1) is modeled by SCFGs, one for each concept. 
The concepts are classes of phrases with the same meaning. Each 
SCFG is compiled into a stochastic recursive transition network 
(STRN). Our grammar is a semantic grammar since the 
nonterminals correspond to semantic concepts instead of 
syntactic constituents. The set of task specific concepts is 
augmented with a single word, multiple word and a small number 
of broad but unambigious part of speech (POS) classes to 
account for the phrases that are not covered by the grammar. 
These classes are considered as "filler" concepts within a unified 
framework. The second term in (1) is modeled as a pool of 
concept n-gram LMs. That is, we have a separate LM for each 
dialog context. At the moment, the dialog context is selected as 
the last question prompted by the system, as it is very simple and 
yet strongly predictive and constraining. SCFG and n-gram 
probabilities are learned by simple counting and smoothing. Our 
semantic grammars have a low degree of ambiguity and therefore 
do not require computationally intensive stochastic training and 
parsing techniques. 
 
Experimental results with N-best list rescoring were found 
promising (5-6% relative improvement in WER).  In addition, we 
have shown that a dynamic combining of our new LM and the 
standard class word n-gram (the LM currently in use in our 
system) should result in further improvements. At the present, we 
are interfacing the grammar LM to the speech recognizer using a 
word graph. 
1.4 Confidence Server 
Our prior work on confidence assessment has considered 
detection and rejection of word-level speech recognition errors 
and out-of-domain phrases using language model features [8].  
More recently [9], we have considered detection and rejection of 
misrecognized units at the concept level.  Because concepts are 
used to update the state of the dialog system, we believe that 
concept level confidence is vitally important to ensuring a 
graceful human-computer interaction.  Our current work on 
concept error detection has considered language model features 
(e.g., LM back-off behavior, language model score) as well as 
acoustic features from the speech recognizer (e.g., normalized 
acoustic score, lattice density, phone perplexity).  Confidence 
Original Utterance 
I want to go from Boston to Portland around nine a_m 
Class-Tagged Utterance 
I want to go from [city:Boston] to [city:Portland] 
around [hour_number:nine] [am_pm:a_m] 
Concept-Tagged Utterance 
[I_want: I want to go] [depart_loc: from Boston] 
[arrive_loc: to Portland] [time:around nine a_m] 
features are combined to compute word-level, concept-level, and 
utterance-level confidence scores.  
1.5 Language Understanding 
We use a modified version of the Phoenix [10] parser to map the 
speech recognizer output onto a sequence of semantic frames. A 
Phoenix frame is a named set of slots, where the slots represent 
related pieces of information. Each slot has an associated 
context-free semantic grammar that specifies word string patterns 
that can fill the slot. The grammars are compiled into Recursive 
Transition Networks, which are matched against the recognizer 
output to fill slots. Each filled slot contains a semantic parse tree 
with the slot name as root.  
Phoenix has been modified to also produce an extracted 
representation of the parse that maps directly onto the task 
concept structures. For example, the utterance  
?I want to go from Boston to Denver Tuesday morning?  
would produce the extracted parse: 
Flight_Constraint: Depart_Location.City.Boston 
Flight_Constraint: Arrive_Location.City.Denver 
Flight Constraints:[Date_Time].[Date].[Day_Name].tuesday 
                             [Time_Range].[Period_Of_Day].morning 
1.6 Dialog Management 
The Dialogue Manager controls the system?s interaction with the 
user and the application server. It is responsible for deciding 
what action the system will take at each step in the interaction. 
The Dialogue Manager has several functions. It resolves 
ambiguities in the current interpretation; Estimates confidence in 
the extracted information; Clarifies the interpretation with the 
user if required; Integrates new input with the dialogue context; 
Builds database queries (SQL); Sends information to NL 
generation for presentation to user; and prompts the user for 
missing information. 
We have developed a flexible, event driven dialogue manager in 
which the current context of the system is used to decide what to 
do next. The system does not use a dialogue network or a 
dialogue script, rather a general engine operates on the semantic 
representations and the current context to control the interaction 
flow.  The Dialogue Manager receives the extracted parse. It then 
integrates the parse into the current context. Context consists of a 
set of frames and a set of global variables. As new extracted 
information arrives, it is put into the context frames and 
sometimes used to set global variables. The system provides a 
general-purpose library of routines for manipulating frames. 
This ?event driven? architecture functions similar to a production 
system. An incoming parse causes a set of actions to fire which 
modify the current context. After the parse has been integrated 
into the current context, the DM examines the context to decide 
what action to take next. The DM attempts the following actions 
in the order listed: 
? Clarify if necessary  
? Sign off if all done  
? Retrieve data and present to user  
? Prompt user for required information  
The rules for deciding what to prompt for next are very 
straightforward. The frame in focus is set to be the frame 
produced in response to the user, or to the last system prompt.  
? If there are unfilled required slots in the focus frame, then 
prompt for the highest priority unfilled slot in the frame. 
? If there are no unfilled required slots in the focus frame, 
then prompt for the highest priority missing piece of 
information in the context.  
Our mechanism does not have separate ?user initiative? and 
?system initiative? modes. If the system has enough information 
to act on, then it does it. If it needs information, then it asks for 
it. The system does not require that the user respond to the 
prompt. The user can respond with anything and the system will 
parse the utterance and set the focus to the resulting frame. This 
allows the user to drive the dialog, but doesn?t require it. The 
system prompts are organized locally, at the frame level. The 
dialog manager or user puts a frame in focus, and the system tries 
to fill it. This representation is easy to author, there is no separate 
dialog control specification required. It is also robust in that it 
has a simple control that has no state to lose track of. 
An additional benefit of Dialog Manager mechanism is that it is 
very largely declarative. Most of the work done by a developer 
will be the creation of frames, forms and grammars. The system 
developer creates a task file that specifies the system ontology 
and templates for communicating about nodes in the hierarchy. 
The templates are filled in from the values in the frames to 
generate output in the desired language. This is the way we 
currently generate SQL queries and user prompts. An example 
task frame specification is: 
Frame:Air 
 [Depart_Loc]+ 
    Prompt: "where are you departing from" 
    [City_Name]* 
 Confirm: "You are departing from $([City_Name]).  
    Is that correct?" 
 Sql: "dep_$[leg_num] in (select airport_code from 
 airport_codes where city like '!%' $(and state_province 
like '[Depart_Loc].[State]' ) )" 
    [Airport_Code]* 
 
This example defines a frame with name Air and slot 
[Depart_Loc]. The child nodes of Depart_Loc are are 
[City_Name] and [Airport_Code]. The ?+? after [Depart_Loc] 
indicates that it is a mandatory field. The Prompt string is the 
template for prompting for the node information. The ?*? after 
[City_Name] and [Airport_Code] indicate that if either of them is 
filled, the parent node [Depart_Loc] is filled. The Confirm string 
is a template to prompt the user to confirm the values. The SQL 
string is the template to use the value in an SQL query to the 
database. 
The system will prompt for all mandatory nodes that have 
prompts. Users may specify information in any order, but the 
system will prompt for whatever information is missing until the 
frame is complete.   
1.7 Database & Internet Interface  
The back-end interface consists of an SQL database and domain-
specific Perl scripts for accessing information from the Internet.  
During operation, database requests are transmitted by the Dialog 
Manager to the database server via a formatted frame. 
The back-end consists of a static and dynamic information 
component.  Static tables contain data such as conversions 
between 3-letter airport codes and the city, state, and country of 
the airport (e.g., BOS for Boston Massachusetts).  There are over 
8000 airports in our database, 200 hotel chains, and 50 car rental 
companies.  The dynamic information content consists of 
database tables for car, hotel, and airline flights.   
When a database request is received, the Dialog Manager?s SQL 
command is used to select records in local memory.  If no 
records are found to match, the back-end can submit an HTTP-
based request for the information via the Internet.  Records 
returned from the Internet are then inserted as rows into the local 
SQL database and the SQL statement is once again applied.   
1.8 Language Generation 
The language generation module uses templates to generate text 
based on dialog speech acts.  Example dialog acts include 
?prompt? for prompting the user for needed information, 
?summarize? for summarization of flights, hotels, and rental cars, 
and ?clarify? for clarifying information such as departure and 
arrival cities that share the same name. 
1.9 Text-to-Speech Synthesis 
For audio output, we have developed a domain-dependent 
concatenative speech synthesizer.  Our concatenative synthesizer 
can adjoin units ranging from phonemes, to words, to phrases 
and sentences.   For domain modeling, we use a voice talent to 
record entire task-dependent utterances  (e.g., ?What are your 
travel plans??) as well as short phrases with carefully determined 
break points (e.g., ?United flight?, ?ten?, ?thirty two?, ?departs 
Anchorage at?).    Each utterance is orthographically transcribed 
and phonetically aligned using a HMM-based recognizer.   Our 
research efforts for data collection are currently focused on 
methods for reducing the audible distortion at segment 
boundaries, optimization schemes for prompt generation, as well 
as tools for rapidly correcting boundary misalignments.  In 
general, we find that some degree of hand-correction is always 
required in order to reduce distortions at concatenation points. 
During synthesis, the text is automatically divided into individual 
sentences that are then synthesized and pipelined to the audio 
server.  A text-to-phoneme conversion is applied using a 
phonetic dictionary.  Words that do not appear in the phonetic 
dictionary are automatically pronounced using a multi-layer 
perceptron based pronunciation module.  Here, a 5-letter context 
is extracted from the word to be pronounced.  The letter input is 
fed through the MLP and a phonetic symbol (or possibly epsilon) 
is output by the network.  By sliding the context window, we can 
extract the phonetic pronunciation of the word.   The MLP is 
trained using letter-context and symbol output pairs from a large 
phonetic dictionary. 
The selection of units to concatenate is determined using a hybrid 
search algorithm that operates at the word or phoneme level.  
During synthesis, sections of word-level text that have been 
recorded are automatically concatenated.  Unrecorded words or 
word sequences are synthesized using a Viterbi beam search 
across all available phonetic units.  The cost function includes 
information regarding phonetic context, pitch, duration, and 
signal amplitude.  Audio segments making up the best-path are 
then concatenated to generate the final sentence waveform.   
2. DATA COLLECTION & EVALUATION 
2.1 Data Collection Efforts 
Local Collection Effort 
The Center for Spoken Language Research maintains a dialup 
Communicator system for data collection1. Users wishing to use 
the dialogue system can register at our web site [1] and receive a 
PIN code and system telephone number. To date, our system has 
fielded over 1750 calls totaling over 25,000 utterances from 
nearly 400 registered users.  
NIST Multi-Site Data Collection 
During the months of June and July of 2000, The National 
Institute of Standards (NIST) conducted a multi-site data 
collection effort for the nine DARPA Communicator 
participants.  Participating sites included: AT&T, IBM, BBN, 
SRI, CMU, Colorado, MIT, Lucent, and MITRE.  In this data 
collection, a pool of potential users was selected from various 
parts of the United States by a market research firm.  The 
selected subjects were native speakers of American English who 
were possible frequent travelers.  Users were asked to perform 
nine tasks.  The first seven tasks consisted of fixed scenarios for 
one-way and round-trip flights both within and outside of the 
United States. The final two tasks consisted of users making 
open-ended business or vacation.   
2.2 System Evaluation 
Task Completion 
A total of 72 calls from NIST participants were received by the 
CU Communicator system.  Of these, 44 callers were female and 
28 were male.  Each scenario was inspected by hand and 
compared against the scenario provided by NIST to the subject. 
For the two open-ended tasks, judgment was made based on what 
the user asked for with that of the data provided to the user. In 
total, 53/72 (73.6%) of the tasks were completed successfully.   
A detailed error analysis can be found in [11]. 
Word Error Rate Analysis 
A total of 1327 utterances were recorded from the 72 NIST calls.  
Of these, 1264 contained user speech.  At the time of the June 
2000 NIST evaluation, the CU Communicator system did not 
implement voice-based barge-in.  We noticed that one source of 
error was due to users who spoke before the recording process 
was started.  Even though a tone was presented to the user to 
signify the time to speak, 6.9% of the utterances contained 
instances in which the user spoke before the tone.  Since all users 
were exposed to several other Communicator systems that 
                                                          
2
 The system can be accessed toll-free at 1-866-735-5189 
employed voice barge-in, there may be some effect from 
exposure to those systems. Table 3 summarizes the word error 
rates for the system utilizing the June 2000 NIST data as the test 
set.  Overall, the system had a word error rate (WER) of 26.0% 
when parallel gender-dependent decoders were utilized. Since 
June of 2000, we have collected an additional 15,000 task-
dependent utterances.  With the extra data, we were able to 
remove our dependence on the CMU Communicator training 
data [12].  When the language model was reestimated and 
language model weights reoptimized using only CU 
Communicator data, the WER dropped from 26.0% to 22.5%.  
This amounts to a 13.5% relative reduction in WER. 
Table 1: CU Communicator Word Error Rates for (A) 
Speaker Independent acoustic models and June 2000 
language model, (B) Gender-dependent parallel recognizers 
with June 2000 Language Model, and (C) Language Model 
retrained in December 2000. 
June 2000 NIST Evaluation Data, 1264 
utterances, 72 speakers 
Word Error 
Rate 
(A) Speaker Indep. HMMs (LM#1) 29.8% 
(B) Gender Dependent HMMs (LM#1) 26.0% 
(C) Gender Dependent HMMs (LM#2)  22.5% 
 
Core Metrics 
Sites in the DARPA Communicator program agreed to log a 
common set of metrics for their systems. The proposed set of 
metrics was: Task Completion, Time to Completion, Turns to 
Completion, User Words/Turn, System Words/Turn, User 
Concepts/Turn, Concept Efficiency, State of Itinerary, Error 
Messages, Help Messages, Response Latency, User Words to 
Completion, System Words to Completion, User Repeats, System 
Repeats/Reprompts, Word Error, Mean Length of System 
Utterance, and Mean Length of System Turn. 
Table 2: Dialogue system evaluation metrics 
Item Min Mean Max 
Time to Completion (secs) 120.9 260.3 537.2 
Total Turns to Completion 23 37.6 61 
Response Latency (secs) 1.5 1.9 2.4 
User Words to Task End 19 39.4 105 
System Words to End 173 331.9 914 
Number of Reprompts 0 2.4 15 
 
Table 2 summarizes results obtained from metrics derived 
automatically from the logged timing markers for the calls in 
which the user completed the task assigned to them.  The average 
time to task completion is 260.  During this period there are an 
average of 19 user turns and 19 computer turns (37.6 average 
total turns).  The average response latency was 1.86 seconds.  
The response latency also includes the time required to access the 
data live from the Internet travel information provider. 
3. CU MOVE 
3.1 Task Overview 
The ?CU Move? system represents our work towards achieving 
graceful human-computer interaction in automobile 
environments.  Initially, we have considered the task of vehicle 
route planning and navigation.  As our work progresses, we will 
expand our dialog system to new tasks such as information 
retrieval and summarization and multimedia access. 
The problem of voice dialog within vehicle environments offers 
some important speech research challenges. Speech recognition 
in car environments is in general fragile, with word-error-rates 
(WER) ranging from 30-65% depending on driving conditions. 
These changing environmental conditions include speaker 
changes (task stress, emotion, Lombard effect, etc.) as well as the 
acoustic environment (road/wind noise from windows, air 
conditioning, engine noise, exterior traffic, etc.).   
In developing the CU-Move system [13,14], there are a number 
of research challenges that must be overcome to achieve reliable 
and natural voice interaction within the car environment. Since 
the speaker is performing a task (driving the vehicle), the driver 
will experience a measured level of user task stress and therefore 
this should be included in the speaker-modeling phase. Previous 
studies have clearly shown that the effects of speaker stress and 
Lombard effect can cause speech recognition systems to fail 
rapidly. In addition, microphone type and placement for in-
vehicle speech collection can impact the level of acoustic 
background noise and speech recognition performance.    
3.2 Signal Processing  
Our research for robust recognition in automobile environments 
is concentrated on development of an intelligent microphone 
array.  Here, we employ a Gaussian Mixture Model (GMM) 
based environmental classification scheme to characterize the 
noise conditions in the automobile.  By integrating an 
environmental classification system into the microphone array 
design, decisions can be made as to how best to utilize a noise-
adaptive frequency-partitioned iterative enhancement algorithm 
[15,16] or model-based adaptation algorithms [17,18] during 
decoding to optimize speech recognition accuracy on the beam-
formed signal. 
3.3 Data Collection 
A five-channel microphone array was constructed using Knowles 
microphones and a multi-channel data recorder housing built 
(Fostex) for in-vehicle data collection. An additional reference 
microphone is situated behind the driver?s seat.  Fig. 3 shows the 
constructed microphone array and data recorder housing. 
      
Figure 3: Microphone array and reference microphone (left), 
Fostex multi-channel data recorder (right). 
As part of the CU-Move system formulation, a two phase data 
collection plan has been initiated. Phase I focuses on collecting 
acoustic noise and probe speech from a variety of cars and 
driving conditions. Phase II focuses on a extensive speaker 
collection across multiple U.S. sites. A total of eight vehicles 
have been selected for acoustic noise analysis. These include the 
following: a compact car, minivan, cargo van, sport utility 
vehicle (SUV), compact and full size trucks, sports car, full size 
luxury car.  A fixed 10 mile route through Boulder, CO was used 
for Phase I data collection. The route consisted of city (25 & 
45mph) and highway driving (45 & 65mph). The route included 
stop-and-go traffic, and prescribed locations where 
driver/passenger windows, turn signals, wiper blades, air 
conditioning were operated. Each data collection run per car 
lasted approximately 35-45 minutes.  A detailed acoustic analysis 
of Phase I data can be found in [13]. Our plan is to begin Phase 
II speech/dialogue data collection during spring 2001, which will 
include (i) phonetically balanced utterances, (ii) task-specific 
vocabularies, (iii) natural extemporaneous speech, and (iv) 
human-to-human and Wizard-of-Oz (WOZ) interaction with CU-
Communicator and CU-Move dialog systems. 
3.4 Prototype Dialog System 
Finally, we have developed a prototype dialog system for data 
collection in the car environment.  The dialog system is based on 
the MIT Galaxy-II Hub architecture with base system 
components derived from the CU Communicator system [1].  
Users interacting with the dialog system can enter their origin 
and destination address by voice. Currently, 1107 street names 
for Boulder, CO area are modeled.  The system can resolve street 
addresses by business name via interaction with an Internet 
telephone book.  This allows users to ask more natural route 
queries (e.g., ?I need an auto repair shop?, or ?I need to get to the 
Boulder Marriott?).  The dialog system automatically retrieves 
the driving instructions from the Internet using an online WWW 
route direction provider.  Once downloaded, the driving 
directions are queried locally from an SQL database.  During 
interaction, users mark their location on the route by providing 
spoken odometer readings.  Odometer readings are needed since 
GPS information has not yet been integrated into the prototype 
dialog system. Given the odometer reading of the vehicle as an 
estimate of position, route information such as turn descriptions, 
distances, and summaries can be queried during travel (e.g., 
"What's my next turn", "How far is it", etc.).  
The prototype system uses the CMU Sphinx-II speech recognizer 
with cellular telephone acoustic models along with the Phoenix 
Parser [10] for semantic parsing.  The dialog manager is mixed-
initiative and event driven.  For route guidance, the natural 
language generator formats the driving instructions before 
presentation to the user by the text-to-speech server.   For 
example, the direction,  "Park Ave W. becomes 22nd St." is 
reformatted to, "Park Avenue West becomes Twenty Second  
Street".  Here, knowledge of the task-domain can be used to 
significantly improve the quality of the output text.   For speech 
synthesis, we have developed a Hub-compliant server that 
interfaces to the AT&T NextGen speech synthesizer.   
3.5 Future Work 
We have developed a Hub compliant server that interfaces a 
Garmin GPS-III global positioning device to a mobile computer 
via a serial port link.  The GPS server reports vehicle velocity in 
the X,Y,Z directions as well as real-time updates of  vehicle 
position in latitude and longitude.  HRL Laboratories has 
developed a route server that interfaces to a major navigation 
content provider.  The HRL route server can take GPS 
coordinates as inputs and can describe route maneuvers in terms 
of GPS coordinates.  In the near-term, we will interface our GPS 
server to the HRL route server in order to provide real-time 
updating of vehicle position.  This will eliminate the need for 
periodic location update by the user and also will allow for more 
interesting dialogs to be established (e.g., the computer might 
proactively tell the user about upcoming points of interest, etc.). 
 
4. REFERENCES 
[1] http://communicator.colorado.edu 
[2] W. Ward, B. Pellom, "The CU Communicator System," IEEE 
Workshop on Automatic Speech Recognition and Understanding, 
Keystone Colorado, December, 1999. 
[3] http://fofoca.mitre.org 
[4] Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., Zue,  V., 
?Galaxy-II: A Reference Architecture for Conversational System 
Development,? Proc. ICSLP, Sydney Australia, Vol. 3, pp. 931-
934, 1998. 
[5] Ravishankar, M.K., ?Efficient Algorithms for Speech 
Recognition?. Unpublished Dissertation CMU-CS-96-138, 
Carnegie Mellon University, 1996 
[6] K. Hacioglu, W. Ward, "Dialog-Context Dependent Language 
Modeling Using N-Grams and Stochastic Context-Free Grammars", 
Proc. IEEE ICASSP, Salt Lake City, May 2001. 
[7] K. Hacioglu, W. Ward, "Combining Language Models : Oracle 
Approach", Proc. Human Language Technology Conference, San 
Diego, March 2001. 
[8] R. San-Segundo, B. Pellom, W. Ward, J. M. Pardo, "Confidence 
Measures for Dialogue Management in the CU Communicator 
System," Proc. IEEE ICASSP, Istanbul Turkey, June 2000. 
[9] R. San-Segundo, B. Pellom, K. Hacioglu, W. Ward, J.M. Pardo, 
"Confidence Measures for Dialogue Systems," Proc. IEEE ICASSP, 
Salt Lake City, May 2001.  
[10] Ward, W., ?Extracting Information From Spontaneous Speech?, 
Proc. ICSLP, September 1994. 
[11] B. Pellom, W. Ward, S. Pradhan, "The CU Communicator: An 
Architecture for Dialogue Systems", Proc. ICSLP, Beijing China, 
November 2000. 
[12] Eskenazi,  M., Rudnicky, A., Gregory, K., Constantinides, P.,  
Brennan, R., Bennett, K., Allen, J., ?Data Collection and 
Processing in the Carnegie Mellon Communicator,?   Proc. 
Eurospeech-99, Budapest, Hungary. 
[13] J.H.L. Hansen, J. Plucienkowski, S. Gallant, B.L. Pellom, W. Ward, 
"CU-Move: Robust Speech Processing for In-Vehicle Speech 
Systems," Proc. ICSLP, vol. 1, pp. 524-527, Beijing, China, Oct. 
2000. 
[14] http://cumove.colorado.edu/ 
[15] J.H.L. Hansen, M.A. Clements, ?Constrained Iterative Speech 
Enhancement with Application to Speech Recognition,? IEEE 
Trans. Signal Proc., 39(4):795-805, 1991. 
[16] B. Pellom, J.H.L. Hansen, ?An Improved Constrained Iterative 
Speech Enhancement Algorithm for Colored Noise Environments," 
IEEE Trans. Speech & Audio Proc., 6(6):573-79, 1998. 
[17] R. Sarikaya, J.H.L. Hansen, "Improved Jacobian Adaptation for 
Fast Acoustic Model Adaptation in Noisy Speech Recognition," 
Proc. ICSLP, vol. 3, pp. 702-705, Beijing, China, Oct. 2000. 
[18] R. Sarikaya, J.H.L. Hansen, "PCA-PMC: A novel use of a priori 
knowledge for fast model combination," Proc. ICASSP, vol. II, pp. 
1113-1116, Istanbul, Turkey, June 2000. 
Shallow Semantic Parsing using Support Vector Machines?
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
In this paper, we propose a machine learning al-
gorithm for shallow semantic parsing, extend-
ing the work of Gildea and Jurafsky (2002),
Surdeanu et al (2003) and others. Our al-
gorithm is based on Support Vector Machines
which we show give an improvement in perfor-
mance over earlier classifiers. We show perfor-
mance improvements through a number of new
features and measure their ability to general-
ize to a new test set drawn from the AQUAINT
corpus.
1 Introduction
Automatic, accurate and wide-coverage techniques that
can annotate naturally occurring text with semantic argu-
ment structure can play a key role in NLP applications
such as Information Extraction, Question Answering and
Summarization. Shallow semantic parsing ? the process
of assigning a simple WHO did WHAT to WHOM, WHEN,
WHERE, WHY, HOW, etc. structure to sentences in text,
is the process of producing such a markup. When pre-
sented with a sentence, a parser should, for each predicate
in the sentence, identify and label the predicate?s seman-
tic arguments. This process entails identifying groups of
words in a sentence that represent these semantic argu-
ments and assigning specific labels to them.
In recent work, a number of researchers have cast this
problem as a tagging problem and have applied vari-
ous supervised machine learning techniques to it (Gildea
and Jurafsky (2000, 2002); Blaheta and Charniak (2000);
Gildea and Palmer (2002); Surdeanu et al (2003); Gildea
and Hockenmaier (2003); Chen and Rambow (2003);
Fleischman and Hovy (2003); Hacioglu and Ward (2003);
Thompson et al (2003); Pradhan et al (2003)). In this
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025
paper, we report on a series of experiments exploring this
approach.
For the initial experiments, we adopted the approach
described by Gildea and Jurafsky (2002) (G&J) and eval-
uated a series of modifications to improve its perfor-
mance. In the experiments reported here, we first re-
placed their statistical classification algorithm with one
that uses Support Vector Machines and then added to the
existing feature set. We evaluate results using both hand-
corrected TreeBank syntactic parses, and actual parses
from the Charniak parser.
2 Semantic Annotation and Corpora
We will be reporting on results using PropBank1 (Kings-
bury et al, 2002), a 300k-word corpus in which predi-
cate argument relations are marked for part of the verbs
in the Wall Street Journal (WSJ) part of the Penn Tree-
Bank (Marcus et al, 1994). The arguments of a verb are
labeled ARG0 to ARG5, where ARG0 is the PROTO-
AGENT (usually the subject of a transitive verb) ARG1
is the PROTO-PATIENT (usually its direct object), etc.
PropBank attempts to treat semantically related verbs
consistently. In addition to these CORE ARGUMENTS,
additional ADJUNCTIVE ARGUMENTS, referred to as
ARGMs are also marked. Some examples are ARGM-
LOC, for locatives, and ARGM-TMP, for temporals. Fig-
ure 1 shows the syntax tree representation along with the
argument labels for an example structure extracted from
the PropBank corpus.
Most of the experiments in this paper, unless speci-
fied otherwise, are performed on the July 2002 release
of PropBank. A larger, cleaner, completely adjudicated
version of PropBank was made available in Feb 2004.
We will also report some final best performance numbers
on this corpus. PropBank was constructed by assigning
semantic arguments to constituents of the hand-corrected
TreeBank parses. The data comprise several sections of
the WSJ, and we follow the standard convention of using
1http://www.cis.upenn.edu/?ace/
Section-23 data as the test set. Section-02 to Section-
21 were used for training. In the July 2002 release, the
training set comprises about 51,000 sentences, instantiat-
ing about 132,000 arguments, and the test set comprises
2,700 sentences instantiating about 7,000 arguments. The
Feb 2004 release training set comprises about 85,000 sen-
tences instantiating about 250,000 arguments and the test
set comprises 5,000 sentences instantiating about 12,000
arguments.
[ARG0 He] [predicate talked] for [ARGM?TMP about
20 minutes].
S
hhhh
((((
NP
PRP
He
ARG0
VP
hhhh
((((
VBD
talked
predicate
PP
hhh
(((
IN
for
NULL
NP
hhhhh
(((((
about 20 minutes
ARGM ? TMP
Figure 1: Syntax tree for a sentence illustrating the Prop-
Bank tags.
3 Problem Description
The problem of shallow semantic parsing can be viewed
as three different tasks.
Argument Identification ? This is the process of identi-
fying parsed constituents in the sentence that represent
semantic arguments of a given predicate.
Argument Classification ? Given constituents known to
represent arguments of a predicate, assign the appropri-
ate argument labels to them.
Argument Identification and Classification ? A combina-
tion of the above two tasks.
Each node in the parse tree can be classified as either
one that represents a semantic argument (i.e., a NON-
NULL node) or one that does not represent any seman-
tic argument (i.e., a NULL node). The NON-NULL nodes
can then be further classified into the set of argument la-
bels. For example, in the tree of Figure 1, the node IN
that encompasses ?for? is a NULL node because it does
not correspond to a semantic argument. The node NP
that encompasses ?about 20 minutes? is a NON-NULL
node, since it does correspond to a semantic argument
? ARGM-TMP.
4 Baseline Features
Our baseline system uses the same set of features in-
troduced by G&J. Some of the features, viz., predicate,
voice and verb sub-categorization are shared by all the
nodes in the tree. All the others change with the con-
stituent under consideration.
? Predicate ? The predicate itself is used as a feature.
? Path ? The syntactic path through the parse tree
from the parse constituent to the predicate being
classified. For example, in Figure 1, the path from
ARG0 ? ?He? to the predicate talked, is represented
with the string NP?S?VP?VBD. ? and ? represent
upward and downward movement in the tree respec-
tively.
? Phrase Type ? This is the syntactic category (NP,
PP, S, etc.) of the phrase/constituent corresponding
to the semantic argument.
? Position ? This is a binary feature identifying
whether the phrase is before or after the predicate.
? Voice ? Whether the predicate is realized as an ac-
tive or passive construction.
? Head Word ? The syntactic head of the phrase. This
is calculated using a head word table described by
(Magerman, 1994) and modified by (Collins, 1999,
Appendix. A).
? Sub-categorization ? This is the phrase struc-
ture rule expanding the predicate?s parent node
in the parse tree. For example, in Figure 1, the
sub-categorization for the predicate talked is
VP?VBD-PP.
5 Classifier and Implementation
We formulate the parsing problem as a multi-class clas-
sification problem and use a Support Vector Machine
(SVM) classifier (Hacioglu et al, 2003; Pradhan et al
2003). Since SVMs are binary classifiers, we have to con-
vert the multi-class problem into a number of binary-class
problems. We use the ONE vs ALL (OVA) formalism,
which involves training n binary classifiers for a n-class
problem.
Since the training time taken by SVMs scales exponen-
tially with the number of examples, and about 80% of the
nodes in a syntactic tree have NULL argument labels, we
found it efficient to divide the training process into two
stages, while maintaining the same accuracy:
1. Filter out the nodes that have a very high probabil-
ity of being NULL. A binary NULL vs NON-NULL
classifier is trained on the entire dataset. A sigmoid
function is fitted to the raw scores to convert the
scores to probabilities as described by (Platt, 2000).
2. The remaining training data is used to train OVA
classifiers, one of which is the NULL-NON-NULL
classifier.
With this strategy only one classifier (NULL vs NON-
NULL) has to be trained on all of the data. The remaining
OVA classifiers are trained on the nodes passed by the
filter (approximately 20% of the total), resulting in a con-
siderable savings in training time.
In the testing stage, we do not perform any filtering
of NULL nodes. All the nodes are classified directly
as NULL or one of the arguments using the classifier
trained in step 2 above. We observe no significant per-
formance improvement even if we filter the most likely
NULL nodes in a first pass.
For our experiments, we used TinySVM2 along with
YamCha3 (Kudo and Matsumoto, 2000)
(Kudo and Matsumoto, 2001) as the SVM training and
test software. The system uses a polynomial kernel with
degree 2; the cost per unit violation of the margin, C=1;
and, tolerance of the termination criterion, e=0.001.
6 Baseline System Performance
Table 1 shows the baseline performance numbers on the
three tasks mentioned earlier; these results are based on
syntactic features computed from hand-corrected Tree-
Bank (hence LDC hand-corrected) parses.
For the argument identification and the combined iden-
tification and classification tasks, we report the precision
(P), recall (R) and the F14 scores, and for the argument
classification task we report the classification accuracy
(A). This test set and all test sets, unless noted otherwise
are Section-23 of PropBank.
Classes Task P R F1 A
(%) (%) (%)
ALL Id. 90.9 89.8 90.4
ARGs Classification - - - 87.9
Id. + Classification 83.3 78.5 80.8
CORE Id. 94.7 90.1 92.3
ARGs Classification - - - 91.4
Id. + Classification 88.4 84.1 86.2
Table 1: Baseline performance on all three tasks using
hand-corrected parses.
7 System Improvements
7.1 Disallowing Overlaps
The system as described above might label two con-
stituents NON-NULL even if they overlap in words. This
is a problem since overlapping arguments are not allowed
in PropBank. Among the overlapping constituents we re-
tain the one for which the SVM has the highest confi-
dence, and label the others NULL. The probabilities ob-
tained by applying the sigmoid function to the raw SVM
scores are used as the measure of confidence. Table 2
shows the performance of the parser on the task of iden-
tifying and labeling semantic arguments using the hand-
corrected parses. On all the system improvements, we
perform a ?2 test of significance at p = 0.05, and all the
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
4F1 = 2PRP+R
significant improvements are marked with an ?. In this
system, the overlap-removal decisions are taken indepen-
dently of each other.
P R F1
(%) (%)
Baseline 83.3 78.5 80.8
No Overlaps 85.4 78.1 ?81.6
Table 2: Improvements on the task of argument identi-
fication and classification after disallowing overlapping
constituents.
7.2 New Features
We tested several new features. Two were obtained from
the literature ? named entities in constituents and head
word part of speech. Other are novel features.
1. Named Entities in Constituents ? Following
Surdeanu et al (2003), we tagged 7 named en-
tities (PERSON, ORGANIZATION, LOCATION,
PERCENT, MONEY, TIME, DATE) using Identi-
Finder (Bikel et al, 1999) and added them as 7
binary features.
2. Head Word POS ? Surdeanu et al (2003) showed
that using the part of speech (POS) of the head word
gave a significant performance boost to their system.
Following that, we experimented with the addition
of this feature to our system.
3. Verb Clustering ? Since our training data is rel-
atively limited, any real world test set will con-
tain predicates that have not been seen in training.
In these cases, we can benefit from some informa-
tion about the predicate by using predicate clus-
ter as a feature. The verbs were clustered into 64
classes using the probabilistic co-occurrence model
of Hofmann and Puzicha (1998). The clustering al-
gorithm uses a database of verb-direct-object rela-
tions extracted by Lin (1998). We then use the verb
class of the current predicate as a feature.
4. Partial Path ? For the argument identification task,
path is the most salient feature. However, it is also
the most data sparse feature. To overcome this prob-
lem, we tried generalizing the path by adding a new
feature that contains only the part of the path from
the constituent to the lowest common ancestor of the
predicate and the constituent, which we call ?Partial-
Path?.
5. Verb Sense Information ? The arguments that a
predicate can take depend on the word sense of the
predicate. Each predicate tagged in the PropBank
corpus is assigned a separate set of arguments de-
pending on the sense in which it is used. Table 3
illustrates the argument sets for the predicate talk.
Depending on the sense of the predicate talk, either
ARG1 or ARG2 can identify the hearer. Absence of
this information can be potentially confusing to the
learning mechanism.
Talk sense 1: speak sense 2: persuade/dissuade
Tag Description Tag Description
ARG0 Talker ARG0 Talker
ARG1 Subject ARG1 Talked to
ARG2 Hearer ARG2 Secondary action
Table 3: Argument labels associated with the two senses
of predicate talk in PropBank corpus.
We added the oracle sense information extracted
from PropBank, to our features by treating each
sense of a predicate as a distinct predicate.
6. Head Word of Prepositional Phrases ? Many ad-
junctive arguments, such as temporals and locatives,
occur as prepositional phrases in a sentence, and
it is often the case that the head words of those
phrases, which are always prepositions, are not very
discriminative, eg., ?in the city?, ?in a few minutes?,
both share the same head word ?in? and neither
contain a named entity, but the former is ARGM-
LOC, whereas the latter is ARGM-TMP. Therefore,
we tried replacing the head word of a prepositional
phrase, with that of the first noun phrase inside the
prepositional phrase. We retained the preposition in-
formation by appending it to the phrase type, eg.,
?PP-in? instead of ?PP?.
7. First and Last Word/POS in Constituent ? Some
arguments tend to contain discriminative first and
last words so we tried using them along with their
part of speech as four new features.
8. Ordinal constituent position ? In order to avoid
false positives of the type where constituents far
away from the predicate are spuriously identified as
arguments, we added this feature which is a concate-
nation of the constituent type and its ordinal position
from the predicate.
9. Constituent tree distance ? This is a finer way of
specifying the already present position feature.
10. Constituent relative features ? These are nine fea-
tures representing the phrase type, head word and
head word part of speech of the parent, and left and
right siblings of the constituent in focus. These were
added on the intuition that encoding the tree context
this way might add robustness and improve general-
ization.
11. Temporal cue words ? There are several temporal
cue words that are not captured by the named entity
tagger and were considered for addition as a binary
feature indicating their presence.
12. Dynamic class context ? In the task of argument
classification, these are dynamic features that repre-
sent the hypotheses of at most previous two nodes
belonging to the same tree as the node being classi-
fied.
8 Feature Performance
Table 4 shows the effect each feature has on the ar-
gument classification and argument identification tasks,
when added individually to the baseline. Addition of
named entities improves the F1 score for adjunctive ar-
guments ARGM-LOC from 59% to ?68% and ARGM-
TMP from 78.8% to ?83.4%. But, since these arguments
are small in number compared to the core arguments, the
overall accuracy does not show a significant improve-
ment. We found that adding this feature to the NULL vs
NON-NULL classifier degraded its performance. It also
shows the contribution of replacing the head word and the
head word POS separately in the feature where the head
of a prepositional phrase is replaced by the head word
of the noun phrase inside it. Apparently, a combination
of relative features seem to have a significant improve-
ment on either or both the classification and identification
tasks, and so do the first and last words in the constituent.
Features Class ARGUMENT ID
Acc.
P R F1
Baseline 87.9 93.7 88.9 91.3
+ Named entities 88.1 - - -
+ Head POS ?88.6 94.4 90.1 ?92.2
+ Verb cluster 88.1 94.1 89.0 91.5
+ Partial path 88.2 93.3 88.9 91.1
+ Verb sense 88.1 93.7 89.5 91.5
+ Noun head PP (only POS) ?88.6 94.4 90.0 ?92.2
+ Noun head PP (only head) ?89.8 94.0 89.4 91.7
+ Noun head PP (both) ?89.9 94.7 90.5 ?92.6
+ First word in constituent ?89.0 94.4 91.1 ?92.7
+ Last word in constituent ?89.4 93.8 89.4 91.6
+ First POS in constituent 88.4 94.4 90.6 ?92.5
+ Last POS in constituent 88.3 93.6 89.1 91.3
+ Ordinal const. pos. concat. 87.7 93.7 89.2 91.4
+ Const. tree distance 88.0 93.7 89.5 91.5
+ Parent constituent 87.9 94.2 90.2 ?92.2
+ Parent head 85.8 94.2 90.5 ?92.3
+ Parent head POS ?88.5 94.3 90.3 ?92.3
+ Right sibling constituent 87.9 94.0 89.9 91.9
+ Right sibling head 87.9 94.4 89.9 ?92.1
+ Right sibling head POS 88.1 94.1 89.9 92.0
+ Left sibling constituent ?88.6 93.6 89.6 91.6
+ Left sibling head 86.9 93.9 86.1 89.9
+ Left sibling head POS ?88.8 93.5 89.3 91.4
+ Temporal cue words ?88.6 - - -
+ Dynamic class context 88.4 - - -
Table 4: Effect of each feature on the argument identifi-
cation and classification tasks when added to the baseline
system.
We tried two other ways of generalizing the head word:
i) adding the head word cluster as a feature, and ii) replac-
ing the head word with a named entity if it belonged to
any of the seven named entities mentioned earlier. Nei-
ther method showed any improvement. We also tried gen-
eralizing the path feature by i) compressing sequences of
identical labels, and ii) removing the direction in the path,
but none showed any improvement on the baseline.
8.1 Argument Sequence Information
In order to improve the performance of their statistical ar-
gument tagger, G&J used the fact that a predicate is likely
to instantiate a certain set of arguments. We use a similar
strategy, with some additional constraints: i) argument
ordering information is retained, and ii) the predicate is
considered as an argument and is part of the sequence.
We achieve this by training a trigram language model on
the argument sequences, so unlike G&J, we can also es-
timate the probability of argument sets not seen in the
training data. We first convert the raw SVM scores to
probabilities using a sigmoid function. Then, for each
sentence being parsed, we generate an argument lattice
using the n-best hypotheses for each node in the syn-
tax tree. We then perform a Viterbi search through the
lattice using the probabilities assigned by the sigmoid
as the observation probabilities, along with the language
model probabilities, to find the maximum likelihood path
through the lattice, such that each node is either assigned
a value belonging to the PROPBANK ARGUMENTs, or
NULL.
CORE ARGs/ P R F1
Hand-corrected parses (%) (%)
Baseline w/o overlaps 90.0 86.1 88.0
Common predicate 90.8 86.3 88.5
Specific predicate lemma 90.5 87.4 ?88.9
Table 5: Improvements on the task of argument identifi-
cation and tagging after performing a search through the
argument lattice.
The search is constrained in such a way that no two
NON-NULL nodes overlap with each other. To simplify
the search, we allowed only NULL assignments to nodes
having a NULL likelihood above a threshold. While train-
ing the language model, we can either use the actual pred-
icate to estimate the transition probabilities in and out
of the predicate, or we can perform a joint estimation
over all the predicates. We implemented both cases con-
sidering two best hypotheses, which always includes a
NULL (we add NULL to the list if it is not among the
top two). On performing the search, we found that the
overall performance improvement was not much differ-
ent than that obtained by resolving overlaps as mentioned
earlier. However, we found that there was an improve-
ment in the CORE ARGUMENT accuracy on the combined
task of identifying and assigning semantic arguments,
given hand-corrected parses, whereas the accuracy of the
ADJUNCTIVE ARGUMENTS slightly deteriorated. This
seems to be logical considering the fact that the ADJUNC-
TIVE ARGUMENTS are not linguistically constrained in
any way as to their position in the sequence of argu-
ments, or even the quantity. We therefore decided to
use this strategy only for the CORE ARGUMENTS. Al-
though, there was an increase in F1 score when the lan-
guage model probabilities were jointly estimated over all
the predicates, this improvement is not statistically signif-
icant. However, estimating the same using specific predi-
cate lemmas, showed a significant improvement in accu-
racy. The performance improvement is shown in Table 5.
9 Best System Performance
The best system is trained by first filtering the most
likely nulls using the best NULL vs NON-NULL classi-
fier trained using all the features whose argument identi-
fication F1 score is marked in bold in Table 4, and then
training a ONE vs ALL classifier using the data remain-
ing after performing the filtering and using the features
that contribute positively to the classification task ? ones
whose accuracies are marked in bold in Table 4. Table 6
shows the performance of this system.
Classes Task Hand-corrected parses
P R F1 A
(%) (%) (%)
ALL Id. 95.2 92.5 93.8
ARGs Classification - - - 91.0
Id. + Classification 88.9 84.6 86.7
CORE Id. 96.2 93.0 94.6
ARGs Classification - - - 93.9
Id. + Classification 90.5 87.4 88.9
Table 6: Best system performance on all tasks using
hand-corrected parses.
10 Using Automatic Parses
Thus far, we have reported results using hand-corrected
parses. In real-word applications, the system will have
to extract features from an automatically generated
parse. To evaluate this scenario, we used the Charniak
parser (Chaniak, 2001) to generate parses for PropBank
training and test data. We lemmatized the predicate using
the XTAG morphology database5 (Daniel et al, 1992).
Table 7 shows the performance degradation when
automatically generated parses are used.
11 Using Latest PropBank Data
Owing to the Feb 2004 release of much more and com-
pletely adjudicated PropBank data, we have a chance to
5ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph-
1.5.tar.gz
Classes Task Automatic parses
P R F1 A
(%) (%) (%)
ALL Id. 89.3 82.9 86.0
ARGs Classification - - - 90.0
Id. + Classification 84.0 75.3 79.4
CORE Id. 92.0 83.3 87.4
ARGs Classification - - - 90.5
Id. + Classification 86.4 78.4 82.2
Table 7: Performance degradation when using automatic
parses instead of hand-corrected ones.
report our performance numbers on this data set. Table 8
shows the same information as in previous Tables 6 and
7, but generated using the new data. Owing to time limi-
tations, we could not get the results on the argument iden-
tification task and the combined argument identification
and classification task using automatic parses.
ALL ARGs Task P R F1 A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Classification - - - 90.1
Table 8: Best system performance on all tasks using
hand-corrected parses using the latest PropBank data.
12 Feature Analysis
In analyzing the performance of the system, it is useful
to estimate the relative contribution of the various feature
sets used. Table 9 shows the argument classification ac-
curacies for combinations of features on the training and
test data, using hand-corrected parses, for all PropBank
arguments.
Features Accuracy
(%)
All 91.0
All except Path 90.8
All except Phrase Type 90.8
All except HW and HW -POS 90.7
All except All Phrases ?83.6
All except Predicate ?82.4
All except HW and FW and LW -POS ?75.1
Path, Predicate 74.4
Path, Phrase Type 47.2
Head Word 37.7
Path 28.0
Table 9: Performance of various feature combinations on
the task of argument classification.
In the upper part of Table 9 we see the degradation in
performance by leaving out one feature or a feature fam-
ily at a time. After the addition of all the new features,
it is the case that removal of no individual feature except
predicate degrades the classification performance signifi-
cantly, as there are some other features that provide com-
plimentary information. However, removal of predicate
information hurts performance significantly, so does the
removal of a family of features, eg., all phrase types, or
the head word (HW), first word (FW) and last word (LW)
information. The lower part of the table shows the per-
formance of some feature combinations by themselves.
Table 10 shows the feature salience on the task of ar-
gument identification. One important observation we can
make here is that the path feature is the most salient fea-
ture in the task of argument identification, whereas it is
the least salient in the task of argument classification. We
could not provide the numbers for argument identifica-
tion performance upon removal of the path feature since
that made the SVM training prohibitively slow, indicating
that the SVM had a very hard time separating the NULL
class from the NON-NULL class.
Features P R F1
(%) (%)
All 95.2 92.5 93.8
All except HW 95.1 92.3 93.7
All except Predicate 94.5 91.9 93.2
Table 10: Performance of various feature combinations
on the task of argument identification
13 Comparing Performance with Other
Systems
We compare our system against 4 other shallow semantic
parsers in the literature. In comparing systems, results are
reported for all the three types of tasks mentioned earlier.
13.1 Description of the Systems
The Gildea and Palmer (G&P) System.
The Gildea and Palmer (2002) system uses the same
features and the same classification mechanism used by
G&J. These results are reported on the December 2001
release of PropBank.
The Surdeanu et al System.
Surdeanu et al (2003) report results on two systems
using a decision tree classifier. One that uses exactly the
same features as the G&J system. We call this ?Surdeanu
System I.? They then show improved performance of an-
other system ? ?Surdeanu System II,? which uses some
additional features. These results are are reported on the
July 2002 release of PropBank.
The Gildea and Hockenmaier (G&H) System
The Gildea and Hockenmaier (2003) system uses fea-
tures extracted from Combinatory Categorial Grammar
(CCG) corresponding to the features that were used by
G&J and G&P systems. CCG is a form of dependency
grammar and is hoped to capture long distance relation-
ships better than a phrase structure grammar. The fea-
tures are combined using the same algorithm as in G&J
and G&P. They use a slightly newer ? November 2002 re-
lease of PropBank. We will refer to this as ?G&H System
I?.
The Chen and Rambow (C&R) System
Chen and Rambow report on two different systems,
also using a decision tree classifier. The first ?C&R Sys-
tem I? uses surface syntactic features much like the G&P
system. The second ?C&R System II? uses additional
syntactic and semantic representations that are extracted
from a Tree Adjoining Grammar (TAG) ? another gram-
mar formalism that better captures the syntactic proper-
ties of natural languages.
Classifier Accuracy
(%)
SVM 88
Decision Tree (Surdeanu et al, 2003) 79
Gildea and Palmer (2002) 77
Table 11: Argument classification using same features
but different classifiers.
13.2 Comparing Classifiers
Since two systems, in addition to ours, report results us-
ing the same set of features on the same data, we can
directly assess the influence of the classifiers. G&P sys-
tem estimates the posterior probabilities using several dif-
ferent feature sets and interpolate the estimates, while
Surdeanu et al (2003) use a decision tree classifier. Ta-
ble 11 shows a comparison between the three systems for
the task of argument classification.
13.3 Argument Identification (NULL vs NON-NULL)
Table 12 compares the results of the task of identify-
ing the parse constituents that represent semantic argu-
ments. As expected, the performance degrades consider-
ably when we extract features from an automatic parse as
opposed to a hand-corrected parse. This indicates that the
syntactic parser performance directly influences the argu-
ment boundary identification performance. This could be
attributed to the fact that the two features, viz., Path and
Head Word that have been seen to be good discriminators
of the semantically salient nodes in the syntax tree, are
derived from the syntax tree.
Classes System Hand Automatic
P R F1 P R F1
ALL SVM 95 92 94 89 83 86
ARGs Surdeanu System II - - 89 - - -
Surdeanu System I 85 84 85 - - -
Table 12: Argument identification
13.4 Argument Classification
Table 13 compares the argument classification accuracies
of various systems, and at various levels of classification
granularity, and parse accuracy. It can be seen that the
SVM System performs significantly better than all the
other systems on all PropBank arguments.
Classes System Hand Automatic
Accuracy Accuracy
ALL SVM 91 90
ARGs G&P 77 74
Surdeanu System II 84 -
Surdeanu System I 79 -
CORE SVM 93.9 90.5
ARGs C&R System II 93.5 -
C&R System I 92.4 -
Table 13: Argument classification
13.5 Argument Identification and Classification
Table 14 shows the results for the task where the system
first identifies candidate argument boundaries and then
labels them with the most likely argument. This is the
hardest of the three tasks outlined earlier. SVM does a
very good job of generalizing in both stages of process-
ing.
Classes System Hand Automatic
P R F1 P R F1
ALL SVM 89 85 87 84 75 79
ARGs G&H System I 76 68 72 71 63 67
G&P 71 64 67 58 50 54
CORE SVM System 90 87 89 86 78 82
ARGs G&H System I 82 79 80 76 73 75
C&R System II - - - 65 75 70
Table 14: Identification and classification
14 Generalization to a New Text Source
Thus far, in all experiments our unseen test data was
selected from the same source as the training data.
In order to see how well the features generalize to
texts drawn from a similar source, we used the classifier
trained on PropBank training data to test data drawn from
the AQUAINT corpus (LDC, 2002). We annotated 400
sentences from the AQUAINT corpus with PropBank
arguments. This is a collection of text from the New
York Times Inc., Associated Press Inc., and Xinhua
News Service (PropBank by comparison is drawn from
Wall Street Journal). The results are shown in Table 15.
Task P R F1 A
(%) (%) (%)
ALL Id. 75.8 71.4 73.5 -
ARGs Classification - - - 83.8
Id. + Classification 65.2 61.5 63.3 -
CORE Id. 88.4 74.4 80.8 -
ARGs Classification - - - 84.0
Id. + Classification 75.2 63.3 68.7 -
Table 15: Performance on the AQUAINT test set.
There is a significant drop in the precision and recall
numbers for the AQUAINT test set (compared to the pre-
cision and recall numbers for the PropBank test set which
were 84% and 75% respectively). One possible reason
for the drop in performance is relative coverage of the
features on the two test sets. The head word, path and
predicate features all have a large number of possible val-
ues and could contribute to lower coverage when moving
from one domain to another. Also, being more specific
they might not transfer well across domains.
Features Arguments non-Arguments
(%) (%)
Predicate, Path 87.60 2.91
Predicate, Head Word 48.90 26.55
Cluster, Path 96.31 4.99
Cluster, Head Word 83.85 60.14
Path 99.13 15.15
Head Word 93.02 90.59
Table 16: Feature Coverage on PropBank test set using
parser trained on PropBank training set.
Features Arguments non-Arguments
(%) (%)
Predicate, Path 62.11 4.66
Predicate, Head Word 30.26 17.41
Cluster, Path 87.19 10.68
Cluster, Head Word 65.82 45.43
Path 96.50 29.26
Head Word 84.65 83.54
Table 17: Coverage of features on AQUAINT test set us-
ing parser trained on PropBank training set.
Table 16 shows the coverage for features on the hand-
corrected PropBank test set. The tables show feature
coverage for constituents that were Arguments and con-
stituents that were NULL. About 99% of the predicates in
the AQUAINT test set were seen in the PropBank train-
ing set. Table 17 shows coverage for the same features on
the AQUAINT test set. We believe that the drop in cover-
age of the more predictive feature combinations explains
part of the drop in performance.
15 Conclusions
We have described an algorithm which significantly im-
proves the state-of-the-art in shallow semantic parsing.
Like previous work, our parser is based on a supervised
machine learning approach. Key aspects of our results
include significant improvement via an SVM classifier,
improvement from new features and a series of analytic
experiments on the contributions of the features. Adding
features that are generalizations of the more specific fea-
tures seemed to help. These features were named enti-
ties, head word part of speech and verb clusters. We also
analyzed the transferability of the features to a new text
source.
We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use their named entity tagger ? Iden-
tiFinder; Martha Palmer for providing us with the PropBank
data, Valerie Krugler for tagging the AQUAINT test set with
PropBank arguments, and all the anonymous reviewers for their
helpful comments.
References
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.
1999. An algorithm that learns what?s in a name. Machine Learning, 34:211?
231.
[Blaheta and Charniak2000] Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In NAACL, pages 234?240.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head parsing for language
models. In ACL-01.
[Chen and Rambow2003] John Chen and Owen Rambow. 2003. Use of deep
linguistics features for the recognition and labeling of semantic arguments.
EMNLP-03.
[Collins1999] Michael John Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania,
Philadelphia.
[Daniel et al1992] K. Daniel, Y. Schabes, M. Zaidel, and D. Egedi. 1992. A freely
available wide coverage morphological analyzer for English. In COLING-92.
[Fleischman and Hovy2003] Michael Fleischman and Eduard Hovy. 2003. A
maximum entropy approach to framenet tagging. In HLT-03.
[Gildea and Hockenmaier2003] Dan Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using combinatory categorial grammar. In EMNLP-03.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In ACL-00, pages 512?520.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer. 2002. The necessity
of syntactic parsing for predicate argument recognition. In ACL-02.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward. 2003. Target word
detection and semantic role chunking using support vector machines. In HLT-
03.
[Hacioglu et al2003] Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Mar-
tin, and Dan Jurafsky. 2003. Shallow semantic parsing using support vector
machines. Technical Report TR-CSLR-2003-1, Center for Spoken Language
Research, Boulder, Colorado.
[Hofmann and Puzicha1998] Thomas Hofmann and Jan Puzicha. 1998. Statistical
models for co-occurrence data. Memo, MIT AI Laboratory.
[Kingsbury et al2002] Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the Penn Treebank. In HLT-02.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In CoNLL-00.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL-01.
[LDC2002] LDC. 2002. The AQUAINT Corpus of English News Text, Catalog
no. LDC2002t31.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and clustering of similar words.
In COLING-98.
[Magerman1994] David Magerman. 1994. Natural Language Parsing as Statisti-
cal Pattern Recognition. Ph.D. thesis, Stanford University, CA.
[Marcus et al1994] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn TreeBank: Annotating predicate argument structure.
[Platt2000] John Platt. 2000. Probabilities for support vector machines. In
A. Smola, P. Bartlett, B. Scolkopf, and D. Schuurmans, editors, Advances in
Large Margin Classifiers. MIT press.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Mar-
tin, and Dan Jurafsky. 2003. Semantic role parsing: Adding semantic struc-
ture to unstructured text. In ICDM-03.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for information extrac-
tion. In ACL-03.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic role labeling. In ECML-03.
Parsing Arguments of Nominalizations in English and Chinese?
Sameer Pradhan, Honglin Sun,
Wayne Ward, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303
{spradhan,sunh,whw,martin}@cslr.colorado.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
In this paper, we use a machine learning frame-
work for semantic argument parsing, and apply
it to the task of parsing arguments of eventive
nominalizations in the FrameNet database. We
create a baseline system using a subset of fea-
tures introduced by Gildea and Jurafsky (2002),
which are directly applicable to nominal pred-
icates. We then investigate new features which
are designed to capture the novelties in nom-
inal argument structure and show a significant
performance improvement using these new fea-
tures. We also investigate the parsing perfor-
mance of nominalizations in Chinese and com-
pare the salience of the features for the two lan-
guages.
1 Introduction
The field of NLP had seen a resurgence of research in
shallow semantic analysis. The bulk of this recent work
views semantic analysis as a tagging, or labeling prob-
lem, and has applied various supervised machine learn-
ing techniques to it (Gildea and Jurafsky (2000, 2002);
Gildea and Palmer (2002); Surdeanu et al (2003); Ha-
cioglu and Ward (2003); Thompson et al (2003); Prad-
han et al (2003)). Note that, while all of these systems
are limited to the analysis of verbal predicates, many un-
derlying semantic relations are expressed via nouns, ad-
jectives, and prepositions. This paper presents a prelimi-
nary investigation into the semantic parsing of eventive
nominalizations (Grimshaw, 1990) in English and Chi-
nese.
2 Semantic Annotation and Corpora
For our experiments, we use the FrameNet database
(Baker et al, 1998) which contains frame-specific se-
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025
mantic annotation of a number of predicates in English.
Predicates are grouped by the semantic frame that they
instantiate, depending on the sense of their usage, and
their arguments assume one of the frame elements or
roles specific to that frame. The predicate can be a verb,
noun, adjective, prepositional phrase, etc. FrameNet
contains about 500 different frame types and about 700
distinct frame elements. The following example illus-
trates the general idea. Here, the predicate ?complain?
instantiates a ?Statement? frame once as a nominal
predicate and once as a verbal predicate.
Did [Speaker she] make an official [Predicate:nominal com-
plaint] [Addressee to you] [Topic about the attack.]
[Message?Justice has not been done?] [Speaker he]
[Predicate:verbal complained.]
Nominal predicates in FrameNet include ultra-nominals
(Barker and Dowty, 1992), nominals and nominal-
izations. For the purposes of this study, a human analyst
went through the nominal predicates in FrameNet and
selected those that were identified as nominalizations
in NOMLEX (Macleod et al, 1998). Out of those,
the analyst then selected ones that were eventive
nominalizations.
These data comprise 7,333 annotated sentences, with
11,284 roles. There are 105 frames with about 190 dis-
tinct frame role1 types. A stratified sampling over predi-
cates was performed to select 80% of this data for train-
ing, 10% for development and another 10% for testing.
For the Chinese semantic parsing experiments, we se-
lected 22 nominalizations from the Penn Chinese Tree-
bank and tagged all the sentences containing these predi-
cates with PropBank (Kingsbury and Palmer, 2002) style
arguments ? ARG0, ARG1, etc. These consisted of 630
sentences. These are then split into two parts: 503 (80%)
for training and 127 (20%) for testing.
1We will use the terms role and arguments interchangeably
3 Baseline System
The primary assumption in our system is that a seman-
tic argument aligns with some syntactic constituent. The
goal is to identify and label constituents in a syntactic
tree that represent valid semantic arguments of a given
predicate. Unlike PropBank, there are no hand-corrected
parses available for the sentences in FrameNet, so we
cannot quantify the possible mis-alignment of the nomi-
nal arguments with syntactic constituents. The arguments
that do not align with any constituent are simply missed
by the current system.
3.1 Features We created a baseline system using
all and only those features introduced by Gildea and
Jurafsky that are directly applicable to nominal pred-
icates. Most of the features are extracted from the
syntactic parse of a sentence. We used the Charniak
parser (Chaniak, 2001) to parse the sentences in order to
perform feature extraction. The features are listed below:
Predicate ? The predicate lemma is used as a feature.
Path ? The syntactic path through the parse tree from the
parse constituent being classified to the predicate.
Constituent type ? This is the syntactic category (NP, PP,
S, etc.) of the constituent corresponding to the semantic
argument.
Position ? This is a binary feature identifying whether
the constituent is before or after the predicate.
Head word ? The syntactic head of the constituent.
3.2 Classifier and Implementation We formulate the
parsing problem as a multi-class classification problem
and use a Support Vector Machine (SVM) classifier in the
ONE vs ALL (OVA) formalism, which involves training
n classifiers for a n-class problem ? including the NULL
class. We use TinySVM2 along with YamCha3 (Kudo
and Matsumoto (2000, 2001)) as the SVM training and
test software.
3.3 Performance We evaluate our system on three
tasks: i) Argument Identification: Identifying parse con-
stituents that represent arguments of a given predicate, ii)
Argument Classification: Labeling the constituents that
are known to represent arguments with the most likely
roles, and iii) Argument Identification and Classification:
Finding constituents that represent arguments of a pred-
icate, and labeling them with the most likely roles. The
baseline performance on the three tasks is shown in Ta-
ble 1.
4 New Features
To improve the baseline performance we investigated ad-
ditional features that would provide useful information in
identifying arguments of nominalizations. Following is a
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
Task P R F?=1 A
(%) (%) (%)
Id. 81.7 65.7 72.8
Classification - - - 70.9
Id. + Classification 65.7 42.1 51.4
Table 1: Baseline performance on all three tasks.
description of each feature along with an intuitive justifi-
cation. Some of these features are not instantiated for a
particular constituent. In those cases, the respective fea-
ture values are set to ?UNK?.
1. Frame ? The frame instantiated by the particular sense
of the predicate in a sentence. This is an oracle feature.
2. Selected words/POS in constituent ? Nominal predi-
cates tend to assign arguments, most commonly through
postnominal of-complements, possessive prenominal
modifiers, etc. We added the values of the first and last
word in the constituent as two separate features. Another
two features represent the part of speech of these words.
3. Ordinal constituent position ? Arguments of nouns
tend to be located closer to the predicate than those
for verbs. This feature captures the ordinal position
of a particular constituent to the left or right of the
predicate on a left or right tree traversal, eg., first PP
from the predicate, second NP from the predicate, etc.
This feature along with the position will encode the
before/after information for the constituent.
4. Constituent tree distance ? Another way of quan-
tifying the position of the constituent is to identify its
index in the list of constituents that are encountered
during linear traversal of the tree from the predicate to
the constituent.
5. Intervening verb features ? Support verbs play an
important role in realizing the arguments of nominal
predicates. We use three classes of intervening verbs:
i) auxiliary verbs ? ones with part of speech AUX, ii)
light verbs ? a small set of known light verbs: took, take,
make, made, give, gave, went and go, and iii) other verbs
? with part of speech VBx. We added three features for
each: i) a binary feature indicating the presence of the
verb in between the predicate and the constituent ii) the
actual word as a feature, and iii) the path through the
tree from the constituent to the verb, as the subject of
intervening verbs sometimes tend to be arguments of
nominalizations. The following example could explain
the intuition behind this feature:
[Speaker Leapor] makes general [Predicate assertions] [Topic
about marriage]
6. Predicate NP expansion rule ? This is the noun
equivalent of the verb sub-categorization feature used by
Gildea and Jurafsky (2002). This is the expansion rule
instantiated by the parser, for the lowermost NP in the
tree, encompassing the predicate. This would tend to
cluster NPs with a similar internal structure and would
thus help finding argumentive modifiers.
7. Noun head of prepositional phrase constituents
? Instead of using the standard head word rule for
prepositional phrases, we use the head word of the first
NP inside the PP as the head of the PP and replace the
constituent type PP with PP-<preposition>.
8. Constituent sibling features ? These are six features
representing the constituent type, head word and part of
speech of the head word of the left and right siblings
of the constituent in consideration. These are used
to capture arguments represented by the modifiers of
nominalizations.
9. Partial-path from constituent to predicate ? This
is the path from the constituent to the lowest common
parent of the constituent and the predicate. This is used
to generalize the path statistics.
10. Is predicate plural ? A binary feature indicating
whether the predicate is singular or plural as they tend to
have different argument selection properties.
11. Genitives in constituent ? This is a binary feature
which is true if there is a genitive word (one with the part
of speech POS, PRP, PRP$ or WP$) in the constituent,
as these tend to be markers for nominal arguments as in
[Speaker Burma ?s] [Phenomenon oil] [Predicate search] hits
virgin forests
12. Constituent parent features ? Same as the sibling
features, except that that these are extracted from the
constituent?s parent.
13. Verb dominating predicate ? The head word of the
first VP ancestor of the predicate.
14. Named Entities in Constituent ? As in Surdeanu
et al (2003), this is represented as seven binary fea-
tures extracted after tagging the sentence with BBN?s
IdentiFinder (Bikel et al, 1999) named entity tagger.
5 Feature Analysis and Best System
Performance
5.1 English For the task of argument identification,
features 2, 3, 4, 5 (the verb itself, path to light-verb and
presence of a light verb), 6, 7, 9, 10 an 13 contributed pos-
itively to the performance. The Frame feature degrades
performance significantly. This could be just an artifact
of the data sparsity. We trained a new classifier using all
the features that contributed positively to the performance
and the F?=1 score increased from the baseline of 72.8%
to 76.3% (?2; p < 0.05).
For the task of argument classification, adding the
Frame feature to the baseline features, provided the most
significant improvement, increasing the classification
accuracy from 70.9% to 79.0% (?2; p < 0.05). All
other features added one-by-one to the baseline did
not bring any significant improvement to the baseline,
which might again be owing to the comparatively small
training and test data sizes. All the features together
produced a classification accuracy of 80.9%. Since the
Frame feature is an oracle, we were interested in finding
out what all the other features combined contributed.
We ran an experiment with all features, except Frame,
added to the baseline, and this produced an accuracy of
73.1%, which however, is not a statistically significant
improvement over the baseline of 70.9%.
For the task of argument identification and classifi-
cation, features 8 and 11 (right sibling head word part
of speech) hurt performance. We trained a classifier
using all the features that contributed positively to the
performance and the resulting system had an improved
F?=1 score of 56.5% compared to the baseline of 51.4%
(?2; p < 0.05).
We found that a significant subset of features that con-
tribute marginally to the classification performance, hurt
the identification task. Therefore, we decided to perform
a two-step process in which we use the set of features that
gave optimum performance for the argument identifica-
tion task and identify all likely argument nodes. Then, for
those nodes, we use all the available features and classify
them into one of the possible classes. This ?two-pass?
system performs slightly better than the ?one-pass? men-
tioned earlier. Again, we performed the second pass of
classification with and without the Frame feature.
Table 2 shows the improved performance numbers.
Task P R F?=1 A
(%) (%) (%)
Id. 83.8 70.0 76.3
Classification (w/o Frame) - - - 73.1
Classification (with Frame) - - - 80.9
Id. + Classification 69.4 47.6 56.5
(one-pass, w/o Frame)
Id. + Classification 62.2 53.1 57.3
(two-pass, w/o Frame)
Id. + Classification 69.4 59.2 63.9
(two-pass, with Frame)
Table 2: Best performance on all three tasks.
5.2 Chinese For the Chinese task, we use the one-pass
algorithm as used for English. A baseline system was
created using the same features as used for English (Sec-
tion 3). We evaluate this system on just the combined task
of argument identification and classification. The base-
line performance is shown in Table 3.
To improve the system?s performance over the base-
line, we added all the features discussed in Section 4, ex-
cept features Frame ? as the data was labeled in a Prop-
Bank fashion, there are no frames involved as in Frame-
Net; Plurals and Genitives ? as they are not realized the
same way morphologically in Chinese, and Named En-
tities ? owing to the unavailability of a Chinese Named
Entity tagger. We found that of these features, 2, 3, 4, 6, 7
and 13 hurt the performance when added to the baseline,
but the other features helped to some degree, although
not significantly. The improved performance is shown in
Table 3
Features P R F?=1
(%) (%)
Baseline 86.2 32.2 46.9
Baseline 83.9 44.1 57.8
+ more features
Table 3: Parsing performance for Chinese on the com-
bined task of identifying and classifying semantic argu-
ments.
An interesting linguistic phenomenon was observed
which explains part of the reason why recall for Chinese
argument parsing is so low. In Chinese, arguments
which are internal to the NP which encompasses the
nominalized predicate, tend to be multi-word, and are
not associated with any node in the parse tree. These
violates our basic assumption of the arguments aligning
with parse tree constituents, and are guaranteed to be
missed. In the case of English however, these tend to be
single word arguments which are represented by a leaf
in the parse tree and stand a chance of getting classified
correctly.
6 Conclusion
In this paper we investigated the task of identifying and
classifying arguments of eventive nominalizations in
FrameNet. The best system generates an F1 score of
57.3% on the combined task of argument identification
and classification using automatically extracted features
on a test set of about 700 sentences using a classifier
trained on about 6,000 sentences.
As noted earlier, the bulk of past research in this area
has focused on verbal predicates. Two notable exceptions
to this include the work of (Hull and Gomez, 1996) ? a
rule based system for identifying the semantic arguments
of nominal predicates, and the work of (Lapata, 2002)
on interpreting the relation between the head of a nom-
inalized compound and its modifier noun. Unfortunately,
meaningful comparisons to these efforts are difficult due
to differing evaluation metrics.
We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use BBN?s named entity tagger ? Iden-
tiFinder; Ashley Thornton for identifying the sentences from
FrameNet with predicates that are eventive nominalizations.
References
[Baker et al1998] Collin F. Baker, Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley FrameNet project. In
COLING/ACL-98, pages 86?90, Montreal.
[Barker and Dowty1992] Chris Barker and David Dowty. 1992.
Non-verbal thematic proto-roles. In NELS-23, Amy Schafer,
ed., GSLA, Amherst, pages 49?62.
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34:211?231.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head pars-
ing for language models. In ACL, Toulouse, France.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky.
2000. Automatic labeling of semantic roles. In ACL, pages
512?520, Hong Kong, October.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky.
2002. Automatic labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer.
2002. The necessity of syntactic parsing for predicate ar-
gument recognition. In ACL, PA.
[Grimshaw1990] Jane Grimshaw. 1990. Argument Structure.
The MIT Press, US.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward.
2003. Target word detection and semantic role chunking us-
ing support vector machines. In HLT, Edmonton, Canada.
[Hull and Gomez1996] Richard D. Hull and Fernando Gomez.
1996. Semantic interpretation of nominalizations. In AAAI
Conference, Oregon, pages 1062?1068.
[Kingsbury and Palmer2002] Paul Kingsbury and Martha Pal-
mer. 2002. From Treebank to PropBank. In LREC-2002,
Las Palmas, Canary Islands, Spain.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto.
2000. Use of support vector learning for chunk identifica-
tion. In CoNLL-2000, pages 142?144.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto.
2001. Chunking with support vector machines. In NAACL.
[Lapata2002] Maria Lapata. 2002. The disambiguation of nom-
inalizations. Computational Linguistics, 28(3):357?388.
[Macleod et al1998] C. Macleod, R. Grishman, A. Meyers,
L. Barrett, and R. Reeves. 1998. Nomlex: A lexicon of
nominalizations.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne
Ward, James Martin, and Dan Jurafsky. 2003. Semantic role
parsing: Adding semantic structure to unstructured text. In
ICDM, Melbourne, Florida.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John
Williams, and Paul Aarseth. 2003. Using predicate-
argument structures for information extraction. In ACL, Sap-
poro, Japan.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and
Christopher D. Manning. 2003. A generative model for se-
mantic role labeling. In ECML.
Proceedings of the 43rd Annual Meeting of the ACL, pages 581?588,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling Using Different Syntactic Views?
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado,
Boulder, CO 80303
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
Daniel Jurafsky
Department of Linguistics,
Stanford University,
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
Semantic role labeling is the process of
annotating the predicate-argument struc-
ture in text with semantic labels. In this
paper we present a state-of-the-art base-
line semantic role labeling system based
on Support Vector Machine classifiers.
We show improvements on this system
by: i) adding new features including fea-
tures extracted from dependency parses,
ii) performing feature selection and cali-
bration and iii) combining parses obtained
from semantic parsers trained using dif-
ferent syntactic views. Error analysis of
the baseline system showed that approx-
imately half of the argument identifica-
tion errors resulted from parse errors in
which there was no syntactic constituent
that aligned with the correct argument. In
order to address this problem, we com-
bined semantic parses from a Minipar syn-
tactic parse and from a chunked syntac-
tic representation with our original base-
line system which was based on Charniak
parses. All of the reported techniques re-
sulted in performance improvements.
1 Introduction
Semantic Role Labeling is the process of annotat-
ing the predicate-argument structure in text with se-
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grants IS-9978025 and ITR/HCI 0086132
mantic labels (Gildea and Jurafsky, 2000; Gildea
and Jurafsky, 2002; Gildea and Palmer, 2002; Sur-
deanu et al, 2003; Hacioglu and Ward, 2003; Chen
and Rambow, 2003; Gildea and Hockenmaier, 2003;
Pradhan et al, 2004; Hacioglu, 2004). The architec-
ture underlying all of these systems introduces two
distinct sub-problems: the identification of syntactic
constituents that are semantic roles for a given pred-
icate, and the labeling of the those constituents with
the correct semantic role.
A detailed error analysis of our baseline system
indicates that the identification problem poses a sig-
nificant bottleneck to improving overall system per-
formance. The baseline system?s accuracy on the
task of labeling nodes known to represent semantic
arguments is 90%. On the other hand, the system?s
performance on the identification task is quite a bit
lower, achieving only 80% recall with 86% preci-
sion. There are two sources of these identification
errors: i) failures by the system to identify all and
only those constituents that correspond to semantic
roles, when those constituents are present in the syn-
tactic analysis, and ii) failures by the syntactic ana-
lyzer to provide the constituents that align with cor-
rect arguments. The work we present here is tailored
to address these two sources of error in the identifi-
cation problem.
The remainder of this paper is organized as fol-
lows. We first describe a baseline system based on
the best published techniques. We then report on
two sets of experiments using techniques that im-
prove performance on the problem of finding argu-
ments when they are present in the syntactic analy-
sis. In the first set of experiments we explore new
581
features, including features extracted from a parser
that provides a different syntactic view ? a Combi-
natory Categorial Grammar (CCG) parser (Hocken-
maier and Steedman, 2002). In the second set of
experiments, we explore approaches to identify opti-
mal subsets of features for each argument class, and
to calibrate the classifier probabilities.
We then report on experiments that address the
problem of arguments missing from a given syn-
tactic analysis. We investigate ways to combine
hypotheses generated from semantic role taggers
trained using different syntactic views ? one trained
using the Charniak parser (Charniak, 2000), another
on a rule-based dependency parser ? Minipar (Lin,
1998), and a third based on a flat, shallow syntactic
chunk representation (Hacioglu, 2004a). We show
that these three views complement each other to im-
prove performance.
2 Baseline System
For our experiments, we use Feb 2004 release of
PropBank1 (Kingsbury and Palmer, 2002; Palmer
et al, 2005), a corpus in which predicate argument
relations are marked for verbs in the Wall Street
Journal (WSJ) part of the Penn TreeBank (Marcus
et al, 1994). PropBank was constructed by as-
signing semantic arguments to constituents of hand-
corrected TreeBank parses. Arguments of a verb
are labeled ARG0 to ARG5, where ARG0 is the
PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc.
In addition to these CORE ARGUMENTS, additional
ADJUNCTIVE ARGUMENTS, referred to as ARGMs
are also marked. Some examples are ARGM-LOC,
for locatives; ARGM-TMP, for temporals; ARGM-
MNR, for manner, etc. Figure 1 shows a syntax tree
along with the argument labels for an example ex-
tracted from PropBank. We use Sections 02-21 for
training, Section 00 for development and Section 23
for testing.
We formulate the semantic labeling problem as
a multi-class classification problem using Support
Vector Machine (SVM) classifier (Hacioglu et al,
2003; Pradhan et al, 2003; Pradhan et al, 2004)
TinySVM2 along with YamCha3 (Kudo and Mat-
1http://www.cis.upenn.edu/?ace/
2http://chasen.org/?taku/software/TinySVM/
3http://chasen.org/?taku/software/yamcha/
S
hhhh
((((
NP
hhhh
((((
The acquisition
ARG1
VP
```
   
VBD
was
NULL
VP
XXX
VBN
completed
predicate
PP
```
   
in September
ARGM?TMP
[ARG1 The acquisition] was [predicate completed] [ARGM?TMP in September].
Figure 1: Syntax tree for a sentence illustrating the
PropBank tags.
sumoto, 2000; Kudo and Matsumoto, 2001) are used
to implement the system. Using what is known as
the ONE VS ALL classification strategy, n binary
classifiers are trained, where n is number of seman-
tic classes including a NULL class.
The baseline feature set is a combination of fea-
tures introduced by Gildea and Jurafsky (2002) and
ones proposed in Pradhan et al, (2004), Surdeanu et
al., (2003) and the syntactic-frame feature proposed
in (Xue and Palmer, 2004). Table 1 lists the features
used.
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
VOICE
PREDICATE SUB-CATEGORIZATION
PREDICATE CLUSTER
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the head word
NAMED ENTITIES IN CONSTITUENTS: 7 named entities as 7 binary features.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
VERB SENSE INFORMATION: Oracle verb sense information from PropBank
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
TEMPORAL CUE WORDS
DYNAMIC CLASS CONTEXT
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
Table 1: Features used in the Baseline system
As described in (Pradhan et al, 2004), we post-
process the n-best hypotheses using a trigram lan-
guage model of the argument sequence.
We analyze the performance on three tasks:
? Argument Identification ? This is the pro-
cess of identifying the parsed constituents in
the sentence that represent semantic arguments
of a given predicate.
582
? Argument Classification ? Given constituents
known to represent arguments of a predicate,
assign the appropriate argument labels to them.
? Argument Identification and Classification ?
A combination of the above two tasks.
ALL ARGs Task P R F1 A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Id. 86.8 80.0 83.3
Classification - - - 90.1
Id. + Classification 80.9 76.8 78.8
Table 2: Baseline system performance on all tasks
using hand-corrected parses and automatic parses on
PropBank data.
Table 2 shows the performance of the system us-
ing the hand corrected, TreeBank parses (HAND)
and using parses produced by a Charniak parser
(AUTOMATIC). Precision (P), Recall (R) and F1
scores are given for the identification and combined
tasks, and Classification Accuracy (A) for the clas-
sification task.
Classification performance using Charniak parses
is about 3% absolute worse than when using Tree-
Bank parses. On the other hand, argument identifi-
cation performance using Charniak parses is about
12.7% absolute worse. Half of these errors ? about
7% are due to missing constituents, and the other
half ? about 6% are due to mis-classifications.
Motivated by this severe degradation in argument
identification performance for automatic parses, we
examined a number of techniques for improving
argument identification. We made a number of
changes to the system which resulted in improved
performance. The changes fell into three categories:
i) new features, ii) feature selection and calibration,
and iii) combining parses from different syntactic
representations.
3 Additional Features
3.1 CCG Parse Features
While the Path feature has been identified to be very
important for the argument identification task, it is
one of the most sparse features and may be diffi-
cult to train or generalize (Pradhan et al, 2004; Xue
and Palmer, 2004). A dependency grammar should
generate shorter paths from the predicate to depen-
dent words in the sentence, and could be a more
robust complement to the phrase structure grammar
paths extracted from the Charniak parse tree. Gildea
and Hockenmaier (2003) report that using features
extracted from a Combinatory Categorial Grammar
(CCG) representation improves semantic labeling
performance on core arguments. We evaluated fea-
tures from a CCG parser combined with our baseline
feature set. We used three features that were intro-
duced by Gildea and Hockenmaier (2003):
? Phrase type ? This is the category of the max-
imal projection between the two words ? the
predicate and the dependent word.
? Categorial Path ? This is a feature formed by
concatenating the following three values: i) cat-
egory to which the dependent word belongs, ii)
the direction of dependence and iii) the slot in
the category filled by the dependent word.
? Tree Path ? This is the categorial analogue of
the path feature in the Charniak parse based
system, which traces the path from the depen-
dent word to the predicate through the binary
CCG tree.
Parallel to the hand-corrected TreeBank parses,
we also had access to correct CCG parses derived
from the TreeBank (Hockenmaier and Steedman,
2002a). We performed two sets of experiments.
One using the correct CCG parses, and the other us-
ing parses obtained using StatCCG4 parser (Hocken-
maier and Steedman, 2002). We incorporated these
features in the systems based on hand-corrected
TreeBank parses and Charniak parses respectively.
For each constituent in the Charniak parse tree, if
there was a dependency between the head word of
the constituent and the predicate, then the corre-
sponding CCG features for those words were added
to the features for that constituent. Table 3 shows the
performance of the system when these features were
added. The corresponding baseline performances
are mentioned in parentheses.
3.2 Other Features
We added several other features to the system. Po-
sition of the clause node (S, SBAR) seems to be
4Many thanks to Julia Hockenmaier for providing us with
the CCG bank as well as the StatCCG parser.
583
ALL ARGs Task P R F1
(%) (%)
HAND Id. 97.5 (96.2) 96.1 (95.8) 96.8 (96.0)
Id. + Class. 91.8 (89.9) 90.5 (89.0) 91.2 (89.4)
AUTOMATIC Id. 87.1 (86.8) 80.7 (80.0) 83.8 (83.3)
Id. + Class. 81.5 (80.9) 77.2 (76.8) 79.3 (78.8)
Table 3: Performance improvement upon adding
CCG features to the Baseline system.
an important feature in argument identification (Ha-
cioglu et al, 2004) therefore we experimented with
four clause-based path feature variations. We added
the predicate context to capture predicate sense vari-
ations. For some adjunctive arguments, punctuation
plays an important role, so we added some punctu-
ation features. All the new features are shown in
Table 4
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an ?*?.
For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP?S?S?VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:
NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
Table 4: Other Features
4 Feature Selection and Calibration
In the baseline system, we used the same set of fea-
tures for all the n binary ONE VS ALL classifiers.
Error analysis showed that some features specifi-
cally suited for one argument class, for example,
core arguments, tend to hurt performance on some
adjunctive arguments. Therefore, we thought that
selecting subsets of features for each argument class
might improve performance. To achieve this, we
performed a simple feature selection procedure. For
each argument, we started with the set of features in-
troduced by (Gildea and Jurafsky, 2002). We pruned
this set by training classifiers after leaving out one
feature at a time and checking its performance on
a development set. We used the ?2 significance
while making pruning decisions. Following that, we
added each of the other features one at a time to the
pruned baseline set of features and selected ones that
showed significantly improved performance. Since
the feature selection experiments were computation-
ally intensive, we performed them using 10k training
examples.
SVMs output distances not probabilities. These
distances may not be comparable across classifiers,
especially if different features are used to train each
binary classifier. In the baseline system, we used the
algorithm described by Platt (Platt, 2000) to convert
the SVM scores into probabilities by fitting to a sig-
moid. When all classifiers used the same set of fea-
tures, fitting all scores to a single sigmoid was found
to give the best performance. Since different fea-
ture sets are now used by the classifiers, we trained
a separate sigmoid for each classifier.
Raw Scores Probabilities
After lattice-rescoring
Uncalibrated Calibrated
(%) (%) (%)
Same Feat. same sigmoid 74.7 74.7 75.4
Selected Feat. diff. sigmoids 75.4 75.1 76.2
Table 5: Performance improvement on selecting fea-
tures per argument and calibrating the probabilities
on 10k training data.
Foster and Stine (2004) show that the pool-
adjacent-violators (PAV) algorithm (Barlow et al,
1972) provides a better method for converting raw
classifier scores to probabilities when Platt?s algo-
rithm fails. The probabilities resulting from either
conversions may not be properly calibrated. So, we
binned the probabilities and trained a warping func-
tion to calibrate them. For each argument classifier,
we used both the methods for converting raw SVM
scores into probabilities and calibrated them using
a development set. Then, we visually inspected
the calibrated plots for each classifier and chose the
method that showed better calibration as the calibra-
tion procedure for that classifier. Plots of the pre-
dicted probabilities versus true probabilities for the
ARGM-TMP VS ALL classifier, before and after cal-
ibration are shown in Figure 2. The performance im-
provement over a classifier that is trained using all
the features for all the classes is shown in Table 5.
Table 6 shows the performance of the system af-
ter adding the CCG features, additional features ex-
584
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Predicted Probability
Tr
ue
 P
ro
ba
bi
lity
Before Calibration
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Predicted Probability
Tr
ue
 P
ro
ba
bi
lity
After Calibration
Figure 2: Plots showing true probabilities versus predicted probabilities before and after calibration on the
test set for ARGM-TMP.
tracted from the Charniak parse tree, and performing
feature selection and calibration. Numbers in paren-
theses are the corresponding baseline performances.
TASK P R F1 A
(%) (%) (%)
Id. 86.9 (86.8) 84.2 (80.0) 85.5 (83.3)
Class. - - - 92.0 (90.1)
Id. + Class. 82.1 (80.9) 77.9 (76.8) 79.9 (78.8)
Table 6: Best system performance on all tasks using
automatically generated syntactic parses.
5 Alternative Syntactic Views
Adding new features can improve performance
when the syntactic representation being used for
classification contains the correct constituents. Ad-
ditional features can?t recover from the situation
where the parse tree being used for classification
doesn?t contain the correct constituent representing
an argument. Such parse errors account for about
7% absolute of the errors (or, about half of 12.7%)
for the Charniak parse based system. To address
these errors, we added two additional parse repre-
sentations: i) Minipar dependency parser, and ii)
chunking parser (Hacioglu et al, 2004). The hope is
that these parsers will produce different errors than
the Charniak parser since they represent different
syntactic views. The Charniak parser is trained on
the Penn TreeBank corpus. Minipar is a rule based
dependency parser. The chunking parser is trained
on PropBank and produces a flat syntactic represen-
tation that is very different from the full parse tree
produced by Charniak. A combination of the three
different parses could produce better results than any
single one.
5.1 Minipar-based Semantic Labeler
Minipar (Lin, 1998; Lin and Pantel, 2001) is a rule-
based dependency parser. It outputs dependencies
between a word called head and another called mod-
ifier. Each word can modify at most one word. The
dependency relationships form a dependency tree.
The set of words under each node in Minipar?s
dependency tree form a contiguous segment in the
original sentence and correspond to the constituent
in a constituent tree. We formulate the semantic la-
beling problem in the same way as in a constituent
structure parse, except we classify the nodes that
represent head words of constituents. A similar for-
mulation using dependency trees derived from Tree-
Bank was reported in Hacioglu (Hacioglu, 2004).
In that experiment, the dependency trees were de-
rived from hand-corrected TreeBank trees using
head word rules. Here, an SVM is trained to as-
sign PropBank argument labels to nodes in Minipar
dependency trees using the following features:
Table 8 shows the performance of the Minipar-
based semantic parser.
Minipar performance on the PropBank corpus is
substantially worse than the Charniak based system.
This is understandable from the fact that Minipar
is not designed to produce constituents that would
exactly match the constituent segmentation used in
TreeBank. In the test set, about 37% of the argu-
585
PREDICATE LEMMA
HEAD WORD: The word representing the node in the dependency tree.
HEAD WORD POS: Part of speech of the head word.
POS PATH: This is the path from the predicate to the head word through
the dependency tree connecting the part of speech of each node in the tree.
DEPENDENCY PATH: Each word that is connected to the head
word has a particular dependency relationship to the word. These
are represented as labels on the arc between the words. This
feature is the dependencies along the path that connects two words.
VOICE
POSITION
Table 7: Features used in the Baseline system using
Minipar parses.
Task P R F1
(%) (%)
Id. 73.5 43.8 54.6
Id. + Classification 66.2 36.7 47.2
Table 8: Baseline system performance on all tasks
using Minipar parses.
ments do not have corresponding constituents that
match its boundaries. In experiments reported by
Hacioglu (Hacioglu, 2004), a mismatch of about
8% was introduced in the transformation from hand-
corrected constituent trees to dependency trees. Us-
ing an errorful automatically generated tree, a still
higher mismatch would be expected. In case of
the CCG parses, as reported by Gildea and Hock-
enmaier (2003), the mismatch was about 23%. A
more realistic way to score the performance is to
score tags assigned to head words of constituents,
rather than considering the exact boundaries of the
constituents as reported by Gildea and Hocken-
maier (2003). The results for this system are shown
in Table 9.
Task P R F1
(%) (%)
CHARNIAK Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
MINIPAR Id. 83.3 61.1 70.5
Id. + Classification 72.9 53.5 61.7
Table 9: Head-word based performance using Char-
niak and Minipar parses.
5.2 Chunk-based Semantic Labeler
Hacioglu has previously described a chunk based se-
mantic labeling method (Hacioglu et al, 2004). This
system uses SVM classifiers to first chunk input text
into flat chunks or base phrases, each labeled with
a syntactic tag. A second SVM is trained to assign
semantic labels to the chunks. The system is trained
on the PropBank training data.
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ?before?, ?after? and ?-? (for
the predicate)
PATH: It defines a flat path between the token and the predicate
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
Table 10: Features used by chunk based classifier.
Table 10 lists the features used by this classifier.
For each token (base phrase) to be tagged, a set of
features is created from a fixed size context that sur-
rounds each token. In addition to the above features,
it also uses previous semantic tags that have already
been assigned to the tokens contained in the linguis-
tic context. A 5-token sliding window is used for the
context.
P R F1
(%) (%)
Id. and Classification 72.6 66.9 69.6
Table 11: Semantic chunker performance on the
combined task of Id. and classification.
SVMs were trained for begin (B) and inside (I)
classes of all arguments and outside (O) class for a
total of 78 one-vs-all classifiers. Again, TinySVM5
along with YamCha6 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used as the SVM
training and test software.
Table 11 presents the system performances on the
PropBank test set for the chunk-based system.
5http://chasen.org/?taku/software/TinySVM/
6http://chasen.org/?taku/software/yamcha/
586
6 Combining Semantic Labelers
We combined the semantic parses as follows: i)
scores for arguments were converted to calibrated
probabilities, and arguments with scores below a
threshold value were deleted. Separate thresholds
were used for each parser. ii) For the remaining ar-
guments, the more probable ones among overlap-
ping ones were selected. In the chunked system,
an argument could consist of a sequence of chunks.
The probability assigned to the begin tag of an ar-
gument was used as the probability of the sequence
of chunks forming an argument. Table 12 shows
the performance improvement after the combina-
tion. Again, numbers in parentheses are respective
baseline performances.
TASK P R F1
(%) (%)
Id. 85.9 (86.8) 88.3 (80.0) 87.1 (83.3)
Id. + Class. 81.3 (80.9) 80.7 (76.8) 81.0 (78.8)
Table 12: Constituent-based best system perfor-
mance on argument identification and argument
identification and classification tasks after combin-
ing all three semantic parses.
The main contribution of combining both the
Minipar based and the Charniak-based parsers was
significantly improved performance on ARG1 in ad-
dition to slight improvements to some other argu-
ments. Table 13 shows the effect on selected argu-
ments on sentences that were altered during the the
combination of Charniak-based and Chunk-based
parses.
Number of Propositions 107
Percentage of perfect props before combination 0.00
Percentage of perfect props after combination 45.95
Before After
P R F1 P R F1
(%) (%) (%) (%)
Overall 94.8 53.4 68.3 80.9 73.8 77.2
ARG0 96.0 85.7 90.5 92.5 89.2 90.9
ARG1 71.4 13.5 22.7 59.4 59.4 59.4
ARG2 100.0 20.0 33.3 50.0 20.0 28.5
ARGM-DIS 100.0 40.0 57.1 100.0 100.0 100.0
Table 13: Performance improvement on parses
changed during pair-wise Charniak and Chunk com-
bination.
A marked increase in number of propositions for
which all the arguments were identified correctly
from 0% to about 46% can be seen. Relatively few
predicates, 107 out of 4500, were affected by this
combination.
To give an idea of what the potential improve-
ments of the combinations could be, we performed
an oracle experiment for a combined system that
tags head words instead of exact constituents as we
did in case of Minipar-based and Charniak-based se-
mantic parser earlier. In case of chunks, first word in
prepositional base phrases was selected as the head
word, and for all other chunks, the last word was se-
lected to be the head word. If the correct argument
was found present in either the Charniak, Minipar or
Chunk hypotheses then that was selected. The re-
sults for this are shown in Table 14. It can be seen
that the head word based performance almost ap-
proaches the constituent based performance reported
on the hand-corrected parses in Table 3 and there
seems to be considerable scope for improvement.
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 98.4 90.6 94.3
Id. + Classification 93.1 86.0 89.4
C+CH Id. 98.9 88.8 93.6
Id. + Classification 92.5 83.3 87.7
C+M+CH Id. 99.2 92.5 95.7
Id. + Classification 94.6 88.4 91.5
Table 14: Performance improvement on head word
based scoring after oracle combination. Charniak
(C), Minipar (M) and Chunker (CH).
Table 15 shows the performance improvement in
the actual system for pairwise combination of the
parsers and one using all three.
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 91.7 89.9 90.8
Id. + Classification 85.0 83.9 84.5
C+CH Id. 91.5 91.1 91.3
Id. + Classification 84.9 84.3 84.7
C+M+CH Id. 91.5 91.9 91.7
Id. + Classification 85.1 85.5 85.2
Table 15: Performance improvement on head word
based scoring after combination. Charniak (C),
Minipar (M) and Chunker (CH).
587
7 Conclusions
We described a state-of-the-art baseline semantic
role labeling system based on Support Vector Ma-
chine classifiers. Experiments were conducted to
evaluate three types of improvements to the sys-
tem: i) adding new features including features ex-
tracted from a Combinatory Categorial Grammar
parse, ii) performing feature selection and calibra-
tion and iii) combining parses obtained from seman-
tic parsers trained using different syntactic views.
We combined semantic parses from a Minipar syn-
tactic parse and from a chunked syntactic repre-
sentation with our original baseline system which
was based on Charniak parses. The belief was that
semantic parses based on different syntactic views
would make different errors and that the combina-
tion would be complimentary. A simple combina-
tion of these representations did lead to improved
performance.
8 Acknowledgements
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
We would like to thank Ralph Weischedel and
Scott Miller of BBN Inc. for letting us use their
named entity tagger ? IdentiFinder; Martha Palmer
for providing us with the PropBank data; Dan Gildea
and Julia Hockenmaier for providing the gold stan-
dard CCG parser information, and all the anony-
mous reviewers for their helpful comments.
References
R. E. Barlow, D. J. Bartholomew, J. M. Bremmer, and H. D. Brunk. 1972. Statis-
tical Inference under Order Restrictions. Wiley, New York.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of
NAACL, pages 132?139, Seattle, Washington.
John Chen and Owen Rambow. 2003. Use of deep linguistics features for
the recognition and labeling of semantic arguments. In Proceedings of the
EMNLP, Sapporo, Japan.
Dean P. Foster and Robert A. Stine. 2004. Variable selection in data mining:
building a predictive model for bankruptcy. Journal of American Statistical
Association, 99, pages 303?313.
Dan Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using com-
binatory categorial grammar. In Proceedings of the EMNLP, Sapporo, Japan.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic labeling of semantic roles.
In Proceedings of ACL, pages 512?520, Hong Kong, October.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity of syntactic parsing for
predicate argument recognition. In Proceedings of ACL, Philadelphia, PA.
Kadri Hacioglu. 2004. Semantic role labeling using dependency trees. In Pro-
ceedings of COLING, Geneva, Switzerland.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic role
chunking using support vector machines. In Proceedings of HLT/NAACL,
Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Juraf-
sky. 2004. Semantic role labeling by tagging syntactic chunks. In Proceed-
ings of CoNLL-2004, Shared Task ? Semantic Role Labeling.
Kadri Hacioglu. 2004a. A lightweight semantic chunking model based on tag-
ging. In Proceedings of HLT/NAACL, Boston, MA.
Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical
parsing with combinatory grammars. In Proceedings of the ACL, pages 335?
342.
Julia Hockenmaier and Mark Steedman. 2002a. Acquiring compact lexicalized
grammars from a cleaner treebank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation (LREC-2002), Las Pal-
mas, Canary Islands, Spain.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings of LREC, Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of CoNLL and LLL, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question
answering. Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In In Workshop
on the Evaluation of Parsing Systems, Granada, Spain.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann
Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument structure.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles. To appear Computational Linguistics.
John Platt. 2000. Probabilities for support vector machines. In A. Smola,
P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large
Margin Classifiers. MIT press, Cambridge, MA.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of ICDM, Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings of HLT/NAACL, Boston, MA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
of ACL, Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings of EMNLP, Barcelona, Spain.
588
Semantic Role Labeling by Tagging Syntactic Chunks?
Kadri Hacioglu1, Sameer Pradhan1, Wayne Ward1, James H. Martin1, Daniel Jurafsky2
1University of Colorado at Boulder, 2Stanford University
{hacioglu,spradhan,whw}@cslr.colorado.edu, martin@cs.colorado.edu, jurafsky@stanford.edu
Abstract
In this paper, we present a semantic role la-
beler (or chunker) that groups syntactic chunks
(i.e. base phrases) into the arguments of a pred-
icate. This is accomplished by casting the se-
mantic labeling as the classification of syntactic
chunks (e.g. NP-chunk, PP-chunk) into one of
several classes such as the beginning of an ar-
gument (B-ARG), inside an argument (I-ARG)
and outside an argument (O). This amounts to
tagging syntactic chunks with semantic labels
using the IOB representation. The chunker is
realized using support vector machines as one-
versus-all classifiers. We describe the represen-
tation of data and information used to accom-
plish the task. We participate in the ?closed
challenge? of the CoNLL-2004 shared task and
report results on both development and test
sets.
1 Introduction
In semantic role labeling the goal is to group sequences
of words together and classify them by using semantic la-
bels. For meaning representation the predicate-argument
structure that exists in most languages is used. In this
structure a word (most frequently a verb) is specified as
a predicate, and a number of word groups are considered
as arguments accompanying the word (or predicate).
In this paper, we select support vector machines
(SVMs) (Vapnik, 1995; Burges, 1998) to implement
the semantic role classifiers, due to their ability to han-
dle an extremely large number of (overlapping) features
with quite strong generalization properties. Support vec-
tor machines for semantic role chunking were first used
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IIS-9978025
in (Hacioglu and Ward, 2003) as word-by-word (W-by-
W) classifiers. The system was then applied to the
constituent-by-constituent (C-by-C) classification in (Ha-
cioglu et al, 2003). In (Pradhan et al, 2003; Prad-
han et al, 2004), several extensions to the basic system
have been proposed, extensively studied and systemati-
cally compared to other systems. In this paper, we imple-
ment a system that classifies syntactic chunks (i.e. base
phrases) instead of words or the constituents derived from
syntactic trees. This system is referred to as the phrase-
by-phrase (P-by-P) semantic role classifier. We partici-
pate in the ?closed challenge? of the CoNLL-2004 shared
task and report results on both development and test sets.
A detailed description of the task, data and related work
can be found in (Carreras and Ma`rquez, 2004).
2 System Description
2.1 Data Representation
In this paper, we change the representation of the original
data as follows:
? Bracketed representation of roles is converted into
IOB2 representation (Ramhsaw and Marcus, 1995;
Sang and Veenstra, 1995)
? Word tokens are collapsed into base phrase (BP) to-
kens.
Since the semantic annotation in the PropBank corpus
does not have any embedded structure there is no loss of
information in the first change. However, this results in
a simpler representation with a reduced set of tagging la-
bels. In the second change, it is possible to miss some
information in cases where the semantic chunks do not
align with the sequence of BPs. However, in Section 3.2
we show that the loss in performance due to the misalign-
ment is much less than the gain in performance that can
be achieved by the change in representation.
from
million
251.2
$
to
declined VBD
CD
NN
IN
CD
$
TO
Sales
%
CD
10
$
278.7
$
CD
NNS
*A2)
*
*
*A4)
*
*
million CD I?NP
O
*
*
B?NP
B?VP
B?NP
I?NP
B?PP
B?NP
I?NP
I?NP
B?PP
B?NP
I?NP
*
(S*
*
*
*
*
*
*
*
*
*
decline
?
?
?
?
?
?
?
?
?
?
?
?
*A3)
(A3*
(A4*
(A2*
(V*V)
(A1*A1)
*S)
O
NP
VP
NP
PP
NP
PP
NP
Sales
declined
NNS
VBD
% NN
TOto
million
from
million
B?NP
CD
CD
O
I?NP
B?PP
I?NP
B?PP
I?NP
?
?
?
?
?
?
?
*
*
*
*
*
* B?V
B?A2
B?A1
B?A3
B?A4
O
O
O
B?VP
IN
*S)
(S*
(b)(a)
. .
. .
decline
Figure 1: Illustration of change in data representation; (a) original word-by-word data representation (b) phrase-by-
phrase data representation used in this paper. Words are collapsed into base phrase types retaining only headwords
with their respective features. Bracketed representation of semantic role labels is converted into IOB2 representation.
See text for details.
The new representation is illustrated in Figure 1 along
with the original representation. Comparing both we note
the following differences and advantages in the new rep-
resentation:
? BPs are being classified instead of words.
? Only the BP headwords (rightmost words) are re-
tained as word information.
? The number of tagging steps is smaller.
? A fixed context spans a larger segment of a sentence.
Therefore, the P-by-P semantic role chunker classifies
larger units, ignores some of the words, uses a relatively
larger context for a given window size and performs the
labeling faster.
2.2 Features
The following features, which we refer to as the base fea-
tures, are provided in the shared task data for each sen-
tence;
? Words
? Predicate lemmas
? Part of Speech tags
? BP Positions: The position of a token in a BP using
the IOB2 representation (e.g. B-NP, I-NP, O etc.)
? Clause tags: The tags that mark token positions in a
sentence with respect to clauses. (e.g *S)*S) marks
a position that two clauses end)
? Named entities: The IOB tags of named entities.
There are four categories; LOC, ORG, PERSON
and MISC.
Using available information we have created the fol-
lowing token level features:
? Token Position: The position of the phrase with re-
spect to the predicate. It has three values as ?be-
fore?, ?after? and ?-? for the predicate.
? Path: It defines a flat path between the token and
the predicate as a chain of base phrases. At both
ends, the chain is terminated with the POS tags of
the predicate and the headword of the token.
? Clause bracket patterns: We use two patterns of
clauses for each token. One is the clause bracket
chain between the token and the predicate, and the
other is from the token to sentence begin or end de-
pending on token?s position with respect to the pred-
icate.
? Clause Position: a binary feature that indicates the
token is inside or outside of the clause which con-
tains the predicate
? Headword suffixes: suffixes of headwords of length
2, 3 and 4.
? Distance: we have two notions of distance; the first
is the distance of the token from the predicate as a
number of base phrases, and the second is the same
distance as the number of VP chunks.
? Length: the number of words in a token.
We also use some sentence level features:
? Predicate POS tag: the part of speech category of
the predicate
? Predicate Frequency; this is a feature which indi-
cates whether the predicate is frequent or rare with
respect to the training set. The threshold on the
counts is currently set to 3.
? Predicate BP Context : The chain of BPs centered
at the predicate within a window of size -2/+2.
? Predicate POS Context : The POS tags of the
words that immediately precede and follow the pred-
icate. The POS tag of a preposition is replaced with
the preposition itself.
? Predicate Argument Frames: We used the left and
right patterns of the core arguments (A0 through A5)
for each predicate . We used the three most frequent
argument frames for both sides depending on the po-
sition of the token in focus with respect to the pred-
icate. (e.g. raise has A0 and A1 AO (A0 being the
most frequent) as its left argument frames, and A1,
A1 A2 and A2 as the three most frequent right argu-
ment frames)
? Number of predicates: This is the number of pred-
icates in the sentence.
For each token (base phrase) to be tagged, a set of or-
dered features is created from a fixed size context that
surrounds each token. In addition to the above features,
we also use previous semantic IOB tags that have already
been assigned to the tokens contained in the context. A
5-token sliding window is used for the context. A greedy
left-to-right tagging is performed.
All of the above features are designed to implicitly cap-
ture the patterns of sentence constructs with respect to
different word/predicate usages and senses. We acknowl-
edge that they significantly overlap and extensive exper-
iments are required to determine the impact of each fea-
ture on the performance.
2.3 Classifier
All SVM classifiers were realized using TinySVM1 with
a polynomial kernel of degree 2 and the general purpose
SVM based chunker YamCha 2. SVMs were trained for
begin (B) and inside (I) classes of all arguments and one
outside (O) class for a total of 78 one-vs-all classifiers
(some arguments do not have an I-tag).
1http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM
2http://cl.aist-nara.ac.jp/taku-ku/software/yamcha
Table 1: Comparison of W-by-W and P-by-P methods.
Both systems use the base features provided (i.e. no fea-
ture engineering is done). Results are on dev set.
Method Precision Recall F?=1
P-by-P 69.04% 54.68% 61.02
W-by-W 68.34% 45.16% 54.39
Table 2: Number of sentences and unique training exam-
ples in each method.
Method Sentences Training Examples
P-by-P 19K 347K
W-by-W 19K 534K
3 Experimental Results
3.1 Data and Evaluation Metrics
The data provided for the shared task is a part of the
February 2004 release of the PropBank corpus. It con-
sists of sections from the Wall Street Journal part of the
Penn Treebank. All experiments were carried out using
Sections 15-18 for training Section-20 for development
and Section-21 for testing. The results were evaluated for
precision, recall and F?=1 numbers using the srl-eval.pl
script provided by the shared task organizers.
3.2 W-by-W and P-by-P Experiments
In these experiments we used only the base features to
compare the two approaches. Table 1 illustrates the over-
all performance on the dev set. Although both systems
were trained using the same number of sentences, the ac-
tual number of training examples in each case were quite
different. Those numbers are presented in Table 2. It is
clear that P-by-P method uses much less data for the same
number of sentences. Despite this we particularly note a
considerable improvement in recall. Actually, the data
reduction was not without a cost. Some arguments have
been missed as they do not align with the base phrase
chunks due to inconsistencies in semantic annotation and
due to errors in automatic base phrase chunking. The per-
centage of this misalignment was around 2.5% (over the
dev set). We observed that nearly 45% of the mismatches
were for the ?outside? chunks. Therefore, sequences of
words with outside tags were not collapsed.
3.3 Best System Results
In these experiments all of the features described earlier
were used with the P-by-P system. Table 3 presents our
best system performance on the development set. Ad-
ditional features have improved the performance from
61.02 to 71.72. The performance of the same system on
the test set is similarly illustrated in Table 4.
Table 3: System results on development set.
Precision Recall F?=1
Overall 74.17% 69.42% 71.72
A0 82.86% 78.50% 80.62
A1 72.82% 73.97% 73.39
A2 60.16% 56.18% 58.10
A3 59.66% 47.65% 52.99
A4 83.21% 74.15% 78.42
A5 100.00% 75.00% 85.71
AM-ADV 52.52% 41.48% 46.35
AM-CAU 61.11% 41.51% 49.44
AM-DIR 47.37% 15.00% 22.78
AM-DIS 76.47% 76.47% 76.47
AM-EXT 74.07% 40.82% 52.63
AM-LOC 51.21% 46.09% 48.51
AM-MNR 51.04% 36.83% 42.78
AM-MOD 99.47% 95.63% 97.51
AM-NEG 99.20% 94.66% 96.88
AM-PNC 70.00% 28.00% 40.00
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 69.33% 58.37% 63.38
R-A0 91.55% 80.25% 85.53
R-A1 72.46% 67.57% 69.93
R-A2 100.00% 52.94% 69.23
R-AM-LOC 100.00% 25.00% 40.00
R-AM-TMP 0.00% 0.00% 0.00
V 99.05% 99.05% 99.05
4 Conclusions
We have described a semantic role chunker using SVMs.
The chunking method has been based on a chunked sen-
tence structure at both syntactic and semantic levels. We
have jointly performed semantic chunk segmentation and
labeling using a set of one-vs-all SVM classifiers on a
phrase-by-phrase basis. It has been argued that the new
representation has several advantages as compared to the
original representation. It yields a semantic role labeler
that classifies larger units, exploits relatively larger con-
text, uses less data (possibly, redundant and noisy data
are filtered out), runs faster and performs better.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 Shared Task: Semantic Role La-
beling in the same volume of Proc. of CoNLL?2004
Shared Task.
Christopher J. C. Burges. 1997. A Tutorial on Support
Vector Machines for Pattern Recognition. Data Min-
ing and Knowledge Discovery, 2(2), pages 1-47.
Kadri Hacioglu and Wayne Ward. 2003. Target word
Table 4: System results on test set.
Precision Recall F?=1
Overall 72.43% 66.77% 69.49
A0 82.93% 79.88% 81.37
A1 71.92% 71.33% 71.63
A2 49.37% 49.30% 49.33
A3 57.50% 46.00% 51.11
A4 87.10% 54.00% 66.67
A5 0.00% 0.00% 0.00
AM-ADV 53.36% 38.76% 44.91
AM-CAU 57.89% 22.45% 32.35
AM-DIR 37.84% 28.00% 32.18
AM-DIS 66.83% 62.44% 64.56
AM-EXT 70.00% 50.00% 58.33
AM-LOC 46.63% 36.40% 40.89
AM-MNR 50.31% 31.76% 38.94
AM-MOD 98.12% 92.88% 95.43
AM-NEG 91.11% 96.85% 93.89
AM-PNC 52.00% 15.29% 23.64
AM-PRD 0.00% 0.00% 0.00
AM-TMP 64.57% 50.74% 56.82
R-A0 90.21% 81.13% 85.43
R-A1 83.02% 62.86% 71.54
R-A2 100.00% 33.33% 50.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 60.00% 21.43% 31.58
V 98.46% 98.46% 98.46
Detection and Semantic Role Chunking Using Support
Vector Machines. Proc. of the HLT-NAACL-03.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James
Martin, and Dan Jurafsky. 2003. Shallow Semantic
Parsing Using Support Vector Machines. CSLR Tech.
Report, CSLR-TR-2003-1.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James
Martin, and Dan Jurafsky. 2003. Semantic Role Pars-
ing: Adding Semantic Structure to Unstructured Text.
Proc. of Int. Conf. on Data Mining (ICDM03).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James
Martin, and Dan Jurafsky. 2004. Support Vector
Learning for Semantic Argument Classification. to ap-
pear in Journal of Machine Learning.
Lance E. Ramhsaw and Mitchell P. Marcus. 1995.
Text Chunking Using Transformation Based Learning.
Proc. of the 3rd ACL Workshop on Very Large Cor-
pora, pages 82-94.
Erik F. T. J. Sang, John Veenstra. 1999. Representing
Text Chunks. Proc. of EACL?99, pages 173-179.
Vladamir Vapnik 1995. The Nature of Statistical Learn-
ing Theory. Springer Verlag, New York, USA.
Mixing Weak Learners in Semantic Parsing
Rodney D. Nielsen
Dept of Computer Science
University of Colorado, UCB-430
Boulder, CO 80309-0430
USA
Rodney.Nielsen@Colorado.edu
Sameer Pradhan
Center for Spoken Language Research
University of Colorado
Boulder, CO 80303
USA
Sameer.Pradhan@Colorado.edu
Abstract
We apply a novel variant of Random Forests
(Breiman, 2001) to the shallow semantic parsing
problem and show extremely promising results.
The final system has a semantic role classification
accuracy of 88.3% using PropBank gold-standard
parses. These results are better than all others
published except those of the Support Vector Ma-
chine (SVM) approach implemented by Pradhan
et al (2003) and Random Forests have numerous
advantages over SVMs including simplicity, faster
training and classification, easier multi-class classi-
fication, and easier problem-specific customization.
We also present new features which result in a 1.1%
gain in classification accuracy and describe a tech-
nique that results in a 97% reduction in the feature
space with no significant degradation in accuracy.
1 Introduction
Shallow semantic parsing is the process of finding
sentence constituents that play a semantic role rela-
tive to a target predicate and then labeling those con-
stituents according to their respective roles. Speci-
fying an event?s agent, patient, location, time of oc-
currence, etc, can be useful for NLP tasks such as
information extraction (c.f., Surdeanu et al, 2003),
dialog understanding, question answering, text sum-
marization, and machine translation. Example 1 de-
picts a semantic parse.
(1) [Agent She] [P bought] [Patient the vase]
[Locative in Egypt]
We expand on previous semantic parsing work
(Gildea and Jurafsky, 2002; Pradhan et al, 2003;
Surdeanu et al, 2003) by presenting a novel algo-
rithm worthy of further exploration, describing a
technique to drastically reduce feature space size,
and presenting statistically significant new features.
The accuracy of the final system is 88.3% on the
classification task using the PropBank (Kingsbury
et al, 2002) corpus. This is just 0.6% off the best
accuracy reported in the literature.
The classification algorithm used here is a vari-
ant of Random Forests (RFs) (Breiman, 2001).
This was motivated by Breiman?s empirical stud-
ies of numerous datasets showing that RFs often
have lower generalize error than AdaBoost (Fre-
und and Schapire, 1997), are less sensitive to noise
in the training data, and learn well from weak in-
puts, while taking much less time to train. RFs
are also simpler to understand and implement than
SVMs, leading to, among other things, easier in-
terpretation of feature importance and interactions
(c.f., Breiman, 2004), easier multi-class classifica-
tion (requiring only a single training session versus
one for each class), and easier problem-specific cus-
tomization (e.g., by introducing prior knowledge).
The algorithm described here is considerably differ-
ent from those in (Breiman, 2001). It was signifi-
cantly revised to better handle high dimensional cat-
egorical inputs and as a result provides much better
accuracy on the shallow semantic parsing problem.
The experiments reported here focus on the clas-
sification task ? given a parsed constituent known
to play a semantic role relative to a given predicate,
decide which role is the appropriate one to assign
to that constituent. Gold-standard sentence parses
for test and training are taken from the PropBank
dataset. We report results on two feature sets from
the literature and a new feature set described here.
In section 2, we describe the data used in the ex-
periments. Section 3 details the classification algo-
rithm. Section 4 presents the experimental results
and describes each experiment?s feature set. Sec-
tion 5 provides a discussion and thoughts on future
work.
2 The Data
The classifiers were trained on data derived from
the PropBank corpus (Kingsbury et al, 2002). The
same observations and features are used as de-
scribed by (Pradhan et al, 2003). They acquired
the original data from the July 15, 2002 release
of PropBank, which the University of Pennsylva-
nia created by manually labeling the constituents
S
NP VP
She bought
NP PP
the vase in Egypt
Arg0 Predicate Arg1 ArgM-Loc
Figure 1: Syntactic parse of the sentence in (2)
of the Penn TreeBank gold-standard parses (Marcus
et al, 1994). Predicate usages (at present, strictly
verbs) are hand annotated with 22 possible seman-
tic roles plus the null role to indicate grammatical
constituents that are not arguments of the predicate.
The argument labels can have different meanings
depending on their target predicate, but the anno-
tation method attempted to assign consistent mean-
ings to labels, especially when associated with sim-
ilar verbs. There are seven core roles or arguments,
labeled ARG0-5 and ARG9. ARG0 usually corre-
sponds to the semantic agent and ARG1 to the entity
most affected by the action. In addition to the core
arguments, there are 15 adjunctive arguments, such
as ARGM-LOC which identifies locatives. Thus our
previous example, ?She bought the vase in Egypt?,
would be parsed as shown in example 2. Figure
1 shows the associated syntactic parse without the
parts of speech.
(2) [Arg0 She] [P bought] [Arg1 the vase]
[ArgM-Loc in Egypt]
Development tuning is based on PropBank sec-
tion 00 and final results are reported for section 23.
We trained and tested on the same subset of obser-
vations as did Pradhan et al (2003). They indicated
that a small number of sentences (less than 1%)
were discarded due to manual tagging errors in the
original PropBank labeling process, (e.g., an empty
role tag). This one percent reduction applies to all
sections of the corpus (training, development and
test). They removed an additional 2% of the train-
ing data due to issues involving the named entity
tagger splitting corpus tokens into multiple words.
However, where these issues occurred in tagging the
section 23 test sentences, they were manually cor-
rected. The size of the dataset is shown in Table 1.
3 The Algorithm
3.1 Random Forests
Breiman (2001) defines a random forest as ?a clas-
sifier consisting of a collection of tree structured
classifiers {h(x,?k), k=1, ...} where the {?k} are
independently identically distributed random [train-
ing] vectors and each tree casts a unit vote for
Section # sent # words # preds # args
training 28 651 50 129
development 1.2 28 2.2 5.7
test 1.5 33 2.7 7.0
Table 1: Number of sentences, words, marked pred-
icates, and labeled arguments in thousands
the most popular class at input x.? Thus Bagging
(Breiman, 1996) is a form of Random Forest, where
each tree is grown based on the selection, with re-
placement, of N random training examples, where
N is the number of total examples in the training
set.
Breiman (2001) describes two new subclasses of
Random Forests, Forest-RI and Forest-RC. In each,
he combines Bagging, using the CART methodol-
ogy to create trees, with random feature selection
(Amit and Geman, 1997) at each node in the tree.
That is, at each node he selects a different random
subset of the input features and considers only these
in establishing the decision at that node.
The big idea behind Random Forests is that by in-
jecting randomness into the individual trees via ran-
dom feature selection, the correlation between their
classification results is minimized. A lower correla-
tion combined with reasonably good classification
accuracy for individual trees leads to a much higher
accuracy for the composite forest. In fact, Breiman
shows that a theoretical upper bound can be estab-
lished for the generalization error in terms of the
strength of the forest, s, and the mean value of the
classification correlation from individual trees, ??.
The strength, s, is the expected margin over the in-
put space, where the margin of an ensemble classi-
fier is defined as the difference between the fraction
of the ensemble members that vote for the correct
class versus the fraction voting for the most popular
alternative class. See (Breiman, 2001) for a detailed
description of s and ?? and how they are calculated.
The upper bound on the generalization error is given
by the following equation:
E? ? ??(1? s
2)
s2 (1)
Breiman found that Forest-RI and Forest-RC
compare favorably to AdaBoost in general, are far
less sensitive to noise in the training data, and can
learn well using weak inputs.
3.2 Feature Issues
Before describing the variant of Random Forests we
use here, it is helpful to discuss a couple of impor-
tant issues related to the input features. In the exper-
iments here, the true input features to the algorithm
are all categorical. Breiman?s approach to handling
categorical inputs is as follows. He modifies their
selection probability such that they are V -1 times as
likely as a numeric input to be selected for evalu-
ation at each node, where V is the number of val-
ues the categorical feature can take. Then when a
categorical input is selected he randomly chooses a
subset of the category values and converts the input
into a binary-valued feature whose value is one if
the training observation?s corresponding input value
is in the chosen subset and zero otherwise.
In many machine learning approaches, a categor-
ical feature having V different values would be con-
verted to V (or V -1) separate binary-valued features
(e.g., this is the case with SVMs). Here, we process
them as categorical features, but conceptually think
of them as separate binary-valued features. In an
attempt to minimize confusion, we will refer to the
categorical input features simply as inputs or as in-
put features, the equivalent set of binary-valued fea-
tures as the binary-valued features, and the features
that are randomly composed in the tree building pro-
cess (via random category value subset selection) as
composed features.
3.3 Algorithm Description
Take any tree building algorithm (e.g., C5.0 (Quin-
lan, 2002)) and modify it such that instead of exam-
ining all of the input features at each node, it con-
siders only a random subset of those features. Con-
struct a large number of trees using all of the train-
ing data (we build 128 trees in each experiment). Fi-
nally, allow the trees to individually cast unit votes
for each test observation. The majority vote deter-
mines the classification and ties are broken in favor
of the class that occurs most frequently in the train-
ing set.
Our implementation is the most similar to Forest-
RI, but has several differences, some significant.
These differences involve not using Bagging, the
use of a single forest rather than two competing
forests, the assumed size of V?i (the number of rele-
vant values for input i), the probability of selecting
individual inputs, how composed features are cre-
ated, and the underlying tree building algorithm. We
delineate each of these differences in the following
paragraphs.
Forest-RI combines random feature selection
with Bagging. Surprisingly, we found that, in our
experiments, the use of Bagging was actually hurt-
ing the classification accuracy of the forests and so
we removed this feature from the algorithm. This
means that we use all training observations to con-
struct each tree in the forest. This is somewhat
counter-intuitive given that it should increase cor-
relation in the outputs of the trees. However, the
strength of the forest is based in part on the accu-
racy of its trees, which will increase when utilizing
more training data. We also hypothesize that, given
the feature sets here, the correlation isn?t affected
significantly by the removal of Bagging. The rea-
son for this is the massive number of binary-valued
features in the problem (577,710 in just the baseline
feature set). Given this fact, using random feature
selection alone might result in substantially uncor-
related trees. As seen in equation 1 and shown em-
pirically in (Breiman, 2001), the lack of correlation
produced by random feature selection directly im-
proves the error bound.
Forest-RI involves growing two forests and se-
lecting the one most likely to provide the best re-
sults. These two forests are constructed using dif-
ferent values for F , the number of random features
evaluated at each node. The choice of which forest
is more likely to provide the best results is based on
estimates using the observations not included in the
training data (the out-of-bag observations). Since
we did not use Bagging, all of our observations are
used in the training of each tree and we could not
take this approach. Additionally, it is not clear that
this provided better results in (Breiman, 2001) and
preliminary experiments (not reported here) suggest
that it might be more effective to simply find a good
value for F .
To create composed features, we randomly select
a number of the input?s category values, C, given by
the following equation:
C = 1, V? ? 4
C = ?1.5 + log2 V? ?, V? > 4
(2)
where V? is the number of category values still po-
tentially relevant. Random category value selec-
tion is consistent with Breiman?s work, as noted in
section 3.2. This random selection method should
act to further reduce the correlation between trees
and Breiman notes that it gets around the problem
caused by categorical inputs with large numbers of
values. However, he leaves the number of values
chosen unspecified. There is also no indication of
what to do as the categorical input becomes more
sparse near the leaves of the tree (e.g., if the algo-
rithm sends every constituent whose head word is in
a set ? down the right branch of the node, what ef-
fect does this have on future random value selection
in each branch). This is the role of V? in the above
equation.
A value is potentially relevant if it is not known
to have been effectively removed by a previous de-
cision. The decision at a given node typically sends
all of the observations whose input is in the se-
lected category value subset down one branch, and
the remaining observations are sent down the other
(boolean compositions would result in exceptions).
The list of relevant category values for a given in-
put is immediately updated when the decision has
obvious consequences (e.g., the values in ? are re-
moved from the list of relevant values used by the
left branch in the previous example and the list for
the right branch is set to ?). However, a decision
based on one input can also affect the remaining rel-
evant category values of other inputs (e.g., suppose
that at the node in our previous example, all prepo-
sitional phrase (PP) constituents had the head word
with and with was a member of ?, then the phrase
type PP would no longer be relevant to decisions
in the left branch, since all associated observations
were sent down the right branch). Rather than up-
date all of these lists at each node (a computation-
ally expensive proposition), we only determine the
unique category values when there are fewer than
1000 observations left on the path, or the number of
observations has been cut to less than half what it
was the last time unique values were determined. In
early experimentation, this reduced the accuracy by
about 0.4% relative to calculating the remaining cat-
egory values after each decision. So when speed is
not important, one should take the former approach.
Breiman indicates that, when several of the in-
puts are categorical, in order to increase strength
enough to obtain a good accuracy rate the number
of inputs evaluated at each node must be increased
to two-three times ?1 + log2 M? (where M is the
number of inputs). It is not clear whether the input
selection process is with or without replacement.
Some of the inputs in the semantic parsing prob-
lem have five orders of magnitude more category
values than others. Given this issue, if the selec-
tion is without replacement, it leads to evaluating
features composed from each of our seven baseline
inputs (figure 2) at each node. This would likely
increase correlation, since those inputs with a very
small number of category values will almost always
be the most informative near the root of the tree and
would be consistently used for the upper most deci-
sions in the tree. On the other hand, if selection is
with replacement, then using the Forest-RI method
for calculating the input selection probability will
result in those inputs with few category values al-
most never being chosen. For example, the baseline
feature set has 577710 equivalent binary-valued fea-
tures by the Forest-RI definition, including two true
binary inputs. The probability of one of these two
inputs not being chosen in a given random draw ac-
cording to the Forest-RI method is 577709/577710
(see section 3.2 above). With M=7 inputs, generat-
ing 3?1 + log2 M? = 9 random composed features
results in these two binary inputs having a selection
probability of 1? (577709/577710)9, or 0.000016.
Our compromise is first to use C and V? from
equation 2 to calculate a baseline number of com-
posable features for each input i. This quantity is
the total number of potentially relevant category val-
ues divided by the number used to create a com-
posed feature:
fi =
V?i
Ci
(3)
Second, given the large number of composable fea-
tures fi, we also evaluate a larger number, F , of
random features at each node in the tree:
F = max(?
?
f?,min(f, ?1.5 + 3 log2(f)?)) (4)
where f is the sum of fi over all inputs. Finally,
selection and feature composition is done with re-
placement. The final feature selection process has at
least two significant effects we find positive. First,
the number of composable features reflects the fact
that several category values are considered simul-
taneously, effectively splitting on Ci binary-valued
features. This has the effect of reducing the selec-
tion probability of many-valued inputs and increas-
ing the probability of selecting inputs with fewer
category values. Using the baseline feature set as
an example, the probability of evaluating one of the
binary-valued inputs at the root of the tree increases
from 0.000016 to 0.0058. Second, as category val-
ues are used they are periodically removed from the
set under consideration, reducing the correspond-
ing size of Vi, and the input selection probabilities
are then adjusted accordingly. This has the effect
of continuously raising the selection probability for
those inputs that have not yet been utilized.
Finally, we use ID3 to grow trees rather than
CART, which is the tree algorithm Forest-RI uses.
We don?t believe this should have any significant
effect on the final results. The choice was purely
based on already having an implementation of ID3.
From a set of possible split decisions, ID3 chooses
the decision which leads to the minimum weighted
average entropy among the training observations as-
signed to each branch, as determined by class labels
(Quinlan, 1986; Mitchell, 1997).
These algorithm enhancements are appropriate
for any task with high dimensional categorical in-
puts, which includes many NLP applications.
PREDICATE: the lemma of the predicate whose
arguments are to be classified ? the infinitive form
of marked verbs in the corpus
CONSTITUENT PHRASE TYPE: the syntactic type
assigned to the constituent/argument being classi-
fied
HEAD WORD (HW): the head word of the target
constituent
PARSE TREE PATH (PATH): the sequence of parse
tree constituent labels from the argument to its
predicate
POSITION: a binary value indicating whether the
target argument precedes or follows its predicate
VOICE: a binary value indicating whether the
predicate was used in an active or passive phrase
SUB-CATEGORIZATION: the parse tree expansion
of the predicate?s grandparent constituent
Figure 2: Baseline feature set of experiment 1, see
(Gildea and Jurafsky, 2002) for details
4 The Experiments
Four experiments are reported: the first uses the
baseline features of Gildea and Jurafsky (2002); the
second is composed of features proposed by Prad-
han et al (2003) and Surdeanu et al (2003); the
third experiment evaluates a new feature set; and the
final experiment addresses a method of reducing the
feature space. The experiments all focus strictly on
the classification task ? given a syntactic constituent
known to be an argument of a given predicate, de-
cide which argument role is the appropriate one to
assign to the constituent.
4.1 Experiment 1: Baseline Feature Set
The first experiment compares the random for-
est classifier to three other classifiers, a statisti-
cal Bayesian approach with backoff (Gildea and
Palmer, 2002), a decision tree classifier (Surdeanu
et al, 2003), and a Support Vector Machine (SVM)
(Pradhan et al, 2003). The baseline feature set uti-
lized in this experiment is described in Figure 2 (see
(Gildea and Jurafsky, 2002) for details).
Surdeanu et al omit the
SUB-CATEGORIZATION feature, but add a
binary-valued feature that indicates the governing
category of noun-phrase argument constituents.
This feature takes on the value S or VP depending
on which constituent type (sentence or verb phase
respectively) eventually dominates the argument in
the parse tree. This generally indicates grammatical
subjects versus objects, respectively. They also
used the predicate with its case and morphology
intact, in addition to using its lemma. Surdeanu
et al indicate that, due to memory limitations on
Classifier Accuracy
Bayesian (Gildea and Palmer, 2002) 82.8
Decision Tree (Surdeanu et al, 2003) 78.8
SVM (Pradhan et al, 2003) 87.1
First Tree 78.3
Random Forest 84.6
Table 2: Results of baseline feature set experiment
their hardware, they trained on only 75 KB of the
PropBank argument constituents ? about 60% of
the annotated data.
Table 2 shows the results of experiment 1, com-
paring the classifier accuracies as trained on the
baseline feature set. Using a difference of two pro-
portions test as described in (Dietterich, 1998), the
accuracy differences are all statistically significant
at p=0.01. The Random Forest approach outper-
forms the Bayesian method and the Decision Tree
method. However, it does not perform as well as the
SVM classifier. Interestingly, the classification ac-
curacy of the first tree in the Random Forest, given
in row four, is almost as high as that of the C5 deci-
sion trees (Quinlan, 2002) of Surdeanu et al
4.2 Experiment 2: Extended Feature Set
The second experiment compares the random for-
est classifier to the boosted decision tree and the
SVM using all of the features reported by Pradhan
et al The additional features used in this experi-
ment are listed in Figure 3 (see sources for further
details). In addition to the extra features noted in the
previous experiment, Surdeanu et al report on four
more features, not included here (content word part
of speech (CW PoS)1, CW named entity class, and
two phrasal verb collocation features).
Table 3 shows the results of experiment 2, com-
paring the classifier accuracies using the full feature
sets reported in each source. Surdeanu et al also ap-
plied boosting in this experiment and chose the out-
come of the boosting iteration that performed best.
Using the difference of two proportions test, the ac-
curacy differences are all statistically significant at
p=0.01. The Random Forest approach outperforms
the Boosted Decision Tree method by 3.5%, but
trails the SVM classifier by 2.3%. In analyzing the
performance on individual argument classes using
McNemar?s test, Random Forest performs signifi-
cantly better on ARG0 (p=0.001) then the SVM, and
the SVM has significantly better results on ARG1
(p=0.001). The large number of degrees of freedom
1We also tested the CW PoS, but it did not improve the de-
velopment results and was omitted.
NAMED ENTITIES: seven binary-valued fea-
tures indicating whether specific named enti-
ties (PERSON, ORGANIZATION, DATE, TIME,
MONEY, LOCATION, and PERCENT) occurred
anywhere in the target constituent (Surdeanu et al,
2003)
HW POS: the grammatical part of speech of the
target constituent?s head word (Surdeanu et al,
2003)
CONTENT WORD (CW): ?lexicalized feature that
selects an informative word from the constituent,
different from the head word?(Surdeanu et al,
2003)
VERB CLUSTER: a generalization of the verb
predicate by clustering verbs into 64 classes
(Pradhan et al, 2003)
HALF PATH: the sequence of parse tree con-
stituent labels from the argument to the lowest
common ancestor of the predicate (Pradhan et al,
2003)
Figure 3: Additional features in experiment 2
Classifier Accuracy
Boosted Decision Tree (Surdeanu et al,
2003)
83.7
Random Forest (trained with CW) 87.2
SVM (Pradhan et al, 2003) 88.9
Random Forest (trained without CW) 86.6
Table 3: Results of experiment 2
prevent significance at p=0.1 for any other argu-
ments, but the SVM appears to perform much better
on ARG2 and ARG3.
4.3 Experiment 3: New Features
We evaluated several new features and report on the
most significant here, as described in figure 4.2 The
results are reported in table 4. The accuracy im-
provements relative to the results from experiment
2 are all statistically significant at p=0.001 (McNe-
mar?s test is used for all significance tests in this sec-
tion). Comparing the SVM results in experiment 2
to the best results here shows statistical significance
2Due to space, we cannot report all experiments; contact the
first author for more information. The other features we eval-
uated involved: the phrase type of the parent constituent, the
list of phrase types encompassing the sentence fragment be-
tween the target predicate and constituent, the prefix and suffix
of the cw and hw, animacy, high frequency words preceding
and following the predicate, and the morphological form of the
predicate. All of these improved accuracy on the development
set (some with statistical significance at p=0.01), but we sus-
pect the development baseline was at a low point, since these
features largely did not improve performance when combined
with CW Base and GP.
GOVERNING PREPOSITION (GP): if the con-
stituent?s parent is a PP, this is the associated
preposition (e.g., in ?made of [Arg2 gallium ar-
senide]?, this feature is ?of?, since the Arg2-NP is
governed by an ?of?-based PP)
CW BASE: starting with the CW, convert it to its
singular form, remove any prefix, and convert dig-
its to ?n? (e.g., this results in the following CW ?
CW Base mappings: accidents ? accident, non-
binding ? binding, repayments ? payment, and
1012 ? nnnn)
Figure 4: Features in experiment 3
Feature Set Accuracy
Extended (see figures 2 & 3) 86.6
Extended + CW BASE 87.4
Extended + GOVERNING PREPOSITION 87.4
Extended + CW BASE & GP 88.3
Table 4: Results of experiment 2
only at p=0.1.
In analyzing the effect on individual argument
classes, seven have high ?2 values (ARG2-4,
ARGM-DIS (discourse), ARGM-LOC (locative),
ARGM-MNR (manner), and ARGM-TMP (temporal)),
but given the large number of degrees of free-
dom, only ARGM-TMP is significant (p=0.05). Ex-
ample section-00 sentence fragments including the
target predicate (P) and ARG2 role whose classi-
fication was corrected by the GP feature include
?[P banned] to [everyday visitors]?, ?[P consid-
ered] as [an additional risk for the investor]?, and
?[P made] of [gallium arsenide]?. Comparing the
SVM results to the best results here, the Ran-
dom Forest performs significantly better on Arg0
(p=0.001), and the SVM is significantly better on
Arg1 (p=0.001). Again the degrees of freedom pre-
vent significance at p=0.1, but the Random Forest
outperforms the SVM with a fairly high ?2 value on
ARG4, ARGM-DIS, ARGM-LOC, and ARGM-TMP.
4.4 Experiment 4: Dimensionality Reduction
We originally assumed we would be using binary-
valued features with sparse matrices, much like in
the SVM approach. Since many of the features have
a very large number of values (e.g., the PATH fea-
ture has over 540k values), we sought ways to re-
duce the number of equivalent binary-valued fea-
tures. This section reports on one of these meth-
ods, which should be of interest to others in resource
constrained environments.
In this experiment, we preprocess the baseline in-
puts described in Figure 2 to reduce their number
of category values. Specifically, for each original
category value, vi ? V , we determine whether it
occurs in observations associated with one or more
than one semantic role label, R. If it is associated
with more than one R, vi is left as is. When vi maps
to only a single Rj , we replace vi with an arbitrary
value, vk /? V , which is the same for all such v oc-
curring strictly in association with Rj . The PATH
input starts with 540732 original feature values and
has only 1904 values after this process, while HEAD
WORD is reduced from 33977 values to 13208 and
PHRASE TYPE is reduced from 62 to 44 values.
The process has no effect on the other baseline input
features. The total reduction in equivalent binary-
valued features is 97%. We also test the effect of
disregarding feature values during training if they
only occur once in the training data. This has a
more modest effect, reducing PATH to 156788 val-
ues and HEAD WORD to 29482 values, with no other
reductions. The total reduction in equivalent binary-
valued features is 67%.
Training on the baseline feature set, the net effect
of these two procedures was less than a 0.3% loss
of accuracy on the development set. The McNemar
test indicates this is not significant at p=0.1. In the
end, our implementation used categorical features,
rather than binary-valued features (e.g., rather than
use 577710 binary-valued features to represent the
baseline inputs, we use 7 features which might take
on a large number of values ? PATH has 540732 val-
ues). In this case, the method does not result in as
significant a reduction in the memory requirements.
While we did not use this feature reduction in any
of the experiments reported previously, we see it as
being very beneficial to others whose implementa-
tion may be more resource constrained, particularly
those using a binary-valued feature representation.
The method also reduced training time by 17%
and should lead to much larger reductions for im-
plementations using binary-valued features. For ex-
ample, the worst case training time for SVMs is
quadratic in the number of features and this method
reduced the dimensionality to 3% of its original
size. Therefore, the method has the theoretical
potential to reduce training time by up to 100(1-
0.032) = 99.91%. While it is unlikely to ap-
proach this in practice, it should provide signifi-
cant savings. This may be especially helpful during
model selection or feature evaluation, after which,
one could revert to the full dimensionality for fi-
nal training to improve classification accuracy. The
slight decrement in accuracy may also be overcome
by the ability to handle larger datasets.
5 Discussion and Future Research
The version of Random Forests described here out-
performs the Bayesian algorithm (Gildea and Juraf-
sky, 2002; Gildea and Palmer, 2002) by 1.8% on the
same feature set and outperforms the boosted deci-
sion tree classifier (Surdeanu et al, 2003) by 3.5%
on the extended feature set with 5 fewer features.
The SVM classifier (Pradhan et al, 2003) was 2.3%
better training on the same data, but only 0.6% bet-
ter than our best results.
The Random Forest (RF) approach has advan-
tages that might make it a better choice than an
SVM in certain circumstances. Conceptually, it
is simpler to understand and can be implemented
more easily. This also makes it easier to modify
the algorithm to evaluate new techniques. RFs al-
low one to more easily implement multi-class clas-
sifiers. The RFs here were implemented as a single
classifier, rather than as the 22 one-against-all clas-
sifiers required by the SVM approach. Since RFs
are not overly sensitive to noise in the training data
(Breiman, 2001), it might be the case that they will
narrow the performance gap when training is based
on automatically parsed sentences. Further research
is required in this area. Additionally, RFs have an
advantage in training time. It takes about 40% of
the SVM time (8 versus 20 hours) to train on the
extended feature set for the classification task and
we expect this time to be cut by up to a factor of 10
in porting from MatLab to C. Classification time is
generally faster for RFs as well, which is important
for real-time tasks.
In a class-by-class comparison, using the same
features, the RF performed significantly better than
the SVM on Arg0 roles, the same or slightly better
on 12 of the other 21 arguments, and slightly bet-
ter overall on the 14 adjunctive arguments (77.8%
versus 77.3% accuracy on 1882 observations). Re-
viewing performance on data not seen during train-
ing, both algorithms degraded to about 94% of their
accuracy on seen data.
The RF algorithm should be evaluated on the
identification task and on the combined identifica-
tion and classification task. This will provide addi-
tional comparative evidence to contrast it with the
SVM approach. Further research is also required to
determine how RFs generalize to new genres.
Another area for future research involves the es-
timation of class probabilities. MOB-ESP, a variant
of Random Forests which outputs class probability
estimates, has been shown to produce very good re-
sults (Nielsen, 2004). Preliminary experiments sug-
gest that using these probability estimates in con-
junction with an SVM classifier might be more ef-
fective than estimating probabilities based on the
example?s distance from the decision surface as in
(Platt, 2000). Class probabilities are useful for sev-
eral semantic parsing and more general NLP tasks,
such as selective use of labeled examples during
training (c.f., Pradhan et al, 2003) and N-best list
processing.
6 Conclusion
The results documented in these experiments are
very promising and mandate further research. The
final classification accuracy of the Random For-
est was 88.3%, just 0.6% behind the SVM results
(Pradhan et al, 2003) and 4.6% higher than the next
best results (Surdeanu et al, 2003) ? results that
were based on a number of additional features.
We defined several modifications to the RF algo-
rithm that increased accuracy. These improvements
are important for any application with high dimen-
sional categorical inputs, which includes many NLP
tasks. We introduced new features which provided
a 1.1% improvement in accuracy over the best re-
sults using features from the literature. We also in-
troduced a technique to reduce the dimensionality
of the feature space, resulting in a reduction to just
3% of the original feature space size. This could
be an important enabler for handling larger datasets
and improving the efficiency of feature and model
selection.
Acknowledgements
We thank Dan Jurafsky for miscellaneous support
and for valuable feedback on a draft of this paper.
Thanks also go to the anonymous reviewers whose
feedback improved the paper.
References
Yali Amit and Donald Geman. 1997. Shape Quan-
tization and Recognition with Randomized Trees.
Neural Computation, 9:1545?1588.
Leo Breiman. 2001. Random Forests. Journal of
Machine Learning, 45(1):5?32.
Leo Breiman. 2004. Random Forests. http://stat-
www.berkeley.edu/users/breiman/RandomForests/
Leo Breiman. 1996. Bagging Predictors. Machine
Learning, 26(2):123?140.
Thomas G. Dietterich. 1998. Approximate statis-
tical tests for comparing supervised classifica-
tion learning algorithms. Neural Computation,
10(7):1895?1924.
Y. Freund and R. E. Schapire. 1997. A decision-
theoretic generalization of on-line learning and
an application to boosting. Journal of Computer
and Systems Sciences, 55(1):119?139.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic Labeling of Semantic Roles. Computa-
tional Linguistics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The Ne-
cessity of Parsing for Predicate Argument Recog-
nition. Proceedings of ACL-02.
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn
Treebank. Proceedings of the HLT-02.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn TreeBank: Annotating
predicate argument structure.
Tom M. Mitchell. 1997. Machine Learning.
McGraw-Hill, Boston, MA.
Rodney D. Nielsen. 2004. MOB-ESP and other Im-
provements in Probability Estimation. Proceed-
ings of the 20th Conference on Uncertainty in Ar-
tificial Intelligence.
John Platt. 2000. Probabilities for Support Vector
Machines. In A. Smola, P. Bartlett, B. Scolkopf,
and D. Schuurmans (Eds), Advances in Large
Margin Classifiers. MIT Press, Cambridge, MA.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, Daniel Jurafsky.
2003. Shallow Semantic Parsing using Support
Vector Machines. University of Colorado Tech-
nical Report: TR-CSLR-2003-03.
J. R. Quinlan. 1986. Induction of decision trees.
Machine Learning, 1:81?106.
J. R. Quinlan. 2002. Data Mining Tools See5 and
C5.0. http://www.rulequest.com/see5-info.html.
Mihai Surdeanu, Sanda Harabagiu, John Williams
and Paul Aarseth. 2003. Using Predicate-
Argument Structures for Information Extraction.
Proceedings of ACL-03.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 217?220, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Chunking Combining Complementary Syntactic Views
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Martin and Daniel Jurafsky?
Center for Spoken Language Research, University of Colorado, Boulder, CO 80303
?Department of Linguistics, Stanford University, Stanford, CA 94305
{spradhan,hacioglu,whw,martin}@cslr.colorado.edu, jurafsky@stanford.edu
Abstract
This paper describes a semantic role la-
beling system that uses features derived
from different syntactic views, and com-
bines them within a phrase-based chunk-
ing paradigm. For an input sentence, syn-
tactic constituent structure parses are gen-
erated by a Charniak parser and a Collins
parser. Semantic role labels are assigned
to the constituents of each parse using
Support Vector Machine classifiers. The
resulting semantic role labels are con-
verted to an IOB representation. These
IOB representations are used as additional
features, along with flat syntactic chunks,
by a chunking SVM classifier that pro-
duces the final SRL output. This strategy
for combining features from three differ-
ent syntactic views gives a significant im-
provement in performance over roles pro-
duced by using any one of the syntactic
views individually.
1 Introduction
The task of Semantic Role Labeling (SRL) involves
tagging groups of words in a sentence with the se-
mantic roles that they play with respect to a particu-
lar predicate in that sentence. Our approach is to use
supervised machine learning classifiers to produce
the role labels based on features extracted from the
input. This approach is neutral to the particular set
of labels used, and will learn to tag input according
to the annotated data that it is trained on. The task
reported on here is to produce PropBank (Kingsbury
and Palmer, 2002) labels, given the features pro-
vided for the CoNLL-2005 closed task (Carreras and
Ma`rquez, 2005).
We have previously reported on using SVM clas-
sifiers for semantic role labeling. In this work, we
formulate the semantic labeling problem as a multi-
class classification problem using Support Vector
Machine (SVM) classifiers. Some of these systems
use features based on syntactic constituents pro-
duced by a Charniak parser (Pradhan et al, 2003;
Pradhan et al, 2004) and others use only a flat syn-
tactic representation produced by a syntactic chun-
ker (Hacioglu et al, 2003; Hacioglu and Ward,
2003; Hacioglu, 2004; Hacioglu et al, 2004). The
latter approach lacks the information provided by
the hierarchical syntactic structure, and the former
imposes a limitation that the possible candidate roles
should be one of the nodes already present in the
syntax tree. We found that, while the chunk based
systems are very efficient and robust, the systems
that use features based on full syntactic parses are
generally more accurate. Analysis of the source
of errors for the parse constituent based systems
showed that incorrect parses were a major source
of error. The syntactic parser did not produce any
constituent that corresponded to the correct segmen-
tation for the semantic argument. In Pradhan et al
(2005), we reported on a first attempt to overcome
this problem by combining semantic role labels pro-
duced from different syntactic parses. The hope is
that the syntactic parsers will make different errors,
and that combining their outputs will improve on
217
either system alone. This initial attempt used fea-
tures from a Charniak parser, a Minipar parser and a
chunk based parser. It did show some improvement
from the combination, but the method for combin-
ing the information was heuristic and sub-optimal.
In this paper, we report on what we believe is an im-
proved framework for combining information from
different syntactic views. Our goal is to preserve the
robustness and flexibility of the segmentation of the
phrase-based chunker, but to take advantage of fea-
tures from full syntactic parses. We also want to
combine features from different syntactic parses to
gain additional robustness. To this end, we use fea-
tures generated from a Charniak parser and a Collins
parser, as supplied for the CoNLL-2005 closed task.
2 System Description
We again formulate the semantic labeling problem
as a multi-class classification problem using Sup-
port Vector Machine (SVM) classifiers. TinySVM1
along with YamCha2 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used to implement
the system. Using what is known as the ONE VS
ALL classification strategy, n binary classifiers are
trained, where n is number of semantic classes in-
cluding a NULL class.
The general framework is to train separate seman-
tic role labeling systems for each of the parse tree
views, and then to use the role arguments output by
these systems as additional features in a semantic
role classifier using a flat syntactic view. The con-
stituent based classifiers walk a syntactic parse tree
and classify each node as NULL (no role) or as one
of the set of semantic roles. Chunk based systems
classify each base phrase as being the B(eginning)
of a semantic role, I(nside) a semantic role, or
O(utside) any semantic role (ie. NULL). This
is referred to as an IOB representation (Ramshaw
and Marcus, 1995). The constituent level roles are
mapped to the IOB representation used by the chun-
ker. The IOB tags are then used as features for a
separate base-phase semantic role labeler (chunker),
in addition to the standard set of features used by
the chunker. An n-fold cross-validation paradigm
is used to train the constituent based role classifiers
1http://chasen.org/?taku/software/TinySVM/
2http://chasen.org/?taku/software/yamcha/
and the chunk based classifier.
For the system reported here, two full syntactic
parsers were used, a Charniak parser and a Collins
parser. Features were extracted by first generating
the Collins and Charniak syntax trees from the word-
by-word decomposed trees in the CoNLL data. The
chunking system for combining all features was
trained using a 4-fold paradigm. In each fold, sepa-
rate SVM classifiers were trained for the Collins and
Charniak parses using 75% of the training data. That
is, one system assigned role labels to the nodes in
Charniak based trees and a separate system assigned
roles to nodes in Collins based trees. The other 25%
of the training data was then labeled by each of the
systems. Iterating this process 4 times created the
training set for the chunker. After the chunker was
trained, the Charniak and Collins based semantic la-
belers were then retrained using all of the training
data.
Two pieces of the system have problems scaling
to large training sets ? the final chunk based clas-
sifier and the NULL VS NON-NULL classifier for
the parse tree syntactic views. Two techniques were
used to reduce the amount of training data ? active
sampling and NULL filtering. The active sampling
process was performed as follows. We first train
a system using 10k seed examples from the train-
ing set. We then labeled an additional block of data
using this system. Any sentences containing an er-
ror were added to the seed training set. The sys-
tem was retrained and the procedure repeated until
there were no misclassified sentences remaining in
the training data. The set of examples produced by
this procedure was used to train the final NULL VS
NON-NULL classifier. The same procedure was car-
ried out for the chunking system. After both these
were trained, we tagged the training data using them
and removed all most likely NULLs from the data.
Table 1 lists the features used in the constituent
based systems. They are a combination of features
introduced by Gildea and Jurafsky (2002), ones pro-
posed in Pradhan et al (2004), Surdeanu et al
(2003) and the syntactic-frame feature proposed in
(Xue and Palmer, 2004). These features are ex-
tracted from the parse tree being labeled. In addition
to the features extracted from the parse tree being
labeled, five features were extracted from the other
parse tree (phrase, head word, head word POS, path
218
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
PREDICATE SUB-CATEGORIZATION
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the head word
NAMED ENTITIES IN CONSTITUENTS: Person, Organization, Location
and Miscellaneous.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an ?*?.
For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP?S?S?VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:
NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
Table 1: Features used by the constituent-based sys-
tem
and predicate sub-categorization). So for example,
when assigning labels to constituents in a Charniak
parse, all of the features in Table 1 were extracted
from the Charniak tree, and in addition phrase, head
word, head word POS, path and sub-categorization
were extracted from the Collins tree. We have pre-
viously determined that using different sets of fea-
tures for each argument (role) achieves better results
than using the same set of features for all argument
classes. A simple feature selection was implemented
by adding features one by one to an initial set of
features and selecting those that contribute signifi-
cantly to the performance. As described in Pradhan
et al (2004), we post-process lattices of n-best de-
cision using a trigram language model of argument
sequences.
Table 2 lists the features used by the chunker.
These are the same set of features that were used
in the CoNLL-2004 semantic role labeling task by
Hacioglu, et al (2004) with the addition of the two
semantic argument (IOB) features. For each token
(base phrase) to be tagged, a set of features is created
from a fixed size context that surrounds each token.
In addition to the features in Table 2, it also uses pre-
vious semantic tags that have already been assigned
to the tokens contained in the linguistic context. A
5-token sliding window is used for the context.
SVMs were trained for begin (B) and inside (I)
classes of all arguments and an outside (O) class.
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ?before?, ?after? and ?-? (for
the predicate)
PATH: It defines a flat path between the token and the predicate
HIERARCHICAL PATH: Since we have the syntax tree for the sentences,
we also use the hierarchical path from the phrase being classified to the
base phrase containing the predicate.
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
DYNAMIC CLASS CONTEXT: Hypotheses generated for two preceeding
phrases.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
CHARNIAK-BASED SEMANTIC IOB TAG: This is the IOB tag generated
using the tagger trained on Charniak trees
COLLINS-BASED SEMANTIC IOB TAG: This is the IOB tag generated
using the tagger trained on Collins? trees
Table 2: Features used by phrase-based chunker.
3 Experimental Results
Table 3 shows the results obtained on the WSJ de-
velopment set (Section 24), the WSJ test set (Section
23) and the Brown test set (Section ck/01-03)
4 Acknowledgments
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
219
Precision Recall F?=1
Development 80.90% 75.38% 78.04
Test WSJ 81.97% 73.27% 77.37
Test Brown 73.73% 61.51% 67.07
Test WSJ+Brown 80.93% 71.69% 76.03
Test WSJ Precision Recall F?=1
Overall 81.97% 73.27% 77.37
A0 91.39% 82.23% 86.57
A1 79.80% 76.23% 77.97
A2 68.61% 62.61% 65.47
A3 73.95% 50.87% 60.27
A4 78.65% 68.63% 73.30
A5 75.00% 60.00% 66.67
AM-ADV 61.64% 46.05% 52.71
AM-CAU 76.19% 43.84% 55.65
AM-DIR 53.33% 37.65% 44.14
AM-DIS 80.56% 63.44% 70.98
AM-EXT 100.00% 46.88% 63.83
AM-LOC 64.48% 51.52% 57.27
AM-MNR 62.90% 45.35% 52.70
AM-MOD 98.64% 92.38% 95.41
AM-NEG 98.21% 95.65% 96.92
AM-PNC 56.67% 44.35% 49.76
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 83.37% 71.94% 77.23
R-A0 94.29% 88.39% 91.24
R-A1 85.93% 74.36% 79.73
R-A2 100.00% 37.50% 54.55
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 90.00% 42.86% 58.06
R-AM-MNR 66.67% 33.33% 44.44
R-AM-TMP 75.00% 40.38% 52.50
V 98.86% 98.86% 98.86
Table 3: Overall results (top) and detailed results on
the WSJ test (bottom).
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
Special thanks to Matthew Woitaszek, Theron Vo-
ran and the other administrative team of the Hemi-
sphere and Occam Beowulf clusters. Without these
the training would never be possible.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. n Introduction to the CoNLL-2005
Shared Task: Semantic Role Labeling. In Proceedings of CoNLL-2005.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic
role chunking using support vector machines. In Proceedings of the Human
Language Technology Conference, Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Ju-
rafsky. 2004. Semantic role labeling by tagging syntactic chunks. In Pro-
ceedings of the 8th Conference on CoNLL-2004, Shared Task ? Semantic Role
Labeling.
Kadri Hacioglu. 2004. A lightweight semantic chunking model based on tagging.
In Proceedings of the Human Language Technology Conference /North Amer-
ican chapter of the Association of Computational Linguistics (HLT/NAACL),
Boston, MA.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings of the 3rd International Conference on Language Resources and
Evaluation (LREC-2002), Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of the 4th Conference on CoNLL-2000 and
LLL-2000, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the 2nd Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-2001).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of the International Conference on Data Mining (ICDM 2003),
Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings of the Human Language Technology Conference/North American chapter
of the Association of Computational Linguistics (HLT/NAACL), Boston, MA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2005. Semantic role labeling using different syntactic views. In Proceedings
of the Association for Computational Linguistics 43rd annual meeting (ACL-
2005), Ann Arbor, MI.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-
based learning. In Proceedings of the Third Annual Workshop on Very Large
Corpora, pages 82?94. ACL.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
of the 41st Annual Meeting of the Association for Computational Linguistics,
Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain.
220
Proceedings of NAACL HLT 2009: Tutorials, pages 11?12,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
OntoNotes: The 90% Solution
Sameer S. Pradhan and Nianwen Xue
OntoNotes is a five year multi-site collaboration between BBN Technologies, Information Sciences In-
stitute of University of Southern California, University of Colorado, University of Pennsylvania and Bran-
deis University. The goal of the OntoNotes project is to provide linguistic data annotated with a skeletal
representation of the literal meaning of sentences including syntactic parse, predicate-argument structure,
coreference, and word senses linked to an ontology, allowing a new generation of language understanding
technologies to be developed with new functional capabilities.
In its third year of existence, the OntoNotes project has generated a large amount of high quality data
covering various layers of linguistic annotation. This is probably the first time that data of such quality has
been available in large quantities covering multiple genres (newswire, broadcast news, broadcast conver-
sation and weblogs) and languages (English, Chinese and Arabic). The guiding principle has been to find
a ?sweet spot? in the space of inter-tagger agreement, productivity, and depth of representation. The most
effective use of this resource for research requires simultaneous access to multiple layers of annotation. This
has been made possible by representing the corpus with a relational database to accommodate the dense
connectedness of the data and ensure consistency across layers. In order to facilitate ease of understanding
and manipulability, the database has also been supplemented with a object-oriented Python API.
The tutorial consists of two parts. In the first part we will familiarize the user with this new resource,
describe the various layers of annotations in some detail and discuss the linguistic principles and some-
times practical considerations behind the important design decisions that shapes the corpus. We will also
describe the salient differences between the three languages at each layer of annotation and how linguistic
peculiarities of different languages were handled in the data.
In the second part, we will describe the data formats of each of the layers and talk about various design
decisions that went into the creation of the architecture of the database and the individual tables comprising
it, along with issues that came up during the representation process and compromises that were made
without sacrificing some primary objectives one of which being the independent existence of each layer
that is necessary to allow multi-site collaboration. We will explain how the database schema attempts to
interconnect all the layers. Then we will go into the details of the Python API that allows easy access to each
of the layers and show that by making the objects closely resemble database tables, the API allows for their
flexible integration. This will be followed by a hands-on working session.
1 Tutorial Outline
1. Annotation Layers
? Overview of OntoNotes
? Design principles
? Depth of annotation
? Consistency (ITA)
? Linguistics principles
? Potential applications
? Question Answering
? Machine Translation
? Layers of Annotation in English, Chinese and Arabic
? Treebank
? PropBank
? Word Sense
111
? Name
? Coreference
? Ontology
? Comparison with existing multi-layer annotation corpora
2. Data Access API
? Data
? File format
? Metadata specification
? Database schema representing each layer of annotation
? ER diagram
? Inter-connection between the annotation layers (database tables)
? Python Access API
? Introduction to the Python modules
? Correspondence between MySQL tables and Python classes
? Introduction to some frequently used module functionalities
? Extending the API to add a new layer of annotation
? Hands on Session
? Creating a sample OntoNotes database from MySQL dump file
? Loading it into memory
? Creating Python objects representing various annotation layers
? Performing cross-layer queries using a combination of API and database
? We will provide some sample queries
? Users can use their own experience to generate novel queries
? Manipulating the data as in Python world and MySQL world
? Writing the modified versions back to the database
2 Target Audience
This tutorial is designed for people interested in using one or more layers of OntoNotes in their research to
further language understanding through improved shallow semantic analysis. Detailed knowledge of any of
the layers is not necessary. Some familiarity with Python would be preferable.
Sameer Pradhan is a Research Scientist at BBN Technologies. His research interests include computational
semantics, question answering, application of machine learning to language understanding and annotation
science. He have been leading the data integration and coreference annotation effort in the DARPA funded
GALE OntoNotes project at BBN. In the past he was the technical lead on the AQUAINT project at BBN.
He serves on the ACL SIGANN committee and has been one of the organizers of the Linguistics Annotation
Workshops (LAW II and III) He has been on the programme committees of Workshop on UIMA for NLP,
and Conference on Global Interoperability of Language Resources (ICGL) He has also served on the guest
Editorial Board of Computational Linguistics: Special issue on Semantic Role Labeling. He got his PhD in
Computer Science at the University of Colorado at Boulder.
Nianwen Xue is an Assistant Professor of Language & Linguistics and Computer Science at Brandeis Univer-
sity. His research interests include formal representation of linguistic structures and its impact on natural
language processing, aspects of syntax, computational linguistics, corpus linguistics and Chinese language
processing. He is currently leading the effort to expand the Chinese Treebank, Proposition Bank and word
sense annotation, funded by DARPA as part of the GALE OntoNotes project. He serves on the ACL SIGANN
committee. He is one of the organizers of the Linguistics Annotation Workshops (LAW II and III) and is also
on the organizing committee of the CoNLL Shared Task on Syntactic and Semantic Dependencies in Multiple
Languages. He has also served on the guest Editorial Board of Computational Linguistics: Special issue on
Semantic Role Labeling. He got his PhD in linguistics from University of Delaware.
212
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 24?29,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
An Extension of BLANC to System Mentions
Xiaoqiang Luo
Google Inc.
111 8th Ave, New York, NY 10011
xql@google.com
Sameer Pradhan
Harvard Medical School
300 Longwood Ave., Boston, MA 02115
sameer.pradhan@childrens.harvard.edu
Marta Recasens
Google Inc.
1600 Amphitheatre Pkwy,
Mountain View, CA 94043
recasens@google.com
Eduard Hovy
Carnegie Mellon University
5000 Forbes Ave.
Pittsburgh, PA 15213
hovy@cmu.edu
Abstract
BLANC is a link-based coreference eval-
uation metric for measuring the qual-
ity of coreference systems on gold men-
tions. This paper extends the original
BLANC (?BLANC-gold? henceforth) to
system mentions, removing the gold men-
tion assumption. The proposed BLANC
falls back seamlessly to the original one if
system mentions are identical to gold men-
tions, and it is shown to strongly correlate
with existing metrics on the 2011 and 2012
CoNLL data.
1 Introduction
Coreference resolution aims at identifying natu-
ral language expressions (or mentions) that refer
to the same entity. It entails partitioning (often
imperfect) mentions into equivalence classes. A
critically important problem is how to measure the
quality of a coreference resolution system. Many
evaluation metrics have been proposed in the past
two decades, including the MUC measure (Vilain
et al, 1995), B-cubed (Bagga and Baldwin, 1998),
CEAF (Luo, 2005) and, more recently, BLANC-
gold (Recasens and Hovy, 2011). B-cubed and
CEAF treat entities as sets of mentions and mea-
sure the agreement between key (or gold standard)
entities and response (or system-generated) enti-
ties, while MUC and BLANC-gold are link-based.
In particular, MUC measures the degree of
agreement between key coreference links (i.e.,
links among mentions within entities) and re-
sponse coreference links, while non-coreference
links (i.e., links formed by mentions from different
entities) are not explicitly taken into account. This
leads to a phenomenon where coreference systems
outputting large entities are scored more favorably
than those outputting small entities (Luo, 2005).
BLANC (Recasens and Hovy, 2011), on the other
hand, considers both coreference links and non-
coreference links. It calculates recall, precision
and F-measure separately on coreference and non-
coreference links in the usual way, and defines
the overall recall, precision and F-measure as the
mean of the respective measures for coreference
and non-coreference links.
The BLANC-gold metric was developed with
the assumption that response mentions and key
mentions are identical. In reality, however, men-
tions need to be detected from natural language
text and the result is, more often than not, im-
perfect: some key mentions may be missing in
the response, and some response mentions may be
spurious?so-called ?twinless? mentions by Stoy-
anov et al (2009). Therefore, the identical-
mention-set assumption limits BLANC-gold?s ap-
plicability when gold mentions are not available,
or when one wants to have a single score mea-
suring both the quality of mention detection and
coreference resolution. The goal of this paper is
to extend the BLANC-gold metric to imperfect re-
sponse mentions.
We first briefly review the original definition of
BLANC, and rewrite its definition using set nota-
tion. We then argue that the gold-mention assump-
tion in Recasens and Hovy (2011) can be lifted
without changing the original definition. In fact,
the proposed BLANC metric subsumes the origi-
nal one in that its value is identical to the original
one when response mentions are identical to key
mentions.
The rest of the paper is organized as follows.
We introduce the notions used in this paper in
Section 2. We then present the original BLANC-
gold in Section 3 using the set notation defined in
Section 2. This paves the way to generalize it to
24
imperfect system mentions, which is presented in
Section 4. The proposed BLANC is applied to the
CoNLL 2011 and 2012 shared task participants,
and the scores and its correlations with existing
metrics are shown in Section 5.
2 Notations
To facilitate the presentation, we define the nota-
tions used in the paper.
We use key to refer to gold standard mentions or
entities, and response to refer to system mentions
or entities. The collection of key entities is denoted
by K = {k
i
}
|K|
i=1
, where k
i
is the i
th
key entity;
accordingly, R = {r
j
}
|R|
j=1
is the set of response
entities, and r
j
is the j
th
response entity. We as-
sume that mentions in {k
i
} and {r
j
} are unique;
in other words, there is no duplicate mention.
Let C
k
(i) and C
r
(j) be the set of coreference
links formed by mentions in k
i
and r
j
:
C
k
(i) = {(m
1
,m
2
) : m
1
? k
i
,m
2
? k
i
,m
1
6= m
2
}
C
r
(j) = {(m
1
,m
2
) : m
1
? r
j
,m
2
? r
j
,m
1
6= m
2
}
As can be seen, a link is an undirected edge be-
tween two mentions, and it can be equivalently
represented by a pair of mentions. Note that when
an entity consists of a single mention, its corefer-
ence link set is empty.
Let N
k
(i, j) (i 6= j) be key non-coreference
links formed between mentions in k
i
and those
in k
j
, and let N
r
(i, j) (i 6= j) be response non-
coreference links formed between mentions in r
i
and those in r
j
, respectively:
N
k
(i, j) = {(m
1
,m
2
) : m
1
? k
i
,m
2
? k
j
}
N
r
(i, j) = {(m
1
,m
2
) : m
1
? r
i
,m
2
? r
j
}
Note that the non-coreference link set is empty
when all mentions are in the same entity.
We use the same letter and subscription with-
out the index in parentheses to denote the union of
sets, e.g.,
C
k
= ?
i
C
k
(i), N
k
= ?
i 6=j
N
k
(i, j)
C
r
= ?
j
C
r
(j), N
r
= ?
i6=j
N
r
(i, j)
We use T
k
= C
k
? N
k
and T
r
= C
r
? N
r
to
denote the total set of key links and total set of
response links, respectively. Clearly, C
k
and N
k
form a partition of T
k
since C
k
? N
k
= ?, T
k
=
C
k
?N
k
. Likewise, C
r
and N
r
form a partition of
T
r
.
We say that a key link l
1
? T
k
equals a response
link l
2
? T
r
if and only if the pair of mentions
from which the links are formed are identical. We
write l
1
= l
2
if two links are equal. It is easy to
see that the gold mention assumption?same set
of response mentions as the set of key mentions?
can be equivalently stated as T
k
= T
r
(this does
not necessarily mean that C
k
= C
r
or N
k
= N
r
).
We also use | ? | to denote the size of a set.
3 Original BLANC
BLANC-gold is adapted from Rand Index (Rand,
1971), a metric for clustering objects. Rand Index
is defined as the ratio between the number of cor-
rect within-cluster links plus the number of correct
cross-cluster links, and the total number of links.
When T
k
= T
r
, Rand Index can be applied di-
rectly since coreference resolution reduces to a
clustering problem where mentions are partitioned
into clusters (entities):
Rand Index =
|C
k
? C
r
|+ |N
k
?N
r
|
1
2
(
|T
k
|(|T
k
| ? 1)
)
(1)
In practice, though, the simple-minded adoption
of Rand Index is not satisfactory since the number
of non-coreference links often overwhelms that of
coreference links (Recasens and Hovy, 2011), or,
|N
k
|  |C
k
| and |N
r
|  |C
r
|. Rand Index, if
used without modification, would not be sensitive
to changes of coreference links.
BLANC-gold solves this problem by averaging
the F-measure computed over coreference links
and the F-measure over non-coreference links.
Using the notations in Section 2, the recall, pre-
cision, and F-measure on coreference links are:
R
(g)
c
=
|C
k
? C
r
|
|C
k
? C
r
|+ |C
k
?N
r
|
(2)
P
(g)
c
=
|C
k
? C
r
|
|C
r
? C
k
|+ |C
r
?N
k
|
(3)
F
(g)
c
=
2R
(g)
c
P
(g)
c
R
(g)
c
+ P
(g)
c
; (4)
Similarly, the recall, precision, and F-measure on
non-coreference links are computed as:
R
(g)
n
=
|N
k
?N
r
|
|N
k
? C
r
|+ |N
k
?N
r
|
(5)
P
(g)
n
=
|N
k
?N
r
|
|N
r
? C
k
|+ |N
r
?N
k
|
(6)
F
(g)
n
=
2R
(g)
n
P
(g)
n
R
(g)
n
+ P
(g)
n
. (7)
25
Finally, the BLANC-gold metric is the arithmetic
average of F
(g)
c
and F
(g)
n
:
BLANC
(g)
=
F
(g)
c
+ F
(g)
n
2
. (8)
Superscript
g
in these equations highlights the fact
that they are meant for coreference systems with
gold mentions.
Eqn. (8) indicates that BLANC-gold assigns
equal weight to F
(g)
c
, the F-measure from coref-
erence links, and F
(g)
n
, the F-measure from non-
coreference links. This avoids the problem that
|N
k
|  |C
k
| and |N
r
|  |C
r
|, should the original
Rand Index be used.
In Eqn. (2) - (3) and Eqn. (5) - (6), denominators
are written as a sum of disjoint subsets so they can
be related to the contingency table in (Recasens
and Hovy, 2011). Under the assumption that T
k
=
T
r
, it is clear that C
k
= (C
k
? C
r
) ? (C
k
?N
r
),
C
r
= (C
k
? C
r
) ? (N
k
? C
r
), and so on.
4 BLANC for Imperfect Response
Mentions
Under the assumption that the key and response
mention sets are identical (which implies that
T
k
= T
r
), Equations (2) to (7) make sense. For
example, R
c
is the ratio of the number of correct
coreference links over the number of key corefer-
ence links; P
c
is the ratio of the number of cor-
rect coreference links over the number of response
coreference links, and so on.
However, when response mentions are not iden-
tical to key mentions, a key coreference link may
not appear in either C
r
or N
r
, so Equations (2) to
(7) cannot be applied directly to systems with im-
perfect mentions. For instance, if the key entities
are {a,b,c} {d,e}; and the response entities
are {b,c} {e,f,g}, then the key coreference
link (a,b) is not seen on the response side; sim-
ilarly, it is possible that a response link does not
appear on the key side either: (c,f) and (f,g)
are not in the key in the above example.
To account for missing or spurious links, we ob-
serve that
? C
k
\ T
r
are key coreference links missing in
the response;
? N
k
\ T
r
are key non-coreference links miss-
ing in the response;
? C
r
\ T
k
are response coreference links miss-
ing in the key;
? N
r
\ T
k
are response non-coreference links
missing in the key,
and we propose to extend the coreference F-
measure and non-coreference F-measure as fol-
lows. Coreference recall, precision and F-measure
are changed to:
R
c
=
|C
k
? C
r
|
|C
k
? C
r
|+ |C
k
?N
r
|+ |C
k
\ T
r
|
(9)
P
c
=
|C
k
? C
r
|
|C
r
? C
k
|+ |C
r
?N
k
|+ |C
r
\ T
k
|
(10)
F
c
=
2R
c
P
c
R
c
+ P
c
(11)
Non-coreference recall, precision and F-measure
are changed to:
R
n
=
|N
k
?N
r
|
|N
k
? C
r
|+ |N
k
?N
r
|+ |N
k
\ T
r
|
(12)
P
n
=
|N
k
?N
r
|
|N
r
? C
k
|+ |N
r
?N
k
|+ |N
r
\ T
k
|
(13)
F
n
=
2R
n
P
n
R
n
+ P
n
. (14)
The proposed BLANC continues to be the arith-
metic average of F
c
and F
n
:
BLANC =
F
c
+ F
n
2
. (15)
We observe that the definition of the proposed
BLANC, Equ. (9)-(14) subsume the BLANC-
gold (2) to (7) due to the following proposition:
If T
k
= T
r
, then BLANC = BLANC
(g)
.
Proof. We only need to show that R
c
= R
(g)
c
,
P
c
= P
(g)
c
, R
n
= R
(g)
n
, and P
n
= P
(g)
n
. We prove
the first one (the other proofs are similar and elided
due to space limitations). Since T
k
= T
r
and
C
k
? T
k
, we have C
k
? T
r
; thus C
k
\T
r
= ?, and
|C
k
? T
r
| = 0. This establishes that R
c
= R
(g)
c
.
Indeed, since C
k
is a union of three disjoint sub-
sets: C
k
= (C
k
? C
r
) ? (C
k
? N
r
) ? (C
k
\ T
r
),
R
(g)
c
and R
c
can be unified as
|C
k
?C
r
|
|C
K
|
. Unification
for other component recalls and precisions can be
done similarly. So the final definition of BLANC
can be succinctly stated as:
R
c
=
|C
k
? C
r
|
|C
k
|
, P
c
=
|C
k
? C
r
|
|C
r
|
(16)
R
n
=
|N
k
?N
r
|
|N
k
|
, P
n
=
|N
k
?N
r
|
|N
r
|
(17)
F
c
=
2|C
k
? C
r
|
|C
k
|+ |C
r
|
, F
n
=
2|N
k
?N
r
|
|N
k
|+ |N
r
|
(18)
BLANC =
F
c
+ F
n
2
(19)
26
4.1 Boundary Cases
Care has to be taken when counts of the BLANC
definition are 0. This can happen when all key
(or response) mentions are in one cluster or are
all singletons: the former case will lead to N
k
= ?
(or N
r
= ?); the latter will lead to C
k
= ? (or
C
r
= ?). Observe that as long as |C
k
|+ |C
r
| > 0,
F
c
in (18) is well-defined; as long as |N
k
|+|N
r
| >
0, F
n
in (18) is well-defined. So we only need to
augment the BLANC definition for the following
cases:
(1) If C
k
= C
r
= ? and N
k
= N
r
= ?, then
BLANC = I(M
k
= M
r
), where I(?) is an in-
dicator function whose value is 1 if its argument
is true, and 0 otherwise. M
k
and M
r
are the key
and response mention set. This can happen when a
document has no more than one mention and there
is no link.
(2) If C
k
= C
r
= ? and |N
k
| + |N
r
| > 0, then
BLANC = F
n
. This is the case where the key
and response side has only entities consisting of
singleton mentions. Since there is no coreference
link, BLANC reduces to the non-coreference F-
measure F
n
.
(3) If N
k
= N
r
= ? and |C
k
| + |C
r
| > 0, then
BLANC = F
c
. This is the case where all mentions
in the key and response are in one entity. Since
there is no non-coreference link, BLANC reduces
to the coreference F-measure F
c
.
4.2 Toy Examples
We walk through a few examples and show how
BLANC is calculated in detail. In all the examples
below, each lower-case letter represents a mention;
mentions in an entity are closed in {}; two letters
in () represent a link.
Example 1. Key entities are {abc} and {d}; re-
sponse entities are {bc} and {de}. Obviously,
C
k
= {(ab), (bc), (ac)};
N
k
= {(ad), (bd), (cd)};
C
r
= {(bc), (de)};
N
r
= {(bd), (be), (cd), (ce)}.
Therefore, C
k
? C
r
= {(bc)}, N
k
? N
r
=
{(bd), (cd)}, and R
c
=
1
3
, P
c
=
1
2
, F
c
=
2
5
; R
n
=
2
3
, P
n
=
2
4
, F
n
=
4
7
. Finally, BLANC =
17
35
.
Example 2. Key entity is {a}; response entity
is {b}. This is boundary case (1): BLANC = 0.
Example 3. Key entities are {a}{b}{c}; re-
sponse entities are {a}{b}{d}. This is boundary
case (2): there are no coreference links. Since
N
k
= {(ab), (bc), (ca)},
Participant R P BLANC
lee 50.23 49.28 48.84
sapena 40.68 49.05 44.47
nugues 47.83 44.22 45.95
chang 44.71 47.48 45.49
stoyanov 49.37 29.80 34.58
santos 46.74 37.33 41.33
song 36.88 39.69 30.92
sobha 35.42 39.56 36.31
yang 47.95 29.12 36.09
charton 42.32 31.54 35.65
hao 45.41 32.75 36.98
zhou 29.93 45.58 34.95
kobdani 32.29 33.01 32.57
xinxin 36.83 34.39 35.02
kummerfeld 34.84 29.53 30.98
zhang 30.10 43.96 35.71
zhekova 26.40 15.32 15.37
irwin 3.62 28.28 6.28
Table 1: The proposed BLANC scores of the
CoNLL-2011 shared task participants.
N
r
= {(ab), (bd), (ad)},
we have
N
k
?N
r
= {(ab)}, and R
n
=
1
3
, P
n
=
1
3
.
So BLANC = F
n
=
1
3
.
Example 4. Key entity is {abc}; response entity
is {bc}. This is boundary case (3): there are no
non-coreference links. Since
C
k
= {(ab), (bc), (ca)}, and C
r
= {(bc)},
we have
C
k
? C
r
= {(bc)}, and R
c
=
1
3
, P
c
= 1,
So BLANC = F
c
=
2
4
=
1
2
.
5 Results
5.1 CoNLL-2011/12
We have updated the publicly available CoNLL
coreference scorer
1
with the proposed BLANC,
and used it to compute the proposed BLANC
scores for all the CoNLL 2011 (Pradhan et al,
2011) and 2012 (Pradhan et al, 2012) participants
in the official track, where participants had to au-
tomatically predict the mentions. Tables 1 and 2
report the updated results.
2
5.2 Correlation with Other Measures
Figure 1 shows how the proposed BLANC mea-
sure works when compared with existing met-
rics such as MUC, B-cubed and CEAF, us-
ing the BLANC and F1 scores. The proposed
BLANC is highly positively correlated with the
1
http://code.google.com/p/reference-coreference-scorers
2
The order is kept the same as in Pradhan et al (2011) and
Pradhan et al (2012) for easy comparison.
27
Participant R P BLANC
Language: Arabic
fernandes 33.43 44.66 37.99
bjorkelund 32.65 45.47 37.93
uryupina 31.62 35.26 33.02
stamborg 32.59 36.92 34.50
chen 31.81 31.52 30.82
zhekova 11.04 62.58 18.51
li 4.60 56.63 8.42
Language: English
fernandes 54.91 63.66 58.75
martschat 52.00 58.84 55.04
bjorkelund 52.01 59.55 55.42
chang 52.85 55.03 53.86
chen 50.52 56.82 52.87
chunyang 51.19 55.47 52.65
stamborg 54.39 54.88 54.42
yuan 50.58 54.29 52.11
xu 45.99 54.59 46.47
shou 49.55 52.46 50.44
uryupina 44.15 48.89 46.04
songyang 40.60 50.85 45.10
zhekova 41.46 33.13 34.80
xinxin 44.39 32.79 36.54
li 25.17 52.96 31.85
Language: Chinese
chen 48.45 62.44 54.10
yuan 53.15 40.75 43.20
bjorkelund 47.58 45.93 44.22
xu 44.11 36.45 38.45
fernandes 42.36 61.72 49.63
stamborg 39.60 55.12 45.89
uryupina 33.44 56.01 41.88
martschat 27.24 62.33 37.89
chunyang 37.43 36.18 36.77
xinxin 36.46 39.79 37.85
li 21.61 62.94 30.37
chang 18.74 40.76 25.68
zhekova 21.50 37.18 22.89
Table 2: The proposed BLANC scores of the
CoNLL-2012 shared task participants.
R P F1
MUC 0.975 0.844 0.935
B-cubed 0.981 0.942 0.966
CEAF-m 0.941 0.923 0.966
CEAF-e 0.797 0.781 0.919
Table 3: Pearson?s r correlation coefficients be-
tween the proposed BLANC and the other coref-
erence measures based on the CoNLL 2011/2012
results. All p-values are significant at < 0.001.
l
lll
l
l
l
llll lll
l
l
l
l
ll
l
l
lll
l
l l
llllll
lll
ll l
l
lll
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
MUC
BLA
NC
l
lll
l
l
l
lllllll
l
l
l
l
ll
l
lllll
l
ll
llllll
ll
lll
ll
l
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
B?cubed
BLA
NC
l
lll
l
l
l
lllllll
l
l
l
l
ll
l
lllll
l
ll
lllllll
ll
lll
ll
ll
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
CEAF?m
BLA
NC
l
ll
l
l
l
llll lll l
l
l
l
l
ll
l
llllll
l
ll
llllllll
ll
l
l
ll
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
CEAF?e
BLA
NC
Figure 1: Correlation plot between the proposed
BLANC and the other measures based on the
CoNLL 2011/2012 results. All values are F1
scores.
other measures along R, P and F1 (Table 3),
showing that BLANC is able to capture most
entity-based similarities measured by B-cubed and
CEAF. However, the CoNLL data sets come from
OntoNotes (Hovy et al, 2006), where singleton
entities are not annotated, and BLANC has a wider
dynamic range on data sets with singletons (Re-
casens and Hovy, 2011). So the correlations will
likely be lower on data sets with singleton entities.
6 Conclusion
The original BLANC-gold (Recasens and Hovy,
2011) requires that system mentions be identical
to gold mentions, which limits the metric?s utility
since detected system mentions often have missing
key mentions or spurious mentions. The proposed
BLANC is free from this assumption, and we
have shown that it subsumes the original BLANC-
gold. Since BLANC works on imperfect system
mentions, we have used it to score the CoNLL
2011 and 2012 coreference systems. The BLANC
scores show strong correlation with existing met-
rics, especially B-cubed and CEAF-m.
Acknowledgments
We would like to thank the three anonymous re-
viewers for their invaluable suggestions for im-
proving the paper. This work was partially sup-
ported by grants R01LM10090 from the National
Library of Medicine.
28
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the Linguistic Coreference Workshop at The First In-
ternational Conference on Language Resources and
Evaluation (LREC?98), pages 563?566.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages
57?60, New York City, USA, June. Association for
Computational Linguistics.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proc. of Human Language
Technology (HLT)/Empirical Methods in Natural
Language Processing (EMNLP).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning: Shared Task, pages 1?
27, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task, pages
1?40, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
W. M. Rand. 1971. Objective criteria for the evalua-
tion of clustering methods. Journal of the American
Statistical Association, 66(336):846?850.
M. Recasens and E. Hovy. 2011. BLANC: Implement-
ing the Rand index for coreference evaluation. Nat-
ural Language Engineering, 17:485?510, 10.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, , and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In In Proc. of MUC6, pages 45?52.
29
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30?35,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Scoring Coreference Partitions of Predicted Mentions:
A Reference Implementation
Sameer Pradhan
1
, Xiaoqiang Luo
2
, Marta Recasens
3
,
Eduard Hovy
4
, Vincent Ng
5
and Michael Strube
6
1
Harvard Medical School, Boston, MA,
2
Google Inc., New York, NY
3
Google Inc., Mountain View, CA,
4
Carnegie Mellon University, Pittsburgh, PA
5
HLTRI, University of Texas at Dallas, Richardson, TX,
6
HITS, Heidelberg, Germany
sameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com,
hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.org
Abstract
The definitions of two coreference scoring
metrics?B
3
and CEAF?are underspeci-
fied with respect to predicted, as opposed
to key (or gold) mentions. Several varia-
tions have been proposed that manipulate
either, or both, the key and predicted men-
tions in order to get a one-to-one mapping.
On the other hand, the metric BLANC was,
until recently, limited to scoring partitions
of key mentions. In this paper, we (i) ar-
gue that mention manipulation for scoring
predicted mentions is unnecessary, and po-
tentially harmful as it could produce unin-
tuitive results; (ii) illustrate the application
of all these measures to scoring predicted
mentions; (iii) make available an open-
source, thoroughly-tested reference imple-
mentation of the main coreference eval-
uation measures; and (iv) rescore the re-
sults of the CoNLL-2011/2012 shared task
systems with this implementation. This
will help the community accurately mea-
sure and compare new end-to-end corefer-
ence resolution algorithms.
1 Introduction
Coreference resolution is a key task in natural
language processing (Jurafsky and Martin, 2008)
aiming to detect the referential expressions (men-
tions) in a text that point to the same entity.
Roughly over the past two decades, research in
coreference (for the English language) had been
plagued by individually crafted evaluations based
on two central corpora?MUC (Hirschman and
Chinchor, 1997; Chinchor and Sundheim, 2003;
Chinchor, 2001) and ACE (Doddington et al,
2004). Experimental parameters ranged from us-
ing perfect (gold, or key) mentions as input for
purely testing the quality of the entity linking al-
gorithm, to an end-to-end evaluation where pre-
dicted mentions are used. Given the range of
evaluation parameters and disparity between the
annotation standards for the two corpora, it was
very hard to grasp the state of the art for the
task of coreference. This has been expounded in
Stoyanov et al (2009). The activity in this sub-
field of NLP can be gauged by: (i) the contin-
ual addition of corpora manually annotated for
coreference?The OntoNotes corpus (Pradhan et
al., 2007; Weischedel et al, 2011) in the general
domain, as well as the i2b2 (Uzuner et al, 2012)
and THYME (Styler et al, 2014) corpora in the
clinical domain would be a few examples of such
emerging corpora; and (ii) ongoing proposals for
refining the existing metrics to make them more
informative (Holen, 2013; Chen and Ng, 2013).
The CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus (Prad-
han et al, 2011; Pradhan et al, 2012) were an
attempt to standardize the evaluation settings by
providing a benchmark annotated corpus, scorer,
and state-of-the-art system results that would al-
low future systems to compare against them. Fol-
lowing the timely emphasis on end-to-end evalu-
ation, the official track used predicted mentions
and measured performance using five coreference
measures: MUC (Vilain et al, 1995), B
3
(Bagga
and Baldwin, 1998), CEAF
e
(Luo, 2005), CEAF
m
(Luo, 2005), and BLANC (Recasens and Hovy,
2011). The arithmetic mean of the first three was
the task?s final score.
An unfortunate setback to these evaluations had
its root in three issues: (i) the multiple variations
of two of the scoring metrics?B
3
and CEAF?
used by the community to handle predicted men-
tions; (ii) a buggy implementation of the Cai and
Strube (2010) proposal that tried to reconcile these
variations; and (iii) the erroneous computation of
30
the BLANC metric for partitions of predicted men-
tions. Different interpretations as to how to com-
pute B
3
and CEAF scores for coreference systems
when predicted mentions do not perfectly align
with key mentions?which is usually the case?
led to variations of these metrics that manipulate
the gold standard and system output in order to
get a one-to-one mention mapping (Stoyanov et
al., 2009; Cai and Strube, 2010). Some of these
variations arguably produce rather unintuitive re-
sults, while others are not faithful to the original
measures.
In this paper, we address the issues in scor-
ing coreference partitions of predicted mentions.
Specifically, we justify our decision to go back
to the original scoring algorithms by arguing that
manipulation of key or predicted mentions is un-
necessary and could in fact produce unintuitive re-
sults. We demonstrate the use of our recent ex-
tension of BLANC that can seamlessly handle pre-
dicted mentions (Luo et al, 2014). We make avail-
able an open-source, thoroughly-tested reference
implementation of the main coreference evalua-
tion measures that do not involve mention manip-
ulation and is faithful to the original intentions of
the proposers of these metrics. We republish the
CoNLL-2011/2012 results based on this scorer, so
that future systems can use it for evaluation and
have the CoNLL results available for comparison.
The rest of the paper is organized as follows.
Section 2 provides an overview of the variations
of the existing measures. We present our newly
updated coreference scoring package in Section 3
together with the rescored CoNLL-2011/2012 out-
puts. Section 4 walks through a scoring example
for all the measures, and we conclude in Section 5.
2 Variations of Scoring Measures
Two commonly used coreference scoring metrics
?B
3
and CEAF?are underspecified in their ap-
plication for scoring predicted, as opposed to key
mentions. The examples in the papers describing
these metrics assume perfect mentions where pre-
dicted mentions are the same set of mentions as
key mentions. The lack of accompanying refer-
ence implementation for these metrics by its pro-
posers made it harder to fill the gaps in the speci-
fication. Subsequently, different interpretations of
how one can evaluate coreference systems when
predicted mentions do not perfectly align with key
mentions led to variations of these metrics that ma-
nipulate the gold and/or predicted mentions (Stoy-
anov et al, 2009; Cai and Strube, 2010). All these
variations attempted to generate a one-to-one map-
ping between the key and predicted mentions, as-
suming that the original measures cannot be ap-
plied to predicted mentions. Below we first pro-
vide an overview of these variations and then dis-
cuss the unnecessity of this assumption.
Coining the term twinless mentions for those
mentions that are either spurious or missing from
the predicted mention set, Stoyanov et al (2009)
proposed two variations to B
3
? B
3
all
and B
3
0
?to
handle them. In the first variation, all predicted
twinless mentions are retained, whereas the lat-
ter discards them and penalizes recall for twin-
less predicted mentions. Rahman and Ng (2009)
proposed another variation by removing ?all and
only those twinless system mentions that are sin-
gletons before applying B
3
and CEAF.? Follow-
ing upon this line of research, Cai and Strube
(2010) proposed a unified solution for both B
3
and
CEAF
m
, leaving the question of handling CEAF
e
as future work because ?it produces unintuitive
results.? The essence of their solution involves
manipulating twinless key and predicted mentions
by adding them either from the predicted parti-
tion to the key partition or vice versa, depend-
ing on whether one is computing precision or re-
call. The Cai and Strube (2010) variation was used
by the CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus, and
by the i2b2 2011 shared task on coreference res-
olution using an assortment of clinical notes cor-
pora (Uzuner et al, 2012).
1
It was later identified
by Recasens et al (2013) that there was a bug in
the implementation of this variation in the scorer
used for the CoNLL-2011/2012 tasks. We have
not tested the correctness of this variation in the
scoring package used for the i2b2 shared task.
However, it turns out that the CEAF metric (Luo,
2005) was always intended to work seamlessly on
predicted mentions, and so has been the case with
the B
3
metric.
2
In a latter paper, Rahman and Ng
(2011) correctly state that ?CEAF can compare par-
titions with twinless mentions without any modifi-
cation.? We will look at this further in Section 4.3.
We argue that manipulations of key and re-
sponse mentions/entities, as is done in the exist-
ing B
3
variations, not only confound the evalu-
ation process, but are also subject to abuse and
can seriously jeopardize the fidelity of the evalu-
1
Personal communication with Andreea Bodnari, and
contents of the i2b2 scorer code.
2
Personal communication with Breck Baldwin.
31
ation. Given space constraints we use an exam-
ple worked out in Cai and Strube (2010). Let
the key contain an entity with mentions {a, b, c}
and the prediction contain an entity with mentions
{a, b, d}. As detailed in Cai and Strube (2010,
p. 29-30, Tables 1?3), B
3
0
assigns a perfect pre-
cision of 1.00 which is unintuitive as the system
has wrongly predicted a mention d as belonging to
the entity. For the same prediction, B
3
all
assigns a
precision of 0.556. But, if the prediction contains
two entities {a, b, d} and {c} (i.e., the mention c
is added as a spurious singleton), then B
3
all
preci-
sion increases to 0.667 which is counter-intuitive
as it does not penalize the fact that c is erroneously
placed in its own entity. The version illustrated in
Section 4.2, which is devoid of any mention ma-
nipulations, gives a precision of 0.444 in the first
scenario and the precision drops to 0.333 in the
second scenario with the addition of a spurious
singleton entity {c}. This is a more intuitive be-
havior.
Contrary to both B
3
and CEAF, the BLANC mea-
sure (Recasens and Hovy, 2011) was never de-
signed to handle predicted mentions. However, the
implementation used for the SemEval-2010 shared
task as well as the one for the CoNLL-2011/2012
shared tasks accepted predicted mentions as input,
producing undefined results. In Luo et al (2014)
we have extended the BLANC metric to deal with
predicted mentions
3 Reference Implementation
Given the potential unintuitive outcomes of men-
tion manipulation and the misunderstanding that
the original measures could not handle twinless
predicted mentions (Section 2), we redesigned the
CoNLL scorer. The new implementation:
? is faithful to the original measures;
? removes any prior mention manipulation,
which might depend on specific annotation
guidelines among other problems;
? has been thoroughly tested to ensure that it
gives the expected results according to the
original papers, and all test cases are included
as part of the release;
? is free of the reported bugs that the CoNLL
scorer (v4) suffered (Recasens et al, 2013);
? includes the extension of BLANC to handle
predicted mentions (Luo et al, 2014).
This is the open source scoring package
3
that
we present as a reference implementation for the
3
http://code.google.com/p/reference-coreference-scorers/
SYSTEM MD MUC B
3
CEAF BLANC CONLL
m e AVERAGE
F
1
F
1
1
F
2
1
F
1
F
3
1
CoNLL-2011; English
lee 70.7 59.6 48.9 53.0 46.1 48.8 51.5
sapena 68.4 59.5 46.5 51.3 44.0 44.5 50.0
nugues 69.0 58.6 45.0 48.4 40.0 46.0 47.9
chang 64.9 57.2 46.0 50.7 40.0 45.5 47.7
stoyanov 67.8 58.4 40.1 43.3 36.9 34.6 45.1
santos 65.5 56.7 42.9 45.1 35.6 41.3 45.0
song 67.3 60.0 41.4 41.0 33.1 30.9 44.8
sobha 64.8 50.5 39.5 44.2 39.4 36.3 43.1
yang 63.9 52.3 39.4 43.2 35.5 36.1 42.4
charton 64.3 52.5 38.0 42.6 34.5 35.7 41.6
hao 64.3 54.5 37.7 41.9 31.6 37.0 41.3
zhou 62.3 49.0 37.0 40.6 35.0 35.0 40.3
kobdani 61.0 53.5 34.8 38.1 34.1 32.6 38.7
xinxin 61.9 46.6 34.9 37.7 31.7 35.0 37.7
kummerfeld 62.7 42.7 34.2 38.8 35.5 31.0 37.5
zhang 61.1 47.9 34.4 37.8 29.2 35.7 37.2
zhekova 48.3 24.1 23.7 23.4 20.5 15.4 22.8
irwin 26.7 20.0 11.7 18.5 14.7 6.3 15.5
CoNLL-2012; English
fernandes 77.7 70.5 57.6 61.4 53.9 58.8 60.7
martschat 75.2 67.0 54.6 58.8 51.5 55.0 57.7
bjorkelund 75.4 67.6 54.5 58.2 50.2 55.4 57.4
chang 74.3 66.4 53.0 57.1 48.9 53.9 56.1
chen 73.8 63.7 51.8 55.8 48.1 52.9 54.5
chunyang 73.7 63.8 51.2 55.1 47.6 52.7 54.2
stamborg 73.9 65.1 51.7 55.1 46.6 54.4 54.2
yuan 72.5 62.6 50.1 54.5 46.0 52.1 52.9
xu 72.0 66.2 50.3 51.3 41.3 46.5 52.6
shou 73.7 62.9 49.4 53.2 46.7 50.4 53.0
uryupina 70.9 60.9 46.2 49.3 42.9 46.0 50.0
songyang 68.8 59.8 45.9 49.6 42.4 45.1 49.4
zhekova 67.1 53.5 35.7 39.7 32.2 34.8 40.5
xinxin 62.8 48.3 35.7 38.0 31.9 36.5 38.6
li 59.9 50.8 32.3 36.3 25.2 31.9 36.1
CoNLL-2012; Chinese
chen 71.6 62.2 55.7 60.0 55.0 54.1 57.6
yuan 68.2 60.3 52.4 55.8 50.2 43.2 54.3
bjorkelund 66.4 58.6 51.1 54.2 47.6 44.2 52.5
xu 65.2 58.1 49.5 51.9 46.6 38.5 51.4
fernandes 66.1 60.3 49.6 54.4 44.5 49.6 51.5
stamborg 64.0 57.8 47.4 51.6 41.9 45.9 49.0
uryupina 59.0 53.0 41.7 46.9 37.6 41.9 44.1
martschat 58.6 52.4 40.8 46.0 38.2 37.9 43.8
chunyang 61.6 49.8 39.6 44.2 37.3 36.8 42.2
xinxin 55.9 48.1 38.8 42.9 34.5 37.9 40.5
li 51.5 44.7 31.5 36.7 25.3 30.4 33.8
chang 47.6 37.9 28.8 36.1 29.6 25.7 32.1
zhekova 47.3 40.6 28.1 31.4 21.2 22.9 30.0
CoNLL-2012; Arabic
fernandes 64.8 46.5 42.5 49.2 46.5 38.0 45.2
bjorkelund 60.6 47.8 41.6 46.7 41.2 37.9 43.5
uryupina 55.4 41.5 36.1 41.4 35.0 33.0 37.5
stamborg 59.5 41.2 35.9 40.0 32.9 34.5 36.7
chen 59.8 39.0 32.1 34.7 26.0 30.8 32.4
zhekova 41.0 29.9 22.7 31.1 25.9 18.5 26.2
li 29.7 18.1 13.1 21.0 17.3 8.4 16.2
Table 1: Performance on the official, closed track
in percentages using all predicted information for
the CoNLL-2011 and 2012 shared tasks.
community to use. It is written in perl and stems
from the scorer that was initially used for the
SemEval-2010 shared task (Recasens et al, 2010)
and later modified for the CoNLL-2011/2012
shared tasks.
4
Partitioning detected mentions into entities (or
equivalence classes) typically comprises two dis-
tinct tasks: (i) mention detection; and (ii) coref-
erence resolution. A typical two-step coreference
algorithm uses mentions generated by the best
4
We would like to thank Emili Sapena for writing the first
version of the scoring package.
32
a     b
c
de fg
h
a     bc
de
hi i
f    g f    g
h i
cd
a     b
Solid: KeyDashed: Response Solid: KeyDashed: partition wrt Response Solid: Partition wrt KeyDashed: Response
Figure 1: Example key and response entities along
with the partitions for computing the MUC score.
possible mention detection algorithm as input to
the coreference algorithm. Therefore, ideally one
would want to score the two steps independently
of each other. A peculiarity of the OntoNotes
corpus is that singleton referential mentions are
not annotated, thereby preventing the computation
of a mention detection score independently of the
coreference resolution score. In corpora where all
referential mentions (including singletons) are an-
notated, the mention detection score generated by
this implementation is independent of the corefer-
ence resolution score.
We used this reference implementation to
rescore the CoNLL-2011/2012 system outputs for
the official task to enable future comparisons with
these benchmarks. The new CoNLL-2011/2012
results are in Table 1. We found that the over-
all system ranking remained largely unchanged for
both shared tasks, except for some of the lower
ranking systems that changed one or two places.
However, there was a considerable drop in the
magnitude of all B
3
scores owing to the combi-
nation of two things: (i) mention manipulation, as
proposed by Cai and Strube (2010), adds single-
tons to account for twinless mentions; and (ii) the
B
3
metric allows an entity to be used more than
once as pointed out by Luo (2005). This resulted
in a drop in the CoNLL averages (B
3
is one of the
three measures that make the average).
4 An Illustrative Example
This section walks through the process of com-
puting each of the commonly used metrics for
an example where the set of predicted mentions
has some missing key mentions and some spu-
rious mentions. While the mathematical formu-
lae for these metrics can be found in the original
papers (Vilain et al, 1995; Bagga and Baldwin,
1998; Luo, 2005), many misunderstandings dis-
cussed in Section 2 are due to the fact that these
papers lack an example showing how a metric is
computed on predicted mentions. A concrete ex-
ample goes a long way to prevent similar misun-
derstandings in the future. The example is adapted
from Vilain et al (1995) with some slight modifi-
cations so that the total number of mentions in the
key is different from the number of mentions in
the prediction. The key (K) contains two entities
with mentions {a, b, c} and {d, e, f, g} and the re-
sponse (R) contains three entities with mentions
{a, b}; {c, d} and {f, g, h, i}:
K =
K
1
? ?? ?
{a, b, c}
K
2
? ?? ?
{d, e, f, g} (1)
R =
R
1
? ?? ?
{a, b}
R
2
? ?? ?
{c, d}
R
3
? ?? ?
{f, g, h, i}. (2)
Mention e is missing from the response, and men-
tions h and i are spurious in the response. The fol-
lowing sections use R to denote recall and P for
precision.
4.1 MUC
The main step in the MUC scoring is creating the
partitions with respect to the key and response re-
spectively, as shown in Figure 1. Once we have
the partitions, then we compute the MUC score by:
R =
?
N
k
i=1
(|K
i
| ? |p(K
i
)|)
?
N
k
i=1
(|K
i
| ? 1)
=
(3? 2) + (4? 3)
(3? 1) + (4? 1)
= 0.40
P =
?
N
r
i=1
(|R
i
| ? |p
?
(R
i
)|)
?
N
r
i=1
(|R
i
| ? 1)
=
(2? 1) + (2? 2) + (4? 3)
(2? 1) + (2? 1) + (4? 1)
= 0.40,
where K
i
is the i
th
key entity and p(K
i
) is the
set of partitions created by intersecting K
i
with
response entities (cf. the middle sub-figure in Fig-
ure 1); R
i
is the i
th
response entity and p
?
(R
i
) is
the set of partitions created by intersectingR
i
with
key entities (cf. the right-most sub-figure in Fig-
ure 1); and N
k
and N
r
are the number of key and
response entities, respectively.
The MUC F
1
score in this case is 0.40.
4.2 B
3
For computing B
3
recall, each key mention is as-
signed a credit equal to the ratio of the number of
correct mentions in the predicted entity contain-
ing the key mention to the size of the key entity to
which the mention belongs, and the recall is just
33
the sum of credits over all key mentions normal-
ized over the number of key mentions. B
3
preci-
sion is computed similarly, except switching the
role of key and response. Applied to the example:
R =
?
N
k
i=1
?
N
r
j=1
|K
i
?R
j
|
2
|K
i
|
?
N
k
i=1
|K
i
|
=
1
7
? (
2
2
3
+
1
2
3
+
1
2
4
+
2
2
4
) =
1
7
?
35
12
? 0.42
P =
?
N
k
i=1
?
N
r
j=1
|K
i
?R
j
|
2
|R
j
|
?
N
r
i=1
|R
j
|
=
1
8
? (
2
2
2
+
1
2
2
+
1
2
2
+
2
2
4
) =
1
8
?
4
1
= 0.50
Note that terms with 0 value are omitted. The B
3
F
1
score is 0.46.
4.3 CEAF
The first step in the CEAF computation is getting
the best scoring alignment between the key and
response entities. In this case the alignment is
straightforward. Entity R
1
aligns with K
1
and R
3
aligns with K
2
. R
2
remains unaligned.
CEAF
m
CEAF
m
recall is the number of aligned mentions
divided by the number of key mentions, and preci-
sion is the number of aligned mentions divided by
the number of response mentions:
R =
|K
1
? R
1
|+ |K
2
? R
3
|
|K
1
|+ |K
2
|
=
(2 + 2)
(3 + 4)
? 0.57
P =
|K
1
? R
1
|+ |K
2
? R
3
|
|R
1
|+ |R
2
|+ |R
3
|
=
(2 + 2)
(2 + 2 + 4)
= 0.50
The CEAF
m
F
1
score is 0.53.
CEAF
e
We use the same notation as in Luo (2005):
?
4
(K
i
, R
j
) to denote the similarity between a key
entity K
i
and a response entity R
j
. ?
4
(K
i
, R
j
) is
defined as:
?
4
(K
i
, R
j
) =
2? |K
i
? R
j
|
|K
i
|+ |R
j
|
.
CEAF
e
recall and precision, when applied to this
example, are:
R =
?
4
(K
1
, R
1
) + ?
4
(K
2
, R
3
)
N
k
=
(2?2)
(3+2)
+
(2?2)
(4+4)
2
= 0.65
P =
?
4
(K
1
, R
1
) + ?
4
(K
2
, R
3
)
N
r
=
(2?2)
(3+2)
+
(2?2)
(4+4)
3
? 0.43
The CEAF
e
F
1
score is 0.52.
4.4 BLANC
The BLANC metric illustrated here is the one in
our implementation which extends the original
BLANC (Recasens and Hovy, 2011) to predicted
mentions (Luo et al, 2014).
Let C
k
and C
r
be the set of coreference links
in the key and response respectively, and N
k
and
N
r
be the set of non-coreference links in the key
and response respectively. A link between a men-
tion pair m and n is denoted by mn; then for the
example in Figure 1, we have
C
k
= {ab, ac, bc, de, df, dg, ef, eg, fg}
N
k
= {ad, ae, af, ag, bd, be, bf, bg, cd, ce, cf, cg}
C
r
= {ab, cd, fg, fh, fi, gh, gi, hi}
N
r
= {ac, ad, af, ag, ah, ai, bc, bd, bf, bg, bh, bi,
cf, cg, ch, ci, df, dg, dh, di}.
Recall and precision for coreference links are:
R
c
=
|C
k
? C
r
|
|C
k
|
=
2
9
? 0.22
P
c
=
|C
k
? C
r
|
|C
r
|
=
2
8
= 0.25
and the coreference F-measure, F
c
? 0.23. Sim-
ilarly, recall and precision for non-coreference
links are:
R
n
=
|N
k
?N
r
|
|N
k
|
=
8
12
? 0.67
P
n
=
|N
k
?N
r
|
|N
r
|
=
8
20
= 0.40,
and the non-coreference F-measure, F
n
= 0.50.
So the BLANC score is
F
c
+F
n
2
? 0.36.
5 Conclusion
We have cleared several misunderstandings about
coreference evaluation metrics, especially when a
response contains imperfect predicted mentions,
and have argued against mention manipulations
during coreference evaluation. These misunder-
standings are caused partially by the lack of il-
lustrative examples to show how a metric is com-
puted on predicted mentions not aligned perfectly
with key mentions. Therefore, we provide detailed
steps for computing all four metrics on a represen-
tative example. Furthermore, we have a reference
implementation of these metrics that has been rig-
orously tested and has been made available to the
public as open source software. We reported new
scores on the CoNLL 2011 and 2012 data sets,
which can serve as the benchmarks for future re-
search work.
Acknowledgments
This work was partially supported by grants
R01LM10090 from the National Library of
Medicine and IIS-1219142 from the National Sci-
ence Foundation.
34
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
LREC, pages 563?566.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In
Proceedings of SIGDIAL, pages 28?36.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Pro-
ceedings of the Sixth IJCNLP, pages 1366?1374,
Nagoya, Japan, October.
Nancy Chinchor and Beth Sundheim. 2003. Mes-
sage understanding conference (MUC) 6. In
LDC2003T13.
Nancy Chinchor. 2001. Message understanding con-
ference (MUC) 7. In LDC2001T02.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ACE) program-tasks, data, and evaluation. In
Proceedings of LREC.
Lynette Hirschman and Nancy Chinchor. 1997. Coref-
erence task definition (v3.0, 13 jul 97). In Proceed-
ings of the 7th Message Understanding Conference.
Gordana Ilic Holen. 2013. Critical reflections on
evaluation practices in coreference resolution. In
Proceedings of the NAACL-HLT Student Research
Workshop, pages 1?7, Atlanta, Georgia, June.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall. Second
Edition.
Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An extension of BLANC to
system mentions. In Proceedings of ACL, Balti-
more, Maryland, June.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT-EMNLP,
pages 25?32.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A Unified Rela-
tional Semantic Representation. International Jour-
nal of Semantic Computing, 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of CoNLL: Shared Task, pages 1?27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of CoNLL: Shared Task, pages 1?40.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of
EMNLP, pages 968?977.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of
ACL, pages 814?824.
Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand Index for coreference eval-
uation. Natural Language Engineering, 17(4):485?
510.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M. Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of SemEval,
pages 1?8.
Marta Recasens, Marie-Catherine de Marneffe, and
Chris Potts. 2013. The life and death of discourse
entities: Identifying singleton mentions. In Pro-
ceedings of NAACL-HLT, pages 627?633.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP, pages
656?664.
William F. Styler, Steven Bethard an Sean Finan,
Martha Palmer, Sameer Pradhan, Piet C de Groen,
Brad Erickson, Timothy Miller, Chen Lin, Guergana
Savova, and James Pustejovsky. 2014. Temporal
annotation in the clinical domain. Transactions of
Computational Linguistics, 2(April):143?154.
Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. Journal of
American Medical Informatics Association, 19(5),
September.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model theo-
retic coreference scoring scheme. In Proceedings of
the 6th Message Understanding Conference, pages
45?52.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Robert Belvin, Sameer Pradhan,
Lance Ramshaw, and Nianwen Xue. 2011.
OntoNotes: A large training corpus for enhanced
processing. In Joseph Olive, Caitlin Christian-
son, and John McCary, editors, Handbook of Natu-
ral Language Processing and Machine Translation:
DARPA Global Autonomous Language Exploitation.
Springer.
35
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 81?86,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Descending-Path Convolution Kernel for Syntactic Structures
Chen Lin
1
, Timothy Miller
1
, Alvin Kho
1
, Steven Bethard
2
,
Dmitriy Dligach
1
, Sameer Pradhan
1
and Guergana Savova
1
,
1
Children?s Hospital Boston Informatics Program and Harvard Medical School
{firstname.lastname}@childrens.harvard.edu
2
Department of Computer and Information Sciences, University of Alabama at Birmingham
bethard@cis.uab.edu
Abstract
Convolution tree kernels are an efficient
and effective method for comparing syntac-
tic structures in NLP methods. However,
current kernel methods such as subset tree
kernel and partial tree kernel understate the
similarity of very similar tree structures.
Although soft-matching approaches can im-
prove the similarity scores, they are corpus-
dependent and match relaxations may be
task-specific. We propose an alternative ap-
proach called descending path kernel which
gives intuitive similarity scores on compa-
rable structures. This method is evaluated
on two temporal relation extraction tasks
and demonstrates its advantage over rich
syntactic representations.
1 Introduction
Syntactic structure can provide useful features for
many natural language processing (NLP) tasks
such as semantic role labeling, coreference resolu-
tion, temporal relation discovery, and others. How-
ever, the choice of features to be extracted from a
tree for a given task is not always clear. Convolu-
tion kernels over syntactic trees (tree kernels) offer
a potential solution to this problem by providing
relatively efficient algorithms for computing sim-
ilarities between entire discrete structures. These
kernels use tree fragments as features and count
the number of common fragments as a measure of
similarity between any two trees.
However, conventional tree kernels are sensitive
to pattern variations. For example, two trees in Fig-
ure 1(a) sharing the same structure except for one
terminal symbol are deemed at most 67% similar
by the conventional tree kernel (PTK) (Moschitti,
2006). Yet one might expect a higher similarity
given their structural correspondence.
The similarity is further attenuated by trivial
structure changes such as the insertion of an ad-
jective in one of the trees in Figure 1(a), which
would reduce the similarity close to zero. Such
an abrupt attenuation would potentially propel a
model to memorize training instances rather than
generalize from trends, leading towards overfitting.
In this paper, we describe a new kernel over
syntactic trees that operates on descending paths
through the tree rather than production rules as
used in most existing methods. This representation
is reminiscent of Sampson?s (2000) leaf-ancestor
paths for scoring parse similarities, but here it is
generalized over all ancestor paths, not just those
from the root to a leaf. This approach assigns more
robust similarity scores (e.g., 78% similarity in the
above example) than other soft matching tree ker-
nels, is faster than the partial tree kernel (Moschitti,
2006), and is less ad hoc than the grammar-based
convolution kernel (Zhang et al, 2007).
2 Background
2.1 Syntax-based Tree Kernels
Syntax-based tree kernels quantify the similarity
between two constituent parses by counting their
common sub-structures. They differ in their defini-
tion of the sub-structures.
Collins and Duffy (2001) use a subset tree (SST)
representation for their sub-structures. In the SST
representation, a subtree is defined as a subgraph
with more than one node, in which only full pro-
duction rules are expanded. While this approach is
widely used and has been successful in many tasks,
the production rule-matching constraint may be un-
necessarily restrictive, giving zero credit to rules
that have only minor structural differences. For
example, the similarity score between the NPs in
Figure 1(b) would be zero since the production rule
is different (the overall similarity score is above-
zero because of matching pre-terminals).
The partial tree kernel (PTK) relaxes the defi-
nition of subtrees to allow partial production rule
81
a)
NP
DT
a
NN
cat
NP
DT
a
NN
dog
b)
NP
DT
a
NN
cat
NP
DT
a
JJ
fat
NN
cat
c)
S
ADVP
RB
here
NP
PRP
she
VP
VBZ
comes
S
NP
PRP
she
VP
VBZ
comes
ADVP
RB
here
Figure 1: Three example tree pairs.
matching (Moschitti, 2006). In the PTK, a subtree
may or may not expand any child in a production
rule, while maintaining the ordering of the child
nodes. Thus it generates a very large but sparse
feature space. To Figure 1(b), the PTK generates
fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a]
[NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among
others, for the second tree. This allows for partial
matching ? substructure (ii) ? while also generating
some fragments that violate grammatical intuitions.
Zhang et al (2007) address the restrictiveness
of SST by allowing soft matching of production
rules. They allow partial matching of optional
nodes based on the Treebank. For example, the
rule NP ? DT JJ NN indicates a noun phrase
consisting of a determiner, adjective, and common
noun. Zhang et al?s method designates the JJ as
optional, since the Treebank contains instances of
a reduced version of the rule without the JJ node
(NP ? DT NN ). They also allow node match-
ing among similar preterminals such as JJ, JJR, and
JJS, mapping them to one equivalence class.
Other relevant approaches are the spectrum tree
(SpT) (Kuboyama et al, 2007) and the route kernel
(RtT) (Aiolli et al, 2009). SpT uses a q-gram
? a sequence of connected vertices of length q ?
as their sub-structure. It observes grammar rules
by recording the orientation of edges: a?b?c is
different from a?b?c. RtT uses a set of routes as
basic structures, which observes grammar rules by
NP
DT
a
NN
cat
l=0: [NP],[DT],[NN]
l=1: [NP-DT],[NP-NN],
[DT-a],[NN-cat]
l=2: [NP-DT-a],[NP-NN-cat]
Figure 2: A parse tree (left) and its descending
paths according to Definition 1 (l - length).
recording the index of a neighbor node.
2.2 Temporal Relation Extraction
Among NLP tasks that use syntactic informa-
tion, temporal relation extraction has been draw-
ing growing attention because of its wide applica-
tions in multiple domains. As subtasks in Tem-
pEval 2007, 2010 and 2013, multiple systems
were built to create labeled links from events
to events/timestamps by using a variety of fea-
tures (Bethard and Martin, 2007; Llorens et al,
2010; Chambers, 2013). Many methods exist for
synthesizing syntactic information for temporal
relation extraction, and most use traditional tree
kernels with various feature representations. Mir-
roshandel et al (2009) used the path-enclosed tree
(PET) representation to represent syntactic informa-
tion for temporal relation extraction on the Time-
Bank (Pustejovsky et al, 2003) and the AQUAINT
TimeML corpus
1
. The PET is the smallest subtree
that contains both proposed arguments of a relation.
Hovy et al (2012) used bag tree structures to rep-
resent the bag of words (BOW) and bag of part of
speech tags (BOP) between the event and time in
addition to a set of baseline features, and improved
the temporal linking performance on the TempEval
2007 and Machine Reading corpora (Strassel et
al., 2010). Miller at al. (2013) used PET tree, bag
tree, and path tree (PT, which is similar to a PET
tree with the internal nodes removed) to represent
syntactic information and improved the temporal
relation discovery performance on THYME data
2
(Styler et al, 2014). In this paper, we also use
syntactic structure-enriched temporal relation dis-
covery as a vehicle to test our proposed kernel.
3 Methods
Here we decribe the Descending Path Kernel
(DPK).
1
http://www.timeml.org
2
http://thyme.healthnlp.org
82
Definition 1 (Descending Path): Let T be a
parse tree, v any non-terminal node in T , dv a
descendant of v, including terminals. A descending
path is the sequence of indexes of edges connecting
v and dv, denoted by [v ? ? ? ? ? dv]. The length l
of a descending path is the number of connecting
edges. When l = 0, a descending path is the non-
terminal node itself, [v]. Figure 2 illustrates a parse
tree and its descending paths of different lengths.
Suppose that all descending paths of a tree T are
indexed 1, ? ? ? , n, and path
i
(T ) is the frequency
of the i-th descending path in T . We represent T as
a vector of frequencies of all its descending paths:
?(T ) = (path
1
(T ), ? ? ? , path
n
(T )).
The similarity between any two trees T
1
and T
2
can be assessed via the dot product of their respec-
tive descending path frequency vector representa-
tions: K(T
1
, T
2
) = ??(T
1
),?(T
2
)?.
Compared with the previous tree kernels, our
descending path kernel has the following advan-
tages: 1) the sub-structures are simplified so that
they are more likely to be shared among trees,
and therefore the sparse feature issues of previous
kernels could be alleviated by this representation;
2) soft matching between two similar structures
(e.g., NP?DT JJ NN versus NP?DT NN) have
high similarity without reference to any corpus or
grammar rules;
Following Collins and Duffy (2001), we derive
a recursive algorithm to compute the dot product
of the descending path frequency vector represen-
tations of two trees T
1
and T
2
:
K(T
1
, T
2
) = ??(T
1
),?(T
2
)?
=
?
i
path
i
(T
1
) ? path
i
(T
2
)
=
?
n
1
?N
1
?
n
2
?N
2
?
i
I
path
i
(n
1
) ? I
path
i
(n
2
)
=
?
n
1
?N
1
n
2
?N
2
C(n
1
, n
2
)
(1)
where N
1
and N
2
are the sets of nodes in T
1
and
T
2
respectively, i indexes the set of possible paths,
I
path
i
(n) is an indicator function that is 1 iff the
descending path
i
is rooted at node n or 0 other-
wise. C(n
1
, n
2
) counts the number of common
descending paths rooted at nodes n
1
and n
2
:
C(n
1
, n
2
) =
?
i
I
path
i
(n
1
) ? I
path
i
(n
2
)
C(n
1
, n
2
) can be computed in polynomial time by
the following recursive rules:
Rule 1: If n
1
and n
2
have different labels (e.g.,
?DT? versus ?NN?), then C(n1, n2) = 0;
Rule 2: Else if n
1
and n
2
have the same labels
and are both pre-terminals (POS tags), then
C(n
1
, n
2
) = 1 +
{
1 if term(n
1
) = term(n
2
)
0 otherwise.
where term(n) is the terminal symbol under n;
Rule 3: Else if n
1
and n
2
have the same labels
and they are not both pre-terminals, then:
C(n
1
, n
2
) = 1 +
?
n
i
?children(n
1
)
n
j
?children(n
2
)
C(n
i
, n
j
)
where children(m) are the child nodes of m.
As in other tree kernel approaches (Collins and
Duffy, 2001; Moschitti, 2006), we use a discount
parameter ? to control for the disproportionately
large similarity values of large tree structures.
Therefore, Rule 2 becomes:
C(n
1
, n
2
) = 1 +
{
? if term(n
1
) = term(n
2
)
0 otherwise.
and Rule 3 becomes:
C(n
1
, n
2
) = 1 + ?
?
n
i
?children(n
1
)
n
j
?children(n
2
)
C(n
i
, n
j
)
Note that Eq. (1) is a convolution kernel under
the kernel closure properties described in Haus-
sler (1999). Rules 1-3 show the equivalence be-
tween the number of common descending paths
rooted at nodes n
1
and n
2
, and the number of
matching nodes below n
1
and n
2
.
In practice, there are many non-matching nodes,
and most matching nodes will have only a few
matching children, so the running time, as in SST,
will be approximated by the number of matching
nodes between trees.
3.1 Relationship with other kernels
For a given tree, DPK will generate significantly
fewer sub-structures than PTK, since it does not
consider all ordered permutations of a production
rule. Moreover, the fragments generated by DPK
are more likely to be shared among different trees.
For the number of corpus-wide fragments, it is
83
Kernel ID #Frag Sim N(Sim)
SST a 9 3 0.50
O
(
?|N
1
||N
2
|
)
b 15 2 0.25
c 63 7 0.20
DPK a 11 7 0.78
O
(
?
2
|N
1
||N
2
|
)
b 13 9 0.83
c 31 22 0.83
PTK a 20 10 0.67
O
(
?
3
|N
1
||N
2
|
)
b 36 15 0.65
c 127 34 0.42
Table 1: Comparison of the worst case computa-
tional complexicity (? - the maximum branching
factor) and kernel performance on the 3 examples
from Figure 1. #Frag is the number of fragments,
N(Sim) is the normalized similarity. Please see
the online supplementary note for detailed frag-
ments of example (a).
possible that DPK? SST? PTK. In Table 1, given
? = 1, we compare the performance of 3 kernels
on the three examples in Figure 1. Note that for
more complicated structures, i.e., examples b and
c, DPK generates fewer fragments than SST and
PTK, with more shared fragments among trees.
The complexity for all three kernels are at least
O
(
|N
1
||N
2
|
)
since they share the pairwise summa-
tion at the end of Equation 1. SST, due to its re-
quirement of exact production rule matching, only
takes one pass in the inner loop which adds a factor
of ? (the maximum branching factor of any pro-
duction rule). DPK does a pairwise summation
of children, which adds a factor of ?
2
to the com-
plexity. Finally, the efficient algorithm for PTK
is proved by Moschitti (2006) to contain a con-
stant factor of ?
3
. Table 1 orders the tree kernels
according by their listed complexity.
It may seem that the value of DPK is strictly in its
ability to evaluate all paths, which is not explicitly
accounted for by other kernels. However, another
view of the DPK is possible by thinking of it as
cheaply calculating rule production similarity by
taking advantage of relatively strict English word
ordering. Like SST and PTK, the DPK requires
the root category of two subtrees to be the same
for the similarity to be greater than zero. Unlike
SST and PTK, once the root category comparison
is successfully completed, DPK looks at all paths
that go through it and accumulates their similarity
scores independent of ordering ? in other words, it
will ignore the ordering of the children in its pro-
duction rule. This means, for example, that if the
rule production NP? NN JJ DT were ever found
in a tree, to DPK it would be indistinguishable from
the common production NP? DT JJ NN, despite
having inverted word order, and thus would have
a maximal similarity score. SST and PTK would
assign this pair a much lower score for having com-
pletely different ordering, but we suggest that cases
such as these are very rare due to the relatively
strict word ordering of English. In most cases, the
determiner of a noun phrase will be at the front, the
nouns will be at the end, and the adjectives in the
middle. So with small differences in production
rules (one or two adjectives, extra nominal modifier,
etc.) the PTK will capture similarity by compar-
ing every possible partial rule completion, but the
DPK can obtain higher and faster scores by just
comparing one child at a time because the ordering
is constrained by the language. This analysis does
lead to a hypothesis for the general viability of the
DPK, suggesting that in languages with freer word
order it may give inflated scores to structures that
are syntactically dissimilar if they have the same
constituent components in different order.
Formally, Moschitti (2006) showed that SST is
a special case of PTK when only the longest child
sequence from each tree is considered. On the other
end of the spectrum, DPK is a special case of PTK
where the similarity between rules only considers
child subsequences of length one.
4 Evaluation
We applied DPK to two published temporal relation
extraction systems: (Miller et al, 2013) in the
clinical domain and Cleartk-TimeML (Bethard,
2013) in the general domain respectively.
4.1 Narrative Container Discovery
The task here as described by Miller et al (2013) is
to identify the CONTAINS relation between a time
expression and a same-sentence event from clinical
notes in the THYME corpus, which has 78 notes
of 26 patients. We obtained this corpus from the
authors and followed their linear composite kernel
setting:
K
C
(s
1
, s
2
) = ?
P
?
p=1
K
T
(t
p
1
, t
p
2
)+K
F
(f
1
, f
2
) (2)
where s
i
is an instance object composed of flat fea-
tures f
i
and a syntactic tree t
i
. A syntactic tree t
i
84
can have multiple representations, as in Bag Tree
(BT), Path-enclosed Tree (PET), and Path Tree
(PT). For the tree kernel K
T
, subset tree (SST) ker-
nel was applied on each tree representation p. The
final similarity score between two instances is the
? -weighted sum of the similarities of all representa-
tions, combined with the flat feature (FF) similarity
as measured by a feature kernel K
F
(linear or poly-
nomial). Here we replaced the SST kernel with
DPK and tested two feature combinations FF+PET
and FF+BT+PET+PT. To fine tune parameters, we
used grid search by testing on the default develop-
ment data. Once the parameters were tuned, we
tested the system performance on the testing data,
which was set up by the original system split.
4.2 Cleartk-TimeML
We tested one sub-task from TempEval-2013 ?
the extraction of temporal relations between an
event and time expression within the same sen-
tence. We obtained the training corpus (Time-
Bank + AQUAINT) and testing data from the au-
thors (Bethard, 2013). Since the original features
didn?t contain syntactic features, we created a PET
tree extractor for this system. The kernel setting
was similar to equation (2), while there was only
one tree representation, PET tree, P=1. A linear
kernel was used as K
F
to evaluate the exact same
flat features as used by the original system. We
used the built-in cross validation to do grid search
for tuning the parameters. The final system was
tested on the testing data for reporting results.
4.3 Results and Discussion
Results are shown in Table 2. The top section
shows THYME results. For these experiments,
the DPK is superior when a syntactically-rich PET
representation is used. Using the full feature set of
Miller et al (2013), SST is superior to DPK and
obtains the best overall performance. The bottom
section shows results on TempEval-2013 data, for
which there is little benefit from either tree kernel.
Our experiments with THYME data show that
DPK can capture something in the linguistically
richer PET representation that the SST kernel can-
not, but adding BT and PT representations decrease
the DPK performance. As a shallow representation,
BT does not have much in the way of descending
paths for DPK to use. PT already ignores the pro-
duction grammar by removing the inner tree nodes.
DPK therefore cannot get useful information and
may even get misleading cues from these two rep-
Features K
T
P R F
THYME
FF+PET DPK 0.756 0.667 0.708
SST 0.698 0.630 0.662
FF+BT+ DPK 0.759 0.626 0.686
PET+PT SST 0.754 0.711 0.732
TempEval
FF+PET DPK 0.328 0.263 0.292
SST 0.325 0.263 0.290
FF - 0.309 0.266 0.286
Table 2: Comparison of tree kernel performance
for temporal relation extraction on THYME and
TempEval-2013 data.
resentations. These results show that, while DPK
should not always replace SST, there are represen-
tations in which it is superior to existing methods.
This suggests an approach in which tree representa-
tions are matched to different convolution kernels,
for example by tuning on held-out data.
For TempEval-2013 data, adding syntactic fea-
tures did not improve the performance significantly
(comparing F-score of 0.290 with 0.286 in Ta-
ble 3). Probably, syntactic information is not a
strong feature for all types of temporal relations on
TempEval-2013 data.
5 Conclusion
In this paper, we developed a novel convolution
tree kernel (DPK) for measuring syntactic similar-
ity. This kernel uses a descending path represen-
tation in trees to allow higher similarity scores on
partially matching structures, while being simpler
and faster than other methods for doing the same.
Future work will explore 1) a composite kernel
which uses DPK for PET trees, SST for BT and PT,
and feature kernel for flat features, so that different
tree kernels can work with their ideal syntactic rep-
resentations; 2) incorporate dependency structures
for tree kernel analysis 3) applying DPK to other
relation extraction tasks on various corpora.
6 Acknowledgements
Thanks to Sean Finan for technically supporting the
experiments. The project described was supported
by R01LM010090 (THYME) from the National
Library Of Medicine.
85
References
Fabio Aiolli, Giovanni Da San Martino, and Alessan-
dro Sperduti. 2009. Route kernels for trees. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 17?24. ACM.
Steven Bethard and James H Martin. 2007. Cu-tmp:
temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
129?132. Association for Computational Linguis-
tics.
Steven Bethard. 2013. Cleartk-timeml: A minimalist
approach to TempEval 2013. In Second Joint Con-
ference on Lexical and Computational Semantics (*
SEM), volume 2, pages 10?14.
Nate Chambers. 2013. Navytime: Event and time or-
dering from raw text. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 73?77, Atlanta, Georgia, USA, June. Associa-
tion for Computational Linguistics.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Neural Information
Processing Systems.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia in Santa Cruz.
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Pat-
wardhan, and Chris Welty. 2012. When did that
happen?: linking events and relations to timestamps.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 185?193. Association for Compu-
tational Linguistics.
Tetsuji Kuboyama, Kouichi Hirata, Hisashi Kashima,
Kiyoko F Aoki-Kinoshita, and Hiroshi Yasuda.
2007. A spectrum tree kernel. Information and Me-
dia Technologies, 2(1):292?299.
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. Tipsem (english and spanish): Evaluating
CRFs and semantic roles in TempEval-2. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 284?291. Association for
Computational Linguistics.
Timothy Miller, Steven Bethard, Dmitriy Dligach,
Sameer Pradhan, Chen Lin, and Guergana Savova.
2013. Discovering temporal narrative containers
in clinical text. In Proceedings of the 2013 Work-
shop on Biomedical Natural Language Processing,
pages 18?26, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for
classifying temporal relations between events. Proc.
of the PACLIC23, pages 355?364.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318?329.
Springer.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The TimeBank corpus. In Cor-
pus linguistics, volume 2003, page 40.
Geoffrey Sampson. 2000. A proposal for improving
the measurement of parse accuracy. International
Journal of Corpus Linguistics, 5(1):53?68.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA machine reading
program-encouraging linguistic and reasoning re-
search with a series of reading tasks. In LREC.
William Styler, Steven Bethard, Sean Finan, Martha
Palmer, Sameer Pradhan, Piet de Groen, Brad Er-
ickson, Timothy Miller, Lin Chen, Guergana K.
Savova, and James Pustejovsky. 2014. Temporal
annotations in the clinical domain. Transactions
of the Association for Computational Linguistics,
2(2):143?154.
Min Zhang, Wanxiang Che, Ai Ti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for seman-
tic role classification. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 200?207.
86
Temporal Annotation in the Clinical Domain
William F. Styler IV1, Steven Bethard2, Sean Finan3, Martha Palmer1,
Sameer Pradhan3, Piet C de Groen4, Brad Erickson4, Timothy Miller3,
Chen Lin3, Guergana Savova3 and James Pustejovsky5
1 Department of Linguistics, University of Colorado at Boulder
2 Department of Computer and Information Sciences, University of Alabama at Birmingham
3 Children?s Hospital Boston Informatics Program and Harvard Medical School
4 Mayo Clinic College of Medicine, Mayo Clinic, Rochester, MN
5 Department of Computer Science, Brandeis University
Abstract
This article discusses the requirements of
a formal specification for the annotation of
temporal information in clinical narratives.
We discuss the implementation and extension
of ISO-TimeML for annotating a corpus of
clinical notes, known as the THYME cor-
pus. To reflect the information task and the
heavily inference-based reasoning demands
in the domain, a new annotation guideline
has been developed, ?the THYME Guidelines
to ISO-TimeML (THYME-TimeML)?. To
clarify what relations merit annotation, we
distinguish between linguistically-derived and
inferentially-derived temporal orderings in the
text. We also apply a top performing Temp-
Eval 2013 system against this new resource to
measure the difficulty of adapting systems to
the clinical domain. The corpus is available to
the community and has been proposed for use
in a SemEval 2015 task.
1 Introduction
There is a long-standing interest in temporal reason-
ing within the biomedical community (Savova et al.,
2009; Hripcsak et al., 2009; Meystre et al., 2008;
Bramsen et al., 2006; Combi et al., 1997; Keravnou,
1997; Dolin, 1995; Irvine et al., 2008; Sullivan et
al., 2008). This interest extends to the automatic ex-
traction and interpretation of temporal information
from medical texts, such as electronic discharge sum-
maries and patient case summaries. Making effective
use of temporal information from such narratives is
a crucial step in the intelligent analysis of informat-
ics for medical researchers, while an awareness of
temporal information (both implicit and explicit) in a
text is also necessary for many data mining tasks.
It has also been demonstrated that the temporal in-
formation in clinical narratives can be usefully mined
to provide information for some higher-level tempo-
ral reasoning (Zhao et al., 2005). Robust temporal
understanding of such narratives, however, has been
difficult to achieve, due to the complexity of deter-
mining temporal relations among events, the diver-
sity of temporal expressions, and the interaction with
broader computational linguistic issues.
Recent work on Electronic Health Records (EHRs)
points to new ways to exploit and mine the informa-
tion contained therein (Savova et al., 2009; Roberts
et al., 2009; Zheng et al., 2011; Turchin et al., 2009).
We target two main use cases for extracted data. First,
we hope to enable interactive displays and summaries
of the patient?s records to the physician at the time of
visit, making a comprehensive review of the patient?s
history both faster and less prone to oversights. Sec-
ond, we hope to enable temporally-aware secondary
research across large databases of medical records
(e.g., ?What percentage of patients who undergo pro-
cedure X develop side-effect Y within Z months??).
Both of these applications require the extraction of
time and date associations for critical events and the
relative ordering of events during the patient?s period
of care, all from the various records which make up a
patient?s EHR. Although we have these two specific
applications in mind, the schema we have developed
is generalizable and could potentially be embedded
in a wide variety of biomedical use cases.
Narrative texts in EHRs are temporally rich doc-
uments that frequently contain assertions about the
timing of medical events, such as visits, laboratory
values, symptoms, signs, diagnoses, and procedures
(Bramsen et al., 2006; Hripcsak et al., 2009; Zhou
et al., 2008). Temporal representation and reason-
ing in the medical record are difficult due to: (1) the
diversity of time expressions; (2) the complexity of
determining temporal relations among events (which
are often left to inference); (3) the difficulty of han-
dling the temporal granularity of an event; and (4)
143
Transactions of the Association for Computational Linguistics, 2 (2014) 143?154. Action Editor: Ellen Riloff.
Submitted 9/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
general issues in natural language processing (e.g.,
ambiguity, anaphora, ellipsis, conjunction). As a re-
sult, the signals used for reconstructing a timeline can
be both domain-specific and complex, and are often
left implicit, requiring significant domain knowledge
to accurately detect and interpret.
In this paper, we discuss the demands on accurately
annotating such temporal information in clinical
notes. We describe an implementation and extension
of ISO-TimeML (Pustejovsky et al., 2010), devel-
oped specifically for the clinical domain, which we
refer to as the ?THYME Guidelines to ISO-TimeML?
(?THYME-TimeML?), where THYME stands for
?Temporal Histories of Your Medical Events?. A sim-
plified version of these guidelines formed the basis
for the 2012 i2b2 medical-domain temporal relation
challenge (Sun et al., 2013a).
This is being developed in the context of the
THYME project, whose goal is to both create ro-
bust gold standards for semantic information in clini-
cal notes, as well as to develop state-of-the-art algo-
rithms to train and test on this dataset.
Deriving timelines from news text requires the con-
crete realization of context-dependent assumptions
about temporal intervals, orderings and organization,
underlying the explicit signals marked in the text
(Pustejovsky and Stubbs, 2011). Deriving patient
history timelines from clinical notes also involves
these types of assumptions, but there are special de-
mands imposed by the characteristics of the clinical
narrative. Due to both medical shorthand practices
and general domain knowledge, many event-event
relations are not signaled in the text at all, and rely
on a shared understanding and common conceptual
models of the progressions of medical procedures
available only to readers familiar with language use
in the medical community.
Identifying these implicit relations and temporal
properties puts a heavy burden on the annotation
process. As such, in the THYME-TimeML guideline,
considerable effort has gone into both describing and
proscribing the annotation of temporal orderings that
are inferable only through domain-specific temporal
knowledge.
Although the THYME guidelines describe a num-
ber of departures from the ISO-TimeML standard for
expediency and ease of annotation, this paper will
focus on those differences specifically motivated by
the needs of the clinical domain, and on the conse-
quences for systems built to extract temporal data in
both the clinical and general domain.
2 The Nature of Clinical Documents
In the THYME corpus, we have been examining
1,254 de-identified1 notes from a large healthcare
practice (the Mayo Clinic), representing two distinct
fields within oncology: brain cancer, and colon can-
cer. To date, we have principally examined two dif-
ferent general types of clinical narrative in our EHRs:
clinical notes and pathology reports.
Clinical notes are records of physician interactions
with a patient, and often include multiple, clearly
delineated sections detailing different aspects of the
patient?s care and present illness. These notes are
fairly generic across institutions and specialities, and
although some terms and inferences may be specific
to a particular type of practice (such as oncology),
they share a uniform structure and pattern. The ?His-
tory of Present Illness?, for example, summarizes the
course of the patient?s chief complaint, as well as the
interventions and diagnostics which have been thus
far attempted. In other sections, the doctor may out-
line her current plan for the patient?s treatment, then
later describe the patient?s specific medical history,
allergies, care directives, and so forth.
Most critically for temporal reasoning, each clin-
ical note reflects a single time in the patient?s treat-
ment history at which all of the doctor?s statements
are accurate (the DOCTIME), and each section tends
to describe events of a particular timeframe. For
example, ?History of Present illness? predominantly
describes events occuring before DOCTIME, whereas
?Medications? provides a snapshot at DOCTIME and
?Ongoing Care Orders? discusses events which have
not yet occurred.2
Clinical notes contain rich temporal information
and background, moving fluidly from prior treat-
ments and symptoms to present conditions to future
interventions. They are also often rich with hypo-
thetical statements (?if the tumor recurs, we can...?),
each of which can form its own separate timeline.
By constrast, pathology notes are quite different.
Such notes are generated by a medical pathologist
1Although most patient information was removed, dates
and temporal information were not modified according to this
project?s specific data use agreement.
2One complication is the propensity of doctors and automated
systems to later update sections in a note without changing the
timestamp or metadata. We have added a SECTIONTIME to keep
these updated sections from affecting our overall timeline.
144
upon receipt and analysis of specimens (ranging from
tissue samples from biopsy to excised portions of
tumor or organs). Pathology notes provide crucial
information to the patient?s doctor confirming the
malignancy (cancer) in samples, describing surgi-
cal margins (which indicate whether a tumor was
completely excised), and classifying and ?staging? a
tumor, describing the severity and spread of the can-
cer. Because the information in such notes pertains
to samples taken at a single moment in time, they are
temporally sparse, seldom referring to events before
or after the examination of the specimen. However,
they contain critical information about the state of
the patient?s illness and about the cancer itself, and
must be interpreted to understand the history of the
patient?s illness.
Most importantly, in all EHRs, we must contend
with the results of a fundamental tension in mod-
ern medical records: hyper-detailed records provide
a crucial defense against malpractice litigation, but
including such detail takes enormous time, which
doctors seldom have. Given that these notes are writ-
ten by and for medical professionals (who form a
relatively insular speech community), a great many
non-standard expressions, abbreviations, and assump-
tions of shared knowledge are used, which are simul-
taneously concise and detail-rich for others who have
similar backgrounds.
These time-saving devices can range from tempo-
rally loaded acronyms (e.g., ?qid?, Latin for quater in
die, ?four times daily?), to assumed orderings (a diag-
nostic test for a disorder is assumed to come before
the procedure which treats it), and even to completely
implicit events and temporal details. For example,
consider the sentence in (1).
(1) Colonoscopy 3/12/10, nodule biopsies negative
We must understand that during the colonoscopy,
the doctor obtained biopsies of nodules, which were
packaged and sent to a pathologist, who reviewed
them and determined them to be ?negative? (non-
cancerous).
In such documents, we must recover as much tem-
poral detail as possible, even though it may be ex-
pressed in a way which is not easily understood out-
side of the medical community, let alone by linguists
or automated systems. We must also be aware of the
legal relevance of some events (e.g., ?We discussed
the possible side effects?), even when they may not
seem relevant to the patient?s actual care.
Finally, each specialty and note type has separate
conventions. Within colon cancer notes, the Amer-
ican Joint Committee on Cancer (AJCC) Staging
Codes (e.g., T4N1, indicating the nature of the tumor,
lymph node and metastasis involvement) are metic-
ulously recorded, but are largely absent in the brain
cancer notes which make up the second corpus in
our project. So, although clinical notes share many
similarities, annotators without sufficient domain ex-
pertise may require additional training to adapt to the
inferences and nuances of a new clinical subdomain.
3 Interpreting ?Event? and Temporal
Expressions in the Clinical Domain
Much prior work has been done on standardizing
the annotation of events and temporal expressions
in text. The most widely used approach is the ISO-
TimeML specification (Pustejovsky et al., 2010), an
ISO standard that provides a common framework for
annotating and analyzing time, events, and event rela-
tions. As defined by ISO-TimeML, an EVENT refers
to anything that can be said ?to obtain or hold true, to
happen or to occur?. This is a broad notion of event,
consistent with Bach?s use of the term ?eventuality?
(Bach, 1986) as well as the notion of fluents in AI
(McCarthy, 2002).
Because the goals of the THYME project involve
automatically identifying the clinical timeline for
a patient from clincal records, the scope of what
should be admitted into the domain of events is inter-
preted more broadly than in ISO-TimeML3. Within
the THYME-TimeML guideline, an EVENT is any-
thing relevant to the clinical timeline, i.e., anything
that would show up on a detailed timeline of the pa-
tient?s care or life. The best single-word syntactic
head for the EVENT is then used as its span. For
example, a diagnosis would certainly appear on such
a timeline, as would a tumor, illness, or procedure.
On the other hand, entities that persist throughout
the relevant temporal period of the clinical timeline
(endurants in ontological circles) would not be con-
sidered as event-like. This includes the patient, other
humans mentioned (the patient?s mother-in-law or
the doctor), organizations (the emergency room),
non-anatomical objects (the patient?s car), or indi-
vidual parts of the patient?s anatomy (an arm is not
an EVENT unless missing or otherwise notable).
To meet our explicit goals, the THYME-TimeML
guideline introduces two additional levels of interpre-
3Our use of the term ?EVENT? corresponds with the less
specific ISO-TimeML term ?Eventuality?
145
tation beyond that specified by ISO-TimeML: (i) a
well-defined task; and (ii) a clearly identified domain.
By focusing on the creation of a clinical timeline
from clinical narrative, the guideline imposes con-
straints that cannot be assumed for a broadly defined
and domain independent annotation schema.
Some EVENTs annotated under our guideline are
considered meaningful and eventive mostly by virtue
of a specific clinical or legal value. For example,
AJCC Staging Codes (discussed in Section 2) are
eventive only in the sense of the code being assigned
to a tumor at a given moment in the patient?s care.
However, they are of such critical importance and
informative value to doctors that we have chosen to
annotate them specifically so that they will show up
on the patient?s timeline in a clinical setting.
Similarly, because of legal pressures to establish in-
formed consent and patient knowledge of risk, entire
paragraphs of clinical notes are dedicated to docu-
menting the doctor?s discussion of risks, plans, and
alternative strategies. As such, we annotate verbs of
discussion (?We talked about the risks of this drug?),
consent (?She agreed with the current plan?), and
comprehension (?Mrs. Larsen repeated the potential
side effects back to me?), even though they are more
relevant to legal defense than medical treatment.
It is also because of this grounding in clinical lan-
guage that entities and other non-events are often
interpreted in terms of their associated eventive prop-
erties. There are two major types for which this is a
significant shift in semantic interpretation:
(2) a Medication as Event:
Orders: Lariam twice daily.
b Disorder as Event:
Tumor of the left lung.
In both these cases, entities which are not typically
marked as events are identified as such, because they
contribute significant information to the clinical time-
line being constructed. In (2a), for example, the
TIMEX3 ?twice daily? is interpreted as scoping over
the eventuality of the patient taking the medication,
not the prescription event. In sentence (2b), the ?tu-
mor? is interpreted as a stative eventuality of the
patient having a tumor located within an anatomical
region, rather than an entity within an entity.
Within the medical domain, these eventive inter-
pretations of medications, growths and status codes
are unambiguous and consistent. Doctors in clini-
cal notes (unlike in biomedical research texts) do
not discuss medications without an associated (im-
plicit) administering EVENT (though some mentions
may be hypothetical, generic or negated). Similarly,
mentions of symptoms or disorders reflect occur-
rences in a patient?s life, rather than abstract entities.
With these interpretations in mind, we can safely in-
fer, for instance, that all UMLS (Unified Medical
Language System, (Bodenreider, 2004)) entities of
the types Disorder, Chemical/Drug, Procedure and
Sign/Symptom will be EVENTs.
In general, in the medical domain, it is essential to
read ?between the lines? of the shorthand expressions
used by the doctors, and recognize implicit events
that are being referred to by specific anatomical sites
or medications.
4 Modifications to ISO-TimeML for the
Clinical Domain
Overall, we have found that the specification required
for temporal annotation in the clinical domain does
not require substantial modification from existing
specifications for the general domain. The clinical
domain includes no shortage of inferences, short-
hands, and unusual use of language, but the structure
of the underlying timeline is not unique.
As a result of this, we have been able to adopt most
of the framework from ISO-TimeML, adapting the
guidelines where needed, as well as reframing the
focus of what gets annotated. This is reflected in a
comprehensive guideline, incorporating the specific
patterns and uses of events and temporal expressions
as seen in clinical data. This approach allows the
resulting annotations to be interoperable with exist-
ing solutions, while still accommodating the major
differences in the nature of the texts. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org4
Our extensions of the ISO-TimeML specification
to the clinical domain are intended to address specific
constructions, meanings, and phenomena in medical
texts. Our schema differs from ISO-TimeML in a
few notable ways.
EVENT Properties We have both simplified the
ISO-TimeML coding of EVENTs, and extended it to
meet the needs of the clinical domain and the specific
language goals of the clinical narrative.
4Access to the corpus will require a data use agreement.
More information about this process is available from the corpus
website.
146
Consider, for example, how modal subordination is
handled in ISO-TimeML. This involves the semantic
characterization of an event as ?likely?, ?possible?, or
as presented by observation, evidence, or hearsay. All
of these are accounted for compositionally in ISO-
TimeML within the SLINK (Subordinating Link)
relation (Pustejovsky et al., 2005). While accept-
ing ISO-TimeML?s definition of event modality, we
have simplified the annotation task within the cur-
rent guideline, so that EVENTs now carry attributes
for ?contextual modality?, ?contextual aspect? and
?permanence?.
Contextual modality allows the values ACTUAL,
HYPOTHETICAL, HEDGED, and GENERIC. ACTUAL
covers EVENTs which have actually happened, e.g.,
?We?ve noted a tumor?. HYPOTHETICAL covers con-
ditionals and possibilities, e.g., ?If she develops a
tumor?. HEDGED is for situations where doctors
proffer a diagnosis, but do so cautiously, to avoid
legal liability for an incorrect diagnosis or for over-
looking a correct one. For example:
(3) a. The signal in the MRI is not inconsistent
with a tumor in the spleen.
b. The rash appears to be measles, awaiting
antibody test to confirm.
These HEDGED EVENTs are more real than a hypo-
thetical diagnosis, and likely merit inclusion on a
timeline as part of the diagnostic history, but must
not be conflated with confirmed fact. These (and
other forms of uncertainty in the medical domain)
are discussed extensively in (Vincze et al., 2008). In
contrast, GENERIC EVENTs do not refer to the pa-
tient?s illness or treatment, but instead discuss illness
or treatment in general (often in the patient?s specific
demographic). For example:
(4) In other patients without significant comor-
bidity that can tolerate adjuvant chemother-
apy, there is a benefit to systemic adjuvant
chemotherapy.
These sections would be true if pasted into any pa-
tient?s note, and are often identical chunks of text
repeatedly used to justify a course of action or treat-
ment as well as to defend against liability.
Contextual Aspect (to distinguish from grammati-
cal aspect), allows the clinically-necessary category,
INTERMITTENT. This serves to distinguish intermit-
tent EVENTs (such as vomiting or seizures) from
constant, more stative EVENTs (such as fever or sore-
ness). For example, the bolded EVENT in (5a) would
be marked as INTERMITTENT, while that in (5b)
would not:
(5) a She has been vomiting since June.
b She has had swelling since June.
In the first case, we assume that her vomiting has
been intermittent, i.e., there were several points since
June in which she was not actively vomiting. In the
second case, unless made otherwise explicit (?she has
had occasional swelling?), we assume that swelling
was a constant state. This property is also used when
a particular instance of an EVENT is intermittent,
even though it generally would not be:
(6) Since starting her new regime, she has had occa-
sional bouts of fever, but is feeling much better.
The permanence attribute has two values, FINITE
and PERMANENT. Permanence is a property of dis-
eases themselves, roughly corresponding to the med-
ical concept of ?chronic? vs. ?acute? disease, which
marks whether a disease is persistent following diag-
nosis. For example, a (currently) uncurable disease
like Multiple Sclerosis would be classed as PERMA-
NENT, and thus, once mentioned in a patient?s note,
will be assumed to persist through the end of the
patient?s timeline. This is compared with FINITE
disorders like ?Influenza? or ?fever?, which, if not
mentioned in subsequent notes, should be considered
cured and no longer belongs on the patient?s time-
line. Because it requires domain-specific knowledge,
although present in the specification, Permanence
is not currently annotated. However, annotators are
trained on the basic idea and told about subsequent
axiomatic assignment. The addition of this property
to our schema is designed to relieve annotators of any
feeling of obligation to express this inferred informa-
tion in some other way.
TIMEX3 Types Temporal expressions (TIMEX3s)
in the clinical domain function the same as in the gen-
eral linguistic community, with two notable excep-
tions. ISO-TimeML SETs (statements of frequency)
occur quite frequently in the medical domain, par-
ticularly with regard to medications and treatments.
Medication sections within notes often contain long
lists of medications, each with a particular associated
set (?Claritin 30mg twice daily?), and further tempo-
ral specification is not uncommon (e.g., ?three times
per day at meals?, ?once a week at bedtime?).
The second major change for the medical domain
is a new type of TIMEX3 which we call PREPOS-
TEXP. This covers temporally complex terms like
147
?preoperative?, ?postoperative?, and ?intraoperative?.
These temporal expressions designate a span of time
bordered, usually only on one side, by the incorpo-
rated event (an operation, in the previous EVENTs).
In many cases, the referent is clear:
(7) She underwent hemicolectomy last week, and
had some postoperative bleeding.
Here we understand that ?postoperative? refers to
?the period of time following the hemicolectomy?. In
these cases, the PREPOSTEXP makes explicit a tempo-
ral link between the bleeding and the hemicolectomy.
In other cases, no clear referent is present:
(8) Patient shows some post-procedure scarring.
In these situations, where no procedure is mentioned
(or the reference is never explicitly resolved), we
treat the PREPOSTEXP as a narrative container (see
Section 5), covering the span of time following the
unnamed procedure.
Finally, it is worth noting that the process of nor-
malizing those TIMEX3s is significantly more com-
plex relative to the general domain, because many
temporal expressions are anchored not to dates or
times, but to other EVENTs (whose dates are often
not mentioned or not known by the physician). As
we move towards a complete system, we are working
to expand the ISO-TimeML system for TIMEX3 nor-
malization to allow some value to be assigned to a
phrase like ?in the months after her hemicolectomy?
when no referent date is present. ISO-TimeML, in
discussion with ISO TC 37SC 4, plans to reference
to such TIMEX3s in a future release of the standard.
5 Temporal Ordering and Narrative
Containers
The semantic content and informational impact of
a timeline is encoded in the ordering relations that
are identified between the temporal and event expres-
sions present in clinical notes. ISO-TimeML speci-
fies the standard thirteen ?Allen relations? from the
interval calculus (Allen, 1983), which it refers to as
TLINK values. For unguided, general-purpose annota-
tion, the number of relations that could be annotated
grows quadratically with the number of events and
times, and the task quickly becomes unmanageable.
There are, however, strategies that we can adopt to
make this labeling task more tractable. Temporal
ordering relations in text are of three kinds:
1. Relations between two events
2. Relations between two times
3. Relations between a time and an event.
ISO-TimeML, as a formal specification of the tem-
poral information conveyed in language, makes no
distinction between these ordering types. Humans,
however, do make distinctions, based on local tempo-
ral markers and the discourse relations established in
a narrative (Miltsakaki et al., 2004; Poesio, 2004).
Because of the difficulty of humans capturing ev-
ery relationship present in the note (and the disagree-
ment which arises when annotators attempt to do so),
it is vital that the annotation guidelines describe an
approach that reduces the number of relations that
must be considered, but still results in maximally in-
formative temporal links. We have found that many
of the weaknesses in prior annotation approaches
stem from interaction between two competing goals:
? The guideline should specify certain types of an-
notations that should be performed;
? The guideline should not force annotations to be
performed when they need not be.
Failing in the first goal will result in under-annotation
and the neglect of relations which provide necessary
information for inference and analysis. Failure in the
second goal results in over-annotation, creating com-
plex webs of temporal relations which yield mostly
inferable information, but which complicate annota-
tion and adjudication considerably.
Our method of addressing both goals in tempo-
ral relations annotation is that of the narrative con-
tainer, discussed in Pustejovsky and Stubbs (2011).
A narrative container can be thought of as a temporal
bucket into which an EVENT or series of EVENTs
may fall, or a natural cluster of EVENTs around a
given time or situation. These narrative containers
are often represented (or ?anchored?) by dates or
other temporal expressions (within which a variety
of different EVENTs occur), although they can also
be anchored to more abstract concepts (?recovery?
which might involve a variety of EVENTs) or even
durative EVENTs (many other EVENTs can occur dur-
ing a surgery). Rather than marking every possible
TLINK between each EVENT, we instead try to link
all EVENTs to their narrative containers, and then
link those containers so that the contained EVENTs
can be linked by inference.
First, annotators assign each event to one of four
broad narrative containers: before the DOCTIME, be-
fore and overlapping the DOCTIME, just overlapping
the DOCTIME or after the DOCTIME. This narrative
148
container is identified by the EVENT attribute Doc-
TimeRel. After the assignment of DocTimeRel, the
remainder of the narrative container relations must
be specified using temporal links (TLINKs). There
are five different temporal relations used for such
TLINKs: BEFORE, OVERLAP, BEGINS-ON, ENDS-ON
and CONTAINS5. Due to our narrative container ap-
proach, CONTAINS is the most frequent relation by a
large margin.
EVENTs serving as narrative container anchors are
not tagged as containers per-se. Instead, annotators
use the narrative container idea to help them visu-
alize the temporal relations within a document, and
then make a series of CONTAINS TLINK annotations
which establish EVENTs and TIMEX3s as anchors,
and specify their contents. If the annotators do their
jobs correctly, properly implementing DocTimeRel
and creating accurate TLINKs, a good understanding
of the narrative containers present in a document will
naturally emerge from the annotated text.
The major advantage introduced with narrative
containers is this: a narrative event is placed within a
bounding temporal interval which is explicitly men-
tioned in the text. This allows EVENTs within sep-
arate containers to be linked by post-hoc inference,
temporal reasoning, and domain knowledge, rather
than by explicit (and time-consuming) one-by-one
temporal relations annotation.
A secondary advantage is that this approach works
nicely with the general structure of story-telling in
both the general and clinical domains, and provides a
compelling and useful metaphor for interpreting time-
lines. Often, especially in clinical histories, doctors
will cluster discussions of symptoms, interventions
and diagnoses around a given date (e.g. a whole para-
graph starting ?June 2009:?), a specific hospitaliza-
tion (?During her January stay at Mercy?), or a given
illness or treatment (?While she underwent Chemo?).
Even when specific EVENTs are not explicitly or-
dered within a cluster (often because the order can be
easily inferred with domain knowledge), it is often
quite easy to place the EVENTs into containers, and
just a few TLINKs can order the containers relative to
one another with enough detail to create a clinically
useful understanding of the overall timeline.
Narrative containers also allow the inference of re-
lations between sub-events within nested containers:
5This is a subset of the ISO-TimeML TLINK types, excluding
those seldom occurring in medical records, like ?simultaneous?
as well as inverse relations like ?during? or ?after?.
(9) December 19th: The patient underwent an MRI
and EKG as well as emergency surgery. Dur-
ing the surgery, the patient experienced mild
tachycardia, and she also bled significantly
during the initial incision.
1. December 19th CONTAINS MRI
2. December 19th CONTAINS EKG
3. December 19th CONTAINS surgery
a. surgery CONTAINS tachycardia
b. surgery CONTAINS incision
c. incision CONTAINS bled
Through our container nesting, we can automatically
infer that ?bled? occurred on December 19th (because
?19th? CONTAINS ?surgery? which CONTAINS ?inci-
sion? which CONTAINS ?bled?). This also allows the
capture of EVENT/sub-event relations, and the rapid
expression of complex temporal interactions.
6 Explicit vs. Inferable Annotation
Given a specification language, there are essentially
two ways of introducing the elements into the docu-
ment (data source) being annotated:6
? Manual annotation: Elements are introduced into
the document directly by the human annotator fol-
lowing the guideline.
? Automatic (inferred) annotation: Elements are cre-
ated by applying an automated procedure that in-
troduces new elements that are derivable from the
human annotations.
As such, there is a complex interaction between spec-
ification and guideline, and we focus on how the
clinical annotation task has helped shape and refine
the annotation guidelines. It is important to note that
an annotation guideline does not necessarily force
the markup of certain elements in a text, even though
the specification language (and the eventual goal of
the project) might require those annotations to exist.
In some cases, these added annotations are derived
logically from human annotations. Explicitly marked
temporal relations can be used to infer others that are
not marked but exist implicitly through closure. For
instance, given EVENTs A, B and C and TLINKs ?A
BEFORE B? and ?B BEFORE C?, the TLINK ?A BE-
FORE C? can be automatically inferred. Repeatedly
applying such inference rules allows all inferable
6We ignore the application of automatic techniques, such as
classifiers trained on external datasets, as our focus here is on
the preparation of the gold standard used for such classifiers.
149
TLINKs to be generated (Verhagen, 2005). We can
use this idea of closure to show our annotators which
annotations need not be marked explicitly, saving
time and effort. We have also incorporated these clo-
sure rules into our inter-annotator agreement (IAA)
calculation for temporal relations, described further
in Section 7.2.
The automatic application of rules following the
annotation of the text is not limited to the marking
of logically inferable relations or EVENTs. In the
clinical domain, the combination of within-group
shared knowledge and pressure towards concise writ-
ing leads to a number of common, inferred relations.
Take, for example, the sentence:
(10) Jan 2013: Colonoscopy, biopsies. Pathology
showed adenocarcinoma, resected at Mercy.
Diagnosis T3N1 Adenocarcinoma.
In this sentence, only the CONTAINS relations be-
tween ?Jan 2013? and the EVENTs (in bold) are
explicitly stated. However, based on the known
progression-of-care for colon cancer, we can infer
that the colonoscopy occurs first, biopsies occur dur-
ing the colonoscopy, pathology happens afterwards,
a diagnosis (here, adenocarcinoma) is returned after
pathology, and resection of the tumor occurs after
diagnosis. The presence of the AJCC staging infor-
mation in the final sentence (along with the confir-
mation of the adenocarcinoma diagnosis) implies a
post-surgical pathology exam of the resected spec-
imen, as the AJCC staging information cannot be
determined without this additional examination.
These inferences come naturally to domain ex-
perts but are largely inaccessible to people outside
the medical community without considerable anno-
tator training. Making explicit our understanding of
these ?understood orderings? is crucial; although they
are not marked by human annotators in our schema,
the annotators often found it initially frustrating to
leave these (purely inferential) relations unstated. Al-
though many of our (primarily linguistically trained)
annotators learned to see these patterns, we chose to
exclude them from the manual task since newer an-
notators with varying degrees of domain knowledge
may struggle if asked to manually annotate them.
Similar unspoken-but-understood orderings are
found throughout the clinical domain. As mentioned
in Section 3, both Permanence and Contextual As-
pect:Intermittent are properties of symptoms and dis-
eases themselves, rather than of the patient?s particu-
lar situation. As such, these properties could easily
Annotation Type Raw Count
EVENT 15,769
TIMEX3 1,426
LINK 7935
Total 25,130
Table 1: Raw Frequency of Annotation Types
TLINK Type Raw Count % of TLINKs
CONTAINS 5,112 64.42%
OVERLAP 1,205 15.19%
BEFORE 1,004 12.65%
BEGINS-ON 488 6.15%
ENDS-ON 126 1.59%
Total 7,935 100.00%
Table 2: Relative Frequency of TLINK types
be identified and marked across a medical ontology,
and then be automatically assigned to EVENTs rec-
ognized as specific medical named entities.
Finally, due to the peculiarities of EHR systems,
some annotations must be done programatically. Ex-
act dates of patient visit (or of pathology/radiology
consult) are often recorded as metadata on the EHR
itself, rather than within the text, making the canoni-
cal DOCTIME (or time of automatic section modifi-
cations) difficult to access in de-identified plaintext
data, but easy to find automatically.
7 Results
We report results on the annotations from the here-
released subset of the THYME colon cancer corpus,
which includes clinical notes and pathology reports
for 35 patients diagnosed with colon cancer for a
total of 107 documents. Each note was annotated
by a pair of graduate or undergraduate students in
Linguistics at the University of Colorado, then adju-
dicated by a domain expert. These clinical narratives
were sampled from the EHRs of a major healthcare
center (the Mayo Clinic). They were deidentified for
all patient-sensitive information; however, original
dates were retained.
7.1 Descriptive Statistics
Table 1 presents the raw counts for events, temporal
expressions and links in the adjudicated gold anno-
tations. Table 2 presents the number and percentage
of TLINKs by type in the adjudicated relations gold
annotations.
150
Annotation Type F1-Score Alpha
EVENT 0.8038 0.7899
TIMEX3 0.8047 0.6705
LINK: Participants only 0.5012 0.4999
LINK: Participants+type 0.4506 0.4503
LINK: CONTAINS 0.5630 0.5626
Table 3: IAA (F1-Score and Alpha) by annotation type
EVENT Property F1-Score Alpha
DocTimeRel 0.7189 0.6889
Cont.Aspect 0.9947 0.9930
Cont.Modality 0.9547 0.9420
Table 4: IAA (F1-Score and Alpha) for EVENT properties
7.2 Inter-annotator Agreement
We report inter-annotator agreement (IAA) results
on the THYME corpus. Each note was annotated by
two independent annotators. The final gold standard
was produced after disagreement adjudication by a
third annotator was performed.
We computed the IAA as F1-score and Krippen-
dorff?s Alpha (Krippendorff, 2012) by applying clo-
sure, using explicitly marked temporal relations to
identify others that are not marked but exist implicitly.
In the computation of the IAA, inferred-only TLINKs
do not contribute to the score, matched or unmatched.
For instance, if both annotators mark A BEFORE B
and B BEFORE C, to prevent artificially inflating the
agreement score, the inferred A BEFORE C is ignored.
Likewise, if one annotator marked A BEFORE B and
B BEFORE C and the other annotator did not, the
inferred A BEFORE C is not counted. However, if
one annotator did explicitly mark A BEFORE C, then
an equivalent inferred TLINK would be used to match
it. EVENT and TIMEX3 IAA was generated based
on exact and overlapping spans, respectively. These
results are reported in Table 3.
The THYME corpus also differs from ISO-
TimeML in terms of EVENT properties, with the
addition of DocTimeRel, ContextualModality and
ContextualAspect. IAA for these properties is in
Table 4.
7.3 Baseline Systems
To get an idea of how much work will be neces-
sary to adapt existing temporal information extrac-
tion systems to the clinical domain, we took the freely
available ClearTK-TimeML system (Bethard, 2013),
TempEval 2013 THYME Corpus
P R F1 P R F1
TIMEX3 83.2 71.7 77.0 59.3 42.8 49.7
EVENT 81.4 76.4 78.8 78.9 23.9 36.6
DocTimeRel - - - 47.4 47.4 47.4
LINK7 28.6 30.9 26.6 22.7 18.6 20.4
EVENT-TIMEX3 - - - 32.3 60.7 42.1
EVENT-EVENT - - - 7.0 3.0 4.2
Table 5: Performance of ClearTK-TimeML models, as
reported in the TempEval 2013 competition, and as applied
to the THYME Corpus development set.
which was among the top performing systems in
TempEval 2013 (UzZaman et al., 2013), and eval-
uated its performance on the THYME corpus.
ClearTK-TimeML uses support vector machine
classifiers trained on the TempEval 2013 training
data, employing a small set of features including
character patterns, tokens, stems, part-of-speech tags,
nearby nodes in the constituency tree, and a small
time word gazetteer. For EVENTs and TIMEX3s,
the ClearTK-TimeML system could be applied di-
rectly to the THYME corpus. For DocTimeRels, the
relation for an EVENT was taken from the TLINK
between that EVENT and the document creation time,
after mapping INCLUDES to OVERLAP. EVENTs
with no such TLINK were assumed to have a Doc-
TimeRel of OVERLAP. For other temporal relations,
INCLUDES was mapped to CONTAINS.
Results of this system on TempEval 2013 and the
THYME corpus are shown in Table 5. For time ex-
pressions, performance when moving to the clinical
data degrades about 25%, from F1 of 77.0 to 49.7.
For events, the degradation is much larger, about
40%, from 78.8 to 36.6, most likely because of the
large number of clinical symptoms, diseases, disor-
ders, etc. which have never been observed by the
system during training. Temporal relations are a bit
more difficult to compare because TempEval lumped
DocTimeRel and other temporal relations together
and had several differences in their evaluation met-
ric7. However, we at least can see that performance
of the ClearTK-TimeML system on temporal rela-
tions is low on clinical text, achieving only F1 of
20.4.
These results suggest that clinical narratives do
7The TempEval 2013 evaluation metric penalized systems
for parts of the text that were not examined by annotators, and
used different variants of closure-based precision and recall.
151
indeed present new challenges for temporal informa-
tion extraction systems, and that having access to
domain specific training data will be crucial for ac-
curate extraction in the clinical domain. At the same
time, it is encouraging that we were able to apply
existing ISO-TimeML-based systems to our corpus,
despite the several extensions to ISO-TimeML that
were necessary for clinical narratives.
8 Discussion
CONTAINS plays a large role in the THYME cor-
pus, representing 66% of TLINK annotations made,
compared with only 14.6% for OVERLAP, the second
most frequent type. We also see that BEFORE links
are relatively less common than OVERLAP and CON-
TAINS, illustrating that much of the temporal ordering
on the timeline is accomplished by using many ver-
tical links (CONTAINS, OVERLAP) to build contain-
ers, and few horizontal links (BEFORE, BEGINS-ON,
ENDS-ON) to order them.
IAA on EVENTs and Temporal Expressions is
strong, although differentiating implicit EVENTs
(which should not be marked) from explicit, mark-
able EVENTs remains one of the biggest sources of
disagreement. When compared to the data from the
2012 i2b2 challenge (Sun et al., 2013b), our IAA
figures are quite similar. Even with our more com-
plex schema, we achieved an F1-score of 0.8038 for
EVENTs (compared to the i2b2 score of 0.87 for par-
tial match). For TIMEX3s, our F1-score was 0.8047,
compared to an F1-score of 0.89 for i2b2.
TLINKing medical EVENTs remains a very diffi-
cult task. By using our narrative container approach
to constrain the number of necessary annotations and
by eliminating often-confusing inverse relations (like
?after? and ?during?) (neither of which were done for
the i2b2 data), we were able to significantly improve
on the i2b2 TLINK span agreement F1-score of 0.39,
achieving an agreement score of 0.5012 for all LINKs
across our corpus. The majority of remaining an-
notator disagreement comes from different opinions
about whether any two EVENTs require an explicit
TLINK between them or an inferred one, rather than
what type of TLINK it would be (e.g. BEFORE vs.
CONTAINS). Although our results are still signifi-
cantly higher than the results reported for i2b2, and
in line with previously reported general news figures,
we are not satisfied. Improving IAA is an important
goal for future work, and with further training, speci-
fication, experience, and standardization, we hope to
clarify contexts for explicit TLINKS.
News-trained temporal information extraction sys-
tems see a significant drop in performance when ap-
plied to the clinical texts of the THYME corpus. But
as the corpus is an extension of ISO-TimeML, future
work will be able to train ISO-TimeML compliant
systems on the annotations of the THYME corpus to
reduce or eliminate this performance gap.
Some applications that our work may enable in-
clude (1) better understanding of event semantics,
such as whether a disease is chronic or acute and
its usual natural history, (2) typical event duration
for these events, (3) the interaction of general and
domain-specific events and their importance in the fi-
nal timeline, and, more generally, (4) the importance
of rough temporality and narrative containers as a
step towards finer-grained timelines.
We have several avenues of ongoing and future
work. First, we are working to demonstrate the utility
of the THYME corpus for training machine learning
models. We have designed support vector machine
models with constituency tree kernels that were able
to reach an F1-score of 0.737 on an EVENT-TIMEX3
narrative container identification task (Miller et al.,
2013), and we are working on training models to
identify events, times and the remaining types of
temporal relations. Second, as per our motivating
use cases, we are working to integrate this annotation
data with timeline visualization tools and to use these
annotations in quality-of-care research. For example,
we are using temporal reasoning built on this work to
investigate the liver toxicity of methotrexate across
a large corpus of EHRs (Lin et al., under review)].
Finally, we plan to explore the application of our
notion of an event (anything that should be visible on
a domain-appropriate timeline) to other domains. It
should transfer naturally to clinical notes about other
(non-cancer) conditions, and even to other types of
clinical notes, as certain basic events should always
be included in a patient?s timeline. Applying our
notion of event to more distant domains, such as legal
opinions, would require first identifying a consensus
within the domain about which events must appear
on a timeline.
9 Conclusion
Much of the information in clinical notes critical to
the construction of a detailed timeline is left implicit
by the concise shorthand used by doctors. Many
events are referred to only by a term such as ?tu-
152
mor?, while properties of the event itself, such as
?intermittent?, may not be specified. In addition, the
ordering of events on a timeline is often left to the
reader to infer, based on domain-specific knowledge.
It is incumbent upon the annotation guideline to in-
dicate that only informative event orderings should
be annotated, while leaving domain-specific order-
ings to post-annotation inference. This document
has detailed our approach to adapting the existing
ISO-TimeML standard to this recovery of implicit
information, and defining guidelines that support an-
notation within this complex domain. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org, and the full
corpus has been proposed for use in a SemEval 2015
shared task.
Acknowledgments
The project described is supported by Grant Num-
ber R01LM010090 and U54LM008748 from the Na-
tional Library Of Medicine. The content is solely the
responsibility of the authors and does not necessarily
represent the official views of the National Library
Of Medicine or the National Institutes of Health.
We would also like to thank Dr. Piet C. de Groen
and Dr. Brad Erickson at the Mayo Clinic, as well as
Dr. William F. Styler III, for their contributions to the
schema and to our understanding of the intricacies of
clinical language.
References
James F Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Emmon Bach. 1986. The algebra of events. Linguistics
and philosophy, 9(1):5?16.
Steven Bethard. 2013. Cleartk-timeml: A minimalist ap-
proach to tempeval 2013. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
10?14, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): integrating biomedical
terminology. Nucleic acids research, 32(Database
issue):D267?D270, January.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Finding temporal order
in discharge summaries. In AMIA Annual Symposium
Proceedings, volume 2006, page 81. American Medical
Informatics Association.
Carlo Combi, Yuval Shahar, et al. 1997. Temporal reason-
ing and temporal data maintenance in medicine: issues
and challenges. Computers in biology and medicine,
27(5):353?368.
Robert H Dolin. 1995. Modeling the temporal complex-
ities of symptoms. Journal of the American Medical
Informatics Association, 2(5):323?331.
George Hripcsak, Nicholas D Soulakis, Li Li, Frances P
Morrison, Albert M Lai, Carol Friedman, Neil S Cal-
man, and Farzad Mostashari. 2009. Syndromic surveil-
lance using ambulatory electronic health records. Jour-
nal of the American Medical Informatics Association,
16(3):354?361.
Ann K Irvine, Stephanie W Haas, and Tessa Sullivan.
2008. Tn-ties: A system for extracting temporal infor-
mation from emergency department triage notes. In
AMIA Annual Symposium proceedings, volume 2008,
page 328. American Medical Informatics Association.
Elpida T Keravnou. 1997. Temporal abstraction of med-
ical data: Deriving periodicity. In Intelligent Data
Analysis in Medicine and Pharmacology, pages 61?79.
Springer.
Klaus H. Krippendorff. 2012. Content Analysis: An
Introduction to Its Methodology. SAGE Publications,
Inc, third edition edition, April.
Chen Lin, Elizabeth Karlson, Dmitriy Dligach, Mon-
ica Ramirez, Timothy Miller, Huan Mo, Natalie
Braggs, Andrew Cagan, Joshua Denny, and Guer-
gana. Savova. under review. Automatic identification
of methotrexade-induced liver toxicity in rheumatoid
arthritis patients from the electronic medical records.
Journal of the Medical Informatics Association.
John McCarthy. 2002. Actions and other events in sit-
uation calculus. In Proceedings of the International
conference on Principles of Knowledge Representation
and Reasoning, pages 615?628. Morgan Kaufmann
Publishers; 1998.
Ste?phane M Meystre, Guergana K Savova, Karin C Kipper-
Schuler, John F Hurdle, et al. 2008. Extracting infor-
mation from textual documents in the electronic health
record: a review of recent research. Yearb Med Inform,
35:128?44.
Timothy Miller, Steven Bethard, Dmitriy Dligach, Sameer
Pradhan, Chen Lin, and Guergana Savova. 2013. Dis-
covering temporal narrative containers in clinical text.
In Proceedings of the 2013 Workshop on Biomedical
Natural Langua ge Processing, pages 18?26, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
153
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bon-
nie Webber. 2004. The penn discourse treebank. In In
Proceedings of LREC 2004.
Massimo Poesio. 2004. Discourse annotation and seman-
tic annotation in the gnome corpus. In In Proceedings
of the ACL Workshop on Discourse Annotation.
James Pustejovsky and Amber Stubbs. 2011. Increasing
informativeness in temporal annotation. In Proceedings
of the 5th Linguistic Annotation Workshop, pages 152?
160. Association for Computational Linguistics.
James Pustejovsky, Robert Knippen, Jessica Littman, and
Roser Sauri. 2005. Temporal and event information in
natural language text. Language Resources and Evalu-
ation, 39(2-3):123?164.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent
Romary. 2010. Iso-timeml: An international standard
for semantic annotation. In Proceedings of the Seventh
International Conference on Language Resources and
Evaluation (LREC 2010), Valletta, Malta.
Angus Roberts, Robert Gaizauskas, Mark Hepple, George
Demetriou, Yikun Guo, and Ian Roberts. 2009. Build-
ing a semantically annotated corpus of clinical texts.
Journal of biomedical informatics, 42(5):950?966.
Guergana Savova, Steven Bethard, Will Styler, James Mar-
tin, Martha Palmer, James Masanz, and Wayne Ward.
2009. Towards temporal relation discovery from the
clinical narrative. In AMIA Annual Symposium Pro-
ceedings, volume 2009, page 568. American Medical
Informatics Association.
Tessa Sullivan, Ann Irvine, and Stephanie W Haas. 2008.
It?s all relative: usage of relative temporal expressions
in triage notes. Proceedings of the American Society
for Information Science and Technology, 45(1):1?8.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013a.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013b.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association, 20(5):806?813.
Alexander Turchin, Maria Shubina, Eugene Breydo,
Merri L Pendergrass, and Jonathan S Einbinder. 2009.
Comparison of information content of structured and
narrative text data sources on the example of medica-
tion intensification. Journal of the American Medical
Informatics Association, 16(3):362?370.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
James Allen, Marc Verhagen, and James Pustejovsky.
2013. Semeval-2013 task 1: Tempeval-3: Evaluating
time expressions, events, and temporal relations. In Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval 2013), pages 1?9, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2):211?241.
Veronika Vincze, Gyrgy Szarvas, Richrd Farkas, Gyrgy
Mra, and Jnos Csirik. 2008. The bioscope corpus:
biomedical texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics, 9(Suppl 11):1?
9.
Ying Zhao, George Karypis, and Usama M. Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discovery,
10:141?168.
Jiaping Zheng, Wendy W Chapman, Rebecca S Crowley,
and Guergana K Savova. 2011. Coreference resolution:
A review of general methodologies and applications in
the clinical domain. Journal of biomedical informatics,
44(6):1113?1122.
Li Zhou, Simon Parsons, and George Hripcsak. 2008. The
evaluation of a temporal reasoning system in processing
clinical discharge summaries. Journal of the American
Medical Informatics Association, 15(1):99?106.
154
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 63?68,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 14: Word Sense Induction & Disambiguation
Suresh Manandhar
Department of Computer Science
University of York, UK
Ioannis P. Klapaftis
Department of Computer Science
University of York, UK
Dmitriy Dligach
Department of Computer Science
University of Colorado, USA
Sameer S. Pradhan
BBN Technologies
Cambridge, USA
Abstract
This paper presents the description and
evaluation framework of SemEval-2010
Word Sense Induction & Disambiguation
task, as well as the evaluation results of 26
participating systems. In this task, partici-
pants were required to induce the senses of
100 target words using a training set, and
then disambiguate unseen instances of the
same words using the induced senses. Sys-
tems? answers were evaluated in: (1) an
unsupervised manner by using two clus-
tering evaluation measures, and (2) a su-
pervised manner in a WSD task.
1 Introduction
Word senses are more beneficial than simple word
forms for a variety of tasks including Information
Retrieval, Machine Translation and others (Pantel
and Lin, 2002). However, word senses are usually
represented as a fixed-list of definitions of a manu-
ally constructed lexical database. Several deficien-
cies are caused by this representation, e.g. lexical
databases miss main domain-specific senses (Pan-
tel and Lin, 2002), they often contain general defi-
nitions and suffer from the lack of explicit seman-
tic or contextual links between concepts (Agirre
et al, 2001). More importantly, the definitions of
hand-crafted lexical databases often do not reflect
the exact meaning of a target word in a given con-
text (V?eronis, 2004).
Unsupervised Word Sense Induction (WSI)
aims to overcome these limitations of hand-
constructed lexicons by learning the senses of a
target word directly from text without relying on
any hand-crafted resources. The primary aim of
SemEval-2010 WSI task is to allow comparison
of unsupervised word sense induction and disam-
biguation systems.
The target word dataset consists of 100 words,
50 nouns and 50 verbs. For each target word, par-
ticipants were provided with a training set in or-
der to learn the senses of that word. In the next
step, participating systems were asked to disam-
biguate unseen instances of the same words using
their learned senses. The answers of the systems
were then sent to organisers for evaluation.
2 Task description
Figure 1 provides an overview of the task. As
can be observed, the task consisted of three
separate phases. In the first phase, train-
ing phase, participating systems were provided
with a training dataset that consisted of a
set of target word (noun/verb) instances (sen-
tences/paragraphs). Participants were then asked
to use this training dataset to induce the senses
of the target word. No other resources were al-
lowed with the exception of NLP components for
morphology and syntax. In the second phase,
testing phase, participating systems were pro-
vided with a testing dataset that consisted of a
set of target word (noun/verb) instances (sen-
tences/paragraphs). Participants were then asked
to tag (disambiguate) each testing instance with
the senses induced during the training phase. In
the third and final phase, the tagged test instances
were received by the organisers in order to evalu-
ate the answers of the systems in a supervised and
an unsupervised framework. Table 1 shows the to-
tal number of target word instances in the training
and testing set, as well as the average number of
senses in the gold standard.
The main difference of the SemEval-2010 as
compared to the SemEval-2007 sense induction
task is that the training and testing data are treated
separately, i.e the testing data are only used for
sense tagging, while the training data are only used
63
Figure 1: Training, testing and evaluation phases of SemEval-2010 Task 14
Training set Testing set Senses (#)
All 879807 8915 3.79
Nouns 716945 5285 4.46
Verbs 162862 3630 3.12
Table 1: Training & testing set details
for sense induction. Treating the testing data as
new unseen instances ensures a realistic evalua-
tion that allows to evaluate the clustering models
of each participating system.
The evaluation framework of SemEval-2010
WSI task considered two types of evaluation.
In the first one, unsupervised evaluation, sys-
tems? answers were evaluated according to: (1) V-
Measure (Rosenberg and Hirschberg, 2007), and
(2) paired F-Score (Artiles et al, 2009). Nei-
ther of these measures were used in the SemEval-
2007 WSI task. Manandhar & Klapaftis (2009)
provide more details on the choice of this evalu-
ation setting and its differences with the previous
evaluation. The second type of evaluation, super-
vised evaluation, follows the supervised evalua-
tion of the SemEval-2007 WSI task (Agirre and
Soroa, 2007). In this evaluation, induced senses
are mapped to gold standard senses using a map-
ping corpus, and systems are then evaluated in a
standard WSD task.
2.1 Training dataset
The target word dataset consisted of 100 words,
i.e. 50 nouns and 50 verbs. The training dataset
for each target noun or verb was created by follow-
ing a web-based semi-automatic method, similar
to the method for the construction of Topic Signa-
tures (Agirre et al, 2001). Specifically, for each
WordNet (Fellbaum, 1998) sense of a target word,
we created a query of the following form:
<Target Word> AND <Relative Set>
The <Target Word> consisted of the target
word stem. The <Relative Set> consisted of a
disjunctive set of word lemmas that were related
Word Query
Sense
Sense 1 failure AND (loss OR nonconformity OR test
OR surrender OR ?force play? OR ...)
Sense 2 failure AND (ruination OR flop OR bust
OR stall OR ruin OR walloping OR ...)
Table 2: Training set creation: example queries for
target word failure
to the target word sense for which the query was
created. The relations considered were WordNet?s
hypernyms, hyponyms, synonyms, meronyms and
holonyms. Each query was manually checked by
one of the organisers to remove ambiguous words.
The following example shows the query created
for the first
1
and second
2
WordNet sense of the
target noun failure.
The created queries were issued to Yahoo!
search API
3
and for each query a maximum of
1000 pages were downloaded. For each page we
extracted fragments of text that occurred in <p>
</p> html tags and contained the target word
stem. In the final stage, each extracted fragment of
text was POS-tagged using the Genia tagger (Tsu-
ruoka and Tsujii, 2005) and was only retained, if
the POS of the target word in the extracted text
matched the POS of the target word in our dataset.
2.2 Testing dataset
The testing dataset consisted of instances of the
same target words from the training dataset. This
dataset is part of OntoNotes (Hovy et al, 2006).
We used the sense-tagged dataset in which sen-
tences containing target word instances are tagged
with OntoNotes (Hovy et al, 2006) senses. The
texts come from various news sources including
CNN, ABC and others.
1
An act that fails
2
An event that does not accomplish its intended purpose
3
http://developer.yahoo.com/search/ [Access:10/04/2010]
64
G1
G
2
G
3
C
1
10 10 15
C
2
20 50 0
C
3
1 10 60
C
4
5 0 0
Table 3: Clusters & GS senses matrix.
3 Evaluation framework
For the purposes of this section we provide an ex-
ample (Table 3) in which a target word has 181
instances and 3 GS senses. A system has gener-
ated a clustering solution with 4 clusters covering
all instances. Table 3 shows the number of com-
mon instances between clusters and GS senses.
3.1 Unsupervised evaluation
This section presents the measures of unsuper-
vised evaluation, i.e V-Measure (Rosenberg and
Hirschberg, 2007) and (2) paired F-Score (Artiles
et al, 2009).
3.1.1 V-Measure evaluation
Let w be a target word with N instances (data
points) in the testing dataset. Let K = {C
j
|j =
1 . . . n} be a set of automatically generated clus-
ters grouping these instances, and S = {G
i
|i =
1 . . .m} the set of gold standard classes contain-
ing the desirable groupings of w instances.
V-Measure (Rosenberg and Hirschberg, 2007)
assesses the quality of a clustering solution by ex-
plicitly measuring its homogeneity and its com-
pleteness. Homogeneity refers to the degree that
each cluster consists of data points primarily be-
longing to a single GS class, while completeness
refers to the degree that each GS class consists of
data points primarily assigned to a single cluster
(Rosenberg and Hirschberg, 2007). Let h be ho-
mogeneity and c completeness. V-Measure is the
harmonic mean of h and c, i.e. VM =
2?h?c
h+c
.
Homogeneity. The homogeneity, h, of a clus-
tering solution is defined in Formula 1, where
H(S|K) is the conditional entropy of the class
distribution given the proposed clustering and
H(S) is the class entropy.
h =
{
1, if H(S) = 0
1?
H(S|K)
H(S)
, otherwise
(1)
H(S) = ?
|S|
?
i=1
?
|K|
j=1
a
ij
N
log
?
|K|
j=1
a
ij
N
(2)
H(S|K) = ?
|K|
?
j=1
|S|
?
i=1
a
ij
N
log
a
ij
?
|S|
k=1
a
kj
(3)
When H(S|K) is 0, the solution is perfectly
homogeneous, because each cluster only contains
data points that belong to a single class. How-
ever in an imperfect situation, H(S|K) depends
on the size of the dataset and the distribution of
class sizes. Hence, instead of taking the raw con-
ditional entropy, V-Measure normalises it by the
maximum reduction in entropy the clustering in-
formation could provide, i.e. H(S). When there
is only a single class (H(S) = 0), any clustering
would produce a perfectly homogeneous solution.
Completeness. Symmetrically to homogeneity,
the completeness, c, of a clustering solution is de-
fined in Formula 4, where H(K|S) is the condi-
tional entropy of the cluster distribution given the
class distribution and H(K) is the clustering en-
tropy. When H(K|S) is 0, the solution is perfectly
complete, because all data points of a class belong
to the same cluster.
For the clustering example in Table 3, homo-
geneity is equal to 0.404, completeness is equal to
0.37 and V-Measure is equal to 0.386.
c =
{
1, if H(K) = 0
1?
H(K|S)
H(K)
, otherwise
(4)
H(K) = ?
|K|
?
j=1
?
|S|
i=1
a
ij
N
log
?
|S|
i=1
a
ij
N
(5)
H(K|S) = ?
|S|
?
i=1
|K|
?
j=1
a
ij
N
log
a
ij
?
|K|
k=1
a
ik
(6)
3.1.2 Paired F-Score evaluation
In this evaluation, the clustering problem is trans-
formed into a classification problem. For each
cluster C
i
we generate
(
|C
i
|
2
)
instance pairs, where
|C
i
| is the total number of instances that belong to
cluster C
i
. Similarly, for each GS class G
i
we gen-
erate
(
|G
i
|
2
)
instance pairs, where |G
i
| is the total
number of instances that belong to GS class G
i
.
Let F (K) be the set of instance pairs that ex-
ist in the automatically induced clusters and F (S)
be the set of instance pairs that exist in the gold
standard. Precision can be defined as the number
of common instance pairs between the two sets to
the total number of pairs in the clustering solu-
tion (Equation 7), while recall can be defined as
the number of common instance pairs between the
two sets to the total number of pairs in the gold
65
standard (Equation 8). Finally, precision and re-
call are combined to produce the harmonic mean
(FS =
2?P ?R
P+R
).
P =
|F (K) ? F (S)|
|F (K)|
(7)
R =
|F (K) ? F (S)|
|F (S)|
(8)
For example in Table 3, we can generate
(
35
2
)
in-
stance pairs for C
1
,
(
70
2
)
for C
2
,
(
71
2
)
for C
3
and
(
5
2
)
for C
4
, resulting in a total of 5505 instance
pairs. In the same vein, we can generate
(
36
2
)
in-
stance pairs for G
1
,
(
70
2
)
for G
2
and
(
75
2
)
for G
3
. In
total, the GS classes contain 5820 instance pairs.
There are 3435 common instance pairs, hence pre-
cision is equal to 62.39%, recall is equal to 59.09%
and paired F-Score is equal to 60.69%.
3.2 Supervised evaluation
In this evaluation, the testing dataset is split into a
mapping and an evaluation corpus. The first one
is used to map the automatically induced clusters
to GS senses, while the second is used to evaluate
methods in a WSD setting. This evaluation fol-
lows the supervised evaluation of SemEval-2007
WSI task (Agirre and Soroa, 2007), with the dif-
ference that the reported results are an average
of 5 random splits. This repeated random sam-
pling was performed to avoid the problems of the
SemEval-2007 WSI challenge, in which different
splits were providing different system rankings.
Let us consider the example in Table 3 and as-
sume that this matrix has been created by using the
mapping corpus. Table 3 shows that C
1
is more
likely to be associated with G
3
, C
2
is more likely
to be associated with G
2
, C
3
is more likely to be
associated with G
3
and C
4
is more likely to be as-
sociated with G
1
. This information can be utilised
to map the clusters to GS senses.
Particularly, the matrix shown in Table 3 is nor-
malised to produce a matrix M , in which each
entry depicts the estimated conditional probabil-
ity P (G
i
|C
j
). Given an instance I of tw from
the evaluation corpus, a row cluster vector IC is
created, in which each entry k corresponds to the
score assigned to C
k
to be the winning cluster for
instance I . The product of IC and M provides a
row sense vector, IG, in which the highest scor-
ing entry a denotes that G
a
is the winning sense.
For example, if we produce the row cluster vector
[C
1
= 0.8, C
2
= 0.1, C
3
= 0.1, C
4
= 0.0], and
System VM (%) VM (%) VM (%) #Cl
(All) (Nouns) (Verbs)
Hermit 16.2 16.7 15.6 10.78
UoY 15.7 20.6 8.5 11.54
KSU KDD 15.7 18 12.4 17.5
Duluth-WSI 9 11.4 5.7 4.15
Duluth-WSI-SVD 9 11.4 5.7 4.15
Duluth-R-110 8.6 8.6 8.5 9.71
Duluth-WSI-Co 7.9 9.2 6 2.49
KCDC-PCGD 7.8 7.3 8.4 2.9
KCDC-PC 7.5 7.7 7.3 2.92
KCDC-PC-2 7.1 7.7 6.1 2.93
Duluth-Mix-Narrow-Gap 6.9 8 5.1 2.42
KCDC-GD-2 6.9 6.1 8 2.82
KCDC-GD 6.9 5.9 8.5 2.78
Duluth-Mix-Narrow-PK2 6.8 7.8 5.5 2.68
Duluth-MIX-PK2 5.6 5.8 5.2 2.66
Duluth-R-15 5.3 5.4 5.1 4.97
Duluth-WSI-Co-Gap 4.8 5.6 3.6 1.6
Random 4.4 4.2 4.6 4
Duluth-R-13 3.6 3.5 3.7 3
Duluth-WSI-Gap 3.1 4.2 1.5 1.4
Duluth-Mix-Gap 3 2.9 3 1.61
Duluth-Mix-Uni-PK2 2.4 0.8 4.7 2.04
Duluth-R-12 2.3 2.2 2.5 2
KCDC-PT 1.9 1 3.1 1.5
Duluth-Mix-Uni-Gap 1.4 0.2 3 1.39
KCDC-GDC 7 6.2 7.8 2.83
MFS 0 0 0 1
Duluth-WSI-SVD-Gap 0 0 0.1 1.02
Table 4: V-Measure unsupervised evaluation
multiply it with the normalised matrix of Table 3,
then we would get a row sense vector in which G
3
would be the winning sense with a score equal to
0.43.
4 Evaluation results
In this section, we present the results of the 26
systems along with two baselines. The first base-
line, Most Frequent Sense (MFS), groups all test-
ing instances of a target word into one cluster. The
second baseline, Random, randomly assigns an in-
stance to one out of four clusters. The number
of clusters of Random was chosen to be roughly
equal to the average number of senses in the GS.
This baseline is executed five times and the results
are averaged.
4.1 Unsupervised evaluation
Table 4 shows the V-Measure (VM) performance
of the 26 systems participating in the task. The last
column shows the number of induced clusters of
each system in the test set.The MFS baseline has a
V-Measure equal to 0, since by definition its com-
pleteness is 1 and homogeneity is 0. All systems
outperform this baseline, apart from one, whose
V-Measure is equal to 0. Regarding the Random
baseline, we observe that 17 perform better, which
indicates that they have learned useful information
better than chance.
Table 4 also shows that V-Measure tends to
favour systems producing a higher number of clus-
66
System FS (%) FS (%) FS (%) #Cl
(All) (Nouns) (Verbs)
MFS 63.5 57.0 72.7 1
Duluth-WSI-SVD-Gap 63.3 57.0 72.4 1.02
KCDC-PT 61.8 56.4 69.7 1.5
KCDC-GD 59.2 51.6 70.0 2.78
Duluth-Mix-Gap 59.1 54.5 65.8 1.61
Duluth-Mix-Uni-Gap 58.7 57.0 61.2 1.39
KCDC-GD-2 58.2 50.4 69.3 2.82
KCDC-GDC 57.3 48.5 70.0 2.83
Duluth-Mix-Uni-PK2 56.6 57.1 55.9 2.04
KCDC-PC 55.5 50.4 62.9 2.92
KCDC-PC-2 54.7 49.7 61.7 2.93
Duluth-WSI-Gap 53.7 53.4 53.9 1.4
KCDC-PCGD 53.3 44.8 65.6 2.9
Duluth-WSI-Co-Gap 52.6 53.3 51.5 1.6
Duluth-MIX-PK2 50.4 51.7 48.3 2.66
UoY 49.8 38.2 66.6 11.54
Duluth-Mix-Narrow-Gap 49.7 47.4 51.3 2.42
Duluth-WSI-Co 49.5 50.2 48.2 2.49
Duluth-Mix-Narrow-PK2 47.8 37.1 48.2 2.68
Duluth-R-12 47.8 44.3 52.6 2
Duluth-WSI-SVD 41.1 37.1 46.7 4.15
Duluth-WSI 41.1 37.1 46.7 4.15
Duluth-R-13 38.4 36.2 41.5 3
KSU KDD 36.9 24.6 54.7 17.5
Random 31.9 30.4 34.1 4
Duluth-R-15 27.6 26.7 28.9 4.97
Hermit 26.7 24.4 30.1 10.78
Duluth-R-110 16.1 15.8 16.4 9.71
Table 5: Paired F-Score unsupervised evaluation
ters than the number of GS senses, although V-
Measure does not increase monotonically with the
number of clusters increasing. For that reason,
we introduced the second unsupervised evaluation
measure (paired F-Score) that penalises systems
when they produce: (1) a higher number of clus-
ters (low recall) or (2) a lower number of clusters
(low precision), than the GS number of senses.
Table 5 shows the performance of systems us-
ing the second unsupervised evaluation measure.
In this evaluation, we observe that most of the sys-
tems perform better than Random. Despite that,
none of the systems outperform the MFS baseline.
It seems that systems generating a smaller number
of clusters than the GS number of senses are bi-
ased towards the MFS, hence they are not able to
perform better. On the other hand, systems gen-
erating a higher number of clusters are penalised
by this measure. Systems generating a number of
clusters roughly the same as the GS tend to con-
flate the GS senses lot more than the MFS.
4.2 Supervised evaluation results
Table 6 shows the results of this evaluation for a
80-20 test set split, i.e. 80% for mapping and 20%
for evaluation. The last columns shows the aver-
age number of GS senses identified by each sys-
tem in the five splits of the evaluation datasets.
Overall, 14 systems outperform the MFS, while 17
of them perform better than Random. The ranking
of systems in nouns and verbs is different. For in-
System SR (%) SR (%) SR (%) #S
(All) (Nouns) (Verbs)
UoY 62.4 59.4 66.8 1.51
Duluth-WSI 60.5 54.7 68.9 1.66
Duluth-WSI-SVD 60.5 54.7 68.9 1.66
Duluth-WSI-Co-Gap 60.3 54.1 68.6 1.19
Duluth-WSI-Co 60.8 54.7 67.6 1.51
Duluth-WSI-Gap 59.8 54.4 67.8 1.11
KCDC-PC-2 59.8 54.1 68.0 1.21
KCDC-PC 59.7 54.6 67.3 1.39
KCDC-PCGD 59.5 53.3 68.6 1.47
KCDC-GDC 59.1 53.4 67.4 1.34
KCDC-GD 59.0 53.0 67.9 1.33
KCDC-PT 58.9 53.1 67.4 1.08
KCDC-GD-2 58.7 52.8 67.4 1.33
Duluth-WSI-SVD-Gap 58.7 53.2 66.7 1.01
MFS 58.7 53.2 66.6 1
Duluth-R-12 58.5 53.1 66.4 1.25
Hermit 58.3 53.6 65.3 2.06
Duluth-R-13 58.0 52.3 66.4 1.46
Random 57.3 51.5 65.7 1.53
Duluth-R-15 56.8 50.9 65.3 1.61
Duluth-Mix-Narrow-Gap 56.6 48.1 69.1 1.43
Duluth-Mix-Narrow-PK2 56.1 47.5 68.7 1.41
Duluth-R-110 54.8 48.3 64.2 1.94
KSU KDD 52.2 46.6 60.3 1.69
Duluth-MIX-PK2 51.6 41.1 67.0 1.23
Duluth-Mix-Gap 50.6 40.0 66.0 1.01
Duluth-Mix-Uni-PK2 19.3 1.8 44.8 0.62
Duluth-Mix-Uni-Gap 18.7 1.6 43.8 0.56
Table 6: Supervised recall (SR) (test set split:80%
mapping, 20% evaluation)
stance, the highest ranked system in nouns is UoY,
while in verbs Duluth-Mix-Narrow-Gap. It seems
that depending on the part-of-speech of the target
word, different algorithms, features and parame-
ters? tuning have different impact.
The supervised evaluation changes the distri-
bution of clusters by mapping each cluster to a
weighted vector of senses. Hence, it can poten-
tially favour systems generating a high number of
homogeneous clusters. For that reason, we applied
a second testing set split, where 60% of the testing
corpus was used for mapping and 40% for eval-
uation. Reducing the size of the mapping corpus
allows us to observe, whether the above statement
is correct, since systems with a high number of
clusters would suffer from unreliable mapping.
Table 7 shows the results of the second super-
vised evaluation. The ranking of participants did
not change significantly, i.e. we observe only dif-
ferent rankings among systems belonging to the
same participant. Despite that, Table 7 also shows
that the reduction of the mapping corpus has a dif-
ferent impact on systems generating a larger num-
ber of clusters than the GS number of senses.
For instance, UoY that generates 11.54 clusters
outperformed the MFS by 3.77% in the 80-20 split
and by 3.71% in the 60-40 split. The reduction of
the mapping corpus had a minimal impact on its
performance. In contrast, KSU KDD that gener-
ates 17.5 clusters was below the MFS by 6.49%
67
System SR (%) SR (%) SR (%) #S
(All) (Nouns) (Verbs)
UoY 62.0 58.6 66.8 1.66
Duluth-WSI-Co 60.1 54.6 68.1 1.56
Duluth-WSI-Co-Gap 59.5 53.5 68.3 1.2
Duluth-WSI-SVD 59.5 53.5 68.3 1.73
Duluth-WSI 59.5 53.5 68.3 1.73
Duluth-WSI-Gap 59.3 53.2 68.2 1.11
KCDC-PCGD 59.1 52.6 68.6 1.54
KCDC-PC-2 58.9 53.4 67.0 1.25
KCDC-PC 58.9 53.6 66.6 1.44
KCDC-GDC 58.3 52.1 67.3 1.41
KCDC-GD 58.3 51.9 67.6 1.42
MFS 58.3 52.5 66.7 1
KCDC-PT 58.3 52.2 67.1 1.11
Duluth-WSI-SVD-Gap 58.2 52.5 66.7 1.01
KCDC-GD-2 57.9 51.7 67.0 1.44
Duluth-R-12 57.7 51.7 66.4 1.27
Duluth-R-13 57.6 51.1 67.0 1.48
Hermit 57.3 52.5 64.2 2.27
Duluth-R-15 56.5 50.0 66.1 1.76
Random 56.5 50.2 65.7 1.65
Duluth-Mix-Narrow-Gap 56.2 47.7 68.6 1.51
Duluth-Mix-Narrow-PK2 55.7 46.9 68.5 1.51
Duluth-R-110 53.6 46.7 63.6 2.18
Duluth-MIX-PK2 50.5 39.7 66.1 1.31
KSU KDD 50.4 44.3 59.4 1.92
Duluth-Mix-Gap 49.8 38.9 65.6 1.04
Duluth-Mix-Uni-PK2 19.1 1.8 44.4 0.63
Duluth-Mix-Uni-Gap 18.9 1.5 44.2 0.56
Table 7: Supervised recall (SR) (test set split:60%
mapping, 40% evaluation)
in the 80-20 split and by 7.83% in the 60-40 split.
The reduction of the mapping corpus had a larger
impact in this case. This result indicates that the
performance in this evaluation also depends on the
distribution of instances within the clusters. Sys-
tems generating a skewed distribution, in which a
small number of homogeneous clusters tag the ma-
jority of instances and a larger number of clusters
tag only a few instances, are likely to have a bet-
ter performance than systems that produce a more
uniform distribution.
5 Conclusion
We presented the description, evaluation frame-
work and assessment of systems participating in
the SemEval-2010 sense induction task. The eval-
uation has shown that the current state-of-the-art
lacks unbiased measures that objectively evaluate
clustering.
The results of systems have shown that their
performance in the unsupervised and supervised
evaluation settings depends on cluster granularity
along with the distribution of instances within the
clusters. Our future work will focus on the assess-
ment of sense induction on a task-oriented basis as
well as on clustering evaluation.
Acknowledgements
We gratefully acknowledge the support of the EU
FP7 INDECT project, Grant No. 218086, the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team.
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating Word Sense Induction and Dis-
crimination Systems. In Proceedings of SemEval-
2007, pages 7?12, Prague, Czech Republic. ACL.
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching Wordnet Concepts With
Topic Signatures. ArXiv Computer Science e-prints.
Javier Artiles, Enrique Amig?o, and Julio Gonzalo.
2009. The role of named entities in web people
search. In Proceedings of EMNLP, pages 534?542.
ACL.
Christiane Fellbaum. 1998. Wordnet: An Electronic
Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL, Com-
panion Volume: Short Papers on XX, pages 57?60.
ACL.
Suresh Manandhar and Ioannis P. Klapaftis. 2009.
Semeval-2010 Task 14: Evaluation Setting for Word
Sense Induction & Disambiguation Systems. In
DEW ?09: Proceedings of the Workshop on Se-
mantic Evaluations: Recent Achievements and Fu-
ture Directions, pages 117?122, Boulder, Colorado,
USA. ACL.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text. In KDD ?02: Proceedings
of the 8th ACM SIGKDD Conference, pages 613?
619, New York, NY, USA. ACM.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A Conditional Entropy-based External
Cluster Evaluation Measure. In Proceedings of the
2007 EMNLP-CoNLL Joint Conference, pages 410?
420, Prague, Czech Republic.
Yoshimasa Tsuruoka and Jun??chi Tsujii. 2005. Bidi-
rectional Inference With the Easiest-first Strategy
for Tagging Sequence Data. In Proceedings of
the HLT-EMNLP Joint Conference, pages 467?474,
Morristown, NJ, USA.
Jean V?eronis. 2004. Hyperlex: Lexical Cartography
for Information Retrieval. Computer Speech & Lan-
guage, 18(3):223?252.
68
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 54?62,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 7: Analysis of Clinical Text
Sameer Pradhan
1
, No
?
emie Elhadad
2
, Wendy Chapman
3
,
Suresh Manandhar
4
and Guergana Savova
1
1
Harvard University, Boston, MA,
2
Columbia University, New York, NY
3
University of Utah, Salt Lake City, UT,
4
University of York, York, UK
{sameer.pradhan,guergana.savova}@childrens.harvard.edu, noemie.elhadad@columbia.edu,
wendy.chapman@utah.edu, suresh@cs.york.ac.uk
Abstract
This paper describes the SemEval-2014,
Task 7 on the Analysis of Clinical Text
and presents the evaluation results. It fo-
cused on two subtasks: (i) identification
(Task A) and (ii) normalization (Task B)
of diseases and disorders in clinical reports
as annotated in the Shared Annotated Re-
sources (ShARe)
1
corpus. This task was
a follow-up to the ShARe/CLEF eHealth
2013 shared task, subtasks 1a and 1b,
2
but
using a larger test set. A total of 21 teams
competed in Task A, and 18 of those also
participated in Task B. For Task A, the
best system had a strict F
1
-score of 81.3,
with a precision of 84.3 and recall of 78.6.
For Task B, the same group had the best
strict accuracy of 74.1. The organizers
have made the text corpora, annotations,
and evaluation tools available for future re-
search and development at the shared task
website.
3
1 Introduction
A large amount of very useful information?both
for medical researchers and patients?is present
in the form of unstructured text within the clin-
ical notes and discharge summaries that form a
patient?s medical history. Adapting and extend-
ing natural language processing (NLP) techniques
to mine this information can open doors to bet-
ter, novel, clinical studies on one hand, and help
patients understand the contents of their clini-
cal records on the other. Organization of this
1
http://share.healthnlp.org
2
https://sites.google.com/site/shareclefehealth/
evaluation
3
http://alt.qcri.org/semeval2014/task7/
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shared task helps establish state-of-the-art bench-
marks and paves the way for further explorations.
It tackles two important sub-problems in NLP?
named entity recognition and word sense disam-
biguation. Neither of these problems are new to
NLP. Research in general-domain NLP goes back
to about two decades. For an overview of the
development in the field through roughly 2009,
we refer the refer to Nadeau and Sekine (2007).
NLP has also penetrated the field of bimedical
informatics and has been particularly focused on
biomedical literature for over the past decade. Ad-
vances in that sub-field has also been documented
in surveys such as one by Leaman and Gonza-
lez (2008). Word sense disambiguation also has
a long history in the general NLP domain (Nav-
igli, 2009). In spite of word sense annotations in
the biomedical literature, recent work by Savova
et al. (2008) highlights the importance of annotat-
ing them in clinical notes. This is true for many
other clinical and linguistic phenomena as the var-
ious characteristics of the clinical narrative present
a unique challenge to NLP. Recently various ini-
tiatives have led to annotated corpora for clini-
cal NLP research. Probably the first comprehen-
sive annotation performed on a clinical corpora
was by Roberts et al. (2009), but unfortunately
that corpus is not publicly available owing to pri-
vacy regulations. The i2b2 initiative
4
challenges
have focused on such topics as concept recog-
nition (Uzuner et al., 2011), coreference resolu-
tion (Uzuner et al., 2012), temporal relations (Sun
et al., 2013) and their datasets are available to the
community. More recently, the Shared Annotated
Resources (ShARe)
1
project has created a corpus
annotated with disease/disorder mentions in clini-
cal notes as well as normalized them to a concept
unique identifier (CUI) within the SNOMED-CT
subset of the Unified Medical Language System
5
4
http://www.i2b2.org
5
https://uts.nlm.nih.gov/home.html
54
Train Development Test
Notes 199 99 133
Words 94K 88K 153K
Disorder mentions 5,816 5,351 7,998
CUI-less mentions 1,639 (28%) 1,750 (32%) 1,930 (24%)
CUI-ied mentions 4,117 (72%) 3,601 (67%) 6,068 (76%)
Contiguous mentions 5,165 (89%) 4,912 (92%) 7,374 (92%)
Discontiguous mentions 651 (11%) 439 (8%) 6,24 (8%)
Table 1: Distribution of data in terms of notes and disorder mentions across the training, development
and test sets. The disorders are further split according to two criteria ? whether they map to a CUI or
whether they are contiguous.
(UMLS) (Campbell et al., 1998). The task of nor-
malization is a combination of word/phrase sense
disambiguation and semantic similarity where a
phrase is mapped to a unique concept in an on-
tology (based on the description of that concept in
the ontology) after disambiguating potential am-
biguous surface words, or phrases. This is espe-
cially true with abbreviations and acronyms which
are much more common in clinical text (Moon et
al., 2012). The SemEval-2014 task 7 was one of
nine shared tasks organized at the SemEval-2014.
It was designed as a follow up to the shared tasks
organized during the ShARe/CLEF eHealth 2013
evaluation (Suominen et al., 2013; Pradhan et al.,
2013; Pradhan et al., 2014). Like the previous
shared task, we relied on the ShARe corpus, but
with more data for training and a new test set. Fur-
thermore, in this task, we provided the options to
participants to utilize a large corpus of unlabeled
clinical notes. The rest of the paper is organized as
follows. Section 2 describes the characteristics of
the data used in the task. Section 3 describes the
tasks in more detail. Section 4 explains the evalu-
ation criteria for the two tasks. Section 5 lists the
participants of the task. Section 6 discusses the re-
sults on this task and also compares them with the
ShARe/CLEF eHealth 2013 results, and Section 7
concludes.
2 Data
The ShARe corpus comprises annotations over
de-identified clinical reports from a US intensive
care department (version 2.5 of the MIMIC II
database
6
) (Saeed et al., 2002). It consists of
discharge summaries, electrocardiogram, echocar-
diogram, and radiology reports. Access to data
was carried out following MIMIC user agreement
requirements for access to de-identified medical
6
http://mimic.physionet.org ? Multiparameter Intelligent
Monitoring in Intensive Care
data. Hence, all participants were required to reg-
ister for the evaluation, obtain a US human sub-
jects training certificate
7
, create an account to the
password-protected MIMIC site, specify the pur-
pose of data usage, accept the data use agree-
ment, and get their account approved. The anno-
tation focus was on disorder mentions, their var-
ious attributes and normalizations to an UMLS
CUI. As such, there were two parts to the annota-
tion: identifying a span of text as a disorder men-
tion and normalizing (or mapping) the span to a
UMLS CUI. The UMLS represents over 130 lex-
icons/thesauri with terms from a variety of lan-
guages and integrates resources used world-wide
in clinical care, public health, and epidemiology.
A disorder mention was defined as any span of text
which can be mapped to a concept in SNOMED-
CT and which belongs to the Disorder semantic
group
8
. It also provided a semantic network in
which every concept is represented by its CUI
and is semantically typed (Bodenreider and Mc-
Cray, 2003). A concept was in the Disorder se-
mantic group if it belonged to one of the follow-
ing UMLS semantic types: Congenital Abnormal-
ity; Acquired Abnormality; Injury or Poisoning;
Pathologic Function; Disease or Syndrome; Men-
tal or Behavioral Dysfunction; Cell or Molecu-
lar Dysfunction; Experimental Model of Disease;
Anatomical Abnormality; Neoplastic Process; and
Signs and Symptoms. The Finding semantic type
was left out as it is very noisy and our pilot study
showed lower annotation agreement on it. Follow-
ing are the salient aspects of the guidelines used to
7
The course was available free of charge on the Internet, for example,
via the CITI Collaborative Institutional Training Initiative at
https://www.citiprogram.org/Default.asp
or, the US National Institutes of Health (NIH) at
http://phrp.nihtraining.com/users.
8
Note that this definition of Disorder semantic group did not include the
Findings semantic type, and as such differed from the one of UMLS Seman-
tic Groups, available at http://semanticnetwork.nlm.nih.gov/
SemGroups
55
annotate the data.
? Annotations represent the most specific dis-
order span. For example, small bowel ob-
struction is preferred over bowel obstruction.
? A disorder mention is a concept in the
SNOMED-CT portion of the Disorder se-
mantic group.
? Negation and temporal modifiers are not con-
sidered part of the disorder mention span.
? All disorder mentions are annotated?even
the ones related to a person other than the pa-
tient and including acronyms and abbrevia-
tions.
? Mentions of disorders that are coreferen-
tial/anaphoric are also annotated.
Following are a few examples of disorder men-
tions from the data.
Patient found to have lower extremity DVT. (E1)
In example (E1), lower extremity DVT is marked
as the disorder. It corresponds to CUI C0340708
(preferred term: Deep vein thrombosis of lower
limb). The span DVT can be mapped to CUI
C0149871 (preferred term: Deep Vein Thrombo-
sis), but this mapping would be incorrect because
it is part of a more specific disorder in the sen-
tence, namely lower extremity DVT.
A tumor was found in the left ovary. (E2)
In example (E2), tumor ... ovary is annotated as a
discontiguous disorder mention. This is the best
method of capturing the exact disorder mention
in clinical notes and its novelty is in the fact that
either such phenomena have not been seen fre-
quently enough in the general domain to gather
particular attention, or the lack of a manually
curated general domain ontology parallel to the
UMLS.
Patient admitted with low blood pressure. (E3)
There are some disorders that do not have a rep-
resentation to a CUI as part of the SNOMED CT
within the UMLS. However, if they were deemed
important by the annotators then they were anno-
tated as CUI-less mentions. In example (E3), low
blood pressure is a finding and is normalized as
a CUI-less disorder. We constructed the annota-
tion guidelines to require that the disorder be a
reasonable synonym of the lexical description of a
SNOMED-CT disorder. There are a few instances
where the disorders are abbreviated or shortened
in the clinical note. One example is w/r/r, which
is an abbreviation for concepts wheezing (CUI
C0043144), rales (CUI C0034642), and ronchi
(CUI C0035508). This abbreviation is also some-
times written as r/w/r and r/r/w. Another is gsw for
gunshot wound and tachy for tachycardia. More
details on the annotation scheme is detailed in the
guidelines
9
and in a forthcoming manuscript. The
annotations covered about 336K words. Table 1
shows the quantity of the data and the split across
the training, development and test sets as well as
in terms of the number of notes and the number of
words.
2.1 Annotation Quality
Each note in the training and development set was
annotated by two professional coders trained for
this task, followed by an open adjudication step.
By the time we reached annotating the test data,
the annotators were quite familiar with the anno-
tation and so, in order to save time, we decided
to perform a single annotation pass using a senior
annotator. This was followed by a correction pass
by the same annotator using a checklist of frequent
annotation issues faced earlier. Table 2 shows the
inter-annotator agreement (IAA) statistics for the
adjudicated data. For the disorders we measure the
agreement in terms of the F
1
-score as traditional
agreement measures such as Cohen?s kappa and
Krippendorf?s alpha are not applicable for measur-
ing agreement for entity mention annotation. We
computed agreements between the two annotators
as well as between each annotator and the final ad-
judicated gold standard. The latter is to give a
sense of the fraction of corrections made in the
process of adjudication. The strict criterion con-
siders two mentions correct if they agree in terms
of the class and the exact string, whereas the re-
laxed criteria considers overlapping strings of the
9
http://goo.gl/vU8KdW
Disorder CUI
Relaxed Strict Relaxed Strict
F
1
F
1
Acc. Acc.
A1-A2 90.9 76.9 77.6 84.6
A1-GS 96.8 93.2 95.4 97.3
A2-GS 93.7 82.6 80.6 86.3
Table 2: Inter-annotator (A1 and A2) and gold
standard (GS) agreement as F
1
-score for the Dis-
order mentions and their normalization to the
UMLS CUI.
56
Institution User ID Team ID
University of Pisa, Italy attardi UniPI
University of Lisbon, Portugal francisco ULisboa
University of Wisconsin, Milwaukee, USA ghiasvand UWM
University of Colorado, Boulder, USA gung CLEAR
University of Guadalajara, Mexico herrera UG
Taipei Medical University, Taiwan hjdai TMU
University of Turku, Finland kaewphan UTU
University of Szeged, Hungary katona SZTE-NLP
Queensland University of Queensland, Australia kholghi QUT AEHRC
KU Leuven, Belgium kolomiyets KUL
Universidade de Aveiro, Portugal nunes BioinformaticsUA
University of the Basque Country, Spain oronoz IxaMed
IBM, India parikh ThinkMiners
easy data intelligence, India pathak ezDI
RelAgent Tech Pvt. Ltd., India ramanan RelAgent
Universidad Nacional de Colombia, Colombia riveros MindLab-UNAL
IIT Patna, India sikdar IITP
University of North Texas, USA solomon UNT
University of Illinois at Urbana Champaign, USA upadhya CogComp
The University of Texas Health Science Center at Houston, USA wu UTH CCB
East China Normal University, China yi ECNU
Table 3: Participant organization and the respective User IDs and Team IDs.
same class as correct. The reason for checking
the class is as follows. Although we only use the
disorder mention in this task, the corpus has been
annotated with some other UMLS types as well
and therefore there are instances where a differ-
ent UMLS type is assigned to the same character
span in the text by the second annotator. If exact
boundaries are not taken into account then the IAA
agreement score is in the mid-90s. For the task of
normalization to CUIs, we used accuracy to assess
agreement. For the relaxed criterion, all overlap-
ping disorder spans with the same CUI were con-
sidered correct. For the strict criterion, only disor-
der spans with identical spans and the same CUI
were considered correct.
3 Task Description
The participants were evaluated on the following
two tasks:
? Task A ? Identification of the character spans
of disorder mentions.
? Task B ? Normalizing disorder mentions to
SNOMED-CT subset of UMLS CUIs.
For Task A, participants were instructed to develop
a system that predicts the spans for disorder men-
tions. For Tasks B, participants were instructed
to develop a system that predicts the UMLS CUI
within the SNOMED-CT vocabulary. The input to
Task B were the disorder mention predictions from
Task A. Task B was optional. System outputs ad-
hered to the annotation format. Each participant
was allowed to submit up to three runs. The en-
tire set of unlabeled MIMIC clinical notes (exclud-
ing the test notes) were made available to the par-
ticipants for potential unsupervised approaches to
enhance the performance of their systems. They
were allowed to use additional annotations in their
systems, but this counted towards the total allow-
able runs; systems that used annotations outside
of those provided were evaluated separately. The
evaluation for all tasks was conducted using the
blind, withheld test data. The participants were
provided a training set containing clinical text as
well as pre-annotated spans and named entities for
disorders (Tasks A and B).
4 Evaluation Criteria
The following evaluation criteria were used:
? Task A ? The system performance was eval-
uated against the gold standard using the
F
1
-score of the Precision and Recall values.
There were two variations: (i) Strict; and (ii)
Relaxed. The formulae for computing these
metrics are mentioned below.
Precision = P =
D
tp
D
tp
+ D
fp
(1)
Recall = R =
D
tp
D
tp
+ D
fn
(2)
Where, D
tp
= Number of true positives dis-
order mentions; D
fp
= Number of false pos-
itives disorder mentions; D
fn
= Number of
false negative disorder mentions. In the strict
case, a span was counted as correct if it was
identical to the gold standard span, whereas
57
Task A
Strict Relaxed
Team ID User ID Run P R F
1
P R F
1
Data
(%) (%) (%) (%) (%) (%)
UTH CCB wu 0 84.3 78.6 81.3 93.6 86.6 90.0 T+D
UTH CCB wu 1 80.8 80.5 80.6 91.6 90.7 91.1 T+D
UTU kaewphan 1 76.5 76.7 76.6 88.6 89.9 89.3 T+D
UWM ghiasvand 0 78.7 72.6 75.5 91.1 85.6 88.3 T+D
UTH CCB wu 2 68.0 84.9 75.5 83.8 93.5 88.4 T+D
UTU kaewphan 0 77.3 72.4 74.8 90.1 85.6 87.8 T
IxaMed oronoz 1 68.1 78.6 73.0 87.2 89.0 88.1 T+D
UWM ghiasvand 0 77.5 67.9 72.4 90.9 81.2 85.8 T
RelAgent ramanan 0 74.1 70.1 72.0 89.5 84.0 86.7 T+D
IxaMed oronoz 0 72.9 70.1 71.5 88.5 80.8 84.5 T+D
ezDI pathak 1 75.0 68.2 71.4 91.5 82.7 86.9 T
CLEAR gung 0 80.7 63.6 71.2 92.0 72.3 81.0 T
ezDI pathak 0 75.0 67.7 71.2 91.4 81.9 86.4 T
ULisboa francisco 0 75.3 66.3 70.5 91.4 81.5 86.2 T
ULisboa francisco 1 75.2 66.0 70.3 90.9 80.6 85.5 T
ULisboa francisco 2 75.2 66.0 70.3 90.9 80.6 85.5 T
BioinformaticsUA nunes 0 81.3 60.5 69.4 92.9 69.3 79.4 T+D
ThinkMiners parikh 0 73.4 65.0 68.9 89.2 80.2 84.4 T
ThinkMiners parikh 1 74.9 61.7 67.7 90.7 75.8 82.6 T
ECNU yi 0 75.4 61.1 67.5 89.8 72.2 80.0 T+D
UniPI attardi 2 71.2 60.1 65.2 89.7 76.6 82.6 T+D
UNT solomon 0 64.7 62.8 63.8 81.5 79.9 80.7 T+D
UniPI attardi 1 65.9 61.2 63.5 90.2 77.5 83.4 T+D
BioinformaticsUA nunes 2 75.3 53.8 62.8 86.5 62.1 72.3 T+D
BioinformaticsUA nunes 1 60.0 62.1 61.0 69.8 72.3 71.0 T+D
UniPI attardi 0 53.9 68.4 60.2 77.8 88.5 82.8 T+D
CogComp upadhya 1 63.9 52.9 57.9 82.3 68.3 74.6 T+D
CogComp upadhya 2 64.1 52.0 57.4 82.9 67.5 74.4 T+D
CogComp upadhya 0 63.6 51.5 56.9 81.9 66.5 73.4 T+D
TMU hjdai 0 52.4 57.6 54.9 91.4 76.5 83.3 T+D
MindLab-UNAL riveros 2 56.1 53.4 54.7 76.9 67.7 72.0 T
MindLab-UNAL riveros 1 57.8 51.5 54.5 77.7 65.4 71.0 T
TMU hjdai 1 62.2 42.9 50.8 89.9 65.2 75.6 T+D
IITP sikdar 0 50.0 47.9 48.9 81.5 79.7 80.6 T+D
IITP sikdar 1 47.3 45.8 46.5 78.9 77.6 78.2 T+D
IITP sikdar 2 45.0 48.1 46.5 76.9 82.6 79.6 T+D
MindLab-UNAL riveros 0 32.1 56.5 40.9 43.9 72.5 54.7 T
SZTE-NLP katona 1 54.7 25.2 34.5 88.4 40.1 55.1 T
SZTE-NLP katona 2 54.7 25.2 34.5 88.4 40.1 55.1 T
QUT AEHRC kholghi 0 38.7 29.8 33.7 90.6 70.9 79.5 T+D
SZTE-NLP katona 0 57.1 20.5 30.2 91.8 32.5 48.0 T
KUL kolomiyets 0 65.5 17.8 28.0 72.1 19.6 30.8 P
UG herrera 0 11.4 23.4 15.3 25.9 49.0 33.9 P
Table 4: Performance on test data for participating systems on Task A ? Identification of disorder men-
tions.
Task A
Strict Relaxed
Team ID User ID Run P R F
1
P R F
1
Data
(%) (%) (%) (%) (%) (%)
hjdai TMU 1 0.687 0.922 0.787 0.952 1.000 0.975 T
wu UTH CCB 0 0.877 0.710 0.785 0.962 0.789 0.867 T
wu UTH CCB 1 0.828 0.747 0.785 0.941 0.853 0.895 T
Best ShARe/CLEF-2013 performance 0.800 0.706 0.750 0.925 0.827 0.873 T
ghiasvand UWM 0 0.827 0.675 0.743 0.958 0.799 0.871 T
pathak ezDI 0 0.813 0.670 0.734 0.954 0.800 0.870 T
pathak ezDI 1 0.809 0.667 0.732 0.954 0.801 0.871 T
wu UTH CCB 2 0.657 0.790 0.717 0.806 0.893 0.847 T
francisco ULisboa 1 0.803 0.646 0.716 0.954 0.781 0.858 T
francisco ULisboa 2 0.803 0.646 0.716 0.954 0.781 0.858 T
francisco ULisboa 0 0.796 0.642 0.711 0.959 0.793 0.868 T
oronoz IxaMed 0 0.766 0.650 0.703 0.936 0.752 0.834 T
oronoz IxaMed 1 0.660 0.721 0.689 0.899 0.842 0.870 T
hjdai TMU 0 0.667 0.414 0.511 0.912 0.591 0.717 T
sikdar IITP 0 0.525 0.430 0.473 0.862 0.726 0.788 T
sikdar IITP 2 0.467 0.440 0.453 0.812 0.775 0.793 T
sikdar IITP 1 0.493 0.410 0.448 0.828 0.706 0.762 T
Table 5: Performance on development data for participating systems on Task A ? Identification of disor-
der mentions.
58
in the relaxed case, a span overlapping with
the gold standard span was also considered
correct.
? Task B ? Accuracy was used as the perfor-
mance measure for Task 1b. It was defined as
follows:
Accuracy
strict
=
D
tp
?N
correct
T
g
(3)
Accuracy
relaxed
=
D
tp
?N
correct
D
tp
(4)
Where, D
tp
= Number of true positive disor-
der mentions with identical spans as in the
gold standard; N
correct
= Number of cor-
rectly normalized disorder mentions; and T
g
= Total number of disorder mentions in the
gold standard. For Task B, the systems were
only evaluated on annotations they identified
in Task A. Relaxed accuracy only measured
the ability to normalize correct spans. There-
fore, it was possible to obtain very high val-
ues for this measure by simply dropping any
mention with a low confidence span.
5 Participants
A total of 21 participants from across the world
participated in Task A and out of them 18 also par-
ticipated in Task B. Unfortunately, although inter-
ested, the ThinkMiners team (Parikh et al., 2014)
could not participate in Task B owing to some
UMLS licensing issues. The participating organi-
zations along with the contact user?s User ID and
their chosen Team ID are mentioned in Table 3.
Eight teams submitted three runs, six submitted
two runs and seven submitted just one run. Out
of these, only 13 submitted system description pa-
pers. We based our analysis on those system de-
scriptions.
6 System Results
Tables 4 and 6 show the performance of the sys-
tems on Tasks A and B. None of the systems used
any additional annotated data so we did not have
to compare them separately. Both tables mention
performance of all the different runs that the sys-
tems submitted. Given the many variables, we de-
liberately left the decision on how many and how
to define these runs to the individual participant.
They used various different ways to differentiate
their runs. Some, for example, UTU (Kaewphan et
al., 2014), did it based on the composition of train-
ing data, i.e., whether they used just the training
data or both the training and the development data
for training the final system, which highlighted
the fact that adding development data to training
bumped the F
1
-score on Task A by about 2 percent
points. Some participants, however, did not make
use of the development data in training their sys-
tems. This was partially due to the fact that we had
not explicitly mentioned in the task description
that participants were allowed to use the develop-
ment data for training their final models. In order
to be fair, we allowed some users an opportunity
to submit runs post evaluation where they used the
exact same system that they used for evaluation
but used the development data as well. We added
a column to the results tables showing whether the
participant used only the training data (T) or both
training and development data (T+D) for training
their system. It can be seen that even though the
addition of development data helps, there are still
systems that perform in the lower percentile who
have used both training and development data for
training, indicating that both the features and the
machine learning classifier contribute to the mod-
els. A novel aspect of the SemEval-2014 shared
task that differentiates it from the ShARE/CLEF
task?other than the fact that it used more data and
a new test set?is the fact that SemEval-2014 al-
lowed the use of a much larger set of unlabeled
MIMIC notes to inform the models. Surprisingly,
only two of the systems (ULisboa (Leal et al.,
2014) and UniPi (Attardi et al., 2014)) used the
unlabeled MIMIC corpus to generalize the lexical
features. Another team?UTH CCB(Zhang et al.,
2014)?used off-the-shelf Brown clusters
10
as op-
posed to training them on the unlabeled MIMIC
II data. For Task B, the accuracy of a system
using the strict metric was positively correlated
with its recall on the disorder mentions that were
input to it (i.e., recall for Task A), and did not
get penalized for lower precision. Therefore one
could essentially gain higher accuracy in Task B
by tuning a system to provide the highest men-
tion recall in Task A potentially at the cost of pre-
cision and the overall F
1
-score and using those
mentions as input for Task B. This can be seen
from the fact that the run 2 for UTH CCB (Zhang
et al., 2014) system with the lowest F
1
-score has
10
Personal conversation with the participants as it was not
very clear in the system description paper.
59
Task B
Strict Relaxed
Team ID User ID Run Acc. Acc. Data
(%) (%)
UTH CCB wu 2 74.1 87.3 T+D
UTH CCB wu 1 70.8 88.0 T+D
UTH CCB wu 0 69.4 88.3 T+D
UWM ghiasvand 0 66.0 90.9 T+D
RelAgent ramanan 0 63.9 91.2 T+D
UWM ghiasvand 0 61.7 90.8 T
IxaMed oronoz 0 60.4 86.2 T+D
UTU kaewphan 1 60.1 78.3 T+D
ezDI pathak 1 59.9 87.8 T
ezDI pathak 0 59.2 87.4 T
UTU kaewphan 0 57.7 79.7 T
BioinformaticsUA nunes 1 53.1 85.5 T+D
BioinformaticsUA nunes 0 52.7 87.0 T+D
CLEAR gung 0 52.5 82.5 T
TMU hjdai 0 48.9 84.9 T+D
UNT solomon 0 47.0 74.8 T+D
UniPI attardi 0 46.7 68.3 T+D
BioinformaticsUA nunes 2 46.3 86.1 T+D
MindLab-UNAL riveros 2 46.1 86.3 T
IxaMed oronoz 1 43.9 55.8 T+D
MindLab-UNAL riveros 0 43.5 77.1 T
UniPI attardi 1 42.8 69.9 T+D
UniPI attardi 2 41.7 69.3 T+D
MindLab-UNAL riveros 1 41.1 79.7 T
ULisboa francisco 2 40.5 61.5 T
ULisboa francisco 1 40.4 61.2 T
ULisboa francisco 0 40.2 60.6 T
ECNU yi 0 36.4 59.5 T+D
TMU hjdai 1 35.8 83.4 T+D
IITP sikdar 0 33.3 69.6 T+D
IITP sikdar 2 33.2 69.1 T+D
IITP sikdar 1 31.9 69.6 T+D
CogComp upadhya 1 25.3 47.9 T+D
CogComp upadhya 2 24.8 47.7 T+D
CogComp upadhya 0 24.4 47.3 T+D
KUL kolomiyets 0 16.5 92.8 P
UG herrera 0 12.5 53.4 P
Table 6: Performance on test data for participat-
ing systems on Task B ? Normalization of disorder
mentions to UMLS (SNOMED-CT subset) CUIs.
Task B
Strict Relaxed
Team ID User ID Run Acc. Acc. Data
(%) (%)
TMU hjdai 0 0.716 0.777 T
TMU hjdai 1 0.716 0.777 T
UTH CCB wu 2 0.713 0.903 T
UTH CCB wu 1 0.680 0.910 T
UTH CCB wu 0 0.647 0.910 T
UWM ghiasvand 0 0.623 0.923 T
ezDI pathak 0 0.603 0.900 T
ezDI pathak 1 0.600 0.899 T
Best ShARe/CLEF-2013 performance 0.589 0.895 T
IxaMed oronoz 0 0.556 0.855 T
IxaMed oronoz 1 0.421 0.584 T
ULisboa francisco 2 0.388 0.601 T
ULisboa francisco 1 0.385 0.596 T
ULisboa francisco 0 0.377 0.588 T
IITP sikdar 2 0.318 0.724 T
IITP sikdar 0 0.312 0.725 T
IITP sikdar 1 0.299 0.730 T
Table 7: Performance on development data
for some participating systems on Task B ?
Normalization of disorder mentions to UMLS
(SNOMED-CT subset) CUIs.
the best accuracy for Task B and vice-versa for
run 0 with run 1 in between the two. In order to
fairly compare the performance between two sys-
tems one would have to provide perfect mentions
as input to Task B. One of the systems?UWM
Ghiasvand and Kate (2014)?did run some abla-
tion experiments using gold standard mentions as
input to Task B and obtained a best performance
of 89.5F
1
-score (Table 5 of Ghiasvand and Kate
(2014)) as opposed to 62.3 F
1
-score (Table 7) in
the more realistic setting which is a huge differ-
ence. In the upcoming SemEval-2014 where this
same evaluation is going to carried out under Task
14, we plan to perform supplementary evaluation
where gold disorder mentions would be input to
the system while attempting Task B. An inter-
esting outcome of planning a follow-on evalua-
tion to the ShARe/CLEF eHealth 2013 task was
that we could, and did, use the test data from the
ShARe/CLEF eHealth 2013 task as the develop-
ment set for this evaluation. After the main eval-
uation we asked participants to provide the sys-
tem performance on the development set using the
same number and run convention that they submit-
ted for the main evaluation. These results are pre-
sented in Tables 5 and 7. We have inserted the best
performing system score from the ShARe/CLEF
eHealth 2013 task in these tables. For Task A, re-
ferring to Tables 4 and 5, there is a boost of 3.7
absolute percent points for the F
1
-score over the
same task (Task 1a) in the ShARe/CLEF eHealth
2013. For Task B, referring to Tables 6 and 7, there
is a boost of 13.7 percent points for the F
1
-score
over the same task (Task 1b) in the ShARe/CLEF
eHealth 2013 evaluation. The participants used
various approaches for tackling the tasks, rang-
ing from purely rule-based/unsupervised (RelA-
gent (Ramanan and Nathan, 2014), (Matos et
al., 2014), KUL
11
) to a hybrid of rules and ma-
chine learning classifiers. The top performing sys-
tems typically used the latter. Various versions
of the IOB formulation were used for tagging the
disorder mentions. None of the standard varia-
tions on the IOB formulation were explicitly de-
signed or used to handle discontiguous mentions.
Some systems used novel variations on this ap-
proach. Probably the simplest variation was ap-
plied by the UWM team (Ghiasvand and Kate,
2014). In this formulation the following labeled
sequence ?the/O left/B atrium/I is/O moderately/O
11
Personal communication with participant.
60
dilated/I? can be used to represent the discontigu-
ous mention left atrium...dilated, and can be con-
structed as such from the output of the classifica-
tion. The most complex variation was the one used
by the UTH CCB team (Zhang et al., 2014) where
they used the following set of tags?B, I, O, DB,
DI, HB, HI. This variation encodes discontiguous
mentions by adding four more tags to the I, O and
B tags. These are variations of the B and I tags
with either a D or a H prefix. The prefix H indi-
cates that the word or word sequence is the shared
head, and the prefix D indicates otherwise. An-
other intermediate approach used by the ULisboa
team (Leal et al., 2014) with the tagset?S, B, I,
O, E and N. Here, S represents the single token
entity to be recognized, E represents the end of an
entity (which is part of one of the prior IOB vari-
ations) and an N tag to identify non-contiguous
mentions. They don?t provide an explicit exam-
ple usage of this tag set in their paper. Yet another
variation was used by the SZTE-NLP team (Ka-
tona and Farkas, 2014). This used tags B, I, L, O
and U. Here, L is used for the last token similar to
E earlier, and U is used for a unit-token mention,
similar to S earlier. We believe that the only ap-
proach that can distinguish between discontiguous
disorders that share the same head word/phrase is
the one used by the UTH CCB team (Zhang et
al., 2014). The participants used various machine
learning classifiers such as MaxEnt, SVM, CRF in
combination with rich syntactic and semantic fea-
tures to capture the disorder mentions. As men-
tioned earlier, a few participants used the avail-
able unlabeled data and also off-the-shelf clusters
to better generalize features. The use of vector
space models such as cosine similarities as well
as continuous distributed word vector representa-
tions was useful in the normalization task. They
also availed of tools such as MetaMap and cTakes
to generate features as well as candidate CUIs dur-
ing normalizations.
7 Conclusion
We have created a reference standard with high
inter-annotator agreement and evaluated systems
on the task of identification and normalization
of diseases and disorders appearing in clinical
reports. The results have demonstrated that an
NLP system can complete this task with reason-
ably high accuracy. We plan to annotate another
evaluation using the same data as part of the in
the SemEval-2015, Task 14
12
adding another task
of template filling where the systems will iden-
tify and normalize ten attributes the identified dis-
ease/disorder mentions.
Acknowledgments
We greatly appreciate the hard work and feed-
back of our program committee members and an-
notators David Harris, Jennifer Green and Glenn
Zaramba. Danielle Mowery, Sumithra Velupillai
and Brett South for helping prepare the manuscript
by summarizing the approaches used by various
systems. This shared task was partially sup-
ported by Shared Annotated Resources (ShARe)
project NIH 5R01GM090187 and Temporal His-
tories of Your Medical Events (THYME) project
(NIH R01LM010090 and U54LM008748).
References
Giuseppe Attardi, Vitoria Cozza, and Daniele Sartiano.
2014. UniPi: Recognition of mentions of disorders
in clinical text. In Proceedings of the International
Workshop on Semantic Evaluations, Dublin, Ireland,
August.
Olivier Bodenreider and Alexa McCray. 2003. Ex-
ploring semantic groups through visual approaches.
Journal of Biomedical Informatics, 36:414?432.
Keith E. Campbell, Diane E. Oliver, and Edward H.
Shortliffe. 1998. The Unified Medical Language
System: Towards a collaborative approach for solv-
ing terminologic problems. J Am Med Inform Assoc,
5(1):12?16.
Omid Ghiasvand and Rohit J. Kate. 2014. UWM: Dis-
order mention extraction from clinical text using crfs
and normalization using learned edit distance pat-
terns. In Proceedings of the International Workshop
on Semantic Evaluations, Dublin, Ireland, August.
Suwisa Kaewphan, Kai Hakaka1, and Filip Ginter.
2014. UTU: Disease mention recognition and nor-
malization with crfs and vector space representa-
tions. In Proceedings of the International Workshop
on Semantic Evaluations, Dublin, Ireland, August.
Melinda Katona and Rich?ard Farkas. 2014. SZTE-
NLP: Clinical text analysis with named entity recog-
nition. In Proceedings of the International Work-
shop on Semantic Evaluations, Dublin, Ireland, Au-
gust.
Andr?e Leal, Diogo Gonc?alves, Bruno Martins, and
Francisco M. Couto. 2014. ULisboa: Identifica-
tion and classification of medical concepts. In Pro-
ceedings of the International Workshop on Semantic
Evaluations, Dublin, Ireland, August.
12
http://alt.qcri.org/semeval2015/task14
61
Robert Leaman and Graciela Gonzalez. 2008. Ban-
ner: an executable survey of advances in biomedical
named entity recognition. In Pacific Symposium on
Biocomputing, volume 13, pages 652?663.
S?ergio Matos, Tiago Nunes, and Jos?e Lu??s Oliveira.
2014. BioinformaticsUA: Concept recognition in
clinical narratives using a modular and highly ef-
ficient text processing framework. In Proceedings
of the International Workshop on Semantic Evalua-
tions, Dublin, Ireland, August.
Sungrim Moon, Serguei Pakhomov, and Genevieve B
Melton. 2012. Automated disambiguation of
acronyms and abbreviations in clinical texts: Win-
dow and training size considerations. In AMIA Annu
Symp Proc, pages 1310?1319.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3?26.
Roberto Navigli. 2009. Word sense disambiguation.
ACM Computing Surveys, 41(2):1?69, February.
Ankur Parikh, Avinesh PVS, Joy Mustafi, Lalit Agar-
walla, and Ashish Mungi. 2014. ThinkMiners:
SemEval-2014 task 7: Analysis of clinical text. In
Proceedings of the International Workshop on Se-
mantic Evaluations, Dublin, Ireland, August.
Sameer Pradhan, No?emie Elhadad, Brett South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, Wendy W. Chapman, and Guergana
Savova. 2013. Task 1: ShARe/CLEF eHealth
Evaluation Lab 2013. In Working Notes of CLEF
eHealth Evaluation Labs.
Sameer Pradhan, No?emie Elhadad, Brett South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, Wendy W. Chapman, and Guergana
Savova. 2014. Evaluating the state of the art in
disorder recognition and normalization of the clin-
ical narrative. In Journal of the American Medical
Informatics Association (to appear).
S. V. Ramanan and P. Senthil Nathan. 2014. RelA-
gent: Entity detection and normalization for diseases
in clinical records: a linguistically driven approach.
In Proceedings of the International Workshop on Se-
mantic Evaluations, Dublin, Ireland, August.
Angus Roberts, Robert Gaizauskas, Mark Hepple,
George Demetriou, Yikun Guo, Ian Roberts, and
Andrea Setzer. 2009. Building a semantically an-
notated corpus of clinical texts. J Biomed Inform,
42(5):950?66.
Mohammed Saeed, C. Lieu, G. Raber, and R.G. Mark.
2002. MIMIC II: a massive temporal ICU patient
database to support research in intelligent patient
monitoring. Comput Cardiol, 29.
Guergana K. Savova, A. R. Coden, I. L. Sominsky,
R. Johnson, P. V. Ogren, P. C. de Groen, and C. G.
Chute. 2008. Word sense disambiguation across
two domains: Biomedical literature and clinical
notes. J Biomed Inform, 41(6):1088?1100, Decem-
ber.
Weiyi Sun, Anna Rumshisky, and
?
Ozlem Uzuner.
2013. Evaluating temporal relations in clinical text:
2012 i2b2 Challenge. Journal of the American Med-
ical Informatics Association, 20(5):806?13.
Hanna Suominen, Sanna Salanter?a, Sumithra Velupil-
lai, Wendy W. Chapman, Guergana Savova,
Noemie Elhadad, Sameer Pradhan, Brett R. South,
Danielle L. Mowery, Gareth J. F. Jones, Johannes
Leveling, Liadh Kelly, Lorraine Goeuriot, David
Martinez, and Guido Zuccon. 2013. Overview of
the ShARe/CLEF eHealth evaluation lab 2013. In
Working Notes of CLEF eHealth Evaluation Labs.
?
Ozlem Uzuner, Brett R South, Shuying Shen, and
Scott L DuVall. 2011. 2010 i2b2/VA challenge on
concepts, assertions, and relations in clinical text.
Journal of the American Medical Informatics Asso-
ciation, 18(5):552?556.
?
Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. Jour-
nal of American Medical Informatics Association,
19(5):786?791, September.
Yaoyun Zhang, Jingqi Wang, Buzhou Tang, Yonghui
Wu, Min Jiang, Yukun Chen, and Hua Xu. 2014.
UTH CCB: A report for SemEval 2014 task 7 anal-
ysis of clinical text. In Proceedings of the Interna-
tional Workshop on Semantic Evaluations, Dublin,
Ireland, August.
62
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 222?226,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Revised Arabic PropBank 
Wajdi Zaghouani? , Mona Diab? , Aous Mansouri?, 
Sameer Pradhan? and Martha Palmer? 
 
?Linguistic Data Consortium, ?Columbia University,  
?University of Colorado, ?BBN Technologies 
 
wajdiz@ldc.upenn.edu, mdiab@ccls.columbia.edu, aous.mansouri@colorado.edu,  
pradhan@bbn.com, martha.palmer@colorado.edu 
 
Abstract 
The revised Arabic PropBank (APB) reflects 
a number of changes to the data and the proc-
ess of PropBanking. Several changes stem 
from Treebank revisions. An automatic proc-
ess was put in place to map existing annota-
tion to the new trees. We have revised the 
original 493 Frame Files from the Pilot APB 
and added 1462 new files for a total of 1955 
Frame Files with 2446 framesets. In addition 
to a heightened attention to sense distinctions 
this cycle includes a greater attempt to ad-
dress complicated predicates such as light 
verb constructions and multi-word expres-
sions. New tools facilitate the data tagging 
and also simplify frame creation. 
1 Introduction 
Recent years have witnessed a surge in available 
automated resources for the Arabic language. 1 
These resources can now be exploited by the 
computational linguistics community with the 
aim of improving the automatic processing of 
Arabic. This paper discusses semantic labeling. 
  
Shallow approaches to semantic processing are 
making large advances in the direction of effi-
ciently and effectively deriving application rele-
vant explicit semantic information from text 
(Pradhan et al, 2003; Gildea and Palmer, 2002; 
Pradhan et al, 2004; Gildea and Jurafsky, 2002; 
Xue and Palmer, 2004; Chen and Rambow, 
2003; Carreras and Marquez, 2005; Moschitti, 
2004; Moschitti et al, 2005; Diab et al, 2008). 
Indeed, the existence of semantically annotated 
resources in English such as FrameNet (Baker et 
al., 1998) and PropBank (Kingsbury and Palmer, 
2003; Palmer et al, 2005) corpora have marked a 
surge in efficient approaches to automatic se-
                                               
1 In this paper, we use Arabic to refer to Modern Standard 
Arabic (MSA). 
mantic labeling of the English language. For ex-
ample, in the English sentence, ?John enjoys 
movies?, the predicate is ?enjoys? and the first 
argument, the subject, is ?John?, and the second 
argument, the object, is ?movies?. ?John? would 
be labeled as the agent/experiencer and ?movies? 
would be the theme/content. According to Prop-
Bank, ?John? is labeled Arg0 (or enjoyer) and 
?movies? is labeled Arg1 (or thing enjoyed). Cru-
cially, that independent of the labeling formalism 
adopted, the labels do not vary in different syn-
tactic constructions, which is why proposition 
annotation is different from syntactic Treebank 
annotation. For instance, if the example above 
was in the passive voice, ?Movies are enjoyed by 
John?, ?movies? is still the Theme/Content (Arg1) 
and (thing enjoyed), while ?John? remains the 
Agent/Experiencer (Arg0) and (enjoyer). Like-
wise for the example ?John opened the door? vs. 
?The door opened?, in both of these examples 
?the door? is the Theme (Arg1). In addition to 
English, there are PropBank efforts in Chinese 
(Xue et al, 2009), Korean (Palmer et al 2006) 
and Hindi (Palmer et al, 2009), as well as Fra-
meNet annotations in Chinese, German, Japa-
nese, Spanish and other languages (Hans 2009). 
Being able to automatically apply this level of 
analysis to Arabic is clearly a desirable goal, and 
indeed, we began a pilot Arabic PropBank effort 
several years ago (Palmer et al, 2008). 
  
In this paper, we present recent work on adapting 
the original pilot Arabic Proposition Bank (APB) 
annotation to the recent changes that have been 
made to the Arabic Treebank (Maamouri et al, 
2008). These changes have presented both lin-
guistic and engineering challenges as described 
in the following sections. In Section 2 we discuss 
major linguistics changes in the Arabic Treebank 
annotation, and any impact they might have for 
the APB effort. In Section 3 we discuss the engi-
neering ramifications of adding and deleting 
nodes from parse trees, which necessitates mov-
222
ing all of the APB label pointers to new tree lo-
cations. Finally, in Section 4 we discuss the cur-
rent APB annotation pipeline, which takes into 
account all of these changes. We conclude with a 
statement of our current goals for the project.  
2 Arabic Treebank Revision and APB 
The Arabic syntactic Treebank Part 3 v3.1 was 
revised according to the new Arabic Treebank 
Annotation Guidelines. Major changes have af-
fected the NP structure and the classification of 
verbs with clausal arguments, as well as im-
provements to the annotation in general.2  
  
The Arabic Treebank (ATB) is at the core of the 
APB annotations. The current revisions have re-
sulted in a more consistent treebank that is closer 
in its analyses to traditional Arabic grammar. 
The ATB was revised for two levels of linguistic 
representation, namely morphological informa-
tion and syntactic structure. Both of these 
changes have implications for APB annotations.  
 
The new ATB introduced more consistency in 
the application of morphological features to POS 
tags, hence almost all relevant words in the ATB 
have full morphological features of number, 
gender, case, mood, and definiteness associated 
with them. This more comprehensive application 
has implications on agreement markers between 
nouns and their modifiers and predicative verbs 
and their arguments, allowing for more consis-
tent semantic analysis in the APB. 
  
In particular, the new ATB explicitly marks the 
gerunds in Arabic known as maSAdir (singular 
maSdar.) MaSAdirs, now annotated as VN, are 
typically predicative nouns that take arguments 
that should receive semantic roles. The nouns 
marked as VN are embedded in a new kind of 
syntactic S structure headed by a VN and having 
subject and object arguments similar to verbal 
arguments. This syntactic structure, namely S-
NOM, was present in previous editions/versions 
of the ATB but it was headed by a regular noun, 
hence it was difficult to find. This explicit VN 
annotation allows the APB effort to take these 
new categories into account as predicates. For 
instance [????]VN [-??]ARG0 [????? ?????]ARG1, 
transliterated as takab~udi-,  meaning 'suffered' 
                                               
2 For a complete description of the new Treebank annotation 
guidelines, see (Arabic Treebank Morphological and Syn-
tactic Annotation Guidelines 2008) at 
http://projects.ldc.upenn.edu/ArabicTreebank/. 
is an example of predicative nominal together 
with its semantically annotated arguments ARG0 
transliterated as -him, meaning 'they' and ARG1 
transliterated as xasA}ira kabiyrap, meaning 
'heavy losses'. 
 
Other changes in the ATB include idafa con-
structions (a means of expressing possession) 
and the addition of a pseudo-verb POS tag for a 
particular group of particles traditionally known 
as ?the sisters of  ??? <in~a 'indeed' ?. These have 
very little impact on the APB annotation. 
3 Revised Treebank processing 
One of the challenges that we faced during the 
process of revising the APB was the transfer of 
the already existing annotation to the newly re-
vised trees -- especially since APB data encoding 
is tightly coupled with the explicit tree structure. 
Some of the ATB changes that affected APB 
projection from the old pilot effort to the new 
trees are listed as follows:  
i. Changes to the tree structure 
ii. Changes to the number of tokens -- both 
modification (insertion and deletion) of 
traces and modification to some tokeni-
zation 
iii. Changes in parts of speech 
iv. Changes to sentence breaks 
The APB modifications are performed within the 
OntoNotes project (Hovy et al 2006), we have 
direct access to the OntoNotes DB Tool, which 
we extended to facilitate a smooth transition. The 
tool is modified to perform a three-step mapping 
process: 
 
a) De-reference the existing (tree) node-level 
annotations to the respective token spans; 
 
b) Align the original token spans to the best pos-
sible token spans in the revised trees. This was 
usually straight forward, but sometimes the to-
kenization affected the boundaries of a span in 
which case careful heuristics had to be employed 
to find the correct mapping. We incorporated the 
standard "diff" utility into the API. A simple 
space separated token-based diff would not com-
pletely align cases where the tokenization had 
been changed in the new tree. For these cases we 
had to back-off to a character based alignment to 
recover the alignments. This two-pass strategy 
works better than using character-based align-
223
ment as a default since the diff tool does not have 
any specific domain-level constraints and gets 
spurious alignments; 
 
c) Create the PropBank (tree) node-pointers for 
the revised spans. 
 
As expected, this process is not completely 
automatic. There are cases where we can deter-
ministically transfer the annotations to the new 
trees, and other cases (especially ones that in-
volve decision making based on newly added 
traces) where we cannot. We automatically trans-
ferred all the annotation that could be done de-
terministically, and flagged all the others for hu-
man review. These cases were grouped into mul-
tiple categories for the convenience of the anno-
tators. Some of the part of speech changes in-
validated some existing annotations, and created 
new predicates to annotate. In the first case, we 
simply dropped the existing annotations on the 
affected nodes, and in the latter we just created 
new pointers to be annotated. We could auto-
matically map roughly 50% of the annotations. 
The rest are being manually reviewed. 
4 Annotation Tools and Pipeline 
4.1 Annotation process 
APB consists of two major portions: the lexicon 
resource of Frame Files and the annotated cor-
pus. Hence, the process is divided into framing 
and annotation (Palmer et al, 2005). 
 
Currently, we have four linguists (framers) creat-
ing predicate Frame Files. Using the frame crea-
tion tool Cornerstone, a Frame File is created for 
a specific lemma found in the Arabic Treebank. 
The information in the Frame File must include 
the lemma and at least one frameset.  
 
Previously, senses were lumped together into a 
single frame if they shared the same argument 
structure. In this effort, however, we are attempt-
ing to be more sensitive to the different senses 
and consequently each unique sense has its own 
frameset. A frameset contains an English defini-
tion, the argument structure for the frameset, a 
set of (parsed) Arabic examples as an illustration, 
and it may include Arabic synonyms to further 
help the annotators with sense disambiguation.  
 
Figure 1 illustrates the Frameset for the verb 
????? } isotamaE 'to listen' 
 
Predicate: {isotamaE ????? 
Roleset id: f1, to listen 
Arg0: entity listening 
Arg1: thing listened 
 
Figure 1. The frameset of the verb {isotamaE 
         
 
Rel: {isotamaE, ????? 
Arg0: -NONE- * 
Gloss: He 
Arg1: ??? ??????? 
Gloss: to their demands 
Example: ?????  ??? ??????? 
 
Figure 2. An example annotation for a sentence 
containing the verb {isotamaE 
 
In addition to the framers, we also have five na-
tive Arabic speakers as annotators on the team, 
using the annotation tool Jubilee (described be-
low). Treebanked sentences from the ATB are 
clearly displayed in Jubilee, as well as the raw 
text for that sentence at the bottom of the screen. 
The verb that needs to be tagged is clearly 
marked on the tree for the annotators. A drop-
down menu is available for the annotators to use 
so that they may choose a particular frameset for 
that specific instance. Once a frameset is chosen 
the argument structure will be displayed for them 
to see. As a visual aid, the annotators may also 
click on the ?example? button in order to see the 
examples for that particular frameset. Finally, the 
complements of the predicate are tagged directly 
on the tree, and the annotators may move on to 
the next sentence. Figure 2 illustrates a sample 
annotation. 
 
Once the data has been double-blind annotated, 
the adjudication process begins. An adjudicator, 
a member of the framing team, provides the Gold 
Standard annotation by going over the tagged 
instances to settle any differences in the choices. 
Occasionally a verb will be mis-lemmatized (e.g. 
the instance may actually be ????? sah~al 'to cause 
to become easy' but it is lemmatized under ????? 
sahul-u 'to be easy' which looks identical without 
vocalization.) At this point the lemmas are cor-
rected and sent back to the annotators to tag be-
fore the adjudicators can complete their work. 
 
The framers and annotators meet regularly at 
least every fortnight. These meetings are impor-
tant for the framers since they may need to con-
vey to the annotators any changes or issues with 
the frames, syntactic matters, or anything else 
that may require extra training or preparation for 
224
the annotators. It is important to note that while 
the framers are linguists, the annotators are not. 
This means that the annotators must be instructed 
on a number of things including, but not limited 
to, how to read trees, and what forms a constitu-
ent, as well as how to get familiar with the tools 
in order to start annotating the data. Therefore, 
little touches, such as the addition of Arabic 
synonyms to the framesets (especially since not 
all of the annotators have the same level of flu-
ency in English), or confronting specific linguis-
tic phenomena via multiple modalities are a nec-
essary part of the process. To these meetings, the 
annotators mostly bring their questions and con-
cerns about the data they are working on. We 
rely heavily on the annotator?s language skills. 
They take note of whether a frame appears to be 
incorrect, is missing an argument, or is missing a 
sense. And since they go through every instance 
in the data, annotators are instrumental for point-
ing out any errors the ATB. Since everything is 
discussed together as a group people frequently 
benefit from the conversations and issues that are 
raised. These bi-monthly meetings not only help 
maintain a certain level of quality control but 
establish a feeling of cohesion in the group. 
 
The APB has decided to thoroughly tackle light 
verb constructions and multi-word expressions as 
part of an effort to facilitate mapping between 
the different languages that are being Prop-
Banked. In the process of setting this up a num-
ber of challenges have surfaced which include: 
how can we cross-linguistically approach these 
phenomena in a (semi) integrated manner, how 
to identify one construction from the other, figur-
ing out a language specific reliable diagnostic 
test, and whether we deal with these construc-
tions as a whole unit or as separate parts; and 
how? (Hwang, et al, 2010) 
4.2 Tools 
Frameset files are created in an XML format. 
During the Pilot Propbank project these files 
were created manually by editing the XML file 
related to a particular predicate. This proved to 
be time consuming and prone to many formatting 
errors. The Frame File creation for the revised 
APB is now performed with the recently devel-
oped Cornerstone tool (Choi et al, 2010a), which 
is a PropBank frameset editor that allows the 
creation and editing of Propbank framesets with-
out requiring any prior knowledge of XML. 
Moreover, the annotation is now performed by 
Jubilee, a new annotation tool, which has im-
proved the annotation process by displaying sev-
eral types of relevant syntactic and semantic in-
formation at the same time. Having everything 
displayed helps the annotator quickly absorb and 
apply the necessary syntactic and semantic in-
formation pertinent to each predicate for consis-
tent and efficient annotation (Choi et al, 
20010b). Both tools are available as Open Source 
tools on Google code.3 
4.3 Current Annotation Status and Goals 
We have currently created 1955 verb predicate 
Frame Files which correspond to 2446 framesets, 
since one verb predicate Frame File can contain 
one or more framesets. We will reconcile the 
previous Arabic PropBank with the new Tree-
bank and create an additional 3000 Frame files to 
cover the rest of the ATB3 verb types.  
5 Conclusion  
This paper describes the recently revived and 
revised APB. The changes in the ATB have af-
fected the APB in two fundamentally different 
ways. More fine-grained POS tags facilitate the 
tasks of labeling predicate argument structures. 
However, all of the tokenization changes have 
rendered the old pointers obsolete, and new 
pointers to the new constituent boundaries have 
to be supplied. This task is underway, as well as 
the task of creating several thousand additional 
Frame Files to complete predicate coverage of 
ATB3. 
 
Acknowledgments 
 
We gratefully acknowledge a grant from the De-
fense Advanced Research Projects Agency 
(DARPA/IPTO) under the GALE program, 
DARPA/CMO Contract No. HR0011-06-C-
0022, subcontract from BBN, Inc. We also thank 
Abdel-Aati Hawwary and Maha Saliba Foster 
and our annotators for their invaluable contribu-
tions to this project. 
References  
Boas, Hans C. 2009. Multilingual FrameNets. In 
Computational Lexicography: Methods and Appli-
cations. Berlin: Mouton de Gruyter. pp. x+352 
Carreras, Xavier & Llu?s M?rquez. 2005. Introduction 
to the CoNLL-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI, USA. 
                                               
3 http://code.google.com/p/propbank/ 
225
Chen, John & Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Choi, Jinho D., Claire Bonial, & Martha Palmer. 
2010a. Propbank Instance Annotation Guidelines 
Using a Dedicated Editor,Cornerstone. In Proceed-
ings of the 7th International Conference on Lan-
guage Resources and Evaluation 
(LREC'10),Valletta, Malta. 
Choi, Jinho D., Claire Bonial, & Martha Palmer. 
2010b. Propbank Instance Annotation Guidelines 
Using a Dedicated Editor,Jubilee. In Proceedings 
of the 7th International Conference on Language 
Resources and Evaluation (LREC'10),Valletta, 
Malta. 
Diab, Mona, Alessandro Moschitti, & Daniele Pighin. 
2008. Semantic Role Labeling Systems for Arabic 
using Kernel Methods. In Proceedings of ACL. As-
sociation for Computational Linguistics, Colum-
bus, OH, USA. 
Gildea, Daniel & Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Gildea, Daniel & Martha Palmer. 2002. The necessity 
of parsing for predicate argument recognition. In 
Proceedings of the 40th Annual Conference of the 
Association for Computational Linguistics (ACL-
02), Philadelphia, PA, USA. 
Gusfield, Dan. 1997. Algorithms on Strings, Trees 
and Sequences. Cambridge University Press, 
Cambridge, UK. 
Habash, Nizar & Owen Rambow. 2007. Arabic dia-
critization through full morphological tagging. In 
HLT-NAACL 2007; Companion Volume, Short Pa-
pers, Association for Computational Linguistics, 
pages 53?56, Rochester, NY, USA. 
Hovy, Eduard, Mitchell Marcus, Martha Palmer, 
Lance Ramshaw & Ralph Weischedel. 2006. 
OntoNotes: The 90% Solution. In Proceedings of 
HLT-NAACL 2006, New York, USA. 
Hwang, Jena D., Archna Bhatia, Clare Bonial, Aous 
Mansouri, Ashwini Vaidya, Nianwen Xue & Mar-
tha Palmer. 2010. PropBank Annotation of Multi-
lingual Light Verb Constructions. In Proceedings 
of the LAW-ACL 2010. Uppsala, Sweden. 
Maamouri, Mohamed, Ann Bies, Seth Kulick. 2008. 
Enhanced Annotation and Parsing of the Arabic 
Treebank. In Proceedings of INFOS 2008, Cairo, 
Egypt. 
M?rquez, Llu?s. 2009. Semantic Role Labeling. Past, 
Present and Future . TALP Research Center. Tech-
nical University of Catalonia. Tutorial at ACL-
IJCNLP 2009. 
Moschitti, Alessandro. 2004. A study on convolution 
kernels for shallow semantic parsing. In proceed-
ings of the 42th Conference on Association for 
Computational Linguistic (ACL-2004), Barcelona, 
Spain.  
Moschitti, Alessandro, Ana-Maria Giuglea, Bonaven-
tura Coppola, & Roberto Basili. 2005. Hierarchical 
semantic role labeling. In Proceedings of CoNLL-
2005, Ann Arbor, MI, USA. 
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona 
Diab, Mohamed Maamouri, Aous Mansouri, Wajdi 
Zaghouani. 2008. A Pilot Arabic Propbank. In 
Proceedings of LREC 2008, Marrakech, Morocco. 
Palmer, Martha, Rajesh Bhatt, Bhuvana Narasimhan, 
Owen Rambow, Dipti Misra Sharma, & Fei Xia. 
2009. Hindi Syntax: Annotating Dependency, 
Lexical Predicate-Argument Structure, and Phrase 
Structure. In The 7th International Conference on 
Natural Language Processing (ICON-2009), Hy-
derabad, India. 
Palmer, Martha, Daniel Gildea, & Paul Kingsbury.  
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 31, 
1 (Mar. 2005), 71-106. 
Palmer, Martha, Shijong Ryu, Jinyoung Choi, Sinwon 
Yoon, & Yeongmi Jeon. 2006. LDC Catalog 
LDC2006T03. 
Pradhan, Sameer, Kadri Hacioglu, Wayne Ward, 
James H. Martin, & Daniel Jurafsky. 2003. Seman-
tic role parsing: Adding semantic structure to un-
structured text. In Proceedings of ICDM-2003, 
Melbourne, USA. 
Pradhan, Sameer S., Wayne H Ward, Kadri Hacioglu, 
James H Martin, & Dan Jurafsky. 2004. Shallow 
semantic parsing using support vector machines. In 
Susan Dumais, Daniel Marcu, & Salim Roukos, 
editors, HLT-NAACL 2004: Main Proceedings, 
pages 233?240, Boston, MA, USA. 
Xue, Nianwen & Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
& Dekai Wu, editors, Proceedings of ACL-EMNLP 
2004, pages 88?94, Barcelona, Spain.  
Xue, Nianwen & Martha Palmer. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural 
Language Engineering, 15 Jan. 2009, 143-172. 
226
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1?27,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
CoNLL-2011 Shared Task:
Modeling Unrestricted Coreference in OntoNotes
Sameer Pradhan
BBN Technologies,
Cambridge, MA 02138
pradhan@bbn.com
Lance Ramshaw
BBN Technologies,
Cambridge, MA 02138
lramshaw@bbn.com
Mitchell Marcus
University of Pennsylvania,
Philadelphia, 19104
mitch@linc.cis.upenn.edu
Martha Palmer
University of Colorado,
Boulder, CO 80309
martha.palmer@colorado.edu
Ralph Weischedel
BBN Technologies,
Cambridge, MA 02138
weischedel@bbn.com
Nianwen Xue
Brandeis University,
Waltham, MA 02453
xuen@cs.brandeis.edu
Abstract
The CoNLL-2011 shared task involved pre-
dicting coreference using OntoNotes data. Re-
sources in this field have tended to be lim-
ited to noun phrase coreference, often on a
restricted set of entities, such as ACE enti-
ties. OntoNotes provides a large-scale corpus
of general anaphoric coreference not restricted
to noun phrases or to a specified set of en-
tity types. OntoNotes also provides additional
layers of integrated annotation, capturing ad-
ditional shallow semantic structure. This pa-
per briefly describes the OntoNotes annota-
tion (coreference and other layers) and then
describes the parameters of the shared task
including the format, pre-processing informa-
tion, and evaluation criteria, and presents and
discusses the results achieved by the partic-
ipating systems. Having a standard test set
and evaluation parameters, all based on a new
resource that provides multiple integrated an-
notation layers (parses, semantic roles, word
senses, named entities and coreference) that
could support joint models, should help to en-
ergize ongoing research in the task of entity
and event coreference.
1 Introduction
The importance of coreference resolution for the
entity/event detection task, namely identifying all
mentions of entities and events in text and clustering
them into equivalence classes, has been well recog-
nized in the natural language processing community.
Automatic identification of coreferring entities and
events in text has been an uphill battle for several
decades, partly because it can require world knowl-
edge which is not well-defined and partly owing to
the lack of substantial annotated data. Early work
on corpus-based coreference resolution dates back
to the mid-90s by McCarthy and Lenhert (1995)
where they experimented with using decision trees
and hand-written rules. A systematic study was
then conducted using decision trees by Soon et al
(2001). Significant improvements have been made
in the field of language processing in general, and
improved learning techniques have been developed
to push the state of the art in coreference resolu-
tion forward (Morton, 2000; Harabagiu et al, 2001;
McCallum and Wellner, 2004; Culotta et al, 2007;
Denis and Baldridge, 2007; Rahman and Ng, 2009;
Haghighi and Klein, 2010). Various different knowl-
edge sources from shallow semantics to encyclo-
pedic knowledge are being exploited (Ponzetto and
Strube, 2005; Ponzetto and Strube, 2006; Versley,
2007; Ng, 2007). Researchers continued finding
novel ways of exploiting ontologies such as Word-
Net. Given that WordNet is a static ontology and
as such has limitation on coverage, more recently,
there have been successful attempts to utilize in-
formation from much larger, collaboratively built
resources such as Wikipedia (Ponzetto and Strube,
2006). In spite of all the progress, current techniques
still rely primarily on surface level features such as
string match, proximity, and edit distance; syntac-
tic features such as apposition; and shallow seman-
tic features such as number, gender, named entities,
semantic class, Hobbs? distance, etc. A better idea
of the progress in the field can be obtained by read-
ing recent survey articles (Ng, 2010) and tutorials
(Ponzetto and Poesio, 2009) dedicated to this sub-
ject.
Corpora to support supervised learning of this
task date back to the Message Understanding Con-
ferences (MUC). These corpora were tagged with
coreferring entities identified by noun phrases in the
text. The de facto standard datasets for current coref-
erence studies are the MUC (Hirschman and Chin-
1
chor, 1997; Chinchor, 2001; Chinchor and Sund-
heim, 2003) and the ACE1 (G. Doddington et al,
2000) corpora. The MUC corpora cover all noun
phrases in text, but represent small training and test
sets. The ACE corpora, on the other hand, have much
more annotation, but are restricted to a small subset
of entities. They are also less consistent, in terms of
inter-annotator agreement (ITA) (Hirschman et al,
1998). This lessens the reliability of statistical ev-
idence in the form of lexical coverage and seman-
tic relatedness that could be derived from the data
and used by a classifier to generate better predic-
tive models. The importance of a well-defined tag-
ging scheme and consistent ITA has been well rec-
ognized and studied in the past (Poesio, 2004; Poe-
sio and Artstein, 2005; Passonneau, 2004). There
is a growing consensus that in order for these to be
most useful for language understanding applications
such as question answering or distillation ? both of
which seek to take information access technology
to the next level ? we need more consistent anno-
tation of larger amounts of broad coverage data for
training better automatic techniques for entity and
event identification. Identification and encoding of
richer knowledge ? possibly linked to knowledge
sources ? and development of learning algorithms
that would effectively incorporate them is a neces-
sary next step towards improving the current state
of the art. The computational learning community,
in general, is also witnessing a move towards eval-
uations based on joint inference, with the two pre-
vious CoNLL tasks (Surdeanu et al, 2008; Hajic? et
al., 2009) devoted to joint learning of syntactic and
semantic dependencies. A principle ingredient for
joint learning is the presence of multiple layers of
semantic information.
One fundamental question still remains, and that
is ? what would it take to improve the state of the art
in coreference resolution that has not been attempted
so far? Many different algorithms have been tried in
the past 15 years, but one thing that is still lacking
is a corpus comprehensively tagged on a large scale
with consistent, multiple layers of semantic infor-
mation. One of the many goals of the OntoNotes
project2 (Hovy et al, 2006; Weischedel et al, 2011)
is to explore whether it can fill this void and help
push the progress further ? not only in coreference,
but with the various layers of semantics that it tries
to capture. As one of its layers, it has created a
corpus for general anaphoric coreference that cov-
1http://projects.ldc.upenn.edu/ace/data/
2http://www.bbn.com/nlp/ontonotes
ers entities and events not limited to noun phrases
or a limited set of entity types. A small portion of
this corpus from the newswire and broadcast news
genres (?120k) was recently used for a SEMEVAL
task (Recasens et al, 2010). As mentioned earlier,
the coreference layer in OntoNotes constitutes just
one part of a multi-layered, integrated annotation of
shallow semantic structure in text with high inter-
annotator agreement, which also provides a unique
opportunity for performing joint inference over a
substantial body of data.
The remainder of this paper is organized as
follows. Section 2 presents an overview of the
OntoNotes corpus. Section 3 describes the coref-
erence annotation in OntoNotes. Section 4 then de-
scribes the shared task, including the data provided
and the evaluation criteria. Sections 5 and 6 then de-
scribe the participating system results and analyze
the approaches, and Section 7 concludes.
2 The OntoNotes Corpus
The OntoNotes project has created a corpus of large-
scale, accurate, and integrated annotation of multi-
ple levels of the shallow semantic structure in text.
The idea is that this rich, integrated annotation cov-
ering many layers will allow for richer, cross-layer
models enabling significantly better automatic se-
mantic analysis. In addition to coreference, this
data is also tagged with syntactic trees, high cov-
erage verb and some noun propositions, partial verb
and noun word senses, and 18 named entity types.
However, such multi-layer annotations, with com-
plex, cross-layer dependencies, demands a robust,
efficient, scalable mechanism for storing them while
providing efficient, convenient, integrated access to
the the underlying structure. To this effect, it uses a
relational database representation that captures both
the inter- and intra-layer dependencies and also pro-
vides an object-oriented API for efficient, multi-
tiered access to this data (Pradhan et al, 2007a).
This should facilitate the creation of cross-layer fea-
tures in integrated predictive models that will make
use of these annotations.
Although OntoNotes is a multi-lingual resource
with all layers of annotation covering three lan-
guages: English, Chinese and Arabic, for the scope
of this paper, we will just look at the English por-
tion. Over the years of the development of this cor-
pus, there were various priorities that came into play,
and therefore not all the data in the English portion is
annotated with all the different layers of annotation.
There is a core portion, however, which is roughly
2
1.3M words which has been annotated with all the
layers. It comprises ?450k words from newswire,
?150k from magazine articles, ?200k from broad-
cast news, ?200k from broadcast conversations and
?200k web data.
OntoNotes comprises the following layers of an-
notation:
? Syntax ? A syntactic layer representing a re-
vised Penn Treebank (Marcus et al, 1993;
Babko-Malaya et al, 2006).
? Propositions ? The proposition structure of
verbs in the form of a revised PropBank(Palmer
et al, 2005; Babko-Malaya et al, 2006).
? Word Sense ? Coarse grained word senses
are tagged for the most frequent polysemous
verbs and nouns, in order to maximize cov-
erage. The word sense granularity is tailored
to achieve 90% inter-annotator agreement as
demonstrated by Palmer et al (2007). These
senses are defined in the sense inventory files
and each individual sense has been connected
to multiple WordNet senses. This provides a
direct access to the WordNet semantic struc-
ture for users to make use of. There is also a
mapping from the word senses to the PropBank
frames and to VerbNet (Kipper et al, 2000) and
FrameNet (Fillmore et al, 2003).
? Named Entities ? The corpus was tagged with
a set of 18 proper named entity types that
were well-defined and well-tested for inter-
annotator agreement by Weischedel and Burn-
stein (2005).
? Coreference ? This layer captures general
anaphoric coreference that covers entities and
events not limited to noun phrases or a limited
set of entity types (Pradhan et al, 2007b). We
will take a look at this in detail in the next sec-
tion.
3 Coreference in OntoNotes
General anaphoric coreference that spans a rich set
of entities and events ? not restricted to a few types,
as has been characteristic of most coreference data
available until now ? has been tagged with a high
degree of consistency. Attributive coreference is
tagged separately from the more common identity
coreference.
Two different types of coreference are distin-
guished in the OntoNotes data: Identical (IDENT),
and Appositive (APPOS). Appositives are treated
separately because they function as attributions, as
described further below. The IDENT type is used
for anaphoric coreference, meaning links between
pronominal, nominal, and named mentions of spe-
cific referents. It does not include mentions of
generic, underspecified, or abstract entities.
Coreference is annotated for all specific entities
and events. There is no limit on the semantic types
of NP entities that can be considered for coreference,
and in particular, coreference is not limited to ACE
types.
The mentions over which IDENT coreference ap-
plies are typically pronominal, named, or definite
nominal. The annotation process begins by auto-
matically extracting all of the NP mentions from the
Penn Treebank, though the annotators can also add
additional mentions when appropriate. In the fol-
lowing two examples (and later ones), the phrases
notated in bold form the links of an IDENT chain.
(1) She had a good suggestion and it was unani-
mously accepted by all.
(2) Elco Industries Inc. said it expects net income
in the year ending June 30, 1990, to fall below a
recent analyst?s estimate of $ 1.65 a share. The
Rockford, Ill. maker of fasteners also said it
expects to post sales in the current fiscal year
that are ?slightly above? fiscal 1989 sales of $
155 million.
3.1 Verbs
Verbs are added as single-word spans if they can
be coreferenced with a noun phrase or with an-
other verb. The intent is to annotate the VP, but we
mark the single-word head for convenience. This in-
cludes morphologically related nominalizations (3)
and noun phrases that refer to the same event, even
if they are lexically distinct from the verb (4). In the
following two examples, only the chains related to
the growth event are shown.
(3) Sales of passenger cars grew 22%. The strong
growth followed year-to-year increases.
(4) Japan?s domestic sales of cars, trucks and buses
in October rose 18% from a year earlier to
500,004 units, a record for the month, the Japan
Automobile Dealers? Association said. The
strong growth followed year-to-year increases
of 21% in August and 12% in September.
3
3.2 Pronouns
All pronouns and demonstratives are linked to any-
thing that they refer to, and pronouns in quoted
speech are also marked. Expletive or pleonastic pro-
nouns (it, there) are not considered for tagging, and
generic you is not marked. In the following exam-
ple, the pronoun you and it would not be marked. (In
this and following examples, an asterisk (*) before a
boldface phrase identifies entity/event mentions that
would not be tagged as coreferent.)
(5) Senate majority leader Bill Frist likes to tell a
story from his days as a pioneering heart sur-
geon back in Tennessee. A lot of times, Frist re-
calls, *you?d have a critical patient lying there
waiting for a new heart, and *you?d want to
cut, but *you couldn?t start unless *you knew
that the replacement heart would make *it to
the operating room.
3.3 Generic mentions
Generic nominal mentions can be linked with refer-
ring pronouns and other definite mentions, but are
not linked to other generic nominal mentions. This
would allow linking of the bracketed mentions in (6)
and (7), but not (8).
(6) Officials said they are tired of making the same
statements.
(7) Meetings are most productive when they are
held in the morning. Those meetings, however,
generally have the worst attendance.
(8) Allergan Inc. said it received approval to
sell the PhacoFlex intraocular lens, the first
foldable silicone lens available for *cataract
surgery. The lens? foldability enables it to be
inserted in smaller incisions than are now pos-
sible for *cataract surgery.
Bare plurals, as in (6) and (7), are always consid-
ered generic. In example (9) below, there are two
generic instances of parents. These are marked as
distinct IDENT chains (with separate chains distin-
guished by subscripts X, Y and Z), each containing
a generic and the related referring pronouns.
(9) ParentsX should be involved with theirX chil-
dren?s education at home, not in school. TheyX
should see to it that theirX kids don?t play tru-
ant; theyX should make certain that the children
spend enough time doing homework; theyX
should scrutinize the report card. ParentsY are
too likely to blame schools for the educational
limitations of theirY children. If parentsZ are
dissatisfied with a school, theyZ should have
the option of switching to another.
In (10) below, the verb ?halve? cannot be linked
to ?a reduction of 50%?, since ?a reduction? is in-
definite.
(10) Argentina said it will ask creditor banks to
*halve its foreign debt of $64 billion ? the
third-highest in the developing world . Ar-
gentina aspires to reach *a reduction of 50%
in the value of its external debt.
3.4 Pre-modifiers
Proper pre-modifiers can be coreferenced, but
proper nouns that are in a morphologically adjecti-
val form are treated as adjectives, and not corefer-
enced. For example, adjectival forms of GPEs such
as Chinese in ?the Chinese leader?, would not be
linked. Thus we could coreference United States in
?the United States policy? with another referent, but
not American ?the American policy.? GPEs and Na-
tionality acronyms (e.g. U.S.S.R. or U.S.). are also
considered adjectival. Pre-modifier acronyms can be
coreferenced unless they refer to a nationality. Thus
in the examples below, FBI can be coreferenced to
other mentions, but U.S. cannot.
(11) FBI spokesman
(12) *U.S. spokesman
Dates and monetary amounts can be considered
part of a coreference chain even when they occur as
pre-modifiers.
(13) The current account deficit on France?s balance
of payments narrowed to 1.48 billion French
francs ($236.8 million) in August from a re-
vised 2.1 billion francs in July, the Finance
Ministry said. Previously, the July figure was
estimated at a deficit of 613 million francs.
(14) The company?s $150 offer was unexpected.
The firm balked at the price.
3.5 Copular verbs
Attributes signaled by copular structures are not
marked; these are attributes of the referent they mod-
ify, and their relationship to that referent will be
captured through word sense and propositional ar-
gument tagging.
4
(15) JohnX is a linguist. PeopleY are nervous
around JohnX, because heX always corrects
theirY grammar.
Copular (or ?linking?) verbs are those verbs that
function as a copula and are followed by a sub-
ject complement. Some common copular verbs are:
be, appear, feel, look, seem, remain, stay, become,
end up, get. Subject complements following such
verbs are considered attributes, and not linked. Since
Called is copular, neither IDENT nor APPOS corefer-
ence is marked in the following case.
(16) Called Otto?s Original Oat Bran Beer, the brew
costs about $12.75 a case.
3.6 Small clauses
Like copulas, small clause constructions are not
marked. The following example is treated as if the
copula were present (?John considers Fred to be an
idiot?):
(17) John considers *Fred *an idiot.
3.7 Temporal expressions
Temporal expressions such as the following are
linked:
(18) John spent three years in jail. In that time...
Deictic expressions such as now, then, today, to-
morrow, yesterday, etc. can be linked, as well as
other temporal expressions that are relative to the
time of the writing of the article, and which may
therefore require knowledge of the time of the writ-
ing to resolve the coreference. Annotators were al-
lowed to use knowledge from outside the text in re-
solving these cases. In the following example, the
end of this period and that time can be coreferenced,
as can this period and from three years to seven
years.
(19) The limit could range from three years to
seven yearsX, depending on the composition
of the management team and the nature of its
strategic plan. At (the end of (this period)X)Y,
the poison pill would be eliminated automati-
cally, unless a new poison pill were approved
by the then-current shareholders, who would
have an opportunity to evaluate the corpora-
tion?s strategy and management team at that
timeY.
In multi-date temporal expressions, embedded
dates are not separately connected to to other men-
tions of that date. For example in Nov. 2, 1999, Nov.
would not be linked to another instance of November
later in the text.
3.8 Appositives
Because they logically represent attributions, appos-
itives are tagged separately from Identity corefer-
ence. They consist of a head, or referent (a noun
phrase that points to a specific object/concept in the
world), and one or more attributes of that referent.
An appositive construction contains a noun phrase
that modifies an immediately-adjacent noun phrase
(separated only by a comma, colon, dash, or paren-
thesis). It often serves to rename or further define
the first mention. Marking appositive constructions
allows us to capture the attributed property even
though there is no explicit copula.
(20) Johnhead, a linguistattribute
The head of each appositive construction is distin-
guished from the attribute according to the following
heuristic specificity scale, in a decreasing order from
top to bottom:
Type Example
Proper noun John
Pronoun He
Definite NP the man
Indefinite specific NP a man I know
Non-specific NP man
This leads to the following cases:
(21) Johnhead, a linguistattribute
(22) A famous linguistattribute, hehead studied at ...
(23) a principal of the firmattribute, J. Smithhead
In cases where the two members of the appositive
are equivalent in specificity, the left-most member of
the appositive is marked as the head/referent. Defi-
nite NPs include NPs with a definite marker (the) as
well as NPs with a possessive adjective (his). Thus
the first element is the head in all of the following
cases:
(24) The chairman, the man who never gives up
(25) The sheriff, his friend
(26) His friend, the sheriff
In the specificity scale, specific names of diseases
and technologies are classified as proper names,
whether they are capitalized or not.
(27) A dangerous bacteria, bacillium, is found
5
Type Description
Annotator Error An annotator error. This is a catch-all category for cases of errors that do not fit in the other
categories.
Genuine Ambiguity This is just genuinely ambiguous. Often the case with pronouns that have no clear an-
tecedent (especially this & that)
Generics One person thought this was a generic mention, and the other person didn?t
Guidelines The guidelines need to be clear about this example
Callisto Layout Something to do with the usage/design of Callisto
Referents Each annotator thought this was referring to two completely different things
Possessives One person did not mark this possessive
Verb One person did not mark this verb
Pre Modifiers One person did not mark this Pre Modifier
Appositive One person did not mark this appositive
Extent Both people marked the same entity, but one person?s mention was longer
Copula Disagreement arose because this mention is part of a copular structure
a) Either each annotator marked a different half of the copula
b) Or one annotator unnecessarily marked both
Figure 1: Description of various disagreement types
Figure 1: The distribution of disagreements across the various types in Table 2
Sheet1
Page 1
Copulae 2%Appositives 3%Pre Modifiers 3%Verbs 3%Possessives 4%Refer nts 7%Callisto Layout 8%Guidelines 8%Generics 11%Genuine Ambiguity 25%Annotator Error 26%
Copulae
Appositives
Pre Modifiers
Verbs
Possessives
Referents
Callisto Layout
Guidelines
Generics
Genuine Ambiguity
Annotator Error
0% 5% 10% 15% 20% 25% 30%
Figure 2: The distribution of disagreements across the various types in Table 1
When the entity to which an appositive refers is
also mentioned elsewhere, only the single span con-
taining the entire appositive construction is included
in the larger IDENT chain. None of the nested NP
spans are linked. In the example below, the en-
tire span can be linked to later mentions to Richard
Godown. The sub-spans are not included separately
in the IDENT chain.
(28) Richard Godown, president of the Indus-
trial Biotechnology Association
Ages are tagged as attributes (as if they were el-
lipses of, for example, a 42-year-old):
(29) Mr.Smithhead, 42attribute,
3.9 Special Issues
In addition to the ones above, there are some special
cases such as:
? No coreference is marked between an organi-
zation and its members.
Genre ANN1-ANN2 ANN1-ADJ ANN2-ADJ
Newswire 80.9 85.2 88.3
Broadcast News 78.6 83.5 89.4
Broadcast Conversation 86.7 91.6 93.7
Magazine 78.4 83.2 88.8
Web 85.9 92.2 91.2
Table 1: Inter Annotator and Adjudicator agreement for
the Coreference Layer in OntoNotes measured in terms
of the MUC score.
? GPEs are linked to references to their govern-
ments, even when the references are nested
NPs, or the modifier and head of a single NP.
3.10 Annotator Agreement and Analysis
Table 1 shows the inter-annotator and annotator-
adjudicator agreement on all the genres of
OntoNotes. We also analyzed about 15K dis-
agreements in various parts of the data, and grouped
them into one of the categories shown in Figure 1.
Figure 2 shows the distribution of these different
types that were found in that sample. It can be
6
seen that genuine ambiguity and annotator error
are the biggest contributors ? the latter of which is
usually captured during adjudication, thus showing
the increased agreement between the adjudicated
version and the individual annotator version.
4 CoNLL-2011 Coreference Task
This section describes the CoNLL-2011 Corefer-
ence task, including its closed and open track ver-
sions, and characterizes the data used for the task
and how it was prepared.
4.1 Why a Coreference Task?
Despite close to a two-decade history of evaluations
on coreference tasks, variation in the evaluation cri-
teria and in the training data used have made it dif-
ficult for researchers to be clear about the state of
the art or to determine which particular areas require
further attention. There are many different parame-
ters involved in defining a coreference task. Looking
at various numbers reported in literature can greatly
affect the perceived difficulty of the task. It can seem
to be a very hard problem (Soon et al, 2001) or one
that is somewhat easier (Culotta et al, 2007). Given
the space constraints, we refer the reader to Stoy-
anov et al (2009) for a detailed treatment of the
issue.
Limitations in the size and scope of the available
datasets have also constrained research progress.
The MUC and ACE corpora are the two that have
been used most for reporting comparative results,
but they differ in the types of entities and corefer-
ence annotated. The ACE corpus is also one that
evolved over a period of almost five years, with dif-
ferent incarnations of the task definition and dif-
ferent corpus cross-sections on which performance
numbers have been reported, making it hard to un-
tangle and interpret the results.
The availability of the OntoNotes data offered an
opportunity to define a coreference task based on a
larger, more broad-coverage corpus. We have tried
to design the task so that it not only can support the
current evaluation, but also can provide an ongoing
resource for comparing different coreference algo-
rithms and approaches.
4.2 Task Description
The CoNLL-2011 shared task was based on the En-
glish portion of the OntoNotes 4.0 data. The task
was to automatically identify mentions of entities
and events in text and to link the coreferring men-
tions together to form entity/event chains. The target
coreference decisions could be made using automat-
ically predicted information on the other structural
layers including the parses, semantic roles, word
senses, and named entities.
As is customary for CoNLL tasks, there were two
tracks, closed and open. For the closed track, sys-
tems were limited to using the distributed resources,
in order to allow a fair comparison of algorithm per-
formance, while the open track allowed for almost
unrestricted use of external resources in addition to
the provided data.
4.2.1 Closed Track
In the closed track, systems were limited to the pro-
vided data, plus the use of two pre-specified external
resources: i) WordNet and ii) a pre-computed num-
ber and gender table by Bergsma and Lin (2006).
For the training and test data, in addition to the
underlying text, predicted versions of all the supple-
mentary layers of annotation were provided, where
those predictions were derived using off-the-shelf
tools (parsers, semantic role labelers, named entity
taggers, etc.) as described in Section 4.4.2. For the
training data, however, in addition to predicted val-
ues for the other layers, we also provided manual
gold-standard annotations for all the layers. Partici-
pants were allowed to use either the gold-standard or
predicted annotation for training their systems. They
were also free to use the gold-standard data to train
their own models for the various layers of annota-
tion, if they judged that those would either provide
more accurate predictions or alternative predictions
for use as multiple views, or wished to use a lattice
of predictions.
More so than previous CoNLL tasks, corefer-
ence predictions depend on world knowledge, and
many state-of-the-art systems use information from
external resources such as WordNet, which can
add a layer that helps the system to recognize se-
mantic connections between the various lexical-
ized mentions in the text. Therefore, the use of
WordNet was allowed, even for the closed track.
Since word senses in OntoNotes are predominantly3
coarse-grained groupings of WordNet senses, sys-
tems could also map from the predicted or gold-
standard word senses provided to the sets of under-
lying WordNet senses. Another significant piece of
knowledge that is particularly useful for coreference
but that is not available in the layers of OntoNotes is
that of number and gender. There are many different
3There are a few instances of novel senses introduced in
OntoNotes which were not present in WordNet, and so lack a
mapping back to the WordNet senses
7
ways of predicting these values, with differing accu-
racies, so in order to ensure that participants in the
closed track were working from the same data, thus
allowing clearer algorithmic comparisons, we spec-
ified a particular table of number and gender predic-
tions generated by Bergsma and Lin (2006), for use
during both training and testing.
Following the recent CoNLL tradition, partici-
pants were allowed to use both the training and the
development data for training the final model.
4.2.2 Open Track
In addition to resources available in the closed track,
the open track, systems were allowed to use external
resources such as Wikipedia, gazetteers etc. This
track is mainly to get an idea of a performance ceil-
ing on the task at the cost of not getting a compar-
ison across all systems. Another advantage of the
open track is that it might reduce the barriers to par-
ticipation by allowing participants to field existing
research systems that already depend on external re-
sources ? especially if there were hard dependen-
cies on these resources. They can participate in the
task with minimal or no modification to their exist-
ing system.
4.3 Coreference Task Data
Since there are no previously reported numbers on
the full version of OntoNotes, we had to create
a train/development/test partition. The only por-
tion of OntoNotes that has a previously determined,
widely used, standard split is the WSJ portion of the
newswire data. For that subcorpus, we maintained
the same partition. For all the other portions we cre-
ated stratified training, development and test parti-
tions over all the sources in OntoNotes using the pro-
cedure shown in Algorithm 1. The list of training,
development and test document IDs can be found on
the task webpage.4
4.4 Data Preparation
This section gives details of the different annota-
tion layers including the automatic models that were
used to predict them, and describes the formats in
which the data were provided to the participants.
4.4.1 Manual Annotation Gold Layers
We will take a look at the manually annotated, or
gold layers of information that were made available
for the training data.
4http://conll.bbn.com/download/conll-train.id
http://conll.bbn.com/download/conll-dev.id
http://conll.bbn.com/download/conll-test.id
Algorithm 1 Procedure used to create OntoNotes
training, development and test partitions.
Procedure: GENERATE PARTITIONS(ONTONOTES) returns TRAIN,
DEV, TEST
1: TRAIN? ?
2: DEV? ?
3: TEST? ?
4: for all SOURCE ? ONTONOTES do
5: if SOURCE = WALL STREET JOURNAL then
6: TRAIN? TRAIN ? SECTIONS 02 ? 21
7: DEV? DEV ? SECTIONS 00, 01, 22, 24
8: TEST? TEST ? SECTION 23
9: else
10: if Number of files in SOURCE ? 10 then
11: TRAIN? TRAIN ? FILE IDS ending in 1 ? 8
12: DEV? DEV ? FILE IDS ending in 0
13: TEST? TEST ? FILE IDS ending in 9
14: else
15: DEV? DEV ? FILE IDS ending in 0
16: TEST? TEST ? FILE ID ending in the highest number
17: TRAIN? TRAIN ? Remaining FILE IDS for the
SOURCE
18: end if
19: end if
20: end for
21: return TRAIN, DEV, TEST
Coreference The manual coreference annotation
is stored as chains of linked mentions connecting
multiple mentions of the same entity. Coreference is
the only document-level phenomenon in OntoNotes,
and the complexity of annotation increases non-
linearly with the length of a document. Unfortu-
nately, some of the documents ? especially ones in
the broadcast conversation, weblogs, and telephone
conversation genre ? are very long which prohib-
ited us from efficiently annotating them in entirety.
These had to be split into smaller parts. We con-
ducted a few passes to join some adjacent parts, but
since some documents had as many as 17 parts, there
are still multi-part documents in the corpus. Since
the coreference chains are coherent only within each
of these document parts, for this task, each such part
is treated as a separate document. Another thing
to note is that there were some cases of sub-token
annotation in the corpus owing to the fact that to-
kens were not split at hyphens. Cases such as pro-
WalMart had the sub-span WalMart linked with another
instance of the same. The recent Treebank revision
which split tokens at most hyphens, made a majority
of these sub-token annotations go away. There were
still some residual sub-token annotations. Since
subtoken annotations cannot be represented in the
CoNLL format, and they were a very small quantity
? much less than even half a percent ? we decided to
ignore them.
For various reasons, not all the documents in
OntoNotes have been annotated with all the differ-
8
Corpora Words Documents
Total Train Dev Test Total Train Dev Test
MUC-6 25K 12K 13K 60 30 30
MUC-7 40K 19K 21K 67 30 37
ACE (2000-2004) 1M 775K 235K - - -
OntoNotes5 1.3M 1M 136K 142K 2,083(2,999) 1,674(2,374) 202(303) 207(322)
Table 2: Number of documents in the OntoNotes data, and some comparison with the MUC and ACE data sets. The
numbers in parenthesis for the OntoNotes corpus indicate the total number of parts that correspond to the documents.
Each part was considered a separate document for evaluation purposes.
Syntactic category Train Development Test
Count % Count % Count %
NP 60,345 59.71 8,463 59.31 8,629 53.09
PRP 25,472 25.21 3,535 24.78 5,012 30.84
PRP$ 8,889 8.80 1,208 8.47 1,466 9.02
NNP 2,643 2.62 468 3.28 475 2.92
NML 900 0.89 151 1.06 118 0.73
Vx 1,915 1.89 317 2.22 314 1.93
Other 893 0.88 126 0.88 239 1.47
Overall 101,057 100.00 14,268 100.00 16,253 100.00
Table 3: Distribution of mentions in the data by their syn-
tactic category.
Train Development Test
Entities/Chains 26,612 3,752 3,926
Links 74,652 10,539 12,365
Mentions 101,264 14,291 16,291
Table 4: Number of entities, links and mentions in the
OntoNotes 4.0 data.
ent layers of annotation, with full coverage.6 There
is a core portion, however, which is roughly 1.3M
words which has been annotated with all the layers.
This is the portion that we used for the shared task.
The number of documents in the corpus for this
task, for each of the different genres, are shown in
Table 2. Tables 3 and 4 shows the distribution of
mentions by the syntactic categories, and the counts
of entities, links and mentions in the corpus respec-
tively. All of this data has been Treebanked and
PropBanked either as part of the OntoNotes effort
or some preceding effort.
For comparison purposes, Table 2 also lists the
number of documents in the MUC-6, MUC-7, and
ACE (2000-2004) corpora. The MUC-6 data was
taken from the Wall Street Journal, whereas the
MUC-7 data was from the New York Times. The
ACE data spanned many different genres similar to
6Given the nature of word sense annotation, and changes in
project priorities, we could not annotate all the low frequency
verbs and nouns in the corpus. Furthermore, PropBank annota-
tion currently only covers verb predicates.
the ones in OntoNotes.
Parse Trees This represents the syntactic layer
that is a revised version of the Penn Treebank. For
purposes of this task, traces were removed from the
syntactic trees, since the CoNLL-style data format,
being indexed by tokens, does not provide any good
means of conveying that information. Function tags
were also removed, since the parsers that we used
for the predicted syntax layer did not provide them.
One thing that needs to be dealt with in conversa-
tional data is the presence of disfluencies (restarts,
etc.). In the original OntoNotes parses, these are
marked using a special EDITED7 phrase tag ? as was
the case for the Switchboard Treebank. Given the
frequency of disfluencies and the performance with
which one can identify them automatically,8 a prob-
able processing pipeline would filter them out be-
fore parsing. Since we did not have a readily avail-
able tagger for tagging disfluencies, we decided to
remove them using oracle information available in
the Treebank.
Propositions The propositions in OntoNotes con-
stitute PropBank semantic roles. Most of the verb
predicates in the corpus have been annotated with
their arguments. Recent enhancements to the Prop-
Bank to make it synchronize better with the Tree-
bank (Babko-Malaya et al, 2006) have enhanced
the information in the proposition by the addition of
two types of LINKs that represent pragmatic corefer-
ence (LINK-PCR) and selectional preferences (LINK-
SLC). More details can be found in the addendum to
the PropBank guidelines9 in the OntoNotes 4.0 re-
7There is another phrase type ? EMBED in the telephone con-
versation genre which is similar to the EDITED phrase type, and
sometimes identifies insertions, but sometimes contains logical
continuation of phrases, so we decided not to remove that from
the data.
8A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 Precision
and 67 recall.
9doc/propbank/english-propbank.pdf
9
lease. Since the community is not used to this rep-
resentation which relies heavily on the trace struc-
ture in the Treebank which we are excluding, we de-
cided to unfold the LINKs back to their original rep-
resentation as in the Release 1.0 of the Proposition
Bank. This functionality is part of the OntoNotes
DB Tool.10
Word Sense Gold word sense annotation was
supplied using sense numbers as specified in
the OntoNotes list of senses for each lemma.11
The sense inventories that were provided in the
OntoNotes 4.0 release were not all mapped to the lat-
est version 3.0 of WordNet, so we provided a revised
version of the sense inventories, containing mapping
to WordNet 3.0, on the task page for the participants.
Named Entities Named Entities in OntoNotes
data are specified using a catalog of 18 Name types.
Other Layers Discourse plays a vital role in
coreference resolution. In the case of broadcast con-
versation, or telephone conversation data, it partially
manifests in the form of speakers of a given utter-
ance, whereas in weblogs or newsgroups it does so
as the writer, or commenter of a particular article
or thread. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be au-
tomatically deduced, but since it would add addi-
tional complexity to the already complex task, we
decided to provide oracle information of this meta-
data both during training and testing. In other words,
speaker and author identification was not treated
as an annotation layer that needed to be predicted.
This information was provided in the form of an-
other column in the .conll table. There were some
cases of interruptions and interjections that ideally
would associate parts of a sentence to two different
speakers, but since the frequency of this was quite
small, we decided to make an assumption of one
speaker/writer per sentence.
4.4.2 Predicted Annotation Layers
The predicted annotation layers were derived using
automatic models trained using cross-validation on
other portions of OntoNotes data. As mentioned ear-
lier, there are some portions of the OntoNotes corpus
that have not been annotated for coreference but that
have been annotated for other layers. For training
10http://cemantix.org/ontonotes.html
11It should be noted that word sense annotation in OntoNotes
is note complete, so only some of the verbs and nouns have
word sense tags specified.
Senses Lemmas
1 1,506
2 1,046
> 2 1,016
Table 6: Word sense polysemy over verb and noun lem-
mas in OntoNotes
models for each of the layers, where feasible, we
used all the data that we could for that layer from
the training portion of the entire OntoNotes release.
Parse Trees Predicted parse trees were produced
using the Charniak parser (Charniak and Johnson,
2005).12 Some additional tag types used in the
OntoNotes trees were added to the parser?s tagset,
including the NML tag that has recently been added
to capture internal NP structure, and the rules used to
determine head words were appropriately extended.
The parser was then re-trained on the training por-
tion of the release 4.0 data using 10-fold cross-
validation. Table 5 shows the performance of the
re-trained Charniak parser on the CoNLL-2011 test
set. We did not get a chance to re-train the re-ranker,
and since the stock re-ranker crashes when run on n-
best parses containing NMLs, because it has not seen
that tag in training, we could not make use of it.
Word Sense We trained a word sense tagger us-
ing a SVM classifier and contextual word and part
of speech features on all the training portion of the
OntoNotes data. The OntoNotes 4.0 corpus com-
prises a total of 14,662 sense definitions across 4877
verb and noun lemmas13. The distribution of senses
per lemma is as shown in Table 6. Table 7 shows
the performance of this classifier over both the verbs
and nouns in the CoNLL-2011 test set. Again this
performance is not directly comparable to any re-
ported in the literature before, and it seems lower
then performances reported on previous versions
of OntoNotes because this is over all the genres
of OntoNotes, and aggregated over both verbs and
nouns in the CoNLL-2011 test set.
Propositions To predict propositional structure,
ASSERT14 (Pradhan et al, 2005) was used, re-
trained also on all the training portion of the release
12http://bllip.cs.brown.edu/download/reranking-
parserAug06.tar.gz
13The number of lemmas in Table 6 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
7933.
14http://cemantix.org/assert.html
10
All Sentences Sentence len < 40
N POS R P F N R P F
Broadcast Conversation (BC) 2,194 95.93 84.30 84.46 84.38 2124 85.83 85.97 85.90
Broadcast News (BN) 1,344 96.50 84.19 84.28 84.24 1278 85.93 86.04 85.98
Magazine (MZ) 780 95.14 87.11 87.46 87.28 736 87.71 88.04 87.87
Newswire (NW) 2,273 96.95 87.05 87.45 87.25 2082 88.95 89.27 89.11
Telephone Conversation (TC) 1,366 93.52 79.73 80.83 80.28 1359 79.88 80.98 80.43
Weblogs and Newsgroups (WB) 1,658 94.67 83.32 83.20 83.26 1566 85.14 85.07 85.11
Overall 9,615 96.03 85.25 85.43 85.34 9145 86.86 87.02 86.94
Table 5: Parser performance on the CoNLL-2011 test set
Frameset Total Total % Perfect Argument ID + Class
Accuracy Sentences Propositions Propositions P R F
Broadcast Conversation (BC) 0.92 2,037 5,021 52.18 82.55 64.84 72.63
Broadcast News (BN) 0.91 1,252 3,310 53.66 81.64 64.46 72.04
Magazine (MZ) 0.89 780 2,373 47.16 79.98 61.66 69.64
Newswire (NW) 0.93 1,898 4,758 39.72 80.53 62.68 70.49
Weblogs and Newsgroups (WB) 0.92 929 2,174 39.19 81.01 60.65 69.37
Overall 0.91 6,896 17,636 46.82 81.28 63.17 71.09
Table 8: Performance on the propositions and framesets in the CoNLL-2011 test set.
Accuracy
Broadcast Conversation (BC) 0.70
Broadcast News (BN) 0.68
Magazine (MZ) 0.60
Newswire (NW) 0.62
Weblogs and Newsgroups (WB) 0.63
Overall 0.65
Table 7: Word sense performance over both verbs and
nouns in the CoNLL-2011 test set
4.0 data. Given time constraints, we had to per-
form two modifications: i) Instead of a single model
that predicts all arguments including NULL argu-
ments, we had to use the two-stage mode where the
NULL arguments are first filtered out and the remain-
ing NON-NULL arguments are classified into one of
the argument types, and ii) The argument identifi-
cation module used an ensemble of ten classifiers
? each trained on a tenth of the training data and
performed an unweighted voting among them. This
should still give a close to state of the art perfor-
mance given that the argument identification perfor-
mance tends to start to be asymptotic around 10k
training instances. At first glance, the performance
on the newswire genre is much lower than what has
been reported for WSJ Section 23. This could be
attributed to two factors: i) the fact that we had to
compromise on the training method, but more im-
portantly because ii) the newswire in OntoNotes not
only contains WSJ data, but also Xinhua news. One
could try to verify using just the WSJ portion of the
data, but it would be hard as it is not only a sub-
set of the documents that the performance has been
reported on previously, but also the annotation has
been significantly revised; it includes propositions
for be verbs missing from the original PropBank,
and the training data is a subset of the original data
as well. Table 8 shows the detailed performance
numbers.
In addition to automatically predicting the argu-
ments, we also trained a classifier to tag PropBank
frameset IDs in the data using the same word sense
module as mentioned earlier. OntoNotes 4.0 con-
tains a total of 7337 framesets across 5433 verb
lemmas.15 An overwhelming number of them are
monosemous, but the more frequent verbs tend to be
polysemous. Table 9 gives the distribution of num-
ber of framesets per lemma in the PropBank layer of
the OntoNotes 4.0 data.
During automatic processing of the data, we
tagged all the tokens that were tagged with a part
of speech VBx. This means that there would be cases
where the wrong token would be tagged with propo-
sitions. The CoNLL-2005 scorer was used to gener-
ate the scores.
Named Entities BBN?s IdentiFinderTMsystem
was used to predict the named entities. Given the
15The number of lemmas in Table 9 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
4229.
11
Framesets Lemmas
1 2,722
2 321
> 2 181
Table 9: Frameset polysemy across lemmas
Overall BC BN MZ NW TC WB
F F F F F F F
ALL Named Entities 71.8 64.8 72.2 61.5 84.3 39.5 55.2
Cardinal 68.7 51.8 71.1 66.1 82.8 34.0 68.7
Date 76.1 63.7 77.9 66.7 83.7 60.5 56.0
Event 27.6 00.0 34.8 30.8 47.6 - 13.3
Facility 41.9 55.0 16.7 23.1 66.7 00.0 22.9
GPE 87.9 87.5 90.3 73.7 92.9 65.9 88.7
Language 41.2 - 50.0 50.0 00.0 20.0 75.0
Law 63.0 00.0 85.7 00.0 67.9 00.0 50.0
Location 58.4 59.1 59.6 53.3 68.0 00.0 23.5
Money 74.6 16.7 66.7 73.2 79.4 30.8 61.5
NORP 00.0 00.0 00.0 00.0 00.0 00.0 00.0
Ordinal 73.4 73.8 73.4 78.1 78.4 88.9 37.0
Organization 71.0 57.8 67.1 52.9 86.9 21.2 32.1
Percent 71.2 88.9 76.9 69.6 92.1 01.2 71.6
Person 79.6 78.9 87.7 66.7 91.6 65.1 64.8
Product 46.9 00.0 43.8 00.0 81.8 00.0 00.0
Quantity 47.5 25.3 58.3 61.1 71.9 00.0 22.2
Time 58.6 56.9 64.1 42.9 80.0 23.8 51.7
Work of Art 41.9 26.9 37.1 16.0 77.9 00.0 05.6
Table 10: Named Entity performance on the CoNLL-
2011 test set
time constraints, we could not re-train it on the
OntoNotes data and so an existing, pre-trained
model was used, therefore the results are not a
good indicator of the model?s best performance.
The pre-trained model had also used a somewhat
different catalog of name types, which did not
include the OntoNotes NORP type (for nationalities,
organizations, religions, and political parties),
so that category was never predicted. Table 10
shows the overall performance of the tagger on the
CoNLL-2011 test set, as well as the performance
broken down by individual name types. IdentiFinder
performance has been reported to be in the low 90?s
on WSJ test set.
Other Layers As noted above, systems were al-
lowed to make use of gender and number predic-
tions for NPs using the table from Bergsma and Lin
(Bergsma and Lin, 2006).
4.4.3 Data Format
In order to organize the multiple, rich layers of anno-
tation, the OntoNotes project has created a database
representation for the raw annotation layers along
with a Python API to manipulate them (Pradhan et
al., 2007a). In the OntoNotes distribution the data is
organized as one file per layer, per document. The
API requires a certain hierarchical structure with
documents at the leaves inside a hierarchy of lan-
guage, genre, source and section. It comes with var-
ious ways of cleanly querying and manipulating the
data and allows convenient access to the sense in-
ventory and propbank frame files instead of having
to interpret the raw .xml versions. However, main-
taining format consistency with earlier CoNLL tasks
was deemed convenient for sites that already had
tools configured to deal with that format. Therefore,
in order to distribute the data so that one could make
the best of both worlds, we created a new file type
called .conll which logically served as another layer
in addition to the .parse, .prop, .name and .coref
layers. Each .conll file contained a merged repre-
sentation of all the OntoNotes layers in the CoNLL-
style tabular format with one line per token, and with
multiple columns for each token specifying the input
annotation layers relevant to that token, with the fi-
nal column specifying the target coreference layer.
Because OntoNotes is not authorized to distribute
the underlying text, and many of the layers contain
inline annotation, we had to provide a skeletal form
(.skel of the .conll file which was essentially the
.conll file, but with the word column replaced with
a dummy string. We provided an assembly script
that participants could use to create a .conll file tak-
ing as input the .skel file and the top-level directory
of the OntoNotes distribution that they had sepa-
rately downloaded from the LDC16 Once the .conll
file is created, it can be used to create the individual
layers such as .parse, .name, .coref etc. using an-
other set of scripts. Since the propositions and word
sense layers are inherently standoff annotation, they
were provided as is, and did not require that extra
merging step. One thing thing that made this data
creation process a bit tricky was the fact that we had
dissected some of the trees for the conversation data
to remove the EDITED phrases. Table 11 describes
the data provided in each of the column of the .conll
format. Figure 3 shows a sample from a .conll file.
4.5 Evaluation
This section describes the evaluation criteria used.
Unlike for propositions, word sense and named en-
tities, where it is simply a matter of counting the
correct answers, or for parsing, where there are sev-
eral established metrics, evaluating the accuracy of
coreference continues to be contentious. Various al-
16OntoNotes is deeply grateful to the Linguistic Data Con-
sortium for making the source data freely available to the task
participants.
12
Column Type Description
1 Document ID This is a variation on the document filename
2 Part number Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.
3 Word number This is the word index in the sentence
4 Word The word itself
5 Part of Speech Part of Speech of the word
6 Parse bit This is the bracketed structure broken before the first open parenthesis in the parse, and the
word/part-of-speech leaf replaced with a *. The full parse can be created by substituting
the asterix with the ([pos] [word]) string (or leaf) and concatenating the items in the
rows of that column.
7 Predicate lemma The predicate lemma is mentioned for the rows for which we have semantic role informa-
tion. All other rows are marked with a -
8 Predicate Frameset ID This is the PropBank frameset ID of the predicate in Column 7.
9 Word sense This is the word sense of the word in Column 3.
10 Speaker/Author This is the speaker or author name where available. Mostly in Broadcast Conversation and
Web Log data.
11 Named Entities These columns identifies the spans representing various named entities.
12:N Predicate Arguments There is one column each of predicate argument structure information for the predicate
mentioned in Column 7.
N Coreference Coreference chain information encoded in a parenthesis structure.
Table 11: Format of the .conll file used on the shared task
#begin document (nw/wsj/07/wsj_0771); part 000
...
...
nw/wsj/07/wsj_0771 0 0 ?? ?? (TOP(S(S* - - - - * * (ARG1* * * -
nw/wsj/07/wsj_0771 0 1 Vandenberg NNP (NP* - - - - (PERSON) (ARG1* * * * (8|(0)
nw/wsj/07/wsj_0771 0 2 and CC * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 3 Rayburn NNP *) - - - - (PERSON) *) * * *(23)|8)
nw/wsj/07/wsj_0771 0 4 are VBP (VP* be 01 1 - * (V*) * * * -
nw/wsj/07/wsj_0771 0 5 heroes NNS (NP(NP*) - - - - * (ARG2* * * * -
nw/wsj/07/wsj_0771 0 6 of IN (PP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 7 mine NN (NP*)))) - - 5 - * *) * * * (15)
nw/wsj/07/wsj_0771 0 8 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 9 ?? ?? *) - - - - * * *) * * -
nw/wsj/07/wsj_0771 0 10 Mr. NNP (NP* - - - - * * (ARG0* (ARG0* * (15
nw/wsj/07/wsj_0771 0 11 Boren NNP *) - - - - (PERSON) * *) *) * 15)
nw/wsj/07/wsj_0771 0 12 says VBZ (VP* say 01 1 - * * (V*) * * -
nw/wsj/07/wsj_0771 0 13 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 14 referring VBG (S(VP* refer 01 2 - * * (ARGM-ADV* (V*) * -
nw/wsj/07/wsj_0771 0 15 as RB (ADVP* - - - - * * * (ARGM-DIS* * -
nw/wsj/07/wsj_0771 0 16 well RB *) - - - - * * * *) * -
nw/wsj/07/wsj_0771 0 17 to IN (PP* - - - - * * * (ARG1* * -
nw/wsj/07/wsj_0771 0 18 Sam NNP (NP(NP* - - - - (PERSON* * * * * (23
nw/wsj/07/wsj_0771 0 19 Rayburn NNP *) - - - - *) * * * * -
nw/wsj/07/wsj_0771 0 20 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 21 the DT (NP(NP* - - - - * * * * (ARG0* -
nw/wsj/07/wsj_0771 0 22 Democratic JJ * - - - - (NORP) * * * * -
nw/wsj/07/wsj_0771 0 23 House NNP * - - - - (ORG) * * * * -
nw/wsj/07/wsj_0771 0 24 speaker NN *) - - - - * * * * *) -
nw/wsj/07/wsj_0771 0 25 who WP (SBAR(WHNP*) - - - - * * * * (R-ARG0*) -
nw/wsj/07/wsj_0771 0 26 cooperated VBD (S(VP* cooperate 01 1 - * * * * (V*) -
nw/wsj/07/wsj_0771 0 27 with IN (PP* - - - - * * * * (ARG1* -
nw/wsj/07/wsj_0771 0 28 President NNP (NP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 29 Eisenhower NNP *))))))))))) - - - - (PERSON) * *) *) *) 23)
nw/wsj/07/wsj_0771 0 30 . . *)) - - - - * * * * * -
nw/wsj/07/wsj_0771 0 0 ?? ?? (TOP(S* - - - - * * * -
nw/wsj/07/wsj_0771 0 1 They PRP (NP*) - - - - * (ARG0*) * (8)
nw/wsj/07/wsj_0771 0 2 allowed VBD (VP* allow 01 1 - * (V*) * -
nw/wsj/07/wsj_0771 0 3 this DT (S(NP* - - - - * (ARG1* (ARG1* (6
nw/wsj/07/wsj_0771 0 4 country NN *) - - 3 - * * *) 6)
nw/wsj/07/wsj_0771 0 5 to TO (VP* - - - - * * * -
nw/wsj/07/wsj_0771 0 6 be VB (VP* be 01 1 - * * (V*) (16)
nw/wsj/07/wsj_0771 0 7 credible JJ (ADJP*))))) - - - - * *) (ARG2*) -
nw/wsj/07/wsj_0771 0 8 . . *)) - - - - * * * -
#end document
Figure 3: Sample portion of the .conll file.
13
ternative metrics have been proposed, as mentioned
below, which weight different features of a proposed
coreference pattern differently. The choice is not
clear in part because the value of a particular set of
coreference predictions is integrally tied to the con-
suming application.
A further issue in defining a coreference metric
concerns the granularity of the mentions, and how
closely the predicted mentions are required to match
those in the gold standard for a coreference predic-
tion to be counted as correct.
Our evaluation criterion was in part driven by the
OntoNotes data structures. OntoNotes coreference
distinguishes between identity coreference and ap-
positive coreference, treating the latter separately
because it is already captured explicitly by other lay-
ers of the OntoNotes annotation. Thus we evaluated
systems only on the identity coreference task, which
links all categories of entities and events together
into equivalent classes.
The situation with mentions for OntoNotes is also
different than it was for MUC or ACE. OntoNotes
data does not explicitly identify the minimum ex-
tents of an entity mention, but it does include hand-
tagged syntactic parses. Thus for the official evalua-
tion, we decided to use the exact spans of mentions
for determining correctness. The NP boundaries
for the test data were pre-extracted from the hand-
tagged Treebank for annotation, and events trig-
gered by verb phrases were tagged using the verbs
themselves. This choice means that scores for the
CoNLL-2011 coreference task are likely to be lower
than for coref evaluations based on MUC, where the
mention spans are specified in the input,17 or those
based on ACE data, where an approximate match is
often allowed based on the specified head of the NP
mention.
4.5.1 Metrics
As noted above, the choice of an evaluation met-
ric for coreference has been a tricky issue and there
does not appear to be any silver bullet approach that
addresses all the concerns. Three metrics have been
proposed for evaluating coreference performance
over an unrestricted set of entity types: i) The link
based MUC metric (Vilain et al, 1995), ii) The men-
tion based B-CUBED metric (Bagga and Baldwin,
1998) and iii) The entity based CEAF (Constrained
Entity Aligned F-measure) metric (Luo, 2005). Very
recently BLANC (BiLateral Assessment of Noun-
Phrase Coreference) measure (Recasens and Hovy,
17as is the case in this evaluation with Gold Mentions
2011) has been proposed as well. Each of the met-
ric tries to address the shortcomings or biases of the
earlier metrics. Given a set of key entities K, and
a set of response entities R, with each entity com-
prising one or more mentions, each metric generates
its variation of a precision and recall measure. The
MUC measure if the oldest and most widely used. It
focuses on the links (or, pairs of mentions) in the
data.18 The number of common links between en-
tities in K and R divided by the number of links
in K represents the recall, whereas, precision is the
number of common links between entities in K and
R divided by the number of links in R. This met-
ric prefers systems that have more mentions per en-
tity ? a system that creates a single entity of all
the mentions will get a 100% recall without signifi-
cant degradation in its precision. And, it ignores re-
call for singleton entities, or entities with only one
mention. The B-CUBED metric tries to addresses
MUCS?s shortcomings, by focusing on the mentions
and computes recall and precision scores for each
mention. If K is the key entity containing mention M,
and R is the response entity containing mention M,
then recall for the mention M is computed as |K?R||K|
and precision for the same is is computed as |K?R||R| .
Overall recall and precision are the average of the
individual mention scores. CEAF aligns every re-
sponse entity with at most one key entity by finding
the best one-to-one mapping between the entities us-
ing an entity similarity metric. This is a maximum
bipartite matching problem and can be solved by
the Kuhn-Munkres algorithm. This is thus a entity
based measure. Depending on the similarity, there
are two variations ? entity based CEAF ? CEAFe and
a mention based CEAF ? CEAFe. Recall is the total
similarity divided by the number of mentions in K,
and precision is the total similarity divided by the
number of mentions in R. Finally, BLANC uses a
variation on the Rand index (Rand, 1971) suitable
for evaluating coreference. There are a few other
measures ? one being the ACE value, but since this
is specific to a restricted set of entities (ACE types),
we did not consider it.
4.5.2 Official Evaluation Metric
In order to determine the best performing system
in the shared task, we needed to associate a single
number with each system. This could have been
one of the metrics above, or some combination of
more than one of them. The choice was not sim-
ple, and while we consulted various researchers in
18The MUC corpora did not tag single mention entities.
14
the field, hoping for a strong consensus, their con-
clusion seemed to be that each metric had its pros
and cons. We settled on the MELA metric by Denis
and Baldridge (2009), which takes a weighted av-
erage of three metrics: MUC, B-CUBED, and CEAF.
The rationale for the combination is that each of the
three metrics represents a different important dimen-
sion, the MUC measure being based on links, the
B-CUBED based on mentions, and the CEAF based
on entities. For a given task, a weighted average
of the three might be optimal, but since we don?t
have an end task in mind, we decided to use the un-
weighted mean of the three metrics as the score on
which the winning system was judged. We decided
to use CEAFe instead of CEAFm.
4.5.3 Scoring Metrics Implementation
We used the same core scorer implementation19 that
was used for the SEMEVAL-2010 task, and which
implemented all the different metrics. There were a
couple of modifications done to this scorer after it
was used for the SEMEVAL-2010 task.
1. Only exact matches were considered cor-
rect. Previously, for SEMEVAL-2010 non-exact
matches were judged partially correct with a
0.5 score if the heads were the same and the
mention extent did not exceed the gold men-
tion.
2. The modifications suggested by Cai and Strube
(2010) were incorporated in the scorer.
Since there are differences in the version used for
CoNLL and the one available on the download site,
and it is possible that the latter would be revised in
the future, we have archived the version of the scorer
on the CoNLL-2011 task webpage.20
5 Systems and Results
About 65 different groups demonstrated interest in
the shared task by registering on the task webpage.
Of these, 23 groups submitted system outputs on the
test set during the evaluation week. 18 groups sub-
mitted only closed track results, 3 groups only open
track results, and 2 groups submitted both closed and
open track results. 2 participants in the closed track,
did not write system papers, so we don?t use their re-
sults in the discussion. Their results will be reported
on the task webpage.
19http://www.lsi.upc.edu/ esapena/downloads/index.php?id=3
20http://conll.bbn.com/download/scorer.v4.tar.gz
The official results for the 18 systems that submit-
ted closed track outputs are shown in Table 12, with
those for the 5 systems that submitted open track
results in Table 13. The official ranking score, the
arithmetic mean of the F-scores of MUC, B-CUBED
and CEAFe, is shown in the rightmost column. For
convenience, systems will be referred to here using
the first portion of the full name, which is unique
within each table.
For completeness, the tables include the raw pre-
cision and recall scores from which the F-scores
were derived. The tables also include two additional
scores (BLANC and CEAFm) that did not factor into
the official ranking score. Useful further analysis
may be possible based on these results beyond the
preliminary results presented here.
As discussed previously in the task description,
we will consider three different test input conditions:
i) Predicted only (Official), ii) Predicted plus gold
mention boundaries, and iii) Predicted plus gold
mentions
5.1 Predicted only (Official)
For the official test, beyond the raw source text,
coreference systems were provided only with the
predictions from automatic engines as to the other
annotation layers (parses, semantic roles, word
senses, and named entities).
In this evaluation it is important to note that the
mention detection score cannot be considered in iso-
lation of the coreference task as has usually been the
case. This is mainly owing to the fact that there are
no singleton entities in the OntoNotes data. Most
systems removed singletons from the response as a
post-processing step, so not only will they not get
credit for the singleton entities that they correctly re-
moved from the data, but they will be penalized for
the ones that they accidentally linked with another
mention. What this number does indicate is the ceil-
ing on recall that a system would have got in absence
of being penalized for making mistakes in corefer-
ence resolution. A close look at the Table 12 indi-
cates a possible outlier in case of the sapena system.
The recall for this system is very high, and precision
way lower than any other system. Further investi-
gations uncovered that the reason for this aberrant
behavior was that fact that this system opted to keep
singletons in the response. By design, the scorer re-
moves singletons that might be still present in the
system, but it does so after the mention detection
accuracy is computed.
The official scores top out in the high 50?s. While
this is lower than the figures cited in previous coref-
15
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
75
.07
66
.81
70
.7
0
61
.76
57
.53
59
.57
68
.40
68
.23
68
.31
56
.37
56
.37
56
.3
7
43
.41
47
.75
45
.4
8
70
.63
76
.21
73
.02
57
.7
9
sap
en
a
92
.39
28
.19
43
.20
56
.32
63
.16
59
.55
62
.75
72
.08
67
.09
53
.51
53
.51
53
.51
44
.75
38
.38
41
.32
69
.50
73
.07
71
.10
55
.99
ch
an
g
68
.08
61
.96
64
.88
57
.15
57
.15
57
.15
67
.14
70
.53
68
.7
9
54
.40
54
.40
54
.40
41
.94
41
.94
41
.94
71
.19
77
.09
73
.7
1
55
.96
nu
gu
es
69
.87
68
.08
68
.96
60
.20
57
.10
58
.61
66
.74
64
.23
65
.46
51
.45
51
.45
51
.45
38
.09
41
.06
39
.52
71
.99
70
.31
71
.11
54
.53
san
tos
67
.80
63
.25
65
.45
59
.21
54
.30
56
.65
68
.79
62
.81
65
.66
49
.54
49
.54
49
.54
35
.86
40
.21
37
.91
73
.37
66
.91
69
.46
53
.41
son
g
57
.81
80
.41
67
.26
53
.73
67
.79
59
.9
5
60
.65
66
.05
63
.23
46
.29
46
.29
46
.29
43
.37
30
.71
35
.96
69
.49
59
.71
61
.47
53
.05
sto
ya
no
v
70
.84
64
.98
67
.78
63
.61
54
.04
58
.43
72
.58
53
.27
61
.44
46
.08
46
.08
46
.08
32
.00
40
.82
35
.88
73
.21
58
.93
60
.88
51
.92
sob
ha
67
.82
62
.09
64
.83
51
.08
49
.88
50
.48
62
.63
65
.43
64
.00
49
.48
49
.48
49
.48
40
.65
41
.82
41
.23
61
.40
68
.35
63
.88
51
.90
ko
bd
an
i
62
.06
60
.04
61
.03
55
.64
51
.50
53
.49
69
.66
62
.43
65
.85
42
.70
42
.70
42
.70
32
.33
35
.40
33
.79
61
.86
63
.51
62
.61
51
.04
zh
ou
61
.08
63
.59
62
.31
45
.65
52
.79
48
.96
57
.14
72
.91
64
.07
47
.53
47
.53
47
.53
43
.19
36
.79
39
.74
61
.10
73
.94
64
.72
50
.92
ch
art
on
65
.90
62
.77
64
.30
55
.09
50
.05
52
.45
66
.26
58
.44
62
.10
46
.82
46
.82
46
.82
34
.33
39
.05
36
.54
69
.94
62
.23
64
.80
50
.36
ya
ng
71
.92
57
.53
63
.93
59
.91
46
.43
52
.31
71
.64
55
.14
62
.32
46
.55
46
.55
46
.55
30
.28
42
.39
35
.33
71
.11
61
.75
64
.63
49
.99
ha
o
64
.50
64
.11
64
.30
57
.89
51
.42
54
.47
67
.83
55
.43
61
.01
45
.07
45
.07
45
.07
30
.08
35
.76
32
.67
72
.61
62
.37
65
.35
49
.38
xin
xin
65
.49
58
.71
61
.92
48
.54
44
.85
46
.62
61
.59
62
.28
61
.93
44
.75
44
.75
44
.75
35
.19
38
.62
36
.83
63
.04
65
.83
64
.27
48
.46
zh
an
g
55
.35
68
.25
61
.13
42
.03
55
.62
47
.88
52
.57
73
.05
61
.14
44
.46
44
.46
44
.46
42
.00
30
.28
35
.19
62
.84
69
.22
65
.21
48
.07
ku
mm
erf
eld
69
.77
56
.97
62
.72
46
.39
39
.56
42
.70
63
.60
57
.30
60
.29
45
.35
45
.35
45
.35
35
.05
42
.26
38
.32
58
.74
61
.58
59
.91
47
.10
zh
ek
ov
a
67
.49
37
.60
48
.29
28
.87
20
.66
24
.08
67
.14
56
.67
61
.46
40
.43
40
.43
40
.43
31
.57
41
.21
35
.75
52
.77
57
.05
53
.77
40
.43
irw
in
17
.06
61
.09
26
.67
12
.45
50
.60
19
.98
35
.07
89
.90
50
.46
31
.68
31
.68
31
.68
45
.84
17
.38
25
.21
51
.48
56
.83
51
.12
31
.88
Ta
ble
12
:P
erf
orm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
cl
os
ed
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
74
.31
67
.87
70
.9
4
62
.83
59
.34
61
.0
3
68
.85
69
.01
68
.9
3
56
.70
56
.70
56
.7
0
43
.29
46
.80
44
.9
8
71
.90
76
.55
73
.96
58
.3
1
cai
67
.15
67
.64
67
.40
56
.73
58
.90
57
.80
64
.60
71
.03
67
.66
53
.37
53
.37
53
.37
42
.71
40
.68
41
.67
69
.77
73
.96
71
.62
55
.71
ury
up
ina
70
.60
66
.31
68
.39
59
.70
55
.70
57
.63
66
.29
64
.12
65
.18
51
.42
51
.42
51
.42
38
.34
42
.17
40
.16
69
.23
68
.54
68
.88
54
.32
kle
nn
er
64
.41
60
.28
62
.28
49
.04
50
.71
49
.86
61
.70
68
.61
64
.97
50
.03
50
.03
50
.03
41
.28
39
.70
40
.48
66
.05
73
.90
69
.05
51
.77
irw
in
24
.60
62
.27
35
.27
18
.56
51
.01
27
.21
38
.97
85
.57
53
.55
33
.86
33
.86
33
.86
43
.33
19
.36
26
.76
51
.62
52
.91
51
.76
35
.84
Ta
ble
13
:P
erf
orm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
op
en
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
79
.52
71
.25
75
.1
6
65
.87
62
.05
63
.9
0
69
.52
70
.55
70
.0
3
59
.26
59
.26
59
.2
6
46
.29
50
.48
48
.3
0
72
.00
78
.55
74
.7
7
60
.7
4
nu
gu
es
74
.18
70
.74
72
.42
64
.33
60
.05
62
.12
68
.26
65
.17
66
.68
53
.84
53
.84
53
.84
39
.86
44
.23
41
.93
72
.53
71
.04
71
.75
56
.91
ch
an
g
63
.37
73
.18
67
.92
55
.00
65
.50
59
.79
62
.16
76
.65
68
.65
54
.95
54
.95
54
.95
46
.77
37
.17
41
.42
70
.97
79
.30
74
.29
56
.62
san
tos
65
.82
69
.90
67
.80
57
.76
61
.39
59
.52
64
.49
70
.27
67
.26
51
.87
51
.87
51
.87
41
.42
38
.16
39
.72
72
.72
71
.97
72
.34
55
.50
ko
bd
an
i
67
.11
65
.09
66
.08
62
.63
56
.80
59
.57
73
.20
62
.22
67
.27
44
.49
44
.49
44
.49
32
.87
37
.25
34
.92
64
.07
64
.13
64
.10
53
.92
sto
ya
no
v
76
.90
64
.73
70
.29
69
.81
55
.01
61
.54
77
.07
52
.54
62
.48
48
.08
48
.08
48
.08
30
.97
44
.84
36
.64
76
.57
60
.33
62
.96
53
.55
zh
an
g
59
.62
71
.19
64
.89
46
.06
58
.75
51
.64
53
.89
73
.41
62
.16
46
.62
46
.62
46
.62
43
.49
32
.11
36
.95
64
.11
70
.47
66
.54
50
.25
son
g
58
.43
77
.64
66
.68
46
.66
68
.40
55
.48
54
.40
70
.19
61
.29
43
.62
43
.62
43
.62
43
.77
25
.88
32
.53
66
.29
58
.76
60
.22
49
.77
zh
ek
ov
a
69
.19
57
.27
62
.67
33
.48
37
.15
35
.22
55
.47
68
.23
61
.20
41
.31
41
.31
41
.31
38
.29
34
.65
36
.38
53
.45
63
.33
54
.79
44
.27
Ta
ble
14
:P
erf
orm
an
ce
of
sys
tem
sin
the
sup
ple
me
nta
ry
cl
os
ed
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
bo
un
da
ri
es
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
78
.71
72
.33
75
.39
66
.93
63
.91
65
.39
70
.09
71
.49
70
.78
59
.78
59
.78
59
.78
46
.34
49
.62
47
.92
73
.38
79
.00
75
.83
61
.36
Ta
ble
15
:P
erf
orm
an
ce
of
sys
tem
sin
the
sup
ple
me
nta
ry
op
en
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
bo
un
da
ri
es
16
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
ch
an
g
10
0
10
0
10
0
80
.46
84
.75
82
.55
72
.84
74
.57
73
.70
69
.71
69
.71
69
.71
70
.45
60
.75
65
.24
78
.01
76
.57
77
.26
73
.83
Ta
ble
16
:P
erf
orm
an
ce
of
sys
tem
sin
the
su
pp
le
m
en
ta
ry
,c
lo
se
d
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
m
en
ti
on
s
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
83
.37
10
0
90
.93
74
.79
89
.68
81
.56
67
.46
86
.88
75
.95
70
.73
70
.73
70
.73
77
.75
51
.05
61
.64
76
.65
85
.85
80
.35
73
.05
Ta
ble
17
:P
erf
orm
an
ce
of
sys
tem
sin
the
su
pp
le
m
en
ta
ry
,o
pe
n
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
m
en
ti
on
s
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
76
.79
68
.34
72
.3
2
63
.29
58
.96
61
.05
68
.84
68
.72
68
.78
57
.28
57
.28
57
.2
8
44
.19
48
.75
46
.3
6
70
.93
76
.58
73
.36
58
.7
3
sap
en
a
95
.27
29
.07
44
.55
56
.99
63
.91
60
.25
62
.89
72
.31
67
.27
53
.90
53
.90
53
.90
45
.22
38
.70
41
.71
69
.71
73
.32
71
.32
56
.41
ch
an
g
69
.88
63
.61
66
.60
58
.48
58
.48
58
.48
67
.42
70
.91
69
.1
2
55
.21
55
.21
55
.21
42
.66
42
.66
42
.66
71
.42
77
.36
73
.9
6
56
.75
nu
gu
es
72
.96
71
.08
72
.01
62
.68
59
.46
61
.03
67
.24
64
.89
66
.04
52
.82
52
.82
52
.82
39
.25
42
.50
40
.81
72
.57
70
.86
71
.68
55
.96
san
tos
70
.39
65
.67
67
.95
61
.28
56
.20
58
.63
69
.25
63
.16
66
.07
50
.47
50
.47
50
.47
36
.51
41
.15
38
.69
73
.92
67
.32
69
.93
54
.46
son
g
59
.24
82
.39
68
.92
54
.92
69
.29
61
.27
60
.89
66
.27
63
.46
46
.97
46
.97
46
.97
44
.49
31
.15
36
.65
69
.73
59
.87
61
.61
53
.79
sto
ya
no
v
74
.43
68
.28
71
.22
67
.18
57
.08
61
.7
2
74
.06
53
.45
62
.09
47
.40
47
.40
47
.40
32
.78
42
.52
37
.02
74
.10
59
.34
61
.31
53
.61
sob
ha
71
.06
65
.06
67
.93
53
.91
52
.64
53
.27
63
.17
66
.14
64
.62
50
.80
50
.80
50
.80
41
.77
43
.03
42
.39
61
.91
69
.15
64
.49
53
.43
ko
bd
an
i
65
.98
63
.83
64
.89
59
.22
54
.81
56
.93
70
.49
63
.12
66
.60
44
.17
44
.14
44
.15
33
.19
36
.50
34
.77
62
.52
64
.25
63
.32
52
.77
zh
ou
64
.11
66
.74
65
.40
48
.00
55
.51
51
.48
57
.18
73
.71
64
.40
48
.40
48
.40
48
.40
44
.18
37
.35
40
.48
61
.54
74
.86
65
.30
52
.12
ch
art
on
71
.01
67
.64
69
.28
59
.24
53
.82
56
.40
67
.10
59
.02
62
.80
48
.91
48
.91
48
.91
35
.96
41
.39
38
.48
70
.65
62
.71
65
.34
52
.56
ya
ng
73
.73
58
.97
65
.53
61
.23
47
.45
53
.47
71
.88
55
.13
62
.40
47
.05
47
.05
47
.05
30
.54
43
.16
35
.77
71
.39
61
.92
64
.83
50
.55
ha
o
66
.79
66
.38
66
.59
59
.55
52
.89
56
.02
68
.27
55
.46
61
.20
45
.95
45
.95
45
.95
30
.76
36
.81
33
.51
73
.22
62
.73
65
.78
50
.24
xin
xin
69
.05
61
.91
65
.28
50
.99
47
.11
48
.97
61
.59
62
.70
62
.14
45
.64
45
.64
45
.64
35
.86
39
.57
37
.62
63
.42
66
.29
64
.68
49
.58
zh
an
g
57
.41
70
.78
63
.40
43
.48
57
.53
49
.53
52
.44
73
.60
61
.24
44
.97
44
.97
44
.97
42
.71
30
.44
35
.55
63
.12
69
.63
65
.53
48
.77
ku
mm
erf
eld
71
.05
58
.01
63
.87
47
.42
40
.44
43
.65
63
.73
57
.39
60
.39
45
.76
45
.76
45
.76
35
.30
42
.72
38
.66
58
.89
61
.77
60
.07
47
.57
zh
ek
ov
a
72
.65
40
.48
51
.99
31
.73
22
.70
26
.46
66
.92
56
.68
61
.37
41
.04
41
.04
41
.04
31
.93
42
.17
36
.34
53
.09
57
.86
54
.22
41
.39
irw
in
17
.58
62
.96
27
.49
12
.69
51
.59
20
.37
34
.88
89
.98
50
.27
31
.71
31
.71
31
.71
46
.13
17
.33
25
.20
51
.51
56
.93
51
.14
31
.95
Ta
ble
18
:H
ea
d
w
or
d
ba
se
d
pe
rfo
rm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
cl
os
ed
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
76
.01
69
.43
72
.5
7
64
.40
60
.83
62
.5
7
69
.34
69
.57
69
.4
5
57
.68
57
.68
57
.6
8
44
.15
47
.85
45
.9
2
72
.23
76
.94
74
.3
2
59
.3
1
cai
69
.32
69
.82
69
.57
58
.39
60
.63
59
.49
64
.88
71
.53
68
.04
54
.36
54
.36
54
.36
43
.74
41
.58
42
.64
70
.13
74
.39
72
.01
56
.72
ury
up
ina
72
.10
67
.72
69
.84
60
.74
56
.68
58
.64
66
.43
64
.25
65
.32
52
.00
52
.00
52
.00
38
.87
42
.85
40
.76
69
.43
68
.73
69
.07
54
.91
kle
nn
er
71
.73
67
.14
69
.36
55
.17
57
.04
56
.09
62
.67
70
.69
66
.44
53
.25
53
.25
53
.25
44
.27
42
.39
43
.31
67
.45
75
.92
70
.68
55
.28
irw
in
25
.24
63
.87
36
.18
18
.90
51
.94
27
.71
38
.79
85
.64
53
.40
33
.89
33
.89
33
.89
43
.59
19
.31
26
.76
51
.66
52
.98
51
.80
35
.96
Ta
ble
19
:H
ea
d
w
or
d
ba
se
d
pe
rfo
rm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
op
en
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
17
erence evaluations, that is as expected, given that the
task here includes predicting the underlying men-
tions and mention boundaries, the insistence on ex-
act match, and given that the relatively easier appos-
itive coreference cases are not included in this mea-
sure. The top-performing system (lee) had a score
of 57.79 which is about 1.8 points higher than that
of the second (sapena) and third (chang) ranking
systems, which scored 55.99 and 55.96 respectively.
Another 1.5 points separates them from the fourth
best score of 54.53 (nugues). Thus the performance
differences between the better-scoring systems were
not large, with only about three points separating the
top four systems.
This becomes even clearer if we merge in the re-
sults of systems that participated only in the open
track but that made relatively limited use of outside
resources.21 Comparing that way, the cai system
scores in the same ball park as the second rank sys-
tems (sapena and chang). The uryupina system sim-
ilarly scores very close to nugues?s 54.53
Given that our choice of the official metric was
somewhat arbitrary, if is also useful to look at
the individual metrics, including the mention-based
CEAFm and BLANC metrics that were not part of
the official metric. The lee system which scored
the best using the official metric does slightly worse
than song on the MUC metric, and also does slightly
worse than chang on the B-CUBED and BLANC met-
rics. However, it does much better than every other
group on the entity-based CEAFe, and this is the pri-
mary reason for its 1.8 point advantage in the offi-
cial score. If the CEAFe measure does indicate the
accuracy of entities in the response, this suggests
that the lee system is doing better on getting coher-
ent entities than any other system. This could be
partly due to the fact that that system is primarily
a precision-based system that would tend to create
purer entities. The CEAFe measure also seems to pe-
nalize other systems more harshly than do the other
measures.
We cannot compare these results to the ones ob-
tained in the SEMEVAL-2010 coreference task using
a small portion of OntoNotes data because it was
only using nominal entities, and had heuristically
added singleton mentions to the OntoNotes data22
21The cai system specifically mentions that, and the only re-
source that the uryupina system used outside of the closed track
setting was the Stanford named entity tagger.
22The documentation that comes with the SEMEVAL data
package from LDC (LDC2011T01) states: ?Only nominal
mentions and identical (IDENT) types were taken from the
OntoNotes coreference annotation, thus excluding coreference
5.2 Predicted plus gold mention boundaries
We also explored performance when the systems
were provided with the gold mention boundaries,
that is, with the exact spans (expressed in terms of
token offsets) for all of the NP constituents in the
human-annotated parse trees for the test data. Sys-
tems could use this additional data to ensure that the
output mention spans in their entity chains would not
clash with those in the answer set. Since this was
a secondary evaluation, it was an optional element,
and not all participants ran their systems on this task
variation. The results for those systems that did par-
ticipate in this optional task are shown in Tables 14
(closed track) and 15 (open track).
Most of the better scoring systems did supply
these results. While all systems did slightly better
here in terms of raw scores, the performance was
not much different from the official task, indicating
that mention boundary errors resulting from prob-
lems in parsing do not contribute significantly to the
final output.23
One side benefit of performing this supplemental
evaluation was that it revealed a subtle bug in the
automatic scoring routine that we were using that
could double-count duplicate correct mentions in a
given entity chain. These can occur, for example, if
the system considers a unit-production NP-PRP com-
bination as two mentions that identify the exact same
token in the text, and reports them as separate men-
tions. Most systems had a filter in their processing
that selected only one of these duplicate mentions,
but the kobdani system considered both as potential
mentions, and its developers tuned their algorithm
using that flawed version of the scorer.
When we fixed the scorer and re-evaluated all of
the systems, the kobdani system was the only one
whose score was affected significantly, dropping by
about 8 points, which lowered that system?s rank
from second to ninth. It is not clear how much of
this was owing to the fact that the system?s param-
relations with verbs and appositives. Since OntoNotes is only
annotated with multi-mention entities, singleton referential ele-
ments were identified heuristically: all NPs and possessive de-
terminers were annotated as singletons excluding those func-
tioning as appositives or as pre-modifiers but for NPs in the
possessive case. In coordinated NPs, single constituents as well
as the entire NPs were considered to be mentions. There is no
reliable heuristic to automatically detect English expletive pro-
nouns, thus they were (although inaccurately) also annotated as
singletons.?
23It would be interesting to measure the overlap between the
entity clusters for these two cases, to see whether there was
any substantial difference in the mention chains, besides the ex-
pected differences in boundaries for individual mentions.
18
eters had been tuned using the scorer with the bug,
which double-credited duplicate mentions. To find
out for sure, one would have to re-tune the system
using the modified scorer.
One difficulty with this supplementary evaluation
using gold mention boundaries is that those bound-
aries alone provide only very partial information.
For the roughly 10% of mentions that the automatic
parser did not correctly identify, while the systems
knew the correct boundaries, they had no hierarchi-
cal parser or semantic role label information, and
they also had to further approximate the already
heuristic head word identification. This incomplete
data complicated the systems? task and also compli-
cates interpretation of the results.
5.3 Predicted plus gold mentions
The final supplementary condition that we explored
was if the systems were supplied with the manually-
annotated spans for exactly those mentions that did
participate in the gold standard coreference chains.
This supplies significantly more information than
the previous case, where exact spans were supplied
for all NPs, since the gold mentions list here will also
include verb headwords that are linked to event NPs,
but will not include singleton mentions, which do
not end up as part of any chain. The latter constraint
makes this test seem somewhat artificial, since it di-
rectly reveals part of what the systems are designed
to determine, but it still has some value in quanti-
fying the impact that mention detection has on the
overall task and what the results are if the mention
detection is perfect.
Since this was a logical extension of the task and
since the data was available to the participants for
the development set, a few of the sites did run ex-
periments of this type. Therefore we decided to pro-
vide the gold mentions data to a few sites who had
reported these scores, so that we could compute the
performance on the test set. The results of these ex-
periments are shown in Tables 16 and 17. The results
show that performance does go up significantly, in-
dicating that it is markedly easier for the systems
to generate better entities given gold mentions. Al-
though, ideally, one would expect a perfect mention
detection score, it is the case that one of the two sys-
tems ? lee ? did not get a 100% Recall. This could
possibly be owing to unlinked singletons that were
removed in post-processing.
The lee system developers also ran a further ex-
periment where both gold mentions for the elements
of the coreference chains and also gold annota-
tions for all the other layers were available to the
system. Surprisingly, the improvement in corefer-
ence performance from having gold annotation of
the other layers was almost negligible. This sug-
gests that either: i) the automatic models are pre-
dicting those layers well enough that switching to
gold doesn?t make much difference; ii) information
from the other layers does not provide much lever-
age for coreference resolution; or iii) current coref-
erence models are not capable of utilizing the infor-
mation from these other layers effectively. Given
the performance numbers on the individual layers
cited earlier, (i) seems unlikely, and we hope that
further research in how best to leverage these lay-
ers will result in models that can benefit from them
more definitively.
5.4 Head word based scoring
In order to check how stringent the official, exact
match scoring is, we also performed a relaxed scor-
ing. Unlike ACE and MUC, the OntoNotes data does
not have manually annotated minimum spans that
a mention must contain to be considered correct.
However, OntoNotes does have manual syntactic
analysis in the form of the Treebank. Therefore, we
decided to approximate the minimum spans by using
the head words of the mentions using the gold stan-
dard syntax tree. If the response mention contained
the head word and did not exceed the true mention
boundary, then it was considered correct ? both from
the point of view of mention detection, and corefer-
ence resolution. The scores using this relaxed strat-
egy for the open and closed track submissions using
predicted data are shown in Tables 18 and 19. It
can be observed that the relaxed, head word based,
scoring does not improve performance very much.
The only exception was the klenner system whose
performance increased from 51.77 to 55.28. Over-
all, the ranking remained quite stable, though it did
change for some adjacent systems which had very
close exact match scores.
5.5 Genre variation
In order to check how the systems did on various
genres, we scored their performance per genre as
well. Tables 20 and 21 summarize genre based per-
formance for the closed and open track participants
respectively. System performance does not seem
to vary as much across the different genres as is
normally the case with language processing tasks,
which could suggest that coreference is relatively
genre insensitive, or it is possible that scores are
two low for the difference to be apparent. Compar-
isons are difficult, however, because the spoken gen-
19
MD MUC BCUB Cm Ce BLANC O MD MUC BCUB Cm Ce BLANC O
F F F F F F F F F F F F F F
lee GENRE zhou GENRE
BC 72.2 60.0 66.2 53.9 43.7 71.7 56.7 BC 64.1 49.5 62.1 45.3 38.8 61.8 50.1
BN 72.0 59.0 68.7 57.6 48.7 68.8 58.8 BN 60.8 45.9 64.4 49.5 41.2 66.8 50.5
MZ 70.1 58.0 72.2 61.6 50.9 75.0 60.4 MZ 58.8 44.4 66.9 50.1 41.8 64.6 51.0
NW 65.4 54.3 69.4 56.5 45.5 70.4 56.4 NW 57.7 44.8 65.7 48.7 40.3 63.1 50.2
TC 75.9 66.8 69.5 59.3 41.3 81.6 59.2 TC 69.2 58.1 60.8 43.1 35.7 62.6 51.5
WB 73.0 63.9 65.7 54.2 42.7 73.4 57.5 WB 67.4 55.4 62.8 47.9 39.2 69.1 52.5
sapena charton
BC 48.7 58.8 64.6 50.8 39.4 70.4 54.3 BC 65.8 53.1 59.1 44.6 35.2 64.4 49.1
BN 47.1 60.0 69.1 57.4 45.0 74.3 58.0 BN 65.5 52.0 64.0 50.0 39.6 65.9 51.9
MZ 35.3 59.2 72.3 60.4 48.2 75.0 59.9 MZ 61.7 46.3 64.6 49.7 39.9 64.1 50.3
NW 35.2 57.9 69.7 55.3 41.9 73.8 56.5 NW 57.6 44.6 64.5 48.2 37.7 67.0 48.9
TC 60.4 64.3 63.3 48.3 35.1 68.8 54.2 TC 73.1 66.8 56.2 42.8 29.9 58.1 51.0
WB 46.3 60.1 62.5 49.1 37.4 67.4 53.3 WB 67.6 57.6 59.3 45.1 33.3 66.6 50.0
chang yang
BC 65.5 56.4 67.1 51.5 39.8 71.6 54.4 BC 65.7 53.8 62.3 46.8 35.0 67.5 50.3
BN 66.6 57.4 69.1 56.0 45.6 70.5 57.4 BN 66.0 53.1 63.8 49.1 40.0 63.1 52.3
MZ 61.6 52.7 71.3 57.6 46.4 72.9 56.8 MZ 58.8 43.9 59.7 42.6 32.8 55.5 45.5
NW 61.0 53.3 69.1 54.1 42.1 71.9 54.8 NW 57.2 44.7 62.9 45.3 35.0 62.7 47.6
TC 72.2 68.5 71.4 59.6 37.7 81.7 59.2 TC 74.2 66.8 66.3 55.3 36.0 76.1 56.4
WB 66.4 59.7 66.7 52.7 39.4 74.7 55.3 WB 67.6 57.6 57.0 42.6 32.1 60.1 48.9
nugues hao
BC 71.4 59.2 62.4 48.2 37.2 68.4 52.9 BC 68.9 58.7 58.9 44.8 31.7 64.9 49.8
BN 70.0 58.5 67.4 54.5 43.1 73.1 56.3 BN 62.0 51.1 63.0 46.2 35.5 64.1 49.9
MZ 65.4 53.6 68.6 54.2 42.2 70.1 54.8 MZ 60.3 46.7 61.5 46.3 34.3 61.9 47.5
NW 61.8 51.9 67.0 51.3 39.2 69.4 52.7 NW 57.2 47.7 63.3 45.5 32.9 66.0 48.0
TC 77.2 69.2 63.9 53.0 37.9 72.2 57.0 TC 67.9 60.4 58.8 44.7 30.3 68.3 49.8
WB 72.9 64.2 63.4 51.1 38.5 74.3 55.4 WB 71.4 61.8 55.7 42.6 30.0 64.4 49.2
santos xinxin
BC 66.6 57.2 64.8 48.5 37.2 68.6 53.0 BC 64.8 47.8 60.2 43.9 35.5 65.1 47.9
BN 66.9 57.3 66.9 52.3 41.0 71.8 55.1 BN 61.5 44.7 63.2 47.0 38.9 65.8 48.9
MZ 62.7 51.0 65.9 48.9 37.8 64.5 51.6 MZ 54.6 35.5 64.5 45.7 37.7 61.0 45.9
NW 58.4 49.5 66.2 48.1 37.4 66.9 51.0 NW 54.3 39.5 64.0 45.0 37.5 61.1 47.0
TC 74.2 66.9 65.9 52.5 35.5 72.5 56.1 TC 74.2 62.0 57.9 45.4 33.4 66.5 51.1
WB 70.4 63.2 63.4 49.5 38.2 70.3 55.0 WB 66.9 52.6 58.5 42.2 35.9 63.4 49.0
song zhang
BC 68.9 61.4 61.0 44.1 34.3 59.5 52.2 BC 65.8 50.6 61.1 45.3 35.5 67.3 49.1
BN 66.2 58.4 64.8 49.0 38.2 65.2 53.8 BN 56.3 43.9 61.0 45.8 35.8 66.8 46.9
MZ 63.7 53.4 65.5 49.9 39.0 63.4 52.6 MZ 57.1 35.1 62.2 44.4 36.1 59.4 44.5
NW 62.4 53.6 64.3 48.0 37.2 62.7 51.7 NW 49.9 37.8 61.8 43.2 35.2 59.8 44.9
TC 76.9 74.4 62.0 43.3 33.2 58.1 56.5 TC 75.4 65.9 60.2 46.0 32.1 67.1 52.7
WB 70.0 63.0 60.1 43.3 31.8 60.8 51.6 WB 69.2 55.4 57.4 42.5 34.6 64.7 49.1
stoyanov kummerfield
BC 69.5 59.1 57.6 43.5 34.0 58.7 50.2 BC 66.4 41.5 55.6 41.7 36.2 57.9 44.4
BN 69.2 59.1 65.4 50.4 40.0 65.5 54.8 BN 68.3 48.2 63.4 51.7 44.7 61.6 52.1
MZ 66.7 55.1 65.5 51.0 39.9 63.7 53.5 MZ 58.0 39.9 65.8 51.0 43.4 64.1 49.7
NW 61.8 52.0 63.3 46.2 36.1 62.0 50.5 NW 55.2 41.3 64.7 46.8 37.0 63.5 47.6
TC 72.6 66.6 57.6 42.3 31.0 57.6 51.7 TC 61.8 34.5 51.5 34.7 30.0 54.1 38.7
WB 71.5 63.9 58.3 44.8 33.1 61.1 51.8 WB 68.2 48.1 56.0 44.4 38.6 59.6 47.6
sobha zhekova
BC 68.3 51.7 61.4 47.8 40.4 62.9 51.2 BC 50.5 23.8 60.6 39.4 35.1 53.4 39.8
BN 66.5 51.9 66.5 53.7 45.5 66.3 54.6 BN 51.2 26.0 62.4 42.5 37.5 54.3 42.0
MZ 68.8 54.9 70.3 58.9 49.3 69.8 58.1 MZ 44.0 22.6 63.4 43.3 37.3 56.0 41.1
NW 55.1 43.1 65.8 48.6 39.0 64.9 49.3 NW 39.7 19.4 62.8 41.0 35.8 53.7 39.3
TC 71.5 55.1 57.5 44.2 36.7 60.5 49.7 TC 59.4 31.6 58.2 37.7 33.6 54.1 41.1
WB 70.5 55.7 59.2 46.6 39.8 62.6 51.6 WB 54.1 27.8 58.7 38.5 34.7 53.0 40.4
kobdani irwin
BC 63.2 56.3 65.8 40.6 32.4 61.9 51.5 BC 23.5 16.1 46.0 29.4 23.6 49.8 28.6
BN 63.5 55.7 68.5 46.9 37.5 64.6 53.9 BN 24.9 20.0 49.7 34.2 27.1 52.9 32.3
MZ 57.5 52.2 69.8 45.7 36.4 61.7 52.8 MZ 23.2 17.9 55.9 36.2 28.5 53.0 34.1
NW 52.2 41.7 64.4 43.2 33.7 62.6 46.6 NW 27.5 21.6 56.4 33.9 27.3 52.6 35.1
TC 67.7 60.2 65.3 36.6 28.5 57.6 51.3 TC 28.0 19.3 38.2 24.5 18.7 49.0 25.4
WB 68.7 62.8 62.4 42.5 32.9 64.0 52.7 WB 33.6 24.8 47.6 29.7 23.0 50.2 31.8
Table 20: Detailed look at the performance per genre for the official, closed track using automatic performance. MD
represents MENTION DETECTION; BCUB represents B-CUBED; Cm represents CEAFm; Ce represents CEAFe and O
represents the OFFICIAL score.
20
res were treated here with perfect speech recognition
accuracy and perfect speaker turn information. Un-
der more realistic application conditions, the spread
in performance between genres might be greater.
MD MUC BCUB Cm Ce BLANC O
F F F F F F F
lee GENRE
BC 72.7 61.7 67.0 54.5 43.6 72.7 57.4
BN 72.0 60.6 69.4 57.9 48.1 70.3 59.3
MZ 69.9 58.4 72.1 61.2 50.1 75.2 60.2
NW 65.3 55.8 70.0 56.7 44.9 71.7 56.9
TC 76.6 68.4 70.4 59.6 40.8 82.1 59.9
WB 73.8 65.5 66.2 54.5 42.1 74.2 57.9
cai
BC 69.7 59.1 66.0 50.5 39.9 69.2 55.0
BN 68.6 57.6 67.8 55.4 45.5 68.2 56.9
MZ 64.0 51.1 69.5 55.9 45.6 71.2 55.4
NW 60.3 49.9 67.8 52.7 41.2 69.1 53.0
TC 75.6 70.5 72.2 59.6 38.0 80.3 60.2
WB 71.7 63.9 65.0 51.8 39.8 72.8 56.2
uryupina
BC 70.2 58.3 62.7 48.7 38.0 68.7 53.0
BN 69.0 57.6 66.8 53.6 43.1 69.2 55.8
MZ 65.7 52.4 68.3 54.3 43.6 68.8 54.8
NW 62.6 52.1 68.3 53.2 41.2 71.3 53.9
TC 75.7 67.1 61.0 50.7 34.6 67.1 54.2
WB 72.0 61.7 60.9 48.8 38.3 67.6 53.6
klenner
BC 63.2 50.3 63.4 48.2 38.9 66.8 50.8
BN 63.1 48.6 65.0 51.0 42.6 66.0 52.1
MZ 59.1 43.7 67.1 52.9 45.3 65.0 52.0
NW 55.3 41.3 65.0 48.0 39.6 64.5 48.7
TC 73.9 64.9 67.9 56.4 39.0 78.0 57.3
WB 66.8 58.1 64.0 50.1 39.6 72.7 53.9
irwin
BC 36.6 27.6 50.9 32.0 25.5 50.2 34.7
BN 30.8 24.6 51.9 36.4 28.6 54.8 35.0
MZ 26.1 20.0 57.3 37.6 29.4 54.3 35.6
NW 32.3 24.7 58.4 34.7 27.9 51.1 37.0
TC 46.4 34.3 44.6 29.4 21.9 51.7 33.6
WB 41.7 32.9 50.5 32.9 25.1 53.2 36.2
Table 21: Detailed look at the performance per genre for
the official, open track using predicted information. MD
represents MENTION DETECTION; BCUB represents B-
CUBED; Cm represents CEAFm; Ce represents CEAFe and
O represents the OFFICIAL score.
6 Approaches
Tables 22 and 23 summarize the approaches of the
participating systems along with some of the impor-
tant dimensions.
Most of the systems broke the problem into two
phases, first identifying the potential mentions in the
text and then linking the mentions to form corefer-
ence chains. Most participants also used rule-based
approaches for mention detection, though two did
use trained models. While trained morels seem able
to better balance precision and recall, and thus to
achieve a higher F-score on the mention task itself,
their recall tends to be quite a bit lower than that
achievable by rule-based systems designed to fa-
vor recall. This impacts coreference scores because
the full coreference system has no way to recover
if the mention detection stage misses a potentially
anaphoric mention.
Only one of the participating systems cai at-
tempted to do joint mention detection and corefer-
ence resolution. While it did not happen to be among
the top-performing systems, the difference in perfor-
mance could be due to the richer features used by
other systems rather than to the use of a joint model.
Most systems represented the markable mentions
internally in terms of the parse tree NP constituent
span, but some systems used shared attribute mod-
els, where the attributes of the merged entity are
determined collectively by heuristically merging the
attribute types and values of the different constituent
mentions.
Various types of trained models were used for pre-
dicting coreference. It is interesting to note that
some of the systems, including the best-performing
one, used a completely rule-based approach even for
this component.
Most participants appear not to have focused
much on eventive coreference, those coreference
chains that build off verbs in the data. This usu-
ally meant that mentions that should have linked to
the eventive verb were instead linked in with some
other entity. Participants may have chosen not to fo-
cus on events because they pose unique challenges
while making up only a small portion of the data.
Roughly 91% of mentions in the data are NPs and
pronouns.
In the systems that used trained models, many
systems used the approach described in Soon et al
(2001) for selecting the positive and negative train-
ing examples, while others used some of the al-
ternative approaches that have been introduced in
the research literature more recently. Many of the
trained systems also were able to improve their per-
formance by using feature selection, though things
varied some depending on the example selection
strategy and the classifier used. Almost half of the
trained systems used the feature selection strategy
from Soon et al (2001) and found it beneficial. It is
not clear whether the other systems did not explore
this path, or whether it just did not prove as useful in
their case.
7 Conclusions
In this paper we described the anaphoric coreference
information and other layers of annotation in the
21
Ta
sk
Sy
nta
x
Le
arn
ing
Fra
me
wo
rk
Ma
rka
ble
Ide
nti
fic
ati
on
Ma
rka
ble
Ve
rb
Fe
atu
re
Se
lec
tio
n
#F
eat
ure
s
Tra
ini
ng
lee
C+
O
P
Ru
le-
ba
sed
Ru
les
to
ex
clu
de
Co
pu
lar
co
nst
ruc
tio
n,
Ap
po
sit
ive
s,P
leo
na
sti
ci
t,e
tc.
Fe
atu
re
de
pe
nd
en
t
wi
th
sha
red
att
rib
ute
s
?
?
?
sap
en
a
C
P
De
cis
ion
Tre
e+
Re
lax
ati
on
La
be
lin
g
NP
(m
ax
im
al
spa
n)
+P
RP
+N
E
+C
ap
ita
liz
ed
no
un
he
uri
sti
c
Fu
llp
hra
se
?
?
Tra
in
+D
ev
ch
an
g
C
P
Le
arn
ing
Ba
sed
Jav
a
NP
,N
E,
PR
P,
PR
P$
Fu
llp
hra
se
?
?
Tra
in
+D
ev
cai
O
P
Co
mp
ute
hy
pe
red
ge
we
igh
tso
n
30
%
of
tra
ini
ng
da
ta
NP
,P
RP
,P
RP
$,
Ba
se
ph
ras
ec
hu
nk
s,
Ple
on
ast
ic
it
filt
er
Fu
llp
hra
se
?
?
?
nu
gu
es
C
D
Lo
gis
tic
Re
gre
ssi
on
(LI
BL
IN
EA
R)
NP
,P
RP
$a
nd
seq
ue
nc
eo
fN
NP
(s)
in
po
st
pro
ces
sin
gu
sin
gA
LI
AS
an
dS
TR
IN
GM
AT
CH
He
ad
wo
rd
?
Fo
rw
ard
+B
ack
wa
rd
sta
rtin
gf
rom
So
on
fea
tur
es
et
24
Tra
in
+D
ev
ury
up
ina
O
P
De
cis
ion
Tre
e.
Di
ffe
ren
t
cla
ssi
fie
rs
for
Pro
no
mi
na
la
nd
no
n-P
ron
om
ina
lm
en
tio
ns
NP
,N
E,
PR
P,
PR
P$
,a
nd
rul
es
to
ex
clu
de
som
es
pe
cifi
cc
ase
s
Fu
llp
hra
se
?
Mu
lti-
Ob
jec
tiv
e
Op
tim
iza
tio
no
nt
hre
e
spl
its
.N
SG
A-
II
46
Tra
in
+D
ev
san
tos
C
P
ET
L
(E
ntr
op
yg
uid
ed
Tra
nsf
orm
ati
on
al
Le
arn
ing
)
co
mm
itte
ea
nd
Ra
nd
om
Fo
res
t
(W
EK
A)
Al
lN
Pa
nd
all
pro
no
un
sa
nd
PE
R,
OR
G,
GP
E
in
NP
Fu
llp
hra
se
?
Inh
ere
nt
to
the
cla
ssi
fie
rs
Tra
in
+D
ev
son
g
C
P
Ma
xE
nt
(O
pe
nN
LP
)
Me
nti
on
de
tec
tio
nc
las
sifi
er
Fu
llp
hra
se
?
Sa
me
fea
tur
es
et,
bu
t
pe
rc
las
sifi
er
40
Tra
in
sto
ya
no
v
C
P
Av
era
ge
dp
erc
ep
tro
n
NE
an
dp
oss
ess
ive
sin
ad
dit
ion
to
AC
E
ba
sed
sys
tem
Fu
llp
hra
se
?
?
76
?
sob
ha
C
P
CR
Ff
or
no
n-p
ron
om
ina
la
nd
sal
ien
ce
fac
tor
for
pro
no
mi
na
l
res
olu
tio
n
Ma
ch
ine
lea
rne
dp
leo
na
sti
ci
t,p
lus
NP
,P
RP
,
PR
P$
an
dN
E
Mi
nim
al
(C
hu
nk
/N
E)
an
dM
ax
im
um
spa
n
?
?
Tra
in
kle
nn
er
O
D
Ru
le-
ba
sed
.S
ali
en
ce
me
asu
re
usi
ng
de
pe
nd
en
cie
sg
en
era
ted
fro
m
tra
ini
ng
da
ta
NP
,N
E,
PR
P,
PR
P$
Sh
are
d
att
rib
ute
d/t
ran
sit
ivi
ty
by
usi
ng
av
irtu
al
pro
tot
yp
e
?
?
?
ko
bd
an
i
C
P
De
cis
ion
Tre
e
NP
(no
me
nti
on
of
PR
P$
)
Sta
rtw
ord
,E
nd
wo
rd
an
dH
ead
of
NP
?
Inf
orm
ati
on
ga
in
rat
io
Tra
in
zh
ou
C
P
SV
M
tre
ek
ern
el
usi
ng
BC
po
rtio
n
of
the
da
ta
Ru
le-
ba
sed
;F
ive
rul
es:
PR
P$
,P
RP
,N
E,
sm
all
est
NP
sub
sum
ing
NE
an
dD
ET
+N
P
Fu
llp
hra
se
?
?
17
Tra
in
+D
ev
ch
art
on
C
P
Mu
lti-
lay
er
pe
rce
ptr
on
Ru
les
ba
sed
on
PO
S,
NE
an
dfi
lte
ro
ut
ple
on
ast
ic
it
usi
ng
rul
e-b
ase
dfi
lte
r
Fu
llp
hra
se
?
?
22
Tra
in
ya
ng
C
P
Ma
xE
nt
(M
AL
LE
T)
NP
,P
RP
,P
RP
$,
pre
-m
od
ifie
rs
an
dv
erb
s
Fu
llp
hra
se
?
?
40
Tra
in
+D
ev
ha
o
C
P
Ma
xE
nt
NP
,P
RP
,P
RP
$,
VB
D
ful
lp
hra
se
?
?
Tra
in
+D
ev
xin
xin
C
P
IL
P/I
nfo
rm
ati
on
ga
in
NP
,P
RP
,P
RP
$
Fu
llp
hra
se
?
Inf
orm
ati
on
ga
in
rat
io
65
?
zh
an
g
C
P
SV
M
IO
B
cla
ssi
fic
ati
on
Fu
llp
hra
se
?
?
?
ku
mm
erfi
eld
C
P
Un
sup
erv
ise
dg
en
era
tiv
em
od
el
NP
,P
RP
,P
RP
$w
ith
ma
xim
al
spa
n
Fu
llp
hra
se
?
?
?
zh
ek
ov
a
C
P
TI
M
BL
me
mo
ry
ba
sed
lea
rne
r
NP
,P
rop
er
no
un
s,P
RP
,P
RP
$,
plu
sv
erb
wi
th
pre
dic
ate
lem
ma
He
ad
wo
rd
?
?
Tra
in
+D
ev
irw
in
C+
O
P
Cl
ass
ific
ati
on
-ba
sed
ran
ke
r
NP
,P
RP
,P
RP
$
Sh
are
da
ttri
bu
tes
?
?
?
Ta
ble
22
:P
art
ici
pa
tin
gs
yst
em
pro
file
s?
Pa
rt
I.I
nt
he
Ta
sk
co
lum
n,
C/
O
rep
res
en
ts
wh
eth
er
the
sys
tem
pa
rtic
ipa
ted
in
the
cl
os
ed
,o
pe
n
or
bo
th
tra
ck
s.
In
the
Sy
nta
xc
olu
mn
,a
Pr
ep
res
en
ts
tha
tth
es
yst
em
su
sed
ap
hra
se
str
uc
tur
eg
ram
ma
rr
ep
res
en
tat
ion
of
syn
tax
,w
he
rea
sa
D
rep
res
en
ts
tha
tth
ey
use
da
de
pe
nd
en
cy
rep
res
en
tat
ion
.
22
Po
sit
ive
Tra
ini
ng
Ex
am
ple
s
Ne
ga
tiv
eT
rai
nin
gE
xa
mp
les
De
co
din
g
Pa
rse
Co
nfi
gu
rat
ion
lee
?
?
Mu
lti-
pa
ss
Sie
ve
s
sap
en
a
Al
lm
en
tio
np
air
sa
nd
lon
ge
ro
fn
est
ed
me
nti
on
sw
ith
co
mm
on
he
ad
ke
pt
Me
nti
on
pa
irs
wi
th
les
sth
an
thr
esh
old
(5)
nu
mb
er
of
dif
fer
en
ta
ttri
bu
te
val
ue
sa
re
co
nsi
de
red
(22
%
ou
to
f9
9%
ori
gin
al
are
dis
car
de
d)
Ite
rat
ive
1-b
est
ch
an
g
Cl
ose
sta
nte
ced
en
t
Al
lp
rec
ed
ing
me
nti
on
sin
au
nio
no
fo
fg
ol
d
an
dp
re
di
ct
ed
me
nti
on
s.
Me
nti
on
sw
he
re
the
firs
tis
pro
no
un
an
do
the
rn
ot
are
no
t
co
nsi
de
red
Be
stl
ink
an
dA
lll
ink
ss
tra
teg
y;
wi
th
an
d
wi
tho
ut
co
nst
rai
nts
?B
est
lin
kw
ith
ou
t
co
nst
rai
nts
wa
ss
ele
cte
df
or
the
offi
cia
lru
n
cai
We
igh
tsa
re
tra
ine
do
np
art
of
the
tra
ini
ng
da
ta
Re
cu
rsi
ve
2-w
ay
Sp
ect
ral
clu
ste
rin
g
(A
ga
rw
al,
20
05
)
nu
gu
es
Cl
ose
stA
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Cl
ose
st-
firs
tc
lus
ter
ing
for
pro
no
un
sa
nd
Be
st-
firs
tc
lus
ter
ing
for
no
n-p
ron
ou
ns
1-b
est
ury
up
ina
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
me
nti
on
pa
irm
od
el
wi
tho
ut
ran
kin
ga
sin
So
on
20
01
san
tos
Ex
ten
de
dv
ers
ion
of
So
on
(20
01
)w
he
re
in
ad
dit
ion
to
the
irs
tra
teg
y,
po
sit
ive
an
dn
ega
tiv
e
ex
am
ple
sf
rom
me
nti
on
sin
the
sen
ten
ce
of
the
clo
ses
tp
rec
ed
ing
an
tec
ed
en
ta
re
co
nsi
de
red
Lim
ite
dn
um
be
ro
fp
rec
ed
ing
me
nti
on
s6
0
for
au
tom
ati
ca
nd
40
giv
en
go
ld
bo
un
da
rie
s;
Ag
gre
ssi
ve
-m
erg
ec
lus
ter
ing
(M
cca
rth
ya
nd
Le
nh
ert
,1
99
5)
son
g
Pre
-cl
ust
er
pa
irm
od
els
sep
ara
te
for
eac
hp
air
NP
-N
P,
NP
-PR
Pa
nd
PR
P-P
RP
Pre
-cl
ust
ers
,w
ith
sin
gle
ton
pro
no
un
pre
-cl
ust
ers
,a
nd
use
clo
ses
t-fi
rst
clu
ste
rin
g.
Di
ffe
ren
tli
nk
mo
de
lsb
ase
do
nt
he
typ
eo
f
lin
kin
gm
en
tio
ns
?N
P-P
RP
,P
RP
-PR
Pa
nd
NP
-N
P
sto
ya
no
v
Sm
art
Pa
irG
en
era
tio
n(
Sm
art
PG
)w
he
re
the
typ
eo
fa
nte
ced
en
tis
de
ter
mi
ne
db
yt
he
typ
eo
f
an
ap
ho
ru
sin
ga
set
of
rul
es
Sin
gle
-lin
kc
lus
ter
ing
by
co
mp
uti
ng
tra
nsi
tiv
ec
los
ure
be
tw
een
pa
irw
ise
po
sit
ive
s.
sob
ha
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Pro
no
mi
na
l:a
llp
rec
ed
ing
NP
sin
the
sen
ten
ce
an
dp
rec
ed
ing
4s
en
ten
ces
kle
nn
er
?
?
Inc
rem
en
tal
en
tity
cre
ati
on
ko
bd
an
i
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
st-
firs
tc
lus
ter
ing
.T
hre
sho
ld
of
10
0w
ord
s
use
df
or
lon
gd
oc
um
en
ts
1-b
est
zh
ou
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
?
ch
art
on
Fro
m
the
en
do
fth
ed
oc
um
en
t,u
nti
la
n
an
tec
ed
en
tis
fou
nd
,o
r1
0m
en
tio
ns
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t
ML
Pw
ith
sco
re
of
0.5
use
df
or
lin
kin
ga
nd
10
me
nti
on
s
ya
ng
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Ma
xim
um
23
sen
ten
ces
to
the
lef
t;
Co
nst
rai
ne
dc
lus
ter
ing
ha
o
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
am
sea
rch
(L
uo
,2
00
4)
Pa
ck
ed
for
est
xin
xin
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
st-
firs
tc
lus
ter
ing
fol
low
ed
by
IL
P
op
tim
iza
tio
n
zh
an
g
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Wi
nd
ow
of
10
0m
ark
ab
les
ku
mm
erfi
eld
?
?
Pre
-a
nd
po
st-
res
olu
tio
nfi
lte
rs
Gi
ve
n+
Be
rke
ley
pa
rse
rp
ars
es;
pa
rse
s
wi
tho
ut
NM
Ls
im
pro
ve
dp
erf
orm
an
ce
sli
gh
tly
;re
-tr
ain
ed
Be
rke
ley
pa
rse
r
zh
ek
ov
a
Ex
am
ple
sin
the
pa
stt
hre
es
en
ten
ces
Fro
m
las
tp
oss
ibl
em
en
tio
ni
nd
oc
um
en
t
irw
in
Cl
ust
er
qu
ery
wi
th
NU
LL
clu
ste
rfo
rd
isc
ou
rse
new
me
nti
on
s
Cl
ust
er-
ran
kin
ga
pp
roa
ch
(ra
hm
an
,2
00
9)
Ta
ble
23
:P
art
ici
pa
tin
gs
yst
em
pro
file
s?
Pa
rtI
I.T
his
foc
use
so
nt
he
wa
yp
osi
tiv
ea
nd
ne
ga
tiv
ee
xa
mp
les
we
re
ge
ne
rat
ed
an
dt
he
de
co
din
gs
tra
teg
yu
sed
.
23
OntoNotes corpus, and presented the results from an
evaluation on learning such unrestricted entities and
events in text. The following represent our conclu-
sions on reviewing the results:
? Perhaps the most surprising finding was that the
best-performing system (lee) was completely
rule-based, rather than trained. This suggests
that their rule-based approach was able to do
a more effective job of combining the multiple
sources of evidence than the trained systems.
The features for coreference prediction are cer-
tainly more complex than for many other lan-
guage processing tasks, which makes it more
challenging to generate effective feature com-
binations. The rule-based approach used by
the best-performing system seemed to benefit
from a heuristic that captured the most con-
fident links before considering less confident
ones, and also made use of the information in
the guidelines in a slightly more refined man-
ner than other systems. They also included ap-
positives and copular constructions in their cal-
culations. Although OntoNotes does not count
those as instances of IDENT coreference, using
that information may have helped their system
discover additional useful links.
? It is interesting to note that the developers of
the lee system also did the experiment of run-
ning their system using gold standard informa-
tion on the individual layers, rather than auto-
matic model predictions. The somewhat sur-
prising result was that using perfect informa-
tion for the other layers did not end up improv-
ing coreference performance much, if at all. It
is not clear whether this means that: i) Auto-
matic predictors for the individual layers are
accurate enough already; ii) Information cap-
tured by those supplementary layers actually
does not provide much leverage for resolving
coreference; or iii) researchers have yet have
found an effective way of capturing and utiliz-
ing the extra information provided by these lay-
ers.
? It does seem that collecting information about
an entity by merging information across the
various attributes of the mentions that comprise
it can be useful, though not all systems that at-
tempted this achieved a benefit.
? System performance did not seem to vary as
much across the different genres as is nor-
mally the case with language processing tasks,
which could suggest that coreference is rela-
tively genre insensitive, or it is possible that
scores are two low for the difference to be ap-
parent. Comparisons are difficult, however, be-
cause the spoken genres were treated here with
perfect speech recognition accuracy and perfect
speaker turn information. Under more realis-
tic application conditions, the spread in perfor-
mance between genres might be greater.
? It is noteworthy that systems did not seem to
attempt the kind of joint inference that could
make use of the full potential of various layers
available in OntoNotes, but this could well have
been owing to the limited time available for the
shared task.
? We had expected to see more attention paid to
event coreference, which is a novel feature in
this data, but again, given the time constraints
and given that events represent only a small
portion of the total, it is not surprising that most
systems chose not to focus on it.
? Scoring coreference seems to remain a signif-
icant challenge. There does not seem to be an
objective way to establish one metric in prefer-
ence to another in the absence of a specific ap-
plication. On the other hand, the system rank-
ings do not seem terribly sensitive to the par-
ticular metric chosen. It is interesting that both
versions of the CEAF metric ? which tries to
capture the goodness of the entities in the out-
put ? seem much lower than the other metric,
though it is not clear whether that means that
our systems are doing a poor job of creating
coherent entities or whether that metric is just
especially harsh.
Finally, it is interesting to note that the problem of
coreference does not seem to be following the same
kind of learning curve that we are used to with other
problems of this sort. While performance has im-
proved somewhat, it is not clear how far we will be
able to go given the strategies at hand, or whether
new techniques will be needed to capture additional
information from the texts or from world knowl-
edge. We hope that this corpus and task will provide
a useful resource for continued experimentation to
help resolve this issue.
Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
24
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022.
We would like to thank all the participants. Without
their hard work, patience and perseverance this eval-
uation would not have been a success. We would
also like to thank the Linguistic Data Consortium
for making the OntoNotes 4.0 corpus freely and
timely available to the participants. Emili Sapena,
who graciously allowed the use of his scorer
implementation, and made available enhancements
and immediately fixed issues that were uncovered
during the evaluation. Finally, we offer our special
thanks to Llu??s Ma`rquez and Joakim Nivre for their
wonderful support and guidance without which this
task would not have been successful.
References
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Li-
bin Shen. 2006. Issues in synchronizing the English
treebank and propbank. In Workshop on Frontiers in
Linguistically Annotated Corpora 2006, July.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 33?40, Sydney,
Australia, July.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In Pro-
ceedings of the 11th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
?10, pages 28?36.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the Second Meeting of North American Chapter
of the Association of Computational Linguistics, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL), Ann
Arbor, MI, June.
Nancy Chinchor and Beth Sundheim. 2003. Message
understanding conference (MUC) 6. In LDC2003T13.
Nancy Chinchor. 2001. Message understanding confer-
ence (MUC) 7. In LDC2001T02.
Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2007. First-order probabilistic models for
coreference resolution. In HLT/NAACL, pages 81?88.
Pascal Denis and Jason Baldridge. 2007. Joint de-
termination of anaphoricity and coreference resolu-
tion using integer programming. In Proceedings of
HLT/NAACL.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
(42):87?96.
Charles Fillmore, Christopher Johnson, and Miriam R. L.
Petruck. 2003. Background to framenet. Interna-
tional Journal of Lexicography, 16(3).
G. G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassell, and R. Weischedel.
2000. The automatic content extraction (ACE)
program-tasks, data, and evaluation. In Proceedings
of LREC.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1?18, Boulder, Colorado, June.
Sanda M. Harabagiu, Razvan C. Bunescu, and Steven J.
Maiorano. 2001. Text and knowledge mining for
coreference resolution. In NAACL.
L. Hirschman and N. Chinchor. 1997. Coreference task
definition (v3.0, 13 jul 97). In Proceedings of the Sev-
enth Message Understanding Conference.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of HLT/NAACL,
pages 57?60, New York City, USA, June. Association
for Computational Linguistics.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2000. A large-scale classification of
english verbs. Language Resources and Evaluation,
42(1):21 ? 40.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
25
Methods in Natural Language Processing, pages 25?
32, Vancouver, British Columbia, Canada, October.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330, June.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems (NIPS).
Joseph McCarthy and Wendy Lehnert. 1995. Using de-
cision trees for coreference resolution. In Proceedings
of the Fourteenth International Conference on Artifi-
cial Intelligence, pages 1050?1055.
Thomas S. Morton. 2000. Coreference for nlp applica-
tions. In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics, Octo-
ber.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the IJCAI.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
R. Passonneau. 2004. Computing reliability for corefer-
ence annotation. In Proceedings of LREC.
Massimo Poesio and Ron Artstein. 2005. The reliability
of anaphoric annotation, reconsidered: Taking ambi-
guity into account. In Proceedings of the Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.
Massimo Poesio. 2004. The mate/gnome scheme for
anaphoric annotation, revisited. In Proceedings of
SIGDIAL.
Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art nlp approaches to coreference resolu-
tion: Theory and practical recipes. In Tutorial Ab-
stracts of ACL-IJCNLP 2009, page 6, Suntec, Singa-
pore, August.
Simone Paolo Ponzetto and Michael Strube. 2005. Se-
mantic role labeling for coreference resolution. In
Companion Volume of the Proceedings of the 11th
Meeting of the European Chapter of the Associa-
tion for Computational Linguistics, pages 143?146,
Trento, Italy, April.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the HLT/NAACL, pages 192?199, New York City,
N.Y., June.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal, 60(1):11?39.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007a. OntoNotes: A Unified Relational Semantic
Representation. International Journal of Semantic
Computing, 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007b.
Unrestricted Coreference: Indentifying Entities and
Events in OntoNotes. In in Proceedings of the
IEEE International Conference on Semantic Comput-
ing (ICSC), September 17-19.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977, Singapore, Au-
gust. Association for Computational Linguistics.
W. M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66(336).
Marta Recasens and Eduard Hovy. 2011. Blanc: Im-
plementing the rand index for coreference evaluation.
Natural Language Engineering.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 1?8,
Uppsala, Sweden, July.
W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing approach to coreference resolution of noun phrase.
Computational Linguistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
26
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August.
Yannick Versley. 2007. Antecedent selection techniques
for high-recall coreference resolution. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model theoretic coreference
scoring scheme. In Proceedings of the Sixth Message
Undersatnding Conference (MUC-6), pages 45?52.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog
no.: LDC2005T33. BBN Technologies.
Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue. 2011. OntoNotes: A
Large Training Corpus for Enhanced Processing. In
Joseph Olive, Caitlin Christianson, and John McCary,
editors, Handbook of Natural Language Processing
and Machine Translation. Springer.
27
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1?40,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
CoNLL-2012 Shared Task:
Modeling Multilingual Unrestricted Coreference in OntoNotes
Sameer Pradhan
Raytheon BBN Technologies,
Cambridge, MA 02138
USA
pradhan@bbn.com
Alessandro Moschitti
University of Trento,
38123 Povo (TN)
Italy
moschitti@disi.unitn.it
Nianwen Xue
Brandeis University,
Waltham, MA 02453
USA
xuen@cs.brandeis.edu
Olga Uryupina
University of Trento,
38123 Povo (TN)
Italy
uryupina@gmail.com
Yuchen Zhang
Brandeis University,
Waltham, MA 02453
USA
yuchenz@brandeis.edu
Abstract
The CoNLL-2012 shared task involved pre-
dicting coreference in English, Chinese, and
Arabic, using the final version, v5.0, of the
OntoNotes corpus. It was a follow-on to the
English-only task organized in 2011. Un-
til the creation of the OntoNotes corpus, re-
sources in this sub-field of language process-
ing were limited to noun phrase coreference,
often on a restricted set of entities, such as
the ACE entities. OntoNotes provides a large-
scale corpus of general anaphoric coreference
not restricted to noun phrases or to a spec-
ified set of entity types, and covers multi-
ple languages. OntoNotes also provides ad-
ditional layers of integrated annotation, cap-
turing additional shallow semantic structure.
This paper describes the OntoNotes annota-
tion (coreference and other layers) and then
describes the parameters of the shared task in-
cluding the format, pre-processing informa-
tion, evaluation criteria, and presents and dis-
cusses the results achieved by the participat-
ing systems. The task of coreference has
had a complex evaluation history. Potentially
many evaluation conditions, have, in the past,
made it difficult to judge the improvement in
new algorithms over previously reported re-
sults. Having a standard test set and stan-
dard evaluation parameters, all based on a re-
source that provides multiple integrated anno-
tation layers (syntactic parses, semantic roles,
word senses, named entities and coreference)
and in multiple languages could support joint
modeling and help ground and energize on-
going research in the task of entity and event
coreference.
1 Introduction
The importance of coreference resolution for the
entity/event detection task, namely identifying all
mentions of entities and events in text and clustering
them into equivalence classes, has been well recog-
nized in the natural language processing community.
Early work on corpus-based coreference resolu-
tion dates back to the mid-90s by McCarthy and
Lenhert (1995) where they experimented with deci-
sion trees and hand-written rules. Corpora to support
supervised learning of this task date back to the Mes-
sage Understanding Conferences (MUC) (Hirschman
and Chinchor, 1997; Chinchor, 2001; Chinchor and
Sundheim, 2003). The de facto standard datasets
for current coreference studies are the MUC and the
ACE1 (Doddington et al, 2004) corpora. These cor-
pora were tagged with coreferring entities in the
form of noun phrases in the text. The MUC corpora
cover all noun phrases in text but are relatively small
in size. The ACE corpora, on the other hand, cover
much more data, but the annotation is restricted to a
small subset of entities.
Automatic identification of coreferring entities
and events in text has been an uphill battle for sev-
eral decades, partly because it is a problem that re-
quires world knowledge to solve and word knowl-
edge is hard to define, and partly owing to the lack
of substantial annotated data. Aside from the fact
that resolving coreference in text is simply a very
hard problem, there have been other hindrances that
further contributed to the slow progress in this area:
(i) Smaller sized corpora such as MUC which cov-
ered coreference across all noun phrases. Cor-
pora such as ACE which are larger in size, but
cover a smaller set of entities; and
(ii) low consistency in existing corpora annotated
with coreference ? in terms of inter-annotator
agreement (ITA) (Hirschman et al, 1998) ?
owing to attempts at covering multiple coref-
erence phenomena that are not equally anno-
tatable with high agreement which likely less-
ened the reliability of statistical evidence in the
form of lexical coverage and semantic related-
ness that could be derived from the data and
1http://projects.ldc.upenn.edu/ace/data/
1
used by a classifier to generate better predic-
tive models. The importance of a well-defined
tagging scheme and consistent ITA has been
well recognized and studied in the past (Poe-
sio, 2004; Poesio and Artstein, 2005; Passon-
neau, 2004). There is a growing consensus that
in order to take language understanding appli-
cations such as question answering or distilla-
tion to the next level, we need more consistent
annotation for larger amounts of broad cover-
age data to train better automatic models for
entity and event detection.
(iii) Complex evaluation with multiple evaluation
metrics and multiple evaluation scenarios,
complicated with varying training and test
partitions, led to situations where many re-
searchers report results with only one or a few
of the available metrics and under a subset of
evaluation scenarios. This has made it hard to
gauge the improvement in algorithms over the
years (Stoyanov et al, 2009), or to determine
which particular areas require further attention.
Looking at various numbers reported in litera-
ture can greatly affect the perceived difficulty
of the task. It can seem to be a very hard prob-
lem (Soon et al, 2001) or one that is relatively
easy (Culotta et al, 2007).
(iv) the knowledge bottleneck which has been a
well-accepted ceiling that has kept the progress
in this task at bay.
These issues suggest that the following steps
might take the community in the right direction to-
wards improving the state of the art in coreference
resolution:
(i) Create a large corpus with high inter-
annotator agreement possibly by restricting
the coreference annotating to phenomena that
can be annotated with high consistency, and
covering an unrestricted set of entities and
events; and
(ii) Create a standard evaluation scenario with an
official evaluation setup, and possibly several
ablation settings to capture the range of perfor-
mance. This can then be used as a standard
benchmark by the research community.
(iii) Continue to improve learning algorithms that
better incorporate world knowledge and jointly
incorporate information from other layers of
syntactic and semantic annotation to improve
the state of the art.
One of the many goals of the OntoNotes
project2 (Hovy et al, 2006; Weischedel et al, 2011)
2http://www.bbn.com/nlp/ontonotes
was to explore whether it could fill this void and help
push the progress further ? not only in coreference,
but with the various layers of semantics that it tries
to capture. As one of its layers, it has created a
corpus for general anaphoric coreference that cov-
ers entities and events not limited to noun phrases
or a subset of entity types. The coreference layer
in OntoNotes constitutes just one part of a multi-
layered, integrated annotation of shallow semantic
structures in text with high inter-annotator agree-
ment. This addresses the first issue.
In the language processing community, the field
of speech recognition probably has the longest his-
tory of shared evaluations held primary by NIST3
(Pallett, 2002). In the past decade machine trans-
lation has been a topic of shared evaluations also
by NIST4. There are many syntactic and semantic
processing tasks that are not quite amenable to such
continued evaluation efforts. The CoNLL shared
tasks over the past 15 years have filled that gap, help-
ing establish benchmarks and advance the state of
the art in various sub-fields within NLP. The impor-
tance of shared tasks is now in full display in the
domain of clinical NLP (Chapman et al, 2011) and
recently a coreference task was organized as part
of the i2b2 workshop (Uzuner et al, 2012). The
computational learning community is also witness-
ing a shift towards joint inference based evaluations,
with the two previous CoNLL tasks (Surdeanu et al,
2008; Hajic? et al, 2009) devoted to joint learning of
syntactic and semantic dependencies. A SemEval-
2010 coreference task (Recasens et al, 2010) was
the first attempt to address the second issue. It
included six different Indo-European languages ?
Catalan, Dutch, English, German, Italian, and Span-
ish. Among other corpora, a small subset (?120K)
of English portion of OntoNotes was used for this
purpose. However, the lack of a strong participa-
tion prevented the organizers from reaching any firm
conclusions. The CoNLL-2011 shared task was an-
other attempt to address the second issue. It was well
received, but the shared task was only limited to the
English portion of OntoNotes. In addition, the coref-
erence portion of OntoNotes did not have a concrete
baseline prior to the 2011 evaluation, thereby mak-
ing it challenging for participants to gauge the per-
formance of their algorithms in the absence of es-
tablished state of the art on this flavor of annotation.
The closest comparison was to the results reported
by Pradhan et al (2007b) on the newswire portion of
OntoNotes. Since the corpus also covers two other
languages from completely different language fami-
lies, Chinese and Arabic, it provided a great oppor-
tunity to have a follow-on task in 2012 covering all
3http://www.itl.nist.gov/iad/mig/publications/ASRhistory/index.html
4http://www.itl.nist.gov/iad/mig/tests/mt/
2
three languages. As we will see later, peculiarities
of each of these languages had to be considered in
creating the evaluation framework.
The first systematic learning-based study in coref-
erence resolution was conducted on the MUC cor-
pora, using a decision tree learner, by Soon et al
(2001). Significant improvements have been made
in the field of language processing in general, and
improved learning techniques have pushed the state
of the art in coreference resolution forward (Mor-
ton, 2000; Harabagiu et al, 2001; McCallum and
Wellner, 2004; Culotta et al, 2007; Denis and
Baldridge, 2007; Rahman and Ng, 2009; Haghighi
and Klein, 2010). Researchers have continued to
find novel ways of exploiting ontologies such as
WordNet. Various knowledge sources from shallow
semantics to encyclopedic knowledge have been ex-
ploited (Ponzetto and Strube, 2005; Ponzetto and
Strube, 2006; Versley, 2007; Ng, 2007). Given
that WordNet is a static ontology and as such has
limitation on coverage, more recently, there have
been successful attempts to utilize information from
much larger, collaboratively built resources such as
Wikipedia (Ponzetto and Strube, 2006). More re-
cently researchers have used graph based algorithms
(Cai et al, 2011a) rather than pair-wise classifica-
tions. For a detailed survey of the progress in this
field, we refer the reader to a recent article (Ng,
2010) and a tutorial (Ponzetto and Poesio, 2009)
dedicated to this subject. In spite of all the progress,
current techniques still rely primarily on surface
level features such as string match, proximity, and
edit distance; syntactic features such as apposition;
and shallow semantic features such as number, gen-
der, named entities, semantic class, Hobbs? distance,
etc. Further research to reduce the knowledge gap is
essential to take coreference resolution techniques to
the next level.
The rest of the paper is organized as follows: Sec-
tion 2 presents an overview of the OntoNotes cor-
pus. Section 3 describes the range of phenomena
annotated in OntoNotes, and language-specific is-
sues. Section 4 describes the shared task data and
the evaluation parameters, with Section 4.4.2 exam-
ining the performance of the state-of-the-art tools
on all/most intermediate layers of annotation. Sec-
tion 5 describes the participants in the task. Sec-
tion 6 briefly compares the approaches taken by var-
ious participating systems. Section 7 presents the
system results with some analysis. Section 8 com-
pares the performance of the systems on the a subset
of the Engish test set that corresponds with the test
set used for the CoNLL-2011 evaluation. Section 9
draws some conclusions.
2 The OntoNotes Corpus
The OntoNotes project has created a large-scale
corpus of accurate and integrated annotation of mul-
tiple levels of the shallow semantic structure in text.
The English and Chinese language portion com-
prises roughly one million words per language of
newswire, magazine articles, broadcast news, broad-
cast conversations, web data and conversational
speech data. The English subcorpus also contains
an additional 200K words of the English translation
of the New Testament as Pivot Text. The Arabic por-
tion is smaller, comprising 300K words of newswire
articles. The hope is that this rich, integrated an-
notation covering many layers will allow for richer,
cross-layer models and enable significantly better
automatic semantic analysis. In addition to coref-
erence, this data is also tagged with syntactic trees,
propositions for most verb and some noun instances,
partial verb and noun word senses, and 18 named en-
tity types. Manual annotation of a large corpus with
multiple layers of syntax and semantic information
is a costly endeavor. Over the years in the devel-
opment of this corpus, there were various priorities
that came into play, and therefore not all the data in
the corpus could be annotated with all the different
layers of annotation. However, such multi-layer an-
notations, with complex, cross-layer dependencies,
demands a robust, efficient, scalable storage mech-
anism while providing efficient, convenient, inte-
grated access to the the underlying structure. To
this effect, it uses a relational database representa-
tion that captures both the inter- and intra-layer de-
pendencies and also provides an object-oriented API
for efficient, multi-tiered access to this data (Prad-
han et al, 2007a). This facilitates the extraction of
cross-layer features in integrated predictive models
that will make use of these annotations.
OntoNotes comprises the following layers of an-
notation:
? Syntax ? A layer of syntactic annotation for
English, Chinese and Arabic based on a revised
guidelines for the Penn Treebank (Marcus et
al., 1993; Babko-Malaya et al, 2006), the Chi-
nese Treebank (Xue et al, 2005) and the Arabic
Treebank (Maamouri and Bies, 2004).
? Propositions ? The proposition structure of
verbs based on revised guidelines for the En-
glish PropBank (Palmer et al, 2005; Babko-
Malaya et al, 2006), the Chinese PropBank
(Xue and Palmer, 2009) and the Arabic Prop-
Bank (Palmer et al, 2008; Zaghouani et al,
2010).
? Word Sense ? Coarse-grained word senses
are tagged for the most frequent polysemous
verbs and nouns, in order to maximize token
3
coverage. The word sense granularity is tai-
lored to achieve 90% inter-annotator agreement
as demonstrated by Palmer et al (2007). These
senses are defined in the sense inventory files.
In case of English and Arabic languages, the
sense-inventories (and frame files) are defined
separately for each part of speech that is real-
ized by the lemma in the text. For Chinese,
however the sense inventories (and frame files)
are defined per lemma ? independent of the
part of speech realized in the text. For the
English portion of OntoNotes, each individual
sense has been connected to multiple WordNet
senses. This provides users direct access to the
WordNet semantic structure. There is also a
mapping from the OntoNotes word senses to
PropBank frames and to VerbNet (Kipper et
al., 2000) and FrameNet (Fillmore et al, 2003).
Unfortunately, owing to lack of comparable re-
sources as comprehensive as WordNet in Chi-
nese or Arabic, neither language has any inter-
resource mappings available.
? Named Entities ? The corpus was tagged
with a set of 18 well-defined proper named en-
tity types that have been tested extensively for
inter-annotator agreement by Weischedel and
Burnstein (2005).
? Coreference ? This layer captures general
anaphoric coreference that covers entities and
events not limited to noun phrases or a lim-
ited set of entity types (Pradhan et al, 2007b).
It considers all pronouns (PRP, PRP$), noun
phrases (NP) and heads of verb phrases (VP)
as potential mentions. Unlike English, Chinese
and Arabic have dropped subjects and objects
which were also considered during coreference
annotation5. We will take a look at this in detail
in the next section.
3 Coreference in OntoNotes
General anaphoric coreference that spans a rich
set of entities and events ? not restricted to a few
types, as has been characteristic of most coreference
data available until now ? has been tagged with a
high degree of consistency in the OntoNotes corpus.
Two different types of coreference are distinguished:
Identity (IDENT), and Appositive (APPOS). Identity
coreference (IDENT) is used for anaphoric corefer-
ence, meaning links between pronominal, nominal,
and named mentions of specific referents. It does not
include mentions of generic, underspecified, or ab-
stract entities. Appositives (APPOS) are treated sep-
arately because they function as attributions, as de-
scribed further below. Coreference is annotated for
all specific entities and events. There is no limit on
5As we will see later these are not used during the task.
the semantic types of NP entities that can be consid-
ered for coreference, and in particular, coreference
is not limited to ACE types. The guidelines are fairly
language independent. We will look at some salient
aspects of the coreference annotation in OntoNotes.
For more details, and examples, we refer the reader
to the release documentation. We will primarily use
English examples to describe various aspects of the
annotation and use Chinese and Arabic examples es-
pecially to illustrate phenomena not observed in En-
glish, or that have some language specific peculiari-
ties.
3.1 Noun Phrases
The mentions over which IDENT coreference ap-
plies are typically pronominal, named, or definite
nominal. The annotation process begins by automat-
ically extracting all of the NP mentions from parse
trees in the syntactic layer of OntoNotes annotation,
though the annotators can also add additional men-
tions when appropriate. In the following two exam-
ples (and later ones), the phrases in bold form the
links of an IDENT chain.
(1) She had a good suggestion and it was unani-
mously accepted by all.
(2) Elco Industries Inc. said it expects net income
in the year ending June 30, 1990, to fall below a
recent analyst?s estimate of $ 1.65 a share. The
Rockford, Ill. maker of fasteners also said it
expects to post sales in the current fiscal year
that are ?slightly above? fiscal 1989 sales of $
155 million.
Noun phrases (NPs) in Chinese can be complex
noun phrases or bare nouns (nouns that lack a de-
terminer such as ?the? or ?this?). Complex noun
phrases contain structures modifying the head noun,
as in the following examples:
(3) (??????????? ? (???
????? (???))).
((His last APEC (summit meeting)) as the
President)
(4) (?? ?? ? (?? ? ?? ?? ?? ?
(????)))
((The first (U.S. president)) who went to visit
Vietnam after its unification)
In these examples, the smallest phrase in paren-
theses is the bare noun. The longer phrase in paran-
theses includes modifying structures. All the expres-
sions in the parantheses, however, share the same
head noun, i.e., ???? (summit meeting)?, and
????? (U.S. president)? respectively. Nested
noun phrases, or nested NPs, are contained within
4
longer noun phrases. In the above example, ?sum-
mit meeting? and ?U.S. president? are nested NPs.
Wherever NPs are nested, the largest logical span is
used in coreference.
3.2 Verbs
Verbs are added as single-word spans if they can
be coreferenced with a noun phrase or with another
verb. The intent is to annotate the VP, but the single-
word verb head is marked for convenience. This
includes morphologically related nominalizations as
in (5) and noun phrases that refer to the same event,
even if they are lexically distinct from the verb as in
(6). In the following two examples, only the chains
related to the growth event are shown in bold. The
Arabic translation of the same example identifies
mentions using parantheses.
(5) The European economy grew rapidly over the
past years, this growth helped raising ....
H@?
	
J??@ ?C
	
g

??Q??. ?

G
.
?P?

B@ XA?

J

?B

@ ( A? 	? ) Y??
. . . ?
	
P? ?


	
? ??A? ( ??	J? @ @ 	Y? ) , ?J

	
?A?? @
(6) Japan?s domestic sales of cars, trucks and buses
in October rose 18% from a year earlier to
500,004 units, a record for the month, the Japan
Automobile Dealers? Association said. The
strong growth followed year-to-year increases
of 21% in August and 12% in September.
3.3 Pronouns
All pronouns and demonstratives are linked to
anything that they refer to, and pronouns in quoted
speech are also marked. Expletive or pleonastic pro-
nouns (it, there) are not considered for tagging, and
generic you is not marked. In the following exam-
ple, the pronoun you and it would not be marked. (In
this and following examples, an asterisk (*) before a
boldface phrase identifies entity/event mentions that
would not be tagged in the coreference annotation.)
(7) Senate majority leader Bill Frist likes to tell
a story from his days as a pioneering heart
surgeon back in Tennessee. A lot of times,
Frist recalls, *you?d have a critical patient ly-
ing there waiting for a new heart, and *you?d
want to cut, but *you couldn?t start unless *you
knew that the replacement heart would make
*it to the operating room.
In Chinese, all the following pronouns ? ??
???, ?, ?? ????????????
?, ?, ?? (you, me, he, she, and so on), and
demonstrative pronouns ?????????,?
? (this, that, these, those) in singular, plural or pos-
sessive forms are linked to anything they refer to.
Pronouns from classical Chinese such as ? ?
(among which),? (he/she/it),? (he/she/it) are also
linked with other mentions to which they refer.
In Arabic, the following pronouns are corefer-
enced ? nominative personal pronouns (subject) and
demonstrative pronouns which are detached. Sub-
ject pronouns are often null in Arabic; overt subject
pronouns are rare, but do occur.
	?
	
K @ / ?

?
	
K @ / A?

J
	
K @ / 	?mProceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 18?26,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discovering Narrative Containers in Clinical Text
Timothy A. Miller1, Steven Bethard2, Dmitriy Dligach1,
Sameer Pradhan1, Chen Lin1, and Guergana K. Savova1
1 Children?s Hospital Informatics Program, Boston Children?s Hospital and Harvard Medical School
firstname.lastname@childrens.harvard.edu
2 Center for Computational Language and Education Research, University of Colorado Boulder
steven.bethard@colorado.edu
Abstract
The clinical narrative contains a great deal
of valuable information that is only under-
standable in a temporal context. Events,
time expressions, and temporal relations
convey information about the time course
of a patient?s clinical record that must be
understood for many applications of inter-
est. In this paper, we focus on extracting
information about how time expressions
and events are related by narrative con-
tainers. We use support vector machines
with composite kernels, which allows for
integrating standard feature kernels with
tree kernels for representing structured
features such as constituency trees. Our
experiments show that using tree kernels
in addition to standard feature kernels im-
proves F1 classification for this task.
1 Introduction
Clinical narratives are a rich source of unstruc-
tured information that hold great potential for im-
pacting clinical research and clinical care. These
narratives consist of unstructured natural language
descriptions of various stages of clinical care,
which makes them information dense but chal-
lenging to use computationally. Information ex-
tracted from these narratives is already being used
for clinical research tasks such as automatic phe-
notype classification for collecting disease cohorts
retrospectively (Ananthakrishnan et al, 2013),
which can in turn be used for a variety of studies,
including pharmacogenomics (Lin et al, 2012;
Wilke et al, 2011). Future applications may use
information extracted from the clinical narrative at
the point of care to assist physicians in decision-
making in a real time fashion.
One of the most interesting and challenging as-
pects of clinical text is the pervasiveness of tempo-
rally grounded information. This includes a num-
ber of clinical concepts which are events with fi-
nite time spans (e.g., surgery or x-ray), time ex-
pressions (December, postoperatively), and links
that relate events to times or other events. For ex-
ample, surgery last May relates the time last May
with the event surgery via the CONTAINS relation,
while Vicodin after surgery relates the medication
event Vicodin with the procedure event surgery via
the AFTER relation. There are many potential ap-
plications of clinical information extraction that
are only possible with an understanding of the or-
dering and duration of the events in a clinical en-
counter.
In this work we focus on extracting a particu-
lar temporal relation, CONTAINS, that holds be-
tween a time expression and an event expression.
This level of representation is based on the compu-
tational discourse model of narrative containers
(Pustejovsky and Stubbs, 2011), which are time
expressions or events which are central to a sec-
tion of a text, usually manifested by being rela-
tive hubs of temporal relation links. We argue that
containment relations are useful as an intermediate
level of granularity between full temporal relation
extraction and ?coarse? temporal bins (Raghavan
et al, 2012) like before admission, on admission,
and after admission. Correctly extracting CON-
TAINS relations will, for example, allow for more
accurate placement of events on a timeline, to
the resolution possible by the number of time ex-
pressions in the document. We suspect that this
finer grained information will also be more useful
for downstream applications like coreference, for
which coarse information was found to be useful.
The approach we develop is a supervised machine
18
learning approach in which pairs of time expres-
sions and events are classified as CONTAINS or
not. The specific approach is a support vector ma-
chine using both standard feature kernels and tree
kernels, a novel approach to this problem in this
domain that has shown promise on other relation
extraction tasks.
This work makes use of a new corpus we devel-
oped as part of the THYME1 project (Temporal
History of Your Medical Events) focusing on tem-
poral events and relations in clinical text. This cor-
pus consists of clinical and pathology notes on col-
orectal cancer from Mayo Clinic. Gold standard
annotations include Penn Treebank-style phrase
structure in addition to clinically relevant temporal
annotations like clinical events, temporal expres-
sions, and various temporal relations.
2 Background and Related Work
2.1 Annotation Methodology
The THYME annotation guidelines2 detail the ex-
tension of TimeML (Pustejovsky et al, 2003b)
to the annotations of events, temporal expres-
sions and temporal relations in the clinical do-
main. In summary, an EVENT is anything that is
relevant to the clinical timeline. Temporal expres-
sions (TIMEX3s) in the clinical domain are simi-
lar to those in the general domain with two excep-
tions. First, TimeML sets and frequencies occur
much more often in the clinical domain, especially
with regard to medications and treatments (Clar-
itin 30mg twice daily). The second deviation is a
new type of TIMEX3 ? PREPOSTEXP which covers
temporally complex terms like preoperative, post-
operative, and intraoperative.
EVENTs and TIMEX3s are ordered on a timeline
through temporal TLINKs which range from fairly
coarse (the relation to document time creation) to
fairly granular (the explicit pairwise TLINKs be-
tween EVENTs and/or TIMEX3s). Of note for this
work, the CONTAINS relation between a TIMEX3
and an EVENT means that the span of the EVENT
is completely within the span of the TIMEX3. The
interannotator agreement F1-score for CONTAINS
for the set of documents used here was 0.60.
2.2 Narrative Containers
One relatively new concept for marking temporal
relations is that of narrative containers, as in Puste-
1http://clear.colorado.edu/TemporalWiki
2Annotation guidelines are posted on the THYME wiki.
jovsky and Stubbs (2011). Narrative containers
are time spans which are central to the discourse
and often subsume multiple events and time ex-
pressions. They are often anchored by a time ex-
pression, though more abstract events may also act
as anchors. Using the narrative container frame-
work significantly reduces the number of explicit
TLINK annotations yet retains a relevant degree of
granularity enabling inferencing.
Consider the following clinical text example
with DocTime of February 8.
The patient recovered well after her ini-
tial first surgery on December 16th to
remove the adenocarcinoma, although
on the evening of January 3rd she was
admitted with a fever and treated with
antibiotics.
There are three narrative containers in this snip-
pet ? (1) the broad period leading up to the docu-
ment creation time which includes the events of re-
covered and adenocarcinoma, (2) December 16th,
which includes the events of surgery and remove,
and (3) January 3rd, which includes the events of
admitted, fever, and treated.
Using only the relation to the document creation
time would provide too coarse of a timeline result-
ing in collapsing the three narrative containers (the
coarse time bins of Raghavan et al (2012) would
collapse all events into the before admission cat-
egory). On the other hand, marking explicit links
between every pair of events and temporal expres-
sions would be tedious and redundant. In this ex-
ample, there is no need to explicitly mark that, for
instance, fever was AFTER surgery, because we
know that the fever happened on January 3rd and
that the surgery happened on December 16th, and
that January 3rd is AFTER December 16th. With
the grouping of EVENTs in this way, we can infer
the links between them and reduce annotator ef-
fort. Narrative containers strike the right balance
between parsimony and expressiveness.
2.3 Related Work
Of course, the possibility of annotating temporal
containment relations was allowed by even the ear-
liest versions of the TimeML specification using
TLINKs with the relation type INCLUDES. How-
ever, TimeML is a specification not a guideline,
and as such, the way in which temporal relations
have been annotated has varied widely and no
19
corpus has previously been annotated with narra-
tive containers in mind. In the TimeBank corpus
(Pustejovsky et al, 2003a), annotators annotated
only a sparse, mostly disconnected graph of the
temporal relations that seemed salient to them. In
TempEval 2007 and 2010 (Verhagen et al, 2007;
Verhagen et al, 2010), annotators annotated only
relations in specific constructions ? e.g. all pairs
of events and times in a sentence ? and used a re-
stricted set of relation types that excluded the IN-
CLUDES relation. TempEval 2013 (UzZaman et
al., 2013) allowed INCLUDES relations, but again
only in particular constructions or when the rela-
tion seemed salient to the annotators. The 2012
i2b2 Challenge3, which provided TimeML anno-
tations on clinical data, annotated the INCLUDES
relation, but merged it with other relations for the
evaluation due to low inter-annotator agreement.
Since no narrative container-annotated corpora
exist, there are also no existing models for extract-
ing narrative container relations. However, we
can draw on the various methods applied to re-
lated temporal relation tasks. Most relevant is the
work on linking events to timestamps. This was
one of the subtasks in TempEval 2007 and 2010,
and systems used a variety of features including
words, part-of-speech tags, and the syntactic path
between the event and the time (Bethard and Mar-
tin, 2007; Llorens et al, 2010). Syntactic path
features were also used in the 2012 i2b2 Chal-
lenge, where they provided gains especially for
intra-sentential temporal links (Xu et al, 2013).
Recent research has also looked to syntac-
tic tree kernels for temporal relation extraction.
Mirroshandel et al (2009) used a path-enclosed
tree (i.e., selecting only the sub-tree containing
the event and time), and used various weighting
scheme variants of this approach on the Time-
Bank (Pustejovsky et al, 2003a) and Opinion4
corpora. Hovy et al (2012) used a flat tree struc-
ture for each event-time pair, including only token-
based information (words, part of speech tags) be-
tween the event and time, and found that adding
such tree kernels on top of a baseline set of fea-
tures improved event-time linking performance on
the TempEval 2007 and Machine Reading cor-
pora (Strassel et al, 2010). While Mirroshandel et
al. saw improvements using a representation with
syntactic structure, Hovy et al used the flat tree
3http://i2b2.org/NLP/TemporalRelations
4Also known as the AQUAINT TimeML corpus ?
http://www.timeml.org
structure because they found that ?using a full-
parse syntactic tree as input representation did not
help performance.? Thus, it remains an open ques-
tion exactly where and when syntactic tree kernels
will help temporal relation extraction.
3 Methods
Inspired by this prior work, we treat the narrative
container extraction task as a within-sentence rela-
tion extraction task between time and event men-
tions. For each sentence, this approach iterates
over every gold standard annotated EVENT, pair-
ing it with each TIMEX3 in the sentence, and uses
a supervised machine learning algorithm to clas-
sify each pair as related by the CONTAINS relation
or not. Training examples are generated in the
same way, with pairs corresponding to annotated
links marked as positive examples and all others
marked as negative. We investigate a variety of
features for the classifier as well as a variety of
tree kernel combinations.
This straightforward approach does not address
all relation pairs, setting aside event-event rela-
tions and inter-sentential relations, which are both
likely to require different approaches.
3.1 SVM with Tree Kernels
The machine learning approach we use is support
vector machine (SVM) with standard feature ker-
nels, tree kernels, and composite kernels that com-
bine the two. SVMs are used extensively for clas-
sification tasks in natural language processing, due
to robust performance and widely available soft-
ware packages. We take advantage of the ability
in SVMs to represent structured features such as
trees using convolution kernels (Collins and Duffy,
2001), also known as tree kernels. This kernel
computes similarity between two tree structures
by computing the number of common sub-trees,
with a weight parameter to discount the influence
of larger structural similarities. The specific for-
malism we use is sometimes called a subset tree
kernel (Moschitti, 2006), which checks for simi-
larity on subtrees of all sizes, as long as each sub-
tree has its production rule completely expanded.
A useful property of kernels is that a linear com-
bination of two kernels is guaranteed to be a ker-
nel (Cristianini and Shawe-Taylor, 2000). In ad-
dition, the product of two kernels is also a ker-
nel. This means that it is simple to combine tradi-
tional feature-based kernels used in SVMs (linear,
20
polynomial, radial basis function) with tree ker-
nels representing structural information. This ap-
proach of using composite kernels has been widely
used in the task of relation extraction where syn-
tactic information is presumed to be useful, but is
hard to represent as traditional numeric features.
We investigate a few different composite ker-
nels here, including a linear combination:
KC(o1, o2) = ? ?KT (t1, t2) +KF (f1, f2) (1)
where a composite kernel KC operates on objects
oj composed of features fj and tree tj , by adding
a tree kernel KT weighted by ? to a feature kernel
KF . We also use a composite kernel that takes the
product of kernels:
KC(o1, o2) = KT (t1, t2) ?KF (f1, f2) (2)
Sometimes it is beneficial to make use of multi-
ple syntactic ?views? of the same instance. Below
we will describe many different tree representa-
tions, and the tree kernel framework allows them
to all be used simultaneously, by simply summing
the similarities of the different representations and
taking the combined sum as the tree kernel value:
KT ({t
1
1, t
2
1 . . . , t
N
1 }, {t
1
2, t
2
2, . . . , t
N
2 }) =
N?
i=1
KT (t
i
1, t
i
2) (3)
where i indexes the N different tree views. In all
kernel combinations we compute the normalized
version of both the feature and tree kernels so that
they can be combined on an even footing.
The actual implementations we use for train-
ing are the SVM-LIGHT-TK package (Mos-
chitti, 2006), which is a tree kernel extension to
SVMlight (Joachims, 1999). At test time, we
use the SVM-LIGHT-TK bindings of the ClearTK
toolkit (Ogren et al, 2009) in a module built on
top of Apache cTAKES (Savova et al, 2010), to
take advantage of the pre-processing stages.
3.2 Flat Features
The flat features developed for the standard fea-
ture kernel include the text of each argument as a
whole, the tokens of each argument represented as
a bag of words, the first and last word of each ar-
gument, and the preceding and following words of
each argument as bags of words. The token con-
text between arguments is also represented using
the text span as a whole, the first and last words,
the set of words represented as a bag of words, and
the distance between the arguments. In addition,
part of speech (POS) tag features are extracted for
each mention, with separate bag of POS tag fea-
tures for each argument. The POS features are
generated by the cTAKES POS tagger.
We also include semantic features of each argu-
ment. For event mentions, we include a feature
marking the contextual modality, which can take
on the possible values Actual, Hedged, Hypothet-
ical, or Generic, which is part of the gold stan-
dard annotations. This feature was included as it
was presumed that actual events are more likely
to have definite time spans, and thus be related
to times, than hypothetical or generic mentions of
events. For time mentions we include a feature for
the time class, with possible values of Date, Time,
Duration, Quantifier, Set, or Prepostexp. The time
class feature was used as it was hypothesized that
dates and times are more likely to contain events
than sets (e.g., once a month).
3.3 Tree Kernel Representations
We leverage existing tree kernel representations
for this work, using some directly and others as
starting point to a domain-specific representation.
First, we take advantage of the (relatively) flat
structured tree kernel representations of Hovy et
al. (2012). This representation uses lexical items
such as POS tags rather than constituent struc-
ture, but places them into an ordered tree struc-
ture, which allows tree kernels to use them as a
bag of items while also taking advantage of order-
ing structure when it is useful. Figure 1 shows an
example tree for an event-time pair for which a re-
lation exists, where the lexical information used is
POS tag information for each term (the represen-
tation that Hovy et al found most useful). We also
used a version of this representation where the sur-
face form is used instead of the POS tag.
While Hovy et al showed positive results using
this representation over just standard features, it is
still somewhat constrained in its ability to repre-
sent long distance relations. This is because the
subset tree kernel compares only complete rule
productions, and with long distance relations a flat
tree structure will have a production that is too big
to learn. Alternatively, tree kernel representations
can be based on constituent structure, as is com-
mon in the relation extraction literature. This will
21
BOP
Event-Actual
TOK
VBN
TOK
TO
TOK
VB
TOK
NN
Timex-Date
TOK
JJ
TOK
NN
BOW
Event-Actual
TOK
scheduled
TOK
to
TOK
undergo
TOK
surgery
Timex-Date
TOK
next
TOK
week
Figure 1: Two trees indicating the flat tree kernel
representation. Above is the bag of POS tags ver-
sion; below is the bag of words version.
hopefully allow for the representation of longer
distance relations by taking advantage of syntactic
sub-structure with smaller productions. The rep-
resentations used here are known as Feature Trees
(FT), Path Trees (PT) and Path-Enclosed Trees
(PET).
The Feature Tree representation takes the en-
tire syntax tree for the sentence containing both
arguments and inserts semantic information about
those arguments. That information includes the ar-
gument type (EVENT or TIMEX) as an additional
tree node above the constituent enclosing the argu-
ment. We also append semantic class information
to the argument (contextual modality for events,
time class for times), as in the flat features.
The Feature Tree representation is not com-
monly used, as it includes an entire sentence
around the arguments of interest, and that may in-
clude a great deal of unrelated structure that adds
noise to the classifier. Here we include it in an at-
tempt to get to the root of an apparent discrepancy
in the tree kernel literature, as explained in Sec-
tion 2, in which Hovy et al (2012) report a nega-
tive result and Mirroshandel et al (2009) report a
positive result for using constituency structure in
tree kernels for temporal relation extraction.
The Path Tree representation uses a sub-tree of
the whole constituent tree, but removes all nodes
that are not along the path between the two argu-
ments. Path information has been used in standard
feature kernels (Pradhan et al, 2008), with each
individual path being a possible boolean feature.
VP
Arg1-Event-Actual
arg1
S
VP
VP
Arg2-Timex-Date
arg2
Figure 2: Path Tree (PT) representation
Another representation making use of the path tree
takes contiguous subsections of the path tree, or
?path n-grams,? in an attempt to combat the spar-
sity of using the whole path (Zheng et al, 2012).
By using the path representation with a tree ker-
nel, the model should get the benefit of all different
sizes of path n-grams, up to the size of the whole
path. This representation is augmented by adding
in argument nodes with event and time features, as
in the Feature Tree. Unlike the Feature Tree and
the PET below, the Path Tree representation does
not include word nodes, because the important as-
pect of this representation is the labels of the nodes
on the path between arguments. Figure 2 shows an
example of what this representation looks like.
The Path-Enclosed Tree representation is based
on the smallest sub-tree that encloses the two pro-
posed arguments. This is a representation that has
shown value in other work using tree kernels for
relation extraction (Zhang et al, 2006; Mirroshan-
del et al, 2009). The information contained in
the PET representation is a superset of that con-
tained in the Path Tree representation, since it in-
cludes the full path between arguments as well
as the structure between arguments and the ar-
gument text. This means that it can take into
account path information while also considering
constituent structure between arguments that may
play a role in determining whether the two ar-
guments are related. For example, temporal cue
words like after or during may occur between ar-
guments and will not be captured by Path Trees.
Like the PT representation, the PET representa-
tion is augmented with the semantic information
specified above in the Feature Tree representation.
Figure 3 shows an example of this representation.
22
VP
Arg1-Event-Actual
VBN
scheduled
VP
TO
to
VP
VB
undergo
NP
NN
surgery
Arg2-Timex-Date
NP
JJ
next
NN
week
Figure 3: Path-Enclosed Tree representation
4 Evaluation
The corpus we used for evaluations was described
in Section 2. There are 78 total notes in the corpus,
with three notes for each of 26 patients. The data
is split into training (50%), development (25%),
and test (25%) sections based on patient number,
so that each patient?s notes are all in the same
section. The combined training and development
set used for final training consists of 4378 sen-
tences with 49,050 tokens, and 7372 events, 849
time expressions, and 2287 CONTAINS relations.
There were 774 positive instances of CONTAINS
in the training data, with 1513 negative instances.
For constituent structure and features we use the
gold standard treebank and event and time features
from our corpus. Preliminary work suggests that
automatic parses from cTAKES do not harm per-
formance very much, but the focus of this work is
on the relation extraction so we use gold standard
parses. All preliminary experiments were done us-
ing the development set for testing.
We designed a set of experiments to exam-
ine several hypotheses regarding extraction of the
CONTAINS relation and the efficacy of different
tree kernel representations. The first two config-
urations test simple rule-based baseline systems,
CLOSEST-P and CLOSEST-R, for distance-related
decision rule systems meant to optimize precision
and recall, respectively. CLOSEST-P hypothesizes
a CONTAINS link between every TIMEX3 and the
closest annotated EVENT, which will make few
links overall. CLOSEST-R hypothesizes a CON-
TAINS link between every EVENT and the closest
TIMEX3, which will make many more links.
The next configuration, Flat Features, uses the
token and part of speech features along with ar-
gument semantics features, as described in Sec-
tion 3. While this feature set may not seem ex-
haustive, in preliminary work many traditional re-
lation extraction features were tried and found to
not have much effect. This particular configura-
tion was tested because it is most comparable to
the bag of word and bag of POS kernels from
Hovy et al (2012), and should help show whether
the tree kernel is providing anything over an equiv-
alent set of basic features.
We then examine several composite kernels, all
using the same feature kernel, but using different
tree kernel-based representations. First, we use a
composite kernel which uses the bag of word and
bag of POS tree views, as in Hovy et al (2012).
Next, we add in two additional tree views to the
tree kernel, Path-Enclosed Tree and Path Tree,
which are intended to examine the effect of using
traditional syntax, and the long distance features
that they enable. The final experimental config-
uration replaces the PET and PT representations
from the last configuration with the Feature Tree
representation. This tests the hypothesis that the
difference between positive results for tree kernels
in this task (as in, say, Mirroshandel et al (2009))
and negative results reported by Hovy et al (2012)
is the difference between using a full-parse tree
and using standard sub-tree representations.
For the rule-based systems, there are no param-
eters to tune. Our machine-learning systems are
based on support vector machines (SVM), which
require tuning of several parameters, including
kernel type (linear, polynomial, and radial basis
function), the parameters for each kernel, and c,
the cost of misclassification. Tree kernels intro-
duce an additional parameter ? for weighting large
structures, and the use of a composite kernel in-
troduces parameters for which kernel combination
operator to use, and how to weight the different
kernels for the sum operator.
For each machine learning configuration, we
performed a large grid search over the combined
parameter space, where we trained on the train-
ing set and tested on the development set. For
the final experiments, the parameters were chosen
that optimized the F1 score on the development
set. Qualitatively, the parameter tuning strongly
favored configurations which combined the ker-
nels using the sum operator, and recall and pre-
cision were strongly correlated with the SVM pa-
rameter c. Using these parameters, we then trained
23
on the combined training and development sets
and tested on the official test set.
4.1 Evaluation Metrics
The state of evaluating temporal relations has been
evolving over the past decade. This is partially
due to the inferential properties of temporal rela-
tions, because it is possible to define the same set
of relations using different set of axioms. To take
a very simple example, given a gold set of rela-
tions A<B and B<C, and given the system output
A<B, A<C and B<C, if one were to compute a
plain precision/recall metric, then the axiom A<C
would be counted against the system, when one
can easily infer from the gold set of relations that
it is indeed correct. With more relations the infer-
ence process becomes more complex.
Recently there has been some work trying
to address the shortcomings of the plain F1
score (Muller and Tannier, 2004; Setzer et al,
2006; UzZaman and Allen, 2011; Tannier and
Muller, 2008; Tannier and Muller, 2011). How-
ever, the community has not yet come to a consen-
sus on the best evaluation approach. Two recent
evaluations, TempEval-3 (UzZaman et al, 2013)
and the 2012 i2b2 Challenge (Sun et al, 2013),
used an implementation of the proposal by (Uz-
Zaman and Allen, 2011). However, as described
in Cherry et al (2013), this algorithm, which uses
a greedy graph minimization approach, is sensi-
tive to the order in which the temporal relations
are presented to the scorer. In addition, the scorer
is not able to give credit for non-redundant, non-
minimum links (Cherry et al, 2013) as with the
the case of the relation A<C mentioned earlier.
Considering that the measures for evaluating
temporal relations are still evolving, we decided to
use plain F-score, with recall and precision scores
also reported. This score is computed across all
intra-sentential EVENT-TIMEX3 pairs in the gold
standard, where precision = # correct predictions# predictions ,
recall = # correct predictions# gold standard relations , and F1 score =
2?precision?recall
precision+recall .
4.2 Experimental Results
Results are shown in Table 1. Rule-based base-
lines perform reasonably well, but are heavily bi-
ased in terms of precision or recall. The ma-
chine learning baseline cannot even obtain the
same performance as the CLOSEST-R rule-based
system, though it is more balanced in terms of pre-
System Precision Recall F1
CLOSEST-P 0.754 0.537 0.627
CLOSEST-R 0.502 0.947 0.656
Flat Features (FF) 0.705 0.593 0.645
FF+Bag Trees (BT) 0.649 0.728 0.686
FF+BT+PET+PT 0.770 0.707 0.737
FF+BT+FT 0.691 0.691 0.691
Table 1: Table of results of main experiments.
cision and recall. Using a composite kernel which
adds in the flat token-based tree kernels improves
performance over the standard feature kernel by
4.1 points. Adding in the Path Tree and Path-
Enclosed Tree constituency-based trees along with
the flat trees improves F1 score to our best result
of 73.7. Finally, replacing PT and PET representa-
tions with the Feature Tree representation does not
offer any performance improvement over the Flat
Features + Bag Trees configuration.
4.3 Error Analysis
We performed error analysis on the outputs of the
best-performing system (FF+BT+PET+PT in Ta-
ble 1). First, we note that the parameter search
was optimized for F1. This resulted in the highest-
scoring configuration using a composite kernel
with the sum operator, polynomial kernel for the
secondary kernel, ? = 0.5, tree kernel weight (T )
of 0.1, and c = 10.0. This high value of c and low
value of T results in higher precision and lower
recall, but there were configurations with lower c
and higher T which made the opposite tradeoff,
with only marginally worse F1-score. For the pur-
poses of error analysis, however, this configuration
leads to a focus on false negatives.
First, the false positives contained many rela-
tions that were legitimately ambiguous or possible
annotator errors. An example ambiguous case is
She is currently being treated on the Surgical Ser-
vice for..., in which the system generates the re-
lation CONTAINS(currently, treated), but the gold
standard labels as OVERLAP. This example is am-
biguous because it is not clear from just the lin-
guistic context whether the treatment is wholly
contained in the small time window denoted by
currently, or whether it started a while ago or will
continue into the future. There are many similar
cases where the event is a disease/disorder type,
and the specific nature of the disease is impor-
tant to understanding whether this is a CONTAINS
24
or OVERLAP relation, specifically understanding
whether the disease is chronic or more acute.
Another source of false positives were where
the event and time were clearly related, but not
with CONTAINS. In the example reports that she
has been having intermittent bleeding since May
of 1998, the term since clearly indicates that this
is a BEGINS-ON relation between bleeding and
May of 1998. This is a case where having other
temporal relation classifiers may be useful, as they
can compete and the relation can be assigned to
whichever classifier is more confident.
False negatives frequently occurred in contexts
where the event and time were far apart. Syn-
tactic tree kernels were introduced to help im-
prove recall on longer-distance relations, and were
successful up to a limit. However, certain ex-
amples are so far apart that the algorithm may
have had difficulty sorting noise from important
structure. For example, the system did not find
the CONTAINS(October 27, 2010, oophorectomy)
relation in the sentence:
October 27, 2010, Dr. XXX performed
exploratory laparotomy with an trans-
verse colectomy and Dr. YYY performed
a total abdominal hysterectomy with a
bilateral salpingo-oophorectomy.
Here, while the date may be part of the same sen-
tence as the event, the syntactic relation between
the pair is not what makes the relation; the date is
acting as a kind of discourse marker that indicates
that the following events are contained. This sug-
gests that discourse-level features may be useful
even for the intra-sentential classification task.
Other false negatives occurred where there was
syntactic complexity, even on shorter examples.
The subset tree kernel used here matches com-
plete rule productions, and across complex struc-
ture with large productions, the chances of finding
similarity decreases substantially. Thus, events
within coordination or separated from the time by
clause breaks are more difficult to relate to the
time due to the multiple different ways of relating
these different syntactic elements.
Finally, there are some examples where the an-
chor of a narrative container is an event with mul-
tiple sub-events. In these cases, the system per-
forms well at relating a time expression to the an-
chor event, but may miss the sub-events that are
farther away. This is a case where having an event-
event TLINK classifier, then applying determinis-
tic closure rules, would allow a combined system
to link the sub-events to the time expression.
5 Discussion and Conclusion
In this paper we have developed a system for auto-
matically identifying CONTAINS relations in clini-
cal text. The experiments show first that a machine
learning approach that intelligently integrates con-
stituency information can greatly improve perfor-
mance over rule-based baselines. We also show
that the tree kernel approach, which can model se-
quence better than a bag of tokens-style approach,
is beneficial even when it uses the same features.
Finally, the experiments show that choosing the
correct representation is important for tree kernel
approaches, and specifically that using a full parse
tree may give inferior performance compared to
sub-trees focused on the structure of interest.
In general, there is much work to be done in the
area of representing temporal information in clin-
ical records. Many of the inputs to the algorithm
described in this paper need to be extracted auto-
matically, including time expressions and events.
Work on relations will focus on adding features
to represent discourse information and richer rep-
resentation of event semantics. Discourse infor-
mation may help with the longer-distance errors,
where the time expression acts almost as a topic
for an extended description of events. Better un-
derstanding of event semantics, such as whether
a disease is chronic or acute, or typical duration
for a treatment, may help constrain relations. In
addition, we will explore the effectiveness of us-
ing dependency tree structure, which has been use-
ful in the domain of extracting relations from the
biomedical literature (Tikk et al, 2013).
Acknowledgements
The work described was supported by Tempo-
ral History of Your Medical Events (THYME)
NLM R01LM010090 and Integrating Informat-
ics and Biology to the Bedside (i2b2) NCBO
U54LM008748. Thanks to the anonymous re-
viewers for thorough and insightful comments.
References
Naushad UzZaman, Hector Llorens, et al 2013. Semeval-
2013 task 1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Second Joint Confer-
ence on Lexical and Computational Semantics (*SEM),
25
Volume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Computa-
tional Linguistics.
Ashwin N Ananthakrishnan, Tianxi Cai, et al 2013. Improv-
ing case definition of Crohn?s disease and ulcerative col-
itis in electronic medical records using natural language
processing: a novel informatics approach. Inflammatory
bowel diseases.
Steven Bethard and James H. Martin. 2007. CU-TMP: Tem-
poral relation classification using syntactic and semantic
features. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007), pages
129?132.
Colin Cherry, Xiaodan Zhu, et al 2013. A la recherche du
temps perdu: extracting temporal relations from medical
text in the 2012 i2b2 NLP challenge. Journal of the Amer-
ican Medical Informatics Association, March.
Michael Collins and Nigel Duffy. 2001. Convolution kernels
for natural language. In Neural Information Processing
Systems.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Dirk Hovy, James Fan, et al 2012. When did that happen?:
linking events and relations to timestamps. In Proceedings
of the 13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 185?193.
Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector Learn-
ing. Universita?t Dortmund.
Chen Lin, Helena Canhao, et al 2012. Feature engineering
and selection for rheumatoid arthritis disease activity clas-
sification using electronic medical records. In Proceed-
ings of ICML Workshop on Machine Learning for Clinical
Data.
Hector Llorens, Estela Saquete, and Borja Navarro. 2010.
TIPSem (english and spanish): Evaluating crfs and seman-
tic roles in tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 284?291.
Association for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for clas-
sifying temporal relations between events. Proc. of the
PACLIC23, pages 355?364.
Alessandro Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In Ma-
chine Learning: ECML 2006, pages 318?329. Springer.
Philippe Muller and Xavier Tannier. 2004. Annotating and
measuring temporal relations in texts. In Proceedings of
the 20th international conference on Computational Lin-
guistics, COLING ?04, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Philip V. Ogren, Philipp G. Wetzler, and Steven J. Bethard.
2009. ClearTK: a framework for statistical natural lan-
guage processing. In Unstructured Information Manage-
ment Architecture Workshop at the Conference of the Ger-
man Society for Computational Linguistics and Language
Technology, 9.
Sameer S Pradhan, Wayne Ward, and James H Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics, 34(2):289?310.
James Pustejovsky and Amber Stubbs. 2011. Increasing in-
formativeness in temporal annotation. In Proceedings of
the 5th Linguistic Annotation Workshop, pages 152?160.
James Pustejovsky, Patrick Hanks, et al 2003a. The time-
bank corpus. In Corpus linguistics, volume 2003, page 40.
James Pustejovsky, Jose? Casta no, et al 2003b. Timeml:
Robust specification of event and temporal expressions in
text. In Fifth International Workshop on Computational
Semantics (IWCS-5).
Preethi Raghavan, Eric Fosler-Lussier, and Albert M Lai.
2012. Temporal classification of medical events. In Pro-
ceedings of the 2012 Workshop on Biomedical Natural
Language Processing, pages 29?37. Association for Com-
putational Linguistics.
Guergana K. Savova, James J. Masanz, et al 2010. Mayo
clinical text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and appli-
cations. J Am Med Inform Assoc, 17(5):507?513.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple. 2006.
The role of inference in the temporal annotation and anal-
ysis of text. Language Resources and Evaluation, 39(2-
3):243?265, February.
Stephanie Strassel, Dan Adams, et al 2010. The DARPA
machine reading program - encouraging linguistic and rea-
soning research with a series of reading tasks. In Proceed-
ings of the Seventh International Conference on Language
Resources and Evaluation (LREC?10), Valletta, Malta.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informatics
Association, April.
Xavier Tannier and Philippe Muller. 2008. Evaluation met-
rics for automatic temporal annotation of texts. Proceed-
ings of the Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08).
Xavier Tannier and Philippe Muller. 2011. Evaluating tem-
poral graphs built from texts via transitive reduction. J.
Artif. Int. Res., 40(1):375413, January.
Domonkos Tikk, Ille?s Solt, et al 2013. A detailed error anal-
ysis of 13 kernel methods for protein-protein interaction
extraction. BMC bioinformatics, 14(1):12.
Naushad UzZaman and James Allen. 2011. Temporal eval-
uation. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Lan-
guage Technologies, page 351?356.
Marc Verhagen, Robert Gaizauskas, et al 2007. Semeval-
2007 task 15: Tempeval temporal relation identification.
In Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 75?80.
Marc Verhagen, Roser Sauri, et al 2010. Semeval-2010 task
13: Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Uppsala,
Sweden, July. Association for Computational Linguistics.
RA Wilke, H Xu, et al 2011. The emerging role of electronic
medical records in pharmacogenomics. Clinical Pharma-
cology & Therapeutics, 89(3):379?386.
Yan Xu, Yining Wang, et al 2013. An end-to-end system to
identify temporal relation in discharge summaries: 2012
i2b2 challenge. Journal of the American Medical Infor-
matics Association : JAMIA.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring syn-
tactic features for relation extraction using a convolution
tree kernel. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of the
ACL, pages 288?295.
Jiaping Zheng, Wendy W Chapman, et al 2012. A sys-
tem for coreference resolution for the clinical narrative.
Journal of the American Medical Informatics Association,
19:660?667.
26
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143?152,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Towards Robust Linguistic Analysis Using OntoNotes
Sameer Pradhan1, Alessandro Moschitti2,3, Nianwen Xue4, Hwee Tou Ng5
Anders Bjo?rkelund6, Olga Uryupina2, Yuchen Zhang4 and Zhi Zhong5
1 Boston Childrens Hospital and Harvard Medical School, Boston, MA 02115, USA
2 University of Trento, University of Trento, 38123 Povo (TN), Italy
3 QCRI, Qatar Foundation, 5825 Doha, Qatar
4 Brandeis University, Brandeis University, Waltham, MA 02453, USA
5 National University of Singapore, Singapore, 117417
6 University of Stuttgart, 70174 Stuttgart, Germany
Abstract
Large-scale linguistically annotated cor-
pora have played a crucial role in advanc-
ing the state of the art of key natural lan-
guage technologies such as syntactic, se-
mantic and discourse analyzers, and they
serve as training data as well as evaluation
benchmarks. Up till now, however, most
of the evaluation has been done on mono-
lithic corpora such as the Penn Treebank,
the Proposition Bank. As a result, it is still
unclear how the state-of-the-art analyzers
perform in general on data from a vari-
ety of genres or domains. The completion
of the OntoNotes corpus, a large-scale,
multi-genre, multilingual corpus manually
annotated with syntactic, semantic and
discourse information, makes it possible
to perform such an evaluation. This paper
presents an analysis of the performance of
publicly available, state-of-the-art tools on
all layers and languages in the OntoNotes
v5.0 corpus. This should set the bench-
mark for future development of various
NLP components in syntax and semantics,
and possibly encourage research towards
an integrated system that makes use of the
various layers jointly to improve overall
performance.
1 Introduction
Roughly a million words of text from the Wall
Street Journal newswire (WSJ), circa 1989, has
had a significant impact on research in the lan-
guage processing community ? especially those
in the area of syntax and (shallow) semantics, the
reason for this being the seminal impact of the
Penn Treebank project which first selected this text
for annotation. Taking advantage of a solid syn-
tactic foundation, later researchers who wanted to
annotate semantic phenomena on a relatively large
scale, also used it as the basis of their annota-
tion. For example the Proposition Bank (Palmer et
al., 2005), BBN Name Entity and Pronoun coref-
erence corpus (Weischedel and Brunstein, 2005),
the Penn Discourse Treebank (Prasad et al, 2008),
and many other annotation projects, all annotate
the same underlying body of text. It was also con-
verted to dependency structures and other syntac-
tic formalisms such as CCG (Hockenmaier and
Steedman, 2002) and LTAG (Shen et al, 2008),
thereby creating an even bigger impact through
these additional syntactic resources. The most re-
cent one of these efforts is the OntoNotes corpus
(Weischedel et al, 2011). However, unlike the
previous extensions of the Treebank, in addition
to using roughly a third of the same WSJ subcor-
pus, OntoNotes also added several other genres,
and covers two other languages ? Chinese and
Arabic: portions of the Chinese Treebank (Xue et
al., 2005) and the Arabic Treebank (Maamouri and
Bies, 2004) have been used to sample the genre of
text that they represent.
One of the current hurdles in language process-
ing is the problem of domain, or genre adaptation.
Although genre or domain are popular terms, their
definitions are still vague. In OntoNotes, ?genre?
means a type of source ? newswire (NW), broad-
cast news (BN), broadcast conversation (BC), mag-
azine (MZ), telephone conversation (TC), web data
(WB) or pivot text (PT). Changes in the entity and
event profiles across source types, and even in the
same source over a time duration, as explicitly ex-
pressed by surface lexical forms, usually account
for a lot of the decrease in performance of mod-
els trained on one source and tested on another,
usually because these are the salient cues that are
relied upon by statistical models.
Large-scale corpora annotated with multiple
layers of linguistic information exist in various
languages, but they typically consist of a single
source or collection. The Brown corpus, which
consists of multiple genres, have been usually used
to investigate issues of genres of sensitivity, but it
is relatively small and does not include any infor-
1A portion of the English data in the OntoNotes corpus
is a selected set of sentences that were annotated for parse
and word sense information. These sentences are present in a
document of their own, and so the documents for parse layers
for English are inflated by about 3655 documents and for the
word sense are inflated by about 8797 documents.
143
Language Parse Proposition Sense Name Coreference
Documents Words Documents Verb Prop. Noun Prop. Documents Verb Sense Noun Sense Documents Words Documents Words
English 7,9671 2.6M 6,124 300K 18K 12K 173K 120K 3,637 2.0M 2,384(3493) 1.7M
Chinese 2002 1.0M 1861 148K 7K 1573 83K 1K 1,911 988K 1,729(2,280) 950K
Arabic 599 402K 599 30K - 310 4.3K 8.7K 446 298K 447(447) 300K
Table 1: Coverage for each layer in the OntoNotes v5.0 corpus, by number of documents, words, and
some other attributes. The numbers in parenthesis are the total number of parts in the documents.
mal genres such as web data. Very seldom has it
been the case that the exact same phenomena have
been annotated on a broad cross-section of the
same language before OntoNotes. The OntoNotes
corpus thus provides an opportunity for studying
the genre effect on different syntactic, semantic
and discourse analyzers.
Parts of the OntoNotes Corpus have been used
for various shared tasks organized by the language
processing community. The word sense layer was
the subject of prediction in two SemEval-2007
tasks, and the coreference layer was the subject
of prediction in the SemEval-20102 (Recasens et
al., 2010), CoNLL-2011 and 2012 shared tasks
(Pradhan et al, 2011; Pradhan et al, 2012). The
CoNLL-2012 shared task provided predicted in-
formation to the participants, however, that did not
include a few layers such as the named entities
for Chinese and Arabic, propositions for Arabic,
and for better comparison of the English data with
the CoNLL-2011 task, a smaller OntoNotes v4.0
portion of the English parse and propositions was
used for training.
This paper is a first attempt at presenting a co-
herent high-level picture of the performance of
various publicly available state-of-the-art tools on
all the layers of OntoNotes in all three languages,
so as to pave the way for further explorations in
the area of syntax and semantics processing.
The possible avenues for exploratory studies
on various fronts are enormous. However, given
space considerations, in this paper, we will re-
strict our presentation of the performance on all
layers of annotation in the data by using a strat-
ified cross-section of the corpus for training, de-
velopment, and testing. The paper is organized
as follows: Section 2 gives an overview of the
OntoNotes corpus. Section 3 explains the param-
eters of the evaluation and the various underlying
assumptions. Section 4 presents the experimental
results and discussion, and Section 5 concludes the
paper.
2 OntoNotes Corpus
The OntoNotes project has created a large-scale
corpus of accurate and integrated annotation of
2A small portion 125K words in English was used for this
evaluation.
multiple layers of syntactic, semantic and dis-
course information in text. The English lan-
guage portion comprises roughly 1.7M words and
Chinese language portion comprises roughly 1M
words of newswire, magazine articles, broadcast
news, broadcast conversations, web data and con-
versational speech data3. The Arabic portion is
smaller, comprising 300K words of newswire ar-
ticles. This rich, integrated annotation covering
many layers aims at facilitating the development
of richer, cross-layer models and enabling bet-
ter automatic semantic analysis. The corpus is
tagged with syntactic trees, propositions for most
verb and some noun instances, partial verb and
noun word senses, coreference, and named enti-
ties. Table 1 gives an overview of the number of
documents that have been annotated in the entire
OntoNotes corpus.
2.1 Layers of Annotation
This section provides a very concise overview of
the various layers of annotations in OntoNotes.
For a more detailed description, the reader is re-
ferred to (Weischedel et al, 2011) and the docu-
mentation accompanying the v5.04 release.
2.1.1 Syntax
This represents the layer of syntactic annotation
based on revised guidelines for the Penn Tree-
bank (Marcus et al, 1993; Babko-Malaya et al,
2006), the Chinese Treebank (Xue et al, 2005)
and the Arabic Treebank (Maamouri and Bies,
2004). There were two updates made to the parse
trees as part of the OntoNotes project: i) the in-
troduction of NML phrases, in the English portion,
to mark nominal sub-constituents of flat NPs that
do not follow the default right-branching structure,
and ii) re-tokenization of hyphenated tokens into
multiple tokens in English and Chinese. The Ara-
bic Treebank on the other hand was also signifi-
cantly revised in an effort to increase consistency.
2.1.2 Word Sense
Coarse-grained word senses are tagged for the
most frequent polysemous verbs and nouns, in or-
3These numbers are for the portion that has all layers of
annotations. The word count for each layer is mentioned in
Table 1
4For all the layers of data used in this study, the
OntoNotes v4.99 pre-release that was used for the CoNLL-
2012 shared task is identical to the v5.0 release.
144
der to maximize token coverage. The word sense
granularity is tailored to achieve very high inter-
annotator agreement as demonstrated by Palmer et
al. (2007). These senses are defined in the sense
inventory files. In the case of English and Arabic
languages, the sense-inventories (and frame files)
are defined separately for each part of speech that
is realized by the lemma in the text. For Chinese,
however the sense inventories (and frame files) are
defined per lemma ? independent of the part of
speech realized in the text.
2.1.3 Proposition
The propositions in OntoNotes are PropBank-style
semantic roles for English, Chinese and Arabic.
Most English verbs and few nouns were anno-
tated using the revised guidelines for the English
PropBank (Babko-Malaya et al, 2006) as part of
the OntoNotes effort. Some enhancements were
made to the English PropBank and Treebank to
make them synchronize better with each other:
one of the outcomes of this effort was that two
types of LINKs that represent pragmatic coref-
erence (LINK-PCR) and selectional preferences
(LINK-SLC) were added to the original PropBank
(Palmer et al, 2005). More details can be found in
the addendum to the PropBank guidelines5 in the
OntoNotes v5.0 release. A part of speech agnostic
Chinese PropBank (Xue and Palmer, 2009) guide-
lines were used to annotate most frequent lem-
mas in Chinese. Many verbs and some nouns and
adjectives were annotated using the revised Ara-
bic PropBank guidelines (Palmer et al, 2008; Za-
ghouani et al, 2010).
2.1.4 Named Entities
The corpus was tagged with a set of 18 well-
defined proper named entity types that have been
tested extensively for inter-annotator agreement
by Weischedel and Burnstein (2005).
2.1.5 Coreference
This layer captures general anaphoric corefer-
ence that covers entities and events not limited
to noun phrases or a limited set of entity types
(Pradhan et al, 2007). It considers all pronouns
(PRP, PRP$), noun phrases (NP) and heads of verb
phrases (VP) as potential mentions. Unlike En-
glish, Chinese and Arabic have dropped subjects
and objects which were also considered during
coreference annotation6. The mentions formed by
these dropped pronouns total roughly about 11%
for both Chinese and Arabic. Coreference is the
only document-level phenomenon in OntoNotes.
Some of the documents in the corpus ? especially
the ones in the broadcast conversation, web data,
5doc/propbank/english-propbank.pdf
6As we will see later these are not used during the task.
and telephone conversation genre ? are very long
which prohibited efficient annotation in their en-
tirety. These are split into smaller parts, and each
part is considered a separate document for the sake
of coreference evaluation.
3 Evaluation Setting
Given the scope of the corpus and the multitude of
settings one can run evaluations, we had to restrict
this study to a relatively focused subset. There has
already been evidence of models trained on WSJ
doing poorly on non-WSJ data on parses (Gildea,
2001; McClosky et al, 2006), semantic role label-
ing (Carreras and Ma`rquez, 2005; Pradhan et al,
2008), word sense (Escudero et al, 2000; ?), and
named entities. The phenomenon of coreference is
somewhat of an outlier. The winning system in the
CoNLL-2011 shared task was one that was com-
pletely rule-based and not directly trained on the
OntoNotes corpus. Given this overwhelming evi-
dence, we decided not to focus on potentially com-
plex cross-genre evaluations. Instead, we decided
on evaluating the performance on each layer of an-
notation using an appropriately selected, stratified
training, development and test set, so as to facili-
tate future studies.
3.1 Training, Development and Test
Partitions
In this section we will have a brief discussion
on the logic behind the partitioning of the data
into training, development and test sets. Before
we do that, it would help to know that given the
range and peculiarities of the layers of annota-
tion and presence of various resource and techni-
cal constraints, not all the documents in the cor-
pus are annotated with all the layers of informa-
tion, and token-centric phenomena (such as word
sense and propositions of predicates) were not an-
notated with 100% coverage. Most of the propo-
sition annotation in English and Arabic is for the
verb predicates, with a few nouns annotated in
English and some adjectives in Arabic. In Chi-
nese, the selection is part of speech agnostic, and is
based on the lemmas that can be considered predi-
cates. Some documents in the corpora are actually
snippets from larger documents, and have been an-
notated for a combination of parse, propositions,
word sense and names, but not coreference. If one
considers each layer independently, then an ideal
partitioning scheme would create a separate parti-
tion for each layer such that it maximizes the num-
ber of examples that can be extracted for that layer
from the corpus. The upside is that one would
get as much data there is to train and estimate the
performance of each layer across the entire cor-
pus. The downside is that this might cover vari-
145
ous cross sections of the documents in the corpus,
and would not provide a clean picture when look-
ing at the collective performance for all the lay-
ers. The documents that are annotated with coref-
erence correspond to the intersection of all anno-
tations. These are the documents that have also
been annotated with all the other layers of infor-
mation. The amount of data we can get together
in such a test set is big enough to be represen-
tative. Therefore, we decided that it would be
ideal to choose a portion of these documents as
the test collection for all layers. An additional ad-
vantage is that it is the exact same test set used
in the CoNLL-2012 shared task, and so in a way
is already a standard. On the training and devel-
opment side however, one can still imagine using
all possible information for training models for a
particular layer, and that is what we decided to
do. The training and development data is gener-
ated by providing all documents with all available
layers of annotation for input, however, the test
set is generated by providing as input to the algo-
rithm the set of documents in the corpus that have
been annotated for coreference. This algorithm
tries to reuse previously established partitions for
English, i.e., the WSJ portion. Unfortunately, in
the case of Chinese and Arabic, either the histor-
ical partitions were not in the selection used for
OntoNotes, or were partially overlapping with the
ones created using this scheme, and/or had a very
small portion of OntoNotes covered in the test set.
Therefore, we decided to create a fresh partition
for the Chinese and Arabic data. Note, however,
that the these test sets also match the ones used
in the CoNLL-2012 evaluation. The algorithm for
selecting the training, development and test parti-
tions is described on the CoNLL-2012 shared task
webpage, along with the list of training, develop-
ment, and test document IDs7.
3.2 Assumptions
Next we had to decide on a set of assumptions
to use while designing the experiments to mea-
sure the automatic prediction accuracy for each of
the layers. Since some of these decisions affect
more than one layer of annotation, we will de-
scribe these in this section instead of in the section
where we discuss the experiment with a particular
layer of annotation.
7http://conll.cemantix.org/2012/download/ids/
For each language there are two sub-directories ? ?all?
contains more general lists which include documents
that had at least one of the layers of annotation, and
?coref? contains the lists that include documents that
have coreference annotation. The former were used to
generate training, development, test sets for layers other
than coreference, and the latter was used to generate
training/development/test sets for the coreference layer
used in the CoNLL-2012 shared task.
Word Segmentation The three languages that
we are evaluating are from quite different lan-
guage families. Arabic has a complex morphol-
ogy, English has limited morphology, whereas
Chinese has very little morphology. English word
segmentation amounts to rule-based tokenization,
and is close to perfect. In the case of Chinese and
Arabic, although the tokenization/segmentation is
not as good as English, the accuracies are in the
high 90s. Given this we decided to use gold,
Treebank segmentation for all languages. In the
case of Chinese, the words themselves are lem-
mas, whereas in English they can be predicted
with very high accuracy. For Arabic, by default
written text is unvocalised, and lemmatization is a
complex process which we considered out of the
scope of this study, so we decided to use correct,
gold standard lemmas, along with the correct vo-
calized version of the tokens.
Traces and Function Tags Treebank traces
have hardly played a role in the mainstream parser
and semantic role labeling evaluation. Function
tags also have received similar treatment in the
parsing community, and though they are impor-
tant, there is also a significant information overlap
between them and the proposition structure pro-
vided by the PropBank layer. Whereas in English,
most traces represent syntactic phenomena such
as movement and raising, in Chinese and Arabic,
they can also represent dropped subjects/objects.
These subset of traces directly affect the corefer-
ence layer, since, unlike English, traces in Chinese
and Arabic (*pro* and * respectively) are legit-
imate targets of mentions and are considered for
coreference annotation in OntoNotes. Recovering
traces in text is a hard problem, and the most re-
cently reported numbers in literature for Chinese
are around a F-score of 50 (Yang and Xue, 2010;
Cai et al, 2011). For Arabic there have not been
much studies on recovering these. A study by
Gabbard (2010) shows that these can be recovered
with an F-score of 55 with automatic parses and
roughly 65 using gold parses. Considering the low
level of prediction accuracy of these tokens, and
their relative low frequency, we decided to con-
sider predicting traces in trees out of the scope of
this study. In other words, we removed the man-
ually identified traces and function tags from the
Treebanks across all three languages, in all the
three ? training, development and test partitions.
This meant removing any and all dependent an-
notation in layers such as PropBank and Coref-
erence. In the case of PropBank these are the
argument bearing traces, whereas in coreference
these are the mentions formed by these elided sub-
jects/objects.
146
Disfluencies One thing that needs to be dealt
with in conversational data is the presence of dis-
fluencies (restarts, etc.). In the English parses of
the OntoNotes, disfluencies are marked using a
special EDITED8 phrase tag ? as was the case for
the Switchboard Treebank. Computing the accu-
racy of identifying disfluencies is also out of the
scope of this study. Given the frequency of dis-
fluencies and the performance with which one can
identify them automatically,9 a probable process-
ing pipeline would filter them out before parsing.
We decided to remove them using oracle infor-
mation available in the English Treebank, and the
coreference chains were remapped to trees with-
out disfluencies. Owing to various technical con-
straints, we decided to retain the disfluencies in the
Chinese data.
Spoken Genre Given the scope of this study, we
make another significant assumption. For the spo-
ken genres ? BC, BN and TC ? we use the manual
transcriptions rather than the output of a speech
recognizer, as would be the case in real world. The
performance on various layers for these genres
would therefore be artificially inflated, and should
be taken into account while analyzing results. Not
many studies have previously reported on syntac-
tic and semantic analysis for spoken genre. Favre
et al (2010) report the performance on the English
subset of an earlier version of OntoNotes.
Discourse The corpus contains information on
the speaker for broadcast communication, conver-
sation, telephone conversation and writer for the
web data. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be
automatically deduced, but is also not within the
scope of our study. Therefore, we decided to pro-
vide gold, instead of predicted, data both during
training and testing. Table 2 lists the status of the
layers.
4 Experiments
In this section, we will report on the experiments
carried out using all available data in the train-
ing set for training models for a particular layer,
and using the CoNLL-2012 test set as the test set.
8There is another phrase type ? EMBED in the telephone
conversation genre which is similar to the EDITED phrase
type, and sometimes identifies insertions, but sometimes con-
tains logical continuation of phrases by different speakers, so
we decided not to remove that from the data.
9A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 precision
and 67 recall.
10The predicted part of speech for Arabic are a mapped
down version of the richer gold version present in the Tree-
bank
Layer English Chinese Arabic
Segmentation ? ? ?
Lemma ? ? ?
Parse ? ? ?10
Proposition ? ? ?
Predicate Frame ? ? ?
Word Sense ? ? ?
Name Entities ? ? ?
Coreference ? ? ?
Speaker ? ? ?
Number ? ? ?
Gender ? ? ?
Table 2: Status of layers used during prediction
of other layers. A ??? indicates gold annotation,
a ??? indicates predicted, a ??? indicates an ab-
sence of the predicted layer, and a ??? indicates
that the layer is not applicable to the language.
The predicted annotation layers input to down-
stream models were automatically annotated by
using NLP processors learned with n-cross fold
validation on the training data. This way, the n
chunks of training data are annotated avoiding de-
pendencies with the data used for training the NLP
processors.
4.1 Syntax
Predicted parse trees for English were produced
using the Charniak parser11 (Charniak and John-
son, 2005). Some additional tag types used in
the OntoNotes trees were added to the parser?s
tagset, including the nominal (NML) tag, and the
rules used to determine head words were extended
correspondingly. Chinese and Arabic parses were
generated using the Berkeley parser (Petrov and
Klein, 2007). In the case of Arabic, the pars-
ing community uses a mapping from rich Arabic
part of speech tags to Penn-style part of speech
tags. We used the mapping that is included with
the Arabic Treebank. The predicted parses for
the training portion of the data were generated us-
ing 10-fold (5-folds for Arabic) cross-validation.
For testing, we used a model trained on the entire
training portion. Table 3 shows the precision, re-
call and F1-scores of the re-trained parsers on the
CoNLL-2012 test along with the part of speech ac-
curacies (POS) using the standard evalb scorer.
The performance on the PT genre for English is
the highest among other English genres. This is
possibly because of the professional, clean trans-
lations of the underlying text, and are mostly
shorter sentences. The MZ genre and the NW both
of which contain well edited text, share similar
scores. There is a few points gap between these
and the other genres. As for Chinese, the per-
formance on MZ is the highest followed by BN.
Surprisingly, the WB genre has a similar score and
the others are close behind except for TC. As ex-
pected, the Arabic parser performance is the low-
11http://bllip.cs.brown.edu/download/reranking-parserAug06.tar.gz
147
All Sentences
N POS P R F
English BC 2,211 97.33 86.36 86.11 86.23
BN 1,357 97.32 87.61 87.03 87.32
MZ 780 96.58 89.90 89.49 89.70
NW 2,327 97.15 87.68 87.25 87.47
TC 1,366 96.11 85.09 84.13 84.60
WB 1,787 96.03 85.46 85.26 85.36
PT 1,869 98.77 95.29 94.66 94.98
Overall 11,697 97.09 88.08 87.65 87.87
Chinese BC 885 94.79 80.17 79.35 79.76
BN 929 93.85 83.49 80.13 81.78
MZ 451 97.06 88.48 83.85 86.10
NW 481 94.07 82.26 77.28 79.69
TC 968 92.22 71.90 69.19 70.52
WB 758 92.37 82.57 78.92 80.70
Overall 4,472 94.12 82.23 78.93 80.55
Arabic NW 1,003 94.12 74.71 75.67 75.19
Table 3: Parser performance on the CoNLL-2012
test set.
est among the three languages.
4.2 Word Sense
We used the IMS12 (It Makes Sense) (Zhong and
Ng, 2010) word sense tagger. IMS was trained on
all the word sense data that is present in the train-
ing portion of the OntoNotes corpus using cross-
validated predictions on the input layers similar
to the proposition tagger. During testing, for En-
glish and Arabic, IMS must first use the auto-
matic POS information to identify the nouns and
verbs in the test data, and then assign senses to
the automatically identified nouns and verbs. In
the case of Arabic, IMS uses gold lemmas. Since
automatic POS tagging is not perfect, IMS does
not always output a sense to all word tokens that
need to be sense tagged due to wrongly predicted
POS tags. As such, recall is not the same as pre-
cision on the English and Arabic test data. For
Chinese the measure of performance is just the
accuracy since the senses are defined per lemma
rather than per part of speech. Since we provide
gold word segmentation, IMS attempts to sense
tag all correctly segmented Chinese words, so re-
call and precision are the same and so is the F1-
score. Table 4 shows the performance of this clas-
sifier aggregated over both the verbs and nouns
in the CoNLL-2012 test set and an overall score
split by nouns and verbs for English and Ara-
bic. For both nouns and verbs in English, the
F1-score is over 80%. The performance on En-
glish nouns is slightly higher than English verbs.
Comparing to the other two languages, the perfor-
mance on Arabic is relatively lower, especially the
performance on Arabic verbs, whose F1-score is
less than 70%. For English, genres PT and TC,
and for Chinese genres TC and WB, no gold stan-
dard senses were available, and so their accuracies
could not be computed. Previously, Zhong et al
(2008) reported the word sense performance on
the Wall Street Journal portion of an earlier ver-
12http://www.comp.nus.edu.sg/?nlp/sw/IMS v0.9.2.1.tar.gz
Performance
P R F A
English BC 81.2 81.3 81.2 -
BN 82.0 81.5 81.7 -
MZ 79.1 78.8 79.0 -
NW 85.7 85.7 85.7 -
WB 77.5 77.6 77.5 -
Overall 82.5 82.5 82.5 -
Nouns 83.4 83.1 83.2 -
Verbs 81.8 81.9 81.8 -
Chinese BC - - - 80.5
BN - - - 85.4
MZ - - - 82.4
NW - - - 89.1
Overall - - - 84.3
Arabic NW 75.9 75.2 75.6 -
Nouns 79.2 77.7 78.4 -
Verbs 68.8 69.5 69.1 -
Table 4: Word sense performance on the CoNLL-
2012 test set.
sion of OntoNotes, but the results are not directly
comparable.
4.3 Proposition
The revised PropBank has introduced two new
links ? LINK-SLC and LINK-PCR. Since the com-
munity is not used to the new PropBank represen-
tation which (i) relies heavily on the trace struc-
ture in the Treebank and (ii) we decided to ex-
clude, we unfold the LINKs back to their original
representation as in the PropBank 1.0 release. We
used ASSERT15 (Pradhan et al, 2005) to predict
the propositional structure for English. We made
a small modification to ASSERT, and replaced
the TinySVM classifier with a CRF16 to speed
up training the model on all the data. The Chi-
nese propositional structure was predicted with the
Chinese semantic role labeler described in (Xue,
2008), retrained on the OntoNotes v5.0 data. The
Arabic propositional structure was predicted us-
ing the system described in Diab et al (2008).
(Diab et al, 2008) Table 5 shows the detailed per-
14The Frame ID column indicates the F-score for English
and Arabic, and accuracy for Chinese for the same reasons as
word sense.
15http://cemantix.org/assert.html
16http://leon.bottou.org/projects/sgd
Frame Total Total % Perfect Argument ID + Class
ID Sent. Prop. Prop. P R F
English BC 93.2 1994 5806 52.89 80.76 69.69 74.82
BN 92.7 1218 4166 54.78 80.22 69.36 74.40
MZ 90.8 740 2655 50.77 79.13 67.78 73.02
NW 92.8 2122 6930 46.45 79.80 66.80 72.72
TC 91.8 837 1718 49.94 79.85 72.35 75.91
WB 90.7 1139 2751 42.86 80.51 69.06 74.35
PT 96.6 1208 2849 67.53 89.35 84.43 86.82
Overall 92.8 9,261 26,882 51.66 81.30 70.53 75.53
Chinese BC 87.7 885 2,323 31.34 53.92 68.60 60.38
BN 93.3 929 4,419 35.44 64.34 66.05 65.18
MZ 92.3 451 2,620 31.68 65.04 65.40 65.22
NW 96.6 481 2,210 27.33 69.28 55.74 61.78
TC 82.2 968 1,622 32.74 48.70 59.12 53.41
WB 87.8 758 1,761 35.21 62.35 68.87 65.45
Overall 90.9 4,472 14,955 32.62 61.26 64.48 62.83
Arabic NW 85.6 1,003 2337 24.18 52.99 45.03 48.68
Table 5: Proposition and frameset disambiguation
performance14 in the CoNLL-2012 test set.
148
formance numbers17. The CoNLL-2005 scorer18
was used to compute the scores. At first glance,
the performance on the English newswire genre is
much lower than what has been reported for WSJ
Section 23. This could be attributed to several fac-
tors: i) the newswire in OntoNotes not only con-
tains WSJ data, but also Xinhua news, and some
other newswire evaluation data, ii) The WSJ train-
ing and test portions in OntoNotes are a subset of
the standard ones that have been used to report
performance earlier; iii) the PropBank guidelines
were significantly revised during the OntoNotes
project in order to synchronize well with the Tree-
bank, and finally iv) it includes propositions for
be verbs missing from the original PropBank. It
looks like the newly added Pivot Text data (com-
prised of the New Testament) shows very good
performance. The Chinese and Arabic19 accuracy
is much worse. In addition to automatically pre-
dicting the arguments, we also trained the IMS
system to tag PropBank frameset IDs.
Language Genre Entity Performance
Count P R F
English BC 1671 80.17 77.20 78.66
BN 2180 88.95 85.69 87.29
MZ 1161 82.74 82.17 82.45
NW 4679 86.79 84.25 85.50
TC 362 74.09 61.60 67.27
WB 1133 77.72 68.05 72.56
Overall 11186 84.04 80.86 82.42
Chinese BC 667 72.49 58.47 64.73
BN 3158 82.17 71.50 76.46
NW 1453 86.11 76.39 80.96
MZ 1043 65.16 56.66 60.62
TC 200 48.00 60.00 53.33
WB 886 80.60 51.13 62.57
Overall 7407 78.20 66.45 71.85
Arabic NW 2550 74.53 62.55 68.02
Table 6: Performance of the named entity recog-
nizer on the CoNLL-2012 test set.
4.4 Named Entities
We retrained the Stanford named entity recog-
nizer20 (Finkel et al, 2005) on the OntoNotes data.
Table 6 shows the performance details for all the
languages across all 18 name types broken down
by genre. In English, BN has the highest perfor-
mance followed by the NW genre. There is a sig-
nificant drop from those and the TC and WB genre.
Somewhat similar trend is observed in the Chi-
nese data, with Arabic having the lowest scores.
Since the Pivot Text portion (PT) of OntoNotes
was not tagged with names, we could not com-
pute the accuracy for that cross-section of the data.
Previously Finkel and Manning (2009) performed
17The number of sentences in this table are a subset of the
ones in the table showing parser performance, since these are
the sentences for which at least one predicate has been tagged
with its arguments
18http://www.lsi.upc.es/?srlconll/srl-eval.pl
19The system could not not use the morphology features in
Diab et al (2008).
20http://nlp.stanford.edu/software/CRF-NER.shtml
a joint estimation of named entity and parsing.
However, it was on an earlier version of the En-
glish portion of OntoNotes using a different cross-
section for training and testing and therefore is not
directly comparable.
4.5 Coreference
The task is to automatically identify mentions of
entities and events in text and to link the corefer-
ring mentions together to form entity/event chains.
The coreference decisions are made using auto-
matically predicted information on other structural
and semantic layers including the parses, seman-
tic roles, word senses, and named entities that
were produced in the earlier sections. Each docu-
ment part from the documents that were split into
multiple parts during coreference annotation were
treated as separate document.
We used the number and gender predictions
generated by Bergsma and Lin (2006). Unfortu-
nately neither Arabic, nor Chinese have compara-
ble data available. Chinese, in particular, does not
have number or gender inflections for nouns, but
(Baran and Xue, 2011) look at a way to infer such
information.
We trained the Bjo?rkelund and Farkas (2012)
coreference system21 which uses a combination of
two pair-wise resolvers, the first is an incremen-
tal chain-based resolution algorithm (Bjo?rkelund
and Farkas, 2012), and the second is a best-first
resolver (Ng and Cardie, 2002). The two resolvers
are combined by stacking, i.e., the output of the
first resolver is used as features in the second one.
The system uses a large feature set tailored for
each language which, in addition to classic coref-
erence features, includes both lexical and syntactic
information.
Recently, it was discovered that there is pos-
sibly a bug in the official scorer used for the
CoNLL 2011/2012 and the SemEval 2010 corefer-
ence tasks. This relates to the mis-implementation
of the method proposed by (Cai and Strube, 2010)
for scoring predicted mentions. This issue has also
been recently reported in Recasens et al, (2013).
As of this writing, the BCUBED metric has been
fixed, and the correctness of the CEAFm, CEAFe
and BLANC metrics is being verified. We will
be updating the CoNLL shared task webpages22
with more detailed information and also release
the patched scripts as soon as they are available.
We will also re-generate the scores for previous
shared tasks, and the coreference layer in this pa-
per and make them available along with the mod-
els and system outputs for other layers. Table
7 shows the performance of the system on the
21http://www.ims.uni-stuttgart.de/?anders/coref.html
22http://conll.cemantix.org
149
CoNLL-2012 test set, broken down by genre. The
same metrics that were used for the CoNLL-2012
shared task are computed, with the CONLL col-
umn being the official CONLL measure.
Language Genre MD MUC BCUBED CEAFm CEAFe BLANC CONLL
PREDICTED MENTIONS
English BC 73.43 63.92 61.98 54.82 42.68 73.04 56.19
BN 73.49 63.92 65.85 58.93 48.14 72.74 59.30
MZ 71.86 64.94 71.38 64.03 50.68 78.87 62.33
NW 68.54 60.20 65.11 57.54 45.10 73.72 56.80
PT 86.95 79.09 68.33 65.52 50.83 77.74 66.08
TC 80.81 76.78 71.35 65.41 45.44 82.45 64.52
WB 74.43 66.86 61.43 54.76 42.05 73.54 56.78
Overall 75.38 67.58 65.78 59.20 45.87 75.8 59.74
Chinese BC 68.02 59.6 59.44 53.12 40.77 73.63 53.27
BN 68.57 61.34 67.83 60.90 48.10 77.39 59.09
MZ 55.55 48.89 58.83 55.63 46.04 74.25 51.25
NW 89.19 80.71 73.64 76.30 70.89 82.56 75.08
TC 77.72 73.59 71.65 64.30 48.52 83.14 64.59
WB 72.61 65.79 62.32 56.71 43.67 77.45 57.26
Overall 66.37 58.61 66.56 59.01 48.19 76.07 57.79
Arabic NW 60.55 47.82 61.16 53.42 44.30 69.63 51.09
GOLD MENTIONS
English BC 85.63 76.09 68.70 61.73 49.87 76.24 64.89
BN 82.11 73.56 71.52 63.67 52.29 75.70 65.79
MZ 85.65 77.73 78.82 72.75 60.09 83.88 72.21
NW 80.68 73.52 73.08 65.63 51.96 81.06 66.19
PT 93.20 85.72 73.25 70.76 58.81 79.78 72.59
TC 90.68 86.83 78.94 73.87 56.26 85.82 74.01
WB 88.12 80.61 69.86 63.45 51.13 76.48 67.20
Overall 86.16 78.7 72.67 66.32 53.23 79.22 68.2
Chinese BC 84.88 76.34 69.89 62.02 49.29 76.89 65.17
BN 80.97 74.89 76.88 68.91 55.56 81.94 69.11
MZ 78.85 73.06 70.15 61.68 46.86 78.78 63.36
NW 93.23 86.54 86.70 80.60 76.60 85.75 83.28
TC 92.91 88.31 84.51 79.49 63.87 90.04 78.90
WB 85.87 77.61 69.24 60.71 47.47 77.67 64.77
Overall 83.47 76.85 76.30 68.30 56.61 81.56 69.92
Arabic NW 76.43 60.81 67.29 59.50 49.32 74.61 59.14
Table 7: Performance of the coreference system
on the CoNLL-2012 test set.
The varying results across genres mostly meet
our expectations. In English, the system does best
on TC and the PT genres. The text in the TC set
often involve long chains where the speakers re-
fer to themselves which, given speaker informa-
tion, is fairly easy to resolve. The PT section
includes many references to god (e.g. god and
the lord) which the lexicalized resolver is quite
good at picking up during training. The more dif-
ficult genres consist of texts where references to
many entities are interleaved in the discourse and
is as such harder to resolve correctly. For Chi-
nese the numbers on the TC genre are also quite
good, and the explanation above also holds here
? many mentions refer to either of the speak-
ers. For Chinese the NW section displays by far
the highest scores, however, and the reason for
this is not clear to us. Not surprisingly, restricting
the set of mentions only to gold mentions gives
a large boost across all genres and all languages.
This shows that mention detection (MD) and sin-
gleton detection (which is not part of the annota-
tion) remain a big source of errors for the coref-
erence resolver. For these experiments we used
a combination of training and development data
for training ? following the CoNLL-2012 shared
task specification. Leaving out the development
set has a very negligible effect on the CoNLL-
score for all the languages (English: 0.14; Chi-
nese 0.06; Arabic: 0.40 F-score respectively). The
effect on Arabic is the most (0.40 F-score) most
likely because of its much smaller size. To gauge
the performance improvement between 2011 and
2012 shared tasks, we performed a clean com-
parison of over the best performing system and
an earlier version of this system (Bjo?rkelund and
Nugues, 2011) on the CoNLL 2011 test set us-
ing the CoNLL 2011 train and development set
for training. The current system has a CoNLL
score of 60.09 (64.92+69.84+45.513 )23 as opposed tothe 54.53 reported in bjo?rkelund (Bjo?rkelund and
Nugues, 2011), and the 57.79 reported for the best
performing system of CoNLL-2011. One caveat
is that these score comparison are done using the
earlier version (v4) of the CoNLL scorer. Nev-
ertheless, it is encouraging to see that within a
short span of a year, there has been significant
improvement in system performance ? partially
owing to cross-pollination of research generated
through the shared tasks.
5 Conclusion
In this paper we reported work on finding a rea-
sonable training, development and test split for
the various layers of annotation in the OntoNotes
v5.0 corpus, which consists of multiple genres in
three typologically very different languages. We
also presented the performance of publicly avail-
able, state-of-the-art algorithms on all the different
layers of the corpus for the different languages.
The trained models as well as their output will
be made publicly available24 to serve as bench-
marks for language processing community. Train-
ing so many different NLP components is very
time-consuming, thus, we hope the work reported
here has lifted the burden of having to create rea-
sonable baselines for researchers who wish to use
this corpus to evaluate their systems. We created
just one data split in training, development and test
set, covering a collection of genres for each layer
of annotation in each language in order to keep the
workload manageable However, the results do not
discriminate the performance on individual gen-
res: we believe such a setup is still a more realistic
gauge for the performance of the state-of-the-art
NLP components than a monolithic corpus such
as the Wall Street Journal section of the Penn Tree-
bank. It can be used as a starting point for devel-
oping the next generation of NLP components that
are more robust and perform well on a multitude
of genres for a variety of different languages.
23(MUC + BCUBED + CEAFe)/3
24http://cemantix.org
150
6 Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022for sponsoring the creation of the OntoNotes
corpus. This work was partially supported
by grants R01LM10090 and U54LM008748
from the National Library Of Medicine, and
R01GM090187 from the National Institutes ofGeneral Medical Sciences. We are indebted toSlav Petrov for helping us to retrain his syntactic
parser for Arabic. Alessandro Moschitti and
Olga Uryupina have been partially funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under the grant
number 288024 (LIMOSINE). The content
is solely the responsibility of the authors and
does not necessarily represent the official views
of the National Institutes of Health. NianwenXue and Yuchen Zhang are supported in part
by the DAPRA via contract HR0011-11-C-0145
entitled ?Linguistic Resources for Multilingual
Processing.?
References
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Libin
Shen. 2006. Issues in synchronizing the English treebank
and propbank. In Workshop on Frontiers in Linguistically
Annotated Corpora 2006, July.
Elizabeth Baran and Nianwen Xue. 2011. Singular or plural?
exploiting parallel corpora for Chinese number prediction.
In Proceedings of Machine Translation Summit XIII, Xia-
men, China.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-
based pronoun resolution. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 33?40, Sydney, Australia, July.
Anders Bjo?rkelund and Richa?rd Farkas. 2012. Data-driven
multilingual coreference resolution using resolver stack-
ing. In Joint Conference on EMNLP and CoNLL - Shared
Task, pages 49?55, Jeju Island, Korea, July. Association
for Computational Linguistics.
Anders Bjo?rkelund and Pierre Nugues. 2011. Exploring lex-
icalized features for coreference resolution. In Proceed-
ings of the Fifteenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 45?50, Port-
land, Oregon, USA, June. Association for Computational
Linguistics.
Jie Cai and Michael Strube. 2010. Evaluation metrics for
end-to-end coreference resolution systems. In Proceed-
ings of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, SIGDIAL ?10, pages
28?36.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty elements. In
Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 212?216, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL), Ann Arbor, MI,
June.
Eugene Charniak and Mark Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings of the
Second Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Ann Arbor, MI,
June.
Mona Diab, Alessandro Moschitti, and Daniele Pighin. 2008.
Semantic role labeling systems for Arabic using kernel
methods. In Proceedings of ACL-08: HLT, pages 798?
806, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Gerard Escudero, Lluis Marquez, and German Rigau. 2000.
An empirical study of the domain dependence of super-
vised word disambiguation systems. In 2000 Joint SIG-
DAT Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages 172?
180, Hong Kong, China, October. Association for Com-
putational Linguistics.
Benoit Favre, Bernd Bohnet, and D. Hakkani-Tur. 2010.
Evaluation of semantic role labeling and dependency
parsing of automatic speech recognition output. In
Proceedings of 2010 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), page
5342?5345.
Jenny Rose Finkel and Christopher D. Manning. 2009. Joint
parsing and named entity recognition. In Proceedings of
Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 326?334, Boulder,
Colorado, June. Association for Computational Linguis-
tics.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into in-
formation extraction systems by Gibbs sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics, page 363?370.
Ryan Gabbard. 2010. Null Element Restoration. Ph.D. the-
sis, University of Pennsylvania.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In 2001 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2002. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of the Third LREC Conference, page
1974?1981.
Mohamed Maamouri and Ann Bies. 2004. Developing an
Arabic treebank: Methods, guidelines, procedures, and
tools. In Ali Farghaly and Karine Megerdoomian, edi-
tors, COLING 2004 Computational Approaches to Arabic
Script-based Languages, pages 2?9, Geneva, Switzerland,
August 28th. COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn treebank. Computational Linguis-
tics, 19(2):313?330, June.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceedings
of the Human Language Technology Conference/North
American Chapter of the Association for Computational
Linguistics (HLT/NAACL), New York City, NY, June.
151
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the Association for Computational Linguistics
(ACL-02), pages 104?111.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum.
2007. Making fine-grained and coarse-grained sense dis-
tinctions, both manually and automatically. Journal of
Natural Language Engineering, 13(2).
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona Diab,
Mohammed Maamouri, Aous Mansouri, and Wajdi Za-
ghouani. 2008. A pilot Arabic propbank. In Proceedings
of the International Conference on Language Resources
and Evaluation (LREC), Marrakech, Morocco, May 28-
30.
Slav Petrov and Dan Klein. 2007. Improved inferencing for
unlexicalized parsing. In Proc of HLT-NAACL.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Dan Jurafsky. 2005. Support
vector learning for semantic argument classification. Ma-
chine Learning, 60(1):11?39.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel, Jes-
sica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Indentifying entities and events in
OntoNotes. In Proceedings of the IEEE International
Conference on Semantic Computing (ICSC), September
17-19.
Sameer Pradhan, Wayne Ward, and James H. Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics Special Issue on Semantic Role Labeling,
34(2).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha
Palmer, Ralph Weischedel, and Nianwen Xue. 2011.
CoNLL-2011 shared task: Modeling unrestricted corefer-
ence in OntoNotes. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learning:
Shared Task, pages 1?27, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga
Uryupina, and Yuchen Zhang. 2012. CoNLL-2012 shared
task: Modeling multilingual unrestricted coreference in
OntoNotes. In Joint Conference on EMNLP and CoNLL -
Shared Task, pages 1?40, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki,
Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008.
The Penn discourse treebank 2.0. In Proceedings of the
Sixth International Conference on Language Resources
and Evaluation (LREC?08), Marrakech, Morocco, May.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena, M. Anto`nia
Mart??, Mariona Taule?, Ve?ronique Hoste, Massimo Poesio,
and Yannick Versley. 2010. Semeval-2010 task 1: Coref-
erence resolution in multiple languages. In Proceedings of
the 5th International Workshop on Semantic Evaluation,
pages 1?8, Uppsala, Sweden, July.
Marta Recasens, Marie-Catherine de Marneffe, and Christo-
pher Potts. 2013. The life and death of discourse enti-
ties: Identifying singleton mentions. In Proceedings of
the 2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, pages 627?633, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Libin Shen, Lucas Champollion, and Aravind K. Joshi. 2008.
LTAG-spinal and the treebank. Language Resources and
Evaluation, 42(1):1?19, March.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog no.:
LDC2005T33. BBN Technologies.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Robert Belvin, Sameer Pradhan, Lance Ramshaw,
and Nianwen Xue. 2011. OntoNotes: A large train-
ing corpus for enhanced processing. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language Ex-
ploitation. Springer.
Nianwen Xue and Martha Palmer. 2009. Adding semantic
roles to the Chinese Treebank. Natural Language Engi-
neering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer.
2005. The Penn Chinese TreeBank: phrase structure an-
notation of a large corpus. Natural Language Engineer-
ing, 11(2):207?238.
Nianwen Xue. 2008. Labeling Chinese predicates with se-
mantic roles. Computational Linguistics, 34(2):225?255.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese treebank.
In Proceedings of the 23rd International Conference on
Computational Linguistics (COLING), Beijing, China.
Wajdi Zaghouani, Mona Diab, Aous Mansouri, Sameer Prad-
han, and Martha Palmer. 2010. The revised Arabic prop-
bank. In Proceedings of the Fourth Linguistic Annotation
Workshop, pages 222?226, Uppsala, Sweden, July.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-
coverage word sense disambiguation system for free text.
In Proceedings of the ACL 2010 System Demonstrations,
pages 78?83, Uppsala, Sweden.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An empiri-
cal study. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 1002?
1010.
152
