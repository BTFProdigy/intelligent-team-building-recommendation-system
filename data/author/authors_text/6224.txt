Using Machine Learning to Maintain Rule-based Named-Entity
Recognition and Classification Systems
Georgios Petasis ?, Frantz Vichot ?, Francis Wolinski ?
Georgios Paliouras ?, Vangelis Karkaletsis ? and Constantine D. Spyropoulos ?
? Institute of Informatics and Telecommunications,
National Centre for Scientific Research ?Demokritos?,
153 10 Ag. Paraskevi, Athens, Greece
? Informatique-CDC
4, rue Berthollet
94114 Arcueil, France
{petasis,paliourg,vangelis,costass}@iit.demokritos.gr
{frantz.vichot, francis.wolinski}@caissedesdepots.fr
Abstract
This paper presents a method that as-
sists in maintaining a rule-based
named-entity recognition and classifi-
cation system. The underlying idea is to
use a separate system, constructed with
the use of machine learning, to monitor
the performance of the rule-based sys-
tem. The training data for the second
system is generated with the use of the
rule-based system, thus avoiding the
need for manual tagging. The dis-
agreement of the two systems acts as a
signal for updating the rule-based sys-
tem. The generality of the approach is
illustrated by applying it to large cor-
pora in two different languages: Greek
and French. The results are very en-
couraging, showing that this alternative
use of machine learning can assist sig-
nificantly in the maintenance of rule-
based systems.
1 Introduction
Machine learning has recently been proposed as
a promising solution to a major problem in lan-
guage engineering: the construction of lexical
resources. Most of the real-world language en-
gineering systems make use of a variety of lexi-
cal resources, in particular grammars and lexi-
cons. The use of general-purpose resources is
ineffective, since in most applications a special-
ised vocabulary is used, which is not supported
by general-purpose lexicons and grammars. For
this reason, significant effort is currently put
into the construction of generic tools that can
quickly adapt to a particular thematic domain.
The adaptation of these tools mainly involves
the adaptation of domain-specific semantic lexi-
cal resources.
Named-entity recognition and classification
(NERC) is the identification of proper names in
text and their classification as different types of
named entity (NE), e.g. persons, organisations,
locations, etc. This is an important subtask in
most language engineering applications, in par-
ticular information retrieval and extraction. The
lexical resources that are typically included in a
NERC system are a lexicon, in the form of gaz-
etteer lists, and a grammar, responsible for rec-
ognising the entities that are either not in the
lexicon or appear in more than one gazetteer
lists. The manual adaptation of those two re-
sources to a particular domain is time-
consuming and in some cases impossible, due to
the lack of experts. The exploitation of learning
techniques to support this adaptation task has
attracted the attention of researchers in language
engineering.
However, the adaptation of lexical resources
to a specific domain at a certain point in time is
not sufficient on its own. The performance of a
NERC system degrades over time (Vichot et al,
1999; Wolinski et al, 2000) due to the introduc-
tion of new NEs or the change in the meaning of
existing ones. We need to find ways that facili-
tate the maintenance of rule-based NERC sys-
tems. This paper presents such a method, ex-
ploiting machine learning in an innovative way.
Our method controls rule-based NERC systems
with NERC systems constructed by a machine
learning algorithm. The method comprises two
stages: the training stage, during which a super-
vised machine learning algorithm constructs a
new system using data generated by the rule-
based system, and the deployment stage, in
which the results of the two systems are com-
pared on new data and their disagreements are
used as signals for change in the rule-based sys-
tem. Note that, unlike most applications of su-
pervised machine learning, the training data for
the new system are not produced manually.
In order to illustrate the generality of this ap-
proach, we have tested it with two different
NERC systems, one for Greek and another one
for French. The results are very encouraging and
show that machine learning techniques can be
used for the maintenance of rule-based systems.
Section 2 presents existing work on the do-
main adaptation of NERC systems using ma-
chine learning (ML) techniques. Section 3 pre-
sents the two rule-based NERC systems for
Greek and French. Section 4 explains our
method and Section 5 describes the two experi-
ments and presents the evaluation results. Fi-
nally, Section 6 concludes and presents our fu-
ture plans.
2 Related Work
As mentioned above, the exploitation of learning
techniques to support the domain adaptation of
NERC systems has recently attracted the atten-
tion of several researchers. Some of these ap-
proaches are briefly discussed in this section.
Nymble (Bikel et al, 1997) uses statistical
learning to acquire a Hidden Markov Model
(HMM) that recognises NEs in text. Nymble did
particularly well in the MUC-7 competition
(DARPA, 1998), due mainly to the use of the
correct features in the encoding of words, e.g.
capitalisation, and the probabilistic modelling of
the recognition system.
Named-entity recognition in Alembic (Vilain
and Day, 1996) uses the transformation-based
rule learning approach introduced in Brill?s
work on part-of-speech tagging (Brill, 1993). An
important aspect of this approach is the fact that
the system learns rules that can be freely inter-
mixed with hand-engineered ones.
The RoboTag system presented in (Bennett
et al, 1997) constructs decision trees that clas-
sify words as being start or end points of a par-
ticular named-entity type. A variant of this ap-
proach was used in the system presented by the
New York University (NYU) in the Multilingual
Entity Task (MET-2) of MUC-7 (Sekine, 1998).
The system developed for Italian in ECRAN
(Cuchiarelli et al, 1998), uses unsupervised
learning to expand a manually constructed sys-
tem and improve its performance. The learning
algorithm tries to supplement the manually con-
structed system by classifying recognised but
unclassified NEs. In (Petasis et al, 2000) the
manually constructed system was replaced by
the supervised tree induction algorithm C4.5
(Quinlan, 1993), reaching very good perform-
ance on the MUC-6 corpora.
The partially supervised multi-level boot-
strapping approach presented in (Riloff and
Jones, 1999) induces a set of information extrac-
tion patterns, which can be used to identify and
classify NEs. The system starts by generating
exhaustively all candidate extraction patterns,
using an earlier system called AutoSlog (Riloff,
1993). Given a small number of seed examples
of NEs, the most useful patterns for recognising
the seed examples are selected and used to ex-
pand the set of classified NEs. The end result is
a dictionary of NEs and the extraction patterns
that correspond to them.
Our method follows an alternative innovative
approach to the use of learning for NERC. In-
stead of using ML to construct a NERC system
that will be used autonomously, the system con-
structed by ML, according to our approach is
used to monitor the performance of an existing
rule-based NERC system. In this manner, the
new system provides feedback on whether the
rule-based system under control has become
obsolete and needs to be updated. An important
advantage of this approach is that no manual
tagging of training data is needed, despite the
use of a supervised learning algorithm.
Our method bears some similarities with sys-
tems based on active learning (Thompson et al,
1999). According to this technique, multiple
classifiers performing the same task are used in
order to actively create training data, through
their disagreements. Usually, this involves an
iterative procedure. First a few initial labelled
examples are used to train the classifiers and
then, unlabelled examples are presented to the
classifiers. Examples that cause the classifiers to
disagree are good candidates to retrain the clas-
sifiers on. The difference of active learning to
our method is the use of a manually-constructed
rule-based NERC system as the basic system.
The ML method is used only to identify when
the rule-based NERC system should be updated,
but not for creating new training instances. An-
other approach, which bears some similarity to
ours, is presented in (Kushmerick, 1999) where
a heuristic algorithm is used to monitor the per-
formance of web-page wrappers.
3 Rule-based NERC Systems
A typical NERC system consists of a lexicon
and a grammar. The lexicon is a set of NEs that
are known beforehand and have been classified
into semantic classes. The grammar is used to
recognize and classify NEs that are not in the
lexicon and to decide upon the final classes of
NEs in ambiguous cases.
Manual construction of NERC systems is a
complicated and time-consuming process, even
for experts. The meaning of a single sentence
may vary a lot according to which category a
NE is assigned to. For example, the sentence
?Express group intends to sell Le Point for 700
MF? indicates a sale of a newspaper company, if
?Le Point? is classified as an organisation.
Whereas the following sentence, which is
grammatically identical to the previous one,
?Compagnie des Signaux intends to sell
TVM430 for 700 MF? gives only a price for an
industrial product.
In order for a NERC system to be able to
recognise and categorise correctly NEs, both the
lexicon and the grammar have to be validated on
large corpora, testing their efficiency and their
robustness. However, this process does not en-
sure that the performance of the developed sys-
tem will remain steady over time. Almost under
all thematic domains, the introduction of new
NEs or the change in the meaning of existing
ones can increase the error rate of the system.
Our approach tries to identify such cases, facili-
tating the maintenance of the NERC system.
The following subsections briefly describe
the Greek and French rule-based NERC systems
that have been used in our experiments.
3.1 The Greek NERC System
The Greek NERC system (Farmakiotou et al,
2000) used for the purposes of this experiment
forms part of a larger Greek information extrac-
tion system, being developed in the context of
the R&D project MITOS.1 The NERC compo-
nent of this system mainly consists of three
processing stages: linguistic pre-processing, NE
identification and NE classification. The linguis-
tic pre-processing stage involves some basic
tasks: tokenisation, sentence splitting, part-of-
speech tagging and stemming. Once the text has
been annotated with part of speech tags, a
stemmer is used. The aim of the stemmer is to
reduce the size of the lexicon as well as the size
and complexity of the NERC grammar.
The NE identification stage involves the de-
tection of their boundaries, i.e., the start and the
end of all the possible spans of tokens that are
likely to belong to a NE. Identification consists
of three sub-stages: initial delimitation, separa-
tion and exclusion. Initial delimitation involves
the application of general patterns. These pat-
terns are combinations of a limited number of
words, selected types of tokens (e.g. tokens con-
sisting of capital characters), special symbols
and punctuation marks. At the separation sub-
stage, possible NEs that are likely to contain
more than one NE or a NE attached to a non-
NE, are detected and attachment problems are
resolved. Finally, at the exclusion sub-stage two
types of criteria are used for exclusion from the
possible NE list: the context of the phrase and
being part of an exclusion list. Suggestive con-
text for exclusion consists of common names
that refer to products, services or artifacts. The
exclusion list includes capitalized abbreviations
of common nouns, financial terms, capitalized
person titles, which are not ambiguous, and
nouns commonly found in names of products,
artifacts and services.
Once the possible NEs have been identified,
the classification stage begins. Classification
involves three sub-stages: application of classi-
fication rules, gazetteer-based classification, and
partial matching of classified named-entities
with unclassified ones. Classification rules take
into account both internal and external evidence
(McDonald, 1996), i.e., the words and symbols
that comprise the possible name and the context
in which it occurs. Gazetteer-based classifica-
tion involves the look up of pre-stored lists of
known proper names (gazetteers). The gazet-
teers contain stemmed forms and have been
compiled from Web sites and an annotated train-
1 http://www.iit.demokritos.gr/skel/mitos
ing corpus. The size of the gazetteers is rather
small (3,059 names). At the partial matching
sub-stage, classified names are matched against
unclassified ones aiming at the recognition of
the truncated or variable forms of names.
3.2 The French NERC System
The French NERC system has been imple-
mented with the use of a rule-based inference
engine (Wolinski et al, 1995). It is based on a
large knowledge base (lexicon) including 8,000
proper names that share 10,000 forms and con-
sist of 11,000 words. It has been used continu-
ously since 1995 in several real-time document
filtering applications (Wolinski et al, 2000).
The uses of the NERC system in these applica-
tions are the following:
1. Segmentation of NEs, in order to improve
the performance of the syntactic analyser, par-
ticularly in the case of long proper names which
contain grammatical markers (e.g. prepositions,
conjunctions, commas, full stops).
2. Recognition of known NEs in order to sup-
ply precise information to a document filtering
module.
3. Classification of NEs in order to feed a
document filtering module with information
dealing with the very nature of the NEs quoted
in the documents.
The NERC system tries to classify each NE
in one of four different categories: association
(non-commercial organisation), person, location
or company.
For the classification of known entities, a
crucial problem appears when several NEs share
a single form. To deal with these cases, two sets
of rules have been implemented:
1. Local context: For instance, ?Saint-Louis?
may be interpreted in one of the following ways:
the capital of Missouri, a French group in the
food production industry, a small industry ?les
Cristalleries de Saint Louis?, a small town in
France, a hospital in Paris, etc. Exploration of
the local context using the proper name may
enable, in certain cases, a choice to be made
between these various interpretations. If the text
speaks of ?St-Louis (Missouri)?, only the first
interpretation should be adopted. In order to do
this the knowledge base should contain informa-
tion that ?Saint-Louis? is in Missouri, and a rule
should exist to interpret the affixing of a paren-
thesis.
2. Global context: Abbreviated NEs and acro-
nyms are much more frequent sources of ambi-
guity and are almost always common to several
NEs. In general, such ambiguous forms of NEs
do not occur on their own in news but almost
always together with non-ambiguous forms that
enable the ambiguity to be removed. For in-
stance, if the NEs ?Saint-Louis? and ?H?pital
Saint-Louis? appear in a single news item, the
interpretation corresponding to the hospital is
more likely to be the one that should be adopted.
For unknown entities, three sets of rules have
been implemented:
1. Prototypes: Many NEs are constructed ac-
cording to some prototypes. These can be cate-
gorised using pattern matching rules. Mr Andr?
Blavier, Kyocera Corp, Cond?-sur-Huisne,
Honda Motor, IBM-Asia, Bernard Tapie
Finance, Siam Nissan Automobile Co Ltd are
good examples of such prototypes.
2. Local context: Many single-word unknown
NEs (some known NEs as well) may also be
categorised using the local context. For instance,
the small sentences ?Peskine, director of the
group?, ?the shareholders of Fibaly ? or ?the
mayor of Gisenyi? are used as categorisation
rules.
3. Global context: After the first appearance of
a NE in full, its head (e.g. family name, main
company) is often used alone in the text instead
of the full name. The company Kyocera Corp,
for example, may be designated by the single
word Kyocera in the remainder of the text. For
each such unknown word, starting with a capital
letter, a special rule examines whether it appears
inside another NE in the text.
4 Controlling a Rule-based System Us-
ing Machine Learning
Machine learning has been used successfully to
control a rule-based system that performs a dif-
ferent task, namely document filtering (Wolinski
et al, 2000). The learning method used in that
case was a neural network (Stricker et al, 2001).
In our present study, we control the rule-
based NERC systems that have been presented
in section 3, with NERC systems constructed by
the C4.5 algorithm. Our method comprises two
stages: the training stage, during which C4.5
constructs a new system using data generated by
the rule-based system, and the deployment stage,
in which the results of the two systems are com-
pared on new data and their disagreements are
used as signals for change in the rule-based sys-
tem. This section describes the basic principles
of our control method.
4.1 Control method: training stage
The training stage of our method consists of the
following processing steps (Figure 1):
Running the rule-based NERC system on a
large training corpus (containing several thou-
sands of NEs in our case). The aim of this proc-
ess is to recognise and classify the NEs in the
corpus. The end product is a set of NEs, associ-
ated with their class.
Constructing a separate NERC system by ap-
plying C4.5 on the data generated by the rule-
based system. In this process, the classified NEs
are used as training data by C4.5, in order to
construct the second NERC system (trained
NERC). For each classified NE a training exam-
ple (vector) is created, containing information
about the part of speech and gazetteer tags of the
first and the last two words of the NE, as well as
the two words preceding and the two following
the NE. It is important to note that, unlike other
uses of supervised machine learning methods,
this approach does not require manual tagging of
training data.
Training
Corpus
Rule-based
NERC
Training
Data
C4.5 Trained
NERC
Figure 1: Training stage.
4.2 Control method: deployment stage
In the deployment stage, the two NERC systems
are compared on a new corpus to identify dis-
agreements. Despite the fact that the second
method is trained on data generated by the first,
the different nature of the NERC system gener-
ated by C4.5, i.e., a decision tree, leads to inter-
esting disagreements between the two methods.
The deployment stage consists of the following
processing steps (Figure 2):
1. Running the rule-based NERC system on a
new corpus. It should be stressed here that the
documents in this corpus differ in some charac-
teristic way from those in the training corpus. In
our experiments the difference is chronological,
i.e., the new corpus consists of recent news arti-
cles. The reason for adopting this approach is
that we are interested in the maintenance of a
rule-based system through time. An alternative
approach might be for the new corpus to be from
a slightly different thematic domain. In that
case, the goal of the process would be the cus-
tomisation of the rule-based system to a new
domain.
2. Running the trained NERC system on the
same corpus.
3. Comparing the results provided by both sys-
tems to identify cases of disagreement. The re-
sult is a set of data where the two systems dis-
agree: in our case, disagreements deal with the
different categories assigned by the NERC sys-
tems to NEs (see Section 5 for detailed results).
These cases are then provided to the language
engineer, who needs to evaluate them and de-
cide on changes for the rule-based system.
New
Corpus
Rule-based
NERC
Cases of
disagree
ment
Identify
disagree
ments
Trained
NERC
Figure 2: Deployment stage.
5 Results
In order to evaluate the proposed method, two
different experiments were contacted, one for
each language. The exact experimental settings
as well as the evaluation results are presented in
the following sections.
5.1 Results for the Greek System
For the experiment regarding the Greek lan-
guage, we used three NE classes: organisations,
persons and locations. For the purposes of the
experiment, two corpora of financial news were
used.2 The first corpus that was used for training
purposes, consisted of 5,000 news articles from
the years 1996 and 1997, containing 10,010
instances of NEs (1,885 persons, 1,781 loca-
tions, 6,344 organisations). The second corpus
2 The corpora were provided by the Greek publishing com-
pany Kapa-TEL.
that was used for evaluation purposes consisted
of 5,779 news from the years 1999 and 2000 and
contained 11,786 instances of NEs (1,137 per-
sons, 810 locations, 9,839 organisations).
5.1.1 Aggregate Results
A good way to give an overview of the cases of
disagreement of the two systems is through a
contingency matrix, as shown in Table 1. The
rows of this table correspond to the classifica-
tion of the rule-based system, while the columns
to the classification of the system constructed by
C4.5.
Table 1: Overview of the results for Greek.
organisation. person location
organisation 9,906 250 32
person 230 649 14
location 24 6 675
As we can see from Table 1, in 95% of the cases
the two systems are in agreement. This means,
that in order to update the rule-based NERC
system, we have to examine only 5% of the
cases, where the two systems disagree. Examin-
ing these cases gave us important insight regard-
ing problems of the rule-based NERC system.
Some examples are presented in the following
sections.
5.2 Recognition problems
The examination of cases in disagreement re-
vealed some interesting problems regarding NE
recognition. These problems concern NEs that
the rule-based system identified only partially
and as a result classified them incorrectly.
For example, in the stage of initial delimita-
tion, the general patterns fail to identify NEs that
contain numbers in their names, like the organi-
sation ?????? 2004? (Athens 2004) represent-
ing the organising committee of 2004 Olympics.
In addition, during the separation phase some
of the rules have not taken into account some
inflexional endings, causing failures in separat-
ing some NEs. For example, in the phrase ?? ??.
?????????? ?. ????????? (the under-secretary
of Culture ?. ????????) the recogniser failed to
separate the person name from its title, due to
the last accented character of the word ???????-
?????.
Finally, we were able to locate several stop-
words and update our exclusion list. For in-
stance, the phrase ???????? ISDN? (ISDN
lines) was recognised as an organisation (as the
word ????????? is a frequent constituent of
airline or shipping companies), but in reality the
text was referring to ISDN telephone lines.
5.2.1 Classification problems
Except from the problems identified in the rec-
ognition phase, the examination of the cases of
disagreement revealed various problems regard-
ing mainly the classification grammar. In fact,
some of our classification rules were found to be
too general, leading to wrong classifications.
For example, according to one of the rules, a
sequence of two words, starting with capital
letters, constitutes a person name if it is pre-
ceded by a definite article and the endings of
these two words belong in a specific set that
usually denote person names. This rule caused
the classification of various non-NEs as persons,
including ???? ?????????? ??????? (the
Olympic Village).
Another example of an overly general rule is
a rule that classifies a sequence of abbreviations
or nouns starting with capital letter as an organi-
sation, if this sequence is preceded by a comma
that in turn is preceded by a NE already classi-
fied as an organisation. This rule caused the
classification of few person names as organisa-
tions, such as ?? ????????? ??? ??????? ?????-
???, ?.????????? (the director of National
Bank, ?.????????).
5.3 Results for the French System
The corpus used for the French experiment con-
tained dispatches from the Agence France-
Presse from April 1998 until January 2001. The
thematic domain of the corpus was shareholding
events. This corpus contained six thousand
documents, including 180,983 instances of NEs
with the following distribution: companies
(45%), locations (45%), persons (7%) and asso-
ciations (non commercial organisations) (3%).
For the purposes of this experiment, the corpus
was chronologically split in two parts. The part
containing the chronologically earlier messages
was used for training purposes while the second
part, containing the most recent messages, was
used in order to evaluate our approach. In this
experiment, we mainly focused on four NE
categories, instead of the three categories used
for the Greek experiment. This differentiation
originates in the fact that the French NERC sys-
tem further categorises organisations into asso-
ciations (non-profit organisations) and compa-
nies.
5.3.1 Aggregate Results
The contingency matrix giving an overview of
the cases of disagreement of the two systems is
shown in Table 2. It appears that in 91% of the
cases the two systems are in agreement.
Table 2: Overview of the results for French.
associat. person location company
associat. 808 6 31 618
person 3 4,498 46 509
location 11 51 6,870 2,526
company 296 67 534 34,946
Examining the disagreement cases gave us im-
portant insight regarding problems of the rule-
based system. The following sections present
some interesting examples.
5.3.2 Recognition problems
Similarly to the Greek experiment, the examina-
tion of disagreements revealed some interesting
problems in the recognition of NEs. For in-
stance, ?Europe 1? is a well-known French radio
station, also written sometimes as ?Europe Un?
(Europe One). The rule-based system failed to
identify ?Europe Un? and only identified
?Europe? as a location. The source of the prob-
lem is the lack of a mapping between fully writ-
ten numbers and numerical figures.
Another example is the phrase ?Le Mans
Re?, which is a shortened version of the com-
pany name ?Les mutuelles du Mans
Reassurance? (a Reinsurance company). The
rule-based system recognised only ?Le Mans? as
a location, due to the well-known French city.
What is needed here is an extension of the seg-
mentation rules to include ?Re? as a ?company
designator?, such as ?Motor?, ?Bank? or ?Tele-
com?.
5.3.3 Classification problems
Most of the classification problems that were
identified concerned NEs already known to the
system that meanwhile have acquired new
meanings. For example, ?Ariane II rach?te?
(Ariane II buys) is classified as a person, due to
the word ?Ariane? contained in the lexicon as a
person forename. In reality, ?Ariane II? is a new
company that should also be included in the
lexicon database. Another example is ?Orange?
already included in the lexicon as an old French
city. In the meanwhile, a new French company
has been created having the same name, as in
the example ?Orange, valoris?e par les analys-
tes? (Orange, estimated by analysts). Also in this
case, the lexicon must be updated with a second
entry for this entity, categorised as a company.
Besides lexicon omissions, some problems
regarding the classification grammar were also
revealed. First, overly general rules were identi-
fied, such as the one that classifies entities start-
ing from ?A? and followed by numbers as
French highway names. This rule wrongly clas-
sified the NE ?A3XX? as a highway, while the
text was referring to an airplane model:
?L?A3XX, un avion? (The A3XX, an air plane).
Our approach also succeeded in locating
well-known NEs used in a new context. For
example, the rule-based NERC system recog-
nises ?Taittinger? as a company while the sys-
tem learned by C4.5 disagrees with this classifi-
cation in the sentence ?la famille Taittinger? (the
family Taittinger). In this case, the grammar
should be updated with a rule saying that the
word ?family? in front of a proper name sug-
gests a person name.
6 Conclusions
In this paper, we have proposed an alternative
use of machine learning in named-entity recog-
nition and classification. Instead of constructing
an autonomous NERC system, the system con-
structed with the use of machine learning assists
in the maintenance of a rule-based NERC sys-
tem. An important feature of the approach is the
use of a supervised learning method, without the
need for manual tagging of training data. The
proposed approach was evaluated with success
for two different languages: Greek and French.
On-going work aims at reducing the number
of disagreements between the two systems down
to those that are essential for the improvement
of the system. Currently, there are many cases
where the two systems disagree, but the rule-
based system is correct.
Another extension that we are examining is
to train a NERC system to not only classify, but
also recognise NEs. We believe that this exten-
sion will lead to the identification of more prob-
lematic cases in the recognition phase.
In conclusion, the method presented in this
paper proposes a simple and effective use of
machine learning for the maintenance of rule-
based systems. The scope of this approach is
clearly wider than that examined here, i.e.,
named-entity recognition.
Acknowledgements
This research has been carried out thanks to the
Hellenic ? French scientific cooperation project
ADIET (PLATON no. 00521 TH). It also used
results of the Greek R&D project MITOS
(EPET II ? 1.3 ? 102).
References
Bennett S.W., Aone C. and Lovell C., 1997. Learning
to Tag Multilingual Texts through Observation.
Proc. of the Second Conference on Empirical
Methods in NLP, pp. 109-116.
Bikel D., Miller S., Schwartz R. and Weischedel R.,
1997. Nymble: a High-Performance Learning
Name-finder. Proc. of 5th Conference on Applied
natural Language Processing, Washington.
Defense Advanced Research Projects Agency, 1998.
Proc. of the Seventh Message Understanding Con-
ference (MUC-7), Morgan Kaufmann.
Brill E., 1993. A corpus-based approach to language
learning. PhD Dissertation, Univ. of Pennsylvania.
Cuchiarelli A., Luzi D., and Velardi P., 1998. Auto-
matic Semantic Tagging of Unknown Proper
Names. Proc. of COLING-98, Montreal.
Farmakiotou D., Karkaletsis V., Koutsias J., Sigletos
G., Spyropoulos C.D. and Stamatopoulos P., 2000.
Rule-based Named Entity Recognition for Greek
Financial Texts. Proc. of the Workshop on Compu-
tational lexicography and Multimedia Dictionaries
(COMLEX 2000), pp. 75-78.
Kushmerick N., 1999. Regression testing for wrapper
maintenance. Proc. of National Conference on Ar-
tificial Intelligence, pp. 74-79.
McDonald D., 1996. Internal and External Evidence
in the Identification and Semantic Categorization
of Proper Names. In B. Boguraev & J. Pustejovski
(eds.) Corpus Processing for Lexical Acquisition,
MIT Press, pp 21?39.
Petasis G., Cucchiarelli A., Velardi P., Paliouras G.,
Karkaletsis V., Spyropoulos C.D., 2000. Automatic
adaptation of Proper Noun Dictionaries through
cooperation of machine learning and probabilistic
methods. Proc. of ACM SIGIR-2000, Athens,
Greece.
Quinlan J. R., 1993. C4.5: Programs for machine
learning. Morgan-Kaufmann, San Mateo, CA.
Riloff E., 1993. Automatically Constructing a Dic-
tionary for Information Extraction Tasks. Proc. of
the National Conference on Artificial Intelligence,
pp. 811-816.
Riloff E. and Jones R., 1999. Learning Dictionaries
for Information Extraction by Multi-Level Boot-
strapping. Proc. of the National Conference on Ar-
tificial Intelligence, pp. 474-479.
Sekine, S., 1998. NYU: Description of the Japanese
NE System used for MET-2. Proc. of the Seventh
Message Understanding Conference (MUC-7).
Stricker M., Vichot F., Dreyfus G., Wolinski F.,
2001. Training Context-sensitive Neural Networks
with few Relevant Examples for TREC-9 Routing.
In Text Retrieval Conference, TREC-9, NIST Spe-
cial Publication, Gaithersburg, USA, to appear.
Thompson C., Califf M., Mooney R., 1999. Active
Learning for Natural Language Parsing and Infor-
mation Extraction. Proc. of the International Con-
ference on Machine Learning, pp. 406-414.
Vichot F., Wolinski F., Ferri H. C., Urbani D., 1999.
Using Information Extraction for Knowledge En-
tering, In Advances in Intelligent Systems - Con-
cepts, Tools and Applications, S. G. Tzafestas
(Ed.), Kluwer academic publishers, Dordrecht, The
Netherlands, pp. 191-200.
Vilain M., and Day D., 1996. Finite-state phrase
parsing by rule sequences. Proc. of COLING-96,
vol. 1, pp. 274-279.
Wolinski F., Vichot F., Dillet B., 1995. Automatic
Processing of Proper Names in Texts. In European
Chapter of the Association for Computer Linguis-
tics, EACL, Dublin, Ireland, pp.23-30.
Wolinski F., Vichot F., Stricker M., 2000. Using
Learning-based Filters to Detect Rule-based Filter-
ing Obsolescence. In Recherche d? Information
Assist?e par Ordinateur, RIAO, Paris, France,
pp.1208-1220.
In Proceedings of the 6th Conference on Empirical Methods in Natural Language Processing (EMNLP 2001), L. 
Lee and D. Harman (Eds.), pp. 44?50, Carnegie Mellon University, Pittsburgh, PA, USA, 2001. 
Stacking classifiers for anti-spam filtering of e-mail 
 
Georgios Sakkis?, Ion Androutsopoulos?, Georgios Paliouras?, Vangelis Karkaletsis?,  
Constantine D. Spyropoulos?, and Panagiotis Stamatopoulos? 
 
?Department of Informatics  
University of Athens 
TYPA Buildings, Panepistimiopolis  
GR-157 71 Athens, Greece 
e-mail: {stud0926, 
T.Stamatopoulos}@di.uoa.gr 
 
?Software and Knowledge Engineering 
Laboratory 
Institute of Informatics and Telecommunications 
National Centre for Scientific Research 
?Demokritos? 
GR-153 10 Ag. Paraskevi, Athens, Greece 
e-mail: {ionandr, paliourg, vangelis, 
costass}@iit.demokritos.gr 
 
 
Abstract 
We evaluate empirically a scheme for 
combining classifiers, known as stacked 
generalization, in the context of anti-spam 
filtering, a novel cost-sensitive application of 
text categorization. Unsolicited commercial e-
mail, or ?spam?, floods mailboxes, causing 
frustration, wasting bandwidth, and exposing 
minors to unsuitable content. Using a public 
corpus, we show that stacking can improve the 
efficiency of automatically induced anti-spam 
filters, and that such filters can be used in real-
life applications. 
Introduction 
This paper presents an empirical evaluation of 
stacked generalization, a scheme for combining 
automatically induced classifiers, in the context 
of anti-spam filtering, a novel cost-sensitive 
application of text categorization.  
The increasing popularity and low cost of e-
mail have intrigued direct marketers to flood the 
mailboxes of thousands of users with unsolicited 
messages, advertising anything, from vacations 
to get-rich schemes. These messages, known as 
spam or more formally Unsolicited Commercial 
E-mail, are extremely annoying, as they clutter 
mailboxes, prolong dial-up connections, and 
often expose minors to unsuitable content 
(Cranor & Lamacchia, 1998).  
Legal and simplistic technical counter-
measures, like blacklists and keyword-based 
filters, have had a very limited effect so far.1 The 
success of machine learning techniques in text 
categorization (Sebastiani, 2001) has recently 
led to alternative, learning-based approaches 
(Sahami, et al 1998; Pantel & Lin, 1998; 
Drucker, et al 1999). A classifier capable of 
distinguishing between spam and non-spam, 
hereafter legitimate, messages is induced from a 
manually categorized learning collection of 
messages, and is then used to identify incoming 
spam e-mail. Initial results have been promising, 
and experiments are becoming more systematic, 
by exploiting recently introduced benchmark 
corpora, and cost-sensitive evaluation measures 
(Gomez Hidalgo, et al 2000; Androutsopoulos, 
et al 2000a, b, c). 
Stacked generalization (Wolpert, 1992), or 
stacking, is an approach for constructing 
classifier ensembles. A classifier ensemble, or  
committee, is a set of classifiers whose 
individual decisions are combined in some way 
to classify new instances (Dietterich, 1997). 
Stacking combines multiple classifiers to induce 
a higher-level classifier with improved 
performance. The latter can be thought of as the 
president of a committee with the ground-level 
classifiers as members. Each unseen incoming 
message is first given to the members; the 
president then decides on the category of the 
                                                     
1 Consult www.cauce.org, spam.abuse.net, and 
www.junkemail.org. 
 message by considering the opinions of the 
members and the message itself.  Ground-level 
classifiers often make different classification 
errors. Hence, a president that has successfully 
learned when to trust each of the members can 
improve overall performance. 
We have experimented with two ground-
level classifiers for which results on a public 
benchmark corpus are available: a Na?ve Bayes 
classifier (Androutsopoulos, et al 2000a, c) and 
a memory-based classifier (Androutsopoulos, et 
al. 2000b; Sakkis, et al 2001). Using a third, 
memory-based classifier as president, we 
investigated two versions of stacking and two 
different cost-sensitive scenarios. Overall, our 
results indicate that stacking improves the 
performance of the ground-level classifiers, and 
that the performance of the resulting anti-spam 
filter is acceptable for real-life applications.  
Section 1 below presents the benchmark 
corpus and the preprocessing of the messages; 
section 2 introduces cost-sensitive evaluation 
measures; section 3 provides details on the 
stacking approaches that were explored; section 
4 discusses the learning algorithms that were 
employed and the motivation for selecting them; 
section 5 presents our experimental results 
followed by conclusions. 
1 Benchmark corpus and 
preprocessing  
Text categorization has benefited from public 
benchmark corpora. Producing such corpora for 
anti-spam filtering is not straightforward, since 
user mailboxes cannot be made public without 
considering privacy issues. A useful public 
approximation of a user?s mailbox, however, can 
be constructed by mixing spam messages with 
messages extracted from spam-free public 
archives of mailing lists. The corpus that we 
used, Ling-Spam, follows this approach 
(Androutsopoulos, et al 2000a, b; Sakkis, et al 
2001). It is a mixture of spam messages and 
messages sent via the Linguist, a moderated list 
about the science and profession of linguistics. 
The corpus consists of 2412 Linguist messages 
and 481 spam messages. 
Spam messages constitute 16.6% of Ling-
Spam, close to the rates reported by Cranor and 
LaMacchia (1998), and Sahami et al (1998). 
Although the Linguist messages are more topic-
specific than most users? e-mail, they are less 
standardized than one might expect. For 
example, they contain job postings, software 
availability announcements and even flame-like 
responses. Moreover, recent experiments with an 
encoded user mailbox and a Na?ve Bayes (NB) 
classifier (Androutsopoulos, et al 2000c) 
yielded results similar to those obtained with 
Ling-Spam (Androutsopoulos, et al 2000a). 
Therefore, experimentation with Ling-Spam can 
provide useful indicative results, at least in a 
preliminary stage. Furthermore, experiments 
with Ling-Spam can be seen as studies of anti-
spam filtering of open unmoderated lists.  
Each message of Ling-Spam was converted 
into a vector nxxxxx ,,,, 321 h

= , where 
nxx ,,1   are the values of attributes 
nXX ,,1 h . Each attribute shows if a particular 
word (e.g. ?adult?) occurs in the message. All 
attributes are binary: 1=iX  if the word is 
present; otherwise 0=iX . To avoid treating 
forms of the same word as different attributes, a 
lemmatizer was applied, converting each word 
to its base form. 
To reduce the dimensionality, attribute 
selection was performed. First, words occurring 
in less than 4 messages were discarded. Then, 
the Information Gain (IG) of each candidate 
attribute X  was computed: 
)()(
),(log),(),(
},{},1,0{ cPxP
cxPcxPCXIG
legitspamcx ?
?=
?
??
 
The attributes with the m highest IG-scores were 
selected, with m corresponding to the best 
configurations of the ground classifiers that have 
been reported for Ling-Spam (Androutsopoulos, 
et al 2000a; Sakkis, et al 2001); see Section 4.  
2 Evaluation measures 
Blocking a legitimate message is generally more 
severe an error than accepting a spam message. 
Let SL ?  and LS ?  denote the two error 
types, respectively, and let us assume that 
SL ?  is ? times as costly as LS ? .  
Previous research has considered three cost 
scenarios, where ? = 1, 9, or 999 
 (Androutsopoulos, et al 2000a, b, c; Sakkis, et 
al. 2001).  In the scenario where ? = 999, 
blocked messages are deleted immediately. 
SL ?  is taken to be 999 times as costly as 
LS ? , since most users would consider losing 
a legitimate message unacceptable. In the 
scenario where ? = 9, blocked messages are 
returned to their senders with a request to resend 
them to an unfiltered address. In this case, 
SL ?  is penalized more than LS ? , to 
account for the fact that recovering from a 
blocked legitimate message is more costly 
(counting the sender?s extra work) than 
recovering from a spam message that passed the 
filter (deleting it manually). In the third scenario, 
where ? = 1, blocked messages are simply 
flagged as possibly spam. Hence, SL ?  is no 
more costly than LS ? . Previous experiments 
indicate that the Na?ve Bayes ground-classifier 
is unstable when ? = 999 (Androutsopoulos, et 
al. 2000a). Hence, we have considered only the 
cases where ? = 1 or 9.  
Let )(xWL   and )(xWS   be the confidence of 
a classifier (member or president) that message 
x  is legitimate and spam, respectively. The 
classifier classifies x  as spam iff:  
?>)(
)(
xW
xW
L
S


 
If )(xWL   and )(xWS   are accurate estimates of 
)|( xlegitP   and )|( xspamP  , respectively, the 
criterion above achieves optimal results (Duda 
& Hart, 1973).  
To measure the performance of a filter, 
weighted accuracy (WAcc) and its 
complementary weighted error rate (WErr = 1 ?  
WAcc) are used (Androutsopoulos, et al 2000a, 
b, c; Sakkis, et al 2001): 
SL
SSLL
NN
NNWAcc
+??
+??
=
??  
where ZYN ?  is the number of messages in 
category Y  that the filter classified as Z ,  
SLLLL NNN ?? += ,  LSSSS NNN ?? += . 
That is, when a legitimate message is blocked, 
this counts as ? errors; and when it passes the 
filter, this counts as ? successes.  
We consider the case where no filter is 
present as our baseline: legitimate messages are 
never blocked, and spam messages always pass. 
The weighted accuracy of the baseline is: 
SL
Lb
NN
NWAcc
+??
??
=    
The total cost ratio (TCR) compares the 
performance of a filter to the baseline: 
LSSL
S
b
NN
N
WErr
WErrTCR
??
+?
==
?
 
Greater TCR values indicate better performance. 
For TCR < 1, not using the filter is better.  
Our evaluation measures also include spam 
recall (SR) and spam precision (SP): 
LSSS
SS
NN
NSR
??
?
+
=   
SLSS
SS
NN
NSP
??
?
+
=  
SR measures the percentage of spam messages 
that the filter blocks (intuitively, its 
effectiveness), while SP measures how many 
blocked messages are indeed spam (its safety). 
Despite their intuitiveness, comparing different 
filter configurations using SR and SP is difficult: 
each configuration yields a pair of SR and SP 
results; and without a single combining measure, 
like TCR, that incorporates the notion of cost, it 
is difficult to decide which pair is better. 
In all the experiments, stratified 10-fold 
cross-validation was used. That is, Ling-Spam 
was partitioned into 10 equally populated parts, 
maintaining the original spam-legitimate ratio.  
Each experiment was repeated 10 times, each 
time reserving a different part jS  (j = 1, ?, 10) 
for testing, and using the remaining 9 parts as 
the training set jL .  
3 Stacking  
In the first version of stacking that we explored 
(Wolpert, 1992), which we call cross-validation 
stacking, the training set of the president was 
prepared using a second-level 3-fold cross-
validation. Each training set jL  was further 
partitioned into three equally populated parts, 
and the training set of the president was 
prepared in three steps. At each step, a different 
part iLS  (i = 1, 2, 3) of jL  was reserved, and 
 the members were trained on the union iLL  of 
the other two parts. Each mxxx ,,1  =  of 
iLS  was enhanced with the members? 
confidence )(1 xWS
  and )(2 xWS
  that x  is spam, 
yielding an enhanced 'iLS  with vectors 
)(),(,,,' 211 xWxWxxx SSm  = . At the end of 
the 3-fold cross-validation, the president was 
trained on '''' 321 LSLSLSLj = . It was then 
tested on jS , after retraining the members on 
the entire jL  and enhancing the vectors of jS  
with the predictions of the members.   
The second stacking version that we 
explored, dubbed holdout stacking, is similar to 
Kohavi?s (1995) holdout accuracy estimation. It 
differs from the first version, in two ways: the 
members are not retrained on the entire jL ; and 
each partitioning of jL  into iLL  and iLS  leads 
to a different president, trained on 'iLS , which 
is then tested on the enhanced jS . Hence, there 
are 103?  presidents in a 10-fold experiment, 
while in the first version there are only 10. In 
each case, WAcc is averaged over the presidents, 
and TCR is reported as WErrb over the average 
WErr.    
Holdout stacking is likely to be less effective 
than cross-validation stacking, since its 
classifiers are trained on smaller sets. 
Nonetheless, it requires fewer computations, 
because the members are not retrained. 
Furthermore, during classification the president 
consults the same members that were used to 
prepare its training set. In contrast, in cross-
validation stacking the president is tested using 
members that have received more training than 
those that prepared its training set. Hence, the 
model that the president has acquired, which 
shows when to trust each member, may not 
apply to the members that the president consults 
when classifying incoming messages.   
4 Inducers employed  
As already mentioned, we used a Na?ve Bayes 
(NB) and a memory-based learner as members 
of the committee (Mitchell 1997; Aha, et al 
1991). For the latter, we used TiMBL, an 
implementation of the k-Nearest Neighbor 
algorithm (Daelemans, et al 2000).  
With NB, the degree of confidence )(xWS
  
that x  is spam is:  
== )|()( xspamPxW NBS
  
?
?
?
?
=
=
?
?
=
},{ 1
1
)|()(
)|()(
legitspamk
m
i
i
m
i
i
kxPkP
spamxPspamP
 
NB assumes that mXX ,,1   are conditionally 
independent given the category (Duda & Hart, 
1973). 
With k-NN, a distance-weighted method is 
used, with a voting function analogous to the 
inverted cube of distance (Dudani 1976). The k 
nearest neighbors ix  of x  are considered: 
?
?
=
=
?
?
= k
i
i
k
i
ii
NNk
S
xxd
xxdxCspam
xW
1
3
1
3
),(1
),())(,(
)(


 , 
where )( ixC   is the category of neighbor ix , 
),( ji xxd   is the distance between ix  and jx ,  
and 1),( 21 =cc? , if 21 cc = , and 0 otherwise.  
This formula weighs the contribution of each 
neighbor by its distance from the message to be 
classified, and the result is scaled to [0,1]. The 
distance is computed by an attribute-weighted 
function (Wettschereck, et al 1995), employing 
Information Gain (IG):  
),(),(
1
j
r
i
r
n
t
tji xxIGxxd ?
=
?? ?
 ,  
where imii xxx ,,1 l =  , jmjj xxx ,,1 l = , and 
tIG  is the IG score of tX  (Section 1). 
In Tables 1 and 2, we reproduce the best 
performing configurations of the two learners on 
Ling-Spam (Androutsopoulos, et al 2000b; 
Sakkis, et al 2001). These configurations were 
used as members of the committee. 
The same memory-based learner was used as 
the president. However, we experimented with 
several configurations, varying the 
neighborhood size (k) from 1 to 10, and 
 providing the president with the m  best word-
attributes, as in Section 1, with m  ranging from 
50 to 700 by 50. The same attribute- and 
distance-weighting schemes were used for the 
president, as with the ground-level memory-
based learner. 
 
? m SR  SP ?CR 
1 100 82.4% 99.0% 5.41 
9 100 77.6% 99.5% 3.82 
 
? k m SR  SP ?CR 
1 8 600 88.6% 97.4% 7.18 
9 2 700 81.9% 98.8% 3.64 
  
? true class 
only one 
fails both fail
Legitimate 0.66% 0.08%
Spam 12.27% 8.52%1 
All 2.59% 1.49%
Legitimate 0.33% 0.08%
Spam 19.12% 10.19%9 
All 3.46% 1.76%
 
Our motivation for combining NB with k-NN 
emerged from preliminary results indicating that 
the two ground-level learners make rather 
uncorrelated errors. Table 3 shows the average 
percentages of messages where only one, or both 
ground-level classifiers fail, per cost scenario (?) 
and message category. The figures are for the 
configurations of Tables 1 and 2. It can be seen 
that the common errors are always fewer than 
the cases where both classifiers fail. Hence, 
there is much space for improved accuracy, if a 
president can learn to select the correct member.   
5 Experimental results 
Tables 4 and 5 summarize the performance of 
the best configurations of the president in our 
experiments, for each cost scenario. Comparing 
the TCR scores in these tables with the 
corresponding scores of Tables 1 and 2 shows 
that stacking improves the performance of the 
overall filter. From the two stacking versions, 
cross-validation stacking is slightly better than 
holdout stacking. It should also be noted that 
stacking was beneficial for most of the 
configurations of the president that we tested, 
i.e. most sub-optimal presidents outperformed 
the best configurations of the members. This is 
encouraging, since the optimum configuration is 
often hard to determine a priori, and may vary 
from one user to the other.   
  
? k m SR SP ?CR
1 5 100 91.7% 96.5% 8.44 
9 3 200 84.2% 98.9% 3.98 
 
? k m SR SP ?CR
1 7 300 89.6% 98.7% 8.60 
9 3 100 84.8% 98.8% 4.08 
 
There was one interesting exception in the 
positive impact of stacking. The 1-NN and 2-NN 
(k = 1, 2) presidents were substantially worse 
than the other k-NN presidents, often performing 
worse than the ground-level classifiers. We 
witnessed this behavior in both cost scenarios, 
and with most values of m (number of 
attributes). In a ?postmortem? analysis, we 
ascertained that most messages misclassified by 
1-NN and 2-NN, but not the other presidents, are 
legitimate, with their nearest neighbor being 
spam. Therefore, the additional errors of 1-NN 
and 2-NN, compared to the other presidents, are 
of the SL ?  type. Interestingly, in most of 
Table 2: Best configurations of k-NN per usage
scenario and the corresponding performance. 
Table 1: Best configurations of NB per usage
scenario and the corresponding performance. 
Table 5: Best configurations of cross-validation 
stacking per usage scenario and the 
corresponding performance. 
Table 4: Best configurations of holdout 
stacking per usage scenario and the 
corresponding performance. 
Table 3: Analysis of the common errors of the
best configurations of NB and k-NN per
scenario (?) and message class. 
 those cases, both members of the committee 
classify the instance correctly, as legitimate. 
This is an indication, that for small values of the 
parameter k the additional two features, i.e., the 
members? confidence )(1 xWS
  and )(2 xWS
 , do 
not enhance but distort the representation of 
instances. As a result, the close neighborhood of 
the unclassified instance is not a legitimate, but a 
spam e-mail. This behavior of the memory-
based classifier is also noted in (Sakkis, et al 
2001). The suggested solution there was to use a 
larger value for k, combined with a strong 
distance weighting function, such as the one 
presented in section 4. 
Conclusion 
In this paper we adopted a stacked 
generalization approach to anti-spam filtering, 
and evaluated its performance. The 
configuration that we examined combined a 
memory-based and a Na?ve Bayes classifier in a 
two-member committee, in which another 
memory-based classifier presided. The 
classifiers that we chose as members of the 
committee have been evaluated individually on 
the same data as in our evaluation, i.e. the Ling-
Spam corpus. The results of these earlier studies 
were used as a basis for comparing the 
performance of our method.  
Our experiments, using two different 
approaches to stacking and two different 
misclassification cost scenarios, show that 
stacking consistently improves the performance 
of anti-spam filtering. This is explained by the 
fact that the two members of the committee 
disagree more often than agreeing in their 
misclassification errors. Thus, the president is 
able to improve the overall performance of the 
filter, by choosing the right member?s decision 
when they disagree. 
The results presented here motivate further 
work in the same direction. In particular, we are 
interested in combining more classifiers, such as 
decision trees (Quinlan, 1993) and support 
vector machines (Drucker, et al 1999), within 
the stacking framework. A larger variety of 
classifiers is expected to lead the president to 
more informed decisions, resulting in further 
improvement of the filter?s performance. 
Furthermore, we would like to evaluate other 
classifiers in the role of the president. Finally, it 
would be interesting to compare the 
performance of the stacked generalization 
approach to other multi-classifier methods, such 
as boosting (Schapire & Singer, 2000). 
References  
Aha, W. D., Kibler D., and Albert, M.K., (1991) 
Instance-Based Learning Algorithms. ?Machine 
Learning?, Vol. 6, pp. 37?66. 
Androutsopoulos, I., Koutsias, J., Chandrinos, K.V., 
Paliouras, G., and Spyropoulos, C.D. (2000a) ?An 
evaluation of na?ve Bayesian anti-spam filtering?. 
In Proceedings of the Workshop on Machine 
Learning in the New Information Age, 11th 
European Conference on Machine Learning 
(ECML 2000), Barcelona, Spain, pp. 9?17. 
Androutsopoulos, I., Paliouras, G., Karkaletsis, V., 
Sakkis, G., Spyropoulos, C.D., and Stamatopoulos, 
P. (2000b). ?Learning to filter spam e-mail: a 
comparison of a na?ve Bayesian and a memory-
based approach?. In Proceedings of the Workshop 
on Machine Learning and Textual Information 
Access, PKDD 2000, Lyon, France, pp. 1? 3. 
Androutsopoulos, I, Koutsias, J, Chandrinos, K.V., 
and Spyropoulos, C.D. (2000c) ?An experimental 
comparison of na?ve Bayesian and keyword-based 
anti-spam filtering with encrypted personal e-mail 
messages?. In Proceedings of SIGIR 2000, Athens, 
Greece, pp. 160?167. 
Cranor, L.F., and LaMacchia, B.A. (1998). ?Spam!?, 
Communications of ACM, 41(8):74?83.  
Daelemans, W., Zavrel, J., van der Sloot, K., and van 
den Bosch, A. (2000) TiMBL: Tilburg Memory 
Based Learner, version 3.0, Reference Guide. ILK, 
Computational Linguistics, Tilburg University. 
http:/ilk.kub.nl/~ilk/papers. 
Dietterich, G. T. (1997). ?Machine Learning 
Research: Four Current Directions?. AI Magazine 
18(4):97-136.     
Drucker, H. D. ,Wu, D., and Vapnik V. (1999). 
?Support Vector Machines for Spam 
Categorization?. IEEE Transactions On Neural 
Networks, 10(5). 
Duda, R.O, and Hart, P.E. (1973). ?Bayes decision 
theory?. Chapter 2 in Pattern Classification and 
Scene Analysis, pp. 10?43, John Wiley. 
Dudani, A. S. (1976). ?The distance-weighted k-
nearest neighbor rule?. IEEE Transactions on 
Systems, Man and Cybernetics, 6(4):325?327. 
G?mez Hidalgo, J.M., Ma?a L?p?z, M., and Puertas 
Sanz, E. (2000). ?Combining text and heuristics for 
 cost-sensitive spam filtering?. In Proceedings of 
the 4th Computational Natural Language Learning 
Workshop, CoNLL-2000, Lisbon, Portugal, pp. 99?
102.   
Kohavi, R. (1995). ?A study of cross-validation and 
bootstrap for accuracy estimation and model 
selection?. In Proceedings of the 12th International 
Joint Conference on Artificial Intelligence (IJCAI-
1995), Morgan Kaufmann, pp. 1137?1143.  
Mitchell, T.M. (1997). Machine Learning. McGraw-
Hill. 
Pantel, P., and Lin, D. (1998). ?SpamCop: a spam 
classification and organization program?. In 
Learning for Text Categorization ? Papers from 
the AAAI Workshop, pp. 95?98, Madison 
Wisconsin. AAAI Technical Report WS-98-05. 
Quinlan, J.R. (1993). C4.5: Programs for Machine 
Learning, Morgan Kaufmann, San Mateo, 
California.  
Sahami, M., Dumais, S., Heckerman D., and Horvitz, 
E. (1998). ?A Bayesian approach to filtering junk 
e-mail?. In Learning for Text Categorization ? 
Papers from the AAAI Workshop, pp. 55?62, 
Madison Wisconsin. AAAI Technical Report WS-
98-05. 
Sakkis, G., Androutsopoulos, I., Paliouras, G., 
Karkaletsis, V., Spyropoulos, C.D., and 
Stamatopoulos, P. (2001) ?A memory-based 
approach to anti-spam filtering?. NCSR 
?Demokritos? Technical Report, Athens, Greece. 
Schapire, R.E., and Singer, Y. (2000). ?BoosTexter: a 
boosting-based system for text categorization?. 
Machine Learning, 39(2/3):135?168. 
Sebastiani, F. (2001). Machine Learning in 
Automated Text Categorization. Revised version of 
Technical Report IEI-B4-31-1999, Istituto di 
Elaborazione dell?Informazione, Consiglio 
Nazionale delle Ricerche, Pisa, Italy. 
Wettschereck, D., Aha, W. D., and Mohri, T. (1995). 
A Review and Comparative Evaluation of Feature 
Weighting Methods for Lazy Learning Algorithms. 
Technical Report AIC-95-012, Naval Research 
Laboratory, Navy Center for Applied Research in 
Artificial Intelligence, Washington, D.C. 
Wolpert, D. (1992). ?Stacked Generalization?. 
Neural Networks, 5(2):241?260. 
