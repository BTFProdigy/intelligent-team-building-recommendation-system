Amount of Information Presented in a Complex List:   
Effects on User Performance 
Dawn Dutton 
AT&T Labs - Research 
180 Park Avenue 
D103, Bldg. 103 
Florham Park, NJ 07932 
+1.973.236.6522 
dldutton@att.com 
Selina Chu 
Information and Computer Science 
444 Computer Sci. Bldg. 
UC Irvine 
Irvine, CA 92697 
+1.949.509.9762 
selina@ics.uci.edu 
James Hubbell 
12 Lawrence Avenue 
West Long Branch, NJ 07932 
+1.732.728.9351 
jameshubbell@hotmail.com 
Marilyn Walker 
AT&T Labs - Research 
180 Park Avenue 
E103, Bldg. 103 
Florham Park, NJ 07932 
+1.973.360.8956 
walker@research.att.com 
Shrikanth Narayanan 
Speech and Image Proc. Institute 
Department of EE-Systems 
EEB 430, USC 
Los Angeles, CA 90089 
+1.213.740.6432 
shri@sipi.usc.edu 
 
 
ABSTRACT 
AT&T Communicator is a state-of-the-art speech-enabled 
telephony-based application that allows the end-user to, among 
other things, select and reserve airline itineraries.  This 
experiment explores how the amount and structure of information 
presented in complex lists influences the user experience and the 
ability of subjects to successfully complete a selection task.  
Presenting all the relevant information needed for a decision at 
once was the factor that most positively influenced successful task 
completion and the user experience.  Subjects preferred hearing 
all of the relevant information about each flight, without initiating 
additional dialog with the system.  Additionally, successful task 
completion rates improved when all of the flights were presented 
at once, without any intervening questions from the system.  
Keywords 
dialogue, user interface, human factors, spoken language, lists, 
selection 
1. INTRODUCTION 
AT&T Communicator is a state-of-the-art spoken dialogue system 
that allows the end-user to select and reserve various travel related 
resources, in particular, airfare, hotel, and rental cars [2].  One of 
its most challenging dialog modules is that for presenting 
information that allows the user to select a desired air travel 
itinerary. While selecting between multiple possible itineraries in 
a visual domain is a relatively simple task because most of the 
selection criteria are listed in a single scrollable page, the same 
task is likely to have a much higher cognitive load in an audio-
only domain [1, 3]. The selection criteria for each candidate flight 
must be presented to the user serially, leading to higher cognitive 
demands that could result in errors in itinerary selection.     
A sample AT&T Communicator dialog with the prototype 
used in this experiment, showing the outbound flight only, 
follows. 
System: Hello, this is AT&T Communicator.   
How may I help with your travel plans? 
User:   Yeah, I?d like to fly from Newark to San Francisco. 
System: OK, a flight from Newark, New Jersey to San 
Francisco, California.  And will this be a round 
trip? 
User:   Yes, and I need to arrive in San Francisco by 4 PM 
on November 22nd. 
System: I found five outbound United Airlines flights.   
Option 1 is flight number 635.  It has one stop; it 
departs at 10:15 A.M. and arrives at 2:42 P.M. 
<pause>   
Option 2 is flight number 7535.  It has one stop; It 
departs at 12:20 P.M., and arrives at 4:37 P.M.  
<pause>   
Option 3 is flight number 837.  It' s a non-stop 
flight; it departs at 2:00 P.M.  and arrives at 5:01 
P.M.  <pause>   
Would you like to hold any of these flights? 
 
 
 
User:   I guess I?ll take option one. 
System: Got it.   
Note that the second to the last system utterance, in which a 
set of itineraries are presented to the user for selection, is by far 
the longest system utterance in the dialog.  This utterance 
includes a complex list of information.  Unlike a more simple list, 
each list item (option) contains a number of sub-items, for 
example flight number, number of stops, etc.  In early dialog 
evaluations of the Communicator system, the selection of a single 
itinerary from such a complex list of flights was a source of 
substantial usability problems; users frequently became frustrated.  
In fact, many users dropped out of the dialog before successfully 
selecting an itinerary.   
This paper describes an experiment in which we vary the 
amount and structure of information presented about available 
itineraries.  We predict that the amount and structure of 
information presented affects the ability of users to successfully 
select the optimal itinerary within a set, and influences subjective 
measures such as user satisfaction. 
2. METHODS AND PROCEDURES 
2.1 Subjects 
Sixty-four subjects were run at a local shopping mall over a five 
day period.  Subjects were recruited from the shoppers 
frequenting the mall. 
2.2 Wizard of Oz 
A Wizard of Oz (WOZ) experiment was run to determine the 
optimal way for the end-user to select a desired itinerary in the 
Communicator project. 
A Wizard of Oz experiment is one in which no real automatic 
speech recognition (ASR) or natural language understanding 
(NLU) is used.  Instead, the user interface is prototyped and a 
?wizard,? or experimenter, acts in place of the ASR and NLU.  
Consequently, subjects believe that ASR/NLU is being used.  The 
WOZ methodology allows competing user interface strategies to 
be prototyped and tested with end users in a shorter period of time 
than would be required to implement multiple fully-functioning 
systems with competing user interfaces. 
2.3 Apparatus & Materials 
Relevant aspects of the AT&T Communicator user interface were 
prototyped using the Unisys Natural Language Speech Assistant 
(NLSA) software.  NLSA runs on a PC using the Windows NT 
operating system.  Subjects called into the Communicator 
prototype using an analog telephone and interacted with the 
system by voice.  The wizard categorized the subject?s speech 
using the NLSA Wizard graphical user interface (GUI).  Each 
subject completed 5 surveys in pen and paper format.  During the 
course of the experiment, subjects also had access to a pad of 
paper. 
2.4 Experimental Design 
All itineraries presented to the subjects were round-trip. 
2.4.1 Independent Variables 
This was a factorial experiment with two factors, one factor 
between subjects and the other within subject (see Table 1). 
Selection Itinerary Content.
  There were two levels of this 
between subjects factor: 
--Terse.  The presented itineraries included:  airline, number 
of stops, and departure time1.  In order to get additional 
information, the user could ask the system questions (e.g. 
?When does that flight arrive??). 
--Verbose.  The presented itineraries included:  airline, flight 
number, number of stops, departure time, and arrival time.  
All the information relevant to the tasks specified in the 
experiment are presented about each flight; the user did not 
need to ask questions to get additional information. 
Number of Flights Before Question.
  Each level is actually a 
combination of two separate, but related, factors. 
--Combined vs. Separate.  Whether outbound and return 
flights are presented separately or in combination.  
--Number of flights.  The number of flights that are presented 
before asking the subject to make a decision. 
Four levels of this factor were chosen.  In all cases (1) the 
total number of flights ?found? was 5, and, (2) the question was, 
?Would you like to hold [that flight/any of  those flights]??. 
--Separate 1.  The outbound and return flights of the trip are 
presented separately and after each flight the subject is asked 
the question. 
--Separate 3.  The outbound and return flights of the trip are 
presented separately and after the third flight the subject is 
asked the question. 
--Separate 5.  The outbound and return flights of the trip are 
presented separately and after the last flight the subject is 
asked the question.   
--Combined.  The outbound and return flights of the trip are 
presented at the same time and after each set of two flights 
the subject is asked the question.   
Table 1:  Factors used in this experiment. 
  
Example.
  The following example could have been used in 
the Separate 3 condition.  Text that is unformatted is common to 
both the terse and verbose conditions.  Text in italics is found 
only in the verbose condition. 
?I found 5 outbound Delta flights. Option 1 is
 flight number 
323.  It?s a non-stop leaving at 9:10 and arriving at 2:01.   
Option 2
 is flight number 798.  It has one stop; it departs at 
11:13 and arrives at 5:07. Option 3 is flight number 295.  It 
                                                                
1
 All times in this experiment were presented to the minute, with either 
AM or PM, e.g. ??departs at 6:01 AM.? 
Selection Itinerary Content (Between) 
Terse Verbose 
Outbound 
/ Return 
#  of 
Flights 
Before 
Question 
Outbound 
/ Return 
# of Flights 
Before 
Question 
(Within) (Within) 
Separate 1 Separate 1 
Separate 3 Separate 3 
Separate 5 Separate 5 
Combined 2 Combined 2 
has two stops; it departs at 1:52 and arrives at 6:57. Would 
you like to hold any of those flights?? 
2.4.2 Mixed Initiative and Flow of Control 
The dialog strategy was mixed initiative.  The first prompt was 
open-ended, e.g. ?How may I help you with your travel plans??  
All subsequent prompts requested specific information from the 
user (e.g. ?What date did you want to depart??)  The prototypes 
were built to allow the user to provide multiple informational 
elements (e.g. departure city and departure date) to either open-
ended or specific requests.  Subsequent steps in the flow of 
control could be skipped if multiple pieces of information were 
presented at a single dialog point. 
2.4.3 Tasks 
Each subject was asked to complete four tasks in the course of 
this experiment.  In each task the subject was given a set of 
criteria that the subject had to meet in selecting both an outbound 
and a return flight.  The tasks used in this experiment exercise 
selection criteria that are representative of selection criteria 
typically used by individuals actually purchasing airline tickets.  
The four tasks given to subjects follow: 
Departure Only.  The task criteria for both the outbound and 
return flights require the subject to choose flights based on 
departure time only.  
Arrival Only.  The task criteria for both the outbound and return 
flights require the subject to choose flights based on arrival time 
only. 
Departure & Arrival.   The task criteria require the subject to 
choose the outbound flight based on departure time and the return 
flight based on arrival time. 
Specific Flight.  The task requires the subject to book a particular 
flight for both the outbound and return flights. 
Example.  The following example was used for the Departure 
& Arrival task (it has been edited for presentation here). 
You want a round trip ticket from Boston to Charleston.  
You want to leave Boston about 5 in the evening of 
Friday November 10th.  You want to arrive in Boston no 
later than 8 PM on Tuesday November 14th. 
An important selection criterion for many purchasers of 
airline tickets is price.  The price of the ticket was not a selection 
criterion used in this experiment because it would introduce 
possible confounds.  Many users are willing to trade-off other 
important selection criteria, e.g. arrival time and departure time, 
in order to minimize price.  Therefore, it was decided, a priori, to 
postpone the use of price as a selection criterion to a later 
experiment. 
2.4.4 Counterbalancing 
A Balanced Greco-Latin Square was used to counterbalance the 
orders of the conditions and tasks. 
2.4.5 Dependent Measures 
A rich set of dependent measures were gathered in this 
experiment:   
-- After each system prompt was played, NLSA recorded what 
subjects said. 
-- At the end of each task, the wizard determined whether that 
task was successfully completed. 
-- At the end of each task, subjects completed paper and pen 
surveys rating the overall dialog for that task. 
-- After experiencing all four tasks, subjects told the 
experimenter which of the flight selection criteria were 
important to them. 
Objective measure.  Successful task completion was the one 
objective measure used in determining the optimal method for 
presenting complex lists in an audio-only domain.  For each task 
the subject was given a set of required criteria for selecting both 
the outbound and a return flight.  Task completion was binary, 
successful or unsuccessful, and was determined by the 
experimenter (wizard) at the time the subject completed each task.  
In order for a subject to successfully complete a task, the subject 
had to select both the outbound and return flight that best fit the 
clear criteria given to subjects in the task description. 
Subjective measures.  Other data gathered in this experiment 
included a number of subjective measures.  After each task, 
subjects were asked: 
Overall, how satisfied were you with AT&T Communicator while 
booking this flight?  
[1] Very Satisfied  
[2] Somewhat Satisfied  
[3] Neither Satisfied, Nor Dissatisfied  
[4] Somewhat Dissatisfied  
[5] Very Dissatisfied  
Overall, how easy was it to use AT&T Communicator while 
booking this flight?  
[1] Very Easy  
[2] Somewhat Easy  
[3] Neither Easy, Nor Difficult  
[4] Somewhat Difficult  
[5] Very Difficult  
Overall, how quickly did AT&T Communicator respond to your 
speech?   
[1] Much Too Fast  
[2] A Little Too Fast  
[3] Just the Right Speed  
[4] A Little Too Slow  
[5] Much Too Slow  
After you told Communicator the date and time to book your 
flight, Communicator responded with possible flights to choose 
from. For EACH of the possible flights, did Communicator 
present the right amount of information?   
[1] Too Much Information about Each Flight  
[2] Just the Right Amount of Information About Each 
Flight  
[3] Too Little Information about Each Flight  
After completing all four tasks, subjects were asked to (1) 
rank order the criteria they personally use when selecting between 
multiple itineraries, and (2) specify the information that 
Communicator should present about every flight for selection 
purposes in the future. 
3. RESULTS AND CONCLUSIONS 
3.1 Terse or Verbose? 
A two-way, 2x4, Analysis of Variance (ANOVA) was run for 
each of 5 dependent measures:  successful task completion, 
amount of information presented about each flight, satisfaction, 
ease of use, and speed of interaction.   For each dependent 
measure, no significant interactions were found2.  A significant 
main effect for Terse/Verbose was found for the subjective 
measure of the amount of information presented about each flight 
(p=.001), see Fig. 1.  
 
Amount of Information
1.80
1.90
2.00
2.10
2.20
2.30
2.40
1-Sep 3-Sep 5-Sep Comb
# of Flights
verbose (N=30)
terse (N=29)
 
Figure 1:  Verbose and Terse subject ratings to the Amount of 
Information question (2=Just the Right Amount of 
Information about each flight). 
No other significant main effects were found for any of the 
dependent measures.  The optimum value for the dependent 
measure amount of information is ?2? (Just the right amount of 
information about each flight).  The average value for the Verbose 
condition (across the 4 levels of # of Flights) was 2.06, while the 
equivalent average for the Terse condition was 2.24.   
Rank Order of 
Personal Selection Criteria
0 100 200 300 400 500
Price
Stops - Number
Time - Arrival
Time - Departure
Airline
Stops - Length
Flight Number
Weighted ScoreVerbose
Terse
 
Figure 2:  Summed Weighted Scores of subjects? rank 
ordering of their personal selection criteria. 
Related to these results is a question that was asked of all 
subjects at the end of the experiment.  Figure 2 shows the 
weighted scores based on the  rank ordering of the selection 
                                                                
2
 Throughout the experiment, the alpha level used to determine 
significance of an effect was p<.05. 
criteria subjects personally use when selecting among multiple 
flights.   A rank order of 1 was given a score 7 points, a rank 
order of 7 was given a score of 1 point, etc.  The Weighted Score 
for each selection criteria shown in Figure 2 is the sum of the 
Weighted Scores for all  subjects. 
Similarly, a second question was asked of all subjects at 
the end of the experiment:  ?In the future, what information 
should AT&T Communicator present about each flight 
when you are choosing between multiple flights?? Figure 3 
shows the compiled responses to this question.  
 
What Information Should AT&T 
Communicator Present?
0 10 20 30 40 50 60 70
Price
Time - Arrival
Time - Departure
Stops - Number
Airline
Stops - Length
Flight Number
Number of Subjects SelectingVerbose
Terse
 
Figure 3:  Number of subjects indicating that each selection 
criterion should, by default, be presented by AT&T 
Communicator. 
Information that should definitely be presented to subjects 
when selecting between multiple flights includes:  price, arrival 
time, departure time, number of stops and airline.    The value to 
users of the length of stops is ambiguous.  It probably should not 
be presented by default, although it might be useful to present the 
length of stops if they will be inordinately long, e.g. greater than 
2 hours, or inordinately short, e.g. less than 45 minutes.  Flight 
number was judged to be least valuable and should not be 
presented. 
3.2 Number of Flights? 
The above analyses indicate that the amount of information 
presented in the Verbose condition better met the expectations of 
subjects.   The next question then was, within the verbose 
condition, which level of the number of flights before the question 
factor showed the best performance.  A one-way, 1x4, ANOVA 
was run for the verbose condition for each of five dependent 
measures:  successful task completion, amount of information 
about each flight, satisfaction, ease of use, and speed of 
interaction3.  A significant main effect was found for successful 
                                                                
3
 As noted at the beginning of the Results section, subject responses to the 
satisfaction, ease of use, and speed of the interaction questions may be 
attributable to the subject?s reactions to the novel user-system 
task completion (p=.005).  Figure 4 shows the percentage of 
successful task completions in the Verbose condition only.  No 
significant effects were found for the other four dependent 
measures. 
Successful Task Completion 
- Verbose Condition Only
0.00
0.20
0.40
0.60
0.80
1.00
1-Sep 3-Sep 5-Sep Comb
# of Flights
 
Figure 4:  Successful task completion (in percent) across the 
four levels of the # of Flights Before Question condition 
(Verbose only). 
The significant main effect was probed using the Tukey test4.  
Separate 5 was the condition with the highest successful task 
completion rate.  Only one pairwise comparison was significant 
(p<.05).  Tasks attempted in the Separate 5 condition were 
significantly more likely to be completed successfully than tasks 
attempted in the Separate 3 condition.   
Table 2:  Results of Tukey test for successful task completion 
across the four levels of # of Flights Before Question condition 
(Verbose only). 
 Sep. 3 
=.567 
Sep. 1 
=.600 
Comb. 
=.833 
Sep. 5 
=.900 
Sep. 3 
=.567 
 p>=.05 p>=.05 p<.05* 
Sep. 1 
=.600 
  p>=.05 p>=.05 
Comb. 
=.833 
   p>=.05 
Sep. 5 
=.900 
    
 
Among the three Separate conditions (Separate 1, Separate 3, 
and Separate 5), subjects were much more likely to successfully 
complete a task in Separate 5.  That is, when all the flights for a 
given flight (outbound or return) were presented at once, without 
any intervening questions. Also, based on subject comments, it 
appeared that at least some subjects in the Separate 3 condition 
were confused about the number of flights they had available to 
select between.  These subjects didn?t realize that there were more 
flights available after the system presented them with the first 
                                                                                                          
interaction style, rather than to the experimentally varied presentation of 
the flight selection criteria. 
4
 The Tukey is a test of significance for pairwise comparisons of treatment 
means that controls for familywise error. 
three in a total set of five flights.  This is in spite of the fact that in 
all tasks, including the Separate 3 condition, the subjects heard a 
sentence like ?I found five outbound Northwest Airlines flights,? 
before the options were presented for selection.   
It not possible, on the basis of the experimental data gathered 
in this study, to unambiguously choose one of the # of flights 
before question conditions over the others.  It may be that a more 
difficult set of tasks would elicit stronger differences in both the 
objective and subjective measures for the levels of this factor.  
However, in absolute terms, the task completion rates with 
Separate 5 and Combined were both high (90% and 83%, 
respectively), relative to the Separate 1 and Separate 3 conditions 
(60% and 57%, respectively). 
Anecdotal evidence sheds some additional light on the issue 
of which condition (Separate 5 or Combined) is preferred by 
subjects.  In the Verbose condition, the last 17 subjects run in the 
experiment were asked a few questions that provide evidence 
concerning their subjective impressions of the four levels of the 
number of flights before question factor.  The first question was 
?Did you notice any difference between the different versions of 
the system??  Twelve of seventeen subjects stated that they had 
noticed a difference between the four versions.  Those 12 subjects 
were then asked to choose the version they liked the best, and 
then the version they considered to be the worst.   
Best? 
3-Did not specify a ?best? version 
2-Combined only 
2-Separate 1 only 
1-Separate 3 only 
1-Separate 5 only 
2-Any of the Separate versions 
1-Either Separate 3 or Separate 5 
Worst? 
7-Did not specify a ?worst? version 
5-Combined only 
0-Separate 1 only 
0-Separate 3 only 
0-Separate 5 only 
In response to the question of which version of the system 
was best, the subjects stated no consistent preference for any of 
the versions of the system.  On the other hand, the responses to 
the question concerning which version of the system was ?worst? 
resulted in a more consistent set of responses; the Combined 
version was selected by 5 of 12 of the subjects as the version they 
considered to be the ?worst.?  From subject comments, it appeared 
that subjects didn?t like it when they heard one flight that matched 
their constraints (e.g. outbound), while the other flight did not 
match their constraints (e.g. return).  Some subjects found this to 
be frustrating, confusing, and/or tedious. 
4. DISCUSSION 
Presenting all the relevant information about a given flight at once 
seemed to be the single overarching factor that most positively 
influenced successful task completion and the user experience.  
Subjects wanted to hear all of the relevant information about a 
flight needed to make the best choice.    
Within the Separate conditions (Separate 1, Separate 3 and 
Separate 5), the task completion rate was highest for the Separate 
5 condition.  That is, when all of the flights were presented at 
once, without any intervening system questions. The Separate 5 
and Combined conditions had similar task completion rates and 
were not significantly different.  However, the Combined 
condition was the only condition considered ?worst? by subjects. 
Thus, the condition that maximized both successful task 
completion and user experience was the Verbose Separate 5 
condition.   
A major concern in the design of this experiment was that the 
audio presentation of lists of complex information, in this case 
lists of multiple airline flights each containing multiple pieces of 
information, would result in cognitive overload.  These findings 
argue that, for this task, our concern about the increased cognitive 
load in an audio-only domain was unfounded.  There are a couple 
of possible reasons that cognitive load did not appear to have the 
influence that we anticipated.  First, users knew their constraints 
when listening to the flights.  Some users may have used a 
strategy of only remembering the options that potentially matched 
their constraints.  Second, many subjects apparently dealt with the 
increased cognitive load by taking notes, with flight times, etc., 
while completing the experimental tasks.  Such behaviour is 
certainly common when making airline reservations with a human 
travel agent over the telephone.  Further, it is anticipated that 
users would generally book flights in a hands-free environment, 
and would be able to take notes, when it is desirable.  Although 
less ecologically valid, future experiments primarily concerned 
with the issue of cognitive load should explicitly preclude 
subjects from taking notes. 
From the above data, it appears that the standard user 
interface practices for Prompt & Collect (in which the system 
?prompts? the user for a specific piece of information and then 
?collects? their response) should be different than the standard 
practice for Selecting from a List.  For Prompt & Collect, the 
standard UI practice is for the system to be as terse as possible 
when prompting the user.  The user generally knows the 
information the system is asking for, they just need to be 
prompted so that they know what piece of information the system 
is currently ready to accept.  When Selecting from a List, 
however, the level of knowledge of the system and user are very 
different.  The system knows the possible items that can be 
selected.  The user begins the interaction only knowing their own 
constraints (and their relative priority).  The system must present 
all the relevant information so that the user can select the optimal 
item from the list based on their constraints and priorities.  
Consequently, when the user must select an item from a list, the 
system should be as verbose as necessary and present all the 
information relevant to their decision at once. 
For the task of selecting between multiple airline flights, the 
following information should definitely be presented to users 
about each flight: price, arrival time, departure time, number of 
stops and airline.  While there was agreement between subjects on 
the important selection criteria for flights, there are also some 
individual differences.  For example, some users like to fly only a 
particular airline or have strong preferences concerning the 
locations of layovers.  This experiment indicates that users like to 
hear all of the information relevant to their decision at once.  
Therefore, if a user asks a question concerning information that is 
not by default presented by the system about each flight, the 
information presented to that user for each subsequent flight on 
that call, should include the information they have requested.  For 
example, if after the first flight the user asks ?Where does that 
flight stop??, the information for all subsequent flights in that call 
should include the location of any stops. 
It thus appears likely that subjects in this experiment were 
using AT&T Communicator as a tool, rather than as a 
conversational or negotiating partner.  That is, their goal was to 
use Communicator to quickly and efficiently select the single 
flight that best matched the criteria given to them in each task.  
Asking the system questions in order to get information relevant 
to this selection process, would decrease the speed and efficiency 
with which they were able to accomplish this task.  So, subjects 
preferred the most verbose presentation of information in order to 
increase the speed and efficiency of the overall task. 
Nevertheless, eighty percent of the subjects stated that the 
most important criterion when personally selecting a flight was 
price.   A number of subjects commented that they were willing to 
trade off other important criterion, e.g. airline, number of stops, in 
order to get a better price.  In a more complex selection task, 
where the user is choosing a set of flights based on multiple, 
competing selection criteria, the user may wish to use AT&T 
Communicator as a negotiating partner in order to get the best 
flight available.  Such a negotiation might lend itself well to 
exploring machine-user dialog in a natural language telephony-
based system.   
5. ACKNOWLEDGMENTS 
We would like to thank the AT&T Communicator team for their 
assistance on this project. 
6. REFERENCES 
[1] Blanchard, H.E. & Lewis, S.H. (1999), The Voice 
messaging user interface, in D. Gardner-Bonneau (ed.), 
Human factors and voice interactive systems, Kluwer 
Academic Publishers, pp.257-284. 
[2] Levin, E., Narayanan, S., Pieraccini, R., Biatov, K., 
Bocchieri, E., Di Fabbrizio, G., Eckert, W., Lee, S., 
Pokrovsky, A., Rahim, M., Ruscitti, P., and Walker, M. 
(2000), The AT&T-DARPA Communicator mixed-
initiative spoken dialog system, Proc. of the 
International Conference of Spoken Langurage 
Processing, (Beijing, China), pp. 122-125. 
[3] Schneiderman, B. (1992).  Designing the user interface 
(2nd ed.).  Reading MA:  Addison Wesley. 
 
 
 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 145?148,
New York, June 2006. c?2006 Association for Computational Linguistics
Selecting relevant text subsets from web-data for building topic specic
language models
Abhinav Sethy, Panayiotis G. Georgiou, Shrikanth Narayanan
Speech Analysis and Interpretation Lab
Integrated Media Systems Center
Viterbi School of Engineering
Department of Electrical Engineering-Systems
University of Southern California
Abstract
In this paper we present a scheme to se-
lect relevant subsets of sentences from a
large generic corpus such as text acquired
from the web. A relative entropy (R.E)
based criterion is used to incrementally se-
lect sentences whose distribution matches
the domain of interest. Experimental re-
sults show that by using the proposed sub-
set selection scheme we can get signif-
icant performance improvement in both
Word Error Rate (WER) and Perplexity
(PPL) over the models built from the en-
tire web-corpus by using just 10% of the
data. In addition incremental data selec-
tion enables us to achieve significant re-
duction in the vocabulary size as well as
number of n-grams in the adapted lan-
guage model. To demonstrate the gains
from our method we provide a compar-
ative analysis with a number of methods
proposed in recent language modeling lit-
erature for cleaning up text.
1 Introduction
One of the main challenges in the rapid deployment
of NLP applications is the lack of in-domain data
required for training statistical models. Language
models, especially n-gram based, are key compo-
nents of most NLP applications, such as speech
recognition and machine translation, where they
serve as priors in the decoding process. To estimate
a n-gram language model we require examples of
in-domain transcribed utterances, which in absence
of readily available relevant corpora have to be col-
lected manually. This poses severe constraints in
terms of both system turnaround time and cost.
This led to a growing interest in using the World
Wide Web (WWW) as a corpus for NLP (Lapata,
2005; Resnik and Smith, 2003). The web can serve
as a good resource for automatically gathering data
for building task-specific language models. Web-
pages of interest can be identified by generating
query terms either manually or automatically from
an initial set of in-domain sentences by measures
such as TFIDF or Relative Entropy (R.E). These
webpages can then be converted to a text corpus
(which we will refer to as web-data) by appropri-
ate preprocessing. However text gathered from the
web will rarely fit the demands or the nature of the
domain of interest completely. Even with the best
queries and web crawling schemes, both the style
and content of the web-data will usually differ sig-
nificantly from the specific needs. For example, a
speech recognition system requires conversational
style text whereas most of the data on the web is
literary.
The mismatch between in-domain data and web-
data can be seen as a semi-supervised learning prob-
lem. We can model the web-data as a mix of sen-
tences from two classes: in-domain (I) and noise
(N) (or out-of-domain). The labels I and N are la-
tent and unknown for the sentences in web-data but
we usually have a small number of examples of in-
domain examples I. Selecting the right labels for
the unlabeled set is important for benefiting from it.
145
Recent research on semi-supervised learning shows
that in many cases (Nigam et al, 2000; Zhu, 2005)
poor preprocessing of unlabeled data might actually
lower the performance of classifiers. We found sim-
ilar results in our language modeling experiments
where the presence of a large set of noisy N ex-
amples in training actually lowers the performance
slightly in both perplexity and WER terms. Recent
literature on building language models from text ac-
quired from the web addresses this issue partly by
using various rank-and-select schemes for identify-
ing the set I (Ostendorf et al, 2005; Sethy, 2005;
Sarikaya, 2005). However we believe that simi-
lar to the question of balance (Zhu, 2005) in semi-
supervised learning for classification, we need to ad-
dress the question of distributional similarity while
selecting the appropriate utterances for building a
language model from noisy data. The subset of sen-
tences from web-data which are selected to build the
adaptation language should have a distribution sim-
ilar to the in-domain data model.
To address the issue of distributional similarity we
present an incremental algorithm which compares
the distribution of the selected set and the in-domain
examples by using a relative entropy (R.E) criterion.
We will review in section 2 some of the ranking
schemes which provide baselines for performance
comparison and in section 3 we describe the pro-
posed algorithm. Experimental results are provided
in section 4, before we conclude with a summary of
this work and directions for the future.
2 Rank and select methods for text
cleaning
The central idea behind text cleanup schemes in re-
cent literature, on using web-data for language mod-
eling, is to use a scoring function that measures the
similarity of each observed sentence in the web-
data to the in-domain set and assigns an appropri-
ate score. The subsequent step is to set a threshold
in terms of either the minimum score or the num-
ber of top scoring sentences. The threshold can usu-
ally be fixed using a heldout set. Ostendorf (2005)
use perplexity from an in-domain n-gram language
model as a scoring function. More recently, a mod-
ified version of the BLEU metric which measures
sentence similarity in machine translation has been
proposed by Sarikaya (2005) as a scoring function.
Instead of explicit ranking and thresholding it is also
possible to design a classifier in a learning from pos-
itive and unlabeled examples framework (LPU) (Liu
et al, 2003). In this system, a subset of the unla-
beled set is selected as the negative or the noise set
N. A two class classifier is then trained using the
in-domain set and the negative set. The classifier
is then used to label the sentences in the web-data.
The classifier can then be iteratively refined by us-
ing a better and larger subset of the I/N sentences
selected in each iteration.
Rank ordering schemes do not address the issue of
distributional similarity and select many sentences
which already have a high probability in the in-
domain text. Adapting models on such data has the
tendency to skew the distribution even further to-
wards the center. For example, in our doctor-patient
interaction task short sentences containing the word
?okay? such as ?okay?,?yes okay?, ?okay okay? were
very frequent in the in-domain data. Perplexity or
other similarity measures give a high score to all
such examples in the web-data boosting the prob-
ability of these words even further while other perti-
nent sentences unseen in the in-domain data such as
?Can you stand up please?? are ranked low and get
rejected.
3 Incremental Selection
To address the issue of distributional similarity we
developed an incremental greedy selection scheme
based on relative entropy which selects a sentence
if adding it to the already selected set of sentences
reduces the relative entropy with respect to the in-
domain data distribution.
Let us denote the language model built from in-
domain data as P and let Pinit be a language modelfor initialization purposes which we estimate by
bagging samples from the same in-domain data. To
describe our algorithm we will employ the paradigm
of unigram probabilities though the method general-
izes to higher n-grams also.
Let W (i) be a initial set of counts for the words
i in the vocabulary V initialized using Pinit. We de-note the count of word i in the j th sentence sj ofweb-data with mij . Let nj = ?i mij be the num-ber of words in the sentence and N = ?i W (i) be
146
the total number of words already selected. The rel-
ative entropy of the maximum likelihood estimate of
the language model of the selected sentences to the
initial model P is given by
H(j ? 1) = ?
?
i
P (i) ln P (i)W (i)/N
If we select the sentence sj , the updated R.E
H(j) = ?
?
i
P (i) ln P (i)(W (i) + mij)/(N + nj)
Direct computation of R.E using the above ex-
pressions for every sentence in the web-data will
have a very high computational cost since O(V )
computations per sentence in the web-data are re-
quired. However given the fact that mij is sparse,we can split the summation H(j) into
H(j) = ?
?
i
P (i) ln P (i) +
+
?
i
P (i) ln W (i) + mijN + nj
= H(j ? 1) + ln N + njN
? ?? ?
T1
?
?
i,mij 6=0
P (i) ln (W (i) + mij)W (i)
? ?? ?
T2
Intuitively, the term T1 measures the decrease
in probability mass because of adding nj wordsmore to the corpus and the term T2 measures the
in-domain distribution P weighted improvement in
probability for words with non-zero mij .For the R.E to decrease with selection of sentence
sj we require T1 < T2. To make the selection morerefined we can impose a condition T1 + thr(j) <
T2 where thr(j) is a function of j. A good choice
for thr(j) based on empirical study is a function that
declines at the same rate as the ratio ln (N+nj)N ?
nj/N ? 1/kj where k is the average number ofwords for every sentence.
The proposed algorithm is sequential and greedy
in nature and can benefit from randomization of the
order in which it scans the corpus. We generate per-
mutes of the corpus by scanning through the corpus
and randomly swapping sentences. Next we do se-
quential selection on each permutation and merge
the selected sets.
The choice of using maximum likelihood estima-
tion for estimating the intermediate language mod-
els for W (j) is motivated by the simplification in
the entropy calculation which reduces the order from
O(V ) to O(k). However, maximum likelihood esti-
mation of language models is poor when compared
to smoothing based estimation. To balance the com-
putation cost and estimation accuracy, we modify
the counts W (j) using Kneser-Ney smoothing pe-
riodically after fixed number of sentences.
4 Experiments
Our experiments were conducted on medical do-
main data collected for building the English ASR
of our English-Persian Speech to Speech translation
project (Georgiou et al, 2003). We have 50K in-
domain sentences for this task available. We down-
loaded around 60GB data from the web using au-
tomatically generated queries which after filtering
and normalization amount to 150M words. The test
set for perplexity evaluations consists of 5000 sen-
tences(35K words) and the heldout set had 2000
sentences (12K words). The test set for word er-
ror rate evaluation consisted of 520 utterances. A
generic conversational speech language model was
built from the WSJ, Fisher and SWB corpora in-
terpolated with the CMU LM. All language models
built from web-data and in-domain data were inter-
polated with this language model with the interpola-
tion weight determined on the heldout set.
We first compare our proposed algorithm against
baselines based on perplexity(PPL), BLEU and LPU
classification in terms of test set perplexity. As the
comparison shows the proposed algorithm outper-
forms the rank-and-select schemes with just 1/10th
of data. Table 1 shows the test set perplexity with
different amounts of initial in-domain data. Table 2
shows the number of sentences selected for the best
perplexity on the heldout set by the above schemes.
The average relative perplexity reduction is around
6%. In addition to the PPL and WER improvements
we were able to acheive a factor of 5 reduction in
the number of estimated language model parameters
(bigram+trigram) and a 30% reduction in the vocab-
147
10K 20K 30K 40K
No Web 60 49.6 42.2 39.7
AllWeb 57.1 48.1 41.8 38.2
PPL 56.1 48.1 41.8 38.2
BLEU 56.3 48.2 42.0 38.3
LPU 56.3 48.2 42.0 38.3
Proposed 54.8 46.8 40.7 38.1
Table 1: Perplexity of testdata with the web adapted
model for different number of initial sentences.
ulary size. No Web refers to the language model built
from just in-domain data with no web-data. All-
Web refers to the case where the entire web-data was
used.
The WER results in Table 3 show that adding data
from the web without proper filtering can actually
harm the performance of the speech recognition sys-
tem when the initial in-domain data size increases.
This can be attributed to the large increase in vo-
cabulary size which increases the acoustic decoder
perplexity. The average reduction in WER using the
proposed scheme is close to 3% relative. It is inter-
esting to note that for our data selection scheme the
perplexity improvments correlate surprisingly well
with WER improvments. A plausible explanation
is that the perplexity improvments are accompanied
by a significant reduction in the number of language
model parameters.
5 Conclusion and Future Work
In this paper we have presented a computationally
efficient scheme for selecting a subset of data from
an unclean generic corpus such as data acquired
from the web. Our results indicate that with this
scheme, we can identify small subsets of sentences
(about 1/10th of the original corpus), with which we
can build language models which are substantially
smaller in size and yet have better performance in
10K 20K 30K 40K
PPL 93 92 91 91
BLEU 91 90 89 89
LPU 90 88 87 87
Proposed 12 11 11 12
Table 2: Percentage of web-data selected for differ-
ent number of initial sentences.
10K 20K 30K 40K
No Web 19.8 18.9 18.3 17.9
AllWeb 19.5 19.1 18.7 17.9
PPL 19.2 18.8 18.5 17.9
BLEU 19.3 18.8 18.5 17.9
LPU 19.2 18.8 18.5 17.8
Proposed 18.3 18.2 18.2 17.3
Table 3: Word Error Rate (WER) with web adapted
models for different number of initial sentences.
both perplexity and WER terms compared to models
built using the entire corpus. Although our focus in
the paper was on web-data, we believe the proposed
method can be used for adaptation of topic specific
models from large generic corpora.
We are currently exploring ways to use multiple
bagged in-domain language models for the selection
process. Instead of sequential scan of the corpus, we
are exploring the use of rank-and-select methods to
give a better search sequence.
References
Abhinav Sethy and Panayiotis Georgiou et al. Building topic
specific language models from web-data using competitive
models. Proceedings of Eurospeech. 2005
Bing Liu and Yang Dai et al. Building Text Classifiers Using
Positive and Unlabeled Examples. Proceedings of ICDM.
2003
Kamal Nigam and Andrew Kachites McCallum et al. Text
Classification from Labeled and Unlabeled Documents using
EM. Journal of Machine Learning. 39(2:3)103?134. 2000
Mirella Lapata and Frank Keller. Web-based models for natu-
ral language processing. ACM Transactions on Speech and
Language Processing. 2(1),2005.
Philip Resnik and Noah A. Smith. The Web as a parallel cor-
pus. Computational Linguistics. 29(3),2003.
P.G. Georgiou and S.Narayanan et al. Transonics: A speech to
speech system for English-Persian Interactions. Proceedings
of IEEE ASRU. 2003
Ruhi Sarikaya and Agustin Gravano et al Rapid Language
Model Development Using External Resources For New
Spoken Dialog Domains Proceedings of ICASSP. 2005
Tim Ng and Mari Ostendorf et al. Web-data Augmented Lan-
guage Model for Mandarin Speech Recognition. Proceed-
ings of ICASSP. 2005
Xiaojin Zhu. Semi-Supervised Learning Literature Survey.
Computer Science, University of Wisconsin-Madison.
148
Proceedings of NAACL HLT 2007, pages 1?8,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Exploiting acoustic and syntactic features for prosody labeling in
a maximum entropy framework
Vivek Rangarajan, Shrikanth Narayanan
Speech Analysis and Interpretation Laboratory
University of Southern California
Viterbi School of Electrical Engineering
vrangara@usc.edu,shri@sipi.usc.edu
Srinivas Bangalore
AT&T Research Labs
180 Park Avenue
Florham Park, NJ 07932, U.S.A.
srini@research.att.com
Abstract
In this paper we describe an automatic
prosody labeling framework that exploits
both language and speech information.
We model the syntactic-prosodic informa-
tion with a maximum entropy model that
achieves an accuracy of 85.2% and 91.5%
for pitch accent and boundary tone la-
beling on the Boston University Radio
News corpus. We model the acoustic-
prosodic stream with two different mod-
els, one a maximum entropy model and
the other a traditional HMM. We finally
couple the syntactic-prosodic and acoustic-
prosodic components to achieve signifi-
cantly improved pitch accent and bound-
ary tone classification accuracies of 86.0%
and 93.1% respectively. Similar experimen-
tal results are also reported on Boston Di-
rections corpus.
1 Introduction
Prosody refers to intonation, rhythm and lexical
stress patterns of spoken language that convey lin-
guistic and paralinguistic information such as em-
phasis, intent, attitude and emotion of a speaker.
Prosodic information associated with a unit of
speech, say, syllable, word, phrase or clause, influ-
ence all the segments of the unit in an utterance. In
this sense they are also referred to as suprasegmen-
tals (Lehiste, 1970). Prosody in general is highly
dependent on individual speaker style, gender, di-
alect and other phonological factors. The difficulty in
reliably characterizing suprasegmental information
present in speech has resulted in symbolic and para-
meteric prosody labeling standards like ToBI (Tones
and Break Indices) (Silverman et al, 1992) and Tilt
model (Taylor, 1998) respectively.
Prosody in spoken language can be characterized
through acoustic features or lexical features or both.
Acoustic correlates of duration, intensity and pitch,
like syllable nuclei duration, short time energy and
fundamental frequency (f0) are some acoustic fea-
tures that are perceived to confer prosodic promi-
nence or stress in English. Lexical features like parts-
of-speech, syllable nuclei identity, syllable stress of
neighboring words have also demonstrated high de-
gree of discriminatory evidence in prosody detection
tasks.
The interplay between acoustic and lexical fea-
tures in characterizing prosodic events has been suc-
cessfully exploited in text-to-speech synthesis (Bu-
lyko and Ostendorf, 2001; Ma et al, 2003), speech
recognition (Hasegawa-Johnson et al, 2005) and
speech understanding (Wightman and Ostendorf,
1994). Text-to-speech synthesis relies on lexical fea-
tures derived predominantly from the input text to
synthesize natural sounding speech with appropri-
ate prosody. In contrast, output of a typical auto-
matic speech recognition (ASR) system is noisy and
hence, the acoustic features are more useful in pre-
dicting prosody than the hypothesized lexical tran-
script which may be erroneous. Speech understand-
ing systems model both the lexical and acoustic fea-
tures at the output of an ASR to improve natural
language understanding. Another source of renewed
interest has come from spoken language translation
(No?th et al, 2000; Agu?ero et al, 2006). A pre-
requisite for all these applications is accurate prosody
detection, the topic of the present work.
In this paper, we describe our framework for build-
ing an automatic prosody labeler for English. We
report results on the Boston University (BU) Ra-
dio Speech Corpus (Ostendorf et al, 1995) and
Boston Directions Corpus (BDC) (Hirschberg and
Nakatani, 1996), two publicly available speech cor-
pora with manual ToBI annotations intended for ex-
periments in automatic prosody labeling. We con-
dition prosody not only on word strings and their
parts-of-speech but also on richer syntactic informa-
tion encapsulated in the form of Supertags (Banga-
lore and Joshi, 1999). We propose a maximum en-
tropy modeling framework for the syntactic features.
We model the acoustic-prosodic stream with two dif-
ferent models, a maximum entropy model and a more
traditional hidden markov model (HMM). In an au-
tomatic prosody labeling task, one is essentially try-
1
ing to predict the correct prosody label sequence for
a given utterance and a maximum entropy model of-
fers an elegant solution to this learning problem. The
framework is also robust in the selection of discrim-
inative features for the classification problem. So,
given a word sequence W = {w1, ? ? ? , wn} and a set
of acoustic-prosodic features A = {o1, ? ? ? , oT }, the
best prosodic label sequence L? = {l1, l2, ? ? ? , ln} is
obtained as follows,
L? = argmax
L
P (L|A,W ) (1)
= argmax
L
P (L|W ).P (A|L,W ) (2)
? argmax
L
P (L|?(W )).P (A|L,W ) (3)
where ?(W ) is the syntactic feature encoding of the
word sequence W . The first term in Equation (3)
corresponds to the probability obtained through our
maximum entropy syntactic model. The second term
in Equation (3), computed by an HMM corresponds
to the probability of the acoustic data stream which
is assumed to be dependent only on the prosodic la-
bel sequence.
The paper is organized as follows. In section 2
we describe related work in automatic prosody la-
beling followed by a description of the data used in
our experiments in section 3. We present prosody
prediction results from off-the-shelf synthesizers in
section 4. Section 5 details our proposed maximum
entropy syntactic-prosodic model for prosody label-
ing. In section 6, we describe our acoustic-prosodic
model and discuss our results in section 7. We finally
conclude in section 8 with directions for future work.
2 Related work
Automatic prosody labeling has been an active re-
search topic for over a decade. Wightman and Os-
tendorf (Wightman and Ostendorf, 1994) developed
a decision-tree algorithm for labeling prosodic pat-
terns. The algorithm detected phrasal prominence
and boundary tones at the syllable level. Bulyko
and Ostendorf (Bulyko and Ostendorf, 2001) used
a prosody prediction module to synthesize natural
speech with appropriate prosody. Verbmobil (No?th
et al, 2000) incorporated prosodic labeling into a
translation framework for improved linguistic analy-
sis and speech understanding.
Prosody has typically been represented either sym-
bolically, e.g., ToBI (Silverman et al, 1992) or
parametrically, e.g., Tilt Intonation Model (Tay-
lor, 1998). Parametric approaches either restrict
the variants of prosody by definition or automati-
cally learn prosodic patterns from data (Agu?ero et
al., 2006). The BU corpus is a widely used cor-
pus with symbolic representation of prosody. The
hand-labeled ToBI annotations make this an attrac-
tive corpus to perform prosody labeling experiments.
The main drawback of this corpus is that it com-
prises only read speech. Prosody labeling on sponta-
neous speech corpora like Boston Directions corpus
(BDC), Switchboard (SWBD) has garnered atten-
tion in (Hirschberg and Nakatani, 1996; Gregory and
Altun, 2004).
Automatic prosody labeling has been achieved
through various machine learning techniques, such
as decision trees (Hirschberg, 1993; Wightman and
Ostendorf, 1994; Ma et al, 2003), rule-based sys-
tems (Shimei and McKeown, 1999), bagging and
boosting on CART (Sun, 2002), hidden markov
models (Conkie et al, 1999), neural networks
(Hasegawa-Johnson et al, 2005),maximum-entropy
models (Brenier et al, 2005) and conditional ran-
dom fields (Gregory and Altun, 2004).
Prosody labeling of the BU corpus has been re-
ported in many studies (Hirschberg, 1993; Hasegawa-
Johnson et al, 2005; Ananthakrishnan and
Narayanan, 2005). Hirschberg (Hirschberg, 1993)
used a decision-tree based system that achieved
82.4% speaker dependent accent labeling accuracy
at the word level on the BU corpus using lexical fea-
tures. (Ross and Ostendorf, 1996) also used an ap-
proach similar to (Wightman and Ostendorf, 1994)
to predict prosody for a TTS system from lexical fea-
tures. Pitch accent accuracy at the word-level was
reported to be 82.5% and syllable-level accent accu-
racy was 80.2%. (Hasegawa-Johnson et al, 2005)
proposed a neural network based syntactic-prosodic
model and a gaussian mixture model based acoustic-
prosodic model to predict accent and boundary tones
on the BU corpus that achieved 84.2% accuracy in
accent prediction and 93.0% accuracy in intonational
boundary prediction. With syntactic information
alone they achieved 82.7% and 90.1% for accent and
boundary prediction, respectively. (Ananthakrish-
nan and Narayanan, 2005) modeled the acoustic-
prosodic information using a coupled hidden markov
model that modeled the asynchrony between the
acoustic streams. The pitch accent and boundary
tone detection accuracy at the syllable level were
75% and 88% respectively. Our proposed maximum
entropy syntactic model outperforms previous work.
On the BU corpus, with syntactic information alone
we achieve pitch accent and boundary tone accuracy
of 85.2% and 91.5% on the same training and test
sets used in (Chen et al, 2004; Hasegawa-Johnson
et al, 2005). Further, the coupled model with both
acoustic and syntactic information results in accura-
cies of 86.0% and 93.1% respectively. On the BDC
corpus, we achieve pitch accent and boundary tone
accuracies of 79.8% and 90.3%.
3 Data
The BU corpus consists of broadcast news stories in-
cluding original radio broadcasts and laboratory sim-
2
BU BDC
Corpus statistics f2b f1a m1b m2b h1 h2 h3 h4
# Utterances 165 69 72 51 10 9 9 9
# words (w/o punc) 12608 3681 5058 3608 2234 4127 1456 3008
# pitch accents 6874 2099 2706 2016 1006 1573 678 1333
# boundary tones (w IP) 3916 1059 1282 1023 498 727 361 333
# boundary tones (w/o IP) 2793 684 771 652 308 428 245 216
Table 1: BU and BDC dataset used in experiments
ulations recorded from seven FM radio announcers.
The corpus is annotated with orthographic transcrip-
tion, automatically generated and hand-corrected
part-of-speech tags and automatic phone alignments.
A subset of the corpus is also hand annotated with
ToBI labels. In particular, the experiments in this
paper are carried out on 4 speakers similar to (Chen
et al, 2004), 2 male and 2 female referred to here-
after asm1b, m2b, f1a and f2b. The BDC corpus is
made up of elicited monologues produced by subjects
who were instructed to perform a series of direction-
giving tasks. Both spontaneous and read versions of
the speech are available for four speakers h1, h2, h3
and h4 with hand-annotated ToBI labels and auto-
matic phone alignments, similar to the BU corpus.
Table 1 shows some of the statistics of the speakers
in the BU and BDC corpora.
In Table 1, the pitch accent and boundary tone
statistics are obtained by decomposing the ToBI la-
bels into binary classes using the mapping shown in
Table 2.
BU Labels Intermediate Mapping Coarse Mapping
H*,!H*
L* Single Accent
*,*?,X*? accent
H+!H*,L+H*,L+!H* Bitonal Accent
L*+!H,L*+H
L-L%,!H-L%,H-L%
H-H% Final Boundary tone
L-H%
%?,X%?,%H btone
L-,H-,!H- Intermediate Phrase (IP) boundary
-X?,-?
<,>,no label none none
Table 2: ToBI label mapping used in experiments
In all our prosody labeling experiments we adopt
a leave-one-out speaker validation similar to the
method in (Hasegawa-Johnson et al, 2005) for the
four speakers with data from one speaker for testing
and from the other three for training. For the BU
corpus, f2b speaker was always used in the training
set since it contains the most data. In addition to
performing experiments on all the utterances in BU
corpus, we also perform identical experiments on the
train and test sets reported in (Chen et al, 2004)
which is referred to as Hasegawa-Johnson et al set.
4 Baseline Experiments
We present three baseline experiments. One is sim-
ply based on chance where the majority class label is
predicted. The second is a baseline only for pitch ac-
cents derived from the lexical stress obtained through
look-up from a pronunciation lexicon labeled with
stress. Finally, the third and more concrete base-
line is obtained through prosody detection in current
speech synthesis systems.
4.1 Prosody labels derived from lexical
stress
Pitch accents are usually carried by the stressed syl-
lable in a particular word. Lexicons with phonetic
transcription and lexical stress are available in many
languages. Hence, one can use these lexical stress
markers within the syllables and evaluate the corre-
lation with pitch accents. Eventhough the lexicon
has a closed vocabulary, letter-to-sound rules can be
derived from it for unseen words. For each word car-
rying a pitch accent, we find the particular syllable
where the pitch accent occurs from the manual anno-
tation. For the same syllable, we predict pitch accent
based on the presence or absence of a lexical stress
marker in the phonetic transcription. The results are
presented in Table 3.
4.2 Prosody labeling with Festival and
AT&T Natural Voices R? Speech
Synthesizer
Festival (Black et al, 1998) and AT&T Natural
Voices R? (NV) speech synthesizer (att, ) are two
publicly available speech synthesizers that have a
prosody prediction module available. We performed
automatic prosody labeling using the two synthesiz-
ers to get a baseline.
4.2.1 AT&T Natural Voices R? Speech
Synthesizer
The AT&T NV R? speech synthesizer is a half
phone speech synthesizer. The toolkit accepts
an input text utterance and predicts appropriate
ToBI pitch accent and boundary tones for each of
3
Pitch accent Boundary tone
Corpus Speaker Set Prediction Module Chance Accuracy Chance Accuracy
Lexical stress 54.33 72.64 - -
Entire Set AT&T Natural Voices 54.33 81.51 81.14 89.10
Festival 54.33 69.55 81.14 89.54
Lexical stress 56.53 74.10 - -
BU Hasegawa-Johnson et al set AT&T Natural Voices 56.53 81.73 82.88 89.67
Festival 56.53 68.65 82.88 90.21
Lexical stress 57.60 67.42 - -
BDC Entire Set AT&T Natural Voices 57.60 68.49 88.90 84.90
Festival 57.60 64.94 88.90 85.17
Table 3: Classification results of pitch accents and boundary tones (in %) using Festival and AT&T NV R? synthesizer
the selected units (in this case, a pair of phones)
from the database. We reverse mapped the se-
lected half phone units to words, thus obtaining
the ToBI labels for each word in the input utter-
ance. The toolkit uses a rule-based procedure to
predict the ToBI labels from lexical information.
The pitch accent labels predicted by the toolkit are
Laccent  {H?,L?,none} and the boundary tones
are Lbtone  {L-L%,H-H%,L-H%,none}.
4.2.2 Festival Speech Synthesizer
Festival (Black et al, 1998) is an open-source unit
selection speech synthesizer. The toolkit includes
a CART-based prediction system that can predict
ToBI pitch accents and boundary tones for the input
text utterance. The pitch accent labels predicted by
the toolkit are Laccent  {H?,L+H?, !H?,none}
and the boundary tones are
Lbtone  {L-L%,H-H%,L-H%,none}. The
prosody labeling results obtained through both the
speech synthesis engines are presented in Table
3. The chance column in Table 3 is obtained by
predicting the most frequent label in the data set.
In the next sections, we describe our proposed
maximum entropy based syntactic model and HMM
based acoustic-prosodic model for automatic prosody
labeling.
5 Syntactic-prosodic Model
We propose a maximum entropy approach to model
the words, syntactic information and the prosodic
labels as a sequence. We model the prediction prob-
lem as a classification task as follows: given a se-
quence of words wi in a sentence W = {w1, ? ? ? , wn}
and a prosodic label vocabulary (li  L), we need
to predict the best prosodic label sequence L? =
{l1, l2, ? ? ? , ln}. We approximate the conditional
probability to be within a bounded n-gram context.
Thus,
L? = argmax
L
P (L|W,T, S) (4)
? argmax
L
n?
i
p(li|w
i+k
i?k, t
i+k
i?k, s
i+k
i?k) (5)
where W = {w1, ? ? ? , wn} is the word sequence and
T = {t1, ? ? ? , tn}, S = {s1, ? ? ? , sn} are the corre-
sponding part-of-speech and additional syntactic in-
formation sequences. The variable k controls the
context.
The BU corpus is automatically labeled (and
hand-corrected) with part-of-speech (POS) tags.
The POS inventory is the same as the Penn treebank
which includes 47 POS tags: 22 open class categories,
14 closed class categories and 11 punctuation labels.
We also automatically tagged the utterances using
the AT&T POS tagger. The POS tags were mapped
to function and content word categories 1 which was
added as a discrete feature. In addition to the POS
tags, we also annotate the utterance with Supertags
(Bangalore and Joshi, 1999). Supertags encapsulate
predicate-argument information in a local structure.
They are composed with each other using substi-
tution and adjunction operations of Tree-Adjoining
Grammars (TAGs) to derive a dependency analysis
of an utterance and its predicate-argument structure.
Even though there is a potential to exploit the de-
pendency structure between supertags and prosody
labels as demonstrated in (Hirschberg and Rambow,
2001), for this paper we use only the supertag labels.
Finally, we generate one feature vector (?) for
each word in the data set (with local contextual fea-
tures). The best prosodic label sequence is then,
L? = argmax
L
n?
i
P (li|?) (6)
To estimate the conditional distribution P (li|?) we
use the general technique of choosing the maximum
entropy (maxent) distribution that estimates the av-
erage of each feature over the training data (Berger
et al, 1996). This can be written in terms of Gibbs
distribution parameterized with weights ?, where V
is the size of the prosodic label set. Hence,
P (li|?) =
e?li .?
?V
l=1 e
?li .?
(7)
1function and content word features were obtained
through a look-up table based on POS
4
k=3
Corpus Speaker Set Syntactic features accent btone
correct POS tags 84.75 91.39
Entire Set AT&T POS + supertags 84.59 91.34
BU Joint Model (w AT&T POS + supertags) 84.60 91.36
correct POS tags 85.22 91.33
Hasegawa-Johnson et al set AT&T POS + supertags 84.95 91.21
Joint Model (w AT&T POS + supertags) 84.78 91.54
BDC Entire Set AT&T POS + supertags 79.81 90.28
Joint Model (w AT&T POS + supertags) 79.57 89.76
Table 4: Classification results (%) of pitch accents and boundary tones for different syntactic representation (k = 3)
We use the machine learning toolkit LLAMA
(Haffner, 2006) to estimate the conditional distribu-
tion using maxent. LLAMA encodes multiclass max-
ent as binary maxent to increase the training speed
and to scale the method to large data sets. Each of
the V classes in the label set L is encoded as a bit
vector such that, in the vector for class i, the ith bit
is one and all other bits are zero. Finally, V one-
versus-other binary classifiers are used as follows.
P (y|?) = 1? P (y?|?) =
e?y.?
e?y.? + e?y?.?
(8)
where ?y? is the parameter vector for the anti-label y?.
To compute P (li|?), we use the class independence
assumption and require that yi = 1 and for all j 6=
i, yj = 0.
P (li|?) = P (yi|?)
V?
j 6=i
P (yj |?) (9)
5.1 Joint Modeling of Accents and
Boundary Tones
Prosodic prominence and phrasing can also be
viewed as joint events occurring simultaneously. Pre-
vious work by (Wightman and Ostendorf, 1994) sug-
gests that a joint labeling approach may be more
beneficial in prosody labeling. In this scenario,
we treat each word to have one of the four labels
li  L = {accent-btone, accent-none, none-
btone, none-none}. We trained the classifier on
the joint labels and then computed the error rates for
individual classes. The results of prosody prediction
using the set of syntactic-prosodic features for k = 3
is shown in Table 4. The joint modeling approach
provides a marginal improvement in the boundary
tone prediction but is slightly worse for pitch accent
prediction.
5.2 Supertagger performance on
Intermediate Phrase boundaries
Perceptual experiments have indicated that inter-
annotator agreement for ToBI intermediate phrase
boundaries is very low compared to full-intonational
boundaries (Syrdal and McGory, 2000). Interme-
diate phrasing is important in TTS applications to
synthesize appropriate short pauses to make the ut-
terance sound natural. The significance of syntactic
features in the boundary tone prediction prompted
us to examine the effect of predicting intermediate
phrase boundaries in isolation. It is intuitive to ex-
pect supertags to perform well in this task as they
essentially form a local dependency analysis on an
utterance and provide an encoding of the syntactic
phrasal information. We performed this task as a
three way classification where li  L = {btone, ip,
none}. The results of the classifier on IPs is shown
in Table 5.
Model Syntactic features IP accuracy
correct POS tags 83.25
k=2 (bigram context) AT&T POS tags 83.32
supertags 83.37
correct POS tags 83.30
k=3 (trigram context) AT&T POS tags 83.46
supertags 83.74
Table 5: Accuracy (in %) obtained by leave-one out
speaker validation using IPs as a separate class on
entire speaker set
6 Acoustic-prosodic model
We propose two approaches to modeling the
acoustic-prosodic features for prosody prediction.
First, we propose a maximum entropy framework
similar to the syntactic model where we quantize
the acoustic features and model them as discrete
sequences. Second, we use a more traditional ap-
proach where we train continuous observation den-
sity HMMs to represent pitch accents and bound-
ary tones. We first describe the features used in the
acoustic modeling followed by a more detailed de-
scription of the acoustic-prosodic model.
6.1 Acoustic-prosodic features
The BU corpus contains the corresponding acoustic-
prosodic feature file for each utterance. The f0, RMS
energy (e) of the utterance along with features for
5
Pitch accent Boundary tone
Corpus Speaker Set Model Acoustics Acoustics+syntax Acoustics Acoustics+syntax
Entire Set Maxent acoustic model 80.09 84.53 84.10 91.56
HMM acoustic model 70.58 85.13 71.28 92.91
BU Hasegawa-Johnson et al set Maxent acoustic model 80.12 84.84 82.70 91.76
HMM acoustic model 71.42 86.01 73.43 93.09
BDC Entire Set Maxent acoustic model 74.51 78.64 83.53 90.49
Table 6: Classification results of pitch accents and boundary tones (in %) with acoustics only and acoustics+syntax
using both our models
distinction between voiced/unvoiced segment, cross-
correlation values at estimated f0 value and ratio of
first two cross correlation values are computed over
10 msec frame intervals. In our experiments, we use
these values rather than computing them explicitly
which is straightforward with most audio toolkits.
Both the energy and the f0 levels were normalized
with speaker specific means and variances. Delta
and acceleration coefficients were also computed for
each frame. The final feature vector is 6-dimensional
comprising of f0, ?f0, ?2f0, e, ?e, ?2e per frame.
6.2 Maximum Entropy acoustic-prosodic
model
We propose a maximum entropy modeling frame-
work to model the continuous acoustic-prosodic ob-
servation sequence as a discrete sequence through
the means of quantization. The quantized acoustic
stream is then used as a feature vector and the condi-
tional probabilities are approximated by an n-gram
model. This is equivalent to reducing the vocabu-
lary of the acoustic-prosodic features and hence of-
fers better estimates of the conditional probabilities.
Such an n-gram model of quantized continuous fea-
tures is similar to representing the set of features
with a linear fit as done in the tilt intonational model
(Taylor, 1998).
The quantized acoustic-prosodic feature stream is
modeled with a maxent acoustic-prosodic model sim-
ilar to the one described in section 5. Finally, we ap-
pend the syntactic and acoustic features to model the
combined stream with the maxent acoustic-syntactic
model, where the objective criterion for maximiza-
tion is Equation (1). The pitch accent and bound-
ary tone prediction accuracies for quantization per-
formed by considering only the first decimal place
is reported in Table 6. As expected, we found the
classification accuracy to drop with increasing num-
ber of bins used in the quantization due to the small
amount of training data.
6.3 HMM acoustic-prosodic model
We also investigated the traditional HMM approach
to model the high variability exhibited by the
acoustic-prosodic features. First, we trained sepa-
rate context independent single state Gaussian mix-
ture density HMMs for pitch accents and boundary
tones in a generative framework. The label sequence
was decoded using the viterbi algorithm. Next, we
trained HMMs with 3 state left-to-right topology
with uniform segmentation. The segmentations need
to be uniform due to lack of an acoustic-prosodic
model trained on the features pertinent to our task
to obtain forced segmentation.
The final label sequence using the maximum en-
tropy syntactic-prosodic model and the HMM based
acoustic-prosodic model was obtained by combin-
ing the syntactic and acoustic probabilities shown in
Equation (3). The syntactic-prosodic maxent model
outputs a posterior probability for each class per
word. We formed a lattice out of this structure and
composed it with the lattice generated by the HMM
acoustic-prosodic model. The best path was chosen
from the composed lattice through a Viterbi search.
The acoustic-prosodic probability P (A|L,W ) was
raised by a power of ? to adjust the weighting be-
tween the acoustic and syntactic model. The value of
? was chosen as 0.008 and 0.015 for pitch accent and
boundary tone respectively, by tuning on the train-
ing set. The results of the acoustic-prosodic model
and the coupled model are shown in Table 6.
7 Discussion
The baseline experiment with lexical stress obtained
from a pronunciation lexicon for prediction of pitch
accent yields substantially higher accuracy than
chance. This could be particularly useful in resource-
limited languages where prosody labels are usually
not available but one has access to a reasonable lex-
icon with lexical stress markers. Off-the-shelf speech
synthesizers like Festival and AT&T speech synthe-
sizer perform reasonably well in pitch accent and
boundary tone prediction. AT&T speech synthesizer
performs better than Festival in pitch accent predic-
tion and the latter performs better in boundary tone
prediction. This can be attributed to better rules
in the AT&T synthesizer for pitch accent prediction.
Boundary tones are usually highly correlated with
punctuation and Festival seems to capture this well.
However, both these synthesizers generate a high de-
6
gree of false alarms.
Our syntactic-prosodic maximum entropy model
proposed in section 5 outperforms previously re-
ported results on pitch accent and boundary tone
classification. Much of the gain comes from the ro-
bustness of the maximum entropy modeling in cap-
turing the uncertainty in the classification task. Con-
sidering the inter-annotator agreement for ToBI la-
bels is only about 81% for pitch accents and 93% for
boundary tones, the maximum entropy framework is
able to capture the uncertainty present in manual an-
notation. The supertag feature offers additional dis-
criminative information over the part-of-speech tags
(also as shown by (Hirschberg and Rambow, 2001).
The maximum entropy acoustic-prosodic model
discussed in section 6.2 performs reasonably well in
isolation. This is a simple method and the quantiza-
tion resolution can be adjusted based on the amount
of data available for training. However, the model
does not perform as well when combined with the
syntactic features. We conjecture that the gener-
alization provided by the acoustic HMM model is
complementary to that provided by the maximum
entropy model, resulting in better accuracy when
combined together as compared to that of a maxent-
based acoustic and syntactic model.
The weighted maximum entropy syntactic-
prosodic model and HMM acoustic-prosodic model
performs the best in pitch accent and boundary tone
classification. The classification accuracies are as
good as the inter-annotator agreement for the ToBI
labels. Our HMM acoustic-prosodic model is a gen-
erative model and does not assume the knowledge
of word boundaries in predicting the prosodic labels
as in most approaches (Hirschberg, 1993; Wightman
and Ostendorf, 1994; Hasegawa-Johnson et al,
2005). This makes it possible to have true parallel
prosody prediction during speech recognition. The
weighted approach also offers flexibility in prosody
labeling for either speech synthesis or speech recog-
nition. While the syntactic-prosodic model would
be more discriminative for speech synthesis, the
acoustic-prosodic model is more appropriate for
speech recognition.
8 Conclusions and Future Work
In this paper, we described a maximum entropy
modeling framework for automatic prosody label-
ing. We presented two schemes for prosody label-
ing that utilize the acoustic and syntactic informa-
tion from the input utterance, a maximum entropy
model that models the acoustic-syntactic informa-
tion as a sequence and the other that combines the
maximum entropy syntactic-prosodic model and a
HMM based acoustic-prosodic model. We also used
enriched syntactic information in the form of su-
pertags in addition to POS tags. The supertags
provide an improvement in both the pitch accent
and boundary tone classification. Especially, in the
case where the input utterance is automatically POS
tagged (and not hand-corrected), supertags provide
a marginal but definite improvement in prosody la-
beling. The maximum entropy syntactic-prosodic
model alone resulted in pitch accent and bound-
ary tone accuracies of 85.2% and 91.5% on training
and test sets identical to (Chen et al, 2004). As
far as we know, these are the best results on the
BU corpus using syntactic information alone and a
train-test split that does not contain the same speak-
ers. The acoustic-syntactic maximum entropy model
performs better than its syntactic-prosodic counter-
part for the boundary tone case but is slightly worse
for pitch accent scenario partly due to the approx-
imation involved in quantization. But these results
are still better than the baseline results from out-
of-the-box speech synthesizers. Finally, our com-
bined maximum entropy syntactic-prosodic model
and HMM acoustic-prosodic model performs the best
with pitch accent and boundary tone labeling accu-
racies of 86.0% and 93.1% respectively.
As a continuation of our work, we are incorpo-
rating our automatic prosody labeler in a speech-
to-speech translation framework. Typically, state-
of-the-art speech translation systems have a source
language recognizer followed by a machine transla-
tion system. The translated text is then synthesized
in the target language with prosody predicted from
text. In this process, some of the critical prosodic
information present in the source data is lost during
translation. With reliable prosody labeling in the
source language, one can transfer the prosody to the
target language (this is feasible for languages with
phrase level correspondence). The prosody labels by
themselves may or may not improve the translation
accuracy but they provide a framework where one
can obtain prosody labels in the target language from
the speech signal rather than depending on a lexical
prosody prediction module in the target language.
Acknowledgements
We would like to thank Vincent Goffin, Stephan
Kanthak, Patrick Haffner, Enrico Bocchieri for their
support with acoustic modeling tools. We are also
thankful to Alistair Conkie, Yeon-Jun Kim, Ann
Syrdal and Julia Hirschberg for their help and guid-
ance with the synthesis components and ToBI label-
ing standard.
References
P. D. Agu?ero, J. Adell, and A. Bonafonte. 2006.
Prosody generation for speech-to-speech transla-
7
tion. In Proceedings of ICASSP, Toulouse, France,
May.
S. Ananthakrishnan and S. Narayanan. 2005. An au-
tomatic prosody recognizer using a coupled multi-
stream acoustic model and a syntactic-prosodic
language model. In In Proceedings of ICASSP,
Philadelphia, PA, March.
AT&T Natural Voices speech synthesizer.
http://www.naturalvoices.att.com.
S. Bangalore and A. K. Joshi. 1999. Supertagging:
An approach to almost parsing. Computational
Linguistics, 25(2), June.
A. Berger, S. D. Pietra, and V. D. Pietra. 1996. A
maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
A. W. Black, P. Taylor, and R. Caley.
1998. The Festival speech synthesis system.
http://festvox.org/festival.
J. M. Brenier, D. Cer, and D. Jurafsky. 2005. The
detection of emphatic words using acoustic and
lexical features. In In Proceedings of Eurospeech.
I. Bulyko and M. Ostendorf. 2001. Joint prosody
prediction and unit selection for concatenative
speech synthesis. In Proc. of ICASSP.
K. Chen, M. Hasegawa-Johnson, and A. Cohen.
2004. An automatic prosody labeling system using
ANN-based syntactic-prosodic model and GMM-
based acoustic-prosodic model. In Proceedings of
ICASSP.
A. Conkie, G. Riccardi, and R. C. Rose. 1999.
Prosody recognition from speech utterances using
acoustic and linguistic based models of prosodic
events. In Proc. Eurospeech, pages 523?526, Bu-
dapest, Hungary.
M. Gregory and Y. Altun. 2004. Using conditional
random fields to predict pitch accent in conver-
sational speech. In 42nd Annual Meeting of the
Association for Computational Linguistics (ACL).
P. Haffner. 2006. Scaling large margin classifiers for
spoken language understanding. Speech Commu-
nication, 48(iv):239?261.
M. Hasegawa-Johnson, K. Chen, J. Cole, S. Borys,
S. Kim, A. Cohen, T. Zhang, J. Choi, H. Kim,
T. Yoon, and S. Chavara. 2005. Simultaneous
recognition of words and prosody in the boston
university radio speech corpus. Speech Communi-
cation, 46:418?439.
J. Hirschberg and C. Nakatani. 1996. A prosodic
analysis of discourse segments in direction-giving
monologues. In Proceedings of the 34th confer-
ence on Association for Computational Linguis-
tics, pages 286?293.
J. Hirschberg and O. Rambow. 2001. Learning
prosodic features using a tree representation. In
Proceedings of Eurospeech, pages 1175?1180, Aal-
borg.
J. Hirschberg. 1993. Pitch accent in context: Pre-
dicting intonational prominence from text. Artifi-
cial Intelligence, 63(1-2).
I. Lehiste. 1970. Suprasegmentals. MIT Press, Cam-
bridge, MA.
X. Ma, W. Zhang, Q. Shi, W. Zhu, and L. Shen.
2003. Automatic prosody labeling using both
text and acoustic information. In Proceedings of
ICASSP, volume 1, pages 516?519, April.
E. No?th, A. Batliner, A. Kie?ling, R. Kompe, and
H. Niemann. 2000. VERBMOBIL: The use of
prosody in the linguistic components of a speech
understanding system. IEEE Transactions on
Speech and Audio processing, 8(5):519?532.
M. Ostendorf, P. J. Price, and S. Shattuck-Hufnagel.
1995. The Boston University Radio News Corpus.
Technical Report ECS-95-001, Boston University,
March.
K. Ross and M. Ostendorf. 1996. Prediction of ab-
stract prosodic labels for speech synthesis. Com-
puter Speech and Language, 10:155?185, Oct.
P. Shimei and K. McKeown. 1999. Word infor-
mativeness and automatic pitch accent modeling.
In In Proceedings of EMNLP/VLC, College Park,
Maryland.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert,
and J. Hirschberg. 1992. ToBI: A standard for la-
beling English prosody. In Proceedings of ICSLP,
pages 867?870.
X. Sun. 2002. Pitch accent prediction using ensem-
ble machine learning. In Proc. of ICSLP.
A. K. Syrdal and J. McGory. 2000. Inter-transcriber
reliability of tobi prosodic labeling. In Proc. IC-
SLP, pages 235?238, Beijing, China.
P. Taylor. 1998. The tilt intonation model. In Proc.
ICSLP, volume 4, pages 1383?1386.
C. W. Wightman and M. Ostendorf. 1994. Auto-
matic labeling of prosodic patterns. IEEE Trans-
actions on Speech and Audio Processing, 2(3):469?
481.
8
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 89?92, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Transonics: A Practical Speech-to-Speech Translator for English-Farsi
Medical Dialogues
Emil Ettelaie, Sudeep Gandhe, Panayiotis Georgiou,
Kevin Knight, Daniel Marcu, Shrikanth Narayanan ,
David Traum
University of Southern California
Los Angeles, CA 90089
ettelaie@isi.edu, gandhe@ict.usc.edu,
georgiou@sipi.usc.edu, knight@isi.edu,
marcu@isi.edu, shri@sipi.usc.edu,
traum@ict.use.edu
Robert Belvin
HRL Laboratories, LLC
3011 Malibu Canyon Rd.
Malibu, CA 90265
rsbelvin@hrl.com
Abstract
We briefly describe a two-way speech-to-
speech English-Farsi translation system
prototype developed for use in doctor-
patient interactions.  The overarching
philosophy of the developers has been to
create a system that enables effective
communication, rather than focusing on
maximizing component-level perform-
ance.  The discussion focuses on the gen-
eral approach and evaluation of the
system by an independent government
evaluation team.
1 Introduction
In this paper we give a brief description of a
two-way speech-to-speech translation system,
which was created under a collaborative effort
between three organizations within USC (the
Speech Analysis and Interpretation Lab of the
Electrical Engineering department, the Information
Sciences Institute, and the Institute for Creative
Technologies) and the Information Sciences Lab of
HRL Laboratories.  The system is intended to pro-
vide a means of enabling communication between
monolingual English speakers and monolingual
Farsi (Persian) speakers.  The system is targeted at
a domain which may be roughly characterized as
"urgent care" medical interactions, where the Eng-
lish speaker is a medical professional and the Farsi
speaker is the patient.  In addition to providing a
brief description of the system (and pointers to pa-
pers which contain more detailed information), we
give an overview of the major system evaluation
activities.
2 General Design of the system
Our system is comprised of seven speech and
language processing components, as shown in Fig.
1. Modules communicate using a centralized mes-
sage-passing system. The individual subsystems
are the Automatic Speech Recognition (ASR) sub-
system, which uses n-gram Language Models
(LM) and produces n-best lists/lattices along with
the decoding confidence scores. The output of the
ASR is sent to the Dialog Manager (DM), which
displays the n-best and passes one hypothesis on to
the translation modules, according to a user-
configurable state. The DM sends translation re-
quests to the Machine Translation (MT) unit. The
MT unit works in two modes: Classifier based MT
and a fully Stochastic MT. Depending on the dia-
logue manager mode, translations can be sent to
the unit selection based Text-To-Speech synthe-
sizer (TTS), to provide the spoken output. The
same basic pipeline works in both directions: Eng-
lish ASR, English-Persian MT, Persian TTS, or
Persian ASR, Persian-English MT, English TTS.
There is, however, an asymmetry in the dia-
logue management and control, given the desire for
the English-speaking doctor to be in control of the
device and the primary "director" of the dialog.
The English ASR used the University of Colo-
rado Sonic recognizer, augmented primarily with
LM data collected from multiple sources, including
89
our own large-scale simulated doctor-patient dia-
logue corpus based on recordings of medical stu-
dents examining standardized patients (details in
Belvin et al 2004).
1
 The Farsi acoustic models r e-
quired an eclectic approach due to the lack of ex-
isting labeled speech corpora.  The approach
included borrowing acoustic data from English by
means of developing a sub-phonetic mapping be-
tween the two languages, as detailed in (Srini-
vasamurthy & Narayanan 2003), as well as use of
a small existing Farsi speech corpus (FARSDAT),
and our own team-internally generated acoustic
data.  Language modeling data was also obtained
from multiple sources.  The Defense Language
Institute translated approximately 600,000 words
of English medical dialogue data (including our
standardized patient data mentioned above), and in
addition, we were able to obtain usable Farsi text
from mining the web for electronic news sources.
Other  smaller  amounts of  training  data  were ob
tained from various sources, as detailed in  (Nara-
yanan et al 2003, 2004).  Additional detail on de-
velopment methods for all of these components,
system integration and evaluation can also be
found in the papers just cited.
The MT components, as noted, consist of both a
Classifier and a stochastic translation engine,  both
                                                           
1
 Standardized Patients are typically actors who have been
trained by doctors or nurses to portray symptoms of particular
illnesses or injuries.  They are used extensively in medical
education so that doctors in training don't have to "practice"
on real patients.
developed by USC-ISI team members.  The Eng-
lish Classifier uses approximately 1400 classes
consisting mostly of standard questions used by
medical care providers in medical interviews.
Each class has a large number of paraphrases asso-
ciated with it, such that if the care provider speaks
one of those phrases, the system will identify it
with the class and translate it to Farsi via table-
lookup.  If the Classifier cannot succeed in finding
a match exceeding a confidence threshold, the sto-
chastic MT engine will be employed.  The sto-
chastic MT engine relies on n-gram
correspondences between the source and target
languages.  As with ASR, the performance of the
component is highly dependent on very large
amounts of training data.  Again, there were multi-
ple sources of training data used, the most signifi-
cant being the data generated by our own team's
English collection effort, supported by translation
into Farsi by DLI. Further details of the MT com-
ponents can be found in Narayanan et al, op.cit.
3 Enabling Effective Communication
The approach taken in the development of Tran-
sonics was what can be referred to as the total
communication pathway.  We are not so concerned
with trying to maximize the performance of a
given component of the system, but rather with the
effectiveness of the system as a whole in facilitat-
ing actual communication.  To this end, our design
and development included the following:
MT
English to Farsi
Farsi to English
ASR
English
Prompts or TTS
Farsi
Prompts or TTS
English
ASR
Farsi
GUI:
prompts,
 confirmations,
 ASR switch
Dialog
Manager
SMT
English to Farsi
Farsi to English
Figure 1: Architecture of the Transonics system.  The Dialogue Manager acts as the hub through which the
individual components interact.
90
i. an "educated guess" capability (system
guessing at the meaning of an utterance) from the
Classifier translation mechanism?this proved very
useful for noisy ASR output, especially for the re-
stricted domain of medical interviews.
ii. a flexible and robust SMT good for filling in
where the more accurate Classifier misses.
iii. exploitation of a partial n-best list as part of
the GUI used by the doctor/medic for the English
ASR component and the Farsi-to-English transla-
tion component.
iv. a dialog manager which in essence occa-
sionally makes  "suggestions" (for next questions
for the doctor to ask) based on query sets which are
topically related to the query the system believes it
recognized the doctor to have spoken.
Overall, the system achieves a respectable level of
performance in terms of allowing users to follow a
conversational thread in a fairly coherent way, de-
spite the presence of frequent ungrammatical or
awkward translations (i.e. despite what we might
call non-catastrophic errors).
4 Testing and Evaluation
In addition to our own laboratory tests, the sys-
tem was evaluated by MITRE as part of the
DARPA program.  There were two parts to the
MITRE evaluations, a "live" part, designed pri-
marily to evaluate the overall task-oriented effec-
tiveness of the systems, and a "canned" part,
designed primarily to evaluate individual compo-
nents of the systems.
The live evaluation consisted of six medical
professionals (doctors, corpsmen and physician?s
assistants from the Naval Medical Center at Quan-
tico, and a nurse from a civilian institution) con-
ducting unrehearsed "focused history and physical
exam" style interactions with Farsi speakers play-
ing the role of patients, where the English-speaking
doctor and the Farsi-speaking patient communi-
cated by means of the Transonics system.  Since
the cases were common enough to be within the
realm of general internal medicine, there was no
attempt to align ailments with medical specializa-
tions among the medical professionals.
MITRE endeavored to find primarily monolin-
gual Farsi speakers to play the role of patient, so as
to provide a true test of the system to enable com-
munication between people who would otherwise
have no way to communicate.  This goal was only
partially realized, since one of the two Farsi patient
role-players was partially competent in English.
2
The Farsi-speaking role-players were trained by a
medical education specialist in how to simulate
symptoms of someone with particular injuries or
illnesses.  Each Farsi-speaking patient role-player
received approximately 30 minutes of training for
any given illness or injury.  The approach was
similar to that used in training standardized pa-
tients, mentioned above (footnote 1) in connection
with generation of the dialogue corpus.
MITRE established a number of their own met-
rics for measuring the success of the systems, as
well as using previously established metrics.  A
full discussion of these metrics and the results ob-
tained for the Transonics system is beyond the
scope of this paper, though we will note that one of
the most important of these was task-completion.
There were 5 significant facts (5 distinct facts for
each of 12 different scenarios) that the medical
professional should have discovered in the process
of interviewing/examining each Farsi patient.  The
USC/HRL system averaged 3 out of the 5 facts,
which was a slightly above-average score among
the 4 systems evaluated.  A "significant fact" con-
sisted of determining a fact which was critical for
diagnosis, such as the fact that the patient had been
injured in a fall down a stairway, the fact that the
patient was experiencing blurred vision, and so on.
Significant facts did not include items such as a
patient's age or marital status.
3
  We report on this
measure in that it is perhaps the single most im-
portant component in the assessment, in our opin-
ion, in that it is an indication of many aspects of
the system, including both directions of the trans-
lation system.  That is, the doctor will very likely
conclude correct findings only if his/her question is
translated correctly to the patient, and also if the
patient's answer is translated correctly for the doc-
tor.  In a true medical exam, the doctor may have
                                                           
2
 There were additional difficulties encountered as well, hav-
ing to do with one of the role-players not adequately grasping
the goal of role-playing.  This experience highlighted the
many challenges inherent in simulating domain-specific
spontaneous dialogue.
3
 Unfortunately, there was no baseline evaluation this could be
compared to,  such as assessing whether any of the critical
facts could be determined without the use of the system at all.
91
other means of determining some critical facts
even in the absence of verbal communication, but
in the role-playing scenario described, this is very
unlikely.  Although this measure is admittedly
coarse-grained, it simultaneously shows, in a crude
sense, that the USC/HRL system compared fa-
vorably against the other 3 systems in the evalua-
tion, and also that there is still significant room for
improvement in the state of the art.
As noted, MITRE devised a component evalua-
tion process also consisting of running 5 scripted
dialogs through the systems and then measuring
ASR and MT performance.  The two primary
component measures were a version of BLEU for
the MT component (modified slightly to handle the
much shorter sentences typical of this kind of dia-
log) and a standard Word-Error Rate for the ASR
output.  These scores are shown below.
Table 1:  Farsi BLEU Scores
IBM BLEU
ASR
IBM BLEU
TEXT
English to Farsi
0.2664 0.3059
Farsi  to English 0.2402 0.2935
The reason for the two different BLEU scores is
that one was calculated based on the ASR compo-
nent output being translated to the other language,
while the other was calculated from human tran-
scribed text being translated to the other language.
Table 2:  HRL/USC WER for Farsi and English
English Farsi
WER 11.5% 13.4%
5 Conclusion
In this paper we have given an overview of the
design, implementation and evaluation of the Tran-
sonics speech-to-speech translation system for nar-
row domain two-way translation.  Although there
are still many significant hurdles to be overcome
before this kind of technology can be called truly
robust, with appropriate training and two coopera-
tive interlocutors, we can now see some degree of
genuine communication being enabled.  And this is
very encouraging indeed.
6 Acknowledgements
This work was supported primarily by the DARPA
CAST/Babylon program, contract N66001-02-C-
6023.
References
R. Belvin, W. May, S. Narayanan, P. Georgiou, S. Gan-
javi.  2004. Creation of a Doctor-Patient Dialogue
Corpus Using Standardized Patients. In Proceedings of
the Language Resources and Evaluation Conference
(LREC), Lisbon, Portugal.
S. Ganjavi, P. G. Georgiou, and S. Narayanan. 2003.
Ascii based transcription schemes for languages with
the Arabic script: The case of Persian. In Proc. IEEE
ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,
K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy,
D. Traum and D. Wang.  2003. Transonics: A speech
to speech system for English-Persian Interactions,
Proc. IEEE ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C. M.
Hein, S. Kadambe, K. Knight, D. Marcu, H. E.
Neely, N. Srinivasamurthy, D. Traum, and D. Wang.
2004. The Transonics Spoken Dialogue Translator:
An aid for English-Persian Doctor-Patient interviews,
in Working Notes of the AAAI Fall symposium on
Dialogue Systems for Health Communication, pp 97-
-103.
N. Srinivasamurthy, and S. Narayanan. 2003. Language
adaptive Persian speech recognition. In proceedings
of Eurospeech 2003.
92
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 225?228,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Enriching spoken language translation with dialog acts
Vivek Kumar Rangarajan Sridhar
Shrikanth Narayanan
Speech Analysis and Interpretation Laboratory
University of Southern California
vrangara@usc.edu,shri@sipi.usc.edu
Srinivas Bangalore
AT&T Labs - Research
180 Park Avenue
Florham Park, NJ 07932, U.S.A.
srini@research.att.com
Abstract
Current statistical speech translation ap-
proaches predominantly rely on just text tran-
scripts and do not adequately utilize the
rich contextual information such as conveyed
through prosody and discourse function. In
this paper, we explore the role of context char-
acterized through dialog acts (DAs) in statis-
tical translation. We demonstrate the integra-
tion of the dialog acts in a phrase-based statis-
tical translation framework, employing 3 lim-
ited domain parallel corpora (Farsi-English,
Japanese-English and Chinese-English). For
all three language pairs, in addition to produc-
ing interpretable DA enriched target language
translations, we also obtain improvements in
terms of objective evaluation metrics such as
lexical selection accuracy and BLEU score.
1 Introduction
Recent approaches to statistical speech translation
have relied on improving translation quality with
the use of phrase translation (Och and Ney, 2003;
Koehn, 2004). The quality of phrase translation
is typically measured using n-gram precision based
metrics such as BLEU (Papineni et al, 2002) and
NIST scores. However, in many dialog based speech
translation scenarios, vital information beyond what
is robustly captured by words and phrases is car-
ried by the communicative act (e.g., question, ac-
knowledgement, etc.) representing the function of
the utterance. Our approach for incorporating di-
alog act tags in speech translation is motivated by
the fact that it is important to capture and convey
not only what is being communicated (the words)
but how something is being communicated (the con-
text). Augmenting current statistical translation
frameworks with dialog acts can potentially improve
translation quality and facilitate successful cross-
lingual interactions in terms of improved informa-
tion transfer.
Dialog act tags have been previously used in the
VERBMOBIL statistical speech-to-speech transla-
tion system (Reithinger et al, 1996). In that work,
the predicted DA tags were mainly used to improve
speech recognition, semantic evaluation, and infor-
mation extraction modules. Discourse information
in the form of speech acts has also been used in in-
terlingua translation systems (Mayfield et al, 1995)
to map input text to semantic concepts, which are
then translated to target text.
In contrast with previous work, in this paper we
demonstrate how dialog act tags can be directly ex-
ploited in phrase based statistical speech translation
systems (Koehn, 2004). The framework presented
in this paper is particularly suited for human-human
and human-computer interactions in a dialog set-
ting, where information loss due to erroneous con-
tent may be compensated to some extent through the
correct transfer of the appropriate dialog act. The
dialog acts can also be potentially used for impart-
ing correct utterance level intonation during speech
synthesis in the target language. Figure 1 shows an
example where the detection and transfer of dialog
act information is beneficial in resolving ambiguous
intention associated with the translation output.
Figure 1: Example of speech translation output enriched with
dialog act
The remainder of this paper is organized as fol-
lows: Section 2 describes the dialog act tagger used
in this work, Section 3 formulates the problem, Sec-
tion 4 describes the parallel corpora used in our ex-
periments, Section 5 summarizes our experimental
results and Section 6 concludes the paper with a
brief discussion and outline for future work.
2 Dialog act tagger
In this work, we use a dialog act tagger trained on
the Switchboard DAMSL corpus (Jurafsky et al,
225
1998) using a maximum entropy (maxent) model.
The Switchboard-DAMSL (SWBD-DAMSL) cor-
pus consists of 1155 dialogs and 218,898 utterances
from the Switchboard corpus of telephone conver-
sations, tagged with discourse labels from a shal-
low discourse tagset. The original tagset of 375
unique tags was clustered to obtain 42 dialog tags
as in (Jurafsky et al, 1998). In addition, we also
grouped the 42 tags into 7 disjoint classes, based
on the frequency of the classes and grouped the re-
maining classes into an ?Other? category constitut-
ing less than 3% of the entire data. The simplified
tagset consisted of the following classes: statement,
acknowledgment, abandoned, agreement, question,
appreciation, other.
We use a maximum entropy sequence tagging
model for the automatic DA tagging. Given a se-
quence of utterances U = u1, u2, ? ? ? , un and a
dialog act vocabulary (di ? D, |D| = K), we
need to assign the best dialog act sequence D? =
d1, d2, ? ? ? , dn. The classifier is used to assign to
each utterance a dialog act label conditioned on a
vector of local contextual feature vectors comprising
the lexical, syntactic and acoustic information. We
used the machine learning toolkit LLAMA (Haffner,
2006) to estimate the conditional distribution using
maxent. The performance of the maxent dialog act
tagger on a test set comprising 29K utterances of
SWBD-DAMSL is shown in Table 1.
Accuracy (%)
Cues used (current utterance) 42 tags 7 tags
Lexical 69.7 81.9
Lexical+Syntactic 70.0 82.4
Lexical+Syntactic+Prosodic 70.4 82.9
Table 1: Dialog act tagging accuracies for various cues on the
SWBD-DAMSL corpus.
3 Enriched translation using DAs
If Ss, Ts and St, Tt are the speech signals and equiv-
alent textual transcription in the source and target
language, and Ls the enriched representation for the
source speech, we formalize our proposed enriched
S2S translation in the following manner:
S?t = argmaxSt
P (St|Ss) (1)
P (St|Ss) =
?
Tt,Ts,Ls
P (St, Tt, Ts, Ls|Ss) (2)
?
?
Tt,Ts,Ls
P (St|Tt, Ls).P (Tt, Ts, Ls|Ss) (3)
where Eq.(3) is obtained through conditional inde-
pendence assumptions. Even though the recogni-
tion and translation can be performed jointly (Ma-
tusov et al, 2005), typical S2S translation frame-
works compartmentalize the ASR, MT and TTS,
with each component maximized for performance
individually.
max
St
P (St|Ss) ? maxSt P (St|T
?
t , L?s)
?max
Tt
P (Tt|T ?s , L?s) (4)
?max
Ls
P (Ls|T ?s , Ss)?maxTs P (Ts|Ss)
where T ?s , T ?t and S?t are the arguments maximiz-
ing each of the individual components in the transla-
tion engine. L?s is the rich annotation detected from
the source speech signal and text, Ss and T ?s respec-
tively. In this work, we do not address the speech
synthesis part and assume that we have access to the
reference transcripts or 1-best recognition hypothe-
sis of the source utterances. The rich annotations
(Ls) can be syntactic or semantic concepts (Gu et
al., 2006), prosody (Agu?ero et al, 2006), or, as in
this work, dialog act tags.
3.1 Phrase-based translation with dialog acts
One of the currently popular and predominant
schemes for statistical translation is the phrase-
based approach (Koehn, 2004). Typical phrase-
based SMT approaches obtain word-level align-
ments from a bilingual corpus using tools such as
GIZA++ (Och and Ney, 2003) and extract phrase
translation pairs from the bilingual word alignment
using heuristics. Suppose, the SMT had access to
source language dialog acts (Ls), the translation
problem may be reformulated as,
T ?t = argmaxTt
P (Tt|Ts, Ls)
= argmax
Tt
P (Ts|Tt, Ls).P (Tt|Ls) (5)
The first term in Eq.(5) corresponds to a dialog act
specific MT model and the second term to a dia-
log act specific language model. Given sufficient
amount of training data such a system can possibly
generate hypotheses that are more accurate than the
scheme without the use of dialog acts. However, for
small scale and limited domain applications, Eq.(5)
leads to an implicit partitioning of the data corpus
226
Training Test
Farsi Eng Jap Eng Chinese Eng Farsi Eng Jap Eng Chinese Eng
Sentences 8066 12239 46311 925 604 506
Running words 76321 86756 64096 77959 351060 376615 5442 6073 4619 6028 3826 3897
Vocabulary 6140 3908 4271 2079 11178 11232 1487 1103 926 567 931 898
Singletons 2819 1508 2749 1156 4348 4866 903 573 638 316 600 931
Table 2: Statistics of the training and test data used in the experiments.
and might generate inferioir translations in terms of
lexical selection accuracy or BLEU score.
A natural step to overcome the sparsity issue is
to employ an appropriate back-off mechanism that
would exploit the phrase translation pairs derived
from the complete data. A typical phrase transla-
tion table consists of 5 phrase translation scores for
each pair of phrases, source-to-target phrase transla-
tion probability (?1), target-to-source phrase transla-
tion probability (?2), source-to-target lexical weight
(?3), target-to-word lexical weight (?4) and phrase
penalty (?5= 2.718). The lexical weights are the
product of word translation probabilities obtained
from the word alignments. To each phrase trans-
lation table belonging to a particular DA-specific
translation model, we append those entries from the
baseline model that are not present in phrase table
of the DA-specific translation model. The appended
entries are weighted by a factor ?.
(Ts ? Tt)L?s = (Ts ? Tt)Ls ? {?.(Ts ? Tt)
s.t. (Ts ? Tt) 6? (Ts ? Tt)Ls} (6)
where (Ts ? Tt) is a short-hand1 notation for a
phrase translation table. (Ts ? Tt)Ls is the DA-
specific phrase translation table, (Ts ? Tt) is the
phrase translation table constructed from entire data
and (Ts ? Tt)L?s is the newly interpolated phrase
translation table. The interpolation factor ? is used
to weight each of the four translation scores (phrase
translation and lexical probabilities for the bilan-
guage) with the phrase penalty remaining a con-
stant. Such a scheme ensures that phrase translation
pairs belonging to a specific DA model are weighted
higher and also ensures better coverage than a parti-
tioned data set.
4 Data
We report experiments on three different paral-
lel corpora: Farsi-English, Japanese-English and
1(Ts ? Tt) represents the mapping between source alpha-
bet sequences to target alhabet sequences, where every pair
(ts1, ? ? ? , tsn, tt1, ? ? ? , ttm) has a weight sequence ?1, ? ? ? , ?5
(five weights).
Chinese-English. The Farsi-English data used in
this paper was collected for human-mediated doctor-
patient mediated interactions in which an English
speaking doctor interacts with a Persian speaking
patient (Narayanan et al, 2006). We used a subset
of this corpus consisting of 9315 parallel sentences.
The Japanese-English parallel corpus is a part
of the ?How May I Help You? (HMIHY) (Gorin
et al, 1997) corpus of operator-customer conversa-
tions related to telephone services. The corpus con-
sists of 12239 parallel sentences. The conversations
are spontaneous even though the domain is lim-
ited. The Chinese-English corpus corresponds to the
IWSLT06 training and 2005 development set com-
prising 46K and 506 sentences respectively (Paul,
2006).
5 Experiments and Results
In all our experiments we assume that the same di-
alog act is shared by a parallel sentence pair. Thus,
even though the dialog act prediction is performed
for English, we use the predicted dialog act as the di-
alog act for the source language sentence. We used
the Moses2 toolkit for statistical phrase-based trans-
lation. The language models were trigram models
created only from the training portion of each cor-
pus. Due to the relatively small size of the corpora
used in the experiments, we could not devote a sep-
arate development set for tuning the parameters of
the phrase-based translation scheme. Hence, the ex-
periments are strictly performed on the training and
test sets reported in Table 23.
The lexical selection accuracy and BLEU scores
for the three parallel corpora is presented in Table 3.
Lexical selection accuracy is measured in terms of
the F-measure derived from recall ( |Res?Ref ||Ref | ? 100)
and precision ( |Res?Ref ||Res| ? 100), where Ref is the
set of words in the reference translation and Res is
2http://www.statmt.org/moses
3A very small subset of the data was reserved for optimizing
the interpolation factor (?) described in Section 3.1
227
F-score (%) BLEU (%)
w/o DA tags w/ DA tags w/o DA tags w/ DA tags
Language pair 7tags 42tags 7tags 42tags
Farsi-English 56.46 57.32 57.74 22.90 23.50 23.75
Japanese-English 79.05 79.40 79.51 54.15 54.21 54.32
Chinese-English 65.85 67.24 67.49 48.59 52.12 53.04
Table 3: F-measure and BLEU scores with and without use of dialog act tags.
the set of words in the translation output. Adding di-
alog act tags (either 7 or 42 tag vocabulary) consis-
tently improves both the lexical selection accuracy
and BLEU score for all the language pairs. The im-
provements for Farsi-English and Chinese-English
corpora are more pronounced than the improve-
ments in Japanese-English corpus. This is due to the
skewed distribution of dialog acts in the Japanese-
English corpus; 80% of the test data are statements
while other and questions category make up 16%
and 3.5% of the data respectively. The important
observation here is that, appending DA tags in the
form described in this work, can improve translation
performance even in terms of conventional objective
evaluation metrics. However, the performance gain
measured in terms of objective metrics that are de-
signed to reflect only the orthographic accuracy dur-
ing translation is not a complete evaluation of the
translation quality of the proposed framework. We
are currently planning of adding human evaluation
to bring to fore the usefulness of such rich anno-
tations in interpreting and supplementing typically
noisy translations.
6 Discussion and Future Work
It is important to note that the dialog act tags used
in our translation system are predictions from the
maxent based DA tagger described in Section 2. We
do not have access to the reference tags; thus, some
amount of error is to be expected in the DA tagging.
Despite the lack of reference DA tags, we are still
able to achieve modest improvements in the trans-
lation quality. Improving the current DA tagger and
developing suitable adaptation techniques are part of
future work.
While we have demonstrated here that using dia-
log act tags can improve translation quality in terms
of word based automatic evaluation metrics, the real
benefits of such a scheme would be attested through
further human evaluations. We are currently work-
ing on conducting subjective evaluations.
References
P. D. Agu?ero, J. Adell, and A. Bonafonte. 2006. Prosody
generation for speech-to-speech translation. In Proc.
of ICASSP, Toulouse, France, May.
A. Gorin, G. Riccardi, and J. Wright. 1997. How May I
Help You? Speech Communication, 23:113?127.
L. Gu, Y. Gao, F. H. Liu, and M. Picheny. 2006.
Concept-based speech-to-speech translation using
maximum entropy models for statistical natural con-
cept generation. IEEE Transactions on Audio, Speech
and Language Processing, 14(2):377?392, March.
P. Haffner. 2006. Scaling large margin classifiers for spo-
ken language understanding. Speech Communication,
48(iv):239?261.
D. Jurafsky, R. Bates, N. Coccaro, R. Martin, M. Meteer,
K. Ries, E. Shriberg, S. Stolcke, P. Taylor, and C. Van
Ess-Dykema. 1998. Switchboard discourse language
modeling project report. Technical report research
note 30, Johns Hopkins University, Baltimore, MD.
P. Koehn. 2004. Pharaoh: A beam search decoder for
phrasebased statistical machine translation models. In
Proc. of AMTA-04, pages 115?124.
E. Matusov, S. Kanthak, and H. Ney. 2005. On the in-
tegration of speech recognition and statistical machine
translation. In Proc. of Eurospeech.
L. Mayfield, M. Gavalda, W. Ward, and A. Waibel.
1995. Concept-based speech translation. In Proc. of
ICASSP, volume 1, pages 97?100, May.
S. Narayanan et al 2006. Speech recognition engineer-
ing issues in speech to speech translation system de-
sign for low resource languages and domains. In Proc.
of ICASSP, Toulose, France, May.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. Technical report, IBM T.J. Watson Re-
search Center.
M. Paul. 2006. Overview of the IWSLT 2006 Evaluation
Campaign. In Proc. of the IWSLT, pages 1?15, Kyoto,
Japan.
N. Reithinger, R. Engel, M. Kipp, and M. Klesen. 1996.
Predicting dialogue acts for a speech-to-speech trans-
lation system. In Proc. of ICSLP, volume 2, pages
654?657, Oct.
228
A Transcription Scheme for Languages Employing the Arabic Script 
Motivated by Speech Processing Application 
Shadi GANJAVI 
*Department of Linguistics  
University of Southern California 
ganajvi@usc.edu 
Panayiotis G. GEORGIOU,  
Shrikanth NARAYANAN* 
Department of Electrical Engineering 
Speech Analysis & Interpretation 
Laboratory (sail.usc.edu) 
[georgiou, shri]@sipi.usc.edu 
 
Abstract 
This paper offers a transcription system for 
Persian, the target language in the Transonics 
project, a speech-to-speech translation system 
developed as a part of the DARPA Babylon 
program (The DARPA Babylon Program; 
Narayanan, 2003).  In this paper, we discuss 
transcription systems needed for automated 
spoken language processing applications in 
Persian that uses the Arabic script for writing.  
This system can easily be modified for Arabic, 
Dari, Urdu and any other language that uses 
the Arabic script. The proposed system has 
two components. One is a phonemic based 
transcription of sounds for acoustic modelling 
in Automatic Speech Recognizers and for Text 
to Speech synthesizer, using ASCII based 
symbols, rather than International Phonetic 
Alphabet symbols.  The other is a hybrid 
system that provides a minimally-ambiguous 
lexical representation that explicitly includes 
vocalic information; such a representation is 
needed for language modelling, text to speech 
synthesis and machine translation. 
1 Introduction 
Speech-to-speech (S2S) translation systems 
present many challenges, not only due to the 
complex nature of the individual technologies 
involved, but also due to the intricate interaction 
that these technologies have to achieve.  A great 
challenge for the specific S2S translation system 
involving Persian and English would arise from 
not only the linguistics differences between the 
two languages but also from the limited amount of 
data available for Persian.  The other major hurdle 
in achieving a S2S system involving these 
languages is the Persian writing system, which is 
based on the Arabic script, and hence lacks the 
explicit inclusion of vowel sounds, resulting in a 
very large amount of one-to-many mappings from 
transcription to acoustic and semantic 
representations.   
In order to achieve our goal, the system that was 
designed comprised of the following components: 
 
 
 Fig 1. Block diagram of the system. Note that the communication server allows interaction between all 
subsystems and the broadcast of messages. Our vision is that only the doctor will have access to the GUI and 
the patient will only be given a phone handset. 
(1) a visual and control Graphical User Interface 
(GUI); (2) an Automatic Speech Recognition 
(ASR) subsystem, which works both using Fixed 
State Grammars (FSG) and Language Models 
(LM), producing n-best lists/lattices along with the 
decoding confidence scores; (3) a Dialog Manager 
(DM), which receives the output of the speech 
recognition and machine translation units and 
subsequently ?re-scores?? the data according to the 
history of the conversation; (4) a Machine 
Translation (MT) unit, which works in two modes: 
Classifier based MT and a fully Stochastic MT; 
and finally  (5) a unit selection based Text To 
Speech synthesizer (TTS), which provides the 
spoken output.  A functional block diagram is 
shown in Figure 1. 
 
1.1 The Language Under Investigation: 
Persian 
Persian is an Indo-European language with a 
writing system based on the Arabic script.  
Languages that use this script have posed a 
problem for automated language processing such 
as speech recognition and translation systems.  For 
instance, the CSLU Labeling Guide (Lander, 
http://cslu.cse.ogi.edu/corpora/corpPublications.ht
ml) offers orthographic and phonetic transcription 
systems for a wide variety of languages, from 
German to Spanish with a Latin-based writing 
system to languages like Mandarin and Cantonese, 
which use Chinese characters for writing.  
However, there seems to be no standard 
transcription system for languages like Arabic, 
Persian, Dari, Urdu and many others, which use 
the Arabic script (ibid; Kaye, 1876; Kachru, 1987, 
among others).   
Because Persian and Arabic are different, 
Persian has modified the writing system and 
augmented it in order to accommodate the 
differences.  For instance, four letters were added 
to the original system in order to capture the 
sounds available in Persian that Arabic does not 
have.  Also, there are a number of homophonic 
letters in the Persian writing system, i.e., the same 
sound corresponding to different orthographic 
representations.  This problem is unique to Persian, 
since in Arabic different orthographic 
representations represent different sounds.  The 
other problem that is common in all languages 
using the Arabic script is the existance of a large 
number of homographic words, i.e., orthographic 
representations that have a similar form but 
different pronunciation.  This problem arises due 
to limited vowel presentation in this writing 
system.   
Examples of the homophones and homographs 
are represented in Table 1.  The words ?six? and 
?lung? are examples of homographs, where the 
identical (transliterated Arabic) orthographic 
representations (Column 3) correspond to different 
pronunciations [SeS] and [SoS] respectively 
(Column 4). The words ?hundred? and ?dam? are 
examples of homophones, where the two words 
have similar pronunciation [sad] (Column 4), 
despite their different spellings (Column 3).   
 
 Persian USCPers USCPron USCPers+ 
?six?  
 
SS SeS SeS 
?lung?  
 
SS SoS SoS 
?100? 
 
$d sad $ad 
?dam? 
 
sd sad sad 
Table 1 Examples of the transcription methods 
and their limitation.  Purely orthographic 
transcription schemes (such as USCPers) fail to 
distinctly represent homographs while purely 
phonetic ones (such as USCPron) fail to distinctly 
represent the homophones. 
The former is the sample of the cases in which 
there is a many-to-one mapping between 
orthography and pronunciation, a direct result of 
the basic characteristic of the Arabic script, viz., 
little to no representation of the vowels.   
As is evident by the data presented in this table, 
there are two major sources of problems for any 
speech-to-speech machine translation.  In other 
words, to employ a system with a direct 1-1 
mapping between Arabic orthography and a Latin 
based transcription system (what we refer to as 
USCPers in our paper) would be highly ambiguous 
and insufficient to capture distinct words as 
required by our speech-to-speech translation 
system, thus resulting in ambiguity at the text-to-
speech output level, and internal confusion in the 
language modelling and machine translation units.  
The latter, on the other hand, is a representative of 
the cases in which the same sequence of sounds 
would correspond to more than one orthographic 
representation.  Therefore, using a pure phonetic 
transcription, e.g., USCPron, would be acceptable 
for the Automatic Speech Recognizer (ASR), but 
not for the Dialog Manager (DM) or the Machine 
Translator (MT).  The goal of this paper is twofold 
(i) to provide an ASCII based phonemic 
transcription system similar to the one used in the 
International Phonetic Alphabet (IPA), in line of 
Worldbet (Hieronymus, 
http://cslu.cse.ogi.edu/corpora/corpPublications.ht
ml) and (ii) to argue for an ASCII based hybrid 
transcription scheme, which provides an easy way 
to transcribe data in languages that use the Arabic 
script. 
We will proceed in Section 2 to provide the 
USCPron ASCII based phonemic transcription 
system that is similar to the one used by the 
International Phonetic Alphabet (IPA), in line of 
Worldbet (ibid).  In Section 3, we will present the 
USCPers orthographic scheme, which has a one-
to-one mapping to the Arabic script.  In Section 4 
we will present and analyze USCPers+, a hybrid 
system that keeps the orthographic information, 
while providing the vowels.  Section 5 discusses 
some further issues regarding the lack of data.   
2 Phonetic Labels (USCPron) 
One of the requirements of an ASR system is a 
phonetic transcription scheme to represent the 
pronunciation patterns for the acoustic models. 
Persian has a total of 29 sounds in its inventory, six 
vowels (Section 2.1) and 23 consonants (Section 
2.2).  The system that we created to capture these 
sounds is a modified version of the International 
Phonetic Alphabet (IPA), called 
USCPron(unciation).  In USCPron, just like the 
IPA, there is a one-to-one correspondence between 
the sounds and the symbols representing them.  
However, this system, unlike IPA does not require 
special fonts and makes use of ASCII characters.  
The advantage that our system has over other 
systems that use two characters to represent a 
single sound is that following IPA, our system 
avoids all ambiguities. 
2.1 Vowels 
Persian has a six-vowel system, high to low and 
front and back.  These vowels are: [i, e, a, u, o, A], 
as are exemplified by the italicized vowels in the 
following English examples: ?beat?, ?bet?, ?bat?, 
?pull?, ?poll? and ?pot?. The high and mid vowels 
are represented by the IPA symbols. The low front 
vowel is represented as [a], while the low back 
vowel is represented as [A].  There are no 
diphthongs in Persian, nor is there a tense/lax 
distinction among the vowels (Windfuhr, Gernot 
L.1987). 
 
  Front Back 
High i u 
Mid e o 
Low a A 
Table 2: Vowels 
2.2 Consonants 
In addition to the six vowels, there are 23 
distinct consonantal sounds in Persian.  Voicing is 
phonemic in Persian, giving rise to a quite 
symmetric system.  These consonants are 
represented in Table 3 based on the place (bilabial 
(BL), lab-dental (LD), dental (DE), alveopalatal 
(AP), velar (VL), uvular (UV) and glottal (GT)) 
and manner of articulation (stops (ST), fricatives 
(FR), affricates (AF), liquids (LQ), nasals (NS) 
and glides (GL)) and their voicing ([-v(oice)] and 
[+v(oice)]. 
 
 BL LD DE AP VL UV GT 
ST [-v] p  t  k  ? 
 [+v] b  d  g q  
FR [-v]  f s S x  h 
 [+v]  v z Z    
AF [-v]    C    
 [+v]    J    
LQ   l, r     
NS m  n     
GL    y    
Table 3: Consonants 
Many of these sounds are similar to English 
sounds. For instance, the stops, [p, b, t, d, k, g] are 
similar to the italicized letters in the following 
English words: ?potato?, ?ball?, ?tree?, ?doll?, ?key? 
and ?dog? respectively.  The glottal stop [?] can be 
found in some pronunciations of ?button?, and the 
sound in between the two syllables of ?uh oh?.  The 
uvular stop [q] does not have a correspondent in 
English.  Nor does the velar fricative [x].  But the 
rest of the fricatives [f, v, s, z, S, Z, h] have a 
corresponding sound in English, as demonstrated 
by the following examples ?fine?, ?value?, ?sand?, 
?zero?, ?shore?, ?pleasure? and ?hello?.  The 
affricates [C] and [J] are like their English 
counterparts in the following examples: ?church? 
and ?judge?.  The same is true of the nasals [m, n] 
as in ?make? and ?no?; liquids [r, l], as in ?rain? and 
?long? and the glide [y], as in ?yesterday?.  (The 
only distinction between Persian and English is 
that in Persian [t, d, s, z, l, r, n] are dental sounds, 
while in English they are alveolar.)  As is evident, 
whenever possible, the symbols used are those of 
the International Phonetic Alphabet (IPA). 
However, as mentioned before because IPA 
requires special fonts, which are not readily 
available for a few of the sounds, we have used an 
ASCII symbol that resembled the relevant IPA 
symbol.  The only difference between our symbols 
and the ones used by IPA are in voiceless and 
voiced alveopalatal fricatives [S] and [Z], the 
voiceless and voiced affricates [C] and [J], and the 
palatal glide [y].  In the case of the latter, we did 
not want to use the lower case ?j?, in order to 
decrease confusion.   
3 Orthographic Labels (USCPers) 
We proceed in this section to present an 
alternative orthographic system for Persian, as a 
first step in the creation of the USCPers+ system 
that will be presented later. The Persian writing 
system is a consonantal system with 32 letters in 
its alphabet (Windfuhr, 1987).  All but four of 
these letters are direct borrowing from the Arabic 
writing system.  It is important to note that this 
borrowing was not a total borrowing, i.e., many 
letters were borrowed without their corresponding 
sound.  This has resulted in having many letters 
with the same sound (homophones).  However, 
before discussing these cases, let us consider the 
cases in which there is no homophony, i.e., the 
cases in which a single letter of the alphabet is 
represented by a single sound. 
In order to assign a symbol to each letter of the 
alphabet, the corresponding letter representing the 
sound of that letter was chosen.  So, for instance 
for the letter ?   ?, which is represented as [p] in 
USCPron, the letter ?p? was used in USCPers(ian).   
These letters are: 
 
ST FR AF LQ NS 

   p     f    C     r     m 

   b     S     J 	    l 
    n 

   d     Z    

   k     x    

   g 
 
 
 
 

   ?     
Table 4: USCPers(ian) Symbols:  
Non-Homophonic Consonants 
As mentioned above, this partial borrowing of the 
Arabic writing system has given rise to many 
homophonic letters.  In fact, thirteen letters of the 
alphabet are represented by only five sounds.  
These sounds and the corresponding letters are 
presented below:   
 
? [t] for ?  ? and ? ?;  
? [q] for ?  ? and ?  ?;  
? [h] for ?  ? and ?  ?;  
? [s] for ?  ?, ?  ? and ?  ? and 
? [z] for ? Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 382?389,
Sydney, July 2006. c?2006 Association for Computational Linguistics
382
383
384
385
386
387
388
389
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 103?108, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level
Semantic Similarity Estimation
Nikolaos Malandrakis1, Elias Iosif2, Vassiliki Prokopi2, Alexandros Potamianos2,
Shrikanth Narayanan1
1Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2Department of ECE, Technical University of Crete, 73100 Chania, Greece
malandra@usc.edu, iosife@telecom.tuc.gr, vprokopi@isc.tuc.gr, potam@telecom.tuc.gr,
shri@sipi.usc.edu
Abstract
This paper describes our submission for the
*SEM shared task of Semantic Textual Sim-
ilarity. We estimate the semantic similarity
between two sentences using regression mod-
els with features: 1) n-gram hit rates (lexical
matches) between sentences, 2) lexical seman-
tic similarity between non-matching words, 3)
string similarity metrics, 4) affective content
similarity and 5) sentence length. Domain
adaptation is applied in the form of indepen-
dent models and a model selection strategy
achieving a mean correlation of 0.47.
1 Introduction
Text semantic similarity estimation has been an ac-
tive research area, thanks to a variety of potential ap-
plications and the wide availability of data afforded
by the world wide web. Semantic textual similar-
ity (STS) estimates can be used for information ex-
traction (Szpektor and Dagan, 2008), question an-
swering (Harabagiu and Hickl, 2006) and machine
translation (Mirkin et al, 2009). Term-level simi-
larity has been successfully applied to problems like
grammar induction (Meng and Siu, 2002) and affec-
tive text categorization (Malandrakis et al, 2011). In
this work, we built on previous research and our sub-
mission to SemEval?2012 (Malandrakis et al, 2012)
to create a sentence-level STS model for the shared
task of *SEM 2013 (Agirre et al, 2013).
Semantic similarity between words has been
well researched, with a variety of knowledge-based
(Miller, 1990; Budanitsky and Hirst, 2006) and
corpus-based (Baroni and Lenci, 2010; Iosif and
Potamianos, 2010) metrics proposed. Moving to
sentences increases the complexity exponentially
and as a result has led to measurements of simi-
larity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al, 2009), and semantic (Rinaldi
et al, 2003; Bos and Markert, 2005). Machine trans-
lation evaluation metrics can be used to estimate lex-
ical level similarity (Finch et al, 2005; Perez and
Alfonseca, 2005), including BLEU (Papineni et al,
2002), a metric using word n-gram hit rates. The pi-
lot task of sentence STS in SemEval 2012 (Agirre et
al., 2012) showed a similar trend towards multi-level
similarity, with the top performing systems utilizing
large amounts of partial similarity metrics and do-
main adaptation (the use of separate models for each
input domain) (Ba?r et al, 2012; S?aric? et al, 2012).
Our approach is originally motivated by BLEU
and primarily utilizes ?hard? and ?soft? n-gram hit
rates to estimate similarity. Compared to last year,
we utilize different alignment strategies (to decide
which n-grams should be compared with which).
We also include string similarities (at the token and
character level) and similarity of affective content,
expressed through the difference in sentence arousal
and valence ratings. Finally we added domain adap-
tation: the creation of separate models per domain
and a strategy to select the most appropriate model.
2 Model
Our model is based upon that submitted for the same
task in 2012 (Malandrakis et al, 2012). To esti-
mate semantic similarity metrics we use a super-
vised model with features extracted using corpus-
103
based word-level similarity metrics. To combine
these metrics into a sentence-level similarity score
we use a modification of BLEU (Papineni et al,
2002) that utilizes word-level semantic similarities,
string level comparisons and comparisons of affec-
tive content, detailed below.
2.1 Word level semantic similarity
Co-occurrence-based. The semantic similarity be-
tween two words, wi and wj , is estimated as their
pointwise mutual information (Church and Hanks,
1990): I(i, j) = log p?(i,j)p?(i)p?(j) , where p?(i) and p?(j) are
the occurrence probabilities of wi and wj , respec-
tively, while the probability of their co-occurrence
is denoted by p?(i, j). In our previous participation
in SemEval12-STS task (Malandrakis et al, 2012)
we employed a modification of the pointwise mutual
information based on the maximum sense similar-
ity assumption (Resnik, 1995) and the minimization
of the respective error in similarity estimation. In
particular, exponential weights ? were introduced in
order to reduce the overestimation of denominator
probabilities. The modified metric Ia(i, j), is de-
fined as:
Ia(i, j)=
1
2
[
log
p?(i, j)
p??(i)p?(j) + log
p?(i, j)
p?(i)p??(j)
]
. (1)
The weight ? was estimated on the corpus of (Iosif
and Potamianos, 2012) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of ? = 0.8 was shown to significantly
outperform I(i, j) and to achieve state-of-the-art
results on standard semantic similarity datasets
(Rubenstein and Goodenough, 1965; Miller and
Charles, 1998; Finkelstein et al, 2002).
Context-based: The fundamental assumption
behind context-based metrics is that similarity
of context implies similarity of meaning (Harris,
1954). A contextual window of size 2H + 1 words
is centered on the word of interest wi and lexical
features are extracted. For every instance of wi
in the corpus the H words left and right of wi
formulate a feature vector vi. For a given value of
H the context-based semantic similarity between
two words, wi and wj , is computed as the cosine
of their feature vectors: QH(i, j) = vi.vj||vi|| ||vj || .
The elements of feature vectors can be weighted
according various schemes [(Iosif and Potamianos,
2010)], while, here we use a binary scheme.
Network-based: The aforementioned similarity
metrics were used for the definition of a semantic
network (Iosif and Potamianos, 2013; Iosif et al,
2013). A number of similarity metrics were pro-
posed under either the attributional similarity (Tur-
ney, 2006) or the maximum sense similarity (Resnik,
1995) assumptions of lexical semantics1.
2.2 Sentence level similarities
To utilize word-level semantic similarities in the
sentence-level task we use a modified version of
BLEU (Papineni et al, 2002). The model works in
two passes: the first pass identifies exact matches
(similar to baseline BLEU), the second pass com-
pares non-matched terms using semantic similarity.
Non-matched terms from the hypothesis sentence
are compared with all terms of the reference sen-
tence (regardless of whether they were matched dur-
ing the first pass). In the case of bigram and higher
order terms, the process is applied recursively: the
bigrams are decomposed into two words and the
similarity between them is estimated by applying the
same method to the words. All word similarity met-
rics used are peak-to-peak normalized in the [0,1]
range, so they serve as a ?degree-of-match?. The se-
mantic similarity scores from term pairs are summed
(just like n-gram hits) to obtain a BLEU-like hit-rate.
Alignment is performed via maximum similarity:
we iterate on the hypothesis n-grams, left-to-right,
and compare each with the most similar n-gram in
the reference. The features produced by this process
are ?soft? hit-rates (for 1-, 2-, 3-, 4-grams)2. We also
use the ?hard? hit rates produced by baseline BLEU
as features of the final model.
2.3 String similarities
We use the following string-based similarity fea-
tures: 1) Longest Common Subsequence Similarity
(LCSS) (Lin and Och, 2004) based on the Longest
Common Subsequence (LCS) character-based dy-
1The network-based metrics were applied only during the
training phase of the shared task, due to time limitations. They
exhibited almost identical performance as the metric defined by
(1), which was used in the test runs.
2Note that the features are computed twice on each sentence
pair and then averaged.
104
namic programming algorithm. LCSS represents the
length of the longest string (or strings) that is a sub-
string (or are substrings) of two or more strings. 2)
Skip bigram co-occurrence measures the overlap of
skip-bigrams between two sentences or phrases. A
skip-bigram is defined as any pair of words in the
sentence order, allowing for arbitrary gaps between
words (Lin and Och, 2004). 3) Containment is de-
fined as the percentage of a sentence that is con-
tained in another sentence. It is a number between
0 and 1, where 1 means the hypothesis sentence is
fully contained in the reference sentence (Broder,
1997). We express containment as the amount of n-
grams of a sentence contained in another. The con-
tainment metric is not symmetric and is calculated
as: c(X,Y ) = |S(X) ? S(Y )|/S(X), where S(X)
and S(Y ) are all the n-grams of sentences X and Y
respectively.
2.4 Affective similarity
We used the method proposed in (Malandrakis et al,
2011) to estimate affective features. Continuous (va-
lence and arousal) ratings in [?1, 1] of any term are
represented as a linear combination of a function of
its semantic similarities to a set of seed words and
the affective ratings of these words, as follows:
v?(wj) = a0 +
N
?
i=1
ai v(wi) dij , (2)
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
andwj (for the purposes of this work, cosine similar-
ity between context vectors is used). The weights ai
are estimated over the Affective norms for English
Words (ANEW) (Bradley and Lang, 1999) corpus.
Using this model we generate affective ratings for
every content word (noun, verb, adjective or adverb)
of every sentence. We assume that these can ad-
equately describe the affective content of the sen-
tences. To create an ?affective similarity metric? we
use the difference of means of the word affective rat-
ings between two sentences.
d?affect = 2? |?(v?(s1))? ?(v?(s2))| (3)
where ?(v?(si)) the mean of content word ratings in-
cluded in sentence i.
2.5 Fusion
The aforementioned features are combined using
one of two possible models. The first model is a
Multiple Linear Regression (MLR) model
D?L = a0 +
k
?
n=1
an fk, (4)
where D?L is the estimated similarity, fk are the un-
supervised semantic similarity metrics and an are
the trainable parameters of the model.
The second model is motivated by an assumption
of cognitive scaling of similarity scores: we expect
that the perception of hit rates is non-linearly af-
fected by the length of the sentences. We call this the
hierarchical fusion scheme. It is a combination of
(overlapping) MLR models, each matching a range
of sentence lengths. The first model DL1 is trained
with sentences with length up to l1, i.e., l ? l1, the
second model DL2 up to length l2 etc. During test-
ing, sentences with length l ? [1, l1] are decoded
with DL1, sentences with length l ? (l1, l2] with
model DL2 etc. Each of these partial models is a
linear fusion model as shown in (4). In this work,
we use four models with l1 = 10, l2 = 20, l3 = 30,
l4 = ?.
Domain adaptation is employed, by creating sep-
arate models per domain (training data source). Be-
yond that, we also create a unified model, trained
on all data to be used as a fallback if an appropriate
model can not be decided upon during evaluation.
3 Experimental Procedure and Results
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al, 2005; Toutanova et al,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. We evaluated
multiple types of preprocessing per unsupervised
metric and chose different ones depending on the
metric. Word-level semantic similarities, used for
soft comparisons and affective feature extraction,
were computed over a corpus of 116 million web
snippets collected by posing one query for every
word in the Aspell spellchecker (asp, ) vocabulary to
the Yahoo! search engine. Word-level emotional rat-
ings in continuous valence and arousal scales were
produced by a model trained on the ANEW dataset
105
and using contextual similarities. Finally, string sim-
ilarities were calculated over the original unmodified
sentences.
Next, results are reported in terms of correla-
tion between the generated scores and the ground
truth, for each corpus in the shared task, as well as
their weighted mean. Feature selection is applied
to the large candidate feature set using a wrapper-
based backward selection approach on the train-
ing data.The final feature set contains 15 features:
soft hit rates calculated over content word 1- to 4-
grams (4 features), soft hit rates calculated over un-
igrams per part-of-speech, for adjectives, nouns, ad-
verbs, verbs (4 features), BLEU unigram hit rates
for all words and content words (2 features), skip
and containment similarities, containment normal-
ized by sum of sentence lengths or product of sen-
tence lengths (3 features) and affective similarities
for arousal and valence (2 features).
Domain adaptation methods are the only dif-
ference between the three submitted runs. For all
three runs we train one linear model per training set
and a fallback model. For the first run, dubbed lin-
ear, the fallback model is linear and model selection
during evaluation is performed by file name, there-
fore results for the OnWN set are produced by a
model trained with OnWN data, while the rest are
produced by the fallback model. The second run,
dubbed length, uses a hierarchical fallback model
and model selection is performed by file name. The
third run, dubbed adapt, uses the same models as
the first run and each test set is assigned to a model
(i.e., the fallback model is never used). The test set -
model (training) mapping for this run is: OnWN ?
OnWN, headlines ? SMTnews, SMT ? Europarl
and FNWN? OnWN.
Table 1: Correlation performance for the linear model us-
ing lexical (L), string (S) and affect (A) features
Feature headl. OnWN FNWN SMT mean
L 0.68 0.51 0.23 0.25 0.46
L+S 0.69 0.49 0.23 0.26 0.46
L+S+A 0.69 0.51 0.27 0.28 0.47
Results are shown in Tables 1 and 2. Results for
the linear run using subsets of the final feature set
are shown in Table 1. Lexical features (hit rates) are
obviously the most valuable features. String similar-
ities provided us with an improvement in the train-
Table 2: Correlation performance on the evaluation set.
Run headl. OnWN FNWN SMT mean
linear 0.69 0.51 0.27 0.28 0.47
length 0.65 0.51 0.25 0.28 0.46
adapt 0.62 0.51 0.33 0.30 0.46
ing set which is not reflected in the test set. Af-
fect proved valuable, particularly in the most diffi-
cult sets of FNWN and SMT.
Results for the three submission runs are shown
in Table 2. Our best run was the simplest one, using
a purely linear model and effectively no adaptation.
Adding a more aggressive adaptation strategy im-
proved results in the FNWN and SMT sets, so there
is definitely some potential, however the improve-
ment observed is nowhere near that observed in the
training data or the same task of SemEval 2012. We
have to question whether this improvement is an ar-
tifact of the rating distributions of these two sets
(SMT contains virtually only high ratings, FNWN
contains virtually only low ratings): such wild mis-
matches in priors among training and test sets can
be mitigated using more elaborate machine learning
algorithms (rather than employing better semantic
similarity features or algorithms). Overall the sys-
tem performs well in the two sets containing large
similarity rating ranges.
4 Conclusions
We have improved over our previous model of sen-
tence semantic similarity. The inclusion of string-
based similarities and more so of affective content
measures proved significant, but domain adaptation
provided mixed results. While expanding the model
to include more layers of similarity estimates is
clearly a step in the right direction, further work is
required to include even more layers. Using syntac-
tic information and more levels of abstraction (e.g.
concepts) are obvious next steps.
5 Acknowledgements
The first four authors have been partially funded
by the PortDial project (Language Resources for
Portable Multilingual Spoken Dialog Systems) sup-
ported by the EU Seventh Framework Programme
(FP7), grant number 296170.
106
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.
2012. Semeval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proc. SemEval, pages 385?393.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In Proc. *SEM.
Gnu aspell. http://www.aspell.net.
D. Ba?r, C. Biemann, I. Gurevych, and T. Zesch. 2012.
Ukp: Computing semantic textual similarity by com-
bining multiple content similarity measures. In Proc.
SemEval, pages 435?440.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
M. Bradley and P. Lang. 1999. Affective norms for En-
glish words (ANEW): Stimuli, instruction manual and
affective ratings. Technical report C-1. The Center for
Research in Psychophysiology, University of Florida.
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. In In Compression and Com-
plexity of Sequences (SEQUENCES97, pages 21?29.
IEEE Computer Society.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:13?47.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 363?370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116?131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings of the 21st InternationalCon-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905?912.
Z. Harris. 1954. Distributional structure. Word,
10(23):146?162.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):1637?1647.
E. Iosif and A. Potamianos. 2012. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. In Proc. Eighth International Con-
ference on Language Resources and Evaluation, pages
3499?3504.
Elias Iosif and Alexandros Potamianos. 2013. Similarity
Computation Using Semantic Networks Created From
Web-Harvested Data. Natural Language Engineering,
(submitted).
E. Iosif, A. Potamianos, M. Giannoudaki, and K. Zer-
vanou. 2013. Semantic similarity computation for ab-
stract and concrete nouns using network-based distri-
butional semantic models. In 10th International Con-
ference on Computational Semantics (IWCS), pages
328?334.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42?47.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing of AFNLP, pages 42?47.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
N. Malandrakis, E. Iosif, and A. Potamianos. 2012.
DeepPurple: Estimating sentence semantic similarity
using n-gram regression models and web snippets. In
Proc. Sixth International Workshop on Semantic Eval-
uation (SemEval) ? The First Joint Conference on
Lexical and Computational Semantics (*SEM), pages
565?570.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
107
specific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172?181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1?28.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings of the 47th AnnualMeeting of ACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791?799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conference for Artificial Intelligence,
pages 448?453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 25?32.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849?856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173?180.
P. Turney. 2006. Similarity of semantic relations. Com-
putational Linguistics, 32(3):379?416.
F. S?aric?, G. Glavas?, M. Karan, J. S?najder, and B. Dal-
belo Bas?ic?. 2012. Takelab: Systems for measuring
semantic text similarity. In Proc. SemEval, pages 441?
448.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
108
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 438?442, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SAIL: A hybrid approach to sentiment analysis
Nikolaos Malandrakis1, Abe Kazemzadeh2, Alexandros Potamianos3, Shrikanth Narayanan1
1 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2 Annenberg Innovation Laboratory (AIL), USC, Los Angeles, CA 90089, USA
3Department of ECE, Technical University of Crete, 73100 Chania, Greece
malandra@usc.edu, kazemzad@usc.edu, potam@telecom.tuc.gr, shri@sipi.usc.edu
Abstract
This paper describes our submission for Se-
mEval2013 Task 2: Sentiment Analysis in
Twitter. For the limited data condition we use
a lexicon-based model. The model uses an af-
fective lexicon automatically generated from a
very large corpus of raw web data. Statistics
are calculated over the word and bigram af-
fective ratings and used as features of a Naive
Bayes tree model. For the unconstrained data
scenario we combine the lexicon-based model
with a classifier built on maximum entropy
language models and trained on a large exter-
nal dataset. The two models are fused at the
posterior level to produce a final output. The
approach proved successful, reaching rank-
ings of 9th and 4th in the twitter sentiment
analysis constrained and unconstrained sce-
nario respectively, despite using only lexical
features.
1 Introduction
The analysis of the emotional content of text, is
relevant to numerous natural language processing
(NLP), web and multi-modal dialogue applications.
To that end there has been a significant scientific
effort towards tasks like product review analysis
(Wiebe and Mihalcea, 2006; Hu and Liu, 2004),
speech emotion extraction (Lee and Narayanan,
2005; Lee et al, 2002; Ang et al, 2002) and pure
text word (Esuli and Sebastiani, 2006; Strappar-
ava and Valitutti, 2004) and sentence (Turney and
Littman, 2002; Turney and Littman, 2003) level
emotion extraction.
The rise of social media in recent years has seen
a shift in research focus towards them, particularly
twitter. The large volume of text data available is
particularly useful, since it allows the use of com-
plex machine learning methods. Also important is
the interest on the part of companies that are actively
looking for ways to mine social media for opinions
and attitudes towards them and their products. Sim-
ilarly, in journalism there is interest in sentiment
analysis for a way to process and report on the public
opinion about current events (Petulla, 2013).
Analyzing emotion expressed in twitter borrows
from other tasks related to affective analysis, but
also presents unique challenges. One common is-
sue is the breadth of content available in twitter: a
more limited domain would make the task easier,
however there are no such bounds. There is also a
significant difference in the form of language used
in tweets. The tone is informal and typographical
and grammatical errors are very common, making
even simple tasks, like Part-of-Speech tagging much
harder. Features like hashtags and emoticons can
also be helpful (Davidov et al, 2010).
This paper describes our submissions for Se-
mEval 2013 task 2, subtask B, which deals pri-
marily with sentiment analysis in twitter. For the
constrained condition (using only the organizer-
provided twitter sentences) we implemented a sys-
tem based on the use of an affective lexicon and part-
of-speech tag information, which has been shown
relevant to the task (Pak and Paroubek, 2010).
For the unconstrained condition (including external
sources of twitter sentences) we combine the con-
strained model with a maximum entropy language
438
model trained on external data.
2 Experimental procedure
We use two separate models, one for the constrained
condition and a combination for the unconstrained
condition. Following are short descriptions.
2.1 Lexicon-based model
The method used for the constrained condition is
based on an affective lexicon containing out-of-
context affective ratings for all terms contained in
each sentence. We use an automated algorithm of
affective lexicon expansion based on the one pre-
sented in (Malandrakis et al, 2011), which in turn
is an expansion of (Turney and Littman, 2002).
We assume that the continuous (in [?1, 1]) va-
lence and arousal ratings of any term can be repre-
sented as a linear combination of its semantic simi-
larities to a set of seed words and the affective rat-
ings of these words, as follows:
v?(wj) = a0 +
N
?
i=1
ai v(wi) dij , (1)
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
and wj . For the purposes of this work, the seman-
tic similarity metric is the cosine similarity between
context vectors computed over a corpus of 116 mil-
lion web snippets collected by posing one query for
every word in the Aspell spellchecker?s vocabulary
to the Yahoo! search engine and collecting up to 500
of the top results.
Given a starting, manually annotated, lexicon we
can select part of it to serve as seed words and then
use 1 to create a system of linear equations where
the only unknowns are the weights ai. The system
is solved using Least Squares Estimation. That pro-
vides us with an equation that can generate affective
ratings for every term (not limited to words), as long
as we can estimate the semantic similarity between
it and the seed words.
Seed word selection is performed by a simple
heuristic (though validated through experiments):
we want seed words to have extreme affective rat-
ings (maximum absolute value) and we want the set
to be as closed to balanced as possible (sum of seed
ratings equal to zero).
Given these term ratings, the next step is combin-
ing them through statistics. To do that we use sim-
ple statistics (mean, min, max) and group by part
of speech tags. The results are statistics like ?max-
imum valence among adjectives?, ?mean arousal
among proper nouns? and ?number of verbs and
nouns?. The dimensions used are: valence, absolute
valence and arousal. The grouping factors are the 39
Penn treebank pos tags plus higher order tags (adjec-
tives, verbs, nouns, adverbs and combinations of 2,3
and 4 of them). The statistics extracted are: mean,
min, max, most extreme, sum, number, percentage
of sentence coverage. In the case of bigram terms no
part-of-speech filtering/grouping is applied. These
statistics form the feature vectors.
Finally we perform feature selection on the mas-
sive set of candidates and use them to train a model.
The model selected is a Naive Bayes tree, a tree with
Naive Bayes classifiers on each leaf. The motivation
comes by considering this a two stage problem: sub-
jectivity detection and polarity classification, mak-
ing a hierarchical model a natural choice. NB trees
proved superior to other types of trees during our
testing, presumably due to the smoothing of obser-
vation distributions.
2.2 N-gram language model
The method used for the unconstrained condition
is based on a combination of the automatically ex-
panded affective lexicon described in the previ-
ous section together with a bigram language model
based on the work of (Wang et al, 2012), which
uses a large set of twitter data from the U.S. 2012
Presidential election. As a part of the unconstrained
system, we were able to leverage external annotated
data apart from those provided by the SEMEVAL
2013 sentiment task dataset. Of the 315 million
tweets we collected about the election, we anno-
tated a subset of 40 thousand tweets using Ama-
zon Mechanical Turk. The annotation labels that
we used were ?positive?, ?negative?, ?neutral?, and
?unsure?, and additionally raters could mark tweets
for sarcasm and humor. We excluded tweets marked
as ?unsure? as well as tweets that had disagree-
439
ment in labels if they were annotated by more than
one annotator. To extract the bigram features, we
used a twitter-specific tokenizer (Potts, 2011), which
marked uniform resource locators (URLs), emoti-
cons, and repeated characters, and which lowercased
words that began with capital letters followed by
lowercase letters (but left words in all capitals). The
bigram features were computed as presence or ab-
sense in the tweet rather than counts due to the small
number of words in tweets. The machine learning
model used to classify the tweets was the Megam
maximum entropy classifier (Daume? III, 2004) in
the Natural Language Toolkit (NLTK) (Bird et al,
2009).
2.3 Fusion
The submitted system for the unconstrained condi-
tion leverages both the lexicon-based and bigram
language models. Due to the very different nature
of the models we opt to not fuse them at the feature
level, using a late fusion scheme instead. Both par-
tial models are probabilistic, therefore we can use
their per-class posterior probabilities as features of
a fusion model. The fusion model is a linear kernel
SVM using six features, the three posteriors from
each partial model, and trained on held out data.
3 Results
Following are results from our method, evaluated
on the testing sets (of sms and twitter posts) of
SemEval2013 task 2. We evaluate in terms of 3-
class classification, polarity classification (positive
vs. negative) and subjectivity detection (neutral vs.
other). Results shown in terms of per category f-
measure.
3.1 Constrained
The preprocessing required for the lexicon-based
model is just part-of-speech tagging using Treetag-
ger (Schmid, 1994). The lexicon expansion method
is used to generate valence and arousal ratings for
all words and ngrams in all datasets and the part of
speech tags are used as grouping criteria to gener-
ate statistics. Finally, feature selection is performed
using a correlation criterion (Hall, 1999) and the re-
sulting feature set is used to train a Naive Bayes
tree model. The feature selection and model train-
Table 1: F-measure results for the lexicon-based model,
using different machine learning methods, evaluated on
the 3-class twitter testing data.
model
per-class F-measure
neg neu pos
Nbayes 0.494 0.652 0.614
SVM 0.369 0.677 0.583
CART 0.430 0.676 0.593
NBTree 0.561 0.662 0.643
Table 2: F-measure results for the constrained condition,
evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.561 0.662 0.643
pos vs neg 0.679 0.858
neu vs other 0.685 0.699
sms
3-class 0.506 0.709 0.531
pos vs neg 0.688 0.755
neu vs other 0.730 0.628
ing/classification was conducted using Weka (Wit-
ten and Frank, 2000).
The final model uses a total of 72 features, which
can not be listed here due to space constraints. The
vast majority of these features are necessary to de-
tect the neutral category: positive-negative separa-
tion can be achieved with under 30 features.
One aspect of the model we felt worth investigat-
ing, was the type of model to be used. Using a multi-
stage model, performing subjectivity detection be-
fore positive-negative classification, has been shown
to provide an improvement, however single models
have also been used extensively. We compared some
popular models: Naive Bayes, linear kernel SVM,
CART-trained tree and Naive Bayes tree, all using
the same features, on the twitter part of the SemEval
testing data. The results are shown in Table 1. The
two Naive Bayes-based models proved significantly
better, with NBTree being clearly the best model for
these features.
Results from the submitted constrained model are
shown in Table 2. Looking at the twitter data re-
sults and comparing the positive-negative vs the
440
3-class results, it appears the main weakness of
this model is subjectivity detection, mostly on the
neutral-negative side. It is not entirely clear to us
whether that is an artifact of the model (the nega-
tive class has the lowest prior probability, thus may
suffer compared to neutral) or of the more complex
forms of negativity (sarcasm, irony) which we do not
directly address. There is a definite drop in perfor-
mance when using the same twitter-trained model on
sms data, which we would not expect, given that the
features used are not twitter-specific. We believe this
gap is caused by lower part-of-speech tagger perfor-
mance: visual inspection reveals the output on twit-
ter data is fairly bad.
Overall this model ranked 9th out of 35 in the
twitter set and 11th out of 28 in the sms set, among
all constrained submissions.
3.2 Unconstrained
Table 3: F-measure results for the maximum entropy
model with bigram features, evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.403 0.661 0.623
pos vs neg 0.586 0.804
neu vs other 0.661 0.704
sms
3-class 0.390 0.587 0.542
pos vs neg 0.710 0.648
neu vs other 0.587 0.641
Table 4: F-measure results for the unconstrained condi-
tion, evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.565 0.679 0.655
pos vs neg 0.672 0.881
neu vs other 0.667 0.732
sms
3-class 0.502 0.723 0.538
pos vs neg 0.625 0.772
neu vs other 0.710 0.637
In order to create the submitted unconstrained
model we train an SVM model using the lexicon-
based and bigram language model posterior proba-
bilities as features. This fusion model is trained on
held-out data (the development set of the SemEval
data). The results of classification using the bigram
language model alone are shown in Table 3 and the
results from the final fused model are shown in Ta-
ble 4. Looking at relative per-class performance, the
results follow a form most similar to the constrained
model, though there are gains in all cases. These
gains are less significant when evaluated on the sms
data, resulting in a fair drop in ranks: the bigram lan-
guage model (expectedly) suffers more when mov-
ing to a different domain, since it uses words as
features rather than the more abstract affective rat-
ings used by the lexicon-based model. Also, because
the external data used to train the bigram language
model was from discussions of politics on Twitter,
the subject matter also varied in terms of prior senti-
ment distribution in that the negative class was pre-
dominant in politics, which resulted in high recall
but low precision for the negative class.
This model ranked 4th out of 16 in the twitter set
and 7th out of 17 in the sms set, among all uncon-
strained submissions.
4 Conclusions
We presented a system of twitter sentiment analy-
sis combining two approaches: a hierarchical model
based on an affective lexicon and a language model-
ing approach, fused at the posterior level. The hier-
archical lexicon-based model proved very successful
despite using only n-gram affective ratings and part-
of-speech information. The language model was
not as good individually, but provided a noticeable
improvement to the lexicon-based model. Overall
the models achieved good performance, ranking 9th
of 35 and 4th of 16 in the constrained and uncon-
strained twitter experiments respectively, despite us-
ing only lexical information.
Future work will focus on incorporating im-
proved tokenization (including part-of-speech tag-
ging), making better use of twitter-specific features
like emoticons and hashtags, and performing affec-
tive lexicon generation on twitter data.
441
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. ICSLP, pages 2037?2040.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O?Reilly Media.
H. Daume? III. 2004. Notes on cg and lm-bfgs op-
timization of logistic regression. Paper available at
http://pub. hal3. name# daume04cg-bfgs, implementa-
tion available at http://hal3. name/megam.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Enhanced
sentiment learning using twitter hashtags and smileys.
In Proc. COLING, pages 241?249.
A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A pub-
licly available lexical resource for opinion mining. In
Proc. LREC, pages 417?422.
M. A. Hall. 1999. Correlation-based feature selection
for machine learning. Ph.D. thesis, The University of
Waikato.
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proc. SIGKDD, KDD ?04, pages
168?177. ACM.
C. M. Lee and S. Narayanan. 2005. Toward detecting
emotions in spoken dialogs. IEEE Transactions on
Speech and Audio Processing, 13(2):293?303.
C. M. Lee, S. Narayanan, and R. Pieraccini. 2002. Com-
bining acoustic and language information for emotion
recognition. In Proc. ICSLP, pages 873?876.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
A. Pak and P. Paroubek. 2010. Twitter as a corpus
for sentiment analysis and opinion mining. In Proc.
LREC, pages 1320?1326.
S. Petulla. 2013. Feelings, nothing more than feelings:
The measured rise of sentiment analysis in journalism.
Neiman Journalism Lab, January.
C. Potts. 2011. Sentiment symposium tutorial: Tokeniz-
ing. Technical report, Stanford Linguistics.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proc. International Confer-
ence on New Methods in Language Processing, vol-
ume 12, pages 44?49.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of WordNet. In Proc. LREC,
volume 4, pages 1083?1086.
P. Turney and M. L. Littman. 2002. Unsupervised
Learning of Semantic Orientation from a Hundred-
Billion-Word Corpus. Technical report ERC-1094
(NRC 44929). National Research Council of Canada.
P. Turney and M. L. Littman. 2003. Measuring praise
and criticism: Inference of semantic orientation from
association. ACM Transactions on Information Sys-
tems, 21:315?346.
H. Wang, D. Can, A. Kazemzadeh, F. Bar, and
S. Narayanan. 2012. A system for real-time twitter
sentiment analysis of 2012 u.s. presidential election
cycle. In Proc. ACL, pages 115?120.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proc. COLING/ACL, pages 1065?1072.
Ian H.Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
442
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 508?511,
Dublin, Ireland, August 23-24, 2014.
SAIL-GRS: Grammar Induction for Spoken Dialogue Systems using
CF-IRF Rule Similarity
Kalliopi Zervanou, Nikolaos Malandrakis and Shrikanth Narayanan
Signal Analysis and Interpretation Laboratory (SAIL),
University of Southern California, Los Angeles, CA 90089, USA
kzervanou@gmail.com, malandra@usc.edu , shri@sipi.usc.edu
Abstract
The SAIL-GRS system is based on a
widely used approach originating from in-
formation retrieval and document index-
ing, the TF -IDF measure. In this im-
plementation for spoken dialogue system
grammar induction, rule constituent fre-
quency and inverse rule frequency mea-
sures are used for estimating lexical and
semantic similarity of candidate grammar
rules to a seed set of rule pattern instances.
The performance of the system is evalu-
ated for the English language in three dif-
ferent domains, travel, tourism and finance
and in the travel domain, for Greek. The
simplicity of our approach makes it quite
easy and fast to implement irrespective of
language and domain. The results show
that the SAIL-GRS system performs quite
well in all three domains and in both lan-
guages.
1 Introduction
Spoken dialogue systems typically rely on gram-
mars which define the semantic frames and re-
spective fillers in dialogue scenarios (Chen et al.,
2013). Such systems are tailored for specific
domains for which the respective grammars are
mostly manually developed (Ward, 1990; Seneff,
1992). In order to address this issue, numerous
current approaches attempt to infer these grammar
rules automatically (Pargellis et al., 2001; Meng
and Siu, 2002; Yoshino et al., 2011; Chen et al.,
2013).
The acquisition of grammar rules for spoken
language systems is defined as a task comprising
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
of two subtasks (Meng and Siu, 2002; Iosif and
Potamianos, 2007), the acquisition of:
(i) Low-level rules These are rules defining
domain-specific entities, such as names of lo-
cations, hotels, airports, e.g. CountryName:
?USA?, Date: ?July 15th, 2014?, CardType:
?VISA? and other common domain multi-word ex-
pressions, e.g. DoYouKnowQ: ?do you know?.
(ii) High-level rules These are larger,
frame-like rule patterns which contain as
semantic slot fillers multi-word entities
identified by low-level rules. For exam-
ple: DirectionsQ: ?<DoYouKnowQ>
<where> the <MuseumName> is lo-
cated?, ExpressionCardProblem: ?my
<CardType> has expired?.
The shared task of Grammar Induction for Spo-
ken Dialogue Systems, where our system partic-
ipated, focused on the induction of high-level
grammar rules and in particular on the identifica-
tion and semantic classification of new rule pat-
terns based on their semantic similarity to known
rule instances.
Within this research framework, the work de-
scribed in this paper proposes a methodology for
estimating rule semantic similarity using a varia-
tion of the well-known measure of TF -IDF as
rule constituent frequency vs. inverse rule fre-
quency, henceforth CF -IRF .
In the remainder of this paper, we start in Sec-
tion 2 by a detailed description of our system. Sub-
sequently, in Section 3, we present the datasets
used and the evaluation process, and in Section 4
we discuss our results. We conclude in Section 5
with a summary of our observations and directions
for future work.
2 System Description
The SAIL-GRS system is based on a widely used
approach in information retrieval and document
indexing, the TF -IDF measure. TF -IDF is
508
an approach that has found numerous applications
in information management applications, such as
document keyword extraction, (e.g., Dillon and
Gray (1983)), document clustering, summarisa-
tion, (e.g., Gong and Liu (2001)), event cluster-
ing, (e.g., De Smet and Moens (2013)). In dia-
logue systems, TF -IDF has been used, among
other applications, for discovering local coher-
ence (Gandhe and Traum, 2007) and for acquir-
ing predicate-argument rule fragments in an open
domain, information extraction-based spoken dia-
logue system (Yoshino et al., 2011). In their ap-
proach, Yoshino et al. (2011) use the TF -IDF
measure to determine the importance of a given
word for a given domain or topic, so as to select
the most salient predicate-argument structure rule
patterns from their corpus.
In our implementation for spoken dialogue
system grammar induction, rule constituent fre-
quency (CF ) and inverse rule frequency (IRF )
measures are used for estimating lexical and se-
mantic similarity of candidate grammar rules to a
seed set of rule pattern instances. As illustrated in
Table 1, the SAIL-GRS algorithm has two main
steps, the training stage and the rule induction
stage.
Input: known rule pattern instances
Output: new candidate rule patterns
Training stage:
1. Known rule instance parsing
2. Rule constituent extraction (uni-/bigrams)
3. Rule constituent frequency count (CF )
4. Inverse rule frequency count (IRF )
5. CF -IRF rule instance vector creation
Rule induction stage:
1. Unknown text fragment parsing
2. Unigram & bigram extraction
3. Uni-/bigram CF -IRF value lookup
4. Creation of CF -IRF vector for
unknown text fragment
5. Estimation of cosine similarity of
unknown fragment to rule instances
6. New candidate rule selection & rule
semantic category classification using
maximum cosine similarity
Table 1: The SAIL-GRS system algorithm.
In the first, the Training stage, known rule in-
stances are parsed and, for each rule semantic cat-
egory, the respective high-level rule pattern in-
stances are acquired. These patterns are subse-
quently split into unigram and bigram constituents
and the respective constituent frequencies and in-
verse rule frequencies are estimated. Finally, for
each rule category, a vector representation is cre-
ated for the respective rule pattern instance, based
on the CF -IRF value of its unigram and bigram
constituents.
In the second step, the Rule induction stage, the
unknown text fragments are parsed and split into
unigrams and bigrams. Subsequently, we lookup
the known rule instance unigram and bigram rep-
resentations for potential lexical matches to these
new unigrams and bigrams. If these are found,
then the new n-grams acquire the respective CF -
IRF values found in the training instances and the
respective CF -IRF vector for the unknown text
fragments is created. Finally, we estimate the co-
sine similarity of this unknown text vector to each
known rule vector. The unknown text fragments
that are most similar to a given rule category are
selected as candidate rule patterns and are classi-
fied in the known rule semantic category. An un-
known text fragment that is selected as candidate
rule pattern is assigned only to one, the most sim-
ilar, rule category.
3 Experimental Setup
The overall objective in spoken dialogue system
grammar induction is the fast and efficient devel-
opment and portability of grammar resources. In
the Grammar Induction for Spoken Dialogue Sys-
tems task, this challenge was addressed by pro-
viding datasets in three different domains, travel,
tourism and finance, and by attempting to cover
more than one language for the travel domain,
namely English and Greek.
As illustrated in Table 2, the travel domain data
for the two languages are comparable, with 32 and
35 number of known rule categories, for English
and Greek, comprising of 982 and 956 high-level
rule pattern instances respectively. The smallest
dataset is the finance dataset, with 9 rule categories
and 136 rule pattern instances, while the tourism
dataset has a relatively low number of rule cate-
gories comprising of the highest number of rule
pattern instances. Interestingly, as indicated in the
column depicting the percent of unknown n-grams
in the test-set, i.e. the unigrams and the bigrams
without a CF -IRF value in the training data, the
tourism domain test-set appears also to be the one
509
with the greatest overlap with the training data,
with a mere 0.72% and 4.84% of unknown uni-
grams and bigrams respectively.
For the evaluation, the system performance is
estimated in terms of precision (P ), recall (R) and
F -score measures, for the correct classification of
an unknown text fragment to a given rule cate-
gory cluster of pattern instances. In addition to
these measures, the weighted average of the per
rule scores is computed as follows:
P
w
=
?
N?1
i=1
P
i
c
i
?
N?1
i=1
c
i
, R
w
=
?
N?1
i=1
R
i
n
i
?
N?1
i=1
n
i
(1)
F
w
=
2 ? P
w
?R
w
P
w
+ R
w
(2)
where N ? 1 is the total number of rule cate-
gories, P
i
and R
i
are the per rule i scores for preci-
sion and recall, c
i
the unknown patterns correctly
assigned to rule i, and n
i
the total number of cor-
rect rule instance patterns for rule i indicated in
the ground truth data.
4 Results
The results of the SAIL-GRS system outperform
the Baseline in all dataset categories, except the
Tourism domain, as illustrated in Table 3. In this
domain, both systems present the highest scores
compared to the other domains. The high results
in the travel domain are probably due to the high
data overlap between the train and the test data, as
discussed in the previous section and illustrated in
Table 2. However, this domain was also the one
with the highest average number of rule instances
per rule category, compared to the other domains,
thus presenting an additional challenge in the cor-
rect classification of unknown rule fragments.
We observe that the overall higher F measures
of the SAIL-GRS system in the travel and fi-
nance domains are due to higher precision scores,
whereas Baseline system displays higher recall but
lower precision scores and lower F-measure in
these domains.
The overall lowest scores for both systems are
reached in the Travel domain for Greek, which
is also the dataset with the lowest overlap with
the training data. However, the performance of
the SAIL-GRS system does not deteriorate to the
same extent as the Baseline, the precision of which
falls to a mere 0.16-0.17, compared to 0.49-0.46
for the SAIL-GRS system.
5 Conclusion
In this work, we have presented the SAIL-GRS
system used for the Grammar Induction for Spo-
ken Dialogue Systems task. Our approach uses
a fairly simple, language independent method for
measuring lexical and semantic similarity of rule
pattern instances. Our rule constituent frequency
vs. inverse rule frequency measure, CF -IRF is a
modification the TF -IDF measure for estimating
rule similarity in the induction process of new rule
instances.
The performance of our system in rule induc-
tion and rule pattern semantic classification was
tested in three different domains, travel, tourism
and finance in four datasets, three for English
and an additional dataset for the travel domain
in Greek. SAIL-GRS outperforms the Baseline
in all datasets, except the travel domain for En-
glish. Moreover, our results showed that our sys-
tem achieved an overall better score in precision
and respective F-measure, in the travel and finance
domains, even when applied to a language other
than English. Finally, in cases of a larger percent-
age of unknown data in the test set, as in the Greek
travel dataset, the smooth degradation of SAIL-
GRS results compared to the Baseline indicates
the robustness of our method.
A limitation of our system in its current version
lies in the requirement for absolute lexical match
with unknown rule unigrams and bigrams. Fu-
ture extensions of the system could include rule
constituent expansion using synonyms, variants or
semantically or lexically similar words, so as to
improve recall and the overall F-measure perfor-
mance.
References
Yun-Nung Chen, William Yang Wang, and Alexan-
der I. Rudnicky. 2013. Unsupervised induction and
filling of semantic slots for spoken dialogue systems
using frame-semantic parsing. In Proceedings of the
2013 IEEEWorkshop on Automatic Speech Recogni-
tion and Understanding, pages 120?125.
Wim De Smet and Marie-Francine Moens. 2013. Rep-
resentations for multi-document event clustering.
Data Mining and Knowledge Discovery, 26(3):533?
558.
Martin Dillon and Ann S. Gray. 1983. FASIT: A
fully automatic syntactically based indexing system.
Journal of the American Society for Information Sci-
ence, 34(2):99?108.
510
High-Level Rule Rule Patterns # Test-set: Unknown n-grams %
Domain Categories #
Training-set Test-set Unigrams Bigrams
Travel EN 32 982 284 5.13% 20.71%
Travel GR 35 956 324 17.26% 33.09%
Tourism EN 24 1004 285 0.72% 4.84%
Finance EN 9 136 37 12.35% 36.74%
Table 2: Characteristics of training and test datasets.
Domain SAIL-GRS Baseline
P P
w
R R
w
F F
w
P P
w
R R
w
F F
w
Travel EN 0.57 0.54 0.66 0.62 0.61 0.58 0.38 0.40 0.67 0.69 0.48 0.51
Travel GR 0.49 0.46 0.62 0.51 0.55 0.49 0.16 0.17 0.73 0.65 0.26 0.26
Tourism EN 0.75 0.75 0.90 0.90 0.82 0.82 0.82 0.80 0.94 0.94 0.87 0.87
Finance EN 0.67 0.78 0.62 0.78 0.65 0.78 0.40 0.48 0.63 0.78 0.49 0.60
Table 3: Evaluation results for SAIL-GRS system compared to the baseline in all four datasets in terms
of per rule Precision P , Recall R, and F-score F . In the grey column, P
w
, R
w
, and F
w
stand for the
weighted average of the per rule precision, recall and F-score respectively, as defined in Equ. 1 and 2.
Sudeep Gandhe and David Traum. 2007. First steps
towards dialogue modelling from an un-annotated
human-human corpus. In Proceedings of the Fifth
IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, pages 22?27.
Yihong Gong and Xin Liu. 2001. Generic text summa-
rization using relevance measure and latent semantic
analysis. In Proceedings of the 24th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?01,
pages 19?25, New York, NY, USA. ACM.
Elias Iosif and Alexandros Potamianos. 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. In Proceedings of the 8th Annual
Conference of the International Speech Communi-
cation Association, pages 1609?1612. ISCA.
Helen M. Meng and Kai-Chung Siu. 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1):172?181.
Andrew N. Pargellis, Eric Fosler-Lussier, Alexandros
Potamianos, and Chin-Hui Lee. 2001. Metrics
for measuring domain independence of semantic
classes. In Proceedings of the 7th European Con-
ference on Speech Communication and Technology,
pages 447?450. ISCA.
Stephanie Seneff. 1992. TINA: A natural language
system for spoken language applications. Computa-
tional Linguistics, 18(1):61?86, March.
Wayne Ward. 1990. The CMU air travel informa-
tion service: Understanding spontaneous speech.
In Speech and Natural Language: Proceedings of
a Workshop Held at Hidden Valley, Pennsylvania,
pages 127?129.
Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2011. Spoken dialogue system based on in-
formation extraction using similarity of predicate ar-
gument structures. In Proceedings of the SIGDIAL
2011 Conference, pages 59?66.
511
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 512?516,
Dublin, Ireland, August 23-24, 2014.
SAIL: Sentiment Analysis using Semantic Similarity and Contrast
Features
Nikolaos Malandrakis, Michael Falcone, Colin Vaz, Jesse Bisogni,
Alexandros Potamianos, Shrikanth Narayanan
Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
{malandra,mfalcone,cvaz,jbisogni}@usc.edu,
potam@telecom.tuc.gr, shri@sipi.usc.edu
Abstract
This paper describes our submission to Se-
mEval2014 Task 9: Sentiment Analysis in
Twitter. Our model is primarily a lexi-
con based one, augmented by some pre-
processing, including detection of Multi-
Word Expressions, negation propagation
and hashtag expansion and by the use of
pairwise semantic similarity at the tweet
level. Feature extraction is repeated for
sub-strings and contrasting sub-string fea-
tures are used to better capture complex
phenomena like sarcasm. The resulting
supervised system, using a Naive Bayes
model, achieved high performance in clas-
sifying entire tweets, ranking 7th on the
main set and 2nd when applied to sarcastic
tweets.
1 Introduction
The analysis of the emotional content of text is
relevant to numerous natural language process-
ing (NLP), web and multi-modal dialogue appli-
cations. In recent years the increased popularity
of social media and increased availability of rele-
vant data has led to a focus of scientific efforts on
the emotion expressed through social media, with
Twitter being the most common subject.
Sentiment analysis in Twitter is usually per-
formed by combining techniques used for related
tasks, like word-level (Esuli and Sebastiani, 2006;
Strapparava and Valitutti, 2004) and sentence-
level (Turney and Littman, 2002; Turney and
Littman, 2003) emotion extraction. Twitter how-
ever does present specific challenges: the breadth
of possible content is virtually unlimited, the writ-
ing style is informal, the use of orthography and
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
grammar can be ?unconventional? and there are
unique artifacts like hashtags. Computation sys-
tems, like those submitted to SemEval 2013 task
2 (Nakov et al., 2013) mostly use bag-of-words
models with specific features added to model emo-
tion indicators like hashtags and emoticons (Davi-
dov et al., 2010).
This paper describes our submissions to Se-
mEval 2014 task 9 (Rosenthal et al., 2014), which
deals with sentiment analysis in twitter. The sys-
tem is an expansion of our submission to the same
task in 2013 (Malandrakis et al., 2013a), which
used only token rating statistics as features. We
expanded the system by using multiple lexica and
more statistics, added steps to the pre-processing
stage (including negation and multi-word expres-
sion handling), incorporated pairwise tweet-level
semantic similarities as features and finally per-
formed feature extraction on substrings and used
the partial features as indicators of irony, sarcasm
or humor.
2 Model Description
2.1 Preprocessing
POS-tagging / Tokenization was performed
using the ARK NLP tweeter tagger (Owoputi et
al., 2013), a Twitter-specific tagger.
Negations were detected using the list from
Christopher Potts? tutorial. All tokens up to the
next punctuation were marked as negated.
Hashtag expansion into word strings was per-
formed using a combination of a word insertion
Finite State Machine and a language model. A
normalized perplexity threshold was used to
detect if the output was a ?proper? English string
and expansion was not performed if it was not.
Multi-word Expressions (MWEs) were detected
using the MIT jMWE library (Kulkarni and
Finlayson, 2011). MWEs are non-compositional
expressions (Sag et al., 2002), which should be
512
handled as a single token instead of attempting to
reconstruct their meaning from their parts.
2.2 Lexicon-based features
The core of the system was formed by the lexicon-
based features. We used a total of four lexica and
some derivatives.
2.2.1 Third party lexica
We used three third party affective lexica.
SentiWordNet (Esuli and Sebastiani, 2006) pro-
vides continuous positive, negative and neutral rat-
ings for each sense of every word in WordNet.
We created two versions of SentiWordNet: one
where ratings are averaged over all senses of a
word (e.g., one ratings for ?good?) and one where
ratings are averaged over lexeme-pos pairs (e.g.,
one rating for the adjective ?good? and one for the
noun ?good?).
NRC Hashtag (Mohammad et al., 2013) Senti-
ment Lexicon provides continuous polarity ratings
for tokens, generated from a collection of tweets
that had a positive or a negative word hashtag.
Sentiment140 (Mohammad et al., 2013) Lexi-
con provides continuous polarity ratings for to-
kens, generated from the sentiment140 corpus of
1.6 million tweets, with emoticons used as posi-
tive and negative labels.
2.2.2 Emotiword: expansion and adaptation
To create our own lexicon we used an automated
algorithm of affective lexicon expansion based on
the one presented in (Malandrakis et al., 2011;
Malandrakis et al., 2013b), which in turn is an ex-
pansion of (Turney and Littman, 2002).
We assume that the continuous (in [?1, 1]) va-
lence, arousal and dominance ratings of any term
t
j
can be represented as a linear combination of
its semantic similarities d
ij
to a set of seed words
w
i
and the known affective ratings of these words
v(w
i
), as follows:
v?(t
j
) = a
0
+
N
?
i=1
a
i
v(w
i
) d
ij
, (1)
where a
i
is the weight corresponding to seed word
w
i
(that is estimated as described next). For the
purposes of this work, d
ij
is the cosine similarity
between context vectors computed over a corpus
of 116 million web snippets (up to 1000 for each
word in the Aspell spellchecker) collected using
the Yahoo! search engine.
Given the starting, manually annotated, lexi-
con Affective Norms for English Words (Bradley
and Lang, 1999) we selected 600 out of the 1034
words contained in it to serve as seed words and
all 1034 words to act as the training set and used
Least Squares Estimation to estimate the weights
a
i
. Seed word selection was performed by a sim-
ple heuristic: we want seed words to have extreme
affective ratings (high absolute value) and the set
to be close to balanced (sum of seed ratings equal
to zero). The equation learned was used to gener-
ate ratings for any new terms.
The lexicon created by this method is task-
independent, since both the starting lexicon and
the raw text corpus are task-independent. To cre-
ate task-specific lexica we used corpus filtering on
the 116 million sentences to select ones that match
our domain, using either a normalized perplex-
ity threshold (using a maximum likelihood trigram
model created from the training set tweets) or a
combination of pragmatic constraints (keywords
with high mutual information with the task) and
perplexity threshold (Malandrakis et al., 2014).
Then we re-calculated semantic similarities on the
filtered corpora. In total we created three lexica: a
task-independent (base) version and two adapted
versions (filtered by perplexity alone and filtered
by combining pragmatics and perplexity), all con-
taining valence, arousal and dominance token rat-
ings.
2.2.3 Statistics extraction
The lexica provide up to 17 ratings for each to-
ken. To extract tweet-level features we used sim-
ple statistics and selection criteria. First, all token
unigrams and bigrams contained in a tweet were
collected. Some of these n-grams were selected
based on a criterion: POS tags, whether a token is
(part of) a MWE, is negated or was expanded from
a hashtag. The criteria were applied separately
to token unigrams and token bigrams (POS tags
only applied to unigrams). Then ratings statistics
were extracted from the selected n-grams: length
(cardinality), min, max, max amplitude, sum, av-
erage, range (max minus min), standard deviation
and variance. We also created normalized versions
by dividing by the same statistics calculated over
all tokens, e.g., the maximum of adjectives over
the maximum of all unigrams. The results of this
process are features like ?maximum of Emotiword
valence over unigram adjectives? and ?average of
SentiWordNet objectivity among MWE bigrams?.
513
2.3 Tweet-level similarity ratings
Our lexicon was formed under the assumption
that semantic similarity implies affective similar-
ity, which should apply to larger lexical units like
entire tweets. To estimate semantic similarity
scores between tweets we used the publicly avail-
able TakeLab semantic similarity toolkit (
?
Sari?c et
al., 2012) which is based on a submission to Se-
mEval 2012 task 6 (Agirre et al., 2012). We used
the data of SemEval 2012 task 6 to train three
semantic similarity models corresponding to the
three datasets of that task, plus an overall model.
Using these models we created four similarity rat-
ings between each tweet of interest and each tweet
in the training set. These similarity ratings were
used as features of the final model.
2.4 Character features
Capitalization features are frequencies and rela-
tive frequencies at the word and letter level, ex-
tracted from all words that either start with a capi-
tal letter, have a capital letter in them (but the first
letter is non-capital) or are in all capital letters.
Punctuation features are frequencies, relative fre-
quencies and punctuation unigrams.
Character repetition features are frequencies,
relative frequencies and longest string statistics of
words containing a repetition of the same letter.
Emoticon features are frequencies, relative fre-
quencies, and emoticon unigrams.
2.5 Contrast features
Cognitive Dissonance is an important phe-
nomenon associated with complex linguistic cases
like sarcasm, irony and humor (Reyes et al., 2012).
To estimate it we used a simple approach, inspired
by one-liner joke detection: we assumed that the
final few tokens of each tweet (the ?suffix?) con-
trast the rest of the tweet (the ?prefix?) and created
split versions of the tweet where the last N tokens
are the suffix and all other tokens are the prefix,
for N = 2 and N = 3. We repeated the fea-
ture extraction process for all features mentioned
above (except for the semantic similarity features)
for the prefix and suffix, nearly tripling the total
number of features.
2.6 Feature selection and Training
The extraction process lead to tens of thousands
of candidate features, so we performed forward
stepwise feature selection using a correlation crite-
Table 1: Performance and rank achieved by our
submission for all datasets of subtasks A and B.
task dataset avg. F1 rank
A
LJ2014 70.62 16
SMS2013 74.46 16
TW2013 78.47 14
TW2014 76.89 13
TW2014SC 65.56 15
B
LJ2014 69.34 15
SMS2013 56.98 24
TW2013 66.80 10
TW2014 67.77 7
TW2014SC 57.26 2
rion (Hall, 1999) and used the resulting set of 222
features to train a model. The model chosen is a
Naive Bayes tree, a tree with Naive Bayes clas-
sifiers on each leaf. The motivation comes from
considering this a two stage problem: subjectivity
detection and polarity classification, making a hi-
erarchical model a natural choice. The feature se-
lection and model training/classification was con-
ducted using Weka (Witten and Frank, 2000).
Table 2: Selected features for subtask B.
Features number
Lexicon-derived 178
By lexicon
Ewrd / S140 / SWNet / NRC 71 / 53 / 33 / 21
By POS tag
all (ignore tag) 103
adj / verb / proper noun 25 / 11 / 11
other tags 28
By function
avg / min / sum / max 45 / 40 / 38 / 26
other functions 29
Semantic similarity 29
Punctuation 7
Emoticon 5
Other features 3
Contrast 72
prefix / suffix 54 / 18
3 Results
We took part in subtasks A and B of SemEval
2014 task 9, submitting constrained runs trained
with the data the task organizers provided. Sub-
task B was the priority and the subtask A model
was created as an afterthought: it only uses the
lexicon-based and morphology features for the tar-
get string and the entire tweet as features of an NB
Tree.
The overall performance of our submission
on all datasets (LiveJournal, SMS, Twitter 2013,
Twitter 2014 and Twitter 2014 Sarcasm) can be
seen in Table 1. The subtask A system performed
514
Table 3: Performance on all data sets of subtask B after removing 1 set of features. Performance differ-
ence with the complete system listed if greater than 1%.
Features removed
LJ2014 SMS2013 TW2013 TW2014 TW2014SC
avg. F1 diff avg. F1 diff avg. F1 diff avg. F1 diff avg. F1 diff
None (Submitted) 69.3 57.0 66.8 67.8 57.3
Lexicon-derived 43.6 -25.8 38.2 -18.8 49.5 -17.4 51.5 -16.3 43.5 -13.8
Emotiword 67.5 -1.9 56.4 63.5 -3.3 66.1 -1.7 54.8 -2.5
Base 68.4 56.3 65.0 -1.9 66.4 -1.4 59.6 2.3
Adapted 69.3 57.4 66.7 67.5 50.8 -6.5
Sentiment140 68.1 -1.3 54.5 -2.5 64.4 -2.4 64.2 -3.6 45.4 -11.9
NRC Tag 70.6 1.3 58.5 1.6 66.3 66.0 -1.7 55.3 -2.0
SentiWordNet 68.7 56.0 66.2 68.1 52.7 -4.6
per Lexeme 69.3 56.7 66.1 68.0 52.7 -4.5
per Lexeme-POS 68.8 57.1 66.7 67.4 55.0 -2.2
Semantic Similarity 69.0 58.2 1.2 64.9 -2.0 65.5 -2.2 52.2 -5.0
Punctuation 69.7 57.4 66.6 67.1 53.9 -3.4
Emoticon 69.3 57.0 66.8 67.8 57.3
Contrast 69.2 57.5 66.7 67.0 51.9 -5.4
Prefix 69.5 57.2 66.8 67.2 47.4 -9.9
Suffix 68.6 57.2 66.5 67.9 56.3
badly, ranking near the bottom (among 20 submis-
sions) on all datasets, a result perhaps expected
given the limited attention we gave to the model.
The subtask B system did very well on the three
Twitter datasets, ranking near the top (among 42
teams) on all three sets and placing second on the
sarcastic tweets set, but did notably worse on the
two non-Twitter sets.
A compact list of the features selected by the
subtask B system can be seen in Table 2. The ma-
jority of features (178 of 222) are lexicon-based,
29 are semantic similarities to known tweets and
the rest are mainly punctuation and emoticon fea-
tures. The lexicon-based features mostly come
from Emotiword, though that is probably because
Emotiword contains a rating for every unigram
and bigram in the tweets, unlike the other lexica.
The most important part-of-speech tags are adjec-
tives and verbs, as expected, with proper nouns
being also highly important, presumably as indi-
cators of attribution. Still, most features are cal-
culated over all tokens (including stop words). Fi-
nally it is worth noting the 72 contrast features se-
lected.
We also conducted a set of experiments using
partial feature sets: each time we use all features
minus one set, then apply feature selection and
classification. The results are presented in Ta-
ble 3. As expected, the lexicon-based features are
the most important ones by a wide margin though
the relative usefulness of the lexica changes de-
pending on the dataset: the twitter-specific NRC
lexicon actually hurts performance on non-tweets,
while the task-independent Emotiword hurts per-
formance on the sarcastic tweets set. Overall
though using all is the optimal choice. Among the
other features only semantic similarity provides a
relatively consistent improvement.
A lot of features provide very little benefit on
most sets, but virtually everything is important for
the sarcasm set. Lexica, particularly the twitter
specific ones like Sentiment 140 and the adapted
version of Emotiword make a big difference, per-
haps indicating some domain-specific aspects of
sarcasm expression (though such assumptions are
shaky at best due to the small size of the test
set). The contrast features perform their intended
function well, providing a large performance boost
when dealing with sarcastic tweets and perhaps
explaining our high ranking on that dataset.
Overall the subtask B system performed very
well and the semantic similarity features and con-
trast features provide potential for further growth.
4 Conclusions
We presented a system of twitter sentiment anal-
ysis combining lexicon-based features with se-
mantic similarity and contrast features. The sys-
tem proved very successful, achieving high ranks
among all competing systems in the tasks of senti-
ment analysis of generic and sarcastic tweets.
Future work will focus on the semantic similar-
ity and contrast features by attempting more accu-
rately estimate semantic similarity and using some
more systematic way of identifying the ?contrast-
ing? text areas.
515
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In proc. Se-
mEval, pages 385?393.
Margaret Bradley and Peter Lang. 1999. Affective
Norms for English Words (ANEW): Stimuli, in-
struction manual and affective ratings. technical re-
port C-1. The Center for Research in Psychophysi-
ology, University of Florida.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proc. COLING, pages 241?249.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource
for opinion mining. In Proc. LREC, pages 417?422.
Mark A. Hall. 1999. Correlation-based feature selec-
tion for machine learning. Ph.D. thesis, The Univer-
sity of Waikato.
Nidhi Kulkarni and Mark Alan Finlayson. 2011.
jMWE: A java toolkit for detecting multi-word ex-
pressions. In proc. Workshop on Multiword Expres-
sions, pages 122?124.
Nikolaos Malandrakis, Alexandros Potamianos, Elias
Iosif, and Shrikanth Narayanan. 2011. Kernel mod-
els for affective lexicon creation. In Proc. Inter-
speech, pages 2977?2980.
Nikolaos Malandrakis, Abe Kazemzadeh, Alexandros
Potamianos, and Shrikanth Narayanan. 2013a.
SAIL: A hybrid approach to sentiment analysis. In
proc. SemEval, pages 438?442.
Nikolaos Malandrakis, Alexandros Potamianos, Elias
Iosif, and Shrikanth Narayanan. 2013b. Distri-
butional semantic models for affective text analy-
sis. Audio, Speech, and Language Processing, IEEE
Transactions on, 21(11):2379?2392.
Nikolaos Malandrakis, Alexandros Potamianos,
Kean J. Hsu, Kalina N. Babeva, Michelle C. Feng,
Gerald C. Davison, and Shrikanth Narayanan. 2014.
Affective language model adaptation via corpus
selection. In proc. ICASSP, pages 4871?4874.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In proc. Se-
mEval, pages 321?327.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment analysis in
Twitter. In Proc. SemEval, pages 312?320.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
proc. NAACL, pages 380?390.
Antonio Reyes, Paolo Rosso, and Davide Buscaldi.
2012. From humor recognition to irony detection:
The figurative language of social media. Data &
Knowledge Engineering, 74(0):1 ? 12.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment analysis in Twitter. In Proc. SemEval.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Compu-
tational Linguistics and Intelligent Text Processing,
volume 2276 of Lecture Notes in Computer Science,
pages 189?206.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet-Affect: an affective extension of WordNet.
In Proc. LREC, volume 4, pages 1083?1086.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus. technical report ERC-
1094 (NRC 44929). National Research Council of
Canada.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on
Information Systems, 21:315?346.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Systems
for measuring semantic text similarity. In proc. Se-
mEval, pages 441?448.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann.
516
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 1?4
Manchester, August 2008
Mitigation of data sparsity in classifier-based translation
Emil Ettelaie, Panayiotis G. Georgiou, Shrikanth S. Narayanan
Signal Analysis and Interpretation Laboratory
Ming Hsieh Department of Electrical Engineering
Viterbi School of Engineering
University of Southern California
ettelaie@usc.edu
Abstract
The concept classifier has been used as a
translation unit in speech-to-speech trans-
lation systems. However, the sparsity of
the training data is the bottle neck of its
effectiveness. Here, a new method based
on using a statistical machine translation
system has been introduced to mitigate the
effects of data sparsity for training classi-
fiers. Also, the effects of the background
model which is necessary to compensate
the above problem, is investigated. Exper-
imental evaluation in the context of cross-
lingual doctor-patient interaction applica-
tion show the superiority of the proposed
method.
1 Introduction
Statistical machine translation (SMT) methods
are well established in speech-to-speech transla-
tion systems as the main translation technique
(Narayanan et al, 2003; Hsiao et al, 2006). Due
to their flexibility these methods provide a good
coverage of the dialog domain. The fluency of
the translation, however, is not guaranteed. Dis-
fluencies of spoken utterances plus the speech rec-
ognizer errors degrade the translation quality even
more. All these ultimately affect the quality of the
synthesized speech output in the target language,
and the effectiveness of the concept transfer.
It is quite common, though, to use other means of
translation in parallel to the SMT methods (Gao et
al., 2006; Stallard et al, 2006). Concept classifica-
tion, as an alternative translation method, has been
successfully integrated in speech-to-speech transla-
tors (Narayanan et al, 2003; Ehsani et al, 2006).
A well defined dialog domain, e.g. doctor-patient
dialog, can be partly covered by a number of con-
cept classes. Upon a successful classification of
the input utterance, the translation task reduces to
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
synthesizing a previously created translation of the
concept, as a mere look up. Since the main goal in
such applications is an accurate exchange of con-
cepts, this method would serve the purpose as long
as the input utterance falls within the coverage of
the classifier. This process can be viewed as a quan-
tization of a continuous ?semantic? sub-space. The
classifier is adequate when the quantization error is
small (i.e. the derived concept and input utterance
are good matches), and when the utterance falls in
the same sub-space (domain) as the quantizer at-
tempts to cover. Since it is not feasible to accu-
rately cover the whole dialog domain (since a large
number of quantization levels needed) the classi-
fier should be accompanied by a translation system
with a much wider range such as an SMT engine.
A rejection mechanism can help identify the cases
that the input utterance falls outside the classifier
coverage (Ettelaie et al, 2006).
In spite of this short coming, the classifier-
based translator is an attractive option for speech-
to-speech applications because of its tolerance to
?noisy? input and the fluency of its output, when it
operates close to its design parameters. In practice
this is attainable for structured dialog interactions
with high levels of predictability. In addition, it can
provide the users with both an accurate feedback
and different translation options to choose from.
The latter feature, specially, is useful for applica-
tions like doctor-patient dialog.
Building a concept classifier starts with identify-
ing the desired concepts and representing them with
canonical utterances that express these concepts. A
good set of concepts should consist of the ones that
are more frequent in a typical interaction in the do-
main. For instance in a doctor-patient dialog, the
utterance ?Where does it hurt?? is quite common
and therefore its concept is a good choice. Phrase
books, websites, and experts? judgment are some of
the resources that can be used for concept selection.
Other frequently used concepts include those that
correspond to basic communicative and social as-
pects of the interaction such as greeting, acknowl-
edgment and confirmation.
After forming the concept space, for each class,
1
utterances that convey its concept must be gath-
ered. Hence, this training corpus would consist of
a group of paraphrases for each class. This form of
data are often very difficult to collect as the number
of classes grow. Therefore, the available training
data are usually sparse and cannot produce a classi-
fication accuracy to the degree possible. Since the
classifier range is limited, high accuracy within that
range is quite crucial for its effectiveness. One of
the main issues is dealing with data sparsity. Other
techniques have also been proposed to improve the
classification rates. For example in (Ettelaie et al,
2006) the accuracy has been improved by introduc-
ing a dialog model. Also, a background model has
been used to improve the discrimination ability of a
given concept class model.
In this work a novel method for handling the
sparsity is introduced. This method utilizes an SMT
engine to map a single utterance to a group of them.
Furthermore, the effect of the background model on
classification accuracy is investigated.
Section 2 reviews the concept classification pro-
cess and the background model. In Section 3 the
sparsity handling method using an SMT is intro-
duced. Data and experiments are described in Sec-
tion 4. The results are discussed in Section 5.
2 Concept classifier and background
model
The concept classifier based on the maximum like-
lihood criterion can be implemented as a language
model (LM) scoring process. For each class a lan-
guage model is built using data expressing the class
concept. The classifier scores the input utterance
using the class LM?s and selects the class with high-
est score. In another word if C is the set of concept
classes and e is the input utterance, the classifica-
tion process is,
c? = argmax
c?C
{P
c
(e | c)} (1)
where P
c
(e | c) is the score of e from the LM of
class c. The translation job is concluded by playing
out a previously constructed prompt that expresses
the concept c? in the target language.
It is clear that a class with limited training data
items will have an undertrained associated LM with
poor coverage. In practice such a model fails to pro-
duce a usable LM score and leads to a poor classifi-
cation accuracy. Interpolating the LM with a back-
ground language model results in a smoother model
(Stolcke, 2002) and increases the overall accuracy
of the classifier.
The background model should be built from a
larger corpus that fairly covers the domain vocab-
ulary. The interpolation level can be optimized for
the best performance based on heldout set.
3 Handling sparsity by statistical
machine translation
The goal is to employ techniques that limit the ef-
fects of data sparsity. What is proposed here is to
generate multiple utterances ? possibly with lower
quality ? from a single original one. One approach
is to use an SMT to generate n-best lists of trans-
lation candidates for the original utterances. Such
lists are ranked based on a combination of scores
from different models (Ney et al, 2000). The hy-
pothesis here is that for an SMT trained on a large
corpus, the quality of the candidates would not de-
grade rapidly as one moves down the n-best list.
Therefore a list with an appropriate length would
consist of translations with acceptable quality with-
out containing a lot of poor candidates. This pro-
cess would result in more data, available for train-
ing, at the cost of using noisier data.
Although the source language of the SMT must
be the same as the classifier?s, its target language
can be selected deliberately. It is clear that a lan-
guage with large available resources (in the form of
parallel corpora with the source language) must be
selected. For simplicity this language is called the
?intermediate language? here.
A classifier in the intermediate language can be
built by first generating an n-best list for every
source utterance in the classifier?s training corpus.
Then the n-best lists associated with each class are
combined to form a new training set. The class
LM?s are now built from these training sets rather
than the original sets of the source utterances.
To classify a source utterance e, first the SMT
is deployed to generate an n-best list (in the inter-
mediate language) from it. The list will consist of
candidates f
1
, f
2
,..., f
n
. The classification process
can be reformulated as,
c? = argmax
c?C
{
n
?
i=1
?
P
c
(f
i
| c)
}
(2)
Here,
?
P
c
(f
i
| c) is the score of the i
th
candidate f
i
from the LM of class c. The scores are considered
in the probability domain.
The new class LM?s can also be smoothed by in-
terpolation with a background model in the inter-
mediate language.
4 Data and Experiments
4.1 Data
The data used in this work were originally collected
for, and used in, the Transonics project (Narayanan
et al, 2003) to develop an English/Farsi speech-to-
speech translator in the doctor-patient interaction
domain. For the doctor side, 1,269 concept classes
were carefully chosen using experts? judgment and
medical phrase books. Then, for each concept, En-
glish data were collected from a website, a web-
based game, and multiple paraphrasing sessions at
the Information Sciences Institute of the University
2
Conventional n-best length
(baseline) 100 500 1,000 2,000
Accuracy [%]
74.9 77.4 77.5 76.8 76.4
Relative error
reduction [%]
0.0 10.0 10.4 7.6 6.0
Accuracy in
4-best [%]
88.6 90.7 91.0 91.3 90.5
Relative error
reduction [%]
0.0 18.4 21.1 23.7 16.7
Table 1: Classification accuracy for the conventional method
and the proposed method with different lengths of n-best list
of Southern California. The total size of the data
set consists of 9,893 English phrases.
As the test corpus for this work, 1,000 phrases
were randomly drawn from the above set and the
rest were used for training. To make sure that the
training set covered every class, one phrase per
class was excluded from the test set selection pro-
cess.
To generate the n-best lists, a phrase based SMT
(Koehn et al, 2003) was used. The intermedi-
ate language was Farsi and the SMT was trained
on a parallel English/Farsi corpus with 148K lines
(1.2M words) on the English side. This corpus
was also used to build the classification background
models in both languages. The SMT was opti-
mized using a parallel development set with 915
lines (7.3K words) on the English side.
4.2 Classification Accuracy Measures
Classifier accuracy is often used as the the qual-
ity indicator of the classification task. However, it
is common in the speech-to-speech translation sys-
tems to provide the user with a short list of potential
translations to choose from. For example the user
of system in (Narayanan et al, 2003) is provided
with the top four classifier outputs. In such cases, it
is practically useful to measure the accuracy of the
classifier within its n-best outputs (e.g., n = 4 for
the above system). In this work the classification
accuracy was measured on both the single output
and the 4-best outputs.
4.3 Experiments
To compare the proposed method with the con-
ventional classification, a classifier based on each
method was put to test. In the proposed method,
it is expected that the accuracy is affected by the
length of the n-best lists. To observe that, n-best
lists of lengths 100, 500, 1000, and 2000 were used
in the experiments. The results are shown in Table
1. In all of the above experiments the background
interpolation factor was set to 0.9 which is close
to the optimum value obtained in (Ettelaie et al,
2006).
To examine the effect of the background model,
the conventional and proposed methods were tried
with different values of the interpolation factor ?
(the background model is weighted by 1 ? ?). For
the conventional method the length of the n-best
list was set to 500. Figure 1 shows the accuracy
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.045%
50%55%
60%65%
70%75%
80%85%
90%95%
Conv. 4-bestConv.New 4-bestNew
Background Interpolation Factor
Accu
racy
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00%
5%
Background Interpolation Factor (?)
Accu
racy
Figure 1: The effect of background model on classification
accuracy
changes with respect to the interpolation factor for
these two methods.
5 Discussion
Table 1 shows the advantage of the proposed
method over the conventional classification with a
relative error rate reduction up to 10.4% (achieved
when the length of the SMT n-best list was 500).
However, as expected, this number decreases with
longer SMT n-best lists due to the increased noise
present in lower ranked outputs of the SMT.
Table 1 also shows the accuracy within 4-best
classifier outputs for each method. In that case
the proposed method showed an error rate which
was relatively 23.7% lower than the error rate of
the conventional method. That was achieved at the
peak of the accuracy within 4-best, when the length
of the SMT n-best list was 1,000. In this case too,
further increase in the length of the n-best list led
to an accuracy degradation as the classifier models
became noisier.
The effect of the background model on classifier
accuracy is shown in Figure 1. The figure shows
the one-best accuracy and the accuracy within 4-
best outputs, versus the background interpolation
factor (?) for both conventional and proposed meth-
ods. As the curves indicate, with ? equal to zero the
classifier has no discriminating feature since all the
class scores are driven solely from the background
model. However, a slight increase in ?, leads to
a large jump in the accuracy. The reason is that
the background model was built from a large gen-
eral domain corpus and hence, had no bias toward
any of the classes. With a small ?, the score from
the background model dominates the overall class
scores. In spite of that, the score differences caused
by the class LM?s are notable in improving the clas-
sifier performance.
As ? increases the role of the class LM?s be-
comes more prominent. This makes the classifier
models more discriminative and increases its accu-
racy as shown in Figure 1. When the factor is in
the close vicinity of one, the smoothing effect of
the background model diminishes and leaves the
3
classes with spiky models with very low vocabu-
lary coverage (lots of zeros). This leads to a rapid
drop in accuracy as ? reaches one.
Both the conventional and proposed methods
follow the above trend as Figure 1 shows, al-
though, the proposed method maintains its supe-
riority throughout the range of ? that was exam-
ined. The maximum measured accuracies for con-
ventional and proposed methods were 75.2% and
78.7% respectively and was measured at ? = 0.999
for both methods. Therefore, the error rate of the
proposed method was relatively 14.1% lower than
its counterpart from the conventional method.
Figure 1 also indicates that when the accuracy is
measured within the 4-best outputs, again the pro-
posed method outperforms the conventional one.
The maximum 4-best accuracy for the conventional
method was measured at the sample point ? = 0.9
and was equal to 88.6%. For the proposed method,
that number was measured as 91.5% achieved at the
sample point ? = 0.999. In another words, consid-
ering the 4-best classifier outputs, the error rate of
the proposed method was relatively 25.4% lower.
6 Conclusion
The proposed language model based method can be
used to improve the accuracy of the concept classi-
fiers specially in the case of sparse training data.
It outperformed the conventional classifier, trained
on the original source language paraphrases, in the
experiments. With this method, when the input ut-
terance is within the classification domain, the clas-
sifier can be viewed as a filter that produces fluent
translations (removes the ?noise?) from the SMT
output.
The experiments also emphasized the impor-
tance of the background model, although indicated
that the classification accuracy was not very sen-
sitive to the value of the background interpolation
factor. This relieves the developers from the fine
tuning of that factor and eliminates the need for a
development data set when a suboptimal solution is
acceptable.
We believe that significant improvements to the
technique can be made through the use of weighted
n-best lists based on the SMT scores. In addition
we believe that using a much richer SMT engine
could provide significant gains through increased
diversity in the output vocabulary. We intend to ex-
tend on this work through the use of enriched, mul-
tilingual SMT engines, and the creation of multiple
classifiers (in several intermediate languages).
7 Acknowledgment
This work was supported in part by funds from
DARPA.
References
Ehsani, F., J. Kinzey, D. Master, K. Sudre, D. Domingo,
and H. Park. 2006. S-MINDS 2-way speech-to-
speech translation system. In Proc. of the Medi-
cal Speech Translation Workshop, Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy (NAACL-HLT), pages 44?45, New York, NY,
USA, June.
Ettelaie, E., P. G. Georgiou, and S. Narayanan. 2006.
Cross-lingual dialog model for speech to speech
translation. In Proc. of the Ninth International Con-
ference on Spoken Language Processing (ICLSP),
pages 1173?1176, Pittsburgh, PA, USA, September.
Gao, Y., L. Gu, B. Zhou, R. Sarikaya, M. Afify, H. Kuo,
W. Zhu, Y. Deng, C. Prosser, W. Zhang, and L. Be-
sacier. 2006. IBM MASTOR SYSTEM: Multilin-
gual automatic speech-to-speech translator. In Proc.
of the Medical Speech Translation Workshop, Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology (NAACL-HLT), pages 53?56,
New York, NY, USA, June.
Hsiao, R., A. Venugopal, T. Kohler, Y. Zhang,
P. Charoenpornsawat, A. Zollmann, S. Vogel, A. W.
Black, T. Schultz, and A. Waibel. 2006. Optimiz-
ing components for handheld two-way speech trans-
lation for an English-Iraqi Arabic system. In Proc. of
the Ninth International Conference on Spoken Lan-
guage Processing (ICLSP), pages 765?768, Pitts-
burgh, PA, USA, September.
Koehn, P., F. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology (NAACL-HLT), volume 1, pages 48?54,
Edmonton, AB, Canada, May-June.
Narayanan, S., S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,
K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy,
D. Traum, and D. Wang. 2003. Transonics: A
speech to speech system for English-Persian inter-
actions. In Proc. of IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU),
pages 670?675, St.Thomas, U.S. Virgin Islands,
November-Decmeber.
Ney, H., S. Nie?en, F. J. Och, C. Tillmann, H. Sawaf,
and S. Vogel. 2000. Algorithms for statistical trans-
lation of spoken language. IEEE Trans. on Speech
and Audio Processing, Special Issue on Language
Modeling and Dialogue Systems, 8(1):24?36, Jan-
uary.
Stallard, D., F. Choi, K. Krstovski, P. Natarajan,
R. Prasad, and S. Saleem. 2006. A hybrid
phrase-based/statistical speech translation system.
In Proc. of the Ninth International Conference on
Spoken Language Processing (ICLSP), pages 757?
760, Pittsburgh, PA, USA, September.
Stolcke, A. 2002. SRILM - an extensible language
modeling toolkit. In Proc. of the International Con-
ference on Spoken Language Processing (ICSLP),
pages 901?904, Denver, CO, USA, September.
4
Proceedings of the SIGDIAL 2013 Conference, pages 394?403,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Which ASR should I choose for my dialogue system?
Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Artstein,
Dog?an Can, Panayiotis Georgiou, Shri Narayanan, Anton Leuski and David Traum
University of Southern California
Los Angeles, California, USA
{morbini,sagae,artstein,leuski,traum}@ict.usc.edu
{audhkhas,dogancan}@usc.edu {georgiou,shri}@sipi.usc.edu
Abstract
We present an analysis of several pub-
licly available automatic speech recogniz-
ers (ASRs) in terms of their suitability for
use in different types of dialogue systems.
We focus in particular on cloud based
ASRs that recently have become available
to the community. We include features
of ASR systems and desiderata and re-
quirements for different dialogue systems,
taking into account the dialogue genre,
type of user, and other features. We then
present speech recognition results for six
different dialogue systems. The most in-
teresting result is that different ASR sys-
tems perform best on the data sets. We
also show that there is an improvement
over a previous generation of recognizers
on some of these data sets. We also inves-
tigate language understanding (NLU) on
the ASR output, and explore the relation-
ship between ASR and NLU performance.
1 Introduction
Dialogue system developers who are not also
speech recognition experts are in a better posi-
tion than ever before in terms of the ease of in-
tegrating existing speech recognizers in their sys-
tems. While there have been commercial solutions
and toolkits for a number of years, there were a
number of problems in getting these systems to
work. For example, early toolkits relied on spe-
cific machine hardware, software, and firmware
to function properly, often had a difficult instal-
lation process, and moreover often didn?t work
well for complex dialogue domains, or challeng-
ing acoustic environments. Fortunately the situ-
ation has greatly improved in recent years. Now
there are a number of easy to use solutions, in-
cluding open-source systems (like PocketSphinx),
as well as cloud-based approaches.
While this increased choice of quality recogniz-
ers is of great benefit to dialogue system develop-
ers, it also creates a dilemma ? which recognizer
to use? Unfortunately, the answer is not simple ?
it depends on a number of issues, including the
type of dialogue domain, availability and amount
of training data, availability of internet connectiv-
ity for the runtime system, and speed of response
needed. In this paper we assess several freely
available speech recognition engines, and exam-
ine their suitability and performance in several di-
alogue systems. Here we extend the work done in
Yao et al (2010) focusing in particular on cloud
based freely available ASR systems. We include
2 local ASRs for reference, one of which was also
used in the earlier work for easy comparison.
2 Speech Recognizer Features and
Engines
The following are some of the major criteria for
selection of a speech recognizer.
Customization Some of the available speech
recognizers allow the users to tune the recognizer
to the environment it will operate in, by providing
a specialized lexicon, trained language models or
acoustic models. Customization is especially im-
portant for dialogue systems whose input contains
specialized vocabulary (see section 4).
Output options A basic recognizer will output
a string of text, representing its best hypothesis
about the transcription of the speech input. Some
recognizers offer additional outputs which are use-
ful for dialogue systems: ranked n-best hypothe-
ses allow later processing to use context for dis-
ambiguation, and incremental results allow the
system to react while the user is still speaking.
Performance characteristics Dialogue systems
differ in their requirements for response speed; a
394
System Customization Output options Open Source PerformanceN-best Incremental Speed Installation
Pocketsphinx Full Yes Yes Yes realtime Local
Apple No Noa No No network Cloud
Google No Yes Yesb No network Cloud
AT&T Partialc Yes No No network Cloud
Otosense-Kaldi Full Yes No Yesd variablee Local
aSingle output annotated with alternative hypotheses. bOnly for web-delivered applications in a Google Chrome browser.
cCustom language models. dRelease scheduled for Fall 2013. eUser controls trade-off between speed and output quality.
Table 1: Speech recognizer features important for use in dialogue systems
speech recognizer that runs locally can help by
avoiding network latencies.
Output quality Typically, a dialogue system
would want the best recognition accuracy pos-
sible given the constraints. Ultimately, dialogue
systems want the output that would yield the best
performance for Natural Language Understand-
ing and other downstream processes. As a rule,
better speech recognition leads to better language
understanding, though this is not necessarily the
case for specific applications (see section 5).
We evaluated 5 freely available speech recog-
nizers. Their features are summarized in Table 1.
We did not include the MIT WAMI toolkit1 as we
are focused on speech services that can directly
be used by stand alone applications as opposed to
web delivered ones. We did not include commer-
cial recognizers such as Nuance, because licensing
terms can be difficult for research institutions, and
in particular, disallow publishing benchmarks.
Pocketsphinx is a version of the CMU Sphinx
ASR system optimized to run also on embedded
systems (Huggins-Daines et al, 2006). Pocket-
sphinx is fast, runs locally, and requires relatively
modest computational resources. It provides n-
best lists and lattices, and supports incremental
output. It also provides a voice activity detec-
tion functionality for continuous ASR. This ASR
is fully customizable and trainable, but users are
expected to provide language models suitable for
their applications. A few acoustic models are pro-
vided, and can be adapted using the CMUSphinx
tools.2
1http://wami.csail.mit.edu/
2http://cmusphinx.sourceforge.net/wiki/tutorialadapt
Apple Dictation is the OS level feature in both
MacOSX and iOS.3 It is integrated into the text in-
put system pipeline so a user can replace her key-
board with a microphone for entering text in any
application. Dictation is often associated with the
Siri personal assistant feature of iOS. While it is
likely that Dictation and Siri share the same ASR
technology, Dictation only does speech recogni-
tion. Apple states that Dictation learns the charac-
teristics of the user?s voice and adapts to her accent
(Apple Inc, 2012). Dictation requires an internet
connection to send recorded user speech to Ap-
ple?s servers and receive ASR results. Processing
starts as soon as the user starts speaking so the de-
lay of getting the recognition results after the user
finishes speaking is minimal.
To integrate Dictation into a dialogue system,
a system designer needs to include any system de-
fined text input control into her application and use
the control APIs to observe text changes. The user
would need to press a key when starting to speak
and push the key again once she is done speak-
ing. The ASR result is a text string annotated with
alternative interpretations of individual words or
phrases in the text. There is an API for extract-
ing those interpretations from the result. While the
Dictation feature is reasonably fast and easy to in-
tegrate, dialogue system developers have no con-
trol over the ASR process, which must be treated
as a black box. Apple dictation is limited in that
no customization is possible, no partial recogni-
tion results are provided, and there is an unspeci-
fied limit on the number of utterances dictated for
a period of time, which is not a problem for inter-
action between a single user and a dialogue sys-
tem, but may be an issue in dialogue systems that
support multiple concurrent users.
3Dictation was introduced in iOS 5.0 and MacOSX 10.8.
395
Google Speech API provides support for the
HTML 5 speech input feature.4 It is a cloud based
service in which a user submits audio data using
an HTML POST request and receives as reply the
ASR output in the form of an n-best list. The au-
dio data is limited to roughly 10 seconds in length,
longer clips are rejected and return no ASR results.
The user can (1) customize the number of hy-
potheses returned by the ASR, (2) specify which
language the audio file contains and (3) enable a
filter to remove profanities from the output text.
As is the case with Apple Dictation, ASR must be
treated as a black box, and no task customization
is possible for dialogue system developers. Users
cannot specify or provide custom language models
or acoustic models. The service returns only the fi-
nal hypothesis, there is no incremental output.5 In
addition, results for the same inputs may change
unpredictably, since Google may update or other-
wise change its service and models, and models
may be adapted using specific audio data supplied
by users. In our experiments, we observed accu-
racy improvements when submitting the same au-
dio files over repeated trials over two weeks.
AT&T Watson is the ASR engine available
through the AT&T Speech Mashup service.6 It is
a cloud based service that can be accessed through
HTML POST requests, like the Google Speech
API. AT&T Watson is designed to support the de-
mands of online spoken dialogue systems, and can
be customized with data specific to a dialogue sys-
tem. Additionally, in our tests we did not observe
any limitation in the maximum length of the in-
put audio data. However, AT&T does not provide
a default general-purpose language model, and
application-specific models must be built within
the Speech Mashup service using user-provided
text data. The acoustic model must be selected
from a list provided by the AT&T service, and
acoustic models can be further customized within
the Speech Mashup service. The ASR returns an
n-best list of hypotheses but does not provide in-
cremental output.
Otosense-Kaldi Another ASR we employed
was the Kaldi-based OtoSense-Kaldi engine de-
4https://www.google.com/speech-api/v1/recognize
5The demo page shows continuous speech understanding
with incremental results but requires Google Chrome to run
and is specific to web delivered applications:
http://www.google.com/intl/en/chrome/demos/speech.html
6https://service.research.att.com/smm
veloped at SAIL.7 OtoSense-Kaldi8 is an on-line,
multi-threaded architecture based on the Kaldi
toolkit (Povey et al, 2011) that allows for dynam-
ically configurable and distributed ASR.
3 Dialogue Systems, Users, and Data
All spoken dialogue systems are similar in some
respects, in that there is speech by a user (or users)
that needs to be recognized, and this speech is
punctuated by speech from the system. More-
over, the speech is not fully independent, but ut-
terances are connected to other utterances, e.g. an-
swers to questions, or clarifications. There are,
however many ways in which systems can differ,
that have implications for which speech recogniz-
ers are most appropriate. Some of the dimensions
to consider are:
Type of microphone(s) One of the biggest im-
pacts on ASR is the acoustic environment. Will
the audio be clean, coming from a close-talking
head or lapel-mounted microphone, or will it need
to be picked up from a broader directional micro-
phone or microphone array?
Number of speakers/microphones Will there
be one designated microphone per person, or will
speaker identification need to be performed? Will
audio from the system confuse the ASR?
Push to talk or continuous speech Will the
user clearly identify the start and end of speech,
or will the system need to detect speech acousti-
cally?
Type of Users Will there be designated long-
term users, where user-training or system model
adaptation is feasible, or will there be many un-
known users, where training is not feasible? See
also section 3.1 for more on user types.
Genre What kinds of things will people be say-
ing to the system? Is it mostly commands or short
answers to questions, or more open-ended conver-
sation? See section 3.2 for more on genre issues.
Training Data Is within-domain training data
available, and if so how much?
3.1 Types of Users
The type of user is important for the overall
design of the system and has implications for
7http://sail.usc.edu
8OtoSense-Kaldi will be released (BSD license) in 2013.
396
ASR performance as well. One important as-
pect is the broad physical differences among
speakers, such as male vs female, adult vs child
(e.g. Bell and Gustafson, 2003), or language pro-
ficiency/accent, that will have implications for the
acoustics of what is said, and ASR results. Other
aspects of users have implications for what will
be said, and how successful the interface may
be, overall. Many (e.g. Hassel and Hagen, 2006;
Jokinen and Kanto, 2004) have looked at the dif-
ferences between novice and expert users. Ai et
al. (2007a) also points out a difference between
real users and recruited subjects. Real users also
come in many different flavors, depending on their
purposes. E.g. are they interacting with the system
for fun, to do a specific task that they need to get
done, to learn something (specific or general), or
with some other purpose in mind?
We considered the following classes of users,
ordered from easiest to hardest to get to acceptable
performance and robustness levels:
Demonstrators are generally the easiest for a sys-
tem to understand ? a demonstrator is trained in
use of the system, knows what can and can?t be
said, is motivated toward success, and is gener-
ally interested in showing off the most impres-
sive/successful aspects of the system to an audi-
ence rather than using it for its own sake.
Trained/Expert Users are similar to demonstra-
tors, but use the system to achieve specific results
rather than just to show off its capabilities. This
means that users may be forced down lines that
are not ideal for the system, if these are necessary
to accomplish the task.
Motivated Users do not have the training of ex-
pert users, and may say many things that the sys-
tem can not handle as opposed to equivalent ex-
pressions that could be handled. However moti-
vated users do want the system to succeed, and in
general are willing to do whatever they think is
necessary to improve system performance. Unlike
expert users, motivated users might be incorrect
about what will help the system (e.g. hyperarticu-
lation in response to system misunderstanding).
Casual Users are interested in finding out what
the system can do, but do not have particular moti-
vations to help or hinder the system. Casual Users
may also leave in the middle of an interaction, if it
is not engaging enough.
Red Teams are out to test or ?break? the system,
or show it as not-competent, and may try to do
things the system can?t understand or react well
to, even when an alternative formulation is known
to work.
3.2 Types of Dialogue System Genres
Dialogue Genres can be distinguished along many
lines, e.g. the number and relationship of partic-
ipants, specific conversational rules, purposes of
the participants, etc. We distinguish here four gen-
res of dialogue system that have been in use at
the Institute for Creative Technologies and that we
have available corpora for (there are many other
types of dialogue genres, including tutoring, ca-
sual conversation, interviewing,. . . ). Each genre
has implications for the internal representations
and system architectures needed to engage in that
genre of dialogue.
Simple Question-answering This genre in-
volves strong user-initiative and weak global di-
alogue coherence. The user can ask any ques-
tion to the system at any time, and the system
should respond, with an appropriate answer if
able, or with some other reply indicating either
inability or unwillingness to provide the answer.
This genre allows modeling dialogue at a surface-
text level (Gandhe, 2013), without internal se-
mantic representations of the input, and where
the result of ?understanding? input is the system?s
expected output. The NCPEditor9 (Leuski and
Traum, 2011) is a toolkit that provides an author-
ing environment, classification, and dialogue ca-
pability for simple question-answering characters.
The SGT Blackwell, SGT Star, and Twins systems
described below are all systems in this genre.
Advanced Question-answering This genre is
similar to the simple question-answering charac-
ters, in that the main task of the user is to elicit
information from the system character. The differ-
ence is that there is more long-range and interme-
diate dialogue coherence, in that questions can be
answered several utterances after they have been
asked, there can be intervening sub-dialogues, and
characters sometimes take the initiative to pursue
their own goals rather than just responding to the
user. Because of the requirements for somewhat
deeper understanding, and relation of input to con-
9Available free for academic research purposes from
https://confluence.ict.usc.edu/display/VHTK/Home
397
text and character goals and policies, there is a
need of at least a shallow semantic representa-
tion and representation of the dialogue informa-
tion state, and the character must distinguish un-
derstanding of the input from the character out-
put (since the latter will depend on the dialogue
policy and information state, not just the under-
standing of input). The tactical questioning archi-
tecture (Gandhe et al, 2009)10 provides author-
ing and run-time support for advanced question-
answering characters, and has been used to build
over a dozen characters for purposes such as train-
ing tactical questioning, training culture, and psy-
chology experiments (Gandhe et al, 2011). The
Amani character described below is in this genre.
Slot-filling Probably the most common type of
dialogue system (at least in the research commu-
nity) is slot-filling. Here the dialogue is fairly
structured, with an initial greeting phase, then one
or more tasks, which all start with the user se-
lecting the task, and the system taking over ini-
tiative to ?fill? and possibly confirm the needed
slots, before retrieving some information from a
database, or performing a simple service.11 This
genre also requires a semantic representation, at
least of the slots and acceptable values. Gener-
ally, the set of possible values is large enough, that
some form of NLG is needed (at least template
filling), rather than authoring of all full sentences.
There are a number of toolkits and development
frameworks that are well suited to slot-filling sys-
tems, e.g. Ravenclaw (Bohus and Rudnicky, 2003)
or Trindikit (Larsson and Traum, 2000). The Ra-
diobots system, described below is in this genre.
Negotiation and Planning In this genre, the
system is more of an equal partner with the user,
than a servant, as in the slot-filling systems. The
system must not merely understand user requests,
but must also evaluate whether they meet the sys-
tem goals, what the consequences and precondi-
tions of requests are, and whether there are better
alternatives. For this kind of inference, a more de-
tailed semantic representation is required than just
filling in slots. While we are not aware of publicly
available software that makes this kind of system
easy to construct, there have been several built us-
ing an information-state approach, or the soar cog-
10Soon to be released as part of the virtual human toolkit.
11Mixed-initiative versions of this genre exist, where the
user can also provide unsolicited information, which reduces
the number of system queries needed.
nitive architecture. The TRIPS system (Allen et
al., 2001) also has many similarities.
3.3 ICT Dialogue Systems Tested
We tested the recognizers described in section 2
on data sets collected from six different dialogue
domains. Five are the same ones tested in Yao et
al. (2010), to which we added the Twins set. De-
tails on the size of the training and development
sets may be found in Yao et al (2010), here we
report only the numbers relevant to the Twins do-
main and to the NLU analysis, which are not in
Yao et al (2010).
SGT Blackwell was created as a virtual human
technology demonstration for the 2004 Army Sci-
ence Conference. This is a question-answering
character, with no internal semantic representation
and the primary NLU task merged with Dialogue
management as selecting the best response.
The original users were ICT demonstrators.
However, there were also some experiments with
recruited participants (Leuski et al, 2006a; Leuski
et al, 2006b). Later SGT Blackwell became a part
of the ?best design in America? triennial at the
Cooper-Hewitt Museum in New York City, and
the data set here is from visitors to the museum,
who are mostly casual users, but range from expert
to red-team. Users spoke into a mounted direc-
tional microphone (see Robinson et al, 2008 for
more details).
SGT STAR (Artstein et al, 2009a) is a question-
answering character similar to SGT Blackwell, al-
though designed to talk about Army careers rather
than general knowledge. The users are Army per-
sonnel who went to job fairs and visited schools in
the mobile Army adventure vans, speaking using
headset microphones, and performing for an audi-
ence. The users are somewhere between demon-
strators and expert users. They are speaking to
SGT STAR for the benefit of an audience, but their
primary purpose is to convey information to the
audience in a memorable way (through dialogue
with SGT STAR) rather than to show off the high-
lights of the character.
The Twins are two life-size virtual characters
who serve as guides at the Museum of Science
in Boston (Swartout et al, 2010). The charac-
ters promote interest in Science, Technology, En-
gineering and Mathematics (STEM) in children
between the ages of 7 and 14. They are question-
398
answering characters, but unlike SGTs Blackwell
and Star, the response is a whole dialogue se-
quence, potentially involving interchange from
both characters, rather than a single character turn.
There are two types of users for the Twins:
demonstrators, who are museum staff members,
using head-mounted microphones, and museum
visitors, who use a Shure 522 table-top mounted
microphone (Traum et al, 2012). More on analy-
sis of the museum data can be found in (Aggarwal
et al, 2012). We also investigated speech recog-
nition and NLU performance in this domain in
Morbini et al (2012).
This dataset contains 14K audio files each an-
notated with one of the 168 possible response se-
quences. The division in training development and
test is the same used in Morbini et al (2012) (10K
for training, the rest equally divided between de-
velopment and test).
Amani (Artstein et al, 2009b; Artstein et al,
2011) is an advanced question-answering char-
acter used as a prototype for systems meant to
train soldiers to perform tactical questioning. The
users are in between real users and test subjects:
they were cadets at the U.S. Military Academy in
April 2009, who interacted with Amani as a uni-
versity course exercise on negotiation techniques.
They used head-mounted microphones to talk with
Amani.
This dataset comprises of 1.8K audio files each
annotated with one of the 105 possible NLU se-
mantic classes.
Radiobots (Roque et al, 2006) is a training pro-
totype that responds to military calls for artillery
fire in a virtual reality urban combat environment.
This is a domain in the slot-filling genre, where
there is a preferred protocol for the order in which
information is provided and confirmed. Users are
generally trainees, learning how to do calls for fire,
they are motivated users with some training. The
semantic processing involved tagging each word
with the dialogue act and parameter that it was as-
sociated with (Ai et al, 2007b).
This data set was collected during the develop-
ment of the system in 2006 at Fort Sill, Oklahoma,
during two evaluation sessions from recruited vol-
unteer trainees who performed calls for specific
missions (Robinson et al, 2006). These subjects
used head-mounted microphones rather than the
ASTI simulated radios from later data collection.
SASO-EN (Traum et al, 2008) is a negotiation
training prototype in which two virtual characters
negotiate with a human ?trainee? about moving a
medical clinic. The genre is negotiation and plan-
ning, where the human participant must try to form
a coalition, and the characters reason about utili-
ties of different proposals, as well as causes and
effects. The output of NLU is a frame represen-
tation including both semantic elements, like the-
matic argument structure, and pragmatic elements,
such as addressee and referring expressions. Fur-
ther contextual interpretation is performed by each
of the virtual characters to match the (possibly par-
tial) representation to actions and states in their
task model, resolve other referring expressions,
and determine a full set of dialogue acts (Traum,
2003). Speech was collected at the USC Insti-
tute for Creative Technologies (ICT) during 2006?
2009, mostly from visitors and new hires, who
acted as test subjects.
This dataset has 4K audio files each anno-
tated with one of the 117 different NLU semantic
classes.
4 ASR Performance
We tested each of the Datasets described in Sec-
tion 3.3 with some of the recognizers described
in Section 2. All recognizers were tested on the
Amani, SASO-EN, and Twins domains, and we
also tested a natural language understanding com-
ponent on these data sets (Section 5). For SGT
Blackwell, SGT STAR, and Radiobots, we report
the performance on the same development set used
in Yao et al (2010). For Amani and SASO-EN
(where we also report the NLU performance), we
run a 10-fold cross-validation in which 9 folds
where used to train the NLU and ASR language
model and the 10th was used for testing. For the
Twins dialogue system, we used the same partition
into training, development and testing reported in
Morbini et al (2012) and the results reported here
are from the development set. Due to differences
in training/testing regimens, performance of sys-
tems are only comparable within each domain.
Table 2 summarizes the performance of the var-
ious ASR engines on the evaluation data sets. Per-
formance is measured as Word Error Rate and was
obtained using the NIST SCLITE tool.12
Note that only Otosense-Kaldi in the Twins do-
main had adapted acoustic models. In the remain-
12http://www.itl.nist.gov/iad/mig/tools/
399
Speech recognizer Evaluation data setAmani Radiobots SASO-EN SGT Blackwell SGT Star Twins
Pocketsphinx 39.7 11.8 28.4 51 28.6 81
Apple 28 ? 30.9 ? ? 29
AT&T 29 12.1 16.3 27.3 21.7 28.8
Google 23.8 36.3 20 18 26 20.6
Otosense-Kaldi 33.7 ? 22.1 ? ? 18.7
Table 2: Word Error Rates (%) for the various dialogue systems and ASR systems tested.
ing cases only the language model was adapted.
Looking at the results on the development set re-
ported in Yao et al (2010), we have improvements
in 3 out of 5 domains: Amani (?11.8% Google),
SASO-EN (?11.7% AT&T) and SGT Blackwell
(?13% Google). In Radiobots and SGT Star the
performance achieved with just language model
adaptation, when permitted, is worse: +4.8% and
+1.7% respectively.
We find that there is no single best performing
speech recognizer: results vary greatly between
the evaluation test sets. In 4 of the 6 datasets over-
all, and 2 of the 3 datatests tested with Otosense-
Kaldi, the best performer is a cloud-based ser-
vice (Google or AT&T). There are two datasets
for which a local, fully customizable recognizer
performs better than the cloud-based services. Ra-
diobots, consisting of military calls for artillery
fire, has a fairly limited and very specialized vo-
cabulary, and indeed the two recognizers with cus-
tom language models (Pocketsphinx and AT&T)
perform much better than the non-customizable
recognizer (Google).
The Twins dataset is unique in that for the
Otosense-Kaldi system we custom-trained acous-
tic and language models, while standard WSJ
acoustic models and adapted language models
were used for the other dialogue systems. In
both cases the models were triphone based with
a Linear Discriminant Analysis (LDA) front end,
and Maximum Likelihood Linear Transforma-
tion (MLLT) and Maximum Mutual Information
(MMI) training. This reflects on the very good
performance in the Twins domain, decent perfor-
mance on the SASO-EN domain (reasonable mis-
match of WSJ and SASO-EN) and very degraded
performance in Amani (highly mismatched Amani
and WSJ domains). The observed degradation in
performance is accentuated by the MMI discrim-
inative training on the mismatched-WSJ data. As
with PocketSphinx and Watson, and unlike with
Apple Dictation and Google Speech API, with
Kaldi we fully control experimental conditions
and can guarantee no contamination of the train-
test data.
In summary, our evaluation shows that cus-
tomizable recognizers are useful when the ex-
pected speech is highly specialized, or when sub-
stantial resources are available for tuning the rec-
ognizer.
5 NLU Accuracy & Relation between
ASR and NLU
While the different genres of system have different
types of output for NLU: response text, dialogue
act and parameter tags, speech acts, or semantic
frames, many of them can be coerced into a se-
lection task, in which the NLU selects the right
output from a set of possible outputs. This allows
any multiclass classification algorithm to be used
for NLU. A possible drawback is that for some
inputs, the right output might not be available in
the set considered by the training data, even if it
might easily be constructed from known parts us-
ing a generative approach.
A second issue is that even though we can cast
the problem as multi-class classification, classifi-
cation accuracy is not always the most appropriate
metric of NLU quality. For question-answering
characters, getting an appropriate and relevant re-
ply is more important than picking the exact re-
ply selected by a human domain designer or an-
notator: there might be multiple good answers, or
even the best available answer might not be very
good. For that reason, the question-answering
characters allow an ?off-topic? answer and Error-
return plots (Artstein, 2011) might be necessary
to choose an optimal threshold. For the SASO-EN
system, slot-filler metrics such as precision, recall,
and f-score are more appropriate than frame accu-
400
racy, because some frames may have many slots
in common and few that are different (e.g. just a
different addressee). Nonetheless, we begin our
analysis within this common framework. For sim-
plicity, we start with just three domains: Twins,
Amani, and SASO-EN. SGT STAR and Blackwell
are very similar to Twins in terms of NLU. Ra-
diobots is more challenging to coerce to multiclass
classification.
Conventional wisdom in the speech and lan-
guage processing community is that performance
of ASR and NLU are closely tied: improved
speech recognition leads to better language under-
standing, while deficiencies in speech recognition
cause difficulty in understanding. This conven-
tional wisdom is borne out by decades of experi-
ence with speech and dialogue systems, though we
are not aware of attempts to systematically demon-
strate it. The present study shows that the expected
relation between speech recognition and language
understanding holds for the systems we tested.
Accepted assumptions about the relation be-
tween speech recognition and language under-
standing have been repeatedly challenged. Direct
challenges are typically limited to specific appli-
cations. Wang et al (2003) show that for a slot-
filling NLU, ASR can be specifically tuned to rec-
ognize those words that are relevant to the slot-
filling task, resulting in improved understanding
despite a decrease in performance on overall word
recognition. However, Boros et al (1996) found
that when not optimizing the ASR for the specific
slot filling task there is a nearly linear correlation
between word accuracy and NLU accuracy. Al-
shawi (2003) and Huang and Cox (2006) show that
in call-routing applications the word level can be
dispensed with altogether and calls routed based
on phonetic information alone without noticeable
loss in performance. These challenges suggest that
the speech-language divide is not as clean as the
theory suggests.
To investigate the relation between ASR and
NLU, we ran each ASR output from each of
the 5 recognizers through an understanding com-
ponent to obtain an NLU output (each dataset
had a separate NLU component, which was held
constant for all speech recognizers). ASR and
NLU performance are conventionally measured on
scales of opposite polarity: better performance
shows up as lower word error rates but higher
NLU accuracies. For the correlations we invert the
conventional ASR scale and use word accuracy, so
that higher numbers signify better performance on
both scales.13
Figure 1 shows the results obtained in the 3 di-
alogue systems by the various ASR systems. The
figures plot ASR performance against NLU per-
formance; NLU results on manual transcriptions
are included for comparison. There are too few
data points for the correlations between ASR and
NLU performance to be significant, but the trends
are positive, as expected.
Our experiments lend supporting evidence to
the claim that in general, ASR performance is pos-
itively linked to NLU performance (special cases
notwithstanding). The 3 datasets exhibit posi-
tive correlations between speech recognition and
language understanding performance. Thus, we
claim that the basis of the conventional wisdom
is sound: speech recognition directly affects lan-
guage understanding. This conclusion holds when
the speech recognizer has been optimized to pro-
duce the most accurate transcript, rather than for a
specific NLU.
6 Conclusion and Future Work
We have extended here the ASR system evaluation
published in Yao et al (2010) including some new
cloud based ASR services that achieve very good
performance showing an improvement of around
12%. We also showed that ASR and NLU perfor-
mance are correlated.
One possible avenue of future work is to ex-
tract importance weights for each word from the
learnt NLU models and use these weights to try
to explain those cases that diverge from the corre-
lation between ASR and NLU performance. This
may also give us a better measure than WER for
assessing ASR performance in dialogue systems.
Another avenue of future work involves examin-
ing different types of NLU engines, and different
metrics for the different dialogue system genres,
which, again, may lead to a more relevant assess-
ment of ASR performance.
Acknowledgments
The effort described here has been sponsored by
the U.S. Army. Any opinions, content or informa-
tion presented does not necessarily reflect the posi-
13We define ?accuracy? as 1 minus WER, so this number
can in principle dip below zero if there are more errors than
words.
401
Amani
r = 0.54, df = 3, p = 0.345
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
50 60 70 80 90 100
51
54
57
60
63
66
69
?
?
?
? ?
?
SASO-EN
r = 0.77, df = 3, p = 0.130
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
60 70 80 90 100
30
40
50
60
70
80
90
?
???
?
?
Twins
r = 0.99, df = 3, p = 0.002
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
0 20 40 60 80 100
40
50
60
70
80
90
100
?
?
?
?
?
Figure 1: Relation between ASR and NLU performance (red dots are manual transcriptions)
tion or the policy of the United States Government,
and no official endorsement should be inferred.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athana-
sios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David Traum. 2012. The Twins cor-
pus of museum visitor questions. In LREC-2012,
Istanbul, Turkey, May.
Hua Ai, Antoine Raux, Dan Bohus, Maxine Eskenazi,
and Diane Litman. 2007a. Comparing spoken dia-
log corpora collected with recruited subjects versus
real users. In SIGdial 2007.
Hua Ai, Antonio Roque, Anton Leuski, and David
Traum. 2007b. Using information state to improve
dialogue move identification in a spoken dialogue
system. In Proceedings of the 10th Interspeech Con-
ference, Antwerp, Belgium, August.
James F. Allen, George Ferguson, and Amanda Stent.
2001. An architecture for more realistic conversa-
tional systems. In IUI, pages 1?8.
Hiyan Alshawi. 2003. Effective utterance classifica-
tion with unsupervised phonotactic models. In HLT-
NAACL 2003, pages 1?7, Edmonton, Alberta, May.
Apple Inc. 2012. Mac basics: Dictation (Technote
HT5449), November.
R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and
D. Traum. 2009a. Semi-formal evaluation of con-
versational characters. In O. Grumberg, M. Kamin-
ski, S. Katz, and S. Wintner, editors, Languages:
From Formal to Natural. Essays Dedicated to Nis-
sim Francez on the Occasion of His 65th Birthday,
volume 5533 of Lecture Notes in Computer Science,
pages 22?35. Springer, Berlin.
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009b. Viability of a simple dia-
logue act scheme for a tactical questioning dialogue
system. In DiaHolmia 2009: Proceedings of the
13th Workshop on the Semantics and Pragmatics of
Dialogue, page 43?50, Stockholm, Sweden, June.
Ron Artstein, Michael Rushforth, Sudeep Gandhe,
David Traum, and MAJ Aram Donigian. 2011.
Limits of simple dialogue acts for tactical question-
ing dialogues. In 7th IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
Barcelona, Spain, July.
Ron Artstein. 2011. Error return plots. In 12th SIG-
dial Workshop on Discourse and Dialogue, Port-
land, OR, June.
Linda Bell and Joakim Gustafson. 2003. Child and
adult speaker adaptation during error resolution in a
publicly available spoken dialogue system. In IN-
TERSPEECH 2003.
Dan Bohus and Alexander I. Rudnicky. 2003. Raven-
claw: dialog management using hierarchical task de-
composition and an expectation agenda. In INTER-
SPEECH 2003.
M. Boros, W. Eckert, F. Gallwitz, G. Grz, G. Han-
rieder, and H. Niemann. 1996. Towards understand-
ing spontaneous speech: Word accuracy vs. concept
accuracy. In In Proceedings of (ICSLP 96), pages
1009?1012.
Sudeep Gandhe, Nicolle Whitman, David R. Traum,
and Ron Artstein. 2009. An integrated authoring
tool for tactical questioning dialogue systems. In 6th
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Pasadena, California, July.
Sudeep Gandhe, Michael Rushforth, Priti Aggarwal,
and David R. Traum. 2011. Evaluation of
an integrated authoring tool for building advanced
question-answering characters. In Proceedings of
Interspeech-11, Florence, Italy, 08/2011.
Sudeep Gandhe. 2013. Rapid prototyping and evalu-
ation of dialogue systems for virtual humans. Ph.D.
thesis, University of Southern California.
402
Liza Hassel and Eli Hagen. 2006. Adaptation of an
automotive dialogue system to users? expertise and
evaluation of the system. Language Resources and
Evaluation, 40(1):67?85.
Quiang Huang and Stephen Cox. 2006. Task-
independent call-routing. Speech Communication,
48(3?4):374?389.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Acoustics,
Speech and Signal Processing, 2006. ICASSP 2006
Proceedings. 2006 IEEE International Conference
on, volume 1, pages I?I.
Kristiina Jokinen and Kari Kanto. 2004. User ex-
pertise modeling and adaptivity in a speech-based
e-mail system. In Donia Scott, Walter Daelemans,
and Marilyn A. Walker, editors, ACL, pages 87?94.
ACL.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language En-
gineering, 6:323?340, September. Special Issue on
Spoken Language Dialogue System Engineering.
Anton Leuski and David R. Traum. 2011. NPCEditor:
Creating virtual human dialogue using information
retrieval techniques. AI Magazine, 32:42?56.
Anton Leuski, Brandon Kennedy, Ronakkumar Patel,
and David Traum. 2006a. Asking questions to
limited domain virtual characters: How good does
speech recognition have to be? In 25th Army Sci-
ence Conference.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006b. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18?27.
Fabrizio Morbini, Kartik Audhkhasi, Ron Artstein,
Maarten Van Segbroeck, Kenji Sagae, Panayio-
tis S. Georgiou, David R. Traum, and Shrikanth S.
Narayanan. 2012. A reranking approach for recog-
nition and classification of speech input in conversa-
tional dialogue systems. In SLT, pages 49?54. IEEE.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka?s?
Burget, Ondr?ej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motl??c?ek, Yanmin Qian, Petr
Schwarz, Jan Silovsky?, Georg Stemmer, and Karel
Vesely?. 2011. The Kaldi speech recognition
toolkit. In IEEE 2011 Workshop on Automatic
Speech Recognition and Understanding, December.
S.M. Robinson, A. Roque, A. Vaswani, D. Traum,
C. Hernandez, and B. Millspaugh. 2006. Evalua-
tion of a spoken dialogue system for virtual reality
call for fire training. In 25th Army Science Confer-
ence, Orlando, Florida, USA.
S. Robinson, D. Traum, M. Ittycheriah, and J. Hen-
derer. 2008. What would you ask a conversational
agent? Observations of human-agent dialogues in
a museum setting. In Proc. of Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
A. Roque, A. Leuski, V. Rangarajan, S. Robinson,
A. Vaswani, S. Narayanan, and D. Traum. 2006.
Radiobot-CFF: A spoken dialogue system for mil-
itary training. In Proc. of Interspeech, Pittsburgh,
Pennsylvania, USA.
W. Swartout, D. Traum, R. Artstein, D. Noren, P. De-
bevec, K. Bronnenkant, J. Williams, A. Leuski,
S. Narayanan, D. Piepol, C. Lane, J. Morie, P. Ag-
garwal, M. Liewer, J. Chiang, J. Gerten, S. Chu,
and K. White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In
J. Allbeck, N. Badler, T. Bickmore, C. Pelachaud,
and A. Safonova, editors, Intelligent Virtual Agents:
10th International Conference, IVA 2010, Philadel-
phia, PA, USA, September 20?22, 2010 Proceed-
ings, volume 6356 of Lecture Notes in Artificial In-
telligence, pages 286?300. Springer, Heidelberg.
David R. Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal
virtual agents. In IVA, pages 117?130.
David Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William Swartout. 2012.
Ada and grace: Direct interaction with museum
visitors. In The 12th International Conference on
Intelligent Virtual Agents (IVA), Santa Cruz, CA,
September.
David Traum. 2003. Semantics and pragmatics of
questions and answers for dialogue agents. In pro-
ceedings of the International Workshop on Compu-
tational Semantics, pages 380?394.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is
word error rate a good indicator for spoken lan-
guage understanding accuracy. In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing (ASRU ?03), pages 577?582.
Xuchen Yao, Pravin Bhutada, Kallirroi Georgila, Kenji
Sagae, Ron Artstein, and David R. Traum. 2010.
Practical evaluation of speech recognizers for vir-
tual human dialogue systems. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, LREC. European Language
Resources Association.
403
