Identifying Terms by their Family and Friends 
Diana Maynard  
Dept. of Computer Science 
University of Sheffield 
Regent Court, 211 Portobello St 
Sheffield, $1 4DP, UK 
d. maynard0dcs, shef. ac. uk 
Sophia Anan iadou 
Computer Science, School of Sciences 
University of Saltbrd, Newton Building 
Saltbrd, M5 4WT, U.K. 
s. ananiadou@salf ord. ac. uk 
Abstract 
Multi-word terms are traditionally identified using 
statistical techniques or, more recently, using hybrid 
techniques combining statistics with shallow linguis- 
tic information. Al)proaches to word sense disam- 
biguation and machine translation have taken ad- 
vantage of contextual information in a more mean- 
ingflfl way, but terminology has rarely followed suit. 
We present an approach to term recognition which 
identifies salient parts of the context and measures 
their strength of association to relevant candidate 
terms. The resulting list of ranked terms is shown 
to improve on that produced by traditional method- 
s, in terms of precision and distribution, while the 
information acquired in the process can also be used 
for a variety of other applications, such as disam- 
biguation, lexical tuning and term clustering. 
1 Introduction 
Although statistical approaches to automatic term 
recognition, e.g. (Bourigault, 1992; Daille et al, 
1994; Enguehard and Pantera, 1994; 3usteson and 
Katz, 1995; Lauriston, 1996), have achieved rela- 
tive success over the years, the addition of suitable 
linguistic information has the potential to enhance 
results still further, particularly in the case of small 
corpora or very specialised omains, where statis- 
tical information may not be so accurate. One of 
the main reasons for the current lack of diversity in 
approaches to term recognition lies in the difficul- 
ty of extracting suitable semantic information from 
speeialised corpora, particularly in view of the lack 
of appropriate linguistic resources. The increasing 
development of electronic lexieal resources, coupled 
with new methods for automatically creating and 
fine-tuning them from corpora, has begun to pave 
the way for a more dominant appearance of natural 
language processing techniques in the field of termi- 
nology. 
The TRUCKS approach to term recognition (Ter- 
m Recognition Using Combined Knowledge Sources) 
focuses on identifying relevant contextual informa- 
tion from a variety of sources, in order to enhance 
traditional statistical techniques of term recognition. 
Although contextual information has been previous- 
ly used, e.g. in general language (Grefenstette, 1994) 
mid in the NC-Value method for term recognition 
(Frantzi, 1998; Frantzi and Ananiadou, 1999), only 
shallow syntactic information is used in these cas- 
es. The TRUCKS approach identifies different; el- 
ements of the context which are combined to form 
the Information Weight, a measure of how strong- 
ly related the context is to a candidate term. The 
hffbrmation Weight is then combined with the sta- 
tistical information about a candidate term and its 
context, acquired using the NC-Value method, to 
form the SNC-Value. Section 2 describes the NC- 
Value method. Section 3 discusses the importance 
of contextual information and explains how this is 
acquired. Sections 4 and 5 describe the hffbrmation 
Weight and the SNC-VMue respectively. We finish 
with an evaluation of the method and draw some 
conclusions about the work and its fllture. 
2 The NC-Value method 
The NC-Value method uses a combination of lin- 
guistic and statistical information. Terms are first 
extracted from a corpus using the C-Value method 
(Frantzi and Ananiadou, 1999), a measure based on 
frequency of occurrence and term length. This is 
defined formally as: 
is not nested 
C-Value(a) = Zo.q~l(~l l~('n,) ~b~T~ f(b)) 
a is nested 
where 
a is the candidate string, 
f(a) is its frequency in the corpus, 
eT, is the set of candidate terms that contain a, 
P(Ta) is the number of these candidate terms. 
Two different cases apply: one for terms that are 
found as nested, and one for terms that are not. If a 
candidate string is not found as nested, its termhood 
is calculated from its total frequency and length. If 
it is found as nested, termhood is calculated from its 
total frequency, length, frequency as a nested string, 
530 
and the tmmber of longer candidate terms it; ai)l)ears 
in. 
The NC-Value metho(1 builds oil this by incorl)o- 
rating contextual information in the form of a con- 
text factor for each candidate term. A context word 
can be any noun, adjective or verb apI)earing with- 
in a fixed-size window of tim candidate term. Each 
context word is assigned a weight, based on how fre- 
quently it appears with a ca lldidate term. Ttmse 
weights m'e titan SUllslned for all colltext words rel- 
ative to a candidate term. The Context l"actor is 
combined with the C-Value to form tlm NC-Value: 
NCvaluc(a) = 0.8 * Cvalue(a) + 0.2 * C l,'(a) (1) 
where 
a is tile candidate term, 
Cvahte(a) is the Cvalue fin' tlm candidate term, 
CF(a) is the context factor tbr the candidate 
term. 
3 Contextua l  In fo rmat ion :  a Term's  
Social Life 
Just as a person's social life can provide valuable 
clues al)out their i)ersonality, so we can gather much 
information about the nature of a term by investi- 
gating the coral)any it keeps. We acquire this knowl- 
edge by cxtra{:ting three different ypes of contextual 
information: 
1. syntactic; 
2. terminologic~fl; 
3. semantic. 
3.1 Syntact i c  knowledge  
Syntactic knowledge is based on words in the con- 
text which occur immediately t)efore or afl;er a can- 
didatc term, wtfich we call boundary words. Follow- 
ing "barrier word" al)proaches to term recoglfition 
(Bourigault, 1992; Nelson et al, 1995), where par- 
titular syntactic ategories are used to delimit era> 
didate terms, we develop this idea fllrther by weight- 
ing boundary words according to tlmir category. The 
weight for each category, shown in Table 1, is all{)- 
cate(1 according to its relative likelihood of occur- 
ring with a term as opposed to a non-term. A verb, 
therefore, occurring immediately before or after a 
candidate, term, is statistically a better indicator of 
a term than an adjective is. By "a better indica- 
tor", we mean that a candidate term occurring with 
it is more likely to be valid. Each candidate term is 
assigned a syntactic weight, calculated by summing 
the category weights tbr the context bomsdary words 
occurring with it. 
Category Weight 
Verb 1.2 
Prep 1.1 
Noun 0.9 
Adj 0.7 
Table 1: We.ights for categories of boundary words 
3.2 Termino log ica l  knowledge  
Ternfinological knowledge concerns the terminologi- 
cal sta.tus of context words. A context word whicll 
is also a term (whicll we call a context erm) is like- 
ly to 1)e a better indicator than one wlfich is not. 
The terminological status is determined by applying 
the NC-Value at)proach to the corlms, and consider- 
ing tile top third of the list; of ranked results as valid 
terms. A context erm (CT) weight is then produced 
fin" each candidate term, based on its total frequency 
of occurrence with all relewmt context terms. The 
CT weight is formally described as follows: 
where 
a is the candidate term, 
7', is the set: of context erms of a, 
d is a word from Ta, 
fa(d) is the frequency of d as a context term of a. 
3.3 Semant ic  knowledge  
Semantic knowledge is obtained about context erms 
using the UMLS Metathesaurus and Semantic Net- 
work (NLM, 1997). The former provides a seman- 
tic tag for each term, such as Acquired Abnormality. 
The latte, r provides a hierarchy of semantic type- 
s, from wlfich we compute the similarity between a 
candidate term and the context I;erms it occurs with. 
An example of part of tim network is shown in Figure 
\]. 
Similarity is measured because we believe that a 
context erm which is semantically similar to a can- 
didate term is more likely to be significant han one 
wlfieh is less similar. We use tim method for seman- 
tic distance described in (M~\ynard and Ananiadou, 
1999a), wtfich is based on calculating the vertical 
position and horizontal distance between odes in a 
hierarchy. Two weights are cMculated: 
? positionah measured by the combined istance 
from root to each node 
? commonality: measured by the number of 
shared common ancestors multiplied by the 
munber of words (usuMly two). 
Similarity between the nodes is calculated by divid- 
ing tim commomflity weight by the 1)ositional weight 
to t)roduce a figure between 0 and 1, I being the ease 
531 
1'1'1 
\['rM 
ENTII'? 
\[ 'rAi l  
PIIYSICM, ()IHECr 
/ ,  
/ 
\[TAIII 
OIIGANISM 
ITAIItl rrAtl21 
PI,ANT I"UN(;US 
ITAIIlll 
ALGA 
\['rlq 
EVI,:NT 
\[TA2I 
CONCEI~I'UAI, ~N'I'I'I'Y 
ITAI21 
ANATOMII2AL STIIUCTURI,: 
/ /  
ITAI211 \[TAI221 
EMIIRYONIC ANA'I'OM \[IUA 1, 
STllUC'I'UItE AIINOILMALrI'Y 
Figure 1: Fragment of the Semantic Network 
where tile two nodes are identical, and 0 being the 
case where there is no common ancestor. This is 
formally defined as follows: 
sim(w,. . .w, , )  - com(w,...w,,) (3) 
pOS(~Ul...Wn) 
where 
corn(w1 ...w,~) is the commonality weight of words 
1. . .n  
pos('wl...w,~) is the positional weight of words 
l...n. 
Let us take an example from the UMLS. The sim- 
ilarity between a term t)elonging to the semantic 
category Plant and one belonging to the category 
Fungus would be calculated as follows:- 
? Plant has the semantic ode TA l l l  and Fungus 
has the semantic ode TAl l2.  
? The commonality weight is the number of nodes 
in common, multiplied by the number of terms 
we are considering. TA l l l  and TA l l2  have 4 
nodes in common (T, TA, TA1 and TAl l ) .  So 
the weight will be 4 * 2 = 8. 
? The positional weight is the total height of each 
of the terms (where tile root node has a height of 
1). TA l l l  has a height of 5 (T, TA, TA1, TA l l  
and TAl l1) ,  and TAl12 also has a height of 5 
(T, TA, TA1, TA l l  and TAl l2) .  The weight 
will therefore be 5 + 5 = 10. 
? The similarity weight is tile comlnonality 
weight divided by the positional weight, i.e. 
8/10 = 0.8. 
4 The  In fo rmat ion  Weight  
The three individual weights described above are 
calculated for all relevant context words or context 
terms. The total weights for the context are then 
combined according to the following equation: 
IW(a) = ~ .syria(b) + ~ f,(d) . sim,(d) (4) 
beC. (l~7~ 
where 
a is the candidate term, 
Cais the set of context words of a, 
b is a word from C,,  
f,(b) is tlm frequency of b as a context word of a, 
syn~(b) is the syntactic weight of b as a context 
word of a, 
T. is the set of context terms of a, 
d is a word fl'om T., 
fi,(d) is the frequency of d as a context erm of a, 
sims(d) is the similarity weight of d as a context 
term of a. 
This basically means that the Infornlation Weight 
is composed of the total terminological weight, 511151- 
tiplied by tile total semantic weight, and then added 
to the total syntactic weight of all the context words 
or context erms related to the candidate term. 
5 The  SNC-Va lue  
Tile Information Weight gives a score for each candi- 
date term based on the ilnt)ortance of the contextual 
intbrmation surrounding it. To obtain the final SNC- 
Value ranking, the Information Weight is combined 
with the statistical information obtained using the 
NC-Vahm nmthod, as expressed formally below: 
SlVCV,a.,c(a) = NCVal~u~(a) + IW(a) (5) 
where  
a is the candidate term 
NCValue(a) is the NC-Value of a 
IW is the Inqmrtance Weight of a 
For details of the NC-Value, see (l:5'antzi and Ana- 
niadou, 1999). 
An example of the final result is shown in Table 
2. This corot)ares tile top 20 results from the SNC- 
Value list with the top 20 from the NC-Value list. 
The terms in italics are those which were considered 
as not valid. We shall discuss the results in more de- 
tail in the next section, but we can note here three 
points. Firstly, the weights for the SNC-Value are 
substantially greater than those for the NC-Vahm. 
This, in itself, is not important, since it, is the posi- 
tion in the list, i.e. the relative weight, rather than 
the absolute weight, which is important. Secondly, 
we can see that there are more valid terms in the 
SNC-Value results than in the NC-Value results. It 
532 
Term SNC '\].L'rm NC 
l)owlllall ~S_lllelllbralle 
\]nalignant_melanoma 
hyaline_fibrous_tissue 
planes_of_section 
tral) ecularJneshwork 
keratinous_del)ris 
l)ruch~s_inenll)r &lie 
plane_of_section= 
mclanoma_of_choroid 
lymphocytieAnfiltration 
ciliary_processes 
cellularAibrous_tissue 
squamous_ct)ithelium 
oI)tic_nerve_head 
l)Ul)illary_border 
(:orlmal_el)ithelium 
seleraldnw~sion 
granulation_tissue 
stratified_squamous_epithelium 
ocular~structures 
605782 
231237 
215843 
170016 
157353 
101644 
94996.2 
90109.4 
71.615.1 
53822 
52355.7 
51486.8 
46928.9 
39054.5 
36510.8 
31.335.9 
31017.4 
28010.1 
27445.5 
26143.6 
pla'ne_@section 
dencelnel;~s_ill(~.llll)r~/iEe 
basal_cell_carcinoma 
stump_of_optic_nerve 
1)asal_cell_l)at)illoma 
planc_of_section= 
rnclano,na_of_ch, oroid 
pla'ncs_@scction 
malignant _melanoma 
optic_nerveAmad 
ciliaryq)rocesses 
1)ruth's_membrane 
keratinous_eyst 
ellipse_of_skin 
wcdgc_of_lid_ma~yin 
scaT"_tT'ack 
conImctive_tissue 
vertical_plane 
carcinoma_of_lid 
excision_biopsy 
1752.71 
1.345.76 
1.268.21 
993.15 
616.614 
506.517 
497.673 
453.716 
448.591 
422.211 
421.204 
413.027 
392.944 
267.636 
211.41.4 
228.217 
167.053 
167.015 
164 
155.257 
Table 2: Top 20 results for the SNC-VaIue and NC-Value 
in hard to make flu:ther judgements based on this 
list alone, 1)ecause we cmmot s~3; wlmther on(; ter- 
\]u is 1)etter than another, if tiE(; two terms are both 
valid. Thirdly, we can nee that more of the top 20 
terms are valid tin' tim SNC-Vahm than for the NC- 
Value: 17 (851X,) as ot)t)osed to 10 (50%). 
6 Eva luat ion  
The SNC-Value method wan initially t(;sted on a eor- 
l)US of 800,000 eye t)athoh)gy reI)ortn , which had 
1)een tagged with the Brill t)art-of-nl)eeeh tagger 
(Brill, 1992). The ca.ndidate terms we,'e first ex- 
tracted using the NC-Value method (lhantzi, 1998), 
and the SNC-Value was then (:alculated. To exvdu- 
ate the results, we examined the p(.'rformanee of the 
similarity weight alone, and the overall 1)erformance 
of the system. 
6.1 Evaluation methods 
The main evaluation i)rocedure was carried out with 
resl)ect o a manual assessment of tim list of terms 
l)y 2 domain exI)erts. There are, however, 1)roblems 
associated with such an evaluation. Firstly, there ix 
no gold standm:d of evaluation, and secondly, man- 
ual evaluation is both fallil)le and sul)jective. To 
avoid this 1)rol)lem, we measure the 1)erformance of
the system ill relative termn rather than in abso- 
lute terms, by measuring the improveln(mt over the 
results of tile NC-Value as eomt)ared with mmmal 
evahlation. Although we could have used the list 
of terms 1)rovided in the UMLS, instead of a manu~ 
ally evahlated list, we found that there was a huge 
discrei)an(:y 1)etween this lint and the lint validated 
by the manual experts (only 20% of the terms they 
judged valid were fOtlEl(1 ill the UMLS). There are 
also further limitations to the UMLS, such as the 
fact that it is only nl)e(:ific to medicine in general, 
1)ut not to eye t)athology, and the fact that it; is or- 
ganised ill nllch a way that only the preferred terms, 
and not lexical variants, m'e actively and (:onnistent- 
ly 1)r(~sent. 
We first evaluate the similarity weight individu- 
ally, since this is the main 1)rinciple on which the 
SNC-\Sflue method relies. We then ewduate the 
SNC-VaIue as a whole t)y comparing it with the NC- 
Value, so I;hat we can ewfluate the impact of tile ad- 
dition of the deel)er forms of linguistic information 
incorl)orated in {:he hnI)ortance Weight. 
6.2 Similarity Weight 
One of the 1)roblems with our method of calculat- 
ing similarity is that it relies on a 1)re-existing lexi- 
(:al resource, which Eneans it is 1)rone to errors and 
omissions. Bearing in mind its innate inadequacies, 
we can nevertheless evaluate the expected theoretical 
performance of tilt measure by concerning ourselves 
only with what is covered by the thesaurus. This 
means that we assume COml)leteness (although we 
know that this in not the case) and evahtate it ac- 
cordingly, ignoring anything which may be inissing. 
The semantic weight ix based on the premise that 
tile more similar a context term is to the candidate 
term it occurs with, the better an indicator that con- 
text term is. So the higher the total semantic weight 
533 
Section Term Non-Term 
top set 76% 24% 
middle set 56% 44% 
bottom set 49% 51% 
Table 3: Semantic weights of terms and non-terms 
for the candidate term, the higher the ranking of the 
term and the better the chance that the candidate 
term is a valid one. To test the performmme of the 
semantic weight, we sorted the terms in descending 
order of their semantic weights and divided the list 
into 3, such that the top third contained the terms 
with the highest semantic weights, and the bottom 
third contained those with the lowest. We then com- 
pared how many valid and non-valid terms (accord- 
ing to the manual evaluation) were contained in each 
section of the list,. 
Tile results, depicted in Table 3, can be interpret- 
ed as follows. In the top third of the list;, 76% were 
terms and 24% were non-terms, whilst in the middle 
third, 56% were terms and 44% were non-terms, and 
so on. This means that most of the valid terms are 
contained in the top third of tile list mid the fewest 
valid terms are contained in the bottom third of the 
list. Also, the proportion of terms to non-terms in 
tile top of tile list is such that there are more terms 
than non-terms, whereas in the bottom of the list; 
there are more non-terms than ternis. This there- 
fore demonstrates two things: 
? more of' the terms with the highest semantic 
weights are valid, and fewer of those with the 
lowest semmitic weights are valid; 
? more valid terms have high semantic weights 
than non-terms, mid more non-terms have lower 
semantic weights than valid terms. 
We also tested the similarity measure to see 
whether adding sosne statistical information would 
improve its results, and regulate any discrepancies 
in tile uniformity of the hierarchy. The method- 
s which intuitively seem most plausible are based 
on information content, e.g.(Resnik, 1995; Smeaton 
and Quigley, 1996). The informatiosl content of a n- 
ode is related to its probability of occurrence in the 
corpus. Tile snore fi'equently it appears, the snore 
likely it is to be important in terms of conveying 
information, and therefore the higher weighting it 
should receive. We performed experiments to cosn- 
pare two such methods with our similarity measure. 
The first considers the probability of the MSCA of 
the two terms (the lowest node which is an ancestor 
of both), whilst the second considers the probability 
of the nodes of the terms being colnpared. However, 
the tindings showed a negligible difference between 
the three methods, so we conchlde that there is no 
SNC-Value NC-Vahm 
Section Valid Precision Valid Precision 
1 163 64% 160 62% 
2 84 aa% 98 38% 
3 89 35% 69 27% 
4 89 35% 78 30% 
5 76 30% 87 34% 
6 57 22% 78 30% 
7 66 26% 92 36% 
8 75 29% 100 39% 
9 70 27% 42 16% 
10 59 23% 68 27% 
Table 4: Precision of SNC-Vahle and NC-Value 
advantage to be gained by adding statistical int'or- 
mation, fbr this particular corpus. It; is possible that 
with a larger corlms or different hierarchy, this might 
slot be the case. 
6.3 Overall Evaluat ion of the SNC-Value 
We first; compare the precision rates for the SNC- 
Value and the NC-Value (Table 4), by dividing tile 
ranked lists into 10 equal sections. Each section con- 
tains 250 terms, marked as valid or invalid by the 
manual experts. In the top section, the precision is 
higher for the SNC-Value, and in the bottom section, 
it is lower. This indicates that the precision span is 
greater fl~r the SNC-Value, and therefore that the 
ranking is improved. The distribution of valid terms 
is also better for the SNC-Value, since of the valid 
terms, more appear at the top of the list than at the 
bottom. 
Looking at Figure 2, we can see that the SNC- 
Value graph is smoother than that of the NC-Vahle. 
We can compare the graphs niore accurately using 
a method we call comparative upward trend. Be- 
cruise there is no one ideal graph, we instead mea- 
sure how much each graph deviates from a mono- 
tonic line downwards. This is calculated by dividing 
the total rise in precision percentage by the length 
of the graph. A graph with a lower upward trend 
will therefore be better than a graph with a higher 
upward trend. If we compare the upward trends of 
the two graphs, we find that the trend for the SNC- 
Value is 0.9, whereas the trend for the NC-Value is 
2.7. This again shows that the SNC-Value rmiking 
is better thmi the NC-Value ranking, since it is more 
consistent. 
Table 5 shows a more precise investigation of the 
top portion of the list, (where it is to be expected 
that ternis are most likely to be wflid, and which 
is therefore the inost imi)ortant part of the list) We 
see that the precision is most iml)roved here, both 
in terms of accuracy and in terms of distribution 
of weights. At the I)ottom of the top section, the 
534 
9O 
U{} 
71} 
60 
PlccJshm 50 
,111 
30 
211 
10 
SN{" Vah,c 
. . . .  NC-Vah,c 
\ 
\ 
T ~  T T I 
I 3 4 ~ 6 7 8 9 10 
Scct iono l l i s t  
Figure 2: Precision of SNC-Value and NC-Vatue 
SNC-\Sflue 
Section Valid I Precision 
1 21 184% 
2 19 176% 
3 ~" '68% i i 
4: 16 164% 
5 1.8 172% 
6 12 148% 
7 13 152% 
8 : 7 : 68{/{) 
9 \] 3 I 52% 
10 \] 4 i 56% 
\] N C-Value 
Valid Precision 
z 
19 76% 
23 92% 
21 84% 
13 52% 
13 52% 
19 76% 
18 72% 
14 56% 
10 40% 
8 32% 
Table 5: Precision of SNC-\Sdue and NC-Vahm for 
top 250 terms 
precision is much higher for the SNC-Value. This is 
important because ideally, all the terms in this part 
of the list should be valid, 
7 Conc lus ions  
In this paper, we have described a method for multi- 
word term extraction which improves on traditional 
statistical at)proaches by incorporating more specific 
contextual information. It focuses particularly on 
measuring the strength of association (in semantic 
terms) l)etween a candidate term and its context. 
Evahlation shows imi)rovement over the NC-Vahm 
approach, although the percentages are small. This 
is largely l)ecmlse we have used a very small corpus 
for testing. 
The contextuM information acquired can also be 
used for a mmlber of other related tasks, such as 
disambiguation and clustering. At present, the se- 
mantic information is acquired from a 1)re-existing 
domain-slmcitic thesaurus, but there m:c 1)ossibili- 
tics for creating such a thesaurus automatically, or 
entrancing an existing one, using the contextual in- 
formation we acquire (Ushioda, 1996; MaynaM and 
Anmfiadou, 1999b). 
There is much scope tbr filrther extensions of this 
research. Firstly, it; could be extended to other (lo- 
mains and larger corpora, in order to see the true 
benefit of such a.n apl)roach. Secondly, the thesaurus 
could be tailored to the corpus, as we have men- 
tioncd. An incremental approach might be possible, 
whereby the similarity measure is combined with s- 
tatistical intbrmation to tune an existing ontology. 
Also, the UMLS is not designed as a linguistic re- 
source, but as an information resource. Some kind 
of integration of the two types of resource would be 
usefifl so that, for example, lexical variation could 
be more easily handled. 
References  
D. Bourigault. 1992. Surface grammatical analysis 
for tile extraction of terminological noun phras- 
es. In Proc. of l~th International Co~@rcncc 
on Computational Linguistics (COL\[NG), pages 
977-981, Nantes, bYance. 
Eric Brill. 1992. A simple rule-based part of speech 
tagger. In Pwc. of 3rd Confc~vnce of Applied Nat- 
ural Language Processing. 
B. l)aille, E. Gaussicr, and J.M. Lang5. 1994. To- 
wards automatic extraction of monolingual and 
t)ilingual terminology. In Proc. of iSth Interna- 
tional Conference on Computational Linguistics 
(COLIN(;), pages 515-521. 
Chantal Enguehard and Lmu'ent Pantera. 1994. 
Autoumtic natural a(:quisition of a terminology. 
Journal of Quantitative Linguistics, 2(1):27-32. 
K.T. li'r;mtzi and S. Ananiadou. 1.999. The C- 
Value/NC-Vahm domain independent method ~br 
multi-word term extraction. Journal of Natural 
Language PTvccssing, 6(3):1.45 179. 
K.T. Frantzi. 1.998. Automatic Recognition of 
Multi-Word Terms. Ph.D. thesis, Manchester 
Metropolitan University, England. 
G. Grefenstette. 1994. E:rplorations in Automatic 
Thesaurus Discovcry. Kluwer Aca(temic Publish- 
ers .  
J.S. Justcson and S.M. Katz. 1995. Technical ter- 
minology: some linguistic properties and an algo- 
rithm for identification in text. Natural Language 
Engineering, 1:9-27. 
Andy Lauriston. 1996. Automatic term recognition: 
performance of lin9uistic and statistical learning 
techniques. Ph.D. thesis, UMIST, Manchester, 
UK. 
D.G. Maynard and S. Anmfiadou. 1999a. hlentify- 
ing contextual information tbr term extraction. In 
i}Tvc, of 5th International Congress on 7~rminol- 
535 
ogy and Knowlc@c Engineering (TKE '99), pt~ges 
212-221, Innsbruck, Austria. 
D.G. Maynard and S. Anmfiadou. 1999b. A linguis- 
tic ~I)proach to context clustering. In Proc. of Nat- 
n~nl Language Proecssinfl Pacific \]~im Symposium 
(NLPRS), pages 346-351, Beijing, China. 
S.J. Nelson, N.E. Olson, L. Fuller, M.S. Turtle, W.G. 
Cole, and D.D. Sherertz. 1995. Identifying con- 
cepts in medical knowledge. In Proc. of 8th World 
Congress on Medical Informatics (MEDINFO), 
1)~ges 33-36. 
NLM, 1997. UMLS K?wwlcdgc Sourccs. National 
Library of Medicine, U.S. Dept. of Health and Hu- 
man Services, 8th edition, January. 
P. Resnik. 1995. Disambiguating noun groupings 
with respect o WordNet senses. In Proc. of 3rd 
Workshop on Very Large Corpora. MIT. 
A. Smeaton and I. Quigley. 1996. Experiments on 
using semantic distances between words in image 
caption retrieval. In Proc. of 19t.h htternationaI 
Conferc'ncc on Research and Development i~. I'n- 
formation Retrieval, Zurich, Switzerland. 
Akira Ushioda. 1996. IIierarchical clustering of 
words. In Proc. of 16th I'ntcrnational ConfcT~cncc 
on Computational Linguistics (COLING), pages 
1159 1162. 
536 
219
220
221
222
Information Extraction: Algorithms and Prospects in a
Retrieval Context
Marie-Francine Moens
(Katholieke Universiteit Leuven)
Springer (Information retrieval series, edited by W. Bruce Croft), 2006, xiii+246 pp;
ISBN 978-1-4020-4987-3, $119.00
Reviewed by
Diana Maynard
University of Sheffield
Published as part of the Information Retrieval Series, this book aims to present both
a historical overview of information extraction (IE) and a description of current ap-
proaches and applications. Essentially, it introduces the topic of information extraction
to an information retrieval (IR) audience, aiming as much at students as established
researchers, and focusing primarily on the algorithms used. Although it claims to
give equal importance to early technologies developed in the field and to the ?most
advanced and recent technologies,? for the latter it concentrates mainly on machine-
learning approaches rather than knowledge-engineering (rule-based) techniques.
The book is well-structured and progresses from an introductory chapter that gives
a brief explanation of information extraction and how it fits into the IR paradigm,
through a historical overview of the field in Chapter 2, and on to the meat of the book,
which describes different approaches to IE in a set of four chapters leading from sym-
bolic techniques and pattern recognition through to supervised and then unsupervised
classification techniques. Chapter 7 discusses information retrieval models and makes
some suggestions as to why and how information extraction should be incorporated
into these models. Chapter 8 then discusses evaluation of IE technologies, focusing
on the most commonly used measures from the Message Understanding Conference
(MUC) and Automatic Content Extraction (ACE) competitions, and suggesting other
ways in which evaluation might be measured. Chapter 9 describes some case studies
in which information extraction is commonly used. Finally, the author takes a look at
the future of information extraction within an information retrieval context, discussing
some of the findings and challenges for current research.
The aim of the book is quite ambitious in attempting to cater to the rather different
needs of both students and established researchers. On the one hand, the chapters on
the history of information extraction would be interesting for students; on the other
hand, this could be of lesser interest to researchers concerned about the techniques and
applications. The first half of the book is quite readable; the second half, however, targets
those with quite some knowledge already of statistical processing, language modeling,
and so on. Although the book is designed more for reading right through than dipping
into individual chapters, the middle sections are certainly not easy reading, except
perhaps for those already familiar with the topic.
It is slightly disappointing that the book is angled almost exclusively towards
machine-learning approaches to IE, and consequently omits any mention of some of the
leading tools in the field such as GATE (Cunningham et al 2002) and KIM (Popov et al
2004), which are based on knowledge-engineering approaches. Although the book does
not concern itself with examining individual IE tools, one might expect some reference
to such tools, at least in the historical chapters if nowhere else. It is also unfortunate
Computational Linguistics Volume 34, Number 2
that the quality of the English is quite poor: The dozens of spelling and grammatical
mistakes impair the quality and, at times, readability of the work.
The first two chapters of the book, introducing the topic and detailing the history
of IE, are most interesting and present an overview of the development of the field that
is hard to find in other work. One might have expected to find more about the MUC
competitions in this section, which shaped the way for much current research in IE by
pushing the technology towards real applications and providing amechanism bywhich
future tools could be evaluated and compared. There is some discussion of the MUC
evaluationmethodology later in the book, however. The chapter on symbolic techniques
is also written very much from a historical perspective, but does not seem to contribute
much to the book as a whole, and is probably of most interest to linguistics students.
The chapter on pattern recognition, which one might expect to deal with approaches for
rule-based IE, focuses more on investigating the different information units and features
that are used for machine-learning techniques, described in the following chapters.
Whereas the classification of which features are useful for which tasks is most useful,
the omission of more discussion about hand-crafted approaches is a little disappointing.
The section on active learning also has some omissions. There is no reference to some of
the earliest work such as that of Thompson, Califf, and Mooney (1999), and the author?s
claim that experiments with such techniques are ?recent and limited? is perhaps a little
too dismissive.
The chapters on supervised and unsupervised classification are geared towards
describing different machine-learning methods with particular respect to IE tasks such
as named entity recognition, and give a very detailed overview of the approaches
possible. A useful addition would have been a summary of the pros and cons of each
method: The reader has to delve deep into each section in order to find this information.
The material is well described but still quite difficult to follow for someone without
a statistics background?it?s definitely not a ?Machine Learning for Dummies??and
again, more examples would have been useful.
Chapter 7 discusses how the results of IE could be applied to retrieval models. It
first describes the IR models used, which is interesting for someone unfamiliar with the
field. However, the discussion of the integration of IE does not seem convincing and
there is once again a lack of real examples which would strengthen the arguments.
Chapter 8 is devoted to evaluation techniques for IE. Although the explanation is
thorough, it is quite complicated and would have benefited from a discussion of the
difference between the approaches and the appropriateness of each for different tasks.
The complicated ACE value, as used in the Automatic Content Extraction series (a
successor of MUC) is explained in great detail, but even so it is hard for the reader to
grasp the intricacies of the model and a real understanding of the benefits of each part
of it. Most of the discussion is limited to MUC and ACE measures; however it would
have been even more interesting if the author had investigated other possible criteria,
which are only briefly mentioned at the end, and with no indication of how to evaluate
them. For example, the difficulty of evaluating information extraction with respect to an
ontology is not discussed, yet this is a very timely and important issue with the advent
of systems that categorize entities into many related classes, rather than the handful of
classes traditionally used.
Chapter 9 aims to illustrate the different IE approaches and tasks with a set of
applications, looking at six domains in which IE is commonly used: news, biomedicine,
criminal intelligence, and business, legal, and informal texts. The focus is more on the
needs and problems in these areas than on specific applications that have been devel-
oped. Although the chapter gives quite a good overview of these domains, it would
316
Book Reviews
have been useful to see some more concrete examples explaining exactly how the use of
IE technology would be of use, as is done for the intelligence domain. For example, the
section on news texts just describes the MUC and ACE competitions but might leave
the uninitiated reader still guessing as to what the technology might practically be used
for in this field. Some interesting problems are highlighted but unfortunately there are
no real solutions to these as yet: for example, problems in the scalability of IE systems
are still a major impediment to the more-widespread uptake of such tools in industry.
The final chapter sums up the most important findings from the previous chapters
and makes some suggestions for the future of Information Extraction in a retrieval
context. Although it does not offer any earth shattering new ideas here, it does make
some very valid points about the importance of relation finding and advanced retrieval
techniques.
In summary, the book offers a useful overview of the field of IE for those unfamiliar
with the topic, but it is probably not for those seeking novel ideas or an application-
oriented approach. It could be an appropriate textbook for those in the IR field looking
for an introduction to IE, but probably not as useful for those in the IE field looking to
expand their horizons into IR. Overall, I would suggest the book as one for the library
rather than the personal bookshelf.
References
Cunningham, Hamish, Diana Maynard,
Kalina Bontcheva, and Valentin Tablan.
2002. GATE: A framework and graphical
development environment for robust NLP
tools and applications. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL ?02),
pages 168?175, Philadelphia, PA.
Popov, Borislav, Atanas Kiryakov, Damyan
Ognyanoff, Dimitar Manov, and Angel
Kirilov. 2004. KIM?A semantic platform
for information extraction and retrieval.
Natural Language Engineering,
10(3/4):375?392.
Thompson, Cynthia A., Mary Elaine
Califf, and Raymond J. Mooney. 1999.
Active learning for natural language
parsing and information extraction. In
Proceedings of the 16th International Machine
Learning Conference, pages 406?414,
Bled, Slovenia.
Diana Maynard is a Research Associate at the University of Sheffield. She has interests in in-
formation extraction, tools for language engineering, terminology, evaluation, and accessibility
of technology. Her address is Department of Computer Science, University of Sheffield, Regent
Court, 211 Portobello St., Sheffield, S1 4DP, UK; e-mail: diana@dcs.shef.ac.uk.
317

 
	
Using GATE as an Environment for Teaching NLP
Kalina Bontcheva, Hamish Cunningham, Valentin Tablan, Diana Maynard, Oana Hamza
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
{kalina,hamish,valyt,diana,oana}@dcs.shef.ac.uk
Abstract
In this paper we argue that the GATE
architecture and visual development
environment can be used as an effec-
tive tool for teaching language engi-
neering and computational linguistics.
Since GATE comes with a customis-
able and extendable set of components,
it allows students to get hands-on ex-
perience with building NLP applica-
tions. GATE also has tools for cor-
pus annotation and performance eval-
uation, so students can go through the
entire application development process
within its graphical development en-
vironment. Finally, it offers com-
prehensive Unicode-compliant multi-
lingual support, thus allowing stu-
dents to create components for lan-
guages other than English. Unlike
other NLP teaching tools which were
designed specifically and only for this
purpose, GATE is a system developed
for and used actively in language en-
gineering research. This unique dual-
ity allows students to contribute to re-
search projects and gain skills in em-
bedding HLT in practical applications.
1 Introduction
When students learn programming, they have
the benefit of integrated development environ-
ments, which support them throughout the en-
tire application development process: from writ-
ing the code, through testing, to documenta-
tion. In addition, these environments offer sup-
port and automation of common tasks, e.g., user
interfaces can be designed easily by assembling
them visually from components like menus and
windows. Similarly, NLP and CL students can
benefit from the existence of a graphical devel-
opment environment, which allows them to get
hands-on experience in every aspect of develop-
ing and evaluating language processing modules.
In addition, such a tool would enable students to
see clearly the practical relevance and need for
language processing, by allowing them to exper-
iment easily with building NLP-powered (Web)
applications.
This paper shows how an existing infrastruc-
ture for language engineering research ? GATE
(Cunningham et al, 2002a; Cunningham, 2002)
? has been used successfully as an NLP teach-
ing environment, in addition to being a suc-
cessful vehicle for building NLP applications
and reusable components (Maynard et al, 2002;
Maynard et al, 2001). The key features of
GATE which make it particularly suitable for
teaching are:
? The system is designed to separate cleanly
low-level tasks such as data storage, data
visualisation, location and loading of com-
ponents and execution of processes from the
data structures and algorithms that actu-
ally process human language. In this way,
the students can concentrate on studying
and/or modifying the NLP data and algo-
rithms, while leaving the mundane tasks to
GATE.
                     July 2002, pp. 54-62.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
? Automating measurement of performance
of language processing components and fa-
cilities for the creation of the annotated cor-
pora needed for that.
? Providing a baseline set of language pro-
cessing components that can be extended
and/or replaced by students as required.
These modules typically separate clearly
the linguistic data from the algorithms that
use it, thus allowing teachers to present
them separately and the students to adapt
the modules to new domains/languages by
just modifying the linguistic data.
? It comes with exhaustive documenta-
tion, tutorials, and online movie demon-
strations, available on its Web site
(http://gate.ac.uk).
GATE and its language processing modules
were developed to promote robustness and scala-
bility of NLP approaches and applications, with
an emphasis on language engineering research.
Therefore, NLP/LE courses based on GATE
offer students the opportunity to learn from
non-toy applications, running on big, realistic
datasets (e.g., British National corpus or news
collected by a Web crawler). This unique re-
search/teaching duality also allows students to
contribute to research projects and gain skills in
embedding HLT in practical applications.
2 GATE from a Teaching
Perspective
GATE (Cunningham et al, 2002a) is an archi-
tecture, a framework and a development envi-
ronment for human language technology mod-
ules and applications. It comes with a set of
reusable modules, which are able to perform ba-
sic language processing tasks such as POS tag-
ging and semantic tagging. These eliminate the
need for students to re-implement useful algo-
rithms and modules, which are pre-requisites
for completing their assignments. For exam-
ple, Marin Dimitrov from Sofia University suc-
cessfully completed his masters? degree by im-
plementing a lightweight approach to pronom-
inal coreference resolution for named entities1,
which uses GATE?s reusable modules for the
earlier processing and builds upon their results
(see Section 4).
For courses where the emphasis is more on
linguistic annotation and corpus work, GATE
can be used as a corpus annotation environment
(see http://gate.ac.uk/talks/tutorial3/). The
annotation can be done completely manually
or it can be bootstrapped by running some
of GATE?s processing resources over the cor-
pus and then correcting/adding new annota-
tions manually. These facilities can also be used
in courses and assignments where the students
need to learn how to create data for quantitative
evaluation of NLP systems.
If evaluated against the requirements for
teaching environments discussed in (Loper and
Bird, 2002), GATE covers them all quite well.
The graphical development environment and
the JAPE language facilitate otherwise difficult
tasks. Inter-module consistency is achieved by
using the annotations model to hold language
data, while extensibility and modularity are the
very reason why GATE has been successfully
used in many research projects (Maynard et al,
2000). In addition, GATE also offers robustness
and scalability, which allow students to experi-
ment with big corpora, such as the British Na-
tional Corpus (approx. 4GB). In the following
subsections we will provide further detail about
these aspects of GATE.
2.1 GATE?s Graphical Development
Environment
GATE comes with a graphical development en-
vironment (or GATE GUI) that facilitates stu-
dents in inspecting the language processing re-
sults and debugging the modules. The envi-
ronment has facilities to view documents, cor-
pora, ontologies (including the popular Prote?ge?
editor (Noy et al, 2001)), and linguistic data
(expressed as annotations, see below), e.g., Fig-
ure 1 shows the document viewer with some
annotations highlighted. It also shows the re-
source panel on the left with all loaded appli-
1The thesis is available at
http://www.ontotext.com/ie/thesis-m.pdf
Figure 1: GATE?s visual development environment
cations, language resources, and processing re-
sources (i.e., modules). There are also view-
ers/editors for complex linguistic data like coref-
erence chains (Figure 2) and syntax trees (Fig-
ure 3). New graphical components can be in-
tegrated easily, thus allowing lecturers to cus-
tomise the environment as necessary. The
GATE team is also developing new visualisation
modules, especially a visual JAPE rule develop-
ment tool.
2.2 GATE API and Data Model
The central concept that needs to be learned by
the students before they start using GATE is
the annotation data model, which encodes all
linguistic data and is used as input and out-
put for all modules. GATE uses a single uni-
fied model of annotation - a modified form of
the TIPSTER format (Grishman, 1997) which
has been made largely compatible with the Atlas
format (Bird and Liberman, 1999). Annotations
are characterised by a type and a set of features
represented as attribute-value pairs. The anno-
tations are stored in structures called annotation
sets which constitute independent layers of an-
notation over the text content. The annotations
format is independent of any particular linguis-
tic formalism, in order to enable the use of mod-
ules based on different linguistic theories. This
generality enables the representation of a wide-
variety of linguistic information, ranging from
very simple (e.g., tokeniser results) to very com-
Figure 2: The coreference chains viewer
plex (e.g., parse trees and discourse representa-
tion: examples in (Saggion et al, 2002)). In
addition, the annotation format allows the rep-
resentation of incomplete linguistic structures,
e.g., partial-parsing results. GATE?s tree view-
ing component has been written especially to
be able to display such disconnected and incom-
plete trees.
GATE is implemented in Java, which makes
it easier for students to use it, because typi-
cally they are already familiar with this lan-
guage from their programming courses. The
GATE API (Application Programming Inter-
face) is fully documented in Javadoc and also
examples are given in the comprehensive User
Guide (Cunningham et al, 2002b). However,
students generally do not need to familiarise
themselves with Java and the API at all, be-
cause the majority of the modules are based on
GATE?s JAPE language, so customisation of ex-
isting and development of new modules only re-
quires knowledge of JAPE and the annotation
model described above.
JAPE is a version of CPSL (Common Pattern
Specification Language) (Appelt, 1996) and is
used to describe patterns to match and annota-
tions to be created as a result (for further de-
tails see (Cunningham et al, 2002b)). Once fa-
miliar with GATE?s data model, students would
not find it difficult to write the JAPE pattern-
based rules, because they are effectively regular
expressions, which is a concept familiar to most
Figure 3: The syntax tree viewer, showing a par-
tial syntax tree for a sentence from a telecom
news text
CS students.
An example rule from an existing named en-
tity recognition grammar is:
Rule: Company1
Priority: 25
(
({Token.orthography == upperInitial})+
{Lookup.kind == companyDesignator}
):companyMatch
-->
:companyMatch.NamedEntity =
{kind = "company", rule = "Company1"}
The rule matches a pattern consisting of any
kind of word, which starts with an upper-cased
letter (recognised by the tokeniser), followed by
one of the entries in the gazetteer list for com-
pany designators (words which typically indi-
cate companies, such as ?Ltd.? and ?GmBH?). It
then annotates this pattern with the entity type
?NamedEntity?, and gives it a feature ?kind?
with value company and another feature ?rule?
with value ?Company1?. The rule feature is
simply used for debugging purposes, so it is clear
which particular rule has fired to create the an-
notation.
The grammars (which are sets of rules) do not
need to be compiled by the students, because
they are automatically analysed and executed by
the JAPE Transducer module, which is a finite-
Figure 4: The visual evaluation tool
state transducer over the annotations in the doc-
ument. Since the grammars are stored in files in
a plain text format, they can be edited in any
text editor such as Notepad or Vi. The rule de-
velopment process is performed by the students
using GATE?s visual environment (see Figure 1)
to execute the grammars and visualise the re-
sults. The process is actually a cycle, where the
students write one or more rules, re-initialise the
transducer in the GATE GUI by right-clicking
on it, then run it on the test data, check the re-
sults, and go back to improving the rules. The
evaluation part of this cycle is performed using
GATE?s visual evaluation tools which also pro-
duce precision, recall, and f-measure automati-
cally (see Figure 4).
The advantage of using JAPE for the student
assignments is that once learned by the students,
it enables them to experiment with a variety
of NLP tasks from tokenisation and sentence
splitter, to chunking, to template-based infor-
mation extraction. Because it does not need to
be compiled and supports incremental develop-
ment, JAPE is ideal for rapid prototyping, so
students can experiment with alternative ideas.
Students who are doing bigger projects, e.g., a
final year project, might want to develop GATE
modules which are not based on the finite-state
machinery and JAPE. Or the assignment might
require the development of more complex gram-
mars in JAPE, in which case they might have
to use Java code on the right-hand side of the
rule. Since such GATE modules typically only
access and manipulate annotations, even then
the students would need to learn only that part
of GATE?s API (i.e., no more than 5 classes).
Our experience with two MSc students ? Partha
Lal and Marin Dimitrov ? has shown that they
do not have significant problems with using that
either.
2.3 Some useful modules
The tokeniser splits text into simple tokens,
such as numbers, punctuation, symbols, and
words of different types (e.g. with an initial capi-
tal, all upper case, etc.). The tokeniser does not
generally need to be modified for different ap-
plications or text types. It currently recognises
many types of words, whitespace patterns, num-
bers, symbols and punctuation and should han-
dle any language from the Indo-European group
without modifications. Since it is available as
open source, one student assignment could be
to modify its rules to cope with other languages
or specific problems in a given language. The to-
keniser is based on finite-state technology, so the
rules are independent from the algorithm that
executes them.
The sentence splitter is a cascade of finite-
state transducers which segments the text into
sentences. This module is required for the tag-
ger. Both the splitter and tagger are domain-
and application-independent. Again, the split-
ter grammars can be modified as part of a stu-
dent project, e.g., to deal with specifically for-
matted texts.
The tagger is a modified version of the Brill
tagger, which assigns a part-of-speech tag to
each word or symbol. To modify the tagger?s
behaviour, students will have to re-train it on
relevant annotated texts.
The gazetteer consists of lists such as cities,
organisations, days of the week, etc. It not only
consists of entities, but also of names of useful
indicators, such as typical company designators
(e.g. ?Ltd.?), titles, etc. The gazetteer lists are
compiled into finite state machines, which anno-
tate the occurrence of the list items in the given
document. Students can easily extend the exist-
ing lists and add new ones by double-clicking on
the Gazetteer processing resource, which brings
up the gazetteer editor if it has been installed,
or using GATE?s Unicode editor.
The JAPE transducer is the module that
runs JAPE grammars, which could be doing
tasks like chunking, named entity recognition,
etc. By default, GATE is supplied with an NE
transducer which performs named entity recog-
nition for English and a VP Chunker which
shows how chunking can be done using JAPE.
An even simpler (in terms of grammar rules
complexity) and somewhat incomplete NP chun-
ker can be obtained by request from the first
author.
The orthomatcher is a module, whose pri-
mary objective is to perform co-reference, or en-
tity tracking, by recognising relations between
entities, based on orthographically matching
their names. It also has a secondary role in im-
proving named entity recognition by assigning
annotations to previously unclassified names,
based on relations with existing entities.
2.4 Support for languages other than
English
GATE uses Unicode (Unicode Consortium,
1996) throughout, and has been tested on a va-
riety of Slavic, Germanic, Romance, and Indic
languages. The ability to handle Unicode data,
along with the separation between data and al-
gorithms, allows students to perform easily even
small-scale experiments with porting NLP com-
ponents to new languages. The graphical devel-
opment environment supports fully the creation,
editing, and visualisation of linguistic data, doc-
uments, and corpora in Unicode-supported lan-
guages (see (Tablan et al, 2002)). In order to
make it easier for foreign students to use the
GUI, we are planning to localise its menus, er-
ror messages, and buttons which currently are
only in English.
2.5 Installation and Programming
Languages Support
Since GATE is 100% Java, it can run on any
platform that has a Java support. To make it
easier to install and maintain, GATE comes with
installation wizards for all major platforms. It
also allows the creation and use of a site-wide
GATE configuration file, so settings need only
be specified once and all copies run by the stu-
dents will have the same configuration and mod-
ules available. In addition, GATE allows stu-
dents to have their own configuration settings,
e.g., specify modules which are available only
to them. The personal settings override those
from GATE?s default and site-wide configura-
tions. Students can also easily install GATE
on their home computers using the installation
program. GATE also allows applications to be
saved and moved between computers and plat-
forms, so students can easily work both at home
and in the lab and transfer their data and ap-
plications between the two.
GATE?s graphical environment comes config-
ured by default to save its own state on exit,
so students will automatically get their applica-
tions, modules, and data restored automatically
the next time they load GATE.
Although GATE is Java-based, modules writ-
ten in other languages can also be integrated
and used. For example, Prolog modules are eas-
ily executable using the Jasper Java-Prolog link-
ing library. Other programming languages can
be used if they support Java Native Interface
(JNI).
3 Existing Uses of GATE for
Teaching
Postgraduates in locations as diverse as Bul-
garia, Copenhagen and Surrey are using the
system in order to avoid having to write sim-
ple things like sentence splitters from scratch,
and to enable visualisation and management
of data. For example, Partha Lal at Impe-
rial College is developing a summarisation sys-
tem based on GATE and ANNIE as a final-
year project for an MEng Degree in Comput-
ing (http://www.doc.ic.ac.uk/? pl98/). His site
includes the URL of his components and once
given this URL, GATE loads his software over
the network. Another student project will be
discussed in more detail in Section 4.
Our colleagues in the Universities of Ed-
inburgh, UMIST in Manchester, and Sussex
(amongst others) have reported using previous
versions of the system for teaching, and the Uni-
versity of Stuttgart produced a tutorial in Ger-
man for the same purposes. Educational users of
early versions of GATE 2 include Exeter Univer-
sity, Imperial College, Stuttgart University, the
University of Edinburgh and others. In order to
facilitate the use of GATE as a teaching tool,
we have provided a number of tutorials, online
demonstrations, and exhaustive documentation
on GATE?s Web site (http://gate.ac.uk).
4 An Example MSc Project
The goal of this work was to develop a corefer-
ence resolution module to be integrated within
the named entity recognition system provided
with GATE. This required a number of tasks to
be performed by the student: (i) corpus anal-
ysis; (ii) implementation and integration; (iii)
testing and quantitative evaluation.
The student developed a lightweight approach
to resolving pronominal coreference for named
entities, which was implemented as a GATE
module and run after the existing NE modules
provided with the framework. This enabled him
also to use an existing annotated corpus from
an Information Extraction evaluation competi-
tion and the GATE evaluation tools to establish
how his module compared with results reported
in the literature. Finally, the testing process was
made simple, thanks to GATE?s visualisation fa-
cilities, which are already capable of displaying
coreference chains in documents.
GATE not only allowed the student to achieve
verifiable results quickly, but it also did not in-
cur substantial integration overheads, because
it comes with a bootstrap tool which automates
the creation of GATE-compliant NLP modules.
The steps that need to be followed are:2
? use the bootstrap tool to create an empty
Java module, then add the implementation
to it. A JAVA development environment
like JBuilder and VisualCafe can be used
for this and the next stages, if the students
are familiar with them;
? compile the class, and any others that it
uses, into a Java Archive (JAR) file (GATE
2For further details and an example see (Cunningham
et al, 2002b).
Figure 5: BootStrap Wizard Dialogue
generates automatically a Makefile too, to
facilitate this process);
? write some XML configuration data for the
new resource;
? tell GATE the URL of the new JAR and
XML files.
5 Example Topics
Since GATE has been used for a wide range of
tasks, it can be used for the teaching of a number
of topics. Topics that can be covered in (part of)
a course, based on GATE are:
? Language Processing, Language Engineer-
ing, and Computational Linguistics: differ-
ences, methodologies, problems.
? Architectures, portability, robustness, cor-
pora, and the Web.
? Corpora, annotation, and evaluation: tools
and methodologies.
? Basic modules: tokenisation, sentence split-
ting, gazetteer lookup.
? Part-of-speech tagging.
? Information Extraction: issues, tasks, rep-
resenting linguistic data in the TIPSTER
annotation format, MUC, results achieved.
? Named Entity Recognition.
? Coreference Resolution
? Template Elements and Relations
? Scenario Templates
? Parsing and chunking
? Document summarisation
? Ontologies and discourse interpretation
? Language generation
While language generation, parsing, summari-
sation, and discourse interpretation modules are
not currently distributed with GATE, they can
be obtained by contacting the authors. Modules
for text classification and learning algorithms in
general are to be developed in the near future.
A lecturer willing to contribute any such mod-
ules to GATE will be very welcome to do so and
will be offered integration support.
6 Example Assignments
The availability of example modules for a vari-
ety of NLP tasks allows students to use them
as a basis for the development of an entire NLP
application, consisting of separate modules built
during their course. For example, let us consider
two problems: recognising chemical formulae in
texts and making an IE system that extracts
information from dialogues. Both tasks require
students to make changes in a number of existing
components and also write some new grammars.
Some example assignments for the chemical
formulae recognition follow:
? tokeniser : while it will probably work well
for the dialogues, the first assignment would
be to make modifications to its regular ex-
pression grammar to tokenise formulae like
H4ClO2 and Al-Li-Ti in a more suitable
way.
? gazetteer : create new lists containing new
useful clues and types of data, e.g., all
chemical elements and their abbreviations.
? named entity recognition: write a new
grammar to be executed by a new JAPE
transducer module for the recognition of the
chemical formulae.
Some assignments for the dialogue application
are:
? sentence splitter : modify it so that it splits
correctly dialogue texts, by taking into ac-
count the speaker information (because dia-
logues often do not have punctuation). For
example:
A: Thank you, can I have your full name?
C: Err John Smith
A: Can you also confirm your postcode and
telephone number for security?
C: Erm it?s 111 111 11 11
A: Postcode?
C: AB11 1CD
? corpus annotation and evaluation: use the
default named entity recogniser to boot-
strap the manual annotation of the test
data for the dialogue application; evaluate
the performance of the default NE gram-
mars on the dialogue texts; suggest possi-
ble improvements on the basis of the infor-
mation about missed and incorrect anno-
tations provided by the corpus benchmark
tool.
? named entity recognition: implement the
improvements proposed at the previous
step, by changing the default NE grammar
rules and/or by introducing rules specific to
your dialogue domain.
Finally, some assignments which are not con-
nected to any particular domain or application:
? chunking : implement an NP chunker using
JAPE. Look at the VP chunker grammars
for examples.
? template-based IE : experiment with ex-
tracting information from the dialogues us-
ing templates and JAPE (an example im-
plementation will be provided soon).
? (for a group of students) building NLP-
enabled Web applications: embed one of the
IE applications developed so far into a Web
application, which takes a Web page and
returns it annotated with the entities. Use
http://gate.ac.uk/annie/index.jsp as an ex-
ample.
In the near future it will be also possible to
have assignments on summarisation and genera-
tion, but these modules are still under develop-
ment. It will be possible to demonstrate parsing
and discourse interpretation, but because these
modules are implemented in Prolog and some-
what difficult to modify, assignments based on
them are not recommended. However, other
such modules, e.g., those from NLTK (Loper
and Bird, 2002), can be used for such assign-
ments.
7 Conclusions
In this paper we have outlined the GATE sys-
tem and its key features that make it an effective
tool for teaching NLP and CL. The main advan-
tage is that GATE is a framework and a graph-
ical development environment which is suitable
both for research and teaching, thus making it
easier to connect the two, e.g., allow a student to
carry out a final-year project which contributes
to novel research, carried out by their lectur-
ers. The development environment comes with
a comprehensive set of tools, which cover the
entire application development cycle. It can be
used to provide students with hands-on experi-
ence in a wide variety of tasks. Universities will-
ing to use GATE as a teaching tool will benefit
from the comprehensive documentation, several
tutorials, and online demonstrations.
References
D.E. Appelt. 1996. The Common Pattern Specifi-
cation Language. Technical report, SRI Interna-
tional, Artificial Intelligence Center.
S. Bird and M. Liberman. 1999. A Formal Frame-
work for Linguistic Annotation. Technical Re-
port MS-CIS-99-01, Department of Computer and
Information Science, University of Pennsylvania.
http://xxx.lanl.gov/abs/cs.CL/9903003.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002a. GATE: A framework and
graphical development environment for robust
NLP tools and applications. In Proceedings of the
40th Anniversary Meeting of the Association for
Computational Linguistics.
H. Cunningham, D. Maynard, K. Bontcheva,
V. Tablan, and C. Ursu. 2002b. The GATE User
Guide. http://gate.ac.uk/.
H. Cunningham. 2002. GATE, a General Archi-
tecture for Text Engineering. Computers and the
Humanities, 36:223?254.
R. Grishman. 1997. TIPSTER Architec-
ture Design Document Version 2.3. Techni-
cal report, DARPA. http://www.itl.nist.gov/-
div894/894.02/related projects/tipster/.
E. Loper and S. Bird. 2002. NLTK: The Natural
Language Toolkit. In ACL Workshop on Effective
Tools and Methodologies in Teaching NLP.
D. Maynard, H. Cunningham, K. Bontcheva,
R. Catizone, George Demetriou, Robert
Gaizauskas, Oana Hamza, Mark Hepple, Patrick
Herring, Brian Mitchell, Michael Oakes, Wim
Peters, Andrea Setzer, Mark Stevenson, Valentin
Tablan, Christian Ursu, and Yorick Wilks. 2000.
A Survey of Uses of GATE. Technical Report
CS?00?06, Department of Computer Science,
University of Sheffield.
D. Maynard, V. Tablan, C. Ursu, H. Cunningham,
and Y. Wilks. 2001. Named Entity Recognition
from Diverse Text Types. In Recent Advances
in Natural Language Processing 2001 Conference,
Tzigov Chark, Bulgaria.
D. Maynard, V. Tablan, H. Cunningham, C. Ursu,
H. Saggion, K. Bontcheva, and Y. Wilks. 2002.
Architectural elements of language engineering ro-
bustness. Journal of Natural Language Engineer-
ing ? Special Issue on Robust Methods in Analysis
of Natural Language Data. forthcoming.
N.F. Noy, M. Sintek, S. Decker, M. Crubzy, R.W.
Fergerson, and M.A. Musen. 2001. Creating Se-
mantic Web Contents with Prote?ge?-2000. IEEE
Intelligent Systems, 16(2):60?71.
H. Saggion, H. Cunningham, K. Bontcheva, D. May-
nard, C. Ursu, O. Hamza, and Y. Wilks. 2002.
Access to Multimedia Information through Mul-
tisource and Multilanguage Information Extrac-
tion. In 7th Workshop on Applications of Natural
Language to Information Systems (NLDB 2002),
Stockholm, Sweden.
V. Tablan, C. Ursu, K. Bontcheva, H. Cunningham,
D. Maynard, O. Hamza, Tony McEnery, Paul
Baker, and Mark Leisher. 2002. A unicode-based
environment for creation and use of language re-
sources. In Proceedings of 3rd Language Resources
and Evaluation Conference. forthcoming.
Unicode Consortium. 1996. The Unicode Standard,
Version 2.0. Addison-Wesley, Reading, MA.
 
	Experiments with geographic knowledge for information extraction
Dimitar Manov,
Atanas Kiryakov,
Borislav Popov
Ontotext Lab, Sirma AI Ltd
38A Christo Botev Blvd, Sofia 1000,
Bulgaria
{mitac,naso,borislav}@sirma.bg
Kalina Bontcheva,
Diana Maynard,
Hamish Cunningham
University of Sheffield
Regent Court, 211 Portobello St.,
Sheffield S1 4DP, UK
{kalina,diana,hamish}@dcs.shef.ac.uk
Abstract
Here we present work on using spatial knowl-
edge in conjunction with information extrac-
tion (IE). Considerable volume of location data
was imported in a knowledge base (KB) with
entities of general importance used for seman-
tic annotation, indexing, and retrieval of text.
The Semantic Web knowledge representation
standards are used, namely RDF(S). An exten-
sive upper-level ontology with more than two
hundred classes is designed. With respect to the
locations, the goal was to include the most im-
portant categories considering public and tasks
not specially related to geography or related ar-
eas. The locations data is derived from num-
ber of publicly available resources and com-
bined to assure best performance for domain-
independent named-entity recognition in text.
An evaluation and comparison to high perfor-
mance IE application is given.
1 Introduction
Information Extraction (IE) research has focused mainly
on the recognition of course-grained entities like Loca-
tion, Organization, Person, etc. (Sundheim, 1998). The
application of Information Extraction to new areas like
the Semantic Web and knowledge management has posed
new challenges, from which the most relevant here is the
need for finer-grained recognition of entities, such as lo-
cations.
In this paper we present some experiments with build-
ing a reusable knowledge base of locations which is used
as a component into an IE system, instead of a location
gazetteer. This work is part of the Knowledge and Infor-
mation Management (KIM) platform and still undergoing
development and refinement.
With respect to coverage, the goal was to include the
most important location categories for a wide range of ap-
plications and tasks, not specially related to geography or
related areas. The locations data is derived from a num-
ber of publicly available resources and combined to as-
sure best performance for named-entity recognition. An
evaluation and comparison to high performance IE sys-
tem using very small location gazetteers is given.
One important aspect of our work is that we choose to
create a knowledge base of locations, structured accord-
ing to an ontology and having relations between them, in-
stead of having somewhat flat structures of gazetteer lists
found in other IE systems. While a knowledge base can
be plugged into an IE system instead of a flat gazetteer, it
also has several unique advantages:
? the extra information, especially the transitive sub-
RegionOf relation can be used for disambiguation
and reasoning
? the location entities in the text can be recognised at
the right level of granularity for the target applica-
tion (i.e., as Location or as Country, City, etc).
? the ontology and knowledge base can be modified
by the user and any changes are reflected immedi-
ately in the output of the IE system.
The paper is structured as follows. Section 2 puts
our work in the context of previous research. Section 3
presents briefly the KIM platform, which contains the IE
system and the location knowledge base. Then Section
4 describes the location knowledge base in more detail.
The IE experiments are discussed in Section 5, followed
by a discussion on problems and future work. The paper
concludes by showing how such a knowledge base can be
used to bootstrap a new IE system (Section 7).
2 Related work
In the context of this paper, the two most relevant areas
of work are on large-scale gazetteers and location disam-
biguation. Here we present the Alexandria Digital Li-
brary Gazetteer because we used the ADL Feature Type
Thesaurus as a basis of our location ontology. Related
work on location disambiguation, like the one done in
the Perseus Digital Library project, is relevant because in
future work we will improve the location disambiguation
mechanism in our system.
2.1 Alexandria Digital Library Gazetteer
The Alexandria Digital Library (ADL), an NSF-funded
project at the University of California, Santa Barbara,
has included gazetteer development from its beginning
in 1994. Currently it contains approximately 4.4 mil-
lion entries. The data is taken from various sources, in-
cluding NIMA (National Imagery and Mapping Agency?s
of United States) Gazetteer, a set of countries and U.S.
counties, set of U.S. topographic map quadrangle foot-
prints, set of volcanoes, and set of earthquake epicenters.
The Geographic Names Information System (GNIS) data
from the U.S. Geological Survey has been partly added
to the collection. The results as of today include the-
saurus for feature types, Time Period data for the histori-
cal entries and spatial data with boundaries. The bound-
aries are defined as ?satisficing? rectangles. The term
?satisficing? is described in (Hill, 2000), and additional
information about the project could also be found there
as well as on the ADL gazetteer development page at
http://alexandria.sdc.ucsb.edu/?lhill/adlgaz/.
2.2 Toponym-disambiguation in Perseus Digital
Library project
A disambiguation system for historical place names for
Perseus digital library is described in (Smith and Crane,
2001). The library is concentrated on representing his-
torical data in the humanities from ancient Greece to
nineteenth-century America. The authors present a pro-
cedure for disambiguation of such place names, based on
internal and external evidence from the text. Internal ev-
idence includes the use of honorifics, generic geographic
labels, or linguistic environment. External evidence in-
cludes gazetteers, biographical information, and general
linguistic knowledge. Evaluation of the performance of
the system is given, using standard precision/recall meth-
ods for each of the five corpora: Greek, Roman, London,
California, Upper Midwest. The system is best on Greek
and worst on Upper Midwest corpus, and its overall per-
formance for place names is higher than the most of other
applications.
3 The KIM platform
The KIM Platform provides a novel Knowledge and In-
formation Management (KIM1) infrastructure and ser-
vices for automatic semantic annotation, indexing and re-
trieval of unstructured and semi-structured content. The
ontologies and knowledge bases are kept in Semantic
1KIM, see http://www.ontotext.com/kim
Figure 1: KIM Platform
repositories based on cutting edge Semantic Web technol-
ogy and standards, including RDF(S) repositories2, on-
tology middleware3 (Kiryakov et al 2002) and reason-
ing4. It provides a mature infrastructure for scalable and
customizable information extraction as well as annota-
tion and document management, based on GATE (Cun-
ningham et al, 2002). GATE, a General Architecture
for Text Engineering, is developed by the Sheffield NLP
group and has been used in many language processing
projects; in particular for Information Extraction in a va-
riety of languages (Maynard and Cunningham, 2003).
An essential idea for KIM is the semantic (or entity)
annotation, depicted on figure 1. It can be seen as a clas-
sical named-entity recognition and annotation process.
However, in contrast to most of the existing IE system,
KIM provides for each entity reference in the text (i) a
pointer (URI) to the most specific class in the ontology
and (ii) pointer to the specific instance in the knowledge
base. The latest is (to the best of our knowledge) an
unique KIM feature which allows further indexing and
retrieval of documents with respect to entities.
For the end-user, the usage of a KIM-based application
is straightforward and simple - one can highlight text in
the browser and further explore the available knowledge
for the entity, as shown in figure 3. A semantic query web
user interface allows for queries such as ?Organization-
2Sesame (http://sesame.aidministrator.nl/) is an open source
RDF(S)-based repository and querying facility.
RDF, http://www.w3.org/RDF/. Resource Description Frame-
work is an open standard for knowledge exchange over the Web,
developed by W3C (www.w3.org).
3OMM, http://www.ontotext.com/omm. Ontology Middle-
ware Module is an enterprise back-end for formal knowledge
management.
4BOR, http://www.ontotext.com/bor/, is a DAML+OIL rea-
soner, compliant with the latest OWL specifications.
Figure 2: KIM architecture.
locatedIn-Country? to be executed.
Information retrieval functionality is available, based
on Lucene5, which is adapted to measure relevance to en-
tities instead of tokens and stems. The full architecture is
shown in figure 2. It is important to note that KIM as a
software platform is domain and task independent.
3.1 The ontology
KIM Ontology (KIMO) covers the most general 250
classes of entities and 40 relations. The main classes are
Entity, EntitySource and LexicalResource. The most im-
portant class in the ontology is Entity, further specialized
into Object, Abstract and Happening. LexicalResource
class and its subclasses are used for different IE-related
information. The instances of the Alias class represent
different names of instances of Entity. hasAlias relation
is used to link Entity to its aliases (one-to-many rela-
tion). The hasMainAlias links to the main alias (the of-
ficial name). Each instance of Entity is linked to an in-
stance of EntitySource via generatedBy relation. There
are two types of EntitySource - Trusted and Recognized.
The ?trusted? entities are those pre-defined. The recog-
nized are the ones which were recognized from text as
part of the IE tasks.
The upper part of the ontology can be seen on the same
figure 3 in the left frame.
For ontology representation we choose RDF(S), mainly
because it allows easy extension to OWL6 (Lite).
Location sub-ontology
Because the Geographic features (Locations) form a
large part of the entities of general importance, we de-
5 Lucene, http://jakarta.apache.org/lucene/, high perfor-
mance full text search engine
6Ontology Web Language (OWL),
http://www.w3.org/TR/owl-semantics/
veloped a Location sub-ontology as part of the KIM on-
tology. The goal was to include the most important and
frequently used types of Locations (which are specializa-
tions of Entity), including relations between them (such
as hasCapital, subRegionOf (more specific than part-of)),
relations between Locations and other Entities (Organiza-
tion locatedIn Location) and various attributes.
The Location entity denotes an area in 3D space7,
which includes geographic entities with physical bound-
aries, such as geographical areas and landmasses, bodies
of water, geological formations and also politically
defined areas (e.g. ?U.S. Administered areas?).
The classification hierarchy (consisting of 97 classes)
is based on the ADL Feature Type Thesaurus version
070203. The differences target simplicity; a number of
distinctions and unnecessary levels of abstraction were
removed where irrelevant to general (non-geographic)
context, as we wanted the ontology to be easy to un-
derstand for an average user. Examples of sub-classes
omitted: Territorial waters, Tribal areas, Administrative
Areas (its sub-types are put directly under Location).
The Location ontology provides the following addi-
tional information:
? the exact type of a feature, for example to be able
to recognize a geographic feature as CountryCapital
instead of just Location.
? relations between geographic feature and other en-
tities (e.g. ?Diego Garcia? is a MilitaryBase, lo-
cated somewhere in the Indian Ocean and it is sub-
RegionOf USA).
? the different names of a location (?Peking? and
?Beijing? are two aliases for one location).
? the transitive subRegionOf relation allows one to
search for Entities located in a continent (e.g. ?Mor-
gan Stanley? - locatedIn - ?New York? - subRe-
gionOf - ?NY? - subRegionOf - ?USA? - subRe-
gionOf - ?North America?)
? ?trusted? vs ?recognized? sources in generatedBy
property of a Location is an extra hint in disam-
biguation tasks. The class hierarchy is shown in fig-
ure 5.
7Actually, the instances of Location are Entities with spa-
tial identity criteria (Guarino and Welty, 2000). For instance
a building can be considered as Property, Location or Cultural
Artifact, but the focus in the ontology is placed on the Location
aspect.
Figure 3: KIM usage - highlight and explore. The upper part of KIM ontology (KIMO) is shown in the left frame.
3.2 The knowledge base
Geographic information usually introduces a high level
of ambiguity between named entities, for the following
three reasons:
? there could be several Locations with the same name
(this includes sharing common alias);
? a name of a Location could match a common En-
glish word (e.g. ?Has?, ?The?);
? other named entities (Company, Person, even Date
or Numeric data) could share a common alias
with a Location (examples: ?Paris Corporation?,
?O?Brian? county, ?10? district, ?Departamento de
Nueve de Julio? with alias ?9 de Julio?).
In order to allow easy bootstrapping of applications
based on KIM and to eliminate the need for them to
write a Geo-gazetteer, the KIM knowledge base pro-
vides exhaustive coverage of entities of general impor-
tance. By limiting the Locations to only ?important?
ones, we also keep the system as generic, domain- and
task-independent as possible. The term ?importance? of
a location is hard to define, and part of the problem is that
it is dependent on the domain where the IE tasks are fo-
cused. Yet it is common sense that such locations include
continents, countries, big cities, some rivers, mountains,
etc. In addition to the above predefined locations, KIM:
? learns from the texts it analyses;
? has a comprehensive set of rules and patterns help-
ing it to recognize unknown entities;
? has a Hidden Markov Model learner, capable of cor-
recting symbolic patterns.
As a test domain, KIM uses political and economic news
articles from leading newswires8.
4 Populating the location knowledge base
As a main source of geographic knowledge we used
NIMA?s GEOnet Names Server (GNS) data. GNS
database is the official repository of foreign place-name
decisions approved by the U.S. Board on Geographic
Names (US BGN) and contains approximately 3.9 mil-
lion features with 5.37 million names. Approximately
20,000 of the database?s features are updated monthly.
The data is available for download in standard formatted
text files, which contain: unique feature index (UFI), sev-
eral names per Location (the official name, short name,
sometimes different transcriptions of the name), geo-
graphic coordinates (one point; no bounding rectangle).
Geographic coverage of the data is worldwide, exclud-
ing United States and Antarctica. For U.S. geographic
8See News Collector, http://news.ontotext.com
Figure 4: RDF representation of a Location.
data we used partially USGS/GNIS data9, which fol-
lows similar format as GNS data. For country names we
followed FIPS10, which was natural choice since GNS
data is structured that way. A list of big cities was ob-
tained from UN Statistics site, which covers city data
(http://unstats.un.org/unsd/citydata/).
We then created a mapping between our location classes
and GNS feature designators. Some of the features were
completely ignored (e.g. ?abandoned populated places?,
?drainage ditch?), other were combined into one (e.g.
?ADM2?, ?ADMD? into County).
There is some inconsistency in the way the data is entered
for different countries, mostly because of improper usage
of designators (using different designators for similar ge-
ographic features and vice versa). This made creation of
the mapping a bit harder, as we needed to include more
designators mapped to one class. The per-country files
were almost consistently entered (with some exceptions,
for example in UK, ?England?, ?Scotland?, ?Northern
Ireland? and ?Wales? are entered as AREA, which hints
the same importance as the other 40 areas in UK). We
expect that a per-country mapping instead of a global one
will lead to better performance results, yet we haven?t ex-
perimented with this as it will require manual tuning for
about 250 countries.
The different names of the geographic features are
mapped to aliases of the Location entities, with a main
alias pointing to the official name. The RDF represen-
tation of a Location is shown in figure 4. Because these
names sometimes match common English words and Per-
son names a list of stop words is created and the aliases
are filtered.
The import procedure uses the mapping described
9US Geological Survey (UGCS); Geographic Names Infor-
mation System (GNIS)
10Federal Information Processing Standards,
http://www.itl.nist.gov/fipspubs/
above but can also be restricted by list of countries and
classes to be imported. Currently imported classes are:
Continent, GlobalRegion, Country, Province, County,
CountryCapital, LocalCapital, City, Ocean, Sea, Gulf,
OilField, Monument, Bridge, Plateau, Mountain, Moun-
tainRange, Plain. These classes were selected as ?impor-
tant?, based on common sense and statistical information
derived from GNS data.
The GNS data has three main problems when it comes
to extracting only geographical entities of global impor-
tance and the relations between them:
? There is no way to tell the importance of a location
(e.g. is Chirpan a big city or a small town);
? The only part-of relations available are between a
location and its country, but not province or county;
? Some locations are not country-specific (e. g.
oceans, seas, mountains) but are listed as separate
locations with different identifiers in different per-
country lists.
We addressed the first problem by limiting the types of lo-
cations to a small subset of important ones (as explained
above). The importance of cities was determined by us-
ing a list of all big cities (with population over 100,000).
We attempted to solve the second problem by using an al-
gorithm to calculate the distance between a location and
all provinces/counties in this country, and then to create
a part-of relation with the nearest one. However, our ex-
periments showed that the accuracy of the results was not
satisfactory. This is mostly due to the fact that in GNS
data only the location footprint is given, but not the ex-
tent. Comparing the geographic coordinates of the loca-
tions with a common alias and type and then combining
the matching ones into a single entity in the knowledge
base solved the third problem.
Currently the KB contains about 50,000 Locations
grouped into 6 Continents, 27 GlobalRegions (such as
?Caribbean? or ?Eastern Europe?), 282 Countries, all
country capitals and 4,700 Cities (including all the cities
with population over 100,000). Each location has sev-
eral aliases (usually including English, French and some-
times the local transcription of the location), geographic
coordinates, the designator (DSG) and Unique Feature
Index (UFI), according to GNS. The figures for entities
of global importance in KIM KB are shown in table 1.
5 Experiments with direct use for IE
The locations KB is used for Information Extraction (IE)
as part of the KIM system, combining symbolic and
stochastic approaches, based on the ANNIE IE compo-
nents from GATE. As a baseline, using a gazetteer mod-
ule, the aliases of the entities (including all locations) are
Entities 77,561
Aliases 110,308
Locations 49,348
Cities 4,720
Companies 7,906
Public companies 5,150
Key people 5,500
Organizations 8,365
Table 1: Instances per subclass of Entity.
being looked up in the text. Further, unknown or not
precisely matching entities are recognized with pattern-
based grammars:
? using location pre/post keys to identify locations,
e.g. ?The River Thames?
? using location pre/post keys + Location, e.g. ?north
Egypt?, ?south Wales?
? context-based recognition, such as: ?in? + Token-
with-first-uppercase Number of disambiguation
problems (mostly in the case of Location names oc-
curring in the composite name of other Entities) are
also detected and resolved:
? ambiguity between Person and Organization, e.g.
?U.S. Navy? (this would normally be recognized as
a Person name from the pattern ?two initials + Fam-
ily name?, but in this case the initials match a loca-
tion alias)
? occurrence of locations in person names, e.g. ?Jack
London? (disambiguated because in the KB there is
LexicalResource ?Jack? is a first name of Person)
? occurrence of locations in Organization names, e.g.
?Scotland Yard? (disambiguated because in the KB
there is such Organization)
Finally, some of the recognized Entities (including
Locations), which are not marked as noun by the part of
speech tagger are discarded.
Some of the newly recognized Locations appear fre-
quently in the analyzed texts. Those, which could be
found in the GNS data are potential candidates to be en-
tered in the knowledge base, because there is an extra
evidence for their importance. This is a way to extend the
knowledge base and make it contain all the ?important?
Locations in the sense of frequently used in the one or
more application domain(s).
The performance of the KIM system was measured on
a news corpus using GATE?s evaluation tools. The sys-
tem was also compared to an high-precision named entity
recognition system, which uses small flat gazetteer lists.
Entity Number
Location 792
Organisation 773
Person 764
Date 603
Percent 54
Money 94
Table 2: Distribution of entities in the corpus
5.1 Evaluation Corpus
The corpus was collected from 3 online English news-
papers: the Independent, the Guardian and the Financial
Times. In total it contains 101 documents with 56,221
words. The corpus was manually annotated with entities.
Table 2 shows the number of entities of each type in the
corpus.
5.2 Corpus Benchmark Tool
The Corpus Benchmark Tool(CBT) is one of the compo-
nents in GATE which enables automatic evaluation of an
application in terms of Precision, Recall and F-measure,
against a set of ground truths. Furthermore, it also en-
ables two versions of a system to be compared against
each other (e.g. for regression testing) or two different
systems to be compared. Each system is evaluated by
comparing the annotations produced with a set of key an-
notations (produced manually) and producing a score ?
two systems can therefore be compared with each other
and indications are given as to where they differ from
each other.
5.3 MUSE
MUSE is an information extraction system developed
within GATE which aims to perform named entity recog-
nition on different types of text (Maynard et al 2002).
MUSE recognises the standard MUC entity types of Per-
son, Location, Organisation, Date, Time, Percent, and
some additional types such as Addresses and Identifiers.
The system is based on ANNIE, the default IE system
within GATE, but has been extended to deal with a vari-
ety of text sources and genres, and incorporates a mecha-
nism for automatically selecting the most appropriate set
of resources depending on the text type.
MUSE uses flat-list gazetteers which primarily contain
contextual clues that help with the identification of named
entities, e.g., company designators (such as Ltd, GmbH),
job titles, person titles (such as Mr, Mrs), common first
names, typical organisation types (e.g., Ministry, Univer-
sity). In addition, MUSE has lists enumerating concrete
types of locations which have about 27 500 entries, in-
cluding 25,000 UK ones. Further breakdown is given in
Table 3:
global regions (including continents) 71
aliases of countries 450
provinces 1215
mountains 5
water regions (oceans, lakes, etc) 15
cities world wide 1900
UK regions (such as East Sussex, Essex) 140
cities in UK 23792
UK rivers 3
Table 3: MUSE Location gazetteer entries
As can be seen from the location entries in the MUSE
gazetteers, the system is specifically tailored to recognise
UK locations with high recall and precision, whereas the
KIM locations KB is not skewed towards any particular
country.
We ran the MUSE system over our test corpus to see
how KIM matched up to it.
5.4 Results
MUSE vs KIM performance comparison is given in ta-
ble 4. When interpreting these results one also must bear
in mind that the high-performance IE system is only tag-
ging geographical entities as locations, whereas the GNS-
based system is actually disambiguating them with re-
spect to their specific type (e.g., City, Province, Country).
Investigation of the reasons behind the lower recall shows
that:
? the KB is too coarse-grained, i.e., there are no
?smaller? locations, such as small towns/counties in
UK, we do not import military bases in KB from
GNS data (?Diego Garcia?), etc.
? The application was not specifically tuned for the
corpus/news texts, e.g. we do not use the fact, that
the texts often clarify the locations when they are
first mentioned (e.g., Aberdeen, UK).
? there are not any historical Locations, such as
?Soviet Union?.
It is expected that the first two problems will be fixed
with enhancement of the KB with regard to domain
targeting of a KIM-based application. To check this
assumption we did another experiment. Because the
corpus contains a lot of UK-related information (the
articles are from three English newspapers) and MUSE
is specifically tailored to UK locations, we needed extra
UK-specific information in the KB. As we mentioned
earlier the import procedure is flexible to the extend that
allowed to add all the locations from UK GNS data. The
performance of this enhanced KB is shown in table 5.
The recall is higher than in MUSE (increased to 95% vs
93%).
The precision is 10% behind MUSE (85% vs 95%).
An obvious reason is that we have more entities in KB,
and we do not control the aliases (except for stop words
list), while all the locations in MUSE gazetteer lists
are manually entered and therefore produce very little
ambiguity.
6 Discussion
We produced a KB of locations with world wide cover-
age using GNS data. The size of about 50,000 Location is
more than most other IE systems have. It is not big (com-
pared to 4M locations in ADL Gazetteer), but provides
good coverage of Locations (91%). Because the KB was
not tuned for the test corpus specifics we could expect
similar coverage for other corpora.
Our flexible import procedure allows for domain-
targeted versions of the KB (by means of importing more
Location types) to be produced, which is expected to have
good-enough coverage on locations.
The impact of the location KB on the IE performance
is still under evaluation and improvement. We are work-
ing on improvements in two directions: i) decreasing the
amount of GNS-data entered in KB - for both locations
and their aliases; ii) changing the way in which the IE
system uses the KB to improve precision. On the latter,
we are currently experimenting with applying the regular
named entity recognition grammars first and then using
the location KB to lookup only the unclassified entities,
instead of using it as a gazetteer prior to named entity
recognition as we do now.
7 Bootstrapping IE for new languages
from the KB
We were able to make use of the KB as part of the TIDES
Surprise Language Exercise, a collaborative effort be-
tween a number of sites to develop resources and tools
for various language engineering tasks on an unknown
language. A dry run of this program took place in March
2003, whereby participants were given a week from the
time the language was announced, to collect tools and re-
sources for processing that language. The language cho-
sen was Cebuano, spoken by 24% of the population in
the Phillipines. The University of Sheffield developed a
Named Entity recognition system for Cebuano, to which
we contributed a list of locations from the Philippines.
This was particularly useful as this kind of information
was not readibly available from the Internet, and time was
of the essence. The NE system (developed within a week)
achieved scores for the recognition of locations at 73%
System Correct Partially Correct Missing Spurious Precision Recall F-Measure
MUSE 744 9 54 37 0.947 0.928 0.937
KIM 726 24 61 113 0.855 0.910 0.881
Table 4: MUSE vs KIM performance comparison
System Correct Partially Correct Missing Spurious Precision Recall F-Measure
MUSE 744 9 54 37 0.947 0.928 0.937
KIM-UK 759 28 27 167 0.810 0.950 0.874
Table 5: MUSE vs KB with all UK locations
Precision, 78% Recall and 76% F-measure. We predict
that this kind of information will be very useful for the
full Surprise Language Program in June, where partici-
pants will have more time (a month) to create resources
on another surprise language ? not only for Information
Extraction but also for tasks such as Cross-Language In-
formation Retrieval and Machine Translation.
8 Conclusion and future work
This paper presented work on the creation of a locations
knowledge base and its use for information extraction.
In order to allow easy bootstrapping of IE to different
languages and applications, we are building a knowledge
base (KB) with entities of general importance, including
geographic locations. The aim is to include the most im-
portant and frequently used types of Locations. An evalu-
ation and comparison to high performance IE application
was given.
The system is still under development and future im-
provements are envisaged, mainly related to implement-
ing better disambiguation techniques (e.g., like those de-
scribed in (Smith and Crane, 2001)) and experimenting
with new ways of using the KB from the IE application.
Acknowledgements
Work on GATE has been supported by the Engineering
and Physical Sciences Research Council (EPSRC) un-
der grants GR/K25267 and GR/M31699, and by several
smaller grants. The last author is currently supported by
the EPSRC-funded AKT project (http://www.aktors.org)
grant GR/N15764/01.
References
Atanas Kiryakov, Kiril Simov, Damyan Ognyanov. 2002.
Ontology Middleware and Reasoning In the ?Towards
the Semantic Web: Ontology-Driven Knowledge Man-
agement?, editors John Davies, Dieter Fensel, Frank
van Harmelen. John Wiley & Sons, Europe, 2002.
Beth Sundheim, editor. Proceedings of the Seventh
Message Understanding Conference (MUC-7). ARPA,
Morgan Kaufmann, 1998.
David A. Smith and Gregory Crane 2001. Disambiguat-
ing Geographic Names in a Historical Digital Library.
In Proceedings of ECDL, pages 127-136, Darmstadt,
4-9 September 2001.
Diana Maynard, Valentin Tablan, Hamish Cunningham,
Cristian Ursu, Horacio Saggion, Kalina Bontcheva,
Yorick Wilks 2002. Architectural Elements of Lan-
guage Engineering Robustness. In Journal of Natu-
ral Language Engineering ? Special Issue on Robust
Methods in Analysis of Natural Language Data, 8 (1)
pp 257-274
Diana Maynard and Hamish Cunningham. 2003. Multi-
lingual Adaptations of a Reusable Information Extrac-
tion Tool. In Proceedings of EACL 2003, Budapest,
Hungary, 2003.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva
and Valentin Tablan. 2002. GATE: A Framework and
Graphical Development Environment for Robust NLP
Tools and Applications. In Proceedings of the 40th
Anniversary Meeting of the Association for Compu-
tational Linguistics, 2002.
Linda L. Hill. 2000. Core elements of digital gazetteers:
placenames, categories, and footprints. In J. Borbinha
& T. Baker (Eds.), Research and Advanced Tech-
nology for Digital Libraries : Proceedings of the
4th European Conference, ECDL 2000 Lisbon, Por-
tugal, September 18-20, 2000 (pp. 280-290). Berlin:
Springer.
Nicola Guarino and Christopher Welty. 2000. Towards
a methodology for ontology-based model engineering.
In Proceedings of ECOOP-2000 Workshop on Model
Engineering. Cannes, France.
Apendix A. Ontology screenshots
Figure 5: Location sub-ontology.
Figure 6: Upper level of KIM ontology.
OLLIE: On-Line Learning for Information Extraction
Valentin Tablan, Kalina Bontcheva, Diana Maynard, Hamish Cunningham
University of Sheffield
Regent Court, 211 Portobello St.
Sheffield S1 4DP, UK
{V.Tablan,diana,kalina,hamish}@dcs.shef.ac.uk
Abstract
This paper reports work aimed at develop-
ing an open, distributed learning environ-
ment, OLLIE, where researchers can ex-
periment with different Machine Learning
(ML) methods for Information Extraction.
Once the required level of performance is
reached, the ML algorithms can be used to
speed up the manual annotation process.
OLLIE uses a browser client while data
storage and ML training is performed on
servers. The different ML algorithms use
a unified programming interface; the inte-
gration of new ones is straightforward.
1 Introduction
OLLIE is an on-line application for corpus annota-
tion that harnesses the power of Machine Learning
(ML) and Information Extraction (IE) in order to
make the annotator?s task easier and more efficient.
A normal OLLIE working session starts with the
user uploading a set of documents, selecting which
ML method to use from the several supplied by the
system, choosing the parameters for the learning
module and starting to annotate the texts. During
the initial phase of the manual annotation process,
the system learns in the background (i.e. on the
server) from the user?s actions and, when a certain
degree of confidence is reached, it starts making sug-
gestions by pre-annotating the documents. Initially,
some of these suggestions may be erroneous but, as
the user makes the necessary corrections, the system
will learn from its mistakes and the performance will
increase leading to a reduction in the amount of hu-
man input required.
The implementation is based on a client-server ar-
chitecture where the client is any Java-enabled web
browser and the server is responsible for storing
data, training ML models and providing access ser-
vices for the users.
The client side of OLLIE is implemented as a set
of Java Server Pages (JSPs) and a small number of
Java applets are used for tasks where the user inter-
face capabilities provided by HTML are not enough.
The server side comprises a JSP/servlet server,
a relational database server and an instance of the
GATE architecture for language engineering which
is used for driving all the language-related process-
ing. The general architecture is presented in Figure
1.
The next section describes the client side of the
OLLIE system while Section 3 details the imple-
mentation of the server with a subsection on the inte-
gration of Machine Learning. Section 4 talks about
security; Section 6 about future improvements and
Section 7 concludes the paper.
2 The OLLIE client
The OLLIE client consists of several web pages,
each of them giving the user access to a particular
service provided by the server.
One such page provides facilities for uploading
documents in the system from a URL, a local file,
or created from text pasted in a form. A variety
of formats including XML, HTML, email and plain
text are supported. When a document is created, the
Figure 1: The general architecture of OLLIE
original markup ?if present? is separated from tex-
tual content to prevent it from interfering with the
subsequent language processing.
Another page lets the user choose which machine
learning algorithm is to be used, the attributes that
characterise the instances (e.g., orthography, part-
of-speech), and parameters for the chosen learning
method (e.g., thresholds, smoothing values). The
classes to be learnt (e.g., Person, Organisation) are
provided as part of the user profile, which can be
edited on a dedicated page. All ML methods com-
patible with OLLIE have a uniform way of describ-
ing attributes and classes (see Section 3.1 for more
details on the ML integration); this makes possible
the use of a single user interface for all the ML al-
gorithms available. The fine-tuning parameters are
specific to each ML method and, although the ML
methods can be run with their default parameters, as
established by (Daelemans and Hoste, 2002), sub-
stantial variation in performance can be obtained by
changing algorithm options.
Since OLLIE needs to support the users with the
annotation process by learning in the background
and suggesting annotations, it offers control over
the accuracy threshold for these suggestions. This
avoids annoying the users with wrong suggestions
while assuring that suggestions the system is confi-
dent about are used to pre-annotate the data, reduc-
ing the workload of the user.
The document editor can then be used to annotate
the text (see Figure 2). The right-hand side shows
the classes of annotations (as specified in the user
profile) and the user selects the text to be annotated
(e.g., ?McCarthy?) and clicks on the desired class
(e.g., Person). The new annotation is added to the
document and the server is updated immediately (so
the new data becomes available to the ML algorithm
too). The document editor also provides facilities
for deleting wrong annotations, which are then prop-
agated to the server, in a similar way.
The graphical interface facilities provided by a
web browser could be used to design an interface for
annotating documents but that would mean stretch-
ing them beyond their intended use and it is hard to
believe that such an interface would rate very high
on a usability scale. In order to provide a more er-
gonomic interface, OLLIE uses a Java applet that
integrates seamlessly with the page displayed by the
browser. Apart from better usability, this allows for
greater range of options for the user.
The communication between the editor applet and
the server is established using Java Remote Method
Invocation (a protocol similar to the C++ Remote
Procedure Call ? RPC) which allows the instant no-
tification when updates are needed for the document
stored on the server. The continuous communication
between the client and the server adds the benefit of
data security in case of network failure. The data
on the server always reflects the latest version of the
document so no data loss can occur. The session
data stored by the server expires automatically after
an idle time of one hour. This releases the resources
used on the server in case of persistent network fail-
ures.
The data structures used to store documents on the
server are relatively large because of the numerous
indices stored to allow efficient access to the annota-
tions. The copy downloaded by the client when the
annotation process is initiated is greatly reduced by
filtering out all the unnecessary information. Most
of the data transferred during the client-server com-
munication is also compressed, which reduces the
level of network traffic ? always a problem in client
server architectures that run over the Internet.
Figure 2: Annotating text in the OLLIE client
Another utility provided by the client is a page
that lets the user specify the access rights to the doc-
ument/corpus, which determine whether it can be
shared for viewing or collaborative annotation (see
Section 4 for details on security).
3 Implementation of the OLLIE server
While the client side of the OLLIE application is
presented as set of web pages, the server part is
based on the open source GATE architecture.
GATE is an infrastructure for developing and de-
ploying software components that process human
language (Cunningham et al, 2002). It is written
in Java and exploits component-based software de-
velopment, object orientation and mobile code. One
quality of GATE is that it uses Unicode through-
out (Tablan et al, 2002). Its Unicode capabilities
have been tested on a variety of Slavic, Germanic,
Romance, and Indic languages (Gamba?ck and Ols-
son, 2000; Baker et al, 2002). This allows OL-
LIE to handle documents in languages other than
English. The back-end of OLLIE uses the GATE
framework to provide language processing compo-
nents, services for persistent storage of user data,
security, and application management.
When a document is loaded in the OLLIE client
and subsequently uploaded to the server, its format
is analysed and converted into a GATE document
which consists of textual content and one or more
layers of annotation. The annotation format is a
modified form of the TIPSTER format (Grishman,
1997), is largely isomorphic with the Atlas format
(Bird and Liberman, 1999) and successfully sup-
ports I/O to/from XCES and TEI (Ide et al, 2000).1
An annotation has a type, a pair of nodes pointing
to positions inside the document content, and a set
of attribute-values, encoding further linguistic infor-
mation. Attributes are strings; values can be any
Java object. An annotation layer is organised as a
Directed Acyclic Graph on which the nodes are par-
ticular locations in the document content and the
arcs are made out of annotations. All the markup
contained in the original document is automatically
extracted into a special annotation layer and can be
used for processing or for exporting the document
back to its original format.
1The American National Corpus is using GATE for a large
TEI-based project.
Linguistic data (i.e., annotated documents and
corpora) is stored in a database on the server (see
Figure 1), in order to achieve optimal performance,
concurrent data access, and persistence between
working sessions.
One of the most important tasks for the OLLIE
server is the execution and control of ML algo-
rithms. In order to be able to use ML in OLLIE,
a new processing resource was designed that adds
ML support to GATE.
3.1 Machine Learning Support
Our implementation for ML uses classification al-
gorithms for which annotations of a given type are
instances while the attributes for them are collected
from the context in which the instances occur in the
documents.
Three types of attributes are defined: nominal,
boolean and numeric. The nominal attributes can
take a value from a specified set of possible values
while the boolean and numeric ones have the usual
definitions.
When collecting training data, all the annotations
of the type specified as instances are listed, and for
each of them, the set of attribute values is deter-
mined. All attribute values for an instance refer to
characteristics of a particular instance annotation,
which may be either the current instance or one sit-
uated at a specified relative position.
Boolean attributes refer to the presence (or ab-
sence) of a particular type of annotation overlapping
at least partially with the required instance. Nominal
and numeric attributes refer to features on a partic-
ular type of annotation that (partially) overlaps the
instance in scope.
One of the boolean or nominal attributes is
marked as the class attribute, and the values which
that attribute can take are the labels for the classes
to be learnt by the algorithm. Figure 3 depicts some
types of attributes and the values they would take
in a particular example. The boxes represent an-
notations, Token annotations are used as instances,
the one in the centre being the current instance for
which attribute values are being collected.
Since linguistic information, such as part-of-
speech and gazetteer class, is often used as at-
tributes for ML, OLLIE provides support for iden-
tifying a wide range of linguistic information - part-
of-speech, sentence boundaries, gazetteer lists, and
named entity class. This information, together with
tokenisation information (kind, orthography, and to-
ken length) is obtained by using the language pro-
cessing components available with GATE, as part of
the ANNIE system (Cunningham et al, 2002). See
Section 5 for more details on the types of linguistic
features that can be used. The user chooses which of
this information is to be used as attributes.
An ML implementation has two modes of func-
tioning: training ? when the model is being built,
and application ? when the built model is used to
classify new instances. Our implementation consists
of a GATE processing resource that handles both the
training and application phases. It is responsible for
detecting all the instances in a document and col-
lecting the attribute values for them. The data thus
obtained can then be forwarded to various external
implementations of ML algorithms.
Depending on the type of the attribute that is
marked as class, different actions will be performed
when a classification occurs. For boolean attributes,
a new annotation of the type specified in the attribute
definition will be created. Nominal attributes trigger
the addition of the feature specified in the attribute
definition on an annotation of the required type sit-
uated at the position of the classified instance. If no
such annotation is present, it will be created.
Once an ML model is built it can be stored as part
of the user profile and reloaded for use at a later time.
The execution of the ML processing resource is
controlled through configuration data that selects the
type of annotation to be used as instances, defines all
the attributes and selects which ML algorithm will
be used and with what parameters.
One good source of implementations for many
well-known ML algorithms is the WEKA library
(Witten and Frank, 1999).2 It also provides a wealth
of tools for performance evaluation, testing, and at-
tribute selection, which were used during the devel-
opment process.
OLLIE uses the ML implementations provided by
WEKA which is accessed through a simple wrap-
per that translates the requests from GATE into API
calls ?understood? by WEKA. The main types of re-
quests dealt with by the wrapper are the setting of
2WEKA homepage: http://www.cs.waikato.ac.nz/ml/weka/
Figure 3: Example of attributes and their values.
configuration data, the addition of a new training in-
stance and the classification of an application-time
instance.
4 Security
Because OLLIE is deployed as a web application
? accessible by anyone with Internet access, secu-
rity is an important issue. Users store documents on
the server and the system also keeps some personal
data about the users for practical reasons.3 All users
need to be provided with a mechanism to authen-
ticate themselves to the system and they need to be
able to select who else, apart from them, will be able
to see or modify the data they store on the server.
Every user has a username and a password, used
to retrieve their profiles and determine which doc-
uments they can access. The profiles also contain
information specifying the types of annotations that
they will be creating during the annotation process.
For example, in the case of a basic named entity
3Storing email addresses for instance is useful for sending
password reminders.
recognition task, the profile will specify Person, Or-
ganisation, and Location. These tags will then be
provided in the document annotation pages.
The access rights for the documents are handled
by GATE which implements a security model simi-
lar to that used in Unix file systems. Table 1 shows
the combination of rights that are possible. They
give a good granularity for access rights, ranging
from private to world readable.
The set of known users is shared between GATE
and OLLIE and, once a user is authenticated with
the system, the login details are kept in session data
which is stored on the OLLIE server. This allows for
automatic logins to the underlying GATE platform
and transparent management of GATE sessions.
5 ML Experiments and Evaluation
To the end of evaluating the suitability of the ML
algorithms provided by WEKA for use in OLLIE
we performed several experiments for named entity
recognition on the MUC-7 corpus (SAIC, 1998). We
concentrated on the recognition of ENAMEX enti-
User User?s Group Other Users
Mode Read Write Read Write Read Write
?World Read/Group Write? + + + + + -
?Group Read/Group Write? + + + + - -
?Group Read/Owner Write? + + + - - -
?Owner Read/Owner Write? + + - - - -
Table 1: Security model ? the access rights
ties, i.e., Person, Organisation, and Location. The
MUC-7 corpus contains 1880 Organisation (46%),
1324 Location (32%), and 887 Person (22%) an-
notations in 100 documents. The task has two ele-
ments: recognition of the entity boundaries and clas-
sification of the entities in the three classes. The re-
sults are summarised below.
We first tested the ability of the learners to iden-
tify correctly the boundaries of named entities. Us-
ing 10-fold cross-validation on the MUC 7 corpus
described above, we experimented with different
machine learning algorithms and parameters (using
WEKA), and using different attributes for training.
5 different algorithms have been evaluated: Zero
R and OneR ? as baselines, Naive Bayes, IBK (an
implementation of K Nearest Neighbour) and J48
(an implementation of a C4.5 decision tree).
As expected, the baseline algorithms performed
very poorly (at around 1%). For IBK small windows
gave low results, while large windows were very in-
efficient. The best results (f-measure of around 60%)
were achieved using the J48 algorithm.
The types of linguistic data used for the attribute
collection included part of speech information, or-
thography (upper case, lower case, initial upper case
letter, mixture of upper and lower case), token kind
(word, symbol, punctuation or number), sentence
boundary, the presence of certain known names and
keywords from the gazetteer lists provided by the
ANNIE system. Tokens were used as instance an-
notations and, for each token, the window used for
collecting the attributes was of size 5 (itself plus two
other tokens in each direction).
Additional information, such as features on a
wider window of tokens, tended to improve the re-
call marginally, but decreased the precision substan-
tially, resulting in a lower F-measure, and therefore
the trade off was not worthwhile.
We also tested the algorithms on a smaller news
corpus (which contained around 68,000 instances as
opposed to 300,000 for the MUC7 corpus). Again,
the J48 algorithm scored highest, with the decision
table and the K nearest neighbour algorithms both
scoring approximately 1 percentage point lower than
the J48.
The second set of experiments was to classify the
named entities identified into the three ENAMEX
categories: Organisations, Persons and Locations.
Using 10-fold cross-validation on the MUC 7 corpus
described above, we experimented with the WEKA
machine learning algorithms and parameters, and
using attributes for training similar to those used for
boundary detection. The best results were achieved
again with the J48 algorithm, and, for this easier
task, they were situated at around 90%. The at-
tributes were chosen on the basis of their informa-
tion gain, calculated using WEKA?s attribute selec-
tion facilities.
The named entity recognition experiments were
performed mainly to evaluate the WEKA ML algo-
rithms on datasets of different sizes, ranging from
small to fairly large ones (300,000 instances). The
different ML algorithms had different memory re-
quirements and execution speed, tested on a PIII
1.5GHz PC running Windows 2000 with 1GB RAM.
From all algorithms tested, the decision table and
decision tree were the slowest (325 and 122 seconds
respectively on 68,000 instances) and required most
memory - up to 800MB on the big datasets. Naive
Bayes was very fast (only 0.25 seconds) with 1R fol-
lowing closely (0.28 seconds).
6 Further Work
OLLIE is very much work-in-progress and there are
several possible improvements we are considering.
When dealing with a particular corpus, it is rea-
sonable to assume that the documents may be quite
similar in terms of subject, genre or style. Because
of that, it is possible that the quality of the user ex-
perience can be improved by simply using a list of
positive and negative examples. This would allow
the system not to make the same mistakes by always
missing a particular example or always annotating a
false positive ? which can be very annoying for the
user.
The big difference in execution time for differ-
ent ML algorithms shows that there are practical
advantages that can be gained from having more
than one ML algorithm integrated in OLLIE, when
it comes to supporting the user with the annotation
task. Since the two experiments showed that Naive
Bayes performs only slightly worse than the best,
but slower algorithms, it may be feasible to train
both a fast Naive Bayes classifier and a slower, but
more precise one. In this way, while the slower ML
algorithm is being re-trained on the latest data, OL-
LIE can choose between using the older model of
this algorithm or the newly re-trained faster base-
line, depending on which ones gives better results,
and suggest annotations for the current document.
As with other such environments, this performance
is measured with respect to the latest annotated doc-
ument.
We hope to be able to integrate more learning
methods, e.g., TiMBL (Daelemans et al, 1998) and
we will also provide support for other people willing
to integrate theirs and make them available from our
OLLIE server or run their own server.
We plan to experiment with other NLP tasks, e.g,
relation extraction, coreference resolution and text
planning for language generation.
Finally, we are working on a Web service imple-
mentation of OLLIE for other distributed, Grid and
e-science applications.
7 Conclusion
OLLIE is an advanced collaborative annotation en-
vironment, which allows users to share and annotate
distributed corpora, supported by adaptive informa-
tion extraction that trains in the background and pro-
vides suggestions.
The option of sharing access to documents with
other users gives several users the possibility to en-
gage in collaborative annotation of documents. For
example, one user can annotate a text with organi-
sations, then another annotate it with locations. Be-
cause the documents reside on the shared server one
user can see errors or questionable markup intro-
duced by another user and initiate a discussion. Such
collaborative annotation is useful in the wider con-
text of creating and sharing language resources (Ma
et al, 2002).
A number of Machine Learning approaches for
Information Extraction have been developed re-
cently, e.g., (Collins, 2002; Bikel et al, 1999), in-
cluding some that use active learning, e.g., (Thomp-
son et al, 1999) or offer automated support, e.g,
(Ciravegna et al, 2002), in order to lower the over-
head of annotating training data. While there ex-
ist corpora used for comparative evaluation, (e.g.,
MUC or the CMU seminar corpus), there is no easy
way to test those ML algorithms on other data, eval-
uate their portability to new domains, or experiment
with different parameters of the models. While some
of the algorithms are available for experimentation,
they are implemented in different languages, require
different data formats, and run on different plat-
forms. All of this makes it hard to ensure experimen-
tal repeatability and eliminate site-specific skew ef-
fects. Also, since not all systems are freely available,
we propose an open, distributed environment where
researchers can experiment with different learning
methods on their own data.
Another advantage of OLLIE is that it defines
a simple API (Application Programming Interface)
which is used by the different ML algorithms to ac-
cess the training data (see Section 3.1). Therefore,
the integration of a new machine learning algorithm
in OLLIE amounts to providing a wrapper that im-
plements this API (a straightforward process). We
have already provided a wrapper for the ML algo-
rithms provided by the WEKA toolkit which can be
used as an example.
Although OLLIE shares features with other adap-
tive IE environments (e.g., (Ciravegna et al, 2002))
and collaborative annotation tools (e.g., (Ma et al,
2002)), it combines them in a unique fashion. In ad-
dition, OLLIE is the only adaptive IE system that al-
lows users to choose which ML approach they want
to use and to comparatively evaluate different ap-
proaches.
References
[Baker et al2002] P. Baker, A. Hardie, T. McEnery,
H. Cunningham, and R. Gaizauskas. 2002. EMILLE,
A 67-Million Word Corpus of Indic Languages: Data
Collection, Mark-up and Harmonisation. In Proceed-
ings of 3rd Language Resources and Evaluation Con-
ference (LREC?2002), pages 819?825.
[Bikel et al1999] D. Bikel, R. Schwartz, and R.M.
Weischedel. 1999. An Algorithm that Learns What?s
in a Name. Machine Learning, Special Issue on Natu-
ral Language Learning, 34(1-3), Feb.
[Bird and Liberman1999] S. Bird and M. Liberman.
1999. A Formal Framework for Linguistic Anno-
tation. Technical Report MS-CIS-99-01, Depart-
ment of Computer and Information Science, Uni-
versity of Pennsylvania. http://xxx.lanl.gov/-
abs/cs.CL/9903003.
[Ciravegna et al2002] F. Ciravegna, A. Dingli, D. Pe-
trelli, and Y. Wilks. 2002. User-System Coop-
eration in Document Annotation Based on Informa-
tion Extraction. In 13th International Conference on
Knowledge Engineering and Knowledge Management
(EKAW02), pages 122?137, Siguenza, Spain.
[Collins2002] M. Collins. 2002. Ranking algorithms for
named entity extraction: Boosting and the voted per-
ceptron. In Proceedings of the 40th Annual Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia,PA.
[Cunningham et al2002] H. Cunningham, D. Maynard,
K. Bontcheva, and V. Tablan. 2002. GATE: A Frame-
work and Graphical Development Environment for
Robust NLP Tools and Applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
[Daelemans and Hoste2002] Walter Daelemans and
Ve?ronique Hoste. 2002. Evaluation of Machine
Learning Methods for Natural Language Processing
Tasks. In LREC 2002 Third International Conference
on Language Resources and Evaluation, pages 755?
760, CNTS Language Technology Group, University
of Antwerp, UIA, Universiteitsplein 1 (bldng A),
B-2610 Antwerpen, Belgium.
[Daelemans et al1998] W. Daelemans, J. Zavrel,
K. van der Sloot, and A. van den Bosch. 1998. Timbl:
Tilburg memory based learner version 1.0. Technical
Report Technical Report 98-03, ILK.
[Gamba?ck and Olsson2000] B. Gamba?ck and F. Olsson.
2000. Experiences of Language Engineering Algo-
rithm Reuse. In Second International Conference on
Language Resources and Evaluation (LREC), pages
155?160, Athens, Greece.
[Grishman1997] R. Grishman. 1997. TIPSTER Ar-
chitecture Design Document Version 2.3. Techni-
cal report, DARPA. http://www.itl.nist.gov/-
div894/894.02/related projects/tipster/.
[Ide et al2000] N. Ide, P. Bonhomme, and L. Romary.
2000. XCES: An XML-based Standard for Linguis-
tic Corpora. In Proceedings of the Second Interna-
tional Language Resources and Evaluation Confer-
ence (LREC), pages 825?830, Athens, Greece.
[Ma et al2002] X. Ma, H. Lee, S. Bird, and K. Maeda.
2002. Models and tools for collaborative annotation.
In Proceedings of 3rd Language Resources and Evalu-
ation Conference (LREC?2002), Gran Canaria, Spain.
[SAIC1998] SAIC. 1998. Proceedings of the Sev-
enth Message Understanding Conference (MUC-
7). http://www.itl.nist.gov/iaui/894.02/-
related projects/muc/index.html.
[Tablan et al2002] V. Tablan, C. Ursu, K. Bontcheva,
H. Cunningham, D. Maynard, O. Hamza, Tony
McEnery, Paul Baker, and Mark Leisher. 2002. A
unicode-based environment for creation and use of
language resources. In Proceedings of 3rd Language
Resources and Evaluation Conference.
[Thompson et al1999] C. A. Thompson, M. E. Califf, and
R. J. Mooney. 1999. Active learning for natural lan-
guage parsing and information extraction. In Pro-
ceedings of the International Conference on Machine
Learning, pages 406?414.
[Witten and Frank1999] I. H. Witten and E. Frank. 1999.
Data Mining: Practical Machine Learning Tools and
Techniques with Java Implementations. Morgan Kauf-
mann.
  	
