Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 181?184,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Evolving new lexical association measures using genetic programming
Jan ?Snajder Bojana Dalbelo Bas?ic? Sas?a Petrovic? Ivan Sikiric?
Faculty of Electrical Engineering and Computing, University of Zagreb
Unska 3, Zagreb, Croatia
{jan.snajder, bojana.dalbelo, sasa.petrovic, ivan.sikiric}@fer.hr
Abstract
Automatic extraction of collocations from
large corpora has been the focus of many re-
search efforts. Most approaches concentrate
on improving and combining known lexical
association measures. In this paper, we de-
scribe a genetic programming approach for
evolving new association measures, which is
not limited to any specific language, corpus,
or type of collocation. Our preliminary experi-
mental results show that the evolved measures
outperform three known association measures.
1 Introduction
A collocation is an expression consisting of two or
more words that correspond to some conventional
way of saying things (Manning and Schu?tze, 1999).
Related to the term collocation is the term n-gram,
which is used to denote any sequence of n words.
There are many possible applications of colloca-
tions: automatic language generation, word sense
disambiguation, improving text categorization, in-
formation retrieval, etc. As different applications
require different types of collocations that are of-
ten not found in dictionaries, automatic extraction
of collocations from large textual corpora has been
the focus of much research in the last decade; see,
for example, (Pecina and Schlesinger, 2006; Evert
and Krenn, 2005).
Automatic extraction of collocations is usually
performed by employing lexical association mea-
sures (AMs) to indicate how strongly the words
comprising an n-gram are associated. However, the
use of lexical AMs for the purpose of collocation
extraction has reached a plateau; recent research
in this field has focused on combining the existing
AMs in the hope of improving the results (Pecina
and Schlesinger, 2006). In this paper, we propose
an approach for deriving new AMs for collocation
extraction based on genetic programming. A simi-
lar approach has been usefully applied in text min-
ing (Atkinson-Abutridy et al, 2004) as well as in
information retrieval (Gordon et al, 2006).
Genetic programming is an evolutionary compu-
tational technique designed to mimic the process of
natural evolution for the purpose of solving complex
optimization problems by stochastically searching
through the whole space of possible solutions (Koza,
1992). The search begins from an arbitrary seed
of possible solutions (the initial population), which
are then improved (evolved) through many iterations
by employing the operations of selection, crossover,
and mutation. The process is repeated until a termi-
nation criterion is met, which is generally defined by
the goodness of the best solution or the expiration of
a time limit.
2 Genetic programming of AMs
2.1 AM representation
In genetic programming, possible solutions (in our
case lexical AMs) are mathematical expressions rep-
resented by a tree structure (Koza, 1992). The leaves
of the tree can be constants, or statistical or linguistic
information about an n-gram. A constant can be any
real number in an arbitrarily chosen interval; our ex-
periments have shown that variation of this interval
does not affect the performance. One special con-
stant that we use is the number of words in the cor-
pus. The statistical information about an n-gram can
be the frequency of any part of the n-gram. For ex-
181
ample, for a trigram abc the statistical information
can be the frequency f(abc) of the whole trigram,
frequencies f(ab) and f(bc) of the digrams, and
the frequencies of individual words f(a), f(b), and
f(c). The linguistic information about an n-gram is
the part-of-speech (POS) of any one of its words.
Inner nodes in the tree are operators. The binary
operators are addition, subtraction, multiplication,
and division. We also use one unary operator, the
natural logarithm, and one ternary operator, the IF-
THEN-ELSE operator. The IF-THEN-ELSE node
has three descendant nodes: the left descendant is
the condition in the form ?i-th word of the n-gram
has a POS tag T,? and the other two descendants are
operators or constants. If the condition is true, then
the subexpression corresponding to the middle de-
scendant is evaluated, otherwise the subexpression
corresponding to the right descendant is evaluated.
The postfix expression of an AM can be obtained
by traversing its tree representation in postorder.
Figure 1 shows the representation of the Dice co-
efficient using our representation.
2.2 Genetic operators
The crossover operator combines two parent solu-
tions into a new solution. We defined the crossover
operator as follows: from each of the two parents,
one node is chosen randomly, excluding any nodes
that represent the condition of the IF-THEN-ELSE
operator. A new solution is obtained by replacing
the subtree of the chosen node of the first parent with
the subtree of the chosen node of the second parent.
This method of defining the crossover operator is the
same as the one described in (Gordon et al, 2006).
The mutation operator introduces new ?genetic
material? into a population by randomly changing
a solution. In our case, the mutation operator can
do one of two things: either remove a randomly se-
lected inner node (with probability of 25%), or insert
an inner node at a random position in the tree (with
probability of 75%). If a node is being removed
from the tree, one of its descendants (randomly cho-
sen) takes its place. An exception to this rule is the
IF-THEN-ELSE operator, which cannot be replaced
by its condition node. If a node is being inserted,
a randomly created operator node replaces an exist-
ing node that then becomes a descendant of the new
node. If the inserted node is not a unary operator,
the required number of random leaves is created.
The selection operator is used to copy the best so-
lutions into the next iteration. The goodness of the
solution is determined by the fitness function, which
assigns to each solution a number indicating how
good that particular solution actually is. We mea-
sure the goodness of an AM in terms of its F1 score,
obtained from the precision and recall computed on
a random sample consisting of 100 positive n-grams
(those considered collocations) and 100 negative n-
grams (non-collocations). These n-grams are ranked
according to the AM value assigned to them, after
which we compute the precision and recall by con-
sidering first n best-ranked n-grams as positives and
the rest as negatives, repeating this for each n be-
tween 1 and 200. The best F1 score is then taken as
the AM?s goodness.
Using the previous definition of the fitness func-
tion, preliminary experiments showed that solutions
soon become very complex in terms of number of
nodes in the tree (namely, on the order of tens
of thousands). This is a problem both in terms
of space and time efficiency; allowing unlimited
growth of the tree quickly consumes all computa-
tional resources. Also, it is questionable whether
the performance benefits from the increased size of
the solution. Thus, we modified the fitness func-
tion to also take into account the size of the tree
(that is, the less nodes a tree has, the better). Fa-
voring shorter solutions at the expense of some loss
in performance is known as parsimony, and it has
already been successfully used in genetic program-
ming (Koza, 1992). Therefore, the final formula for
the fitness function we used incorporates the parsi-
mony factor and is given by
fitness(j) = F1(j) + ?
Lmax ? L(j)
Lmax
, (1)
where F1(j) is the F1 score (ranging from 0 to 1) of
the solution j, ? is the parsimony factor, Lmax is the
maximal size (measured in number of nodes), and
L(j) is the size of solution j. By varying ? we can
control how much loss of performance we will tol-
erate in order to get smaller, more elegant solutions.
Genetic programming algorithms usually iterate
until a termination criterion is met. In our case, the
algorithm terminates when a certain number, k, of
iterations has passed without an improvement in the
182
Dice(a, b, c) = f(abc)f(a)+f(b)+f(c)
Figure 1: Dice coefficient for digrams represented by tree
results. To prevent the overfitting problem, we mea-
sure this improvement on another sample (valida-
tion sample) that also consists of 100 collocations
and 100 non-collocations.
3 Preliminary results
3.1 Experimental setting
We use the previously described genetic program-
ming approach to evolve AMs for extracting collo-
cations consisting of three words from a corpus of
7008 Croatian legislative documents. Prior to this,
words from the corpus were lemmatized and POS
tagged. Conjunctions, propositions, pronouns, in-
terjections, and particles were treated as stop-words
and tagged with a POS tag X . N-grams starting or
ending with a stopword, or containing a verb, were
filtered out. For evaluation purposes we had a hu-
man expert annotate 200 collocations and 200 non-
collocations, divided into the evaluation and valida-
tion sample. We considered an n-gram to be a collo-
cation if it is a compound noun, terminological ex-
pression, or a proper name. Note that we could have
adopted any other definition of a collocation, since
this definition is implicit in the samples provided.
In our experiments, we varied a number of ge-
netic programming parameters. The size of the ini-
tial population varied between 50 and 50 thousand
randomly generated solutions. To examine the ef-
fects of including some known AMs on the perfor-
mance, the following AMs had a 50% chance of
being included in the initial population: pointwise
mutual information (Church and Hanks, 1990), the
Dice coefficient, and the heuristic measure defined
in (Petrovic? et al, 2006):
H(a, b, c) =
?
?
?
2 log f(abc)f(a)f(c) if POS (b) = X,
log f(abc)f(a)f(b)f(c) otherwise.
For the selection operator we used the well-known
three-tournament selection. The probability of mu-
tation was chosen from the interval [0.0001, 0.3],
and the parsimony factor ? from the interval
[0, 0.05], thereby allowing a maximum of 5% loss
of F1 in favor of smaller solutions. The maximal
size of the tree in nodes was chosen from the inter-
val [20, 1000]. After the F1 score for the validation
sample began dropping, the algorithm would con-
tinue for another k iterations before stopping. The
parameter k was chosen from the interval [104, 107].
The experiments were run with 800 different random
combinations of the aforementioned parameters.
3.2 Results
Around 20% of the evolved measures (that is, the so-
lutions that remained after the algorithm terminated)
achieved F1 scores of over 80% on both the evalu-
ation and validation samples. This proportion was
13% in the case when the initial population did not
include any known AMs, and 23% in the case when
it did, thus indicating that including known AMs in
the initial population is beneficial. The overall best
solution had 205 nodes and achieved an F1 score of
88.4%. In search of more elegant AMs, we singled
out solutions that had less than 30 nodes. Among
these, a solution that consisted of 13 nodes achieved
the highest F1. This measure is given by
M13(a, b, c) =
?
?
?
?0.423f(a)f(c)f2(abc) if POS (b) = X,
1 ? f(b)f(abc) otherwise.
The association measure M13 is particularly inter-
esting because it takes into account whether the
middle word in a trigram is a stopword (denoted
by the POS tag X). This supports the claim laid
out in (Petrovic? et al, 2006) that the trigrams con-
taining stopwords (e.g., cure for cancer) should be
treated differently, in that the frequency of the stop-
word should be ignored. It is important to note that
the aforementioned measure H was not included in
the initial population from which M13 evolved. It
is also worthwhile noting that in such populations,
out of 100 best evolved measures, all but four of
them featured a condition identical to that of M13
(POS (b) = X). In other words, the majority of
the measures evolved this condition completely in-
dependently, without H being included in the initial
population.
183
1 2 3 4 5 6 7 8 9 10
0
10
20
30
40
50
60
70
80
90
100
Number of n?grams (? 105)
F 1
 
sc
o
re
 
 
Dice
PMI
H
M13
M205
Figure 2: Comparison of association measures on a cor-
pus of 7008 Croatian documents
Figure 2 shows the comparison of AMs in terms
of their F1 score obtained on the corpus of 7008
documents. The x axis shows the number of n
best ranked n-grams that are considered positives
(we show only the range of n in which all the AMs
achieve their maximum F1; all measures tend to per-
form similarly with increasing n). The maximum
F1 score is achieved if we take 5 ? 105 n-grams
ranked best by the M205 measure. From Fig. 2 we
can see that the evolved AMs M13 and M205 outper-
formed the other three considered AMs. For exam-
ple, collocations kosilica za travu (lawn mower) and
digitalna obrada podataka (digital data processing)
were ranked at the 22th and 34th percentile accord-
ing to Dice, whereas they were ranked at the 97th
and 87th percentile according to M13.
4 Conclusion
In this paper we described a genetic programming
approach for evolving new lexical association mea-
sures in order to extract collocations.
The evolved association measure will perform at
least as good as any other AM included in the initial
population. However, the evolved association mea-
sure may be a complex expression that defies inter-
pretation, in which case it may be treated as a black-
box suitable for the specific task of collocation ex-
traction. Our approach only requires an evaluation
sample, thus it is not limited to any specific type of
collocation, language or corpus.
The preliminary experiments, conducted on a cor-
pus of Croatian documents, showed that the best
evolved measures outperformed other considered as-
sociation measures. Also, most of the best evolved
association measures took into account the linguis-
tic information about an n-gram (the POS of the in-
dividual words).
As part of future work, we intend to apply our ap-
proach to corpora in other languages and compare
the results with existing collocation extraction sys-
tems. We also intend to apply our approach to collo-
cations consisting of more than three words, and to
experiment with additional linguistic features.
Acknowledgments
This work has been supported by the Government
of Republic of Croatia, and Government of Flan-
ders under the grant No. 036-1300646-1986 and
KRO/009/06.
References
John Atkinson-Abutridy, Chris Mellish, and Stuart
Aitken. 2004. Combining information extraction with
genetic algorithms for text mining. IEEE Intelligent
Systems, 19(3):22?30.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Stephan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statisti-
cal evaluation measures. Computer Speech and Lan-
guage, 19(4):450?466.
Michael Gordon, Weiguo Fan, and Praveen Pathak.
2006. Adaptive web search: Evolving a program
that finds information. IEEE Intelligent Systems,
21(5):72?77.
John R. Koza. 1992. Genetic programming: On the pro-
gramming of computers by means of natural selection.
MIT Press.
Christopher Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA, USA.
Pavel Pecina and Pavel Schlesinger. 2006. Combin-
ing association measures for collocation extraction. In
Proc. of the COLING/ACL 2006, pages 651?658.
Sas?a Petrovic?, Jan ?Snajder, Bojana Dalbelo Bas?ic?, and
Mladen Kolar. 2006. Comparison of collocation ex-
traction measures for document indexing. J. of Com-
puting and Information Technology, 14(4):321?327.
184
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201?1211,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
DERIVBASE: Inducing and Evaluating a
Derivational Morphology Resource for German
Britta Zeller? Jan ?najder? Sebastian Pad??
?Heidelberg University, Institut f?r Computerlinguistik
69120 Heidelberg, Germany
?University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
{zeller, pado}@cl.uni-heidelberg.de jan.snajder@fer.hr
Abstract
Derivational models are still an under-
researched area in computational morphol-
ogy. Even for German, a rather resource-
rich language, there is a lack of large-
coverage derivational knowledge. This pa-
per describes a rule-based framework for
inducing derivational families (i.e., clus-
ters of lemmas in derivational relation-
ships) and its application to create a high-
coverage German resource, DERIVBASE,
mapping over 280k lemmas into more than
17k non-singleton clusters. We focus on the
rule component and a qualitative and quan-
titative evaluation. Our approach achieves
up to 93% precision and 71% recall. We
attribute the high precision to the fact that
our rules are based on information from
grammar books.
1 Introduction
Morphological processing is generally recognized
as an important step for many NLP tasks. Morpho-
logical analyzers such as lemmatizers and part of
speech (POS) taggers are commonly the first NLP
tools developed for any language (Koskenniemi,
1983; Brill, 1992). They are also applied in NLP
applications where little other linguistic analysis is
performed, such as linguistic annotation of corpora
or terminology acquisition; see Daille et al (2002)
for an informative summary.
Most work on computational morphology has
focused on inflectional morphology, that is, the
handling of grammatically determined variation of
form (Bickel and Nichols, 2001), which can be
understood, overimplifying somewhat, as a normal-
ization step. Derivational morphology, which is
concerned with the formation of new words from
existing ones, has received less attention. Exam-
ples are nominalization (to understand? the un-
derstanding), verbalization (the shelf ? to shelve),
and adjectivization (the size ? sizable). Part of
the reason for the relative lack of attention lies in
the morphological properties of English, such as
the presence of many zero derivations (the fish?
to fish), the dominance of suffixation, and the rel-
ative absence of stem changes in derivation. For
these reasons, simple stemming algorithms (Porter,
1980) provide a cheap and accurate approximation
to English derivation.
Two major NLP resources deal with derivation.
WordNet lists so-called ?morphosemantic? rela-
tions (Fellbaum et al, 2009) for English, and a
number of proposals exist for extending WordNets
in other languages with derivational relations (Bil-
gin et al, 2004; Pala and Hlav?c?kov?, 2007). Cat-
Var, the ?Categorial Variation Database of English?
(Habash and Dorr, 2003), is a lexicon aimed specif-
ically at derivation. It groups English nouns, verbs,
adjectives, and adverbs into derivational equiva-
lence classes or derivational families such as
askV askerN askingN askingA
Derivational families are commonly understood as
groups of derivationally related lemmas (Daille et
al., 2002; Milin et al, 2009). The lemmas in CatVar
come from various open word classes, and multiple
words may be listed for the same POS. The above
family lists two nouns: an event noun (asking) and
an agentive noun (asker). However, CatVar does
not consider prefixation, which is why, e.g., the
adjective unasked is missing.
CatVar has found application in different areas
of English NLP. Examples are the acquisition of
paraphrases that cut across POS lines, applied, for
example, in textual entailment (Szpektor and Da-
gan, 2008; Berant et al, 2012). Then there is the
induction and extension of semantic roles resources
for predicates of various parts of speech (Meyers et
al., 2004; Green et al, 2004). Finally, CatVar has
1201
been used as a lexical resource to generate sentence
intersections (Thadani and McKeown, 2011).
In this paper, we describe the project of obtain-
ing derivational knowledge for German to enable
similar applications. Even though there are two
derivational resources for this language, IMSLEX
(Fitschen, 2004) and CELEX (Baayen et al, 1996),
both have shortcomings. The former does not ap-
pear to be publicly available, and the latter has a
limited coverage (50k lemmas) and does not ex-
plicitly represent derivational relationships within
families, which are necessary for fine-grained op-
timization of families. For this reason, we look
into building a novel derivational resource for Ger-
man. Unfortuantely, the approach used to build
CatVar cannot be adopted: it builds on a collection
of high-quality lexical-semantic resources such as
NOMLEX (Macleod et al, 1998), which are not
available for German.
Instead, we employ a rule-based framework to
define derivation rules that cover both suffixation
and prefixation and describes stem changes. Fol-
lowing the work of ?najder and Dalbelo Ba?ic?
(2010), we define the derivational processes using
derivational rules and higher-order string transfor-
mation functions. The derivational rules induce
a partition of the language?s lemmas into deriva-
tional families. Our method is applicable to many
languages if the following are available: (1) a com-
prehensive set of lemmas (optionally including gen-
der information); (2) knowledge about admissible
derivational patterns, which can be gathered, for
example, from linguistics textbooks.
The result is a freely available high-precision
high-coverage resource for German derivational
morphology that has a structure parallel to Cat-
Var, but was obtained without using manually con-
structed lexical-semantic resources. We conduct
a thorough evaluation of the induced derivational
families both regarding precision and recall.
Plan of the paper. Section 2 discusses prior
work. Section 3 defines our derivation model that
is applied to German in Section 4. Sections 5 and
6 present our evaluation setup and results. Section
7 concludes the paper and outlines future work.
2 Related Work
Computational models of morphology have a long
tradition. Koskenniemi (1983) was the first who
analyzed and generated morphological phenomena
computationally. His two-level theory has been
applied in finite state transducers (FST) for several
languages (Karttunen and Beesley, 2005).
Many recent approaches automatically induce
morphological information from corpora. They
are either based solely on corpus statistics (D?jean,
1998), measure semantic similarity between input
and output lemma (Schone and Jurafsky, 2000),
or bootstrap derivation rules starting from seed ex-
amples (Piasecki et al, 2012). Hammarstr?m and
Borin (2011) give an extensive overview of state-
of-the-art unsupervised learning of morphology.
Unsupervised approaches operate at the level of
word-forms and have complementary strengths and
weaknesses to rule-based approaches. On the up-
side, they do not require linguistic knowledge; on
the downside, they have a harder time distinguish-
ing between derivation and inflection, which may
result in lower precision, and are not guaranteed
to yield analyses that correspond to linguistic intu-
ition. An exception is the work by Gaussier (1999),
who applies an unsupervised model to construct
derivational families for French.
For German, several morphological tools exist.
Morphix is a classification-based analyzer and gen-
erator of German words on the inflectional level
(Finkler and Neumann, 1988). SMOR (Schmid
et al, 2004) employs a finite-state transducer to
analyze German words at the inflectional, deriva-
tional, and compositional level, and has been used
in other morphological analyzers, e.g., Morphisto
(Zielinski and Simon, 2008). The site canoonet1 of-
fers broad-coverage information about the German
language including derivational word formation.
3 Framework
In this section, we describe our rule-based model of
derivation, its operation to define derivational fam-
ilies, and the application of the model to German.
We note that the model is purely surface-based,
i.e., it does not model any semantic regularities be-
yond those implicit in string transformations. We
begin by outlining the characteristics of German
derivational morphology.
3.1 German Derivational Morphology
As German is a morphologically complex language,
we analyzed its derivation processes before imple-
menting our rule-based model. We relied on tradi-
tional grammar books and lexicons, e.g., Hoeppner
(1980) and Augst (1975), in order to linguistically
1http://canoo.net
1202
justify our assumptions as well as to achieve the
best possible precision and coverage.
We concentrate on German derivational pro-
cesses that involve nouns, verbs, and adjectives.2
Nouns are simple to recognize due to capitaliza-
tion: stauenV ? StauN (to jam ? jam), essenV ?
EssenN (to eat ? food). Verbs bear three typical
suffixes (-en, -eln, -ern). An example of a derived
verb is festA ? festigenV (tight ? to tighten), where
-ig is the derivational suffix. Adjectivization works
similarlty: TagN ? t?glichA (day ? daily).
This example shows that derivation can also in-
volve stem changes in the form of umlaut (e.g.,
a ? ?) and ablaut shift, e.g., siedenV ? SudN
(to boil ? infusion). Other frequent processes
in German derivation are circumfixation (HaftN
? inhaftierenV (arrest ? to arrest)) and prefixation
(hebenV ? behebenV (to raise ? to remedy)). Pre-
fixation often indicates a semantic shift, either in
terms of the general meaning (as above) or in terms
of the polarity ( klarA ? unklarA (clear ? unclear)).
Also note that affixes can be either Germanic, e.g.,
?len ? ?lung (to oil ? oiling), or Latin/Greek, e.g.,
generieren ? Generator (to generate ? generator).
As this analysis shows, derivation in German
involves transformation as well as affixation pro-
cesses, which has to be taken into account when
modeling a derivational resource.
3.2 A Rule-based Derivation Model
The purpose of a derivational model is to define
a set of transformations that correspond to valid
derivational word formation rules. Rule-based
frameworks offer convenient representations for
derivational morphology because they can take ad-
vantage of linguistic knowledge about derivation,
have interpretable representations, and can be fine-
tuned for high precision. The choice of the frame-
work is in principle arbitrary, as long as it can con-
veniently express the derivational phenomena of
a language. Typically used for this purpose are
two-level formalism rules (Karttunen and Beesley,
1992) or XFST replace rules (Beesley and Kart-
tunen, 2003).
In this paper, we adopt the modeling framework
proposed by ?najder and Dalbelo Ba?ic? (2010).
The framework corresponds closely to simple,
human-readable descriptions in traditional gram-
2We ignore adverb derivation; the German language dis-
tinguishes between adverbial adjectives and adverbs, the latter
being a rather unproductive class and thus of no interest for
derivation (Schiller et al, 1999).
mar books. The expressiveness of the formalism
is equivalent to the replacement rules commonly
used in finite state frameworks, thus the rules can
be compiled into FSTs for efficient processing.
The framework makes a clear distinction be-
tween inflectional and derivational morphology and
provides separate modeling components for these
two; we only make use of the derivation modeling
component. We use an implementation of the mod-
eling framework in Haskell. For details, see the
studies by ?najder and Dalbelo Ba?ic? (2008) and
?najder and Dalbelo Ba?ic? (2010).
The building blocks of the derivational compo-
nent are derivational rules (patterns) and transfor-
mation functions. A derivational rule describes the
derivation of a derived word from a basis word. A
derivational rule d is defined as a triple:
d = (t,P1,P2) (1)
where t is the transformation function that maps
the word?s stem (or lemma) into the derived word?s
stem (or lemma), while P1 and P2 are the sets of
inflectional paradigms of the basis word and the
derived word, respectively, which specify the mor-
phological properties of the rule?s input and output.
For German, our study assumes that inflectional
paradigms are combinations of part-of-speech and
gender information (for nouns).
A transformation function t : S ? ?(S) maps
strings to a set of strings, representing possible
transformations. At the lowest level, t is defined
in terms of atomic string replacement operations
(replacement of prefixes, suffixes, and infixes). The
framework then uses the notion of higher-order
functions ? functions that take other transforma-
tions as arguments and return new transformations
as results ? to succinctly define common deriva-
tional processes such as prefixation, suffixation,
and stem change. More complex word-formation
rules, such as those combining prefixation and suf-
fixation, can be obtained straightforwardly by func-
tional composition.
Table 1 summarizes the syntax we use for trans-
formation functions and shows two example deriva-
tional rules. Rule 1 defines an English adjectiviza-
tion rule. It uses the conditional try operator to
apply to nouns with and without the -ion suffix
(action ? active, instinct ? instinctive). Infix re-
placement is used to model stem alternation, as
shown in rule 2 for German nominalization, e.g.,
vermachtA ? Verm?chtnisN (bequethed ? bequest).
1203
Function Description
sfx (s) concatenate the suffix s
dsfx (s) delete the suffix s
aifx (s1, s2) alternate the infix s1 to s2
try(t) perform transformation t, if possible
opt(t) optionally perform transformation t
uml alternate infixes for an umlaut shift:
uml = aifx ({(a, ?), (o, ?), (u, ?)})
Examples
1 (EN) (sfx (ive) ? try(dsfx (ion)),N ,A)
?derive -ive adjectives from nouns poten-
tially ending in -ion?
2 (DE) (sfx (nis) ? try(uml),A,N )
?derive -nis nouns from adjectives with
optional umlaut creation?
Table 1: Transformation functions and exemplary
derivational rules in the framework by ?najder and
Dalbelo Ba?ic? (2010)
N and A denote the paradigms for nouns (without
gender restriction) and adjectives, respectively.
3.3 Induction of Derivational Families
Recall that our goal is to induce derivational fami-
lies, that is, classes of derivationally related words.
We define derivational families on the basis of
derivational rules as follows.
Given a lemma-paradigm pair (l, p) as input,
a single derivational rule d = (t,P1,P2) gen-
erates a set of possible derivations Ld(l, p) =
{(l1, p1), . . . , (ln, pn)}, where p ? P1 and pi ? P2
for all i. Given a set of derivational rules D, we de-
fine a binary derivation relation?D between two
lemma-paradigm pairs that holds if the second pair
can be derived from the first one as:
(l1, p1)?D (l2, p2) (2)
iff ?d ? D. (l2, p2) ? Ld(l1, p1)
Let L denote the set of lemma-paradigm pairs. The
set of derivational families defined by D on L is
given by the equivalence classes of the transitive,
symmetric, and reflexive closure of?D over L.
Note that in addition to the quality of the rules,
the properties ofL plays a central role in the quality
of the induced families. High coverage of L is im-
portant because the transitivity of?D ranges only
over lemmas in L, so low coverage of L may result
in fragmented derivational families. However, L
should also not contain erroneous lemma-paradigm
pairs. The reason is that the derivational rules only
define admissible derivations, which need not be
morphologically valid, and therefore routinely over-
generate; L plays an important role in filtering out
derivations that are not attested in the data.
4 Building the Resource
4.1 Derivational Rules
We implemented the derivational rules from Hoepp-
ner (1980) for verbs, nouns, and adjectives, cov-
ering all processes described in Section 3.1 (zero
derivation, prefixation, suffixation, circumfixation,
and stem changes). We found many derivational
patterns in German to be conceptually simple (e.g.,
verb-noun zero derivation) so that substantial cov-
erage can already be achieved with very simple
transformation functions. However, there are many
more complex patterns (e.g., suffixation combined
with optional stem changes) that in sum also af-
fect a considerable number of lemmas, which re-
quired us to either implement low-coverage rules
or generalize existing rules. In order to preserve
precision as much as possible, we restricted rule
application by using try instead of opt, and by using
gender information from the noun paradigms (for
example, some rules only apply to masculine nouns
and produce female nouns). As a result, we end
up with high-coverage rules, such as derivations
of person-denoting nouns (SchuleN ? Sch?lerN
(school ? pupil)) as well as high-accuracy rules
such as negation prefixes (PolN ? GegenpolN (pole
? antipole)).
Even though we did not focus on the explana-
tory relevance of rules, we found that the under-
lying modeling formalism, and the methodology
used to develop the model, offer substantial lin-
guistic plausibility in practice. We had to resort to
heuristics mostly for words with derivational trans-
formations that are motivated by Latin or Greek
morphology and do not occur regularly in German,
e.g., selegierenV ? SelektionN (select ? selection).
In the initial development phase, we imple-
mented 154 rules, which took about 22 person-
hours. We then revised the rules with the aim of
increasing both precision and recall. To this end,
we constructed a development set comprised of a
sample of 1,000 derivational families induced us-
ing our rules. On this set, we inspected the deriva-
tional families for false positives, identified the
problematic rules, and identified unused and redun-
dant rules. In order to identify the false negatives,
we additionally sampled a list of 1,000 lemmas and
used string distance measures (cf. Section 5.1) to re-
trieve the 10 most similar words for each lemma not
1204
Process N-N N-A N-V A-A A-V V-V
Zero derivation ? 1 5 ? ? ?
Prefixation 10 ? 5 5 2 9
+ Stem change ? ? 3 ? 1 ?
Suffixation 15 35 20 1 14 ?
+ Stem change 2 8 7 ? 3 1
Circumfixation ? ? 1 ? ? ?
+ Stem change ? ? 1 ? ? ?
Stem change ? ? 7 ? ? 2
Total 27 44 49 6 20 12
Table 2: Breakdown of derivation rules by category
of the basis and the derived word
already covered by the derivational families. The
refinement process took another 8 person-hours. It
revealed three redundant rules and seven missing
rules, leading us to a total of 158 rules.
Table 2 shows the distribution of rules with re-
spect to the derivational processes they implement
and the part of speech combinations for the ba-
sis and the derived words. All affixations occur
both with and without stem changes, mostly um-
laut shifts. Suffixation is by far the most frequently
used derivation process, and noun-verb derivation
is most diverse in terms of derivational processes.
We also estimated the reliability of derivational
rules by analyzing the accuracy of each rule on
the development set. We assigned each rule a con-
fidence rating on a three-level scale: L3 ? very
reliable (high-accuracy rules), L2 ? generally reli-
able, and L1 ? less reliable (low-accuracy rules).
We manually analyzed the correctness of rule ap-
plications for 100 derivational families of different
size (counting 2 up to 114 lemmas), and assigned
55, 79, and 24 rules to L3, L2 and L1, respectively.
4.2 Data and Preprocessing
For an accurate application of nominal derivation
rules, we need a lemma list with POS and gender
information. We POS-tag and lemmatize SDEWAC,
a large German-language web corpus from which
boilerplate paragraphs, ungrammatical sentences,
and duplicate pages were removed (Faa? et al,
2010). For POS tagging and lemmatization, we use
TreeTagger (Schmid, 1994) and determine gram-
matical gender with the morphological layer of
the MATE Tools (Bohnet, 2010). We treat proper
nouns like common nouns.
We apply three language-specific filtering steps
based on observations in Section 3.1. First, we dis-
card non-capitalized nominal lemmas. Second, we
deleted verbal lemmas not ending in verb suffixes.
Third, we removed frequently occurring erroneous
comparative forms of adjectives (usually formed
by adding -er, like neuer / newer) by checking for
the presence of lemmas without -er (neu / new).
An additional complication in German concerns
prefix verbs, because prefix is separated in tensed
instances. For example, the 3rd person male singu-
lar of aufh?ren (to stop) is er h?rt auf (he stops).
Since most prefixes double as prepositions, the cor-
rect lemmas can only be reconstructed by parsing.
We parse the corpus using the MST parser (Mc-
Donald et al, 2006) and recover prefix verbs by
searching for instances of the dependency relation
labeled PTKVZ.
Since SDEWAC, as a web corpus, still contains
errors, we only take into account lemmas that occur
three times or more in the corpus. Considering the
size of SDEWAC, we consider this as a conservative
filtering step that preserves high recall and provides
a comprehensive basis for evaluation. After prepro-
cessing and filtering, we run the induction of the
derivational families as explained in Section 3 to
obtain the DERIVBASE resource.
4.3 Statistics on DERIVBASE
The preparation of the SDEWAC corpus as ex-
plained in Section 4.2 yields 280,336 lemmas,
which we cover with our resource. We induced
a total of 239,680 derivational families from this
data, with 17,799 non-singletons and 221,881 sin-
gletons (most of them due to compound nouns).
11,039 of the families consist of two lemmas, while
the biggest contains 116 lemmas (an overgenerated
family). The biggest family with perfect precision
(i.e., it contains only morphologically related lem-
mas) contains 40 lemmas, e.g., haltenV , erhaltenV ,
Verh?ltnisN (to hold, to uphold, relation), etc. For
comparison, CatVar v2.1 contains only 82,676 lem-
mas in 13,368 non-singleton clusters and 38,604
singletons.
The following sample family has seven members
across all three POSes and includes prefixation,
suffixation, and infix umlaut shifts:
taubA (numbA), TaubheitNf (numbnessN ),
bet?ubenV (to anesthetizeV ), Bet?ubungNf
(anesthesiaN ), bet?ubtA (anesthetizedA),
bet?ubendA (anestheticA), Bet?ubenNn
(act of anesthetizingN )
1205
5 Evaluation
5.1 Baselines
We use two baselines against which we compare
the induced derivational families: (1) clusters ob-
tained with the German version of Porter?s stem-
mer (Porter, 1980)3 and (2) clusters obtained us-
ing string distance-based clustering. We have con-
sidered a number of string distance measures and
tested them on the development set (cf. Section
4.1). The measure proposed by Majumder et al
(2007) turned out to be the most effective in cap-
turing suffixal variation. For words X and Y , it is
defined as
D4(X,Y ) =
n?m+ 1
n+ 1
n?
i=m
1
2i?m (3)
where m is the position of left-most character mis-
match, and n + 1 is the length of the longer of
the two strings. To capture prefixal variation and
stem changes, we use the n-gram based measure
proposed by Adamson and Boreham (1974):
Dicen(X,Y ) = 1?
2c
x+ y (4)
where x and y are the total number of distinct n-
grams inX and Y , respectively, and c is the number
of distinct n-grams shared by both words. In our
experiments, the best performance was achieved
with n = 3.
We used hierarchical agglomerative clustering
with average linkage. To reduce the computational
complexity, we performed a preclustering step by
recursively partitioning the set of lemmas sharing
the same prefix into partitions of manageable size
(1000 lemmas). Initially, we set the number of clus-
ters to be roughly equal to the number of induced
derivational families. For the final evaluation, we
optimized the number of clusters based on F1 score
on calibration and validation sets (cf. Section 5.3).
5.2 Evaluation Methodology
The induction of derivational families could be eval-
uated globally as a clustering problem. Unfortu-
nately, cluster evaluation is a non-trivial task for
which there is no consensus on the best approach
(Amig? et al, 2009). We decided to perform our
evaluation at the level of pairs: we manually judge
for a set of pairs whether they are derivationally
related or not.
3http://snowball.tartarus.org
We obtain the gold standard for this evaluation
by sampling lemmas from the lemma list. With ran-
dom sampling, the evaluation would be unrealistic
because a vast majority of pairs would be deriva-
tionally unrelated and count as true negatives in our
analysis. Moreover, in order to reliably estimate the
overall precision of the obtained derivational fam-
ilies, we need to evaluate on pairs sampled from
these families. On the other hand, in order to assess
recall, we need to sample from pairs that are not
included in our derivational families.
To obtain reliable estimates of both precision
and recall, we decided to draw two different sam-
ples: (1) a sample of lemma pairs sampled from
the induced derivational families, on which we
estimate precision (P-sample) and (2) a sample
of lemma pairs sampled from the set of possibly
derivationally related lemma pairs, on which we
estimate recall (R-sample). In both cases, pairs
(l1, l2) are sampled in two steps: first a lemma l1
is drawn from a non-singleton family, then the sec-
ond lemma l2 is drawn from the derivational family
of l1 (P-sample) or the set of lemmas possibly re-
lated to l1 (R-sample). The set of possibly related
lemmas is a union of the derivational family of l1,
the clusters of l1 obtained with the baseline meth-
ods, and k lemmas most similar to l1 according to
the two string distance measures. We use k = 7
in our experiments. This is based on preliminary
experiments on the development set (cf. Section
4.1), which showed that k = 7 retrieves about 92%
of the related lemmas retrieved for k = 20 with
a much smaller number of true negatives. Thus,
the evaluation on the R-sample might overestimate
the recall, but only slightly so, while the P-sample
yields a reliable estimate of precision by reducing
the number of true negatives in the sample.
Both samples contain 2400 lemma pairs each.
Lemmas included in the development set (Sec-
tion 4.1) were excluded from sampling.
5.3 Gold Standard Annotation
Two German native speakers annotated the pairs
from the P-sample and R-samples. We defined five
categories into which all lemma pairs are classified
as shown in Table 3. We count R and M as positives
and N, C, L as negatives (cf. Section 3).4 Note
that this binary distinction would be sufficient to
compute recall and precision. However, the more
4Ambiguous lemmas are categorized as positive (R or M)
if there is a matching sense.
1206
Label Description Example
R l1 and l2 are morphologi-
cally and semantically re-
lated
kratzigA ? verkratztA(scratchy ? scuffed)
M l1 and l2 are morphologi-
cally but not semantically
related
bombenV ? bombigA(to bomb ? smashing)
N no morphological relation belebtA ? lobenV
(lively ? to praise)
C no derivational relation,
but the pair is composi-
tionally related
FilmendeN ? filmenV(end of film ? to film)
L not a valid lemma (mis-
lemmatization, wrong
gender, foreign words)
HaufeN ? H?ufungN(N/A ? accumulation)
Table 3: Categories for lemma pair classification
Agreement Cohen?s ?
R-sample 0.85 0.79
P-sample 0.86 0.70
Table 4: Inter-annotator agreement on validation
sample
fine-grained five-class annotation scheme provides
a more detailed picture. The separation between R
and M gives a deeper insight into the semantics of
the derivational families. Distinguishing between
C and N, in turn, allows us to identify the pairs that
are derivationally unrelated, but compositionally
related, e.g., EhemannN ? EhefrauN (husband ?
wife).
We first carried out a calibration phase in which
the annotators double-annotated 200 pairs from
each of the two samples and refined the annotation
guidelines. In a subsequent validation phase, we
computed inter-annotator agreements on the anno-
tations of another 200 pairs each from the P- and
the R-samples. Table 4 shows the proportion of
identical annotations by both annotators as well as
Cohen?s ? score (Cohen, 1968). We achieve sub-
stantial agreement for ? (Carletta, 1996). On the
P-sample, ? is a little lower because the distribu-
tion of the categories is skewed towards R, which
makes an agreement by chance more probable.
In our opinion, the IAA results were sufficiently
high to switch to single annotation for the produc-
tion phase. Here, each annotator annotated another
1000 pairs from the P-sample and R-sample so
that the final test set consists of 2000 pairs from
each sample. The P-sample contains 1663 positive
(R+M) and 337 negative (N+C+L) pairs, respec-
tively, the R-sample contains 575 positive and 1425
negative pairs. As expected, there are more positive
Precision Recall
Method P-sample R-sample
DERIVBASE (initial) 0.83 0.58
DERIVBASE-L123 0.83 0.71
DERIVBASE-L23 0.88 0.61
DERIVBASE-L3 0.93 0.35
R-sample
Stemming 0.66 0.07
String distance D4 0.36 0.20
String distance Dice3 0.23 0.23
Table 5: Precision and recall on test samples
pairs in the P-sample and more negative pairs in
the R-sample.
6 Results
6.1 Quantitative Evaluation
Table 5 presents the overall results. We eval-
uate four variants of the induced derivational
families: those obtained before rule refinement
(DERIVBASE initial), and three variants after rule
refinement: using all rules (DERIVBASE-L123),
excluding the least reliable rules (DERIVBASE-
L23), and using only highly reliable rules
(DERIVBASE-L3).
We measure the precision of our method on the
P-sample and recall on the R-sample. For the base-
lines, precision was also computed on the R-sample
(computing it on P-sample, which is obtained from
the induced derivational families, would severely
underestimate the number of false positives). We
omit the F1 score because its use for precision and
recall estimates from different samples is unclear.
DERIVBASE reaches 83% precision when us-
ing all rules and 93% precision when using only
highly reliable rules. DERIVBASE-L123 achieves
the highest recall, outperforming other methods
and variants by a large margin. Refinement of the
initial model has produced a significant improve-
ment in recall without losses in precision. The base-
lines perform worse than our method: the stemmer
we use is rather conservative, which fragments the
families and leads to a very low recall. The string
distance-based approaches achieve more balanced
precision and recall scores. Note that for these
methods, precision and recall can be traded off
against each other by varying the number of clus-
ters; we chose the number of clusters by optimizing
the F1 score on the calibration and validaton sets.
All subsequent analyses refer to DERIVBASE-
1207
Accuracy
Coverage High Low Total
High 18 ? 18
Low 53 21 74
Total 71 21 92
Table 6: Proportions of accuracy and coverage for
direct derivations (measured on P-sample)
P R P R
N-N 0.78 0.68 N-A 0.89 0.83
A-A 0.87 0.70 N-V 0.79 0.68
V-V 0.55 0.24 A-V 0.88 0.73
Table 7: Precision and recall across different part
of speech (first POS: basis; second POS: derived
word)
L123, which is the model with the highest recall.
If optimal precision is required, DERIVBASE-L3
should however be preferred.
Analysis by frequency. We cross-classified our
rules according to high/low accuracy and high/low
coverage based on the pairs in the P-sample.
We only considered directly derivationally related
(?D) pairs and defined ?high accuracy? and ?high
coverage? as all rules above the 25th percentile in
terms of accuracy and coverage, respectively. The
results are shown in Table 6: all high-coverage
rules are also highly accurate. Most rules are ac-
curate but infrequent. Only 21 rules have a low
accuracy, but all of them apply infrequently.
Analysis by parts of speech. Table 7 shows pre-
cision and recall values for different part of speech
combinations for the basis and derived words. High
precision and recall are achieved for N-A deriva-
tions. The recall is lowest for V-V derivations,
suggesting that the derivational phenomena for this
POS combination are not yet covered satisfactorily.
6.2 Error analysis
Table 8 shows the frequencies of true positives and
false positives on the P-sample and false negatives
on the R-sample for each annotated category. True
negatives are not reported, since their analysis gives
no deeper insight.
True positives. In our analysis we treated both R
and M pairs as related, but it is interesting to see
how many of the true positives are in fact semanti-
cally unrelated. Out of 1,663 pairs, 90% are seman-
tically as well as morphologically related (R), e.g.,
TPs FPs FNs
Label P-sample P-sample R-sample
R 1,492 ? 107
M 171 ? 60
N ? 216 ?
C ? 7 ?
L ? 114 ?
Total 1,663 337 167
Table 8: Predictions over annotated categories
alkoholisierenV ? antialkoholischA (to alcoholize
? nonalcoholic), BeschuldigungN ? unschuldigA
(accusation ? innocent). Most R pairs result from
high-accuracy rules, i.e., zero derivation, negation
prefixation and simple suffixation. The remaining
10% are only morphologically related (M), e.g.,
beschwingtA ? schwingenV (cheerful ? to swing),
StolzierenN ? stolzA (strut ? proud). In both pairs,
the two lemmas share a common semantic concept
? i.e., being in motion or being proud ? but nowa-
day?s meanings have grown apart from each other.
Among the M true positives, we observe prefixa-
tion derivations in 66% of the cases, often involv-
ing prefixation at both lemmas, e.g., ErdenklicheN
? bedenklichA (imaginable ? questionable).
False positives. We observe many errors in pairs
involving short lemmas, e.g., GenN ? genierenV
(gene ? to be embarrassed), where orthographic
context is unsufficient to reject the derivation.
About 64% of the 337 incorrect pairs are of class
N (unrelated lemmas). For example, the rule for
deriving nouns denoting a male person incorrectly
links MorseN ? M?rserN (Morse ? mortar). Tran-
sitively applied rules often produce incorrect pairs;
e.g., SpeicheN ? speicherbarA (spoke ? storable)
results from the rule chain SpeicheN ? SpeicherN
? speichernV ? speicherbarA (spoke? storage
? to store? storable). Chains that involve ablaut
shifts (cf. Section 3.1) can lead to surprising re-
sults, e.g., ErringungN ? rangiertA (achievement ?
shunted). Meanwhile, some pairs judged as un-
related by the annotators might conceivably be
weakly related, such as schl?rfenV and schlurfenV
(to sip ? to shuffle), both of which refer to specific
long drawn out sounds. About 20% out of these un-
related lemma pairs is due to derivations between
proper nouns (PNs) and common nouns. This hap-
pens especially for short PNs (cf. the above exam-
ple of Morse). However, since PNs also participate
in valid derivations (e.g., Chaplin ? chaplinesque),
1208
one could investigate their impact on derivations
rather than omitting them.
Errors of the category L ? 34% of the false posi-
tives ? are caused during preprocessing by the lem-
matizer. They cannot be blamed on our derivational
model, but of course form part of the output.
False negatives. Errors of this type are due to
missing derivation rules, erroneous rules that leave
some lemmas undiscovered, or the absence of lem-
mas in the corpus required for transitive closure.
About 64% of the 167 missed pairs are of category
R. About half of these pairs result from a lack of
prefixation rules ? mainly affecting verbs ? with a
wide variety of prefixes (zu-, um-, etc.), including
prepositional prefixes like herum- (around) or ?ber-
(over). We intentionally ignored these derivations,
since they frequently lead to semantically unrelated
pairs. In fact, merely five of the remaining 36%
false negative pairs (M) do not involve prefixation.
However, this analysis as well as the rather low cov-
erage for verb-involved rules (cf. Table 7) shows
that DERIVBASE might benefit from more prefix
rules. Apart from the lack of prefixation coverage
and a few other, rather infrequent rules, we did not
find any substantial deficits. Most of the remaining
errors are due to German idiosyncrasies and excep-
tional derivations, e.g., fahrenV ? FahrtN (drive ?
trip), where the regular zero derivation would result
in Fahr.
7 Conclusion and Future Work
In this paper, we present DERIVBASE, a deriva-
tional resource for German based on a rule-based
framework. A few work days were enough to build
the underlying rules with the aid of grammar text-
books. We collected derivational families for over
280,000 lemmas with high accuracy as well as solid
coverage. The resource is freely available.5
Our approach for compiling a derivational re-
source is not restricted to German. In addition
to the typologically most similar Germanic and
Romance languages, it is also applicable to agglu-
tinative languages like Finnish, or other fusional
languages like Russian. Its main requirements are
a large list of lemmas for the language (optionally
with further morphological features) and linguistic
literature on morphological patterns.
We have employed an evaluation method that
uses two separate samples to assess precision and
5http://goo.gl/7KG2U; license cc-by-sa 3.0
recall to deal with the high number of false neg-
atives. Our analyses indicate two interesting di-
rections for future work: (a) specific handling of
proper nouns, which partake in specific derivations;
and (b) the use of graph clustering instead of the
transitive closure to avoid errors resulting from
long transitive chains.
Finally, we plan to employ distributional seman-
tics methods (Turney and Pantel, 2010) to help re-
move semantically unrelated pairs as well as distin-
guish automatically between only morphologically
(M) or both morphologically and semantically (R)
related pairs. Last, but not least, this allows us to
group derivation rules according to their semantic
properties. For example, nouns with -er suffixes
often denote persons and are agentivizations of a
basis word (Bilgin et al, 2004).
Acknowledgments
The first and third authors were supported by
the EC project EXCITEMENT (FP7 ICT-287923).
The second author was supported by the Croatian
Science Foundation (project 02.03/162: ?Deriva-
tional Semantic Models for Information Retrieval?).
We thank the reviewers for their constructive com-
ments.
References
George W. Adamson and Jillian Boreham. 1974. The
use of an association measure based on character
structure to identify semantically related pairs of
words and document titles. Information Processing
and Management, 10(7/8):253?260.
Enrique Amig?, Julio Gonzalo, Javier Artiles, and Fe-
lisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Information Retrieval, 12(4):461?486.
Gerhard Augst. 1975. Lexikon zur Wortbil-
dung. Forschungsberichte des Instituts f?r Deutsche
Sprache. Narr, T?bingen.
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1996. The CELEX Lexical Database. Re-
lease 2. LDC96L14. Linguistic Data Consortium,
University of Pennsylvania, Philadelphia, PA.
Kenneth R Beesley and Lauri Karttunen. 2003. Finite
state morphology, volume 18. CSLI publications
Stanford.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
1209
Balthazar Bickel and Johanna Nichols. 2001. Inflec-
tional morphology. In Timothy Shopen, editor, Lan-
guage Typology and Syntactic Description, Volume
III: Grammatical categories and the lexicon, pages
169?240. CUP, Cambridge.
Orhan Bilgin, ?zlem ?etinog?lu, and Kemal Oflazer.
2004. Morphosemantic relations in and across
Wordnets. In Proceedings of the Global WordNet
Conference, pages 60?66, Brno, Czech Republic.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 89?97, Beijing, China.
Eric Brill. 1992. A simple rule-based part of speech
tagger. In Proceedings of the Workshop on Speech
and Natural Language, pages 112?116, Harriman,
New York.
Jean C. Carletta. 1996. Assessing agreement on clas-
sification tasks: the kappa statistic. Computational
Linguistics, 22(2):249?254.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement with provision for scaled disagreement or
partial credit. Psychological Bulletin, 70:213?220.
B?atrice Daille, C?cile Fabre, and Pascale S?billot.
2002. Applications of computational morphology.
In Paul Boucher, editor, Many Morphologies, pages
210?234. Cascadilla Press.
Herv? D?jean. 1998. Morphemes as necessary concept
for structures discovery from untagged corpora. In
Proceedings of the Joint Conferences on New Meth-
ods in Language Processing and Computational Nat-
ural Language Learning, pages 295?298, Sydney,
Australia.
Gertrud Faa?, Ulrich Heid, and Helmut Schmid. 2010.
Design and application of a gold standard for mor-
phological analysis: SMOR in validation. In Pro-
ceedings of the Seventh International Conference
on Language Resources and Evaluation, pages 803?
810.
Christiane Fellbaum, Anne Osherson, and Peter Clark.
2009. Putting semantics into WordNet?s "morphose-
mantic" links. In Proceedings of the Third Language
and Technology Conference, pages 350?358, Poz-
nan?, Poland.
Wolfgang Finkler and G?nter Neumann. 1988. Mor-
phix - a fast realization of a classification-based ap-
proach to morphology. In Proceedings of 4th Aus-
trian Conference of Artificial Intelligence, pages 11?
19, Vienna, Austria.
Arne Fitschen. 2004. Ein computerlinguistisches
Lexikon als komplexes System. Ph.D. thesis, IMS,
Universit?t Stuttgart.
?ric Gaussier. 1999. Unsupervised learning of deriva-
tional morphology from inflectional lexicons. In
ACL?99 Workshop Proceedings on Unsupervised
Learning in Natural Language Processing, pages
24?30, College Park, Maryland, USA.
Rebecca Green, Bonnie J. Dorr, and Philip Resnik.
2004. Inducing frame semantic verb classes from
wordnet and ldoce. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, pages 375?382, Barcelona, Spain.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proceedings of
the Anuual Meeting of the North American Associ-
ation for Computational Linguistics, pages 96?102,
Edmonton, Canada.
Harald Hammarstr?m and Lars Borin. 2011. Unsuper-
vised learning of morphology. Computational Lin-
guistics, 37(2):309?350.
Wolfgang Hoeppner. 1980. Derivative Wortbildung
der deutschen Gegenwartssprache und ihre algorith-
mische Analyse. Narr, T?bingen.
Lauri Karttunen and Kenneth R Beesley. 1992. Two-
level rule compiler. Xerox Corporation. Palo Alto
Research Center.
Lauri Karttunen and Kenneth R. Beesley. 2005.
Twenty-five years of finite-state morphology. In
Antti Arppe, Lauri Carlson, Krister Lind?n, Jussi Pi-
itulainen, Mickael Suominen, Martti Vainio, Hanna
Westerlund, and Anssi Yli-Jyr, editors, Inquiries
into Words, Constraints and Contexts. Festschrift for
Kimmo Koskenniemi on his 60th Birthday, pages 71?
83. CSLI Publications, Stanford, California.
Kimmo Koskenniemi. 1983. Two-level Morphology:
A General Computational Model for Word-Form
Recognition and Production. Ph.D. thesis, Univer-
sity of Helsinki.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In In Proceedings of
Euralex98, pages 187?193.
Prasenjit Majumder, Mandar Mitra, Swapan K. Parui,
Gobinda Kole, Pabitra Mitra, and Kalyankumar
Datta. 2007. YASS: Yet another suffix strip-
per. ACM Transactions on Information Systems,
25(4):18:1?18:20.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In In Proceedings
of the Conference on Computational Natural Lan-
guage Learning, pages 216?220, New York, NY.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. Annotating noun ar-
gument structure for NomBank. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, Lisbon, Portugal.
1210
Petar Milin, Victor Kuperman, Aleksandar Kostic, and
R Harald Baayen. 2009. Paradigms bit by bit: An
information theoretic approach to the processing of
paradigmatic structure in inflection and derivation.
Analogy in grammar: Form and acquisition, pages
214?252.
Karel Pala and Dana Hlav?c?kov?. 2007. Derivational
relations in Czech WordNet. In Proceedings of the
ACL Workshop on Balto-Slavonic Natural Language
Processing: Information Extraction and Enabling
Technologies, pages 75?81.
Maciej Piasecki, Radoslaw Ramocki, and Marek
Maziarz. 2012. Recognition of Polish derivational
relations based on supervised learning scheme. In
Proceedings of the Eighth International Conference
on Language Resources and Evaluation, pages 916?
922, Istanbul, Turkey.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Anne Schiller, Simone Teufel, Christine St?ckert, and
Christine Thielen. 1999. Guidelines f?r das Tag-
ging deutscher Textcorpora mit STTS. Technical
report, Institut fur maschinelle Sprachverarbeitung,
Stuttgart.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
Smor: A German computational morphology cover-
ing derivation, composition and inflection. In Pro-
ceedings of the Fourth International Conference on
Language Resources and Evaluation, Lisbon, Portu-
gal.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Patrick Schone and Daniel Jurafsky. 2000.
Knowledge-free induction of morphology us-
ing latent semantic analysis. In Proceedings of the
Conference on Natural Language Learning, pages
67?72, Lisbon, Portugal.
Jan ?najder and Bojana Dalbelo Ba?ic?. 2008. Higher-
order functional representation of Croatian inflec-
tional morphology. In Proceedings of the 6th In-
ternational Conference on Formal Approaches to
South Slavic and Balkan Languages, pages 121?130,
Dubrovnik, Croatia.
Jan ?najder and Bojana Dalbelo Ba?ic?. 2010. A
computational model of Croatian derivational mor-
phology. In Proceedings of the 7th International
Conference on Formal Approaches to South Slavic
and Balkan Languages, pages 109?118, Dubrovnik,
Croatia.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary templates. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 849?856, Manchester, UK.
Kapil Thadani and Kathleen McKeown. 2011. To-
wards strict sentence intersection: Decoding and
evaluation strategies. In Proceedings of the ACL
Workshop on Monolingual Text-To-Text Generation,
pages 43?53, Portland, Oregon.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Andrea Zielinski and Christian Simon. 2008. Mor-
phisto - an open source morphological analyzer for
German. In Proceedings of the 7th International
Workshop on Finite-State Methods and Natural Lan-
guage Processing, pages 224?231, Ispra, Italy.
1211
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731?735,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Derivational Smoothing for Syntactic Distributional Semantics
Sebastian Pado?? Jan S?najder? Britta Zeller?
?Heidelberg University, Institut fu?r Computerlinguistik
69120 Heidelberg, Germany
?University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
{pado, zeller}@cl.uni-heidelberg.de jan.snajder@fer.hr
Abstract
Syntax-based vector spaces are used widely
in lexical semantics and are more versatile
than word-based spaces (Baroni and Lenci,
2010). However, they are also sparse, with
resulting reliability and coverage problems.
We address this problem by derivational
smoothing, which uses knowledge about
derivationally related words (oldish? old)
to improve semantic similarity estimates.
We develop a set of derivational smoothing
methods and evaluate them on two lexical
semantics tasks in German. Even for mod-
els built from very large corpora, simple
derivational smoothing can improve cover-
age considerably.
1 Introduction
Distributional semantics (Turney and Pantel, 2010)
builds on the assumption that the semantic similar-
ity of words is strongly correlated to the overlap
between their linguistic contexts. This hypothesis
can be used to construct context vectors for words
directly from large text corpora in an unsupervised
manner. Such vector spaces have been applied suc-
cessfully to many problems in NLP (see Turney and
Pantel (2010) or Erk (2012) for current overviews).
Most distributional models in computational lex-
ical semantics are either (a) bag-of-words models,
where the context features are words within a sur-
face window around the target word, or (b) syn-
tactic models, where context features are typically
pairs of dependency relations and context words.
The advantage of syntactic models is that they
incorporate a richer, structured notion of context.
This makes them more versatile; the Distributional
Memory framework by Baroni and Lenci (2010) is
applicable to a wide range of tasks. It is also able ?
at least in principle ? to capture more fine-grained
types of semantic similarity such as predicate-
argument plausibility (Erk et al, 2010). At the
same time, syntactic spaces are much more prone
to sparsity problems, as their contexts are sparser.
This leads to reliability and coverage problems.
In this paper, we propose a novel strategy
for combating sparsity in syntactic vector spaces,
derivational smoothing. It follows the intuition that
derivationally related words (feed ? feeder, blocked
? blockage) are, as a rule, semantically highly simi-
lar. Consequently, knowledge about derivationally
related words can be used as a ?back off? for sparse
vectors in syntactic spaces. For example, the pair
oldish ? ancient should receive a high semantic
similarity, but in practice, the vector for oldish will
be very sparse, which makes this result uncertain.
Knowing that oldish is derivationally related to old
allows us to use the much less sparse vector for old
as a proxy for oldish.
We present a set of general methods for smooth-
ing vector similarity computations given a resource
that groups words into derivational families (equiv-
alence classes) and evaluate these methods on Ger-
man for two distributional tasks (similarity predic-
tion and synonym choice). We find that even for
syntactic models built from very large corpora, a
simple derivational resource that groups words on
morphological grounds can improve the results.
2 Related Work
Smoothing techniques ? either statistical, distribu-
tional, or knowledge-based ? are widely applied in
all areas of NLP. Many of the methods were first
applied in Language Modeling to deal with unseen
n-grams (Chen and Goodman, 1999; Dagan et al,
1999). Query expansion methods in Information
Retrieval are also prominent cases of smoothing
that addresses the lexical mismatch between query
and document (Voorhees, 1994; Gonzalo et al,
1998; Navigli and Velardi, 2003). In lexical se-
mantics, smoothing is often achieved by backing
731
off from words to semantic classes, either adopted
from a resource such as WordNet (Resnik, 1996) or
induced from data (Pantel and Lin, 2002; Wang et
al., 2005; Erk et al, 2010). Similarly, distributional
features support generalization in Named Entity
Recognition (Finkel et al, 2005).
Although distributional information is often used
for smoothing, to our knowledge there is little
work on smoothing distributional models them-
selves. We see two main precursor studies for our
work. Bergsma et al (2008) build models of se-
lectional preferences that include morphological
features such as capitalization and the presence of
digits. However, their approach is task-specific and
requires a (semi-)supervised setting. Allan and Ku-
maran (2003) make use of morphology by building
language models for stemming-based equivalence
classes. Our approach also uses morphological
processing, albeit more precise than stemming.
3 A Resource for German Derivation
Derivational morphology describes the process of
building new (derived) words from other (basis)
words. Derived words can, but do not have to, share
the part-of-speech (POS) with their basis (oldA?
oldishA vs. warmA? warmV , warmthN ). Words
can be grouped into derivational families by form-
ing the transitive closure over individual derivation
relations. The words in these families are typically
semantically similar, although the exact degree de-
pends on the type of relation and idiosyncratic fac-
tors (bookN ? bookishA, Lieber (2009)).
For German, there are several resources with
derivational information. We use version 1.3
of DERIVBASE (Zeller et al, 2013),1 a freely
available resource that groups over 280,000 verbs,
nouns, and adjectives into more than 17,000 non-
singleton derivational families. It has a precision of
84% and a recall of 71%. Its higher coverage com-
pared to CELEX (Baayen et al, 1996) and IMSLEX
(Fitschen, 2004) makes it particularly suitable for
the use in smoothing, where the resource should
include low-frequency lemmas.
The following example illustrates a family that
covers three POSes as well as a word with a pre-
dominant metaphorical reading (to kneel? to beg):
knieenV (to kneelV ), beknieenV (to
begV ), KniendeN (kneeling personN ),
kniendA (kneelingA), KnieNn (kneeN )
1Downloadable from: http://goo.gl/7KG2U
Using derivational knowledge for smoothing raises
the question of how semantically similar the lem-
mas within a family really are. Fortunately, DE-
RIVBASE provides information that can be used in
this manner. It was constructed with hand-written
derivation rules, employing string transformation
functions that map basis lemmas onto derived lem-
mas. For example, a suffixation rule using the affix
?heit? generates the derivation dunkel ? Dunkel-
heit (darkA ? darknessN ). Since derivational fam-
ilies are defined as transitive closures, each pair
of words in a family is connected by a derivation
path. Because the rules do not have a perfect pre-
cision, our confidence in pairs of words decreases
the longer the derivation path between them. To re-
flect this, we assign each pair a confidence of 1/n,
where n is the length of the shortest path between
the lemmas. For example, bekleiden (enrobeV ) is
connected to Verkleidung (disguiseN ) through three
steps via the lemmas kleiden (dressV ) and verklei-
den (disguiseV ) and is assigned the confidence 1/3.
4 Models for Derivational Smoothing
Derivational smoothing exploits the fact that deriva-
tionally related words are also semantically related,
by combining and/or comparing distributional rep-
resentations of derivationally related words. The
definition of a derivational smoothing algorithm
consists of two parts: a trigger and a scheme.
Notation. Given a word w, we use w to denote
its distributional vector and D(w) to denote the set
of vectors for the derivational family of w. We
assume that w ? D(w). For words that have no
derivations in DERIVBASE, D(w) is a singleton
set, D(w) = {w}. Let ?(w,w?) denote the confi-
dence of the pair (w,w?), as explained in Section 3.
Smoothing trigger. As discussed above, there is
no guarantee for high semantic similarity within a
derivational family. For this reason, smoothing may
also drown out information. In this paper, we report
on two triggers: smooth always always performs
smoothing; smooth if sim=0 smooths only when
the unsmoothed similarity sim(w1,w2) is zero or
unknown (due to w1 or w2 not being in the model).
Smoothing scheme. We present three smoothing
schemes, all of which apply to the level of complete
families. The first two schemes are exemplar-based
schemes, which define the smoothed similarity for
a word pair as a function of the pairwise similarities
between all words of the two derivational families.
732
The first one, maxSim, checks for particularly simi-
lar words in the families:
maxSim(w1, w2) = max
w?1?D(w1)
w?2?D(w2)
sim(w?1,w?2)
The second one, avgSim, computes the average
pairwise similarity (N is the number of pairs):
avgSim(w1, w2) =
1
N
?
w?1?D(w1)
w?2?D(w2)
sim(w?1,w?2)
The third scheme, centSim, is prototype-based. It
computes a centroid vector for each derivational
family, which can be thought of as a representation
for the concept(s) that it expresses:
centSim(w1, w2) = sim
(
c(D(w1)), c(D(w2))
)
where c(D(w)) =?w??D(w) ?(w,w?) ?w? is the
confidence-weighted centroid vector. centSim is
similar to avgSim. It is more efficient to calculate
and effectively introduces a kind or regularization,
where outliers in either family have less impact on
the overall result.
These models only represents a sample of possi-
ble derivational smoothing methods. We performed
a number of additional experiments (POS-restricted
smoothing, word-based, and pair-based smoothing
triggers), but they did not yield any improvements
over the simpler models we present here.
5 Experimental Evaluation
Syntactic Distributional Model. The syntactic
distributional model that we use represents target
words by pairs of dependency relations and context
words. More specifically, we use the W ? LW
matricization of DM.DE, the German version (Pado?
and Utt, 2012) of Distributional Memory (Baroni
and Lenci, 2010). DM.DE was created on the basis
of the 884M-token SDEWAC web corpus (Faa? et
al., 2010), lemmatized, tagged, and parsed with the
German MATE toolkit (Bohnet, 2010).
Experiments. We evaluate the impact of smooth-
ing on two standard tasks from lexical semantics.
The first task is predicting semantic similarity. We
lemmatized and POS-tagged the German GUR350
dataset (Zesch et al, 2007), a set of 350 word pairs
with human similarity judgments, created analo-
gously to the well-known Rubenstein and Good-
enough (1965) dataset for English.2 We predict
2Downloadable from: http://goo.gl/bFokI
semantic similarity as cosine similarity. We make
a prediction for a word pair if both words are repre-
sented in the semantic space and their vectors have
a non-zero similarity.
The second task is synonym choice on the Ger-
man version of the Reader?s Digest WordPower
dataset (Wallace and Wallace, 2005).2 This dataset,
which we also lemmatized and POS-tagged, con-
sists of 984 target words with four synonym can-
didates each (including phrases), one of which is
correct. Again, we compute semantic similarity as
the cosine between target and a candidate vector
and pick the highest-similarity candidate as syn-
onym. For phrases, we compute the maximum
pairwise word similarity. We make a prediction for
an item if the target as well as at least one candi-
date are represented in the semantic space and their
vectors have a non-zero similarity.
We expect differences between the two tasks
with regard to derivational smoothing, since the
words within derivational families are generally re-
lated but often not synonymous (cf. the example
in Section 3). Thus, semantic similarity judgments
should profit more easily from derivational smooth-
ing than synonym choice.
Baseline. Our baseline is a standard bag-of-
words vector space (BOW), which represents target
words by the words occurring in their context. We
use standard parameters (?10 word window, 8.000
most frequent verb, noun, and adjective lemmas).
The model was created from the same corpus as
DM.DE. We also applied derivational smoothing
to this model, but did not obtain improvements.
Evaluation. To analyze the impact of smoothing,
we evaluate the coverage of models and the quality
of their predictions separately. In both tasks, cover-
age is the percentage of items for which we make
a prediction. We measure quality of the semantic
similarity task as the Pearson correlation between
the model predictions and the human judgments
for covered items (Zesch et al, 2007). For syn-
onym choice, we follow the method established by
Mohammad et al (2007), measuring accuracy over
covered items, with partial credit for ties.
Results for Semantic Similarity. Table 1 shows
the results for the first task. The unsmoothed
DM.DE model attains a correlation of r = 0.44
and a coverage of 58.9%. Smoothing increases the
coverage substantially to 88%. Additionally, con-
servative, prototype-based smoothing (if sim = 0)
733
Smoothing
trigger
Smoothing
scheme
r Cov
%
DM.DE, unsmoothed .44 58.9
DM.DE,
smooth always
avgSim .30 88.0
maxSim .43 88.0
centSim .44 88.0
DM.DE,
smooth if sim = 0
avgSim .43 88.0
maxSim .42 88.0
centSim .47 88.0
BOW baseline .36 94.9
Table 1: Results on the semantic similarity task
(r: Pearson correlation, Cov: Coverage)
increases correlation somewhat to r = 0.47. The
difference to the unsmoothed model is not signif-
icant at p = 0.05 according to Fisher?s (1925)
method of comparing correlation coefficients.
The bag-of-words baseline (BOW) has a greater
coverage than DM.DE models, but at the cost
of lower correlation across the board. The only
DM.DE model that performs worse than the BOW
baseline is the non-conservative avgSim (average
similarity) scheme. We attribute this weak per-
formance to the presence of many pairwise zero
similarities in the data, which makes the avgSim
predictions unreliable.
To our knowledge, there are no previous pub-
lished papers on distributional approaches to mod-
eling this dataset. The best previous result is a
GermaNet/Wikipedia-based model by Zesch et al
(2007). It reports a higher correlation (r = 0.59)
but a very low coverage at 33.1%.
Results for Synonym Choice. The results for
the second task are shown in Table 2. The un-
smoothed model achieves an accuracy of 53.7%
and a coverage of 80.8%, as reported by Pado?
and Utt (2012). Smoothing increases the cover-
age by almost 6% to 86.6% (for example, a ques-
tion item for inferior becomes covered after back-
ing off from the target to Inferiorita?t (inferiority)).
All smoothed models show a loss in accuracy, al-
beit small. The best model is again a conservative
smoothing model (sim = 0) with a loss of 1.1% ac-
curacy. Using bootstrap resampling (Efron and Tib-
shirani, 1993), we established that the difference
to the unsmoothed DM.DE model is not signifi-
cant at p < 0.05. This time, the avgSim (average
similarity) smoothing scheme performs best, with
the prototype-based scheme in second place. Thus,
the results for synonym choice are less clear-cut:
derivational smoothing can trade accuracy against
Smoothing
trigger
Smoothing
scheme
Acc
%
Cov
%
DM.DE, unsmoothed (Pado? & Utt 2012) 53.7 80.8
DM.DE,
smooth always
avgSim 46.0 86.6
maxSim 50.3 86.6
centSim 49.1 86.6
DM.DE,
smooth if sim = 0
avgSim 52.6 86.6
maxSim 51.2 86.6
centSim 51.3 86.6
BOW ?baseline? 56.9 98.5
Table 2: Results on the synonym choice task
(Acc: Accuracy, Cov: Coverage)
coverage but does not lead to a clear improvement.
What is more, the BOW ?baseline? significantly
outperforms all syntactic models, smoothed and
unsmoothed, with an almost perfect coverage com-
bined with a higher accuracy.
6 Conclusions and Outlook
In this paper, we have introduced derivational
smoothing, a novel strategy to combating sparsity
in syntactic vector spaces by comparing and com-
bining the vectors of morphologically related lem-
mas. The only information strictly necessary for
the methods we propose is a grouping of lemmas
into derivationally related classes. We have demon-
strated that derivational smoothing improves two
tasks, increasing coverage substantially and also
leading to a numerically higher correlation in the
semantic similarity task, even for vectors created
from a very large corpus. We obtained the best re-
sults for a conservative approach, smoothing only
zero similarities. This also explains our failure
to improve less sparse word-based models, where
very few pairs are assigned a similarity of zero.
A comparison of prototype- and exemplar-based
schemes did not yield a clear winner. The estima-
tion of generic semantic similarity can profit more
from derivational smoothing than the induction of
specific lexical relations.
In future work, we plan to work on other eval-
uation tasks, application to other languages, and
more sophisticated smoothing schemes.
Acknowledgments. Authors 1 and 3 were sup-
ported by the EC project EXCITEMENT (FP7 ICT-
287923). Author 2 was supported by the Croatian
Science Foundation (project 02.03/162: ?Deriva-
tional Semantic Models for Information Retrieval?).
We thank Jason Utt for his support and expertise.
734
References
James Allan and Giridhar Kumaran. 2003. Stemming
in the Language Modeling Framework. In Proceed-
ings of SIGIR, pages 455?456.
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1996. The CELEX Lexical Database. Re-
lease 2. LDC96L14. Linguistic Data Consortium,
University of Pennsylvania, Philadelphia, Pennsyl-
vania.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A General Framework for
Corpus-based Semantics. Computational Linguis-
tics, 36(4):673?721.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative Learning of Selectional Preference
from Unlabeled Text. In Proceedings of EMNLP,
pages 59?68, Honolulu, Hawaii.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 89?97, Beijing, China.
Stanley F. Chen and Joshua Goodman. 1999. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Computer Speech and Language,
13(4):359?394.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-Based Models of Word Cooccur-
rence Probabilities. Machine Learning, 34(1?3):43?
69.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and Hall,
New York.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010.
A Flexible, Corpus-driven Model of Regular and In-
verse Selectional Preferences. Computational Lin-
guistics, 36(4):723?763.
Katrin Erk. 2012. Vector Space Models of Word Mean-
ing and Phrase Meaning: A Survey. Language and
Linguistics Compass, 6(10):635?653.
Gertrud Faa?, Ulrich Heid, and Helmut Schmid. 2010.
Design and Application of a Gold Standard for Mor-
phological Analysis: SMOR in Validation. In Pro-
ceedings of LREC-2010, pages 803?810.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 363?370.
Ronald Aylmer Fisher. 1925. Statistical methods for
research workers. Oliver and Boyd, Edinburgh.
Arne Fitschen. 2004. Ein computerlinguistisches
Lexikon als komplexes System. Ph.D. thesis, IMS,
Universita?t Stuttgart.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and
Juan M. Cigarra?n. 1998. Indexing with WordNet
Synsets Can Improve Text Retrieval. In Proceed-
ings of the COLING/ACL Workshop on Usage of
WordNet in Natural Language Processing Systems,
Montre?al, Canada.
Rochelle Lieber. 2009. Morphology and Lexical Se-
mantics. Cambridge University Press.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-Lingual Distributional
Profiles of Concepts for Measuring Semantic Dis-
tance. In Proceedings of the 2007 Joint Conference
on EMNLP and CoNLL, pages 571?580, Prague,
Czech Republic.
Roberto Navigli and Paola Velardi. 2003. An Analysis
of Ontology-based Query Expansion Strategies. In
Workshop on Adaptive Text Extraction and Mining,
Dubrovnik, Croatia.
Sebastian Pado? and Jason Utt. 2012. A Distributional
Memory for German. In Proceedings of KONVENS
2012 workshop on lexical-semantic resources and
applications, pages 462?470, Vienna, Austria.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text. In In Proceedings of ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 613?619.
Philip Resnik. 1996. Selectional Constraints: An
Information-theoretic Model and its Computational
Realization. Cognition, 61(1-2):127?159.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communica-
tions of the ACM, 8(10):627?633.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Ellen M. Voorhees. 1994. Query Expansion Using
Lexical-semantic Relations. In Proceedings of SI-
GIR, pages 61?69.
DeWitt Wallace and Lila Acheson Wallace. 2005.
Reader?s Digest, das Beste fu?r Deutschland. Verlag
Das Beste, Stuttgart.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2005. Strictly Lexical Dependency Parsing. In Pro-
ceedings of IWPT, pages 152?159.
Britta Zeller, Jan S?najder, and Sebastian Pado?. 2013.
DErivBase: Inducing and Evaluating a Derivational
Morphology Resource for German. In Proceedings
of ACL, Sofia, Bulgaria.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007. Comparing Wikipedia and German Word-
net by Evaluating Semantic Relatedness on Multi-
ple Datasets. In Proceedings of NAACL/HLT, pages
205?208, Rochester, NY.
735
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 784?789,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Building and Evaluating a Distributional Memory for Croatian
Jan S?najder? Sebastian Pado?? Z?eljko Agic??
?University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
?Heidelberg University, Institut fu?r Computerlinguistik
69120 Heidelberg, Germany
?University of Zagreb, Faculty of Humanities and Social Sciences
Ivana Luc?ic?a 3, 10000 Zagreb, Croatia
jan.snajder@fer.hr pado@cl.uni-heidelberg.de zagic@ffzg.hr
Abstract
We report on the first structured dis-
tributional semantic model for Croatian,
DM.HR. It is constructed after the model
of the English Distributional Memory (Ba-
roni and Lenci, 2010), from a dependency-
parsed Croatian web corpus, and covers
about 2M lemmas. We give details on the
linguistic processing and the design prin-
ciples. An evaluation shows state-of-the-
art performance on a semantic similarity
task with particularly good performance on
nouns. The resource is freely available.
1 Introduction
Most current work in lexical semantics is based
on the Distributional Hypothesis (Harris, 1954),
which posits a correlation between the degree of
words? semantic similarity and the similarity of
the contexts in which they occur. Using this hy-
pothesis, word meaning representations can be ex-
tracted from large corpora. Words are typically rep-
resented as vectors whose dimensions correspond
to context features. The vector similarities, which
are interpreted as semantic similarities, are used in
numerous applications (Turney and Pantel, 2010).
Most vector spaces in current use are either word-
based (co-occurrence defined by surface window,
context words as dimensions) or syntax-based (co-
occurrence defined syntactically, syntactic objects
as dimensions). Syntax-based models have sev-
eral desirable properties. First, they are model to
fine-grained types of semantic similarity such as
predicate-argument plausibility (Erk et al, 2010).
Second, they are more versatile ? Baroni and Lenci
(2010) have presented a generic framework, the
Distributional Memory (DM), which is applicable
to a wide range of tasks beyond word similarity.
Third, they avoid the ?syntactic assumption? in-
herent in word-based models, namely that context
words are relevant iff they are in an n-word window
around the target. This property is particularly rele-
vant for free word order languages with many long
distance dependencies and non-projective structure
(Ku?bler et al, 2009). Their obvious problem, of
course, is that they require a large parsed corpus.
In this paper, we describe the construction of
a Distributional Memory for Croatian (DM.HR),
a free word order language. To do so, we parse
hrWaC (Ljubes?ic? and Erjavec, 2011), a 1.2B-token
Croatian web corpus. We evaluate DM.HR on a
synonym choice task, where it outperforms the
standard bag-of-word model for nouns and verbs.
2 Related Work
Vector space semantic models have been applied
to a number of Slavic languages, including Bul-
garian (Nakov, 2001a), Czech (Smrz? and Rychly?,
2001), Polish (Piasecki, 2009; Broda et al, 2008;
Broda and Piasecki, 2008), and Russian (Nakov,
2001b; Mitrofanova et al, 2007). Previous work
on distributional semantic models for Croatian
dealt with similarity prediction (Ljubes?ic? et al,
2008; Jankovic? et al, 2011) and synonym detec-
tion (Karan et al, 2012), however using only word-
based and not syntactic-based models.
So far the only DM for a language other than
English is the German DM.DE by Pado? and Utt
(2012), who describe the process of building
DM.DE and the evaluation on a synonym choice
task. Our work is similar, though each language
has its own challenges. Croatian, like other Slavic
languages, has rich inflectional morphology and
free word order, which lead to errors in linguistic
processing and affect the quality of the DM.
784
3 Distributional Memory
DM represents co-occurrence information in a gen-
eral, non-task-specific manner, as a tensor, i.e., a
three-dimensional matrix, of weighted word-link-
word tuples. Each tuple is mapped onto a number
by scoring function ? : W ? L ?W ? R+, that
reflects the strength of the association. When a par-
ticular task is selected, a vector space for this task
can be generated from the tensor by matricization.
Regarding the examples from Section 1, synonym
discovery would use a word by link-word space
(W ? LW ), which contains vectors for words w
represented by pairs ?l, w? of a link and a context
word. Analogy discovery would use a word-word
by link space (WW ? L), which represents word
pairs ?w1, w2? by vectors over links l.
The links can be chosen to model any relation
of interest between words. However, as noted by
Pado? and Utt (2012), dependency relations are the
most obvious choice. Baroni and Lenci (2010) in-
troduce three dependency-based DM variants: De-
pDM, LexDM, and TypeDM. DepDM uses links
that correspond to dependency relations, with sub-
categorization for subject (subj tr and subj intr)
and object (obj and iobj). Furthermore, all prepo-
sitions are lexicalized into links (e.g., ?sun, on,
Sunday?). Finally, the tensor is symmetrized: for
each tuple ?w1, l, w2?, its inverse ?w2, l?1, w1? is
included. The other two variants are more complex:
LexDM uses more lexicalized links, encoding, e.g.,
lexical material between the words, while TypeDM
extends LexDM with a scoring function based on
lexical variability.
Following the work of Pado? and Utt (2012), we
build a DepDM variant for DM.HR. Although Ba-
roni and Lenci (2010) show that TypeDM can out-
perform the other two variants, DepDM often per-
forms at a comparable level, while being much
simpler to build and more efficient to compute.
4 Building DM.HR
To build DM.HR, we need to collect co-occurrence
counts from a corpus. Since no sufficiently large
suitable corpus exists for Croatian, we first explain
how we preprocessed, tagged, and parsed the data.
Corpus and preprocessing. We adopted hrWaC,
the 1.2B-token Croatian web corpus (Ljubes?ic? and
Erjavec, 2011), as starting point. hrWaC was built
with the aim of obtaining a cleaner-than-usual web
corpus. To this end, a conservative boilerplate re-
moval procedure was used; Ljubes?ic? and Erjavec
(2011) report a precision of 97.9% and a recall of
70.7%. Nonetheless, our inspection revealed that,
apart from the unavoidable spelling and grammati-
cal errors, hrWaC still contains non-textual content
(e.g., code snippets and formatting structure), en-
coding errors, and foreign-language content. As
this severely affects linguistic processing, we addi-
tionally filtered the corpus.
First, we removed from hrWaC the content
crawled from main discussion forum and blog web-
sites. This content is highly ungrammatical and
contains a lot of non-diacriticized text, typical for
user-generated content. This step alone removed
one third of the data. We processed the remaining
content with a tokenizer and a sentence segmenter
based on regular expressions, obtaining 66M sen-
tences. Next, we applied a series of heuristic filters
at the document- and sentence-level. At the doc-
ument level, we discard all documents (1) whose
length is below a specified threshold, (2) contain
no diacritics, (3) contain no words from a list of fre-
quent Croatian words, or (4) contain a single word
from lists of distinctive foreign-language words
(for Serbian). The last two steps serve to eliminate
foreign-language content. In particular, the last
step serves to filter out the text in Serbian, which at
the sentence-level is difficult to automatically dis-
criminate from Croatian. At the sentence-level, we
discard sentences that are (1) shorter than a speci-
fied threshold, (2) contain non-standard symbols,
(3) contain non-diacriticized Croatian words, or
(4) contain too many foreign words from a list of
foreign-language words (for English and Slovene).
The last step filters out specifically the sentences
in English and Slovene, as we found that these of-
ten occur mixed with text in Croatian. The final
filtered version of hrWaC contains 51M sentences
and 1.2B tokens. The corpus is freely available for
download, along with a more detailed description
of the preprocessing steps.1
Tagging, lemmatization, and parsing. For mor-
phosyntactic (MSD) tagging, lemmatization, and
dependency parsing of hrWaC, we use freely avail-
able tools with models trained on the new SETimes
Corpus of Croatian (SETIMES.HR), based on the
Croatian part of the SETimes parallel corpus.2 SE-
TIMES.HR and the derived tools are prototypes
1http://takelab.fer.hr/data
2http://www.nljubesic.net/resources/
corpora/setimes/
785
SETIMES.HR Wikipedia
HunPos (POS only) 97.1 94.1
HunPos (full MSD) 87.7 81.5
CST lemmatizer 97.7 96.5
MSTParser 77.5 68.8
Table 1: Tagging, lemmatization, and parsing accu-
racy
that are about to be released as parts of another
work. Here we give a general description and a
re-evaluation that we consider relevant for building
DM.HR.
SETIMES.HR consists of 90K tokens and 4K
sentences, manually lemmatized and MSD-tagged
according to Multext East v4 tagset (Erjavec, 2012),
with the help of the Croatian Lemmatization Server
(Tadic?, 2005). It is used also as a basis for a novel
formalism for syntactic annotation and dependency
parsing of Croatian (Agic? and Merkler, 2013).
On the basis of previous evaluation for Croa-
tian (Agic? et al, 2008; Agic? et al, 2009; Agic?,
2012) and availability and licensing considerations,
we chose HunPos tagger (Hala?csy et al, 2007),
CST lemmatizer (Ingason et al, 2008), and MST-
Parser (McDonald et al, 2006) to process hrWaC.
We evaluated the tools on 100-sentence test sets
from SETIMES.HR and Wikipedia; performance
on Wikipedia should be indicative of the perfor-
mance on a cross-domain dataset, such as hrWaC.
In Table 1 we show lemmatization and tagging ac-
curacy, as well as dependency parsing accuracy
in terms of labeled attachment score (LAS). The
results show that lemmatization, tagging and pars-
ing accuracy improves on the state of the art for
Croatian. The SETIMES.HR dependency parsing
models are publicly available.3
Syntactic patterns. We collect the co-occur-
rence counts of tuples using a set of syntactic pat-
terns. The patterns effectively define the link types,
and hence the dimensions of the semantic space.
Similar to previous work, we use two sorts of links:
unlexicalized and lexicalized.
For unlexicalized links, we use ten syntactic pat-
terns. These correspond to the main dependency re-
lations produced by our parser: Pred for predicates,
Atr for attributes, Adv for adverbs, Atv for verbal
complements, Obj for objects, Prep for preposi-
tions, and Pnom for nominal predicates. We sub-
categorized the subject relation into Sub tr (sub-
3http://zeljko.agic.me/resources/
Link P (%) R (%) F1 (%)
Unlexicalized
Adv 57.3 52.7 54.9
Atr 85.0 89.3 87.1
Atv 75.3 70.9 73.1
Obj 71.4 71.7 71.5
Pnom 55.7 50.8 53.1
Pred 81.8 70.6 75.8
Prep 50.0 28.6 36.4
Sb tr 67.8 73.8 70.7
Sb intr 64.5 64.8 64.7
Verb 61.6 73.6 67.1
Lexicalized
Prepositions 67.2 67.9 67.5
Verbs 61.6 73.6 67.1
All links 73.7 75.5 74.6
Table 2: Tuple extraction performance on SE-
TIMES.HR
jects of transitive verbs) and Sub intr (subject of
intransitive verbs). The motivation for this is better
modeling of verb semantics by capturing diathe-
sis alternations. In particular, for many Croatian
verbs reflexivization introduces a meaning shift,
e.g., predati (to hand in/out) vs. predati se (to
surrender). With subject subcategorization, re-
flexive and irreflexive readings will have differ-
ent tensor representations; e.g., ?student, Subj tr,
zadac?a? (?student, Subj tr, homework?) vs. ?trupe,
Subj intr, napadac?? (?troops, Subj intr, invadors?).
Finally, similar to Pado? and Utt (2012), we use
Verb as an underspecified link between subjects
and objects linked by non-auxiliary verbs.
For lexicalized links, we use two more extraction
patterns for prepositions and verbs. Prepositions
are directly lexicalized as links; e.g., ?mjesto, na,
sunce? (?place, on, sun?). The same holds for non-
auxiliary verbs linking subjects to objects; e.g.,
?drz?ava, kupiti, kolic?ina? (?state, buy, amount?).
Tuple extraction and scoring. The overall qual-
ity of the DM.HR depends on the accuracy of ex-
tracted tuples, which is affected by all preprocess-
ing steps. We computed the performance of tu-
ple extraction by evaluating a sample of tuples
extracted from a parsed version of SETIMES.HR
against the tuples extracted from the SETIMES.HR
gold annotations (we use the same sample as for
tagging and parsing performance evaluation). Ta-
ble 2 shows Precision, Recall, and F1 score. Over-
all, we achieve the best performance on the Atr
links, followed by Pred links. The performance is
generally higher on unlexicalized links than on lex-
icalized links (note that performance on unlexical-
786
Link Word LMI Link Word LMI
Atv moc?i 225107 Adv moguc?e 9669
Atv z?eljeti 22049 Atv namjeravati 9095
Obj stan 19997 Obj karta 8936
po cijena 18534 prije godina 8584
Pred kada 14408 Adv nedavno 7842
Obj dionica 13720 Atv odluc?iti 7578
Atv morati 12097 Adv godina 7496
Obj ulaznica 11126 Obj zemljis?te 7180
Table 3: Top 16 LMI-scored tuples for the verb
kupiti (to buy)
ized Verb links is identical to overall performance
on lexicalized verb links). The overall F1 score of
tuple extraction is 74.6%.
Following DM and DM.DE, we score each
extracted tuple using Local Mutual Information
(LMI) (Evert, 2005):
LMI(i, j, k) = f(i, j, k) log P (i, j, k)P (i)P (j)P (k)
For a tuple (w1, l, w2), LMI scores the association
strength between word w1 and word w2 via link l
by comparing their joint distribution against the dis-
tribution under the independence assumption, mul-
tiplied with the observed frequency f(w1, l, w2) to
discount infrequent tuples. The probabilities are
computed from tuple counts as maximum likeli-
hood estimates. We exclude from the tensor all
tuples with a negative LMI score. Finally, we sym-
metrize the tensor by introducing inverse links.
Model statistics. The resulting DM.HR tensor
consists of 2.3M lemmas, 121M links and 165K
link types (including inverse links). On average,
each lemma has 53 links. This makes DM.HR
more sparse than English DM (796 link types), but
less sparse than German DM (220K link types; 22
links per lemma). Table 3 shows an example of
the extracted tuples for the verb kupiti (to buy).
DM.HR tensor is freely available for download.4
5 Evaluating DM.HR
Task. We present a pilot evaluation DM.HR on a
standard task from distributional semantics, namely
synonym choice. In contrast to tasks like predict-
ing word similarity We use the dataset created by
Karan et al (2012), with more than 11,000 syn-
onym choice questions. Each question consists of
one target word (nouns, verbs, and adjectives) with
4http://takelab.fer.hr/dmhr
Accuracy (%) Coverage (%)
Model N A V N A V
DM.HR 70.0 66.3 63.2 99.9 99.1 100
BOW-LSA 67.2 68.9 61.0 100 100 100
BOW baseline 59.9 65.7 55.9 99.9 99.7 100
Table 4: Results on synonym choice task
four synonym candidates (one is correct). The ques-
tions were extracted automatically from a machine-
readable dictionary of Croatian. An example item
is tez?ak (farmer): poljoprivrednik (farmer), um-
jetnost (art), radijacija (radiation), bod (point).
We sampled from the dataset questions for nouns,
verbs, and adjectives, with 1000 questions each.5
Additionally, we manually corrected some errors
in the dataset, introduced by the automatic extrac-
tion procedure. To make predictions, we compute
pairwise cosine similarities of the target word vec-
tors with the four candidates and predict the can-
didate(s) with maximal similarity (note that there
may be ties).
Evaluation. Our evaluation follows the scheme
developed by Mohammad et al (2007), who define
accuracy as the average number of correct predic-
tions per covered question. Each correct prediction
with a single most similar candidate receives a full
credit (A), while ties for maximal similarity are
discounted (B: two-way tie, C: three-way tie, D:
four-way tie): A+ 12B+ 13C+ 14D. We consider aquestion item to be covered if the target and at least
one answer word are modeled. In our experiments,
ties occur when vector similarities are zero for all
word pairs (due to vector sparsity). Note that a
random baseline would perform at 0.25 accuracy.
As baseline to compare against the DM.HR, we
build a standard bag-of-word model from the same
corpus. It uses a ?5-word within-sentence con-
text window, and the 10,000 most frequent context
words (nouns, adjectives, and verbs) as dimensions.
We also compare against BOW-LSA, a state-of-
the-art synonym detection model from Karan et
al. (2012), which uses 500 latent dimensions and
paragraphs as contexts. We determine the signifi-
cance of differences between the models by com-
puting 95% confidence intervals with bootstrap re-
sampling (Efron and Tibshirani, 1993).
Results. Table 4 shows the results for the three
considered models on nouns (N), adjectives (A),
5Available at: http://takelab.fer.hr/crosyn
787
and verbs (V). The performance of BOW-LSA
differs slightly from that reported by Karan et al
(2012), because we evaluate on a sample of their
dataset. DM.HR outperforms the baseline BOW
model for nouns and verbs (differences are sig-
nificant at p < 0.05). Moreover, on these cate-
gories DM.HR performs slightly better than BOW-
LSA, but the differences are not statistically sig-
nificant. Conversely, on adjectives BOW-LSA per-
forms slightly better than DM.HR, but the differ-
ence is again not statistically significant. All mod-
els achieve comparable and almost perfect cov-
erage on this dataset (BOW-LSA achieves com-
plete coverage because of the way how the original
dataset was filtered).
Overall, the biggest improvement over the base-
line is achieved for nouns. Nouns occur as heads
and dependents of many link types (unlexicalized
and lexicalized), and are thus well represented in
the semantic space. On the other hand, adjectives
seem to be less well modeled. Although the major-
ity of adjectives occur as heads or dependents of
the Atr relation, for which extraction accuracy is
the highest (cf. Table 2), it is likely that a single link
type is not sufficient. As noted by a reviewer, more
insight could perhaps be gained by comparing the
predictions of BOW-LSA and DM.HR models. The
generally low performance on verbs suggests that
their semantic is not fully covered in word- and
syntax-based spaces.
6 Conclusion
We have described the construction of DM.HR, a
syntax-based distributional memory for Croatian
built from a dependency-parsed web corpus. To the
best of our knowledge, DM.HR is the first freely
available distributional memory for a Slavic lan-
guage. We have conducted a preliminary evalua-
tion of DM.HR on a synonym choice task, where
DM.HR outperformed the bag-of-word model and
performed comparable to an LSA model.
This work provides a starting point for a sys-
tematic study of dependency-based distributional
semantics for Croatian and similar languages. Our
first priority will be to analyze how corpus prepro-
cessing and the choice of link types relates to model
performance on different semantic tasks. Better
modeling of adjectives and verbs is also an impor-
tant topic for future research.
Acknowledgments
The first author was supported by the Croatian
Science Foundation (project 02.03/162: ?Deriva-
tional Semantic Models for Information Retrieval?).
We thank the reviewers for their constructive com-
ments. Special thanks to Hiko Schamoni, Tae-Gil
Noh, and Mladen Karan for their assistance.
References
Z?eljko Agic? and Danijela Merkler. 2013. Three syn-
tactic formalisms for data-driven dependency pars-
ing of Croatian. Proceedings of TSD 2013, Lecture
Notes in Artificial Intelligence.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2008. Improving part-of-speech tagging accuracy
for Croatian by morphological analysis. Informat-
ica, 32(4):445?451.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2009. Evaluating full lemmatization of Croatian
texts. In Recent Advances in Intelligent Information
Systems, pages 175?184. EXIT Warsaw.
Z?eljko Agic?. 2012. K-best spanning tree dependency
parsing with verb valency lexicon reranking. In Pro-
ceedings of COLING 2012: Posters, pages 1?12,
Bombay, India.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Bartosz Broda and Maciej Piasecki. 2008. Superma-
trix: a general tool for lexical semantic knowledge
acquisition. In Speech and Language Technology,
volume 11, pages 239?254. Polish Phonetics Asso-
cation.
Bartosz Broda, Magdalena Derwojedowa, Maciej Pi-
asecki, and Stanis?aw Szpakowicz. 2008. Corpus-
based semantic relatedness for the construction of
Polish WordNet. In Proceedings of LREC, Mar-
rakech, Morocco.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and Hall,
New York.
Tomaz? Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46(1):131?142.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010.
A Flexible, Corpus-driven Model of Regular and In-
verse Selectional Preferences. Computational Lin-
guistics, 36(4):723?763.
788
Stefan Evert. 2005. The statistics of word cooccur-
rences. Ph.D. thesis, PhD Dissertation, Stuttgart
University.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz.
2007. HunPos: An open source trigram tagger. In
Proceedings of ACL 2007, pages 209?212, Prague,
Czech Republic.
Zelig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Anton Karl Ingason, Sigru?n Helgado?ttir, Hrafn Lofts-
son, and Eir??kur Ro?gnvaldsson. 2008. A mixed
method lemmatization algorithm using a hierarchy
of linguistic identities (HOLI). In Proceedings of
GoTAL, pages 205?216.
Vedrana Jankovic?, Jan S?najder, and Bojana Dalbelo
Bas?ic?. 2011. Random indexing distributional se-
mantic models for Croatian language. In Proceed-
ings of Text, Speech and Dialogue, pages 411?418,
Plzen?, Czech Republic.
Mladen Karan, Jan S?najder, and Bojana Dalbelo Bas?ic?.
2012. Distributional semantics approach to detect-
ing synonyms in Croatian language. In Proceedings
of the Language Technologies Conference, Informa-
tion Society, Ljubljana, Slovenia.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures
on Human Language Technologies. Morgan & Clay-
pool.
Nikola Ljubes?ic? and Tomaz? Erjavec. 2011. hrWaC
and slWac: Compiling web corpora for Croatian and
Slovene. In Proceedings of Text, Speech and Dia-
logue, pages 395?402, Plzen?, Czech Republic.
Nikola Ljubes?ic?, Damir Boras, Nikola Bakaric?, and Jas-
mina Njavro. 2008. Comparing measures of seman-
tic similarity. In Proceedings of the ITI 2008 30th
International Conference of Information Technology
Interfaces, Cavtat, Croatia.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL-X, pages 216?220, New York, NY.
Olga Mitrofanova, Anton Mukhin, Polina Panicheva,
and Vyacheslav Savitsky. 2007. Automatic word
clustering in Russian texts. In Proceedings of Text,
Speech and Dialogue, pages 85?91, Plzen?, Czech
Republic.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-lingual distributional
profiles of concepts for measuring semantic distance.
In Proceedings of EMNLP/CoNLL, pages 571?580,
Prague, Czech Republic.
Preslav Nakov. 2001a. Latent semantic analysis
for Bulgarian literature. In Proceedings of Spring
Conference of Bulgarian Mathematicians Union,
Borovets, Bulgaria.
Preslav Nakov. 2001b. Latent semantic analysis for
Russian literature investigation. In Proceedings of
the 120 years Bulgarian Naval Academy Confer-
ence.
Sebastian Pado? and Jason Utt. 2012. A distributional
memory for German. In Proceedings of the KON-
VENS 2012 workshop on lexical-semantic resources
and applications, pages 462?470, Vienna, Austria.
Maciej Piasecki. 2009. Automated extraction of lexi-
cal meanings from corpus: A case study of potential-
ities and limitations. In Representing Semantics in
Digital Lexicography. Innovative Solutions for Lexi-
cal Entry Content in Slavic Lexicography, pages 32?
43. Institute of Slavic Studies, Polish Academy of
Sciences.
Pavel Smrz? and Pavel Rychly?. 2001. Finding semanti-
cally related words in large corpora. In Text, Speech
and Dialogue, pages 108?115. Springer.
Marko Tadic?. 2005. The Croatian Lemmatization
Server. Southern Journal of Linguistics, 29(1):206?
217.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
789
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 797?803,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing Identical Events with Graph Kernels
Goran Glavas? and Jan S?najder
University of Zagreb
Faculty of Electrical Engineering and Computing
Text Analysis and Knowledge Engineering Lab
Unska 3, 10000 Zagreb, Croatia
{goran.glavas,jan.snajder}@fer.hr
Abstract
Identifying news stories that discuss the
same real-world events is important for
news tracking and retrieval. Most exist-
ing approaches rely on the traditional vec-
tor space model. We propose an approach
for recognizing identical real-world events
based on a structured, event-oriented doc-
ument representation. We structure docu-
ments as graphs of event mentions and use
graph kernels to measure the similarity be-
tween document pairs. Our experiments
indicate that the proposed graph-based ap-
proach can outperform the traditional vec-
tor space model, and is especially suitable
for distinguishing between topically simi-
lar, yet non-identical events.
1 Introduction
News stories typically describe real-world events.
Topic detection and tracking (TDT) aims to de-
tect stories that discuss identical or directly related
events, and track these stories as they evolve over
time (Allan, 2002). Being able to identify the sto-
ries that describe the same real-world event is es-
sential for TDT, and event-based information re-
trieval in general.
In TDT, an event is defined as something hap-
pening in a certain place at a certain time (Yang
et al, 1999), while a topic is defined as a set of
news stories related by some seminal real-world
event (Allan, 2002). To identify news stories on
the same topic, most TDT approaches rely on tra-
ditional vector space models (Salton et al, 1975),
as more sophisticated natural language processing
techniques have not yet proven to be useful for
this task. On the other hand, significant advances
in sentence-level event extraction have been made
over the last decade, in particular as the result of
standardization efforts such as TimeML (Puste-
jovsky et al, 2003a) and TimeBank (Pustejovsky
et al, 2003b), as well as dedicated evaluation tasks
(ACE, 2005; Verhagen et al, 2007; Verhagen et
al., 2010). However, these two lines of research
have largely remained isolated from one another.
In this paper we bridge this gap and address
the task of recognizing stories discussing identical
events by considering structured representations
from sentence-level events. More concretely, we
structure news stories into event graphs built from
individual event mentions extracted from text. To
measure event-based similarity of news stories, we
compare their event graphs using graph kernels
(Borgwardt, 2007). We conduct preliminary ex-
periments on two event-oriented tasks and show
that the proposed approach can outperform tradi-
tional vector space model in recognizing identical
real-world events. Moreover, we demonstrate that
our approach is especially suitable for distinguish-
ing between topically similar, yet non-identical
real-world events.
2 Related Work
The traditional vector space model (VSM) (Salton
et al, 1975) computes the cosine between bag-of-
words representations of documents. The VSM is
at the core of most approaches that identify same-
topic news stories (Hatzivassiloglou et al, 2000;
Brants et al, 2003; Kumaran and Allan, 2005;
Atkinson and Van der Goot, 2009). However, it
has been observed that some word classes (e.g.,
named entities, noun phrases, collocations) have
more significance than the others. Among them,
named entities have been considered as particu-
larly important, as they often identify the partici-
pants of an event. In view of this, Hatzivassiloglou
et al (2000) restrict the set of words to be used
for document representation to words constituting
noun phrases and named entities. Makkonen et
797
al. (2004) divide document terms into four seman-
tic categories (locations, temporal expressions,
proper names, and general terms) and construct
separate vector for each of them. Kumaran and
Allan (2004) represent news stories with three dif-
ferent vectors, modeling all words, named-entity
words, and all non-named-entity words occurring
in documents. When available, recognition of
identical events can rely on meta-information as-
sociated with news stories, such as document cre-
ation time (DCT). Atkinson and Van der Goot
(2009) combine DCT with VSM, assuming that
temporally distant news stories are unlikely to de-
scribe the same event.
In research on event extraction, the task of rec-
ognizing identical events is known as event coref-
erence resolution (Bejan and Harabagiu, 2010;
Lee et al, 2012). There, however, the aim is to
identify sentence-level event mentions referring to
the same real-world events, and not stories that
discuss identical events.
3 Kernels on Event Graphs
To identify the news describing the same real-
world event, we (1) structure event-oriented in-
formation from text into event graphs and (2) use
graph kernels to measure the similarity between a
pair of event graphs.
3.1 Event graphs
An event graph is a vertex- and edge-labeled
mixed graph in which vertices represent individ-
ual event mentions and edges represent temporal
relations between event mentions. We adopt a
generic representation of event mentions, as pro-
posed by Glavas? and S?najder (2013): each men-
tion consists of an anchor (a word that conveys
the core meaning) and four types of arguments
(agent, target, time, location). Furthermore, we
consider four types of temporal relations between
event mentions: before, after, overlap, and equal
(Allen, 1983). As relations overlap and equal are
symmetric, whereas before and after are not, an
event graph may contain both directed and undi-
rected edges.
Formally, an event graph G is represented as a
tuple G = (V,E,A,m, r), where V is the set of
vertices, E is the set of undirected edges, A is the
set of directed edges (arcs), m : V ? M is a
bijection mapping the vertices to event mentions,
and r : E ? R is the edge-labeling function, as-
signing temporal relations to edges (cf. Fig. 1).
The construction of an event graph from a news
story involves the extraction of event mentions
(anchors and arguments) and the extraction of
temporal relations between mentions. We use a
supervised model (with 80% F1 extraction perfor-
mance) based on a rich set of features similar to
those proposed by Bethard (2008) to extract event
anchors. We then employ a robust, rule-based ap-
proach proposed by Glavas? and S?najder (2013) to
extract generic event arguments. Finally, we em-
ploy a supervised model (60% micro-averaged F1
classification performance) with a rich set of fea-
tures, similar to those proposed by Bethard (2008),
to extract temporal relations between event men-
tions. A detailed description of the graph con-
struction steps is outside the scope of this paper.
To compute event graph kernels (cf. Section
3.2), we need to determine whether two event
mentions co-refer. To resolve cross-document
event coreference, we use the model proposed
by Glavas? and S?najder (2013). The model de-
termines coreference by comparing factual event
anchors and arguments of four coarse-grained se-
mantic types (agent, target, location, and time),
and achieves an F-score of 67% (79% precision
and 57% recall) on the cross-document mention
pairs from the EventCorefBank dataset (Bejan and
Harabagiu, 2008). In what follows, cf (m1,m2)
denotes whether event mentions m1 and m2 co-
refer (equals 1 if mentions co-refer, 0 otherwise).
3.2 Graph kernels
Graph kernels are fast polynomial alternatives
to traditional graph comparison techniques (e.g.,
subgraph isomorphism), which provide an expres-
sive measure of similarity between graphs (Borg-
wardt, 2007). We employ two different graph ker-
nels: product graph kernel and weighted decom-
position kernel. We chose these kernels because
their general forms have intuitive interpretations
for event matching. These particular kernels have
shown to perform well on a number of tasks from
chemoinformatics (Mahe? et al, 2005; Menchetti
et al, 2005).
Product graph kernel. A product graph kernel
(PGK) counts the common walks between two in-
put graphs (Ga?rtner et al, 2003). The graph prod-
uct of two labeled graphs, G and G? , denoted
GP = G?G?, is a graph with the vertex set
VP =
{
(v, v?) | v ? VG, v? ? VG? , ?(v, v?)
}
798
where ?(v, v?) is a predicate that holds when
vertices v and v? are identically labeled (Ham-
mack et al, 2011). Given event graphs G =
(V,E,A,m, r) and G? = (V ?, E?, A?,m?, r?), we
consider the vertices to be identically labeled if
the corresponding event mentions co-refer, i.e.,
?(v, v?) .= cf (m(v),m?(v?)). The edge set of the
graph product depends on the type of the product.
We experiment with two different products: ten-
sor product and conormal product. In the tensor
product, an edge is introduced iff the correspond-
ing edges exist in both input graphs and the labels
of those edges match (i.e., both edges represent the
same temporal relation). In the conormal product,
an edge is introduced iff the corresponding edge
exists in at least one input graph. Thus, a conor-
mal product may compensate for omitted temporal
relations in the input graphs.
Let AP be the adjacency matrix of the graph
productGP built from input graphsG andG?. The
product graph kernel that counts common walks in
G and G? can be computed efficiently as:
KPG(G,G?) =
|VP |?
i,j=1
[(I ? ?AP )?1]ij (1)
when ? < 1/t , where t is the maximum degree of
a vertex in the graph product GP . In our experi-
ments, we set ? to 1/(t+ 1) .
Weighted decomposition kernel. A weighted
decomposition kernel (WDK) compares small
graph parts, called selectors, being matched ac-
cording to an equality predicate. The importance
of the match is weighted by the similarity of the
contexts in which the matched selectors occur.
For a description of a general form of WDK, see
Menchetti et al (2005).
Let S(G) be the set of all pairs (s, z), where s is
the selector (subgraph of interest) and z is the con-
text of s. We decompose event graphs into individ-
ual vertices, i.e., we define selectors to be the indi-
vidual vertices. In this case, similarly as above, the
equality predicate ?(v, v?) for two vertices v ? G
and v? ? G? holds if and only if the correspond-
ing event mentions m(v) and m?(v?) co-refer. Us-
ing selectors that consist of more than one vertex
would require a more complex and perhaps a less
intuitive definition of the equality predicate ?. The
selector context Zv of vertex v is a subgraph of G
that contains v and all its immediate neighbors. In
other words, we consider as context all event men-
tions that are in a direct temporal relation with the
selected mention. WDK between event graphs G
and G? is computed as:
KWD(G,G?) =
?
v?VG,v??VG?
cf (m(v),m?(v?)) ?(Zv, Z ?v?)
(2)
where ?(Zv, Z ?v?) is the context kernel measuring
the similarity between the context Zv of selector
v ? G and the context Z ?v? of selector v? ? G?.
We compute the context kernel ? as the number of
coreferent mention pairs found between the con-
texts, normalized by the context size:
?(Zv, Z ?v?) =
?
w?VZv ,w??VZ?v?
cf(m(w),m?(w?))
max(|VZv |, |VZ?v? |)
The intuition behind this is that a pair of corefer-
ent mentions m(v) and m?(v?) should contribute
to the overall event similarity according to the
number of pairs of coreferent mentions,m(w) and
m?(w?), that are in temporal relation with v and v?,
respectively.
Graph kernels example. As an example, con-
sider the following two story snippets describing
the same sets of real-world events:
Story 1: A Cezanne masterpiece worth at least $131
million that was the yanked from the wall of a Zurich
art gallery in 2008 has been recovered, Serbian po-
lice said today. Four arrests were made overnight
in connection with the theft, which was one of the
biggest art heists in recent history.
Story 2: Serbian police have recovered a painting
by French impressionist Paul Cezanne worth an esti-
mated 100 million euros (131.7 million U.S. dollars),
media reported on Thursday. The painting ?A boy in
a red vest? was stolen in 2008 from a Zurich museum
by masked perpetrators. Four members of an interna-
tional crime ring were arrested Wednesday.
The corresponding event graphs G and G? are
shown in Fig. 1a and 1b, respectively, while their
product is shown in Fig. 1c. There are three pairs
of coreferent event mentions between G and G?:
(yanked, stolen), (recovered, recovered), and (ar-
rests, arrested). Accordingly, the product graph
P has three nodes. The dashed edge between ver-
tices (yanked, stolen) and (arrests, arrested) exists
only in the conormal product graph. By substi-
tuting into (1) the adjacency matrix and maximum
vertex degree of tensor product graph P , we obtain
799
(a) Event graph G (Story 1) (b) Event graph G? (Story 2) (c) Product graph P
Figure 1: Example event graphs and their product
the tensor PGK score as:
KPG =
3?
i,j=1
?
?
(
I ? 13
(
0 0 1
0 0 1
1 1 0
))?1?
?
i,j
? 5.6
Similarly, for the conormal product graph P we
obtain the conormal PGK score of KPG = 9. By
substitutingG andG? into (2), we obtain the WDK
score as:
KWD =
?
(v,v?)?VP
?(Zv, Z ?v?) =
2
3 +
3
4 +
2
4 ? 1.9
where VP contains pairs of coreferent event men-
tions: (yanked, stolen), (recovered, recovered),
and (arrests, arrested).
4 Experiments
We conducted two preliminary experiments to in-
vestigate whether kernels on event graphs can be
used to recognize identical events.
4.1 Task 1: Recognizing identical events
Dataset. In the first experiment, we classify
pairs of news stories as either describing identical
real-world events or not. For this we need a collec-
tion of stories in which pairs of stories on identi-
cal events have been annotated as such. TDT cor-
pora (Wayne, 2000) is not directly usable because
it has no such annotations. We therefore decided
to build a small annotated dataset.1 To this end,
we use the news clusters of the EMM NewsBrief
service (Steinberger et al, 2009). EMM clusters
news stories from different sources using a docu-
ment similarity score. We acquired 10 randomly
chosen news clusters, manually inspected each of
them, and retained in each cluster only the doc-
uments that describe the same real-world events.
Additionally, we ensured that no documents from
1Datasets for both experiments are available at:
http://takelab.fer.hr/evkernels
Model P R F
Tensor PGK 89.7 82.3 85.8
Conormal PGK 89.3 77.8 83.2
WDK 88.6 73.7 80.5
SVM Graph 91.1 87.6 89.3
SVM Graph + VSM 93.8 96.2 95.0
VSM baseline 90.9 82.9 86.7
Table 1: Results for recognition of identical events
different clusters discuss the same event. To ob-
tain the gold standard dataset, we build all pairs
of documents. The final dataset consists of 64
documents in 10 clusters, with 195 news pairs
from the same clusters (positive pairs) and 1821
news pairs from different clusters (negative pairs).
We divide the dataset into a train and a test set
(7:3 split ratio). Note that, although our dataset
has ground-truth annotations, it is incomplete in
the sense that some pairs of documents describ-
ing the same events, which were not recognized
as such by the EMM, are not included. Further-
more, because EMM similarity score uses VSM
cosine similarity as one of the features, VSM co-
sine similarity constitutes a competitive baseline
on this dataset.
Results. For each graph kernel and the VSM
baseline, we determine the optimal threshold on
the train set and evaluate the classification per-
formance on the test set. The results are given
in Table 1. The precision is consistently higher
than recall for all kernels and the baseline. High
precision is expected, as clusters represent topi-
cally dissimilar events. PGK models (both ten-
sor and conormal) outperform the WDK model,
indicating that common walks correlate better to
event-based document similarity than common
subgraphs. Individually, none of the graph kernels
outperforms the baseline. To investigate whether
the two kernels complement each other, we fed the
800
Original
?Taliban militants have attacked a prison in north-west
Pakistan, freeing at least 380 prisoners. . . ?
Event-preserving paraphrase
?Taliban militants in northwest Pakistan attacked the
prison, liberated at least 380 prisoners . . . ?
Event-shifting paraphrase
?Taliban militants have been arrested in north-west Pak-
istan. At least 380 militants have been arrested. . . ?
Table 2: Event paraphrasing example
individual kernel scores to an SVM model (with
RBF kernel), along with additional graph-based
features such as the number of nodes and the num-
ber of edges (SVM graph model). Finally, we com-
bined the graph-based features with the VSM co-
sine similarity (SVM graph + VSM model). SVM
graph model significantly (at p < 0.05, student?s
2-tailed t-test) outperforms the individual kernel
models and the baseline. The combined model
(SVM graph + VSM) significantly (at p < 0.01)
outperforms the baseline and all kernel models.
4.2 Task 2: Event-based similarity ranking
Dataset. In the second experiment we focus
on the task of distinguishing between news sto-
ries that describe topically very similar, yet dis-
tinct events. For this purpose, we use a small
set of event paraphrases, constructed as fol-
lows. We manually selected 10 news stories from
EMM NewsBrief and altered each of them to
obtain two meaning-preserving (event-preserving)
and two meaning-changing (event-shifting) para-
phrases. To obtain the meaning-preserving para-
phrases, we use Google translate and round-trip
translation via two pairs of arbitrarily chosen lan-
guages (Danish/Finnish and Croatian/Hungarian).
Annotators manually corrected lexical and syn-
tactic errors introduced by the round-trip transla-
tion. To obtain meaning-changing paraphrases, we
asked human annotators to alter each story so that
it topically resembles the original, but describes a
different real-world event. The extent of the al-
teration was left to the annotators, i.e., no specific
transformations were proposed. Paraphrase exam-
ples are given in Table 2. The final dataset consists
of 60 news pairs: 30 positive and 30 negative.
Results. For each method we ranked the pairs
based on the assigned similarity scores. An ideal
method would rank all positive pairs above all neg-
ative pairs. We evaluated the performance using
Model R-prec. Avg. prec.
Tensor PGK 86.7 96.8
Conormal PGK 93.3 97.5
WDK 86.7 95.7
VSM baseline 80.0 77.1
Table 3: Results for event-based similarity ranking
two different rank evaluation metrics: R-precision
(precision at rank 30, as there are 30 positive pairs)
and average precision. The performance of graph
kernel models and the VSM baseline is given in
Table 3. We tested the significance of differences
using stratified shuffling (Yeh, 2000). When con-
sidering average precision, all kernel models sig-
nificantly (at p < 0.01) outperform the baseline.
However, when considering R-precision, only the
conormal PGK model significantly (at p < 0.05)
outperforms the baseline. There is no statistical
significance in performance differences between
the considered kernel methods. Inspection of the
rankings reveals that graph kernels assign very low
scores to negative pairs, i.e., they distinguish well
between textual representations of topically simi-
lar, but different real-world events.
5 Conclusion
We proposed a novel approach for recognizing
identical events that relies on structured, graph-
based representations of events described in a
document. We use graph kernels as an expres-
sive framework for modeling the similarity be-
tween structured events. Preliminary results on
two event-similarity tasks are encouraging, indi-
cating that our approach can outperform tradi-
tional vector-space model, and is suitable for dis-
tinguishing between topically very similar events.
Further improvements could be obtained by in-
creasing the accuracy of event coreference resolu-
tion, which has a direct influence on graph kernels.
The research opens up many interesting direc-
tions for further research. Besides a systematic
evaluation on larger datasets, we intend to inves-
tigate the applications in event tracking and event-
oriented information retrieval.
Acknowledgments
This work has been supported by the Ministry of
Science, Education and Sports, Republic of Croa-
tia under the Grant 036-1300646-1986. We thank
the reviewers for their constructive comments.
801
References
ACE. 2005. Evaluation of the detection and recogni-
tion of ACE: Entities, values, temporal expressions,
relations, and events.
James Allan. 2002. Topic Detection and Tracking:
Event-based Information Organization, volume 12.
Kluwer Academic Pub.
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Martin Atkinson and Erik Van der Goot. 2009. Near
real time information mining in multilingual news.
In Proceedings of the 18th International Conference
on World Wide Web, pages 1153?1154. ACM.
Cosmin Adrian Bejan and Sanda Harabagiu. 2008. A
linguistic resource for discovering event structures
and resolving event coreference. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation (LREC 2008).
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1412?1422. Association for Com-
putational Linguistics.
Steven Bethard. 2008. Finding Event, Temporal and
Causal Structure in Text: A Machine Learning Ap-
proach. Ph.D. thesis, University of Colorado at
Boulder.
Karsten Michael Borgwardt. 2007. Graph Ker-
nels. Ph.D. thesis, Ludwig-Maximilians-Universita?t
Mu?nchen.
Thorsten Brants, Francine Chen, and Ayman Farahat.
2003. A system for new event detection. In Pro-
ceedings of the 26th Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, pages 330?337. ACM.
Thomas Ga?rtner, Peter Flach, and Stefan Wrobel.
2003. On graph kernels: Hardness results and ef-
ficient alternatives. In Learning Theory and Kernel
Machines, pages 129?143. Springer.
Goran Glavas? and Jan S?najder. 2013. Exploring coref-
erence uncertainty of generically extracted event
mentions. In Proceedings of 14th International
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 408?422. Springer.
Richard Hammack, Wilfried Imrich, and Sandi
Klavz?ar. 2011. Handbook of Product Graphs. Dis-
crete Mathematics and Its Applications. CRC Press.
Vasileios Hatzivassiloglou, Luis Gravano, and Anki-
needu Maganti. 2000. An investigation of linguistic
features and clustering algorithms for topical doc-
ument clustering. In Proceedings of the 23rd An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 224?231. ACM.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detec-
tion. In Proceedings of the 27th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 297?304.
ACM.
Giridhar Kumaran and James Allan. 2005. Using
names and topics for new event detection. In Pro-
ceedings of the Conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 121?128. Association for
Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489?500. Association for Computational Linguis-
tics.
Pierre Mahe?, Nobuhisa Ueda, Tatsuya Akutsu, Jean-
Luc Perret, and Jean-Philippe Vert. 2005. Graph
kernels for molecular structure-activity relationship
analysis with support vector machines. Journal
of Chemical Information and Modeling, 45(4):939?
951.
Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2004. Simple semantics in topic detec-
tion and tracking. Information Retrieval, 7(3):347?
368.
Sauro Menchetti, Fabrizio Costa, and Paolo Frasconi.
2005. Weighted decomposition kernels. In Pro-
ceedings of the 22nd International Conference on
Machine Learning, pages 585?592. ACM.
James Pustejovsky, Jose? Castano, Robert Ingria, Roser
Sauri, Robert Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir Radev. 2003a. Timeml: Robust
specification of event and temporal expressions in
text. New Directions in Question Answering, 3:28?
34.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003b. The TimeBank corpus. In Cor-
pus Linguistics, volume 2003, page 40.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
802
Ralf Steinberger, Bruno Pouliquen, and Erik Van
Der Goot. 2009. An introduction to the euro-
pean media monitor family of applications. In Pro-
ceedings of the Information Access in a Multilin-
gual World-Proceedings of the SIGIR 2009 Work-
shop, pages 1?8.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 Task 15: TempEval tempo-
ral relation identification. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 75?80. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 Task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62. Association for Computational Linguistics.
Charles Wayne. 2000. Multilingual topic detection
and tracking: Successful research enabled by cor-
pora and evaluation. In Proceedings of the Second
International Conference on Language Resources
and Evaluation Conference (LREC 2000), volume
2000, pages 1487?1494.
Yiming Yang, Jaime G Carbonell, Ralf D Brown,
Thomas Pierce, Brian T Archibald, and Xin Liu.
1999. Learning approaches for detecting and track-
ing news events. Intelligent Systems and their Ap-
plications, IEEE, 14(4):32?43.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th Conference on Computational
linguistics, pages 947?953. Association for Compu-
tational Linguistics.
803
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 441?448,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
TakeLab: Systems for Measuring Semantic Text Similarity
Frane S?aric?, Goran Glavas?, Mladen Karan,
Jan S?najder, and Bojana Dalbelo Bas?ic?
University of Zagreb
Faculty of Electrical Engineering and Computing
{frane.saric, goran.glavas, mladen.karan, jan.snajder, bojana.dalbelo}@fer.hr
Abstract
This paper describes the two systems for
determining the semantic similarity of short
texts submitted to the SemEval 2012 Task 6.
Most of the research on semantic similarity
of textual content focuses on large documents.
However, a fair amount of information is con-
densed into short text snippets such as social
media posts, image captions, and scientific ab-
stracts. We predict the human ratings of sen-
tence similarity using a support vector regres-
sion model with multiple features measuring
word-overlap similarity and syntax similarity.
Out of 89 systems submitted, our two systems
ranked in the top 5, for the three overall eval-
uation metrics used (overall Pearson ? 2nd
and 3rd, normalized Pearson ? 1st and 3rd,
weighted mean ? 2nd and 5th).
1 Introduction
Natural language processing tasks such as text clas-
sification (Sebastiani, 2002), text summarization
(Lin and Hovy, 2003; Aliguliyev, 2009), informa-
tion retrieval (Park et al, 2005), and word sense dis-
ambiguation (Schu?tze, 1998) rely on a measure of
semantic similarity of textual documents. Research
predominantly focused either on the document sim-
ilarity (Salton et al, 1975; Maguitman et al, 2005)
or the word similarity (Budanitsky and Hirst, 2006;
Agirre et al, 2009). Evaluating the similarity of
short texts such as sentences or paragraphs (Islam
and Inkpen, 2008; Mihalcea et al, 2006; Oliva et
al., 2011) received less attention from the research
community. The task of recognizing paraphrases
(Michel et al, 2011; Socher et al, 2011; Wan et
al., 2006) is sufficiently similar to reuse some of the
techniques.
This paper presents the two systems for auto-
mated measuring of semantic similarity of short
texts which we submitted to the SemEval-2012 Se-
mantic Text Similarity Task (Agirre et al, 2012). We
propose several sentence similarity measures built
upon knowledge-based and corpus-based similarity
of individual words as well as similarity of depen-
dency parses. Our two systems, simple and syn-
tax, use supervised machine learning, more specif-
ically the support vector regression (SVR), to com-
bine a large amount of features computed from pairs
of sentences. The two systems differ in the set of
features they employ.
Our systems placed in the top 5 (out of 89 sub-
mitted systems) for all three aggregate correlation
measures: 2nd (syntax) and 3rd (simple) for overall
Pearson, 1st (simple) and 3rd (syntax) for normal-
ized Pearson, and 2nd (simple) and 5th (syntax) for
weighted mean.
The rest of the paper is structured as follows. In
Section 2 we describe both knowledge-based and
corpus-based word similarity measures. In Section
3 we describe in detail the features used by our sys-
tems. In Section 4 we report the experimental results
cross-validated on the development set as well as the
official results on all test sets. Conclusions and ideas
for future work are given in Section 5.
2 Word Similarity Measures
Approaches to determining semantic similarity of
sentences commonly use measures of semantic sim-
441
ilarity between individual words. Our systems use
the knowledge-based and the corpus-based (i.e., dis-
tributional lexical semantics) approaches, both of
which are commonly used to measure the semantic
similarity of words.
2.1 Knowledge-based Word Similarity
Knowledge-based word similarity approaches rely
on a semantic network of words, such as Word-
Net. Given two words, their similarity can be esti-
mated by considering their relative positions within
the knowledge base hierarchy.
All of our knowledge-based word similarity mea-
sures are based on WordNet. Some measures use
the concept of a lowest common subsumer (LCS)
of concepts c1 and c2, which represents the lowest
node in the WordNet hierarchy that is a hypernym
of both c1 and c2. We use the NLTK library (Bird,
2006) to compute the PathLen similarity (Leacock
and Chodorow, 1998) and Lin similarity (Lin, 1998)
measures. A single word often denotes several con-
cepts, depending on its context. In order to compute
the similarity score for a pair of words, we take the
maximum similarity score over all possible pairs of
concepts (i.e., WordNet synsets).
2.2 Corpus-based Word Similarity
Distributional lexical semantics models determine
the meaning of a word through the set of all con-
texts in which the word appears. Consequently, we
can model the meaning of a word using its distribu-
tion over all contexts. In the distributional model,
deriving the semantic similarity between two words
corresponds to comparing these distributions. While
many different models of distributional semantics
exist, we employ latent semantic analysis (LSA)
(Turney and Pantel, 2010) over a large corpus to es-
timate the distributions.
For each word wi, we compute a vector xi using
the truncated singular value decomposition (SVD)
of a tf-idf weighted term-document matrix. The co-
sine similarity of vectors xi and xj estimates the
similarity of the corresponding words wi and wj .
Two different word-vector mappings were com-
puted by processing the New York Times Annotated
Corpus (NYT) (Sandhaus, 2008) and Wikipedia.
Aside from lowercasing the documents and remov-
ing punctuation, we perform no further preprocess-
Table 1: Evaluation of word similarity measures
Measure ws353 ws353-sim ws353-rel
PathLen 0.29 0.61 -0.05
Lin 0.33 0.64 -0.01
Dist (NYT) 0.50 0.50 0.51
Dist (Wikipedia) 0.62 0.66 0.55
ing (e.g., no stopwords removal or stemming). Upon
removing the words not occurring in at least two
documents, we compute the tf-idf. The word vec-
tors extracted from NYT corpus and Wikipedia have
a dimension of 200 and 500, respectively.
We compared the measures by computing the
Spearman correlation coefficient on the Word-
Sim3531 data set, as well as its similarity and re-
latedness subsets described in (Agirre et al, 2009).
Table 1 provides the results of the comparison.
3 Semantic Similarity of Sentences
Our systems use supervised regression with SVR as
a learning model, where each system exploits differ-
ent feature sets and SVR hyperparameters.
3.1 Preprocessing
We list all of the preprocessing steps our systems
perform. If a preprocessing step is executed by only
one of our systems, the system?s name is indicated
in parentheses.
1. All hyphens and slashes are removed;
2. The angular brackets (< and >) that enclose the
tokens are stripped (simple);
3. The currency values are simplified, e.g.,
$US1234 to $1234 (simple);
4. Words are tokenized using the Penn Treebank
compatible tokenizer;
5. The tokens n?t and ?m are replaced with not and
am, respectively (simple);
6. The two consecutive words in one sentence that
appear as a compound in the other sentence are
replaced by the said compound. E.g., cater pil-
lar in one sentence is replaced with caterpil-
lar only if caterpillar appears in the other sen-
tence;
1http://www.cs.technion.ac.il/?gabr/
resources/data/wordsim353/wordsim353.html
442
7. Words are POS-tagged using Penn Treebank
compatible POS-taggers: NLTK (Bird, 2006)
for simple, and OpenNLP2 for syntax;
8. Stopwords are removed using a list of 36 stop-
words (simple).
While we acknowledge that some of the prepro-
cessing steps we take may not be common, we did
not have the time to determine the influence of each
individual preprocessing step on the results to either
warrant their removal or justify their presence.
Since, for example, sub-par, sub par and subpar
are treated as equal after preprocessing, we believe it
makes our systems more robust to inputs containing
small orthographic differences.
3.2 Ngram Overlap Features
We use many features previously seen in paraphrase
classification (Michel et al, 2011). Several features
are based on the unigram, bigram, and trigram over-
lap. Before computing the overlap scores, we re-
move punctuation and lowercase the words. We con-
tinue with a detailed description of each individual
feature.
Ngram Overlap
Let S1 and S2 be the sets of consecutive ngrams
(e.g., bigrams) in the first and the second sentence,
respectively. The ngram overlap is defined as fol-
lows:
ngo(S1, S2) = 2 ?
(
|S1|
|S1 ? S2|
+
|S2|
|S1 ? S2|
)?1
(1)
The ngram overlap is the harmonic mean of the de-
gree to which the second sentence covers the first
and the degree to which the first sentence covers the
second. The overlap, defined by (1), is computed for
unigrams, bigrams, and trigrams.
Additionally we observe the content ngram over-
lap ? the overlap of unigrams, bigrams, and tri-
grams exclusively on the content words. The con-
tent words are nouns, verbs, adjectives, and adverbs,
i.e., the lemmas having one of the following part-of-
speech tags: JJ, JJR, JJS, NN, NNP, NNS, NNPS,
RB, RBR, RBS, VB, VBD, VBG, VBN, VBP, and
VBZ. Intuitively, the function words (prepositions,
2http://opennlp.apache.org/
conjunctions, articles) carry less semantics than con-
tent words and thus removing them might eliminate
the noise and provide a more accurate estimate of
semantic similarity.
In addition to the overlap of consecutive ngrams,
we also compute the skip bigram and trigram over-
lap. Skip-ngrams are ngrams that allow arbitrary
gaps, i.e., ngram words need not be consecutive in
the original sentence. By redefining S1 and S2 to
represent the sets of skip ngrams, we employ eq. (1)
to compute the skip-n gram overlap.
3.3 WordNet-Augmented Word Overlap
One can expect a high unigram overlap between very
similar sentences only if exactly the same words (or
lemmas) appear in both sentences. To allow for
some lexical variation, we use WordNet to assign
partial scores to words that are not common to both
sentences. We define the WordNet augmented cov-
erage PWN (?, ?):
PWN (S1, S2) =
1
|S2|
?
w1?S1
score(w1, S2)
score(w, S) =
?
?
?
1 if w ? S
max
w??S
sim(w,w?) otherwise
where sim(?, ?) represents the WordNet path length
similarity. The WordNet-augmented word over-
lap feature is defined as a harmonic mean of
PWN (S1, S2) and PWN (S2, S1).
Weighted Word Overlap
When measuring sentence similarities we give
more importance to words bearing more content, by
using the information content
ic(w) = ln
?
w??C freq(w
?
)
freq(w)
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the cor-
pus. We use the Google Books Ngrams (Michel et
al., 2011) to obtain word frequencies because of its
excellent word coverage for English. Let S1 and S2
be the sets of words occurring in the first and second
sentence, respectively. The weighted word cover-
age of the second sentence by the first sentence is
443
given by:
wwc(S1, S2) =
?
w?S1?S2 ic(w)?
w??S2
ic(w?)
The weighted word overlap between two sen-
tences is calculated as the harmonic mean of the
wwc(S1, S2) and wwc(S2, S1).
This measure proved to be very useful, but it
could be improved even further. Misspelled frequent
words are more frequent than some correctly spelled
but rarely used words. Hence dealing with mis-
spelled words would remove the inappropriate heavy
penalty for a mismatch between correctly and incor-
rectly spelled words.
Greedy Lemma Aligning Overlap
This measure computes the similarity between
sentences using the semantic alignment of lem-
mas. First we compute the word similarity be-
tween all pairs of lemmas from the first and the
second sentence, using either the knowledge-based
or the corpus-based semantic similarity. We then
greedily search for a pair of most similar lemmas;
once the lemmas are paired, they are not considered
for further matching. Previous research by Lavie
and Denkowski (2009) proposed a similar alignment
strategy for machine translation evaluation. After
aligning the sentences, the similarity of each lemma
pair is weighted by the larger information content of
the two lemmas:
sim(l1, l2) = max(ic(l1), ic(l2)) ? ssim(l1, l2) (2)
where ssim(l1, l2) is the semantic similarity be-
tween lemmas l1 and l2.
The overall similarity between two sentences is
defined as the sum of similarities of paired lemmas
normalized by the length of the longer sentence:
glao(S1, S2) =
?
(l1,l2)?P sim(l1, l2)
max(length(S1), length(S2))
where P is the set of lemma pairs obtained by greedy
alignment. We take advantage of greedy align over-
lap in two features: one computes glao(?, ?) by us-
ing the Lin similarity for ssim(?, ?) in (2), while the
other feature uses the distributional (LSA) similarity
to calculate ssim(?, ?).
Vector Space Sentence Similarity
This measure is motivated by the idea of composi-
tionality of distributional vectors (Mitchell and La-
pata, 2008). We represent each sentence as a sin-
gle distributional vector u(?) by summing the dis-
tributional (i.e., LSA) vector of each word w in the
sentence S: u(S) =
?
w?S xw, where xw is the
vector representation of the word w. Another sim-
ilar representation uW (?) uses the information con-
tent ic(w) to weigh the LSA vector of each word
before summation: uW (S) =
?
w?S ic(w)xw.
The simple system uses |cos(u(S1), u(S2))| and
|cos(uW (S1), uW (S2))| for the vector space sen-
tence similarity features.
3.4 Syntactic Features
We use dependency parsing to identify the lemmas
with the corresponding syntactic roles in the two
sentences. We also compute the overlap of the de-
pendency relations of the two sentences.
Syntactic Roles Similarity
The similarity of the words or phrases having the
same syntactic roles in the two sentences may be in-
dicative of their overall semantic similarity (Oliva et
al., 2011). For example, two sentences with very dif-
ferent main predicates (e.g., play and eat) probably
have a significant semantic difference.
Using Lin similarity ssim(?, ?), we obtain the sim-
ilarity between the matching lemmas in a sentence
pair for each syntactic role. Additionally, for each
role we compute the similarity of the chunks that
lemmas belong to:
chunksim(C1, C2) =
?
l1?C1
?
l2?C2
ssim(l1, l2)
where C1 and C2 are the sets of chunks of
the first and second sentence, respectively. The
final similarity score of two chunks is the
harmonic mean of chunksim(C1, C2)/|C1| and
chunksim(C1, C2)/|C2| .
Syntactic roles that we consider are predicates (p),
subjects (s), direct (d), and indirect (i) (i.e., preposi-
tional) objects, where we use (o) to mean either (d)
or (i). The Stanford dependency parser (De Marn-
effe et al, 2006) produces the dependency parse of
the sentence. We infer (p), (s), and (d) from the syn-
tactic dependencies of type nsubj (nominal subject),
444
nsubjpass (nominal subject passive), and dobj (di-
rect object). By combining the prep and pobj de-
pendencies (De Marneffe and Manning, 2008), we
identify (i). Since the (d) in one sentence often se-
mantically corresponds to (i) in the other sentence,
we pair all (o) of one sentence with all (o) of the
other sentence and define object similarity between
the two sentences as the maximum similarity among
all (o) pairs. Because the syntactic role might be
absent from both sentences (e.g., the object in sen-
tences ?John sings? and ?John is thinking?), we in-
troduce additional binary features indicating if the
comparison for the syntactic role in question exists.
Many sentences (especially longer ones) have two
or more (p). In such cases it is necessary to align
the corresponding predicate groups (i.e., the (p) with
its corresponding arguments) between the two sen-
tences, while also aggregating the (p), (s), and (o)
similarities of all aligned (p) pairs. The similarity
of two predicate groups is defined as the sum of (p),
(s), and (o) similarities. In each iteration, the greedy
algorithm pairs all predicate groups of the first sen-
tence with all predicate groups of the second sen-
tence and searches for a pair with the maximum sim-
ilarity. Once the predicate groups of two sentences
have been aligned, we compute the (p) similarity as
a weighted sum of (p) similarities for each predicate
pair group. The weight of each predicate group pair
equals the larger information content of two predi-
cates. The (s) and (o) similarities are computed in
the same manner.
Syntactic Dependencies Overlap
Similar to the ngram overlap features, we measure
the overlap between sentences based on matching
dependency relations. A similar measure has been
proposed in (Wan et al, 2006). Two syntactic depen-
dencies are considered equal if they have the same
dependency type, governing lemma, and dependent
lemma. Let S1 and S2 be the set of all dependency
relations in the first and the second sentence, respec-
tively. Dependency overlap is the harmonic mean
between |S1 ? S2|/|S1| and |S1 ? S2|/|S2| . Con-
tent dependency overlap computes the overlap in the
same way, but considers only dependency relations
between content lemmas.
Similarly to weighted word overlap, we com-
pute the weighted dependency relations overlap.
The weighted coverage of the second sentence de-
pendencies with the first sentence dependencies is
given by:
wdrc(S1, S2) =
?
r?S1?S2 max(ic(g(r)), ic(d(r)))?
r?S2 max(ic(g(r)), ic(d(r)))
where g(r) is the governing word of the dependency
relation r, d(r) is the dependent word of the depen-
dency relation r, and ic(l) is the information con-
tent of the lemma l. Finally, the weighted depen-
dency relations overlap is the harmonic mean be-
tween wdrc(S1, S2) and wdrc(S2, S1).
3.5 Other Features
Although we primarily focused on developing the
ngram overlap and syntax-based features, some
other features significantly improve the performance
of our systems.
Normalized Differences
Our systems take advantage of the following fea-
tures that measure normalized differences in a pair
of sentences: (A) sentence length, (B) the noun
chunk, verb chunk, and predicate counts, and (C)
the aggregate word information content (see Nor-
malized differences in Table 2).
Numbers Overlap
The annotators gave low similarity scores to many
sentence pairs that contained different sets of num-
bers, even though their sentence structure was very
similar. Socher et al (2011) improved the perfor-
mance of their paraphrase classifier by adding the
following features that compare the sets of num-
bers N1 and N2 in two sentences: N1 = N2,
N1?N2 6= ?, and N1 ? N2?N2 ? N1. We replace
the first two features with log (1 + |N1|+ |N2|) and
2? |N1 ?N2|/(|N1|+ |N2|) . Additionally, the num-
bers that differ only in the number of decimal places
are treated as equal (e.g., 65, 65.2, and 65.234 are
treated as equal, whereas 65.24 and 65.25 are not).
Named Entity Features
Shallow NE similarity treats capitalized words as
named entities if they are longer than one character.
If a token in all caps begins with a period, it is clas-
sified as a stock index symbol. The simple system
445
Table 2: The usage of feature sets
Feature set simple syntax
Ngram overlap + +
Content-ngram overlap - +
Skip-ngram overlap - +
WordNet-aug. overlap + -
Weighted word overlap + +
Greedy align. overlap - +
Vector space similarity + -
Syntactic roles similarity - +
Syntactic dep. overlap - +
Normalized differences* A,C A,B
Shallow NERC + -
Full NERC - +
Numbers overlap + +
* See Section 3.5
uses the following four features: the overlap of cap-
italized words, the overlap of stock index symbols,
and the two features indicating whether these named
entities were found in either of the two sentences.
In addition to the overlap of capitalized words, the
syntax system uses the OpenNLP named entity rec-
ognizer and classifier to compute the overlap of en-
tities for each entity class separately. We recognize
the following entity classes: persons, organizations,
locations, dates, and rudimentary temporal expres-
sions. The absence of an entity class from both sen-
tences is indicated by a separate binary feature (one
feature for each class).
Feature Usage in TakeLab Systems
Some of the features presented in the previous sec-
tions were used by both of our systems (simple and
syntax), while others were used by only one of the
systems. Table 2 indicates the feature sets used for
the two submitted systems.
4 Results
4.1 Model Training
For each of the provided training sets we trained a
separate Support Vector Regression (SVR) model
using LIBSVM (Chang and Lin, 2011). To ob-
tain the optimal SVR parameters C, g, and p, our
systems employ a grid search with nested cross-
Table 3: Cross-validated results on train sets
MSRvid MSRpar SMTeuroparl
simple 0.8794 0.7566 0.7802
syntax 0.8698 0.7144 0.7308
validation. Table 3 presents the cross-validated per-
formance (in terms of Pearson correlation) on the
training sets. The models tested on the SMTnews
test set were trained on the SMTeuroparl train set.
For the OnWn test set, the syntax model was trained
on the MSRpar set, while the simple system?s model
was trained on the union of all train sets. The final
predictions were trimmed to a 0?5 range.
Our development results indicate that the
weighted word overlap, WordNet-augmented word
overlap, the greedy lemma alignment overlap, and
the vector space sentence similarity individually
obtain high correlations regardless of the devel-
opment set in use. Other features proved to be
useful on individual development sets (e.g., syntax
roles similarity on MSRvid and numbers overlap
on MSRpar). More research remains to be done in
thorough feature analysis and systematic feature
selection.
4.2 Test Set Results
The organizers provided five different test sets to
evaluate the performance of the submitted systems.
Table 4 illustrates the performance of our systems
on individual test sets, accompanied by their rank.
Our systems outperformed most other systems on
MSRvid, MSRpar, and OnWN sets (Agirre et al,
2012). However, they performed poorly on the
SMTeuroparl and SMTnews sets. While the corre-
lation scores on the MSRvid and MSRpar test sets
correspond to those obtained using cross-validation
on the corresponding train sets, the performance on
the SMT test sets is drastically lower than the cross-
validated performance on the corresponding train
set. The sentences in the SMT training set are signif-
icantly longer (30.4 tokens on average) than the sen-
tences in both SMT test sets (12.3 for SMTeuroparl
and 13.5 for SMTnews). Also there are several re-
peated pairs of extremely short and identical sen-
tences (e.g., ?Tunisia? ? ?Tunisia? appears 17 times
446
Table 4: Results on individual test sets
simple syntax
MSRvid 0.8803 (1) 0.8620 (8)
MSRpar 0.7343 (1) 0.6985 (2)
SMTeuroparl 0.4771 (26) 0.3612 (63)
SMTnews 0.3989 (46) 0.4683 (18)
OnWN 0.6797 (9) 0.7049 (6)
Table 5: Aggregate performance on the test sets
All ALLnrm Mean
simple 0.8133 (3) 0.8635 (1) 0.6753 (2)
syntax 0.8138 (2) 0.8569 (3) 0.6601 (5)
in the SMTeuroparl test set). The above measure-
ments indicate that the SMTeuroparl training set was
not representative of the SMTeuroparl test set for our
choice of features.
Table 5 outlines the aggregate performance of our
systems according to the three aggregate evaluation
measures proposed for the task (Agirre et al, 2012).
Both systems performed very favourably compared
to the other systems, achieving very high rankings
regardless of the aggregate evaluation measure.
The implementation of simple system is available
at http://takelab.fer.hr/sts.
5 Conclusion and Future Work
In this paper we described our submission to the
SemEval-2012 Semantic Textual Similarity Task.
We have identified some high performing features
for measuring semantic text similarity. Although
both of the submitted systems performed very well
on all but the two SMT test sets, there is still room
for improvement. The feature selection was ad-hoc
and more systematic feature selection is required
(e.g., wrapper feature selection). Introducing ad-
ditional features for deeper understanding (e.g., se-
mantic role labelling) might also improve perfor-
mance on this task.
Acknowledgments
This work was supported by the Ministry of Science,
Education and Sports, Republic of Croatia under the
Grant 036-1300646-1986. We would like to thank
the organizers for the tremendous effort they put into
formulating this challenging task.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 19?27. As-
sociation for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012). ACL.
Ramiz M. Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764?7772.
Steven Bird. 2006. NLTK: the natural language toolkit.
In Proceedings of the COLING/ACL on Interactive
presentation sessions, COLING-ACL ?06, pages 69?
72. Association for Computational Linguistics.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Marie-Catherine De Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8. Association for Computational
Linguistics.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, volume 6, pages 449?454.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
447
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine translation, 23(2):105?115.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265?283.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 71?78. Association for
Computational Linguistics.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th inter-
national conference on Machine Learning, volume 1,
pages 296?304. San Francisco.
Ana G. Maguitman, Filippo Menczer, Heather Roinestad,
and Alessandro Vespignani. 2005. Algorithmic detec-
tion of semantic similarity. In Proceedings of the 14th
international conference on World Wide Web, pages
107?116. ACM.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,
Adrian Veres, Matthew K. Gray, et al 2011. Quan-
titative analysis of culture using millions of digitized
books. Science, 331(6014):176.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. Proceedings of ACL-
08: HLT, pages 236?244.
Jesu?s Oliva, Jo?se Ignacio Serrano, Mar??a Dolores
Del Castillo, and A?ngel Iglesias. 2011. SyMSS: A
syntax-based measure for short-text semantic similar-
ity. Data & Knowledge Engineering.
Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang. 2005.
Techniques for improving web retrieval effectiveness.
Information processing & management, 41(5):1207?
1223.
Gerard Salton, Andrew Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12):e26752.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational linguistics, 24(1):97?123.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1?47.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. Advances in Neural Infor-
mation Processing Systems, 24.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37(1):141?
188.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.
2006. Using dependency-based features to take the
?para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, volume
2006.
448
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 1?9,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Experiments on Hybrid Corpus-Based Sentiment Lexicon Acquisition
Goran Glavas?, Jan S?najder and Bojana Dalbelo Bas?ic?
Faculty of Electrical Engineering and Computing
University of Zagreb
Zagreb, Croatia
{goran.glavas, jan.snajder, bojana.dalbelo}@fer.hr
Abstract
Numerous sentiment analysis applications
make usage of a sentiment lexicon. In
this paper we present experiments on hy-
brid sentiment lexicon acquisition. The ap-
proach is corpus-based and thus suitable
for languages lacking general dictionary-
based resources. The approach is a hy-
brid two-step process that combines semi-
supervised graph-based algorithms and su-
pervised models. We evaluate the perfor-
mance on three tasks that capture differ-
ent aspects of a sentiment lexicon: polar-
ity ranking task, polarity regression task,
and sentiment classification task. Exten-
sive evaluation shows that the results are
comparable to those of a well-known senti-
ment lexicon SentiWordNet on the polarity
ranking task. On the sentiment classifica-
tion task, the results are also comparable to
SentiWordNet when restricted to monosen-
timous (all senses carry the same senti-
ment) words. This is satisfactory, given the
absence of explicit semantic relations be-
tween words in the corpus.
1 Introduction
Knowing someone?s attitude towards events, en-
tities, and phenomena can be very important in
various areas of human activity. Sentiment anal-
ysis is an area of computational linguistics that
aims to recognize the subjectivity and attitude ex-
pressed in natural language texts. Applications
of sentiment analysis are numerous, including
sentiment-based document classification (Riloff
et al, 2006), opinion-oriented information extrac-
tion (Hu and Liu, 2004), and question answering
(Somasundaran et al, 2007).
Sentiment analysis combines subjectivity anal-
ysis and polarity analysis. Subjectivity analy-
sis answers whether the text unit is subjective
or neutral, while polarity analysis determines
whether a subjective text unit is positive or nega-
tive. The majority of research approaches (Hatzi-
vassiloglou and McKeown, 1997; Turney and
Littman, 2003; Wilson et al, 2009) see subjec-
tivity and polarity as categorical terms (i.e., clas-
sification problems). Intuitively, not all words ex-
press the sentiment with the same intensity. Ac-
cordingly, there has been some research effort in
assessing subjectivity and polarity as graded val-
ues (Baccianella et al, 2010; Andreevskaia and
Bergler, 2006). Most of the work on sentence or
document level sentiment makes usage of senti-
ment annotated lexicon providing subjectivity and
polarity information for individual words (Wilson
et al, 2009; Taboada et al, 2011).
In this paper we present a hybrid approach
for automated acquisition of sentiment lexicon.
The method is language independent and corpus-
based and therefore suitable for languages lack-
ing general lexical resources such as WordNet
(Fellbaum, 2010). The two-step hybrid pro-
cess combines semi-supervised graph-based algo-
rithms and supervised learning models.
We consider three different tasks, each captur-
ing different aspect of a sentiment lexicon:
1. Polarity ranking task ? determine the relative
rankings of words, i.e., order lexicon items
descendingly by positivity and negativity;
2. Polarity regression task ? assign each word
absolute scores (between 0 and 1) for posi-
tivity and negativity;
3. Sentiment classification task ? classify each
1
word into one of the three sentiment classes
(positive, negative, or neutral).
Accordingly, we evaluate our method using three
different measures ? one to evaluate the quality
of the ordering by positivity and negativity, other
to evaluate the absolute sentiment scores assigned
to each corpus word, and another to evaluate the
classification performance.
The rest of the paper is structured as follows.
In Section 2 we present the related work on senti-
ment lexicon acquisition. Section 3 discusses the
semi-supervised step of the hybrid approach. In
Section 4 we explain the supervised step in more
detail. In Section 5 the experimental setup, the
evaluation procedure, and the results of the ap-
proach are discussed. Section 6 concludes the pa-
per and outlines future work.
2 Related Work
Several approaches have been proposed for deter-
mining the prior polarity of words. Most of the
approaches can be classified as either dictionary-
based (Kamps et al, 2004; Esuli and Sebastiani,
2007; Baccianella et al, 2010) or corpus-based
(Hatzivassiloglou and McKeown, 1997; Turney
and Littman, 2003). Regardless of the resource
used, most of the approaches focus on bootstrap-
ping, starting from a small seed set of manually
labeled words (Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003; Esuli and Se-
bastiani, 2007). In this paper we also follow this
idea of the semi-supervised bootstrapping as the
first step of the sentiment lexicon acquisition.
Dictionary-based approaches grow the seed
sets according to the explicit paradigmatic seman-
tic relations (synonymy, antonymy, hyponymy,
etc.) between words in the dictionary. Kamps
et al (2004) build a graph of adjectives based
on synonymy relations gathered from WordNet.
They determine the polarity of the adjective based
on its shortest path distances from positive and
negative seed adjectives good and bad. Esuli and
Sebastiani (2007) first build a graph based on a
gloss relation (i.e., definiens ? definiendum rela-
tion) from WordNet. Afterwards they perform a
variation of the PageRank algorithm (Page et al,
1999) in two runs. In the first run positive PageR-
ank value is assigned to the vertices of the synsets
from the positive seed set and zero value to all
other vertices. In the second run the same is done
for the synsets from the negative seed set. Word?s
polarity is then decided based on the difference
between its PageRank values of the two runs. We
also believe that graph is the appropriate struc-
ture for the propagation of sentiment properties of
words. Unfortunately, for many languages a pre-
compiled lexical resource like WordNet does not
exist. In such a case, semantic relations between
words may be extracted from corpus.
In their pioneering work, Hatzivassiloglou and
McKeown (1997) attempt to determine the po-
larity of adjectives based on their co-occurrences
in conjunctions. They start with a small manu-
ally labeled seed set and build on the observa-
tion that adjectives of the same polarity are often
conjoined with the conjunction and, while adjec-
tives of the opposite polarity are conjoined with
the conjunction but. Turney and Littman (2003)
use pointwise mutual information (PMI) (Church
and Hanks, 1990) and latent semantic analysis
(LSA) (Dumais, 2004) to determine the similarity
of the word of unknown polarity with the words
in both positive and negative seed sets. The afore-
mentioned work presumes that there is a corre-
lation between lexical semantics and sentiment.
We base our work on the same assumption, but
instead of directly comparing the words with the
seed sets, we use distributional semantics to build
a word similarity graph. In contrast to the ap-
proaches above, this allows us to potentially ac-
count for similarities between all pairs of words
from corpus. To the best of our knowledge, such
an approach that combines corpus-based lexical
semantics with graph-based propagation has not
yet been applied to the task of building senti-
ment lexicon. However, similar approaches have
been proven rather efficient on other tasks such
as document level sentiment classification (Gold-
berg and Zhu, 2006) and word sense disambigua-
tion (Agirre et al, 2006).
3 Semi-supervised Graph-based
Methods
The structure of a graph in general provides a
good framework for propagation of object proper-
ties, which, in our case, are the sentiment values
of the words. In a word similarity graph, weights
of edges represent the degree of semantic similar-
ity between words.
In the work presented in this paper we build
graphs from corpus, using different notions of
2
word similarity. Each vertex in the graph repre-
sents a word from corpus. Weights of the edges
are calculated in several different ways, using
measures of word co-occurrence (co-occurrence
frequency and pointwise mutual information) and
distributional semantic models (latent semantic
analysis and random indexing). We manually
compiled positive and negative seed sets, each
consisting of 15 words:
positiveSeeds = {good, best, excel-
lent, happy, well, new, great, nice,
smart, beautiful, smile, win, hope, love,
friend}
negativeSeeds = {bad, worst, violence,
die, poor, terrible, death, war, enemy,
accident, murder, lose, wrong, attack,
loss}
In addition to these, we compiled the third seed
set consisting of neutral words to serve as sen-
timent sinks for the employed label propagation
algorithm:
neutralSeeds = {time, place, company,
work, city, house, man, world, woman,
country, building, number, system, ob-
ject, room}
Once we have built the graph, we label the ver-
tices belonging to the words from the polar seed
set with the sentiment score of 1. All other ver-
tices are initially unlabeled (i.e., assigned a sen-
timent score of 0). We then use the structure of
the graph and one of the two random-walk algo-
rithms to propagate the labels from the labeled
seed set vertices to the unlabeled ones. The ran-
dom walk algorithm is executed twice: once with
the words from the positive seed set being ini-
tially labeled and once with the words from the
negative seed set being initially labeled. Once the
random walk algorithm converges, all unlabeled
vertices will be assigned a sentiment label. How-
ever, the final sentiment values obtained after the
convergence of the random-walk algorithm are di-
rectly dependent on the size of the graph (which,
in turn, depends on the size of the corpus), the
size of the seed set, and the choice of the seed set
words. Thus, they should be interpreted as rela-
tive rather than absolute sentiment scores. Nev-
ertheless, the scores obtained from the graph can
be used to rank the words by their positivity and
negativity.
3.1 Similarity Based on Corpus
Co-occurrence
If the two words co-occur in the corpus within a
window of a given size, an edge in the graph be-
tween their corresponding vertices is added. The
weight of the edge should represent the measure
of the degree to which the two words co-occur.
There are many word collocation measures that
may be used to calculate the weights of edges
(Evert, 2008). In this work, we use raw co-
occurrence frequency and pointwise mutual in-
formation (PMI) (Church and Hanks, 1990). In
the former case the edge between two words is
assigned a weight indicating a total number of
co-occurrences of the corresponding words in the
corpus within the window of a given size. In the
latter case, we use PMI to account for the indi-
vidual frequencies of each of the two words along
with their co-occurrence frequency. The most fre-
quent corpus words tend to frequently co-occur
with most other words in the corpus, including
words from both positive and negative seed sets.
PMI compensates for this shortcoming of the raw
co-occurrence frequency measure.
3.2 Similarity Based on Latent Semantic
Analysis
Latent semantic analysis is a well-known tech-
nique for identifying semantically related con-
cepts and dimensionality reduction in large vector
spaces (Dumais, 2004). The first step is to cre-
ate a sparse word-document matrix. Matrix ele-
ments are frequencies of words occurring in docu-
ments, usually transformed using some weighting
scheme (e.g., tf-idf ). The word-document matrix
is then decomposed using singular value decom-
position (SVD), a well-known linear algebra pro-
cedure. Finally, the dimensionality reduction is
performed by approximating the original matrix
using only the top k largest singular values.
We build two different word-document matri-
ces using different weighting schemes. The el-
ements of the first matrix were calculated using
the tf-idf weighting scheme, while for the sec-
ond matrix the log-entropy weighting scheme was
used. In the log-entropy scheme, each matrix ele-
ment, mw,d, is calculated using logarithmic value
of word-document frequency and the global word
entropy (entropy of word frequency across the
documents), as follows:
3
mw,d = log (tfw ,d + 1 ) ? ge(w)
with
ge(w) = 1 +
1
log n
?
d??D
tfw ,d ?
gf w
log
tfw ,d ?
gf w
where tfw ,d represents occurrence frequency of
word w in document d, parameter gf w represents
global frequency of word w in corpus D, and n
is the number of documents in corpus D. Next,
we decompose each of the two matrices using
SVD in order to obtain a vector for each word
in the vector space of reduced dimensionality k
(k  n). LSA vectors tend to express semantic
properties of words. Moreover, the similarity be-
tween the LSA vectors may be used as a measure
of semantic similarity between the corresponding
words. We compute this similarity using the co-
sine between the LSA vectors and use the ob-
tained values as weights of graph edges. Because
running random-walk algorithms on a complete
graph would be computationally intractable, we
decided to reduce the number of edges by thresh-
olding the similarity values.
3.3 Similarity Based on Random Indexing
Random Indexing (RI) is another word space ap-
proach, which presents an efficient and scalable
alternative to more commonly used word space
methods such as LSA. Random indexing is a di-
mensionality reduction technique in which a ran-
dom matrix is used to project the original word-
context matrix into the vector space of lower di-
mensionality. Each context is represented by its
index vector, a sparse vector with a small number
of randomly distributed +1 and ?1 values, the
remaining values being 0 (Sahlgren, 2006). For
each corpus word its context vector is constructed
by summing index vectors of all context elements
occurring within contexts of all of its occurrences
in the corpus. The semantic similarity of the two
words is then expressed as the similarity between
its context vectors.
We use two different definitions for the context
and context relation. In the first case (referred to
as RI with document context), each corpus docu-
ment is considered as a separate context and the
word is considered to be in a context relation if
it occurs in the document. The context vector of
each word is then simply the sum of random in-
dex vectors of the documents in which the word
occurs. In the second case (referred to as RI with
window context), each corpus word is considered
as a context itself, and the two words are consid-
ered to be in a context relation if they co-occur in
the corpus within the window of a given size. The
context vector of each corpus word is then com-
puted as the sum of random index vectors of all
words with which it co-occurs in the corpus in-
side the window of a given size. Like in the LSA
approach, we use the cosine of the angle between
the context vectors as a measure of semantic simi-
larity between the word pairs. To reduce the num-
ber of edges, we again perform the thresholding
of the similarity values.
3.4 Random-Walk Algorithms
Once the graph building phase is done, we start
propagating the sentiment scores from the vertices
of the seed set words to the unlabeled vertices.
To this end, one can use several semi-supervised
learning algorithms. The most commonly used
algorithm for dictionary-based sentiment lexicon
acquisition is PageRank. Along with the PageR-
ank we employ another random-walk algorithm
called harmonic function learning.
PageRank
PageRank (Page et al, 1999) was initially de-
signed for ranking web pages by their relevance.
The intuition behind PageRank is that a vertex
v should have a high score if it has many high-
scoring neighbours and these neighbours do not
have many other neighbours except the vertex
v. Let W be the weighted row-normalized ad-
jacency matrix of graph G. The algorithm itera-
tively computes the vector of vertex scores a in
the following way:
a(k) = ?a(k?1)W + (1? ?)e
where? is the PageRank damping factor. Vector e
models the normalized internal source of score for
all vertices and its elements sum up to 1. We as-
sign the value of ei to be 1|SeedSet | for the vertices
whose corresponding words belong to the seed set
and ei = 0 for all other vertices.
Harmonic Function
The second graph-based semi-supervised
learning algorithm we use is the harmonic func-
4
tion label propagation (also known as absorbing
random walk) (Zhu and Goldberg, 2009). Har-
monic function tries to propagate labels between
sources and sinks of sentiment. We perform two
runs of the algorithm: one for positive sentiment,
in which we use the words from the positive seed
set as sentiment sources, and one for the negative
sentiment, in which we use the words from the
negative seed set as sentiment sources. In both
cases, we use the precompiled seed set of neutral
words as sentiment sinks. Note that we could
not have used positive seed set words as sources
and negative seed set words as sinks (or vice
versa) because we aim to predict the positive and
negative sentiment scores separately.
The value of the harmonic function for a la-
beled vertex remains the same as initially labeled,
whereas for an unlabeled vertex the value is com-
puted as the weighted average of its neighbours?
values (Zhu and Goldberg, 2009):
f(vk) =
?
j?|V |wkj ? f(vj)
?
j?|V |wkj
where V is the set of vertices of graph G and
wkj is the weight of the edge between the ver-
tices vk and vj . If there is no graph edge be-
tween vertices vk and vj , the value of the weight
wkj is 0. This equation also represents the update
rule for the iterative computation of the harmonic
function. However, it can be shown that there is
a closed-form solution of the harmonic function.
Let W be the unnormalized weighted adjacency
matrix of the graph G, and let D be the diagonal
matrix with the element Dii =
?
j?|V |wij be-
ing the weighted degree of the vertex vi. Then
the unnormalized graph Laplacian is defined with
L = D ?W . Assuming that the labeled seed set
vertices are ordered before the unlabeled ones, the
graph Laplacian can be partitioned in the follow-
ing way:
L =
(
Lll Llu
Lul Luu
)
The closed form solution for the harmonic
function of the unlabeled vertices is then given by:
fu = ?L?1uuLulyl
where yl if the vector of labels of the seed set ver-
tices (Zhu and Goldberg, 2009).
4 Supervised Step Hybridization
The sentiment scores obtained by the semi-
supervised graph-based approaches described
above are relative because they depend on the
graph size as well as on the size and content of
the seed sets. As such, these values can be used to
rank the words by positivity or negativity, but not
as absolute positivity and negativity scores. Thus,
in the second step of our hybrid approach, we use
supervised learning to obtain the absolute senti-
ment scores (polarity regression task) and the sen-
timent labels (sentiment classification task).
Each score obtained on each graph represents
a single feature for supervised learning. There
are altogether 24 different semi-supervised fea-
tures used as input for the supervised learners.
These features are both positive and negative la-
bels generated from six different semi-supervised
graphs (co-occurence frequency, co-occurrence
PMI, LSA log-entropy, LSA tf-idf, random in-
dexing with document context, and random in-
dexing with window context) using two different
random-walk algorithms (harmonic function and
PageRank). We used the occurrence frequency of
words in corpus as an additional feature.
For polarity regression, learning must be per-
formed twice: once for the negative and once for
the positive sentiment score. We performed the
regression using SVM with radial-basis kernel.
The same set of features used for regression was
used for sentiment classification, but the goal was
to predict the class of the word (positive, negative,
or neutral) instead of separate positivity or nega-
tivity scores. SVM with radial-basis kernel was
used to perform classification learning as well.
5 Evaluation and Results
All the experiments were performed on the ex-
cerpt of the New York Times corpus (years 2002?
2007), containing 434,494 articles. The corpus
was preprocessed (tokenized, lemmatized, and
POS tagged) and only the content lemmas (nouns,
verbs, adjectives, and adverbs) occurring at least
80 times in the corpus were considered. Lemmas
occurring less than 80 were mainly named entities
or their derivatives. The final sentiment lexicon
consists of 41,359 lemmas annotated with posi-
tivity and negativity scores and sentiment class.1
1Sentiment lexicon is freely available at
http://takelab.fer.hr/sentilex
5
5.1 Sentiment Annotations
To evaluate our methods on the three tasks, we
compare the results against the Micro-WN(Op)
dataset (Cerini et al, 2007). Micro-WN(Op) con-
tains sentiment annotations for 1105 WordNet 2.0
synsets. Each synset s is manually annotated with
the degree of positivity Pos(s) and negativity
Neg(s), where 0 ? Pos(s) ? 1, 0 ? Neg(s) ?
1, and Pos(s) +Neg(s) ? 1. Objectivity score is
defined as Obj (s) = 1? (Pos(s) + Neg(s)).
This gives us a list of 2800 word-sense pairs
with their sentiment annotations. For reasons that
we explain below, we retain from this list only
those words for which all senses from WordNet
have been sentiment-annotated, which leaves us
with a list of 1645 word-sense pairs. From this
list we then filter out all words that occur less
than 80 times in our corpus, leaving us with a list
of 1125 word-sense pairs (365 distinct words, of
which 152 are monosemous). We refer to this set
of 1125 sentiment-annotated word-sense pairs as
Micro-WN(Op)-0.
Because our corpus-based methods are unable
to discriminate among various senses of a pol-
ysemous word, we wish to be able to eliminate
the negative effect of polysemy in our evalua-
tion. The motivation for this is twofold: first, it
gives us a way of measuring how much polysemy
influences our results. Secondly, it provides us
with the answer how well our method could per-
form in an ideal case where all the words from
corpus have been pre-disambiguated. Because
each of the words in Micro-WN(Op)-0 has all its
senses sentiment-annotated, we can determine for
each of these words how sentiment depends on its
sense. Expectedly, there are words whose senti-
ment differs radically across its senses or parts-
of-speech (e.g., catch, nest, shark, or hot), but
also words whose sentiment is constant or simi-
lar across all its senses. To eliminate the effect
of polysemy on sentiment prediction, we further
filter the Micro-WN(Op)-0 list by retaining only
the words whose sentiment is constant or nearly
constant across all their senses. We refer to such
words as monosentimous. We consider a word
to be monosentimous iff (1) pairwise differences
between all sentiment scores across senses are
less than 0.25 (separately for both positive and
negative sentiment) and (2) the sign of the dif-
ference between positive and negative sentiment
score is constant across all senses. Note that ev-
ery monosemous word is by definition monosen-
timous. Out of 365 words in Micro-WN(Op)-
0, 225 of them are monosentimous. To obtain
the sentiment scores of monosentimous words,
we simply average the scores across their senses.
We refer to the so-obtained set of 225 sentiment-
annotated words as Micro-WN(Op)-1.
5.2 Semi-supervised Step Evaluation
The semi-supervised step was designed to prop-
agate sentiment properties of the labeled words,
ordering the words according to their positivity
or negativity. Therefore, we decided to use the
evaluation metric that measures the quality of
the ranking in ordered lists, Kendall ? distance.
The performance of the semi-supervised graph-
based methods was evaluated both on the Micro-
WN(Op)-1 and Micro-WN(Op)-0 sets.
In order to be able to compare our results to
SentiWordNet (Baccianella et al, 2010), the de
facto standard sentiment lexicon for English, we
use the p-normalized Kendall ? distance between
the rankings generated by our semi-supervised
graph-based methods and the gold standard rank-
ings. The p-normalized Kendall ? distance (Fagin
et al, 2004) is a version of the standard Kendall ?
distance that accounts for ties in the ordering:
? =
nd + p ? nt
Z
where nd is the number of pairs in disagreement
(i.e., pairs of words ordered one way in the gold
standard and the opposite way in the ranking un-
der evaluation), nt is the number of pairs which
are ordered in the gold standard and tied in the
ranking under evaluation, p is the penalization
factor to be assigned to each of the nt pairs (usu-
ally set to p = 12 ), and Z is the number of pairs of
words that are ordered in the gold standard. Table
1 presents the results for each of the methods used
to build the sentiment graph and for both random-
walk algorithms. The results were obtained by
evaluating the relative rankings of words against
the Micro-WN(Op)-1 as gold standard. For com-
parison, the p-normalized Kendall ? scores for
SentiWordNet 1.0 and SentiWordNet 3.0 are ex-
tracted from (Baccianella et al, 2010).
Rankings for the negative scores are consis-
tently better across all methods and algorithms.
We believe that the negative rankings are better
6
Table 1: The results on the polarity ranking task
Harmonic function PageRank
Positive Negative Positive Negative
Co-occurrence freq. 0.395 0.298 0.540 0.544
LSA log-entropy 0.425 0.308 0.434 0.370
LSA tf-idf 0.396 0.320 0.417 0.424
Co-occurrence PMI 0.321 0.256 0.550 0.576
Random indexing document context 0.402 0.433 0.534 0.557
Random indexing window context 0.455 0.398 0.491 0.436
Positive Negative
SentiWordNet 1.0 0.349 0.296
SentiWordNet 3.0 0.281 0.231
for two reasons. Firstly, the corpus contains many
more articles describing negative events such as
wars and accidents than the articles describing
positive events such as celebrations and victo-
ries. In short, the distribution of articles is signif-
icantly skewed towards ?negative? events. Sec-
ondly, the lemma new, which was included in
the positive seed set, occurs in the corpus very
frequently as a part of named entity collocations
such as ?New York? and ?New Jersey? in which
it does not reflect its dominant sense. The har-
monic function label propagation generally out-
performs the PageRank algorithm. The best per-
formance on the Micro-WN(Op)-0 set was 0.380
for the positive ranking and 0.270 for the nega-
tive ranking, showing that the performance de-
teriorates when polysemy is present. However,
the drop in performance, especially for the neg-
ative ranking, is not substantial. Our best method
(graph built based on PMI of corpus words used in
combination with harmonic function label prop-
agation) outperforms SentiWordNet 1.0 and per-
forms slightly worse than SentiWordNet 3.0 for
both positive and negative rankings.
5.3 Evaluation of the Supervised Step
Supervised step deals with the polarity regression
task and the sentiment classification task. Polarity
regression maps the ?virtual? sentiment scores ob-
tained on graphs to the absolute sentiment scores
(on a scale from 0 to 1). The regression was per-
formed twice: once for the positive scores and
once for the negative scores. We evaluate the
performance of the polarity regression against the
Micro-WN(Op)-0 gold standard in terms of root
mean square error (RMSE). We used the aver-
age of the labeled polarity scores (positive and
negative) of all monosentimous words in Micro-
WN(Op)-1 as a baseline for this task.
Sentiment classification uses the scores ob-
tained on graphs as features in order to assign
each word with one of the three sentiment la-
bels (positive, negative, and neutral). The clas-
sification performance is evaluated in terms of
micro-F1 measure. The labels for the classifica-
tion are assigned according to the positivity and
negativity scores (the label neutral is assigned if
Obj (s) = 1?Pos(s)?Neg(s) is larger than both
Pos(s) and Neg(s)). The majority class predictor
was used as a baseline for the classification task.
Due to the small size of the labeled sets (e.g.,
225 for Micro-WN(Op)-1) we performed the 10
? 10 CV evaluation (10 cross-validation trials,
each on randomly permuted data) (Bouckaert,
2003) both for regression and classification. For
comparison, we evaluated the SentiWordNet in
the same way ? we averaged the SentiWordNet
scores for all the senses of monosentimous words
from the Micro-WN(Op)-1.
Although the semi-supervised step itself was
not designed to deal with polarity regression task
and sentiment classification task, we decided to
evaluate the results gained from graphs on these
tasks as well. This gives us an insight to how
much the supervised step adds in terms of perfor-
mance. The positivity and negativity scores ob-
tained from graphs were directly evaluated on the
regression task measuring the RMSE against the
gold standard. Classification labels were deter-
7
mined by comparing the positive rank of the word
against the negative rank of the word. The word
was classified as neutral if the absolute difference
between its positive and negative rank was below
the given treshold t. Empirically determined opti-
mal value of the treshold was t = 1000.
Table 2 we present the results of the hybrid
method on both the regression (for both positive
and negative scores) and classification tasks com-
pared with the performance of the SentiWordNet
and the baselines. Additionally, we present the
results obtained using only the semi-supervised
step. On both the regression and classification
task our method outperforms the baseline. The
performance is comparable to SentiWordNet on
the sentiment classification task. However, the
performance of our corpus-based approach is sig-
nificantly lower than SentiWordNet on the polar-
ity regression task ? a more detailed analysis is
required to determine the cause of this. The hy-
brid approach performs significantly better than
the semi-supervised method alone, confirming the
importance of the supervised step.
Models trained on the Micro-WN(Op)-1 were
applied on the set of words from the Micro-
WN(Op)-0 not present in the Micro-WN(Op)-1
(i.e., the difference between the two sets) in order
to test the performance on non-monosentimous
words. The obtained results on this set are, sur-
prisingly, slightly better (positivity regression ?
0.337; negativity regression ? 0.313; and classi-
fication ? 57.55%). This is most likely due to the
fact that, although not all senses have the same
sentiment, most of them have similar sentiment,
which is often also the sentiment of the dominant
sense in the corpus.
6 Conclusion
We have described a hybrid approach to sentiment
lexicon acquisition from corpus. On one hand, the
approach combines corpus-based lexical seman-
tics with graph-based label propagation, while on
the other hand it combines semi-supervised and
supervised learning. We have evaluated the per-
formance on three sentiment prediction tasks: po-
larity ranking task, polarity regression task, and
sentiment classification task. Our experiments
suggest that the results on the polarity ranking
task are comparable to SentiWordNet. On the
sentiment classification task, the results are also
comparable to SentiWordNet when restricted to
monosentimous words. On the polarity regression
task, our results are worse than SentiWordNet, al-
though still above the baseline.
Unlike with the WordNet-based approaches, in
which sentiment is predicted based on sentiment-
preserving semantic relations between synsets,
the corpus-based approach operates at the level
of words and thus suffers from two major limi-
tations. Firstly, the semantic relations extracted
from corpus are inherently unstructured, vague,
and ? besides paradigmatic relations ? also in-
clude syntagmatic and very loose topical rela-
tions. Thus, sentiment labels propagate in a less
controlled manner and get influenced more easily
by the context. For example, words ?understand-
able? and ?justifiable? get labeled as predomi-
nately negative, because they usually occur in
negative contexts. Secondly, in the approach we
described, polysemy is not accounted for, which
introduces sentiment prediction errors for words
that are not monosentimous. It remains to be
seen whether this could be remedied by employ-
ing WSD prior to sentiment lexicon acquisition.
For future work we intend to investigate how
syntax-based information can be used to intro-
duce more semantic structure into the graph.
We will experiment with other hybridization ap-
proaches that combine semantic links from Word-
Net with corpus-derived semantic relations.
Acknowledgments
We thank the anonymous reviewers for their
useful comments. This work has been sup-
ported by the Ministry of Science, Education and
Sports, Republic of Croatia under the Grant 036-
1300646-1986.
References
E. Agirre, D. Mart??nez, O.L. de Lacalle, and A. Soroa.
2006. Two graph-based algorithms for state-of-the-
art wsd. In Proc. of the 2006 Conference on Em-
pirical Methods in Natural Language Processing,
pages 585?593. Association for Computational Lin-
guistics.
A. Andreevskaia and S. Bergler. 2006. Mining word-
net for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In Proc. of EACL, volume 6,
pages 209?216.
S. Baccianella, A. Esuli, and F. Sebastiani. 2010.
Sentiwordnet 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In
8
Table 2: The performance on the polarity regression task and sentiment classification task
Regression (RMSE) Classification (micro-F1)
Positivity Negativity
Hybrid approach 0.363 ? 0.005 0.387 ? 0.003 0.548 ? 0.126
Baseline 0.383 0.413 0.427
Semi-supervised 0.443 0.466 0.484
SentiWordNet 0.284 0.294 0.582
Proc. of the Seventh International Conference on
Language Resources and Evaluation (LREC?10),
Valletta, Malta. European Language Resources As-
sociation (ELRA).
R.R. Bouckaert. 2003. Choosing between two
learning algorithms based on calibrated tests.
In Machine learning-International workshop then
conference-, volume 20, pages 51?58.
S. Cerini, V. Compagnoni, A. Demontis, M. For-
mentelli, and G. Gandini. 2007. Micro-WNOp:
A gold standard for the evaluation of automati-
cally compiled lexical resources for opinion mining.
Language resources and linguistic theory: Typol-
ogy, second language acquisition, English linguis-
tics, pages 200?210.
K.W. Church and P. Hanks. 1990. Word associa-
tion norms, mutual information, and lexicography.
Computational linguistics, 16(1):22?29.
S.T. Dumais. 2004. Latent semantic analysis. An-
nual Review of Information Science and Technol-
ogy, 38(1):188?230.
A. Esuli and F. Sebastiani. 2007. Pageranking word-
net synsets: An application to opinion mining. In
Annual meeting-association for computational lin-
guistics, volume 45, pages 424?431.
S. Evert. 2008. Corpora and collocations. Cor-
pus Linguistics. An International Handbook, pages
1212?1248.
R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and
E. Vee. 2004. Comparing and aggregating rank-
ings with ties. In Proc. of the twenty-third ACM
SIGMOD-SIGACT-SIGART symposium on Princi-
ples of database systems, pages 47?58. ACM.
C. Fellbaum. 2010. Wordnet. Theory and Applica-
tions of Ontology: Computer Applications, pages
231?243.
A.B. Goldberg and X. Zhu. 2006. Seeing stars
when there aren?t many stars: graph-based semi-
supervised learning for sentiment categorization. In
Proc. of the First Workshop on Graph Based Meth-
ods for Natural Language Processing, pages 45?52.
Association for Computational Linguistics.
V. Hatzivassiloglou and K.R. McKeown. 1997. Pre-
dicting the semantic orientation of adjectives. In
Proc. of the eighth conference on European chap-
ter of the Association for Computational Linguis-
tics, pages 174?181. Association for Computational
Linguistics.
M. Hu and B. Liu. 2004. Mining opinion features in
customer reviews. In Proc. of the National Confer-
ence on Artificial Intelligence, pages 755?760.
J. Kamps, MJ Marx, R.J. Mokken, and M. De Rijke.
2004. Using WordNet to measure semantic orienta-
tions of adjectives.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1999.
The PageRank citation ranking: Bringing order to
the web.
E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Feature
subsumption for opinion analysis. In Proc. of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 440?448. Association
for Computational Linguistics.
M. Sahlgren. 2006. The Word-Space Model: Us-
ing Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations between Words
in High-Dimensional Vector Spaces. Ph.D. thesis,
Stockholm University, Stockholm, Sweden.
S. Somasundaran, T. Wilson, J. Wiebe, and V. Stoy-
anov. 2007. Qa with attitude: Exploiting opinion
type analysis for improving question answering in
on-line discussions and the news. In Proc. of the In-
ternational Conference on Weblogs and Social Me-
dia (ICWSM). Citeseer.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and
M. Stede. 2011. Lexicon-based methods for sen-
timent analysis. Computational Linguistics, (Early
Access):1?41.
P. Turney and M.L. Littman. 2003. Measuring praise
and criticism: Inference of semantic orientation
from association. In ACM Transactions on Infor-
mation Systems (TOIS).
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing contextual polarity: an exploration of fea-
tures for phrase-level sentiment analysis. Computa-
tional Linguistics, 35(3):399?433.
X. Zhu and A.B. Goldberg. 2009. Introduction to
semi-supervised learning. Synthesis lectures on ar-
tificial intelligence and machine learning, 3(1):1?
130.
9
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 18?23,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Aspect-Oriented Opinion Mining from User Reviews in Croatian
Goran Glava?? Damir Korenc?ic?? Jan ?najder?
?University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
?Rud?er Bo?kovic? Institute, Department of Electronics
Bijenic?ka cesta 54, 10000 Zagreb, Croatia
{goran.glavas,jan.snajder}@fer.hr damir.korencic@irb.hr
Abstract
Aspect-oriented opinion mining aims to
identify product aspects (features of prod-
ucts) about which opinion has been ex-
pressed in the text. We present an approach
for aspect-oriented opinion mining from
user reviews in Croatian. We propose meth-
ods for acquiring a domain-specific opinion
lexicon, linking opinion clues to product
aspects, and predicting polarity and rating
of reviews. We show that a supervised ap-
proach to linking opinion clues to aspects
is feasible, and that the extracted clues and
aspects improve polarity and rating predic-
tions.
1 Introduction
For companies, knowing what customers think of
their products and services is essential. Opinion
mining is being increasingly used to automatically
recognize opinions about products in natural lan-
guage texts. Numerous approaches to opinion min-
ing have been proposed, ranging from domain-
specific (Fahrni and Klenner, 2008; Qiu et al, 2009;
Choi et al, 2009) to cross-domain approaches (Wil-
son et al, 2009; Taboada et al, 2011), and from
lexicon-based methods (Popescu and Etzioni, 2007;
Jijkoun et al, 2010; Taboada et al, 2011) to ma-
chine learning approaches (Boiy and Moens, 2009;
Go et al, 2009).
While early attempts focused on classifying
overall document opinion (Turney, 2002; Pang et
al., 2002), more recent approaches identify opin-
ions expressed about individual product aspects
(Popescu and Etzioni, 2007; Fahrni and Klenner,
2008; Mukherjee and Liu, 2012). Identifying opin-
ionated aspects allows for aspect-based comparison
across reviews and enables opinion summarization
for individual aspects. Furthermore, opinionated
aspects may be useful for predicting overall review
polarity and rating.
While many opinion mining systems and re-
sources have been developed for major languages,
there has been considerably less development for
less prevalent languages, such as Croatian. In this
paper we present a method for domain-specific,
aspect-oriented opinion mining from user reviews
in Croatian. We address two tasks: (1) identifica-
tion of opinion expressed about individual product
aspects and (2) predicting the overall opinion ex-
pressed by a review. We assume that solving the
first task successfully will help improve the perfor-
mance on the second task. We propose a simple
semi-automated approach for acquiring domain-
specific lexicon of opinion clues and prominent
product aspects. We use supervised machine learn-
ing to detect the links between opinion clues (e.g.,
excellent, horrible) and product aspects (e.g., pizza,
delivery). We conduct preliminary experiments on
restaurant reviews and show that our method can
successfully pair opinion clues with the targeted
aspects. Furthermore, we show that the extracted
clues and opinionated aspects help classify review
polarity and predict user-assigned ratings.
2 Related Work
Aspect-based opinion mining typically consists
of three subtasks: sentiment lexicon acquisition,
aspect-clue pair identification, and overall review
opinion prediction. Most approaches to domain-
specific sentiment lexicon acquisition start from a
manually compiled set of aspects and opinion clues
and then expand it with words satisfying certain
co-occurrence or syntactic criteria in a domain-
specific corpus (Kanayama and Nasukawa, 2006;
Popescu and Etzioni, 2007; Fahrni and Klenner,
2008; Mukherjee and Liu, 2012). Kobayashi et
18
al. (2007) extract aspect-clue pairs from weblog
posts using a supervised model with parts of de-
pendency trees as features. Kelly et al (2012)
use a semi-supervised SVM model with syntactic
features to classify the relations between entity-
property pairs. Opinion classification of reviews
has been approached using supervised text cate-
gorization techniques (Pang et al, 2002; Funk et
al., 2008) and semi-supervised methods based on
the similarity between unlabeled documents and a
small set of manually labeled documents or clues
(Turney, 2002; Goldberg and Zhu, 2006).
Sentiment analysis and opinion mining ap-
proaches have been proposed for several Slavic
languages (Chetviorkin et al, 2012; Buczynski and
Wawer, 2008; Smr?, 2006; Smailovic? et al, 2012).
Methods that rely on translation, using resources
developed for major languages, have also been pro-
posed (Smr?, 2006; Steinberger et al, 2012). Thus
far, there has been little work on opinion mining
for Croatian. Glava? et al (2012) use graph-based
algorithms to acquire a sentiment lexicon from a
newspaper corpus. Agic? et al (2010) describe a
rule-based method for detecting polarity phrases
in financial domain. To the best of our knowledge,
our work is the first that deals with aspect-oriented
opinion mining for Croatian.
3 Aspect-Oriented Opinion Mining
Our approach consists of three steps: (1) acquisi-
tion of an opinion lexicon of domain-specific opin-
ion clues and product aspects, (2) recognition of
aspects targeted by opinion clues, and (3) predic-
tion of overall review polarity and opinion rating.
The linguistic preprocessing includes sentence
segmentation, tokenization, lemmatization, POS-
tagging, and dependency parsing. We use the in-
flectional lexicon from ?najder et al (2008) for
lemmatization, POS tagger from Agic? et al (2008),
and dependency parser from Agic? (2012). As we
are dealing with noisy user-generated text, prior to
any of these steps, we use GNU Aspell tool1 for
spelling correction.
Step 1: Acquisition of the opinion lexicon. We
use a simple semi-automatic method to acquire
opinion clues and aspects. We identify candidates
for positive clues as lemmas that appear much more
frequently in positive than in negative reviews (we
determine review polarity based on user-assigned
1http://aspell.net/
rating). Analogously, we consider as negative
clue candidates lemmas that occur much more fre-
quently in negative than in positive reviews. As-
suming that opinion clues target product aspects,
we extract as aspect candidates all lemmas that
frequently co-occur with opinion clues. We then
manually filter out the false positives from the lists
of candidate clues and aspects.
Unlike some approaches (Popescu and Etzioni,
2007; Kobayashi et al, 2007), we do not require
that clues or aspects belong to certain word cate-
gories or to a predefined taxonomy. Our approach
is pragmatic ? clues are words that express opin-
ions about aspects, while aspects are words that
opinion clues target. For example, we treat words
like stic?i (to arrive) and sve (everything) as aspects,
because they can be targets of opinion clues, as in
?pizza je stigla kasno" (?pizza arrived late") and
?sve super!" (?everything?s great!").
Step 2: Identifying opinionated aspects. We
aim to pair in each sentence the aspects with the
opinion clues that target them. For example, in
?dobra pizza, ali lazanje su u?asne" (?good pizza,
but lasagna was terrible"), the clue dobra (good)
should be paired with the aspect pizza, and u?asne
(terrible) should be paired with lazanje (lasagne).
In principle, the polarity of an opinion is deter-
mined by both the opinion clue and the aspect. At
an extreme, an aspect can invert the prior polarity
of an opinion clue (e.g., ?cold pizza" has a negative,
whereas ?cold ice-cream" has a positive polarity).
However, given that no such cases occurred in our
dataset, we chose not to consider this particular
type of inversion. On the other hand, the polarity
of an opinion may be inverted explicitly by the use
of negations. To account for this, we use a very
simple rule to recognize negations: we consider an
aspect-clue pair to be negated if there is a negation
word within a?3 token window of the opinion clue
(e.g., ?pizza im nikad nije hladna" ? ?their pizza is
never cold").
To identify the aspect-clue pairs, we train a super-
vised model that classifies all possible pairs within
a sentence as either paired or not paired. We use
four sets of features:
(1) Basic features: the distance between the as-
pect and the clue (in number of tokens); the number
of aspects and clues in the sentence; the sentence
length (in number of tokens); punctuation, other
aspects, and other clues in between the aspect and
the clue; the order of the aspect and the clue (i.e.,
19
which one comes before);
(2) Lexical features: the aspect and clue lemmas;
bag-of-words in between the aspect and the clue; a
feature indicating whether the aspect is conjoined
with another aspect (e.g., ?pizza i sendvic? su bili
izvrsni" ? ?pizza and sandwich were amazing");
a feature indicating whether the clue is conjoined
with another clue (e.g., ?velika i slasna pizza" ?
?large and delicious pizza");
(3) Part-of-speech features: POS tags of the as-
pect and the clue word; set of POS tags in between
the aspect and the clue; set of POS tags preced-
ing the aspect/clue; set of POS tags following the
aspect/clue; an agreement of gender and number
between the aspect and the clue;
(4) Syntactic dependency features: dependency
relation labels along the path from the aspect to the
clue in the dependency tree (two features: a con-
catenation of these labels and a set of these labels);
a feature indicating whether the given aspect is syn-
tactically the closest to the given clue; a feature
indicating whether the given clue is syntactically
the closest to given aspect.
Step 3: Predicting overall review opinion. We
use extracted aspects, clues, and aspect-clue pairs
to predict the overall review opinion. We consider
two separate tasks: (1) prediction of review po-
larity (positive or negative) and (2) prediction of
user-assigned rating that accompanies a review. We
frame the first task as a binary classification prob-
lem, and the second task as a regression problem.
We use the following features for both tasks:
(1) Bag-of-word (BoW): the standard tf-idf
weighted BoW representation of the review;
(2) Review length: the number of tokens in the
review (longer reviews are more likely to contain
more opinion clues and aspects);
(3) Emoticons: the number of positive (e.g.,
?:)?) and negative emoticons (e.g., ?:(?);
(4) Opinion clue features: the number and the
lemmas of positive and negative opinion clues;
(5) Opinionated aspect features: the number and
the lemmas of positively and negatively opinion-
ated aspects.
4 Evaluation
For experimental evaluation, we acquired a
domain-specific dataset of restaurant reviews2 from
2Available under CC BY-NC-SA license from
http://takelab.fer.hr/cropinion
(HR) Zaista za svaku pohvalu! Jelo su nam dostavili
15 minuta ranije. Naruc?ili smo pizzu koja je bila
prepuna dodataka, dobro pec?ena, i vrlo ukusna.
(EN) Really laudable! Food was delivered 15 minutes
early. We ordered pizza which was filled with ex-
tras, well-baked, and very tasteful.
Rating: 6/6
Table 1: Example of a review (text and rating)
Pauza.hr,3 Croatia?s largest food ordering website.
The dataset contains 3310 reviews, totaling about
100K tokens. Each review is accompanied by an
opinion rating on a scale from 0.5 (worst) to 6
(best). The average user rating is 4.5, with 74%
of comments rated above 4. We use these user-
assigned ratings as gold-standard labels for super-
vised learning. Table 1 shows an example of a
review (clues are bolded and aspects are under-
lined). We split the dataset into a development and
a test set (7:3 ratio) and use the former for lexicon
acquisition and model training.
Experiment 1: Opinionated aspects. To build
a set on which we can train the aspect-clue pair-
ing model, we sampled 200 reviews from the de-
velopment set and extracted from each sentence
all possible aspect-clue pairs. We obtained 1406
aspect-clue instances, which we then manually la-
beled as either paired or not paired. Similarly for
the test set, we annotated 308 aspect-clue instances
extracted from a sample of 70 reviews. Among
the extracted clues, 77% are paired with at least
one aspect and 23% are unpaired (the aspect is
implicit).
We trained a support vector machine (SVM) with
radial basis kernel and features described in Section
3. We optimized the model using 10-fold cross-
validation on the training set. The baseline assigns
to each aspect the closest opinion clue within the
same sentence. We use stratified shuffling test (Yeh,
2000) to determine statistical significance of per-
formance differences.
Results are shown in Table 2. All of our
supervised models significantly outperform the
closest clue baseline (p < 0.01). The Ba-
sic+Lex+POS+Synt model outperforms Basic
model (F-score difference is statistically significant
at p < 0.01), while the F-score differences between
Basic and both Basic+Lex and Basic+Lex+POS
are pairwise significant at p < 0.05. The F-score
3http://pauza.hr/
20
Model Precision Recall F1
Baseline 31.8 71.0 43.9
Basic 77.2 76.1 76.6
Basic+Lex 78.1 82.6 80.3
Basic+Lex+POS 80.9 79.7 80.3
Basic+Lex+POS+Synt 84.1 80.4 82.2
Table 2: Aspect-clue pairing performance
Review polarity Review rating
Model Pos Neg Avg r MAE
BoW 94.1 79.1 86.6 0.74 0.94
BoW+E 94.4 80.3 87.4 0.75 0.91
BoW+E+A 95.7 85.2 90.5 0.80 0.82
BoW+E+C 95.7 85.6 90.7 0.81 0.79
BoW+E+A+C 96.0 86.2 91.1 0.83 0.76
E ? emoticons; A ? opinionated aspects; C ? opinion clues
Table 3: Review polarity and rating performance
differences between Basic+Lex, Basic+Lex+POS,
and Basic+Lex+POS+Synt are pairwise not statis-
tically significant (p < 0.05). This implies that
linguistic features increase the classification per-
formance, but there are no significant differences
between models employing different linguistic fea-
ture sets. We also note that improvements over the
Basic model are not as large as we expected; we
attribute this to the noisy user-generated text and
the limited size of the training set.
Experiment 2: Overall review opinion. We
considered two models: a classification model for
predicting review polarity and a regression model
for predicting user-assigned rating. We trained the
models on the full development set (2276 reviews)
and evaluated on the full test set (1034 reviews).
For the classification task, we consider reviews
rated lower than 2.5 as negative and those rated
higher than 4 as positive. Ratings between 2.5 and
4 are mostly inconsistent (assigned to both positive
and negative reviews), thus we did not consider
reviews with these ratings. For classification, we
used SVM with radial basis kernel, while for re-
gression we used support vector regression (SVR)
model. We optimized both models using 10-fold
cross-validation on the training set.
Table 3 shows performance of models with dif-
ferent feature sets. The model with bag-of-words
features (BoW) is the baseline. For polarity classi-
fication, we report F1-scores for positive and nega-
tive class. For rating prediction, we report Pearson
correlation (r) and mean average error (MAE).
The models that use opinion clue features
(BoW+E+C) or opinionated aspect features
(BoW+E+A and BoW+E+A+C) outperform the
baseline model (difference in classification and re-
gression performance is significant at p < 0.05
and p < 0.01, respectively; tested using stratified
shuffling test). This confirms our assumption that
opinion clues and opinionated aspects improve the
prediction of overall review opinion. Performance
on negative reviews is consistently lower than for
positive reviews; this can be ascribed to the fact
that the dataset is biased toward positive reviews.
Models BoW+E+A and BoW+E+C perform simi-
larly (the difference is not statistically significant at
p < 0.05), suggesting that opinion clues improve
the performance just as much as opinionated as-
pects. We believe this is due to (1) the existence of
a considerable number (23%) of unpaired opinion
clues (e.g., u?asno (terrible) in ?Bilo je u?asno!"
(?It was terrible!")) and (2) the fact that most opin-
ionated aspects inherit the prior polarity of the clue
that targets them (also supported by the fact the
BoW+E+A+C model does not significantly outper-
form the BoW+E+C nor the BoW+E+A models).
Moreover, note that, in general, user-assigned rat-
ings may deviate from the opinions expressed in
text (e.g., because some users chose to comment
only on some aspects). However, the issue of an-
notation quality is out of scope and we leave it for
future work.
5 Conclusion
We presented a method for aspect-oriented opinion
mining from user reviews in Croatian. We proposed
a simple, semi-automated approach for acquiring
product aspects and domain-specific opinion clues.
We showed that a supervised model with linguistic
features can effectively assign opinions to the in-
dividual product aspects. Furthermore, we demon-
strated that opinion clues and opinionated aspects
improve prediction of overall review polarity and
user-assigned opinion rating.
For future work we intend to evaluate our
method on other datasets and domains, varying
in level of language complexity and correctness.
Of particular interest are the domains with aspect-
focused ratings and reviews (e.g., electronic prod-
uct reviews). Aspect-based opinion summarization
is another direction for future work.
21
Acknowledgments
This work has been supported by the Ministry of
Science, Education and Sports, Republic of Croatia
under the Grant 036-1300646-1986 and Grant 098-
0982560-2566.
References
?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2008. Improving part-of-speech tagging accuracy
for Croatian by morphological analysis. Informat-
ica, 32(4):445?451.
?eljko Agic?, Nikola Ljube?ic?, and Marko Tadic?. 2010.
Towards sentiment analysis of financial texts in
Croatian. In Nicoletta Calzolari, editor, Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC?10), Val-
letta, Malta. European Language Resources Associ-
ation (ELRA).
?eljko Agic?. 2012. K-best spanning tree dependency
parsing with verb valency lexicon reranking. In Pro-
ceedings of 24th international Conference on Com-
putational Linguistics (COLING 2012): Posters,
pages 1?12.
Erik Boiy and Marie-Francine Moens. 2009. A
machine learning approach to sentiment analysis
in multilingual web texts. Information retrieval,
12(5):526?558.
Aleksander Buczynski and Aleksander Wawer. 2008.
Shallow parsing in sentiment analysis of product re-
views. In Proceedings of the Partial Parsing work-
shop at LREC, pages 14?18.
Ilia Chetviorkin, Pavel Braslavskiy, and Natalia
Loukachevich. 2012. Sentiment analysis track at
romip 2011. Dialog.
Yoonjung Choi, Youngho Kim, and Sung-Hyon
Myaeng. 2009. Domain-specific sentiment analysis
using contextual feature generation. In Proceedings
of the 1st international CIKM workshop on Topic-
sentiment analysis for mass opinion, pages 37?44.
ACM.
Angela Fahrni and Manfred Klenner. 2008. Old wine
or warm beer: Target-specific sentiment analysis of
adjectives. In Proc. of the Symposium on Affective
Language in Human and Machine, AISB, pages 60?
63.
Adam Funk, Yaoyong Li, Horacio Saggion, Kalina
Bontcheva, and Christian Leibold. 2008. Opin-
ion analysis for business intelligence applications.
In Proceedings of the first international workshop
on Ontology-supported business intelligence, page 3.
ACM.
Goran Glava?, Jan ?najder, and Bojana Dalbelo Ba?ic?.
2012. Semi-supervised acquisition of Croatian sen-
timent lexicon. In Text, Speech and Dialogue, pages
166?173. Springer.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Andrew B Goldberg and Xiaojin Zhu. 2006. See-
ing stars when there aren?t many stars: Graph-based
semi-supervised learning for sentiment categoriza-
tion. In Proceedings of the First Workshop on Graph
Based Methods for Natural Language Processing,
pages 45?52. Association for Computational Lin-
guistics.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 585?594. Association for
Computational Linguistics.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?06, pages 355?363,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Colin Kelly, Barry Devereux, and Anna Korhonen.
2012. Semi-supervised learning for automatic con-
ceptual property extraction. In Proceedings of the
3rd Workshop on Cognitive Modeling and Com-
putational Linguistics, CMCL ?12, pages 11?20,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
1065?1074.
Arjun Mukherjee and Bing Liu. 2012. Aspect ex-
traction through semi-supervised modeling. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Long Papers-
Volume 1, pages 339?348. Association for Computa-
tional Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79?86. Asso-
ciation for Computational Linguistics.
Ana-Maria Popescu and Oren Etzioni. 2007. Extract-
ing product features and opinions from reviews. In
Natural language processing and text mining, pages
9?28. Springer.
22
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through dou-
ble propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
pages 1199?1204.
Jasmina Smailovic?, Miha Grc?ar, and Martin ?nidar?ic?.
2012. Sentiment analysis on tweets in a financial do-
main. In Proceedings of the 4th Jozef Stefan Inter-
national Postgraduate School Students Conference,
pages 169?175.
Pavel Smr?. 2006. Using WordNet for opinion mining.
In Proceedings of the Third International WordNet
Conference, pages 333?335. Masaryk University.
Jan ?najder, Bojana Dalbelo Ba?ic?, and Marko Tadic?.
2008. Automatic acquisition of inflectional lexica
for morphological normalisation. Information Pro-
cessing & Management, 44(5):1720?1731.
Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova,
Ralf Steinberger, Hristo Tanev, Silvia V?zquez, and
Vanni Zavarella. 2012. Creating sentiment dictio-
naries via triangulation. Decision Support Systems.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Peter D Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th an-
nual meeting on association for computational lin-
guistics, pages 417?424. Association for Computa-
tional Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational linguistics, 35(3):399?433.
Alexander Yeh. 2000. More accurate tests for the
statistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics-Volume 2, pages 947?953. Association
for Computational Linguistics.
23
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 24?33,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Frequently Asked Questions Retrieval for Croatian
Based on Semantic Textual Similarity
Mladen Karan? Lovro ?mak? Jan ?najder?
?University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
?Studio Artlan, Andrije ?tangera 18, 51410 Opatija, Croatia
{mladen.karan,jan.snajder}@fer.hr lovro.zmak@studioartlan.hr
Abstract
Frequently asked questions (FAQ) are an
efficient way of communicating domain-
specific information to the users. Unlike
general purpose retrieval engines, FAQ re-
trieval engines have to address the lexi-
cal gap between the query and the usu-
ally short answer. In this paper we de-
scribe the design and evaluation of a FAQ
retrieval engine for Croatian. We frame
the task as a binary classification prob-
lem, and train a model to classify each
FAQ as either relevant or not relevant for
a given query. We use a variety of se-
mantic textual similarity features, includ-
ing term overlap and vector space features.
We train and evaluate on a FAQ test col-
lection built specifically for this purpose.
Our best-performing model reaches 0.47
of mean reciprocal rank, i.e., on average
ranks the relevant answer among the top
two returned answers.
1 Introduction
The amount of information available online is
growing at an exponential rate. It is becoming in-
creasingly difficult to navigate the vast amounts
of data and isolate relevant pieces of informa-
tion. Thus, providing efficient information access
for clients can be essential for many businesses.
Frequently asked questions (FAQ) databases are a
popular way to present domain-specific informa-
tion in the form of expert answers to users ques-
tions. Each FAQ consists of a question and an
answer, possibly complemented with additional
metadata (e.g., keywords). A FAQ retrieval engine
provides an interface to a FAQ database. Given a
user query in natural language as input, it retrieves
a ranked list of FAQs relevant to the query.
FAQ retrieval can be considered half way be-
tween traditional document retrieval and question
answering (QA). Unlike in full-blown QA, in FAQ
retrieval the questions and the answers are already
extracted. On the other hand, unlike in document
retrieval, FAQ queries are typically questions and
the answers are typically much shorter than doc-
uments. While FAQ retrieval can be approached
using simple keyword matching, the performance
of such systems will be severely limited due to the
lexical gap ? a lack of overlap between the words
that appear in a query and words from a FAQ pair.
As noted by Sneiders (1999), there are two causes
for this. Firstly, the FAQ database creators in gen-
eral do not know the user questions in advance.
Instead, they must guess what the likely questions
would be. Thus, it is very common that users? in-
formation needs are not fully covered by the pro-
vided questions. Secondly, both FAQs and user
queries are generally very short texts, which di-
minishes the chances of a keyword match.
In this paper we describe the design and the
evaluation of a FAQ retrieval engine for Croat-
ian. To address the lexical gap problem, we take
a supervised learning approach and train a model
that predicts the relevance of a FAQ given a query.
Motivated by the recent work on semantic textual
similarity (Agirre et al, 2012), we use as model
features a series of similarity measures based on
word overlap and semantic vector space similar-
ity. We train and evaluate the model on a FAQ
dataset from a telecommunication domain. On this
dataset, our best performing model achieves 0.47
of mean reciprocal rank, i.e., on average ranks the
relevant FAQ among the top two results.
In summary, the contribution of this paper is
twofold. Firstly, we propose and evaluate a
FAQ retrieval model based on supervised machine
learning. To the best of our knowledge, no previ-
24
ous work exists that addresses IR for Croatian in
a supervised setting. Secondly, we build a freely
available FAQ test collection with relevance judg-
ments. To the best of our knowledge, this is the
first IR test collection for Croatian.
The rest of the paper is organized as follows.
In the next section we give an overview of related
work. In Section 3 we describe the FAQ test col-
lection, while in Section 4 we describe the retrieval
model. Experimental evaluation is given in Sec-
tion 5. Section 6 concludes the paper and outlines
future work.
2 Related Work
Most prior work on FAQ retrieval has focused on
the problem of lexical gap, and various approaches
have been proposed for bridging it. Early work,
such as Sneiders (1999), propose to manually en-
rich the FAQ databases with additional meta data
such as the required, optional, and forbidden key-
words and keyphrases. This effectively reduces
FAQ retrieval to simple keyword matching, how-
ever in this case it is the manually assigned meta-
data that bridges the lexical gap and provides the
look and feel of semantic search.
For anything but a small-sized FAQ database,
manual creation of metadata is tedious and cost
intensive, and in addition requires expert knowl-
edge. An alternative is to rely on general lin-
guistic resources. FAQ finder (Burke et al, 1997)
uses syntax analysis to identify phrases, and then
performs matching using shallow lexical semantic
knowledge from WordNet (Miller, 1995). Yet an-
other way to bridge the lexical gap is smoothing
via clustering, proposed by Kim and Seo (2006).
First, query logs are expanded with word defini-
tions from a machine readable dictionary. Subse-
quently, query logs are clustered, and query simi-
larity is computed against the clusters, instead of
against the individual FAQs. As an alternative to
clustering, query expansion is often used to per-
form lexical smoothing (Voorhees, 1994; Navigli
and Velardi, 2003).
In some domains a FAQ engine additionally
must deal with typing errors and noisy user-
generated content. An example is the FAQ re-
trieval for SMS messages, described by Kothari et
al. (2009) and Contractor et al (2010).
Although low lexical overlap is identified as
the primary problem in FAQ retrieval, sometimes
it is the high lexical overlap that also presents a
problem. This is particularly true for large FAQ
databases in which a non-relevant document can
?accidentally? have a high lexical overlap with
a query. Moreo et al (2012) address the prob-
lem of false positives using case based reason-
ing. Rather than considering only the words, they
use phrases (?differentiator expressions?) that dis-
criminate well between FAQs.
The approaches described so far are essentially
unsupervised. A number of supervised FAQ re-
trieval methods have been described in the litera-
ture. To bridge the lexical gap, Xue et al (2008)
use machine translation models to ?translate? the
user query into a FAQ. Their system is trained on
very large FAQ knowledge bases, such as Yahoo
answers. Soricut and Brill (2004) describe another
large-scale FAQ retrieval system, which uses lan-
guage and transformation models. A good gen-
eral overview of supervised approaches to ranking
tasks is the work by Liu (2009).
Our system falls into the category of supervised
methods. In contrast to the above-described ap-
proaches, we use a supervised model with word
overlap and semantic similarity features. Taking
into account that FAQs are short texts, we use fea-
tures that have been recently proposed for deter-
mining the semantic similarity between pairs of
sentences (?aric? et al, 2012). Because we train
our model to output a relevance score for each
document, our approach is essentially a pointwise
learning-to-rank approach (Qin et al, 2008).
3 Croatian FAQ test collection
The standard procedure for IR evaluation requires
a test collection consisting of documents, queries,
and relevance judgments. We additionally require
an annotated dataset to train the model. As there
currently exists no standard IR test collection for
Croatian, we decided to build a FAQ test collec-
tion from scratch. We use this collection for both
model training and retrieval evaluation.
To obtain a FAQ test collection, we crawled the
web FAQ of Vip,1 a Croatian mobile phone opera-
tor. For each FAQ, we retrieved both the question
and the answer. In the Vip FAQ database ques-
tions are categorized into several broad categories
(e.g., by type of service). For each FAQ, we also
extract the category name assigned to it. We ob-
tained a total of 1344 FAQs. After removing the
1http://www.vipnet.hr/
pitanja-i-odgovori/ (accessed Sep 2009)
25
Query FAQ question FAQ answer
Kako se spaja na internet? (How to con-
nect to the internet?)
?to mi je potrebno da bih spojio rac?u-
nalo i koristio se internetom? (What do
I need to connect my computer and use
the internet)
Morate spojiti rac?unalo sa Homebox
ured?ajem LAN kabelom. . . (You must
connect your computer to the Homebox
device using a LAN cable . . . )
Putujem izvan Hrvatske i ?elim koristiti
svoj Vip mobilni ured?aj. Koliko c?e me
to ko?tati? (I am traveling abroad and
want to use my Vip mobile device. How
much will this cost?)
Koja je mre?a najpovoljnija za razgov-
ore, a koja za slanje SMS i MMS
poruka u roamingu? (Which network is
the best for conversations, and which
one for SMS and MMS messages in
roaming?)
Cijene za odlazne pozive u inozemstvu
su najpovoljnije u mre?ama Vodafone
partnera. . . (Outgoing calls cost less on
networks of Vodafone partners . . . )
Kako pogledati e-mail preko mobitela?
(How to check e-mail using a mobile
phone?)
Koja je cijena kori?tenja BlackBerry
Office usluge? (What is the price of us-
ing the BlackBerry Office service?)
. . . business e-mail usluga urac?unata je u
cijenu. . . (. . . business e-mail is included
in the price . . . )
Table 1: Examples of relevant answers to queries from the dataset
duplicates, 1222 unique FAQ pairs remain.
Next, we asked ten annotators to create at least
twelve queries each. They were instructed to in-
vent queries that they think would be asked by real
users of Vip services. To ensure that the queries
are as original as possible, the annotators were not
shown the original FAQ database. Following Lyti-
nen and Tomuro (2002), after creating the queries,
the annotators were instructed to rephrase them.
We asked the annotators to make between three
and ten paraphrases of each query. The paraphrase
strategies suggested were the following: (1) turn a
query into a multi-sentence query, (2) change the
structure (syntax) of the query, (3) substitute some
words with synonyms, while leaving the structure
intact, (4) turn the query into a declarative sen-
tence, and (5) any combination of the above. The
importance of not changing the underlying mean-
ing of a query was particularly stressed.
The next step was to obtain the binary relevance
judgments for each query. Annotating relevance
for the complete FAQ database is not feasible, as
the total number of query-FAQ pairs is too large.
On the other hand, not considering some of the
FAQs would make it impossible to estimate re-
call. A feasible alternative is the standard pooling
method predominantly used in IR evaluation cam-
paigns (Voorhees, 2002). In the pooling method,
the top-k ranked results of each evaluated system
are combined into a single list, which is then an-
notated for relevance judgments. For a sufficiently
large k, the recall estimate will be close to real
recall, as the documents that are not in the pool
are likely to be non-relevant. We simulate this set-
ting using several standard retrieval models: key-
word search, phrase search, tf-idf, and language
modeling. The number of combined results per
query is between 50 and 150. To reduce the an-
notators? bias towards top-ranked examples, the
retrieved results were presented in random order.
For each query, the annotators gave binary judg-
ments (?relevant? or ?not relevant?) to each FAQ
from the pooled list; FAQs not in the pool are as-
sumed to be not relevant. Although the appropri-
ateness of binary relevance has been questioned
(e.g., by Kek?l?inen (2005)), it is still commonly
used for FAQ and QA collections (Wu et al, 2006;
Voorhees and Tice, 2000). Table 1 shows exam-
ples of queries and relevant FAQs.
The above procedure yields a set of pairs
(Qr, Frel ), where Qr is a set of query paraphrases
and Frel is the set of relevant FAQs for any query
paraphrase from Qr. The total number of such
pairs is 117. From this set we generate a set of
pairs (q, Frel ), where q ? Qr is a single query.
The total number of such pairs is 419, of which
327 have at least one answer (Frel 6= ?), while
92 are not answered (Frel = ?). In this work
we focus on optimizing the performance on an-
swered queries and leave the detection and han-
dling of unanswered queries for future work. The
average number of relevant FAQs for a query is
1.26, while on average each FAQ is relevant for
1.44 queries. Test collection statistics is shown in
Table 2. We make the test collection freely avail-
able for research purposes.2
For further processing, we lemmatized the
query and FAQ texts using the morphological lex-
icon from ?najder et al (2008). We removed the
stopwords using a list of 179 Croatian stopwords.
2Available under CC BY-SA-NC license from
http://takelab.fer.hr/faqir
26
Word counts Form
Min Max Avg Quest. Decl.
Queries 1 25 8 372 47
FAQ questions 4 63 7 287 4
FAQ answers 1 218 30 ? ?
Table 2: FAQ test collection statistics
We retained the stopwords that constitute a part of
a service name (e.g., the pronoun ?me? (?me?) in
?Nazovi me? (?Call me?)).
4 Retrieval model
The task of the retrieval model is to rank the FAQs
by relevance to a given query. In an ideal case,
the relevant FAQs will be ranked above the non-
relevant ones. The retrieval model we propose is
a confidence-rated classifier trained on binary rel-
evance judgments, which uses as features the se-
mantic textual similarity between the query and
the FAQ. For a given a query-FAQ pair, the clas-
sifier outputs whether the FAQ is relevant (posi-
tive) or irrelevant (negative) for the query. More
precisely, the classifier outputs a confidence score,
which can be interpreted as the degree of rele-
vance. Given a single query as input, we run the
classifier on all query-FAQ pairs to obtain the con-
fidence scores for all FAQs from the database. We
then use these confidence scores to produce the fi-
nal FAQ ranking.
The training set consists of pairs (q, f) from the
test collection, where q ? Qr is a query from
the set of paraphrase queries and f ? Frel is a
FAQ from the set of relevant FAQs for this query
(cf. Section 3). Each (q, f) pair represents a posi-
tive training instance. To create a negative training
instance, we randomly select a (q, f) pair from the
set of positive instances and substitute the relevant
FAQ f with a randomly chosen non-relevant FAQ
f ?. As generating all possible negative instances
would give a very imbalanced dataset, we chose to
generate only 2N negative instances, where N is
the number of positive instances. Because |Frel|
varies depending on query q, number of instances
N per query also varies; on average, N is 329.
To train the classifier, we compute a feature vec-
tor for each (q, f) instance. The features measure
the semantic textual similarity between q and f .
More precisely, the features measure (1) the sim-
ilarity between query q and the question from f
and (2) the similarity between query q and the an-
swer from f . Considering both FAQ question and
answer has proven to be beneficial (Tomuro and
Lytinen, 2004). Additionally, ngram overlap fea-
tures are computed between the query and FAQ
category name.
As the classification model, we use the Support
Vector Machine (SVM) with radial basis kernel.
We use the LIBSVM implementation from Chang
and Lin (2011).
4.1 Term overlap features
We expect that FAQ relevance to be positively cor-
related with lexical overlap between FAQ text and
the user query. We use several lexical overlap
features. Similar features have been proposed by
Michel et al (2011) for paraphrase classification
and by ?aric? et al (2012) for semantic textual sim-
ilarity.
Ngram overlap (NGO). Let T1 and T2 be the
sets of consecutive ngrams (e.g., bigrams) in the
first and the second text, respectively. NGO is de-
fined as
ngo(T1, T2) = 2?
(
|T1|
|T1 ? T2|
+
|T2|
|T1 ? T2|
)?1
(1)
NGO measures the degree to which the first text
covers the second and vice versa. The two scores
are combined via a harmonic mean. We compute
NGO for unigrams and bigrams.
IC weighted word overlap (ICNGO). NGO
gives equal importance to all words. In practice,
we expect some words to be more informative than
others. The informativeness of a word can be mea-
sured by its information content (Resnik, 1995),
defined as
ic(w) = ln
?
w??C freq(w
?
)
freq(w)
(2)
where C is the set of words from the corpus and
freq(w) is the frequency of word w in the corpus.
We use the HRWAC corpus from Ljube?ic? and Er-
javec (2011) to obtain the word counts.
Let S1 and S2 be the sets of words occurring
in the first and second text, respectively. The IC-
weighted word coverage of the second text by the
first text is given by
wwc(S1, S2) =
?
w?S1?S2 ic(w)?
w??S2
ic(w?)
(3)
We compute the ICNGO feature as the harmonic
mean of wwc(S1, S2) and wwc(S2, S1).
27
4.2 Vector space features
Tf-idf similarity (TFIDF). The tf-idf (term fre-
quency/inverse document frequency) similarity of
two texts is computed as the cosine similarity of
their tf-idf weighted bag-of-words vectors. The tf-
idf weights are computed on the FAQ test collec-
tion. Here we treat each FAQ (without distinction
between question, answer, and category parts) as a
single document.
LSA semantic similarity (LSA). Latent seman-
tic analysis (LSA), first introduced by Deerwester
et al (1990), has been shown to be very effective
for computing word and document similarity. To
build the LSA model, we proceed along the lines
of Karan et al (2012). We build the model from
Croatian web corpus HrWaC from Ljube?ic? and
Erjavec (2011). For lemmatization, we use the
morphological lexicon from ?najder et al (2008).
Prior to the SVD, we weight the matrix elements
with their tf-idf values. Preliminary experiments
showed that system performance remained satis-
factory when reducing the vector space to only 25
dimensions, but further reduction caused deterio-
ration. We use 25 dimensions in all experiments.
LSA represents the meaning of a w by a vector
v(w). Motivated by work on distributional seman-
tic compositionality (Mitchell and Lapata, 2008),
we compute the semantic representation of text T
as the semantic composition (defined as vector ad-
dition) of the individual words constituting T :
v(T ) =
?
w?T
v(w) (4)
We compute the similarity between texts T1 and
T2 as the cosine between v(T1) and v(T2).
IC weighted LSA similarity (ICLSA). In the
LSA similarity feature all words occurring in a text
are considered to be equally important when con-
structing the compositional vector, ignoring the
fact that some words are more informative than
others. To acknowledge this, we use information
content weights defined by (2) and compute the IC
weighted compositional vector of a text T as
c(T ) =
?
wi?T
ic(wi)v(wi) (5)
Aligned lemma overlap (ALO). This feature
measures the similarity of two texts by semanti-
cally aligning their words in a greedy fashion. To
compare texts T1 and T2, first all pairwise sim-
ilarities between words from T1 and words from
T2 are computed. Then, the most similar pair is
selected and removed from the list. The procedure
is repeated until all words are aligned. The aligned
pairs are weighted by the larger information con-
tent of the two words:
sim(w1, w2) = (6)
max(ic(w1), ic(w2))? ssim(w1, w2)
where ssim(w1, w2) is the semantic similarity of
words w1 and w2 computed as the cosine similar-
ity of their LSA vectors, and ic is the information
content given by (2). The overall similarity be-
tween two texts is defined as the sum of weighted
pair similarities, normalized by the length of the
longer text:
alo(T1, T2) =
?
(w1,w2)?P sim(w1, w2)
max(length(T1), length(T2))
(7)
where P is the set of aligned lemma pairs. A sim-
ilar measure is proposed by Lavie and Denkowski
(2009) for machine translation evaluation, and has
been found out to work well for semantic textual
similarity (?aric? et al, 2012).
4.3 Question type classification (QC)
Related work on QA (Lytinen and Tomuro, 2002)
shows that the accuracy of QA systems can be im-
proved by question type classification. The intu-
ition behind this is that different types of ques-
tions demand different types of answers. Conse-
quently, information about the type of answer re-
quired should be beneficial as a feature.
To explore this line of improvement, we train
a simple question classifier on a dataset from
Lombarovic? et al (2011). The dataset consists
of 1300 questions in Croatian, classified into six
classes: numeric, entity, human, description, lo-
cation, and abbreviation. Following Lombarovic?
et al (2011), we use document frequency to select
the most frequent 300 words and 600 bigrams to
use as features. An SVM trained on this dataset
achieves 80.16% accuracy in a five-fold cross-
validation. This is slightly worse than the best re-
sult from Lombarovic? et al (2011), however we
use a smaller set of lexical features. We use the
question type classifier to compute two features:
the question type of the query and the question
type of FAQ question.
28
Feature RM1 RM2 RM3 RM4 RM5
NGO + + + + +
ICNGO + + + + +
TFIDF ? + + + +
LSA ? ? + + +
ICLSA ? ? + + +
ALO ? ? + + +
QED ? ? ? + +
QC ? ? ? ? +
Table 4: Features used by our models
4.4 Query expansion dictionary (QED)
Our error analysis revealed that some false nega-
tives could easily be eliminated by expanding the
query with similar/related words. To this end, we
constructed a small, domain-specific query expan-
sion dictionary. We aimed to (1) mitigate minor
spelling variances, (2) make the high similarity of
some some cross-POS or domain-specific words
explicit, and (3) introduce a rudimentary ?world
knowledge? useful for the domain at hand. The fi-
nal dictionary contains 53 entries; Table 3 shows
some examples.
5 Evaluation
5.1 Experimental setup
Because our retrieval model is supervised, we
evaluate it using five-fold cross-validation on the
FAQ test collection. In each fold we train our sys-
tem on the training data as described in Section
4, and evaluate the retrieval performance on the
queries from the test set. While each (q, Frel) oc-
curs in the test set exactly once, the same FAQ may
occur in both the train and test set. Note that this
does not pose a problem because the query part of
the pair will differ (due to paraphrasing).
To gain a better understanding of which features
contribute the most to retrieval performance, we
created several models. The models use increas-
ingly complex feature sets; an overview is given
in Table 4. We leave exhaustive feature analysis
and selection for future work.
As a baseline to compare against, we use a stan-
dard tf-idf weighted retrieval model. This model
ranks the FAQs by the cosine similarity of tf-idf
weighted vectors representing the query and the
FAQ. When computing the vector of the FAQ pair,
the question, answer, and category name are con-
catenated into a single text unit.
Model P R F1
RM1 14.1 68.5 23.1
RM2 25.8 75.1 37.8
RM3 24.4 75.4 36.3
RM4 25.7 77.7 38.2
RM5 25.3 76.8 37.2
Table 5: Classification results
5.2 Results
Relevance classification performance. Recall
that we use a binary classifier as a retrieval model.
The performance of this classifier directly deter-
mines the performance of the retrieval system as a
whole. It is therefore interesting to evaluate clas-
sifier performance separately. To generate the test
set, in each of the five folds we sample from the
test set the query-FAQ instances using the proce-
dure described in Section 4 (N positive and 2N
negative instance).
Precision, recall, and F1-score for each model
are shown in Table 5. Model RM4 outperforms
the other considered models. Model RM5, which
additionally uses question type classification, per-
forms worse than RM4, suggesting that the ac-
curacy of question type classification is not suf-
ficiently high. Our analysis of the test collection
revealed that this can be attributed to a domain
mismatch: the questions (mobile phone opera-
tor FAQ) are considerably different than those on
which the question classifier was trained (factoid
general questions). Moreover, some of the queries
and questions in our FAQ test collection are not
questions at all (cf. Table 2); e.g., ?Popravak mo-
bitela.? (?Mobile phone repair.?). Consequently,
it is not surprising that question classification fea-
tures do not improve the performance.
Retrieval performance. Retrieval results of the
five considered models are given in Table 6. We
report the standard IR evaluation measures: mean
reciprocal rank (MRR), average precision (AP),
and R-precision (RP). The best performance was
obtained with RM4 model, which uses all features
except the question type. The best MRR result
of 0.479 (with standard deviation over five folds
of ?0.04) indicates that, on average, model RM4
ranks the relevant answer among top two results.
Performance of other models expectedly in-
crease with the complexity of features used. How-
ever, RM5 is again an exception, performing
worse than RM4 despite using additional question
29
Query word Expansion words Remark
face facebook A lexical mismatch that would often occur
ogranic?iti (to limit) ogranic?enje (limit) Cross POS similarity important in the domain explicit
cijena (price) tro?ak (cost), ko?tati (to cost) Synonyms very often used in the domain
inozemstvo (abroad) roaming (roaming) Introduces world knowledge
ADSL internet Related words often used in the domain
Table 3: Examples from query expansions dictionary
Model MRR MAP RP
Baseline 0.341 21.77 15.28
RM1 0.326 20.21 17.6
RM2 0.423 28.78 24.37
RM3 0.432 29.09 24.90
RM4 0.479 33.42 28.74
RM5 0.475 32.37 27.30
Table 6: Retrieval results
type features, for the reasons elaborated above.
Expectedly, classification performance and
retrieval performance are positively correlated
(cf. Tables 5 and 6). A noteworthy case is RM4,
which improves the F1-score by only 5% over
RM3, yet improves IR measures by more than
10%. This suggest that, in addition to improving
the classifier decisions, the QED boosts the confi-
dence scores of already correct decisions.
A caveat to the above analysis is the fact that
the query expansion dictionary was constructed
base on the cross-validation result. While only
a small amount of errors were corrected with the
dictionary, this still makes models RM4 and RM5
slightly biased to the given dataset. An objective
estimate of maximum performance on unseen data
is probably somewhere between RM3 and RM4.
5.3 Error analysis
By manual inspection of false positive and false
negative errors, we have identified several char-
acteristic cases that account for the majority of
highly ranked irrelevant documents.
Lexical interference. While a query does have
a significant lexical similarity with relevant FAQ
pairs, it also has (often accidental) lexical simi-
larity with irrelevant FAQs. Because the classifier
appears to prefer lexical overlap, such irrelevant
FAQs interfere with results by taking over some of
the top ranked positions from relevant pairs.
Lexical gap. Some queries ask a very similar
question to an existing FAQ from the database, but
paraphrase it in such a way that almost no lexical
overlap remains. Even though the effect of this is
partly mitigated by our semantic vector space fea-
tures, in extreme cases the relevant FAQs will be
ranked rather low.
Semantic gap. Taken to the extreme, a para-
phrase can change a query to the extent that it
not only introduces a lexical gap, but also a se-
mantic gap, whose bridging would require logi-
cal inference and world knowledge. An exam-
ple of such query is ?Postoji li moguc?nost ko-
ri?tenja Vip kartice u Australiji?? (?Is it possi-
ble to use Vip sim card in Australia??). The asso-
ciated FAQ question is ?Kako mogu saznati pos-
toji li GPRS/EDGE ili UMTS/HSDPA roaming u
zemlji u koju putujem?? (?How can I find out if
there is GPRS/EDGE or UMTS/SPA roaming in
the country to which I am going??).
Word matching errors. In some cases words
which should match do not. This is most often
the case when one of the words is missing from
the morphological lexicon, and thus not lemma-
tized. A case in point is the word ?Facebook?, or
its colloquial Croatian variants ?fejs? and ?face?,
along with their inflected forms. Handling this is
especially important because a significant number
of FAQs from our dataset contain such words. An
obvious solution would be to complement lemma-
tization with stemming.
5.4 Cutoff strategies
Our model outputs a list of all FAQs from the
database, ranked by relevance to the input query.
As low-ranked FAQs are mostly not relevant, pre-
senting the whole ranked list puts an unnecessary
burden on the user. We therefore explored some
strategies for limiting the number of results.
First N (FN). This simply returns the N best
ranked documents.
Measure threshold criterion (MTC). We define
a threshold on FAQ relevance score, and re-
30
Figure 1: Recall vs. average number of documents
retrieved (for various cutoff strategies)
turn only the FAQs for which the classifier
confidence is above a specified threshold.
Cumulative threshold criterion (CTC). We de-
fine a threshold for cumulative relevance
score. The top-ranked FAQs for which the
sum of classifier confidences is below the
threshold are returned.
Relative threshold criterion (RTC). Returns all
FAQs whose relevance is within the given
percentage of the top-ranked FAQ relevance.
A good cutoff strategy should on average re-
turn a smaller number of documents, while still re-
taining high recall. To reflect this requirement we
measure the recall vs. average number of retrieved
documents (Fig. 1). While there is no substantial
difference between the four strategies, MTC and
RTC perform similarly and slightly better than FN
and CTC. As the number of documents increases,
the differences between the different cutoff strate-
gies diminish.
5.5 Performance and scalability
We have implemented the FAQ engine using in-
house code in Java. The only external library used
is the Java version of LIBSVM. Regarding system
performance, the main bottleneck is in generating
the features. Since all features depend on the user
query, they cannot be precomputed. Computation-
ally most intensive feature is ALO (cf. Section
4.2), which requires computing a large number of
vector cosines.
The response time of our FAQ engine is accept-
able ? on our 1222 FAQs test collection, the re-
sults are retrieved within one second. However, to
retrieve the results, the engine must generate fea-
tures and apply a classifier to every FAQ from the
database. This makes the response time linearly
dependent on the number of FAQs. For larger
databases, a preprocessing step to narrow down
the scope of the search would be required. To this
end, we could use a standard keyword-based re-
trieval engine, optimized for high recall. Unfortu-
nately, improving efficiency by precomputing the
features is impossible because it would require the
query to be known in advance.
6 Conclusion and Perspectives
We have described a FAQ retrieval engine for
Croatian. The engine uses a supervised retrieval
model trained on a FAQ test collection with bi-
nary relevance judgments. To bridge the notorious
lexical gap problem, we have employed a series of
features based on semantic textual similarity be-
tween the query and the FAQ. We have built a FAQ
test collection on which we have trained and evalu-
ated the model. On this test collection, our model
achieves a very good performance with an MRR
score of 0.47.
We discussed a number of open problems. Er-
ror analysis suggests that our models prefer the
lexical overlap features. Consequently, most er-
rors are caused by deceivingly high or low word
overlap. One way to address the former is to con-
sider not only words themselves, but also syntactic
structures. A simple way to do this is to use POS
patterns to detect similar syntactic structures. A
more sophisticated version could make use of de-
pendency relations obtained by syntactic parsing.
We have demonstrated that even a small,
domain-specific query expansion dictionary can
provide a considerable performance boost. An-
other venue of research could consider the auto-
matic methods for constructing a domain-specific
query expansion dictionary. As noted by a re-
viewer, one possibility would be to mine query
logs collected over a longer period of time, as em-
ployed in web search (Cui et al, 2002) and also
FAQ retrieval (Kim and Seo, 2006).
From a practical perspective, future work shall
focus on scaling up the system to large FAQ
databases and multi-user environments.
31
Acknowledgments
This work has been supported by the Ministry of
Science, Education and Sports, Republic of Croa-
tia under the Grant 036-1300646-1986. We thank
the reviewers for their constructive comments.
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics, pages 385?393. Association for
Computational Linguistics.
Robin D. Burke, Kristian J. Hammond, Vladimir Ku-
lyukin, Steven L. Lytinen, Noriko Tomuro, and Scott
Schoenberg. 1997. Question answering from fre-
quently asked question files: experiences with the
FAQ Finder system. AI magazine, 18(2):57.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/~cjlin/libsvm.
Danish Contractor, Govind Kothari, Tanveer A.
Faruquie, L. Venkata Subramaniam, and Sumit
Negi. 2010. Handling noisy queries in cross lan-
guage FAQ retrieval. In Proceedings of the EMNLP
2010, pages 87?96. Association for Computational
Linguistics.
Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying
Ma. 2002. Probabilistic query expansion using
query logs. In Proceedings of the 11th interna-
tional conference on World Wide Web, pages 325?
332. ACM.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American society for information science, 41(6).
Mladen Karan, Jan ?najder, and Bojana Dalbelo Ba?ic?.
2012. Distributional semantics approach to detect-
ing synonyms in Croatian language. In Information
Society 2012 - Eighth Language Technologies Con-
ference, pages 111?116.
Jaana Kek?l?inen. 2005. Binary and graded rele-
vance in IR evaluations?comparison of the effects
on ranking of IR systems. Information processing &
management, 41(5):1019?1033.
Harksoo Kim and Jungyun Seo. 2006. High-
performance FAQ retrieval using an automatic clus-
tering method of query logs. Information processing
& management, 42(3):650?661.
Govind Kothari, Sumit Negi, Tanveer A. Faruquie,
Venkatesan T. Chakaravarthy, and L. Venkata Sub-
ramaniam. 2009. SMS based interface for FAQ re-
trieval. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, volume 2, pages 852?860.
Alon Lavie and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine translation, 23(2-3):105?115.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval, 3(3):225?331.
Nikola Ljube?ic? and Toma? Erjavec. 2011. HrWaC
and SlWaC: compiling web corpora for Croatian and
Slovene. In Text, Speech and Dialogue, pages 395?
402. Springer.
Tomislav Lombarovic?, Jan ?najder, and Bojana Dal-
belo Ba?ic?. 2011. Question classification for a
Croatian QA system. In Text, Speech and Dialogue,
pages 403?410. Springer.
Steven Lytinen and Noriko Tomuro. 2002. The use
of question types to match questions in FAQ Finder.
In AAAI Spring Symposium on Mining Answers from
Texts and Knowledge Bases, pages 46?53.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,
Adrian Veres, Matthew K. Gray, et al 2011. Quan-
titative analysis of culture using millions of digitized
books. Science, 331(6014):176.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. Proceedings of
ACL-08: HLT, pages 236?244.
Alejandro Moreo, Maria Navarro, Juan L. Castro, and
Jose M. Zurita. 2012. A high-performance FAQ re-
trieval method using minimal differentiator expres-
sions. Knowledge-Based Systems.
Roberto Navigli and Paola Velardi. 2003. An anal-
ysis of ontology-based query expansion strategies.
In Proceedings of the 14th European Conference
on Machine Learning, Workshop on Adaptive Text
Extraction and Mining, Cavtat-Dubrovnik, Croatia,
pages 42?49.
Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2008.
How to make letor more useful and reliable. In Pro-
ceedings of the ACM Special Interest Group on In-
formation Retrieval 2008 Workshop on Learning to
Rank for Information Retrieval, pages 52?58.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In eprint
arXiv: cmp-lg/9511007, volume 1, page 11007.
32
Frane ?aric?, Goran Glava?, Mladen Karan, Jan ?na-
jder, and Bojana Dalbelo Ba?ic?. 2012. TakeLab:
systems for measuring semantic text similarity. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics, pages 441?448.
Association for Computational Linguistics.
Jan ?najder, Bojana Dalbelo Ba?ic?, and Marko Tadic?.
2008. Automatic acquisition of inflectional lexica
for morphological normalisation. Information Pro-
cessing & Management, 44(5).
Eriks Sneiders. 1999. Automated FAQ answering:
continued experience with shallow language under-
standing. In Question Answering Systems. Papers
from the 1999 AAAI Fall Symposium, pages 97?107.
Radu Soricut and Eric Brill. 2004. Automatic question
answering: beyond the factoid. In Proceedings of
HLT-NAACL, volume 5764.
Noriko Tomuro and Steven Lytinen. 2004. Retrieval
models and Q and A learning with FAQ files. New
Directions in Question Answering, pages 183?194.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceedings
of the 23rd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 200?207. ACM.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In SIGIR?94, pages 61?
69. Springer.
Ellen M. Voorhees. 2002. The philosophy of infor-
mation retrieval evaluation. In Evaluation of cross-
language information retrieval systems, pages 355?
370. Springer.
Chung-Hsien Wu, Jui-Feng Yeh, and Yu-Sheng Lai.
2006. Semantic segment extraction and matching
for internet FAQ retrieval. Knowledge and Data En-
gineering, IEEE Transactions on, 18(7):930?940.
Xiaobing Xue, Jiwoon Jeon, and W Bruce Croft. 2008.
Retrieval models for question and answer archives.
In Proceedings of the 31st annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 475?482. ACM.
33
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 43?47,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
GPKEX: Genetically Programmed Keyphrase Extraction
from Croatian Texts
Marko Bekavac and Jan S?najder
University of Zagreb, Faculty of Electrical Engineering and Computing
Text Analysis and Knowledge Engineering Lab
Unska 3, 10000 Zagreb, Croatia
{marko.bekavac2,jan.snajder}@fer.hr
Abstract
We describe GPKEX, a keyphrase extrac-
tion method based on genetic programming.
We represent keyphrase scoring measures
as syntax trees and evolve them to pro-
duce rankings for keyphrase candidates ex-
tracted from text. We apply and evalu-
ate GPKEX on Croatian newspaper arti-
cles. We show that GPKEX can evolve
simple and interpretable keyphrase scoring
measures that perform comparably to more
complex machine learning methods previ-
ously developed for Croatian.
1 Introduction
Keyphrases are an effective way of summariz-
ing document contents, useful for text categoriza-
tion, document management, and search. Unlike
keyphrase assignment, in which documents are as-
signed keyphrases from a predefined taxonomy,
keyphrase extraction selects phrases from the text
of the document. Extraction is preferred in cases
when a taxonomy is not available or when its con-
struction is not feasible, e.g., if the set of possible
keyphrases is too large or changes often. Manual
keyphrase extraction is extremely tedious and in-
consistent, thus methods for automatic keyphrase
extraction have attracted a lot of research interest.
In this paper we describe GPKEX, a keyphrase
extraction method based on genetic programming
(GP), an evolutionary optimization technique in-
spired by biological evolution (Koza and Poli,
1992). GP is similar to genetic algorithms except
that the individual solutions are expressions, rather
than values. We use GP to evolve keyphrase scor-
ing measures, represented as abstract syntax trees.
The advantage of using GP over black-box ma-
chine learning methods is in the interpretability of
the results: GP yields interpretable expressions,
revealing the relevant features and their relation-
ships, thus offering some insight into keyphrase
usage. Furthermore, GP can evolve simple scoring
measures, providing an efficient alternative to more
complex machine learning methods.
We apply GPKEX to Croatian language and eval-
uate it on a dataset of newspaper articles with man-
ually extracted keyphrases. Our results show that
GPKEX performs comparable to previous super-
vised and unsupervised approaches for Croatian,
but has the advantage of generating simple and
interpretable keyphrase scoring measures.
2 Related Work
Keyphrase extraction typically consist of two steps:
candidate extraction and candidate scoring. Su-
pervised approaches include decision tree models
(Turney, 1999; Ercan and Cicekli, 2007), na??ve
Bayes classifier (Witten et al, 1999; McCallum
and Nigam, 1998; Frank et al, 1999), and SVM
(Zhang et al, 2006). Unsupervised approaches
include clustering (Liu et al, 2009), graph-based
methods (Mihalcea and Tarau, 2004), and language
modeling (Tomokiyo and Hurst, 2003). Many
more methods were proposed and evaluated within
the SemEval shared task (Kim et al, 2010). Re-
cent approaches (Jiang et al, 2009; Wang and Li,
2011; Eichler and Neumann, 2010) acknowledge
keyphrase extraction as a highly subjective task and
frame it as a learning-to-rank problem.
Keyphrase extraction for Croatian has been ad-
dressed in both supervised and unsupervised set-
ting. Ahel et al (2009) use a na??ve Bayes clas-
sifier with phrase position and tf-idf (term fre-
quency/inverse document frequency) as features.
Saratlija et al (2011) use distributional seman-
tics to build topically related word clusters, from
which they extract keywords and expand them to
keyphrases. Mijic? et al (2010) use filtering based
on morphosyntactic tags followed by tf-idf scoring.
43
To the best of our knowledge, GPKEX is the
first application of GP to keyphrase extraction. Al-
though we essentially approach the problem as a
classification task (we train on binary relevance
judgments), GPKEX produces continuous-valued
scoring measures, thus keyphrases can eventually
be ranked and evaluated in a rank-based manner.
3 GPKEX
GPKEX (Genetically Programmed Keyphrase Ex-
traction) consists of two steps: keyphrase candi-
date extraction and the genetic programming of
keyphrase scoring measures (KSMs).1
3.1 Step 1: Keyphrase candidate extraction
Keyphrase candidate extraction starts with text pre-
processing followed by keyphrase feature extrac-
tion. A keyphrase candidate is any sequence of
words from the text that (1) does not span over
(sub)sentence boundaries and (2) matches any of
the predefined POS patterns (sequences of POS
tags). The POS patterns are chosen based on the
analysis of the training set (cf. Section 4).
After the candidates have been extracted, each
candidate is assigned 11 features. We distinguish
between three groups of features. The first group
are the frequency-based features: the relative term
frequency (the ratio between the number of phrase
occurrences in a document and the total number
of phrases in the document), inverse document fre-
quency (the ratio between the total number of doc-
uments in the training set and the number of doc-
uments in which the phrase occurs), and the tf-idf
value. These features serve to eliminate the irrele-
vant and non-discriminative phrases. The second
group are the position-based features: the position
of the first occurrence of a phrase in the text (i.e.,
the number of phrases in the text preceding the first
occurrence of the candidate phrase), the position
of the last occurrence, the occurrence in document
title, and the number of occurrences in the first, sec-
ond, and the last third of the document. These fea-
tures serve to capture the relation between phrase
relevance and the distribution of the phase within
the document. The last group of features concerns
the keyphrase surface form: its length and the num-
ber of discriminative words it contains (these being
defined as the 10 words from the document with
the highest tf-idf score).
1GPKEX is freely available for download from
http://takelab.fer.hr/gpkex
3.2 Step 2: Genetic programming
Genetic expressions. Each keyphrase scoring
measure (KSM) corresponds to one genetic expres-
sion, represented as a syntax tree (see Fig. 1). We
use the above-described keyphrase features as outer
nodes of an expression. For inner nodes we use
binary (+, ?, ?, and /) and unary operators (log ?,
??10, ?/10, 1/?). We randomly generate the initial
population of KSMs and use fitness-proportionate
selection to guide the evolution process.
Fitness function. The fitness function scores
KSMs according to their ability to extract cor-
rect keyphrases. We measure this by comparing
the extracted keyphrases against the gold-standard
keyphrases (cf. Section 4). We experimented with a
number of fitness functions; simple functions, such
as Precision at n (P@n) or Mean Reciprocal Rank
(MRR), did not give satisfactory results. Instead,
we define the fitness of a KSM s as
f(s) =
1
|D|
?
d?D
?
?
?
|Ckd |
minRank(Ckd )
Ckd 6= ?,
1
minRank(C?d )
otherwise
(1)
where D is the set of training documents, Ckd is
the set of correct keyphrases within top k-ranked
keyphrases extracted from document d ? D, and
minRank(Ckd ) is the highest rank (the smallest
number) of keyphrase from set Ckd . Parameter k
defines a cutoff threshold, i.e., keyphrase ranked be-
low rank k are discarded. If two KSMs extract the
same number of correct keyphrases in top k results,
the one with the highest-ranked correct keyphrase
will be scored higher. To ensure that the gradient of
the fitness function is non-zero, a KSM that extracts
no correct keyphrases within the first k results is
assigned a score based on the complete set of cor-
rectly extracted keyphrases (denoted C?d ). The fit-
ness scores are averaged over the whole document
collection. Based on preliminary experiments, we
set the cutoff value to k = 15.
Parsimony pressure. Supervised models often
face the problem of overfitting. In GP, overfitting is
typically controlled by parsimony pressure, a regu-
larization term that penalizes complex expressions.
We define the regularized fitness function as
freg =
f
1 +N/?
(2)
where f is the non-regularized fitness function
given by (1), N is the number of nodes in the ex-
pression, and parameter ? defines the strength of
44
parsimony pressure. Note that in both regularized
and non-regularized case we limit the size of an
expression to a maximum depth of 17, which is
often used as the limit (Riolo and Soule, 2009).
Crossover and mutation. Two expressions cho-
sen for crossover exchange subtrees rooted at ran-
dom nodes, resulting in a child expression with
parts from both parent expressions. We use a pop-
ulation of 500 expressions and limit the number
of generations to 50, as we observe that results
stagnate after that point. To retain the quality of
solution throughout the generations, we employ the
elitist strategy and copy the best-fitted individual
into the next generation. Moreover, we use muta-
tion to prevent early local optimum trapping. We
implement mutation as a randomly grown subtree
rooted at a randomly chosen node. Each expression
has a 5% probability of being mutated, with 10%
probability of mutation at inner nodes.
4 Evaluation
Data set and preprocessing. We use the dataset
developed by Mijic? et al (2010), comprising 1020
Croatian newspaper articles provided by the Croat-
ian News Agency. The articles have been manually
annotated by expert annotators, i.e., each document
has an associated list of keyphrases. The number of
extracted keyphrases per document varies between
1 and 7 (3.4 on average). The dataset is divided
in two parts: 960 documents each annotated by a
single annotator and 60 documents independently
annotated by eight annotators. We use the first part
for training and the second part for testing.
Based on dataset analysis, we chose the follow-
ing POS patterns for keyphrase candidate filtering:
N, AN, NN, NSN, V, U (N ? noun, A ? adjective, S
? preposition, V ? verb, U ? unknown). Although a
total of over 200 patterns would be needed to cover
all keyphrases from the training set, we use only
the six most frequent ones in order to reduce the
number of candidates. These patterns account for
cca. 70% of keyphrases, while reducing the num-
ber of candidates by cca. 80%. Note that we chose
to only extract keyphrases of three words or less,
thereby covering 93% of keyphrases. For lemmati-
zation and (ambiguous) POS tagging, we use the
inflectional lexicon from S?najder et al (2008), with
additional suffix removal after lemmatization.
Evaluation methodology. Keyphrase extraction
is a highly subjective task and there is no agreed-
upon evaluation methodology. Annotators are often
inconsistent: they extract different keyphrases and
also keyphrases of varying length. What is more,
an omission of a keyphrase by one of the annota-
tors does not necessarily mean that the keyphrase
is incorrect; it may merely indicate that it is less rel-
evant. To account for this, we use rank-based eval-
uation measures. As our method produces a ranked
list of keyphrases for each document, we can com-
pare this list against a gold-standard keyphrase
ranking for each document. We obtain the latter
by aggregating the judgments of all annotators; the
more annotators have extracted a keyphrase, the
higher its ranking will be.2 Following Zesch and
Gurevych (2009), we consider the morphological
variants when matching the keyphrases; however,
we do not consider partial matches.
To evaluate a ranked list of extracted keyphrases,
we use the generalized average precision (GAP)
measure proposed by Kishida (2005). GAP gener-
alizes average precision to multi-grade relevance
judgments: it takes into account both precision (all
correct items are ranked before all incorrect ones)
and the quality of ranking (more relevant items are
ranked before less relevant ones).
Another way of evaluating against keyphrases
extracted by multiple annotators is to consider
the different levels of agreement. We consider as
strong agreement the cases in which a keyphrase
is extracted by at least five annotators, and as
weak agreement the cases in which at least two
annotators have extracted a keyphrase. For both
agreement levels separately, we compare the ex-
tracted keyphrases against the manually extracted
keyphrases using rank-based IR measures of Pre-
cision at Rank 10 (P@10) and Recall at Rank 10
(R@10). Because GP is a stochastic algorithm, to
account for randomness we made 30 runs of each
experiment and report the average scores. On these
samples, we use the unpaired t-test to determine the
significance in performance differences. As base-
line to compare against GPKEX, we use keyphrase
extraction based on tf-idf scores (with the same
preprocessing and filtering setup as for GPKEX).
Tested configurations. We tested four evolution
configurations. Configuration A uses the param-
eter setting described in Section 3.2, but without
parsimony pressure. Configurations B and C use
parsimony pressure defined by (2), with ? = 1000
2The annotated dataset is available under CC BY-NC-SA
license from http://takelab.fer.hr/gpkex
45
Strong agreement Weak agreement
Config. GAP P@10 R@10 P@10 R@10
A 13.0 8.3 28.7 28.7 8.4
B 12.8 8.2 30.2 28.4 8.5
C 12.5 7.7 27.3 27.3 7.7
D 9.9 5.1 25.9 20.4 7.3
tf-idf 7.4 5.8 22.3 21.5 12.4
UKE 6.0 5.8 32.6 15.3 15.8
Table 1: Keyphrase ranking results.
and ? = 100, respectively. Configuration D is
similar to A, but uses all POS patterns attested for
keyphrases in the dataset.
Results. Results are shown in Table 1. Configu-
rations A and B perform similarly across all evalu-
ation measures (pairwise differences are not signif-
icant at p<0.05, except for R@10) and outperform
the baseline (differences are significant at p<0.01).
Configuration C is outperformed by configuration
A (differences are significant at p<0.05). Config-
uration D outperforms the baseline, but is outper-
formed by other configurations (pairwise differ-
ences in GAP are significant at p<0.05), indicating
that conservative POS filtering is beneficial. Since
A and B perform similar, we conclude that apply-
ing parsimony pressure in our case only marginally
improved GAP (although it has reduced KSM size
from an average 30 nodes for configuration A to
an average of 20 and 9 nodes for configurations B
and C, respectively). We believe there are two rea-
sons for this: first, the increase in KSM complexity
also increases the probability that the KSM will be
discarded as not computable (e.g., the right subtree
of a ?/? node evaluates to zero). Secondly, our fit-
ness function is perhaps not fine-grained enough
to allow more complex KSMs to emerge gradu-
ally, as small changes in keyphrase scores do not
immediately affect the value of the fitness function.
In absolute terms, GAP values are rather low.
This is mostly due to wrong ranking, rather than the
omission of correct phrases. Furthermore, the pre-
cision for strong agreement is considerably lower
than for weak agreement. This indicates that GP-
KEX often assigns high scores to less relevant
keyphrases. Both deficiencies may be attributed
to the fact that we do not learn to rank, but train on
dataset with binary relevance judgments.
The best-performing KSM from configuration
A is shown in Fig. 1 (simplified form). Length is
the length of the phrase, First is the position of the
1
Tf?Tf + Tfidf ? (Length + First) +
Rare
log(log Length)
Figure 1: The best-performing KSM expression.
first occurrence, and Rare is the number of discrim-
inative words in a phrase (cf. Section 3.1). Tfidf,
First, and Rare features seem to be positively cor-
related with keyphraseness. This particular KSM
extracts on average three correct keyphrases (weak
agreement) within the first 10 results.
Our results are not directly comparable to pre-
vious work for Croatian (Ahel et al, 2009; Mijic?
et al, 2010; Saratlija et al, 2011) because we use
a different dataset and/or evaluation methodology.
However, to allow for an indirect comparison, we
re-evaluated the results of unsupervised keyphrase
extraction (UKE) from Saratlija et al (2011); we
show the result in the last row of Table 1. GPKEX
(configuration A) outperforms UKE in terms of
precision (GAP and P@10), but performs worse
in terms of recall. In terms of F1@10 (harmonic
mean of P@10 and R@10), GPKEX performs bet-
ter than UKE at the strong agreement level (12.9
vs. 9.9), but worse at the weak agreement level
(13.0 vs. 15.6). For comparison, Saratlija et al
(2011) report UKE to be comparable to supervised
method from Ahel et al (2009), but better than the
tf-idf extraction method from Mijic? et al (2010).
5 Conclusion
GPKEX uses genetically programmed scoring mea-
sures to assign rankings to keyphrase candidates.
We evaluated GPKEX on Croatian texts and showed
that it yields keyphrase scoring measures that per-
form comparable to other machine learning meth-
ods developed for Croatian. Thus, scoring mea-
sures evolved by GPKEX provide an efficient alter-
native to these more complex models. The focus of
this work was on Croatian, but our method could
easily be applied to other languages as well.
We have described a preliminary study. The next
step is to apply GPKEX to directly learn keyphrase
ranking. Using additional (e.g., syntactic) features
might further improve the results.
46
Acknowledgments
This work has been supported by the Ministry of
Science, Education and Sports, Republic of Croatia
under the Grant 036-1300646-1986. We thank the
reviewers for their constructive comments.
References
Renee Ahel, B Dalbelo Bas?ic, and Jan S?najder. 2009.
Automatic keyphrase extraction from Croatian news-
paper articles. The Future of Information Sciences,
Digital Resources and Knowledge Sharing, pages
207?218.
Kathrin Eichler and Gu?nter Neumann. 2010. DFKI
KeyWE: Ranking keyphrases extracted from scien-
tific articles. In Proceedings of the 5th international
workshop on semantic evaluation, pages 150?153.
Association for Computational Linguistics.
Gonenc Ercan and Ilyas Cicekli. 2007. Using lexical
chains for keyword extraction. Information Process-
ing & Management, 43(6):1705?1714.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of IJCAI ?99, pages 668?673. Morgan Kauf-
mann Publishers Inc.
Xin Jiang, Yunhua Hu, and Hang Li. 2009. A ranking
approach to keyphrase extraction. In Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 756?757. ACM.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. SemEval-2010 task 5: Au-
tomatic keyphrase extraction from scientific articles.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 21?26. Association for
Computational Linguistics.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. Na-
tional Institute of Informatics.
John R. Koza and Riccardo Poli. 1992. Genetic Pro-
gramming: On the programming of computers by
Means of Natural Selection. MIT Press.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of EMNLP
2009, pages 257?266, Singapore. ACL.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Na??ve Bayes text classi-
fication. In AAAI-98 workshop on learning for text
categorization, pages 41?48. AAAI Press.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of
EMNLP, volume 4. Barcelona, Spain.
Jure Mijic?, B Dalbelo Bas?ic, and Jan S?najder. 2010.
Robust keyphrase extraction for a large-scale Croa-
tian news production system. In Proceedings of
FASSBL, pages 59?66.
Rick Riolo and Terence Soule. 2009. Genetic Pro-
gramming Theory and Practice VI. Springer.
Josip Saratlija, Jan S?najder, and Bojana Dalbelo Bas?ic?.
2011. Unsupervised topic-oriented keyphrase ex-
traction and its application to Croatian. In Text,
Speech and Dialogue, pages 340?347. Springer.
Jan S?najder, Bojana Dalbelo Bas?ic?, and Marko Tadic?.
2008. Automatic acquisition of inflectional lexica
for morphological normalisation. Information Pro-
cessing & Management, 44(5):1720?1731.
Takashi Tomokiyo and Matthew Hurst. 2003. A
language model approach to keyphrase extraction.
In Proceedings of the ACL 2003 workshop on
Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 33?40. Association for
Computational Linguistics.
Peter Turney. 1999. Learning to extract keyphrases
from text. Technical report, National Research
Council, Institute for In- formation Technology.
C. Wang and S. Li. 2011. CoRankBayes: Bayesian
learning to rank under the co-training framework
and its application in keyphrase extraction. In Pro-
ceedings of the 20th ACM international conference
on Information and knowledge management, pages
2241?2244. ACM.
Ian H Witten, Gordon W Paynter, Eibe Frank, Carl
Gutwin, and Craig G Nevill-Manning. 1999. Kea:
Practical automatic keyphrase extraction. In Pro-
ceedings of the fourth ACM conference on Digital
libraries, pages 254?255. ACM.
Torsten Zesch and Iryna Gurevych. 2009. Approxi-
mate matching for evaluating keyphrase extraction.
In Proceedings of the 7th International Conference
on Recent Advances in Natural Language Process-
ing, pages 484?489.
Kuo Zhang, Hui Xu, Jie Tang, and Juanzi Li. 2006.
Keyword extraction using support vector machine.
In Advances in Web-Age Information Management,
volume 4016 of LNCS, pages 85?96. Springer Berlin
/ Heidelberg.
47
Proceedings of the TextGraphs-8 Workshop, pages 1?5,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Event-Centered Information Retrieval Using Kernels on Event Graphs
Goran Glavas? and Jan S?najder
University of Zagreb
Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
{goran.glavas,jan.snajder}@fer.hr
Abstract
Traditional information retrieval models as-
sume keyword-based queries and use unstruc-
tured document representations. There is
an abundance of event-centered texts (e.g.,
breaking news) and event-oriented informa-
tion needs that often involve structure that
cannot be expressed using keywords. We
present a novel retrieval model that uses a struc-
tured event-based representation. We struc-
ture queries and documents as graphs of event
mentions and employ graph kernels to measure
the query-document similarity. Experimental
results on two event-oriented test collections
show significant improvements over state-of-
the-art keyword-based models.
1 Introduction
The purpose of an information retrieval (IR) system is
to retrieve the documents relevant to user?s informa-
tion need expressed in the form of a query. Many in-
formation needs are event-oriented, while at the same
time there exists an abundance of event-centered texts
(e.g., breaking news, police reports) that could satisfy
these needs. Furthermore, event-oriented information
needs often involve structure that cannot easily be
expressed with keyword-based queries (e.g., ?What
are the countries that President Bush has visited and
in which has his visit triggered protests??). Tradi-
tional IR models (Salton et al, 1975; Robertson and
Jones, 1976; Ponte and Croft, 1998) rely on shal-
low unstructured representations of documents and
queries, making no use of syntactic, semantic, or
discourse level information. On the other hand, mod-
els utilizing structured event-based representations
have not yet proven useful in IR. However, signifi-
cant advances in event extraction have been achieved
in the last decade as the result of standardization ef-
forts (Pustejovsky et al, 2003) and shared evaluation
tasks (Verhagen et al, 2010), renewing the interest
in structured event-based text representations.
In this paper we present a novel retrieval model
that relies on structured event-based representation
of text and addresses event-centered queries. We
define an event-oriented query as a query referring
to one or more real-world events, possibly includ-
ing their participants, the circumstances under which
the events occurred, and the temporal relations be-
tween the events. We account for such queries by
structuring both documents and queries into event
graphs (Glavas? and S?najder, 2013b). The event
graphs are built from individual event mentions ex-
tracted from text, capturing their protagonists, times,
locations, and temporal relations. To measure the
query-document similarity, we compare the corre-
sponding event graphs using graph kernels (Borg-
wardt, 2007). Experimental results on two news story
collections show significant improvements over state-
of-the-art keyword-based models. We also show that
our models are especially suitable for retrieval from
collections containing topically similar documents.
2 Related Work
Most IR systems are a variant of the vector space
model (Salton et al, 1975), probabilistic model
(Robertson and Jones, 1976), or language model
(Ponte and Croft, 1998), which do not account for
associations between query terms. Recent models in-
troduce co-occurrence-based (Park et al, 2011) and
syntactic (Shinzato et al, 2012) dependencies. How-
ever, these dependencies alone in most cases cannot
capture in sufficient detail the semantics of events.
A more comprehensive set of dependencies can be
modeled with graph-based representations. Graph-
1
based IR approaches come in two flavors: (1) the
entire document collection is represented as a sin-
gle graph in which queries are inserted as additional
vertices (Mihalcea and Tarau, 2004); (2) each query
and each document are represented as graphs of con-
cepts, and the relevance of a document for a query is
determined by comparing the corresponding graphs
(Montes-y Go?mez et al, 2000). Our approach fits
into the latter group but we represent documents as
graphs of events rather than graphs of concepts. In
NLP, graph kernels have been used for question type
classification (Suzuki, 2005), cross-lingual retrieval
(Noh et al, 2009), and recognizing news stories on
the same event (Glavas? and S?najder, 2013b).
Event-based IR is addressed explicitly by Lin et
al. (2007), who compare predicate-argument struc-
tures extracted from queries to those extracted from
documents. However, queries have to be manually
decomposed into semantic roles and can contain only
a single predicate. Kawahara et al (2013) propose a
similar approach and demonstrate that ranking based
on semantic roles outperforms ranking based on syn-
tactic dependencies. Both these approaches target the
problem of syntactic alternation but do not consider
the queries made of multiple predicates, such as those
expressing temporal relations between events.
3 Kernels on Event Graphs
Our approach consists of two steps. First, we con-
struct event graphs from both the document and the
query. We then use a graph kernel to measure the
query-document similarity and rank the documents.
3.1 Event Graphs
An event graph is a mixed graph in which vertices rep-
resent the individual event mentions and edges repre-
sent temporal relations between them. More formally,
an event graph is a tuple G = (V,E,A,m, r), where
V is the set of vertices, E is the set of undirected
edges, A is the set of directed edges, m : V ? M
maps the vertices to event mentions, and r : E ? R
assigns temporal relations to edges.
We use a generic representation of a factual event
mention, which consists of an event anchor and event
arguments of four coarse types (agent, target, time,
and location) (Glavas? and S?najder, 2013a; Glavas?
and S?najder, 2013b). We adopt the set of temporal
relations used in TempEval-2 (Verhagen et al, 2010)
(before, after, and overlap), with additional temporal
equivalence relation (equal).
To build an event graph, we first extract the event
mentions and then extract the temporal relations be-
tween them. To extract the event anchors, we use
a supervised model based on a rich feature set pro-
posed by Glavas? and S?najder (2013b), performing
at 80% F1-score. We then use a robust rule-based
approach from Glavas? and S?najder (2013a) to extract
event arguments. Finally, we extract the temporal
relations using a supervised model with a rich fea-
ture set proposed by Glavas? and S?najder (2013b).
Relation classification performs at 60% F1-score.
To compute the product graph kernels, we must
identify event mentions from the query that corefer
with mentions from the document. To this end, we
employ the model from Glavas? and S?najder (2013a),
which compares the anchors and four types of argu-
ments between a pair of event mentions. The model
performs at 67% F-score on the EventCorefBank
dataset (Bejan and Harabagiu, 2008).
3.2 Product Graph Kernels
Graph kernels provide an expressive measure of sim-
ilarity between graphs (Borgwardt, 2007). In this
work, we use product graph kernel (PGK), a type of
random walk graph kernel that counts the common
walks between two graphs (Ga?rtner et al, 2003).
Product graph. The graph product of two labeled
graphs, G and G
?
, denoted GP = G?G?, is a graph
with the vertex set
VP =
{
(v, v?) | v ? VG, v
? ? VG? , ?(v, v
?)
}
where predicate ?(v, v?) holds iff vertices v and v? are
identically labeled (Hammack et al, 2011). Vertices
of event graphs have the same label if the event men-
tions they denote corefer. The edge set of the product
is conditioned on the type of the graph product. In the
tensor product, an edge exists in the product iff the
corresponding edges exist in both input graphs and
have the same label, i.e., denote the same temporal
relation. In the conormal product, an edge is intro-
duced iff the corresponding edge exists in at least one
input graph. A conormal product may compensate
for omitted temporal relations in the input graphs but
may introduce spurious edges that do not represent
2
(a) Query graph (b) Document graph (c) Tensor product (d) Conormal product
Figure 1: Examples of event graphs and their products
true overlap between queries and documents. Fig. 1
shows an example of input graphs and their products.
PGK computation. The PGK for input graphs G
and G? is computed as
kPG(G,G
?) =
|VP |?
i,j=1
[(I ? ?AP )
?1]ij
provided ? < 1/d , where d is the maximum vertex
degree in the product graph GP with the adjacency
matrix AP . In experiments, we set ? to 1/(d+ 1) .
PGK suffers from tottering (Mahe? et al, 2005), a phe-
nomenon due to the repetition of edges in a random
walk. A walk that totters between neighboring ver-
tices produces an unrealistically high similarity score.
To prevent tottering between neighboring vertices,
Mahe? et al (2005) transform the input graphs before
computing the kernel score on their product: each
edge (vi, vj) is converted into a vertex ve; the edge it-
self gets replaced with edges (ve, vi) and (ve, vj). We
experiment with Mahe? extension for PGK, account-
ing for the increased probability of one-edge-cycle
tottering due the small size of query graphs.
4 Experiments
Test Collections and Queries. To the best of our
knowledge, there is no standard test collection avail-
able for event-centered IR that we could use to evalu-
ate our models. Thus, we decided to build two such
test collections, with 50 queries each: (1) a general
collection of topically diverse news stories and (2) a
topic-specific collection of news on Syria crisis. The
first collection contains 25,948 news stories obtained
from EMM News Brief, an online news clustering
service.1 For the topic-specific collection, we se-
lected from the general collection 1387 documents
that contain the word ?Syria? or its derivations.
1http://emm.newsbrief.eu
General collection (news stories)
q1: An ICT giant purchased the phone maker after the
government approved the acquisition
q2: The warship tried to detain Chinese fishermen but
was obstructed by the Chinese vessels
Topic-specific collection (Syria crisis)
q3: Syrian forces killed civilians, torched houses, and
ransacked stores, overrunning a farmer village
q4: Rebels murdered many Syrian soldiers and the gov-
ernment troops blasted the town in central Syria
Table 1: Example queries from the test collection
For each collection we asked an annotator to com-
pile 50 queries. She was instructed to select at ran-
dom a document from the collection, read the docu-
ment carefully, and compile at least one query con-
sisting of at least two event mentions, in such a way
that the selected document is relevant for the query.
Example queries are shown in Table 1. For instance,
query q1 (whose corresponding event graph is shown
in Fig. 1a) was created based on the following docu-
ment (whose event graph is shown in Fig. 1b):
Google Inc. won approval from Chinese regula-
tors for its $12.5 billion purchase of Motorola
Mobility Holdings Inc., clearing a final hurdle
for a deal that boosts its patents portfolio. . .
Relevance judgments. To create relevance judg-
ments, we use the standard IR pooling method with
two baseline retrieval models ? a TF-IDF weighted
vector space model (VSM) and a language model.
Our graph-based model was not used for pooling be-
cause of time limitations (note that this favors the
baseline models because pool-based evaluation is
biased against models not contributing to the pool
(Bu?ttcher et al, 2007)). Given that EMM News Brief
builds clusters of related news and that most EMM
3
Collection
Model General Specific
Baselines TF-IDF VSM 0.335 0.199
Hiemstra LM 0.300 0.175
In expC2 0.341 0.188
DFR BM25 0.332 0.192
Graph-based Tensor 0.502 0.407
Conormal 0.434 0.359
Mahe? Tensor 0.497 0.412
Mahe? Conormal 0.428 0.362
Table 2: Retrieval performance (MAP)
clusters contain less than 50 news stories, we esti-
mate that there are at most 50 relevant documents per
query. To get an even better estimate of recall, for
each query we pooled the union of top 75 documents
retrieved by each of the two baseline models.
One annotator made the relevance judgments for
all queries. We asked another annotator to provide
judgments for two randomly chosen queries and ob-
tained perfect agreement, which confirmed our intu-
ition that determining relevance for complex event-
centered queries is not difficult. The average number
of relevant documents per query in the general and
topic-specific collection is 12 and 8, respectively.2
Results. Table 2 shows the mean average preci-
sion (MAP) on both test collections for four graph
kernel-based models (tensor/conormal product and
with/without Mahe? extension). We compare our
models to baselines from the three traditional IR
paradigms: a TF-IDF-weighted cosine VSM, the
language model of Hiemstra (2001), and the best-
performing models from the probabilistic Divergence
from Randomness (DFR) framework (In expC2 and
DFR BM25) (Amati, 2003; Ounis et al, 2006). We
evaluate these models using the Terrier IR platform.3
Overall, all models perform worse on the topic-
specific collection, in which all documents are topi-
cally related. Our graph kernel models outperform
all baseline models (p<0.01 for tensor models and
p<0.05 for conormal models; paired student?s t-test)
on both collections, with a wider margin on topic-
specific than on the general collection. This result
2Available at http://takelab.fer.hr/data
3http://terrier.org
[?1;?0.1](?0.1; 0] (0; 0.1] (0.1; 0.3] (0.3; 1]
0
5
10
15
Figure 2: Histogram of AP differences
suggests that the graph-based models are especially
suitable for retrieval over topic-specific collections.
There is no significant difference between the ten-
sor product and conormal product models, indicating
that the conormal product introduces spurious edges
more often than it remedies for incorrect extraction
of temporal relations. The performance differences
due to Mahe? extension are not significant, providing
no conclusive evidence on the effect of tottering.
To gain more insights into the performance of our
event graph-based model, we analyzed per query
differences in average precision between our best-
performing model (Tensor) and the best-performing
baseline (In expC2) on queries from the general col-
lection. Fig. 2 shows the histogram of differences.
Our graph kernel-based model outperforms the base-
line on 42 out of 50 queries. A closer inspection
of the eight queries on which our model performs
worse than the baseline reveals that this is due to (1)
an important event mention not being extracted from
the query (2 cases) or a (2) failure in coreference
resolution between an event mention from the query
and a mention from the document (6 cases).
5 Conclusion and Perspectives
We presented a graph-based model for event-centered
information retrieval. The model represents queries
and documents as event graphs and ranks the docu-
ments based on graph kernel similarity. The experi-
ments demonstrate that for event-based queries our
graph-based model significantly outperforms state-of-
the-art keyword-based retrieval models. Our models
are especially suitable for topic-specific collections,
on which traditional IR models perform poorly.
An interesting topic for further research is the ex-
tension of the model with other types of dependen-
cies between events, such as entailment, causality,
4
and structural relations. Another direction concerns
the effective integration of event graph-based and
keyword-based models. We will also consider ap-
plications of event graphs on other natural language
processing tasks such as text summarization.
Acknowledgments. This work has been supported
by the Ministry of Science, Education and Sports,
Republic of Croatia under the Grant 036-1300646-
1986. We thank the reviewers for their comments.
References
Giambattista Amati. 2003. Probability models for infor-
mation retrieval based on divergence from randomness.
Ph.D. thesis, University of Glasgow.
Cosmin Adrian Bejan and Sanda Harabagiu. 2008. A
linguistic resource for discovering event structures and
resolving event coreference. In Proc. of the LREC
2008.
Karsten Michael Borgwardt. 2007. Graph Kernels. Ph.D.
thesis, Ludwig-Maximilians-Universita?t Mu?nchen.
Stefan Bu?ttcher, Charles LA Clarke, Peter CK Yeung, and
Ian Soboroff. 2007. Reliable information retrieval
evaluation with incomplete and biased judgements. In
Proc. of the ACM SIGIR, pages 63?70. ACM.
Thomas Ga?rtner, Peter Flach, and Stefan Wrobel. 2003.
On graph kernels: Hardness results and efficient alterna-
tives. In Learning Theory and Kernel Machines, pages
129?143. Springer.
Goran Glavas? and Jan S?najder. 2013a. Exploring coref-
erence uncertainty of generically extracted event men-
tions. In Proc. of the CICLing 2013, pages 408?422.
Springer.
Goran Glavas? and Jan S?najder. 2013b. Recognizing iden-
tical events with graph kernels. In Proc. of the ACL
2013, pages 797?803.
Richard Hammack, Wilfried Imrich, and Sandi Klavz?ar.
2011. Handbook of Product Graphs. Discrete Mathe-
matics and Its Applications. CRC Press.
Djoerd Hiemstra. 2001. Using language models for infor-
mation retrieval. Taaluitgeverij Neslia Paniculata.
Daisuke Kawahara, Keiji Shinzato, Tomohide Shibata, and
Sadao Kurohashi. 2013. Precise information retrieval
exploiting predicate-argument structures. In Proc. of
the IJCNLP 2013. In press.
Chia-Hung Lin, Chia-Wei Yen, Jen-Shin Hong, Samuel
Cruz-Lara, et al 2007. Event-based textual document
retrieval by using semantic role labeling and corefer-
ence resolution. In IADIS International Conference
WWW/Internet 2007.
Pierre Mahe?, Nobuhisa Ueda, Tatsuya Akutsu, Jean-Luc
Perret, and Jean-Philippe Vert. 2005. Graph kernels
for molecular structure-activity relationship analysis
with support vector machines. Journal of Chemical
Information and Modeling, 45(4):939?951.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing order into texts. In Proc. of the EMNLP 2004,
volume 4. Barcelona, Spain.
Manuel Montes-y Go?mez, Aurelio Lo?pez-Lo?pez, and
Alexander Gelbukh. 2000. Information retrieval with
conceptual graph matching. In Database and Expert
Systems Applications, pages 312?321. Springer.
Tae-Gil Noh, Seong-Bae Park, Hee-Geun Yoon, Sang-Jo
Lee, and Se-Young Park. 2009. An automatic transla-
tion of tags for multimedia contents using folksonomy
networks. In Proc. of the ACM SIGIR 2009, pages
492?499. ACM.
Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He,
Craig Macdonald, and Christina Lioma. 2006. Terrier:
A high performance and scalable information retrieval
platform. In Proceedings of the OSIR Workshop, pages
18?25.
Jae Hyun Park, W Bruce Croft, and David A Smith. 2011.
A quasi-synchronous dependence model for informa-
tion retrieval. In Proc. of the 20th ACM International
Conference on Information and Knowledge Manage-
ment, pages 17?26. ACM.
Jay Ponte and Bruce Croft. 1998. A language modeling
approach to information retrieval. In Proc. of the ACM
SIGIR, pages 275?281. ACM.
James Pustejovsky, Jose? Castano, Robert Ingria, Roser
Sauri, Robert Gaizauskas, Andrea Setzer, Graham Katz,
and Dragomir Radev. 2003. TimeML: Robust specifi-
cation of event and temporal expressions in text. New
Directions in Question Answering, 3:28?34.
Stephen E Robertson and K Sparck Jones. 1976. Rele-
vance weighting of search terms. Journal of the Ameri-
can Society for Information science, 27(3):129?146.
Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975.
A vector space model for automatic indexing. Commu-
nications of the ACM, 18(11):613?620.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
and Sadao Kurohashi. 2012. Tsubaki: An open search
engine infrastructure for developing information ac-
cess methodology. Journal of Information Processing,
20(1):216?227.
Jun Suzuki. 2005. Kernels for structured data in natural
language processing. Doctor Thesis, Nara Institute of
Science and Technology.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 Task 13:
TempEval-2. In Proc. of the SemEval 2010, pages
57?62.
5
